<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.993977">
A Joint Language Model With Fine-grain Syntactic Tags
</title>
<author confidence="0.993541">
Denis Filimonov1
</author>
<affiliation confidence="0.99316825">
1Laboratory for Computational Linguistics
and Information Processing
Institute for Advanced Computer Studies
University of Maryland, College Park
</affiliation>
<email confidence="0.996477">
den@cs.umd.edu
</email>
<sectionHeader confidence="0.994765" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999885333333333">
We present a scalable joint language
model designed to utilize fine-grain syn-
tactic tags. We discuss challenges such
a design faces and describe our solutions
that scale well to large tagsets and cor-
pora. We advocate the use of relatively
simple tags that do not require deep lin-
guistic knowledge of the language but pro-
vide more structural information than POS
tags and can be derived from automati-
cally generated parse trees – a combina-
tion of properties that allows easy adop-
tion of this model for new languages. We
propose two fine-grain tagsets and evalu-
ate our model using these tags, as well as
POS tags and SuperARV tags in a speech
recognition task and discuss future direc-
tions.
</bodyText>
<sectionHeader confidence="0.998785" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999921">
In a number of language processing tasks, particu-
larly automatic speech recognition (ASR) and ma-
chine translation (MT), there is the problem of se-
lecting the best sequence of words from multiple
hypotheses. This problem stems from the noisy
channel approach to these applications. The noisy
channel model states that the observed data, e.g.,
the acoustic signal, is the result of some input
translated by some unknown stochastic process.
Then the problem of finding the best sequence of
words given the acoustic input, not approachable
directly, is transformed into two separate models:
</bodyText>
<equation confidence="0.884805333333333">
argmax p(wn1 |A) = argmax p(A|wn1) · p(wn1)
wi wi
(1)
</equation>
<bodyText confidence="0.898387">
where A is the acoustic signal and wn1 is a se-
quence of n words. p(A|wn1) is called an acoustic
</bodyText>
<author confidence="0.547916">
Mary Harper1,2
</author>
<affiliation confidence="0.617802">
2Human Language Technology
Center of Excellence
Johns Hopkins University
</affiliation>
<email confidence="0.897908">
mharper@umiacs.umd.edu
</email>
<bodyText confidence="0.99848075">
model and p(wn1) is the language model1.
Typically, these applications use language mod-
els that compute the probability of a sequence in a
generative way:
</bodyText>
<equation confidence="0.807889">
p(wi|wi−1
1 )
</equation>
<bodyText confidence="0.994962">
Approximation is required to keep the parameter
space tractable. Most commonly the context is re-
duced to just a few immediately preceding words.
This type of model is called an ngram model:
p(wi|wi−1
</bodyText>
<equation confidence="0.9656685">
1 ) ≈ p(wi|wi−1
i−n+1)
</equation>
<bodyText confidence="0.999777909090909">
Even with limited context, the parameter space can
be quite sparse and requires sophisticated tech-
niques for reliable probability estimation (Chen
and Goodman, 1996). While the ngram models
perform fairly well, they are only capable of cap-
turing very shallow knowledge of the language.
There is extensive literature on a variety of
methods that have been used to imbue models
with syntactic and semantic information in differ-
ent ways. These methods can be broadly catego-
rized into two types:
</bodyText>
<listItem confidence="0.9791101">
• The first method uses surface words within
its context, sometimes organizing them into
deterministic classes. Models of this type in-
clude: (Brown et al., 1992; Zitouni, 2007),
which use semantic word clustering, and
(Bahl et al., 1990), which uses variable-
length context.
• The other method adds stochastic variables
to express the ambiguous nature of surface
words2. To obtain the probability of the next
</listItem>
<footnote confidence="0.89802">
1Real applications use argmax.. p(A|wl) )·p(w�1 )�·n0
</footnote>
<note confidence="0.4761225">
instead of Eq. 1, where α and ,Q are set to optimize a heldout
set.
</note>
<footnote confidence="0.993573">
2These variables have to be predicted by the model.
</footnote>
<equation confidence="0.988231333333333">
n
p(wn1) =
i=1
</equation>
<page confidence="0.96008">
1114
</page>
<note confidence="0.99655">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1114–1123,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.9963865">
word we need to sum over all assignments of
the stochastic variables, as in Eq. 2.
</bodyText>
<equation confidence="0.990345">
p(witi|wi−1
1 ti−1
1 ) (2)
i-1 i−1
Et1...ti p(witi  |w1 t1 )p(wZ 1 ti−1
1 )
�t1...ti−1
p(wi−1
1 ti−1
1 )
</equation>
<bodyText confidence="0.995925133333333">
Models of this type, which we call joint
models since they essentially predict joint
events of words and some random vari-
able(s), include (Chelba and Jelinek, 2000)
which used POS tags in combination with
“parser instructions” for constructing a full
parse tree in a left-to-right manner; (Wang
et al., 2003) used SuperARVs (complex tu-
ples of dependency information) without re-
solving the dependencies, thus called almost
parsing; (Niesler and Woodland, 1996; Hee-
man, 1999) utilize part of speech (POS) tags.
Note that some models reduce the context by
making the following approximation:
p(witi|wi−1
</bodyText>
<equation confidence="0.989813666666667">
1 ti−1
1 ) ^ p(wi|ti) p(ti|ti−1
1 ) (3)
</equation>
<bodyText confidence="0.999819045454545">
thus, transforming the problem into a stan-
dard HMM application. However, these
models perform poorly and have only been
able to improve over the ngram model when
interpolated with it (Niesler and Woodland,
1996).
Although joint models have the potential
to better express variability in word usage
through the introduction of additional latent
variables, they do not necessarily perform
better because the increased dimensionality
of the context substantially increases the al-
ready complex problem of parameter estima-
tion. The complexity of the space also makes
computation of the probability a challenge
because of space and time constraints. This
makes the choice of the random variables a
matter of utmost importance.
The model presented in this paper has some el-
ements borrowed from prior work, notably (Hee-
man, 1999; Xu and Jelinek, 2004), while others
are novel.
</bodyText>
<subsectionHeader confidence="0.99885">
1.1 Paper Outline
</subsectionHeader>
<bodyText confidence="0.9967985">
The message we aim to deliver in this paper can
be summarized in two theses:
</bodyText>
<listItem confidence="0.8879006875">
• Use fine-grain syntactic tags in a joint LM.
We propose a joint language model that can
be used with a variety of tagsets. In Section
2, we describe those that we used in our ex-
periments. Rather than tailoring our model to
these tagsets, we aim for flexibility and pro-
pose an information theoretic framework for
quick evaluation for tagsets, thus simplifying
the creation of new tagsets. We show that
our model with fine-grain tagsets outperform
the coarser POS model, as well as the ngram
baseline, in Section 5.
• Address the challenges that arise in a joint
language model with fine-grain tags. While
the idea of using joint language modeling is
not novel (Chelba and Jelinek, 2000; Hee-
</listItem>
<bodyText confidence="0.895147727272727">
man, 1999), nor is the idea of using fine-grain
tags (Bangalore, 1996; Wang et al., 2003),
none of prior papers focus on the issues that
arise from the combination of joint language
modeling with fine-grain tags, both in terms
of reliable parameter estimation and scalabil-
ity in the face of the increased computational
complexity. We dedicate Sections 3 and 4 to
this problem.
In Section 6, we summarize conclusions and lay
out directions for future work.
</bodyText>
<sectionHeader confidence="0.977388" genericHeader="method">
2 Structural Information
</sectionHeader>
<bodyText confidence="0.999881454545454">
As we have mentioned, the selection of the ran-
dom variable in Eq. 2 is extremely important for
the performance of the model. On one hand, we
would like for this variable to provide maximum
information. On the other hand, as the number of
parameters grow, we must address reliable param-
eter estimation in the face of sparsity, as well as
increased computational complexity. In the fol-
lowing section we will compare the use of Super-
ARVs, POS tags, and other structural tags derived
from parse trees.
</bodyText>
<subsectionHeader confidence="0.96141">
2.1 POS Tags
</subsectionHeader>
<bodyText confidence="0.999044">
Part-of-speech tags can be easily obtained for
unannotated data using off-the-shelf POS taggers
or PCFG parsers. However, the amount of infor-
mation these tags typically provide is very limited,
</bodyText>
<equation confidence="0.906695">
�
p(wi|wi−1
1 ) =
t1...ti
</equation>
<page confidence="0.970683">
1115
</page>
<figureCaption confidence="0.999909">
Figure 1: A parse tree example
</figureCaption>
<bodyText confidence="0.999988833333333">
e.g., while it is helpful to know whether fly is a
verb or a noun, knowing that you is a personal pro-
noun does not carry the information whether it is
a subject or an object (given the Penn Tree Bank
tagset), which would certainly help to predict the
following word.
</bodyText>
<subsectionHeader confidence="0.985329">
2.2 SuperARV
</subsectionHeader>
<bodyText confidence="0.999976814814815">
The SuperARV essentially organizes information
concerning one consistent set of dependency links
for a word that can be directly derived from its
syntactic parse. SuperARVs encode lexical in-
formation as well as syntactic and semantic con-
straints in a uniform representation that is much
more fine-grained than POS. It is a four-tuple
(C; F; R+; D), where C is the lexical category
of the word, F is a vector of lexical features for
the word, R+ is a set of governor and need labels
that indicate the function of the word in the sen-
tence and the types of words it needs, and D rep-
resents the relative position of the word and its de-
pendents. We refer the reader to the literature for
further details on SuperARVs (Wang and Harper,
2002; Wang et al., 2003).
SuperARVs can be produced from parse trees
by applying deterministic rules. In this work we
use SuperARVs as individual tags and do not clus-
ter them based of their structure. While Super-
ARVs are very attractive for language modeling,
developing such a rich set of annotations for a new
language would require a large amount of human
effort.
We propose two other types of tags which have
not been applied to this task, although similar in-
formation has been used in parsing.
</bodyText>
<subsectionHeader confidence="0.998193">
2.3 Modifee Tag
</subsectionHeader>
<bodyText confidence="0.999397428571429">
This tag is a combination of the word’s POS
tag and the POS tag of its governor role. We
designed it to resemble dependency parse struc-
ture. For example, the sentence in Figure 1 would
be tagged: the/DT-NN black/JJ-NN cat/NN-VBD
sat/VBD-root. Henceforth, we will refer to this
kind of tag as head.
</bodyText>
<subsectionHeader confidence="0.997962">
2.4 Parent Constituent
</subsectionHeader>
<bodyText confidence="0.999952307692308">
This tag is a combination of the word’s POS tag
with its immediate parent in the parse tree, along
with the POS tag’s relative position among its sib-
lings. We refer to this type of tags as parent. The
example in Figure 1 will be tagged: the/DT-NP-
start black/JJ-NP-mid cat/NN-NP-end sat/VB-VP-
single. This tagset is designed to represent con-
stituency information.
Note that the head and parent tagsets are more
language-independent (all they require is a tree-
bank) than the SuperARVs which, not only uti-
lized the treebank, but were explicitly designed by
a linguist for English only.
</bodyText>
<subsectionHeader confidence="0.699427">
2.5 Information Theoretic Comparison of
Tags
</subsectionHeader>
<bodyText confidence="0.999787">
As we have mentioned in Section 1, the choice of
the tagset is very important to the performance of
the model. There are two conflicting intuitions for
tags: on one hand they should be specific enough
to be helpful in the language model’s task; on the
other hand, they should be easy for the LM to pre-
dict.
Of course, in order to argue which tags are more
suitable, we need some quantifiable metrics. We
propose an information theoretic approach:
</bodyText>
<listItem confidence="0.969994">
• To quantify how hard it is to predict a tag, we
compute the conditional entropy:
</listItem>
<equation confidence="0.994519666666667">
Hp(ti|wi) = Hp(tiwi) − Hp(wi)
E= p(tiwi) log p(ti|wi)
witi
</equation>
<bodyText confidence="0.650414">
• To measure how helpful a tagset is in the LM
task, we compute the reduction of the condi-
tional cross entropy:
</bodyText>
<equation confidence="0.913840333333333">
H˜p, (wi|wi−1ti−1) − H˜p, (wi|wi−1) _
E− ˜p(wii−1ti−1) log q(wi|wi−1ti−1)
wii−1ti−1
˜p(wii−1) log q(wi|wi−1)
_ −.E ˜p(wii−1ti−1) log q(wi|wi−1ti−1)
w�-1ti−1 q(wi|wi−1)
+E
wi
i−1
</equation>
<page confidence="0.919612">
1116
</page>
<bodyText confidence="0.999662545454545">
Note that in this case we use conditional
cross entropy because conditional entropy
has the tendency to overfit the data as we se-
lect more and more fine-grain tags. Indeed,
Hp(wi|wi−1ti−1) can be reduced to zero if
the tags are specific enough, which would
never happen in reality. This is not a prob-
lem for the former metric because the con-
text there, wi, is fixed. For this metric, we
use a smoothed distribution p˜ computed on
the training set3 and the test distribution q.
</bodyText>
<figure confidence="0.683507">
POS SuperARV parent head
Tags
</figure>
<figureCaption confidence="0.996085">
Figure 2: Changes in entropy for different tagsets
</figureCaption>
<bodyText confidence="0.99997065">
The results of these measurements are presented
in Figure 2. POS tags, albeit easy to predict, pro-
vide very little additional information about the
following word, and therefore we would not ex-
pect them to perform very well. The parent tagset
seems to perform somewhat better than Super-
ARVs – it provides 0.13 bits more information
while being only 0.09 bits harder to predict based
on the word. The head tagset is interesting: it pro-
vides 0.2 bits more information about the follow-
ing word (which would correspond to 15% per-
plexity reduction if we had perfect tags), but on
the other hand the model is less likely to predict
these tags accurately.
This approach is only a crude estimate (it uses
only unigram and bigram context) but it is very
useful for designing tagsets, e.g., for a new lan-
guage, because it allows us to assess relative per-
formance of tagsets without having to train a full
model.
</bodyText>
<footnote confidence="0.8908945">
3We used one-count smoothing (Chen and Goodman,
1996).
</footnote>
<sectionHeader confidence="0.942961" genericHeader="method">
3 Language Model Structure
</sectionHeader>
<bodyText confidence="0.9998844">
The size and sparsity of the parameter space of the
joint model necessitate the use of dimensionality
reduction measures in order to make the model
computationally tractable and to allow for accu-
rate estimation of the model’s parameters. We also
want the model to be able to easily accommodate
additional sources of information such as morpho-
logical features, prosody, etc. In the rest of this
section, we discuss avenues we have taken to ad-
dress these problems.
</bodyText>
<subsectionHeader confidence="0.997243">
3.1 Decision Tree Clustering
</subsectionHeader>
<bodyText confidence="0.999766375">
Binary decision tree clustering has been shown to
be effective for reducing the parameter space in
language modeling (Bahl et al., 1990; Heeman,
1999) and other language processing applications,
e.g., (Magerman, 1994). Like any clustering algo-
rithm, it can be represented by a function H that
maps the space of histories to a set of equivalence
classes.
</bodyText>
<equation confidence="0.998363">
/ i−1 i−1 / / i−1 i−1
plwiti|wi−41t−n+1) �&apos;plwiti|Hlwi−n+1ti−n+1))
(4)
</equation>
<bodyText confidence="0.999983142857143">
While the tree construction algorithm is fairly
standard – to recursively select binary questions
about the history optimizing some function – there
are important decisions to make in terms of which
questions to ask and which function to optimize.
In the remainder of this section, we discuss the de-
cisions we made regarding these issues.
</bodyText>
<subsectionHeader confidence="0.992501">
3.2 Factors
</subsectionHeader>
<bodyText confidence="0.999868277777778">
The Factored Language Model (FLM) (Bilmes
and Kirchhoff, 2003) offers a convenient view of
the input data: it represents every word in a sen-
tence as a tuple of factors. This allows us to extend
the language model with additional parameters. In
an FLM, however, all factors have to be determin-
istically computed in a joint model; whereas, we
need to distinguish between the factors that are
given or computed and the factors that the model
must predict stochastically. We call these types
of factors overt and hidden, respectively. Exam-
ples of overt factors include surface words, mor-
phological features such as suffixes, case informa-
tion when available, etc., and the hidden factors
are POS, SuperARVs, or other tags.
Henceforth, we will use word to represent the
set of overt factors and tag to represent the set of
hidden factors.
</bodyText>
<figure confidence="0.997623625">
Bits
2.5
0.5
1.5
3
2
0
1
</figure>
<page confidence="0.938961">
1117
</page>
<subsectionHeader confidence="0.989894">
3.3 Hidden Factors Tree
</subsectionHeader>
<bodyText confidence="0.999960857142857">
Similarly to (Heeman, 1999), we construct a bi-
nary tree where each tag is a leaf; we will refer
to this tree as the Hidden Factors Tree (HFT). We
use Minimum Discriminative Information (MDI)
algorithm (Zitouni, 2007) to build the tree. The
HFT represents a hierarchical clustering of the tag
space. One of the reasons for doing this is to allow
questions about subsets of tags rather than individ-
ual tags alone4.
Unlike (Heeman, 1999), where the tree of tags
was only used to create questions, this representa-
tion of the tag space is, in addition, a key feature
of our decoding optimizations, which we discuss
in Section 4.
</bodyText>
<subsectionHeader confidence="0.874568">
3.4 Questions
</subsectionHeader>
<bodyText confidence="0.999511666666667">
The context space is partitioned by means of bi-
nary questions. We use different types of ques-
tions for hidden and overt factors.
</bodyText>
<listItem confidence="0.714998">
• Questions about surface words are con-
</listItem>
<bodyText confidence="0.970738388888889">
structed using the Exchange algorithm (Mar-
tin et al., 1998). This algorithm takes the set
of words that appear at a certain position in
the training data associated with the current
node in the history tree and divides the set
into two complementary subsets greedily op-
timizing some target function (we use the av-
erage entropy of the marginalized word dis-
tribution, the same as for question selection).
Note that since the algorithm only operates
on the words that appear in the training data,
we need to do something more to account for
the unseen words. Thus, to represent this type
of question, we create the history tree struc-
ture depicted in Fig. 4.
For other overt factors with smaller vocabu-
laries, such as suffixes, we use equality ques-
tions.
</bodyText>
<listItem confidence="0.734710857142857">
• As we mentioned in Section 3.3, we use the
Hidden Factors Tree to create questions about
hidden factors. Note that every node in a bi-
nary tree can be represented by a binary path
from the root with all nodes under an inner
node sharing the same prefix. Thus, a ques-
tion about whether a tag belongs to a subset
</listItem>
<footnote confidence="0.8540115">
4Trying all possible subsets of tags is not feasible since
there are 2|T |of them. The tree allows us to reduce the num-
ber to O(T) of the most meaningful (as per the clustering
algorithm) subsets.
</footnote>
<figureCaption confidence="0.9791485">
Figure 3: Recursive smoothing: ˜pn = Anpn +
(1 − An)˜pn′
</figureCaption>
<bodyText confidence="0.999552666666667">
of tags dominated by a node can be expressed
as whether the tag’s path matches the binary
prefix.
</bodyText>
<subsectionHeader confidence="0.6670595">
3.5 Optimization Criterion and Stopping
Rule
</subsectionHeader>
<bodyText confidence="0.999984857142857">
To select questions we use the average entropy of
the marginalized word distribution. We found that
this criterion significantly outperforms the entropy
of the distribution of joint events. This is proba-
bly due to the increased sparsity of the joint distri-
bution and the fact that our ultimate metrics, i.e.,
WER and word perplexity, involve only words.
</bodyText>
<subsectionHeader confidence="0.990491">
3.6 Distribution Representation
</subsectionHeader>
<bodyText confidence="0.99992">
In a cluster Hx, we factor the joint distribution as
follows:
</bodyText>
<equation confidence="0.993337">
p(witi|Hx) = p(wi|Hx) - p(ti|wi, Hx)
</equation>
<bodyText confidence="0.999953166666667">
where p(ti|wi, Hx) is represented in the form of
an HFT, in which each leaf has the probability of a
tag and each internal node contains the sum of the
probabilities of the tags it dominates. This repre-
sentation is designed to assist the decoding process
described in Section 4.
</bodyText>
<subsectionHeader confidence="0.992378">
3.7 Smoothing
</subsectionHeader>
<bodyText confidence="0.998295666666667">
In order to estimate probability distributions at the
leaves of the history tree, we use the following re-
cursive formula:
</bodyText>
<equation confidence="0.999281">
˜pn(witi) = Anpn(witi) + (1 − An)˜pn′(witi) (5)
</equation>
<bodyText confidence="0.9987955">
where n′ is the n-th node’s parent, pn(witi) is
the distribution at node n (see Figure 3). The
</bodyText>
<page confidence="0.986436">
1118
</page>
<bodyText confidence="0.990845941176471">
root of the tree is interpolated with the distribu-
tion punif(witi) = 1
|V |pML(ti|wi)5. To estimate
interpolation parameters An, we use the EM algo-
rithm described in (Magerman, 1994); however,
rather than setting aside a separate development
set of optimizing An, we use 4-fold cross valida-
tion and take the geometric mean of the resulting
coefficients6. We chose this approach because a
small development set often does not overlap with
the training set for low-count nodes, leading the
EM algorithm to set An = 0 for those nodes.
Let us consider one leaf of the history tree in
isolation. Its context can be represented by the
path to the root, i.e., the sequence of questions and
answers q1, ... q(n′)′qn′ (with q1 being the answer
to the topmost question):
</bodyText>
<equation confidence="0.9982852">
˜pn(witi) = ˜p(witi|q1 ... q(n′)′qn′)
Represented this way, Eq. 5 is a variant of Jelinek-
Mercer smoothing:
˜p(witi|q1 ... qn′) = Anp(witi|q1 ... qn′) +
(1 − An)˜p(witi|q1 ... q(n′)′)
</equation>
<bodyText confidence="0.998583">
For backoff nodes (see Fig. 4), we use a lower
order model7 interpolated with the distribution at
the backoff node’s grandparent (see node A in Fig.
4):
</bodyText>
<equation confidence="0.999762">
i−1 i−1
pB(witi|wi−n+1ti−n+1) =
i−1
αA˜pbo(witi |wi−n+2ti−1
i−n+2) + (1 − αA)˜pA(witi)
</equation>
<bodyText confidence="0.998100833333333">
How to compute aA is an open question. For this
study, we use a simple heuristic based on obser-
vation that the further node A is from the root
the more reliable the distribution ˜pA(witi) is, and
hence aA is lower. The formula we use is as fol-
lows:
</bodyText>
<equation confidence="0.9614375">
1
�/1 + distanceToRoot(A)
</equation>
<footnote confidence="0.9697292">
5We use this distribution rather than uniform joint distri-
bution 1
|V ||T |because we do not want to allow word-tag pairs
that have never been observed. The idea is similar to (Thede
and Harper, 1999).
6To avoid a large number of zeros due to the product, we
set a minimum for λ to be 10−7.
7The lower order model is constructed by the same algo-
rithm, although with smaller context. Note that the lower or-
der model can back off on words or tags, or both. In this paper
we backoff both on words and tags, i.e., p(witi|wi−1
i−2ti−1
i−2)
backs off to p(witi|wi−1ti−1), which in turn backs off to the
unigram p(witi).
</footnote>
<figureCaption confidence="0.787802">
Figure 4: A fragment of the decision tree with a
backoff node. 5 U 5 is the set of words observed
in the training data at the node A. To account for
unseen words, we add the backoff node B.
</figureCaption>
<sectionHeader confidence="0.996686" genericHeader="method">
4 Decoding
</sectionHeader>
<bodyText confidence="0.999885541666667">
As in HMM decoding, in order to compute prob-
abilities for i-th step, we need to sum over |T|n−1
possible combinations of tags in the history, where
T is the set of tags and n is the order of the
model. With |T |predictions for the i-th step, we
have O(|T |n) computational complexity per word.
Straightforward computation of these probabili-
ties is problematic even for a trigram model with
POS tags, i.e., n = 3, |T |Pz� 40. A standard ap-
proach to limit computational requirements is to
use beam search where only N most likely paths
are retained. However, with fine-grain tags where
|T |Pt� 1, 500, a tractable beam size would only
cover a small fraction of the whole space, leading
to search errors such as pruning good paths.
Note that we have a history clustering function
(Eq. 4) represented by the decision tree, and we
should be able to exploit this clustering to elimi-
nate unnecessary computations involving equiva-
lent histories. Note that words in the history are
known exactly, thus we can create a projection of
the clustering function H in Eq. 4 to the plane
wi−1i−n+1 = const, i.e., where words in the context
are fixed to be whatever is observed in the history:
</bodyText>
<equation confidence="0.984060666666667">
H(wi−ni+1ti−1
i−1
� Hw&apos;-1 =const(ti−1
i−n+1)
i-n+1
(6)
</equation>
<bodyText confidence="0.9475498">
The number of distinct clusters in the projection
Hˆ depends on the decision tree configuration and
can vary greatly for different words wi−1
i−n+1 in the
history, but generally it is relatively small:
</bodyText>
<equation confidence="0.991427">
 |ˆHwi−1
i−n+1=const(ti−1
i−n+1) |« |Tn−1 |(7)
aA =
</equation>
<page confidence="0.993882">
1119
</page>
<figureCaption confidence="0.992219">
Figure 5: Questions about hidden factors split
states (see Figure 6) in the decoding lattice rep-
resented by HFTs.
</figureCaption>
<bodyText confidence="0.516522">
thus, the number of probabilities that we need to
</bodyText>
<equation confidence="0.9763475">
Hw i−1 =const |- |T|.
i−n+1
</equation>
<bodyText confidence="0.999487">
Our decoding algorithm works similarly to
HMM decoding with the exception that the set of
hidden states is not predetermined. Let us illus-
trate how it works in the case of a bigram model.
Recall that the set of tags T is represented as a
binary tree (HFT) and the only type of questions
about tags is about matching a binary prefix in the
HFT. Such a question dissects the HFT into two
parts as depicted in Figure 5. The cost of this op-
eration is O(log |T |).
We represent states in the decoding lattice as
shown in the Figure 6, where pSzn is the probability
of reaching the state S:
</bodyText>
<equation confidence="0.979095">
�p(t|wi−2HS′) �
</equation>
<bodyText confidence="0.999770769230769">
where INS is the set of incoming links to the
state S from the previous time index, and TSS is
the set of tags generated from the state S′ repre-
sented as a fragment of the HFT. Note, that since
we maintain the property that the probability as-
signed to an inner node of the HFT is the sum
of probabilities of the tags it dominates, the sum
EtETSS p(t|wz_2HSS) is located at the root of TSS,
and therefore this is an O(1) operation.
Now given the state S at time i − 1, in order to
generate tag predictions for i-th word, we apply
questions from the history clustering tree, start-
ing from the top. Questions about overt factors
</bodyText>
<figureCaption confidence="0.800567">
Figure 6: A state S in the decoding lattice. pSzn is
</figureCaption>
<bodyText confidence="0.960735473684211">
the probability of reaching the state S through the
set of links INS. The probabilities of generating
the tags p(tz_1|wz_1, Hs), (tz_1 E TS) are repre-
sented in the form of the HFT.
always follow either a true or false branch, implic-
itly computing the projection in Eq. 6. Questions
about hidden factors, can split the state S into two
states Strue and Sfalse, each retaining a part of TS
as shown in the Figure 5.
The process continues until each fragment of
each state at the time i − 1 reaches the bottom of
the history tree, at which point new states for time
i are generated from the clusters associated with
leaves. The states at i − 1 that generate the cluster
Hs become the incoming links to the state ˜S.
Higher order models work similarly, except that
at each time we consider a state S at time i − 1
along with one of its incoming links (to some
depth according to the size of the context).
</bodyText>
<sectionHeader confidence="0.991565" genericHeader="evaluation">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999986">
To evaluate the impact of fine-grain tags on lan-
guage modeling, we trained our model with five
settings: In the first model, questions were re-
stricted to be about overt factors only, thus making
it a tree-based word model. In the second model,
we used POS tags. To evaluate the effect of fine-
grain tags, we train two models: head and parent
described in Section 2.3 and Section 2.4 respec-
tively. Since our joint model can be used with
any kind of tags, we also trained it with Super-
ARV tags (Wang et al., 2003). The SuperARVs
were created from the same parse trees that were
used to produce POS and fine-grain tags. All our
models, including SuperARV, use trigram context.
We include standard trigram, four-gram, and five-
</bodyText>
<figure confidence="0.4606588">
compute is |
�pSin = � �
S′EINS �p S′
in p(wi−2|HS′)
tETS′
</figure>
<page confidence="0.967573">
1120
</page>
<bodyText confidence="0.999832734693878">
gram models for reference. The ngram models
were trained using SRILM toolkit with interpo-
lated modified Kneser-Ney smoothing.
We evaluate our model with an nbest rescoring
task using 100-best lists from the DARPA WSJ’93
and WSJ’92 20k open vocabulary data sets. The
details on the acoustic model used to produce the
nbest lists can be found in (Wang and Harper,
2002). Since the data sets are small, we com-
bined the 93et and 93dt sets for evaluation and
used 92et for the optimization8. We transformed
the nbest lists to match PTB tokenization, namely
separating possessives from nouns, n’t from auxil-
iary verbs in contractions, as well as contractions
from personal pronouns.
All language models were trained on the NYT
1994-1995 section of the English Gigaword cor-
pus (approximately 70M words). Since the New
York Times covers a wider range of topics than
the Wall Street Journal, we eliminated the most ir-
relevant stories based on their trigram coverage by
sections 00-22 of WSJ. We also eliminated sen-
tences over 120 words, because the parser’s per-
formance drops significantly on long sentences.
After parsing the corpus, we deleted sentences that
were assigned a very low probability by the parser.
Overall we removed only a few percent of the data;
however, we believe that such a rigorous approach
to data cleaning is important for building discrim-
inating models.
Parse trees were produced by an extended ver-
sion of the Berkeley parser (Huang and Harper,
2009). We trained the parser on a combination of
the BN and WSJ treebanks, preprocessed to make
them more consistent with each other. We also
modified the trees for the speech recognition task
by replacing numbers and abbreviations with their
verbalized forms. We pre-processed the NYT cor-
pus in the same way, and parsed it. After that, we
removed punctuation and downcased words. For
the ngram model, we used text processed in the
same way.
In head and parent models, tag vocabularies
contain approximately 1,500 tags each, while the
SuperARV model has approximately 1,400 dis-
tinct SuperARVs, most of which represent verbs
(1,200).
In these experiments we did not use overt fac-
tors other than the surface word because they split
</bodyText>
<footnote confidence="0.9262225">
8We optimized the LM weight and computed WER with
scripts in the SRILM and NIST SCTK toolkits.
</footnote>
<table confidence="0.999144666666667">
Models WER
trigram (baseline) 17.5
four-gram 17.7
five-gram 17.8
Word Tree 17.3
POS Tags 17.0
Head Tags 16.8
Parent Tags 16.7
SuperARV 16.9
</table>
<tableCaption confidence="0.813176333333333">
Table 1: WER results, optimized on 92et set, eval-
uated on combined 93et and 93dt set. The Oracle
WER is 9.5%.
</tableCaption>
<bodyText confidence="0.99985709375">
&lt;unk&gt;, effectively changing the vocabulary thus
making perplexity incomparable to models with-
out these factors, without improving WER notice-
ably. However, we do plan to use more overt
factors in Machine Translation experiments where
a language model faces a wider range of OOV
phenomena, such as abbreviations, foreign words,
numbers, dates, time, etc.
Table 1 summarizes performance of the LMs on
the rescoring task. The parent tags model outper-
forms the trigram baseline model by 0.8% WER.
Note that four- and five-gram models fail to out-
perform the trigram baseline. We believe this is
due to the sparsity as well as relatively short sen-
tences in the test set (16 words on average).
Interestingly, whereas the improvement of the
POS model over the baseline is not statistically
significant (p &lt; 0.10)9, the fine-grain models out-
perform the baseline much more reliably: p &lt;
0.03 (SuperARV) and p &lt; 0.007 (parent).
We present perplexity evaluations in Table 2.
The perplexity was computed on Section 23 of
WSJ PTB, preprocessed as the rest of the data we
used. The head model has the lowest perplexity
outperforming the baseline by 9%. Note, it even
outperforms the five-gram model, although by a
small 2% margin.
Although the improvements by the fine-grain
tagsets over POS are not significant (due to the
small size of the test set), the reductions in per-
plexity suggest that the improvements are not ran-
dom.
</bodyText>
<footnote confidence="0.893663">
9For statistical significance, we used SCTK implementa-
tion of the mapsswe test.
</footnote>
<page confidence="0.933709">
1121
</page>
<table confidence="0.999616777777778">
Models PPL
trigram (baseline) 162
four-gram 152
five-gram 150
Word Tree 160
POS Tags 154
Head Tags 147
Parent Tags 150
SuperARV 150
</table>
<tableCaption confidence="0.9040415">
Table 2: Perplexity results on Section 23 WSJ
PTB
</tableCaption>
<sectionHeader confidence="0.990854" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999992475">
In this paper, we presented a joint language mod-
eling framework. Unlike any prior work known
to us, it was not tailored for any specific tag set,
rather it was designed to accommodate any set
of tags, especially large sets (∼ 1, 000), which
present challenges one does not encounter with
smaller tag sets, such at POS tags. We discussed
these challenges and our solutions to them. Some
of the solutions proposed are novel, particularly
the decoding algorithm.
We also proposed two simple fine-grain tagsets,
which, when applied in language modeling, per-
form comparably to highly sophisticated tag sets
(SuperARV). We would like to stress that, while
our fine-grain tags did not significantly outperform
SuperARVs, the former use much less linguistic
knowledge and can be automatically induced for
any language with a treebank.
Because a joint language model inherently pre-
dicts hidden events (tags), it can also be used to
generate the best sequence of those events, i.e.,
tagging. We evaluated our model in the POS tag-
ging task and observed similar results: the fine-
grain models outperform the POS model, while
both outperform the state-of-the-art HMM POS
taggers. We refer to (Filimonov and Harper, 2009)
for details on these experiments.
We plan to investigate how parser accuracy and
data selection strategies, e.g., based on parser con-
fidence scores, impact the performance of our
model. We also plan on evaluating the model’s
performance on other genres of speech, as well as
in other tasks such as Machine Translation. We
are also working on scaling our model further to
accommodate amounts of data typical for mod-
ern large-scale ngram models. Finally, we plan to
apply the technique to other languages with tree-
banks, such as Chinese and Arabic.
We intend to release the source code of our
model within several months of this publication.
</bodyText>
<sectionHeader confidence="0.998439" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.99989325">
This material is based upon work supported in
part by the Defense Advanced Research Projects
Agency (DARPA) under Contract No. HR0011-
06-C-0023 and NSF IIS-0703859. Any opinions,
findings and/or recommendations expressed in this
paper are those of the authors and do not necessar-
ily reflect the views of the funding agencies or the
institutions where the work was completed.
</bodyText>
<sectionHeader confidence="0.999208" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999881567567568">
Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, and
Robert L. Mercer. 1990. A tree-based statistical
language model for natural language speech recog-
nition. Readings in speech recognition, pages 507–
514.
Srinivas Bangalore. 1996. ‘Almost parsing’ technique
for language modeling. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, volume 2, pages 1173–1176.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff.
In Proceedings ofHLT/NACCL, 2003, pages 4–6.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deS-
ouza, Jennifer C. Lai, and Robert L. Mercer. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467–479.
Ciprian Chelba and Frederick Jelinek. 2000. Struc-
tured language modeling for speech recognition.
CoRR.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th annual meet-
ing on Association for Computational Linguistics,
pages 310–318, Morristown, NJ, USA. Association
for Computational Linguistics.
Denis Filimonov and Mary Harper. 2009. Measuring
tagging performance of a joint language model. In
Proceedings of the Interspeech 2009.
Peter A. Heeman. 1999. POS tags and decision trees
for language modeling. In In Proceedings of the
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Cor-
pora, pages 129–137.
Zhongqiang Huang and Mary Harper. 2009. Self-
Training PCFG grammars with latent annotations
across languages. In Proceedings of the EMNLP
2009.
</reference>
<page confidence="0.87928">
1122
</page>
<reference confidence="0.999760171428571">
David M. Magerman. 1994. Natural language pars-
ing as statistical pattern recognition. Ph.D. thesis,
Stanford, CA, USA.
Sven Martin, Jorg Liermann, and Hermann Ney. 1998.
Algorithms for bigram and trigram word clustering.
In Speech Communication, pages 1253–1256.
Thomas R. Niesler and Phil C. Woodland. 1996.
A variable-length category-based n-gram language
model. Proceedings of the IEEE International Con-
ference on Acoustics, Speech, and Signal Process-
ing, 1:164–167 vol. 1, May.
Scott M. Thede and Mary P. Harper. 1999. A second-
order hidden markov model for part-of-speech tag-
ging. In Proceedings of the 37th Annual Meeting of
the ACL, pages 175–182.
Wen Wang and Mary P. Harper. 2002. The SuperARV
language model: investigating the effectiveness of
tightly integrating multiple knowledge sources. In
EMNLP ’02: Proceedings of the ACL-02 conference
on Empirical methods in natural language process-
ing, pages 238–247, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Wen Wang, Mary P. Harper, and Andreas Stolcke.
2003. The robustness of an almost-parsing language
model given errorful training data. In Proceedings
of the IEEE International Conference on Acoustics,
Speech, and Signal Processing.
Peng Xu and Frederick Jelinek. 2004. Random forests
in language modeling. In in Proceedings of the 2004
Conference on Empirical Methods in Natural Lan-
guage Processing.
Imed Zitouni. 2007. Backoff hierarchical class n-
gram language models: effectiveness to model un-
seen events in speech recognition. Computer Speech
&amp; Language, 21(1):88–104.
</reference>
<page confidence="0.966453">
1123
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.524148">
<title confidence="0.9455735">A Joint Language Model With Fine-grain Syntactic Tags for Computational</title>
<affiliation confidence="0.894599666666667">and Information Institute for Advanced Computer University of Maryland, College</affiliation>
<email confidence="0.999832">den@cs.umd.edu</email>
<abstract confidence="0.993198">We present a scalable joint language model designed to utilize fine-grain syntactic tags. We discuss challenges such a design faces and describe our solutions that scale well to large tagsets and corpora. We advocate the use of relatively simple tags that do not require deep linguistic knowledge of the language but provide more structural information than POS tags and can be derived from automatically generated parse trees – a combination of properties that allows easy adoption of this model for new languages. We propose two fine-grain tagsets and evaluate our model using these tags, as well as POS tags and SuperARV tags in a speech recognition task and discuss future directions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lalit R Bahl</author>
<author>Peter F Brown</author>
<author>Peter V de Souza</author>
<author>Robert L Mercer</author>
</authors>
<title>A tree-based statistical language model for natural language speech recognition. Readings in speech recognition,</title>
<date>1990</date>
<pages>507--514</pages>
<marker>Bahl, Brown, de Souza, Mercer, 1990</marker>
<rawString>Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, and Robert L. Mercer. 1990. A tree-based statistical language model for natural language speech recognition. Readings in speech recognition, pages 507– 514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
</authors>
<title>Almost parsing’ technique for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>1173--1176</pages>
<contexts>
<context position="5985" citStr="Bangalore, 1996" startWordPosition="980" endWordPosition="981">cribe those that we used in our experiments. Rather than tailoring our model to these tagsets, we aim for flexibility and propose an information theoretic framework for quick evaluation for tagsets, thus simplifying the creation of new tagsets. We show that our model with fine-grain tagsets outperform the coarser POS model, as well as the ngram baseline, in Section 5. • Address the challenges that arise in a joint language model with fine-grain tags. While the idea of using joint language modeling is not novel (Chelba and Jelinek, 2000; Heeman, 1999), nor is the idea of using fine-grain tags (Bangalore, 1996; Wang et al., 2003), none of prior papers focus on the issues that arise from the combination of joint language modeling with fine-grain tags, both in terms of reliable parameter estimation and scalability in the face of the increased computational complexity. We dedicate Sections 3 and 4 to this problem. In Section 6, we summarize conclusions and lay out directions for future work. 2 Structural Information As we have mentioned, the selection of the random variable in Eq. 2 is extremely important for the performance of the model. On one hand, we would like for this variable to provide maximum</context>
</contexts>
<marker>Bangalore, 1996</marker>
<rawString>Srinivas Bangalore. 1996. ‘Almost parsing’ technique for language modeling. In Proceedings of the International Conference on Spoken Language Processing, volume 2, pages 1173–1176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff A Bilmes</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Factored language models and generalized parallel backoff.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT/NACCL,</booktitle>
<pages>4--6</pages>
<contexts>
<context position="13393" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="2246" endWordPosition="2249"> 1994). Like any clustering algorithm, it can be represented by a function H that maps the space of histories to a set of equivalence classes. / i−1 i−1 / / i−1 i−1 plwiti|wi−41t−n+1) �&apos;plwiti|Hlwi−n+1ti−n+1)) (4) While the tree construction algorithm is fairly standard – to recursively select binary questions about the history optimizing some function – there are important decisions to make in terms of which questions to ask and which function to optimize. In the remainder of this section, we discuss the decisions we made regarding these issues. 3.2 Factors The Factored Language Model (FLM) (Bilmes and Kirchhoff, 2003) offers a convenient view of the input data: it represents every word in a sentence as a tuple of factors. This allows us to extend the language model with additional parameters. In an FLM, however, all factors have to be deterministically computed in a joint model; whereas, we need to distinguish between the factors that are given or computed and the factors that the model must predict stochastically. We call these types of factors overt and hidden, respectively. Examples of overt factors include surface words, morphological features such as suffixes, case information when available, etc., an</context>
</contexts>
<marker>Bilmes, Kirchhoff, 2003</marker>
<rawString>Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored language models and generalized parallel backoff. In Proceedings ofHLT/NACCL, 2003, pages 4–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="2837" citStr="Brown et al., 1992" startWordPosition="453" endWordPosition="456">ameter space can be quite sparse and requires sophisticated techniques for reliable probability estimation (Chen and Goodman, 1996). While the ngram models perform fairly well, they are only capable of capturing very shallow knowledge of the language. There is extensive literature on a variety of methods that have been used to imbue models with syntactic and semantic information in different ways. These methods can be broadly categorized into two types: • The first method uses surface words within its context, sometimes organizing them into deterministic classes. Models of this type include: (Brown et al., 1992; Zitouni, 2007), which use semantic word clustering, and (Bahl et al., 1990), which uses variablelength context. • The other method adds stochastic variables to express the ambiguous nature of surface words2. To obtain the probability of the next 1Real applications use argmax.. p(A|wl) )·p(w�1 )�·n0 instead of Eq. 1, where α and ,Q are set to optimize a heldout set. 2These variables have to be predicted by the model. n p(wn1) = i=1 1114 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1114–1123, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP word</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Structured language modeling for speech recognition.</title>
<date>2000</date>
<journal>CoRR.</journal>
<contexts>
<context position="3784" citStr="Chelba and Jelinek, 2000" startWordPosition="616" endWordPosition="619">ere α and ,Q are set to optimize a heldout set. 2These variables have to be predicted by the model. n p(wn1) = i=1 1114 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1114–1123, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP word we need to sum over all assignments of the stochastic variables, as in Eq. 2. p(witi|wi−1 1 ti−1 1 ) (2) i-1 i−1 Et1...ti p(witi |w1 t1 )p(wZ 1 ti−1 1 ) �t1...ti−1 p(wi−1 1 ti−1 1 ) Models of this type, which we call joint models since they essentially predict joint events of words and some random variable(s), include (Chelba and Jelinek, 2000) which used POS tags in combination with “parser instructions” for constructing a full parse tree in a left-to-right manner; (Wang et al., 2003) used SuperARVs (complex tuples of dependency information) without resolving the dependencies, thus called almost parsing; (Niesler and Woodland, 1996; Heeman, 1999) utilize part of speech (POS) tags. Note that some models reduce the context by making the following approximation: p(witi|wi−1 1 ti−1 1 ) ^ p(wi|ti) p(ti|ti−1 1 ) (3) thus, transforming the problem into a standard HMM application. However, these models perform poorly and have only been abl</context>
<context position="5911" citStr="Chelba and Jelinek, 2000" startWordPosition="965" endWordPosition="968">int language model that can be used with a variety of tagsets. In Section 2, we describe those that we used in our experiments. Rather than tailoring our model to these tagsets, we aim for flexibility and propose an information theoretic framework for quick evaluation for tagsets, thus simplifying the creation of new tagsets. We show that our model with fine-grain tagsets outperform the coarser POS model, as well as the ngram baseline, in Section 5. • Address the challenges that arise in a joint language model with fine-grain tags. While the idea of using joint language modeling is not novel (Chelba and Jelinek, 2000; Heeman, 1999), nor is the idea of using fine-grain tags (Bangalore, 1996; Wang et al., 2003), none of prior papers focus on the issues that arise from the combination of joint language modeling with fine-grain tags, both in terms of reliable parameter estimation and scalability in the face of the increased computational complexity. We dedicate Sections 3 and 4 to this problem. In Section 6, we summarize conclusions and lay out directions for future work. 2 Structural Information As we have mentioned, the selection of the random variable in Eq. 2 is extremely important for the performance of </context>
</contexts>
<marker>Chelba, Jelinek, 2000</marker>
<rawString>Ciprian Chelba and Frederick Jelinek. 2000. Structured language modeling for speech recognition. CoRR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>310--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2350" citStr="Chen and Goodman, 1996" startWordPosition="372" endWordPosition="375">ter of Excellence Johns Hopkins University mharper@umiacs.umd.edu model and p(wn1) is the language model1. Typically, these applications use language models that compute the probability of a sequence in a generative way: p(wi|wi−1 1 ) Approximation is required to keep the parameter space tractable. Most commonly the context is reduced to just a few immediately preceding words. This type of model is called an ngram model: p(wi|wi−1 1 ) ≈ p(wi|wi−1 i−n+1) Even with limited context, the parameter space can be quite sparse and requires sophisticated techniques for reliable probability estimation (Chen and Goodman, 1996). While the ngram models perform fairly well, they are only capable of capturing very shallow knowledge of the language. There is extensive literature on a variety of methods that have been used to imbue models with syntactic and semantic information in different ways. These methods can be broadly categorized into two types: • The first method uses surface words within its context, sometimes organizing them into deterministic classes. Models of this type include: (Brown et al., 1992; Zitouni, 2007), which use semantic word clustering, and (Bahl et al., 1990), which uses variablelength context.</context>
<context position="12034" citStr="Chen and Goodman, 1996" startWordPosition="2028" endWordPosition="2031">while being only 0.09 bits harder to predict based on the word. The head tagset is interesting: it provides 0.2 bits more information about the following word (which would correspond to 15% perplexity reduction if we had perfect tags), but on the other hand the model is less likely to predict these tags accurately. This approach is only a crude estimate (it uses only unigram and bigram context) but it is very useful for designing tagsets, e.g., for a new language, because it allows us to assess relative performance of tagsets without having to train a full model. 3We used one-count smoothing (Chen and Goodman, 1996). 3 Language Model Structure The size and sparsity of the parameter space of the joint model necessitate the use of dimensionality reduction measures in order to make the model computationally tractable and to allow for accurate estimation of the model’s parameters. We also want the model to be able to easily accommodate additional sources of information such as morphological features, prosody, etc. In the rest of this section, we discuss avenues we have taken to address these problems. 3.1 Decision Tree Clustering Binary decision tree clustering has been shown to be effective for reducing the</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th annual meeting on Association for Computational Linguistics, pages 310–318, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denis Filimonov</author>
<author>Mary Harper</author>
</authors>
<title>Measuring tagging performance of a joint language model.</title>
<date>2009</date>
<booktitle>In Proceedings of the Interspeech</booktitle>
<contexts>
<context position="29925" citStr="Filimonov and Harper, 2009" startWordPosition="5136" endWordPosition="5139">sophisticated tag sets (SuperARV). We would like to stress that, while our fine-grain tags did not significantly outperform SuperARVs, the former use much less linguistic knowledge and can be automatically induced for any language with a treebank. Because a joint language model inherently predicts hidden events (tags), it can also be used to generate the best sequence of those events, i.e., tagging. We evaluated our model in the POS tagging task and observed similar results: the finegrain models outperform the POS model, while both outperform the state-of-the-art HMM POS taggers. We refer to (Filimonov and Harper, 2009) for details on these experiments. We plan to investigate how parser accuracy and data selection strategies, e.g., based on parser confidence scores, impact the performance of our model. We also plan on evaluating the model’s performance on other genres of speech, as well as in other tasks such as Machine Translation. We are also working on scaling our model further to accommodate amounts of data typical for modern large-scale ngram models. Finally, we plan to apply the technique to other languages with treebanks, such as Chinese and Arabic. We intend to release the source code of our model wi</context>
</contexts>
<marker>Filimonov, Harper, 2009</marker>
<rawString>Denis Filimonov and Mary Harper. 2009. Measuring tagging performance of a joint language model. In Proceedings of the Interspeech 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter A Heeman</author>
</authors>
<title>POS tags and decision trees for language modeling. In</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>129--137</pages>
<contexts>
<context position="4093" citStr="Heeman, 1999" startWordPosition="664" endWordPosition="666"> stochastic variables, as in Eq. 2. p(witi|wi−1 1 ti−1 1 ) (2) i-1 i−1 Et1...ti p(witi |w1 t1 )p(wZ 1 ti−1 1 ) �t1...ti−1 p(wi−1 1 ti−1 1 ) Models of this type, which we call joint models since they essentially predict joint events of words and some random variable(s), include (Chelba and Jelinek, 2000) which used POS tags in combination with “parser instructions” for constructing a full parse tree in a left-to-right manner; (Wang et al., 2003) used SuperARVs (complex tuples of dependency information) without resolving the dependencies, thus called almost parsing; (Niesler and Woodland, 1996; Heeman, 1999) utilize part of speech (POS) tags. Note that some models reduce the context by making the following approximation: p(witi|wi−1 1 ti−1 1 ) ^ p(wi|ti) p(ti|ti−1 1 ) (3) thus, transforming the problem into a standard HMM application. However, these models perform poorly and have only been able to improve over the ngram model when interpolated with it (Niesler and Woodland, 1996). Although joint models have the potential to better express variability in word usage through the introduction of additional latent variables, they do not necessarily perform better because the increased dimensionality o</context>
<context position="5926" citStr="Heeman, 1999" startWordPosition="969" endWordPosition="971">n be used with a variety of tagsets. In Section 2, we describe those that we used in our experiments. Rather than tailoring our model to these tagsets, we aim for flexibility and propose an information theoretic framework for quick evaluation for tagsets, thus simplifying the creation of new tagsets. We show that our model with fine-grain tagsets outperform the coarser POS model, as well as the ngram baseline, in Section 5. • Address the challenges that arise in a joint language model with fine-grain tags. While the idea of using joint language modeling is not novel (Chelba and Jelinek, 2000; Heeman, 1999), nor is the idea of using fine-grain tags (Bangalore, 1996; Wang et al., 2003), none of prior papers focus on the issues that arise from the combination of joint language modeling with fine-grain tags, both in terms of reliable parameter estimation and scalability in the face of the increased computational complexity. We dedicate Sections 3 and 4 to this problem. In Section 6, we summarize conclusions and lay out directions for future work. 2 Structural Information As we have mentioned, the selection of the random variable in Eq. 2 is extremely important for the performance of the model. On o</context>
<context position="12705" citStr="Heeman, 1999" startWordPosition="2139" endWordPosition="2140">parameter space of the joint model necessitate the use of dimensionality reduction measures in order to make the model computationally tractable and to allow for accurate estimation of the model’s parameters. We also want the model to be able to easily accommodate additional sources of information such as morphological features, prosody, etc. In the rest of this section, we discuss avenues we have taken to address these problems. 3.1 Decision Tree Clustering Binary decision tree clustering has been shown to be effective for reducing the parameter space in language modeling (Bahl et al., 1990; Heeman, 1999) and other language processing applications, e.g., (Magerman, 1994). Like any clustering algorithm, it can be represented by a function H that maps the space of histories to a set of equivalence classes. / i−1 i−1 / / i−1 i−1 plwiti|wi−41t−n+1) �&apos;plwiti|Hlwi−n+1ti−n+1)) (4) While the tree construction algorithm is fairly standard – to recursively select binary questions about the history optimizing some function – there are important decisions to make in terms of which questions to ask and which function to optimize. In the remainder of this section, we discuss the decisions we made regarding </context>
<context position="14245" citStr="Heeman, 1999" startWordPosition="2396" endWordPosition="2397"> computed in a joint model; whereas, we need to distinguish between the factors that are given or computed and the factors that the model must predict stochastically. We call these types of factors overt and hidden, respectively. Examples of overt factors include surface words, morphological features such as suffixes, case information when available, etc., and the hidden factors are POS, SuperARVs, or other tags. Henceforth, we will use word to represent the set of overt factors and tag to represent the set of hidden factors. Bits 2.5 0.5 1.5 3 2 0 1 1117 3.3 Hidden Factors Tree Similarly to (Heeman, 1999), we construct a binary tree where each tag is a leaf; we will refer to this tree as the Hidden Factors Tree (HFT). We use Minimum Discriminative Information (MDI) algorithm (Zitouni, 2007) to build the tree. The HFT represents a hierarchical clustering of the tag space. One of the reasons for doing this is to allow questions about subsets of tags rather than individual tags alone4. Unlike (Heeman, 1999), where the tree of tags was only used to create questions, this representation of the tag space is, in addition, a key feature of our decoding optimizations, which we discuss in Section 4. 3.4</context>
</contexts>
<marker>Heeman, 1999</marker>
<rawString>Peter A. Heeman. 1999. POS tags and decision trees for language modeling. In In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 129–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
</authors>
<title>SelfTraining PCFG grammars with latent annotations across languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the EMNLP</booktitle>
<contexts>
<context position="25941" citStr="Huang and Harper, 2009" startWordPosition="4474" endWordPosition="4477">f topics than the Wall Street Journal, we eliminated the most irrelevant stories based on their trigram coverage by sections 00-22 of WSJ. We also eliminated sentences over 120 words, because the parser’s performance drops significantly on long sentences. After parsing the corpus, we deleted sentences that were assigned a very low probability by the parser. Overall we removed only a few percent of the data; however, we believe that such a rigorous approach to data cleaning is important for building discriminating models. Parse trees were produced by an extended version of the Berkeley parser (Huang and Harper, 2009). We trained the parser on a combination of the BN and WSJ treebanks, preprocessed to make them more consistent with each other. We also modified the trees for the speech recognition task by replacing numbers and abbreviations with their verbalized forms. We pre-processed the NYT corpus in the same way, and parsed it. After that, we removed punctuation and downcased words. For the ngram model, we used text processed in the same way. In head and parent models, tag vocabularies contain approximately 1,500 tags each, while the SuperARV model has approximately 1,400 distinct SuperARVs, most of whi</context>
</contexts>
<marker>Huang, Harper, 2009</marker>
<rawString>Zhongqiang Huang and Mary Harper. 2009. SelfTraining PCFG grammars with latent annotations across languages. In Proceedings of the EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Natural language parsing as statistical pattern recognition.</title>
<date>1994</date>
<booktitle>Ph.D. thesis,</booktitle>
<location>Stanford, CA, USA.</location>
<contexts>
<context position="12772" citStr="Magerman, 1994" startWordPosition="2147" endWordPosition="2148">onality reduction measures in order to make the model computationally tractable and to allow for accurate estimation of the model’s parameters. We also want the model to be able to easily accommodate additional sources of information such as morphological features, prosody, etc. In the rest of this section, we discuss avenues we have taken to address these problems. 3.1 Decision Tree Clustering Binary decision tree clustering has been shown to be effective for reducing the parameter space in language modeling (Bahl et al., 1990; Heeman, 1999) and other language processing applications, e.g., (Magerman, 1994). Like any clustering algorithm, it can be represented by a function H that maps the space of histories to a set of equivalence classes. / i−1 i−1 / / i−1 i−1 plwiti|wi−41t−n+1) �&apos;plwiti|Hlwi−n+1ti−n+1)) (4) While the tree construction algorithm is fairly standard – to recursively select binary questions about the history optimizing some function – there are important decisions to make in terms of which questions to ask and which function to optimize. In the remainder of this section, we discuss the decisions we made regarding these issues. 3.2 Factors The Factored Language Model (FLM) (Bilmes</context>
<context position="17714" citStr="Magerman, 1994" startWordPosition="3001" endWordPosition="3002">ntains the sum of the probabilities of the tags it dominates. This representation is designed to assist the decoding process described in Section 4. 3.7 Smoothing In order to estimate probability distributions at the leaves of the history tree, we use the following recursive formula: ˜pn(witi) = Anpn(witi) + (1 − An)˜pn′(witi) (5) where n′ is the n-th node’s parent, pn(witi) is the distribution at node n (see Figure 3). The 1118 root of the tree is interpolated with the distribution punif(witi) = 1 |V |pML(ti|wi)5. To estimate interpolation parameters An, we use the EM algorithm described in (Magerman, 1994); however, rather than setting aside a separate development set of optimizing An, we use 4-fold cross validation and take the geometric mean of the resulting coefficients6. We chose this approach because a small development set often does not overlap with the training set for low-count nodes, leading the EM algorithm to set An = 0 for those nodes. Let us consider one leaf of the history tree in isolation. Its context can be represented by the path to the root, i.e., the sequence of questions and answers q1, ... q(n′)′qn′ (with q1 being the answer to the topmost question): ˜pn(witi) = ˜p(witi|q</context>
</contexts>
<marker>Magerman, 1994</marker>
<rawString>David M. Magerman. 1994. Natural language parsing as statistical pattern recognition. Ph.D. thesis, Stanford, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Martin</author>
<author>Jorg Liermann</author>
<author>Hermann Ney</author>
</authors>
<title>Algorithms for bigram and trigram word clustering. In Speech Communication,</title>
<date>1998</date>
<pages>1253--1256</pages>
<contexts>
<context position="15083" citStr="Martin et al., 1998" startWordPosition="2540" endWordPosition="2544">epresents a hierarchical clustering of the tag space. One of the reasons for doing this is to allow questions about subsets of tags rather than individual tags alone4. Unlike (Heeman, 1999), where the tree of tags was only used to create questions, this representation of the tag space is, in addition, a key feature of our decoding optimizations, which we discuss in Section 4. 3.4 Questions The context space is partitioned by means of binary questions. We use different types of questions for hidden and overt factors. • Questions about surface words are constructed using the Exchange algorithm (Martin et al., 1998). This algorithm takes the set of words that appear at a certain position in the training data associated with the current node in the history tree and divides the set into two complementary subsets greedily optimizing some target function (we use the average entropy of the marginalized word distribution, the same as for question selection). Note that since the algorithm only operates on the words that appear in the training data, we need to do something more to account for the unseen words. Thus, to represent this type of question, we create the history tree structure depicted in Fig. 4. For </context>
</contexts>
<marker>Martin, Liermann, Ney, 1998</marker>
<rawString>Sven Martin, Jorg Liermann, and Hermann Ney. 1998. Algorithms for bigram and trigram word clustering. In Speech Communication, pages 1253–1256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas R Niesler</author>
<author>Phil C Woodland</author>
</authors>
<title>A variable-length category-based n-gram language model.</title>
<date>1996</date>
<booktitle>Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<contexts>
<context position="4078" citStr="Niesler and Woodland, 1996" startWordPosition="660" endWordPosition="663"> over all assignments of the stochastic variables, as in Eq. 2. p(witi|wi−1 1 ti−1 1 ) (2) i-1 i−1 Et1...ti p(witi |w1 t1 )p(wZ 1 ti−1 1 ) �t1...ti−1 p(wi−1 1 ti−1 1 ) Models of this type, which we call joint models since they essentially predict joint events of words and some random variable(s), include (Chelba and Jelinek, 2000) which used POS tags in combination with “parser instructions” for constructing a full parse tree in a left-to-right manner; (Wang et al., 2003) used SuperARVs (complex tuples of dependency information) without resolving the dependencies, thus called almost parsing; (Niesler and Woodland, 1996; Heeman, 1999) utilize part of speech (POS) tags. Note that some models reduce the context by making the following approximation: p(witi|wi−1 1 ti−1 1 ) ^ p(wi|ti) p(ti|ti−1 1 ) (3) thus, transforming the problem into a standard HMM application. However, these models perform poorly and have only been able to improve over the ngram model when interpolated with it (Niesler and Woodland, 1996). Although joint models have the potential to better express variability in word usage through the introduction of additional latent variables, they do not necessarily perform better because the increased d</context>
</contexts>
<marker>Niesler, Woodland, 1996</marker>
<rawString>Thomas R. Niesler and Phil C. Woodland. 1996. A variable-length category-based n-gram language model. Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, 1:164–167 vol. 1, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott M Thede</author>
<author>Mary P Harper</author>
</authors>
<title>A secondorder hidden markov model for part-of-speech tagging.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the ACL,</booktitle>
<pages>175--182</pages>
<contexts>
<context position="19191" citStr="Thede and Harper, 1999" startWordPosition="3257" endWordPosition="3260">on at the backoff node’s grandparent (see node A in Fig. 4): i−1 i−1 pB(witi|wi−n+1ti−n+1) = i−1 αA˜pbo(witi |wi−n+2ti−1 i−n+2) + (1 − αA)˜pA(witi) How to compute aA is an open question. For this study, we use a simple heuristic based on observation that the further node A is from the root the more reliable the distribution ˜pA(witi) is, and hence aA is lower. The formula we use is as follows: 1 �/1 + distanceToRoot(A) 5We use this distribution rather than uniform joint distribution 1 |V ||T |because we do not want to allow word-tag pairs that have never been observed. The idea is similar to (Thede and Harper, 1999). 6To avoid a large number of zeros due to the product, we set a minimum for λ to be 10−7. 7The lower order model is constructed by the same algorithm, although with smaller context. Note that the lower order model can back off on words or tags, or both. In this paper we backoff both on words and tags, i.e., p(witi|wi−1 i−2ti−1 i−2) backs off to p(witi|wi−1ti−1), which in turn backs off to the unigram p(witi). Figure 4: A fragment of the decision tree with a backoff node. 5 U 5 is the set of words observed in the training data at the node A. To account for unseen words, we add the backoff node</context>
</contexts>
<marker>Thede, Harper, 1999</marker>
<rawString>Scott M. Thede and Mary P. Harper. 1999. A secondorder hidden markov model for part-of-speech tagging. In Proceedings of the 37th Annual Meeting of the ACL, pages 175–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
<author>Mary P Harper</author>
</authors>
<title>The SuperARV language model: investigating the effectiveness of tightly integrating multiple knowledge sources.</title>
<date>2002</date>
<booktitle>In EMNLP ’02: Proceedings of the ACL-02 conference on Empirical methods in natural language processing,</booktitle>
<pages>238--247</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8178" citStr="Wang and Harper, 2002" startWordPosition="1362" endWordPosition="1365">can be directly derived from its syntactic parse. SuperARVs encode lexical information as well as syntactic and semantic constraints in a uniform representation that is much more fine-grained than POS. It is a four-tuple (C; F; R+; D), where C is the lexical category of the word, F is a vector of lexical features for the word, R+ is a set of governor and need labels that indicate the function of the word in the sentence and the types of words it needs, and D represents the relative position of the word and its dependents. We refer the reader to the literature for further details on SuperARVs (Wang and Harper, 2002; Wang et al., 2003). SuperARVs can be produced from parse trees by applying deterministic rules. In this work we use SuperARVs as individual tags and do not cluster them based of their structure. While SuperARVs are very attractive for language modeling, developing such a rich set of annotations for a new language would require a large amount of human effort. We propose two other types of tags which have not been applied to this task, although similar information has been used in parsing. 2.3 Modifee Tag This tag is a combination of the word’s POS tag and the POS tag of its governor role. We </context>
<context position="24841" citStr="Wang and Harper, 2002" startWordPosition="4292" endWordPosition="4295"> created from the same parse trees that were used to produce POS and fine-grain tags. All our models, including SuperARV, use trigram context. We include standard trigram, four-gram, and fivecompute is | �pSin = � � S′EINS �p S′ in p(wi−2|HS′) tETS′ 1120 gram models for reference. The ngram models were trained using SRILM toolkit with interpolated modified Kneser-Ney smoothing. We evaluate our model with an nbest rescoring task using 100-best lists from the DARPA WSJ’93 and WSJ’92 20k open vocabulary data sets. The details on the acoustic model used to produce the nbest lists can be found in (Wang and Harper, 2002). Since the data sets are small, we combined the 93et and 93dt sets for evaluation and used 92et for the optimization8. We transformed the nbest lists to match PTB tokenization, namely separating possessives from nouns, n’t from auxiliary verbs in contractions, as well as contractions from personal pronouns. All language models were trained on the NYT 1994-1995 section of the English Gigaword corpus (approximately 70M words). Since the New York Times covers a wider range of topics than the Wall Street Journal, we eliminated the most irrelevant stories based on their trigram coverage by section</context>
</contexts>
<marker>Wang, Harper, 2002</marker>
<rawString>Wen Wang and Mary P. Harper. 2002. The SuperARV language model: investigating the effectiveness of tightly integrating multiple knowledge sources. In EMNLP ’02: Proceedings of the ACL-02 conference on Empirical methods in natural language processing, pages 238–247, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
<author>Mary P Harper</author>
<author>Andreas Stolcke</author>
</authors>
<title>The robustness of an almost-parsing language model given errorful training data.</title>
<date>2003</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<contexts>
<context position="3928" citStr="Wang et al., 2003" startWordPosition="639" endWordPosition="642">nce on Empirical Methods in Natural Language Processing, pages 1114–1123, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP word we need to sum over all assignments of the stochastic variables, as in Eq. 2. p(witi|wi−1 1 ti−1 1 ) (2) i-1 i−1 Et1...ti p(witi |w1 t1 )p(wZ 1 ti−1 1 ) �t1...ti−1 p(wi−1 1 ti−1 1 ) Models of this type, which we call joint models since they essentially predict joint events of words and some random variable(s), include (Chelba and Jelinek, 2000) which used POS tags in combination with “parser instructions” for constructing a full parse tree in a left-to-right manner; (Wang et al., 2003) used SuperARVs (complex tuples of dependency information) without resolving the dependencies, thus called almost parsing; (Niesler and Woodland, 1996; Heeman, 1999) utilize part of speech (POS) tags. Note that some models reduce the context by making the following approximation: p(witi|wi−1 1 ti−1 1 ) ^ p(wi|ti) p(ti|ti−1 1 ) (3) thus, transforming the problem into a standard HMM application. However, these models perform poorly and have only been able to improve over the ngram model when interpolated with it (Niesler and Woodland, 1996). Although joint models have the potential to better exp</context>
<context position="6005" citStr="Wang et al., 2003" startWordPosition="982" endWordPosition="985">we used in our experiments. Rather than tailoring our model to these tagsets, we aim for flexibility and propose an information theoretic framework for quick evaluation for tagsets, thus simplifying the creation of new tagsets. We show that our model with fine-grain tagsets outperform the coarser POS model, as well as the ngram baseline, in Section 5. • Address the challenges that arise in a joint language model with fine-grain tags. While the idea of using joint language modeling is not novel (Chelba and Jelinek, 2000; Heeman, 1999), nor is the idea of using fine-grain tags (Bangalore, 1996; Wang et al., 2003), none of prior papers focus on the issues that arise from the combination of joint language modeling with fine-grain tags, both in terms of reliable parameter estimation and scalability in the face of the increased computational complexity. We dedicate Sections 3 and 4 to this problem. In Section 6, we summarize conclusions and lay out directions for future work. 2 Structural Information As we have mentioned, the selection of the random variable in Eq. 2 is extremely important for the performance of the model. On one hand, we would like for this variable to provide maximum information. On the</context>
<context position="8198" citStr="Wang et al., 2003" startWordPosition="1366" endWordPosition="1369"> from its syntactic parse. SuperARVs encode lexical information as well as syntactic and semantic constraints in a uniform representation that is much more fine-grained than POS. It is a four-tuple (C; F; R+; D), where C is the lexical category of the word, F is a vector of lexical features for the word, R+ is a set of governor and need labels that indicate the function of the word in the sentence and the types of words it needs, and D represents the relative position of the word and its dependents. We refer the reader to the literature for further details on SuperARVs (Wang and Harper, 2002; Wang et al., 2003). SuperARVs can be produced from parse trees by applying deterministic rules. In this work we use SuperARVs as individual tags and do not cluster them based of their structure. While SuperARVs are very attractive for language modeling, developing such a rich set of annotations for a new language would require a large amount of human effort. We propose two other types of tags which have not been applied to this task, although similar information has been used in parsing. 2.3 Modifee Tag This tag is a combination of the word’s POS tag and the POS tag of its governor role. We designed it to resem</context>
<context position="24199" citStr="Wang et al., 2003" startWordPosition="4184" endWordPosition="4187">g with one of its incoming links (to some depth according to the size of the context). 5 Experimental Setup To evaluate the impact of fine-grain tags on language modeling, we trained our model with five settings: In the first model, questions were restricted to be about overt factors only, thus making it a tree-based word model. In the second model, we used POS tags. To evaluate the effect of finegrain tags, we train two models: head and parent described in Section 2.3 and Section 2.4 respectively. Since our joint model can be used with any kind of tags, we also trained it with SuperARV tags (Wang et al., 2003). The SuperARVs were created from the same parse trees that were used to produce POS and fine-grain tags. All our models, including SuperARV, use trigram context. We include standard trigram, four-gram, and fivecompute is | �pSin = � � S′EINS �p S′ in p(wi−2|HS′) tETS′ 1120 gram models for reference. The ngram models were trained using SRILM toolkit with interpolated modified Kneser-Ney smoothing. We evaluate our model with an nbest rescoring task using 100-best lists from the DARPA WSJ’93 and WSJ’92 20k open vocabulary data sets. The details on the acoustic model used to produce the nbest lis</context>
</contexts>
<marker>Wang, Harper, Stolcke, 2003</marker>
<rawString>Wen Wang, Mary P. Harper, and Andreas Stolcke. 2003. The robustness of an almost-parsing language model given errorful training data. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Frederick Jelinek</author>
</authors>
<title>Random forests in language modeling.</title>
<date>2004</date>
<booktitle>In in Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5104" citStr="Xu and Jelinek, 2004" startWordPosition="824" endWordPosition="827">joint models have the potential to better express variability in word usage through the introduction of additional latent variables, they do not necessarily perform better because the increased dimensionality of the context substantially increases the already complex problem of parameter estimation. The complexity of the space also makes computation of the probability a challenge because of space and time constraints. This makes the choice of the random variables a matter of utmost importance. The model presented in this paper has some elements borrowed from prior work, notably (Heeman, 1999; Xu and Jelinek, 2004), while others are novel. 1.1 Paper Outline The message we aim to deliver in this paper can be summarized in two theses: • Use fine-grain syntactic tags in a joint LM. We propose a joint language model that can be used with a variety of tagsets. In Section 2, we describe those that we used in our experiments. Rather than tailoring our model to these tagsets, we aim for flexibility and propose an information theoretic framework for quick evaluation for tagsets, thus simplifying the creation of new tagsets. We show that our model with fine-grain tagsets outperform the coarser POS model, as well </context>
</contexts>
<marker>Xu, Jelinek, 2004</marker>
<rawString>Peng Xu and Frederick Jelinek. 2004. Random forests in language modeling. In in Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Imed Zitouni</author>
</authors>
<title>Backoff hierarchical class ngram language models: effectiveness to model unseen events in speech recognition.</title>
<date>2007</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>21</volume>
<issue>1</issue>
<contexts>
<context position="2853" citStr="Zitouni, 2007" startWordPosition="457" endWordPosition="458">quite sparse and requires sophisticated techniques for reliable probability estimation (Chen and Goodman, 1996). While the ngram models perform fairly well, they are only capable of capturing very shallow knowledge of the language. There is extensive literature on a variety of methods that have been used to imbue models with syntactic and semantic information in different ways. These methods can be broadly categorized into two types: • The first method uses surface words within its context, sometimes organizing them into deterministic classes. Models of this type include: (Brown et al., 1992; Zitouni, 2007), which use semantic word clustering, and (Bahl et al., 1990), which uses variablelength context. • The other method adds stochastic variables to express the ambiguous nature of surface words2. To obtain the probability of the next 1Real applications use argmax.. p(A|wl) )·p(w�1 )�·n0 instead of Eq. 1, where α and ,Q are set to optimize a heldout set. 2These variables have to be predicted by the model. n p(wn1) = i=1 1114 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1114–1123, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP word we need to sum </context>
<context position="14434" citStr="Zitouni, 2007" startWordPosition="2429" endWordPosition="2430">f factors overt and hidden, respectively. Examples of overt factors include surface words, morphological features such as suffixes, case information when available, etc., and the hidden factors are POS, SuperARVs, or other tags. Henceforth, we will use word to represent the set of overt factors and tag to represent the set of hidden factors. Bits 2.5 0.5 1.5 3 2 0 1 1117 3.3 Hidden Factors Tree Similarly to (Heeman, 1999), we construct a binary tree where each tag is a leaf; we will refer to this tree as the Hidden Factors Tree (HFT). We use Minimum Discriminative Information (MDI) algorithm (Zitouni, 2007) to build the tree. The HFT represents a hierarchical clustering of the tag space. One of the reasons for doing this is to allow questions about subsets of tags rather than individual tags alone4. Unlike (Heeman, 1999), where the tree of tags was only used to create questions, this representation of the tag space is, in addition, a key feature of our decoding optimizations, which we discuss in Section 4. 3.4 Questions The context space is partitioned by means of binary questions. We use different types of questions for hidden and overt factors. • Questions about surface words are constructed u</context>
</contexts>
<marker>Zitouni, 2007</marker>
<rawString>Imed Zitouni. 2007. Backoff hierarchical class ngram language models: effectiveness to model unseen events in speech recognition. Computer Speech &amp; Language, 21(1):88–104.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>