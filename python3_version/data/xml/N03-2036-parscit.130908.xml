<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014591">
<title confidence="0.992">
A Phrase-Based Unigram Model for Statistical Machine Translation
</title>
<author confidence="0.814495">
Christoph Tillmann and Fei Xia
</author>
<affiliation confidence="0.564238">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.595087">
Yorktown Heights, NY 10598
</address>
<email confidence="0.997584">
{ctill,feixia}@us.ibm.com
</email>
<sectionHeader confidence="0.995616" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999946833333333">
In this paper, we describe a phrase-based un-
igram model for statistical machine transla-
tion that uses a much simpler set of model
parameters than similar phrase-based models.
The units of translation are blocks - pairs of
phrases. During decoding, we use a block un-
igram model and a word-based trigram lan-
guage model. During training, the blocks are
learned from source interval projections using
an underlying word alignment. We show exper-
imental results on block selection criteria based
on unigram counts and phrase length.
</bodyText>
<sectionHeader confidence="0.988084" genericHeader="keywords">
1 Phrase-based Unigram Model
</sectionHeader>
<bodyText confidence="0.9997386">
Various papers use phrase-based translation systems (Och
et al., 1999; Marcu and Wong, 2002; Yamada and Knight,
2002) that have shown to improve translation quality
over single-word based translation systems introduced in
(Brown et al., 1993). In this paper, we present a simi-
lar system with a much simpler set of model parameters.
Specifically, we compute the probability of a block se-
quence bn1 . The block sequence probability Pr(bn1) is de-
composed into conditional probabilities using the chain
rule:
</bodyText>
<equation confidence="0.993604888888889">
n
Pr(bn1) ≈ Pr(bi|bi−1) (1)
i=1
n
= pα(bi|bi−1) · p(1−α)(bi|bi−1)
i=1
n
≈ pα(bi) · p(1−α)(bi|bi−1)
i=1
</equation>
<bodyText confidence="0.9411275">
We try to find the block sequence that maximizes Pr(bn1 ):
bn1 = arg maxbn 1 P r(bn 1 ). The model proposed is a joint
</bodyText>
<figureCaption confidence="0.991188">
Figure 1: A block sequence that jointly generates 4 target
and source phrases.
</figureCaption>
<bodyText confidence="0.9993608">
model as in (Marcu and Wong, 2002), since target and
source phrases are generated jointly. The approach is il-
lustrated in Figure 1. The source phrases are given on the
x-axis and the target phrases are given on the y-axis.
The two types of parameters in Eq 1 are defined as:
</bodyText>
<listItem confidence="0.999266111111111">
• Block unigram model p(bi): we compute unigram
probabilities for the blocks. The blocks are simpler
than the alignment templates in (Och et al., 1999) in
that they do not have any internal structure.
• Trigram language model: the probability
p(bi|bi−1) between adjacent blocks is computed as
the probability of the first target word in the target
clump of bi given the final two words of the target
clump of bi−1.
</listItem>
<bodyText confidence="0.9509764">
The exponent α is set in informal experiments to be 0.5.
No other parameters such as distortion probabilities are
used.
To select blocks b from training data, we compute uni-
gram block co-occurrence counts N(b). N(b) cannot be
</bodyText>
<figure confidence="0.9993030625">
S S S
2 3 4
S
1
T
1
T
4
T
3
T
2
Source
Target
Target
Source
</figure>
<figureCaption confidence="0.998839">
Figure 2: The left picture shows three blocks that are
</figureCaption>
<bodyText confidence="0.968753195121951">
learned from projecting three source intervals. The right
picture shows three blocks that cannot be obtain from
source interval projections.
computed for all blocks in the training data: we would
obtain hundreds of millions of blocks. The blocks are
restricted by an underlying word alignment. The word
alignment is obtained from an HMM Viterbi training (Vo-
gel et al., 1996). The HMM Viterbi training is carried
out twice with English as target language and Chinese as
source language and vice versa. We take the intersection
of the two alignments as described in (Och et al., 1999).
To generate blocks from the intersection, we proceed as
follows: for each source interval [j, j&apos;], we compute the
minimum target index i and maximum target index i&apos; of
the intersection alignment points that fall into the interval
[j, j&apos;]. The approach is illustrated in Figure 2. In the left
picture, for example, the source interval [1, 3] is projected
into the target interval [1, 3] . The pair ([j, j&apos;], [i, i&apos;])
together with the words at the corresponding positions
yields a block learned from this training sentence pair.
For source intervals without alignment points in them, no
blocks are produced. We also extend a block correspond-
ing to the interval pair ([j, j&apos;], [i, i&apos;]) by elements on the
union of the two Viterbi HMM alignments. A similar
block selection scheme has been presented in (Och et al.,
1999). Finally, the target and source phrases are restricted
to be equal or less than 8 words long. This way we obtain
23 millions blocks on our training data including blocks
that occur only once. This baseline set is further filtered
using the unigram count N(b): Nk denotes the set of
blocks b for which N(b) ≥ k. Blocks where the target
and the source clump are of length 1 are kept regardless
of their count.1 We compute the unigram probability p(b)
as relative frequency over all selected blocks.
We also tried a more restrictive projection scheme: source
intervals are projected into target intervals and the reverse
projection of the target interval has to be included in the
original source interval. The results for this symmet-
rical projection are currently worse, since some blocks
with longer target intervals are excluded. An example
of 4 blocks obtained from the training data is shown in
</bodyText>
<footnote confidence="0.965646666666667">
1To apply the restrictions exhaustively, we have imple-
mented tree-based data structures to store the 23 million blocks
with phrases of up to length 8 in about 1.6 gigabyte of RAM.
</footnote>
<figureCaption confidence="0.910504333333333">
Figure 3: An example of 4 recursively nested blocks
b1, b2, b3, b4.
Figure 3. ’$DATE’ is a placeholder for a date expres-
</figureCaption>
<bodyText confidence="0.955022875">
sion. Block b4 contains the blocks b1 to b3. All 4 blocks
are selected in training: the unigram decoder prefers
b4 even if b1,b2, and b3 are much more frequent. The
solid alignment points are elements from the intersec-
tion, the striped alignment points are elements from the
union. Using the union points, we can learn one-to-many
block translations; for example, the pair (c1,’Xinhua news
agency’) is learned from the training data.
We use a DP-based beam search procedure similar to the
one presented in (Tillmann, 2001). We maximize over
all block segmentations bn1 for which the source phrases
yield a segmentation of the input source sentence, gen-
erating the target sentence simultaneously. In the current
experiments, decoding without block re-ordering yields
the best translation results. The decoder translates about
180 words per second.
</bodyText>
<sectionHeader confidence="0.992048" genericHeader="introduction">
2 Experimental Results
</sectionHeader>
<bodyText confidence="0.852093714285714">
The translation system is tested on a Chinese-to-English
translation task. The training data come from several
news sources. For testing, we use the DARPA/NIST MT
2001 dry-run testing data, which consists of 793 sen-
tences with 20,333 words arranged in 80 documents.2
The training data is provided by the LDC and labeled by
NIST as the Large Data condition for the MT 2002 eval-
uation. The Chinese sentences are segmented into words.
The training data contains 23.7 million Chinese and 25.3
million English words.
Experimental results are presented in Table 1 and Ta-
ble 2. Table 1 shows the effect of the unigram threshold.
The second column shows the number of blocks selected.
The third column reports the BLEU score (Papineni et al.,
2002) along with 95% confidence interval. We use IBM
2We did not use the first 25 documents of the 105-document
dry-run test set because they were used as a development test set
before the dry-run and were subsequently added to our training
data.
selected from the training data. Longer phrases which
occur less frequently do not help much.
</bodyText>
<tableCaption confidence="0.881878">
Table 1: Effect of the unigram threshold on the BLEU
score. The maximum phrase length is 8.
</tableCaption>
<table confidence="0.999958571428571">
Selection # blocks BLEUr4n4
Restriction selected
IBM1 baseline 1.23M 0.11 ± 0.01
N2 4.23 M 0.18 ± 0.02
N3 1.22 M 0.18 ± 0.01
N4 0.84 M 0.17 ± 0.01
N5 0.65 M 0.17 ± 0.01
</table>
<tableCaption confidence="0.9883845">
Table 2: Effect of the maximum phrase length on the
BLEU score. The unigram threshold is N(b) ≥ 2.
</tableCaption>
<table confidence="0.9995721">
maximum # blocks BLEUr4n4
phrase length selected
8 4.23 M 0.18 ± 0.02
7 3.76 M 0.17 ± 0.02
6 3.26 M 0.17 ± 0.01
5 2.73 M 0.17 ± 0.01
4 2.16 M 0.17 ± 0.01
3 1.51 M 0.16 ± 0.01
2 0.77 M 0.14 ± 0.01
1 0.16 M 0.12 ± 0.01
</table>
<bodyText confidence="0.9999027">
Model 1 as a baseline model which is similar to our block
model: neither model uses distortion or alignment proba-
bilities. The best results are obtained for the N2 and the
N3 sets.
The N3 set uses only 1.22 million blocks in contrast to
N2 which has 4.23 million blocks. This indicates that the
number of blocks can be reduced drastically without af-
fecting the translation performance significantly. Table 2
shows the effect of the maximum phrase length on the
BLEU score for the N2 block set. Including blocks with
longer phrases actually helps to improve performance, al-
though length 4 already obtains good results.
We also ran the N2 on the June 2002 DARPA TIDES
Large Data evaluation test set. Six research sites and
four commercial off-the-shelf systems were evaluated in
Large Data track. A majority of the systems were phrase-
based translation systems. For comparison with other
sites, we quote the NIST score (Doddington, 2002) on
this test set: N2 system scores 7.44 whereas the official
top two systems scored 7.65 and 7.34 respectively.
</bodyText>
<sectionHeader confidence="0.999456" genericHeader="method">
3 Conclusion
</sectionHeader>
<bodyText confidence="0.999941">
In this paper, we described a phrase-based unigram model
for statistical machine translation. The model is much
simpler than other phrase-based statistical models. We
experimented with different restrictions on the phrases
</bodyText>
<sectionHeader confidence="0.925981" genericHeader="method">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.999645">
This work was partially supported by DARPA and mon-
itored by SPAWAR under contract No. N66001-99-2-
8916.
</bodyText>
<sectionHeader confidence="0.998521" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996587725">
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263–311.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. of the Second International Confer-
ence ofHuman Language Technology Research, pages
138–145, March.
Daniel Marcu and William Wong. 2002. A Phrased-
Based, Joint Probability Model for Statistical Machine
Translation. In Proc. of the Conf. on Empirical Meth-
ods in Natural Language Processing (EMNLP 02),
pages 133–139, Philadelphia, PA, July.
Franz-Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved Alignment Models for Statistical Ma-
chine Translation. In Proc. of the Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Very Large Corpora (EMNLP/VLC 99), pages 20–28,
College Park, MD, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of machine translation. In Proc. of the
40th Annual Conf. of the Association for Computa-
tional Linguistics (ACL 02), pages 311–318, Philadel-
phia, PA, July.
Christoph Tillmann. 2001. Word Re-Ordering and Dy-
namic Programming based Search Algorithm for Sta-
tistical Machine Translation. Ph.D. thesis, University
of Technology, Aachen, Germany.
Stefan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM BasedWord Alignment in Statistical Ma-
chine Translation. In Proc. of the 16th Int. Conf.
on Computational Linguistics (COLING 1996), pages
836–841, Copenhagen, Denmark, August.
Kenji Yamada and Kevin Knight. 2002. A Decoder for
Syntax-based Statistical MT. In Proc. of the 40th An-
nual Conf. of the Association for Computational Lin-
guistics (ACL 02), pages 303–310, Philadelphia, PA,
July.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.001188">
<title confidence="0.999967">A Phrase-Based Unigram Model for Statistical Machine Translation</title>
<author confidence="0.998673">Christoph Tillmann</author>
<author confidence="0.998673">Fei</author>
<affiliation confidence="0.7637705">IBM T.J. Watson Research Yorktown Heights, NY</affiliation>
<abstract confidence="0.985598796954314">In this paper, we describe a phrase-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrase-based models. The units of translation are blocks pairs of phrases. During decoding, we use a block unigram model and a word-based trigram language model. During training, the blocks are learned from source interval projections using an underlying word alignment. We show experimental results on block selection criteria based on unigram counts and phrase length. 1 Phrase-based Unigram Model Various papers use phrase-based translation systems (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002) that have shown to improve translation quality over single-word based translation systems introduced in (Brown et al., 1993). In this paper, we present a similar system with a much simpler set of model parameters. Specifically, we compute the probability of a block se- . The block sequence probability decomposed into conditional probabilities using the chain rule: n n = n ≈ try to find the block sequence that maximizes = arg 1P 1The model proposed is a joint 1: A block sequence that jointly generates and source phrases. model as in (Marcu and Wong, 2002), since target and source phrases are generated jointly. The approach is illustrated in Figure 1. The source phrases are given on the and the target phrases are given on the The two types of parameters in Eq 1 are defined as: Block unigram model compute unigram probabilities for the blocks. The blocks are simpler than the alignment templates in (Och et al., 1999) in that they do not have any internal structure. Trigram language model: adjacent blocks is computed as the probability of the first target word in the target of the final two words of the target of exponent set in informal experiments to be No other parameters such as distortion probabilities are used. select blocks training data, we compute uniblock co-occurrence counts be S S S 2 3 4 S 1 T 1 T 4 T 3 T 2 Source Target Target Source Figure 2: The left picture shows three blocks that are learned from projecting three source intervals. The right picture shows three blocks that cannot be obtain from source interval projections. computed for all blocks in the training data: we would obtain hundreds of millions of blocks. The blocks are restricted by an underlying word alignment. The word alignment is obtained from an HMM Viterbi training (Vogel et al., 1996). The HMM Viterbi training is carried out twice with English as target language and Chinese as source language and vice versa. We take the intersection of the two alignments as described in (Och et al., 1999). To generate blocks from the intersection, we proceed as for each source interval we compute the target index maximum target index of the intersection alignment points that fall into the interval The approach is illustrated in Figure 2. In the left for example, the source interval 3] projected the target interval 3] The pair together with the words at the corresponding positions yields a block learned from this training sentence pair. For source intervals without alignment points in them, no blocks are produced. We also extend a block correspondto the interval pair elements on the union of the two Viterbi HMM alignments. A similar block selection scheme has been presented in (Och et al., 1999). Finally, the target and source phrases are restricted be equal or less than long. This way we obtain blocks on our training data including blocks that occur only once. This baseline set is further filtered the unigram count the set of which Blocks where the target the source clump are of length kept regardless their We compute the unigram probability as relative frequency over all selected blocks. We also tried a more restrictive projection scheme: source intervals are projected into target intervals and the reverse projection of the target interval has to be included in the original source interval. The results for this symmetrical projection are currently worse, since some blocks with longer target intervals are excluded. An example obtained from the training data is shown in apply the restrictions exhaustively, we have impletree-based data structures to store the blocks phrases of up to length about of RAM. 3: An example of nested blocks Figure 3. ’$DATE’ is a placeholder for a date expres- Block the blocks All are selected in training: the unigram decoder prefers if and much more frequent. The solid alignment points are elements from the intersection, the striped alignment points are elements from the union. Using the union points, we can learn one-to-many translations; for example, the pair news agency’) is learned from the training data. We use a DP-based beam search procedure similar to the one presented in (Tillmann, 2001). We maximize over block segmentations for which the source phrases yield a segmentation of the input source sentence, generating the target sentence simultaneously. In the current experiments, decoding without block re-ordering yields the best translation results. The decoder translates about per second. 2 Experimental Results The translation system is tested on a Chinese-to-English translation task. The training data come from several news sources. For testing, we use the DARPA/NIST MT dry-run testing data, which consists of senwith arranged in The training data is provided by the LDC and labeled by NIST as the Large Data condition for the MT 2002 evaluation. The Chinese sentences are segmented into words. training data contains Chinese and million English words. Experimental results are presented in Table 1 and Table 2. Table 1 shows the effect of the unigram threshold. The second column shows the number of blocks selected. third column reports the (Papineni et al., 2002) along with 95% confidence interval. We use IBM did not use the first 25 documents of the 105-document dry-run test set because they were used as a development test set before the dry-run and were subsequently added to our training data. selected from the training data. Longer phrases which occur less frequently do not help much. 1: Effect of the unigram threshold on the The maximum phrase length is Restriction # blocks selected BLEUr4n4 IBM1 baseline 1.23M N2 4.23 M N3 1.22 M N4 0.84 M N5 0.65 M Table 2: Effect of the maximum phrase length on the The unigram threshold is phrase length # blocks selected BLEUr4n4 8 4.23 M 7 3.76 M 6 3.26 M 5 2.73 M 4 2.16 M 3 1.51 M 2 0.77 M 1 0.16 M Model 1 as a baseline model which is similar to our block model: neither model uses distortion or alignment proba- The best results are obtained for the the uses only blocks in contrast to has blocks. This indicates that the number of blocks can be reduced drastically without affecting the translation performance significantly. Table 2 shows the effect of the maximum phrase length on the for the set. Including blocks with longer phrases actually helps to improve performance, allength obtains good results. We also ran the N2 on the June 2002 DARPA TIDES Large Data evaluation test set. Six research sites and four commercial off-the-shelf systems were evaluated in Large Data track. A majority of the systems were phrasebased translation systems. For comparison with other sites, we quote the NIST score (Doddington, 2002) on this test set: N2 system scores 7.44 whereas the official top two systems scored 7.65 and 7.34 respectively. 3 Conclusion In this paper, we described a phrase-based unigram model for statistical machine translation. The model is much simpler than other phrase-based statistical models. We experimented with different restrictions on the phrases Acknowledgment</abstract>
<note confidence="0.842905">This work was partially supported by DARPA and monitored by SPAWAR under contract No. N66001-99-2- 8916.</note>
<title confidence="0.77903">References</title>
<author confidence="0.882132">Peter F Brown</author>
<author confidence="0.882132">Vincent J Della Pietra</author>
<author confidence="0.882132">Stephen A Della</author>
<note confidence="0.740273743589744">Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estima- 19(2):263–311. George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence In of the Second International ConferofHuman Language Technology pages 138–145, March. Daniel Marcu and William Wong. 2002. A Phrased- Based, Joint Probability Model for Statistical Machine In of the Conf. on Empirical Methin Natural Language Processing (EMNLP pages 133–139, Philadelphia, PA, July. Franz-Josef Och, Christoph Tillmann, and Hermann Ney. 1999. Improved Alignment Models for Statistical Ma- Translation. In of the Joint Conf. on Empirical Methods in Natural Language Processing and Large Corpora (EMNLP/VLC pages 20–28, College Park, MD, June. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. BLEU: a Method for Automatic of machine translation. In of the 40th Annual Conf. of the Association for Computa- Linguistics (ACL pages 311–318, Philadelphia, PA, July. Tillmann. 2001. Re-Ordering and Dynamic Programming based Search Algorithm for Sta- Machine Ph.D. thesis, University of Technology, Aachen, Germany. Stefan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM BasedWord Alignment in Statistical Ma- Translation. In of the 16th Int. Conf. Computational Linguistics (COLING pages 836–841, Copenhagen, Denmark, August. Kenji Yamada and Kevin Knight. 2002. A Decoder for Statistical MT. In of the 40th Annual Conf. of the Association for Computational Lin- (ACL pages 303–310, Philadelphia, PA, July.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="988" citStr="Brown et al., 1993" startWordPosition="145" endWordPosition="148">units of translation are blocks - pairs of phrases. During decoding, we use a block unigram model and a word-based trigram language model. During training, the blocks are learned from source interval projections using an underlying word alignment. We show experimental results on block selection criteria based on unigram counts and phrase length. 1 Phrase-based Unigram Model Various papers use phrase-based translation systems (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002) that have shown to improve translation quality over single-word based translation systems introduced in (Brown et al., 1993). In this paper, we present a similar system with a much simpler set of model parameters. Specifically, we compute the probability of a block sequence bn1 . The block sequence probability Pr(bn1) is decomposed into conditional probabilities using the chain rule: n Pr(bn1) ≈ Pr(bi|bi−1) (1) i=1 n = pα(bi|bi−1) · p(1−α)(bi|bi−1) i=1 n ≈ pα(bi) · p(1−α)(bi|bi−1) i=1 We try to find the block sequence that maximizes Pr(bn1 ): bn1 = arg maxbn 1 P r(bn 1 ). The model proposed is a joint Figure 1: A block sequence that jointly generates 4 target and source phrases. model as in (Marcu and Wong, 2002), </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In Proc. of the Second International Conference ofHuman Language Technology Research,</booktitle>
<pages>138--145</pages>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proc. of the Second International Conference ofHuman Language Technology Research, pages 138–145, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A PhrasedBased, Joint Probability Model for Statistical Machine Translation.</title>
<date>2002</date>
<booktitle>In Proc. of the Conf. on Empirical Methods in Natural Language Processing (EMNLP 02),</booktitle>
<pages>133--139</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="837" citStr="Marcu and Wong, 2002" startWordPosition="123" endWordPosition="126">a phrase-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrase-based models. The units of translation are blocks - pairs of phrases. During decoding, we use a block unigram model and a word-based trigram language model. During training, the blocks are learned from source interval projections using an underlying word alignment. We show experimental results on block selection criteria based on unigram counts and phrase length. 1 Phrase-based Unigram Model Various papers use phrase-based translation systems (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002) that have shown to improve translation quality over single-word based translation systems introduced in (Brown et al., 1993). In this paper, we present a similar system with a much simpler set of model parameters. Specifically, we compute the probability of a block sequence bn1 . The block sequence probability Pr(bn1) is decomposed into conditional probabilities using the chain rule: n Pr(bn1) ≈ Pr(bi|bi−1) (1) i=1 n = pα(bi|bi−1) · p(1−α)(bi|bi−1) i=1 n ≈ pα(bi) · p(1−α)(bi|bi−1) i=1 We try to find the block sequence that maximizes Pr(bn1 ): bn1 = arg maxbn 1 P r(bn</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A PhrasedBased, Joint Probability Model for Statistical Machine Translation. In Proc. of the Conf. on Empirical Methods in Natural Language Processing (EMNLP 02), pages 133–139, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz-Josef Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Alignment Models for Statistical Machine Translation.</title>
<date>1999</date>
<booktitle>In Proc. of the Joint Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC 99),</booktitle>
<pages>20--28</pages>
<location>College Park, MD,</location>
<contexts>
<context position="815" citStr="Och et al., 1999" startWordPosition="119" endWordPosition="122">aper, we describe a phrase-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrase-based models. The units of translation are blocks - pairs of phrases. During decoding, we use a block unigram model and a word-based trigram language model. During training, the blocks are learned from source interval projections using an underlying word alignment. We show experimental results on block selection criteria based on unigram counts and phrase length. 1 Phrase-based Unigram Model Various papers use phrase-based translation systems (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002) that have shown to improve translation quality over single-word based translation systems introduced in (Brown et al., 1993). In this paper, we present a similar system with a much simpler set of model parameters. Specifically, we compute the probability of a block sequence bn1 . The block sequence probability Pr(bn1) is decomposed into conditional probabilities using the chain rule: n Pr(bn1) ≈ Pr(bi|bi−1) (1) i=1 n = pα(bi|bi−1) · p(1−α)(bi|bi−1) i=1 n ≈ pα(bi) · p(1−α)(bi|bi−1) i=1 We try to find the block sequence that maximizes Pr(bn1 ): bn</context>
<context position="3165" citStr="Och et al., 1999" startWordPosition="532" endWordPosition="535">icture shows three blocks that are learned from projecting three source intervals. The right picture shows three blocks that cannot be obtain from source interval projections. computed for all blocks in the training data: we would obtain hundreds of millions of blocks. The blocks are restricted by an underlying word alignment. The word alignment is obtained from an HMM Viterbi training (Vogel et al., 1996). The HMM Viterbi training is carried out twice with English as target language and Chinese as source language and vice versa. We take the intersection of the two alignments as described in (Och et al., 1999). To generate blocks from the intersection, we proceed as follows: for each source interval [j, j&apos;], we compute the minimum target index i and maximum target index i&apos; of the intersection alignment points that fall into the interval [j, j&apos;]. The approach is illustrated in Figure 2. In the left picture, for example, the source interval [1, 3] is projected into the target interval [1, 3] . The pair ([j, j&apos;], [i, i&apos;]) together with the words at the corresponding positions yields a block learned from this training sentence pair. For source intervals without alignment points in them, no blocks are p</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz-Josef Och, Christoph Tillmann, and Hermann Ney. 1999. Improved Alignment Models for Statistical Machine Translation. In Proc. of the Joint Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC 99), pages 20–28, College Park, MD, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Conf. of the Association for Computational Linguistics (ACL 02),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="6792" citStr="Papineni et al., 2002" startWordPosition="1140" endWordPosition="1143">urces. For testing, we use the DARPA/NIST MT 2001 dry-run testing data, which consists of 793 sentences with 20,333 words arranged in 80 documents.2 The training data is provided by the LDC and labeled by NIST as the Large Data condition for the MT 2002 evaluation. The Chinese sentences are segmented into words. The training data contains 23.7 million Chinese and 25.3 million English words. Experimental results are presented in Table 1 and Table 2. Table 1 shows the effect of the unigram threshold. The second column shows the number of blocks selected. The third column reports the BLEU score (Papineni et al., 2002) along with 95% confidence interval. We use IBM 2We did not use the first 25 documents of the 105-document dry-run test set because they were used as a development test set before the dry-run and were subsequently added to our training data. selected from the training data. Longer phrases which occur less frequently do not help much. Table 1: Effect of the unigram threshold on the BLEU score. The maximum phrase length is 8. Selection # blocks BLEUr4n4 Restriction selected IBM1 baseline 1.23M 0.11 ± 0.01 N2 4.23 M 0.18 ± 0.02 N3 1.22 M 0.18 ± 0.01 N4 0.84 M 0.17 ± 0.01 N5 0.65 M 0.17 ± 0.01 Tab</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of machine translation. In Proc. of the 40th Annual Conf. of the Association for Computational Linguistics (ACL 02), pages 311–318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>Word Re-Ordering and Dynamic Programming based Search Algorithm for Statistical Machine Translation.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Technology,</institution>
<location>Aachen, Germany.</location>
<contexts>
<context position="5704" citStr="Tillmann, 2001" startWordPosition="968" endWordPosition="969">ecursively nested blocks b1, b2, b3, b4. Figure 3. ’$DATE’ is a placeholder for a date expression. Block b4 contains the blocks b1 to b3. All 4 blocks are selected in training: the unigram decoder prefers b4 even if b1,b2, and b3 are much more frequent. The solid alignment points are elements from the intersection, the striped alignment points are elements from the union. Using the union points, we can learn one-to-many block translations; for example, the pair (c1,’Xinhua news agency’) is learned from the training data. We use a DP-based beam search procedure similar to the one presented in (Tillmann, 2001). We maximize over all block segmentations bn1 for which the source phrases yield a segmentation of the input source sentence, generating the target sentence simultaneously. In the current experiments, decoding without block re-ordering yields the best translation results. The decoder translates about 180 words per second. 2 Experimental Results The translation system is tested on a Chinese-to-English translation task. The training data come from several news sources. For testing, we use the DARPA/NIST MT 2001 dry-run testing data, which consists of 793 sentences with 20,333 words arranged in </context>
</contexts>
<marker>Tillmann, 2001</marker>
<rawString>Christoph Tillmann. 2001. Word Re-Ordering and Dynamic Programming based Search Algorithm for Statistical Machine Translation. Ph.D. thesis, University of Technology, Aachen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM BasedWord Alignment in Statistical Machine Translation.</title>
<date>1996</date>
<booktitle>In Proc. of the 16th Int. Conf. on Computational Linguistics (COLING</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="2957" citStr="Vogel et al., 1996" startWordPosition="495" endWordPosition="499">probabilities are used. To select blocks b from training data, we compute unigram block co-occurrence counts N(b). N(b) cannot be S S S 2 3 4 S 1 T 1 T 4 T 3 T 2 Source Target Target Source Figure 2: The left picture shows three blocks that are learned from projecting three source intervals. The right picture shows three blocks that cannot be obtain from source interval projections. computed for all blocks in the training data: we would obtain hundreds of millions of blocks. The blocks are restricted by an underlying word alignment. The word alignment is obtained from an HMM Viterbi training (Vogel et al., 1996). The HMM Viterbi training is carried out twice with English as target language and Chinese as source language and vice versa. We take the intersection of the two alignments as described in (Och et al., 1999). To generate blocks from the intersection, we proceed as follows: for each source interval [j, j&apos;], we compute the minimum target index i and maximum target index i&apos; of the intersection alignment points that fall into the interval [j, j&apos;]. The approach is illustrated in Figure 2. In the left picture, for example, the source interval [1, 3] is projected into the target interval [1, 3] . Th</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stefan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM BasedWord Alignment in Statistical Machine Translation. In Proc. of the 16th Int. Conf. on Computational Linguistics (COLING 1996), pages 836–841, Copenhagen, Denmark, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A Decoder for Syntax-based Statistical MT.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Conf. of the Association for Computational Linguistics (ACL 02),</booktitle>
<pages>303--310</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="863" citStr="Yamada and Knight, 2002" startWordPosition="127" endWordPosition="130"> model for statistical machine translation that uses a much simpler set of model parameters than similar phrase-based models. The units of translation are blocks - pairs of phrases. During decoding, we use a block unigram model and a word-based trigram language model. During training, the blocks are learned from source interval projections using an underlying word alignment. We show experimental results on block selection criteria based on unigram counts and phrase length. 1 Phrase-based Unigram Model Various papers use phrase-based translation systems (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002) that have shown to improve translation quality over single-word based translation systems introduced in (Brown et al., 1993). In this paper, we present a similar system with a much simpler set of model parameters. Specifically, we compute the probability of a block sequence bn1 . The block sequence probability Pr(bn1) is decomposed into conditional probabilities using the chain rule: n Pr(bn1) ≈ Pr(bi|bi−1) (1) i=1 n = pα(bi|bi−1) · p(1−α)(bi|bi−1) i=1 n ≈ pα(bi) · p(1−α)(bi|bi−1) i=1 We try to find the block sequence that maximizes Pr(bn1 ): bn1 = arg maxbn 1 P r(bn 1 ). The model proposed i</context>
</contexts>
<marker>Yamada, Knight, 2002</marker>
<rawString>Kenji Yamada and Kevin Knight. 2002. A Decoder for Syntax-based Statistical MT. In Proc. of the 40th Annual Conf. of the Association for Computational Linguistics (ACL 02), pages 303–310, Philadelphia, PA, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>