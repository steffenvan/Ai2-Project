<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.894364">
LFP: A LOGIC FOR LINGUISTIC DESCRIPTIONS AND
AN ANALYSIS OF ITS COMPLEXITY
</title>
<author confidence="0.365984">
William C. Rounds
</author>
<affiliation confidence="0.6080925">
Electrical Engineering and Computer Science Department
University of Michigan
</affiliation>
<address confidence="0.553598">
Ann Arbor, MI 48109
</address>
<bodyText confidence="0.997173285714286">
We investigate the weak expressive power of a notation using first-order logic, augmented with a facility
for recursion, to give linguistic descriptions. The notation is precise and easy to read, using ordinary
conventions of logic. Two versions of the notation are presented. One, called CLFP, speaks about strings
and concatenation, and generates the class EXPTIME of languages accepted by Turing machines in time
r&apos;s for some c&gt; 0. The other, called ILFP, speaks about integer positions in strings, and generates the
class PTIME of languages recognizable in polynomial time. An application is given, showing how to code
Head Grammars in ILFP, showing why these grammars generate only polynomial time languages.
</bodyText>
<sectionHeader confidence="0.9855305" genericHeader="abstract">
1 FIRST-ORDER LOGIC AS A TOOL FOR SYNTACTIC
DESCRIPTION
</sectionHeader>
<bodyText confidence="0.999861982142858">
In this paper we investigate the properties of a new
notation for specifying syntax for natural languages. It
is based on the simple idea that first-order logic, though
inadequate as a semantics for natural language, is quite
adequate to express relations between syntactic constit-
uents. This is the insight behind definite clause gram-
mars (DCGs) (Pereira and Warren 1980) and, in fact, our
notation is in some ways a generalization of that nota-
tion. However, we have tried to keep our formalism as
much as possible like that of standard textbook first-
order logic. There are actually two versions of our
notation. The first works with strings of symbols and
uses concatenation as a primitive operation. The second
works with integers and takes the standard arithmetic
operations as primitive. These integers can be regarded
as indexing positions of morphemes in a sentence, but
the sentence itself is not explicitly referenced. Both
versions allow the recursive definition of predicates
over strings and integers. This capacity for recursive
definition is what gives our grammars their generative
ability, and our notation has this feature in common
with DCGs. However, we liberate DCGs from the Horn
clause format, and we do not base the semantics of our
notation on the semantics for Prolog or for logic pro-
grams. We hope that making the logic more familiar and
readable will encourage more people to use logic as a
means for specifying desired syntactic relations be-
tween sentential constituents in grammars. Anyone
knowing the standard conventions of first-order logic
should be able to read or to specify a grammar in our
notation.
We also provide a precise semantics for our two
notations. This involves using the least-fixed-point op-
erator from denotational semantics for programming
languages to explain the recursive definition of predi-
cates. It involves as well using restricted universal and
existential quantification to restrict the class of defin-
able predicates (sets of strings). We prove a complexity
theoretic characterization for both grammar formal-
isms: (1) the formalism using strings and concatenation
defines exactly the class EXPTIME of formal languages
recognizable by deterministic Turing machines within
time T(n) = 2&amp;quot; for some positive c; and (2) the
formalism using integers defines exactly the class
PTIME of languages recognizable in time T(n) = nk for
some integer k.
As an application of the second notation we sketch a
natural way to write Head Grammars (Pollard 1984).
Because these grammars can be expressed in this way,
we immediately obtain the result that head languages
can be recognized in polynomial time. We even obtain
an estimate of the degree of the polynomial that is
required, derived directly from the form of the gram-
matical description. Unfortunately, the estimated de-
gree is at least twice as large as is actually necessary if
one uses the algorithm of Pollard (1984), or Vija-
</bodyText>
<footnote confidence="0.86576475">
Copyright 1988 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided
that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To
copy otherwise, or to republish, requires a fee and/or specific permission.
0362-613X/ 88 /01001-9$03.00
</footnote>
<note confidence="0.66988">
Computational Linguistics, Volume 14, Number 4, December 1988 1
William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its Complexity
</note>
<bodyText confidence="0.99861459375">
yashanker and Joshi (1985). We conjecture that in fact,
this factor of (2) can be removed.
Our complexity theoretic characterizations are ver-
sions of theorems already appearing in the literature.
Shapiro (1984) characterizes the class of languages
definable by logic programs with a linear space restric-
tion as the class EXPTIME. The proof of our first
theorem is very much like his. Our second theorem
characterizing PTIME can be viewed as a specialization
of the results of Chandra and Harel (1982), Immerman
(1982), and Vardi (1982), who show that the class of sets
of finite mathematical structures definable by formulas
of first-order logic augmented with a least-fixed-point
operator is just the class of sets of structures recogniz-
able in polynomial time. We prove both of our results in
the same way, and thus show how these apparently
unconnected theorems are related. The proof uses the
notion of alternating Turing machines, and thereby
explains the significance of this idea for the science of
formal linguistics.
We should also note that our notation will not find
immediate application in current linguistic theory, be-
cause it does not allow structural descriptions to be
described. We are in the process of extending and
modifying the notation for this purpose. However, we
think it is important to explicate the properties of the
individual operations used in building strings and struc-
tures. Our first attempt is therefore to understand how
concatenation of strings can be expressed in a restricted
logic. We can then consider other predicates or func-
tions on both strings and treelike structures in the same
uniform way.
</bodyText>
<sectionHeader confidence="0.999445" genericHeader="introduction">
2 CLFP GRAMMARS: GRAMMARS BASED ON
CONCATENATION THEORY
</sectionHeader>
<subsectionHeader confidence="0.77766">
2.1 SYNTAX OF CLFP
</subsectionHeader>
<bodyText confidence="0.999978">
We present a standard version of the first-order theory
of concatenation, augmented with the least-fixed-point
operator. Before proceeding with the formal descrip-
tion, we give an example to illustrate the scheme we
have in mind. Consider the following context-free frag-
ment, adapted directly from Johnson (1985).
</bodyText>
<equation confidence="0.9287335">
S—* NP VP
NP —&gt; Det Noun
VP—&gt; V NP
Det —&gt; NP[+Gen] I the
</equation>
<bodyText confidence="0.855159">
Here is the corresponding CLFP fragment:
</bodyText>
<equation confidence="0.9997552">
S(x) .(=&gt; 3yz.NP[—Gen](y) A VP(z) A x = yz;
NP[case](x) &lt;=&gt; 3yz.Det(y) A Noun[case](z)
A x = yz;
VP(x) .(=&gt; 3yz.V(y) A NP[—Gen](z) A x = yz;
Det(x) &lt;=&gt; NP[+Getz](x) V x = the.
</equation>
<bodyText confidence="0.972390114285714">
In this formulation, x,y, and z range over strings of
symbols (morphemes) and NP,VP, etc. are predicates
over strings. The second clause is here an abbreviation
for two clauses, where case can take two values,
namely +Gen and —Gen. At present we do not treat the
problem of calculating complex feature structures, but
there seems to be no reason that the notation cannot be
suitably extended.
This example illustrates the most complex case of a
CLFP formula. It is a recursion scheme, which assigns
to predicate variables, S,NP, etc. certain formulas (the
right-hand sides of the corresponding clauses in the
definition). The whole scheme is understood as the
simultaneous recursive definition of the predicate vari-
ables in the left sides of the definition. To handle the
fact that string variables occur on the left-hand side of
each clause, we will understand each clause as a func-
tion assigning both the formula on its right and the set of
individual variables mentioned on the left to the given
predicate symbol.
We now proceed with the formal definition of CLFP.
Let Ivar be a set {x0,x1, . . .} of individual variables
ranging over strings. Let Z be a finite set of terminal
symbols. These are the constants of our theory. A is
another constant denoting the null string. Terms are
built from variables and constants using the binary
operation of concatenation. We also require a set Pvar
of predicate variables, listed as the set {PI,P2, . .1.
Each predicate variable P is equipped with an arity
ar(P), indicating the number of individual arguments
that a relation assigned to this variable will have. (The
example CLFP scheme given above employs only unary
predicate variables S,NP,VP, and Det.) The set of
CLFP formulas is given by the following inductive
clauses.
</bodyText>
<listItem confidence="0.9989007">
1. If P E Pvar and (x1, . . .,x,z) is a sequence of Ivar
with length n = ar(P) then P(xl, . . .,xn) is in CLFP;
2. If t1 and t2 are terms, then t1 = t2 is in CLFP;
3. If x E Ivar and is in CLFP then au,&apos; and Vx4) are
in CLFP;
4. The usual Boolean combinations of CLFP formulas
are in CLFP.
5. This clause requires more definitions. Let be a
finite nonempty subset of Pvar with a distinguished
element S E R. Let (1) : —&gt; P(Ivar) x CLFP. (4)(R)
is going to be the defining clause for the predicate R.)
If t(R) = (X,4)), then we define B.1(R) = X, and
C(R) = 4). We require that IB(1)(R)I = ar(R) and
thus be a finite set of individual variables. Now we
say that the whole map (I) is a recursion scheme if
each P E occurs only positively in (KR) for any
R E a; that is, within the scope of an even number
of negation signs. Finally, condition 5 states that if to
is a recursion scheme, with distinguished variable S,
then /LSI, (the least fixed point of (I)) is in CLFP.
</listItem>
<bodyText confidence="0.327093">
Example 1. Consider the following scheme, which de-
fines a regular language.
</bodyText>
<page confidence="0.765034">
2 Computational Linguistics, Volume 14, Number 4, December 1988
</page>
<equation confidence="0.979785833333333">
William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its Complexity
S(x) &lt;=&gt; 3y((x = ay A (S(y)) V (x = by A 7&apos;(y))) V x = a
T(v) .4#) 3w(v = cw A S(w)).
In this example, a = {S, 7}, /34)(S) = {x}, and
C4)(S) = 3y((x = ay A (S(y)) V (x = by A T(y)))
V x = a.
</equation>
<bodyText confidence="0.985279181818182">
Similarly, 13(1)(T) = {v}, and C4)(T) is the second formula
in the scheme.
In the example, we have written our recursion
scheme in a conventional style to emphasize its direct
connection to the usual grammatical presentations.
Thus the variable x is bound by the left-hand side of (1),
so this clause has been written with S(x) on the left to
make this fact apparent. Also, the use of the .:=&gt; sign is
conventional in writing out 4). In our example, the first
clause is taken as defining the distinguished predicate S
of our scheme. Finally, there are no occurrences of free
predicate variables in this example, but there are in our
first example (e.g., noun).
The usual rules for calculating free individual varia-
bles apply; if Fvar(0) is the set of free variables of (I),
then FIvar(P(xi, . . .,x„))= {x1, . . .,x,J. The quantifier
and Boolean cases are handled as in standard text
presentations. However, if 4) is a recursion scheme then
Flvar(ASO) will be calculated as follows. For each R E
Wt., find Fvar(OP(R)). Remove from this set any varia-
bles in 13(1)(R). The union of the resulting sets for each
R E gt is defined to be the set Fivar(ptS4)).
The rules for free predicate variables are a bit sim-
pler. In the atomic formula P(xi, . . .,x,,), P is a free
predicate variable. In a recursion scheme 4) with do-
main a, the set FPvar(i.tS4))) is the union of the sets
FPvar(kLS4)(R))), minus the set a.
A final remark on notation: we will use the notation
.,t,i) to stand for the formula
3x1 . . . 3x(4)(x1, . . .,x,,) A xi = t1 A . . . A x,, = tn)
where the ti are terms, and the xi are individual variables
not appearing in any ts. This will not affect our com-
plexity results in any way.
</bodyText>
<subsectionHeader confidence="0.985499">
2.2 SEMANTICS FOR CLFP
</subsectionHeader>
<bodyText confidence="0.999910357142857">
We borrow some notation from the Oxford school of
denotational semantics to help us explain the meaning
of our logic grammars. If X and Y are sets, then [X —&gt; Y]
is the set of functions from X to Y. Let A = [Ivar
be the set of assignments of values to individual varia-
bles. We wish to define when a given assignment, say a,
satisfies a given CLFP formula 4). This will depend on
the meaning assigned to the free predicate variables in
0, so we need to consider such assignments. Let PA be
the set of maps p from Pvar to the class of relations on
I,* such that the arity of p(P) is ar(P). We are now ready
to define for each formula 4, and predicate assignment p,
the set MY&amp; C A of individual assignments satisfying
with respect to p.
</bodyText>
<listItem confidence="0.998059375">
1. AtIP(xi, . . = {a I (a(x1), . . .,a(x„)) E p(P)};
2. Atit, = t2lIp = {a I tia = t2a}, where ta is the
evaluation of t with variables assigned values by a;
3. 443x(1)1p = {a I 3u E /* : a(x1u) E NON, and
similarly for universal quantification;
4. .MIOVIMP = Atid4PUAgap, and similarly for other
Boolean connectives.
5. Atip,S4)1p = {a I (3k)(« E AtiC0k(S)1p)}
</listItem>
<bodyText confidence="0.99990625">
where, for each k, 4)1( is a recursion scheme with the
same domain k as 4), and is defined as follows by
induction on k. First, we stipulate that for each P E Jt,
the set BV(P) = 13(1)(P). Then we set
</bodyText>
<equation confidence="0.998976">
010(P) = C4)(P)[R 4-- FALSE : R E a];
cvk I (p) =
CO(P)[R C(1)k(R) : R E gt]
</equation>
<bodyText confidence="0.999927153846154">
where the notation (KR O(R) : R E a] denotes the
simultaneous replacement of atomic subformulas
R(wi, . . .,wk) in 4/ (where R is a free occurrence) by the
formula 0(R)(w1, . . .04/k), in such a way that no free
occurrences of other variables in O(R) are captured by a
quantifier or a woperator in 1/.. (We may always change
the bound variables of tp first, to accomplish this.)
This definition appears much more clumsy than it
really is, and we continue our example to illustrate it.
Refer to the example of a regular grammar in the
previous section. In the clause for S we are required to
substitute the formula FALSE for occurrences of both S
and T. This gives, after simplification,
</bodyText>
<equation confidence="0.998152">
04)°(S)(x) = (x = a).
</equation>
<bodyText confidence="0.9995075">
Similarly, substitution of FALSE into the clause for T
gives 4)°(7)(v) = FALSE. Now substitution of these
new formulae for S and T into 4) gives (after simplifica-
tion):
</bodyText>
<equation confidence="0.9999825">
04)1(S)(x) = 3y(x = ay A x = a) V x = a;
04)1(1)(v) = 3w(v = cw A w = a).
</equation>
<bodyText confidence="0.9997652">
It is easy to see that continuing this process will
simulate all possible derivations in the grammar, and
also that it explains the meaning of the scheme 4) in
terms of the meaning of subformulas.
Some remarks are in order to explain why we use the
term &amp;quot;least-fixed-point&amp;quot;, and to explain why, in a
recursion scheme, all occurrences of recursively called
predicates are required to be positive. Let 4) : 9k —&gt;
CLFP be a recursion scheme. Define the map nil :
PA PA as follows. If R E a, then
</bodyText>
<equation confidence="0.745789">
(ui, . . .,u„) E 71[44p(R) .44) (3« E Ati4P(R)b)(a(xi)
= ui)
</equation>
<bodyText confidence="0.998909">
where (xl, .,x„) is the sequence of variables in
111(1)(R), listed in increasing order of subscripts. If R
gt, then 714)11p(R) = p(R). Next, let
</bodyText>
<equation confidence="0.8769875">
ThAROlp = U 710]](k)(p[R 0 : R E
1
</equation>
<bodyText confidence="0.9382815">
Computational Linguistics, Volume 14, Number 4, December 1988 3
William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its Complexity
where unions are coordinatewise, Pk) is the k-th iterate
of F, and p[R 0 : R E 2/ti is p with the empty relation
assigned to each predicate variable in R. This formula is
just the familiar least-fixed-point formula Lk&gt;, Pk)(1)
from denotational semantics. Then we can check that
11i.LS(1)1p is in PA, and is the least fixed point of the
</bodyText>
<equation confidence="0.8988545">
continuous map It is then possible to prove that
tbiSOlp = (71[AS(11p)(S)
</equation>
<bodyText confidence="0.998454">
where S is the distinguished predicate variable in R.
If we had no conditions on negative formulas in
recursion schemes, then we could entertain schemes
like
</bodyText>
<equation confidence="0.925989">
S(x) &lt;=&gt; S(x)
</equation>
<bodyText confidence="0.999978909090909">
which, although they would receive an interpretation in
our first definition, would give a T which was not
continuous, or even monotonic. We therefore exclude
such cases for reasons of smoothness.
Next we come to the definition of the language or
relation denoted by a formula. A k-ary relation P on 1*
is said to be definable in CLFP if there is a CLFP
formula 4) with no free predicate variables such that
(u1, . . E P &lt;=&gt; 3« E .A4401:(a(x1), . . .,a(x k)) =
(u1, . . .,uk), where x1, . . Xi, is the list of free variables
in 4) arranged in increasing order of subscript. (Notice
that the parameter p has been omitted since there are no
free predicate variables in 4).)
So far, we have not restricted quantification in our
formulas, and every r.e. predicate is definable. We need
to add one other parameter to the definition of A, which
will limit the range of quantification. This will be an
integer n, which will be the length of an input sentence
to be recognized. The occurrences of the formula .114[44p
will thus be changed everywhere in the above clauses to
.M[M]pn. The only change in the substance of the clauses
is in the rule for existential and universal quantification.
</bodyText>
<equation confidence="0.856660833333333">
A113x4Apn = {a I 3u E : I u I n A a(x1u)
E
A predicate P is said to be boundedly definable iff for
some 0:
011, . . c P &lt;=&gt; 3a E At[Min : (a(x1), . .
= . .
</equation>
<bodyText confidence="0.998834666666667">
where n = max(luil). (To abbreviate the right-hand
condition, we write (u1, . . .,uk) 1= 0). Our first theorem
can now be stated.
Theorem 1. A language (unary predicate) is bound-
edly definable in CLFP if it is in EXPTIME.
We defer the proof to the next section.
</bodyText>
<sectionHeader confidence="0.998387" genericHeader="method">
3 EXPTIME AND CLFP
</sectionHeader>
<subsectionHeader confidence="0.989539">
3.1 ALTERNATION
</subsectionHeader>
<bodyText confidence="0.999653327868853">
Before proving Theorem 1, we need to discuss the
method of proof both for this result and for the Integer
LFP characterization of PTIME in the next section.
This material is repeated from the fundamental article of
Chandra, Kozen, and Stockmeyer (1981). Their paper
should be consulted for the full details of what we state
here.
An alternating Turing machine can be regarded as a
Turing machine with unbounded parallelism. In a given
state, and with given tape contents, the machine can
spawn a finite number of successor configurations ac-
cording to its transition rules. These configurations are
thought of as separate processes, each of which runs to
completion in the same way. A completed process is
one which is in a special accepting state with no
successors. The results of the spawned processes are
reported back to the parent, which combines the results
to pass on to its own parent, and so forth. How the
parent does this depends on the state of the finite
control. These states are classified as being either
existential (OR), universal (AND), negating (Nur), or
accepting. If the parent is in an existential state, it
reports back the logical OR of the results of its off-
spring. If it is in a universal state, it reports back the
logical AND; if the state is negating, the parent reports
the negation of its one offspring. An accepting state
generates a logical 1 (TRUE) to be reported back. Thus a
nondeterministic TM can be regarded as an alternating
TM with purely existential states.
An alternating TM is defined as a tuple in a standard
way. It has a read-only input tape with a head capable of
two-way motion. It also has a fixed number of work
tapes. The input tape contains a string u E 1*, while the
work tapes can use a tape alphabet F. The transition
relation 8 is defined as for ordinary nondeterministic
TMs. The state set is partitioned as described above
into universal, existential, negating, and accepting
states. The relation 6 is constrained so that existential
and universal states have at least one successor, negat-
ing states have exactly one successor, and accepting
states have no successors. A configuration is then just a
tuple describing the current state, positions of the
heads, and tape contents as is familiar. The initial
configuration is the one with the machine in its initial
state, all the work tapes empty, and the input head at
the left end of the input u. The successor relation &apos;-
between configurations is defined again as usual.
To determine whether or not a configuration is ac-
cepting, we proceed as follows. Imagine the configura-
tions that succeed the given one arranged in a tree, with
the given configuration at the root. At each node, the
immediate descendants of the configuration are the
successors given by F. The tree is truncated at a level
determined by the length of the input tape (this trunca-
tion is not part of the general definition but will suffice
for our results.) The leaf nodes of this tree are labeled
with (0) if the configuration at that node is not accept-
ing, and with (1) if the configuration is accepting. The
tree is then evaluated according to the description given
above. The configuration at the root is accepting iff it is
labeled (1) by this method. Thus an input is accepted by
</bodyText>
<sectionHeader confidence="0.853663" genericHeader="method">
4 Computational Linguistics, Volume 14, Number 4, December 1988
</sectionHeader>
<bodyText confidence="0.9836442">
William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its Complexity
the machine iff the initial configuration with that input is
accepting. In our application, it will always suffice to
cut off the tree at level 2c&amp;quot;, where n is the length of the
input string, and c is a positive constant depending only
on the description of the machine.
We say that an alternating TM is S(n) space bounded
iff in the above tree, for any initial configuration labeling
the root, no auxiliary tape length ever exceeds S(n)
where n is the length of the input. We are concerned
only with the functions S(n) = log n and S(n) = n in this
paper. We let the class ASPACE(S(n)) be the class of
languages accepted by space-bounded ATMs in this
way. We then have the following theorem (Chandra,
Kozen, Stockmeyer 1987):
</bodyText>
<equation confidence="0.935668">
Lemma 1. If S(n) log n, then
ASPACE(S(n)) = U DTIME(2`5(&amp;quot;))
c&gt;0
</equation>
<bodyText confidence="0.999937884615385">
where DTIME(T(n)) is the class of languages accepted
deterministically by ordinary Turing machines within
T(n) steps.
Our problem in the rest of this section is to show how
linear space bounded ATMs and CLFP grammars sim-
ulate each other. To facilitate the construction of the
next section, it is convenient to add one feature to the
definition of alternating Turing machines. Let U be the
name of a k-ary relation on I. We allow machines to
have oracle states of the form U?(i, , . . .,ik), where the if
are tape numbers. If now the predicate U is interpreted
by an actual relation on I*, then when M executes such
an instruction, it will accept or reject according to
whether the strings on the specified tapes are in the
relation U. We will need such states to simulate recur-
sive invocations of recursion schemes. It is not hard to
modify the definition of acceptance for ordinary ATMs
to that for oracle ATMs. The language or relation
accepted by the ATM will now of course be relative to
an assignment p of relations to the predicate names U.
The next subsections contain our constructions for
the CLFP characterizations. Then, in Section 4 we will
treat Integer LFP grammars and show how these gram-
mars and logspace bounded ATMs simulate each other.
As a consequence of the above lemma, we will then
have our main results.
</bodyText>
<subsectionHeader confidence="0.994548">
3.2 PROOF OF THEOREM 1
</subsectionHeader>
<bodyText confidence="0.980789875">
Our first task is to show that if a language L is
(boundedly) CLFP-definable, then it can be recognized
by a linear space bounded ATM. The idea is simple.
Given an input string, our machine will try to execute
the logical description of the grammar. Its states will
correspond to the logical structure of the CLFP for-
mula. If that formula is, for example, the logical AND of
two subformulas, then the part of our machine for that
formula will have an AND state. A recursion scheme
will be executed with states corresponding to the pred-
icate variables involved in the recursion, and so forth.
To give an explicit construction of an ATM cone-
sponding to a formula 4) of CLFP we need to be precise
about the number of work tapes required. This will be
the sum of the number of free individual variables of 0,
and the number of &amp;quot;declarations&amp;quot; of bound variables in
4). A &amp;quot;declaration&amp;quot; is either the occurrence of a univer-
sal or existential quantifier in 4), or one of the individual
variables bound on the left side of a (non-S) clause in a
recursion scheme. If that clause defines the predicate R,
then the number of variables declared at that point is
ar(R) = IB(R)1. We thus define the number OW of
declarations of bound variables in by induction as
follows:
</bodyText>
<listItem confidence="0.901088">
1. P(R(xl, . . = 0;
2. /3(t1 = = 0;
3. P(OVO = P(4)A = P(d)) + OW;
4. P(70) = i3(0);
5. p(ax0) = p(ix0.) = 1 + 13(0;
6. f3(.1S(D) = go:D(s)) + ERE,t,{s}(ar(R) + 13(C(1)(R))).
</listItem>
<bodyText confidence="0.992900896907217">
The number -y(0) counts the maximum number of tapes
needed, and is defined to be p(0) + IFivar(4))1 + 1.
We can now state the inductive lemma which allows
the construction of ATMs.
Lemma 2. Let 4) be a CLFP formula, with IFIvar(01
= k, and T:Flvar(4)) —&gt; {1, . . Let m =
Then we may construct an m-tape ATM M(4),7)
having the following properties: (i) M has oracle
states P? for each free predicate variable of 0, and (ii)
For any a:Flvar(c/&gt;) —&gt; 1*, and any environment p,
we have the following. Let n = max{la(x1)1}. Then M
with oracle states for the p(P), started with a(x1) on
tape T(x1), . . ., and a(xk) on tape T(xk), and the other
tapes blank, will accept without ever writing more
than n symbols on any tape, if and only if
&lt;a(x,), . . .,a(xk)) E Atic/Ajpn.
Proof: This lemma formalizes the intuitive idea, stated
above, that to calculate the membership of a string x in
the language defined by a recursion scheme, it suffices
to execute the scheme recursively. The full proof would
use the formal definition of the semantics of ATMs,
which themselves are given by least-fixed-point defini-
tions. We have chosen not to give the full proof,
because the amount of explanation would be over-
whelming relative to the actual content of the proof.
Instead we give a reasonably complete account of the
inductive construction involved, and illustrate with the
regular set example of the previous section.
To start the induction over formulas cp, suppose that
4) is R(xi, . . .,xk). Then we may take M to be a machine
with k = y(4)) tapes, with one oracle state P, and the
single instruction P?(T(x,), . . .,T(xk)).
If 4) is t, = t2, then we let M be a simple machine
evaluating t, and t2, using perhaps an extra tape for
bookkeeping. It does a letter-by-letter comparison, so
that it never has to copy more than the maximum length
of any one tape.
If 4) is —RP, then M(4)) consists of adding a negating
Computational Linguistics, Volume 14, Number 4, December 1988 5
William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its Complexity
state before the initial state of MOO, and transferring
control to that initial state.
If 0 is di \/ di2 we construct MI and M2 by inductive
, ,
hypothesis. Then M(4,) is constructed by having disjoint
instruction sets corresponding to each Mi, prefixed by
an OR state which transfers control to either of the two
formerly initial states. The free individual &apos;variables of
the disjunction are those occurring free in either dis-
junct. Let T be an assignment of tapes to the free
variables of the disjunction. Then we construct M, with
a T, such that Ti(x) = T(x), and similarly for M2, where
x is a free individual variable. Otherwise, any tapes
referenced in M, are distinct from any tapes referenced
in M2. In other words, the machine M has shared
storage for the free variables, and private storage for
variables bound in either disjunct. The oracle states in
the two pieces of code are not made disjoint, however,
because a predicate variable is free in the disjunction iff
it is free in either disjunct. It is clear that the number of
tapes of the 0, V tp, is just y(p, V tp,). For the case of
=iii, A 02, we make exactly the same construction,
only using an AND state as the new initial state.
If 0 is 3x0, and T is a tape assignment for the free
variables of 0, then we construct MOO using the ex-
tended tape assignment which assigns a new tape k + 1
to the variable x, and otherwise is the same as T. Now
M is constructed to go through an initial loop of exis-
tential states, which fills tape k + 1 with a string no
longer than the maximum length of any string on tapes
1 through k. It then transfers control to the initial state
of mo. The same construction is used for the universal
quantifier, using an initial loop of universal states.
Finally, we need to treat the case of a recursion
scheme ASO. Suppose that (I) has domain gt, and let T
be a tape assignment for /IA) . For each clause C4)(Q),
where Q E It, we construct a machine M(Q) by induc-
tive hypothesis. The global free variables of each M(Q)
will have tapes assigned by T. However, we construct
the M(Q) all in such a way that the local tape numbers
do not overlap the tape numbers for any other M(R).
This procedure will give tape numbers to all the varia-
bles in the set 134)(Q). Let this set be fzi, in
increasing order. Define TQ(z1) to be the tape assigned to
zi in M(Q).
The machine M(ASI) will consist of the code for the
M(Q), arranged as blocks; the initial state of each such
block will be labeled Q. In all the blocks, recursive
oracle calls to Q? will be replaced by statements trans-
ferring control to Q. Thus, consider an oracle call
Q?(ii, . . .,ik), in any block M(R). Replace this call by
code which copies tape i, to tape TQ(z1), . . ., and tape
ik to tape TQ(zk). Insert code that empties all other tapes
local to M(Q), and insert a statement &amp;quot;go to Q.&amp;quot;
This completes the construction, and we now illus-
trate it with an example. Consider the recursion scheme
introduced in the first section.
</bodyText>
<equation confidence="0.999813">
S(x) &gt; 3y((x = ay A (S(y)) V (x = by A Ry))) V x = a
T(v) &lt;=&gt; 3w(v = cw A S(0).
</equation>
<bodyText confidence="0.710409166666667">
We construct the machine M(S) as follows:&apos;
tape 1 : x
tape 2 : y (initially blank)
Initially : guess a value of y, such that lxi, and
store y on tape 2; go to (ql or q2 or q7);
ql : go to (q3 and q4);
</bodyText>
<equation confidence="0.999146875">
q3 : check x = ay on tapes 1 and 2, and accept or
reject as appropriate;
q4 : S?(tape 2)
q2 : go to (q5 and q6);
q5 : check x = by on tapes 1 and 2, and accept or
reject as appropriate;
q6 : T?(tape 2)
q7 : check x = a and accept or reject.
</equation>
<bodyText confidence="0.997816666666667">
Similarly, we can construct a machine M(T) for the T
clause. Then the result of pasting together the two
constructions is shown in Figure 1.
</bodyText>
<equation confidence="0.722460222222222">
tape 1 : x
tape 2 : y (initially blank)
tape 3 : v (initially blank)
tape 4 : w (initially blank)
S : guess a value of y, such that lxl, on tape
2;
go to (ql or q2 or q7);
q 1 : go to (q3 and q4);
q3 : check x = ay on tapes 1 and 2 , and
</equation>
<bodyText confidence="0.847750368421053">
accept or reject as appropriate;
q4: copy tape 2 to tape 1;
Empty tape 2;
Go to S.
q2 : go to (q5 and q6);
q5 : check x = by on tapes 1 and 2 , and
accept or reject as appropriate;
q6: copy tape 2 to tape 3;
empty tape 4;
go to T.
q7: check x = a and accept or reject.
T: guess a w on tape 4 no longer than v on tape
3;
go to (q9 and q10);
q9 : Check v = cw on tapes 3 and 4, and
return appropriately;
q10: copy tape 4(w) to tape 1;
empty tape 2;
go to S.
</bodyText>
<figureCaption confidence="0.991008">
Figure 1. ATM Program for the Recursion Scheme.
</figureCaption>
<page confidence="0.95747">
6 Computational Linguistics, Volume 14, Number 4, December 1988
</page>
<note confidence="0.500283">
William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its Complexity
</note>
<bodyText confidence="0.985815088235294">
As we remarked, we cannot give a full proof of the
correctness of our construction. However, the con-
struction does correspond to the formal semantics of
CLFP. In particular, the semantics of recursion corre-
sponds to the iterated schemes (bk . Iterating the scheme
k times roughly corresponds to developing the compu-
tation tree of the ATM to k levels, and replacing the
oracle states at the leaves of the k-level tree with
rejecting states corresponds to substituting FALSE into
the kth iteration.
With these remarks, the proof is complete.
Lemma 3. Suppose L is accepted by a S(n) =
n-bounded ATM. Then there is a CLFP formula 4)
such that for all u C1*, we have u C L &lt;=&gt; u 1 4).
Proof: We may assume that M is an ATM with one work
tape, if we allow M to print symbols in an auxiliary tape
alphabet F. By a result in Chandra, Kozen, and Stock-
meyer (1981) M has no negating states. We show how to
construct a formula 4), which has constants ranging over
F, but which has the property stated in the conclusion of
the lemma: for each string x over /, M accepts x iff x
4). The formula 4) will be given as a recursion scheme
AS(1). Each state q of M will become a binary predicate
variable q(x,y) in R. The meaning of q(u,v), where u and
v are specific strings in F*, is that M is in state q,
scanning the first symbol of v, and that u and v are the
portions of the work tape to the left and the right of the
head, respectively.
We give a perfectly general example to illustrate the
construction of (1). In this example, the tape alphabet
is fa,b1. Suppose that q is a universal state of M and
that 8(q,a) = {(r,b,right),(s,a,left)}, and 8(q,b) =
{(p,b,left),(q,a,right)}. Then (1)(q)(x,y) is the following
formula:
</bodyText>
<equation confidence="0.859243666666667">
A vwtRx = wo- A y = at &gt; r(xb,t) A s(w,o-at))
alEfa,b1
A(x = wo- Ay = bt p(w,crbt) A q(xa,t))]
</equation>
<bodyText confidence="0.9999745">
The distinguished element of gt is go, the start state of
M. Notice that all predicate variables in R occur posi-
tively in (I), and that the search for w and t is limited to
strings no longer than the length of the original input to
M. If q is an accepting state of M, then we have a clause
in (1) of the form q(x,y) .(=&gt; TRUE, where TRUE is some
tautology.
Technically speaking, the explicit substitutions
r(xb,t) are not allowed in our formulas, but these can be
expressed by suitable sentences like (3z)(z = xb A
r(z,t)), as remarked in the first section. The cases for
q(x,y) when x and y are null must also be handled
separately because M fails if it tries to leave the original
region.
Finally, we can obtain a formula over the constant
alphabet by a more complicated construction. If we
encode F into / by a homomorphic mapping, then a
machine N can be constructed to simulate M. N will
have tape alphabet 1, but will have a number n of work
tapes bounded linearly by the constant involved in the
encoding. We now make a formula corresponding to N,
but the predicates will have to be 2n-ary, one pair of
arguments for each tape of N. With these remarks, the
proof of the lemma is complete.
Theorem 1 follows immediately from the above lem-
mas.
</bodyText>
<sectionHeader confidence="0.954993" genericHeader="method">
4 ILFP: GRAMMARS WITH INTEGER INDEXING
</sectionHeader>
<subsectionHeader confidence="0.895005">
4.1 SYNTAX OF ILFP
</subsectionHeader>
<bodyText confidence="0.999839804347826">
Our characterization of the defining power of CLFP
relied on the result EXPTIME = ASPACE(n). We also
know that PTIME = ASPACE(log n). Is there a similar
logical notation that gives a grammatical characteriza-
tion of PTIME? This section is devoted to giving an
affirmative answer to this question. As stated in the
introduction, this result is already known (Immerman
1982, Vardi 1982), but the result fits well with the CLFP
theorem, and may in the linguistic domain have some
real applications other than ours to Head Grammars. To
explain the logic, it helps to consider acceptance by a
logspace bounded ATM. In this case, the machine has a
read-only input tape, which can be accessed by a
two-way read head. Writing is strictly disallowed on the
input tape, in contrast to the linear space bounded
ATMs of the previous section. There is also a number k
of work tapes on which computation occurs. Suppose
that these work tapes use a binary alphabet. If their size
always is less than or equal to rlog2 ni, then they are
always capable of representing the numbers from 0
through n —1. We thus think of the contents of the work
tapes as indices of specific positions in the read-only
input string, though in fact they may not serve this
purpose in an arbitrary computation. Since the input is
off-line, substrings of the input will not be quantified.
Instead, we quantify over the integer subscripts, and the
input simply becomes a global parameter appearing in
the semantics. Instead of having equations between
strings as atomic formulas, we will have equations
between integer terms. In order to access the input, we
will have, for each symbol a E 1,, an atomic predicate
symbol a(i) of one argument, which will be true iff in the
given input x, the symbol x(i) at position i is a. (We
number the positions from 0 through n — 1). We allow
individual constant symbols 0,1, and last, which will be
interpreted as 0, 1, and n — 1, respectively, when the
input has size n. As primitive arithmetic operations we
allow addition and subtraction, and multiplication and
integer division by 2. All of these operations are inter-
preted modulo n when the input is given.
We need not give the formal definition of ILFP
formulas, as it is the same as for CLFP, except that
individual variables come from a set fio,ii , . . .1, terms
are formed as above from arithmetic combinations of
individual variables and constants, and the unary pred-
icates a(i) are atomic formulas.
</bodyText>
<figureCaption confidence="0.170623">
Example 2. Consider the CFG
</figureCaption>
<figure confidence="0.36490225">
S —&gt; aSb I bSa I SS I ab I ba
Computational Linguistics, Volume 14, Number 4, December 1988 7
William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its Complexity
This is represented in ILFP as follows:
S(i,j) .(=&gt; a(i) A S(i + 1j — 1) A b(j)
V b(i) A + 1 j — 1) A a(j)
3k &lt; j : S(i,k) A S(k + 1j)
vj=i+1A ((a(i) A b(h) V (b(i) A a(j))
</figure>
<bodyText confidence="0.999481642857143">
(Again, the explicit substitution of terms for variables is
not officially allowed but can be introduced by defini-
tion.)
The meaning of the above scheme should be clear.
The predicate S(ij&amp;quot;) is intended to mean that node S
dominates positions i through j in the input. Thus the
assertion S(0,1ast), with no free variables, will be satis-
fied by a string x iff x is generated by the given CFG.
The relation of this descriptive formalism to the CKY
algorithm for context-free recognition should also sug-
gest itself.
Our definition of the meaning function 4.144 is like
that in Section 2, except that the parameter n is replaced
by a string x E 1*. Thus
</bodyText>
<listItem confidence="0.987803666666667">
1. • • .,idka = {a I (a(ii), • • .a(ik)) E P(P)};
2. Alla(i)lpx = fa I x(a(i)) = al;
3. Ati = tApx = fa I tia = t2a1;
4. A42i4)6x = {a I (3m &lt; lxI)(a(ilm) E AfF/Apx)};
5. Boolean combinations are as before;
6. Ati[44:11px = {a I (3k)(a E AtiCoLok(S)lpx)).
</listItem>
<bodyText confidence="0.924468727272727">
The schemes clik are defined for recursion schemes as
above.
If 4) is a formula of ILFP with no free individual or
predicate variables then Slicapx is either A, the set of all
individual assignments, or 0, independent of p, but
depending on x. We say that x if SMIpx is all of A.
A language L C I* is ILFP-definable if for some 4) in
ILFP, L = Ix I x 44. Our objective is now
Theorem 2. A language is ILFP-definable iff it is in
PTIME.
The proof appears in the next subsection.
</bodyText>
<subsectionHeader confidence="0.953484">
4.2 PROOF OF THEOREM 2
</subsectionHeader>
<bodyText confidence="0.999932426229508">
The idea of our proof is the same as that for Theorem 1,
and only a sketch of the proof is necessary. We first
restate Lemma 2 for ILFP, using the same definition for
/3 and y.
Lemma 4. Let 4) be an ILFP formula, with IFIvar(0)1
= k, and T: Flvar(0) —&gt; {1, . . .,14. Let m =
Then we may construct an m-tape ATM M(4),7)
having the following properties: (i) M has oracle
states P? for each free predicate variable of 4), and (ii)
For any x E X*, any a mapping Flvar(0) to natural
numbers, and any environment p, we have the fol-
lowing: M with oracle states for the p(P), started with
x on the input tape, binary representations of the
integers a(ii) on tape T(ii), . . ., and a(ik) on tape
T(ik), and the other tapes blank, will accept without
ever writing a value j &gt; I x I on any tape, if and only
if WO, • • .,a(i)) E AtIMPx.
Proof: The proof is almost identical to that of Lemma 2.
To evaluate equations M may have to use an extra tape,
because otherwise the given nonblank tapes would be
overwritten by the arithmetic operations. If 4) is a(i) (the
only case not covered in (2), then tape 1 is used as a
counter to locate the input head at the position of the
contents of tape 1. Since arithmetic is modulo lxj, the
machine never writes too great a value in these cases.
The other cases are proved exactly as in (2), so this
completes the proof.
Lemma 5. If L E ASPACE(log n), then L is ILFP-
definable.
Proof: We may assume that L is accepted by an ATM
with p binary work tapes and one input tape. (If the tape
alphabet is not binary, encode with a homomorphism
and expand the number of tapes as necessary.) We may
further assume that the machine M never writes a string
longer than Llog2(n) — 1 on any work tape (remember
one bit on each tape in finite control if necessary). Each
work tape, or portion thereof, is thus guaranteed to
represent a binary number strictly less than n in value,
where n is the length of the input string.
We now proceed as in the proof of Lemma 3, but
coding the contents of the work tapes as binary num-
bers. We need a number h, which tells the position of
the input head. We also have two numbers / and r,
which are the binary values of the tape contents to the
left and right of the work tape head (here we describe
the case of just one work tape). The number r will
actually be the binary value of the reversal of the string
to the right of the tape head, because this makes the
operation of shifting the head a simple multiplication or
division by 2. Since a string may have leading zeroes,
we also need to keep two auxiliary numbers // and rr,
which are the actual lengths of the strings to the left and
right of the head. For each state q of the ATM we thus
have a predicate q(h,1,r,11,rr) of five integer variables.
The reader should have no difficulty in encoding the
transition rules of M exactly as in Lemma 3. For
example, a test as to whether the scanned symbol on the
work tape is 0 or 1 becomes a test of the parity of r, and
so on. Finally, it can be seen that the case of p work
tapes requires 4p + 1-ary predicates. This completes the
proof of our lemma and thus the theorem.
</bodyText>
<subsectionHeader confidence="0.997323">
4.3 WHICH POLYNOMIAL?
</subsectionHeader>
<bodyText confidence="0.999656166666667">
We can get a rough estimate of the degree of the
polynomial time algorithm, which will recognize strings
in the language defined by an ILFP grammar. We saw in
the proof of Lemma 4 that if a scheme 4) has y(4)) = p,
then an ATM with p + 1 binary work tapes can be
constructed to recognize the associated language. The
number of configurations of each tape is thus log n *
210g &apos;1. If there are p + 2 tapes, this gives 0(logP+ I n *
= 0(n&amp;quot;2) possible tape configurations. Multiply-
ing by n for the position of the input head gives O(n3)
possible ATM configurations. From an analysis of the
proof of Lemma 1 in Chandra, Kozen, and Stockmeyer
</bodyText>
<page confidence="0.895412">
8 Computational Linguistics, Volume 14, Number 4, December 1988
</page>
<bodyText confidence="0.926624">
William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its Complexity
(1981), we can see that the polynomial in our determin-
istic TM algorithm is bounded by the square of the
number of ATM configurations. This leads to an
0(n2P+6) recognition algorithm. Since this bound would
give an 0(n12) algorithm for context-free language rec-
ognition, we conjecture that the general estimate can be
improved. In particular, We would like to remove the
factor of 2 from 2p.
</bodyText>
<sectionHeader confidence="0.999571" genericHeader="method">
5 APPLICATIONS TO HEAD GRAMMARS
</sectionHeader>
<bodyText confidence="0.993024804878049">
In this section we express head grammars (Pollard 1984)
in ILFP, and thus show that head languages can be
recognized in polynomial time. Since the class of head
languages is the same as the class of tree adjunct
languages (Vijayashankar, Joshi 1985), we get the same
result for this class. We will actually give only a
simplified version of head grammars to make our ILFP
formulas easy to write. This version corresponds ex-
actly to the Modified Head Grammars of Vijayashankar
and Joshi (1985), and differs only from the original
version in that it does not treat the empty string. (Roach
(1988) has an extended discussion of head languages.)
We define a head grammar as a tuple G =
where N and are finite nonterminal and terminal
alphabets, P is a finite set of productions, and S is the
start nonterminal. The productions are of the form C—&gt;
Op(A,B), where A,B, and C are nonterminals and Op is
chosen from a fixed set of head-wrapping operations.
Productions can also have the form C —&gt; (x,y), where x
and y are terminal strings.
We view nonterminals in N as deriving pairs of
strings (u, v). In the original formulation, this meant that
the head of the string uv occurred at the end of u. The
wrapping operations come from the set {LL1,LL2,LCI,
LC2}. We consider LL2 and LC1 as examples. We define
LL2((w,x),(u,v)) = (wu, vx). Thus if A derives (w,x) and
B derives (u, v), and C LL2(A,B) is a production, then
C derives (wu,vx). Similarly, LCIaw,x),(u,v)) =
(w,xuv), so in the corresponding case, we would have C
derives (w,xuv) if C —&gt; LCi(A,B) were a production. A
string t is in L(G) iff for some u and v, t = uv and S
derives (u, v).
Given a head grammar, we write an ILFP recursion
scheme as follows. For each nonterminal C, we intro-
duce a predicate C(ij,k,1). We think of these four
integers as indexing the positions of symbols in a string,
starting at the left with 0. Then C(ij,k,l) means that the
nonterminal symbol C can derive the pair of substrings
of the input string between i and j, and between k and 1
inclusive. Thus, if C LL2(A,B) is a production, our
scheme would include a clause
</bodyText>
<equation confidence="0.9293035">
C(ij,k,l) &lt;=&gt; (3pq)(A(i,p,q + 1,1) A B(p + 1j,k,q))
Similarly, if C —&gt; LCi(A,B) were a production, we
would have
C(ij,k,l) &lt;=&gt; (3pq)(A(i,j,k,p) A B(p + 1,q,q + 1,/))
</equation>
<bodyText confidence="0.8751235">
Finally, if C—&gt; (a, bb) were a terminating production, we
would have
</bodyText>
<equation confidence="0.868817">
C(i,j,k,1)&lt;=&gt; a(i) Ai= jAk = 1+ IA b(k)
A b(k + 1) A / = k + 1
</equation>
<bodyText confidence="0.999905454545455">
The grammar would be defined by the recursion scheme
and the assertion 3jS(Ojj + 1,last), where S is the start
symbol of G.
It can be seen from this formulation that every head
grammar can be written as an ILFP scheme with at most
six total variables. Section 4 thus gives us an 0(n18)
algorithm. However, the algorithm of Vijayashanker
and Joshi (1985) is at most n6. It would seem that a rule
of thumb for the order of the polynomial algorithm is to
use the number y(4)) for the ILFP scheme 0, but we
have no proof for this conjecture.
</bodyText>
<sectionHeader confidence="0.997847" genericHeader="conclusions">
ACKNOWLEDGMENT
</sectionHeader>
<bodyText confidence="0.79442">
Research supported by NSF Grant MCS-8301022.
</bodyText>
<sectionHeader confidence="0.958949" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.988992774193548">
Chandra, A.K. and Hard, D. 1982 Structure and Complexity of
Relational Queries. Journal of Computer Systems Science 25: 99-
128.
Chandra, A.K.; Kozen, D.C.; and Stockmeyer, L.J. 1981 Alterna-
tion. Journal of Associated Computer Machines 28: 114-133.
Immerman, N. 1982 Relational Queries Computable in Polynomial
Time. In Proceedings of the 14th ACM Symposium on Theory of
Computing: 147-152.
Johnson, M. 1985 Parsing with Discontinuous Constituents. In Pro-
ceedings of the 23rd Annual Meeting of the Association for
Computational Linguistics, Chicago, IL: 127-132.
Pereira, F.C.N. and Warren, D.H.D. 1980 Definite Clause Grammars
for Language Analysis—A Survey of the Formalism and a Com-
parison with Augmented Transition Networks. Artificial Intelli-
gence 13: 231-278.
Pollard, C. 1984 Generalized Phrase Structure Grammars, Head
Grammars, and Natural Language. Ph.D thesis, Stanford Univer-
sity, Stanford, CA.
Roach, K. (1988) Formal Properties of Head Grammars. Manuscript,
Stanford University. In Manaster-Ramer, A. (ed.) Mathematics of
Language. John Benjamins, Amsterdam, Holland.
Shapiro, E.Y. 1984 On the Complexity of Logic Programs. Journal of
Logic Programming 1.
Vardi, M. 1982 The Complexity of Relational Query Languages. In
Proceedings of the 14th ACM Symposium on Theory of Comput-
ing: 137-146.
Vijayashankar, K. and Joshi, A.K. 1985 Some Computational Prop-
erties of Tree Adjoining Languages. In Proceedings of the 23rd
Meeting of the Association for Computational Linguistics, Chi-
cago, IL: 82-93.
NOTE
</reference>
<footnote confidence="0.8540265">
1. Notice that the machines are presented in ATMGOL, a syntacti-
cally ill-defined variant of ATM transition functions. Also, ATMs
and ATNs are not to be confused.
Computational Linguistics, Volume 14, Number 4, December 1988 9
</footnote>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.001438">
<title confidence="0.988349">LFP: A LOGIC FOR LINGUISTIC DESCRIPTIONS</title>
<author confidence="0.602651">AN ANALYSIS OF ITS COMPLEXITY</author>
<affiliation confidence="0.9660305">Electrical Engineering and Computer Science University of Michigan</affiliation>
<address confidence="0.996663">Ann Arbor, MI 48109</address>
<abstract confidence="0.981905564648119">We investigate the weak expressive power of a notation using first-order logic, augmented with a facility for recursion, to give linguistic descriptions. The notation is precise and easy to read, using ordinary conventions of logic. Two versions of the notation are presented. One, called CLFP, speaks about strings and concatenation, and generates the class EXPTIME of languages accepted by Turing machines in time for some The other, called ILFP, speaks about integer positions in strings, and generates the class PTIME of languages recognizable in polynomial time. An application is given, showing how to code Head Grammars in ILFP, showing why these grammars generate only polynomial time languages. 1 FIRST-ORDER LOGIC AS A TOOL FOR DESCRIPTION In this paper we investigate the properties of a new notation for specifying syntax for natural languages. It is based on the simple idea that first-order logic, though inadequate as a semantics for natural language, is quite adequate to express relations between syntactic constituents. This is the insight behind definite clause grammars (DCGs) (Pereira and Warren 1980) and, in fact, our notation is in some ways a generalization of that notation. However, we have tried to keep our formalism as much as possible like that of standard textbook firstorder logic. There are actually two versions of our notation. The first works with strings of symbols and uses concatenation as a primitive operation. The second works with integers and takes the standard arithmetic operations as primitive. These integers can be regarded as indexing positions of morphemes in a sentence, but the sentence itself is not explicitly referenced. Both versions allow the recursive definition of predicates over strings and integers. This capacity for recursive definition is what gives our grammars their generative ability, and our notation has this feature in common with DCGs. However, we liberate DCGs from the Horn clause format, and we do not base the semantics of our notation on the semantics for Prolog or for logic programs. We hope that making the logic more familiar and readable will encourage more people to use logic as a for specifying desired syntactic relations between sentential constituents in grammars. Anyone knowing the standard conventions of first-order logic should be able to read or to specify a grammar in our notation. We also provide a precise semantics for our two notations. This involves using the least-fixed-point operator from denotational semantics for programming languages to explain the recursive definition of predicates. It involves as well using restricted universal and existential quantification to restrict the class of definable predicates (sets of strings). We prove a complexity theoretic characterization for both grammar formalisms: (1) the formalism using strings and concatenation defines exactly the class EXPTIME of formal languages recognizable by deterministic Turing machines within = for some positive (2) the formalism using integers defines exactly the class of languages recognizable in time = for integer As an application of the second notation we sketch a natural way to write Head Grammars (Pollard 1984). Because these grammars can be expressed in this way, we immediately obtain the result that head languages can be recognized in polynomial time. We even obtain an estimate of the degree of the polynomial that is required, derived directly from the form of the grammatical description. Unfortunately, the estimated degree is at least twice as large as is actually necessary if uses the algorithm of Pollard (1984), or Vija- Copyright 1988 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided the copies are not made for direct commercial advantage and the and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/ 88 /01001-9$03.00 Computational Linguistics, Volume 14, Number 4, December 1988 1 William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its Complexity yashanker and Joshi (1985). We conjecture that in fact, this factor of (2) can be removed. Our complexity theoretic characterizations are versions of theorems already appearing in the literature. Shapiro (1984) characterizes the class of languages definable by logic programs with a linear space restriction as the class EXPTIME. The proof of our first theorem is very much like his. Our second theorem characterizing PTIME can be viewed as a specialization of the results of Chandra and Harel (1982), Immerman (1982), and Vardi (1982), who show that the class of sets of finite mathematical structures definable by formulas of first-order logic augmented with a least-fixed-point operator is just the class of sets of structures recognizable in polynomial time. We prove both of our results in the same way, and thus show how these apparently unconnected theorems are related. The proof uses the notion of alternating Turing machines, and thereby explains the significance of this idea for the science of formal linguistics. We should also note that our notation will not find immediate application in current linguistic theory, because it does not allow structural descriptions to be described. We are in the process of extending and modifying the notation for this purpose. However, we think it is important to explicate the properties of the individual operations used in building strings and structures. Our first attempt is therefore to understand how concatenation of strings can be expressed in a restricted logic. We can then consider other predicates or functions on both strings and treelike structures in the same uniform way. CLFP GRAMMARS 2.1 SYNTAX OF CLFP We present a standard version of the first-order theory of concatenation, augmented with the least-fixed-point operator. Before proceeding with the formal description, we give an example to illustrate the scheme we have in mind. Consider the following context-free fragment, adapted directly from Johnson (1985). S—* NP VP NP —&gt; Det Noun VP—&gt; V NP Det —&gt; NP[+Gen] I the Here is the corresponding CLFP fragment: x = yz; &lt;=&gt; 3yz.Det(y) x = .(=&gt; 3yz.V(y) x = &lt;=&gt; NP[+Getz](x) Vx = this formulation, z range over strings of (morphemes) and are predicates over strings. The second clause is here an abbreviation two clauses, where take two values, present we do not treat the problem of calculating complex feature structures, but there seems to be no reason that the notation cannot be This example illustrates the most complex case of a formula. It is a scheme, assigns predicate variables, certain formulas (the right-hand sides of the corresponding clauses in the definition). The whole scheme is understood as the simultaneous recursive definition of the predicate variables in the left sides of the definition. To handle the fact that string variables occur on the left-hand side of each clause, we will understand each clause as a function assigning both the formula on its right and the set of individual variables mentioned on the left to the given predicate symbol. We now proceed with the formal definition of CLFP. a set . . .} of individual variables ranging over strings. Let Z be a finite set of terminal symbols. These are the constants of our theory. A is another constant denoting the null string. Terms are built from variables and constants using the binary of concatenation. We also require a set predicate variables, listed as the set . .1. predicate variable equipped with an arity the number of individual arguments that a relation assigned to this variable will have. (The example CLFP scheme given above employs only unary variables set of CLFP formulas is given by the following inductive clauses. If Pvar and . . is a sequence of Ivar length = ar(P) . . in CLFP; If and are terms, then = is in CLFP; If Ivar and is in CLFP then in CLFP; 4. The usual Boolean combinations of CLFP formulas are in CLFP. 5. This clause requires more definitions. Let be finite nonempty subset of Pvar with a distinguished (1) : —&gt; (4)(R) going to be the defining clause for the predicate t(R) = we define = X, 4). require that IB(1)(R)I = thus be a finite set of individual variables. Now we that the whole map (I) is a scheme only positively in (KR) for any a; is, within the scope of an even number of negation signs. Finally, condition 5 states that if to a recursion scheme, with distinguished variable (the least fixed point of (I)) is in CLFP. Example 1. Consider the following scheme, which defines a regular language. 2 Computational Linguistics, Volume 14, Number 4, December 1988 William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its Complexity &lt;=&gt; 3y((x = ay V (x = by 7&apos;(y))) Vx = a .4#) 3w(v = cw In this example, a = {S, 7}, /34)(S) = {x}, and = 3y((x = V(x = by x = = and C4)(T) is the second formula in the scheme. In the example, we have written our recursion scheme in a conventional style to emphasize its direct connection to the usual grammatical presentations. the variable bound by the left-hand side of (1), this clause has been written with the left to make this fact apparent. Also, the use of the .:=&gt; sign is conventional in writing out 4). In our example, the first is taken as defining the distinguished predicate of our scheme. Finally, there are no occurrences of free predicate variables in this example, but there are in our example (e.g., The usual rules for calculating free individual variaapply; if the set of free variables of (I), . . .,x„))= . . .,x,J. The quantifier and Boolean cases are handled as in standard text presentations. However, if 4) is a recursion scheme then be calculated as follows. For each from this set any variain union of the resulting sets for each gt defined to be the set The rules for free predicate variables are a bit sim- In the atomic formula . . .,x,,), P a free predicate variable. In a recursion scheme 4) with doa, the set the union of the sets the set a. A final remark on notation: we will use the notation stand for the formula . . . . = A . . . A = the are terms, and the are individual variables not appearing in any ts. This will not affect our complexity results in any way. 2.2 SEMANTICS FOR CLFP We borrow some notation from the Oxford school of denotational semantics to help us explain the meaning our logic grammars. If Y are sets, then —&gt; Y] the set of functions from Y. Let A = [Ivar be the set of assignments of values to individual variables. We wish to define when a given assignment, say a, satisfies a given CLFP formula 4). This will depend on the meaning assigned to the free predicate variables in so we need to consider such assignments. Let set of maps Pvar to the class of relations on such that the arity of is ar(P). are now ready define for each formula 4, and predicate assignment set C of individual assignments satisfying respect to 1. . . = {a . . .,a(x„)) E Atit, = {a I= the of variables assigned values by a; 443x(1)1p = {a I 3u /* : NON, and similarly for universal quantification; .MIOVIMP = similarly for other Boolean connectives. Atip,S4)1p I(3k)(« E for each a recursion scheme with the same domain k as 4), and is defined as follows by on we stipulate that for each Jt, set BV(P) = we set = C4)(P)[R 4-- FALSE : R a]; cvk I (p) = : R gt] the notation O(R) : R denotes the simultaneous replacement of atomic subformulas . . in 4/ (where is free occurrence) by the . . in such a way that no free of other variables in captured by a or a woperator in may always change the bound variables of tp first, to accomplish this.) This definition appears much more clumsy than it really is, and we continue our example to illustrate it. Refer to the example of a regular grammar in the section. In the clause for are required to the formula FALSE for occurrences of both gives, after simplification, = a). substitution of FALSE into the clause for = FALSE. Now substitution of these formulae for 4) gives (after simplification): = 3y(x = ay x = V x = a; = 3w(v = cw w = It is easy to see that continuing this process will simulate all possible derivations in the grammar, and also that it explains the meaning of the scheme 4) in terms of the meaning of subformulas. Some remarks are in order to explain why we use the term &amp;quot;least-fixed-point&amp;quot;, and to explain why, in a recursion scheme, all occurrences of recursively called predicates are required to be positive. Let 4) : 9k —&gt; CLFP be a recursion scheme. Define the map nil : PA follows. If a, then . . .,u„) .44) (3« .,x„) is the sequence of variables in in increasing order of subscripts. If then 714)11p(R) = let = U 0 : R 1 Computational Linguistics, Volume 14, Number 4, December 1988 3 William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its Complexity unions are coordinatewise, the k-th iterate 0 : R E 2/ti the empty relation to each predicate variable in formula is the familiar least-fixed-point formula from denotational semantics. Then we can check that is in is the least fixed point of the continuous map It is then possible to prove that tbiSOlp = (71[AS(11p)(S) the distinguished predicate variable in If we had no conditions on negative formulas in recursion schemes, then we could entertain schemes like which, although they would receive an interpretation in first definition, would give a was not continuous, or even monotonic. We therefore exclude such cases for reasons of smoothness. Next we come to the definition of the language or denoted by a formula. A k-ary relation 1* is said to be definable in CLFP if there is a CLFP no free predicate variables such that . . &lt;=&gt; E . . .,a(x . . . . the list of free variables in 4) arranged in increasing order of subscript. (Notice the parameter been omitted since there are no predicate variables in So far, we have not restricted quantification in our formulas, and every r.e. predicate is definable. We need to add one other parameter to the definition of A, which will limit the range of quantification. This will be an will be the length of an input sentence be recognized. The occurrences of the formula will thus be changed everywhere in the above clauses to only change in the substance of the clauses is in the rule for existential and universal quantification. = {a I 3u : I E predicate said to be boundedly definable iff for . c &lt;=&gt; E . . = . . = (To abbreviate the right-hand we write . . 1= 0). first theorem can now be stated. Theorem 1. A language (unary predicate) is boundedly definable in CLFP if it is in EXPTIME. We defer the proof to the next section. EXPTIME 3.1 ALTERNATION Before proving Theorem 1, we need to discuss the method of proof both for this result and for the Integer LFP characterization of PTIME in the next section. This material is repeated from the fundamental article of Chandra, Kozen, and Stockmeyer (1981). Their paper should be consulted for the full details of what we state here. An alternating Turing machine can be regarded as a Turing machine with unbounded parallelism. In a given state, and with given tape contents, the machine can spawn a finite number of successor configurations acto rules. These configurations are thought of as separate processes, each of which runs to completion in the same way. A completed process is one which is in a special accepting state with no successors. The results of the spawned processes are reported back to the parent, which combines the results to pass on to its own parent, and so forth. How the parent does this depends on the state of the finite control. These states are classified as being either (AND), negating or accepting. If the parent is in an existential state, it reports back the logical OR of the results of its offspring. If it is in a universal state, it reports back the logical AND; if the state is negating, the parent reports the negation of its one offspring. An accepting state a logical (TRUE) be reported back. Thus a nondeterministic TM can be regarded as an alternating TM with purely existential states. An alternating TM is defined as a tuple in a standard way. It has a read-only input tape with a head capable of two-way motion. It also has a fixed number of work The input tape contains a string 1*, while the work tapes can use a tape alphabet F. The transition defined as for ordinary nondeterministic TMs. The state set is partitioned as described above into universal, existential, negating, and accepting states. The relation 6 is constrained so that existential and universal states have at least one successor, negating states have exactly one successor, and accepting have no successors. A then just a tuple describing the current state, positions of the heads, and tape contents as is familiar. The initial configuration is the one with the machine in its initial state, all the work tapes empty, and the input head at left end of the input &apos;between configurations is defined again as usual. To determine whether or not a configuration is accepting, we proceed as follows. Imagine the configurations that succeed the given one arranged in a tree, with the given configuration at the root. At each node, the immediate descendants of the configuration are the successors given by F. The tree is truncated at a level determined by the length of the input tape (this truncation is not part of the general definition but will suffice for our results.) The leaf nodes of this tree are labeled with (0) if the configuration at that node is not accepting, and with (1) if the configuration is accepting. The tree is then evaluated according to the description given above. The configuration at the root is accepting iff it is labeled (1) by this method. Thus an input is accepted by 4 Computational Linguistics, Volume 14, Number 4, December 1988 William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its Complexity the machine iff the initial configuration with that input is accepting. In our application, it will always suffice to off the tree at level 2c&amp;quot;, where the length of the string, and a positive constant depending only on the description of the machine. say that an alternating TM is bounded iff in the above tree, for any initial configuration labeling root, no auxiliary tape length ever exceeds the length of the input. We are concerned with the functions = = n this We let the class the class of languages accepted by space-bounded ATMs in this way. We then have the following theorem (Chandra, Kozen, Stockmeyer 1987): 1. If = U c&gt;0 the class of languages accepted deterministically by ordinary Turing machines within Our problem in the rest of this section is to show how linear space bounded ATMs and CLFP grammars simulate each other. To facilitate the construction of the next section, it is convenient to add one feature to the of alternating Turing machines. Let the of a k-ary relation on I.We allow machines to oracle states of the form U?(i, , . . the tape numbers. If now the predicate interpreted an actual relation on I*, then when such an instruction, it will accept or reject according to whether the strings on the specified tapes are in the will need such states to simulate recursive invocations of recursion schemes. It is not hard to modify the definition of acceptance for ordinary ATMs to that for oracle ATMs. The language or relation accepted by the ATM will now of course be relative to assignment relations to the predicate names The next subsections contain our constructions for the CLFP characterizations. Then, in Section 4 we will treat Integer LFP grammars and show how these grammars and logspace bounded ATMs simulate each other. As a consequence of the above lemma, we will then have our main results. 3.2 PROOF OF THEOREM 1 first task is to show that if a language (boundedly) CLFP-definable, then it can be recognized by a linear space bounded ATM. The idea is simple. Given an input string, our machine will try to execute the logical description of the grammar. Its states will correspond to the logical structure of the CLFP formula. If that formula is, for example, the logical AND of two subformulas, then the part of our machine for that formula will have an AND state. A recursion scheme will be executed with states corresponding to the predicate variables involved in the recursion, and so forth. To give an explicit construction of an ATM coneto a formula CLFP we need to be precise about the number of work tapes required. This will be the sum of the number of free individual variables of 0, and the number of &amp;quot;declarations&amp;quot; of bound variables in 4). A &amp;quot;declaration&amp;quot; is either the occurrence of a univeror existential quantifier in one of the individual variables bound on the left side of a (non-S) clause in a scheme. If that clause defines the predicate then the number of variables declared at that point is = IB(R)1. thus define the number declarations of bound variables in by induction follows: 1. . . = 2. = = 3. P(OVO = P(4)A = P(d)) + OW; 4. P(70) = i3(0); p(ax0) = 1 + f3(.1S(D) + + number the maximum number of tapes and is defined to be We can now state the inductive lemma which allows the construction of ATMs. 2. Let 4) be a CLFP formula, with k, . . Let m we may construct an m-tape ATM the following properties: (i) oracle P? for each free predicate variable of (ii) any 1*, any environment have the following. Let = Then oracle states for the with on . . ., tape the other tapes blank, will accept without ever writing more on any tape, if and only if . . Proof: This lemma formalizes the intuitive idea, stated that to calculate the membership of a string the language defined by a recursion scheme, it suffices to execute the scheme recursively. The full proof would use the formal definition of the semantics of ATMs, which themselves are given by least-fixed-point definitions. We have chosen not to give the full proof, because the amount of explanation would be overwhelming relative to the actual content of the proof. Instead we give a reasonably complete account of the inductive construction involved, and illustrate with the regular set example of the previous section. To start the induction over formulas cp, suppose that . . we may take be a machine = y(4)) with one oracle state the instruction P?(T(x,), . . 4) is t, = then we let a simple machine t, and using perhaps an extra tape for bookkeeping. It does a letter-by-letter comparison, so that it never has to copy more than the maximum length of any one tape. 4)is then M(4)) consists of adding a negating Computational Linguistics, Volume 14, Number 4, December 1988 5 William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its Complexity state before the initial state of MOO, and transferring control to that initial state. \/ construct and inductive Then constructed by having disjoint sets corresponding to each by which transfers control to either of the two formerly initial states. The free individual &apos;variables of disjunction are those occurring free in either dis- Let an assignment of tapes to the free of the disjunction. Then we construct T, such that = similarly for x is a free individual variable. Otherwise, any tapes referenced in M, are distinct from any tapes referenced other words, the machine shared storage for the free variables, and private storage for variables bound in either disjunct. The oracle states in the two pieces of code are not made disjoint, however, because a predicate variable is free in the disjunction iff it is free in either disjunct. It is clear that the number of of the 0, Vtp, is just y(p, Vtp,). For the case of we make exactly the same construction, using an as the new initial state. 0 is 3x0, and a tape assignment for the free variables of 0, then we construct MOO using the extape assignment which assigns a new tape + 1 the variable otherwise is the same as constructed to go through an initial loop of exisstates, which fills tape + with a string no longer than the maximum length of any string on tapes through then transfers control to the initial state same construction is used for the universal quantifier, using an initial loop of universal states. Finally, we need to treat the case of a recursion ASO. Suppose that (I) has domain let a tape assignment for . each clause C4)(Q), construct a machine induchypothesis. The global free variables of each have tapes assigned by we construct in such a way that the local tape numbers not overlap the tape numbers for any other procedure will give tape numbers to all the variain the set 134)(Q). Let this set be order. Define to be the tape assigned to in The machine M(ASI) will consist of the code for the as blocks; the initial state of each such will be labeled all the blocks, recursive oracle calls to Q? will be replaced by statements transcontrol to consider an oracle call . . any block this call by which copies tape i, to tape . . ., and tape to tape code that empties all other tapes to insert a statement &amp;quot;go to This completes the construction, and we now illustrate it with an example. Consider the recursion scheme introduced in the first section. &gt; = V (x = by Ry))) V x = &lt;=&gt; 3w(v = cw S(0). construct the machine follows:&apos; 1 : 2 : blank) Initially : guess a value of y, such that lxi, store y on tape 2; go to (ql or q2 or q7); ql : go to (q3 and q4); : check = ay tapes 1 and 2, and accept or reject as appropriate; q4 : S?(tape 2) q2 : go to (q5 and q6); : check x = tapes 2, and accept or reject as appropriate; q6 : T?(tape 2) : check x = accept or reject. we can construct a machine the clause. Then the result of pasting together the two constructions is shown in Figure 1. tape 1 : x tape 2 : y (initially blank) tape 3 : v (initially blank) tape 4 : w (initially blank) S : guess a value of y, such that lxl, on tape 2; go to (ql or q2 or q7); q 1 : go to (q3 and q4); : check = ay tapes 1 and 2 , and accept or reject as appropriate; q4: copy tape 2 to tape 1; Empty tape 2; Go to S. q2 : go to (q5 and q6); : check x = tapes 1 and 2 , and accept or reject as appropriate; q6: copy tape 2 to tape 3; empty tape 4; go to T. check x = accept or reject. T: guess a w on tape 4 no longer than v on tape 3; go to (q9 and q10); : Check v = tapes 3 and 4, and return appropriately; q10: copy tape 4(w) to tape 1; empty tape 2; go to S.</abstract>
<note confidence="0.810547666666667">Program for the Recursion Scheme. Computational Linguistics, Volume Number 4, December 1988 William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its Complexity</note>
<abstract confidence="0.991039394366197">As we remarked, we cannot give a full proof of the of our construction. However, the construction does correspond to the formal semantics of CLFP. In particular, the semantics of recursion correto the iterated schemes . the scheme roughly corresponds to developing the computree of the ATM to and replacing the oracle states at the leaves of the k-level tree with rejecting states corresponds to substituting FALSE into the kth iteration. With these remarks, the proof is complete. 3. Suppose accepted by a = ATM. Then there is a CLFP formula that for all we have C L 1 We may assume that an ATM with one work if we allow print symbols in an auxiliary tape alphabet F. By a result in Chandra, Kozen, and Stock- (1981) no negating states. We show how to construct a formula 4), which has constants ranging over which has the property stated in the conclusion of lemma: for each string /, The formula be given as a recursion scheme state become a binary predicate meaning of are specific strings in F*, is that in state the first symbol of v, and that v are the portions of the work tape to the left and the right of the head, respectively. We give a perfectly general example to illustrate the of (1). In example, the tape alphabet that a universal state of = {(r,b,right),(s,a,left)}, = Then (1)(q)(x,y) the following formula: vwtRx = A y = at &gt; r(xb,t) A s(w,o-at)) alEfa,b1 A(x = wo- Ay = bt p(w,crbt) A q(xa,t))] distinguished element of the start state of that all predicate variables in posiin (I), and that the search for w and limited to strings no longer than the length of the original input to is accepting state of we have a clause (1) of the form .(=&gt; some tautology. Technically speaking, the explicit substitutions not allowed in our formulas, but these can be by suitable sentences like (3z)(z = remarked in the first section. The cases for null must also be handled because if it tries to leave the original region. Finally, we can obtain a formula over the constant alphabet by a more complicated construction. If we encode F into / by a homomorphic mapping, then a be constructed to simulate N tape alphabet 1, but will have a number work tapes bounded linearly by the constant involved in the We now make a formula corresponding to but the predicates will have to be 2n-ary, one pair of for each tape of these remarks, the proof of the lemma is complete. immediately from the above lemmas. ILFP: WITH 4.1 SYNTAX OF ILFP Our characterization of the defining power of CLFP on the result = ASPACE(n). also that = ASPACE(log n). there a similar logical notation that gives a grammatical characterization of PTIME? This section is devoted to giving an affirmative answer to this question. As stated in the</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A K Chandra</author>
<author>D Hard</author>
</authors>
<title>Structure and Complexity of Relational Queries.</title>
<date>1982</date>
<journal>Journal of Computer Systems Science</journal>
<volume>25</volume>
<pages>99--128</pages>
<marker>Chandra, Hard, 1982</marker>
<rawString>Chandra, A.K. and Hard, D. 1982 Structure and Complexity of Relational Queries. Journal of Computer Systems Science 25: 99-128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Chandra</author>
<author>D C Kozen</author>
<author>L J Stockmeyer</author>
</authors>
<title>Alternation.</title>
<date>1981</date>
<journal>Journal of Associated Computer Machines</journal>
<volume>28</volume>
<pages>114--133</pages>
<marker>Chandra, Kozen, Stockmeyer, 1981</marker>
<rawString>Chandra, A.K.; Kozen, D.C.; and Stockmeyer, L.J. 1981 Alternation. Journal of Associated Computer Machines 28: 114-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Immerman</author>
</authors>
<title>Relational Queries Computable in Polynomial Time.</title>
<date>1982</date>
<booktitle>In Proceedings of the 14th ACM Symposium on Theory of Computing:</booktitle>
<pages>147--152</pages>
<contexts>
<context position="4955" citStr="Immerman (1982)" startWordPosition="784" endWordPosition="785">88 1 William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its Complexity yashanker and Joshi (1985). We conjecture that in fact, this factor of (2) can be removed. Our complexity theoretic characterizations are versions of theorems already appearing in the literature. Shapiro (1984) characterizes the class of languages definable by logic programs with a linear space restriction as the class EXPTIME. The proof of our first theorem is very much like his. Our second theorem characterizing PTIME can be viewed as a specialization of the results of Chandra and Harel (1982), Immerman (1982), and Vardi (1982), who show that the class of sets of finite mathematical structures definable by formulas of first-order logic augmented with a least-fixed-point operator is just the class of sets of structures recognizable in polynomial time. We prove both of our results in the same way, and thus show how these apparently unconnected theorems are related. The proof uses the notion of alternating Turing machines, and thereby explains the significance of this idea for the science of formal linguistics. We should also note that our notation will not find immediate application in current lingui</context>
<context position="34140" citStr="Immerman 1982" startWordPosition="6198" endWordPosition="6199">redicates will have to be 2n-ary, one pair of arguments for each tape of N. With these remarks, the proof of the lemma is complete. Theorem 1 follows immediately from the above lemmas. 4 ILFP: GRAMMARS WITH INTEGER INDEXING 4.1 SYNTAX OF ILFP Our characterization of the defining power of CLFP relied on the result EXPTIME = ASPACE(n). We also know that PTIME = ASPACE(log n). Is there a similar logical notation that gives a grammatical characterization of PTIME? This section is devoted to giving an affirmative answer to this question. As stated in the introduction, this result is already known (Immerman 1982, Vardi 1982), but the result fits well with the CLFP theorem, and may in the linguistic domain have some real applications other than ours to Head Grammars. To explain the logic, it helps to consider acceptance by a logspace bounded ATM. In this case, the machine has a read-only input tape, which can be accessed by a two-way read head. Writing is strictly disallowed on the input tape, in contrast to the linear space bounded ATMs of the previous section. There is also a number k of work tapes on which computation occurs. Suppose that these work tapes use a binary alphabet. If their size always</context>
</contexts>
<marker>Immerman, 1982</marker>
<rawString>Immerman, N. 1982 Relational Queries Computable in Polynomial Time. In Proceedings of the 14th ACM Symposium on Theory of Computing: 147-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>Parsing with Discontinuous Constituents.</title>
<date>1985</date>
<booktitle>In Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>127--132</pages>
<location>Chicago, IL:</location>
<contexts>
<context position="6463" citStr="Johnson (1985)" startWordPosition="1023" endWordPosition="1024">r first attempt is therefore to understand how concatenation of strings can be expressed in a restricted logic. We can then consider other predicates or functions on both strings and treelike structures in the same uniform way. 2 CLFP GRAMMARS: GRAMMARS BASED ON CONCATENATION THEORY 2.1 SYNTAX OF CLFP We present a standard version of the first-order theory of concatenation, augmented with the least-fixed-point operator. Before proceeding with the formal description, we give an example to illustrate the scheme we have in mind. Consider the following context-free fragment, adapted directly from Johnson (1985). S—* NP VP NP —&gt; Det Noun VP—&gt; V NP Det —&gt; NP[+Gen] I the Here is the corresponding CLFP fragment: S(x) .(=&gt; 3yz.NP[—Gen](y) A VP(z) A x = yz; NP[case](x) &lt;=&gt; 3yz.Det(y) A Noun[case](z) A x = yz; VP(x) .(=&gt; 3yz.V(y) A NP[—Gen](z) A x = yz; Det(x) &lt;=&gt; NP[+Getz](x) V x = the. In this formulation, x,y, and z range over strings of symbols (morphemes) and NP,VP, etc. are predicates over strings. The second clause is here an abbreviation for two clauses, where case can take two values, namely +Gen and —Gen. At present we do not treat the problem of calculating complex feature structures, but there </context>
</contexts>
<marker>Johnson, 1985</marker>
<rawString>Johnson, M. 1985 Parsing with Discontinuous Constituents. In Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics, Chicago, IL: 127-132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F C N Pereira</author>
<author>D H D Warren</author>
</authors>
<title>Definite Clause Grammars for Language Analysis—A Survey of the Formalism and a Comparison with Augmented Transition Networks.</title>
<date>1980</date>
<journal>Artificial Intelligence</journal>
<volume>13</volume>
<pages>231--278</pages>
<contexts>
<context position="1333" citStr="Pereira and Warren 1980" startWordPosition="202" endWordPosition="205">e class PTIME of languages recognizable in polynomial time. An application is given, showing how to code Head Grammars in ILFP, showing why these grammars generate only polynomial time languages. 1 FIRST-ORDER LOGIC AS A TOOL FOR SYNTACTIC DESCRIPTION In this paper we investigate the properties of a new notation for specifying syntax for natural languages. It is based on the simple idea that first-order logic, though inadequate as a semantics for natural language, is quite adequate to express relations between syntactic constituents. This is the insight behind definite clause grammars (DCGs) (Pereira and Warren 1980) and, in fact, our notation is in some ways a generalization of that notation. However, we have tried to keep our formalism as much as possible like that of standard textbook firstorder logic. There are actually two versions of our notation. The first works with strings of symbols and uses concatenation as a primitive operation. The second works with integers and takes the standard arithmetic operations as primitive. These integers can be regarded as indexing positions of morphemes in a sentence, but the sentence itself is not explicitly referenced. Both versions allow the recursive definition</context>
</contexts>
<marker>Pereira, Warren, 1980</marker>
<rawString>Pereira, F.C.N. and Warren, D.H.D. 1980 Definite Clause Grammars for Language Analysis—A Survey of the Formalism and a Comparison with Augmented Transition Networks. Artificial Intelligence 13: 231-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
</authors>
<title>Generalized Phrase Structure Grammars, Head Grammars, and Natural Language. Ph.D thesis,</title>
<date>1984</date>
<institution>Stanford University,</institution>
<location>Stanford, CA.</location>
<contexts>
<context position="3462" citStr="Pollard 1984" startWordPosition="544" endWordPosition="545"> universal and existential quantification to restrict the class of definable predicates (sets of strings). We prove a complexity theoretic characterization for both grammar formalisms: (1) the formalism using strings and concatenation defines exactly the class EXPTIME of formal languages recognizable by deterministic Turing machines within time T(n) = 2&amp;quot; for some positive c; and (2) the formalism using integers defines exactly the class PTIME of languages recognizable in time T(n) = nk for some integer k. As an application of the second notation we sketch a natural way to write Head Grammars (Pollard 1984). Because these grammars can be expressed in this way, we immediately obtain the result that head languages can be recognized in polynomial time. We even obtain an estimate of the degree of the polynomial that is required, derived directly from the form of the grammatical description. Unfortunately, the estimated degree is at least twice as large as is actually necessary if one uses the algorithm of Pollard (1984), or VijaCopyright 1988 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided that the copies are not made </context>
<context position="42444" citStr="Pollard 1984" startWordPosition="7767" endWordPosition="7768">tics, Volume 14, Number 4, December 1988 William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its Complexity (1981), we can see that the polynomial in our deterministic TM algorithm is bounded by the square of the number of ATM configurations. This leads to an 0(n2P+6) recognition algorithm. Since this bound would give an 0(n12) algorithm for context-free language recognition, we conjecture that the general estimate can be improved. In particular, We would like to remove the factor of 2 from 2p. 5 APPLICATIONS TO HEAD GRAMMARS In this section we express head grammars (Pollard 1984) in ILFP, and thus show that head languages can be recognized in polynomial time. Since the class of head languages is the same as the class of tree adjunct languages (Vijayashankar, Joshi 1985), we get the same result for this class. We will actually give only a simplified version of head grammars to make our ILFP formulas easy to write. This version corresponds exactly to the Modified Head Grammars of Vijayashankar and Joshi (1985), and differs only from the original version in that it does not treat the empty string. (Roach (1988) has an extended discussion of head languages.) We define a h</context>
</contexts>
<marker>Pollard, 1984</marker>
<rawString>Pollard, C. 1984 Generalized Phrase Structure Grammars, Head Grammars, and Natural Language. Ph.D thesis, Stanford University, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Roach</author>
</authors>
<title>Formal Properties of Head Grammars.</title>
<date>1988</date>
<booktitle>Mathematics of Language. John Benjamins,</booktitle>
<tech>Manuscript,</tech>
<editor>In Manaster-Ramer, A. (ed.)</editor>
<institution>Stanford University.</institution>
<location>Amsterdam, Holland.</location>
<contexts>
<context position="42983" citStr="Roach (1988)" startWordPosition="7860" endWordPosition="7861"> TO HEAD GRAMMARS In this section we express head grammars (Pollard 1984) in ILFP, and thus show that head languages can be recognized in polynomial time. Since the class of head languages is the same as the class of tree adjunct languages (Vijayashankar, Joshi 1985), we get the same result for this class. We will actually give only a simplified version of head grammars to make our ILFP formulas easy to write. This version corresponds exactly to the Modified Head Grammars of Vijayashankar and Joshi (1985), and differs only from the original version in that it does not treat the empty string. (Roach (1988) has an extended discussion of head languages.) We define a head grammar as a tuple G = where N and are finite nonterminal and terminal alphabets, P is a finite set of productions, and S is the start nonterminal. The productions are of the form C—&gt; Op(A,B), where A,B, and C are nonterminals and Op is chosen from a fixed set of head-wrapping operations. Productions can also have the form C —&gt; (x,y), where x and y are terminal strings. We view nonterminals in N as deriving pairs of strings (u, v). In the original formulation, this meant that the head of the string uv occurred at the end of u. Th</context>
</contexts>
<marker>Roach, 1988</marker>
<rawString>Roach, K. (1988) Formal Properties of Head Grammars. Manuscript, Stanford University. In Manaster-Ramer, A. (ed.) Mathematics of Language. John Benjamins, Amsterdam, Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Y Shapiro</author>
</authors>
<title>On the Complexity of Logic Programs.</title>
<date>1984</date>
<journal>Journal of Logic Programming</journal>
<volume>1</volume>
<contexts>
<context position="4648" citStr="Shapiro (1984)" startWordPosition="733" endWordPosition="734">hat the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/ 88 /01001-9$03.00 Computational Linguistics, Volume 14, Number 4, December 1988 1 William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its Complexity yashanker and Joshi (1985). We conjecture that in fact, this factor of (2) can be removed. Our complexity theoretic characterizations are versions of theorems already appearing in the literature. Shapiro (1984) characterizes the class of languages definable by logic programs with a linear space restriction as the class EXPTIME. The proof of our first theorem is very much like his. Our second theorem characterizing PTIME can be viewed as a specialization of the results of Chandra and Harel (1982), Immerman (1982), and Vardi (1982), who show that the class of sets of finite mathematical structures definable by formulas of first-order logic augmented with a least-fixed-point operator is just the class of sets of structures recognizable in polynomial time. We prove both of our results in the same way, a</context>
</contexts>
<marker>Shapiro, 1984</marker>
<rawString>Shapiro, E.Y. 1984 On the Complexity of Logic Programs. Journal of Logic Programming 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vardi</author>
</authors>
<title>The Complexity of Relational Query Languages.</title>
<date>1982</date>
<booktitle>In Proceedings of the 14th ACM Symposium on Theory of Computing:</booktitle>
<pages>137--146</pages>
<contexts>
<context position="4973" citStr="Vardi (1982)" startWordPosition="787" endWordPosition="788">s LFP: A Logic for Linguistic Descriptions and an Analysis of its Complexity yashanker and Joshi (1985). We conjecture that in fact, this factor of (2) can be removed. Our complexity theoretic characterizations are versions of theorems already appearing in the literature. Shapiro (1984) characterizes the class of languages definable by logic programs with a linear space restriction as the class EXPTIME. The proof of our first theorem is very much like his. Our second theorem characterizing PTIME can be viewed as a specialization of the results of Chandra and Harel (1982), Immerman (1982), and Vardi (1982), who show that the class of sets of finite mathematical structures definable by formulas of first-order logic augmented with a least-fixed-point operator is just the class of sets of structures recognizable in polynomial time. We prove both of our results in the same way, and thus show how these apparently unconnected theorems are related. The proof uses the notion of alternating Turing machines, and thereby explains the significance of this idea for the science of formal linguistics. We should also note that our notation will not find immediate application in current linguistic theory, becau</context>
<context position="34153" citStr="Vardi 1982" startWordPosition="6200" endWordPosition="6201">have to be 2n-ary, one pair of arguments for each tape of N. With these remarks, the proof of the lemma is complete. Theorem 1 follows immediately from the above lemmas. 4 ILFP: GRAMMARS WITH INTEGER INDEXING 4.1 SYNTAX OF ILFP Our characterization of the defining power of CLFP relied on the result EXPTIME = ASPACE(n). We also know that PTIME = ASPACE(log n). Is there a similar logical notation that gives a grammatical characterization of PTIME? This section is devoted to giving an affirmative answer to this question. As stated in the introduction, this result is already known (Immerman 1982, Vardi 1982), but the result fits well with the CLFP theorem, and may in the linguistic domain have some real applications other than ours to Head Grammars. To explain the logic, it helps to consider acceptance by a logspace bounded ATM. In this case, the machine has a read-only input tape, which can be accessed by a two-way read head. Writing is strictly disallowed on the input tape, in contrast to the linear space bounded ATMs of the previous section. There is also a number k of work tapes on which computation occurs. Suppose that these work tapes use a binary alphabet. If their size always is less than</context>
</contexts>
<marker>Vardi, 1982</marker>
<rawString>Vardi, M. 1982 The Complexity of Relational Query Languages. In Proceedings of the 14th ACM Symposium on Theory of Computing: 137-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijayashankar</author>
<author>A K Joshi</author>
</authors>
<title>Some Computational Properties of Tree Adjoining Languages.</title>
<date>1985</date>
<booktitle>In Proceedings of the 23rd Meeting of the Association for Computational Linguistics,</booktitle>
<pages>82--93</pages>
<location>Chicago, IL:</location>
<contexts>
<context position="42881" citStr="Vijayashankar and Joshi (1985)" startWordPosition="7840" endWordPosition="7843">hat the general estimate can be improved. In particular, We would like to remove the factor of 2 from 2p. 5 APPLICATIONS TO HEAD GRAMMARS In this section we express head grammars (Pollard 1984) in ILFP, and thus show that head languages can be recognized in polynomial time. Since the class of head languages is the same as the class of tree adjunct languages (Vijayashankar, Joshi 1985), we get the same result for this class. We will actually give only a simplified version of head grammars to make our ILFP formulas easy to write. This version corresponds exactly to the Modified Head Grammars of Vijayashankar and Joshi (1985), and differs only from the original version in that it does not treat the empty string. (Roach (1988) has an extended discussion of head languages.) We define a head grammar as a tuple G = where N and are finite nonterminal and terminal alphabets, P is a finite set of productions, and S is the start nonterminal. The productions are of the form C—&gt; Op(A,B), where A,B, and C are nonterminals and Op is chosen from a fixed set of head-wrapping operations. Productions can also have the form C —&gt; (x,y), where x and y are terminal strings. We view nonterminals in N as deriving pairs of strings (u, v</context>
</contexts>
<marker>Vijayashankar, Joshi, 1985</marker>
<rawString>Vijayashankar, K. and Joshi, A.K. 1985 Some Computational Properties of Tree Adjoining Languages. In Proceedings of the 23rd Meeting of the Association for Computational Linguistics, Chicago, IL: 82-93.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>