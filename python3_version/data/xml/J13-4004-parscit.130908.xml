<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996984">
Deterministic Coreference Resolution Based
on Entity-Centric, Precision-Ranked Rules
</title>
<author confidence="0.997033">
Heeyoung Lee*
</author>
<affiliation confidence="0.966972">
Stanford University
</affiliation>
<author confidence="0.99216">
Angel Chang*
</author>
<affiliation confidence="0.964725">
Stanford University
</affiliation>
<author confidence="0.993077">
Yves Peirsman**
</author>
<affiliation confidence="0.996844">
University of Leuven
</affiliation>
<author confidence="0.900142">
Nathanael Chamberst
</author>
<affiliation confidence="0.763546">
United States Naval Academy
</affiliation>
<author confidence="0.977668">
Mihai Surdeanul
</author>
<affiliation confidence="0.993267">
University of Arizona
</affiliation>
<author confidence="0.951659">
Dan Jurafsky§
</author>
<affiliation confidence="0.869053">
Stanford University
</affiliation>
<bodyText confidence="0.999169727272727">
We propose a new deterministic approach to coreference resolution that combines the global
information and precise features of modern machine-learning models with the transparency
and modularity of deterministic, rule-based systems. Our sieve architecture applies a battery of
deterministic coreference models one at a time from highest to lowest precision, where each model
builds on the previous model’s cluster output. The two stages of our sieve-based architecture,
a mention detection stage that heavily favors recall, followed by coreference sieves that are
precision-oriented, offer a powerful way to achieve both high precision and high recall. Further,
our approach makes use of global information through an entity-centric model that encourages
the sharing of features across all mentions that point to the same real-world entity. Despite
its simplicity, our approach gives state-of-the-art performance on several corpora and genres,
and has also been incorporated into hybrid state-of-the-art coreference systems for Chinese and
</bodyText>
<affiliation confidence="0.944281142857143">
* Stanford University, 450 Serra Mall, Stanford, CA 94305. E-mail: heeyoung®stanford.edu,
angelx®cs.stanford.edu.
** University of Leuven, Blijde-Inkomststraat 21 PO Box 03308, B-3000 Leuven, Belgium.
E-mail: yves.peirsman®arts.kuleuven.be.
t United States Naval Academy, 121 Blake Road, Annapolis, MD 21402. E-mail: nchamber®usna.edu.
t University of Arizona, PO Box 210077, Tucson, AZ 85721-0077. E-mail: msurdeanu®email.arizona.edu.
§ Stanford University, 450 Serra Mall, Stanford, CA 94305. E-mail: jurafsky®stanford.edu.
</affiliation>
<note confidence="0.9753282">
Submission received: 27 May 2012; revised submission received: 22 October 2012; accepted for publication:
20 November 2012.
doi:10.1162/COLI a 00152
© 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.774037">
Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems
that has implications throughout computational linguistics.
</bodyText>
<sectionHeader confidence="0.987472" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.982289688888889">
Coreference resolution, the task of finding all expressions that refer to the same entity in
a discourse, is important for natural language understanding tasks like summarization,
question answering, and information extraction.
The long history of coreference resolution has shown that the use of highly precise
lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b;
Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth
2008; Haghighi and Klein 2009). Recent work has also shown the importance of global
inference—performing coreference resolution jointly for several or all mentions in a
document—rather than greedily disambiguating individual pairs of mentions (Morton
2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and
Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein
2010; Cai, Mujdricza-Maydt, and Strube 2011).
Modern systems have met this need for carefully designed features and global or
entity-centric inference with machine learning approaches to coreference resolution.
But machine learning, although powerful, has limitations. Supervised machine learning
systems rely on expensive hand-labeled data sets and generalize poorly to new words
or domains. Unsupervised systems are increasingly more complex, making them hard
to tune and difficult to apply to new problems and genres as well. Rule-based models
like Lappin and Leass (1994) were a popular early solution to the subtask of pronominal
anaphora resolution. Rules are easy to create and maintain and error analysis is more
transparent. But early rule-based systems relied on hand-tuned weights and were not
capable of global inference, two factors that led to poor performance and replacement
by machine learning.
We propose a new approach that brings together the insights of these modern
supervised and unsupervised models with the advantages of deterministic, rule-based
systems. We introduce a model that performs entity-centric coreference, where all men-
tions that point to the same real-world entity are jointly modeled, in a rich feature space
using solely simple, deterministic rules. Our work is inspired both by the seminal early
work of Baldwin (1997), who first proposed that a series of high-precision rules could
be used to build a high-precision, low-recall system for anaphora resolution, and by
more recent work that has suggested that deterministic rules can outperform machine
learning models for coreference (Zhou and Su 2004; Haghighi and Klein 2009) and for
named entity recognition (Chiticariu et al. 2010).
Figure 1 illustrates the two main stages of our new deterministic model: mention
detection and coreference resolution, as well as a smaller post-processing step. In the
mention detection stage, nominal and pronominal mentions are identified using a
high-recall algorithm that selects all noun phrases (NPs), pronouns, and named entity
mentions, and then filters out non-mentions (pleonastic it, i-within-i, numeric entities,
partitives, etc.).
The coreference resolution stage is based on a succession of ten independent coref-
erence models (or ”sieves”), applied from highest to lowest precision. Precision can be
informed by linguistic intuition, or empirically determined on a coreference corpus (see
Section 4.4.3). For example, the first (highest precision) sieve links first-person pronouns
inside a quotation with the speaker of a quotation, and the tenth sieve (i.e., low precision
but high recall) implements generic pronominal coreference resolution.
</bodyText>
<page confidence="0.993049">
886
</page>
<note confidence="0.91349">
Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules
</note>
<figureCaption confidence="0.981483">
Figure 1
</figureCaption>
<bodyText confidence="0.995686379310345">
The architecture of our coreference system.
Crucially, our approach is entity-centric—that is, our architecture allows each coref-
erence decision to be globally informed by the previously clustered mentions and their
shared attributes. In particular, each deterministic rule is run on the entire discourse,
using and extending clusters (i.e., groups of mentions pointing to the same real-world
entity, built by models in previous tiers). Thus, for example, in deciding whether two
mentions i and j should corefer, our system can consider not just the local features of
i and j but also any information (head word, named entity type, gender, or number)
about the other mentions already linked to i and j in previous steps.
Finally, the architecture is highly modular, which means that additional coreference
resolution models can be easily integrated.
The two stage architecture offers a powerful way to balance both high recall and
precision in the system and make use of entity-level information with rule-based
architecture. The mention detection stage heavily favors recall, and the following sieves
favor precision. Our results here and in our earlier papers (Raghunathan et al. 2010;
Lee et al. 2011) show that this design leads to state-of-the-art performance despite the
simplicity of the individual components, and that the lack of language-specific lexical
features makes the system easy to port to other languages. The intuition is not new; in
addition to the prior coreference work mentioned earlier and discussed in Section 6, we
draw on classic ideas that have proved to be important again and again in the history of
natural language processing. The idea of beginning with the most accurate models or
starting with smaller subproblems that allow for high-precision solutions combines the
intuitions of “shaping” or “successive approximations” first proposed for learning by
Skinner (1938), and widely used in NLP (e.g., the successively trained IBM MT models
of Brown et al. [1993]) and the “islands of reliability” approaches to parsing and speech
recognition [Borghesi and Favareto 1982; Corazza et al. 1991]). The idea of beginning
with a high-recall list of candidates that are followed by a series of high-precision filters
dates back to one of the earliest architectures in natural language processing, the part of
speech tagging algorithm of the Computational Grammar Coder (Klein and Simmons
</bodyText>
<page confidence="0.990998">
887
</page>
<note confidence="0.799736">
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.996694">
1963) and the TAGGIT tagger (Greene and Rubin 1971), which begin with a high-recall
list of all possible tags for words, and then used high-precision rules to filter likely tags
based on context.
In the next section we walk through an example of our system applied to a
simple made-up text. We then describe our model in detail and test its performance
on three different corpora widely used in previous work for the evaluation of
coreference resolution. We show that our model outperforms the state-of-the-art
on each corpus. Furthermore, in these sections we describe analytic and ablative
experiments demonstrating that both aspects of our algorithm (the entity-centric aspect
that allows the global sharing of features between mentions assigned to the same
cluster and the precision-based ordering of sieves) independently offer significant
improvements to coreference, perform an error analysis, and discuss the relationship
of our work to previous models and to recent hybrid systems that have used our
algorithm as a component to resolve coreference in English, Chinese, and Arabic.
</bodyText>
<sectionHeader confidence="0.713356" genericHeader="method">
2. Walking Through a Sample Coreference Resolution
</sectionHeader>
<bodyText confidence="0.999862382352941">
Before delving into the details of our method, we illustrate the intuition behind our
approach with the simple pedagogical example listed in Table 1.
In the mention detection step, the system extracts mentions by inspecting all noun
phrases (NP) and other modifier pronouns (PRP) (see Section 3.1 for details). In Table 1,
this step identifies 11 different mentions and assigns them initially to distinct entities
(Entity id and mention id in each step are marked by superscript and subscript).
This component also extracts mention attributes—for example, John:fne:person}, and
A girl:fgender:female, number:singular}. These mentions form the input for the
following sequence of sieves.
The first coreference resolution sieve (the speaker or quotation sieve) matches
pronominal mentions that appear in a quotation block to the corresponding speaker.
In general, in all the coreference resolution sieves we traverse mentions left-to-right in
a given document (see Section 3.2.1). The first match for this model is my99, which is
merged with John10
10 into the same entity (entity id: 9). This illustrates the advantages
of our incremental approach: by assigning a higher priority to the quotation sieve, we
avoid linking my99 with A girl55, a common mistake made by generic coreference models,
since anaphoric candidates (especially in subject position) are generally preferred to
cataphoric ones (Hobbs 1978).
The next sieve searches for anaphoric antecedents that have the exact same string
as the mention under consideration. This component resolves the tenth mention, John910,
by linking it with John11. When searching for antecedents, we sort candidates in the same
sentential clause from left to right, and we prefer sentences that are closer to the mention
under consideration (see Section 3.2.2 for details). Thus, the sorted list of candidates for
John910 is It77, My favorite88, My99, A girl55, the song66, He33, a new song44, John11, a musician22.
The algorithm stops as soon as a matching antecedent is encountered. In this case, the
algorithm finds John11 and does not inspect a musician22.
The relaxed string match sieve searches for mentions satisfying a looser set of
string matching constraints than exact match (details in Section 3.3.3), but makes no
change because there are no such mentions. The precise constructs sieve searches for
several high-precision syntactic constructs, such as appositive relations and predicate
nominatives. In this example, there are two predicate nominative relations in the first
and fourth sentences, so this component clusters together John11 and a musician22, and It77
and my favorite88.
</bodyText>
<page confidence="0.992683">
888
</page>
<note confidence="0.930269">
Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules
</note>
<tableCaption confidence="0.991655">
Table 1
</tableCaption>
<figure confidence="0.908806571428572">
A sample run-through of our approach, applied to a made-up sentence. In each step we mark in
bold the affected mentions; superscript and subscript indicate entity id and mention id.
John is a musician. He played a new song. A girl was listening to
Input:
the song. “It is my favorite,” John said to her.
Mention Detection: [John]1 1 is [a musician]22. [He]3 3
Speaker Sieve: played [a new song]44.
[A girl]55 was listening to [the song]66.
“[It]77 is [[my]9 9 favorite]88,” [John]10
10 said to [her 11.
]11
[John]11 is [a musician]22. [He]33 played [a new song]44.
[A girl]55 was listening to [the song]6 6.
“[It]77 is [[my]9 9 favorite]88,” [John]910 said to [her]11.
11
[John]1 1 is [a musician]22. [He]33 played [a new song]44.
String Match: [A girl]55 was listening to [the song]6 6.
“[It]77 is [[my]1 9 favorite]88,” [John]110 said to [her]11.
11
[John]11 is [a musician]22. [He]33played [a new song]44.
Relaxed String Match: [A girl]55 was listening to [the song]6 6.
“[It]77 is [[my]1 9 favorite]88,” [John]110 said to [her]11.
11
[John]1 1 is [a musician]12. [He]33 played [a new song]44.
Precise Constructs: [A girl]55 was listening to [the song]6 6.
“[It]7 7 is [[my]19 favorite]78,” [John]110 said to [her]11.
11
[John]11 is [a musician]12. [He]33 played [a new song]44.
Strict Head Match A: [A girl]55 was listening to [the song]4 6.
“[It]77 is [[my]1 9 favorite]78,” [John]110said to [her]11.
11
[John]11 is [a musician]12. [He]33 played [a new song]44.
Strict Head Match B,C: [A girl]55 was listening to [the song]4 6.
“[It]77 is [[my]1 9 favorite]78,” [John]110said to [her]11.
11
[John]11 is [a musician]12. [He]33 played [a new song]44.
Proper Head Noun Match: [A girl]55 was listening to [the song]4 6.
“[It]77 is [[my]1 9 favorite]78,” [John]110said to [her]11.
11
[John]11 is [a musician]12. [He]33 played [a new song]44.
Relaxed Head Match: [A girl]55 was listening to [the song]4 6.
“[It]77 is [[my]1 9 favorite]78,” [John]110said to [her]11.
</figure>
<page confidence="0.618261">
11
</page>
<table confidence="0.932354555555556">
Pronoun Match: [John]1 1 is [a musician]12. [He]13 played [a new song]44.
Post Processing: [A girl]55 was listening to [the song]4 6.
Final Output: “[It]4 7 is [[my]19 favorite]48,” [John]110 said to [her]i1.
[John]11 is a musician. [He]13 played [a new song]44.
[A girl]5 5 was listening to [the song]46.
“[It]47 is [my]19 favorite,” [John]110 said to [her]i1.
[John]11 is a musician. [He]13 played [a new song]44.
[A girl]5 5 was listening to [the song]46.
“[It]47 is [my]19 favorite,” [John]110 said to [her]i1.
</table>
<bodyText confidence="0.9359086">
The next four sieves (strict head match A–C, proper head noun match) cluster
mentions that have the same head word with various other constraints. a new song44
and the song66 are linked in this step.
The last resolution component in this example addresses pronominal coreference
resolution. The three pronouns in this text, He33, It77, and her11
</bodyText>
<page confidence="0.733518">
11 are linked to their
889
</page>
<note confidence="0.295408">
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.997554416666667">
compatible antecedents based on their attributes, such as gender, number, and animacy.
In this step we assign He3 3 and her11
11 to entities 1 and 5, respectively (same gender), and
It77 to entity 4, which represents an inanimate concept.
The system concludes with a post-processing component, which implements
corpus-specific rules. For example, to align our output with the OntoNotes annotation
standard, we remove mentions assigned to singleton clusters (i.e., entities with a single
mention in text) and links obtained through predicate nominative patterns. Note that
even though we might remove some coreference links in this step, these links serve an
important purpose in the algorithm flow, as they allow new features to be discovered for
the corresponding entity and shared between its mentions. See Section 3.2.3 for details
on feature extraction.
</bodyText>
<sectionHeader confidence="0.986025" genericHeader="method">
3. The Algorithm
</sectionHeader>
<bodyText confidence="0.99996275">
We first describe our mention detection stage, then introduce the general architecture of
the coreference stage, followed by a detailed examination of the coreference sieves. In
describing the architecture, we will sometimes find it helpful to discuss the precision of
individual components, drawn from our later experiments in Section 4.
</bodyText>
<subsectionHeader confidence="0.999621">
3.1 Mention Detection
</subsectionHeader>
<bodyText confidence="0.916817705882353">
As we suggested earlier, the recall of our mention detection component is more impor-
tant than its precision. This is because for the OntoNotes corpus and for many practical
applications, any missed mentions are guaranteed to affect the final score by decreas-
ing recall, whereas spurious mentions may not impact the overall score if they are
assigned to singleton clusters, because singletons are deleted during post-processing.
Our mention detection algorithm implements this intuition via a series of simple yet
broad-coverage heuristics that take advantage of syntax, named entity recognition and
manually written patterns. Note that those patterns are built based on the OntoNotes
annotation guideline because mention detection in general depends heavily on the
annotation policy.
We start by marking all NPs, pronouns, and named entity mentions (see the named
entity tagset in Appendix A) that were not previously marked (i.e., they appear as
modifiers in other NPs) as candidate mentions. From this set of candidates we remove
the mentions that match any of the following exclusion rules:
1. We remove a mention if a larger mention with the same head word exists
(e.g., we remove The five insurance companies in The five insurance companies
approved to be established this time).
</bodyText>
<listItem confidence="0.9289332">
2. We discard numeric entities such as percents, money, cardinals, and
quantities (e.g., 9%, $10, 000, Tens of thousands, 100 miles).
3. We remove mentions with partitive or quantifier expressions (e.g., a total of
177 projects, none of them, millions of people).1
1 These are NPs with the word ‘of’ preceded by one of nine quantifiers or 34 partitives.
</listItem>
<page confidence="0.99219">
890
</page>
<bodyText confidence="0.716303">
Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules
</bodyText>
<listItem confidence="0.984529428571429">
4. We remove pleonastic it pronouns, detected using a small set of patterns
(e.g., It is possible that ... , It seems that ... , It turns out ... ). The complete set
of patterns, using the tregex2 notation, is shown in Appendix B.
5. We discard adjectival forms of nations or nationality acronyms (e.g.,
American, U.S., U.K.), following the OntoNotes annotation guidelines.
6. We remove stop words from the following list determined by error
analysis on mention detection: there, ltd., etc, ’s, hmm.
</listItem>
<bodyText confidence="0.99752625">
Note that some rules change depending on the corpus we use for evaluation. In
particular, adjectival forms of nations are valid mentions in the Automated Content
Extraction (ACE) corpus (Doddington et al. 2004), thus they would not be removed
when processing this corpus.
</bodyText>
<subsectionHeader confidence="0.999361">
3.2 Resolution Architecture
</subsectionHeader>
<bodyText confidence="0.994203535714286">
Traditionally, coreference resolution is implemented as a quadratic problem, where
potential coreference links between any two mentions in a document are consid-
ered. This is not ideal, however, as it increases both the likelihood of errors and
the processing time. In this article, we argue that it is better to cautiously construct
high-quality mention clusters,3 and use an entity-centric model that allows the shar-
ing of information across these incrementally constructed clusters. We achieve these
goals by: (a) aggressively filtering the search space for which mention to consider
for resolution (Section 3.2.1) and which antecedents to consider for a given men-
tion (Section 3.2.2), and (b) constructing features from partially built mention clusters
(Section 3.2.3).
3.2.1 Mention Selection in a Given Sieve. Recall that our model is a battery of resolution
sieves applied sequentially. Thus, in each given sieve, we have partial mention clusters
produced by the previous model. We exploit this information for mention selection, by
considering only mentions that are currently first in textual order in their cluster. For
example, given the following ordered list of mentions, {mi, m22, m23, m34, m15, m26}, where
the superscript indicates cluster id, our model will attempt to resolve only m22 and m34 (m1 1
is not resolved because it is the first mention in a text). These two are the only mentions
that currently appear first in their respective clusters and have potential antecedents in
the document. The motivation behind this heuristic is two-fold. First, early mentions are
usually better defined than subsequent ones, which are likely to have fewer modifiers or
be pronouns (Fox 1993). Because several of our models use features extracted from NP
modifiers, it is important to prioritize mentions that include such information. Second,
by definition, first mentions appear closer to the beginning of the document, hence there
are fewer antecedent candidates to select from, and thus fewer opportunities to make a
mistake.
We further prune the search space using a simple model of discourse salience. We
disable coreference for mentions appearing first in their corresponding clusters that: (a)
are or start with indefinite pronouns (e.g., some, other), (b) start with indefinite articles
</bodyText>
<footnote confidence="0.998881">
2 http://nlp.stanford.edu/software/tregex.shtml.
3 In this article we use the terms mention cluster and entity interchangeably. We prefer the former when
discussing technical aspects of our approach and the latter in a more theoretical context.
</footnote>
<page confidence="0.986234">
891
</page>
<note confidence="0.294171">
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.800941454545454">
(e.g., a, an), or (c) are bare plurals. One exception to (a) and (b) is the model deployed
in the Exact String Match sieve, which only links mentions if their entire extents match
exactly (see Section 3.3.2). This model is triggered for all nominal mentions regardless
of discourse salience, because it is possible that indefinite mentions are repeated in a
document when concepts are discussed but not instantiated, e.g., a sports bar in the
following:
Hanlon, a longtime Broncos fan, thinks it is the perfect place for a sports bar and has put up
a blue-and-orange sign reading, “Wanted Broncos Sports Bar On This Site.” ... In a Nov. 28
letter, Proper states “while we have no objection to your advertising the property as a location
for a sports bar, using the Broncos’ name and colors gives the false impression that the bar
is or can be affiliated with the Broncos.”
</bodyText>
<listItem confidence="0.9940595">
3.2.2 Antecedent Selection for a Given Mention. Given a mention mi, each model may either
decline to propose a solution (in the hope that one of the subsequent models will solve
it) or deterministically select a single best antecedent from a list of previous mentions
m1, ..., mi−1. We sort candidate antecedents using syntactic information provided by
the Stanford parser. Candidates are sorted using the following criteria:
• In a given sentential clause (i.e., parser constituents whose label starts
with S), candidates are sorted using a left-to-right breadth-first traversal of
the corresponding syntactic constituent (Hobbs 1978). Figure 2 shows an
example of candidate ordering based on this traversal. The left-to-right
ordering favors subjects, which tend to appear closer to the beginning of
the sentence and are more probable antecedents. The breadth-first
traversal promotes syntactic salience by preferring noun phrases that are
closer to the top of the parse tree (Haghighi and Klein 2009).
• If the sentence containing the anaphoric mention contains multiple
</listItem>
<bodyText confidence="0.8049755">
clauses, we repeat the previous heuristic separately in each S* constituent,
starting with the one containing the mention.
</bodyText>
<figureCaption confidence="0.729972">
Figure 2
</figureCaption>
<bodyText confidence="0.853835">
Example of left-to-right breadth-first tree traversal. The numbers indicate the order in which the
NPs are visited.
</bodyText>
<page confidence="0.996254">
892
</page>
<bodyText confidence="0.719316">
Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules
</bodyText>
<listItem confidence="0.993585">
• Clauses in previous sentences are sorted based on their textual proximity
to the anaphoric mention.
</listItem>
<bodyText confidence="0.990537038461538">
The sorting of antecedent candidates is important because our algorithm stops at the
first match. Thus, low-quality sorting negatively impacts the actual coreference links
created.
This antecedent selection algorithm applies to all the coreference resolution sieves
described in this article, with the exception of the speaker identification sieve (Sec-
tion 3.3.1) and the sieve that applies appositive and predicate nominative patterns
(Section 3.3.4).
3.2.3 Feature Sharing in the Entity-Centric Model. In a significant departure from previous
work, each model in our framework gets (possibly incomplete) entity information for
each mention from the clusters constructed by the earlier coreference models. In other
words, each mention mi may already be assigned to an entity Ej containing a set
of mentions: Ej = {mj1,...,mjk}; mi E Ej. Unassigned mentions are unique members of
their own cluster. We use this information to share information between same-entity
mentions.
This is especially important for pronominal coreference resolution (discussed later
in this section), which can be severely affected by missing attributes (which introduce
precision errors because incorrect antecedents are selected due to missing information)
and incorrect attributes (which introduce recall errors because correct links are not
generated due to attribute mismatch between mention and antecedent). To address this
issue, we perform a union of all mention attributes (e.g., number, gender, animacy)
for a given entity and share the result with all corresponding mentions. If attributes
from different mentions contradict each other we maintain all variants. For example,
our naive number detection assigns singular to the mention a group of students and
plural to five students. When these mentions end up in the same cluster, the resulting
number attributes becomes the set {singular, plural}. Thus this cluster can later be
merged with both singular and plural pronouns.
</bodyText>
<subsectionHeader confidence="0.99981">
3.3 Coreference Resolution Sieves
</subsectionHeader>
<bodyText confidence="0.999752">
We describe next the sequence of coreference models proposed in this article. Table 2
lists all these models in the order in which they are applied. We discuss their individual
contribution to the overall system later, in Section 4.4.3.
</bodyText>
<tableCaption confidence="0.891626">
Table 2
</tableCaption>
<table confidence="0.2203708">
Sequence of sieves as they are applied in the overall model.
Sequence Model Name
Pass 1 Speaker Identification Sieve
Pass 2 Exact String Match Sieve
Pass 3 Relaxed String Match Sieve
</table>
<footnote confidence="0.9731792">
Pass 4 Precise Constructs Sieve (e.g., appositives)
Passes 5–7 Strict Head Match Sieves A–C
Pass 8 Proper Head Noun Match Sieve
Pass 9 Relaxed Head Match Sieve
Pass 10 Pronoun Resolution Sieve
</footnote>
<page confidence="0.993267">
893
</page>
<note confidence="0.252664">
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.96879725">
3.3.1 Pass 1 – Speaker Identification. This sieve matches speakers to compatible pronouns,
using shallow discourse understanding to handle quotations and conversation
transcripts, following the early work of Baldwin (1995, 1997). We begin by identifying
speakers within text. In non-conversational text, we use a simple heuristic that searches
for the subjects of reporting verbs (e.g., say) in the same sentence or neighboring
sentences to a quotation. In conversational text, speaker information is provided in the
data set.
The extracted speakers then allow us to implement the following sieve heuristics:
</bodyText>
<listItem confidence="0.999358333333333">
• (I)s4 assigned to the same speaker are coreferent.
• (you)s with the same speaker are coreferent.
• The speaker and (I)s in her text are coreferent.
</listItem>
<bodyText confidence="0.92249125">
Thus for example I, my, and she in the following sentence are coreferent: “[I] voted
for [Nader] because [he] was most aligned with [my] values,” [she] said.
In addition to this sieve, we impose speaker constraints on decisions made by
subsequent sieves:
</bodyText>
<listItem confidence="0.99975825">
• The speaker and a mention which is not (I) in the speaker’s utterance
cannot be coreferent.
• Two (I)s (or two (you)s, or two (we)s) assigned to different speakers
cannot be coreferent.
• Two different person pronouns by the same speaker cannot be coreferent.
• Nominal mentions cannot be coreferent with (I), (you), or (we) in the
same turn or quotation.
• In conversations, (you) can corefer only with the previous speaker.
</listItem>
<bodyText confidence="0.8525025">
The constraints result in causing [my] and [he] to not be coreferent in the earlier
example (due to the third constraint).
</bodyText>
<listItem confidence="0.961470066666667">
3.3.2 Pass 2 – Exact Match. This model links two mentions only if they contain exactly the
same extent text, including modifiers and determiners (e.g., [the Shahab 3 ground-ground
missile] and [the Shahab 3 ground-ground missile]). As expected, this model is very precise,
with a precision over 90% B3 (see Table 8 in Section 4.4.3).
3.3.3 Pass 3 – Relaxed String Match. This sieve considers two nominal mentions as
coreferent if the strings obtained by dropping the text following their head words (such
as relative clauses and PP and participial postmodifiers) are identical (e.g., [Clinton] and
[Clinton, whose term ends in January]).
3.3.4 Pass 4 – Precise Constructs. This model links two mentions if any of the following
conditions are satisfied:
• Appositive – the two nominal mentions are in an appositive construction
(e.g., [Israel’s Deputy Defense Minister], [Ephraim Sneh] , said ... ). We use the
standard Haghighi and Klein (2009) definition to detect appositives: third
children of a parent NP whose expansion begins with (NP , NP), when
there is not a conjunction in the expansion.
</listItem>
<footnote confidence="0.8846775">
4 We define (I) as I, my, me, or mine, (we) as first person plural pronouns, and (you) as second person
pronouns.
</footnote>
<page confidence="0.996092">
894
</page>
<bodyText confidence="0.718113">
Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules
</bodyText>
<listItem confidence="0.98397275">
• Predicate nominative – the two mentions (nominal or pronominal) are
in a copulative subject–object relation (e.g., [The New York-based College
Board] is [a nonprofit organization that administers the SATs and promotes
higher education] [Poon and Domingos 2008]).
• Role appositive – the candidate antecedent is headed by a noun and
appears as a modifier in an NP whose head is the current mention (e.g.,
[[actress] Rebecca Schaeffer]). This feature is inspired by Haghighi and Klein
(2009), who triggered it only if the mention is labeled as a person by the
Stanford named entity recognizer (NER). We constrain this heuristic more
in our work: We allow this feature to match only if: (a) the mention is
labeled as a person, (b) the antecedent is animate (we detail animacy
detection in Section 3.3.9), and (c) the antecedent’s gender is not neutral.
• Relative pronoun – the mention is a relative pronoun that modifies the
head of the antecedent NP (e.g., [the finance street [which] has already formed
in the Waitan district]).
• Acronym – both mentions are tagged as NNP and one of them is an
acronym of the other (e.g., [Agence France Presse] ... [AFP]). Our acronym
detection algorithm marks a mention as an acronym of another if its text
equals the sequence of upper case characters in the other mention. The
algorithm is simple, but our error analysis suggests it nonetheless does
not lead to errors.
• Demonym5 – one of the mentions is a demonym of the other (e.g., [Israel]
... [Israeli]). For demonym detection we use a static list of countries and
their gentilic forms from Wikipedia.6
</listItem>
<bodyText confidence="0.775476923076923">
All of these constructs are very precise; we show in Section 4.4.3 that the B3 precision
of the overall model after adding this sieve is approximately 90%. In the OntoNotes
corpus, this sieve does not enhance recall significantly, mainly because appositions
and predicate nominatives are not annotated in this corpus (they are annotated in
ACE). Regardless of annotation standard, however, this sieve is important because it
grows entities with high quality elements, which has a significant impact on the entity’s
features (as discussed in Section 3.2.3).
3.3.5 Pass 5 – Strict Head Match. Linking a mention to an antecedent based on the naive
matching of their head words generates many spurious links because it completely
ignores possibly incompatible modifiers (Elsner and Charniak 2010). For example, Yale
University and Harvard University have similar head words, but they are obviously
different entities. To address this issue, this pass implements several constraints that
must all be matched in order to yield a link:
</bodyText>
<listItem confidence="0.994263333333333">
• Entity head match – the mention head word matches any head word of
mentions in the antecedent entity. Note that this feature is actually more
relaxed than naive head matching in a pair of mentions because here it is
satisfied when the mention’s head matches the head of any mention in the
candidate entity. We constrain this feature by enforcing a conjunction with
the following features.
</listItem>
<footnote confidence="0.611403">
5 Demonym is not annotated in OntoNotes but we keep it in the system.
6 http://en.wikipedia.org/wiki/List of adjectival and demonymic forms of place names.
</footnote>
<page confidence="0.982726">
895
</page>
<figure confidence="0.309937">
Computational Linguistics Volume 39, Number 4
</figure>
<listItem confidence="0.9087505">
• Word inclusion – all the non-stop7 words in the current entity to be solved
are included in the set of non-stop words in the antecedent entity. This
</listItem>
<bodyText confidence="0.8757815">
heuristic exploits the discourse property that states that it is uncommon to
introduce novel information in later mentions (Fox 1993). Typically,
mentions of the same entity become shorter and less informative as the
narrative progresses. For example, based on this constraint, the model
correctly clusters together the two mentions in the following text:
...intervene in the [Florida Supreme Court]’s move ... does look like very
dramatic change made by [the Florida court]
and avoids clustering the two mentions in the following text:
The pilot had confirmed ... he had turned onto [the correct runway] but pilots
behind him say he turned onto [the wrong runway].
</bodyText>
<listItem confidence="0.986510142857143">
• Compatible modifiers only – the mention’s modifiers are all included in
the modifiers of the antecedent candidate. This feature models the same
discourse property as the previous feature, but it focuses on the two
individual mentions to be linked, rather than their corresponding entities.
For this feature we only use modifiers that are nouns or adjectives.
• Not i-within-i – the two mentions are not in an i-within-i construct, that is,
one cannot be a child NP in the other’s NP constituent (Chomsky 1981).
</listItem>
<bodyText confidence="0.9979565">
This pass continues to maintain high precision (over 86% B3) while improving recall
significantly (approximately 4.5 B3 points).
</bodyText>
<subsubsectionHeader confidence="0.608235">
3.3.6 Passes 6 and 7 – Variants of Strict Head Match. Sieves 6 and 7 are different relaxations
</subsubsectionHeader>
<bodyText confidence="0.9950518">
of the feature conjunction introduced in Pass 5, that is, Pass 6 removes the compatible
modifiers only feature, and Pass 7 removes the word inclusion constraint. All in all,
these two passes yield an improvement of 0.9 B3 F1 points, due to recall improvements.
Table 8 in Section 4.4.3 shows that the word inclusion feature is more precise than
compatible modifiers only, but the latter has better recall.
</bodyText>
<listItem confidence="0.9749131">
3.3.7 Pass 8 – Proper Head Word Match. This sieve marks two mentions headed by
proper nouns as coreferent if they have the same head word and satisfy the following
constraints:
• Not i-within-i - same as in Pass 5.
• No location mismatches - the modifiers of two mentions cannot contain
different location named entities, other proper nouns, or spatial modifiers.
For example, [Lebanon] and [southern Lebanon] are not coreferent.
• No numeric mismatches - the second mention cannot have a number that
does not appear in the antecedent, e.g., [people] and [around 200 people] are
not coreferent.
</listItem>
<page confidence="0.764835">
7 Our stopword list includes person titles as well.
896
</page>
<bodyText confidence="0.972112388888889">
Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules
3.3.8 Pass 9 – Relaxed Head Match. This pass relaxes the entity head match heuristic by
allowing the mention head to match any word in the antecedent entity. For example,
this heuristic matches the mention Sanders to an entity containing the mentions {Sauls,
the judge, Circuit Judge N. Sanders Sauls}. To maintain high precision, this pass requires
that both mention and antecedent be labeled as named entities and the types co-
incide. Furthermore, this pass implements a conjunction of the given features with word
inclusion and not i-within-i. This pass yields less than 0.4 point improvement in
most metrics.
3.3.9 Pass 10 – Pronominal Coreference Resolution. With one exception (Pass 1), all the
previous coreference models focus on nominal coreference resolution. It would be incor-
rect to say that our framework ignores pronominal coreference in the previous passes,
however. In fact, the previous models prepare the stage for pronominal coreference by
constructing precise entities with shared mention attributes. These are crucial factors for
pronominal coreference.
We implement pronominal coreference resolution using an approach standard for
many decades: enforcing agreement constraints between the coreferent mentions. We
use the following attributes for these constraints:
</bodyText>
<listItem confidence="0.999420222222222">
• Number – we assign number attributes based on: (a) a static list for
pronouns; (b) NER labels: mentions marked as a named entity are
considered singular with the exception of organizations, which can be
both singular and plural; (c) part of speech tags: NN*S tags are plural and
all other NN* tags are singular; and (d) a static dictionary from Bergsma
and Lin (2006).
• Gender – we assign gender attributes from static lexicons from Bergsma
and Lin (2006), and Ji and Lin (2009).
• Person – we assign person attributes only to pronouns. We do not enforce
this constraint when linking two pronouns, however, if one appears within
quotes. This is a simple heuristic for speaker detection (e.g., I and she point
to the same person in “[I] voted my conscience,” [she] said).
• Animacy – we set animacy attributes using: (a) a static list for pronouns;
(b) NER labels (e.g., PERSON is animate whereas LOCATION is not); and (c) a
dictionary bootstrapped from the Web (Ji and Lin 2009).
• NER label – from the Stanford NER.
• Pronoun distance - sentence distance between a pronoun and its
antecedent cannot be larger than 3.
</listItem>
<bodyText confidence="0.99909475">
When we cannot extract an attribute, we set the corresponding value to unknown and
treat it as a wildcard—that is, it can match any other value. As expected, pronominal
coreference resolution has a big impact on the overall score (e.g., 5 B3 F1 points in the
development partition of OntoNotes).
</bodyText>
<subsectionHeader confidence="0.985246">
3.4 Post Processing
</subsectionHeader>
<bodyText confidence="0.9996385">
This step implements several transformations required to guarantee that our out-
put matches the annotation specification in the corresponding corpus. Currently this
</bodyText>
<page confidence="0.966498">
897
</page>
<figure confidence="0.313126">
Computational Linguistics Volume 39, Number 4
</figure>
<listItem confidence="0.878572">
step is deployed only for the OntoNotes corpus and it contains the following two
operations:
• We discard singleton clusters.
• We discard the shorter mentions in appositive patterns and the mentions
that appear later in text in copulative relations. For example, in the text
[[Yongkang Zhou] , the general manager] or [Mr. Savoca] had been [a
</listItem>
<bodyText confidence="0.984617">
consultant... ], the mentions Yongkang Zhou and a consultant... are removed
in this stage.
</bodyText>
<sectionHeader confidence="0.990818" genericHeader="method">
4. Experimental Results
</sectionHeader>
<bodyText confidence="0.999989">
We start this section with overall results on three corpora widely used for the evaluation
of coreference resolution systems. We continue with a series of ablative experiments
that analyze the contribution of each aspect of our approach and conclude with error
analysis, which highlights cases currently not solved by our approach.
</bodyText>
<subsectionHeader confidence="0.987185">
4.1 Corpora
</subsectionHeader>
<bodyText confidence="0.999545">
We used the following corpora for development and formal evaluation:
</bodyText>
<listItem confidence="0.999548818181818">
• OntoNotes-Dev – development partition of OntoNotes v4.0 provided in
the CoNLL2011 shared task (Pradhan et al. 2011).
• OntoNotes-Test – test partition of OntoNotes v4.0 provided in the
CoNLL-2011 shared task.
• ACE2004-Culotta-Test – partition of the ACE 2004 corpus reserved for
testing by several previous studies (Culotta et al. 2007; Bengtson and Roth
2008; Haghighi and Klein 2009).
• ACE2004-nwire – newswire subset of the ACE 2004 corpus, utilized by
Poon and Domingos (2008) and Haghighi and Klein (2009) for testing.
• MUC6-Test – test corpus from the sixth Message Understanding
Conference (MUC-6) evaluation.
</listItem>
<bodyText confidence="0.611648916666667">
The corpora statistics are shown in Table 3. We used the first corpus (OntoNotes-Dev)
for development and all others for the formal evaluation. We parsed all documents in
the ACE and MUC corpora using the Stanford parser (Klein and Manning 2003) and
the Stanford NER (Finkel, Grenager, and Manning 2005). We used the provided parse
Table 3
Corpora statistics.
Corpora # Documents # Sentences # Words # Entities # Mentions
OntoNotes-Dev 303 6,894 136K 3,752 14,291
OntoNotes-Test 322 8,262 142K 3,926 16,291
ACE2004-Culotta-Test 107 1,993 33K 2,576 5,455
ACE2004-nwire 128 3,594 74K 4,762 11,398
MUC6-Test 30 576 13K 496 2,136
</bodyText>
<page confidence="0.970329">
898
</page>
<bodyText confidence="0.90548">
Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules
trees and named entity labels (not gold) in the OntoNotes corpora to facilitate the com-
parison with other systems.
</bodyText>
<subsectionHeader confidence="0.987441">
4.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999853">
We use five evaluation metrics widely used in the literature. B3 and CEAF have im-
plementation variations in how to take system mentions into account. We followed the
same implementation as used in CoNLL-2011 shared task.
</bodyText>
<listItem confidence="0.892413666666667">
• MUC (Vilain et al. 1995) – link-based metric which measures how many
predicted and gold mention clusters need to be merged to cover the gold
and predicted clusters, respectively.
</listItem>
<equation confidence="0.99530075">
E (|Gi|−|p(Gi)|)
R = (G,• a gold mention cluster, p(Gi): partitions of Gi)
E (|Gi|−1) t
E (|Si|−|p(Si)|)
P = (S.• a system mention cluster, p(Si): partitions of Si)
E (|Si|−1) t&apos;
F1 = 2PR
P+R
</equation>
<listItem confidence="0.965479">
• B3 (Bagga and Baldwin 1998) – mention-based metric which measures the
proportion of overlap between predicted and gold mention clusters for a
given mention. When Gmi is the gold cluster of mention mi and Smi is the
system cluster of mention mi,
R = Ei |G|Gm |mi |, P = Ei |G |Smi|mi |, F1 = P+RR
• CEAF (Constrained Entity Aligned F-measure) (Luo 2005) – metric based
on entity alignment.
</listItem>
<bodyText confidence="0.985536">
For best alignment g∗ = argmaxg∈GmΦ(g) (Φ(g): total similarity of g, a
one-to-one mapping from G: gold mention clusters to S: system mention
clusters),
</bodyText>
<equation confidence="0.9879975">
R = Φ(g∗)
Ei φ(Gi,Gi), P = Φ(g∗)
Ei φ(Si,Si), F1 = 2PR
P+R
</equation>
<bodyText confidence="0.704040666666667">
If we use φ(G, S) = |G ∩ S|, it is called mention-based CEAF (CEAF-φ3), if
we use φ(G, S) = IRRnS |, it is called entity-based CEAF (CEAF-φ4).
IRI
</bodyText>
<listItem confidence="0.99491975">
• BLANC (BiLateral Assessment of NounPhrase Coreference) (Recasens and
Hovy 2011) – metric applying the Rand index (Rand 1971) to coreference
to deal with imbalance between singletons and coreferent mentions by
considering coreference and non-coreference links.
</listItem>
<equation confidence="0.98986225">
Pc = rc
rc+wc, Pn = rn
rn+wn, Rc = rc
rc+wn, Rn = rn
rn+wc,
Fc = 2PcRcPc+Rc ,Fn = 2PnRn
Pn+Rn , BLANC = Fc+Fn
2
</equation>
<bodyText confidence="0.665059666666667">
(rc: the number of correct coreference links, wc: the number of incorrect
coreference links, rn: the number of correct non-coreference links, wn: the
number of incorrect non-coreference links)
</bodyText>
<listItem confidence="0.7941085">
• CoNLL F1 Average of MUC, B3, and CEAF-φ4 F1. This was the official
metric in the CoNLL-2011 shared task (Pradhan et al. 2011).
</listItem>
<subsectionHeader confidence="0.998544">
4.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9999735">
Tables 4 and 5 compare the performance of our system with other state-of-the-art
systems in the CoNLL-2011 shared task and the ACE and MUC corpora, respectively.
For the CoNLL-2011 shared task we report results in the closed track, which did not
allow the use of external resources, and the open track, which allowed any other
</bodyText>
<page confidence="0.99546">
899
</page>
<table confidence="0.438684">
Computational Linguistics Volume 39, Number 4
</table>
<tableCaption confidence="0.996844">
Table 4
</tableCaption>
<bodyText confidence="0.81571075">
Performance of the top systems in the CoNLL-2011 shared task. All these systems use
automatically detected mentions. We report results for both the closed and the open tracks,
which allowed the use of resources not provided by the task organizers. MD indicates mention
detection, and gold boundaries indicate that mention boundary information is given.
</bodyText>
<table confidence="0.999881189189189">
System MD MUC B3 CEAF-φ4 BLANC CoNLL
R P F1 R P F1 R P F1 R P F1 R P F1 F1
Closed Track
This paper 75.1 66.8 70.7 61.8 57.5 59.6 68.4 68.2 68.3 43.4 47.8 45.5 70.6 76.2 73.0 57.8
Sapena 92.4 28.2 43.2 56.3 63.2 59.6 62.8 72.1 67.1 44.8 38.4 41.3 69.5 73.1 71.1 56.0
Chang 68.1 62.0 64.9 57.2 57.2 57.2 67.1 70.5 68.8 41.9 41.9 41.9 71.2 77.1 73.7 56.0
Nugues 69.9 68.1 69.0 60.2 57.1 58.6 66.7 64.2 65.5 38.1 41.1 39.5 72.0 70.3 71.1 54.5
Santos 67.8 63.3 65.5 59.2 54.3 56.7 68.8 62.8 65.7 35.9 40.2 37.9 73.4 66.9 69.5 53.4
Song 57.8 80.4 67.3 53.7 67.8 60.0 60.7 66.1 63.2 43.4 30.7 36.0 69.5 59.7 61.5 53.1
Stoyanov 70.8 65.0 67.8 63.6 54.0 58.4 72.6 53.3 61.4 32.0 40.8 35.9 73.2 58.9 60.9 51.9
Sobha 67.8 62.1 64.8 51.1 49.9 50.5 62.6 65.4 64.0 40.7 41.8 41.2 61.4 68.4 63.9 51.9
Kobdani 62.1 60.0 61.0 55.6 51.5 53.5 69.7 62.4 65.9 32.3 35.4 33.8 61.9 63.5 62.6 51.0
Zhou 61.1 63.6 62.3 45.7 52.8 49.0 57.1 72.9 64.1 43.2 36.8 39.7 61.1 73.9 64.7 50.9
Charton 65.9 62.8 64.3 55.1 50.1 52.5 66.3 58.4 62.1 34.3 39.1 36.5 69.9 62.2 64.8 50.4
Yang 71.9 57.5 63.9 59.9 46.4 52.3 71.6 55.1 62.3 30.3 42.4 35.3 71.1 61.8 64.6 50.0
Hao 64.5 64.1 64.3 57.9 51.4 54.5 67.8 55.4 61.0 30.1 35.8 32.7 72.6 62.4 65.4 49.4
Xinxin 65.5 58.7 61.9 48.5 44.9 46.6 61.6 62.3 61.9 35.2 38.6 36.8 63.0 65.8 64.3 48.5
Zhang 55.4 68.3 61.1 42.0 55.6 47.9 52.6 73.1 61.1 42.0 30.3 35.2 62.8 69.2 65.2 48.1
Kummerfeld 69.8 57.0 62.7 46.4 39.6 42.7 63.6 57.3 60.3 35.1 42.3 38.3 58.7 61.6 59.9 47.1
Zhekova 67.5 37.6 48.3 28.9 20.7 24.1 67.1 56.7 61.5 31.6 41.2 35.8 52.8 57.1 53.8 40.4
Irwin 17.1 61.1 26.7 12.5 50.6 20.0 35.1 89.9 50.5 45.8 17.4 25.2 51.5 56.8 51.1 31.9
Open Track
This paper 74.3 67.9 70.9 62.8 59.3 61.0 68.9 69.0 68.9 43.3 46.8 45.0 71.9 76.6 74.0 58.3
Cai 67.2 67.6 67.4 56.7 58.9 57.8 64.6 71.0 67.7 42.7 40.7 41.7 69.8 74.0 71.6 55.7
Uryupina 70.6 66.3 68.4 59.7 55.7 57.6 66.3 64.1 65.2 38.3 42.2 40.2 69.2 68.5 68.9 54.3
Klenner 64.4 60.3 62.3 49.0 50.7 49.9 61.7 68.6 65.0 41.3 39.7 40.5 66.1 73.9 69.1 51.8
Irwin 24.6 62.3 35.3 18.6 51.0 27.2 39.0 85.6 53.6 43.3 19.4 26.8 51.6 52.9 51.8 35.8
Closed Track - gold boundaries
This paper 79.5 71.3 75.2 65.9 62.1 63.9 69.5 70.6 70.0 46.3 50.5 48.3 72.0 78.6 74.8 60.7
Nugues 74.2 70.7 72.4 64.3 60.1 62.1 68.3 65.2 66.7 39.9 44.2 41.9 72.5 71.0 71.8 56.9
Chang 63.4 73.2 67.9 55.0 65.5 59.8 62.2 76.7 68.7 46.8 37.2 41.4 71.0 79.3 74.3 56.6
Santos 65.8 69.9 67.8 57.8 61.4 59.5 64.5 70.3 67.3 41.4 38.2 39.7 72.7 72.0 72.3 55.5
Kobdani 67.1 65.1 66.1 62.6 56.8 59.6 73.2 62.2 67.3 32.9 37.3 34.9 64.1 64.1 64.1 53.9
Stoyanov 76.9 64.7 70.3 69.8 55.0 61.5 77.1 52.5 62.5 31.0 44.8 36.6 76.6 60.3 63.0 53.6
Zhang 59.6 71.2 64.9 46.1 58.8 51.6 53.9 73.4 62.2 43.5 32.1 37.0 64.1 70.5 66.5 50.3
Song 58.4 77.6 66.7 46.7 68.4 55.5 54.4 70.2 61.3 43.8 25.9 32.5 66.3 58.8 60.2 49.8
Zhekova 69.2 57.3 62.7 33.5 37.2 35.2 55.5 68.2 61.2 38.3 34.7 36.4 53.5 63.3 54.8 44.3
</table>
<bodyText confidence="0.999786142857143">
resources. For the closed track, the organizers provided dictionaries for gender and
number information, in addition to parse trees and named entity labels (Pradhan et al.
2011). For the open track, we used the following additional resources: (a) a hand-built
list of genders of first names that we created, incorporating frequent names from census
lists and other sources (Vogel and Jurafsky 2012) (b) an animacy list (Ji and Lin 2009), (c)
a country and state gazetteer, and (d) a demonym list. These resources were also used
for the results reported in Table 5.
A significant difference between Tables 4 and 5 is that in the former (other than
its last block) we used predicted mentions (detected with the algorithm described in
Section 3.1), whereas in the latter we used gold mentions. The only reason for this
distinction is to facilitate comparison with previous work (all systems listed in Table 5
used gold mention boundaries).
The two tables show that, regardless of evaluation corpus and methodology, our
system generally outperforms the previous state of the art. In the CoNLL shared task,
</bodyText>
<page confidence="0.976999">
900
</page>
<bodyText confidence="0.986636041666667">
Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules
our system scores 1.8 CoNLL F1 points higher than the next system in the closed track
and 2.6 points higher than the second-ranked system in the open track. The Chang
et al. (2011) system has marginally higher B3 and BLANC F1 scores, but does not
outperform our model on the other two metrics and the average F1 score. Table 5
shows that our model has higher B3 F1 scores than all the other models in the two
ACE corpora. The model of Haghighi and Klein (2009) minimally outperforms ours by
0.6 B3 F1 points in the MUC corpus. All in all, these results prove that our approach
compares favorably with a wide range of models, which include most aspects deemed
important for coreference resolution, among other things, supervised learning using
rich feature sets (Sapena, Padr´o, and Turmo 2011; Chang et al. 2011), joint inference
using spectral clustering (Cai, Mujdricza-Maydt, and Strube 2011), and deterministic
rule-based models (Haghighi and Klein 2009). We discuss in more detail the similarities
and differences between our approach and previous work in Section 6.
Table 4 shows that using additional resources yields minimal improvement: There
is a difference of only 0.5 CoNLL F1 points between our open-track and closed-track
systems. We show in Section 5 that the explanation of this modest improvement is that
most of the remaining errors require complex, context-sensitive semantics to be solved.
Such semantic models cannot be built with our shallow feature set that relies on simple
semantic dictionaries (e.g., animacy or even hyponymy).
It is not trivial to compare the mention detection system alone because its score is
affected by the performance of the coreference resolution model. For example, even if
we start with a perfect set of gold mentions, if we miss all coreference relations in a text,
every mention will remain as a singleton and will be removed by the OntoNotes post
</bodyText>
<tableCaption confidence="0.761787">
Table 5
Comparison of our system with the other reported results on the ACE and MUC corpora. All
these systems use gold mention boundaries.
</tableCaption>
<table confidence="0.948231789473684">
System MUC B3
R P F1 R P F1
ACE2004-Culotta-Test
This paper 70.2 82.7 75.9 74.5 88.7 81.0
Haghighi and Klein (2009) 77.7 74.8 79.6 78.5 79.6 79.0
Culotta et al. (2007) – – – 73.2 86.7 79.3
Bengston and Roth (2008) 69.9 82.7 75.8 74.5 88.3 80.8
ACE2004-nwire
This paper 75.1 84.6 79.6 74.1 87.3 80.2
Haghighi and Klein (2009) 75.9 77.0 76.5 74.5 79.4 76.9
Poon and Domingos (2008) 70.5 71.3 70.9 – – –
Finkel and Manning (2008) 58.5 78.7 67.1 65.2 86.8 74.5
MUC6-Test
This paper 69.1 90.6 78.4 63.1 90.6 74.4
Haghighi and Klein (2009) 77.3 87.2 81.9 67.3 84.7 75.0
Poon and Domingos (2008) 75.8 83.0 79.2 – – –
Finkel and Manning (2008) 55.1 89.7 68.3 49.7 90.9 64.3
901
Computational Linguistics Volume 39, Number 4
</table>
<bodyText confidence="0.994621333333333">
processing, resulting in zero mentions in the final output. Therefore, we included the
score using gold mention boundaries in the last part of Table 4 (“Closed Track – gold
boundaries”) to isolate the performance of the coreference resolution component. This
experiment shows that our system outperforms the others with a considerable margin,
demonstrating that our coreference resolution model, rather than the mention detection
component, is the one responsible for the overall performance.
</bodyText>
<subsectionHeader confidence="0.996237">
4.4 Analysis
</subsectionHeader>
<bodyText confidence="0.999072653846154">
In this section, we present a series of analytic and ablative experiments that demonstrate
that both aspects of our algorithm (the entity-centric approach and the multi-pass
model with precision-ordered sieves) independently offer significant improvements
to coreference. We also analyze the contribution of each proposed sieve and of the
features deployed in our model. We conclude with an experiment that measures the
performance drop as we move from an oracle system that uses gold information for
mention boundaries, syntactic analysis, and named entity labels, to the actual system
where all this information is predicted. For all the experiments reported here we used
the OntoNotes-Dev corpus.
4.4.1 Contribution of the Entity-Centric Model. Table 6 shows the impact of our entity-
centric approach, which enables the sharing of features between mentions assigned to
the same cluster (detailed in Section 3.2.3). As a baseline, we use a typical mention-
pair model where this sharing is disabled. That is, when two mentions are com-
pared, this model uses only the features that were extracted from the corresponding
textual extents. The table shows that feature sharing has a considerable impact on
all evaluation metrics, with an overall contribution of approximately 3.4 CoNLL F1
points. This is further proof that an entity-centric approach is beneficial for coreference
resolution.
As an illustration, the following text shows an example where the incorrect decision
is taken if feature sharing is disabled:
This was the best result of a Chinese gymnast in 4 days of competition.... It was the best result
for Greek gymnasts since they began taking part in gymnastic internationals.
In the example text, the mention-pair model incorrectly links This and It, because all the
features that can be extracted locally are compatible (e.g., number is singular for both
pronouns). On the other hand, the entity-centric model avoids this decision because,
in a previous sieve driven by predicate nominative relations, these pronouns are each
</bodyText>
<tableCaption confidence="0.843927">
Table 6
</tableCaption>
<bodyText confidence="0.659080333333333">
Comparison of our entity-centric model against a baseline that handles mention pairs
independently. The former model shares mention features across entities as they are constructed.
The latter model does not.
</bodyText>
<table confidence="0.840874">
MUC B3 CEAF-φ4 BLANC CoNLL
R P F1 R P F1 R P F1 R P F1 F1
Entity-centric 60.0 60.9 60.3 68.6 73.3 70.9 47.5 46.2 46.9 73.5 79.3 76.0 59.3
Mention-pair 61.4 51.1 55.8 73.2 64.3 68.5 39.1 48.8 43.4 74.6 74.1 74.3 55.9
902
Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules
</table>
<tableCaption confidence="0.715277">
Table 7
Impact of the multi-pass model. The single-pass baseline uses the same sequence of sieves as the
multi-pass model (i.e., all the sieves introduced in Section 3 with the exception of the optional
ones) but it applies all of them at the same time.
</tableCaption>
<table confidence="0.99990175">
MUC B3 CEAF-4&apos;4 BLANC CoNLL
R P F1 R P F1 R P F1 R P F1 F1
Multi-pass 59.6 60.9 60.3 68.6 73.3 70.9 47.5 46.2 46.9 73.5 79.3 76.0 59.3
Single-pass 44.7 63.1 52.3 55.1 80.1 65.3 51.2 34.8 41.5 64.2 78.4 68.5 53.0
</table>
<tableCaption confidence="0.2870495">
linked to incompatible noun phrases, i.e., the best result of a Chinese gymnast and the best
result for Greek gymnasts.
</tableCaption>
<bodyText confidence="0.9500555">
4.4.2 Impact of the Multi-Pass Model. Table 7 shows the contribution of our multi-pass
model. We compare this model with a single-pass baseline, which uses the same sieves
as the multi-pass system but applies all of them at the same time. That is, for each
mention under consideration, we select the first antecedent that matches any of the
available sieves. This experiment shows that our multi-pass model, which sorts and
deploys sieves using precision-based ordering, yields improvements across the board,
with more than 6 CoNLL F1 points overall improvement.
This multi-pass model goes hand-in-hand with the entity-centric approach. That is,
the higher the quality of mention clusters built in the previous sieves, the better the
features extracted from these clusters will be in the current sieve—and, of course, better
features drive better clustering decisions in the next sieve, and so on. This incremental
process is highlighted in the given example: Because the sieve based on predicate nomi-
native patterns runs before pronominal coreference resolution, the two pronouns under
consideration have additional, high-quality features that stops the incorrect clustering
decision.
4.4.3 Contribution of Individual Sieves. Table 8 lists the performance of our system as ten
sieves are incrementally added. This table illustrates our tuning process, which allowed
us to deploy the sieves in descending order of their precision. With respect to individual
</bodyText>
<tableCaption confidence="0.996653">
Table 8
</tableCaption>
<table confidence="0.994974153846154">
Cumulative performance as sieves are added to the system.
MUC B3 CEAF-4&apos;4 BLANC CoNLL
R P F1 R P F1 R P F1 R P F1 F1
Sieve 1 8.7 72.7 15.5 32.4 96.4 48.5 50.6 15.4 23.7 57.2 80.3 60.2 29.2
+ Sieve 2 29.5 71.8 41.9 46.4 90.4 61.4 51.8 23.8 32.6 63.0 82.2 67.8 45.3
+ Sieve 3 29.7 71.2 41.9 46.7 90.1 61.5 51.6 24.0 32.7 63.0 82.0 67.8 45.4
+ Sieve 4 30.2 71.0 42.3 47.1 89.9 61.8 51.5 24.1 32.9 63.2 81.7 68.0 45.7
+ Sieve 5 34.4 66.1 45.2 51.5 86.6 64.6 50.8 27.6 35.8 64.1 80.8 68.8 48.5
+ Sieve 6 34.9 65.8 45.6 51.9 86.1 64.8 50.4 27.8 35.9 64.2 80.6 68.9 48.8
+ Sieve 7 35.8 64.0 45.9 53.3 85.0 65.5 49.8 28.9 36.6 64.4 80.3 69.1 49.3
+ Sieve 8 36.2 63.5 46.1 53.7 84.5 65.7 49.4 29.1 36.6 64.6 79.9 69.2 49.5
+ Sieve 9 36.7 63.2 46.5 54.2 84.0 65.9 49.2 29.4 36.8 64.7 79.5 69.2 49.7
+ Sieve 10 59.6 60.9 60.3 68.6 73.3 70.9 47.5 46.2 46.9 73.5 79.3 76.0 59.3
</table>
<page confidence="0.674524">
903
</page>
<note confidence="0.328575">
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.998931138888889">
contributions, this analysis highlights three significant performance increases. The first
is caused by Sieve 2, exact string match. This sieve accounts for approximately 16
CoNLL F1 points improvement, which proves that a significant percentage of mentions
in text are indeed repetitions of previously seen concepts. The second big jump in
performance, almost 3 CoNLL F1 points, is caused by Sieve 5, strict head match, which
is the first pass that compares individual headwords. These results are consistent with
error analyses from earlier work which have shown the importance of string match in
general (Zhou and Su 2004; Bengtson and Roth 2008; and Recasens, Can, and Jurafsky
2013) and the high precision of strict head match (Recasens and Hovy 2010).
Lastly, pronominal coreference resolution (Sieve 10) is responsible for approxi-
mately 9.5 CoNLL F1 points improvement. Thus it would be possible to build an even
simpler system, with just three sieves, that achieves 97% of the performance of our
best model (based on the CoNLL score). This suggests that what is most important
for coreference resolution, at least relative to today’s state of the art, is not necessarily
the clustering decision mechanism, but rather the entire architecture behind it, and in
particular the use of cautious decision-making based on high precision information,
entity-centric modeling, and so forth.
4.4.4 Contribution of Feature Groups. Table 9 lists the results of an ablative experiment
where each feature group was individually removed from the complete model. When a
feature is eliminated, two mentions under consideration are always considered compat-
ible with respect to that feature. For example, singular and plural mentions are number
compatible when the number feature is removed.
As the table shows, the most significant feature in our model is the number feature.
This feature alone is responsible for 2.6 CoNLL F1 points. Removing this feature has a
considerable negative impact on the pronoun resolution sieve, which makes a consid-
erable number of errors without it (e.g., linking our and Jiaju Hou). The second most
relevant feature is animacy, with an overall contribution of 1 CoNLL F1 point. Animacy
helps disambiguate clustering decisions where the two mentions under consideration
are otherwise number and gender compatible. For example, animacy enables the linking
of firms from Taiwan and they, and avoids the linking of 17 year and she. Lastly, the NE
and gender features contribute 0.5 and 0.4 F1 points, respectively. This relatively minor
contribution is caused by the overlap with the other features (e.g., many errors corrected
by using NE information are corrected also by a combination of animacy and number).
Nevertheless, these features are still useful. For example, the NE feature covers many
mentions that do not exist in our animacy dictionaries, which helps in several decisions,
e.g., avoiding linking it and Saddam Hussein.
</bodyText>
<tableCaption confidence="0.913443">
Table 9
</tableCaption>
<bodyText confidence="0.506064">
Contribution of each feature group. This is an ablative experiment, that is, each feature group
is analyzed by removing it from the complete system listed in the first row.
</bodyText>
<table confidence="0.999551285714286">
MUC B3 CEAF-φ4 BLANC CoNLL
R P F1 R P F1 R P F1 R P F1 F1
Complete system 59.6 60.9 60.3 68.6 73.3 70.9 47.5 46.2 46.9 73.5 79.3 76.0 59.3
− Number 57.0 56.4 56.7 66.2 68.6 67.4 45.6 46.2 45.9 67.6 72.6 69.7 56.7
− Gender 59.3 60.2 59.7 68.2 72.3 70.2 47.2 46.3 46.7 72.6 77.8 74.9 58.9
− Animacy 58.2 58.6 58.4 67.8 71.6 69.6 47.1 46.8 47.0 71.6 77.3 74.0 58.3
− NE 58.5 60.4 59.5 67.5 73.3 70.3 47.6 45.7 46.6 72.3 78.8 75.1 58.8
</table>
<page confidence="0.921778">
904
</page>
<note confidence="0.674646">
Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules
</note>
<tableCaption confidence="0.991569">
Table 10
</tableCaption>
<bodyText confidence="0.514783333333333">
The relevance of gold information. The “no gold” system is our final system used in the formal
evaluation. The system with “gold annotations” uses gold part-of-speech tags, syntactic analysis,
and named entity labels.
</bodyText>
<table confidence="0.999556285714286">
MUC B3 CEAF-φ4 BLANC CoNLL
R P F1 R P F1 R P F1 R P F1 F1
No gold 59.6 60.9 60.3 68.6 73.3 70.9 47.5 46.2 46.9 73.5 79.3 76.0 59.3
Gold NE 60.3 61.1 60.7 69.0 73.3 71.1 47.5 46.7 47.1 74.0 79.5 76.4 59.6
Gold syntax 62.3 62.5 62.4 69.9 73.5 71.7 47.8 47.6 47.7 74.8 80.0 77.1 60.6
Gold annotations 62.8 62.6 62.7 70.3 73.5 71.9 47.9 48.1 48.0 75.1 80.1 77.4 60.9
Gold mentions 73.0 90.3 80.7 69.1 89.5 78.0 79.2 51.4 62.4 78.8 89.4 83.1 73.7
</table>
<bodyText confidence="0.967444551724138">
4.4.5 Gold versus Predicted Information. We conclude this section with an analysis of
the performance penalty suffered when using predicted information as input in our
system (a realistic scenario) versus using gold information. We consider both linguistic
information (i.e., part of speech tags, named entity labels, and syntax) and mention
boundaries. Table 10 shows the results when various inputs were replaced with gold
information.
The table shows that, out of the linguistic resources, syntax is the most important.
This is to be expected, because we use a constituent parser for mention identification,
mention traversal, and for some of the sieves (e.g., the precise constructs model). All in
all, if all linguistic information is replaced with gold annotations, the performance of the
system increases by 1.6 CoNLL F1 points, or 2.7% relative improvement. We consider
this relatively small difference a success story for the quality of natural language pro-
cessors, especially considering our heavy reliance on such tools throughout the entire
system. On the other hand, the difference between our actual system and the oracle
system with gold mentions is 14.4 F1 points. This is because the gold mentions include
the anaphoricity information, detection of which is already a hard task by itself.
4.4.6 Automatic Ordering. The ordering of our sieves was determined using linguistic
intuition about how precise each sieve is (for example exact match is clearly more
precise than partial match). We also supplemented this intuition, early on in our design
process, by measuring the actual precision of some of the sieves on a development set
from ACE.
But because this development set, not to mention our intuition, may not match the
circumstances in the OntoNotes corpus, we performed a study to see if an automatically
learned ordering for sieves could result in superior performance.
We used greedy search to find an ordering, choosing the best precision sieve at each
pass. We tuned the ordering on OntoNotes-Train data, and evaluated this comparison
on the OntoNotes-Dev set.
Our optimization resulted in 0.1 CoNLL F1 improvement, and gave a very similar
ordering to our hand-built order:
</bodyText>
<subsectionHeader confidence="0.273469">
Hand Ordered:
</subsectionHeader>
<footnote confidence="0.5648784">
Speaker Match, String Match, Relaxed String Match, Precise Constructs, Strict Head MatchA-C,
Proper Head Noun Match, Relaxed Head Match, Pronoun Match
Learned Ordering:
String Match, Relaxed String Match, Speaker Match, Proper Head Noun Match, Strict Head
MatchA-C, Relaxed Head Match, Pronoun Match, Precise Constructs
</footnote>
<page confidence="0.993212">
905
</page>
<note confidence="0.490996">
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.999617">
The main change is that the learned ordering downplays the importance of the
precise constructs sieves, which is easily explained by the fact that OntoNotes does not
annotate appositive or predicate nominative relations.
This experiment confirms that hand ordering sieves by linguistic intuition of how
precise they are does remarkably well at choosing an ordering, despite the fact that the
ordering was originally designed for ACE, a completely different corpus.
</bodyText>
<sectionHeader confidence="0.997441" genericHeader="method">
5. Error Analysis
</sectionHeader>
<bodyText confidence="0.999900481481481">
To understand the errors in the system, we analyzed and categorized them into five
distinct groups. The distribution of the errors is given in Table 11, with specific examples
for each category given in Table 12. For this analysis, we inspected 115 precision and
recall errors.
Semantics, discourse. Whereas simple examples can be solved by using shallow semantics
such as knowledge about the semantic compatibility of headwords (e.g., McCain –
senator), most of the errors in this class require context-dependent semantics or dis-
course. For example, to know that the thrift and his property are coreferent, we need
to understand the context and that both the thrift and his property are being seized,
involving relations not only between the coreferent words, but also between other parts
of the sentence as well.
Pronominal resolution errors. Our pronominal resolution algorithm includes several
strong heuristics that model the matching of attributes (e.g., gender, number, animacy),
the position of mentions in discourse (e.g., we model only the first mention in text for
a given entity), or the distance between pronouns and antecedents. This is still far from
language understanding, however. Table 12 shows that our approach often generates
incorrect links when it finds other compatible antecedents that appear closer, according
to our antecedent ordering, to the pronoun under consideration. In the example shown
in the table, the land is selected as the antecedent for the pronoun its, because the land
appears earlier than the correct antecedent, the ANC, in the sentence. Implementing a
richer model of pronominal anaphora using syntactic and discourse information is an
important next step.
Non-referential mentions. The third significant cause of errors is due to non-referential
mentions such as pleonastic it or generic mentions. Our mention detection model
removes some of these non-referential mentions, but there are still many left, which
generate precision errors. For example, in Table 12, the pronoun you is generic, but our
system incorrectly links them. The large number of these errors suggests the need to
</bodyText>
<tableCaption confidence="0.995154">
Table 11
</tableCaption>
<table confidence="0.792646142857143">
Distribution of errors.
Error type Percentage
Semantics, discourse 41.7
Pronominal resolution errors 28.7
Non-referential mentions 14.8
Event mentions 6.1
Miscellaneous 8.7
</table>
<page confidence="0.975879">
906
</page>
<note confidence="0.718577">
Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules
</note>
<tableCaption confidence="0.988695">
Table 12
</tableCaption>
<bodyText confidence="0.874237333333333">
Examples of errors in each class. The mention to be resolved is in boldface, its correct antecedent
is in italics, and we underlined the incorrect antecedent from our system result.
Error type Example
Semantics, discourse • Lincoln’s parent company, American Continental Corp.,
entered bankruptcy - law proceedings this April 13, and
regulators seized the thrift the next day.... Mr. Keating has
filed his own suit, alleging that his property was taken
illegally.
• New pictures reveal the sheer power of that terrorist bomb ...
In these photos obtained by NBC News, the damage much
larger than first imagined ...
• Of all the one-time expenses incurred by a corporation
or professional firm, few are larger or longer term than the
purchase of real estate or the signing of a commercial lease ...
To take full advantage of the financial opportunities in
this commitment, .. .
Under the laws of the land, the ANC remains an illegal
Pronominal resolution errors organization, and its headquarters are still in Lusaka,
Zambia.
Non-referential mentions When you become a federal judge, all of a sudden you are
relegated to a paltry sum.
“Support the troops, not the regime” That ’s a noble idea
Event mentions until you’re supporting the weight of an armoured vehicle
on your chest.
Miscellaneous (inconsistent • Inconsistent annotation - Inclusion of ’s:... that’s without
annotations, parser or NER adding in [Business Week ’s] charge ... Small wonder that
errors, enumerations) [Britain] ’s Labor Party wants credit controls.
• Parser or NER error: Um alright uh Mister Zalisko do you
know anything from your personal experience of having
been on the cruise as to what happened? – Mister Zalisko is
not recognized as a PERSON
• Enumerations: This year, the economies of the five large
special economic zones, namely, Shenzhen, Zhuhai, Shantou,
Xiamen and Hainan, have maintained strong growth
momentum.... A three dimensional traffic frame in Zhuhai
has preliminarily taken shape and the investment
environment improves daily.
add more sophisticated anaphoricity detection to our system (Vieira and Poesio 2000;
Ng and Cardie 2002a; Poesio et al. 2004b; Boyd, Gegg-Harrison, and Byron 2005; Gupta,
Purver, and Jurafsky 2007; Bergsma, Lin, and Goebel 2008; Ng 2009).
Event mentions. Our system was tailored for the resolution of entity coreference and
does not have any event-specific features, such as, for example, matching event partici-
pants. Furthermore, our model considers only noun phrases as antecedent candidates,
thus missing all mentions that are verbal phrases. Therefore, our system misses most
coreference links between event mentions. For example, in Table 12 the pronoun That
</bodyText>
<page confidence="0.985672">
907
</page>
<note confidence="0.487506">
Computational Linguistics Volume 39, Number 4
</note>
<bodyText confidence="0.999863107142857">
is coreferent with the event mention Support. Our system fails to detect the latter event
mention and, as a consequence, incorrectly links That to the regime.
Miscellaneous. There are several other reasons for errors, including inconsistent annota-
tions, parse or NER errors, and incorrect processing of enumerations. For example, the
possessive (’s) is annotated inconsistently in several cases: sometimes it is included in
the possessor mention in the gold mention annotation, but sometimes it is not. This will
penalize the final score twice (once for recall due to the missed mention and once for
precision due to the incorrectly detected mention).
Another considerable source of errors is caused by incorrect NER labels or parse
trees. NER errors can result in incorrect pronoun resolution due to incorrect attributes.
Parser errors are responsible for many additional coreference resolution errors. First, in-
correct syntactic attachments lead to incorrect mention boundaries, which are penalized
by our strict scorer. Second, parser errors often lead to the selection of an incorrect head
word for a given constituent, which influences many of our sieves. Thirdly, because our
parser does not always distinguish between coordinated nominal phrases and appo-
sitions, our system sometimes takes an entire coordinated phrase as a single mention,
leading to a series of mention errors. For example, the last example in the table shows
a compounded syntactic error: first, the parser failed to identify the entire construct
(Shenzhen, Zhuhai, Shantou, Xiamen, and Hainan) as a single enumeration. Second, our
system believed that Zhuhai, Shantou, Xiamen is an appositive phrase and kept it as a
single mention, rather than separate it into three distinct mentions.
Lastly, our processing of enumerations needs to be improved. Because we prefer to
assign content words as head words of syntactic constituents, we take the head word of
the first noun phrase in the enumeration to be the head word of the coordinated nominal
phrase (Kuebler, McDonald, and Nivre 2009; de Marneffe and Manning 2008). Because
of this, the coordinated phrase is often linked to another mention of the first element in
the enumeration. For example, our system marks Zhuhai, Shantou, Xiamen as a unique
mention and incorrectly links it to Zhuhai, because they have the same headword.
</bodyText>
<sectionHeader confidence="0.86516" genericHeader="method">
6. Comparison with Previous Work
</sectionHeader>
<bodyText confidence="0.999746888888889">
Algorithms for coreference (or just pronominal anaphora) include rule-based systems
(Hobbs 1978; Brennan, Friedman, and Pollard 1987; Lappin and Leass 1994; Baldwin
1995; Zhou and Su 2004; Haghighi and Klein 2009, inter alia), supervised systems
(Connolly, Burger, and Day 1994; McCarthy and Lehnert 1995; Kehler 1997; Soon, Ng,
and Lim 2001; Ng and Cardie 2002b; Rahman and Ng 2009, inter alia), and unsupervised
approaches (Cardie and Wagstaff 1999; Haghighi and Klein 2007; Ng 2008; Kobdani
et al. 2011a). Our deterministic system draws from all of these, but specifically from
three strands in the literature that cross-cut this classification.
The idea of doing accurate reference resolution by starting with a set of very high-
precision constraints was first proposed for pronominal anaphora in Baldwin’s (1995)
important but undercited dissertation. Baldwin suggested using seven high-precision
rules as filters, combining them so as to achieve reasonable recall. One of his rules,
for example, resolved pronouns whose antecedents were unique in the discourse, and
another resolved pronouns in quoted speech. Baldwin’s idea of starting with high-
precision knowledge was adopted by later researchers, such as Ng and Cardie (2002b),
who trained to the highest-confidence rather than nearest antecedent, or Haghighi and
Klein (2009), who began with syntactic constraints (which tend to be higher-precision)
before applying semantic constraints. This general idea is known by different names in
</bodyText>
<page confidence="0.995404">
908
</page>
<bodyText confidence="0.994047705882353">
Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules
many NLP applications: Brown et al. (1993) used simple models as “stepping stones”
for more complex word alignment models; Collins and Singer (1999) used “cautious”
decision list learning for named entity classification; Borghesi and Favareto (1982) and
Corazza et al. (1991) used “islands of reliability” approaches to parsing and speech
recognition, and Spitkovsky et al. (2010) used “baby steps” for unsupervised depen-
dency parsing, and so forth. Our work extends the intuition of Baldwin and others
to the full coreference task (i.e., including mention detection and both nominal and
pronominal coreference) and shows that it can result in extremely high-performing
resolution when combined with global inference.
Our second inspiration comes from two works: Zhou and Su (2004) and Haghighi
and Klein (2009), both of which extended Baldwin’s approach to generic nominal coref-
erence. Zhou and Su proposed a multi-agent model that triggers a different agent with
a specific set of deterministic constraints for each anaphor depending on its type and
context (e.g., there are different constraints for noun phrases in appositive constructs,
definite noun phrases, or bare noun phrases). Some of the constraints’ parameters (e.g.,
size of candidate search space for a given anaphor type) are learned from training data.
The authors showed that this model outperforms the state of the art on the MUC-6 and
MUC-7 domains. To our knowledge, Zhou and Su’s approach is the first work to demon-
strate that a deterministic approach obtains state-of-the-art results for both nominal and
pronominal coreference resolution. Our approach extends Zhou and Su’s model in two
significant ways. First, Zhou and Su solve the coreference task in a single pass over the
text. We show that a multi-pass approach, which applies a series of sieves incrementally
from highest to lowest precision, performs considerably better (see Table 7). Second,
Zhou and Su’s model follows a mention-pair approach, where coreference decisions are
taken based only on information extracted from the two mentions under consideration.
We demonstrate that an entity-centric approach, which allows features to be shared
between mentions of the same entity, outperforms the mention-pair model (see Table 6).
Haghighi and Klein’s (2009) two-pass system based on deterministic rules further
proved that deterministic rules could achieve state-of-the-art performance. Haghighi
and Klein’s first, purely syntactic pass, uses high-precision syntactic information to
assign possible coreference. The second, transductive pass identifies Wikipedia arti-
cles relevant to the entity mentions in the test set, and then bootstraps a database of
hyponyms and other semantically related head pairs from known syntactic patterns for
apposition and predicate-nominatives. Haghighi and Klein found that this transductive
learning was essential for semantic knowledge to be useful (Aria Haghighi, personal
communication); other researchers have found that semantic knowledge derived from
Web resources can be quite noisy (Uryupina et al. 2011a). But although transductive
learning (learning using test set mentions) thus offers advantages in precision, running
a Web-based bootstrapping learner whenever a new data set is encountered is not
practical and, ultimately, reduces the usability of this NLP component. Our system
thus offers the deterministic simplicity and high performance of the Haghighi and
Klein (2009) system without the need for gold mention labels or test-time learning.
Furthermore, our work extends the multi-pass model to ten passes and shows that this
approach can be naturally combined with an entity-centric model for better results.
Finally, recent work has shown the importance of performing coreference resolution
jointly for all mentions in a document (McCallum and Wellner 2004; Daum´e III and
Marcu 2005; Denis and Baldridge 2007; Haghighi and Klein 2007; Culotta et al. 2007;
Poon and Domingos 2008; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube
2011) rather than the classic method of simply aggregrating local decisions about pairs
of mentions. Like these systems, our model adopts the entity-mention model (Morton
</bodyText>
<page confidence="0.993266">
909
</page>
<note confidence="0.601724">
Computational Linguistics Volume 39, Number 4
2000; Luo et al. 2004; Yang et al. 2008; Ng 2010)8 in which features can be extracted
</note>
<bodyText confidence="0.998031">
over not just pairs of mentions but over entire clusters of mentions defining an entity.
Previous systems do this by encoding constraints using rich probabilistic models and
complex global inference algorithms. By contrast, global reasoning is implemented in
our system just by allowing the rules in each stage to reason about any features of a
cluster from a previous stage, including attributes like gender and number as well as
headword information derived from the first (most informative) mention. Because our
system begins with high-precision clusters, accurate information naturally propagates
to later stages.
</bodyText>
<listItem confidence="0.530561">
7. Other Systems Incorporating this Algorithm
</listItem>
<bodyText confidence="0.999925032258065">
A number of recent systems have incorporated our algorithm as an important com-
ponent in resolving coreference. For example, the CoNLL-2012 shared task focused on
coreference resolution in a multi-lingual setting: English, Chinese, and Arabic (Pradhan
et al. 2012). Forty percent of the systems in the shared task (6 of the 15 systems) made use
of our sieve architecture (Chen and Ng 2012; Fernandes, dos Santos, and Milidiu 2012;
Shou and Zhao 2012; Xiong and Liu 2012; Yuan et al. 2012; Zhang, Wu, and Zhao 2012),
including the systems that were the highest scoring for each of the three languages
(Fernandes, dos Santos, and Milidiu 2012; Chen and Ng 2012).
The system of Fernandes, dos Santos, and Milidiu (2012) had the highest average
score over all languages, and the best score for English and Arabic, by implement-
ing a stacking of two models. Our sieve-based approach was first used to generate
mention-link candidates, which are then reranked by a supervised model inspired from
dependency parsing. This result demonstrates that our deterministic approach can be
naturally combined with more-complex supervised models for further performance
gains.
The system of Chen and Ng (2012) performed the best for Chinese by making the
observation that most sieves in our model are minimally lexicalized so they can be
easily adapted to other languages. Their coreference model for Chinese incorporated
our English sieves with only four modifications, only two of which were related to the
differences between Chinese and English: The precise constructs sieve was extended
to add patterns for Chinese name abbreviations, and the relaxed head-match sieve
was removed, because Chinese tends not to have post-nominal modifiers.9 Chen and
Ng (2012) then added a second component which first linked mentions with high
string-pair or head-pair probabilities before running the sieve architecture. The strong
performance of our English sieve system on Chinese with only this small number of
changes speaks to the multi-lingual strength of our approach.
The intuition of our system can be further extended to the task of event coreference
resolution. Our recent work (Lee et al. 2012) showed that an iterative method that
cautiously constructs clusters of entity and event mentions, using linear regression to
model cluster merge operations, allows information flow between entity and event
coreference.
</bodyText>
<footnote confidence="0.939248166666667">
8 In this article, we call this approach entity-centric to avoid confusion with individual mentions of entities.
9 Two changes were related to differences between the English and Chinese shared task in the supplied
annotations and data: The pronoun sieve was extended to determine gender for Chinese NPs, because
the gender gazeteer used for the shared task and for our system only provides gender for English, and a
new head-match sieve was added to deal with embedded heads, because the Chinese annotation marked
embedded heads differently than the English annotation.
</footnote>
<page confidence="0.984473">
910
</page>
<bodyText confidence="0.963662363636363">
Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules
A similar easy-first machine learning based approach to entity coreference by
Stoyanov and Eisner (2012) also adopts this intuition. Their system greedily merges
clusters with the highest score (the current easiest decision), using higher precision
classifications (‘easier decisions’) to guide harder decisions later.
In summary, recent systems have used the sieve architecture as a component in hy-
brid machine learning systems, either as a first pass in generating candidate links which
are then incorporated in a probabilistic system, or as a second pass for generating links
after high-probability mention-pairs have already been linked. These hybrid systems
are the state-of-the-art in English, Chinese, and Arabic coreference resolution. Further,
our algorithm can be extended to other tasks, for example, event coreference resolution.
</bodyText>
<sectionHeader confidence="0.710968" genericHeader="conclusions">
8. Conclusion
</sectionHeader>
<bodyText confidence="0.998985470588235">
We have presented a simple deterministic approach to coreference resolution that
incorporates document-level information, which is typically exploited only by more
complex, joint learning models. Our approach exploits document-level information
through an entity-centric model, which allows features to be shared across mentions
that point to the same real-world entity. The sieve architecture applies a battery of
deterministic coreference models one at a time from highest to lowest precision, where
each model builds on the previous model’s entity output. Despite its simplicity, our
approach outperforms or performs comparably to the state of the art on several corpora.
An additional benefit of the sieve framework is its modularity: New features or
models can be inserted in the system with limited understanding of the other features
already deployed. Our code is publicly released10 and can be used both as a stand-alone
coreference system and as a platform for the development of future systems.
The state-of-the-art performance of our system in coreference, either directly or as
a component in hybrid systems, and that of other recent rule-based systems in named
entity recognition (Chiticariu et al. 2010) suggests that rule-based systems are still an
important tool for modern natural language processing. Our results further suggest that
precision-ordered sieves may be an important way to structure rule based systems, and
suggests the use of sieves in other NLP tasks for which a variety of very high-precision
features can be designed and non-local features can be shared. Likely candidates include
relation and event extraction, template slot filling, and author name deduplication.
Our error analysis points to a number of places where our system could be im-
proved, including better performance on pronouns. More sophisticated anaphoricity
detection, drawing on the extensive literature in this area, could also help (Vieira and
Poesio 2000; Ng and Cardie 2002a; Poesio et al. 2004b; Boyd, Gegg-Harrison, and Byron
2005; Gupta, Purver, and Jurafsky 2007; Bergsma, Lin, and Goebel 2008; Ng 2009).
The main conclusion of our error analysis, however, is that the plurality of our errors
are due to shallow knowledge of semantics and discourse. This result points to the
crucial need for more sophisticated methods of incorporating semantic and discourse
knowledge. Unsupervised or semi-supervised approaches to semantics such as Yang
and Su (2007), Kobdani et al. (2011b), Uryupina et al. (2011b), Bansal and Klein (2012),
or Recasens, Can, and Jurafsky (2013) may point the way forward. Although sieve-based
architectures are at the modern state of the art, it is only by incorporating these more
powerful models of meaning that we can eventually deal with the full complexity and
richness of coreference.
</bodyText>
<footnote confidence="0.557618">
10 http://nlp.stanford.edu/software/dcoref.shtml.
</footnote>
<page confidence="0.983487">
911
</page>
<table confidence="0.991439181818182">
Computational Linguistics Volume 39, Number 4
Appendix A: The OntoNotes Named Entity Tag Set
PERSON People, including fictional
NORP Nationalities or religious or political groups
FACILITY Buildings, airports, highways, bridges, etc.
ORGANIZATION Companies, agencies, institutions, etc.
GPE Countries, cities, states
LOCATION Non-GPE locations, mountain ranges, bodies of water
PRODUCT Vehicles, weapons, foods, etc. (Not services)
EVENT Named hurricanes, battles, wars, sports events, etc.
WORK OF ART Titles of books, songs, etc.
LAW Named documents made into laws
LANGUAGE Any named language
DATE Absolute or relative dates or periods
TIME Times smaller than a day
PERCENT Percentage (including “%”)
MONEY Monetary values, including unit
QUANTITY Measurements, as of weight or distance
ORDINAL ”first”, “second”
CARDINAL Numerals that do not fall under another type
Appendix B: Set of Patterns for Detecting Pleonastic it
NP &lt; (PRP=m1) $.. (VP &lt; ((/^V. */ &lt; /^(?:is|was|become|became)/) $.. (VP &lt; (VBN $.. /S|SBAR/))))
NP &lt; (PRP=m1) $.. (VP &lt; ((/^V. */ &lt; /^(?:is|was|become|became)/) $.. (ADJP $.. (/S|SBAR/))))
NP &lt; (PRP=m1) $.. (VP &lt; ((/^V. */ &lt; /^(?:is|was|become|became)/) $.. (ADJP &lt; (/S|SBAR/))))
NP &lt; (PRP=m1) $.. (VP &lt; ((/^V. */ &lt; /^(?:is|was|become|became)/) $.. (NP &lt; /S|SBAR/)))
NP &lt; (PRP=m1) $.. (VP &lt; ((/^V. */ &lt; /^(?:is|was|become|became)/) $.. (NP $.. ADVP $.. /S|SBAR/)))
NP &lt; (PRP=m1) $.. (VP &lt; (MD $ .. (VP &lt; ((/^V.*/ &lt; /^(?:be|become)/) $.. (VP &lt; (VBN $.. /S|SBAR/))))))
NP &lt; (PRP=m1) $.. (VP &lt; (MD $ .. (VP &lt; ((/^V.*/ &lt; /^(?:be|become)/) $.. (ADJP $.. (/S|SBAR/))))))
NP &lt; (PRP=m1) $.. (VP &lt; (MD $ .. (VP &lt; ((/^V.*/ &lt; /^(?:be|become)/) $.. (ADJP &lt; (/S|SBAR/))))))
NP &lt; (PRP=m1) $.. (VP &lt; (MD $ .. (VP &lt; ((/^V.*/ &lt; /^(?:be|become)/) $.. (NP &lt; /S|SBAR/)))))
NP &lt; (PRP=m1) $.. (VP &lt; (MD $ .. (VP &lt; ((/^V.*/ &lt; /^(?:be|become)/) $.. (NP $.. ADVP $.. /S|SBAR/)))))
NP &lt; (PRP=m1) $.. (VP &lt; ((/^V. */ &lt; /^(?:seems|appears|means|follows)/) $.. /S|SBAR/))
NP &lt; (PRP=m1) $.. (VP &lt; ((/^V. */ &lt; /^(?:turns|turned)/) $.. PRT $.. /S|SBAR/)
</table>
<sectionHeader confidence="0.962233" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.788143470588235">
We gratefully acknowledge the support of
the Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program
under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. Any
opinions, findings, and conclusions or
recommendations expressed in this material
are those of the author(s) and do not
necessarily reflect the view of the DARPA,
AFRL, or the U.S. government. We gratefully
thank Aria Haghighi, Marta Recasens,
Karthik Raghunathan, and Chris Manning
for useful suggestions; Sameer Pradhan for
help with the CoNLL infrastructure; the
Stanford NLP Group for help throughout;
and the four anonymous reviewers for
extremely helpful feedback.
</bodyText>
<sectionHeader confidence="0.975461" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.903787823529412">
Bagga, Amit and Breck Baldwin. 1998.
Algorithms for scoring coreference chains.
In The First International Conference on
Language Resources and Evaluation Workshop
on Linguistics Coreference, volume 1,
pages 563–566, Granada.
Baldwin, Breck. 1995. CogNIAC: A Discourse
Processing Engine. University of
Pennsylvania Department of Computer
and Information Sciences. Ph.D. thesis.
Baldwin, Breck. 1997. Cogniac: High
precision coreference with limited
knowledge and linguistic resources.
In Proceedings of a Workshop on Operational
Factors in Practical, Robust Anaphora
Resolution for Unrestricted Texts,
pages 38–45, Madrid.
</reference>
<page confidence="0.989558">
912
</page>
<note confidence="0.575080666666667">
Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules
Bansal, Mohit and Dan Klein. 2012.
Coreference semantics from web
</note>
<reference confidence="0.995095586206896">
features. In Proceedings of ACL 2012,
pages 389–398, Jeju Island.
Bengtson, Eric and Dan Roth. 2008.
Understanding the value of features for
coreference resolution. In Proceedings
of EMNLP 2008, pages 294–303,
Honolulu, HI.
Bergsma, Shane and Dekang Lin. 2006.
Bootstrapping path-based pronoun
resolution. In Proceedings of COLING-ACL,
pages 33–40, Stroudsburg, PA.
Bergsma, Shane, Dekang Lin, and Randy
Goebel. 2008. Distributional identification
of non-referential pronouns. In Proceedings
of ACL-HLT 2008, pages 10–18,
Columbus, OH.
Borghesi, Luigi and Chiara Favareto.
1982. Flexible parsing of discretely
uttered sentences. In Proceedings of
the 9th Conference on Computational
Linguistics-Volume 1, pages 37–42, Prague.
Boyd, Adriane, Whitney Gegg-Harrison,
and Donna Byron. 2005. Identifying
non-referential it: A machine learning
approach incorporating linguistically
motivated features. In Proceedings of the
ACL Workshop on Feature Engineering for
Machine Learning in NLP, pages 40–47,
Ann Arbor, MI.
Brennan, Susan E., Marilyn W. Friedman,
and Carl Pollard. 1987. A centering
approach to pronouns. In Proceedings of
the 25th Annual Meeting on Association for
Computational Linguistics, pages 155–162,
Stanford, CA.
Brown, Peter F., Vincent J. Della Pietra,
Stephen A. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: parameter
estimation. Computational Linguistics,
19(2):263–311.
Cai, Jie, Eva Mujdricza-Maydt, and
Michael Strube. 2011. Unrestricted
coreference resolution via global
hypergraph partitioning. In Proceedings of
the Fifteenth Conference on Computational
Natural Language Learning: Shared Task,
pages 56–60, Portland, OR.
Cardie, Claire and Kiri Wagstaff. 1999.
Noun phrase coreference as clustering.
In Proceedings of the Joint SIGDAT
Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora,
pages 82–89, College Park, MD.
Chang, Kai-Wei, Rajhans Samdani,
Alla Rozovskaya, Nick Rizzolo, Mark
Sammons, and Dan Roth. 2011. Inference
protocols for coreference resolution.
In Proceedings of the Fifteenth Conference
on Computational Natural Language
Learning: Shared Task, pages 40–44,
Portland, OR.
Chen, Chen and Vincent Ng. 2012.
Combining the best of two worlds:
A hybrid approach to multilingual
coreference resolution. In Proceedings
of the CoNLL-2012 Shared Task,
pages 56–63, Jeju Island.
Chiticariu, Laura, Rajasekar Krishnamurthy,
Yunyao Li, Frederick Reiss, and
Shivakumar Vaithyanathan. 2010.
Domain adaptation of rule-based
annotators for named-entity recognition
tasks. In Proceedings of the 2010
Conference on Empirical Methods in Natural
Language Processing, pages 1,002–1,012,
Cambridge, MA.
Chomsky, Noam. 1981. Lectures on
Government and Binding. Mouton de
Gruyter, Berlin.
Collins, Michael and Yoram Singer. 1999.
Unsupervised models for named entity
classification. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods
in Natural Language Processing and
Very Large Corpora, pages 100–110,
College Park, MD.
Connolly, Dennis, John D. Burger, and
David S. Day. 1994. A machine learning
approach to anaphoric reference.
In Proceedings of the International Conference
on New Methods in Language Processing
(NeMLaP-1), pages 255–261, Manchester.
Corazza, A., R. De Mori, R. Gretter, and
G. Satta. 1991. Stochastic context-free
grammars for island-driven probabilistic
parsing. In Proceedings of Second
International Workshop on Parsing
Technologies (IWPT 91), pages 210–217,
Cancun.
Culotta, Aron, Michael Wick, Robert
Hall, and Andrew McCallum. 2007.
First-order probabilistic models for
coreference resolution. In Proceedings
of HLT-NAACL 2007, pages 81–88,
Rochester, NY.
Daum´e III, Hal and Daniel Marcu. 2005.
A large-scale exploration of effective
global features for a joint entity detection
and tracking model. In HLT-EMNLP 2005,
pages 97–104, Vancouver.
de Marneffe, Marie-Catherine and
Christopher D. Manning. 2008.
The Stanford typed dependencies
representation. In Proceedings of COLING
Workshop on Cross-framework and
</reference>
<page confidence="0.995753">
913
</page>
<figure confidence="0.3654755">
Computational Linguistics Volume 39, Number 4
Cross-domain Parser Evaluation,
</figure>
<reference confidence="0.993123623931623">
pages 1–8, Manchester.
Denis, Pascal and Jason Baldridge. 2007.
Joint determination of anaphoricity and
coreference resolution using integer
programming. In Proceedings of
NAACL-HLT 2007, pages 236–243,
Rochester, NY.
Denis, Pascal and Jason Baldridge. 2009.
Global joint models for coreference
resolution and named entity classification.
Procesamiento del Lenguaje Natural, 42:87–96.
Doddington, George, Alexis Mitchell,
Mark Przybocki, Lance Ramshaw,
Stephanie Strassel, and Ralph Weischedel.
2004. The Automatic Content Extraction
(ACE) program—Tasks, data, and
evaluation. In Proceedings of LREC 2004,
pages 837–840, Lisbon.
Elsner, Micha and Eugene Charniak. 2010.
The same-head heuristic for coreference.
In Proceedings of ACL 2010 Short Papers,
pages 33–37, Uppsala.
Fernandes, Eraldo, Cicero dos Santos,
and Ruy Milidiu. 2012. Latent structure
perceptron with feature induction for
unrestricted coreference resolution.
In Proceedings of the CoNLL-2012 Shared
Task, pages 41–48, Jeju Island.
Finkel, Jenny Rose, Trond Grenager, and
Christopher Manning. 2005. Incorporating
non-local information into information
extraction systems by Gibbs sampling.
In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics,
ACL ’05, pages 363–370, Stroudsburg, PA.
Finkel, Jenny Rose and Christopher D.
Manning. 2008. Enforcing transitivity in
coreference resolution. In Proceedings of
the 46th Annual Meeting of the Association
for Computational Linguistics on Human
Language Technologies: Short Papers,
pages 45–48, Columbus, OH.
Fox, Barbara A. 1993. Discourse Structure and
Anaphora: Written and Conversational
English. Cambridge University Press.
Greene, Barbara B. and Gerald M. Rubin.
1971. Automatic Grammatical Tagging of
English. Brown University Press.
Gupta, Surabhi, Matthew Purver, and
Dan Jurafsky. 2007. Disambiguating
between generic and referential “you” in
dialog. In Proceedings of the 45th Annual
Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pages 105–108,
Prague.
Haghighi, Aria and Dan Klein. 2007.
Unsupervised coreference resolution
in a nonparametric Bayesian model.
In Proceedings of ACL 2007, pages 848–855,
Prague.
Haghighi, Aria and Dan Klein. 2009. Simple
coreference resolution with rich syntactic
and semantic features. In Proceedings of
EMNLP 2009, pages 1,152–1,161, Suntec.
Haghighi, Aria and Dan Klein. 2010.
Coreference resolution in a modular,
entity-centered model. In Proceedings
of HLT-NAACL 2010, pages 385–393,
Los Angeles, CA.
Hobbs, Jerry R. 1978. Resolving pronoun
references. Lingua, 44(4):311–338.
Ji, Heng and Dekang Lin. 2009. Gender
and animacy knowledge discovery from
web-scale n-grams for unsupervised
person mention detection. In Proceedings
of the Pacific Asia Conference on Language,
Information and Computation,
pages 220–229, Hong Kong.
Kehler, Andrew. 1997. Probabilistic
coreference in information extraction.
In Proceedings of EMNLP 1997,
pages 163–173, Providence, RI.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing. In
Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics -
Volume 1, ACL ’03, pages 423–430,
Stroudsburg, PA.
Klein, Sheldon and Robert F. Simmons. 1963.
A computational approach to grammatical
coding of English words. Journal of the
Association for Computing Machinery,
10(3):334–347.
Kobdani, Hamidreza, Hinrich Schuetze,
Michael Schiehlen, and Hans Kamp. 2011a.
Bootstrapping coreference resolution using
word associations. In Proceedings of ACL
HLT 2011, pages 783–792, Portland, OR.
Kobdani, Hamidreza, Hinrich Sch¨utze,
Michael Schiehlen, and Hans Kamp. 2011b.
Bootstrapping coreference resolution using
word associations. In Proceedings of ACL,
pages 783–792, Portland, OR.
Kuebler, Sandra, Ryan McDonald, and
Joakim Nivre. 2009. Dependency Parsing.
Morgan and Claypool Publishers.
Lappin, Shalom and Herbert Leass. 1994.
An algorithm for pronominal anaphora
resolution. Computational Linguistics,
20(4):535–561.
Lee, Heeyoung, Yves Peirsman, Angel
Chang, Nathanael Chambers, Mihai
Surdeanu, and Dan Jurafsky. 2011.
Stanford’s multi-pass sieve coreference
resolution system at the CoNLL-2011
shared task. In Proceedings of CoNLL 2011:
Shared Task, pages 28–34, Portland, OR.
</reference>
<page confidence="0.992468">
914
</page>
<reference confidence="0.997582428571428">
Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules
Lee, Heeyoung, Marta Recasens,
Angel Chang, Mihai Surdeanu, and
Dan Jurafsky. 2012. Joint entity and event
coreference resolution across documents.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning
(EMNLP-CoNLL), pages 489–500,
Jeju Island.
Luo, Xiaoqiang. 2005. On coreference
resolution performance metrics.
In Proceedings of HLT-EMNLP 2005,
pages 25–32, Vancouver.
Luo, Xiaoqiang, Abe Ittycheriah, Hongyan
Jing, Nanda Kambhatla, and Salim
Roukos. 2004. A mention-synchronous
coreference resolution algorithm based
on the Bell tree. In Proceedings of ACL 2004,
pages 21–26, Barcelona.
McCallum, Andrew and Ben Wellner. 2004.
Conditional models of identity uncertainty
with application to noun coreference.
In Proceedings of NIPS 2004, pages 905–912,
Vancouver.
McCarthy, Joseph F. and Wendy G. Lehnert.
1995. Using decision trees for coreference
resolution. In Proceedings of IJCAI 1995,
pages 1,050–1,055, Montr´eal.
Morton, Thomas S. 2000. Coreference for
NLP applications. In Proceedings of
ACL 2000, pages 173–180, Hong Kong.
Ng, Vincent. 2008. Unsupervised models
for coreference resolution. In Proceedings
of EMNLP 2008, pages 640–649,
Honolulu, HI.
Ng, Vincent. 2009. Graph-cut-based
anaphoricity determination for
coreference resolution. In Proceedings
of NAACL-HLT 2009, pages 575–583,
Boulder, CO.
Ng, Vincent. 2010. Supervised noun phrase
coreference research: The first fifteen years.
In Proceedings of ACL, pages 1,396–1,411,
Uppsala.
Ng, Vincent and Claire Cardie. 2002a.
Identifying anaphoric and non-anaphoric
noun phrases to improve coreference
resolution. In Proceedings of COLING,
pages 1–7, Taipei.
Ng, Vincent and Claire Cardie. 2002b.
Improving machine learning approaches
to coreference resolution. In Proceedings
of ACL 2002, pages 104–111,
Philadelphia, PA.
Poesio, Massimo, Rahul Mehta, Axel
Maroudas, and Janet Hitzeman. 2004a.
Learning to resolve bridging references.
In Proceedings of ACL, pages 143–150,
Barcelona.
Poesio, Massimo, Olga Uryupina, Renata
Vieira, Mijail Alexandrov-Kabadjov, and
Rodrigo Goulart. 2004b. Discourse-new
detectors for definite description
resolution: A survey and a preliminary
proposal. In ACL 2004: Workshop on
Reference Resolution and its Applications,
pages 47–54, Barcelona.
Poon, Hoifung and Pedro Domingos.
2008. Joint unsupervised coreference
resolution with Markov logic.
In Proceedings of EMNLP 2008,
pages 650–659, Honolulu, HI.
Pradhan, Sameer, Alessandro Moschitti,
Nianwen Xue, Olga Uryupina, and
Yuchen Zhang. 2012. CoNLL-2012 Shared
Task: Modeling Multilingual Unrestricted
Coreference in OntoNotes. In Proceedings
of the Sixteenth Conference on Computational
Natural Language Learning (CoNLL),
page 1, Jeju Island.
Pradhan, Sameer, Lance Ramshaw, Mitchell
Marcus, Martha Palmer, Ralph Weischedel,
and Nianwen Xue. 2011. CoNLL-2011
shared task: Modeling unrestricted
coreference in OntoNotes. In Proceedings
of the Fifteenth Conference on Computational
Natural Language Learning (CoNLL),
pages 1–27, Portland, OR.
Raghunathan, Karthik, Heeyoung Lee,
Sudarshan Rangarajan, Nathanael
Chambers, Mihai Surdeanu, Dan Jurafsky,
and Chris Manning. 2010. A multi-pass
sieve for coreference resolution.
In Proceedings of EMNLP 2010,
pages 492–501, Cambridge, MA.
Rahman, Altaf and Vincent Ng. 2009.
Supervised models for coreference
resolution. In Proceedings of the 2009
Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 968–977, Suntec.
Rand, William M. 1971. Objective criteria for
the evaluation of clustering methods.
Journal of the American Statistical
Association, 66(336):846–850.
Recasens, Marta and Eduard Hovy.
2010. Coreference resolution across
corpora: Languages, coding schemes,
and preprocessing information. In
Proceedings of ACL 2010, pages 1,423–1,432,
Uppsala.
Recasens, Marta, Matthew Can, and Dan
Jurafsky. 2013. Same referent, different
words: Unsupervised mining of opaque
coreferent mentions. In Proceedings of
NAACL 2013, pages 897–906, Atlanta.
Recasens, Marta and Eduard Hovy. 2011.
BLANC: Implementing the Rand index for
</reference>
<page confidence="0.963633">
915
</page>
<reference confidence="0.993729635514019">
Computational Linguistics Volume 39, Number 4
coreference evaluation. Natural Language
Engineering, 17(4):485–510.
Sapena, Emili, Lluis Padr´o, and Jordi Turmo.
2011. Relaxcor participation in
CoNLL-shared task on coreference
resolution. In Proceedings of the Fifteenth
Conference on Computational Natural
Language Learning: Shared Task,
pages 35–39, Portland, OR.
Shou, Heming and Hai Zhao. 2012. System
paper for CoNLL-2012 shared task: Hybrid
rule-based algorithm for coreference
resolution. In Joint Conference on EMNLP
and CoNLL - Shared Task, pages 118–121,
Jeju Island.
Skinner, B. F. 1938. The Behavior of Organisms:
An Experimental Analysis. Appleton-
Century-Crofts.
Soon, Wee M., Hwee T. Ng, and Daniel C. Y.
Lim. 2001. A machine learning approach to
coreference resolution of noun phrases.
Computational Linguistics, 27(4):521–544.
Spitkovsky, Valentin I., Hiyan Alshawi,
and Daniel Jurafsky. 2010. From baby
steps to leapfrog: How “less is more”
in unsupervised dependency parsing.
In Human Language Technologies: The 2010
Annual Conference of the North American
Chapter of the Association for Computational
Linguistics, HLT ’10, pages 751–759,
Stroudsburg, PA.
Stoyanov, Veselin and Jason Eisner. 2012.
Easy-first coreference resolution.
In Proceedings of COLING 2012,
pages 2,519–2,534, Mumbai.
Uryupina, Olga, Massimo Poesio, Claudio
Giuliano, and Kateryna Tymoshenko.
2011a. Disambiguation and filtering
methods in using web knowledge for
coreference resolution. In FLAIRS
Conference, pages 317–322,
Palm Beach, FL.
Uryupina, Olga, Massimo Poesio,
Claudio Giuliano, and Kateryna
Tymoshenko. 2011b. Disambiguation
and filtering methods in using web
knowledge for coreference resolution.
In Proceedings of FLAIRS, pages 317–322,
Palm Beach, FL.
Vieira, Renata and Massimo Poesio.
2000. An empirically based system for
processing definite descriptions.
Computational Linguistics, 26(4):539–593.
Vilain, Marc, John Burger, John Aberdeen,
Dennis Connolly, and Lynette Hirschman.
1995. A model-theoretic coreference
scoring scheme. In Proceedings of MUC-6,
pages 45–52, Columbia, MD.
Vogel, Adam and Dan Jurafsky. 2012.
He Said, She Said: Gender in the ACL
Anthology. In ACL Workshop on
Rediscovering 50 Years of Discoveries,
pages 33–41, Jeju Island.
Xiong, Hao and Qun Liu. 2012. Ict:
System description for CoNLL-2012.
In Joint Conference on EMNLP and
CoNLL - Shared Task, pages 71–75,
Jeju Island.
Yang, Xiaofeng and Jian Su. 2007.
Coreference resolution using
semantic relatedness information
from automatically discovered
patterns. In Proceedings of ACL 2007,
pages 525–535, Prague.
Yang, Xiaofeng, Jian Su, Jun Lang, Chew L.
Tan, Ting Liu, and Sheng Li. 2008. An
entity-mention model for coreference
resolution with inductive logic
programming. In Proceedings of
ACL-HLT 2008, pages 843–851,
Columbus, OH.
Yang, Xiaofeng, Guodong Zhou, Jian Su,
and Chew L. Tan. 2004. An NP-cluster
approach to coreference resolution.
In Proceedings of COLING 2004,
pages 219–225, Geneva.
Yuan, Bo, Qingcai Chen, Yang Xiang,
Xiaolong Wang, Liping Ge, Zengjian Liu,
Meng Liao, and Xianbo Si. 2012. A mixed
deterministic model for coreference
resolution. In Joint Conference on EMNLP
and CoNLL - Shared Task, pages 76–82,
Jeju Island.
Zhang, Xiaotian, Chunyang Wu, and
Hai Zhao. 2012. Chinese coreference
resolution via ordered filtering.
In Joint Conference on EMNLP and
CoNLL - Shared Task, pages 95–99,
Jeju Island.
Zhou, Guodong and Jian Su. 2004.
A high-performance coreference
resolution system using a constraint-based
multi-agent strategy. In Proceedings
of the 16th International Conference on
Computational Linguistics (COLING),
page 522, Geneva.
</reference>
<page confidence="0.998589">
916
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.560193">
<title confidence="0.9750675">Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules</title>
<affiliation confidence="0.999117833333333">Stanford University Stanford University University of Leuven United States Naval Academy University of Arizona Stanford University</affiliation>
<abstract confidence="0.985575363636364">We propose a new deterministic approach to coreference resolution that combines the global information and precise features of modern machine-learning models with the transparency and modularity of deterministic, rule-based systems. Our sieve architecture applies a battery of deterministic coreference models one at a time from highest to lowest precision, where each model builds on the previous model’s cluster output. The two stages of our sieve-based architecture, a mention detection stage that heavily favors recall, followed by coreference sieves that are precision-oriented, offer a powerful way to achieve both high precision and high recall. Further, our approach makes use of global information through an entity-centric model that encourages the sharing of features across all mentions that point to the same real-world entity. Despite its simplicity, our approach gives state-of-the-art performance on several corpora and genres, and has also been incorporated into hybrid state-of-the-art coreference systems for Chinese and</abstract>
<note confidence="0.94995225">University, 450 Serra Mall, Stanford, CA 94305. E-mail: of Leuven, Blijde-Inkomststraat 21 PO Box 03308, B-3000 Leuven, Belgium. States Naval Academy, 121 Blake Road, Annapolis, MD 21402. E-mail: of Arizona, PO Box 210077, Tucson, AZ 85721-0077. E-mail: University, 450 Serra Mall, Stanford, CA 94305. E-mail: Submission received: 27 May 2012; revised submission received: 22 October 2012; accepted for publication: 20 November 2012. doi:10.1162/COLI a 00152 © 2013 Association for Computational Linguistics Computational Linguistics Volume 39, Number 4 Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference,</booktitle>
<volume>1</volume>
<pages>563--566</pages>
<location>Granada.</location>
<contexts>
<context position="41409" citStr="Bagga and Baldwin 1998" startWordPosition="6442" endWordPosition="6445">We use five evaluation metrics widely used in the literature. B3 and CEAF have implementation variations in how to take system mentions into account. We followed the same implementation as used in CoNLL-2011 shared task. • MUC (Vilain et al. 1995) – link-based metric which measures how many predicted and gold mention clusters need to be merged to cover the gold and predicted clusters, respectively. E (|Gi|−|p(Gi)|) R = (G,• a gold mention cluster, p(Gi): partitions of Gi) E (|Gi|−1) t E (|Si|−|p(Si)|) P = (S.• a system mention cluster, p(Si): partitions of Si) E (|Si|−1) t&apos; F1 = 2PR P+R • B3 (Bagga and Baldwin 1998) – mention-based metric which measures the proportion of overlap between predicted and gold mention clusters for a given mention. When Gmi is the gold cluster of mention mi and Smi is the system cluster of mention mi, R = Ei |G|Gm |mi |, P = Ei |G |Smi|mi |, F1 = P+RR • CEAF (Constrained Entity Aligned F-measure) (Luo 2005) – metric based on entity alignment. For best alignment g∗ = argmaxg∈GmΦ(g) (Φ(g): total similarity of g, a one-to-one mapping from G: gold mention clusters to S: system mention clusters), R = Φ(g∗) Ei φ(Gi,Gi), P = Φ(g∗) Ei φ(Si,Si), F1 = 2PR P+R If we use φ(G, S) = |G ∩ S|</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Bagga, Amit and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference, volume 1, pages 563–566, Granada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Breck Baldwin</author>
</authors>
<title>CogNIAC: A Discourse Processing Engine.</title>
<date>1995</date>
<tech>Ph.D. thesis.</tech>
<institution>University of Pennsylvania Department of Computer and Information Sciences.</institution>
<contexts>
<context position="26903" citStr="Baldwin (1995" startWordPosition="4094" endWordPosition="4095">lied in the overall model. Sequence Model Name Pass 1 Speaker Identification Sieve Pass 2 Exact String Match Sieve Pass 3 Relaxed String Match Sieve Pass 4 Precise Constructs Sieve (e.g., appositives) Passes 5–7 Strict Head Match Sieves A–C Pass 8 Proper Head Noun Match Sieve Pass 9 Relaxed Head Match Sieve Pass 10 Pronoun Resolution Sieve 893 Computational Linguistics Volume 39, Number 4 3.3.1 Pass 1 – Speaker Identification. This sieve matches speakers to compatible pronouns, using shallow discourse understanding to handle quotations and conversation transcripts, following the early work of Baldwin (1995, 1997). We begin by identifying speakers within text. In non-conversational text, we use a simple heuristic that searches for the subjects of reporting verbs (e.g., say) in the same sentence or neighboring sentences to a quotation. In conversational text, speaker information is provided in the data set. The extracted speakers then allow us to implement the following sieve heuristics: • (I)s4 assigned to the same speaker are coreferent. • (you)s with the same speaker are coreferent. • The speaker and (I)s in her text are coreferent. Thus for example I, my, and she in the following sentence are</context>
<context position="71641" citStr="Baldwin 1995" startWordPosition="11453" endWordPosition="11454">phrase in the enumeration to be the head word of the coordinated nominal phrase (Kuebler, McDonald, and Nivre 2009; de Marneffe and Manning 2008). Because of this, the coordinated phrase is often linked to another mention of the first element in the enumeration. For example, our system marks Zhuhai, Shantou, Xiamen as a unique mention and incorrectly links it to Zhuhai, because they have the same headword. 6. Comparison with Previous Work Algorithms for coreference (or just pronominal anaphora) include rule-based systems (Hobbs 1978; Brennan, Friedman, and Pollard 1987; Lappin and Leass 1994; Baldwin 1995; Zhou and Su 2004; Haghighi and Klein 2009, inter alia), supervised systems (Connolly, Burger, and Day 1994; McCarthy and Lehnert 1995; Kehler 1997; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Rahman and Ng 2009, inter alia), and unsupervised approaches (Cardie and Wagstaff 1999; Haghighi and Klein 2007; Ng 2008; Kobdani et al. 2011a). Our deterministic system draws from all of these, but specifically from three strands in the literature that cross-cut this classification. The idea of doing accurate reference resolution by starting with a set of very highprecision constraints was first propo</context>
</contexts>
<marker>Baldwin, 1995</marker>
<rawString>Baldwin, Breck. 1995. CogNIAC: A Discourse Processing Engine. University of Pennsylvania Department of Computer and Information Sciences. Ph.D. thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Breck Baldwin</author>
</authors>
<title>Cogniac: High precision coreference with limited knowledge and linguistic resources.</title>
<date>1997</date>
<booktitle>In Proceedings of a Workshop on Operational Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts,</booktitle>
<pages>38--45</pages>
<location>Madrid.</location>
<contexts>
<context position="4550" citStr="Baldwin (1997)" startWordPosition="640" endWordPosition="641">le-based systems relied on hand-tuned weights and were not capable of global inference, two factors that led to poor performance and replacement by machine learning. We propose a new approach that brings together the insights of these modern supervised and unsupervised models with the advantages of deterministic, rule-based systems. We introduce a model that performs entity-centric coreference, where all mentions that point to the same real-world entity are jointly modeled, in a rich feature space using solely simple, deterministic rules. Our work is inspired both by the seminal early work of Baldwin (1997), who first proposed that a series of high-precision rules could be used to build a high-precision, low-recall system for anaphora resolution, and by more recent work that has suggested that deterministic rules can outperform machine learning models for coreference (Zhou and Su 2004; Haghighi and Klein 2009) and for named entity recognition (Chiticariu et al. 2010). Figure 1 illustrates the two main stages of our new deterministic model: mention detection and coreference resolution, as well as a smaller post-processing step. In the mention detection stage, nominal and pronominal mentions are i</context>
</contexts>
<marker>Baldwin, 1997</marker>
<rawString>Baldwin, Breck. 1997. Cogniac: High precision coreference with limited knowledge and linguistic resources. In Proceedings of a Workshop on Operational Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts, pages 38–45, Madrid.</rawString>
</citation>
<citation valid="false">
<authors>
<author>features</author>
</authors>
<booktitle>In Proceedings of ACL 2012,</booktitle>
<pages>389--398</pages>
<location>Jeju Island.</location>
<marker>features, </marker>
<rawString>features. In Proceedings of ACL 2012, pages 389–398, Jeju Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Bengtson</author>
<author>Dan Roth</author>
</authors>
<title>Understanding the value of features for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>294--303</pages>
<location>Honolulu, HI.</location>
<contexts>
<context position="2774" citStr="Bengtson and Roth 2008" startWordPosition="370" endWordPosition="373">new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches </context>
<context position="39662" citStr="Bengtson and Roth 2008" startWordPosition="6160" endWordPosition="6163">f ablative experiments that analyze the contribution of each aspect of our approach and conclude with error analysis, which highlights cases currently not solved by our approach. 4.1 Corpora We used the following corpora for development and formal evaluation: • OntoNotes-Dev – development partition of OntoNotes v4.0 provided in the CoNLL2011 shared task (Pradhan et al. 2011). • OntoNotes-Test – test partition of OntoNotes v4.0 provided in the CoNLL-2011 shared task. • ACE2004-Culotta-Test – partition of the ACE 2004 corpus reserved for testing by several previous studies (Culotta et al. 2007; Bengtson and Roth 2008; Haghighi and Klein 2009). • ACE2004-nwire – newswire subset of the ACE 2004 corpus, utilized by Poon and Domingos (2008) and Haghighi and Klein (2009) for testing. • MUC6-Test – test corpus from the sixth Message Understanding Conference (MUC-6) evaluation. The corpora statistics are shown in Table 3. We used the first corpus (OntoNotes-Dev) for development and all others for the formal evaluation. We parsed all documents in the ACE and MUC corpora using the Stanford parser (Klein and Manning 2003) and the Stanford NER (Finkel, Grenager, and Manning 2005). We used the provided parse Table 3 </context>
<context position="57162" citStr="Bengtson and Roth 2008" startWordPosition="9161" endWordPosition="9164">is highlights three significant performance increases. The first is caused by Sieve 2, exact string match. This sieve accounts for approximately 16 CoNLL F1 points improvement, which proves that a significant percentage of mentions in text are indeed repetitions of previously seen concepts. The second big jump in performance, almost 3 CoNLL F1 points, is caused by Sieve 5, strict head match, which is the first pass that compares individual headwords. These results are consistent with error analyses from earlier work which have shown the importance of string match in general (Zhou and Su 2004; Bengtson and Roth 2008; and Recasens, Can, and Jurafsky 2013) and the high precision of strict head match (Recasens and Hovy 2010). Lastly, pronominal coreference resolution (Sieve 10) is responsible for approximately 9.5 CoNLL F1 points improvement. Thus it would be possible to build an even simpler system, with just three sieves, that achieves 97% of the performance of our best model (based on the CoNLL score). This suggests that what is most important for coreference resolution, at least relative to today’s state of the art, is not necessarily the clustering decision mechanism, but rather the entire architecture</context>
</contexts>
<marker>Bengtson, Roth, 2008</marker>
<rawString>Bengtson, Eric and Dan Roth. 2008. Understanding the value of features for coreference resolution. In Proceedings of EMNLP 2008, pages 294–303, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
</authors>
<title>Bootstrapping path-based pronoun resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>33--40</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="37146" citStr="Bergsma and Lin (2006)" startWordPosition="5753" endWordPosition="5756">cial factors for pronominal coreference. We implement pronominal coreference resolution using an approach standard for many decades: enforcing agreement constraints between the coreferent mentions. We use the following attributes for these constraints: • Number – we assign number attributes based on: (a) a static list for pronouns; (b) NER labels: mentions marked as a named entity are considered singular with the exception of organizations, which can be both singular and plural; (c) part of speech tags: NN*S tags are plural and all other NN* tags are singular; and (d) a static dictionary from Bergsma and Lin (2006). • Gender – we assign gender attributes from static lexicons from Bergsma and Lin (2006), and Ji and Lin (2009). • Person – we assign person attributes only to pronouns. We do not enforce this constraint when linking two pronouns, however, if one appears within quotes. This is a simple heuristic for speaker detection (e.g., I and she point to the same person in “[I] voted my conscience,” [she] said). • Animacy – we set animacy attributes using: (a) a static list for pronouns; (b) NER labels (e.g., PERSON is animate whereas LOCATION is not); and (c) a dictionary bootstrapped from the Web (Ji a</context>
</contexts>
<marker>Bergsma, Lin, 2006</marker>
<rawString>Bergsma, Shane and Dekang Lin. 2006. Bootstrapping path-based pronoun resolution. In Proceedings of COLING-ACL, pages 33–40, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
<author>Randy Goebel</author>
</authors>
<title>Distributional identification of non-referential pronouns.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT</booktitle>
<pages>10--18</pages>
<marker>Bergsma, Lin, Goebel, 2008</marker>
<rawString>Bergsma, Shane, Dekang Lin, and Randy Goebel. 2008. Distributional identification of non-referential pronouns. In Proceedings of ACL-HLT 2008, pages 10–18,</rawString>
</citation>
<citation valid="false">
<authors>
<author>OH Columbus</author>
</authors>
<marker>Columbus, </marker>
<rawString>Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luigi Borghesi</author>
<author>Chiara Favareto</author>
</authors>
<title>Flexible parsing of discretely uttered sentences.</title>
<date>1982</date>
<booktitle>In Proceedings of the 9th Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>37--42</pages>
<location>Prague.</location>
<contexts>
<context position="8089" citStr="Borghesi and Favareto 1982" startWordPosition="1169" endWordPosition="1172">coreference work mentioned earlier and discussed in Section 6, we draw on classic ideas that have proved to be important again and again in the history of natural language processing. The idea of beginning with the most accurate models or starting with smaller subproblems that allow for high-precision solutions combines the intuitions of “shaping” or “successive approximations” first proposed for learning by Skinner (1938), and widely used in NLP (e.g., the successively trained IBM MT models of Brown et al. [1993]) and the “islands of reliability” approaches to parsing and speech recognition [Borghesi and Favareto 1982; Corazza et al. 1991]). The idea of beginning with a high-recall list of candidates that are followed by a series of high-precision filters dates back to one of the earliest architectures in natural language processing, the part of speech tagging algorithm of the Computational Grammar Coder (Klein and Simmons 887 Computational Linguistics Volume 39, Number 4 1963) and the TAGGIT tagger (Greene and Rubin 1971), which begin with a high-recall list of all possible tags for words, and then used high-precision rules to filter likely tags based on context. In the next section we walk through an exa</context>
<context position="73317" citStr="Borghesi and Favareto (1982)" startWordPosition="11698" endWordPosition="11701">nd Cardie (2002b), who trained to the highest-confidence rather than nearest antecedent, or Haghighi and Klein (2009), who began with syntactic constraints (which tend to be higher-precision) before applying semantic constraints. This general idea is known by different names in 908 Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules many NLP applications: Brown et al. (1993) used simple models as “stepping stones” for more complex word alignment models; Collins and Singer (1999) used “cautious” decision list learning for named entity classification; Borghesi and Favareto (1982) and Corazza et al. (1991) used “islands of reliability” approaches to parsing and speech recognition, and Spitkovsky et al. (2010) used “baby steps” for unsupervised dependency parsing, and so forth. Our work extends the intuition of Baldwin and others to the full coreference task (i.e., including mention detection and both nominal and pronominal coreference) and shows that it can result in extremely high-performing resolution when combined with global inference. Our second inspiration comes from two works: Zhou and Su (2004) and Haghighi and Klein (2009), both of which extended Baldwin’s app</context>
</contexts>
<marker>Borghesi, Favareto, 1982</marker>
<rawString>Borghesi, Luigi and Chiara Favareto. 1982. Flexible parsing of discretely uttered sentences. In Proceedings of the 9th Conference on Computational Linguistics-Volume 1, pages 37–42, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adriane Boyd</author>
<author>Whitney Gegg-Harrison</author>
<author>Donna Byron</author>
</authors>
<title>Identifying non-referential it: A machine learning approach incorporating linguistically motivated features.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP,</booktitle>
<pages>40--47</pages>
<location>Ann Arbor, MI.</location>
<marker>Boyd, Gegg-Harrison, Byron, 2005</marker>
<rawString>Boyd, Adriane, Whitney Gegg-Harrison, and Donna Byron. 2005. Identifying non-referential it: A machine learning approach incorporating linguistically motivated features. In Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 40–47, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan E Brennan</author>
<author>Marilyn W Friedman</author>
<author>Carl Pollard</author>
</authors>
<title>A centering approach to pronouns.</title>
<date>1987</date>
<booktitle>In Proceedings of the 25th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>155--162</pages>
<location>Stanford, CA.</location>
<marker>Brennan, Friedman, Pollard, 1987</marker>
<rawString>Brennan, Susan E., Marilyn W. Friedman, and Carl Pollard. 1987. A centering approach to pronouns. In Proceedings of the 25th Annual Meeting on Association for Computational Linguistics, pages 155–162, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="73110" citStr="Brown et al. (1993)" startWordPosition="11669" endWordPosition="11672">whose antecedents were unique in the discourse, and another resolved pronouns in quoted speech. Baldwin’s idea of starting with highprecision knowledge was adopted by later researchers, such as Ng and Cardie (2002b), who trained to the highest-confidence rather than nearest antecedent, or Haghighi and Klein (2009), who began with syntactic constraints (which tend to be higher-precision) before applying semantic constraints. This general idea is known by different names in 908 Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules many NLP applications: Brown et al. (1993) used simple models as “stepping stones” for more complex word alignment models; Collins and Singer (1999) used “cautious” decision list learning for named entity classification; Borghesi and Favareto (1982) and Corazza et al. (1991) used “islands of reliability” approaches to parsing and speech recognition, and Spitkovsky et al. (2010) used “baby steps” for unsupervised dependency parsing, and so forth. Our work extends the intuition of Baldwin and others to the full coreference task (i.e., including mention detection and both nominal and pronominal coreference) and shows that it can result i</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Brown, Peter F., Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jie Cai</author>
<author>Eva Mujdricza-Maydt</author>
<author>Michael Strube</author>
</authors>
<title>Unrestricted coreference resolution via global hypergraph partitioning.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>56--60</pages>
<location>Portland, OR.</location>
<marker>Cai, Mujdricza-Maydt, Strube, 2011</marker>
<rawString>Cai, Jie, Eva Mujdricza-Maydt, and Michael Strube. 2011. Unrestricted coreference resolution via global hypergraph partitioning. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 56–60, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Cardie</author>
<author>Kiri Wagstaff</author>
</authors>
<title>Noun phrase coreference as clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>82--89</pages>
<location>College Park, MD.</location>
<contexts>
<context position="71922" citStr="Cardie and Wagstaff 1999" startWordPosition="11496" endWordPosition="11499">ample, our system marks Zhuhai, Shantou, Xiamen as a unique mention and incorrectly links it to Zhuhai, because they have the same headword. 6. Comparison with Previous Work Algorithms for coreference (or just pronominal anaphora) include rule-based systems (Hobbs 1978; Brennan, Friedman, and Pollard 1987; Lappin and Leass 1994; Baldwin 1995; Zhou and Su 2004; Haghighi and Klein 2009, inter alia), supervised systems (Connolly, Burger, and Day 1994; McCarthy and Lehnert 1995; Kehler 1997; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Rahman and Ng 2009, inter alia), and unsupervised approaches (Cardie and Wagstaff 1999; Haghighi and Klein 2007; Ng 2008; Kobdani et al. 2011a). Our deterministic system draws from all of these, but specifically from three strands in the literature that cross-cut this classification. The idea of doing accurate reference resolution by starting with a set of very highprecision constraints was first proposed for pronominal anaphora in Baldwin’s (1995) important but undercited dissertation. Baldwin suggested using seven high-precision rules as filters, combining them so as to achieve reasonable recall. One of his rules, for example, resolved pronouns whose antecedents were unique i</context>
</contexts>
<marker>Cardie, Wagstaff, 1999</marker>
<rawString>Cardie, Claire and Kiri Wagstaff. 1999. Noun phrase coreference as clustering. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 82–89, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai-Wei Chang</author>
<author>Rajhans Samdani</author>
<author>Alla Rozovskaya</author>
<author>Nick Rizzolo</author>
<author>Mark Sammons</author>
<author>Dan Roth</author>
</authors>
<title>Inference protocols for coreference resolution.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>40--44</pages>
<location>Portland, OR.</location>
<contexts>
<context position="47891" citStr="Chang et al. (2011)" startWordPosition="7617" endWordPosition="7620">tter we used gold mentions. The only reason for this distinction is to facilitate comparison with previous work (all systems listed in Table 5 used gold mention boundaries). The two tables show that, regardless of evaluation corpus and methodology, our system generally outperforms the previous state of the art. In the CoNLL shared task, 900 Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules our system scores 1.8 CoNLL F1 points higher than the next system in the closed track and 2.6 points higher than the second-ranked system in the open track. The Chang et al. (2011) system has marginally higher B3 and BLANC F1 scores, but does not outperform our model on the other two metrics and the average F1 score. Table 5 shows that our model has higher B3 F1 scores than all the other models in the two ACE corpora. The model of Haghighi and Klein (2009) minimally outperforms ours by 0.6 B3 F1 points in the MUC corpus. All in all, these results prove that our approach compares favorably with a wide range of models, which include most aspects deemed important for coreference resolution, among other things, supervised learning using rich feature sets (Sapena, Padr´o, an</context>
</contexts>
<marker>Chang, Samdani, Rozovskaya, Rizzolo, Sammons, Roth, 2011</marker>
<rawString>Chang, Kai-Wei, Rajhans Samdani, Alla Rozovskaya, Nick Rizzolo, Mark Sammons, and Dan Roth. 2011. Inference protocols for coreference resolution. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 40–44, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Chen</author>
<author>Vincent Ng</author>
</authors>
<title>Combining the best of two worlds: A hybrid approach to multilingual coreference resolution.</title>
<date>2012</date>
<booktitle>In Proceedings of the CoNLL-2012 Shared Task,</booktitle>
<pages>56--63</pages>
<location>Jeju Island.</location>
<contexts>
<context position="78456" citStr="Chen and Ng 2012" startWordPosition="12471" endWordPosition="12474">word information derived from the first (most informative) mention. Because our system begins with high-precision clusters, accurate information naturally propagates to later stages. 7. Other Systems Incorporating this Algorithm A number of recent systems have incorporated our algorithm as an important component in resolving coreference. For example, the CoNLL-2012 shared task focused on coreference resolution in a multi-lingual setting: English, Chinese, and Arabic (Pradhan et al. 2012). Forty percent of the systems in the shared task (6 of the 15 systems) made use of our sieve architecture (Chen and Ng 2012; Fernandes, dos Santos, and Milidiu 2012; Shou and Zhao 2012; Xiong and Liu 2012; Yuan et al. 2012; Zhang, Wu, and Zhao 2012), including the systems that were the highest scoring for each of the three languages (Fernandes, dos Santos, and Milidiu 2012; Chen and Ng 2012). The system of Fernandes, dos Santos, and Milidiu (2012) had the highest average score over all languages, and the best score for English and Arabic, by implementing a stacking of two models. Our sieve-based approach was first used to generate mention-link candidates, which are then reranked by a supervised model inspired from</context>
<context position="79813" citStr="Chen and Ng (2012)" startWordPosition="12684" endWordPosition="12687"> for further performance gains. The system of Chen and Ng (2012) performed the best for Chinese by making the observation that most sieves in our model are minimally lexicalized so they can be easily adapted to other languages. Their coreference model for Chinese incorporated our English sieves with only four modifications, only two of which were related to the differences between Chinese and English: The precise constructs sieve was extended to add patterns for Chinese name abbreviations, and the relaxed head-match sieve was removed, because Chinese tends not to have post-nominal modifiers.9 Chen and Ng (2012) then added a second component which first linked mentions with high string-pair or head-pair probabilities before running the sieve architecture. The strong performance of our English sieve system on Chinese with only this small number of changes speaks to the multi-lingual strength of our approach. The intuition of our system can be further extended to the task of event coreference resolution. Our recent work (Lee et al. 2012) showed that an iterative method that cautiously constructs clusters of entity and event mentions, using linear regression to model cluster merge operations, allows inf</context>
</contexts>
<marker>Chen, Ng, 2012</marker>
<rawString>Chen, Chen and Vincent Ng. 2012. Combining the best of two worlds: A hybrid approach to multilingual coreference resolution. In Proceedings of the CoNLL-2012 Shared Task, pages 56–63, Jeju Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Chiticariu</author>
<author>Rajasekar Krishnamurthy</author>
<author>Yunyao Li</author>
<author>Frederick Reiss</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Domain adaptation of rule-based annotators for named-entity recognition tasks.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--002</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="4917" citStr="Chiticariu et al. 2010" startWordPosition="694" endWordPosition="697">at performs entity-centric coreference, where all mentions that point to the same real-world entity are jointly modeled, in a rich feature space using solely simple, deterministic rules. Our work is inspired both by the seminal early work of Baldwin (1997), who first proposed that a series of high-precision rules could be used to build a high-precision, low-recall system for anaphora resolution, and by more recent work that has suggested that deterministic rules can outperform machine learning models for coreference (Zhou and Su 2004; Haghighi and Klein 2009) and for named entity recognition (Chiticariu et al. 2010). Figure 1 illustrates the two main stages of our new deterministic model: mention detection and coreference resolution, as well as a smaller post-processing step. In the mention detection stage, nominal and pronominal mentions are identified using a high-recall algorithm that selects all noun phrases (NPs), pronouns, and named entity mentions, and then filters out non-mentions (pleonastic it, i-within-i, numeric entities, partitives, etc.). The coreference resolution stage is based on a succession of ten independent coreference models (or ”sieves”), applied from highest to lowest precision. P</context>
<context position="83209" citStr="Chiticariu et al. 2010" startWordPosition="13188" endWordPosition="13191">h outperforms or performs comparably to the state of the art on several corpora. An additional benefit of the sieve framework is its modularity: New features or models can be inserted in the system with limited understanding of the other features already deployed. Our code is publicly released10 and can be used both as a stand-alone coreference system and as a platform for the development of future systems. The state-of-the-art performance of our system in coreference, either directly or as a component in hybrid systems, and that of other recent rule-based systems in named entity recognition (Chiticariu et al. 2010) suggests that rule-based systems are still an important tool for modern natural language processing. Our results further suggest that precision-ordered sieves may be an important way to structure rule based systems, and suggests the use of sieves in other NLP tasks for which a variety of very high-precision features can be designed and non-local features can be shared. Likely candidates include relation and event extraction, template slot filling, and author name deduplication. Our error analysis points to a number of places where our system could be improved, including better performance on </context>
</contexts>
<marker>Chiticariu, Krishnamurthy, Li, Reiss, Vaithyanathan, 2010</marker>
<rawString>Chiticariu, Laura, Rajasekar Krishnamurthy, Yunyao Li, Frederick Reiss, and Shivakumar Vaithyanathan. 2010. Domain adaptation of rule-based annotators for named-entity recognition tasks. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1,002–1,012, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<date>1981</date>
<booktitle>Lectures on Government and Binding. Mouton de Gruyter,</booktitle>
<location>Berlin.</location>
<contexts>
<context position="34112" citStr="Chomsky 1981" startWordPosition="5274" endWordPosition="5275"> he had turned onto [the correct runway] but pilots behind him say he turned onto [the wrong runway]. • Compatible modifiers only – the mention’s modifiers are all included in the modifiers of the antecedent candidate. This feature models the same discourse property as the previous feature, but it focuses on the two individual mentions to be linked, rather than their corresponding entities. For this feature we only use modifiers that are nouns or adjectives. • Not i-within-i – the two mentions are not in an i-within-i construct, that is, one cannot be a child NP in the other’s NP constituent (Chomsky 1981). This pass continues to maintain high precision (over 86% B3) while improving recall significantly (approximately 4.5 B3 points). 3.3.6 Passes 6 and 7 – Variants of Strict Head Match. Sieves 6 and 7 are different relaxations of the feature conjunction introduced in Pass 5, that is, Pass 6 removes the compatible modifiers only feature, and Pass 7 removes the word inclusion constraint. All in all, these two passes yield an improvement of 0.9 B3 F1 points, due to recall improvements. Table 8 in Section 4.4.3 shows that the word inclusion feature is more precise than compatible modifiers only, bu</context>
</contexts>
<marker>Chomsky, 1981</marker>
<rawString>Chomsky, Noam. 1981. Lectures on Government and Binding. Mouton de Gruyter, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>100--110</pages>
<location>College Park, MD.</location>
<contexts>
<context position="73216" citStr="Collins and Singer (1999)" startWordPosition="11685" endWordPosition="11688">win’s idea of starting with highprecision knowledge was adopted by later researchers, such as Ng and Cardie (2002b), who trained to the highest-confidence rather than nearest antecedent, or Haghighi and Klein (2009), who began with syntactic constraints (which tend to be higher-precision) before applying semantic constraints. This general idea is known by different names in 908 Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules many NLP applications: Brown et al. (1993) used simple models as “stepping stones” for more complex word alignment models; Collins and Singer (1999) used “cautious” decision list learning for named entity classification; Borghesi and Favareto (1982) and Corazza et al. (1991) used “islands of reliability” approaches to parsing and speech recognition, and Spitkovsky et al. (2010) used “baby steps” for unsupervised dependency parsing, and so forth. Our work extends the intuition of Baldwin and others to the full coreference task (i.e., including mention detection and both nominal and pronominal coreference) and shows that it can result in extremely high-performing resolution when combined with global inference. Our second inspiration comes f</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Collins, Michael and Yoram Singer. 1999. Unsupervised models for named entity classification. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 100–110, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Connolly</author>
<author>John D Burger</author>
<author>David S Day</author>
</authors>
<title>A machine learning approach to anaphoric reference.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing (NeMLaP-1),</booktitle>
<pages>255--261</pages>
<location>Manchester.</location>
<marker>Connolly, Burger, Day, 1994</marker>
<rawString>Connolly, Dennis, John D. Burger, and David S. Day. 1994. A machine learning approach to anaphoric reference. In Proceedings of the International Conference on New Methods in Language Processing (NeMLaP-1), pages 255–261, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Corazza</author>
<author>R De Mori</author>
<author>R Gretter</author>
<author>G Satta</author>
</authors>
<title>Stochastic context-free grammars for island-driven probabilistic parsing.</title>
<date>1991</date>
<booktitle>In Proceedings of Second International Workshop on Parsing Technologies (IWPT 91),</booktitle>
<pages>210--217</pages>
<marker>Corazza, De Mori, Gretter, Satta, 1991</marker>
<rawString>Corazza, A., R. De Mori, R. Gretter, and G. Satta. 1991. Stochastic context-free grammars for island-driven probabilistic parsing. In Proceedings of Second International Workshop on Parsing Technologies (IWPT 91), pages 210–217, Cancun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Michael Wick</author>
<author>Robert Hall</author>
<author>Andrew McCallum</author>
</authors>
<title>First-order probabilistic models for coreference resolution.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>81--88</pages>
<location>Rochester, NY.</location>
<contexts>
<context position="3081" citStr="Culotta et al. 2007" startWordPosition="416" endWordPosition="419">ion, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making them hard to tune and difficult to appl</context>
<context position="39638" citStr="Culotta et al. 2007" startWordPosition="6156" endWordPosition="6159">tinue with a series of ablative experiments that analyze the contribution of each aspect of our approach and conclude with error analysis, which highlights cases currently not solved by our approach. 4.1 Corpora We used the following corpora for development and formal evaluation: • OntoNotes-Dev – development partition of OntoNotes v4.0 provided in the CoNLL2011 shared task (Pradhan et al. 2011). • OntoNotes-Test – test partition of OntoNotes v4.0 provided in the CoNLL-2011 shared task. • ACE2004-Culotta-Test – partition of the ACE 2004 corpus reserved for testing by several previous studies (Culotta et al. 2007; Bengtson and Roth 2008; Haghighi and Klein 2009). • ACE2004-nwire – newswire subset of the ACE 2004 corpus, utilized by Poon and Domingos (2008) and Haghighi and Klein (2009) for testing. • MUC6-Test – test corpus from the sixth Message Understanding Conference (MUC-6) evaluation. The corpora statistics are shown in Table 3. We used the first corpus (OntoNotes-Dev) for development and all others for the formal evaluation. We parsed all documents in the ACE and MUC corpora using the Stanford parser (Klein and Manning 2003) and the Stanford NER (Finkel, Grenager, and Manning 2005). We used the</context>
<context position="49920" citStr="Culotta et al. (2007)" startWordPosition="7953" endWordPosition="7956">ention detection system alone because its score is affected by the performance of the coreference resolution model. For example, even if we start with a perfect set of gold mentions, if we miss all coreference relations in a text, every mention will remain as a singleton and will be removed by the OntoNotes post Table 5 Comparison of our system with the other reported results on the ACE and MUC corpora. All these systems use gold mention boundaries. System MUC B3 R P F1 R P F1 ACE2004-Culotta-Test This paper 70.2 82.7 75.9 74.5 88.7 81.0 Haghighi and Klein (2009) 77.7 74.8 79.6 78.5 79.6 79.0 Culotta et al. (2007) – – – 73.2 86.7 79.3 Bengston and Roth (2008) 69.9 82.7 75.8 74.5 88.3 80.8 ACE2004-nwire This paper 75.1 84.6 79.6 74.1 87.3 80.2 Haghighi and Klein (2009) 75.9 77.0 76.5 74.5 79.4 76.9 Poon and Domingos (2008) 70.5 71.3 70.9 – – – Finkel and Manning (2008) 58.5 78.7 67.1 65.2 86.8 74.5 MUC6-Test This paper 69.1 90.6 78.4 63.1 90.6 74.4 Haghighi and Klein (2009) 77.3 87.2 81.9 67.3 84.7 75.0 Poon and Domingos (2008) 75.8 83.0 79.2 – – – Finkel and Manning (2008) 55.1 89.7 68.3 49.7 90.9 64.3 901 Computational Linguistics Volume 39, Number 4 processing, resulting in zero mentions in the final</context>
<context position="77014" citStr="Culotta et al. 2007" startWordPosition="12249" endWordPosition="12252">ty of this NLP component. Our system thus offers the deterministic simplicity and high performance of the Haghighi and Klein (2009) system without the need for gold mention labels or test-time learning. Furthermore, our work extends the multi-pass model to ten passes and shows that this approach can be naturally combined with an entity-centric model for better results. Finally, recent work has shown the importance of performing coreference resolution jointly for all mentions in a document (McCallum and Wellner 2004; Daum´e III and Marcu 2005; Denis and Baldridge 2007; Haghighi and Klein 2007; Culotta et al. 2007; Poon and Domingos 2008; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011) rather than the classic method of simply aggregrating local decisions about pairs of mentions. Like these systems, our model adopts the entity-mention model (Morton 909 Computational Linguistics Volume 39, Number 4 2000; Luo et al. 2004; Yang et al. 2008; Ng 2010)8 in which features can be extracted over not just pairs of mentions but over entire clusters of mentions defining an entity. Previous systems do this by encoding constraints using rich probabilistic models and complex global inference algorithms</context>
</contexts>
<marker>Culotta, Wick, Hall, McCallum, 2007</marker>
<rawString>Culotta, Aron, Michael Wick, Robert Hall, and Andrew McCallum. 2007. First-order probabilistic models for coreference resolution. In Proceedings of HLT-NAACL 2007, pages 81–88, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daum´e Hal</author>
<author>Daniel Marcu</author>
</authors>
<title>A large-scale exploration of effective global features for a joint entity detection and tracking model. In HLT-EMNLP</title>
<date>2005</date>
<pages>97--104</pages>
<location>Vancouver.</location>
<marker>Hal, Marcu, 2005</marker>
<rawString>Daum´e III, Hal and Daniel Marcu. 2005. A large-scale exploration of effective global features for a joint entity detection and tracking model. In HLT-EMNLP 2005, pages 97–104, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The Stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING Workshop on Cross-framework and</booktitle>
<pages>1--8</pages>
<location>Manchester.</location>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>de Marneffe, Marie-Catherine and Christopher D. Manning. 2008. The Stanford typed dependencies representation. In Proceedings of COLING Workshop on Cross-framework and pages 1–8, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Joint determination of anaphoricity and coreference resolution using integer programming.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL-HLT</booktitle>
<pages>236--243</pages>
<location>Rochester, NY.</location>
<contexts>
<context position="76968" citStr="Denis and Baldridge 2007" startWordPosition="12241" endWordPosition="12244"> not practical and, ultimately, reduces the usability of this NLP component. Our system thus offers the deterministic simplicity and high performance of the Haghighi and Klein (2009) system without the need for gold mention labels or test-time learning. Furthermore, our work extends the multi-pass model to ten passes and shows that this approach can be naturally combined with an entity-centric model for better results. Finally, recent work has shown the importance of performing coreference resolution jointly for all mentions in a document (McCallum and Wellner 2004; Daum´e III and Marcu 2005; Denis and Baldridge 2007; Haghighi and Klein 2007; Culotta et al. 2007; Poon and Domingos 2008; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011) rather than the classic method of simply aggregrating local decisions about pairs of mentions. Like these systems, our model adopts the entity-mention model (Morton 909 Computational Linguistics Volume 39, Number 4 2000; Luo et al. 2004; Yang et al. 2008; Ng 2010)8 in which features can be extracted over not just pairs of mentions but over entire clusters of mentions defining an entity. Previous systems do this by encoding constraints using rich probabilistic </context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>Denis, Pascal and Jason Baldridge. 2007. Joint determination of anaphoricity and coreference resolution using integer programming. In Proceedings of NAACL-HLT 2007, pages 236–243, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Global joint models for coreference resolution and named entity classification.</title>
<date>2009</date>
<booktitle>Procesamiento del Lenguaje Natural,</booktitle>
<pages>42--87</pages>
<contexts>
<context position="3149" citStr="Denis and Baldridge 2009" startWordPosition="428" endWordPosition="431">istory of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making them hard to tune and difficult to apply to new problems and genres as well. Rule-based models like Lappin </context>
</contexts>
<marker>Denis, Baldridge, 2009</marker>
<rawString>Denis, Pascal and Jason Baldridge. 2009. Global joint models for coreference resolution and named entity classification. Procesamiento del Lenguaje Natural, 42:87–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
<author>Alexis Mitchell</author>
<author>Mark Przybocki</author>
<author>Lance Ramshaw</author>
<author>Stephanie Strassel</author>
<author>Ralph Weischedel</author>
</authors>
<title>The Automatic Content Extraction (ACE) program—Tasks, data, and evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC 2004,</booktitle>
<pages>837--840</pages>
<location>Lisbon.</location>
<contexts>
<context position="18928" citStr="Doddington et al. 2004" startWordPosition="2869" endWordPosition="2872">s possible that ... , It seems that ... , It turns out ... ). The complete set of patterns, using the tregex2 notation, is shown in Appendix B. 5. We discard adjectival forms of nations or nationality acronyms (e.g., American, U.S., U.K.), following the OntoNotes annotation guidelines. 6. We remove stop words from the following list determined by error analysis on mention detection: there, ltd., etc, ’s, hmm. Note that some rules change depending on the corpus we use for evaluation. In particular, adjectival forms of nations are valid mentions in the Automated Content Extraction (ACE) corpus (Doddington et al. 2004), thus they would not be removed when processing this corpus. 3.2 Resolution Architecture Traditionally, coreference resolution is implemented as a quadratic problem, where potential coreference links between any two mentions in a document are considered. This is not ideal, however, as it increases both the likelihood of errors and the processing time. In this article, we argue that it is better to cautiously construct high-quality mention clusters,3 and use an entity-centric model that allows the sharing of information across these incrementally constructed clusters. We achieve these goals by</context>
</contexts>
<marker>Doddington, Mitchell, Przybocki, Ramshaw, Strassel, Weischedel, 2004</marker>
<rawString>Doddington, George, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The Automatic Content Extraction (ACE) program—Tasks, data, and evaluation. In Proceedings of LREC 2004, pages 837–840, Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
</authors>
<title>The same-head heuristic for coreference.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL 2010 Short Papers,</booktitle>
<pages>33--37</pages>
<location>Uppsala.</location>
<contexts>
<context position="31950" citStr="Elsner and Charniak 2010" startWordPosition="4919" endWordPosition="4922">Notes corpus, this sieve does not enhance recall significantly, mainly because appositions and predicate nominatives are not annotated in this corpus (they are annotated in ACE). Regardless of annotation standard, however, this sieve is important because it grows entities with high quality elements, which has a significant impact on the entity’s features (as discussed in Section 3.2.3). 3.3.5 Pass 5 – Strict Head Match. Linking a mention to an antecedent based on the naive matching of their head words generates many spurious links because it completely ignores possibly incompatible modifiers (Elsner and Charniak 2010). For example, Yale University and Harvard University have similar head words, but they are obviously different entities. To address this issue, this pass implements several constraints that must all be matched in order to yield a link: • Entity head match – the mention head word matches any head word of mentions in the antecedent entity. Note that this feature is actually more relaxed than naive head matching in a pair of mentions because here it is satisfied when the mention’s head matches the head of any mention in the candidate entity. We constrain this feature by enforcing a conjunction w</context>
</contexts>
<marker>Elsner, Charniak, 2010</marker>
<rawString>Elsner, Micha and Eugene Charniak. 2010. The same-head heuristic for coreference. In Proceedings of ACL 2010 Short Papers, pages 33–37, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eraldo Fernandes</author>
<author>Cicero dos Santos</author>
<author>Ruy Milidiu</author>
</authors>
<title>Latent structure perceptron with feature induction for unrestricted coreference resolution.</title>
<date>2012</date>
<booktitle>In Proceedings of the CoNLL-2012 Shared Task,</booktitle>
<pages>41--48</pages>
<location>Jeju Island.</location>
<marker>Fernandes, Santos, Milidiu, 2012</marker>
<rawString>Fernandes, Eraldo, Cicero dos Santos, and Ruy Milidiu. 2012. Latent structure perceptron with feature induction for unrestricted coreference resolution. In Proceedings of the CoNLL-2012 Shared Task, pages 41–48, Jeju Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by Gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>363--370</pages>
<location>Stroudsburg, PA.</location>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Finkel, Jenny Rose, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 363–370, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Enforcing transitivity in coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers,</booktitle>
<pages>45--48</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="50179" citStr="Finkel and Manning (2008)" startWordPosition="8002" endWordPosition="8005">n as a singleton and will be removed by the OntoNotes post Table 5 Comparison of our system with the other reported results on the ACE and MUC corpora. All these systems use gold mention boundaries. System MUC B3 R P F1 R P F1 ACE2004-Culotta-Test This paper 70.2 82.7 75.9 74.5 88.7 81.0 Haghighi and Klein (2009) 77.7 74.8 79.6 78.5 79.6 79.0 Culotta et al. (2007) – – – 73.2 86.7 79.3 Bengston and Roth (2008) 69.9 82.7 75.8 74.5 88.3 80.8 ACE2004-nwire This paper 75.1 84.6 79.6 74.1 87.3 80.2 Haghighi and Klein (2009) 75.9 77.0 76.5 74.5 79.4 76.9 Poon and Domingos (2008) 70.5 71.3 70.9 – – – Finkel and Manning (2008) 58.5 78.7 67.1 65.2 86.8 74.5 MUC6-Test This paper 69.1 90.6 78.4 63.1 90.6 74.4 Haghighi and Klein (2009) 77.3 87.2 81.9 67.3 84.7 75.0 Poon and Domingos (2008) 75.8 83.0 79.2 – – – Finkel and Manning (2008) 55.1 89.7 68.3 49.7 90.9 64.3 901 Computational Linguistics Volume 39, Number 4 processing, resulting in zero mentions in the final output. Therefore, we included the score using gold mention boundaries in the last part of Table 4 (“Closed Track – gold boundaries”) to isolate the performance of the coreference resolution component. This experiment shows that our system outperforms the ot</context>
</contexts>
<marker>Finkel, Manning, 2008</marker>
<rawString>Finkel, Jenny Rose and Christopher D. Manning. 2008. Enforcing transitivity in coreference resolution. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, pages 45–48, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara A Fox</author>
</authors>
<title>Discourse Structure and Anaphora: Written and Conversational English.</title>
<date>1993</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="20718" citStr="Fox 1993" startWordPosition="3149" endWordPosition="3150">n textual order in their cluster. For example, given the following ordered list of mentions, {mi, m22, m23, m34, m15, m26}, where the superscript indicates cluster id, our model will attempt to resolve only m22 and m34 (m1 1 is not resolved because it is the first mention in a text). These two are the only mentions that currently appear first in their respective clusters and have potential antecedents in the document. The motivation behind this heuristic is two-fold. First, early mentions are usually better defined than subsequent ones, which are likely to have fewer modifiers or be pronouns (Fox 1993). Because several of our models use features extracted from NP modifiers, it is important to prioritize mentions that include such information. Second, by definition, first mentions appear closer to the beginning of the document, hence there are fewer antecedent candidates to select from, and thus fewer opportunities to make a mistake. We further prune the search space using a simple model of discourse salience. We disable coreference for mentions appearing first in their corresponding clusters that: (a) are or start with indefinite pronouns (e.g., some, other), (b) start with indefinite artic</context>
<context position="33068" citStr="Fox 1993" startWordPosition="5104" endWordPosition="5105">any mention in the candidate entity. We constrain this feature by enforcing a conjunction with the following features. 5 Demonym is not annotated in OntoNotes but we keep it in the system. 6 http://en.wikipedia.org/wiki/List of adjectival and demonymic forms of place names. 895 Computational Linguistics Volume 39, Number 4 • Word inclusion – all the non-stop7 words in the current entity to be solved are included in the set of non-stop words in the antecedent entity. This heuristic exploits the discourse property that states that it is uncommon to introduce novel information in later mentions (Fox 1993). Typically, mentions of the same entity become shorter and less informative as the narrative progresses. For example, based on this constraint, the model correctly clusters together the two mentions in the following text: ...intervene in the [Florida Supreme Court]’s move ... does look like very dramatic change made by [the Florida court] and avoids clustering the two mentions in the following text: The pilot had confirmed ... he had turned onto [the correct runway] but pilots behind him say he turned onto [the wrong runway]. • Compatible modifiers only – the mention’s modifiers are all inclu</context>
</contexts>
<marker>Fox, 1993</marker>
<rawString>Fox, Barbara A. 1993. Discourse Structure and Anaphora: Written and Conversational English. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara B Greene</author>
<author>Gerald M Rubin</author>
</authors>
<title>Automatic Grammatical Tagging of English.</title>
<date>1971</date>
<publisher>Brown University Press.</publisher>
<contexts>
<context position="8502" citStr="Greene and Rubin 1971" startWordPosition="1234" endWordPosition="1237">er (1938), and widely used in NLP (e.g., the successively trained IBM MT models of Brown et al. [1993]) and the “islands of reliability” approaches to parsing and speech recognition [Borghesi and Favareto 1982; Corazza et al. 1991]). The idea of beginning with a high-recall list of candidates that are followed by a series of high-precision filters dates back to one of the earliest architectures in natural language processing, the part of speech tagging algorithm of the Computational Grammar Coder (Klein and Simmons 887 Computational Linguistics Volume 39, Number 4 1963) and the TAGGIT tagger (Greene and Rubin 1971), which begin with a high-recall list of all possible tags for words, and then used high-precision rules to filter likely tags based on context. In the next section we walk through an example of our system applied to a simple made-up text. We then describe our model in detail and test its performance on three different corpora widely used in previous work for the evaluation of coreference resolution. We show that our model outperforms the state-of-the-art on each corpus. Furthermore, in these sections we describe analytic and ablative experiments demonstrating that both aspects of our algorith</context>
</contexts>
<marker>Greene, Rubin, 1971</marker>
<rawString>Greene, Barbara B. and Gerald M. Rubin. 1971. Automatic Grammatical Tagging of English. Brown University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Surabhi Gupta</author>
<author>Matthew Purver</author>
<author>Dan Jurafsky</author>
</authors>
<title>Disambiguating between generic and referential “you” in dialog.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>105--108</pages>
<location>Prague.</location>
<marker>Gupta, Purver, Jurafsky, 2007</marker>
<rawString>Gupta, Surabhi, Matthew Purver, and Dan Jurafsky. 2007. Disambiguating between generic and referential “you” in dialog. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 105–108, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Unsupervised coreference resolution in a nonparametric Bayesian model.</title>
<date>2007</date>
<contexts>
<context position="71947" citStr="Haghighi and Klein 2007" startWordPosition="11500" endWordPosition="11503">uhai, Shantou, Xiamen as a unique mention and incorrectly links it to Zhuhai, because they have the same headword. 6. Comparison with Previous Work Algorithms for coreference (or just pronominal anaphora) include rule-based systems (Hobbs 1978; Brennan, Friedman, and Pollard 1987; Lappin and Leass 1994; Baldwin 1995; Zhou and Su 2004; Haghighi and Klein 2009, inter alia), supervised systems (Connolly, Burger, and Day 1994; McCarthy and Lehnert 1995; Kehler 1997; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Rahman and Ng 2009, inter alia), and unsupervised approaches (Cardie and Wagstaff 1999; Haghighi and Klein 2007; Ng 2008; Kobdani et al. 2011a). Our deterministic system draws from all of these, but specifically from three strands in the literature that cross-cut this classification. The idea of doing accurate reference resolution by starting with a set of very highprecision constraints was first proposed for pronominal anaphora in Baldwin’s (1995) important but undercited dissertation. Baldwin suggested using seven high-precision rules as filters, combining them so as to achieve reasonable recall. One of his rules, for example, resolved pronouns whose antecedents were unique in the discourse, and anot</context>
<context position="76993" citStr="Haghighi and Klein 2007" startWordPosition="12245" endWordPosition="12248">tely, reduces the usability of this NLP component. Our system thus offers the deterministic simplicity and high performance of the Haghighi and Klein (2009) system without the need for gold mention labels or test-time learning. Furthermore, our work extends the multi-pass model to ten passes and shows that this approach can be naturally combined with an entity-centric model for better results. Finally, recent work has shown the importance of performing coreference resolution jointly for all mentions in a document (McCallum and Wellner 2004; Daum´e III and Marcu 2005; Denis and Baldridge 2007; Haghighi and Klein 2007; Culotta et al. 2007; Poon and Domingos 2008; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011) rather than the classic method of simply aggregrating local decisions about pairs of mentions. Like these systems, our model adopts the entity-mention model (Morton 909 Computational Linguistics Volume 39, Number 4 2000; Luo et al. 2004; Yang et al. 2008; Ng 2010)8 in which features can be extracted over not just pairs of mentions but over entire clusters of mentions defining an entity. Previous systems do this by encoding constraints using rich probabilistic models and complex global</context>
</contexts>
<marker>Haghighi, Klein, 2007</marker>
<rawString>Haghighi, Aria and Dan Klein. 2007. Unsupervised coreference resolution in a nonparametric Bayesian model.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of ACL 2007,</booktitle>
<pages>848--855</pages>
<location>Prague.</location>
<marker></marker>
<rawString>In Proceedings of ACL 2007, pages 848–855, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Simple coreference resolution with rich syntactic and semantic features.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP 2009,</booktitle>
<pages>1--152</pages>
<contexts>
<context position="2800" citStr="Haghighi and Klein 2009" startWordPosition="374" endWordPosition="377">ng knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution.</context>
<context position="4859" citStr="Haghighi and Klein 2009" startWordPosition="685" endWordPosition="688"> deterministic, rule-based systems. We introduce a model that performs entity-centric coreference, where all mentions that point to the same real-world entity are jointly modeled, in a rich feature space using solely simple, deterministic rules. Our work is inspired both by the seminal early work of Baldwin (1997), who first proposed that a series of high-precision rules could be used to build a high-precision, low-recall system for anaphora resolution, and by more recent work that has suggested that deterministic rules can outperform machine learning models for coreference (Zhou and Su 2004; Haghighi and Klein 2009) and for named entity recognition (Chiticariu et al. 2010). Figure 1 illustrates the two main stages of our new deterministic model: mention detection and coreference resolution, as well as a smaller post-processing step. In the mention detection stage, nominal and pronominal mentions are identified using a high-recall algorithm that selects all noun phrases (NPs), pronouns, and named entity mentions, and then filters out non-mentions (pleonastic it, i-within-i, numeric entities, partitives, etc.). The coreference resolution stage is based on a succession of ten independent coreference models </context>
<context position="23493" citStr="Haghighi and Klein 2009" startWordPosition="3583" endWordPosition="3586">dates are sorted using the following criteria: • In a given sentential clause (i.e., parser constituents whose label starts with S), candidates are sorted using a left-to-right breadth-first traversal of the corresponding syntactic constituent (Hobbs 1978). Figure 2 shows an example of candidate ordering based on this traversal. The left-to-right ordering favors subjects, which tend to appear closer to the beginning of the sentence and are more probable antecedents. The breadth-first traversal promotes syntactic salience by preferring noun phrases that are closer to the top of the parse tree (Haghighi and Klein 2009). • If the sentence containing the anaphoric mention contains multiple clauses, we repeat the previous heuristic separately in each S* constituent, starting with the one containing the mention. Figure 2 Example of left-to-right breadth-first tree traversal. The numbers indicate the order in which the NPs are visited. 892 Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules • Clauses in previous sentences are sorted based on their textual proximity to the anaphoric mention. The sorting of antecedent candidates is important because our algorithm stops a</context>
<context position="29192" citStr="Haghighi and Klein (2009)" startWordPosition="4466" endWordPosition="4469">ection 4.4.3). 3.3.3 Pass 3 – Relaxed String Match. This sieve considers two nominal mentions as coreferent if the strings obtained by dropping the text following their head words (such as relative clauses and PP and participial postmodifiers) are identical (e.g., [Clinton] and [Clinton, whose term ends in January]). 3.3.4 Pass 4 – Precise Constructs. This model links two mentions if any of the following conditions are satisfied: • Appositive – the two nominal mentions are in an appositive construction (e.g., [Israel’s Deputy Defense Minister], [Ephraim Sneh] , said ... ). We use the standard Haghighi and Klein (2009) definition to detect appositives: third children of a parent NP whose expansion begins with (NP , NP), when there is not a conjunction in the expansion. 4 We define (I) as I, my, me, or mine, (we) as first person plural pronouns, and (you) as second person pronouns. 894 Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules • Predicate nominative – the two mentions (nominal or pronominal) are in a copulative subject–object relation (e.g., [The New York-based College Board] is [a nonprofit organization that administers the SATs and promotes higher educa</context>
<context position="39688" citStr="Haghighi and Klein 2009" startWordPosition="6164" endWordPosition="6167">hat analyze the contribution of each aspect of our approach and conclude with error analysis, which highlights cases currently not solved by our approach. 4.1 Corpora We used the following corpora for development and formal evaluation: • OntoNotes-Dev – development partition of OntoNotes v4.0 provided in the CoNLL2011 shared task (Pradhan et al. 2011). • OntoNotes-Test – test partition of OntoNotes v4.0 provided in the CoNLL-2011 shared task. • ACE2004-Culotta-Test – partition of the ACE 2004 corpus reserved for testing by several previous studies (Culotta et al. 2007; Bengtson and Roth 2008; Haghighi and Klein 2009). • ACE2004-nwire – newswire subset of the ACE 2004 corpus, utilized by Poon and Domingos (2008) and Haghighi and Klein (2009) for testing. • MUC6-Test – test corpus from the sixth Message Understanding Conference (MUC-6) evaluation. The corpora statistics are shown in Table 3. We used the first corpus (OntoNotes-Dev) for development and all others for the formal evaluation. We parsed all documents in the ACE and MUC corpora using the Stanford parser (Klein and Manning 2003) and the Stanford NER (Finkel, Grenager, and Manning 2005). We used the provided parse Table 3 Corpora statistics. Corpor</context>
<context position="48171" citStr="Haghighi and Klein (2009)" startWordPosition="7670" endWordPosition="7673">erforms the previous state of the art. In the CoNLL shared task, 900 Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules our system scores 1.8 CoNLL F1 points higher than the next system in the closed track and 2.6 points higher than the second-ranked system in the open track. The Chang et al. (2011) system has marginally higher B3 and BLANC F1 scores, but does not outperform our model on the other two metrics and the average F1 score. Table 5 shows that our model has higher B3 F1 scores than all the other models in the two ACE corpora. The model of Haghighi and Klein (2009) minimally outperforms ours by 0.6 B3 F1 points in the MUC corpus. All in all, these results prove that our approach compares favorably with a wide range of models, which include most aspects deemed important for coreference resolution, among other things, supervised learning using rich feature sets (Sapena, Padr´o, and Turmo 2011; Chang et al. 2011), joint inference using spectral clustering (Cai, Mujdricza-Maydt, and Strube 2011), and deterministic rule-based models (Haghighi and Klein 2009). We discuss in more detail the similarities and differences between our approach and previous work in</context>
<context position="49868" citStr="Haghighi and Klein (2009)" startWordPosition="7943" endWordPosition="7946">cy or even hyponymy). It is not trivial to compare the mention detection system alone because its score is affected by the performance of the coreference resolution model. For example, even if we start with a perfect set of gold mentions, if we miss all coreference relations in a text, every mention will remain as a singleton and will be removed by the OntoNotes post Table 5 Comparison of our system with the other reported results on the ACE and MUC corpora. All these systems use gold mention boundaries. System MUC B3 R P F1 R P F1 ACE2004-Culotta-Test This paper 70.2 82.7 75.9 74.5 88.7 81.0 Haghighi and Klein (2009) 77.7 74.8 79.6 78.5 79.6 79.0 Culotta et al. (2007) – – – 73.2 86.7 79.3 Bengston and Roth (2008) 69.9 82.7 75.8 74.5 88.3 80.8 ACE2004-nwire This paper 75.1 84.6 79.6 74.1 87.3 80.2 Haghighi and Klein (2009) 75.9 77.0 76.5 74.5 79.4 76.9 Poon and Domingos (2008) 70.5 71.3 70.9 – – – Finkel and Manning (2008) 58.5 78.7 67.1 65.2 86.8 74.5 MUC6-Test This paper 69.1 90.6 78.4 63.1 90.6 74.4 Haghighi and Klein (2009) 77.3 87.2 81.9 67.3 84.7 75.0 Poon and Domingos (2008) 75.8 83.0 79.2 – – – Finkel and Manning (2008) 55.1 89.7 68.3 49.7 90.9 64.3 901 Computational Linguistics Volume 39, Number 4</context>
<context position="71684" citStr="Haghighi and Klein 2009" startWordPosition="11459" endWordPosition="11462">the head word of the coordinated nominal phrase (Kuebler, McDonald, and Nivre 2009; de Marneffe and Manning 2008). Because of this, the coordinated phrase is often linked to another mention of the first element in the enumeration. For example, our system marks Zhuhai, Shantou, Xiamen as a unique mention and incorrectly links it to Zhuhai, because they have the same headword. 6. Comparison with Previous Work Algorithms for coreference (or just pronominal anaphora) include rule-based systems (Hobbs 1978; Brennan, Friedman, and Pollard 1987; Lappin and Leass 1994; Baldwin 1995; Zhou and Su 2004; Haghighi and Klein 2009, inter alia), supervised systems (Connolly, Burger, and Day 1994; McCarthy and Lehnert 1995; Kehler 1997; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Rahman and Ng 2009, inter alia), and unsupervised approaches (Cardie and Wagstaff 1999; Haghighi and Klein 2007; Ng 2008; Kobdani et al. 2011a). Our deterministic system draws from all of these, but specifically from three strands in the literature that cross-cut this classification. The idea of doing accurate reference resolution by starting with a set of very highprecision constraints was first proposed for pronominal anaphora in Baldwin’s (1</context>
<context position="73879" citStr="Haghighi and Klein (2009)" startWordPosition="11784" endWordPosition="11787">g for named entity classification; Borghesi and Favareto (1982) and Corazza et al. (1991) used “islands of reliability” approaches to parsing and speech recognition, and Spitkovsky et al. (2010) used “baby steps” for unsupervised dependency parsing, and so forth. Our work extends the intuition of Baldwin and others to the full coreference task (i.e., including mention detection and both nominal and pronominal coreference) and shows that it can result in extremely high-performing resolution when combined with global inference. Our second inspiration comes from two works: Zhou and Su (2004) and Haghighi and Klein (2009), both of which extended Baldwin’s approach to generic nominal coreference. Zhou and Su proposed a multi-agent model that triggers a different agent with a specific set of deterministic constraints for each anaphor depending on its type and context (e.g., there are different constraints for noun phrases in appositive constructs, definite noun phrases, or bare noun phrases). Some of the constraints’ parameters (e.g., size of candidate search space for a given anaphor type) are learned from training data. The authors showed that this model outperforms the state of the art on the MUC-6 and MUC-7 </context>
<context position="76526" citStr="Haghighi and Klein (2009)" startWordPosition="12173" endWordPosition="12176">found that this transductive learning was essential for semantic knowledge to be useful (Aria Haghighi, personal communication); other researchers have found that semantic knowledge derived from Web resources can be quite noisy (Uryupina et al. 2011a). But although transductive learning (learning using test set mentions) thus offers advantages in precision, running a Web-based bootstrapping learner whenever a new data set is encountered is not practical and, ultimately, reduces the usability of this NLP component. Our system thus offers the deterministic simplicity and high performance of the Haghighi and Klein (2009) system without the need for gold mention labels or test-time learning. Furthermore, our work extends the multi-pass model to ten passes and shows that this approach can be naturally combined with an entity-centric model for better results. Finally, recent work has shown the importance of performing coreference resolution jointly for all mentions in a document (McCallum and Wellner 2004; Daum´e III and Marcu 2005; Denis and Baldridge 2007; Haghighi and Klein 2007; Culotta et al. 2007; Poon and Domingos 2008; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011) rather than the classi</context>
</contexts>
<marker>Haghighi, Klein, 2009</marker>
<rawString>Haghighi, Aria and Dan Klein. 2009. Simple coreference resolution with rich syntactic and semantic features. In Proceedings of EMNLP 2009, pages 1,152–1,161, Suntec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coreference resolution in a modular, entity-centered model.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL 2010,</booktitle>
<pages>385--393</pages>
<location>Los Angeles, CA.</location>
<marker>Haghighi, Klein, 2010</marker>
<rawString>Haghighi, Aria and Dan Klein. 2010. Coreference resolution in a modular, entity-centered model. In Proceedings of HLT-NAACL 2010, pages 385–393, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>Resolving pronoun references.</title>
<date>1978</date>
<journal>Lingua,</journal>
<volume>44</volume>
<issue>4</issue>
<contexts>
<context position="11000" citStr="Hobbs 1978" startWordPosition="1615" endWordPosition="1616">r in a quotation block to the corresponding speaker. In general, in all the coreference resolution sieves we traverse mentions left-to-right in a given document (see Section 3.2.1). The first match for this model is my99, which is merged with John10 10 into the same entity (entity id: 9). This illustrates the advantages of our incremental approach: by assigning a higher priority to the quotation sieve, we avoid linking my99 with A girl55, a common mistake made by generic coreference models, since anaphoric candidates (especially in subject position) are generally preferred to cataphoric ones (Hobbs 1978). The next sieve searches for anaphoric antecedents that have the exact same string as the mention under consideration. This component resolves the tenth mention, John910, by linking it with John11. When searching for antecedents, we sort candidates in the same sentential clause from left to right, and we prefer sentences that are closer to the mention under consideration (see Section 3.2.2 for details). Thus, the sorted list of candidates for John910 is It77, My favorite88, My99, A girl55, the song66, He33, a new song44, John11, a musician22. The algorithm stops as soon as a matching antecede</context>
<context position="23125" citStr="Hobbs 1978" startWordPosition="3528" endWordPosition="3529">on for a Given Mention. Given a mention mi, each model may either decline to propose a solution (in the hope that one of the subsequent models will solve it) or deterministically select a single best antecedent from a list of previous mentions m1, ..., mi−1. We sort candidate antecedents using syntactic information provided by the Stanford parser. Candidates are sorted using the following criteria: • In a given sentential clause (i.e., parser constituents whose label starts with S), candidates are sorted using a left-to-right breadth-first traversal of the corresponding syntactic constituent (Hobbs 1978). Figure 2 shows an example of candidate ordering based on this traversal. The left-to-right ordering favors subjects, which tend to appear closer to the beginning of the sentence and are more probable antecedents. The breadth-first traversal promotes syntactic salience by preferring noun phrases that are closer to the top of the parse tree (Haghighi and Klein 2009). • If the sentence containing the anaphoric mention contains multiple clauses, we repeat the previous heuristic separately in each S* constituent, starting with the one containing the mention. Figure 2 Example of left-to-right brea</context>
<context position="71567" citStr="Hobbs 1978" startWordPosition="11442" endWordPosition="11443">ords of syntactic constituents, we take the head word of the first noun phrase in the enumeration to be the head word of the coordinated nominal phrase (Kuebler, McDonald, and Nivre 2009; de Marneffe and Manning 2008). Because of this, the coordinated phrase is often linked to another mention of the first element in the enumeration. For example, our system marks Zhuhai, Shantou, Xiamen as a unique mention and incorrectly links it to Zhuhai, because they have the same headword. 6. Comparison with Previous Work Algorithms for coreference (or just pronominal anaphora) include rule-based systems (Hobbs 1978; Brennan, Friedman, and Pollard 1987; Lappin and Leass 1994; Baldwin 1995; Zhou and Su 2004; Haghighi and Klein 2009, inter alia), supervised systems (Connolly, Burger, and Day 1994; McCarthy and Lehnert 1995; Kehler 1997; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Rahman and Ng 2009, inter alia), and unsupervised approaches (Cardie and Wagstaff 1999; Haghighi and Klein 2007; Ng 2008; Kobdani et al. 2011a). Our deterministic system draws from all of these, but specifically from three strands in the literature that cross-cut this classification. The idea of doing accurate reference resolutio</context>
</contexts>
<marker>Hobbs, 1978</marker>
<rawString>Hobbs, Jerry R. 1978. Resolving pronoun references. Lingua, 44(4):311–338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Dekang Lin</author>
</authors>
<title>Gender and animacy knowledge discovery from web-scale n-grams for unsupervised person mention detection.</title>
<date>2009</date>
<booktitle>In Proceedings of the Pacific Asia Conference on Language, Information and Computation,</booktitle>
<pages>220--229</pages>
<location>Hong Kong.</location>
<contexts>
<context position="37258" citStr="Ji and Lin (2009)" startWordPosition="5773" endWordPosition="5776">r many decades: enforcing agreement constraints between the coreferent mentions. We use the following attributes for these constraints: • Number – we assign number attributes based on: (a) a static list for pronouns; (b) NER labels: mentions marked as a named entity are considered singular with the exception of organizations, which can be both singular and plural; (c) part of speech tags: NN*S tags are plural and all other NN* tags are singular; and (d) a static dictionary from Bergsma and Lin (2006). • Gender – we assign gender attributes from static lexicons from Bergsma and Lin (2006), and Ji and Lin (2009). • Person – we assign person attributes only to pronouns. We do not enforce this constraint when linking two pronouns, however, if one appears within quotes. This is a simple heuristic for speaker detection (e.g., I and she point to the same person in “[I] voted my conscience,” [she] said). • Animacy – we set animacy attributes using: (a) a static list for pronouns; (b) NER labels (e.g., PERSON is animate whereas LOCATION is not); and (c) a dictionary bootstrapped from the Web (Ji and Lin 2009). • NER label – from the Stanford NER. • Pronoun distance - sentence distance between a pronoun and </context>
<context position="46945" citStr="Ji and Lin 2009" startWordPosition="7462" endWordPosition="7465">5 50.3 Song 58.4 77.6 66.7 46.7 68.4 55.5 54.4 70.2 61.3 43.8 25.9 32.5 66.3 58.8 60.2 49.8 Zhekova 69.2 57.3 62.7 33.5 37.2 35.2 55.5 68.2 61.2 38.3 34.7 36.4 53.5 63.3 54.8 44.3 resources. For the closed track, the organizers provided dictionaries for gender and number information, in addition to parse trees and named entity labels (Pradhan et al. 2011). For the open track, we used the following additional resources: (a) a hand-built list of genders of first names that we created, incorporating frequent names from census lists and other sources (Vogel and Jurafsky 2012) (b) an animacy list (Ji and Lin 2009), (c) a country and state gazetteer, and (d) a demonym list. These resources were also used for the results reported in Table 5. A significant difference between Tables 4 and 5 is that in the former (other than its last block) we used predicted mentions (detected with the algorithm described in Section 3.1), whereas in the latter we used gold mentions. The only reason for this distinction is to facilitate comparison with previous work (all systems listed in Table 5 used gold mention boundaries). The two tables show that, regardless of evaluation corpus and methodology, our system generally out</context>
</contexts>
<marker>Ji, Lin, 2009</marker>
<rawString>Ji, Heng and Dekang Lin. 2009. Gender and animacy knowledge discovery from web-scale n-grams for unsupervised person mention detection. In Proceedings of the Pacific Asia Conference on Language, Information and Computation, pages 220–229, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kehler</author>
</authors>
<title>Probabilistic coreference in information extraction.</title>
<date>1997</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>163--173</pages>
<location>Providence, RI.</location>
<contexts>
<context position="71789" citStr="Kehler 1997" startWordPosition="11476" endWordPosition="11477">cause of this, the coordinated phrase is often linked to another mention of the first element in the enumeration. For example, our system marks Zhuhai, Shantou, Xiamen as a unique mention and incorrectly links it to Zhuhai, because they have the same headword. 6. Comparison with Previous Work Algorithms for coreference (or just pronominal anaphora) include rule-based systems (Hobbs 1978; Brennan, Friedman, and Pollard 1987; Lappin and Leass 1994; Baldwin 1995; Zhou and Su 2004; Haghighi and Klein 2009, inter alia), supervised systems (Connolly, Burger, and Day 1994; McCarthy and Lehnert 1995; Kehler 1997; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Rahman and Ng 2009, inter alia), and unsupervised approaches (Cardie and Wagstaff 1999; Haghighi and Klein 2007; Ng 2008; Kobdani et al. 2011a). Our deterministic system draws from all of these, but specifically from three strands in the literature that cross-cut this classification. The idea of doing accurate reference resolution by starting with a set of very highprecision constraints was first proposed for pronominal anaphora in Baldwin’s (1995) important but undercited dissertation. Baldwin suggested using seven high-precision rules as filters</context>
</contexts>
<marker>Kehler, 1997</marker>
<rawString>Kehler, Andrew. 1997. Probabilistic coreference in information extraction. In Proceedings of EMNLP 1997, pages 163–173, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics -Volume 1, ACL ’03,</booktitle>
<pages>423--430</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="40167" citStr="Klein and Manning 2003" startWordPosition="6241" endWordPosition="6244">n of the ACE 2004 corpus reserved for testing by several previous studies (Culotta et al. 2007; Bengtson and Roth 2008; Haghighi and Klein 2009). • ACE2004-nwire – newswire subset of the ACE 2004 corpus, utilized by Poon and Domingos (2008) and Haghighi and Klein (2009) for testing. • MUC6-Test – test corpus from the sixth Message Understanding Conference (MUC-6) evaluation. The corpora statistics are shown in Table 3. We used the first corpus (OntoNotes-Dev) for development and all others for the formal evaluation. We parsed all documents in the ACE and MUC corpora using the Stanford parser (Klein and Manning 2003) and the Stanford NER (Finkel, Grenager, and Manning 2005). We used the provided parse Table 3 Corpora statistics. Corpora # Documents # Sentences # Words # Entities # Mentions OntoNotes-Dev 303 6,894 136K 3,752 14,291 OntoNotes-Test 322 8,262 142K 3,926 16,291 ACE2004-Culotta-Test 107 1,993 33K 2,576 5,455 ACE2004-nwire 128 3,594 74K 4,762 11,398 MUC6-Test 30 576 13K 496 2,136 898 Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules trees and named entity labels (not gold) in the OntoNotes corpora to facilitate the comparison with other systems. 4.2 </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, Dan and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics -Volume 1, ACL ’03, pages 423–430, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sheldon Klein</author>
<author>Robert F Simmons</author>
</authors>
<title>A computational approach to grammatical coding of English words.</title>
<date>1963</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<volume>10</volume>
<issue>3</issue>
<marker>Klein, Simmons, 1963</marker>
<rawString>Klein, Sheldon and Robert F. Simmons. 1963. A computational approach to grammatical coding of English words. Journal of the Association for Computing Machinery, 10(3):334–347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamidreza Kobdani</author>
<author>Hinrich Schuetze</author>
<author>Michael Schiehlen</author>
<author>Hans Kamp</author>
</authors>
<title>Bootstrapping coreference resolution using word associations.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL HLT 2011,</booktitle>
<pages>783--792</pages>
<location>Portland, OR.</location>
<contexts>
<context position="71977" citStr="Kobdani et al. 2011" startWordPosition="11506" endWordPosition="11509">mention and incorrectly links it to Zhuhai, because they have the same headword. 6. Comparison with Previous Work Algorithms for coreference (or just pronominal anaphora) include rule-based systems (Hobbs 1978; Brennan, Friedman, and Pollard 1987; Lappin and Leass 1994; Baldwin 1995; Zhou and Su 2004; Haghighi and Klein 2009, inter alia), supervised systems (Connolly, Burger, and Day 1994; McCarthy and Lehnert 1995; Kehler 1997; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Rahman and Ng 2009, inter alia), and unsupervised approaches (Cardie and Wagstaff 1999; Haghighi and Klein 2007; Ng 2008; Kobdani et al. 2011a). Our deterministic system draws from all of these, but specifically from three strands in the literature that cross-cut this classification. The idea of doing accurate reference resolution by starting with a set of very highprecision constraints was first proposed for pronominal anaphora in Baldwin’s (1995) important but undercited dissertation. Baldwin suggested using seven high-precision rules as filters, combining them so as to achieve reasonable recall. One of his rules, for example, resolved pronouns whose antecedents were unique in the discourse, and another resolved pronouns in quote</context>
<context position="84477" citStr="Kobdani et al. (2011" startWordPosition="13382" endWordPosition="13385">ion, drawing on the extensive literature in this area, could also help (Vieira and Poesio 2000; Ng and Cardie 2002a; Poesio et al. 2004b; Boyd, Gegg-Harrison, and Byron 2005; Gupta, Purver, and Jurafsky 2007; Bergsma, Lin, and Goebel 2008; Ng 2009). The main conclusion of our error analysis, however, is that the plurality of our errors are due to shallow knowledge of semantics and discourse. This result points to the crucial need for more sophisticated methods of incorporating semantic and discourse knowledge. Unsupervised or semi-supervised approaches to semantics such as Yang and Su (2007), Kobdani et al. (2011b), Uryupina et al. (2011b), Bansal and Klein (2012), or Recasens, Can, and Jurafsky (2013) may point the way forward. Although sieve-based architectures are at the modern state of the art, it is only by incorporating these more powerful models of meaning that we can eventually deal with the full complexity and richness of coreference. 10 http://nlp.stanford.edu/software/dcoref.shtml. 911 Computational Linguistics Volume 39, Number 4 Appendix A: The OntoNotes Named Entity Tag Set PERSON People, including fictional NORP Nationalities or religious or political groups FACILITY Buildings, airports</context>
</contexts>
<marker>Kobdani, Schuetze, Schiehlen, Kamp, 2011</marker>
<rawString>Kobdani, Hamidreza, Hinrich Schuetze, Michael Schiehlen, and Hans Kamp. 2011a. Bootstrapping coreference resolution using word associations. In Proceedings of ACL HLT 2011, pages 783–792, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamidreza Kobdani</author>
<author>Hinrich Sch¨utze</author>
<author>Michael Schiehlen</author>
<author>Hans Kamp</author>
</authors>
<title>Bootstrapping coreference resolution using word associations.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>783--792</pages>
<location>Portland, OR.</location>
<marker>Kobdani, Sch¨utze, Schiehlen, Kamp, 2011</marker>
<rawString>Kobdani, Hamidreza, Hinrich Sch¨utze, Michael Schiehlen, and Hans Kamp. 2011b. Bootstrapping coreference resolution using word associations. In Proceedings of ACL, pages 783–792, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Kuebler</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Dependency Parsing.</title>
<date>2009</date>
<publisher>Morgan and Claypool Publishers.</publisher>
<marker>Kuebler, McDonald, Nivre, 2009</marker>
<rawString>Kuebler, Sandra, Ryan McDonald, and Joakim Nivre. 2009. Dependency Parsing. Morgan and Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shalom Lappin</author>
<author>Herbert Leass</author>
</authors>
<title>An algorithm for pronominal anaphora resolution.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="2711" citStr="Lappin and Leass 1994" startWordPosition="358" endWordPosition="361">guistics Volume 39, Number 4 Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and globa</context>
<context position="71627" citStr="Lappin and Leass 1994" startWordPosition="11449" endWordPosition="11452">word of the first noun phrase in the enumeration to be the head word of the coordinated nominal phrase (Kuebler, McDonald, and Nivre 2009; de Marneffe and Manning 2008). Because of this, the coordinated phrase is often linked to another mention of the first element in the enumeration. For example, our system marks Zhuhai, Shantou, Xiamen as a unique mention and incorrectly links it to Zhuhai, because they have the same headword. 6. Comparison with Previous Work Algorithms for coreference (or just pronominal anaphora) include rule-based systems (Hobbs 1978; Brennan, Friedman, and Pollard 1987; Lappin and Leass 1994; Baldwin 1995; Zhou and Su 2004; Haghighi and Klein 2009, inter alia), supervised systems (Connolly, Burger, and Day 1994; McCarthy and Lehnert 1995; Kehler 1997; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Rahman and Ng 2009, inter alia), and unsupervised approaches (Cardie and Wagstaff 1999; Haghighi and Klein 2007; Ng 2008; Kobdani et al. 2011a). Our deterministic system draws from all of these, but specifically from three strands in the literature that cross-cut this classification. The idea of doing accurate reference resolution by starting with a set of very highprecision constraints w</context>
</contexts>
<marker>Lappin, Leass, 1994</marker>
<rawString>Lappin, Shalom and Herbert Leass. 1994. An algorithm for pronominal anaphora resolution. Computational Linguistics, 20(4):535–561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Yves Peirsman</author>
<author>Angel Chang</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of CoNLL 2011: Shared Task,</booktitle>
<pages>28--34</pages>
<location>Portland, OR.</location>
<contexts>
<context position="7192" citStr="Lee et al. 2011" startWordPosition="1031" endWordPosition="1034"> information (head word, named entity type, gender, or number) about the other mentions already linked to i and j in previous steps. Finally, the architecture is highly modular, which means that additional coreference resolution models can be easily integrated. The two stage architecture offers a powerful way to balance both high recall and precision in the system and make use of entity-level information with rule-based architecture. The mention detection stage heavily favors recall, and the following sieves favor precision. Our results here and in our earlier papers (Raghunathan et al. 2010; Lee et al. 2011) show that this design leads to state-of-the-art performance despite the simplicity of the individual components, and that the lack of language-specific lexical features makes the system easy to port to other languages. The intuition is not new; in addition to the prior coreference work mentioned earlier and discussed in Section 6, we draw on classic ideas that have proved to be important again and again in the history of natural language processing. The idea of beginning with the most accurate models or starting with smaller subproblems that allow for high-precision solutions combines the int</context>
</contexts>
<marker>Lee, Peirsman, Chang, Chambers, Surdeanu, Jurafsky, 2011</marker>
<rawString>Lee, Heeyoung, Yves Peirsman, Angel Chang, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011. Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task. In Proceedings of CoNLL 2011: Shared Task, pages 28–34, Portland, OR.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Lee</author>
</authors>
<title>Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules Lee, Heeyoung,</title>
<location>Marta Recasens,</location>
<marker>Lee, </marker>
<rawString>Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules Lee, Heeyoung, Marta Recasens,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angel Chang</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Joint entity and event coreference resolution across documents.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>489--500</pages>
<location>Jeju Island.</location>
<marker>Chang, Surdeanu, Jurafsky, 2012</marker>
<rawString>Angel Chang, Mihai Surdeanu, and Dan Jurafsky. 2012. Joint entity and event coreference resolution across documents. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 489–500, Jeju Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<contexts>
<context position="41734" citStr="Luo 2005" startWordPosition="6504" endWordPosition="6505">merged to cover the gold and predicted clusters, respectively. E (|Gi|−|p(Gi)|) R = (G,• a gold mention cluster, p(Gi): partitions of Gi) E (|Gi|−1) t E (|Si|−|p(Si)|) P = (S.• a system mention cluster, p(Si): partitions of Si) E (|Si|−1) t&apos; F1 = 2PR P+R • B3 (Bagga and Baldwin 1998) – mention-based metric which measures the proportion of overlap between predicted and gold mention clusters for a given mention. When Gmi is the gold cluster of mention mi and Smi is the system cluster of mention mi, R = Ei |G|Gm |mi |, P = Ei |G |Smi|mi |, F1 = P+RR • CEAF (Constrained Entity Aligned F-measure) (Luo 2005) – metric based on entity alignment. For best alignment g∗ = argmaxg∈GmΦ(g) (Φ(g): total similarity of g, a one-to-one mapping from G: gold mention clusters to S: system mention clusters), R = Φ(g∗) Ei φ(Gi,Gi), P = Φ(g∗) Ei φ(Si,Si), F1 = 2PR P+R If we use φ(G, S) = |G ∩ S|, it is called mention-based CEAF (CEAF-φ3), if we use φ(G, S) = IRRnS |, it is called entity-based CEAF (CEAF-φ4). IRI • BLANC (BiLateral Assessment of NounPhrase Coreference) (Recasens and Hovy 2011) – metric applying the Rand index (Rand 1971) to coreference to deal with imbalance between singletons and coreferent mentio</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Luo, Xiaoqiang. 2005. On coreference resolution performance metrics.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of HLT-EMNLP 2005,</booktitle>
<pages>25--32</pages>
<location>Vancouver.</location>
<marker></marker>
<rawString>In Proceedings of HLT-EMNLP 2005, pages 25–32, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Abe Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Salim Roukos</author>
</authors>
<title>A mention-synchronous coreference resolution algorithm based on the Bell tree.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL 2004,</booktitle>
<pages>21--26</pages>
<location>Barcelona.</location>
<contexts>
<context position="3042" citStr="Luo et al. 2004" startWordPosition="408" endWordPosition="411">understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making </context>
<context position="77340" citStr="Luo et al. 2004" startWordPosition="12298" endWordPosition="12301"> entity-centric model for better results. Finally, recent work has shown the importance of performing coreference resolution jointly for all mentions in a document (McCallum and Wellner 2004; Daum´e III and Marcu 2005; Denis and Baldridge 2007; Haghighi and Klein 2007; Culotta et al. 2007; Poon and Domingos 2008; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011) rather than the classic method of simply aggregrating local decisions about pairs of mentions. Like these systems, our model adopts the entity-mention model (Morton 909 Computational Linguistics Volume 39, Number 4 2000; Luo et al. 2004; Yang et al. 2008; Ng 2010)8 in which features can be extracted over not just pairs of mentions but over entire clusters of mentions defining an entity. Previous systems do this by encoding constraints using rich probabilistic models and complex global inference algorithms. By contrast, global reasoning is implemented in our system just by allowing the rules in each stage to reason about any features of a cluster from a previous stage, including attributes like gender and number as well as headword information derived from the first (most informative) mention. Because our system begins with h</context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>Luo, Xiaoqiang, Abe Ittycheriah, Hongyan Jing, Nanda Kambhatla, and Salim Roukos. 2004. A mention-synchronous coreference resolution algorithm based on the Bell tree. In Proceedings of ACL 2004, pages 21–26, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Ben Wellner</author>
</authors>
<title>Conditional models of identity uncertainty with application to noun coreference.</title>
<date>2004</date>
<booktitle>In Proceedings of NIPS 2004,</booktitle>
<pages>905--912</pages>
<location>Vancouver.</location>
<contexts>
<context position="76915" citStr="McCallum and Wellner 2004" startWordPosition="12232" endWordPosition="12235">ping learner whenever a new data set is encountered is not practical and, ultimately, reduces the usability of this NLP component. Our system thus offers the deterministic simplicity and high performance of the Haghighi and Klein (2009) system without the need for gold mention labels or test-time learning. Furthermore, our work extends the multi-pass model to ten passes and shows that this approach can be naturally combined with an entity-centric model for better results. Finally, recent work has shown the importance of performing coreference resolution jointly for all mentions in a document (McCallum and Wellner 2004; Daum´e III and Marcu 2005; Denis and Baldridge 2007; Haghighi and Klein 2007; Culotta et al. 2007; Poon and Domingos 2008; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011) rather than the classic method of simply aggregrating local decisions about pairs of mentions. Like these systems, our model adopts the entity-mention model (Morton 909 Computational Linguistics Volume 39, Number 4 2000; Luo et al. 2004; Yang et al. 2008; Ng 2010)8 in which features can be extracted over not just pairs of mentions but over entire clusters of mentions defining an entity. Previous systems do t</context>
</contexts>
<marker>McCallum, Wellner, 2004</marker>
<rawString>McCallum, Andrew and Ben Wellner. 2004. Conditional models of identity uncertainty with application to noun coreference. In Proceedings of NIPS 2004, pages 905–912, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph F McCarthy</author>
<author>Wendy G Lehnert</author>
</authors>
<title>Using decision trees for coreference resolution.</title>
<date>1995</date>
<booktitle>In Proceedings of IJCAI</booktitle>
<pages>1--050</pages>
<location>Montr´eal.</location>
<contexts>
<context position="71776" citStr="McCarthy and Lehnert 1995" startWordPosition="11472" endWordPosition="11475">neffe and Manning 2008). Because of this, the coordinated phrase is often linked to another mention of the first element in the enumeration. For example, our system marks Zhuhai, Shantou, Xiamen as a unique mention and incorrectly links it to Zhuhai, because they have the same headword. 6. Comparison with Previous Work Algorithms for coreference (or just pronominal anaphora) include rule-based systems (Hobbs 1978; Brennan, Friedman, and Pollard 1987; Lappin and Leass 1994; Baldwin 1995; Zhou and Su 2004; Haghighi and Klein 2009, inter alia), supervised systems (Connolly, Burger, and Day 1994; McCarthy and Lehnert 1995; Kehler 1997; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Rahman and Ng 2009, inter alia), and unsupervised approaches (Cardie and Wagstaff 1999; Haghighi and Klein 2007; Ng 2008; Kobdani et al. 2011a). Our deterministic system draws from all of these, but specifically from three strands in the literature that cross-cut this classification. The idea of doing accurate reference resolution by starting with a set of very highprecision constraints was first proposed for pronominal anaphora in Baldwin’s (1995) important but undercited dissertation. Baldwin suggested using seven high-precision rul</context>
</contexts>
<marker>McCarthy, Lehnert, 1995</marker>
<rawString>McCarthy, Joseph F. and Wendy G. Lehnert. 1995. Using decision trees for coreference resolution. In Proceedings of IJCAI 1995, pages 1,050–1,055, Montr´eal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas S Morton</author>
</authors>
<title>Coreference for NLP applications.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL 2000,</booktitle>
<pages>173--180</pages>
<location>Hong Kong.</location>
<contexts>
<context position="3025" citStr="Morton 2000" startWordPosition="406" endWordPosition="407">ral language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more</context>
</contexts>
<marker>Morton, 2000</marker>
<rawString>Morton, Thomas S. 2000. Coreference for NLP applications. In Proceedings of ACL 2000, pages 173–180, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Unsupervised models for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>640--649</pages>
<contexts>
<context position="71956" citStr="Ng 2008" startWordPosition="11504" endWordPosition="11505">a unique mention and incorrectly links it to Zhuhai, because they have the same headword. 6. Comparison with Previous Work Algorithms for coreference (or just pronominal anaphora) include rule-based systems (Hobbs 1978; Brennan, Friedman, and Pollard 1987; Lappin and Leass 1994; Baldwin 1995; Zhou and Su 2004; Haghighi and Klein 2009, inter alia), supervised systems (Connolly, Burger, and Day 1994; McCarthy and Lehnert 1995; Kehler 1997; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Rahman and Ng 2009, inter alia), and unsupervised approaches (Cardie and Wagstaff 1999; Haghighi and Klein 2007; Ng 2008; Kobdani et al. 2011a). Our deterministic system draws from all of these, but specifically from three strands in the literature that cross-cut this classification. The idea of doing accurate reference resolution by starting with a set of very highprecision constraints was first proposed for pronominal anaphora in Baldwin’s (1995) important but undercited dissertation. Baldwin suggested using seven high-precision rules as filters, combining them so as to achieve reasonable recall. One of his rules, for example, resolved pronouns whose antecedents were unique in the discourse, and another resol</context>
</contexts>
<marker>Ng, 2008</marker>
<rawString>Ng, Vincent. 2008. Unsupervised models for coreference resolution. In Proceedings of EMNLP 2008, pages 640–649,</rawString>
</citation>
<citation valid="false">
<authors>
<author>HI Honolulu</author>
</authors>
<marker>Honolulu, </marker>
<rawString>Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Graph-cut-based anaphoricity determination for coreference resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT 2009,</booktitle>
<pages>575--583</pages>
<location>Boulder, CO.</location>
<contexts>
<context position="3169" citStr="Ng 2009" startWordPosition="434" endWordPosition="435">shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making them hard to tune and difficult to apply to new problems and genres as well. Rule-based models like Lappin and Leass (1994) wer</context>
<context position="68604" citStr="Ng 2009" startWordPosition="10987" endWordPosition="10988">e as to what happened? – Mister Zalisko is not recognized as a PERSON • Enumerations: This year, the economies of the five large special economic zones, namely, Shenzhen, Zhuhai, Shantou, Xiamen and Hainan, have maintained strong growth momentum.... A three dimensional traffic frame in Zhuhai has preliminarily taken shape and the investment environment improves daily. add more sophisticated anaphoricity detection to our system (Vieira and Poesio 2000; Ng and Cardie 2002a; Poesio et al. 2004b; Boyd, Gegg-Harrison, and Byron 2005; Gupta, Purver, and Jurafsky 2007; Bergsma, Lin, and Goebel 2008; Ng 2009). Event mentions. Our system was tailored for the resolution of entity coreference and does not have any event-specific features, such as, for example, matching event participants. Furthermore, our model considers only noun phrases as antecedent candidates, thus missing all mentions that are verbal phrases. Therefore, our system misses most coreference links between event mentions. For example, in Table 12 the pronoun That 907 Computational Linguistics Volume 39, Number 4 is coreferent with the event mention Support. Our system fails to detect the latter event mention and, as a consequence, in</context>
<context position="71854" citStr="Ng 2009" startWordPosition="11489" endWordPosition="11490">ion of the first element in the enumeration. For example, our system marks Zhuhai, Shantou, Xiamen as a unique mention and incorrectly links it to Zhuhai, because they have the same headword. 6. Comparison with Previous Work Algorithms for coreference (or just pronominal anaphora) include rule-based systems (Hobbs 1978; Brennan, Friedman, and Pollard 1987; Lappin and Leass 1994; Baldwin 1995; Zhou and Su 2004; Haghighi and Klein 2009, inter alia), supervised systems (Connolly, Burger, and Day 1994; McCarthy and Lehnert 1995; Kehler 1997; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Rahman and Ng 2009, inter alia), and unsupervised approaches (Cardie and Wagstaff 1999; Haghighi and Klein 2007; Ng 2008; Kobdani et al. 2011a). Our deterministic system draws from all of these, but specifically from three strands in the literature that cross-cut this classification. The idea of doing accurate reference resolution by starting with a set of very highprecision constraints was first proposed for pronominal anaphora in Baldwin’s (1995) important but undercited dissertation. Baldwin suggested using seven high-precision rules as filters, combining them so as to achieve reasonable recall. One of his r</context>
<context position="84105" citStr="Ng 2009" startWordPosition="13327" endWordPosition="13328">f very high-precision features can be designed and non-local features can be shared. Likely candidates include relation and event extraction, template slot filling, and author name deduplication. Our error analysis points to a number of places where our system could be improved, including better performance on pronouns. More sophisticated anaphoricity detection, drawing on the extensive literature in this area, could also help (Vieira and Poesio 2000; Ng and Cardie 2002a; Poesio et al. 2004b; Boyd, Gegg-Harrison, and Byron 2005; Gupta, Purver, and Jurafsky 2007; Bergsma, Lin, and Goebel 2008; Ng 2009). The main conclusion of our error analysis, however, is that the plurality of our errors are due to shallow knowledge of semantics and discourse. This result points to the crucial need for more sophisticated methods of incorporating semantic and discourse knowledge. Unsupervised or semi-supervised approaches to semantics such as Yang and Su (2007), Kobdani et al. (2011b), Uryupina et al. (2011b), Bansal and Klein (2012), or Recasens, Can, and Jurafsky (2013) may point the way forward. Although sieve-based architectures are at the modern state of the art, it is only by incorporating these more</context>
</contexts>
<marker>Ng, 2009</marker>
<rawString>Ng, Vincent. 2009. Graph-cut-based anaphoricity determination for coreference resolution. In Proceedings of NAACL-HLT 2009, pages 575–583, Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Supervised noun phrase coreference research: The first fifteen years.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1--396</pages>
<location>Uppsala.</location>
<contexts>
<context position="77368" citStr="Ng 2010" startWordPosition="12306" endWordPosition="12307">sults. Finally, recent work has shown the importance of performing coreference resolution jointly for all mentions in a document (McCallum and Wellner 2004; Daum´e III and Marcu 2005; Denis and Baldridge 2007; Haghighi and Klein 2007; Culotta et al. 2007; Poon and Domingos 2008; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011) rather than the classic method of simply aggregrating local decisions about pairs of mentions. Like these systems, our model adopts the entity-mention model (Morton 909 Computational Linguistics Volume 39, Number 4 2000; Luo et al. 2004; Yang et al. 2008; Ng 2010)8 in which features can be extracted over not just pairs of mentions but over entire clusters of mentions defining an entity. Previous systems do this by encoding constraints using rich probabilistic models and complex global inference algorithms. By contrast, global reasoning is implemented in our system just by allowing the rules in each stage to reason about any features of a cluster from a previous stage, including attributes like gender and number as well as headword information derived from the first (most informative) mention. Because our system begins with high-precision clusters, accu</context>
</contexts>
<marker>Ng, 2010</marker>
<rawString>Ng, Vincent. 2010. Supervised noun phrase coreference research: The first fifteen years. In Proceedings of ACL, pages 1,396–1,411, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1--7</pages>
<location>Taipei.</location>
<contexts>
<context position="2687" citStr="Ng and Cardie 2002" startWordPosition="354" endWordPosition="357">ics Computational Linguistics Volume 39, Number 4 Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully des</context>
<context position="68470" citStr="Ng and Cardie 2002" startWordPosition="10964" endWordPosition="10967">dit controls. • Parser or NER error: Um alright uh Mister Zalisko do you know anything from your personal experience of having been on the cruise as to what happened? – Mister Zalisko is not recognized as a PERSON • Enumerations: This year, the economies of the five large special economic zones, namely, Shenzhen, Zhuhai, Shantou, Xiamen and Hainan, have maintained strong growth momentum.... A three dimensional traffic frame in Zhuhai has preliminarily taken shape and the investment environment improves daily. add more sophisticated anaphoricity detection to our system (Vieira and Poesio 2000; Ng and Cardie 2002a; Poesio et al. 2004b; Boyd, Gegg-Harrison, and Byron 2005; Gupta, Purver, and Jurafsky 2007; Bergsma, Lin, and Goebel 2008; Ng 2009). Event mentions. Our system was tailored for the resolution of entity coreference and does not have any event-specific features, such as, for example, matching event participants. Furthermore, our model considers only noun phrases as antecedent candidates, thus missing all mentions that are verbal phrases. Therefore, our system misses most coreference links between event mentions. For example, in Table 12 the pronoun That 907 Computational Linguistics Volume 39</context>
<context position="71833" citStr="Ng and Cardie 2002" startWordPosition="11483" endWordPosition="11486"> is often linked to another mention of the first element in the enumeration. For example, our system marks Zhuhai, Shantou, Xiamen as a unique mention and incorrectly links it to Zhuhai, because they have the same headword. 6. Comparison with Previous Work Algorithms for coreference (or just pronominal anaphora) include rule-based systems (Hobbs 1978; Brennan, Friedman, and Pollard 1987; Lappin and Leass 1994; Baldwin 1995; Zhou and Su 2004; Haghighi and Klein 2009, inter alia), supervised systems (Connolly, Burger, and Day 1994; McCarthy and Lehnert 1995; Kehler 1997; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Rahman and Ng 2009, inter alia), and unsupervised approaches (Cardie and Wagstaff 1999; Haghighi and Klein 2007; Ng 2008; Kobdani et al. 2011a). Our deterministic system draws from all of these, but specifically from three strands in the literature that cross-cut this classification. The idea of doing accurate reference resolution by starting with a set of very highprecision constraints was first proposed for pronominal anaphora in Baldwin’s (1995) important but undercited dissertation. Baldwin suggested using seven high-precision rules as filters, combining them so as to achieve reasonable</context>
<context position="83971" citStr="Ng and Cardie 2002" startWordPosition="13304" endWordPosition="13307">-ordered sieves may be an important way to structure rule based systems, and suggests the use of sieves in other NLP tasks for which a variety of very high-precision features can be designed and non-local features can be shared. Likely candidates include relation and event extraction, template slot filling, and author name deduplication. Our error analysis points to a number of places where our system could be improved, including better performance on pronouns. More sophisticated anaphoricity detection, drawing on the extensive literature in this area, could also help (Vieira and Poesio 2000; Ng and Cardie 2002a; Poesio et al. 2004b; Boyd, Gegg-Harrison, and Byron 2005; Gupta, Purver, and Jurafsky 2007; Bergsma, Lin, and Goebel 2008; Ng 2009). The main conclusion of our error analysis, however, is that the plurality of our errors are due to shallow knowledge of semantics and discourse. This result points to the crucial need for more sophisticated methods of incorporating semantic and discourse knowledge. Unsupervised or semi-supervised approaches to semantics such as Yang and Su (2007), Kobdani et al. (2011b), Uryupina et al. (2011b), Bansal and Klein (2012), or Recasens, Can, and Jurafsky (2013) ma</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Ng, Vincent and Claire Cardie. 2002a. Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution. In Proceedings of COLING, pages 1–7, Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>104--111</pages>
<contexts>
<context position="2687" citStr="Ng and Cardie 2002" startWordPosition="354" endWordPosition="357">ics Computational Linguistics Volume 39, Number 4 Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully des</context>
<context position="68470" citStr="Ng and Cardie 2002" startWordPosition="10964" endWordPosition="10967">dit controls. • Parser or NER error: Um alright uh Mister Zalisko do you know anything from your personal experience of having been on the cruise as to what happened? – Mister Zalisko is not recognized as a PERSON • Enumerations: This year, the economies of the five large special economic zones, namely, Shenzhen, Zhuhai, Shantou, Xiamen and Hainan, have maintained strong growth momentum.... A three dimensional traffic frame in Zhuhai has preliminarily taken shape and the investment environment improves daily. add more sophisticated anaphoricity detection to our system (Vieira and Poesio 2000; Ng and Cardie 2002a; Poesio et al. 2004b; Boyd, Gegg-Harrison, and Byron 2005; Gupta, Purver, and Jurafsky 2007; Bergsma, Lin, and Goebel 2008; Ng 2009). Event mentions. Our system was tailored for the resolution of entity coreference and does not have any event-specific features, such as, for example, matching event participants. Furthermore, our model considers only noun phrases as antecedent candidates, thus missing all mentions that are verbal phrases. Therefore, our system misses most coreference links between event mentions. For example, in Table 12 the pronoun That 907 Computational Linguistics Volume 39</context>
<context position="71833" citStr="Ng and Cardie 2002" startWordPosition="11483" endWordPosition="11486"> is often linked to another mention of the first element in the enumeration. For example, our system marks Zhuhai, Shantou, Xiamen as a unique mention and incorrectly links it to Zhuhai, because they have the same headword. 6. Comparison with Previous Work Algorithms for coreference (or just pronominal anaphora) include rule-based systems (Hobbs 1978; Brennan, Friedman, and Pollard 1987; Lappin and Leass 1994; Baldwin 1995; Zhou and Su 2004; Haghighi and Klein 2009, inter alia), supervised systems (Connolly, Burger, and Day 1994; McCarthy and Lehnert 1995; Kehler 1997; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Rahman and Ng 2009, inter alia), and unsupervised approaches (Cardie and Wagstaff 1999; Haghighi and Klein 2007; Ng 2008; Kobdani et al. 2011a). Our deterministic system draws from all of these, but specifically from three strands in the literature that cross-cut this classification. The idea of doing accurate reference resolution by starting with a set of very highprecision constraints was first proposed for pronominal anaphora in Baldwin’s (1995) important but undercited dissertation. Baldwin suggested using seven high-precision rules as filters, combining them so as to achieve reasonable</context>
<context position="83971" citStr="Ng and Cardie 2002" startWordPosition="13304" endWordPosition="13307">-ordered sieves may be an important way to structure rule based systems, and suggests the use of sieves in other NLP tasks for which a variety of very high-precision features can be designed and non-local features can be shared. Likely candidates include relation and event extraction, template slot filling, and author name deduplication. Our error analysis points to a number of places where our system could be improved, including better performance on pronouns. More sophisticated anaphoricity detection, drawing on the extensive literature in this area, could also help (Vieira and Poesio 2000; Ng and Cardie 2002a; Poesio et al. 2004b; Boyd, Gegg-Harrison, and Byron 2005; Gupta, Purver, and Jurafsky 2007; Bergsma, Lin, and Goebel 2008; Ng 2009). The main conclusion of our error analysis, however, is that the plurality of our errors are due to shallow knowledge of semantics and discourse. This result points to the crucial need for more sophisticated methods of incorporating semantic and discourse knowledge. Unsupervised or semi-supervised approaches to semantics such as Yang and Su (2007), Kobdani et al. (2011b), Uryupina et al. (2011b), Bansal and Klein (2012), or Recasens, Can, and Jurafsky (2013) ma</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Ng, Vincent and Claire Cardie. 2002b. Improving machine learning approaches to coreference resolution. In Proceedings of ACL 2002, pages 104–111,</rawString>
</citation>
<citation valid="false">
<location>Philadelphia, PA.</location>
<marker></marker>
<rawString>Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Rahul Mehta</author>
<author>Axel Maroudas</author>
<author>Janet Hitzeman</author>
</authors>
<title>Learning to resolve bridging references.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>143--150</pages>
<location>Barcelona.</location>
<contexts>
<context position="2731" citStr="Poesio et al. 2004" startWordPosition="362" endWordPosition="365">ber 4 Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric </context>
<context position="68491" citStr="Poesio et al. 2004" startWordPosition="10968" endWordPosition="10971">r or NER error: Um alright uh Mister Zalisko do you know anything from your personal experience of having been on the cruise as to what happened? – Mister Zalisko is not recognized as a PERSON • Enumerations: This year, the economies of the five large special economic zones, namely, Shenzhen, Zhuhai, Shantou, Xiamen and Hainan, have maintained strong growth momentum.... A three dimensional traffic frame in Zhuhai has preliminarily taken shape and the investment environment improves daily. add more sophisticated anaphoricity detection to our system (Vieira and Poesio 2000; Ng and Cardie 2002a; Poesio et al. 2004b; Boyd, Gegg-Harrison, and Byron 2005; Gupta, Purver, and Jurafsky 2007; Bergsma, Lin, and Goebel 2008; Ng 2009). Event mentions. Our system was tailored for the resolution of entity coreference and does not have any event-specific features, such as, for example, matching event participants. Furthermore, our model considers only noun phrases as antecedent candidates, thus missing all mentions that are verbal phrases. Therefore, our system misses most coreference links between event mentions. For example, in Table 12 the pronoun That 907 Computational Linguistics Volume 39, Number 4 is corefer</context>
<context position="83992" citStr="Poesio et al. 2004" startWordPosition="13308" endWordPosition="13311">e an important way to structure rule based systems, and suggests the use of sieves in other NLP tasks for which a variety of very high-precision features can be designed and non-local features can be shared. Likely candidates include relation and event extraction, template slot filling, and author name deduplication. Our error analysis points to a number of places where our system could be improved, including better performance on pronouns. More sophisticated anaphoricity detection, drawing on the extensive literature in this area, could also help (Vieira and Poesio 2000; Ng and Cardie 2002a; Poesio et al. 2004b; Boyd, Gegg-Harrison, and Byron 2005; Gupta, Purver, and Jurafsky 2007; Bergsma, Lin, and Goebel 2008; Ng 2009). The main conclusion of our error analysis, however, is that the plurality of our errors are due to shallow knowledge of semantics and discourse. This result points to the crucial need for more sophisticated methods of incorporating semantic and discourse knowledge. Unsupervised or semi-supervised approaches to semantics such as Yang and Su (2007), Kobdani et al. (2011b), Uryupina et al. (2011b), Bansal and Klein (2012), or Recasens, Can, and Jurafsky (2013) may point the way forwa</context>
</contexts>
<marker>Poesio, Mehta, Maroudas, Hitzeman, 2004</marker>
<rawString>Poesio, Massimo, Rahul Mehta, Axel Maroudas, and Janet Hitzeman. 2004a. Learning to resolve bridging references. In Proceedings of ACL, pages 143–150, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Olga Uryupina</author>
<author>Renata Vieira</author>
<author>Mijail Alexandrov-Kabadjov</author>
<author>Rodrigo Goulart</author>
</authors>
<title>Discourse-new detectors for definite description resolution: A survey and a preliminary proposal.</title>
<date>2004</date>
<booktitle>In ACL 2004: Workshop on Reference Resolution and its Applications,</booktitle>
<pages>47--54</pages>
<location>Barcelona.</location>
<contexts>
<context position="2731" citStr="Poesio et al. 2004" startWordPosition="362" endWordPosition="365">ber 4 Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric </context>
<context position="68491" citStr="Poesio et al. 2004" startWordPosition="10968" endWordPosition="10971">r or NER error: Um alright uh Mister Zalisko do you know anything from your personal experience of having been on the cruise as to what happened? – Mister Zalisko is not recognized as a PERSON • Enumerations: This year, the economies of the five large special economic zones, namely, Shenzhen, Zhuhai, Shantou, Xiamen and Hainan, have maintained strong growth momentum.... A three dimensional traffic frame in Zhuhai has preliminarily taken shape and the investment environment improves daily. add more sophisticated anaphoricity detection to our system (Vieira and Poesio 2000; Ng and Cardie 2002a; Poesio et al. 2004b; Boyd, Gegg-Harrison, and Byron 2005; Gupta, Purver, and Jurafsky 2007; Bergsma, Lin, and Goebel 2008; Ng 2009). Event mentions. Our system was tailored for the resolution of entity coreference and does not have any event-specific features, such as, for example, matching event participants. Furthermore, our model considers only noun phrases as antecedent candidates, thus missing all mentions that are verbal phrases. Therefore, our system misses most coreference links between event mentions. For example, in Table 12 the pronoun That 907 Computational Linguistics Volume 39, Number 4 is corefer</context>
<context position="83992" citStr="Poesio et al. 2004" startWordPosition="13308" endWordPosition="13311">e an important way to structure rule based systems, and suggests the use of sieves in other NLP tasks for which a variety of very high-precision features can be designed and non-local features can be shared. Likely candidates include relation and event extraction, template slot filling, and author name deduplication. Our error analysis points to a number of places where our system could be improved, including better performance on pronouns. More sophisticated anaphoricity detection, drawing on the extensive literature in this area, could also help (Vieira and Poesio 2000; Ng and Cardie 2002a; Poesio et al. 2004b; Boyd, Gegg-Harrison, and Byron 2005; Gupta, Purver, and Jurafsky 2007; Bergsma, Lin, and Goebel 2008; Ng 2009). The main conclusion of our error analysis, however, is that the plurality of our errors are due to shallow knowledge of semantics and discourse. This result points to the crucial need for more sophisticated methods of incorporating semantic and discourse knowledge. Unsupervised or semi-supervised approaches to semantics such as Yang and Su (2007), Kobdani et al. (2011b), Uryupina et al. (2011b), Bansal and Klein (2012), or Recasens, Can, and Jurafsky (2013) may point the way forwa</context>
</contexts>
<marker>Poesio, Uryupina, Vieira, Alexandrov-Kabadjov, Goulart, 2004</marker>
<rawString>Poesio, Massimo, Olga Uryupina, Renata Vieira, Mijail Alexandrov-Kabadjov, and Rodrigo Goulart. 2004b. Discourse-new detectors for definite description resolution: A survey and a preliminary proposal. In ACL 2004: Workshop on Reference Resolution and its Applications, pages 47–54, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Joint unsupervised coreference resolution with Markov logic.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP 2008,</booktitle>
<pages>650--659</pages>
<location>Honolulu, HI.</location>
<contexts>
<context position="3123" citStr="Poon and Domingos 2008" startWordPosition="424" endWordPosition="427">n extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making them hard to tune and difficult to apply to new problems and genres as well. Rule</context>
<context position="29821" citStr="Poon and Domingos 2008" startWordPosition="4564" endWordPosition="4567">tion to detect appositives: third children of a parent NP whose expansion begins with (NP , NP), when there is not a conjunction in the expansion. 4 We define (I) as I, my, me, or mine, (we) as first person plural pronouns, and (you) as second person pronouns. 894 Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules • Predicate nominative – the two mentions (nominal or pronominal) are in a copulative subject–object relation (e.g., [The New York-based College Board] is [a nonprofit organization that administers the SATs and promotes higher education] [Poon and Domingos 2008]). • Role appositive – the candidate antecedent is headed by a noun and appears as a modifier in an NP whose head is the current mention (e.g., [[actress] Rebecca Schaeffer]). This feature is inspired by Haghighi and Klein (2009), who triggered it only if the mention is labeled as a person by the Stanford named entity recognizer (NER). We constrain this heuristic more in our work: We allow this feature to match only if: (a) the mention is labeled as a person, (b) the antecedent is animate (we detail animacy detection in Section 3.3.9), and (c) the antecedent’s gender is not neutral. • Relativ</context>
<context position="39784" citStr="Poon and Domingos (2008)" startWordPosition="6180" endWordPosition="6183">ich highlights cases currently not solved by our approach. 4.1 Corpora We used the following corpora for development and formal evaluation: • OntoNotes-Dev – development partition of OntoNotes v4.0 provided in the CoNLL2011 shared task (Pradhan et al. 2011). • OntoNotes-Test – test partition of OntoNotes v4.0 provided in the CoNLL-2011 shared task. • ACE2004-Culotta-Test – partition of the ACE 2004 corpus reserved for testing by several previous studies (Culotta et al. 2007; Bengtson and Roth 2008; Haghighi and Klein 2009). • ACE2004-nwire – newswire subset of the ACE 2004 corpus, utilized by Poon and Domingos (2008) and Haghighi and Klein (2009) for testing. • MUC6-Test – test corpus from the sixth Message Understanding Conference (MUC-6) evaluation. The corpora statistics are shown in Table 3. We used the first corpus (OntoNotes-Dev) for development and all others for the formal evaluation. We parsed all documents in the ACE and MUC corpora using the Stanford parser (Klein and Manning 2003) and the Stanford NER (Finkel, Grenager, and Manning 2005). We used the provided parse Table 3 Corpora statistics. Corpora # Documents # Sentences # Words # Entities # Mentions OntoNotes-Dev 303 6,894 136K 3,752 14,29</context>
<context position="50132" citStr="Poon and Domingos (2008)" startWordPosition="7992" endWordPosition="7995"> relations in a text, every mention will remain as a singleton and will be removed by the OntoNotes post Table 5 Comparison of our system with the other reported results on the ACE and MUC corpora. All these systems use gold mention boundaries. System MUC B3 R P F1 R P F1 ACE2004-Culotta-Test This paper 70.2 82.7 75.9 74.5 88.7 81.0 Haghighi and Klein (2009) 77.7 74.8 79.6 78.5 79.6 79.0 Culotta et al. (2007) – – – 73.2 86.7 79.3 Bengston and Roth (2008) 69.9 82.7 75.8 74.5 88.3 80.8 ACE2004-nwire This paper 75.1 84.6 79.6 74.1 87.3 80.2 Haghighi and Klein (2009) 75.9 77.0 76.5 74.5 79.4 76.9 Poon and Domingos (2008) 70.5 71.3 70.9 – – – Finkel and Manning (2008) 58.5 78.7 67.1 65.2 86.8 74.5 MUC6-Test This paper 69.1 90.6 78.4 63.1 90.6 74.4 Haghighi and Klein (2009) 77.3 87.2 81.9 67.3 84.7 75.0 Poon and Domingos (2008) 75.8 83.0 79.2 – – – Finkel and Manning (2008) 55.1 89.7 68.3 49.7 90.9 64.3 901 Computational Linguistics Volume 39, Number 4 processing, resulting in zero mentions in the final output. Therefore, we included the score using gold mention boundaries in the last part of Table 4 (“Closed Track – gold boundaries”) to isolate the performance of the coreference resolution component. This expe</context>
<context position="77038" citStr="Poon and Domingos 2008" startWordPosition="12253" endWordPosition="12256">ent. Our system thus offers the deterministic simplicity and high performance of the Haghighi and Klein (2009) system without the need for gold mention labels or test-time learning. Furthermore, our work extends the multi-pass model to ten passes and shows that this approach can be naturally combined with an entity-centric model for better results. Finally, recent work has shown the importance of performing coreference resolution jointly for all mentions in a document (McCallum and Wellner 2004; Daum´e III and Marcu 2005; Denis and Baldridge 2007; Haghighi and Klein 2007; Culotta et al. 2007; Poon and Domingos 2008; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011) rather than the classic method of simply aggregrating local decisions about pairs of mentions. Like these systems, our model adopts the entity-mention model (Morton 909 Computational Linguistics Volume 39, Number 4 2000; Luo et al. 2004; Yang et al. 2008; Ng 2010)8 in which features can be extracted over not just pairs of mentions but over entire clusters of mentions defining an entity. Previous systems do this by encoding constraints using rich probabilistic models and complex global inference algorithms. By contrast, global re</context>
</contexts>
<marker>Poon, Domingos, 2008</marker>
<rawString>Poon, Hoifung and Pedro Domingos. 2008. Joint unsupervised coreference resolution with Markov logic. In Proceedings of EMNLP 2008, pages 650–659, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Alessandro Moschitti</author>
<author>Nianwen Xue</author>
<author>Olga Uryupina</author>
<author>Yuchen Zhang</author>
</authors>
<title>CoNLL-2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixteenth Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>page</pages>
<note>1, Jeju Island.</note>
<contexts>
<context position="78332" citStr="Pradhan et al. 2012" startWordPosition="12447" endWordPosition="12450">age to reason about any features of a cluster from a previous stage, including attributes like gender and number as well as headword information derived from the first (most informative) mention. Because our system begins with high-precision clusters, accurate information naturally propagates to later stages. 7. Other Systems Incorporating this Algorithm A number of recent systems have incorporated our algorithm as an important component in resolving coreference. For example, the CoNLL-2012 shared task focused on coreference resolution in a multi-lingual setting: English, Chinese, and Arabic (Pradhan et al. 2012). Forty percent of the systems in the shared task (6 of the 15 systems) made use of our sieve architecture (Chen and Ng 2012; Fernandes, dos Santos, and Milidiu 2012; Shou and Zhao 2012; Xiong and Liu 2012; Yuan et al. 2012; Zhang, Wu, and Zhao 2012), including the systems that were the highest scoring for each of the three languages (Fernandes, dos Santos, and Milidiu 2012; Chen and Ng 2012). The system of Fernandes, dos Santos, and Milidiu (2012) had the highest average score over all languages, and the best score for English and Arabic, by implementing a stacking of two models. Our sieve-ba</context>
</contexts>
<marker>Pradhan, Moschitti, Xue, Uryupina, Zhang, 2012</marker>
<rawString>Pradhan, Sameer, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes. In Proceedings of the Sixteenth Conference on Computational Natural Language Learning (CoNLL), page 1, Jeju Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Ralph Weischedel</author>
<author>Nianwen Xue</author>
</authors>
<title>CoNLL-2011 shared task: Modeling unrestricted coreference in OntoNotes.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>1--27</pages>
<location>Portland, OR.</location>
<contexts>
<context position="39417" citStr="Pradhan et al. 2011" startWordPosition="6122" endWordPosition="6125">ions Yongkang Zhou and a consultant... are removed in this stage. 4. Experimental Results We start this section with overall results on three corpora widely used for the evaluation of coreference resolution systems. We continue with a series of ablative experiments that analyze the contribution of each aspect of our approach and conclude with error analysis, which highlights cases currently not solved by our approach. 4.1 Corpora We used the following corpora for development and formal evaluation: • OntoNotes-Dev – development partition of OntoNotes v4.0 provided in the CoNLL2011 shared task (Pradhan et al. 2011). • OntoNotes-Test – test partition of OntoNotes v4.0 provided in the CoNLL-2011 shared task. • ACE2004-Culotta-Test – partition of the ACE 2004 corpus reserved for testing by several previous studies (Culotta et al. 2007; Bengtson and Roth 2008; Haghighi and Klein 2009). • ACE2004-nwire – newswire subset of the ACE 2004 corpus, utilized by Poon and Domingos (2008) and Haghighi and Klein (2009) for testing. • MUC6-Test – test corpus from the sixth Message Understanding Conference (MUC-6) evaluation. The corpora statistics are shown in Table 3. We used the first corpus (OntoNotes-Dev) for devel</context>
<context position="42823" citStr="Pradhan et al. 2011" startWordPosition="6692" endWordPosition="6695">y 2011) – metric applying the Rand index (Rand 1971) to coreference to deal with imbalance between singletons and coreferent mentions by considering coreference and non-coreference links. Pc = rc rc+wc, Pn = rn rn+wn, Rc = rc rc+wn, Rn = rn rn+wc, Fc = 2PcRcPc+Rc ,Fn = 2PnRn Pn+Rn , BLANC = Fc+Fn 2 (rc: the number of correct coreference links, wc: the number of incorrect coreference links, rn: the number of correct non-coreference links, wn: the number of incorrect non-coreference links) • CoNLL F1 Average of MUC, B3, and CEAF-φ4 F1. This was the official metric in the CoNLL-2011 shared task (Pradhan et al. 2011). 4.3 Experimental Results Tables 4 and 5 compare the performance of our system with other state-of-the-art systems in the CoNLL-2011 shared task and the ACE and MUC corpora, respectively. For the CoNLL-2011 shared task we report results in the closed track, which did not allow the use of external resources, and the open track, which allowed any other 899 Computational Linguistics Volume 39, Number 4 Table 4 Performance of the top systems in the CoNLL-2011 shared task. All these systems use automatically detected mentions. We report results for both the closed and the open tracks, which allowe</context>
<context position="46686" citStr="Pradhan et al. 2011" startWordPosition="7419" endWordPosition="7422">3 55.5 Kobdani 67.1 65.1 66.1 62.6 56.8 59.6 73.2 62.2 67.3 32.9 37.3 34.9 64.1 64.1 64.1 53.9 Stoyanov 76.9 64.7 70.3 69.8 55.0 61.5 77.1 52.5 62.5 31.0 44.8 36.6 76.6 60.3 63.0 53.6 Zhang 59.6 71.2 64.9 46.1 58.8 51.6 53.9 73.4 62.2 43.5 32.1 37.0 64.1 70.5 66.5 50.3 Song 58.4 77.6 66.7 46.7 68.4 55.5 54.4 70.2 61.3 43.8 25.9 32.5 66.3 58.8 60.2 49.8 Zhekova 69.2 57.3 62.7 33.5 37.2 35.2 55.5 68.2 61.2 38.3 34.7 36.4 53.5 63.3 54.8 44.3 resources. For the closed track, the organizers provided dictionaries for gender and number information, in addition to parse trees and named entity labels (Pradhan et al. 2011). For the open track, we used the following additional resources: (a) a hand-built list of genders of first names that we created, incorporating frequent names from census lists and other sources (Vogel and Jurafsky 2012) (b) an animacy list (Ji and Lin 2009), (c) a country and state gazetteer, and (d) a demonym list. These resources were also used for the results reported in Table 5. A significant difference between Tables 4 and 5 is that in the former (other than its last block) we used predicted mentions (detected with the algorithm described in Section 3.1), whereas in the latter we used g</context>
</contexts>
<marker>Pradhan, Ramshaw, Marcus, Palmer, Weischedel, Xue, 2011</marker>
<rawString>Pradhan, Sameer, Lance Ramshaw, Mitchell Marcus, Martha Palmer, Ralph Weischedel, and Nianwen Xue. 2011. CoNLL-2011 shared task: Modeling unrestricted coreference in OntoNotes. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning (CoNLL), pages 1–27, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Raghunathan</author>
<author>Heeyoung Lee</author>
<author>Sudarshan Rangarajan</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
<author>Chris Manning</author>
</authors>
<title>A multi-pass sieve for coreference resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP 2010,</booktitle>
<pages>492--501</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="7174" citStr="Raghunathan et al. 2010" startWordPosition="1027" endWordPosition="1030">s of i and j but also any information (head word, named entity type, gender, or number) about the other mentions already linked to i and j in previous steps. Finally, the architecture is highly modular, which means that additional coreference resolution models can be easily integrated. The two stage architecture offers a powerful way to balance both high recall and precision in the system and make use of entity-level information with rule-based architecture. The mention detection stage heavily favors recall, and the following sieves favor precision. Our results here and in our earlier papers (Raghunathan et al. 2010; Lee et al. 2011) show that this design leads to state-of-the-art performance despite the simplicity of the individual components, and that the lack of language-specific lexical features makes the system easy to port to other languages. The intuition is not new; in addition to the prior coreference work mentioned earlier and discussed in Section 6, we draw on classic ideas that have proved to be important again and again in the history of natural language processing. The idea of beginning with the most accurate models or starting with smaller subproblems that allow for high-precision solution</context>
</contexts>
<marker>Raghunathan, Lee, Rangarajan, Chambers, Surdeanu, Jurafsky, Manning, 2010</marker>
<rawString>Raghunathan, Karthik, Heeyoung Lee, Sudarshan Rangarajan, Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky, and Chris Manning. 2010. A multi-pass sieve for coreference resolution. In Proceedings of EMNLP 2010, pages 492–501, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Supervised models for coreference resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>968--977</pages>
<contexts>
<context position="3169" citStr="Rahman and Ng 2009" startWordPosition="432" endWordPosition="435">lution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making them hard to tune and difficult to apply to new problems and genres as well. Rule-based models like Lappin and Leass (1994) wer</context>
<context position="71854" citStr="Rahman and Ng 2009" startWordPosition="11487" endWordPosition="11490">nother mention of the first element in the enumeration. For example, our system marks Zhuhai, Shantou, Xiamen as a unique mention and incorrectly links it to Zhuhai, because they have the same headword. 6. Comparison with Previous Work Algorithms for coreference (or just pronominal anaphora) include rule-based systems (Hobbs 1978; Brennan, Friedman, and Pollard 1987; Lappin and Leass 1994; Baldwin 1995; Zhou and Su 2004; Haghighi and Klein 2009, inter alia), supervised systems (Connolly, Burger, and Day 1994; McCarthy and Lehnert 1995; Kehler 1997; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Rahman and Ng 2009, inter alia), and unsupervised approaches (Cardie and Wagstaff 1999; Haghighi and Klein 2007; Ng 2008; Kobdani et al. 2011a). Our deterministic system draws from all of these, but specifically from three strands in the literature that cross-cut this classification. The idea of doing accurate reference resolution by starting with a set of very highprecision constraints was first proposed for pronominal anaphora in Baldwin’s (1995) important but undercited dissertation. Baldwin suggested using seven high-precision rules as filters, combining them so as to achieve reasonable recall. One of his r</context>
</contexts>
<marker>Rahman, Ng, 2009</marker>
<rawString>Rahman, Altaf and Vincent Ng. 2009. Supervised models for coreference resolution. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 968–977, Suntec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William M Rand</author>
</authors>
<title>Objective criteria for the evaluation of clustering methods.</title>
<date>1971</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>66</volume>
<issue>336</issue>
<contexts>
<context position="42255" citStr="Rand 1971" startWordPosition="6597" endWordPosition="6598">, P = Ei |G |Smi|mi |, F1 = P+RR • CEAF (Constrained Entity Aligned F-measure) (Luo 2005) – metric based on entity alignment. For best alignment g∗ = argmaxg∈GmΦ(g) (Φ(g): total similarity of g, a one-to-one mapping from G: gold mention clusters to S: system mention clusters), R = Φ(g∗) Ei φ(Gi,Gi), P = Φ(g∗) Ei φ(Si,Si), F1 = 2PR P+R If we use φ(G, S) = |G ∩ S|, it is called mention-based CEAF (CEAF-φ3), if we use φ(G, S) = IRRnS |, it is called entity-based CEAF (CEAF-φ4). IRI • BLANC (BiLateral Assessment of NounPhrase Coreference) (Recasens and Hovy 2011) – metric applying the Rand index (Rand 1971) to coreference to deal with imbalance between singletons and coreferent mentions by considering coreference and non-coreference links. Pc = rc rc+wc, Pn = rn rn+wn, Rc = rc rc+wn, Rn = rn rn+wc, Fc = 2PcRcPc+Rc ,Fn = 2PnRn Pn+Rn , BLANC = Fc+Fn 2 (rc: the number of correct coreference links, wc: the number of incorrect coreference links, rn: the number of correct non-coreference links, wn: the number of incorrect non-coreference links) • CoNLL F1 Average of MUC, B3, and CEAF-φ4 F1. This was the official metric in the CoNLL-2011 shared task (Pradhan et al. 2011). 4.3 Experimental Results Table</context>
</contexts>
<marker>Rand, 1971</marker>
<rawString>Rand, William M. 1971. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association, 66(336):846–850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Eduard Hovy</author>
</authors>
<title>Coreference resolution across corpora: Languages, coding schemes, and preprocessing information.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL 2010,</booktitle>
<pages>1--423</pages>
<location>Uppsala.</location>
<contexts>
<context position="57270" citStr="Recasens and Hovy 2010" startWordPosition="9179" endWordPosition="9182">This sieve accounts for approximately 16 CoNLL F1 points improvement, which proves that a significant percentage of mentions in text are indeed repetitions of previously seen concepts. The second big jump in performance, almost 3 CoNLL F1 points, is caused by Sieve 5, strict head match, which is the first pass that compares individual headwords. These results are consistent with error analyses from earlier work which have shown the importance of string match in general (Zhou and Su 2004; Bengtson and Roth 2008; and Recasens, Can, and Jurafsky 2013) and the high precision of strict head match (Recasens and Hovy 2010). Lastly, pronominal coreference resolution (Sieve 10) is responsible for approximately 9.5 CoNLL F1 points improvement. Thus it would be possible to build an even simpler system, with just three sieves, that achieves 97% of the performance of our best model (based on the CoNLL score). This suggests that what is most important for coreference resolution, at least relative to today’s state of the art, is not necessarily the clustering decision mechanism, but rather the entire architecture behind it, and in particular the use of cautious decision-making based on high precision information, entit</context>
</contexts>
<marker>Recasens, Hovy, 2010</marker>
<rawString>Recasens, Marta and Eduard Hovy. 2010. Coreference resolution across corpora: Languages, coding schemes, and preprocessing information. In Proceedings of ACL 2010, pages 1,423–1,432, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Matthew Can</author>
<author>Dan Jurafsky</author>
</authors>
<title>Same referent, different words: Unsupervised mining of opaque coreferent mentions.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL 2013,</booktitle>
<pages>897--906</pages>
<location>Atlanta.</location>
<marker>Recasens, Can, Jurafsky, 2013</marker>
<rawString>Recasens, Marta, Matthew Can, and Dan Jurafsky. 2013. Same referent, different words: Unsupervised mining of opaque coreferent mentions. In Proceedings of NAACL 2013, pages 897–906, Atlanta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Eduard Hovy</author>
</authors>
<title>BLANC: Implementing the Rand index for Computational Linguistics Volume 39, Number 4 coreference evaluation.</title>
<date>2011</date>
<journal>Natural Language Engineering,</journal>
<volume>17</volume>
<issue>4</issue>
<contexts>
<context position="42210" citStr="Recasens and Hovy 2011" startWordPosition="6587" endWordPosition="6590">mi is the system cluster of mention mi, R = Ei |G|Gm |mi |, P = Ei |G |Smi|mi |, F1 = P+RR • CEAF (Constrained Entity Aligned F-measure) (Luo 2005) – metric based on entity alignment. For best alignment g∗ = argmaxg∈GmΦ(g) (Φ(g): total similarity of g, a one-to-one mapping from G: gold mention clusters to S: system mention clusters), R = Φ(g∗) Ei φ(Gi,Gi), P = Φ(g∗) Ei φ(Si,Si), F1 = 2PR P+R If we use φ(G, S) = |G ∩ S|, it is called mention-based CEAF (CEAF-φ3), if we use φ(G, S) = IRRnS |, it is called entity-based CEAF (CEAF-φ4). IRI • BLANC (BiLateral Assessment of NounPhrase Coreference) (Recasens and Hovy 2011) – metric applying the Rand index (Rand 1971) to coreference to deal with imbalance between singletons and coreferent mentions by considering coreference and non-coreference links. Pc = rc rc+wc, Pn = rn rn+wn, Rc = rc rc+wn, Rn = rn rn+wc, Fc = 2PcRcPc+Rc ,Fn = 2PnRn Pn+Rn , BLANC = Fc+Fn 2 (rc: the number of correct coreference links, wc: the number of incorrect coreference links, rn: the number of correct non-coreference links, wn: the number of incorrect non-coreference links) • CoNLL F1 Average of MUC, B3, and CEAF-φ4 F1. This was the official metric in the CoNLL-2011 shared task (Pradhan</context>
</contexts>
<marker>Recasens, Hovy, 2011</marker>
<rawString>Recasens, Marta and Eduard Hovy. 2011. BLANC: Implementing the Rand index for Computational Linguistics Volume 39, Number 4 coreference evaluation. Natural Language Engineering, 17(4):485–510.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emili Sapena</author>
<author>Lluis Padr´o</author>
<author>Jordi Turmo</author>
</authors>
<title>Relaxcor participation in CoNLL-shared task on coreference resolution.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>35--39</pages>
<location>Portland, OR.</location>
<marker>Sapena, Padr´o, Turmo, 2011</marker>
<rawString>Sapena, Emili, Lluis Padr´o, and Jordi Turmo. 2011. Relaxcor participation in CoNLL-shared task on coreference resolution. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 35–39, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heming Shou</author>
<author>Hai Zhao</author>
</authors>
<title>System paper for CoNLL-2012 shared task: Hybrid rule-based algorithm for coreference resolution.</title>
<date>2012</date>
<booktitle>In Joint Conference on EMNLP and CoNLL - Shared Task,</booktitle>
<pages>118--121</pages>
<location>Jeju Island.</location>
<contexts>
<context position="78517" citStr="Shou and Zhao 2012" startWordPosition="12481" endWordPosition="12484">mention. Because our system begins with high-precision clusters, accurate information naturally propagates to later stages. 7. Other Systems Incorporating this Algorithm A number of recent systems have incorporated our algorithm as an important component in resolving coreference. For example, the CoNLL-2012 shared task focused on coreference resolution in a multi-lingual setting: English, Chinese, and Arabic (Pradhan et al. 2012). Forty percent of the systems in the shared task (6 of the 15 systems) made use of our sieve architecture (Chen and Ng 2012; Fernandes, dos Santos, and Milidiu 2012; Shou and Zhao 2012; Xiong and Liu 2012; Yuan et al. 2012; Zhang, Wu, and Zhao 2012), including the systems that were the highest scoring for each of the three languages (Fernandes, dos Santos, and Milidiu 2012; Chen and Ng 2012). The system of Fernandes, dos Santos, and Milidiu (2012) had the highest average score over all languages, and the best score for English and Arabic, by implementing a stacking of two models. Our sieve-based approach was first used to generate mention-link candidates, which are then reranked by a supervised model inspired from dependency parsing. This result demonstrates that our determ</context>
</contexts>
<marker>Shou, Zhao, 2012</marker>
<rawString>Shou, Heming and Hai Zhao. 2012. System paper for CoNLL-2012 shared task: Hybrid rule-based algorithm for coreference resolution. In Joint Conference on EMNLP and CoNLL - Shared Task, pages 118–121, Jeju Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B F Skinner</author>
</authors>
<title>The Behavior of Organisms: An Experimental Analysis.</title>
<date>1938</date>
<publisher>AppletonCentury-Crofts.</publisher>
<contexts>
<context position="7889" citStr="Skinner (1938)" startWordPosition="1139" endWordPosition="1140"> of the individual components, and that the lack of language-specific lexical features makes the system easy to port to other languages. The intuition is not new; in addition to the prior coreference work mentioned earlier and discussed in Section 6, we draw on classic ideas that have proved to be important again and again in the history of natural language processing. The idea of beginning with the most accurate models or starting with smaller subproblems that allow for high-precision solutions combines the intuitions of “shaping” or “successive approximations” first proposed for learning by Skinner (1938), and widely used in NLP (e.g., the successively trained IBM MT models of Brown et al. [1993]) and the “islands of reliability” approaches to parsing and speech recognition [Borghesi and Favareto 1982; Corazza et al. 1991]). The idea of beginning with a high-recall list of candidates that are followed by a series of high-precision filters dates back to one of the earliest architectures in natural language processing, the part of speech tagging algorithm of the Computational Grammar Coder (Klein and Simmons 887 Computational Linguistics Volume 39, Number 4 1963) and the TAGGIT tagger (Greene an</context>
</contexts>
<marker>Skinner, 1938</marker>
<rawString>Skinner, B. F. 1938. The Behavior of Organisms: An Experimental Analysis. AppletonCentury-Crofts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee M Soon</author>
<author>Hwee T Ng</author>
<author>Daniel C Y Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Soon, Wee M., Hwee T. Ng, and Daniel C. Y. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
</authors>
<title>From baby steps to leapfrog: How “less is more” in unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>751--759</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="73448" citStr="Spitkovsky et al. (2010)" startWordPosition="11718" endWordPosition="11721">yntactic constraints (which tend to be higher-precision) before applying semantic constraints. This general idea is known by different names in 908 Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules many NLP applications: Brown et al. (1993) used simple models as “stepping stones” for more complex word alignment models; Collins and Singer (1999) used “cautious” decision list learning for named entity classification; Borghesi and Favareto (1982) and Corazza et al. (1991) used “islands of reliability” approaches to parsing and speech recognition, and Spitkovsky et al. (2010) used “baby steps” for unsupervised dependency parsing, and so forth. Our work extends the intuition of Baldwin and others to the full coreference task (i.e., including mention detection and both nominal and pronominal coreference) and shows that it can result in extremely high-performing resolution when combined with global inference. Our second inspiration comes from two works: Zhou and Su (2004) and Haghighi and Klein (2009), both of which extended Baldwin’s approach to generic nominal coreference. Zhou and Su proposed a multi-agent model that triggers a different agent with a specific set </context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2010</marker>
<rawString>Spitkovsky, Valentin I., Hiyan Alshawi, and Daniel Jurafsky. 2010. From baby steps to leapfrog: How “less is more” in unsupervised dependency parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 751–759, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Jason Eisner</author>
</authors>
<title>Easy-first coreference resolution.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>2--519</pages>
<location>Mumbai.</location>
<contexts>
<context position="81240" citStr="Stoyanov and Eisner (2012)" startWordPosition="12899" endWordPosition="12902">ences between the English and Chinese shared task in the supplied annotations and data: The pronoun sieve was extended to determine gender for Chinese NPs, because the gender gazeteer used for the shared task and for our system only provides gender for English, and a new head-match sieve was added to deal with embedded heads, because the Chinese annotation marked embedded heads differently than the English annotation. 910 Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules A similar easy-first machine learning based approach to entity coreference by Stoyanov and Eisner (2012) also adopts this intuition. Their system greedily merges clusters with the highest score (the current easiest decision), using higher precision classifications (‘easier decisions’) to guide harder decisions later. In summary, recent systems have used the sieve architecture as a component in hybrid machine learning systems, either as a first pass in generating candidate links which are then incorporated in a probabilistic system, or as a second pass for generating links after high-probability mention-pairs have already been linked. These hybrid systems are the state-of-the-art in English, Chin</context>
</contexts>
<marker>Stoyanov, Eisner, 2012</marker>
<rawString>Stoyanov, Veselin and Jason Eisner. 2012. Easy-first coreference resolution. In Proceedings of COLING 2012, pages 2,519–2,534, Mumbai.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Uryupina</author>
<author>Massimo Poesio</author>
<author>Claudio Giuliano</author>
<author>Kateryna Tymoshenko</author>
</authors>
<title>Disambiguation and filtering methods in using web knowledge for coreference resolution.</title>
<date>2011</date>
<booktitle>In FLAIRS Conference,</booktitle>
<pages>317--322</pages>
<contexts>
<context position="76150" citStr="Uryupina et al. 2011" startWordPosition="12119" endWordPosition="12122">actic pass, uses high-precision syntactic information to assign possible coreference. The second, transductive pass identifies Wikipedia articles relevant to the entity mentions in the test set, and then bootstraps a database of hyponyms and other semantically related head pairs from known syntactic patterns for apposition and predicate-nominatives. Haghighi and Klein found that this transductive learning was essential for semantic knowledge to be useful (Aria Haghighi, personal communication); other researchers have found that semantic knowledge derived from Web resources can be quite noisy (Uryupina et al. 2011a). But although transductive learning (learning using test set mentions) thus offers advantages in precision, running a Web-based bootstrapping learner whenever a new data set is encountered is not practical and, ultimately, reduces the usability of this NLP component. Our system thus offers the deterministic simplicity and high performance of the Haghighi and Klein (2009) system without the need for gold mention labels or test-time learning. Furthermore, our work extends the multi-pass model to ten passes and shows that this approach can be naturally combined with an entity-centric model for</context>
<context position="84502" citStr="Uryupina et al. (2011" startWordPosition="13386" endWordPosition="13389">nsive literature in this area, could also help (Vieira and Poesio 2000; Ng and Cardie 2002a; Poesio et al. 2004b; Boyd, Gegg-Harrison, and Byron 2005; Gupta, Purver, and Jurafsky 2007; Bergsma, Lin, and Goebel 2008; Ng 2009). The main conclusion of our error analysis, however, is that the plurality of our errors are due to shallow knowledge of semantics and discourse. This result points to the crucial need for more sophisticated methods of incorporating semantic and discourse knowledge. Unsupervised or semi-supervised approaches to semantics such as Yang and Su (2007), Kobdani et al. (2011b), Uryupina et al. (2011b), Bansal and Klein (2012), or Recasens, Can, and Jurafsky (2013) may point the way forward. Although sieve-based architectures are at the modern state of the art, it is only by incorporating these more powerful models of meaning that we can eventually deal with the full complexity and richness of coreference. 10 http://nlp.stanford.edu/software/dcoref.shtml. 911 Computational Linguistics Volume 39, Number 4 Appendix A: The OntoNotes Named Entity Tag Set PERSON People, including fictional NORP Nationalities or religious or political groups FACILITY Buildings, airports, highways, bridges, etc.</context>
</contexts>
<marker>Uryupina, Poesio, Giuliano, Tymoshenko, 2011</marker>
<rawString>Uryupina, Olga, Massimo Poesio, Claudio Giuliano, and Kateryna Tymoshenko. 2011a. Disambiguation and filtering methods in using web knowledge for coreference resolution. In FLAIRS Conference, pages 317–322,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Uryupina</author>
<author>Massimo Poesio</author>
<author>Claudio Giuliano</author>
<author>Kateryna Tymoshenko</author>
</authors>
<title>Disambiguation and filtering methods in using web knowledge for coreference resolution.</title>
<date>2011</date>
<booktitle>In Proceedings of FLAIRS,</booktitle>
<pages>317--322</pages>
<location>Palm Beach, FL.</location>
<contexts>
<context position="76150" citStr="Uryupina et al. 2011" startWordPosition="12119" endWordPosition="12122">actic pass, uses high-precision syntactic information to assign possible coreference. The second, transductive pass identifies Wikipedia articles relevant to the entity mentions in the test set, and then bootstraps a database of hyponyms and other semantically related head pairs from known syntactic patterns for apposition and predicate-nominatives. Haghighi and Klein found that this transductive learning was essential for semantic knowledge to be useful (Aria Haghighi, personal communication); other researchers have found that semantic knowledge derived from Web resources can be quite noisy (Uryupina et al. 2011a). But although transductive learning (learning using test set mentions) thus offers advantages in precision, running a Web-based bootstrapping learner whenever a new data set is encountered is not practical and, ultimately, reduces the usability of this NLP component. Our system thus offers the deterministic simplicity and high performance of the Haghighi and Klein (2009) system without the need for gold mention labels or test-time learning. Furthermore, our work extends the multi-pass model to ten passes and shows that this approach can be naturally combined with an entity-centric model for</context>
<context position="84502" citStr="Uryupina et al. (2011" startWordPosition="13386" endWordPosition="13389">nsive literature in this area, could also help (Vieira and Poesio 2000; Ng and Cardie 2002a; Poesio et al. 2004b; Boyd, Gegg-Harrison, and Byron 2005; Gupta, Purver, and Jurafsky 2007; Bergsma, Lin, and Goebel 2008; Ng 2009). The main conclusion of our error analysis, however, is that the plurality of our errors are due to shallow knowledge of semantics and discourse. This result points to the crucial need for more sophisticated methods of incorporating semantic and discourse knowledge. Unsupervised or semi-supervised approaches to semantics such as Yang and Su (2007), Kobdani et al. (2011b), Uryupina et al. (2011b), Bansal and Klein (2012), or Recasens, Can, and Jurafsky (2013) may point the way forward. Although sieve-based architectures are at the modern state of the art, it is only by incorporating these more powerful models of meaning that we can eventually deal with the full complexity and richness of coreference. 10 http://nlp.stanford.edu/software/dcoref.shtml. 911 Computational Linguistics Volume 39, Number 4 Appendix A: The OntoNotes Named Entity Tag Set PERSON People, including fictional NORP Nationalities or religious or political groups FACILITY Buildings, airports, highways, bridges, etc.</context>
</contexts>
<marker>Uryupina, Poesio, Giuliano, Tymoshenko, 2011</marker>
<rawString>Palm Beach, FL. Uryupina, Olga, Massimo Poesio, Claudio Giuliano, and Kateryna Tymoshenko. 2011b. Disambiguation and filtering methods in using web knowledge for coreference resolution. In Proceedings of FLAIRS, pages 317–322, Palm Beach, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Renata Vieira</author>
<author>Massimo Poesio</author>
</authors>
<title>An empirically based system for processing definite descriptions.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>4</issue>
<contexts>
<context position="68450" citStr="Vieira and Poesio 2000" startWordPosition="10960" endWordPosition="10963">’s Labor Party wants credit controls. • Parser or NER error: Um alright uh Mister Zalisko do you know anything from your personal experience of having been on the cruise as to what happened? – Mister Zalisko is not recognized as a PERSON • Enumerations: This year, the economies of the five large special economic zones, namely, Shenzhen, Zhuhai, Shantou, Xiamen and Hainan, have maintained strong growth momentum.... A three dimensional traffic frame in Zhuhai has preliminarily taken shape and the investment environment improves daily. add more sophisticated anaphoricity detection to our system (Vieira and Poesio 2000; Ng and Cardie 2002a; Poesio et al. 2004b; Boyd, Gegg-Harrison, and Byron 2005; Gupta, Purver, and Jurafsky 2007; Bergsma, Lin, and Goebel 2008; Ng 2009). Event mentions. Our system was tailored for the resolution of entity coreference and does not have any event-specific features, such as, for example, matching event participants. Furthermore, our model considers only noun phrases as antecedent candidates, thus missing all mentions that are verbal phrases. Therefore, our system misses most coreference links between event mentions. For example, in Table 12 the pronoun That 907 Computational L</context>
<context position="83951" citStr="Vieira and Poesio 2000" startWordPosition="13300" endWordPosition="13303">r suggest that precision-ordered sieves may be an important way to structure rule based systems, and suggests the use of sieves in other NLP tasks for which a variety of very high-precision features can be designed and non-local features can be shared. Likely candidates include relation and event extraction, template slot filling, and author name deduplication. Our error analysis points to a number of places where our system could be improved, including better performance on pronouns. More sophisticated anaphoricity detection, drawing on the extensive literature in this area, could also help (Vieira and Poesio 2000; Ng and Cardie 2002a; Poesio et al. 2004b; Boyd, Gegg-Harrison, and Byron 2005; Gupta, Purver, and Jurafsky 2007; Bergsma, Lin, and Goebel 2008; Ng 2009). The main conclusion of our error analysis, however, is that the plurality of our errors are due to shallow knowledge of semantics and discourse. This result points to the crucial need for more sophisticated methods of incorporating semantic and discourse knowledge. Unsupervised or semi-supervised approaches to semantics such as Yang and Su (2007), Kobdani et al. (2011b), Uryupina et al. (2011b), Bansal and Klein (2012), or Recasens, Can, an</context>
</contexts>
<marker>Vieira, Poesio, 2000</marker>
<rawString>Vieira, Renata and Massimo Poesio. 2000. An empirically based system for processing definite descriptions. Computational Linguistics, 26(4):539–593.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, </marker>
<rawString>Vilain, Marc, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman.</rawString>
</citation>
<citation valid="true">
<title>A model-theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of MUC-6,</booktitle>
<pages>45--52</pages>
<location>Columbia, MD.</location>
<contexts>
<context position="26910" citStr="(1995, 1997)" startWordPosition="4095" endWordPosition="4096">the overall model. Sequence Model Name Pass 1 Speaker Identification Sieve Pass 2 Exact String Match Sieve Pass 3 Relaxed String Match Sieve Pass 4 Precise Constructs Sieve (e.g., appositives) Passes 5–7 Strict Head Match Sieves A–C Pass 8 Proper Head Noun Match Sieve Pass 9 Relaxed Head Match Sieve Pass 10 Pronoun Resolution Sieve 893 Computational Linguistics Volume 39, Number 4 3.3.1 Pass 1 – Speaker Identification. This sieve matches speakers to compatible pronouns, using shallow discourse understanding to handle quotations and conversation transcripts, following the early work of Baldwin (1995, 1997). We begin by identifying speakers within text. In non-conversational text, we use a simple heuristic that searches for the subjects of reporting verbs (e.g., say) in the same sentence or neighboring sentences to a quotation. In conversational text, speaker information is provided in the data set. The extracted speakers then allow us to implement the following sieve heuristics: • (I)s4 assigned to the same speaker are coreferent. • (you)s with the same speaker are coreferent. • The speaker and (I)s in her text are coreferent. Thus for example I, my, and she in the following sentence are corefe</context>
<context position="72288" citStr="(1995)" startWordPosition="11555" endWordPosition="11555">09, inter alia), supervised systems (Connolly, Burger, and Day 1994; McCarthy and Lehnert 1995; Kehler 1997; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Rahman and Ng 2009, inter alia), and unsupervised approaches (Cardie and Wagstaff 1999; Haghighi and Klein 2007; Ng 2008; Kobdani et al. 2011a). Our deterministic system draws from all of these, but specifically from three strands in the literature that cross-cut this classification. The idea of doing accurate reference resolution by starting with a set of very highprecision constraints was first proposed for pronominal anaphora in Baldwin’s (1995) important but undercited dissertation. Baldwin suggested using seven high-precision rules as filters, combining them so as to achieve reasonable recall. One of his rules, for example, resolved pronouns whose antecedents were unique in the discourse, and another resolved pronouns in quoted speech. Baldwin’s idea of starting with highprecision knowledge was adopted by later researchers, such as Ng and Cardie (2002b), who trained to the highest-confidence rather than nearest antecedent, or Haghighi and Klein (2009), who began with syntactic constraints (which tend to be higher-precision) before </context>
</contexts>
<marker>1995</marker>
<rawString>1995. A model-theoretic coreference scoring scheme. In Proceedings of MUC-6, pages 45–52, Columbia, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Vogel</author>
<author>Dan Jurafsky</author>
</authors>
<date>2012</date>
<contexts>
<context position="46907" citStr="Vogel and Jurafsky 2012" startWordPosition="7454" endWordPosition="7457">.6 53.9 73.4 62.2 43.5 32.1 37.0 64.1 70.5 66.5 50.3 Song 58.4 77.6 66.7 46.7 68.4 55.5 54.4 70.2 61.3 43.8 25.9 32.5 66.3 58.8 60.2 49.8 Zhekova 69.2 57.3 62.7 33.5 37.2 35.2 55.5 68.2 61.2 38.3 34.7 36.4 53.5 63.3 54.8 44.3 resources. For the closed track, the organizers provided dictionaries for gender and number information, in addition to parse trees and named entity labels (Pradhan et al. 2011). For the open track, we used the following additional resources: (a) a hand-built list of genders of first names that we created, incorporating frequent names from census lists and other sources (Vogel and Jurafsky 2012) (b) an animacy list (Ji and Lin 2009), (c) a country and state gazetteer, and (d) a demonym list. These resources were also used for the results reported in Table 5. A significant difference between Tables 4 and 5 is that in the former (other than its last block) we used predicted mentions (detected with the algorithm described in Section 3.1), whereas in the latter we used gold mentions. The only reason for this distinction is to facilitate comparison with previous work (all systems listed in Table 5 used gold mention boundaries). The two tables show that, regardless of evaluation corpus and</context>
</contexts>
<marker>Vogel, Jurafsky, 2012</marker>
<rawString>Vogel, Adam and Dan Jurafsky. 2012.</rawString>
</citation>
<citation valid="false">
<authors>
<author>He Said</author>
</authors>
<title>She Said: Gender in the ACL Anthology.</title>
<booktitle>In ACL Workshop on Rediscovering 50 Years of Discoveries,</booktitle>
<pages>33--41</pages>
<location>Jeju Island.</location>
<marker>Said, </marker>
<rawString>He Said, She Said: Gender in the ACL Anthology. In ACL Workshop on Rediscovering 50 Years of Discoveries, pages 33–41, Jeju Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Xiong</author>
<author>Qun Liu</author>
</authors>
<title>Ict: System description for CoNLL-2012.</title>
<date>2012</date>
<contexts>
<context position="78537" citStr="Xiong and Liu 2012" startWordPosition="12485" endWordPosition="12488"> system begins with high-precision clusters, accurate information naturally propagates to later stages. 7. Other Systems Incorporating this Algorithm A number of recent systems have incorporated our algorithm as an important component in resolving coreference. For example, the CoNLL-2012 shared task focused on coreference resolution in a multi-lingual setting: English, Chinese, and Arabic (Pradhan et al. 2012). Forty percent of the systems in the shared task (6 of the 15 systems) made use of our sieve architecture (Chen and Ng 2012; Fernandes, dos Santos, and Milidiu 2012; Shou and Zhao 2012; Xiong and Liu 2012; Yuan et al. 2012; Zhang, Wu, and Zhao 2012), including the systems that were the highest scoring for each of the three languages (Fernandes, dos Santos, and Milidiu 2012; Chen and Ng 2012). The system of Fernandes, dos Santos, and Milidiu (2012) had the highest average score over all languages, and the best score for English and Arabic, by implementing a stacking of two models. Our sieve-based approach was first used to generate mention-link candidates, which are then reranked by a supervised model inspired from dependency parsing. This result demonstrates that our deterministic approach can</context>
</contexts>
<marker>Xiong, Liu, 2012</marker>
<rawString>Xiong, Hao and Qun Liu. 2012. Ict: System description for CoNLL-2012.</rawString>
</citation>
<citation valid="false">
<booktitle>In Joint Conference on EMNLP and CoNLL - Shared Task,</booktitle>
<pages>71--75</pages>
<location>Jeju Island.</location>
<marker></marker>
<rawString>In Joint Conference on EMNLP and CoNLL - Shared Task, pages 71–75, Jeju Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Jian Su</author>
</authors>
<title>Coreference resolution using semantic relatedness information from automatically discovered patterns.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>525--535</pages>
<location>Prague. Yang, Xiaofeng, Jian Su, Jun Lang, Chew L.</location>
<contexts>
<context position="84455" citStr="Yang and Su (2007)" startWordPosition="13378" endWordPosition="13381"> anaphoricity detection, drawing on the extensive literature in this area, could also help (Vieira and Poesio 2000; Ng and Cardie 2002a; Poesio et al. 2004b; Boyd, Gegg-Harrison, and Byron 2005; Gupta, Purver, and Jurafsky 2007; Bergsma, Lin, and Goebel 2008; Ng 2009). The main conclusion of our error analysis, however, is that the plurality of our errors are due to shallow knowledge of semantics and discourse. This result points to the crucial need for more sophisticated methods of incorporating semantic and discourse knowledge. Unsupervised or semi-supervised approaches to semantics such as Yang and Su (2007), Kobdani et al. (2011b), Uryupina et al. (2011b), Bansal and Klein (2012), or Recasens, Can, and Jurafsky (2013) may point the way forward. Although sieve-based architectures are at the modern state of the art, it is only by incorporating these more powerful models of meaning that we can eventually deal with the full complexity and richness of coreference. 10 http://nlp.stanford.edu/software/dcoref.shtml. 911 Computational Linguistics Volume 39, Number 4 Appendix A: The OntoNotes Named Entity Tag Set PERSON People, including fictional NORP Nationalities or religious or political groups FACILI</context>
</contexts>
<marker>Yang, Su, 2007</marker>
<rawString>Yang, Xiaofeng and Jian Su. 2007. Coreference resolution using semantic relatedness information from automatically discovered patterns. In Proceedings of ACL 2007, pages 525–535, Prague. Yang, Xiaofeng, Jian Su, Jun Lang, Chew L.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ting Liu Tan</author>
<author>Sheng Li</author>
</authors>
<title>An entity-mention model for coreference resolution with inductive logic programming.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT</booktitle>
<pages>843--851</pages>
<marker>Tan, Li, 2008</marker>
<rawString>Tan, Ting Liu, and Sheng Li. 2008. An entity-mention model for coreference resolution with inductive logic programming. In Proceedings of ACL-HLT 2008, pages 843–851,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Guodong Zhou</author>
<author>Jian Su</author>
<author>Chew L Tan</author>
</authors>
<title>An NP-cluster approach to coreference resolution.</title>
<date>2004</date>
<contexts>
<context position="3060" citStr="Yang et al. 2004" startWordPosition="412" endWordPosition="415">ks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making them hard to tune </context>
</contexts>
<marker>Yang, Zhou, Su, Tan, 2004</marker>
<rawString>Columbus, OH. Yang, Xiaofeng, Guodong Zhou, Jian Su, and Chew L. Tan. 2004. An NP-cluster approach to coreference resolution.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of COLING 2004,</booktitle>
<pages>219--225</pages>
<location>Geneva.</location>
<marker></marker>
<rawString>In Proceedings of COLING 2004, pages 219–225, Geneva.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Bo Yuan</author>
<author>Qingcai Chen</author>
</authors>
<title>Yang Xiang, Xiaolong Wang, Liping Ge,</title>
<location>Zengjian Liu,</location>
<marker>Yuan, Chen, </marker>
<rawString>Yuan, Bo, Qingcai Chen, Yang Xiang, Xiaolong Wang, Liping Ge, Zengjian Liu,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meng Liao</author>
<author>Xianbo Si</author>
</authors>
<title>A mixed deterministic model for coreference resolution.</title>
<date>2012</date>
<booktitle>In Joint Conference on EMNLP and CoNLL - Shared Task,</booktitle>
<pages>76--82</pages>
<location>Jeju Island.</location>
<marker>Liao, Si, 2012</marker>
<rawString>Meng Liao, and Xianbo Si. 2012. A mixed deterministic model for coreference resolution. In Joint Conference on EMNLP and CoNLL - Shared Task, pages 76–82, Jeju Island.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Xiaotian Zhang</author>
</authors>
<location>Chunyang Wu, and</location>
<marker>Zhang, </marker>
<rawString>Zhang, Xiaotian, Chunyang Wu, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
</authors>
<title>Chinese coreference resolution via ordered filtering.</title>
<date>2012</date>
<booktitle>In Joint Conference on EMNLP and CoNLL - Shared Task,</booktitle>
<pages>95--99</pages>
<location>Jeju Island.</location>
<contexts>
<context position="78517" citStr="Zhao 2012" startWordPosition="12483" endWordPosition="12484">Because our system begins with high-precision clusters, accurate information naturally propagates to later stages. 7. Other Systems Incorporating this Algorithm A number of recent systems have incorporated our algorithm as an important component in resolving coreference. For example, the CoNLL-2012 shared task focused on coreference resolution in a multi-lingual setting: English, Chinese, and Arabic (Pradhan et al. 2012). Forty percent of the systems in the shared task (6 of the 15 systems) made use of our sieve architecture (Chen and Ng 2012; Fernandes, dos Santos, and Milidiu 2012; Shou and Zhao 2012; Xiong and Liu 2012; Yuan et al. 2012; Zhang, Wu, and Zhao 2012), including the systems that were the highest scoring for each of the three languages (Fernandes, dos Santos, and Milidiu 2012; Chen and Ng 2012). The system of Fernandes, dos Santos, and Milidiu (2012) had the highest average score over all languages, and the best score for English and Arabic, by implementing a stacking of two models. Our sieve-based approach was first used to generate mention-link candidates, which are then reranked by a supervised model inspired from dependency parsing. This result demonstrates that our determ</context>
</contexts>
<marker>Zhao, 2012</marker>
<rawString>Hai Zhao. 2012. Chinese coreference resolution via ordered filtering. In Joint Conference on EMNLP and CoNLL - Shared Task, pages 95–99, Jeju Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
<author>Jian Su</author>
</authors>
<title>A high-performance coreference resolution system using a constraint-based multi-agent strategy.</title>
<date>2004</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>522</pages>
<location>Geneva.</location>
<contexts>
<context position="2750" citStr="Zhou and Su 2004" startWordPosition="366" endWordPosition="369">tem thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with mach</context>
<context position="4833" citStr="Zhou and Su 2004" startWordPosition="681" endWordPosition="684"> the advantages of deterministic, rule-based systems. We introduce a model that performs entity-centric coreference, where all mentions that point to the same real-world entity are jointly modeled, in a rich feature space using solely simple, deterministic rules. Our work is inspired both by the seminal early work of Baldwin (1997), who first proposed that a series of high-precision rules could be used to build a high-precision, low-recall system for anaphora resolution, and by more recent work that has suggested that deterministic rules can outperform machine learning models for coreference (Zhou and Su 2004; Haghighi and Klein 2009) and for named entity recognition (Chiticariu et al. 2010). Figure 1 illustrates the two main stages of our new deterministic model: mention detection and coreference resolution, as well as a smaller post-processing step. In the mention detection stage, nominal and pronominal mentions are identified using a high-recall algorithm that selects all noun phrases (NPs), pronouns, and named entity mentions, and then filters out non-mentions (pleonastic it, i-within-i, numeric entities, partitives, etc.). The coreference resolution stage is based on a succession of ten indep</context>
<context position="57138" citStr="Zhou and Su 2004" startWordPosition="9157" endWordPosition="9160">tions, this analysis highlights three significant performance increases. The first is caused by Sieve 2, exact string match. This sieve accounts for approximately 16 CoNLL F1 points improvement, which proves that a significant percentage of mentions in text are indeed repetitions of previously seen concepts. The second big jump in performance, almost 3 CoNLL F1 points, is caused by Sieve 5, strict head match, which is the first pass that compares individual headwords. These results are consistent with error analyses from earlier work which have shown the importance of string match in general (Zhou and Su 2004; Bengtson and Roth 2008; and Recasens, Can, and Jurafsky 2013) and the high precision of strict head match (Recasens and Hovy 2010). Lastly, pronominal coreference resolution (Sieve 10) is responsible for approximately 9.5 CoNLL F1 points improvement. Thus it would be possible to build an even simpler system, with just three sieves, that achieves 97% of the performance of our best model (based on the CoNLL score). This suggests that what is most important for coreference resolution, at least relative to today’s state of the art, is not necessarily the clustering decision mechanism, but rather</context>
<context position="71659" citStr="Zhou and Su 2004" startWordPosition="11455" endWordPosition="11458">enumeration to be the head word of the coordinated nominal phrase (Kuebler, McDonald, and Nivre 2009; de Marneffe and Manning 2008). Because of this, the coordinated phrase is often linked to another mention of the first element in the enumeration. For example, our system marks Zhuhai, Shantou, Xiamen as a unique mention and incorrectly links it to Zhuhai, because they have the same headword. 6. Comparison with Previous Work Algorithms for coreference (or just pronominal anaphora) include rule-based systems (Hobbs 1978; Brennan, Friedman, and Pollard 1987; Lappin and Leass 1994; Baldwin 1995; Zhou and Su 2004; Haghighi and Klein 2009, inter alia), supervised systems (Connolly, Burger, and Day 1994; McCarthy and Lehnert 1995; Kehler 1997; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Rahman and Ng 2009, inter alia), and unsupervised approaches (Cardie and Wagstaff 1999; Haghighi and Klein 2007; Ng 2008; Kobdani et al. 2011a). Our deterministic system draws from all of these, but specifically from three strands in the literature that cross-cut this classification. The idea of doing accurate reference resolution by starting with a set of very highprecision constraints was first proposed for pronominal</context>
<context position="73849" citStr="Zhou and Su (2004)" startWordPosition="11779" endWordPosition="11782">” decision list learning for named entity classification; Borghesi and Favareto (1982) and Corazza et al. (1991) used “islands of reliability” approaches to parsing and speech recognition, and Spitkovsky et al. (2010) used “baby steps” for unsupervised dependency parsing, and so forth. Our work extends the intuition of Baldwin and others to the full coreference task (i.e., including mention detection and both nominal and pronominal coreference) and shows that it can result in extremely high-performing resolution when combined with global inference. Our second inspiration comes from two works: Zhou and Su (2004) and Haghighi and Klein (2009), both of which extended Baldwin’s approach to generic nominal coreference. Zhou and Su proposed a multi-agent model that triggers a different agent with a specific set of deterministic constraints for each anaphor depending on its type and context (e.g., there are different constraints for noun phrases in appositive constructs, definite noun phrases, or bare noun phrases). Some of the constraints’ parameters (e.g., size of candidate search space for a given anaphor type) are learned from training data. The authors showed that this model outperforms the state of t</context>
</contexts>
<marker>Zhou, Su, 2004</marker>
<rawString>Zhou, Guodong and Jian Su. 2004. A high-performance coreference resolution system using a constraint-based multi-agent strategy. In Proceedings of the 16th International Conference on Computational Linguistics (COLING), page 522, Geneva.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>