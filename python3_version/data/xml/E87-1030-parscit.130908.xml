<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000546">
<note confidence="0.384887285714286">
NATURAL AND SIMULATED POINTING
Dagmar Schmauks
Sonderforschungsbereich 314
FB 10.2 - Informatik
Universitat des Saarlandes
D-6600 Saarbrficken 11
WEST GERMANY
</note>
<email confidence="0.943787">
CSnet: schmauks%sbsvax.uucp@germany.csnet
</email>
<sectionHeader confidence="0.961493" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999671">
Referent identification in human conversation is
performed both by describing the objects in question
and by pointing at them. Up till now, only the linguis-
tic component could be simulated in dialog systems. But
recently, technical innovations have made it possible to
&apos;point&apos; at the objects on a display as well.
The paper has two intentions. First, it investigates nat-
ural pointing in more detail and offers some possibilities
to classify the great variety of pointing actions. Then,
it tries to clarify the extent to which pointing by techni-
cal means (especially mouse-clicks) can be regarded as a
simulation of natural pointing or as a functional equiv-
alent. Furthermore, some steps towards even more ac-
curate simulation are briefly mentioned.
</bodyText>
<sectionHeader confidence="0.997681" genericHeader="introduction">
1. Introduction
</sectionHeader>
<subsectionHeader confidence="0.963579">
1.1 Terminological remarks
</subsectionHeader>
<bodyText confidence="0.999807909090909">
The term &apos;deixis&apos; denotes those referential devices
whose interpretation requires a consideration of the sit-
uation of utterance. &apos;Local deixis&apos; means the specifi-
cation of directions, places and objects relative to the
speaker&apos;s actual orientation. The closely related top-
ics &apos;anaphora&apos; and &apos;text-deixis&apos; are not treated in this
paper. One component of deictic actions are linguis-
tic expressions, mainly demonstrative pronouns (&apos;this&apos;,
&apos;that&apos;) and adverbs (&apos;here&apos;, &apos;there&apos;), the other being ex-
tralinguistic means, especially pointing gestures. In this
paper, the latter are represented by the sign &apos;/
&apos;Natural pointing&apos; denotes pointing actions occur-
ring during interhuman dialog. This includes the use of
easily available aids like pencils or pointers. &apos;Simulated
pointing&apos; means the use of technical pointing devices
during man-computer dialog.
Following the terminology of Clark, Schreuder, and
Buttrick (1983), the object pointed at is called the
&apos;demonstratum&apos;, and the descriptive part of the accom-
panying noun phrase (if there is one) is called the &apos;de-
scriptor&apos;. The &apos;referent&apos; is the object to which the whole
pointing act is intended to refer.
</bodyText>
<subsectionHeader confidence="0.653008">
1.2 Motivation
</subsectionHeader>
<bodyText confidence="0.801958714285714">
In face-to-face interaction, pointing gestures are
used frequently and efficiently. Although their referen-
tial power is beyond any doubt, they have, up till now,
hardly ever been treated in more detail. The disciplines
concerned with them are mainly semiotics, linguistics
and psychology.
Recently, the investigation of natural pointing has
also become interesting for the area of Artificial Intel-
ligence. In dialog systems developed to date, objects
could be referred to by more or less complex verbal de-
scriptions or unique artificial identifiers only. Techni-
cal innovations (e.g., high-resolution graphic displays,
touch-sensitive screens, pointing devices such as mice
or joysticks) have made it possible to simulate pointing
gestures to various degrees as well. Multimodal input
is both more comfortable from the user&apos;s point of view,
and a more natural simulation of interhurnan commu-
nication.
Therefore, several systems have been developed re-
cently which allow the combination of verbal descrip-
tions and pointing gestures for referent identification
(see section 5.2). One example is the dialog system
XTRA (a natural-language access system for expert
systems) which is currently under development at the
University of Saarbriicken. Its current application do-
main is to assist the user in filling out a tax form which
is visible on the screen. In section 5.3, XTRA&apos;s deictic
component TACTILUS is shortly presented. The term
&apos;form deixis&apos; shall henceforth denote all those pointing
actions which are performed in order to specify regions
or entries of a form.
An adequate simulation of pointing gestures pre-
supposes a thorough investigation of the regularities
which underlie natural pointing. Therefore, the next
three sections investigate natural pointing in more de-
tail. Section 2 shows that pointing actions (although
functionally similar) are not a uniform phenomenon but
differ with respect to various aspects. Semiotics, lin-
guistics and psychology (study of nonverbal behavior)
are concerned with these investigations. The interde-
pendency of describing and pointing is the topic of sec-
tion 3. This relationship is relevant with regard to Ian-
</bodyText>
<page confidence="0.998022">
179
</page>
<bodyText confidence="0.99995675">
guage processing, because natural and simulated dialog
have a lot of problems in common. More details on
the issues discussed in section 2 and 3 are to be found
in Schrnauks (1986b). Section 4 treats the peculiarities
of form deixis, which is the special type of deixis oc-
curing in the XTRA system. Section 5 tries to clarify
the extent to which technical pointing devices already
in existence can be regarded as a simulation of natural
pointing, or as a functional equivalent. In section 6,
some steps towards even more accurate simulation are
briefly mentioned. (Thus, the last two sections are in-
teresting from a cognitive science point of view as well.)
</bodyText>
<sectionHeader confidence="0.947238" genericHeader="method">
2. Essential features of natural pointing
</sectionHeader>
<bodyText confidence="0.999895333333333">
All efforts to simulate natural pointing have to
take into account that pointing is not a uniform phe-
nomenon. This section shows that the goal &apos;pointing at
something&apos; is achieved by a great variety of body move-
ments. Up till now, only a small part of these can be
simulated (see sections 5 and 6).
</bodyText>
<subsectionHeader confidence="0.98884">
2.1 The variety of pointing actions
</subsectionHeader>
<bodyText confidence="0.999737652173913">
Pointing actions are those body movements which
are performed by a speaker to direct the hearer&apos;s at-
tention to some part of the shared visual field. In the
normal case, both for their encoding and their reception
by the hearer no other means than the human body are
involved. Successful reference by pointing requires that
the addressee pays attention visually to the person who
is pointing. One may suppose, therefore, that linguistic
material such as demonstrative pronouns or deictic ad-
verbs serve as a request to turn one&apos;s face to the speaker.
Pointing can be performed by various body move-
ments, mainly gestures. The most frequent one is the
&apos;finger point&apos;, by which the index finger is extended in
the direction of the object or place indicated. A much
more vague gesture is pointing with the thumb over
one&apos;s shoulder. Other extralinguistic reference devices
are head movements and line of sight.
All these actions are only interpretable as &apos;Look
there!&apos; if the speaker uses a body movement which be-
longs to the stock of signs s/he shares with the hearer.
For example, the African &apos;mouth point&apos; (Kirk, Burton
1981) will not cause the intended reaction on the part
of a European hearer.
</bodyText>
<subsectionHeader confidence="0.994594">
2.2 &apos;Visual&apos; and &apos;tactile&apos; pointing
</subsectionHeader>
<bodyText confidence="0.992055666666667">
Sometimes it is possible not only to point to an ob-
ject, but also to touch an object within reach. In these
cases, pointing becomes much more precise, because
some of the ambiguities of natural pointing are dropped
(see sections 3.3 and 4.1). If there is a physical con-
tact between finger (or pencil etc.) and the indicated
object, the action in question is called &apos;tactile pointing&apos;
as opposed to &apos;visual pointing&apos; where there is no such
contact.
So far, only a small subset of naturally occurring
pointing gestures can be simulated on a terminal screen,
namely certain kinds of tactile pointing gestures. The
emphasis of the remainder of this paper will therefore
rely upon this type of deictic gesture and its relation
to verbal descriptions. However, many observations to
follow will also hold for pointing gestures in general.
Tactile pointing gestures can be classified according
to various aspects of their observable appearance. Some
distinguishing characteristics are:
- body parts involved in execution of the ges-
ture, i.e. number and position of fingers,
</bodyText>
<listItem confidence="0.687358">
- presence or absence of visual guidance,
- use of aids (pencil, pointer, ...),
- complexity of movement (singular, repeated,
multiple pointing), and
- duration and intensity of gesture.
</listItem>
<bodyText confidence="0.991443">
An adequate simulation of tactile pointing has to take
into account at least some of these features.
</bodyText>
<subsectionHeader confidence="0.9281065">
2.3 The relationship between pointing gesture
and demonstratum
</subsectionHeader>
<bodyText confidence="0.99998585">
One open problem is whether there are correlations
between the physical features of pointing gestures and
the objects thereby indicated. Up till now, it cannot be
taken for granted that different persons point in an iden-
tical manner at objects of a specific size, location, shape,
depth of embedding etc. Empirical investigations are
currently being carried out in the XTRA project to an-
swer these questions.
Pointing is called &apos;punctual&apos;, if the movement of the
arm reaches only one apex and thus indicates one sin-
gle point in space. This gesture is only adequate if the
demonstratum is relatively small and motionless. Dur-
ing non-punctual pointing actions, the apex itself per-
forms a complex motion which corresponds in various
ways to the object in question, e.g. follows its motion,
gives its shape or indicates the part of space the object
is supposed to be in.
Furthermore, pointing gestures differ in accuracy.
Pointing with a pencil, pointer etc. can be more precise
than pointing with a finger or the whole hand.
</bodyText>
<sectionHeader confidence="0.5632265" genericHeader="method">
3. The interdependency of describing and point-
ing
</sectionHeader>
<bodyText confidence="0.999913666666667">
In face-to-face interaction, objects are frequently
referred to by gestures and speech in parallel. Simu-
lation of this multimodal process presupposes the inves-
tigation of the specific limitations of each component
and the advantages of their combination. This is done
in the following section.
</bodyText>
<page confidence="0.982947">
180
</page>
<bodyText confidence="0.999945777777778">
There exist both functional and temporal relations
between gestures and phrases. Gestures can substitute,
repeat, contradict, modify or amplify the vocal output
(Scherer 1979). Pointing gestures usually amplify deic-
tic expressions and therefore belong to the kind of ges-
tures called &apos;illustrators&apos; (Ekman, Friesen 1969). Nor-
mally, pointing gestures and their correlated phrases are
produced simultaneously (Levelt, Richardson, and La
Heij 1985).
</bodyText>
<subsectionHeader confidence="0.995529">
3.1 Obligatory and optional pointing gestures
</subsectionHeader>
<bodyText confidence="0.994082333333333">
Some deictic expressions must be accompanied by
a pointing action (or a linguistic equivalent, Sennholz
1985). These include:
</bodyText>
<listItem confidence="0.780047">
- demonstrative pronouns: &apos;this book&apos;,
- heterodeictic local adverbs: &apos;the tree there&apos;,
- personal pronouns with deictic function: &apos;he
did it&apos;, and
- &apos;such&apos;-constructions: &apos;I like such flowers&apos;.
</listItem>
<bodyText confidence="0.999961333333333">
Syntactically, obligatory pointing gestures are embed-
ded in noun phrases or adverbial phrases. In the former
case, they amplify a linguistic attribute. Within its cor-
responding phrase, the location of the pointing gesture
is arbitrary. Usually, it will accompany the most em-
phasized expression.
A lot of expressions can be accompanied by pointing
gestures, in principle all those which refer to visible ob-
jects, events etc. Optional pointing gestures have var-
ious functions, e.g. to mark whether the speaker uses
adverbs deictically or relative to another orientation sys-
tem.
</bodyText>
<subsectionHeader confidence="0.999789">
3.2 Pointing simplifies describing
</subsectionHeader>
<bodyText confidence="0.999995941176471">
The use of purely verbal descriptions can fail for
variousâ€˜reasons. For example, some descriptions may
not completely specify their referents: They can be
wrong, inconsistent or too subjective. But even ade-
quate descriptions can cause misinterpretations. One
extreme would include descriptions with little inten-
sion and therefore too wide an extension, such &apos;whatsit&apos;
or &apos;thingamajig&apos; (generally used if one doesn&apos;t know a
more precise descriptor). The other extreme includes
very detailed and complex descriptions which are dif-
ficult to process (e.g., &apos;the small red book on the left
side in the second shelf from the top&apos;). A closely related
problem is that of technical terms used in conversation
with non-specialists: Although the description may be
totally adequate, the hearer is not able to understand it.
Therefore, verbal description alone may be too gen-
eral or too specific. Within this range, the speaker has
the task of specifying the referent in enough detail with-
out constructing a verbal expression which is too com-
plex.
One frequent solution is the use of pointing gestures.
They allow successful reference without the need of to-
tally specified verbal descriptions (Pechmann, Deutsch
1982). The use of pointing shortens the accompany-
ing descriptor and the loss of intension is compensated
by the gesture. General nouns amplified by pointing
gestures can substitute for more specific nouns (e.g., &apos;I
like cornflowers&apos; is replacable by &apos;I like these / flow-
ers&apos;). Thus, additional pointing allows unambiguous
(or at least relatively precise) referent specification even
if one doesn&apos;t know an exact descriptor. The process of
referent identification is speeded up, because the orien-
tation to the object&apos;s direction and the processing of the
verbal description are performed simultaneously.
</bodyText>
<subsectionHeader confidence="0.998305">
3.3 Describing disambiguates pointing
</subsectionHeader>
<bodyText confidence="0.9754805">
One essential drawback of pointing gestures is their
inevitable dependency on the here-and-now. Further-
more, pointing without describing the referent is fun-
damentally ambiguous (Wittgenstein 1958). Referent
identification involves the following three steps: First,
one has to recognize the direction indicated. This re-
quires facing the speaker and following his/her gesture
with gaze and eventually a body turn. Thus, the deictic
spaces of both participants are co-oriented by physical
means and not by mental acts (e.g., transformation of
&apos;left&apos; into &apos;right&apos; and vice versa, see Klein 1978).
The second task is the identification of the object
indicated. Usually, there is more than one object sit-
uated in any one direction. Problems arise if possible
demonstrata are:
- next to each other,
- behind each other, or
- embedded in one another.
In these cases, unambiguous reference requires the
naming or describing of the demonstratum.
Thirdly, one has to decide what aspect of the object is
being referred to. Like the second step, this is usually
done by consideration of the descriptor. For example,
pointing at a moving car can refer to its colour (&apos; Nice
green / , isn&apos;t it?&apos;) or its kind of motion (&apos;This speed
/ causes lots of accidents&apos;) etc. Pointing at sets of
objects can even refer to aspects of higher degree such
as number (&apos;I&apos;d like to have that many / books&apos;).
</bodyText>
<sectionHeader confidence="0.973995" genericHeader="method">
4. Form deixis
</sectionHeader>
<bodyText confidence="0.999759428571429">
Pointing at two-dimensional objects (forms, dia-
grams, maps, pictures etc.) differs in various aspects
from pointing at objects within the entire visual field.
This offers a definite advantage from a linguistic point
of view: Some problems of local deixis are reduced in
complexity without the communicative setting having
to become unnatural (Schmauks 1986a). Furthermore,
</bodyText>
<page confidence="0.993702">
181
</page>
<bodyText confidence="0.999507">
this domain is interesting from an artificial intelligence
point of view, since some of the pointing actions with
regard to forms can now be simulated on a terminal
screen.
</bodyText>
<subsectionHeader confidence="0.998935">
4.1 Reduction of problems
</subsectionHeader>
<bodyText confidence="0.9999904375">
Following Baler&apos;s terminology (1982), form deixis
belongs to the kind of deixis called &apos;demonstratio ad
oculos&apos;, because all objects pointed at are visible. Fur-
thermore, it represents an example of the &apos;canonical sit-
uation of utterance&apos; (Lyons 1977): All the participants
are co-present and can thus mutually perceive their
(pointing) gestures etc. Form deixis is relatively pre-
cise, because tactile pointing is always possible. Precise
pointing at small objects (e.g. single words) is frequently
performed by using a pencil etc., larger areas by encir-
cling them. The ambiguity with regard to objects be-
hind each other does not occur, because the deictic space
is only two-dimensional. If speaker and hearer are sit-
uated side by side, their deictic fields are co-oriented.
Therefore, this position makes cooperation easier, and
thus is the most advantageous one.
</bodyText>
<subsectionHeader confidence="0.993729">
4.2 Remaining problems
</subsectionHeader>
<bodyText confidence="0.9994585">
Although form deixis implies a reduction of prob-
lems, referent identification has not at all become a triv-
ial task. It cannot be taken for granted that demonstra-
turn and referent are identical. This might be due to the
fact that the speaker has mistakenly pointed at a wrong
place because s/he doesn&apos;t know the referent&apos;s actual
location or misses the target by accident. Other diver-
gencies emerge intentionally: The speaker doesn&apos;t want
to cover the referent and therefore points a bit lower.
Other essential problems arise because there exist
subset relations among form regions. For example, the
demonstratum can be a part of the referent - this is re-
ferred to as &apos;pars- pro- tow deixis&apos;. In those cases, one
must take into account the verbal description to resolve
the ambiguity.
Furthermore, pointing at one form region can (de-
pending on linguistic context) refer to three different
entities:
</bodyText>
<listItem confidence="0.992561">
1. The form region itself: &apos;What is to be en-
tered here?
2. The actual entry: &apos;I want to increase this
sum.&apos;
3. Correlated concepts: &apos;Are these expenses to
be verified?&apos;
</listItem>
<sectionHeader confidence="0.633108" genericHeader="method">
5. Simulated pointing
</sectionHeader>
<bodyText confidence="0.999523027777778">
This section investigates the extent to which some
features of natural pointing can already be simulated in
dialog systems developed to date. In section 6, some
steps towards more accurate simulation are briefly sug-
gested.
5.1 Different ways of simulating pointing ges-
tures
Face-to-face interaction is performed by gestures
and speech in parallel. In many domains (e.g. form
deixis), objects are often and efficiently referred to by
pointing gestures. Thus, dialog systems will become
more natural if the user has the possibility of &apos;pointing&apos;
at the objects which are visible on the screen.
The goal &apos;reference by pointing&apos; can be achieved
by various strategies. One fundamental decision must
be made first: whether one wants to simulate natural
pointing (as is the aim of TACTILUS) or to offer func-
tional equivalents. In the former case, there is the pre-
supposed but questionable demand that man-machine-
communication should be performed by the same means
as interhuman communication.
If the main emphasis relies on simulation, then the
pointing device and its use must correspond to natural
pointing as accurately as possible. In this case, the most
adequate simulation will be pointing at a touch-sensitive
screen (see section 6). But other devices (e.g. input via
mouse-clicks) can also partially simulate natural point-
ing (see sections 5.3).
Functional equivalents to natural pointing include
the following devices: Framing the referent or zooming
in on it, highlighting it in different colours etc. (see
Fihnrich et al. 1984). On the one hand, the system
can &apos;point&apos; by these means. On the other hand, the
user gets immediate feedback as to whether the system
has recognized the intended referent. This advantage is
paid for by the loss of &apos;naturalness&apos;.
</bodyText>
<subsectionHeader confidence="0.998326">
5.2 Historical remarks
</subsectionHeader>
<bodyText confidence="0.999915333333333">
Multimodal input, especially the possibility of
pointing at visible objects, offers certain crucial ad-
vantages. For example, the use of simple pointing
actions was already possible in the following systems:
SCHOLAR (Carbonell 1970) allows pointing gestures
in order to specify regions of geographic maps. Point-
ing in Woods&apos; (1979) system, combined with simple
descriptions, refers to substructures of a parse tree dis-
played on the screen. In NLG (Brown et al. 1979), the
user can draw simple geometric objects through descrip-
tive NL-commands and simultaneous tactile touches
on the screen. SDMS (Bolt 1980) enables the user to
create and manipulate geometric objects on a screen-
arrangement called &apos;MEDIA ROOM&apos;. In all those sys-
tems, there exist predefined relations between the point-
ing gesture and its demonstratum. Referent identifica-
tion is not dependent on context etc.
Currently, several projects are investigating prob-
lems concerning the integration of pointing actions
and NL input, e.g.: In NLMENU (Thompson 1986),
the user can select parts of a street map by means
</bodyText>
<page confidence="0.996433">
182
</page>
<bodyText confidence="0.9994173">
of a mouse-controlled rubber-band technique. Hayes
(1986) outlines the integration of a deictic component
into the Language Craft System, which should allow
the user to click on items on the screen, e.g. the ma-
chines on a blueprint of a factory floor. ACORD in-
vestigates pointing actions with respect to various two-
dimensional objects, e.g a map of the planetary system
(Hanne, Hoepelmann, and Fahnrich 1986) and a form
for university registration (Wetzel, Hanne, and Hoe-
pelmann 1987).
</bodyText>
<subsectionHeader confidence="0.998739">
5.3 Pointing actions in TACTILUS
</subsectionHeader>
<bodyText confidence="0.999388857142857">
One aim of XTRA is the integration of (typed) ver-
bal descriptions and pointing gestures (currently real-
ized by mouse-clicks) for referent identification (Kobsa
et al. 1986). The user should be able to efficiently
refer to objects on the screen, even when s/he uses
underspecified descriptions and/or imprecise pointing
gestures (Allgayer, Reddig 1986). Hence the process
of specifying referents is speeded up and requires less
knowledge of specialist terms.
The deictic component of XTRA (called TAC-
TILUS) is completely implemented on a Symbolics Lisp
Machine (Allgayer 1986). It offers four types of point-
ing gestures which differ in accuracy. They correspond
to three modes of punctual pointing (with pencil, in-
dex finger, or hand) and to the possibility of encircling
the demonstratum. Thus, pointing becomes a two-step
process: First, one has to select the intended degree of
preciseness and then to &apos;point&apos;.
These pointing actions are natural because of their
ambiguity: There is no predefined relation between the
spot where the mouse is activated and the object which
is thereby referred to. Therefore, the system has to take
into account additional knowledge sources for referent
identification, e.g. verbal descriptions and dialog mem-
ory. From the user&apos;s point of view, the essential indi-
cation of this naturalness is the lack of visual feedback.
In analogy to natural pointing, the identified referent is
not highlighted.
</bodyText>
<subsectionHeader confidence="0.917062">
5.4 Problems in processing mixed input
</subsectionHeader>
<bodyText confidence="0.999840833333333">
One essential problem is to assign a mouse-click to
its corresponding verbal constituent. This task is not
trivial since there is no guarantee that the user &apos;points&apos;
within the range of the deictic expression. Possibly, the
click occurs too late because of the user being inatten-
tive, not familiar with the system etc. One example is:
</bodyText>
<subsectionHeader confidence="0.879884">
What is this sum above the last entry / ?
</subsectionHeader>
<bodyText confidence="0.986905076923077">
Here, the pointing action occurs next to &apos;the last entry&apos;.
But this is an anaphor and doesn&apos;t need to be amplified.
On the other hand, there is the deictic expression &apos;this
sum&apos; without its correlated obligatory pointing action.
Therefore, the system has to recognize that &apos;/ &apos; be-
longs to &apos;this sum&apos;. This problem is aggravated by the
fact that the words &apos;here&apos;/&apos;there&apos; and &apos;this&apos;/&apos; that&apos; are
not only the most frequent deictic expressions but have
anaphoric and text- deictic readings as well.
Matching mouse-clicks and phrases becomes even more
difficult if a singlc utterance requires more than one
pointing action. This case is called &apos;multiple pointing&apos;.
Examples include:
</bodyText>
<subsectionHeader confidence="0.600716">
This sum I would prefer to enter here.
</subsectionHeader>
<bodyText confidence="0.9878065">
Hayes (1986) assumes that pointing actions are per-
formed in the same order as their corresponding
phrases. But until this hypothesis is confirmed empiri-
cally, it can only serve as a heuristic rule.
As soon as reference by pointing is possible, the use of
incomplete expressions will increase. In these cases, ad-
ditional knowledge sources are needed for referent iden-
tification, like descriptor analysis and case frame analy-
sis (Kobsa et al. 1986). For example, the expression
&apos;this&apos; in the sentence &apos;I want to add this/ &apos;surely refers
to a number in the present domain, because &apos;add&apos; is cat-
egorized as an action to be performed with numbers.
</bodyText>
<subsectionHeader confidence="0.987813">
5.5 Problems in generating mixed output
</subsectionHeader>
<bodyText confidence="0.999828666666667">
If the pointing actions of the system are also con-
ceived as a simulation of natural pointing, the user is
confronted with the same problems that have already
been identified in the last subsection (Reithinger 1987).
But, whereas multiple pointing can be simulated during
input, there seems to be no adequate mode for simulat-
ing it during output as well: In normal communication,
the hearer doesn&apos;t need to watch the speaker in order to
understand him/her unless the occurence of a deictic ex-
pression (or the sound of touching during tactile point-
ing) demands his/her visual attentiveness. Also, during
typed dialog, there is no need to observe the output sen-
tences permanently. In the case of multiple pointing,
the possibility cannot be ruled out that the user might
fail to notice one of the pointing actions.
</bodyText>
<sectionHeader confidence="0.889741" genericHeader="method">
6. Prospects of more natural simulation
</sectionHeader>
<bodyText confidence="0.997808166666667">
Up till now, only certain kinds of tactile pointing
gestures can be simulated on a screen. Negroponte
(1981) outlines some future plans, e.g. the considera-
tion of non-tactile actions such as eye tracking and body
movements.
Simulation of tactile pointing gestures by mouse-
clicks has some serious limitations with regard to its
&apos;naturalness&apos;. Empirical investigations are needed to
determine the extent to which mouse-clicks can be re-
garded as an equivalent of natural pointing. These
investigations are currently carried out in the XTRA
project.
</bodyText>
<page confidence="0.997303">
183
</page>
<bodyText confidence="0.99995837037037">
In the case of natural pointing, the choice of a more
or less precise pointing gesture is made automatically
rather than consciously. But in TACTILUS, the user
has to select explicitly the intended degree of accuracy.
Empirical investigations must examine whether the user
regards this as a disadvantage.
Furthermore, pointing via mouse-clicks differs from
natural tactile pointing, because there is no physical
contact between finger and demonstratum. A better
solution would be the use of a touch-sensitive screen
on which &apos;real-world gestures&apos; (see Minsky 1984) are
possible. Touch-sensitive screens allow highly natural
pointing gestures (see Pickering 1986), but have some
shortcomings. e.g. a restricted degree of resolution.
A problem just as serious as the aforementioned is
the temporal dissociation of a pointing gesture and its
corresponding phrase. This problem would be soluble if
the system would accept input via voice. But this alone
wouldn&apos;t be sufficient: There is no guarantee that spo-
ken phrases and correlated mouse-clicks occur simul-
taneously. Furthermore, current voice-input systems
have too small a vocabulary and cannot process fluent
speech.
Therefore, the most adequate simulation would be
the combination of voice input/output and gestures on
a much-sensitive screen. However, the state of the art
with respect to the required devices is not yet sufficient.
</bodyText>
<sectionHeader confidence="0.976889" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999675">
The research described in this paper has been
funded by the German Science Foundation (DFG) in its
Special Collaborative Program on AI and Knowledge-
Based Systems (SFB 314). I am indepted to my col-
leagues of the XTRA project for their helpful comments
on an earlier version of this paper.
</bodyText>
<sectionHeader confidence="0.982205" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999292897959184">
Allgayer, J. (1986): Eine Graphikkomponente zur
Integration von Zeigehandlungen in nattirlich-
sprachliche KI-Systeme. Proceedings der 16. GI-
Jahrestagung. Berlin etc.: Springer.
Allgayer, J. and C. Reddig (1986): Processing De-
scriptions containing Words and Gestures. A Sys-
tem Architecture. In: C.-R. Rollinger, Hrsg.:
GWAI/OGAI 1986. Berlin etc.: Springer.
Bolt, R. A. (1980): &apos;Put-That-There&apos;: Voice and
Gesture at the Graphics Interface. Computer
Graphics 14, 262-270.
Brown, D. C. et al. (1979): An Experimental
Graphics System with Natural Language Input.
Computer and Graphics 4, 13-22.
Bilhler, K. (1982): The Deictic Field of Language
and Deictic Words. Abridged translation of K.
Bfihler (1934): Sprachtheorie, part 2, chapters 7
and 8. In: R. J. Jarvella and W. Klein, eds.:
Speech, Place, and Action. Chichester etc.: Wi-
ley.
Carbonell, J. R. (1970): Mixed-Initiative Man-
Computer Dialogues. Cambridge, MA: Bolt, Be-
ranek and Newman.
Clark, H. H.. R. Schreuder and S. Buttrick
(1983): Common Ground and the Understanding
of Demonstrative Reference. Journal of Verbal
Learning and Verbal Behavior 22, 245-258.
Ekman, P. and W. V. Friesen (1969): The Reper-
toire of Nonverbal Behavior: Categories, Origins,
and Coding. Semiotica 1, 49-98.
Fahnrich, K. P. et al. (1984): The Role of
Graphics. Technical Report E3/GR, FhG, IAO,
Stuttgart.
Hanne, K. H., J. P. Hoepelmann und K. P.
Fahnrieh (1986): Combined Graphics/Natural
Language Interfaces to Knowledge Based Sys-
tems. Proceedings of the Artificial Intelligence
and Advanced Computer Technology Confer-
ence, Wiesbaden, West Germany.
Hayes, P. J. (1986): Steps towards Integrating
Natural Language and Graphical Interaction for
Knowledge-based Systems. Proceedings of the
7th European Conference on Artificial Intelli-
gence, Brighton, England.
Kirk, L. and M. Burton (1981): Physical ver-
sus Semantic Classification of Nonverbal Forms:
A Cross-Cultural Experiment. In: A. Kendon,
ed.: Nonverbal Communication, Interaction, and
Gesture.
</reference>
<page confidence="0.988406">
184
</page>
<reference confidence="0.993078327868852">
Klein, W. (1978): Wo ist hier? Praliminarien zu
einer Untersuchting der lokalen Deixis. Lingui-
stische Berichte 58, 18-40.
Kobsa, A. et al. (1986): Combining Deictic Ges-
tures and Natural Language for Referent Iden-
tification. Proceedings of the 11th International
Conference on Computational Linguistics, Bonn.
West Germany.
Levelt, W. J. M., G. Richardson and W. La Heij
(1985): Pointing and Voicing in Deictic Expres-
sions. Journal of Memory and Language 24, 133-
164.
Lyons, J. (1977): Semantics, Vols 1 and 2. Cam-
bridge: Cambridge University Press.
Minsky, M. (1984): Manipulating Simulated Ob-
jects with Real-world Gestures using a Force and
Position Sensitive Screen. Computer Graphics
18, 195-203.
Negroponte, N. (1981): Media Room. Proceedings
of the Society for Information Display 22, 109-
113.
Pechmann, T. and W. Deutsch (1982): The De-
velopment of Verbal and Nonverbal Devices for
Reference. Journal of Experimental Child Psy-
chology 34, 330-341.
Pickering, J. A. (1986): Touch-sensitive screens:
the technologies and their application. Int. J.
Man-Machine Studies 25, 249-269.
Reithinger, N. (1987): Generating Referring Ex-
pressions and Pointing Gestures. To appear in:
G. Kempen, ed.: Natural Language Generation.
Dordrecht: Kluwer
Scherer, K. R. (1979): Die Funktionen des Non-
verbalen Verhaltens im Gesprach. In: K. R.
Scherer und H. G. Wallbott, Hrsg.: Nonverbale
Kommunikation. Weinheim/Basel: Beltz.
Schmauks, D. (1986a): Formulardeixis .und ihre
Simulation auf dem Bildschirm. EM Uberblick
aus linguistischer Sicht. Memo Nr. 4, Sonder-
forschungsbereich 314, Dept. of Computer Sci-
ence, University of Saarbriicken, FR Germany.
Sclunauks, D. (1986b): Form und Funktion von
Zeigegesten. EM interdisziplinarer Uberblick.
Bericht Nr. 10, Sonderforschungsbereich 314,
Dept. of Computer Science, University of
Saarbriicken, FR Germany.
Sennholz, K. (1985): Grundziige der Deixis.
Bochum: Brockmeyer.
Thompson, C. (1986): Building Menu-Based Nat-
ural Language Interfaces. Texas Engineering
Journal 3, 140-150.
Wetzel, R. P., K. H. Hanne and J. P. Hoe-
pelmann (1987): DIS-QUE: Deictic Interaction
System-Query Environment. LOKI Report KR-
GR 5.3/KR-NL 5, FhG, IAO, Stuttgart.
Wittgenstein, L. (1958): Philosophical investiga-
tions. Oxford: Blackwell.
Woods, W. A. et al. (1979): Research in Natural
Language Understanding: Annual Report. TR
4274, Bolt, Beranek and Newman, Cambridge,
MA.
</reference>
<page confidence="0.998836">
185
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.051090">
<title confidence="0.9898">NATURAL AND SIMULATED POINTING</title>
<author confidence="0.58561">Dagmar Schmauks</author>
<address confidence="0.379251">Sonderforschungsbereich 314 FB 10.2 - Informatik</address>
<affiliation confidence="0.783842">Universitat des Saarlandes</affiliation>
<address confidence="0.5360145">D-6600 Saarbrficken 11 WEST GERMANY</address>
<email confidence="0.918272">CSnet:schmauks%sbsvax.uucp@germany.csnet</email>
<abstract confidence="0.996940933333333">Referent identification in human conversation is performed both by describing the objects in question and by pointing at them. Up till now, only the linguistic component could be simulated in dialog systems. But recently, technical innovations have made it possible to &apos;point&apos; at the objects on a display as well. The paper has two intentions. First, it investigates natural pointing in more detail and offers some possibilities to classify the great variety of pointing actions. Then, it tries to clarify the extent to which pointing by technical means (especially mouse-clicks) can be regarded as a simulation of natural pointing or as a functional equivalent. Furthermore, some steps towards even more accurate simulation are briefly mentioned.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allgayer</author>
</authors>
<title>Eine Graphikkomponente zur Integration von Zeigehandlungen in nattirlichsprachliche KI-Systeme.</title>
<date>1986</date>
<booktitle>Proceedings der 16. GIJahrestagung. Berlin etc.:</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="20542" citStr="Allgayer 1986" startWordPosition="3230" endWordPosition="3231">987). 5.3 Pointing actions in TACTILUS One aim of XTRA is the integration of (typed) verbal descriptions and pointing gestures (currently realized by mouse-clicks) for referent identification (Kobsa et al. 1986). The user should be able to efficiently refer to objects on the screen, even when s/he uses underspecified descriptions and/or imprecise pointing gestures (Allgayer, Reddig 1986). Hence the process of specifying referents is speeded up and requires less knowledge of specialist terms. The deictic component of XTRA (called TACTILUS) is completely implemented on a Symbolics Lisp Machine (Allgayer 1986). It offers four types of pointing gestures which differ in accuracy. They correspond to three modes of punctual pointing (with pencil, index finger, or hand) and to the possibility of encircling the demonstratum. Thus, pointing becomes a two-step process: First, one has to select the intended degree of preciseness and then to &apos;point&apos;. These pointing actions are natural because of their ambiguity: There is no predefined relation between the spot where the mouse is activated and the object which is thereby referred to. Therefore, the system has to take into account additional knowledge sources </context>
</contexts>
<marker>Allgayer, 1986</marker>
<rawString>Allgayer, J. (1986): Eine Graphikkomponente zur Integration von Zeigehandlungen in nattirlichsprachliche KI-Systeme. Proceedings der 16. GIJahrestagung. Berlin etc.: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allgayer</author>
<author>C Reddig</author>
</authors>
<title>Processing Descriptions containing Words and Gestures. A System Architecture.</title>
<date>1986</date>
<publisher>Springer.</publisher>
<location>In: C.-R. Rollinger, Hrsg.: GWAI/OGAI</location>
<marker>Allgayer, Reddig, 1986</marker>
<rawString>Allgayer, J. and C. Reddig (1986): Processing Descriptions containing Words and Gestures. A System Architecture. In: C.-R. Rollinger, Hrsg.: GWAI/OGAI 1986. Berlin etc.: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Bolt</author>
</authors>
<title>Put-That-There&apos;: Voice and Gesture at the Graphics Interface.</title>
<date>1980</date>
<journal>Computer Graphics</journal>
<volume>14</volume>
<pages>262--270</pages>
<contexts>
<context position="18976" citStr="Bolt 1980" startWordPosition="2985" endWordPosition="2986">rks Multimodal input, especially the possibility of pointing at visible objects, offers certain crucial advantages. For example, the use of simple pointing actions was already possible in the following systems: SCHOLAR (Carbonell 1970) allows pointing gestures in order to specify regions of geographic maps. Pointing in Woods&apos; (1979) system, combined with simple descriptions, refers to substructures of a parse tree displayed on the screen. In NLG (Brown et al. 1979), the user can draw simple geometric objects through descriptive NL-commands and simultaneous tactile touches on the screen. SDMS (Bolt 1980) enables the user to create and manipulate geometric objects on a screenarrangement called &apos;MEDIA ROOM&apos;. In all those systems, there exist predefined relations between the pointing gesture and its demonstratum. Referent identification is not dependent on context etc. Currently, several projects are investigating problems concerning the integration of pointing actions and NL input, e.g.: In NLMENU (Thompson 1986), the user can select parts of a street map by means 182 of a mouse-controlled rubber-band technique. Hayes (1986) outlines the integration of a deictic component into the Language Craf</context>
</contexts>
<marker>Bolt, 1980</marker>
<rawString>Bolt, R. A. (1980): &apos;Put-That-There&apos;: Voice and Gesture at the Graphics Interface. Computer Graphics 14, 262-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Brown</author>
</authors>
<title>An Experimental Graphics System with Natural Language Input.</title>
<date>1979</date>
<journal>Computer and Graphics</journal>
<volume>4</volume>
<pages>13--22</pages>
<marker>Brown, 1979</marker>
<rawString>Brown, D. C. et al. (1979): An Experimental Graphics System with Natural Language Input. Computer and Graphics 4, 13-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bilhler</author>
</authors>
<title>The Deictic Field of Language and Deictic Words. Abridged translation of K. Bfihler</title>
<date>1982</date>
<volume>2</volume>
<editor>In: R. J. Jarvella and W. Klein, eds.: Speech, Place, and Action. Chichester etc.:</editor>
<publisher>Sprachtheorie,</publisher>
<marker>Bilhler, 1982</marker>
<rawString>Bilhler, K. (1982): The Deictic Field of Language and Deictic Words. Abridged translation of K. Bfihler (1934): Sprachtheorie, part 2, chapters 7 and 8. In: R. J. Jarvella and W. Klein, eds.: Speech, Place, and Action. Chichester etc.: Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Carbonell</author>
</authors>
<title>Mixed-Initiative ManComputer Dialogues.</title>
<date>1970</date>
<location>Cambridge, MA: Bolt, Beranek and Newman.</location>
<contexts>
<context position="18601" citStr="Carbonell 1970" startWordPosition="2925" endWordPosition="2926">e following devices: Framing the referent or zooming in on it, highlighting it in different colours etc. (see Fihnrich et al. 1984). On the one hand, the system can &apos;point&apos; by these means. On the other hand, the user gets immediate feedback as to whether the system has recognized the intended referent. This advantage is paid for by the loss of &apos;naturalness&apos;. 5.2 Historical remarks Multimodal input, especially the possibility of pointing at visible objects, offers certain crucial advantages. For example, the use of simple pointing actions was already possible in the following systems: SCHOLAR (Carbonell 1970) allows pointing gestures in order to specify regions of geographic maps. Pointing in Woods&apos; (1979) system, combined with simple descriptions, refers to substructures of a parse tree displayed on the screen. In NLG (Brown et al. 1979), the user can draw simple geometric objects through descriptive NL-commands and simultaneous tactile touches on the screen. SDMS (Bolt 1980) enables the user to create and manipulate geometric objects on a screenarrangement called &apos;MEDIA ROOM&apos;. In all those systems, there exist predefined relations between the pointing gesture and its demonstratum. Referent ident</context>
</contexts>
<marker>Carbonell, 1970</marker>
<rawString>Carbonell, J. R. (1970): Mixed-Initiative ManComputer Dialogues. Cambridge, MA: Bolt, Beranek and Newman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H R Schreuder Clark</author>
<author>S Buttrick</author>
</authors>
<title>Common Ground and the Understanding of Demonstrative Reference.</title>
<date>1983</date>
<journal>Journal of Verbal Learning and Verbal Behavior</journal>
<volume>22</volume>
<pages>245--258</pages>
<marker>Clark, Buttrick, 1983</marker>
<rawString>Clark, H. H.. R. Schreuder and S. Buttrick (1983): Common Ground and the Understanding of Demonstrative Reference. Journal of Verbal Learning and Verbal Behavior 22, 245-258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ekman</author>
<author>W V Friesen</author>
</authors>
<title>The Repertoire of Nonverbal Behavior: Categories, Origins, and Coding.</title>
<date>1969</date>
<journal>Semiotica</journal>
<volume>1</volume>
<pages>49--98</pages>
<marker>Ekman, Friesen, 1969</marker>
<rawString>Ekman, P. and W. V. Friesen (1969): The Repertoire of Nonverbal Behavior: Categories, Origins, and Coding. Semiotica 1, 49-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K P Fahnrich</author>
</authors>
<title>The Role of Graphics.</title>
<date>1984</date>
<tech>Technical Report E3/GR,</tech>
<location>FhG, IAO, Stuttgart.</location>
<marker>Fahnrich, 1984</marker>
<rawString>Fahnrich, K. P. et al. (1984): The Role of Graphics. Technical Report E3/GR, FhG, IAO, Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K H Hanne</author>
<author>J P Hoepelmann und K P Fahnrieh</author>
</authors>
<title>Combined Graphics/Natural Language Interfaces to Knowledge Based Systems.</title>
<date>1986</date>
<booktitle>Proceedings of the Artificial Intelligence and Advanced Computer Technology Conference,</booktitle>
<location>Wiesbaden, West</location>
<marker>Hanne, Fahnrieh, 1986</marker>
<rawString>Hanne, K. H., J. P. Hoepelmann und K. P. Fahnrieh (1986): Combined Graphics/Natural Language Interfaces to Knowledge Based Systems. Proceedings of the Artificial Intelligence and Advanced Computer Technology Conference, Wiesbaden, West Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Hayes</author>
</authors>
<title>Steps towards Integrating Natural Language and Graphical Interaction for Knowledge-based Systems.</title>
<date>1986</date>
<booktitle>Proceedings of the 7th European Conference on Artificial Intelligence,</booktitle>
<location>Brighton, England.</location>
<contexts>
<context position="19505" citStr="Hayes (1986)" startWordPosition="3067" endWordPosition="3068">iptive NL-commands and simultaneous tactile touches on the screen. SDMS (Bolt 1980) enables the user to create and manipulate geometric objects on a screenarrangement called &apos;MEDIA ROOM&apos;. In all those systems, there exist predefined relations between the pointing gesture and its demonstratum. Referent identification is not dependent on context etc. Currently, several projects are investigating problems concerning the integration of pointing actions and NL input, e.g.: In NLMENU (Thompson 1986), the user can select parts of a street map by means 182 of a mouse-controlled rubber-band technique. Hayes (1986) outlines the integration of a deictic component into the Language Craft System, which should allow the user to click on items on the screen, e.g. the machines on a blueprint of a factory floor. ACORD investigates pointing actions with respect to various twodimensional objects, e.g a map of the planetary system (Hanne, Hoepelmann, and Fahnrich 1986) and a form for university registration (Wetzel, Hanne, and Hoepelmann 1987). 5.3 Pointing actions in TACTILUS One aim of XTRA is the integration of (typed) verbal descriptions and pointing gestures (currently realized by mouse-clicks) for referent </context>
<context position="22544" citStr="Hayes (1986)" startWordPosition="3557" endWordPosition="3558">n the other hand, there is the deictic expression &apos;this sum&apos; without its correlated obligatory pointing action. Therefore, the system has to recognize that &apos;/ &apos; belongs to &apos;this sum&apos;. This problem is aggravated by the fact that the words &apos;here&apos;/&apos;there&apos; and &apos;this&apos;/&apos; that&apos; are not only the most frequent deictic expressions but have anaphoric and text- deictic readings as well. Matching mouse-clicks and phrases becomes even more difficult if a singlc utterance requires more than one pointing action. This case is called &apos;multiple pointing&apos;. Examples include: This sum I would prefer to enter here. Hayes (1986) assumes that pointing actions are performed in the same order as their corresponding phrases. But until this hypothesis is confirmed empirically, it can only serve as a heuristic rule. As soon as reference by pointing is possible, the use of incomplete expressions will increase. In these cases, additional knowledge sources are needed for referent identification, like descriptor analysis and case frame analysis (Kobsa et al. 1986). For example, the expression &apos;this&apos; in the sentence &apos;I want to add this/ &apos;surely refers to a number in the present domain, because &apos;add&apos; is categorized as an action </context>
</contexts>
<marker>Hayes, 1986</marker>
<rawString>Hayes, P. J. (1986): Steps towards Integrating Natural Language and Graphical Interaction for Knowledge-based Systems. Proceedings of the 7th European Conference on Artificial Intelligence, Brighton, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Kirk</author>
<author>M Burton</author>
</authors>
<title>Physical versus Semantic Classification of Nonverbal Forms: A Cross-Cultural Experiment.</title>
<date>1981</date>
<booktitle>Nonverbal Communication, Interaction, and Gesture.</booktitle>
<editor>In: A. Kendon, ed.:</editor>
<marker>Kirk, Burton, 1981</marker>
<rawString>Kirk, L. and M. Burton (1981): Physical versus Semantic Classification of Nonverbal Forms: A Cross-Cultural Experiment. In: A. Kendon, ed.: Nonverbal Communication, Interaction, and Gesture.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Klein</author>
</authors>
<title>Wo ist hier? Praliminarien zu einer Untersuchting der lokalen Deixis.</title>
<date>1978</date>
<journal>Linguistische Berichte</journal>
<volume>58</volume>
<pages>18--40</pages>
<contexts>
<context position="13234" citStr="Klein 1978" startWordPosition="2058" endWordPosition="2059">ambiguates pointing One essential drawback of pointing gestures is their inevitable dependency on the here-and-now. Furthermore, pointing without describing the referent is fundamentally ambiguous (Wittgenstein 1958). Referent identification involves the following three steps: First, one has to recognize the direction indicated. This requires facing the speaker and following his/her gesture with gaze and eventually a body turn. Thus, the deictic spaces of both participants are co-oriented by physical means and not by mental acts (e.g., transformation of &apos;left&apos; into &apos;right&apos; and vice versa, see Klein 1978). The second task is the identification of the object indicated. Usually, there is more than one object situated in any one direction. Problems arise if possible demonstrata are: - next to each other, - behind each other, or - embedded in one another. In these cases, unambiguous reference requires the naming or describing of the demonstratum. Thirdly, one has to decide what aspect of the object is being referred to. Like the second step, this is usually done by consideration of the descriptor. For example, pointing at a moving car can refer to its colour (&apos; Nice green / , isn&apos;t it?&apos;) or its ki</context>
</contexts>
<marker>Klein, 1978</marker>
<rawString>Klein, W. (1978): Wo ist hier? Praliminarien zu einer Untersuchting der lokalen Deixis. Linguistische Berichte 58, 18-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kobsa</author>
</authors>
<title>Combining Deictic Gestures and Natural Language for Referent Identification.</title>
<date>1986</date>
<booktitle>Proceedings of the 11th International Conference on Computational Linguistics,</booktitle>
<location>Bonn. West</location>
<marker>Kobsa, 1986</marker>
<rawString>Kobsa, A. et al. (1986): Combining Deictic Gestures and Natural Language for Referent Identification. Proceedings of the 11th International Conference on Computational Linguistics, Bonn. West Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J M Levelt</author>
<author>G Richardson</author>
<author>W La Heij</author>
</authors>
<title>Pointing and Voicing in Deictic Expressions.</title>
<date>1985</date>
<journal>Journal of Memory and Language</journal>
<volume>24</volume>
<pages>133--164</pages>
<marker>Levelt, Richardson, Heij, 1985</marker>
<rawString>Levelt, W. J. M., G. Richardson and W. La Heij (1985): Pointing and Voicing in Deictic Expressions. Journal of Memory and Language 24, 133-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lyons</author>
</authors>
<title>Semantics, Vols 1 and 2. Cambridge:</title>
<date>1977</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="14860" citStr="Lyons 1977" startWordPosition="2327" endWordPosition="2328">t of view: Some problems of local deixis are reduced in complexity without the communicative setting having to become unnatural (Schmauks 1986a). Furthermore, 181 this domain is interesting from an artificial intelligence point of view, since some of the pointing actions with regard to forms can now be simulated on a terminal screen. 4.1 Reduction of problems Following Baler&apos;s terminology (1982), form deixis belongs to the kind of deixis called &apos;demonstratio ad oculos&apos;, because all objects pointed at are visible. Furthermore, it represents an example of the &apos;canonical situation of utterance&apos; (Lyons 1977): All the participants are co-present and can thus mutually perceive their (pointing) gestures etc. Form deixis is relatively precise, because tactile pointing is always possible. Precise pointing at small objects (e.g. single words) is frequently performed by using a pencil etc., larger areas by encircling them. The ambiguity with regard to objects behind each other does not occur, because the deictic space is only two-dimensional. If speaker and hearer are situated side by side, their deictic fields are co-oriented. Therefore, this position makes cooperation easier, and thus is the most adva</context>
</contexts>
<marker>Lyons, 1977</marker>
<rawString>Lyons, J. (1977): Semantics, Vols 1 and 2. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Minsky</author>
</authors>
<title>Manipulating Simulated Objects with Real-world Gestures using a Force and Position Sensitive Screen.</title>
<date>1984</date>
<journal>Computer Graphics</journal>
<volume>18</volume>
<pages>195--203</pages>
<contexts>
<context position="25180" citStr="Minsky 1984" startWordPosition="3981" endWordPosition="3982"> currently carried out in the XTRA project. 183 In the case of natural pointing, the choice of a more or less precise pointing gesture is made automatically rather than consciously. But in TACTILUS, the user has to select explicitly the intended degree of accuracy. Empirical investigations must examine whether the user regards this as a disadvantage. Furthermore, pointing via mouse-clicks differs from natural tactile pointing, because there is no physical contact between finger and demonstratum. A better solution would be the use of a touch-sensitive screen on which &apos;real-world gestures&apos; (see Minsky 1984) are possible. Touch-sensitive screens allow highly natural pointing gestures (see Pickering 1986), but have some shortcomings. e.g. a restricted degree of resolution. A problem just as serious as the aforementioned is the temporal dissociation of a pointing gesture and its corresponding phrase. This problem would be soluble if the system would accept input via voice. But this alone wouldn&apos;t be sufficient: There is no guarantee that spoken phrases and correlated mouse-clicks occur simultaneously. Furthermore, current voice-input systems have too small a vocabulary and cannot process fluent spe</context>
</contexts>
<marker>Minsky, 1984</marker>
<rawString>Minsky, M. (1984): Manipulating Simulated Objects with Real-world Gestures using a Force and Position Sensitive Screen. Computer Graphics 18, 195-203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Negroponte</author>
</authors>
<title>Media Room.</title>
<date>1981</date>
<journal>Proceedings of the Society for Information Display</journal>
<volume>22</volume>
<pages>109--113</pages>
<contexts>
<context position="24174" citStr="Negroponte (1981)" startWordPosition="3829" endWordPosition="3830">t as well: In normal communication, the hearer doesn&apos;t need to watch the speaker in order to understand him/her unless the occurence of a deictic expression (or the sound of touching during tactile pointing) demands his/her visual attentiveness. Also, during typed dialog, there is no need to observe the output sentences permanently. In the case of multiple pointing, the possibility cannot be ruled out that the user might fail to notice one of the pointing actions. 6. Prospects of more natural simulation Up till now, only certain kinds of tactile pointing gestures can be simulated on a screen. Negroponte (1981) outlines some future plans, e.g. the consideration of non-tactile actions such as eye tracking and body movements. Simulation of tactile pointing gestures by mouseclicks has some serious limitations with regard to its &apos;naturalness&apos;. Empirical investigations are needed to determine the extent to which mouse-clicks can be regarded as an equivalent of natural pointing. These investigations are currently carried out in the XTRA project. 183 In the case of natural pointing, the choice of a more or less precise pointing gesture is made automatically rather than consciously. But in TACTILUS, the use</context>
</contexts>
<marker>Negroponte, 1981</marker>
<rawString>Negroponte, N. (1981): Media Room. Proceedings of the Society for Information Display 22, 109-113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pechmann</author>
<author>W Deutsch</author>
</authors>
<title>The Development of Verbal and Nonverbal Devices for Reference.</title>
<date>1982</date>
<journal>Journal of Experimental Child Psychology</journal>
<volume>34</volume>
<pages>330--341</pages>
<marker>Pechmann, Deutsch, 1982</marker>
<rawString>Pechmann, T. and W. Deutsch (1982): The Development of Verbal and Nonverbal Devices for Reference. Journal of Experimental Child Psychology 34, 330-341.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Pickering</author>
</authors>
<title>Touch-sensitive screens: the technologies and their application.</title>
<date>1986</date>
<journal>Int. J. Man-Machine Studies</journal>
<volume>25</volume>
<pages>249--269</pages>
<contexts>
<context position="25278" citStr="Pickering 1986" startWordPosition="3993" endWordPosition="3994">a more or less precise pointing gesture is made automatically rather than consciously. But in TACTILUS, the user has to select explicitly the intended degree of accuracy. Empirical investigations must examine whether the user regards this as a disadvantage. Furthermore, pointing via mouse-clicks differs from natural tactile pointing, because there is no physical contact between finger and demonstratum. A better solution would be the use of a touch-sensitive screen on which &apos;real-world gestures&apos; (see Minsky 1984) are possible. Touch-sensitive screens allow highly natural pointing gestures (see Pickering 1986), but have some shortcomings. e.g. a restricted degree of resolution. A problem just as serious as the aforementioned is the temporal dissociation of a pointing gesture and its corresponding phrase. This problem would be soluble if the system would accept input via voice. But this alone wouldn&apos;t be sufficient: There is no guarantee that spoken phrases and correlated mouse-clicks occur simultaneously. Furthermore, current voice-input systems have too small a vocabulary and cannot process fluent speech. Therefore, the most adequate simulation would be the combination of voice input/output and ge</context>
</contexts>
<marker>Pickering, 1986</marker>
<rawString>Pickering, J. A. (1986): Touch-sensitive screens: the technologies and their application. Int. J. Man-Machine Studies 25, 249-269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Reithinger</author>
</authors>
<title>Generating Referring Expressions and Pointing Gestures. To appear</title>
<date>1987</date>
<booktitle>Natural Language Generation.</booktitle>
<editor>in: G. Kempen, ed.:</editor>
<publisher>Kluwer</publisher>
<location>Dordrecht:</location>
<contexts>
<context position="23428" citStr="Reithinger 1987" startWordPosition="3705" endWordPosition="3706">ll increase. In these cases, additional knowledge sources are needed for referent identification, like descriptor analysis and case frame analysis (Kobsa et al. 1986). For example, the expression &apos;this&apos; in the sentence &apos;I want to add this/ &apos;surely refers to a number in the present domain, because &apos;add&apos; is categorized as an action to be performed with numbers. 5.5 Problems in generating mixed output If the pointing actions of the system are also conceived as a simulation of natural pointing, the user is confronted with the same problems that have already been identified in the last subsection (Reithinger 1987). But, whereas multiple pointing can be simulated during input, there seems to be no adequate mode for simulating it during output as well: In normal communication, the hearer doesn&apos;t need to watch the speaker in order to understand him/her unless the occurence of a deictic expression (or the sound of touching during tactile pointing) demands his/her visual attentiveness. Also, during typed dialog, there is no need to observe the output sentences permanently. In the case of multiple pointing, the possibility cannot be ruled out that the user might fail to notice one of the pointing actions. 6.</context>
</contexts>
<marker>Reithinger, 1987</marker>
<rawString>Reithinger, N. (1987): Generating Referring Expressions and Pointing Gestures. To appear in: G. Kempen, ed.: Natural Language Generation. Dordrecht: Kluwer</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R Scherer</author>
</authors>
<title>Die Funktionen des Nonverbalen Verhaltens im Gesprach. In:</title>
<date>1979</date>
<publisher>Weinheim/Basel: Beltz.</publisher>
<contexts>
<context position="9553" citStr="Scherer 1979" startWordPosition="1506" endWordPosition="1507">a pencil, pointer etc. can be more precise than pointing with a finger or the whole hand. 3. The interdependency of describing and pointing In face-to-face interaction, objects are frequently referred to by gestures and speech in parallel. Simulation of this multimodal process presupposes the investigation of the specific limitations of each component and the advantages of their combination. This is done in the following section. 180 There exist both functional and temporal relations between gestures and phrases. Gestures can substitute, repeat, contradict, modify or amplify the vocal output (Scherer 1979). Pointing gestures usually amplify deictic expressions and therefore belong to the kind of gestures called &apos;illustrators&apos; (Ekman, Friesen 1969). Normally, pointing gestures and their correlated phrases are produced simultaneously (Levelt, Richardson, and La Heij 1985). 3.1 Obligatory and optional pointing gestures Some deictic expressions must be accompanied by a pointing action (or a linguistic equivalent, Sennholz 1985). These include: - demonstrative pronouns: &apos;this book&apos;, - heterodeictic local adverbs: &apos;the tree there&apos;, - personal pronouns with deictic function: &apos;he did it&apos;, and - &apos;such&apos;-</context>
</contexts>
<marker>Scherer, 1979</marker>
<rawString>Scherer, K. R. (1979): Die Funktionen des Nonverbalen Verhaltens im Gesprach. In: K. R. Scherer und H. G. Wallbott, Hrsg.: Nonverbale Kommunikation. Weinheim/Basel: Beltz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Schmauks</author>
</authors>
<title>Formulardeixis .und ihre Simulation auf dem Bildschirm. EM Uberblick aus linguistischer Sicht. Memo Nr.</title>
<date>1986</date>
<tech>4, Sonderforschungsbereich 314,</tech>
<institution>Dept. of Computer Science, University of Saarbriicken, FR Germany.</institution>
<contexts>
<context position="14391" citStr="Schmauks 1986" startWordPosition="2254" endWordPosition="2255">er to its colour (&apos; Nice green / , isn&apos;t it?&apos;) or its kind of motion (&apos;This speed / causes lots of accidents&apos;) etc. Pointing at sets of objects can even refer to aspects of higher degree such as number (&apos;I&apos;d like to have that many / books&apos;). 4. Form deixis Pointing at two-dimensional objects (forms, diagrams, maps, pictures etc.) differs in various aspects from pointing at objects within the entire visual field. This offers a definite advantage from a linguistic point of view: Some problems of local deixis are reduced in complexity without the communicative setting having to become unnatural (Schmauks 1986a). Furthermore, 181 this domain is interesting from an artificial intelligence point of view, since some of the pointing actions with regard to forms can now be simulated on a terminal screen. 4.1 Reduction of problems Following Baler&apos;s terminology (1982), form deixis belongs to the kind of deixis called &apos;demonstratio ad oculos&apos;, because all objects pointed at are visible. Furthermore, it represents an example of the &apos;canonical situation of utterance&apos; (Lyons 1977): All the participants are co-present and can thus mutually perceive their (pointing) gestures etc. Form deixis is relatively preci</context>
</contexts>
<marker>Schmauks, 1986</marker>
<rawString>Schmauks, D. (1986a): Formulardeixis .und ihre Simulation auf dem Bildschirm. EM Uberblick aus linguistischer Sicht. Memo Nr. 4, Sonderforschungsbereich 314, Dept. of Computer Science, University of Saarbriicken, FR Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sclunauks</author>
</authors>
<title>Form und Funktion von Zeigegesten. EM interdisziplinarer Uberblick. Bericht Nr.</title>
<date>1986</date>
<tech>10, Sonderforschungsbereich 314,</tech>
<institution>Dept. of Computer Science, University of Saarbriicken, FR Germany.</institution>
<marker>Sclunauks, 1986</marker>
<rawString>Sclunauks, D. (1986b): Form und Funktion von Zeigegesten. EM interdisziplinarer Uberblick. Bericht Nr. 10, Sonderforschungsbereich 314, Dept. of Computer Science, University of Saarbriicken, FR Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sennholz</author>
</authors>
<title>Grundziige der Deixis.</title>
<date>1985</date>
<location>Bochum: Brockmeyer.</location>
<contexts>
<context position="9979" citStr="Sennholz 1985" startWordPosition="1566" endWordPosition="1567">section. 180 There exist both functional and temporal relations between gestures and phrases. Gestures can substitute, repeat, contradict, modify or amplify the vocal output (Scherer 1979). Pointing gestures usually amplify deictic expressions and therefore belong to the kind of gestures called &apos;illustrators&apos; (Ekman, Friesen 1969). Normally, pointing gestures and their correlated phrases are produced simultaneously (Levelt, Richardson, and La Heij 1985). 3.1 Obligatory and optional pointing gestures Some deictic expressions must be accompanied by a pointing action (or a linguistic equivalent, Sennholz 1985). These include: - demonstrative pronouns: &apos;this book&apos;, - heterodeictic local adverbs: &apos;the tree there&apos;, - personal pronouns with deictic function: &apos;he did it&apos;, and - &apos;such&apos;-constructions: &apos;I like such flowers&apos;. Syntactically, obligatory pointing gestures are embedded in noun phrases or adverbial phrases. In the former case, they amplify a linguistic attribute. Within its corresponding phrase, the location of the pointing gesture is arbitrary. Usually, it will accompany the most emphasized expression. A lot of expressions can be accompanied by pointing gestures, in principle all those which re</context>
</contexts>
<marker>Sennholz, 1985</marker>
<rawString>Sennholz, K. (1985): Grundziige der Deixis. Bochum: Brockmeyer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Thompson</author>
</authors>
<title>Building Menu-Based Natural Language Interfaces.</title>
<date>1986</date>
<journal>Texas Engineering Journal</journal>
<volume>3</volume>
<pages>140--150</pages>
<contexts>
<context position="19391" citStr="Thompson 1986" startWordPosition="3048" endWordPosition="3049">e tree displayed on the screen. In NLG (Brown et al. 1979), the user can draw simple geometric objects through descriptive NL-commands and simultaneous tactile touches on the screen. SDMS (Bolt 1980) enables the user to create and manipulate geometric objects on a screenarrangement called &apos;MEDIA ROOM&apos;. In all those systems, there exist predefined relations between the pointing gesture and its demonstratum. Referent identification is not dependent on context etc. Currently, several projects are investigating problems concerning the integration of pointing actions and NL input, e.g.: In NLMENU (Thompson 1986), the user can select parts of a street map by means 182 of a mouse-controlled rubber-band technique. Hayes (1986) outlines the integration of a deictic component into the Language Craft System, which should allow the user to click on items on the screen, e.g. the machines on a blueprint of a factory floor. ACORD investigates pointing actions with respect to various twodimensional objects, e.g a map of the planetary system (Hanne, Hoepelmann, and Fahnrich 1986) and a form for university registration (Wetzel, Hanne, and Hoepelmann 1987). 5.3 Pointing actions in TACTILUS One aim of XTRA is the i</context>
</contexts>
<marker>Thompson, 1986</marker>
<rawString>Thompson, C. (1986): Building Menu-Based Natural Language Interfaces. Texas Engineering Journal 3, 140-150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R P Wetzel</author>
<author>K H Hanne</author>
<author>J P Hoepelmann</author>
</authors>
<title>DIS-QUE: Deictic Interaction System-Query Environment.</title>
<date>1987</date>
<tech>LOKI Report KRGR 5.3/KR-NL 5,</tech>
<location>FhG, IAO, Stuttgart.</location>
<marker>Wetzel, Hanne, Hoepelmann, 1987</marker>
<rawString>Wetzel, R. P., K. H. Hanne and J. P. Hoepelmann (1987): DIS-QUE: Deictic Interaction System-Query Environment. LOKI Report KRGR 5.3/KR-NL 5, FhG, IAO, Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Wittgenstein</author>
</authors>
<title>Philosophical investigations.</title>
<date>1958</date>
<publisher>Blackwell.</publisher>
<location>Oxford:</location>
<contexts>
<context position="12839" citStr="Wittgenstein 1958" startWordPosition="1997" endWordPosition="1998"> cornflowers&apos; is replacable by &apos;I like these / flowers&apos;). Thus, additional pointing allows unambiguous (or at least relatively precise) referent specification even if one doesn&apos;t know an exact descriptor. The process of referent identification is speeded up, because the orientation to the object&apos;s direction and the processing of the verbal description are performed simultaneously. 3.3 Describing disambiguates pointing One essential drawback of pointing gestures is their inevitable dependency on the here-and-now. Furthermore, pointing without describing the referent is fundamentally ambiguous (Wittgenstein 1958). Referent identification involves the following three steps: First, one has to recognize the direction indicated. This requires facing the speaker and following his/her gesture with gaze and eventually a body turn. Thus, the deictic spaces of both participants are co-oriented by physical means and not by mental acts (e.g., transformation of &apos;left&apos; into &apos;right&apos; and vice versa, see Klein 1978). The second task is the identification of the object indicated. Usually, there is more than one object situated in any one direction. Problems arise if possible demonstrata are: - next to each other, - be</context>
</contexts>
<marker>Wittgenstein, 1958</marker>
<rawString>Wittgenstein, L. (1958): Philosophical investigations. Oxford: Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
</authors>
<title>Research in Natural Language Understanding: Annual Report.</title>
<date>1979</date>
<tech>TR 4274,</tech>
<location>Bolt, Beranek and Newman, Cambridge, MA.</location>
<marker>Woods, 1979</marker>
<rawString>Woods, W. A. et al. (1979): Research in Natural Language Understanding: Annual Report. TR 4274, Bolt, Beranek and Newman, Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>