<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001330">
<title confidence="0.5272325">
SemEval-2015 Task 11:
Sentiment Analysis of Figurative Language in Twitter
</title>
<author confidence="0.96726">
Aniruddha Ghosh Guofu Li
</author>
<affiliation confidence="0.988238">
University College Dublin, Ireland. University College Dublin, Ireland.
</affiliation>
<email confidence="0.991477">
arghyaonline@gmail.com li.guofu.l@gmail.com
</email>
<author confidence="0.998127">
Tony Veale
</author>
<affiliation confidence="0.895552">
University College Dublin, Ireland. Paolo Rosso
Tony.Veale@UCD.ie Universitat Politecnica de Valencia, Spain.
</affiliation>
<author confidence="0.834938">
Ekaterina Shutova prosso@dsic.upv.es
</author>
<affiliation confidence="0.981422">
University of Cambridge.
</affiliation>
<author confidence="0.782765">
Ekaterina.Shutova@cl.cam.ac.uk Antonio Reyes
</author>
<affiliation confidence="0.984565">
John Barnden Instituto Superior de Interpretes y Traductores
University of Birmingham, UK Mexico
</affiliation>
<email confidence="0.975203">
J.A.Barnden@cs.bham.ac.uk antonioreyes@isit.edu.mx
</email>
<sectionHeader confidence="0.996348" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999858705882353">
This report summarizes the objectives and
evaluation of the SemEval 2015 task on the
sentiment analysis of figurative language on
Twitter (Task 11). This is the first sentiment
analysis task wholly dedicated to analyzing
figurative language on Twitter. Specifically,
three broad classes of figurative language are
considered: irony, sarcasm and metaphor.
Gold standard sets of 8000 training tweets and
4000 test tweets were annotated using workers
on the crowdsourcing platform CrowdFlower.
Participating systems were required to provide
a fine-grained sentiment score on an 11-point
scale (-5 to +5, including 0 for neutral intent)
for each tweet, and systems were evaluated
against the gold standard using both a Cosine-
similarity and a Mean-Squared-Error measure.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975818181818">
The limitations on text length imposed by micro-
blogging services such as Twitter do nothing to
dampen our willingness to use language creatively.
Indeed, such limitations further incentivize the use
of creative devices such as metaphor and irony, as
such devices allow strongly-felt sentiments to be
expressed effectively, memorably and concisely.
Nonetheless, creative language can pose certain
challenges for NLP tools that do not take account
of how words can be used playfully and in original
ways. In the case of language using figurative
devices such as irony, sarcasm or metaphor – when
literal meanings are discounted and secondary or
extended meanings are intentionally profiled – the
affective polarity of the literal meaning may differ
significantly from that of the intended figurative
meaning. Nowhere is this effect more pronounced
than in ironical language, which delights in using
affirmative language to convey critical meanings.
Metaphor, irony and sarcasm can each sculpt the
affect of an utterance in complex ways, and each
tests the limits of conventional techniques for the
sentiment analysis of supposedly literal texts.
Figurative language thus poses an especially
significant challenge to sentiment analysis systems,
as standard approaches anchored in the dictionary-
defined affect of individual words and phrases are
often shown to be inadequate in the face of indirect
figurative meanings. It would be convenient if such
language were rare and confined to specific genres
of text, such as poetry and literature. Yet the reality
is that figurative language is pervasive in almost
any genre of text, and is especially commonplace
on the texts of the Web and on social media
platforms such as Twitter. Figurative language
often draws attention to itself as a creative artifact,
but is just as likely to be viewed as part of the
general fabric of human communication. In any
case, Web users widely employ figures of speech
(both old and new) to project their personality
through a text, especially when their texts are
limited to the 140 characters of a tweet.
Natural language researchers have attacked the
problems associated with figurative interpretations
</bodyText>
<page confidence="0.973239">
470
</page>
<note confidence="0.9533405">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 470–478,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999872163636364">
at multiple levels of linguistic representation.
Some have focused on the conceptual level, of
which the text is a surface instantiation, to identify
the schemas and mappings that are implied by a
figure of speech (see e.g. Veale and Keane (1992);
Barnden (2008); Veale (2012)). These approaches
yield a depth of insight but not a robustness of
analysis in the face of textual diversity. More
robust approaches focus on the surface level of a
text, to consider word choice, syntactic order,
lexical properties and affective profiles of the
elements that make up a text (e.g. Reyes and Rosso
(2012, 2014)). Surface analysis yields a range of
discriminatory features that can be efficiently
extracted and fed into machine-learning algorithms.
When it comes to analyzing the texts of the Web,
the Web can also be used as a convenient source of
ancillary knowledge and features. Veale and Hao
(2007) describe a means of harvesting a common-
sense knowledge-base of stereotypes from the Web,
by directly targeting simile constructions of the
form “as X as Y” (e.g. “as hot as an oven”, “as
humid as a jungle”, “as big as a mountain”, etc.).
Though largely successful in their efforts, Veale
and Hao were surprised to discover that up to 20%
of Web-harvested similes are ironic (examples
include “as subtle as a freight train”, “as tanned as
an Irishman”, “as sober as a Kennedy”, “as private
as a park bench”). Initially filtering ironic similes
manually – as irony is the worst kind of noise
when acquiring knowledge from the Web – Hao &amp;
Veale (2010) report good results for an automatic,
Web-based approach to distinguishing ironic from
non-ironic similes. Their approach exploits specific
properties of similes and is thus not directly
transferrable to the detection of irony in general.
Reyes, Rosso and Veale (2013) and Reyes, Rosso
and Buscaldi (2012) thus employ a more general
approach that applies machine learning algorithms
to a range of structural and lexical features to learn
a robust basis for detecting humor and irony in text.
The current task is one that calls for such a
general approach. Note that the goal of Task 11 is
not to detect irony, sarcasm or metaphor in a text,
but to perform robust sentiment analysis on a fine-
grained 11-point scale over texts in which these
kinds of linguistic usages are pervasive. A system
may find detection to be a useful precursor to
analysis, or it may not. We present a description of
Task 11 in section 2, before presenting our dataset
in section 3 and the scoring functions in section 4.
Descriptions of each participating system are then
presented in section 5, before an overall evaluation
in reported in section 6. The report then concludes
with some general observations in section 7.
</bodyText>
<sectionHeader confidence="0.95436" genericHeader="method">
2 Task Description
</sectionHeader>
<bodyText confidence="0.999974925925926">
The task concerns itself with the classification of
overall sentiment in micro-texts drawn from the
micro-blogging service Twitter. These texts, called
tweets, are chosen so that the set as a whole
contains a great deal of irony, sarcasm or metaphor,
so no particular tweet is guaranteed to manifest a
specific figurative phenomenon. Since irony and
sarcasm are typically used to criticize or to mock,
and thus skew the perception of sentiment toward
the negative, it is not enough for a system to
simply determine whether the sentiment of a given
tweet is positive or negative. We thus use an 11-
point scale, ranging from –5 (very negative, for
tweets with highly critical meanings) to +5 (very
positive, for tweets with flattering or very upbeat
meanings). The point 0 on this scale is used for
neutral tweets, or those whose positivity and
negativity cancel each other out. While the
majority of tweets will have sentiments in the
negative part of the scale, the challenge for
participating systems is to decide just how negative
or positive a tweet seems to be.
So, given a set of tweets that are rich in metaphor,
sarcasm and irony, the goal is to determine
whether a user has expressed a positive, negative
or neutral sentiment in each, and the degree to
which this sentiment has been communicated.
</bodyText>
<sectionHeader confidence="0.94219" genericHeader="method">
3 Dataset Design and Collection
</sectionHeader>
<bodyText confidence="0.999984538461538">
Even humans have difficulty in deciding whether a
given text is ironic or metaphorical. Irony can be
remarkably subtle, while metaphor takes many
forms, ranging the dead to the conventional to the
novel. Sarcasm is easier for humans to detect, and
is perhaps the least sophisticated form of non-
literal language. We sidestep problems of detection
by harvesting tweets from Twitter that are likely to
contain figurative language, either because they
have been explicitly tagged as such (using e.g. the
hashtags #irony, #sarcasm, #not, #yeahright) or
because they use words commonly associated with
the use of metaphor (ironically, the words
</bodyText>
<page confidence="0.994009">
471
</page>
<bodyText confidence="0.99954905">
“literally” and “virtually” are reliable markers of
metaphorical intent, as in “I literally want to die ”).
Datasets were collected using the Twitter4j API
(http://twitter4j.org/en/index.html), which supports
the harvesting of tweets in real-time using search
queries. Queries for hashtags such as #sarcasm,
#sarcastic and #irony, and for words such as
“figuratively”, yielded our initial corpora of
candidate tweets to annotate. We then developed a
Latent Semantic Analysis (LSA) model to extend
this seed set of hashtags so as to harvest a wider
range of figurative tweets (see Li. et. al., 2014).
This tweet dataset was collected over a period of 4
weeks, from June 1st to June 30th, 2014. Though
URLs have been removed from tweets, all other
content, including hashtags – even those used to
retrieve each tweet – has been left in place. Tweets
must contain at least 30 characters when hashtags
are not counted, or 40 characters when hashtags
are counted. All others are eliminated as too short.
</bodyText>
<subsectionHeader confidence="0.996861">
3.1 Dataset Annotation on an 11-point scale
</subsectionHeader>
<bodyText confidence="0.999573301886792">
A trial dataset, consisting of 1025 tweets, was first
prepared by harvesting tweets from Twitter users
that are known for their use of figurative language
(e.g. comedians). Each trial tweet was annotated by
seven annotators from an internal team, three of
whom are native English speakers, the other four
of whom are competent non-native speakers. Each
annotator was asked to assign a score ranging from
-5 (for any tweets conveying disgust or extreme
discontent) to +5 (for tweets conveying obvious
joy and approval or extreme pleasure), where 0 is
reserved for tweets in which positive and negative
sentiment is balanced. Annotators were asked to
use ±5, ±3 and ±1 as scores for tweets calling for
strong, moderate or weak sentiment, and to use ±4
and ±2 for tweets with nuanced sentiments that fall
between these gross scores. An overall sentiment
score for each tweet was calculated as a weighted
average of all 7 annotators, where a double
weighting was given to native English speakers.
Sentiment was assigned on the basis of the
perceived meaning of each tweet – the meaning an
author presumably intends a reader to unpack from
the text – and not the superficial language of the
tweet. Thus, a sarcastic tweet that expresses a
negative message in language that feigns approval
or delight should be marked with a negative score
(as in “I just love it when my friends throw me
under the bus.”). Annotators were explicitly asked
to consider all of a tweet&apos;s content when assigning
a score, including any hashtags (such as #sarcasm,
#irony, etc.), as participating systems are expected
to use all of the tweet&apos;s content, including hashtags.
Tweets of the training and test datasets –
comprising 8000 and 4000 tweets respectively –
were each annotated on a crowd-sourcing platform,
CrowdFlower.com, following the same annotation
scheme as for the trial dataset. Some examples of
tweets and their ideal scores, given as guidelines to
CrowdFlower annotators, are shown in Table 1.
Tweet Content Score
@ThisIsDeep_ you are about as deep as a -4
turd in a toilet bowl. Internet culture is
#garbage and you are bladder cancer.
A paperless office has about as much chance -3
as a paperless bathroom
Today will be about as close as you&apos;ll ever 3
get to a &amp;quot;PERFECT 10&amp;quot; in the weather
world! Happy Mother&apos;s Day! Sunny and
pleasant! High 80.
I missed voting due to work. But I was 1
behind the Austrian entry all the way, so to
speak. I might enter next year. Who knows?
</bodyText>
<tableCaption confidence="0.99832">
Table 1: Annotation examples, given to Annotators
</tableCaption>
<bodyText confidence="0.985919375">
Scammers tend to give identical or random scores
for all units in a task. To prevent scammers from
abusing the task, trial tweets were thus interwoven
as test questions for annotators on training and test
tweets. Each annotator was expected to provide
judgments for test questions that fall within the
range of scores given by the original members of
the internal team. Annotators are dismissed if their
overall accuracy on these questions is below 70%.
The standard deviation stdu(ui) of all judgments
provided by annotator ui also indicates that ui is
likely to be a scammer when stdu(ui)=0. Likewise,
the standard deviation stdt(tj) of all judgments
given for a tweet tj allows us to judge that
annotation Ai,j as given by ui for tj is an outlier if:
A , i avg(� ,i )
</bodyText>
<equation confidence="0.991086">
− � ʹ′
� ʹ′
</equation>
<bodyText confidence="0.995715333333333">
If 60% or more of an annotator’s judgements are
judged to be outliers in this way then the annotator
is deemed a scammer and dismissed from the task.
Each tweet-set was cleaned of all annotations
provided by those deemed to be scammers. After
cleaning, each tweet has 5 to 7 annotations. The
</bodyText>
<equation confidence="0.6850535">
std t
t( i)
</equation>
<page confidence="0.986655">
472
</page>
<bodyText confidence="0.999861833333333">
ratio of in-range judgments on trial tweets, which
was used to detect scammers on the annotation of
training and test data, can also be used to assign a
reliability score to each annotator. The reliability
of an annotator ui is given by R(ui)=mi/ni, where ni
is the number of judgments contributed by ui on
trial tweets, and mi is the number of these
judgments that fall within the range of scores
provided by the original annotators of the trial
data. The final sentiment score for tweet S(tj) is the
weighted average of scores given for it, where the
reliability of each annotator is used as a weight.
</bodyText>
<equation confidence="0.984015666666667">
∑iR(ui)×Ai,j
S(tj) _
YR(u)
</equation>
<bodyText confidence="0.999955666666667">
The weighted sentiment score is a real number
in the range [-5 É +5], where the most reliable
annotators contribute most to each score. These
scores were provided to task participants in two
CSV formats: tweet-ids mapped to real number
scores, and tweet-ids to rounded integer scores.
</bodyText>
<subsectionHeader confidence="0.99962">
3.2 Tweet Delivery
</subsectionHeader>
<bodyText confidence="0.999910846153846">
The actual text of each tweet was not included in
the released datasets due to copyright and privacy
concerns that are standard for use of Twitter data.
Instead, a script was provided for retrieving the
text of each tweet given its released tweet-id.
Tweets are a perishable commodity and may be
deleted, archived or otherwise made inaccessible
over time by their original creators. To ensure that
tweets did not perish in the interval between their
first release and final submission, all training and
test tweets were re-tweeted via a dedicated account
to give them new, non-perishable tweet-ids. The
distributed tweet-ids refer to this dedicated account.
</bodyText>
<table confidence="0.9993172">
Type # Tweets Mean Sentiment
Sarcasm 746 -1.94
Irony 81 -1.35
Metaphor 198 -0.34
Overall 1025 -1.78
</table>
<tableCaption confidence="0.999118">
Table 2: Overview of the Trial Dataset
</tableCaption>
<subsectionHeader confidence="0.998473">
3.3 Dataset Statistics
</subsectionHeader>
<bodyText confidence="0.9999106875">
The trial dataset contains a mix of figurative tweets
chosen manually from Twitter. It consists of 1025
tweets annotated by an internal team of seven
members. Table 2 shows the number of tweets in
each category. The trial dataset is small enough to
allow these category labels to be applied manually.
The training and test datasets were annotated by
CrowdFlower users from countries where English
is spoken as a native language. The 8,000 tweets of
the training set were allocated as in Table 3. As the
datasets are simply too large for the category labels
Sarcasm, Irony and Metaphor to be assigned
manually, the labels here refer to our expectations
of the kind of tweets in each segment of the dataset,
which were each collated using harvesting criteria
specific to different kinds of figurative language.
</bodyText>
<table confidence="0.9992208">
Type # Tweets Mean Sentiment
Sarcasm 5000 -2.25
Irony 1000 -1.70
Metaphor 2000 -0.54
Overall 8000 -1.99
</table>
<tableCaption confidence="0.999847">
Table 3: Overview of the Training Dataset
</tableCaption>
<bodyText confidence="0.999950888888889">
To provide balance, an additional category Other
was also added to the Test dataset. Tweets in this
category were drawn from general Twitter content,
and so were not chosen to capture any specific
figurative quality. Rather, the category was added
to ensure the ecological validity of the task, as
sentiment analysis is never performed on texts that
are wholly figurative. The 4000 tweets of the Test
set were drawn from four categories as in Figure 4.
</bodyText>
<table confidence="0.9990995">
Type # Tweets Mean Sentiment
Sarcasm 1200 -2.02
Irony 800 -1.87
Metaphor 800 -0.77
Other 1200 -0.26
Overall 4000 -0.50
</table>
<tableCaption confidence="0.999659">
Table 4: Overview of the Test Dataset
</tableCaption>
<sectionHeader confidence="0.970646" genericHeader="method">
4 Scoring Functions
</sectionHeader>
<bodyText confidence="0.999734125">
The Cosine-similarity scoring function represents
the gold-standard annotations for the Test dataset
as a vector of the corresponding sentiment scores.
The scores provided by each participating system
are represented in a comparable vector format, so
that the cosine of the angle between these vectors
captures the overall similarity of both score sets. A
score of 1 is achieved only when a system provides
</bodyText>
<page confidence="0.99865">
473
</page>
<bodyText confidence="0.970875">
all the same scores as the human gold-standard. A
script implementing this scoring function was
released to all registered participants, who were
required in turn to submit the outputs of their
systems as a tab-separated file of tweet-ids and
integer sentiment scores (as systems may be based
either on a regression or a classification model).
A multiplier pcos is applied to all submissions, to
penalize any that do not give scores for all tweets.
Thus,_ #submitted-entries
</bodyText>
<subsectionHeader confidence="0.340698">
Pcos
</subsectionHeader>
<bodyText confidence="0.939839">
#all-entries
E.g., a cherry-picking system that scores just 75%
of the test tweets is hit with a 25% penalty.
Mean-Squared-Error (MSE) offers a standard
basis for measuring the performance of predictive
systems, and is favored by some developers as a
basis for optimization. When calculating MSE, in
which lower measures indicate better performance,
the penalty-coefficient pMSE is instead given by:
</bodyText>
<equation confidence="0.3950755">
#all-entries
PMSE #submitted-entries
</equation>
<sectionHeader confidence="0.795908" genericHeader="method">
5 Overview of Participating Systems
</sectionHeader>
<bodyText confidence="0.999947163636364">
A total of 15 teams participated in Task 11,
submitting results from 29 distinct runs. A clear
preference for supervised learning methods can be
observed, with two types of approach – SVMs and
regression models over carefully engineered
features – making up the bulk of approaches.
Team UPF used regression with a Random-
Sub-Space using M5P as a base algorithm. They
exploited additional external resources such as
SentiWordnet, Depeche Mood, and the American
National Corpus. Team ValenTo used a regression
model combined with affective resources such as
SenticNet (see Poria et al., 2014) to assign polarity
scores. Team Elirf used an SVM-based approach,
with features drawn from character N-grams (2 &lt; N
&lt; 10) and a bag-of-words model of the tf-idf
coefficient of each N-gram feature. Team BUAP
also used an SVM approach, taking features from
dictionaries, POS tags and character n-grams.
Team CLaC used four lexica, one that was
automatically generated and three than were
manually crafted. Term frequencies, POS tags and
emoticons were also used as features. Team
LLT_PolyU used a semi-supervised approach with
a Decision Tree Regression Learner, using word-
level sentiment scores and dependency labels as
features. Team CPH used ensemble methods and
ridge regression (without stopwords), and is
notable for its specific avoidance of sentiment
lexicons. Team DsUniPi combined POS tags and
regular expressions to identify useful syntactic
structures, and brought sentiment lexicons and
WordNet-based similarity measures to bear on
their supervised approach. Team RGU’s system
learnt a sentiment model from the training data,
and used a linear Support Vector Classifier to
generate integer sentiment labels. Team ShellFB%
also used a supervised approach, extracting
grammatical relations for use as features from
dependency tree parses.
Team HLT also used an SVM-based approach,
using lexical features such as negation, intensifiers
and other markers of amusement and irony. Team
%Elab constructed a supervised model based on
term co-occurrence scores and the distribution of
emotion-bearing terms in training tweets. Team
LT3 employed a combined, semi-supervised SVM-
and regression-based approach, exploiting a range
of lexical features, a terminology extraction system
and and both WordNet and DBpedia. Team
PRHLT used a deep auto-encoder to extract
features, employing both words and character 3-
grams as tokens for the autoencoder. Their best
results were obtained with ensembles of Extremely
Random Trees with character n-grams as features.
</bodyText>
<sectionHeader confidence="0.999057" genericHeader="evaluation">
6 Results and Discussions
</sectionHeader>
<bodyText confidence="0.9974398">
For comparison purposes, we constructed three
baseline systems, each implemented as a naïve
classifier with shallow bag-of-word features. The
results of these baseline systems for both the MSE
and Cosine metrics are shown in Table 5.
</bodyText>
<table confidence="0.99814975">
Baseline Cosine MSE
Naïve Bayes 0.390 5.672
MaxEnt 0.426 5.450
Decision Tree 0.547 4.065
</table>
<tableCaption confidence="0.999826">
Table 5: Performance of Three Baseline approaches
</tableCaption>
<bodyText confidence="0.98142975">
Table 6 shows the results for each participating
system using these metrics. Team CLaC achieves
the best overall performance on both, achieving
0.758 on the Cosine metric and 2.117 on the MSE
</bodyText>
<page confidence="0.997882">
474
</page>
<bodyText confidence="0.945255">
metric. Most of the other systems also show a clear
advantage over the baselines reported in Table 5.
</bodyText>
<table confidence="0.9995681875">
Team Cosine MSE
CLaC 0.758 2.117
UPF 0.711 2.458
LLT_PolyU 0.687 2.6
elirf 0.658 3.096
LT3 0.658 2.913
ValenTo 0.634 2.999
HLT 0.63 4.088
CPH 0.625 3.078
PRHLT 0.623 3.023
DsUniPi 0.602 3.925
PKU 0.574 3.746
KELabTeam 0.552 4.177
RGU 0.523 5.143
SHELLFBK 0.431 7.701
BUAP 0.059 6.785
</table>
<tableCaption confidence="0.968759">
Table 6: Overall results, sorted by cosine metric.
Scores are for last run submitted for each system.
</tableCaption>
<bodyText confidence="0.999747615384615">
The best performance on sarcasm and irony tweets
was achieved by teams LLT_PolyU and elirf, who
ranked 3rd and 4th respectively. Team ClaC came
first on tweets in the Metaphor category. One run
of team CPH excelled on the Other (non-figurative)
category, but scored poorly on figurative tweets.
Most teams performed well on sarcasm and irony
tweets, but the Metaphor and Other categories
prove more of a challenge. Table 7 presents the
Spearman’s rank correlation between the ranking
of a system overall, on all tweet categories, and its
ranking of different categories of tweets. The right
column limits this analysis to the top 10 systems.
</bodyText>
<table confidence="0.9950095">
Spearman Spearman
Correl – All Correl – Top10
Sarcasm 0.854 0.539
Irony 0.721 0.382
Metaphor 0.864 0.939
Other 0.857 0.624
</table>
<tableCaption confidence="0.992394">
Table 7. How well does overall performance correlate
with performance on different kinds of tweets?
</tableCaption>
<bodyText confidence="0.999899052631579">
When we consider all systems, their performance
on each category of tweet is strongly correlated to
their overall performance. However, looking only
at the top 10 performing systems, we see a
strikingly strong correlation between performance
overall and performance on the category Metaphor.
Performance on Metaphor tweets is a bellwether
for performance on figurative language overall.
Then category Other also plays an important role
here. Both the trail data and the training datasets
are heavily biased to negative sentiment, given
their concentration of ironic and sarcastic tweets.
In contrast, the distribution of sentiment scores in
the test data is more balanced due to the larger
proportion of Metaphor tweets and the addition of
non-figurative Other tweets. To excel at this task,
systems must not treat all tweets as figurative, but
learn to spot the features that cause figurative
devices to influence the sentiment of a tweet.
</bodyText>
<sectionHeader confidence="0.98598" genericHeader="conclusions">
7 Summary and Conclusions
</sectionHeader>
<bodyText confidence="0.9999694">
This paper has described the design and evaluation
of Task 11, which concerns the determination of
sentiment in tweets which are likely to employ
figurative devices such as irony, sarcasm and
metaphor. The task was constructed so as to avoid
questions of what specific device is used in which
tweet: a glance at Twitter, and the use of the #irony
hashtag in particular, indicates that there are as
many folk theories of irony as there are users of the
hashtag #irony. Instead, we have operationalized
the task to put it on a sound and more ecologically
valid footing. The effect of figurativity in tweets is
instead measured via an extrinsic task: measuring
the polarity of tweets that use figurative language.
The task is noteworthy in its use of an 11-point
sentiment scoring scheme, ranging from -5 to +5.
The use of 11 fine-grained categories precludes the
measurement of inter-annotator agreement as a
reliable guide to annotator/annotation quality, but
it allows us to measure system performance on a
task and a language type in which negativity
dominates. We expect the trial, training and test
datasets will prove useful to future researchers who
wish to explore the complex relation between
figurativity and sentiment. To this end, we have
taken steps to preserve the tweets used in this task,
to ensure that they do not perish through the
actions of their original creators. Detailed results of
the evaluation of all systems and runs are shown in
Tables 9 and 10, or can be found online here:
</bodyText>
<footnote confidence="0.730513">
http://alt.qcri.org/semeval2015/task11/
</footnote>
<page confidence="0.992722">
475
</page>
<table confidence="0.999954533333334">
Team Name Name of Run Rank Overall Sarcasm Irony Metaphor Other
ClaC 1 0.758 0.892 0.904 0.655 0.584
UPF 2 0.711 0.903 0.873 0.520 0.486
LLT_PolyU 3 0.687 0.896 0.918 0.535 0.290
LT3 run 1 4 0.6581 0.891 0.897 0.443 0.346
run 2 0.648 0.872 0.861 0.355 0.357
elirf 5 0.6579 0.904 0.905 0.411 0.247
ValenTo 6 0.634 0.895 0.901 0.393 0.202
HLT 7 0.630 0.887 0.907 0.379 0.365
CPH ridge 8 0.625 0.897 0.886 0.325 0.218
ensemble 0.623 0.900 0.903 0.308 0.226
special-ensemble 0.298 -0.148 0.281 0.535 0.612
PRHLT ETR-ngram 9 0.623 0.891 0.901 0.167 0.218
ETR-word 0.611 0.890 0.901 0.294 0.129
RFR-word 0.613 0.888 0.898 0.282 0.170
RFR-ngram 0.597 0.888 0.898 0.135 0.192
BRR-word 0.592 0.883 0.880 0.280 0.110
BRR-ngram 0.593 0.886 0.879 0.119 0.186
DsUniPi 10 0.601 0.87 0.839 0.359 0.271
PKU 11 0.574 0.883 0.877 0.350 0.137
KELabTeam 0.531 0.883 0.895 0.341 0.117
content based 12 0.552 0.896 0.915 0.341 0.115
emotional pattern based 0.533 0.874 0.900 0.289 0.135
RGU test-sent-final 13 0.523 0.829 0.832 0.291 0.165
test-sent-warppred 0.509 0.842 0.861 0.280 0.090
test-sent-predictions 0.509 0.842 0.861 0.280 0.090
SHELLFBK run3 14 0.431 0.669 0.625 0.35 0.167
run2 0.427 0.681 0.652 0.346 0.146
run1 0.145 0.013 0.104 0.167 0.308
BUAP 15 0.058 0.412 -0.209 -0.023 -0.025
</table>
<tableCaption confidence="0.999136">
Table 9. Detailed evaluation of each submitted run of each system (using the Cosine similarity metric).
</tableCaption>
<affiliation confidence="0.771889666666667">
Key: CLaC= Concordia University; UPF= Universitat Pompeu Fabra; LLT_PolyU=Hong Kong Polytechnic
University; LT3= Ghent University; elirf= Universitat Politecnica de Valencia; ValenTo= Universitat Politecnica
de Valencia; HLT= FBK-Irst, University of Trento; CPH= K¿benhavns Universitet; PRHLT= PRHLT Research
Center; DsUniPi= University of Piraeus; PKU= Peking University; KELabTeam= Yeungnam University; RGU=
Robert Gordon University; SHELLFBK= Fondazione Bruno Kessler; BUAP= Benemerita Universidad Aut—noma
de Puebla
</affiliation>
<page confidence="0.995994">
476
</page>
<table confidence="0.999946033333333">
Team Name Name of Run Rank Overall Sarcasm Irony Metaphor Other
ClaC 1 2.117 1.023 0.779 3.155 3.411
UPF 2 2.458 0.934 1.041 4.186 3.772
LLT_PolyU 3 2.600 1.018 0.673 3.917 4.587
LT3 run1 3.398 1.287 1.224 5.670 5.444
run2 4 2.912 1.286 1.083 4.793 4.503
elirf 8 3.096 1.349 1.034 4.565 5.235
ValenTo 5 2.999 1.004 0.777 4.730 5.315
HLT 11 4.088 1.327 1.184 6.589 7.119
CPH ridge 3.079 1.041 0.904 4.916 5.343
ensemble 7 3.078 0.971 0.774 5.014 5.429
special-ensemble 11.274 19.267 9.124 7.806 7.027
PRHLT ETR-ngram 6 3.023 1.028 0.784 5.446 4.888
ETR-word 3.112 1.041 0.791 5.031 5.448
RFR-word 3.107 1.060 0.809 5.115 5.345
RFR-ngram 3.229 1.059 0.811 5.878 5.243
BRR-word 3.299 1.146 0.934 5.178 5.773
BRR-ngram 3.266 1.100 0.941 5.925 5.205
DsUniPi 10 3.925 1.499 1.656 7.106 5.744
PKU 9 3.746 1.148 1.015 5.876 6.743
KELabTeam 5.552 1.198 1.255 7.264 9.905
content based 6.090 1.756 1.811 8.707 11.526
emotional pattern 12 4.177 1.189 0.809 6.829 7.628
RGU test-sentfinal 13 5.143 1.954 1.867 8.015 8.602
test-sent-warppred 5.323 1.855 1.541 8.033 9.505
test-sent-predictions 5.323 1.855 1.541 8.033 9.505
SHELLFBK run3 15 7.701 4.375 4.516 9.219 12.16
run2 9.265 5.183 5.047 11.058 15.055
run1 10.486 12.326 9.853 10.649 8.957
BUAP 14 6.785 4.339 7.609 8.93 7.253
</table>
<tableCaption confidence="0.998014">
Table 10. Detailed evaluation of each submitted run of each system (using the Mean-Squared-Error metric).
</tableCaption>
<reference confidence="0.988919">
Key: CLaC= Concordia University; UPF= Universitat Pompeu Fabra; LLT_PolyU=Hong Kong Polytechnic
University; LT3= Ghent University; elirf= Universitat Politecnica de Valencia; ValenTo= Universitat Politecnica
de Valencia; HLT= FBK-Irst, University of Trento; CPH= K¿benhavns Universitet; PRHLT= PRHLT Research
Center; DsUniPi= University of Piraeus; PKU= Peking University; KELabTeam= Yeungnam University; RGU=
Robert Gordon University; SHELLFBK= Fondazione Bruno Kessler; BUAP= Benemerita Universidad Aut—noma
de Puebla
</reference>
<page confidence="0.995907">
477
</page>
<bodyText confidence="0.9126571">
Acknowledgements Systems. 40(3): 595-614. DOI: 10.1007/s10115-013-
0652-8.
The authors gratefully acknowledge the support of
the following projects funded by the European
Commission: PROSECCO (Grant No. 600653),
WIQ-EI IRSES (Grant No. 269180) and MICINN
DIANA-Applications (TIN2012-38603-C02-01).
We are also grateful for the support of the
CNGL Centre for Global Intelligent Content,
funded by Science Foundation Ireland (SFI).
</bodyText>
<sectionHeader confidence="0.995386" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999950057692308">
Barnden, J.A. (2008). Metaphor and artificial
intelligence: Why they matter to each other. In R.W.
Gibbs, Jr. (Ed.), The Cambridge Handbook of
Metaphor and Thought, 311-338. Cambridge, U.K.:
Cambridge University Press.
Hao, Y., Veale, T. (2010). An Ironic Fist in a Velvet
Glove: Creative Mis-Representation in the
Construction of Ironic Similes. Minds and Machines
20(4):635–650.
Li G., Ghosh A., Veale T. (2014). Constructing A
corpus of Figurative Language for a Tweet
Classification and Retrieval Task. In the preceedings
of FIRE 2014, the 6th Forum for Information
Retrieval Evaluatio. Bengaluru, India.
Poria, S., Cambria, E., Winterstein, G. and Huang, G.B.
(2014). Sentic patterns Dependency-based rules for
concept-level sentiment analysis. Knowledge-Based
Systems 69, pp. 45-63.
Reyes A., Rosso P. (2014). On the Difficulty of
Automatically Detecting Irony: Beyond a Simple
Case of Negation. Knowledge and Information
Reyes A., Rosso P., Veale T. (2013). A
Multidimensional Approach for Detecting Irony in
Twitter. Languages Resources and Evaluation 47(1):
239-268.
Reyes A., Rosso P. (2012). Making Objective Decisions
from Subjective Data: Detecting Irony in Customers
Reviews. Journal on Decision Support Systems 53(4):
754–760.
Reyes A., Rosso P., Buscaldi D. (2012). From Humor
Recognition to Irony Detection: The Figurative
Language of Social Media. Data &amp; Knowledge
Engineering 74:1-12.
Shutova, E., L. Sun, A. Korhonen. (2010). Metaphor
identification using verb and noun clustering.
Proceedings of the 23rd International Conference on
Computational Linguistics.
Veale, T., Keane, M. T. (1992). Conceptual Scaffolding:
A spatially founded meaning representation for
metaphor comprehension. Computational
Intelligence 8(3): 494-519.
Veale, T. (2012). Detecting and Generating Ironic
Comparisons: An Application of Creative
Information Retrieval. AAAI Fall Symposium Series
2012, Artificial Intelligence of Humor. Arlington,
Virginia.
Veale, T., Hao, Y. (2007). Comprehending and
Generating Apt Metaphors: A Web-driven, Case-
based Approach to Figurative Language. In
proceedings of AAAI 2007, the 22nd AAAI
Conference on Artificial Intelligence. Vancouver,
Canada.
</reference>
<page confidence="0.998019">
478
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.190262">
<title confidence="0.9406395">SemEval-2015 Task 11: Sentiment Analysis of Figurative Language in Twitter</title>
<author confidence="0.99765">Aniruddha Ghosh Guofu Li</author>
<affiliation confidence="0.996796">University College Dublin, Ireland. University College Dublin, Ireland.</affiliation>
<email confidence="0.996339">arghyaonline@gmail.comli.guofu.l@gmail.com</email>
<author confidence="0.99998">Tony Veale</author>
<affiliation confidence="0.835979">College Dublin, Ireland. Rosso Politecnica de Valencia, Spain. Shutova University of Cambridge. Reyes Barnden Superior de Interpretes y Traductores University of Birmingham, UK Mexico</affiliation>
<email confidence="0.767973">J.A.Barnden@cs.bham.ac.ukantonioreyes@isit.edu.mx</email>
<abstract confidence="0.986315235294118">This report summarizes the objectives and evaluation of the SemEval 2015 task on the sentiment analysis of figurative language on Twitter (Task 11). This is the first sentiment analysis task wholly dedicated to analyzing figurative language on Twitter. Specifically, three broad classes of figurative language are Gold standard sets of 8000 training tweets and 4000 test tweets were annotated using workers the crowdsourcing platform Participating systems were required to provide a fine-grained sentiment score on an 11-point scale (-5 to +5, including 0 for neutral intent) for each tweet, and systems were evaluated against the gold standard using both a Cosinesimilarity and a Mean-Squared-Error measure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Pompeu Fabra; LLT_PolyU=Hong Kong Polytechnic University; LT3= Ghent University; elirf= Universitat Politecnica de Valencia; ValenTo= Universitat Politecnica de Valencia; HLT= FBK-Irst,</title>
<institution>Key: CLaC= Concordia University; UPF= Universitat</institution>
<marker></marker>
<rawString>Key: CLaC= Concordia University; UPF= Universitat Pompeu Fabra; LLT_PolyU=Hong Kong Polytechnic University; LT3= Ghent University; elirf= Universitat Politecnica de Valencia; ValenTo= Universitat Politecnica de Valencia; HLT= FBK-Irst, University of Trento; CPH= K¿benhavns Universitet; PRHLT= PRHLT Research Center; DsUniPi= University of Piraeus; PKU= Peking University; KELabTeam= Yeungnam University; RGU= Robert Gordon University; SHELLFBK= Fondazione Bruno Kessler; BUAP= Benemerita Universidad Aut—noma de Puebla</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Barnden</author>
</authors>
<title>Metaphor and artificial intelligence: Why they matter to each other.</title>
<date>2008</date>
<booktitle>In R.W. Gibbs, Jr. (Ed.), The Cambridge Handbook of Metaphor and Thought,</booktitle>
<pages>311--338</pages>
<publisher>Cambridge University Press.</publisher>
<location>Cambridge, U.K.:</location>
<contexts>
<context position="4013" citStr="Barnden (2008)" startWordPosition="583" endWordPosition="584">ly when their texts are limited to the 140 characters of a tweet. Natural language researchers have attacked the problems associated with figurative interpretations 470 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 470–478, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics at multiple levels of linguistic representation. Some have focused on the conceptual level, of which the text is a surface instantiation, to identify the schemas and mappings that are implied by a figure of speech (see e.g. Veale and Keane (1992); Barnden (2008); Veale (2012)). These approaches yield a depth of insight but not a robustness of analysis in the face of textual diversity. More robust approaches focus on the surface level of a text, to consider word choice, syntactic order, lexical properties and affective profiles of the elements that make up a text (e.g. Reyes and Rosso (2012, 2014)). Surface analysis yields a range of discriminatory features that can be efficiently extracted and fed into machine-learning algorithms. When it comes to analyzing the texts of the Web, the Web can also be used as a convenient source of ancillary knowledge a</context>
</contexts>
<marker>Barnden, 2008</marker>
<rawString>Barnden, J.A. (2008). Metaphor and artificial intelligence: Why they matter to each other. In R.W. Gibbs, Jr. (Ed.), The Cambridge Handbook of Metaphor and Thought, 311-338. Cambridge, U.K.: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Hao</author>
<author>T Veale</author>
</authors>
<title>An Ironic Fist in a Velvet Glove: Creative Mis-Representation in the Construction of Ironic Similes.</title>
<date>2010</date>
<journal>Minds and Machines</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="5295" citStr="Hao &amp; Veale (2010)" startWordPosition="799" endWordPosition="802"> a commonsense knowledge-base of stereotypes from the Web, by directly targeting simile constructions of the form “as X as Y” (e.g. “as hot as an oven”, “as humid as a jungle”, “as big as a mountain”, etc.). Though largely successful in their efforts, Veale and Hao were surprised to discover that up to 20% of Web-harvested similes are ironic (examples include “as subtle as a freight train”, “as tanned as an Irishman”, “as sober as a Kennedy”, “as private as a park bench”). Initially filtering ironic similes manually – as irony is the worst kind of noise when acquiring knowledge from the Web – Hao &amp; Veale (2010) report good results for an automatic, Web-based approach to distinguishing ironic from non-ironic similes. Their approach exploits specific properties of similes and is thus not directly transferrable to the detection of irony in general. Reyes, Rosso and Veale (2013) and Reyes, Rosso and Buscaldi (2012) thus employ a more general approach that applies machine learning algorithms to a range of structural and lexical features to learn a robust basis for detecting humor and irony in text. The current task is one that calls for such a general approach. Note that the goal of Task 11 is not to det</context>
</contexts>
<marker>Hao, Veale, 2010</marker>
<rawString>Hao, Y., Veale, T. (2010). An Ironic Fist in a Velvet Glove: Creative Mis-Representation in the Construction of Ironic Similes. Minds and Machines 20(4):635–650.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Li</author>
<author>A Ghosh</author>
<author>T Veale</author>
</authors>
<title>Constructing A corpus of Figurative Language for a Tweet Classification and Retrieval Task.</title>
<date>2014</date>
<booktitle>In the preceedings of FIRE 2014, the 6th Forum for Information Retrieval Evaluatio. Bengaluru,</booktitle>
<marker>Li, Ghosh, Veale, 2014</marker>
<rawString>Li G., Ghosh A., Veale T. (2014). Constructing A corpus of Figurative Language for a Tweet Classification and Retrieval Task. In the preceedings of FIRE 2014, the 6th Forum for Information Retrieval Evaluatio. Bengaluru, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Poria</author>
<author>E Cambria</author>
<author>G Winterstein</author>
<author>G B Huang</author>
</authors>
<title>Sentic patterns Dependency-based rules for concept-level sentiment analysis.</title>
<date>2014</date>
<journal>Knowledge-Based Systems</journal>
<volume>69</volume>
<pages>45--63</pages>
<contexts>
<context position="18433" citStr="Poria et al., 2014" startWordPosition="2979" endWordPosition="2982">view of Participating Systems A total of 15 teams participated in Task 11, submitting results from 29 distinct runs. A clear preference for supervised learning methods can be observed, with two types of approach – SVMs and regression models over carefully engineered features – making up the bulk of approaches. Team UPF used regression with a RandomSub-Space using M5P as a base algorithm. They exploited additional external resources such as SentiWordnet, Depeche Mood, and the American National Corpus. Team ValenTo used a regression model combined with affective resources such as SenticNet (see Poria et al., 2014) to assign polarity scores. Team Elirf used an SVM-based approach, with features drawn from character N-grams (2 &lt; N &lt; 10) and a bag-of-words model of the tf-idf coefficient of each N-gram feature. Team BUAP also used an SVM approach, taking features from dictionaries, POS tags and character n-grams. Team CLaC used four lexica, one that was automatically generated and three than were manually crafted. Term frequencies, POS tags and emoticons were also used as features. Team LLT_PolyU used a semi-supervised approach with a Decision Tree Regression Learner, using wordlevel sentiment scores and d</context>
</contexts>
<marker>Poria, Cambria, Winterstein, Huang, 2014</marker>
<rawString>Poria, S., Cambria, E., Winterstein, G. and Huang, G.B. (2014). Sentic patterns Dependency-based rules for concept-level sentiment analysis. Knowledge-Based Systems 69, pp. 45-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Reyes</author>
<author>P Rosso</author>
</authors>
<title>On the Difficulty of Automatically Detecting Irony: Beyond a Simple Case of Negation. Knowledge and Information</title>
<date>2014</date>
<marker>Reyes, Rosso, 2014</marker>
<rawString>Reyes A., Rosso P. (2014). On the Difficulty of Automatically Detecting Irony: Beyond a Simple Case of Negation. Knowledge and Information</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Reyes</author>
<author>P Rosso</author>
<author>T Veale</author>
</authors>
<title>A Multidimensional Approach for Detecting Irony in Twitter.</title>
<date>2013</date>
<journal>Languages Resources and Evaluation</journal>
<volume>47</volume>
<issue>1</issue>
<pages>239--268</pages>
<marker>Reyes, Rosso, Veale, 2013</marker>
<rawString>Reyes A., Rosso P., Veale T. (2013). A Multidimensional Approach for Detecting Irony in Twitter. Languages Resources and Evaluation 47(1): 239-268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Reyes</author>
<author>P Rosso</author>
</authors>
<title>Making Objective Decisions from Subjective Data: Detecting Irony in Customers Reviews.</title>
<date>2012</date>
<journal>Journal on Decision Support Systems</journal>
<volume>53</volume>
<issue>4</issue>
<pages>754--760</pages>
<contexts>
<context position="4347" citStr="Reyes and Rosso (2012" startWordPosition="637" endWordPosition="640">omputational Linguistics at multiple levels of linguistic representation. Some have focused on the conceptual level, of which the text is a surface instantiation, to identify the schemas and mappings that are implied by a figure of speech (see e.g. Veale and Keane (1992); Barnden (2008); Veale (2012)). These approaches yield a depth of insight but not a robustness of analysis in the face of textual diversity. More robust approaches focus on the surface level of a text, to consider word choice, syntactic order, lexical properties and affective profiles of the elements that make up a text (e.g. Reyes and Rosso (2012, 2014)). Surface analysis yields a range of discriminatory features that can be efficiently extracted and fed into machine-learning algorithms. When it comes to analyzing the texts of the Web, the Web can also be used as a convenient source of ancillary knowledge and features. Veale and Hao (2007) describe a means of harvesting a commonsense knowledge-base of stereotypes from the Web, by directly targeting simile constructions of the form “as X as Y” (e.g. “as hot as an oven”, “as humid as a jungle”, “as big as a mountain”, etc.). Though largely successful in their efforts, Veale and Hao were</context>
</contexts>
<marker>Reyes, Rosso, 2012</marker>
<rawString>Reyes A., Rosso P. (2012). Making Objective Decisions from Subjective Data: Detecting Irony in Customers Reviews. Journal on Decision Support Systems 53(4): 754–760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Reyes</author>
<author>P Rosso</author>
<author>D Buscaldi</author>
</authors>
<title>From Humor Recognition to Irony Detection: The Figurative Language of Social Media.</title>
<date>2012</date>
<journal>Data &amp; Knowledge Engineering</journal>
<pages>74--1</pages>
<marker>Reyes, Rosso, Buscaldi, 2012</marker>
<rawString>Reyes A., Rosso P., Buscaldi D. (2012). From Humor Recognition to Irony Detection: The Figurative Language of Social Media. Data &amp; Knowledge Engineering 74:1-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shutova</author>
<author>L Sun</author>
<author>A Korhonen</author>
</authors>
<title>Metaphor identification using verb and noun clustering.</title>
<date>2010</date>
<booktitle>Proceedings of the 23rd International Conference on Computational Linguistics.</booktitle>
<marker>Shutova, Sun, Korhonen, 2010</marker>
<rawString>Shutova, E., L. Sun, A. Korhonen. (2010). Metaphor identification using verb and noun clustering. Proceedings of the 23rd International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Veale</author>
<author>M T Keane</author>
</authors>
<title>Conceptual Scaffolding: A spatially founded meaning representation for metaphor comprehension.</title>
<date>1992</date>
<journal>Computational Intelligence</journal>
<volume>8</volume>
<issue>3</issue>
<pages>494--519</pages>
<contexts>
<context position="3997" citStr="Veale and Keane (1992)" startWordPosition="579" endWordPosition="582">through a text, especially when their texts are limited to the 140 characters of a tweet. Natural language researchers have attacked the problems associated with figurative interpretations 470 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 470–478, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics at multiple levels of linguistic representation. Some have focused on the conceptual level, of which the text is a surface instantiation, to identify the schemas and mappings that are implied by a figure of speech (see e.g. Veale and Keane (1992); Barnden (2008); Veale (2012)). These approaches yield a depth of insight but not a robustness of analysis in the face of textual diversity. More robust approaches focus on the surface level of a text, to consider word choice, syntactic order, lexical properties and affective profiles of the elements that make up a text (e.g. Reyes and Rosso (2012, 2014)). Surface analysis yields a range of discriminatory features that can be efficiently extracted and fed into machine-learning algorithms. When it comes to analyzing the texts of the Web, the Web can also be used as a convenient source of ancil</context>
</contexts>
<marker>Veale, Keane, 1992</marker>
<rawString>Veale, T., Keane, M. T. (1992). Conceptual Scaffolding: A spatially founded meaning representation for metaphor comprehension. Computational Intelligence 8(3): 494-519.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Veale</author>
</authors>
<title>Detecting and Generating Ironic Comparisons: An Application of Creative Information Retrieval.</title>
<date>2012</date>
<booktitle>AAAI Fall Symposium Series 2012, Artificial Intelligence of Humor.</booktitle>
<location>Arlington, Virginia.</location>
<contexts>
<context position="4027" citStr="Veale (2012)" startWordPosition="585" endWordPosition="586">xts are limited to the 140 characters of a tweet. Natural language researchers have attacked the problems associated with figurative interpretations 470 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 470–478, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics at multiple levels of linguistic representation. Some have focused on the conceptual level, of which the text is a surface instantiation, to identify the schemas and mappings that are implied by a figure of speech (see e.g. Veale and Keane (1992); Barnden (2008); Veale (2012)). These approaches yield a depth of insight but not a robustness of analysis in the face of textual diversity. More robust approaches focus on the surface level of a text, to consider word choice, syntactic order, lexical properties and affective profiles of the elements that make up a text (e.g. Reyes and Rosso (2012, 2014)). Surface analysis yields a range of discriminatory features that can be efficiently extracted and fed into machine-learning algorithms. When it comes to analyzing the texts of the Web, the Web can also be used as a convenient source of ancillary knowledge and features. V</context>
</contexts>
<marker>Veale, 2012</marker>
<rawString>Veale, T. (2012). Detecting and Generating Ironic Comparisons: An Application of Creative Information Retrieval. AAAI Fall Symposium Series 2012, Artificial Intelligence of Humor. Arlington, Virginia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Veale</author>
<author>Y Hao</author>
</authors>
<title>Comprehending and Generating Apt Metaphors: A Web-driven, Casebased Approach to Figurative Language.</title>
<date>2007</date>
<booktitle>In proceedings of AAAI 2007, the 22nd AAAI Conference on Artificial Intelligence.</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="4646" citStr="Veale and Hao (2007)" startWordPosition="685" endWordPosition="688">)). These approaches yield a depth of insight but not a robustness of analysis in the face of textual diversity. More robust approaches focus on the surface level of a text, to consider word choice, syntactic order, lexical properties and affective profiles of the elements that make up a text (e.g. Reyes and Rosso (2012, 2014)). Surface analysis yields a range of discriminatory features that can be efficiently extracted and fed into machine-learning algorithms. When it comes to analyzing the texts of the Web, the Web can also be used as a convenient source of ancillary knowledge and features. Veale and Hao (2007) describe a means of harvesting a commonsense knowledge-base of stereotypes from the Web, by directly targeting simile constructions of the form “as X as Y” (e.g. “as hot as an oven”, “as humid as a jungle”, “as big as a mountain”, etc.). Though largely successful in their efforts, Veale and Hao were surprised to discover that up to 20% of Web-harvested similes are ironic (examples include “as subtle as a freight train”, “as tanned as an Irishman”, “as sober as a Kennedy”, “as private as a park bench”). Initially filtering ironic similes manually – as irony is the worst kind of noise when acqu</context>
</contexts>
<marker>Veale, Hao, 2007</marker>
<rawString>Veale, T., Hao, Y. (2007). Comprehending and Generating Apt Metaphors: A Web-driven, Casebased Approach to Figurative Language. In proceedings of AAAI 2007, the 22nd AAAI Conference on Artificial Intelligence. Vancouver, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>