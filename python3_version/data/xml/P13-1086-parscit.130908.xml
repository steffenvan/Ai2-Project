<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000029">
<title confidence="0.987521">
Semantic Frames to Predict Stock Price Movement
</title>
<author confidence="0.999339">
Boyi Xie, Rebecca J. Passonneau, Leon Wu
</author>
<affiliation confidence="0.9969815">
Center for Computational Learning Systems
Columbia University
</affiliation>
<address confidence="0.907701">
New York, NY USA
</address>
<email confidence="0.998593">
(bx2109|becky|leon.wu)@columbia.edu
</email>
<author confidence="0.619711">
Germ´an G. Creamer
</author>
<affiliation confidence="0.598366">
Howe School of Technology Management
Stevens Institute of Technology
Hoboken, NJ USA
</affiliation>
<email confidence="0.98955">
gcreamer@stevens.edu
</email>
<author confidence="0.444083">
Abstract On Wednesday, April 11th, 2012, Google Inc announced
</author>
<bodyText confidence="0.999743526315789">
Semantic frames are a rich linguistic re-
source. There has been much work
on semantic frame parsers, but less that
applies them to general NLP problems.
We address a task to predict change in
stock price from financial news. Seman-
tic frames help to generalize from spe-
cific sentences to scenarios, and to de-
tect the (positive or negative) roles of spe-
cific companies. We introduce a novel tree
representation, and use it to train predic-
tive models with tree kernels using sup-
port vector machines. Our experiments
test multiple text representations on two
binary classification tasks, change of price
and polarity. Experiments show that fea-
tures derived from semantic frame pars-
ing have significantly better performance
across years on the polarity task.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941833333333">
A growing literature evaluates the financial effects
of media on the market (Tetlock, 2007; Engel-
berg and Parsons, 2011). Recent work has applied
NLP techniques to various financial media (con-
ventional news, tweets) to detect sentiment in con-
ventional news (Devitt and Ahmad, 2007; Haider
and Mehrotra, 2011) or message boards (Chua
et al., 2009), or discriminate expert from non-
expert investors in financial tweets (Bar-Haim et
al., 2011). With the exception of Bar-Haim et al.
(2011), these NLP studies have relied on small
corpora of hand-labeled data for training or evalu-
ation, and the connection to market events is done
indirectly through sentiment detection. We hy-
pothesize that conventional news can be used to
predict changes in the stock price of specific com-
panies, and that the semantic features that best
represent relevant aspects of the news vary across
</bodyText>
<figure confidence="0.985985142857143">
✞ ☎
its first ✝ quarterly earnings report, a week before the April
✆
20 options contracts expiration in contrast to its history
of reporting a day before monthly options expirations.
The stock price of Google surged 3.85% from April
10th’s $626.86 to 12th’s $651.01. On Friday, April 13th,
✞ ☎
would sue ✝Google Inc ,
✆
Google’s Android operating system tramples
✞ ☎
✝ its intellectual property rights . Jury selection was set for
✆
</figure>
<figureCaption confidence="0.78678475">
the next Monday. Google’s stock price tumbled 4.06% on
Friday, and continued to drop in the following week.
Figure 1: Summary of financial news items per-
taining to Google in April, 2012.
</figureCaption>
<bodyText confidence="0.999583038461539">
market sectors. To test this hypothesis, we use
price information to label data from six years of
financial news. Our experiments test several doc-
ument representations for two binary classification
tasks, change of price and polarity. Our main con-
tribution is a novel tree representation based on
semantic frame parses that performs significantly
better than enriched bag-of-words vectors.
Figure 1 shows a constructed example based
on extracts from financial news about Google in
April, 2012. It illustrates how a series of events
reported in the news precedes and potentially
predicts a large change in Google’s stock price.
Google’s early announcement of quarterly earn-
ings possibly presages trouble, and its stock price
falls soon after reports of a legal action against
Google by Oracle. To produce a coherent story,
the original sentences were edited for Figure 1,
but they are in the style of actual sentences from
our dataset. Accurate detection of events and re-
lations that might have an impact on stock price
should benefit from document representation that
captures sentiment in lexical items (e.g., aggres-
sive) combined with the conceptual relations cap-
tured by FrameNet (Ruppenhofer and Rehbein,
2012). A frame is a lexical semantic representa-
</bodyText>
<figure confidence="0.5738615">
news reported Oracle Corp
claiming
</figure>
<page confidence="0.987482">
873
</page>
<note confidence="0.913983">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 873–883,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999967">
tion of the conceptual roles played by parts of a
clause, and relates different lexical items (e.g., re-
port, announce) to the same situation type. In the
figure, some of the words that evoke frames have
been underlined, and role fillers are outlined by
boxes or ovals. Sentiment words are in italics.
To the best of our knowledge, this paper is
the first to apply semantic frames in this do-
main. On the polarity task, the semantic frame fea-
tures encoded as trees perform significantly better
across years and sectors than bag-of-words vectors
(BOW), and outperform BOW vectors enhanced
with semantic frame features, and a supervised
topic modeling approach. The results on the price
change task show the same trend, but are not sta-
tistically significant, possibly due to the volatility
of the market in 2007 and the following several
years. Yet even modest predictive performance
on both tasks could have an impact, as discussed
below, if incorporated into financial models such
as Rydberg and Shephard (2003). We first dis-
cuss the motivation and related work. Section 4
presents vector-based and tree-based features from
semantic frame parses, and section 5 describes our
dataset. The experimental design and results ap-
pear in the following section, followed by discus-
sion and conclusions.
</bodyText>
<sectionHeader confidence="0.986504" genericHeader="keywords">
2 Motivation
</sectionHeader>
<bodyText confidence="0.96793365625">
Financial news is a rich vein for NLP applica-
tions to mine. Many news organizations that fea-
ture financial news, such as Reuters, the Wall
Street Journal and Bloomberg, devote significant
resources to the analysis of corporate news.
Much of the data that would support studies of
a link between the news media and the market are
publicly available. As pointed out by Tetlock et
al. (2008), linguistic communication is a poten-
tially important source of information about firms’
fundamental values. Because very few stock mar-
ket investors directly observe firms’ production ac-
tivities, they get most of their information sec-
ondhand. Their three main sources are analysts’
forecasts, quantifiable publicly disclosed account-
ing variables, and descriptions of firms’ current
and future profit-generating activities. If analyst
and accounting variables are incomplete or biased
measures of firms’ fundamental values, linguis-
tic variables may have incremental explanatory
power for firms’ future earnings and returns.
Consider the following sentences:
Oracle sued Google in August 2010, saying
Google’s Android mobile operating system in-
fringes its copyrights and patents for the Java pro-
gramming language. (a)
Oracle has accused Google of violating its in-
tellectual property rights to the Java programming
language. (b)
Oracle has blamed Google and alleged that the
latter has committed copyright infringement re-
lated to Java programming language held by Ora-
</bodyText>
<equation confidence="0.69968">
cle. (c)
Oracle’s Ellison says couldn’t sway Google on
Java. (d)
</equation>
<bodyText confidence="0.999570484848485">
Sentences a, b and c are semantically similar,
but lexically rather distinct: the shared words are
the company names and Java (programming lan-
guage). Bag-of-Words (BOW) document repre-
sentation is difficult to surpass for many document
classification tasks, but cannot capture the de-
gree of semantic similarity among these sentences.
Methods that have proven successful for para-
phrase detection (Deerwester et al., 1990; Dolan
et al., 2004), as in the main clauses of b and
c, include latent variable models that simultane-
ously capture the semantics of words and sen-
tences, such as latent semantic analysis (LSA) or
latent Dirichlet allocation (LDA). However, our
task goes beyond paraphrase detection. The first
three sentences all indicate an adversarial relation
of Oracle to Google involving a negative judge-
ment. It would be useful to capture the similarities
among all three of these sentences, and to distin-
guish the role of each company (who is suing and
who is being sued). Further, these three sentences
potentially have a greater impact on market per-
ception of Google in contrast to a sentence like d,
that refers to the same conflict more indirectly, and
whose main clause verb is say. We hypothesize
that semantic frames can address these issues.
Most of the NLP literature on semantic frames
addresses how to build robust semantic frame
parsers, with intrinsic evaluation against gold stan-
dard parses. There have been few applications of
semantic frame parsing for extrinsic tasks. To test
for measurable benefits of semantic frame parsing,
this paper poses the following questions:
</bodyText>
<listItem confidence="0.9888452">
1. Are semantic frames useful for document
representation of financial news?
2. What aspects of frames are most useful?
3. What is the relative performance of document
representation that relies on frames?
</listItem>
<page confidence="0.993809">
874
</page>
<bodyText confidence="0.980393333333333">
4. What improvements could be made to best
exploit semantic frames?
Our work is not aimed at investment profit.
Rather, we investigate whether computational lin-
guistic methodologies can improve our under-
standing of a company’s fundamental market
value, and whether linguistic information derived
from news produces a consistent enough result to
benefit more comprehensive financial models.
</bodyText>
<sectionHeader confidence="0.99996" genericHeader="introduction">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999858425">
NLP has recently been applied to financial text
for market analysis, primarily using bag-of-
words (BOW) document representation. Luss
and d’Aspremont (2008) use text classification to
model price movements of financial assets on a
per-day basis. They try to predict the direction
of return, and abnormal returns, defined as an ab-
solute return greater than a predefined threshold.
Kogan et al. (2009) address a text regression prob-
lem to predict the financial risk of investment in
companies. They analyze 10-K reports to predict
stock return volatility. They also predict whether
a company will be delisted following its 10-K re-
port. Ruiz et al. (2012) correlate text with finan-
cial time series volume and price data. They find
that graph centrality measures like page rank and
degree are more strongly correlated to both price
and traded volume for an aggregation of similar
companies, while individual stocks are less corre-
lated. Lavrenko et al. (2000) present an approach
to identify news stories that influence the behavior
of financial markets, and predict trends in stock
prices based on the content of news stories that
precede the trends. Luss and d’Aspremont (2008)
and Lavrenko et al. (2000) both point out the de-
sire for document feature engineering as future re-
search directions. We explore a rich feature space
that relies on frame semantic parsing.
Sentiment analysis figures strongly in NLP
work on news. General Inquirer (GI), a content
analysis program, is used to quantify pessimism of
news in Tetlock (2007) and Tetlock et al. (2008).
Other resources for sentiment detection include
the Dictionary of Affect in Language (DAL) to
score the prior polarity of words, as in Agarwal
et al. (2011) on social media data. Our study in-
corporates DAL scores along with other features.
FrameNet is a rich lexical resource (Fillmore et
al., 2003), based on the theory of frame seman-
tics (Fillmore, 1976). There is active research
</bodyText>
<table confidence="0.999115222222222">
Category Features Value type
Frame F, FT, FE N
attributes wF, wFT, wFE R&gt;0
BOW UniG, BiG, TriG N
wUniG, wBiG, wTriG R&gt;0
pDAL all-Pls, all-Act, all-Img R_µ=0,std=1
VB-Pls, VB-Act, VB-Img R_µ=0,std=1
JJ-Pls, JJ-Act, JJ-Img R_µ=0,std=1
RB-Pls, RB-Act, RB-Img R_µ=0,std=1
</table>
<tableCaption confidence="0.8050945">
Table 1: FWD features (Frame, bag-of-Words,
part-of-speech DAL score) and their value types.
</tableCaption>
<bodyText confidence="0.999870333333333">
to build more accurate parsers (Das and Smith,
2011; Das and Smith, 2012). Semantic role label-
ing using FrameNet has been used to identify an
opinion with its holder and topic (Kim and Hovy,
2006). For deep representation of sentiment anal-
ysis, Ruppenhofer and Rehbein (2012) propose
SentiFrameNet.
Our work addresses classification tasks that
have potential relevance to an influential financial
model (Rydberg and Shephard, 2003). This model
decomposes stock price analysis of financial data
into a three-part ADS model - activity (a binary
process modeling the price move or not), direction
(another binary process modeling the direction of
the moves) and size (a number quantifying the size
of the moves). Our two binary classification tasks
for news, price change and polarity, are analogous
to their activity and direction. In contrast to the
ADS model, our approach does not calculate the
conditional probability of each factor. At present,
our goal is limited to the determination of whether
NLP features can uncover information from news
that could help predict stock price movement or
support analysts’ investigations.
</bodyText>
<sectionHeader confidence="0.998174" genericHeader="method">
4 Methods
</sectionHeader>
<bodyText confidence="0.999977083333333">
We propose two approaches for the use of seman-
tic frames. The first is a rich vector space based
on semantic frames, word forms and DAL affect
scores. The second is a tree representation that
encodes semantic frame features, and depends on
tree kernel measures for support vector machine
classification. The semantic parses of both meth-
ods are derived from SEMAFOR1 (Das and Smith,
2012; Das and Smith, 2011), which solves the se-
mantic parsing problem by rule-based target iden-
tification, log-linear model based frame identifica-
tion and frame element filling.
</bodyText>
<footnote confidence="0.980585">
1http://www.ark.cs.cmu.edu/SEMAFOR.
</footnote>
<page confidence="0.992036">
875
</page>
<table confidence="0.999723">
Frame (F) Judgment comm. Commerce buy
Target (FT) accuse buy
sue purchase
charge bid
Frame COMMUNICATOR BUYER
Element EVALUEE SELLER
(FE) REASON GOODS
</table>
<tableCaption confidence="0.998941">
Table 2: Sample frames.
</tableCaption>
<subsectionHeader confidence="0.991157">
4.1 Semantic Frame based FWD Features
</subsectionHeader>
<bodyText confidence="0.993519512820513">
Table 1 lists 24 types of features, including seman-
tic Frame attributes, bag-of-Words, and scores for
words in the Dictionary of Affect in Language by
part of speech (pDAL). We refer to these features
as FWD features throughout the paper. FWD fea-
tures are used alone and in combinations.
FrameNet defines hundreds of frames, each of
which represents a scenario associated with se-
mantic roles, or frame elements, that serve as
participants in the scenario the frame signifies.
Table 2 shows two frames. The frame Judg-
ment communication (JC or Judgment comm. in
the rest of the paper) represents a scenario in
which a COMMUNICATOR communicates a judg-
ment of an EVALUEE for some REASON. It is
evoked by (target) words such as accuse or sue.
Here we use F for the frame name, FT for the
target words, and FE for frame elements. We use
both frequency and weighted scores. For exam-
ple, we define idf-adjusted weighted frame fea-
tures, such as wF for attribute F in document d as
wFF,d = f(F, d) X log |D|
|d∈D:F∈d|, where f(F, d)
is the frequency of frame F in d, D is the whole
document set and |· |is the cardinality operator.
Bag-of-Words features include term frequency
and tfidf of unigrams, bigrams, and trigrams.
DAL (Dictionary of Affect in Language) is a
psycholinguistic resource to measure the emo-
tional meaning of words and texts (Whissel,
1989). It includes 8,742 words that were anno-
tated for three dimensions: Pleasantness (Pls), Ac-
tivation (Act), and Imagery (Img). Agarwal et
al. (2009) introduced part-of-speech specific DAL
features for sentiment analysis. We follow their
approach by averaging the scores for all words,
verb only, adjective only, and adverb only words.
Feature values are normalized to mean of zero and
standard deviation of one.
</bodyText>
<subsectionHeader confidence="0.997331">
4.2 SemTree Feature Space and Kernels
</subsectionHeader>
<bodyText confidence="0.9999802">
We propose SemTree as another feature space to
encode semantic information in trees. SemTree
can distinguish the roles of each company of in-
terest, or designated object (e.g. who is suing and
who is being sued).
</bodyText>
<subsectionHeader confidence="0.911266">
4.2.1 Construction of Tree Representation
</subsectionHeader>
<bodyText confidence="0.999997653846154">
The semantic frame parse of a sentence is a forest
of trees, each of which corresponds to a semantic
frame. SemTree encodes the original frame struc-
ture and its leaf words and phrases, and highlights
a designated object at a particular node as follows.
For each lexical item (target) that evokes a frame, a
backbone is found by extracting the path from the
root to the role filler mentioning a designated ob-
ject; the backbone is then reversed to promote the
designated object. If multiple frames have been
assigned to the same designated object, their back-
bones are merged. Lastly, the frame elements and
frame targets are inserted at the frame root.
The top of Figure 2 shows the semantic parse
for sentence a from section 2; we use it to illus-
trate tree construction for designated object Ora-
cle. The parse has two frames (Figure 2-(1)&amp;(2)),
one corresponding to the main clause (verb sue),
and the other for the tenseless adjunct (verb say).
The reversed paths extracted from each frame root
to the designated object Oracle become the back-
bones (Figures 2-(3)&amp;(4)). After merging the two
backbones we get the resulting SemTree, as shown
in Figure 2-(5). By the same steps, this sentence
would also yield a SemTree with Google at the
root, in the role of EVALUEE.
</bodyText>
<subsubsectionHeader confidence="0.932878">
4.2.2 Kernels and Tree Substructures
</subsubsectionHeader>
<bodyText confidence="0.996643105263158">
The tree kernel (Moschitti, 2006; Collins and
Duffy, 2002) is a function of tree similarity, based
on common substructures (tree fragments). There
are two types of substructures. A subtree (ST) is
defined as any node of a tree along with all its de-
scendants. A subset tree (SST) is defined as any
node along with its immediate children and, op-
tionally, part or all of the children’s descendants.
Each tree is represented by a d dimensional vec-
tor where the i’th component counts the number
of occurrences of the i’th tree fragment.
Define the function hi(T) as the number of
occurrences of the i’th tree fragment in tree
T, so that T is now represented as h(T) =
(h1(T), h2(T), ..., hd(T)). We define the set of
nodes in tree T1 and T2 as NT, and NT, respec-
tively. We define the indicator function Ii(n) to be
1 if subtree i is seen rooted at node n, and 0 oth-
erwise. It follows that hi(T1) = En1∈NT1 Ii(n1)
</bodyText>
<page confidence="0.993198">
876
</page>
<table confidence="0.996384041666667">
Designated object: Oracle (ORCL)
Sentence: Oracle sued Google in August 2010, saying Google’s Android mobile operating system infringes its copyrights and patents for the Java pro-
gramming language.
SRL: [OracleJC.F E.Communicator,Stmt.F E.Speaker] [suedJC.T arget] [GoogleJC.F E.Evaluee] in August 2010, [sayingStmt.T arget]
[Google´s Android mobile operating system infringes its copyrights and patents for the Java programming languageStmt.F E.Message].
(1) Judgment comm.
(2) Statement
Judgment comm.Target FE.Communicator Statement.Target FE.Speaker
FE.Evaluee FE.Message
sue ORCL GOOG say ORCL GOOG’s Android ... language
(5) ORCL
(3) ORCL (4) ORCL
Speaker
FE.Communicator FE.Speaker
Statement
Judgment comm. Statement
FE.Speak FE.Message
er S
877
CS (N=40) IT (N=69) TS (N=8)
avg # news 5,702±749 13446±1,272 2,177±188
avg # sentences 16,090±2,316 48,929±5,927 6,970±1,383
avg # comisent. 1.07±0.01 1.06±0.20 1.14±0.03
avg # total 17,131±2,339 51,306±8,637 7,947±1,576
</table>
<tableCaption confidence="0.998791">
Table 3: Data statistics of mean and standard devi-
</tableCaption>
<bodyText confidence="0.910837636363636">
ation by year from January 2007 to August 2012,
for three sectors, with the number of companies.
relevant companies. Each data instance is a sen-
tence and one of the target companies it mentions.
Table 3 summarizes the data statistics. For exam-
ple, the consumer staples sector has 40 companies.
It has an average of 5,702 news articles (16,090
sentences) per year. Each sentence that mentions
a consumer staple company mentions 1.07 com-
panies on average. On average, this sector has
17,131 instances per year.
</bodyText>
<sectionHeader confidence="0.999855" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999989473684211">
Our current experiments are carried out for each
year, training on one year and testing on the next.
The choice to use a coarse time interval with no
overlap was an expedience to permit more numer-
ous exploratory experiments, given the computa-
tional resources these experiments require. We test
the influence of news to predict (1) a change in
stock price (change task), and (2) the polarity of
change (increase vs. decrease; polarity task). Ex-
periments evaluate the FWD and SemTree feature
spaces compared to two baselines: bag-of-words
(BOW) and supervised latent Dirichlet allocation
(sLDA) (Blei and McAuliffe, 2007). BOW in-
cludes features of unigram, bigram and trigram.
sLDA is a statistical model to classify documents
based on LDA topic models, using labeled data. It
has been applied to and shown good performance
in topical text classification, collaborative filter-
ing, and web page popularity prediction problems.
</bodyText>
<subsectionHeader confidence="0.99733">
6.1 Labels, Evaluation Metrics, and Settings
</subsectionHeader>
<bodyText confidence="0.999996777777778">
We align publicly available daily stock price data
from Yahoo Finance with the Reuters news us-
ing a method to avoid back-casting. In particular,
we use the daily adjusted closing price - the price
quoted at the end of a trading day (4PM US East-
ern Time), then adjusted by dividends, stock split,
and other corporate actions. We create two types
of labels for news documents using the price data,
to label the existence of a change and the direc-
tion of change. Both tasks are treated as binary
classification problems. Based on the finding of
a one-day delay of the price response to the in-
formation embedded in the news by Tetlock et al.
(2008), we use At = 1 in our experiment. To
constrain the number of parameters, we also use a
threshold value (r) of a 2% change, based on the
distribution of price changes across our data. In
future work, this could be tuned to sector or time.
</bodyText>
<equation confidence="0.933529875">
�
+1 if |pt(0)+ot−pt(−1) |&gt; r
change= pt(−1)
−1 otherwise
�polarity= +1
+1 if pt(0)+Δt &gt; pt(− 1) and change = +1
−1 if pt(0)+Δt &lt; pt(−1) and change = +1
pt(−1) is the adjusted closing price at the end of
</equation>
<bodyText confidence="0.997967487179487">
the last trading day, and pt(0)+Δt is the price of
the end of the trading day after the At day delay.
Only the instances with changes are included in
the polarity task.
There is high variance across years in the pro-
portion of positive labels, and often highly skewed
classes in one direction or the other. The average
ratios of +/- classes for change and polarity over
the six years’ data are 0.73 (std=0.35) and 1.12
(std=0.25), respectively. Because the time frame
for our experiments includes an economic crisis
followed by a recovery period, we note that the
ratio between increase and decrease of price flips
between 2007, where it is 1.40, and 2008, where it
is 0.71. Accuracy is very sensitive to skew: when a
class has low frequency, accuracy can be high us-
ing a baseline that makes prediction on the major-
ity class. Given the high data skew, and the large
changes from year to year in positive versus nega-
tive skew, we use a more robust evaluation metric.
Our evaluation relies on the Matthews corre-
lation coefficient (MCC, also known as the φ-
coefficient) (Matthews, 1975) to avoid the bias of
accuracy due to data skew, and to produce a ro-
bust summary score independent of whether the
positive class is skewed to the majority or minor-
ity. In contrast to f-measure, which is a class-
specific weighted average of precision and recall,
and whose weighted version depends on a choice
of whether the class-specific weights should come
from the training or testing data, MCC is a sin-
gle summary value that incorporates all 4 cells of
a 2 × 2 confusion matrix (TP, FP, TN and FN for
True or False Positive or Negative). We have also
observed that MCC has a lower relative standard
deviation than f-measure.
For a 2 × 2 contingency table, MCC corre-
sponds to the square root of the average x2 statis-
tic V/x2 /n, with values in [-1,1]. It has been sug-
</bodyText>
<page confidence="0.99603">
878
</page>
<table confidence="0.994850566666667">
Change
test years BOW sLDA FWD SemTreeFWD
Consumer Staples
2008-2010 0.1015 0.0774 0.1079 0.1426
2011-2012 0.1663 0.1203 0.1664 0.1736
5 years 0.1274 0.0945 0.1313 0.1550
Information Technology
2008-2010 0.0580 0.0585 0.0701 0.0846
2011-2012 0.0894 0.0681 0.1076 0.1273
5 years 0.0705 0.0623 0.0851 0.1017
Telecommunication Services
2008-2010 0.1501 0.1615 0.1497 0.2409
2011-2012 0.2256 0.2084 0.2191 0.4009
5 years 0.1803 0.1803 0.1774 0.3049
Polarity
Consumer Staples
2008-2010 0.0359 0.0383 0.0956 0.1054
2011-2012 0.0938 0.0270 0.1131 0.1285
5 years 0.0590 0.0338 0.1026 0.1147
p-value »0.1000 0.0918 0.0489
Information Technology
2008-2010 0.0551 0.0332 0.0697 0.0763
2011-2012 0.0591 0.0516 0.0764 0.0857
5 years 0.0567 0.0405 0.0723 0.0801
p-value 0.0626 0.0948 0.0103
Telecommunication Services
2008-2010 0.0402 0.0464 0.0821 0.0745
2011-2012 0.0366 0.0781 0.0611 0.0809
5 years 0.0388 0.0591 0.0737 0.0770
p-value »0.1000 0.0950 0.0222
</table>
<tableCaption confidence="0.62178075">
Table 4: Average MCC for the change and polarity
tasks by feature representation, for 2008-2010; for
2011-2012; for all 5 years and associated p-values
of ANOVAs for comparison to BOW.
</tableCaption>
<bodyText confidence="0.9915695">
gested as one of the best methods to summarize
into a single value the confusion matrix of a binary
classification task (Jurman and Furlanello, 2010;
Baldi et al., 2000). Given the confusion matrix
</bodyText>
<equation confidence="0.99907075">
(TP FN) :
FP TN
MCC = TP·TN−FP·FN
√(T P+FP) (T P+FN) (T N+FP) (T N+F N).
</equation>
<bodyText confidence="0.999878555555555">
All sentences with at least one company men-
tion are used for the experiment. We remove
stop words and use Stanford CoreNLP for part-
of-speech tagging and named entity recognition.
Models are constructed using linear kernel sup-
port vector machines for both classification tasks.
SVM-light with tree kernels3 (Joachims, 2006;
Moschitti, 2006) is used for both the FWD and
SemTree feature spaces.
</bodyText>
<subsectionHeader confidence="0.776284">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.999844666666667">
Table 4 shows the mean MCC values for each task,
for each sector. Separate means are shown for
the test years of financial crisis (2008-2010) and
economic recovery (2011-2012) to highlight the
differences in performance that might result from
market volatility.
</bodyText>
<footnote confidence="0.885717">
3SVM-light: http://svmlight.joachims.org and Tree
Kernels in SVM-light: http://disi.unitn.it/moschitti/Tree-
Kernel.htm.
</footnote>
<bodyText confidence="0.937189368421052">
pos. 1 dow, investors, index, retail, data
pos. 2 costs, food, price, prices, named entity 4
neu. 1 q3, q1, nov, q2, apr
neu. 2 cents, million, share, year, quarter
neg. 1 cut, sales, prices, hurt, disappointing
neg. 2 percent, call, company, fell, named entity 7
Table 5: Sample sLDA topics for consumer staples
for test year 2010 (train on 2009), polarity task.
SemTree combined with FWD (SemTreeFWD)
generally gives the best performance in both
change and polarity tasks. SemTree results here
are based on the subset tree (SST) kernel, be-
cause of its greater precision in computing com-
mon frame structures and consistently better per-
formance over the subtree (ST) kernel. SemTree
also provides interpretable features for manual
analysis as discussed in the next section.
Analysis of Variance (ANOVA) tests were per-
formed on the full 5 years for each sector, to com-
pare each feature representation as a predictor of
MCC score with the baseline BOW. The ANOVAs
yield the p-values shown in Table 4. There were no
significant differences from BOW on the change
task. For polarity detection, SemTreeFWD was
significantly better than BOW for each sector (see
boldface p-values). No other method was sig-
nificantly better than BOW, although FWD ap-
proaches significance on all sectors, and sLDA ap-
proaches significance on IT.
sLDA has promising MCC scores for the
telecommunication sector, which has only 8 com-
panies, thus many fewer data instances. Table 5
displays a sample of sLDA topics with good per-
formance on polarity for the consumer staples sec-
tor for training year 2009. The positive topics are
related to stock index details and retail data. The
negative topics contain many words with negative
sentiment (e.g., hurt, disappointing).
</bodyText>
<sectionHeader confidence="0.99968" genericHeader="method">
7 Discussion
</sectionHeader>
<subsectionHeader confidence="0.999264">
7.1 Semantic Parse Quality
</subsectionHeader>
<bodyText confidence="0.999964">
In general, SEMAFOR parses capture most of
the important frames for our purposes. There is,
however, significant room for improvement. On
a small, randomly selected sample of sentences
from all three sectors, two of the authors working
independently evaluated the semantic parses, with
approximately 80% agreement. Some of the in-
accuracies in frame parses result from errors prior
to the SEMAFOR parse, such as tokenization or
</bodyText>
<page confidence="0.997464">
879
</page>
<table confidence="0.986652692307692">
+ (Target(jump))
+ (RECIPIENT(Receiving))
+ (VICTIM(Defend))
+ (PERCEIVER AGENTIVE(Perception active(Target)
(PERCEIVER AGENTIVE)(PHENOMENON)))
+ (DONOR(Giving(Target)(THEME)(DONOR)))
+ (Target(beats))
...
- (PHENOMENON(Perception active(Target)(PERCEIVER
AGENTIVE)(PHENOMENON)))
- (TRIGGER(Response))
- (Target(cuts))
- (VICTIM(Cause harm(Target(hurt))(VICTIM)))
</table>
<figureCaption confidence="0.977363666666667">
Figure 3: Best performing SemTree fragments for
increase (+) and decrease (-) of price for consumer
staples sector across training years.
</figureCaption>
<bodyText confidence="0.999557153846154">
dependency parsing errors. The average sentence
length for the sample was 33.3 words, with an av-
erage of 14 frames per sentence, 3 of them with a
GICS company as a role filler. Because SemTree
encodes only the frames containing a designated
object (company), these are the frames we eval-
uated. On average, about half the frames with
a designated object were correct, and two thirds
of those frames we judged to be important. Be-
sides errors due to incorrect tokenization or depen-
dency parsing, we observed that about 8% to 10%
of frames were incorrectly assigned to due word
sense ambiguity.
</bodyText>
<subsectionHeader confidence="0.998978">
7.2 Feature Analysis
</subsectionHeader>
<bodyText confidence="0.992471527027027">
The experimental results show the SemTree space
to be the one representation tested here that is sig-
nificantly better than BOW, but only for the po-
larity task. Post hoc analysis indicates this may
be due to the aptness of semantic frame parsing
for polarity. Limitations in our treatment of time
point to directions for improvement regarding the
change task.
Some strengths of our approach are the separate
treatment of different sectors, and the benefits of
SemTree features. To analyze which were the best
performing features within sectors, we extracted
the best performing frame fragments for the po-
larity task using a tree kernel feature engineering
method presented in Pighin and Moschitti (2009).
The algorithm selects the most relevant features in
accordance with the weights estimated by SVM,
and uses these features to build an explicit repre-
sentation of the kernel space. Figure 3 shows the
best performing SemTree fragments of the polar-
ity task for the consumer staples sector.
Recall that we hypothesized differences in
semantic frame features across sectors. This
shows up as large differences in the strength
of features across sectors. More strikingly, the
same feature can differ in polarity across sec-
tors. For example, in consumer staples, (EVAL-
UEE(Judgment communication)) has positive po-
larity, compared with negative polarity in informa-
tion technology sector. The examples we see indi-
cate that the positive cases pertain to aggressive re-
tail practices that lead to lawsuits with only small
fines, but whose larger impact benefits the bottom
line. A typical case is the sentence, The plaintiffs
of discriminating against dis-
abled customers by mounting “point-of-sale” ter-
minals in many stores at elevated heights that can-
not be reached. Lawsuits in the IT sector, on the
other hand, are often about technology patent dis-
putes, and are more negative, as illustrated by our
example sentence in Figure 2.
SemTree features capture the differences be-
tween semantic roles for the same frame, and be-
tween the same semantic role in different frames.
For example, the PERCEIVER AGENTIVE role of
the Perception active frame contributes to predic-
tion of an increase in price, as in R.J. Reynolds
is watching this situation closely and will respond
as appropriate. Conversely, a company that fills
the PHENOMENON role of the same frame con-
tributes to prediction of a price decrease, as in In-
vestors will get a clearer look at how the market
values the Philip Morris tobacco businesses when
Altria Group Inc. “when-issued” shares begin
trading on Tuesday. When a company fills the
VICTIM role in the Cause harm frame, this can
predict a decrease in price, as in Hershey has
been hurt by soaring prices for cocoa, energy and
other commodities, whereas filling the VICTIM
role in the Defend frame is associated with an in-
crease in price, as in At Berkshire’s annual share-
holder meeting earlier this month, Warren Buffett
defended Wal-Mart, saying the scandal did not
change his opinion of the company.
One weakness of our approach that we dis-
cussed above is that there is a strong effect of
time that we do not address. The same SemTree
feature can be predictive for one time period and
not for another. (GOODS(Commerce sell)) is re-
lated to a decrease in price for 2008 and 2009 but
to an increase in price for 2010-2012. There is
clearly an influence of the overall economic con-
text that we do not take into account. For example,
accused Wal-Mart
</bodyText>
<page confidence="0.935234">
880
</page>
<bodyText confidence="0.9999788">
the practices of acquiring or selling a business are
different in downturning versus recovering mar-
kets. An important observation of the MCC val-
ues, especially in the case of SemTreeFWD is that
MCC increases during the years 2011-2012. We
attribute this change to the difficulty of predicting
stock price trends when there is the high volatil-
ity typical of a financial crisis. The effect of news
on volatility, however, can be explored indepen-
dently. For example, Creamer et al. (2012) detect
a strong association.
Another weakness of our approach is that we
take sentences out of context, which can lead
to prediction errors. For example, the sentence
Longs’ real estate assets alone are worth some
$2.9 billion, or $71.50 per share, Ackman wrote,
meaning that CVS would essentially be paying
for real estate, but gaining Longs’ pharmacy ben-
efit management business and retail operations for
free is treated as predicting a positive polarity for
CVS. This would be accurate if CVS was actually
going to acquire Longs’ business. Later in the
same news item, however, there is a sentence indi-
cating that the sale will not go through, which pre-
dicts negative polarity for CVS: Pershing Square
Capital Management said on Thursday it won’t
support a tender offer from CVS Caremark Corp
for rival Longs Drug Stores Corp because the of-
fer price “materially understates the fair value of
the company,” according to a filing.
</bodyText>
<sectionHeader confidence="0.99885" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999948260869565">
We have presented a model for predicting stock
price movement from news. We proposed FWD
(Frames, BOW, and part-of-speech specific DAL)
features and SemTree data representations. Our
semantic frame-based model benefits from tree
kernel learning using support vector machines.
The experimental results for our feature represen-
tation perform significantly better than BOW on
the polarity task, and show promise on the change
task. It also facilitates human interpretable analy-
sis to understand the relation between a company’s
market value and its business activities. The sig-
nals generated by this algorithm could improve the
prediction of a financial time series model, such as
ADS (Rydberg and Shephard, 2003).
Our future work will consider the contextual in-
formation for sentence selection, and an aggrega-
tion of weighted news content based on the decay
effect over time for individual companies. We plan
to use a moving window for training and testing.
We will also explore different labeling methods,
such as a threshold for price change tuned by sec-
tors and background economics.
</bodyText>
<sectionHeader confidence="0.99454" genericHeader="acknowledgments">
9 Acknowledgements
</sectionHeader>
<bodyText confidence="0.997607">
The authors thank the anonymous reviewers for
their insightful comments.
</bodyText>
<sectionHeader confidence="0.998943" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.981296">
Apoorv Agarwal, Fadi Biadsy, and Kathleen Mckeown.
2009. Contextual phrase-level polarity analysis us-
ing lexical affect scoring and syntactic N-grams. In
Proceedings of the 12th Conference of the Euro-
pean Chapter of the ACL (EACL 2009), pages 24–
32, Athens, Greece, March. Association for Compu-
tational Linguistics.
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-
bow, and Rebecca Passonneau. 2011. Sentiment
analysis of twitter data. In Proceedings of the Work-
shop on Languages in Social Media, LSM ’11, pages
30–38. Association for Computational Linguistics.
Pierre Baldi, Søren Brunak, Yves Chauvin, Claus A. F.
Andersen, and Henrik Nielsen. 2000. Assessing the
accuracy of prediction algorithms for classification:
an overview. Bioinformatics, 16:412 – 424.
Roy Bar-Haim, Elad Dinur, Ronen Feldman, Moshe
Fresko, and Guy Goldstein. 2011. Identifying
and following expert investors in stock microblogs.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1310–1319, Edinburgh, Scotland, UK., July. Asso-
ciation for Computational Linguistics.
David M. Blei and Jon D. McAuliffe. 2007. Super-
vised topic models. In Advances in Neural Informa-
tion Processing Systems, Proceedings of the Twenty-
First Annual Conference on Neural Information
Processing Systems, Vancouver, British Columbia,
Canada, December 3-6.
Christopher Chua, Maria Milosavljevic, and James R.
Curran. 2009. A sentiment detection engine for
internet stock message boards. In Proceedings of
the Australasian Language Technology Association
Workshop 2009, pages 89–93, Sydney, Australia,
December.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, ACL ’02, pages 263–
270, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
</reference>
<page confidence="0.99172">
881
</page>
<reference confidence="0.9997994">
Germ´an G. Creamer, Yong Ren, and Jeffrey V. Nicker-
son. 2012. A Longitudinal Analysis of Asset Re-
turn, Volatility and Corporate News Network. In
Business Intelligence Congress 3 Proceedings.
Dipanjan Das and Noah A. Smith. 2011. Semi-
supervised frame-semantic parsing for unknown
predicates. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1,
HLT ’11, pages 1435–1444, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Dipanjan Das and Noah A. Smith. 2012. Graph-based
lexicon expansion with sparsity-inducing penalties.
In HLT-NAACL, pages 677–687. The Association
for Computational Linguistics.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science.
Ann Devitt and Khurshid Ahmad. 2007. Sentiment
polarity identification in financial news: A cohesion-
based approach. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 984–991, Prague, Czech Republic,
June. Association for Computational Linguistics.
William Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
Proceedings of the 20th International Conference on
Computational Linguistics.
Joseph Engelberg and Christopher A. Parsons. 2011.
The causal impact of media in financial markets.
Journal of Finance, 66(1):67–97.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R. L. Petruck. 2003. Background to
Framenet. International Journal of Lexicography,
16(3):235–250, September.
Charles J. Fillmore. 1976. Frame semantics and
the nature of language. Annals of the New York
Academy of Sciences, 280(1):20–32.
Syed Aqueel Haider and Rishabh Mehrotra. 2011.
Corporate news classification and valence predic-
tion: A supervised approach. In Proceedings of
the 2nd Workshop on Computational Approaches to
Subjectivity and Sentiment Analysis (WASSA 2.011),
pages 175–181, Portland, Oregon, June. Association
for Computational Linguistics.
Thorsten Joachims. 2006. Training linear svms in lin-
ear time. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery
and data mining, KDD ’06, pages 217–226, New
York, NY, USA. ACM.
Giuseppe Jurman and Cesare Furlanello. 2010. A uni-
fying view for performance measures in multi-class
prediction. ArXiv e-prints.
Soo-Min Kim and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed in
online news media text. In Proceedings of the Work-
shop on Sentiment and Subjectivity in Text, SST ’06,
pages 1–8, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Shimon Kogan, Dimitry Levin, Bryan R. Routledge,
Jacob S. Sagi, and Noah A. Smith. 2009. Pre-
dicting risk from financial reports with regression.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, NAACL ’09, pages 272–280, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Victor Lavrenko, Matt Schmill, Dawn Lawrie, Paul
Ogilvie, David Jensen, and James Allan. 2000.
Mining of concurrent text and time series. In In pro-
ceedings of the 6th ACM SIGKDD Int’l Conference
on Knowledge Discovery and Data Mining Work-
shop on Text Mining, pages 37–44.
Ronny Luss and Alexandre d’Aspremont. 2008. Pre-
dicting abnormal returns from news using text clas-
sification. CoRR, abs/0809.2792.
Brian W. Matthews. 1975. Comparison of the pre-
dicted and observed secondary structure of t4 phage
lysozyme. Biochimica et Biophysica Acta (BBA) -
Protein Structure, 405(2):442 – 451.
Alessandro Moschitti. 2006. Making tree kernels prac-
tical for natural language learning. In In Proceed-
ings of the 11th Conference of the European Chapter
of the Association for Computational Linguistics.
Daniele Pighin and Alessandro Moschitti. 2009. Re-
verse engineering of tree kernel feature spaces. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2009, 6-7August 2009, Singapore, pages 111–120.
Eduardo J. Ruiz, Vagelis Hristidis, Carlos Castillo,
Aristides Gionis, and Alejandro Jaimes. 2012. Cor-
relating financial time series with micro-blogging
activity. In Proceedings of the fifth ACM interna-
tional conference on Web search and data mining,
WSDM ’12, pages 513–522, New York, NY, USA.
ACM.
Josef Ruppenhofer and Ines Rehbein. 2012. Se-
mantic frames as an anchor representation for sen-
timent analysis. In Proceedings of the 3rd Work-
shop in Computational Approaches to Subjectivity
and Sentiment Analysis, WASSA ’12, pages 104–
109, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Tina H. Rydberg and Neil Shephard. 2003. Dynam-
ics of Trade-by-Trade Price Movements: Decompo-
sition and Models. Journal of Financial Economet-
rics, 1(1):2–25.
</reference>
<page confidence="0.978359">
882
</page>
<reference confidence="0.9988157">
Paul C. Tetlock, Maytal Saar-Tsechansky, and Sofus
Macskassy. 2008. More than Words: Quantifying
Language to Measure Firms’ Fundamentals. The
Journal of Finance.
Paul C. Tetlock. 2007. Giving Content to Investor Sen-
timent: The Role of Media in the Stock Market. The
Journal of Finance.
Cynthia M. Whissel. 1989. The dictionary of affect in
language. Emotion: Theory, Research, and Experi-
ence, 39(4):113–131.
</reference>
<page confidence="0.999215">
883
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.332503">
<title confidence="0.9831">Semantic Frames to Predict Stock Price Movement</title>
<author confidence="0.957646">Boyi Xie</author>
<author confidence="0.957646">Rebecca J Passonneau</author>
<author confidence="0.957646">Leon</author>
<affiliation confidence="0.999046">Center for Computational Learning</affiliation>
<address confidence="0.9329575">Columbia New York, NY USA</address>
<email confidence="0.998583">(bx2109|becky|leon.wu)@columbia.edu</email>
<author confidence="0.537433">G Germ´an</author>
<affiliation confidence="0.989834">Howe School of Technology Stevens Institute of</affiliation>
<address confidence="0.769037">Hoboken, NJ USA</address>
<email confidence="0.999841">gcreamer@stevens.edu</email>
<abstract confidence="0.99959985">On Wednesday, April 11th, 2012, Google Inc announced Semantic frames are a rich linguistic resource. There has been much work on semantic frame parsers, but less that applies them to general NLP problems. We address a task to predict change in stock price from financial news. Semantic frames help to generalize from specific sentences to scenarios, and to detect the (positive or negative) roles of specific companies. We introduce a novel tree representation, and use it to train predictive models with tree kernels using support vector machines. Our experiments test multiple text representations on two binary classification tasks, change of price and polarity. Experiments show that features derived from semantic frame parsing have significantly better performance across years on the polarity task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Fadi Biadsy</author>
<author>Kathleen Mckeown</author>
</authors>
<title>Contextual phrase-level polarity analysis using lexical affect scoring and syntactic N-grams.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>24--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="14908" citStr="Agarwal et al. (2009)" startWordPosition="2367" endWordPosition="2370">ple, we define idf-adjusted weighted frame features, such as wF for attribute F in document d as wFF,d = f(F, d) X log |D| |d∈D:F∈d|, where f(F, d) is the frequency of frame F in d, D is the whole document set and |· |is the cardinality operator. Bag-of-Words features include term frequency and tfidf of unigrams, bigrams, and trigrams. DAL (Dictionary of Affect in Language) is a psycholinguistic resource to measure the emotional meaning of words and texts (Whissel, 1989). It includes 8,742 words that were annotated for three dimensions: Pleasantness (Pls), Activation (Act), and Imagery (Img). Agarwal et al. (2009) introduced part-of-speech specific DAL features for sentiment analysis. We follow their approach by averaging the scores for all words, verb only, adjective only, and adverb only words. Feature values are normalized to mean of zero and standard deviation of one. 4.2 SemTree Feature Space and Kernels We propose SemTree as another feature space to encode semantic information in trees. SemTree can distinguish the roles of each company of interest, or designated object (e.g. who is suing and who is being sued). 4.2.1 Construction of Tree Representation The semantic frame parse of a sentence is a </context>
</contexts>
<marker>Agarwal, Biadsy, Mckeown, 2009</marker>
<rawString>Apoorv Agarwal, Fadi Biadsy, and Kathleen Mckeown. 2009. Contextual phrase-level polarity analysis using lexical affect scoring and syntactic N-grams. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 24– 32, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Boyi Xie</author>
<author>Ilia Vovsha</author>
<author>Owen Rambow</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Sentiment analysis of twitter data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Languages in Social Media, LSM ’11,</booktitle>
<pages>30--38</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10872" citStr="Agarwal et al. (2011)" startWordPosition="1714" endWordPosition="1717">d on the content of news stories that precede the trends. Luss and d’Aspremont (2008) and Lavrenko et al. (2000) both point out the desire for document feature engineering as future research directions. We explore a rich feature space that relies on frame semantic parsing. Sentiment analysis figures strongly in NLP work on news. General Inquirer (GI), a content analysis program, is used to quantify pessimism of news in Tetlock (2007) and Tetlock et al. (2008). Other resources for sentiment detection include the Dictionary of Affect in Language (DAL) to score the prior polarity of words, as in Agarwal et al. (2011) on social media data. Our study incorporates DAL scores along with other features. FrameNet is a rich lexical resource (Fillmore et al., 2003), based on the theory of frame semantics (Fillmore, 1976). There is active research Category Features Value type Frame F, FT, FE N attributes wF, wFT, wFE R&gt;0 BOW UniG, BiG, TriG N wUniG, wBiG, wTriG R&gt;0 pDAL all-Pls, all-Act, all-Img R_µ=0,std=1 VB-Pls, VB-Act, VB-Img R_µ=0,std=1 JJ-Pls, JJ-Act, JJ-Img R_µ=0,std=1 RB-Pls, RB-Act, RB-Img R_µ=0,std=1 Table 1: FWD features (Frame, bag-of-Words, part-of-speech DAL score) and their value types. to build mor</context>
</contexts>
<marker>Agarwal, Xie, Vovsha, Rambow, Passonneau, 2011</marker>
<rawString>Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow, and Rebecca Passonneau. 2011. Sentiment analysis of twitter data. In Proceedings of the Workshop on Languages in Social Media, LSM ’11, pages 30–38. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Baldi</author>
<author>Søren Brunak</author>
<author>Yves Chauvin</author>
<author>Claus A F Andersen</author>
<author>Henrik Nielsen</author>
</authors>
<title>Assessing the accuracy of prediction algorithms for classification: an overview.</title>
<date>2000</date>
<journal>Bioinformatics, 16:412 –</journal>
<pages>424</pages>
<contexts>
<context position="24439" citStr="Baldi et al., 2000" startWordPosition="3945" endWordPosition="3948">12 0.0591 0.0516 0.0764 0.0857 5 years 0.0567 0.0405 0.0723 0.0801 p-value 0.0626 0.0948 0.0103 Telecommunication Services 2008-2010 0.0402 0.0464 0.0821 0.0745 2011-2012 0.0366 0.0781 0.0611 0.0809 5 years 0.0388 0.0591 0.0737 0.0770 p-value »0.1000 0.0950 0.0222 Table 4: Average MCC for the change and polarity tasks by feature representation, for 2008-2010; for 2011-2012; for all 5 years and associated p-values of ANOVAs for comparison to BOW. gested as one of the best methods to summarize into a single value the confusion matrix of a binary classification task (Jurman and Furlanello, 2010; Baldi et al., 2000). Given the confusion matrix (TP FN) : FP TN MCC = TP·TN−FP·FN √(T P+FP) (T P+FN) (T N+FP) (T N+F N). All sentences with at least one company mention are used for the experiment. We remove stop words and use Stanford CoreNLP for partof-speech tagging and named entity recognition. Models are constructed using linear kernel support vector machines for both classification tasks. SVM-light with tree kernels3 (Joachims, 2006; Moschitti, 2006) is used for both the FWD and SemTree feature spaces. 6.2 Results Table 4 shows the mean MCC values for each task, for each sector. Separate means are shown fo</context>
</contexts>
<marker>Baldi, Brunak, Chauvin, Andersen, Nielsen, 2000</marker>
<rawString>Pierre Baldi, Søren Brunak, Yves Chauvin, Claus A. F. Andersen, and Henrik Nielsen. 2000. Assessing the accuracy of prediction algorithms for classification: an overview. Bioinformatics, 16:412 – 424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Elad Dinur</author>
<author>Ronen Feldman</author>
<author>Moshe Fresko</author>
<author>Guy Goldstein</author>
</authors>
<title>Identifying and following expert investors in stock microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1310--1319</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="1598" citStr="Bar-Haim et al., 2011" startWordPosition="241" endWordPosition="244">ange of price and polarity. Experiments show that features derived from semantic frame parsing have significantly better performance across years on the polarity task. 1 Introduction A growing literature evaluates the financial effects of media on the market (Tetlock, 2007; Engelberg and Parsons, 2011). Recent work has applied NLP techniques to various financial media (conventional news, tweets) to detect sentiment in conventional news (Devitt and Ahmad, 2007; Haider and Mehrotra, 2011) or message boards (Chua et al., 2009), or discriminate expert from nonexpert investors in financial tweets (Bar-Haim et al., 2011). With the exception of Bar-Haim et al. (2011), these NLP studies have relied on small corpora of hand-labeled data for training or evaluation, and the connection to market events is done indirectly through sentiment detection. We hypothesize that conventional news can be used to predict changes in the stock price of specific companies, and that the semantic features that best represent relevant aspects of the news vary across ✞ ☎ its first ✝ quarterly earnings report, a week before the April ✆ 20 options contracts expiration in contrast to its history of reporting a day before monthly options</context>
</contexts>
<marker>Bar-Haim, Dinur, Feldman, Fresko, Goldstein, 2011</marker>
<rawString>Roy Bar-Haim, Elad Dinur, Ronen Feldman, Moshe Fresko, and Guy Goldstein. 2011. Identifying and following expert investors in stock microblogs. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1310–1319, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Jon D McAuliffe</author>
</authors>
<title>Supervised topic models.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems, Proceedings of the TwentyFirst Annual Conference on Neural Information Processing Systems,</booktitle>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="19846" citStr="Blei and McAuliffe, 2007" startWordPosition="3168" endWordPosition="3171">ments Our current experiments are carried out for each year, training on one year and testing on the next. The choice to use a coarse time interval with no overlap was an expedience to permit more numerous exploratory experiments, given the computational resources these experiments require. We test the influence of news to predict (1) a change in stock price (change task), and (2) the polarity of change (increase vs. decrease; polarity task). Experiments evaluate the FWD and SemTree feature spaces compared to two baselines: bag-of-words (BOW) and supervised latent Dirichlet allocation (sLDA) (Blei and McAuliffe, 2007). BOW includes features of unigram, bigram and trigram. sLDA is a statistical model to classify documents based on LDA topic models, using labeled data. It has been applied to and shown good performance in topical text classification, collaborative filtering, and web page popularity prediction problems. 6.1 Labels, Evaluation Metrics, and Settings We align publicly available daily stock price data from Yahoo Finance with the Reuters news using a method to avoid back-casting. In particular, we use the daily adjusted closing price - the price quoted at the end of a trading day (4PM US Eastern Ti</context>
</contexts>
<marker>Blei, McAuliffe, 2007</marker>
<rawString>David M. Blei and Jon D. McAuliffe. 2007. Supervised topic models. In Advances in Neural Information Processing Systems, Proceedings of the TwentyFirst Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 3-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Chua</author>
<author>Maria Milosavljevic</author>
<author>James R Curran</author>
</authors>
<title>A sentiment detection engine for internet stock message boards.</title>
<date>2009</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop</booktitle>
<pages>89--93</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="1505" citStr="Chua et al., 2009" startWordPosition="227" endWordPosition="230">Our experiments test multiple text representations on two binary classification tasks, change of price and polarity. Experiments show that features derived from semantic frame parsing have significantly better performance across years on the polarity task. 1 Introduction A growing literature evaluates the financial effects of media on the market (Tetlock, 2007; Engelberg and Parsons, 2011). Recent work has applied NLP techniques to various financial media (conventional news, tweets) to detect sentiment in conventional news (Devitt and Ahmad, 2007; Haider and Mehrotra, 2011) or message boards (Chua et al., 2009), or discriminate expert from nonexpert investors in financial tweets (Bar-Haim et al., 2011). With the exception of Bar-Haim et al. (2011), these NLP studies have relied on small corpora of hand-labeled data for training or evaluation, and the connection to market events is done indirectly through sentiment detection. We hypothesize that conventional news can be used to predict changes in the stock price of specific companies, and that the semantic features that best represent relevant aspects of the news vary across ✞ ☎ its first ✝ quarterly earnings report, a week before the April ✆ 20 opti</context>
</contexts>
<marker>Chua, Milosavljevic, Curran, 2009</marker>
<rawString>Christopher Chua, Maria Milosavljevic, and James R. Curran. 2009. A sentiment detection engine for internet stock message boards. In Proceedings of the Australasian Language Technology Association Workshop 2009, pages 89–93, Sydney, Australia, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>263--270</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="16825" citStr="Collins and Duffy, 2002" startWordPosition="2687" endWordPosition="2690">to illustrate tree construction for designated object Oracle. The parse has two frames (Figure 2-(1)&amp;(2)), one corresponding to the main clause (verb sue), and the other for the tenseless adjunct (verb say). The reversed paths extracted from each frame root to the designated object Oracle become the backbones (Figures 2-(3)&amp;(4)). After merging the two backbones we get the resulting SemTree, as shown in Figure 2-(5). By the same steps, this sentence would also yield a SemTree with Google at the root, in the role of EVALUEE. 4.2.2 Kernels and Tree Substructures The tree kernel (Moschitti, 2006; Collins and Duffy, 2002) is a function of tree similarity, based on common substructures (tree fragments). There are two types of substructures. A subtree (ST) is defined as any node of a tree along with all its descendants. A subset tree (SST) is defined as any node along with its immediate children and, optionally, part or all of the children’s descendants. Each tree is represented by a d dimensional vector where the i’th component counts the number of occurrences of the i’th tree fragment. Define the function hi(T) as the number of occurrences of the i’th tree fragment in tree T, so that T is now represented as h(</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: kernels over discrete structures, and the voted perceptron. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 263– 270, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Germ´an G Creamer</author>
<author>Yong Ren</author>
<author>Jeffrey V Nickerson</author>
</authors>
<title>A Longitudinal Analysis of Asset Return, Volatility and Corporate News Network.</title>
<date>2012</date>
<booktitle>In Business Intelligence Congress 3 Proceedings.</booktitle>
<contexts>
<context position="32594" citStr="Creamer et al. (2012)" startWordPosition="5247" endWordPosition="5250">2012. There is clearly an influence of the overall economic context that we do not take into account. For example, accused Wal-Mart 880 the practices of acquiring or selling a business are different in downturning versus recovering markets. An important observation of the MCC values, especially in the case of SemTreeFWD is that MCC increases during the years 2011-2012. We attribute this change to the difficulty of predicting stock price trends when there is the high volatility typical of a financial crisis. The effect of news on volatility, however, can be explored independently. For example, Creamer et al. (2012) detect a strong association. Another weakness of our approach is that we take sentences out of context, which can lead to prediction errors. For example, the sentence Longs’ real estate assets alone are worth some $2.9 billion, or $71.50 per share, Ackman wrote, meaning that CVS would essentially be paying for real estate, but gaining Longs’ pharmacy benefit management business and retail operations for free is treated as predicting a positive polarity for CVS. This would be accurate if CVS was actually going to acquire Longs’ business. Later in the same news item, however, there is a sentenc</context>
</contexts>
<marker>Creamer, Ren, Nickerson, 2012</marker>
<rawString>Germ´an G. Creamer, Yong Ren, and Jeffrey V. Nickerson. 2012. A Longitudinal Analysis of Asset Return, Volatility and Corporate News Network. In Business Intelligence Congress 3 Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Semisupervised frame-semantic parsing for unknown predicates.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>1435--1444</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11511" citStr="Das and Smith, 2011" startWordPosition="1814" endWordPosition="1817">a. Our study incorporates DAL scores along with other features. FrameNet is a rich lexical resource (Fillmore et al., 2003), based on the theory of frame semantics (Fillmore, 1976). There is active research Category Features Value type Frame F, FT, FE N attributes wF, wFT, wFE R&gt;0 BOW UniG, BiG, TriG N wUniG, wBiG, wTriG R&gt;0 pDAL all-Pls, all-Act, all-Img R_µ=0,std=1 VB-Pls, VB-Act, VB-Img R_µ=0,std=1 JJ-Pls, JJ-Act, JJ-Img R_µ=0,std=1 RB-Pls, RB-Act, RB-Img R_µ=0,std=1 Table 1: FWD features (Frame, bag-of-Words, part-of-speech DAL score) and their value types. to build more accurate parsers (Das and Smith, 2011; Das and Smith, 2012). Semantic role labeling using FrameNet has been used to identify an opinion with its holder and topic (Kim and Hovy, 2006). For deep representation of sentiment analysis, Ruppenhofer and Rehbein (2012) propose SentiFrameNet. Our work addresses classification tasks that have potential relevance to an influential financial model (Rydberg and Shephard, 2003). This model decomposes stock price analysis of financial data into a three-part ADS model - activity (a binary process modeling the price move or not), direction (another binary process modeling the direction of the mov</context>
<context position="13007" citStr="Das and Smith, 2011" startWordPosition="2051" endWordPosition="2054"> factor. At present, our goal is limited to the determination of whether NLP features can uncover information from news that could help predict stock price movement or support analysts’ investigations. 4 Methods We propose two approaches for the use of semantic frames. The first is a rich vector space based on semantic frames, word forms and DAL affect scores. The second is a tree representation that encodes semantic frame features, and depends on tree kernel measures for support vector machine classification. The semantic parses of both methods are derived from SEMAFOR1 (Das and Smith, 2012; Das and Smith, 2011), which solves the semantic parsing problem by rule-based target identification, log-linear model based frame identification and frame element filling. 1http://www.ark.cs.cmu.edu/SEMAFOR. 875 Frame (F) Judgment comm. Commerce buy Target (FT) accuse buy sue purchase charge bid Frame COMMUNICATOR BUYER Element EVALUEE SELLER (FE) REASON GOODS Table 2: Sample frames. 4.1 Semantic Frame based FWD Features Table 1 lists 24 types of features, including semantic Frame attributes, bag-of-Words, and scores for words in the Dictionary of Affect in Language by part of speech (pDAL). We refer to these fea</context>
</contexts>
<marker>Das, Smith, 2011</marker>
<rawString>Dipanjan Das and Noah A. Smith. 2011. Semisupervised frame-semantic parsing for unknown predicates. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 1435–1444, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Graph-based lexicon expansion with sparsity-inducing penalties.</title>
<date>2012</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>677--687</pages>
<contexts>
<context position="11533" citStr="Das and Smith, 2012" startWordPosition="1818" endWordPosition="1821">ates DAL scores along with other features. FrameNet is a rich lexical resource (Fillmore et al., 2003), based on the theory of frame semantics (Fillmore, 1976). There is active research Category Features Value type Frame F, FT, FE N attributes wF, wFT, wFE R&gt;0 BOW UniG, BiG, TriG N wUniG, wBiG, wTriG R&gt;0 pDAL all-Pls, all-Act, all-Img R_µ=0,std=1 VB-Pls, VB-Act, VB-Img R_µ=0,std=1 JJ-Pls, JJ-Act, JJ-Img R_µ=0,std=1 RB-Pls, RB-Act, RB-Img R_µ=0,std=1 Table 1: FWD features (Frame, bag-of-Words, part-of-speech DAL score) and their value types. to build more accurate parsers (Das and Smith, 2011; Das and Smith, 2012). Semantic role labeling using FrameNet has been used to identify an opinion with its holder and topic (Kim and Hovy, 2006). For deep representation of sentiment analysis, Ruppenhofer and Rehbein (2012) propose SentiFrameNet. Our work addresses classification tasks that have potential relevance to an influential financial model (Rydberg and Shephard, 2003). This model decomposes stock price analysis of financial data into a three-part ADS model - activity (a binary process modeling the price move or not), direction (another binary process modeling the direction of the moves) and size (a number</context>
<context position="12985" citStr="Das and Smith, 2012" startWordPosition="2047" endWordPosition="2050">l probability of each factor. At present, our goal is limited to the determination of whether NLP features can uncover information from news that could help predict stock price movement or support analysts’ investigations. 4 Methods We propose two approaches for the use of semantic frames. The first is a rich vector space based on semantic frames, word forms and DAL affect scores. The second is a tree representation that encodes semantic frame features, and depends on tree kernel measures for support vector machine classification. The semantic parses of both methods are derived from SEMAFOR1 (Das and Smith, 2012; Das and Smith, 2011), which solves the semantic parsing problem by rule-based target identification, log-linear model based frame identification and frame element filling. 1http://www.ark.cs.cmu.edu/SEMAFOR. 875 Frame (F) Judgment comm. Commerce buy Target (FT) accuse buy sue purchase charge bid Frame COMMUNICATOR BUYER Element EVALUEE SELLER (FE) REASON GOODS Table 2: Sample frames. 4.1 Semantic Frame based FWD Features Table 1 lists 24 types of features, including semantic Frame attributes, bag-of-Words, and scores for words in the Dictionary of Affect in Language by part of speech (pDAL).</context>
</contexts>
<marker>Das, Smith, 2012</marker>
<rawString>Dipanjan Das and Noah A. Smith. 2012. Graph-based lexicon expansion with sparsity-inducing penalties. In HLT-NAACL, pages 677–687. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science.</journal>
<contexts>
<context position="7370" citStr="Deerwester et al., 1990" startWordPosition="1153" endWordPosition="1156">) Oracle has blamed Google and alleged that the latter has committed copyright infringement related to Java programming language held by Oracle. (c) Oracle’s Ellison says couldn’t sway Google on Java. (d) Sentences a, b and c are semantically similar, but lexically rather distinct: the shared words are the company names and Java (programming language). Bag-of-Words (BOW) document representation is difficult to surpass for many document classification tasks, but cannot capture the degree of semantic similarity among these sentences. Methods that have proven successful for paraphrase detection (Deerwester et al., 1990; Dolan et al., 2004), as in the main clauses of b and c, include latent variable models that simultaneously capture the semantics of words and sentences, such as latent semantic analysis (LSA) or latent Dirichlet allocation (LDA). However, our task goes beyond paraphrase detection. The first three sentences all indicate an adversarial relation of Oracle to Google involving a negative judgement. It would be useful to capture the similarities among all three of these sentences, and to distinguish the role of each company (who is suing and who is being sued). Further, these three sentences poten</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Devitt</author>
<author>Khurshid Ahmad</author>
</authors>
<title>Sentiment polarity identification in financial news: A cohesionbased approach.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>984--991</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1439" citStr="Devitt and Ahmad, 2007" startWordPosition="216" endWordPosition="219">in predictive models with tree kernels using support vector machines. Our experiments test multiple text representations on two binary classification tasks, change of price and polarity. Experiments show that features derived from semantic frame parsing have significantly better performance across years on the polarity task. 1 Introduction A growing literature evaluates the financial effects of media on the market (Tetlock, 2007; Engelberg and Parsons, 2011). Recent work has applied NLP techniques to various financial media (conventional news, tweets) to detect sentiment in conventional news (Devitt and Ahmad, 2007; Haider and Mehrotra, 2011) or message boards (Chua et al., 2009), or discriminate expert from nonexpert investors in financial tweets (Bar-Haim et al., 2011). With the exception of Bar-Haim et al. (2011), these NLP studies have relied on small corpora of hand-labeled data for training or evaluation, and the connection to market events is done indirectly through sentiment detection. We hypothesize that conventional news can be used to predict changes in the stock price of specific companies, and that the semantic features that best represent relevant aspects of the news vary across ✞ ☎ its fi</context>
</contexts>
<marker>Devitt, Ahmad, 2007</marker>
<rawString>Ann Devitt and Khurshid Ahmad. 2007. Sentiment polarity identification in financial news: A cohesionbased approach. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 984–991, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>Proceedings of the 20th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="7391" citStr="Dolan et al., 2004" startWordPosition="1157" endWordPosition="1160">e and alleged that the latter has committed copyright infringement related to Java programming language held by Oracle. (c) Oracle’s Ellison says couldn’t sway Google on Java. (d) Sentences a, b and c are semantically similar, but lexically rather distinct: the shared words are the company names and Java (programming language). Bag-of-Words (BOW) document representation is difficult to surpass for many document classification tasks, but cannot capture the degree of semantic similarity among these sentences. Methods that have proven successful for paraphrase detection (Deerwester et al., 1990; Dolan et al., 2004), as in the main clauses of b and c, include latent variable models that simultaneously capture the semantics of words and sentences, such as latent semantic analysis (LSA) or latent Dirichlet allocation (LDA). However, our task goes beyond paraphrase detection. The first three sentences all indicate an adversarial relation of Oracle to Google involving a negative judgement. It would be useful to capture the similarities among all three of these sentences, and to distinguish the role of each company (who is suing and who is being sued). Further, these three sentences potentially have a greater</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>William Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. Proceedings of the 20th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Engelberg</author>
<author>Christopher A Parsons</author>
</authors>
<title>The causal impact of media in financial markets.</title>
<date>2011</date>
<journal>Journal of Finance,</journal>
<volume>66</volume>
<issue>1</issue>
<contexts>
<context position="1279" citStr="Engelberg and Parsons, 2011" startWordPosition="190" endWordPosition="194">rom specific sentences to scenarios, and to detect the (positive or negative) roles of specific companies. We introduce a novel tree representation, and use it to train predictive models with tree kernels using support vector machines. Our experiments test multiple text representations on two binary classification tasks, change of price and polarity. Experiments show that features derived from semantic frame parsing have significantly better performance across years on the polarity task. 1 Introduction A growing literature evaluates the financial effects of media on the market (Tetlock, 2007; Engelberg and Parsons, 2011). Recent work has applied NLP techniques to various financial media (conventional news, tweets) to detect sentiment in conventional news (Devitt and Ahmad, 2007; Haider and Mehrotra, 2011) or message boards (Chua et al., 2009), or discriminate expert from nonexpert investors in financial tweets (Bar-Haim et al., 2011). With the exception of Bar-Haim et al. (2011), these NLP studies have relied on small corpora of hand-labeled data for training or evaluation, and the connection to market events is done indirectly through sentiment detection. We hypothesize that conventional news can be used to </context>
</contexts>
<marker>Engelberg, Parsons, 2011</marker>
<rawString>Joseph Engelberg and Christopher A. Parsons. 2011. The causal impact of media in financial markets. Journal of Finance, 66(1):67–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
<author>Christopher R Johnson</author>
<author>Miriam R L Petruck</author>
</authors>
<title>Background to Framenet.</title>
<date>2003</date>
<journal>International Journal of Lexicography,</journal>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="11015" citStr="Fillmore et al., 2003" startWordPosition="1738" endWordPosition="1741"> document feature engineering as future research directions. We explore a rich feature space that relies on frame semantic parsing. Sentiment analysis figures strongly in NLP work on news. General Inquirer (GI), a content analysis program, is used to quantify pessimism of news in Tetlock (2007) and Tetlock et al. (2008). Other resources for sentiment detection include the Dictionary of Affect in Language (DAL) to score the prior polarity of words, as in Agarwal et al. (2011) on social media data. Our study incorporates DAL scores along with other features. FrameNet is a rich lexical resource (Fillmore et al., 2003), based on the theory of frame semantics (Fillmore, 1976). There is active research Category Features Value type Frame F, FT, FE N attributes wF, wFT, wFE R&gt;0 BOW UniG, BiG, TriG N wUniG, wBiG, wTriG R&gt;0 pDAL all-Pls, all-Act, all-Img R_µ=0,std=1 VB-Pls, VB-Act, VB-Img R_µ=0,std=1 JJ-Pls, JJ-Act, JJ-Img R_µ=0,std=1 RB-Pls, RB-Act, RB-Img R_µ=0,std=1 Table 1: FWD features (Frame, bag-of-Words, part-of-speech DAL score) and their value types. to build more accurate parsers (Das and Smith, 2011; Das and Smith, 2012). Semantic role labeling using FrameNet has been used to identify an opinion with </context>
</contexts>
<marker>Fillmore, Johnson, Petruck, 2003</marker>
<rawString>Charles J. Fillmore, Christopher R. Johnson, and Miriam R. L. Petruck. 2003. Background to Framenet. International Journal of Lexicography, 16(3):235–250, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
</authors>
<title>Frame semantics and the nature of language.</title>
<date>1976</date>
<journal>Annals of the New York Academy of Sciences,</journal>
<volume>280</volume>
<issue>1</issue>
<contexts>
<context position="11072" citStr="Fillmore, 1976" startWordPosition="1750" endWordPosition="1751">explore a rich feature space that relies on frame semantic parsing. Sentiment analysis figures strongly in NLP work on news. General Inquirer (GI), a content analysis program, is used to quantify pessimism of news in Tetlock (2007) and Tetlock et al. (2008). Other resources for sentiment detection include the Dictionary of Affect in Language (DAL) to score the prior polarity of words, as in Agarwal et al. (2011) on social media data. Our study incorporates DAL scores along with other features. FrameNet is a rich lexical resource (Fillmore et al., 2003), based on the theory of frame semantics (Fillmore, 1976). There is active research Category Features Value type Frame F, FT, FE N attributes wF, wFT, wFE R&gt;0 BOW UniG, BiG, TriG N wUniG, wBiG, wTriG R&gt;0 pDAL all-Pls, all-Act, all-Img R_µ=0,std=1 VB-Pls, VB-Act, VB-Img R_µ=0,std=1 JJ-Pls, JJ-Act, JJ-Img R_µ=0,std=1 RB-Pls, RB-Act, RB-Img R_µ=0,std=1 Table 1: FWD features (Frame, bag-of-Words, part-of-speech DAL score) and their value types. to build more accurate parsers (Das and Smith, 2011; Das and Smith, 2012). Semantic role labeling using FrameNet has been used to identify an opinion with its holder and topic (Kim and Hovy, 2006). For deep repre</context>
</contexts>
<marker>Fillmore, 1976</marker>
<rawString>Charles J. Fillmore. 1976. Frame semantics and the nature of language. Annals of the New York Academy of Sciences, 280(1):20–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Syed Aqueel Haider</author>
<author>Rishabh Mehrotra</author>
</authors>
<title>Corporate news classification and valence prediction: A supervised approach.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA 2.011),</booktitle>
<pages>175--181</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon,</location>
<contexts>
<context position="1467" citStr="Haider and Mehrotra, 2011" startWordPosition="220" endWordPosition="223">h tree kernels using support vector machines. Our experiments test multiple text representations on two binary classification tasks, change of price and polarity. Experiments show that features derived from semantic frame parsing have significantly better performance across years on the polarity task. 1 Introduction A growing literature evaluates the financial effects of media on the market (Tetlock, 2007; Engelberg and Parsons, 2011). Recent work has applied NLP techniques to various financial media (conventional news, tweets) to detect sentiment in conventional news (Devitt and Ahmad, 2007; Haider and Mehrotra, 2011) or message boards (Chua et al., 2009), or discriminate expert from nonexpert investors in financial tweets (Bar-Haim et al., 2011). With the exception of Bar-Haim et al. (2011), these NLP studies have relied on small corpora of hand-labeled data for training or evaluation, and the connection to market events is done indirectly through sentiment detection. We hypothesize that conventional news can be used to predict changes in the stock price of specific companies, and that the semantic features that best represent relevant aspects of the news vary across ✞ ☎ its first ✝ quarterly earnings rep</context>
</contexts>
<marker>Haider, Mehrotra, 2011</marker>
<rawString>Syed Aqueel Haider and Rishabh Mehrotra. 2011. Corporate news classification and valence prediction: A supervised approach. In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA 2.011), pages 175–181, Portland, Oregon, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Training linear svms in linear time.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’06,</booktitle>
<pages>217--226</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="24862" citStr="Joachims, 2006" startWordPosition="4018" endWordPosition="4019">or comparison to BOW. gested as one of the best methods to summarize into a single value the confusion matrix of a binary classification task (Jurman and Furlanello, 2010; Baldi et al., 2000). Given the confusion matrix (TP FN) : FP TN MCC = TP·TN−FP·FN √(T P+FP) (T P+FN) (T N+FP) (T N+F N). All sentences with at least one company mention are used for the experiment. We remove stop words and use Stanford CoreNLP for partof-speech tagging and named entity recognition. Models are constructed using linear kernel support vector machines for both classification tasks. SVM-light with tree kernels3 (Joachims, 2006; Moschitti, 2006) is used for both the FWD and SemTree feature spaces. 6.2 Results Table 4 shows the mean MCC values for each task, for each sector. Separate means are shown for the test years of financial crisis (2008-2010) and economic recovery (2011-2012) to highlight the differences in performance that might result from market volatility. 3SVM-light: http://svmlight.joachims.org and Tree Kernels in SVM-light: http://disi.unitn.it/moschitti/TreeKernel.htm. pos. 1 dow, investors, index, retail, data pos. 2 costs, food, price, prices, named entity 4 neu. 1 q3, q1, nov, q2, apr neu. 2 cents, </context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>Thorsten Joachims. 2006. Training linear svms in linear time. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’06, pages 217–226, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Jurman</author>
<author>Cesare Furlanello</author>
</authors>
<title>A unifying view for performance measures in multi-class prediction. ArXiv e-prints.</title>
<date>2010</date>
<contexts>
<context position="24418" citStr="Jurman and Furlanello, 2010" startWordPosition="3941" endWordPosition="3944"> 0.0332 0.0697 0.0763 2011-2012 0.0591 0.0516 0.0764 0.0857 5 years 0.0567 0.0405 0.0723 0.0801 p-value 0.0626 0.0948 0.0103 Telecommunication Services 2008-2010 0.0402 0.0464 0.0821 0.0745 2011-2012 0.0366 0.0781 0.0611 0.0809 5 years 0.0388 0.0591 0.0737 0.0770 p-value »0.1000 0.0950 0.0222 Table 4: Average MCC for the change and polarity tasks by feature representation, for 2008-2010; for 2011-2012; for all 5 years and associated p-values of ANOVAs for comparison to BOW. gested as one of the best methods to summarize into a single value the confusion matrix of a binary classification task (Jurman and Furlanello, 2010; Baldi et al., 2000). Given the confusion matrix (TP FN) : FP TN MCC = TP·TN−FP·FN √(T P+FP) (T P+FN) (T N+FP) (T N+F N). All sentences with at least one company mention are used for the experiment. We remove stop words and use Stanford CoreNLP for partof-speech tagging and named entity recognition. Models are constructed using linear kernel support vector machines for both classification tasks. SVM-light with tree kernels3 (Joachims, 2006; Moschitti, 2006) is used for both the FWD and SemTree feature spaces. 6.2 Results Table 4 shows the mean MCC values for each task, for each sector. Separa</context>
</contexts>
<marker>Jurman, Furlanello, 2010</marker>
<rawString>Giuseppe Jurman and Cesare Furlanello. 2010. A unifying view for performance measures in multi-class prediction. ArXiv e-prints.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Extracting opinions, opinion holders, and topics expressed in online news media text.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Sentiment and Subjectivity in Text, SST ’06,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11656" citStr="Kim and Hovy, 2006" startWordPosition="1840" endWordPosition="1843">of frame semantics (Fillmore, 1976). There is active research Category Features Value type Frame F, FT, FE N attributes wF, wFT, wFE R&gt;0 BOW UniG, BiG, TriG N wUniG, wBiG, wTriG R&gt;0 pDAL all-Pls, all-Act, all-Img R_µ=0,std=1 VB-Pls, VB-Act, VB-Img R_µ=0,std=1 JJ-Pls, JJ-Act, JJ-Img R_µ=0,std=1 RB-Pls, RB-Act, RB-Img R_µ=0,std=1 Table 1: FWD features (Frame, bag-of-Words, part-of-speech DAL score) and their value types. to build more accurate parsers (Das and Smith, 2011; Das and Smith, 2012). Semantic role labeling using FrameNet has been used to identify an opinion with its holder and topic (Kim and Hovy, 2006). For deep representation of sentiment analysis, Ruppenhofer and Rehbein (2012) propose SentiFrameNet. Our work addresses classification tasks that have potential relevance to an influential financial model (Rydberg and Shephard, 2003). This model decomposes stock price analysis of financial data into a three-part ADS model - activity (a binary process modeling the price move or not), direction (another binary process modeling the direction of the moves) and size (a number quantifying the size of the moves). Our two binary classification tasks for news, price change and polarity, are analogous</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2006. Extracting opinions, opinion holders, and topics expressed in online news media text. In Proceedings of the Workshop on Sentiment and Subjectivity in Text, SST ’06, pages 1–8, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shimon Kogan</author>
<author>Dimitry Levin</author>
<author>Bryan R Routledge</author>
<author>Jacob S Sagi</author>
<author>Noah A Smith</author>
</authors>
<title>Predicting risk from financial reports with regression.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>272--280</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9561" citStr="Kogan et al. (2009)" startWordPosition="1499" endWordPosition="1502">n improve our understanding of a company’s fundamental market value, and whether linguistic information derived from news produces a consistent enough result to benefit more comprehensive financial models. 3 Related Work NLP has recently been applied to financial text for market analysis, primarily using bag-ofwords (BOW) document representation. Luss and d’Aspremont (2008) use text classification to model price movements of financial assets on a per-day basis. They try to predict the direction of return, and abnormal returns, defined as an absolute return greater than a predefined threshold. Kogan et al. (2009) address a text regression problem to predict the financial risk of investment in companies. They analyze 10-K reports to predict stock return volatility. They also predict whether a company will be delisted following its 10-K report. Ruiz et al. (2012) correlate text with financial time series volume and price data. They find that graph centrality measures like page rank and degree are more strongly correlated to both price and traded volume for an aggregation of similar companies, while individual stocks are less correlated. Lavrenko et al. (2000) present an approach to identify news stories</context>
</contexts>
<marker>Kogan, Levin, Routledge, Sagi, Smith, 2009</marker>
<rawString>Shimon Kogan, Dimitry Levin, Bryan R. Routledge, Jacob S. Sagi, and Noah A. Smith. 2009. Predicting risk from financial reports with regression. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 272–280, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>Matt Schmill</author>
<author>Dawn Lawrie</author>
<author>Paul Ogilvie</author>
<author>David Jensen</author>
<author>James Allan</author>
</authors>
<title>Mining of concurrent text and time series. In</title>
<date>2000</date>
<booktitle>In proceedings of the 6th ACM SIGKDD Int’l Conference on Knowledge Discovery and Data Mining Workshop on Text Mining,</booktitle>
<pages>37--44</pages>
<contexts>
<context position="10116" citStr="Lavrenko et al. (2000)" startWordPosition="1590" endWordPosition="1593">ute return greater than a predefined threshold. Kogan et al. (2009) address a text regression problem to predict the financial risk of investment in companies. They analyze 10-K reports to predict stock return volatility. They also predict whether a company will be delisted following its 10-K report. Ruiz et al. (2012) correlate text with financial time series volume and price data. They find that graph centrality measures like page rank and degree are more strongly correlated to both price and traded volume for an aggregation of similar companies, while individual stocks are less correlated. Lavrenko et al. (2000) present an approach to identify news stories that influence the behavior of financial markets, and predict trends in stock prices based on the content of news stories that precede the trends. Luss and d’Aspremont (2008) and Lavrenko et al. (2000) both point out the desire for document feature engineering as future research directions. We explore a rich feature space that relies on frame semantic parsing. Sentiment analysis figures strongly in NLP work on news. General Inquirer (GI), a content analysis program, is used to quantify pessimism of news in Tetlock (2007) and Tetlock et al. (2008). </context>
</contexts>
<marker>Lavrenko, Schmill, Lawrie, Ogilvie, Jensen, Allan, 2000</marker>
<rawString>Victor Lavrenko, Matt Schmill, Dawn Lawrie, Paul Ogilvie, David Jensen, and James Allan. 2000. Mining of concurrent text and time series. In In proceedings of the 6th ACM SIGKDD Int’l Conference on Knowledge Discovery and Data Mining Workshop on Text Mining, pages 37–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronny Luss</author>
<author>Alexandre d’Aspremont</author>
</authors>
<title>Predicting abnormal returns from news using text classification.</title>
<date>2008</date>
<location>CoRR, abs/0809.2792.</location>
<marker>Luss, d’Aspremont, 2008</marker>
<rawString>Ronny Luss and Alexandre d’Aspremont. 2008. Predicting abnormal returns from news using text classification. CoRR, abs/0809.2792.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian W Matthews</author>
</authors>
<title>Comparison of the predicted and observed secondary structure of t4 phage lysozyme.</title>
<date>1975</date>
<journal>Biochimica et Biophysica Acta (BBA) -Protein Structure,</journal>
<volume>405</volume>
<issue>2</issue>
<pages>451</pages>
<contexts>
<context position="22366" citStr="Matthews, 1975" startWordPosition="3616" endWordPosition="3617">or our experiments includes an economic crisis followed by a recovery period, we note that the ratio between increase and decrease of price flips between 2007, where it is 1.40, and 2008, where it is 0.71. Accuracy is very sensitive to skew: when a class has low frequency, accuracy can be high using a baseline that makes prediction on the majority class. Given the high data skew, and the large changes from year to year in positive versus negative skew, we use a more robust evaluation metric. Our evaluation relies on the Matthews correlation coefficient (MCC, also known as the φ- coefficient) (Matthews, 1975) to avoid the bias of accuracy due to data skew, and to produce a robust summary score independent of whether the positive class is skewed to the majority or minority. In contrast to f-measure, which is a classspecific weighted average of precision and recall, and whose weighted version depends on a choice of whether the class-specific weights should come from the training or testing data, MCC is a single summary value that incorporates all 4 cells of a 2 × 2 confusion matrix (TP, FP, TN and FN for True or False Positive or Negative). We have also observed that MCC has a lower relative standar</context>
</contexts>
<marker>Matthews, 1975</marker>
<rawString>Brian W. Matthews. 1975. Comparison of the predicted and observed secondary structure of t4 phage lysozyme. Biochimica et Biophysica Acta (BBA) -Protein Structure, 405(2):442 – 451.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Making tree kernels practical for natural language learning. In</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="16799" citStr="Moschitti, 2006" startWordPosition="2685" endWordPosition="2686">ion 2; we use it to illustrate tree construction for designated object Oracle. The parse has two frames (Figure 2-(1)&amp;(2)), one corresponding to the main clause (verb sue), and the other for the tenseless adjunct (verb say). The reversed paths extracted from each frame root to the designated object Oracle become the backbones (Figures 2-(3)&amp;(4)). After merging the two backbones we get the resulting SemTree, as shown in Figure 2-(5). By the same steps, this sentence would also yield a SemTree with Google at the root, in the role of EVALUEE. 4.2.2 Kernels and Tree Substructures The tree kernel (Moschitti, 2006; Collins and Duffy, 2002) is a function of tree similarity, based on common substructures (tree fragments). There are two types of substructures. A subtree (ST) is defined as any node of a tree along with all its descendants. A subset tree (SST) is defined as any node along with its immediate children and, optionally, part or all of the children’s descendants. Each tree is represented by a d dimensional vector where the i’th component counts the number of occurrences of the i’th tree fragment. Define the function hi(T) as the number of occurrences of the i’th tree fragment in tree T, so that </context>
<context position="24880" citStr="Moschitti, 2006" startWordPosition="4020" endWordPosition="4021"> BOW. gested as one of the best methods to summarize into a single value the confusion matrix of a binary classification task (Jurman and Furlanello, 2010; Baldi et al., 2000). Given the confusion matrix (TP FN) : FP TN MCC = TP·TN−FP·FN √(T P+FP) (T P+FN) (T N+FP) (T N+F N). All sentences with at least one company mention are used for the experiment. We remove stop words and use Stanford CoreNLP for partof-speech tagging and named entity recognition. Models are constructed using linear kernel support vector machines for both classification tasks. SVM-light with tree kernels3 (Joachims, 2006; Moschitti, 2006) is used for both the FWD and SemTree feature spaces. 6.2 Results Table 4 shows the mean MCC values for each task, for each sector. Separate means are shown for the test years of financial crisis (2008-2010) and economic recovery (2011-2012) to highlight the differences in performance that might result from market volatility. 3SVM-light: http://svmlight.joachims.org and Tree Kernels in SVM-light: http://disi.unitn.it/moschitti/TreeKernel.htm. pos. 1 dow, investors, index, retail, data pos. 2 costs, food, price, prices, named entity 4 neu. 1 q3, q1, nov, q2, apr neu. 2 cents, million, share, ye</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Making tree kernels practical for natural language learning. In In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniele Pighin</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Reverse engineering of tree kernel feature spaces.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP</booktitle>
<pages>6--7</pages>
<contexts>
<context position="29353" citStr="Pighin and Moschitti (2009)" startWordPosition="4705" endWordPosition="4708">n tested here that is significantly better than BOW, but only for the polarity task. Post hoc analysis indicates this may be due to the aptness of semantic frame parsing for polarity. Limitations in our treatment of time point to directions for improvement regarding the change task. Some strengths of our approach are the separate treatment of different sectors, and the benefits of SemTree features. To analyze which were the best performing features within sectors, we extracted the best performing frame fragments for the polarity task using a tree kernel feature engineering method presented in Pighin and Moschitti (2009). The algorithm selects the most relevant features in accordance with the weights estimated by SVM, and uses these features to build an explicit representation of the kernel space. Figure 3 shows the best performing SemTree fragments of the polarity task for the consumer staples sector. Recall that we hypothesized differences in semantic frame features across sectors. This shows up as large differences in the strength of features across sectors. More strikingly, the same feature can differ in polarity across sectors. For example, in consumer staples, (EVALUEE(Judgment communication)) has posit</context>
</contexts>
<marker>Pighin, Moschitti, 2009</marker>
<rawString>Daniele Pighin and Alessandro Moschitti. 2009. Reverse engineering of tree kernel feature spaces. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP 2009, 6-7August 2009, Singapore, pages 111–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduardo J Ruiz</author>
<author>Vagelis Hristidis</author>
<author>Carlos Castillo</author>
<author>Aristides Gionis</author>
<author>Alejandro Jaimes</author>
</authors>
<title>Correlating financial time series with micro-blogging activity.</title>
<date>2012</date>
<booktitle>In Proceedings of the fifth ACM international conference on Web search and data mining, WSDM ’12,</booktitle>
<pages>513--522</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="9814" citStr="Ruiz et al. (2012)" startWordPosition="1541" endWordPosition="1544">financial text for market analysis, primarily using bag-ofwords (BOW) document representation. Luss and d’Aspremont (2008) use text classification to model price movements of financial assets on a per-day basis. They try to predict the direction of return, and abnormal returns, defined as an absolute return greater than a predefined threshold. Kogan et al. (2009) address a text regression problem to predict the financial risk of investment in companies. They analyze 10-K reports to predict stock return volatility. They also predict whether a company will be delisted following its 10-K report. Ruiz et al. (2012) correlate text with financial time series volume and price data. They find that graph centrality measures like page rank and degree are more strongly correlated to both price and traded volume for an aggregation of similar companies, while individual stocks are less correlated. Lavrenko et al. (2000) present an approach to identify news stories that influence the behavior of financial markets, and predict trends in stock prices based on the content of news stories that precede the trends. Luss and d’Aspremont (2008) and Lavrenko et al. (2000) both point out the desire for document feature eng</context>
</contexts>
<marker>Ruiz, Hristidis, Castillo, Gionis, Jaimes, 2012</marker>
<rawString>Eduardo J. Ruiz, Vagelis Hristidis, Carlos Castillo, Aristides Gionis, and Alejandro Jaimes. 2012. Correlating financial time series with micro-blogging activity. In Proceedings of the fifth ACM international conference on Web search and data mining, WSDM ’12, pages 513–522, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Ines Rehbein</author>
</authors>
<title>Semantic frames as an anchor representation for sentiment analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis, WASSA ’12,</booktitle>
<pages>104--109</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3863" citStr="Ruppenhofer and Rehbein, 2012" startWordPosition="610" endWordPosition="613">ts a large change in Google’s stock price. Google’s early announcement of quarterly earnings possibly presages trouble, and its stock price falls soon after reports of a legal action against Google by Oracle. To produce a coherent story, the original sentences were edited for Figure 1, but they are in the style of actual sentences from our dataset. Accurate detection of events and relations that might have an impact on stock price should benefit from document representation that captures sentiment in lexical items (e.g., aggressive) combined with the conceptual relations captured by FrameNet (Ruppenhofer and Rehbein, 2012). A frame is a lexical semantic representanews reported Oracle Corp claiming 873 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 873–883, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tion of the conceptual roles played by parts of a clause, and relates different lexical items (e.g., report, announce) to the same situation type. In the figure, some of the words that evoke frames have been underlined, and role fillers are outlined by boxes or ovals. Sentiment words are in italics. To the best of our knowledge, t</context>
<context position="11735" citStr="Ruppenhofer and Rehbein (2012)" startWordPosition="1851" endWordPosition="1854">ry Features Value type Frame F, FT, FE N attributes wF, wFT, wFE R&gt;0 BOW UniG, BiG, TriG N wUniG, wBiG, wTriG R&gt;0 pDAL all-Pls, all-Act, all-Img R_µ=0,std=1 VB-Pls, VB-Act, VB-Img R_µ=0,std=1 JJ-Pls, JJ-Act, JJ-Img R_µ=0,std=1 RB-Pls, RB-Act, RB-Img R_µ=0,std=1 Table 1: FWD features (Frame, bag-of-Words, part-of-speech DAL score) and their value types. to build more accurate parsers (Das and Smith, 2011; Das and Smith, 2012). Semantic role labeling using FrameNet has been used to identify an opinion with its holder and topic (Kim and Hovy, 2006). For deep representation of sentiment analysis, Ruppenhofer and Rehbein (2012) propose SentiFrameNet. Our work addresses classification tasks that have potential relevance to an influential financial model (Rydberg and Shephard, 2003). This model decomposes stock price analysis of financial data into a three-part ADS model - activity (a binary process modeling the price move or not), direction (another binary process modeling the direction of the moves) and size (a number quantifying the size of the moves). Our two binary classification tasks for news, price change and polarity, are analogous to their activity and direction. In contrast to the ADS model, our approach do</context>
</contexts>
<marker>Ruppenhofer, Rehbein, 2012</marker>
<rawString>Josef Ruppenhofer and Ines Rehbein. 2012. Semantic frames as an anchor representation for sentiment analysis. In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis, WASSA ’12, pages 104– 109, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tina H Rydberg</author>
<author>Neil Shephard</author>
</authors>
<title>Dynamics of Trade-by-Trade Price Movements: Decomposition and Models.</title>
<date>2003</date>
<journal>Journal of Financial Econometrics,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="5140" citStr="Rydberg and Shephard (2003)" startWordPosition="814" endWordPosition="817"> this domain. On the polarity task, the semantic frame features encoded as trees perform significantly better across years and sectors than bag-of-words vectors (BOW), and outperform BOW vectors enhanced with semantic frame features, and a supervised topic modeling approach. The results on the price change task show the same trend, but are not statistically significant, possibly due to the volatility of the market in 2007 and the following several years. Yet even modest predictive performance on both tasks could have an impact, as discussed below, if incorporated into financial models such as Rydberg and Shephard (2003). We first discuss the motivation and related work. Section 4 presents vector-based and tree-based features from semantic frame parses, and section 5 describes our dataset. The experimental design and results appear in the following section, followed by discussion and conclusions. 2 Motivation Financial news is a rich vein for NLP applications to mine. Many news organizations that feature financial news, such as Reuters, the Wall Street Journal and Bloomberg, devote significant resources to the analysis of corporate news. Much of the data that would support studies of a link between the news m</context>
<context position="11891" citStr="Rydberg and Shephard, 2003" startWordPosition="1871" endWordPosition="1874">-Pls, VB-Act, VB-Img R_µ=0,std=1 JJ-Pls, JJ-Act, JJ-Img R_µ=0,std=1 RB-Pls, RB-Act, RB-Img R_µ=0,std=1 Table 1: FWD features (Frame, bag-of-Words, part-of-speech DAL score) and their value types. to build more accurate parsers (Das and Smith, 2011; Das and Smith, 2012). Semantic role labeling using FrameNet has been used to identify an opinion with its holder and topic (Kim and Hovy, 2006). For deep representation of sentiment analysis, Ruppenhofer and Rehbein (2012) propose SentiFrameNet. Our work addresses classification tasks that have potential relevance to an influential financial model (Rydberg and Shephard, 2003). This model decomposes stock price analysis of financial data into a three-part ADS model - activity (a binary process modeling the price move or not), direction (another binary process modeling the direction of the moves) and size (a number quantifying the size of the moves). Our two binary classification tasks for news, price change and polarity, are analogous to their activity and direction. In contrast to the ADS model, our approach does not calculate the conditional probability of each factor. At present, our goal is limited to the determination of whether NLP features can uncover inform</context>
</contexts>
<marker>Rydberg, Shephard, 2003</marker>
<rawString>Tina H. Rydberg and Neil Shephard. 2003. Dynamics of Trade-by-Trade Price Movements: Decomposition and Models. Journal of Financial Econometrics, 1(1):2–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul C Tetlock</author>
<author>Maytal Saar-Tsechansky</author>
<author>Sofus Macskassy</author>
</authors>
<title>More than Words: Quantifying Language to Measure Firms’ Fundamentals. The Journal of Finance.</title>
<date>2008</date>
<contexts>
<context position="5823" citStr="Tetlock et al. (2008)" startWordPosition="926" endWordPosition="929">presents vector-based and tree-based features from semantic frame parses, and section 5 describes our dataset. The experimental design and results appear in the following section, followed by discussion and conclusions. 2 Motivation Financial news is a rich vein for NLP applications to mine. Many news organizations that feature financial news, such as Reuters, the Wall Street Journal and Bloomberg, devote significant resources to the analysis of corporate news. Much of the data that would support studies of a link between the news media and the market are publicly available. As pointed out by Tetlock et al. (2008), linguistic communication is a potentially important source of information about firms’ fundamental values. Because very few stock market investors directly observe firms’ production activities, they get most of their information secondhand. Their three main sources are analysts’ forecasts, quantifiable publicly disclosed accounting variables, and descriptions of firms’ current and future profit-generating activities. If analyst and accounting variables are incomplete or biased measures of firms’ fundamental values, linguistic variables may have incremental explanatory power for firms’ future</context>
<context position="10714" citStr="Tetlock et al. (2008)" startWordPosition="1688" endWordPosition="1691"> Lavrenko et al. (2000) present an approach to identify news stories that influence the behavior of financial markets, and predict trends in stock prices based on the content of news stories that precede the trends. Luss and d’Aspremont (2008) and Lavrenko et al. (2000) both point out the desire for document feature engineering as future research directions. We explore a rich feature space that relies on frame semantic parsing. Sentiment analysis figures strongly in NLP work on news. General Inquirer (GI), a content analysis program, is used to quantify pessimism of news in Tetlock (2007) and Tetlock et al. (2008). Other resources for sentiment detection include the Dictionary of Affect in Language (DAL) to score the prior polarity of words, as in Agarwal et al. (2011) on social media data. Our study incorporates DAL scores along with other features. FrameNet is a rich lexical resource (Fillmore et al., 2003), based on the theory of frame semantics (Fillmore, 1976). There is active research Category Features Value type Frame F, FT, FE N attributes wF, wFT, wFE R&gt;0 BOW UniG, BiG, TriG N wUniG, wBiG, wTriG R&gt;0 pDAL all-Pls, all-Act, all-Img R_µ=0,std=1 VB-Pls, VB-Act, VB-Img R_µ=0,std=1 JJ-Pls, JJ-Act, J</context>
<context position="20840" citStr="Tetlock et al. (2008)" startWordPosition="3337" endWordPosition="3340">able daily stock price data from Yahoo Finance with the Reuters news using a method to avoid back-casting. In particular, we use the daily adjusted closing price - the price quoted at the end of a trading day (4PM US Eastern Time), then adjusted by dividends, stock split, and other corporate actions. We create two types of labels for news documents using the price data, to label the existence of a change and the direction of change. Both tasks are treated as binary classification problems. Based on the finding of a one-day delay of the price response to the information embedded in the news by Tetlock et al. (2008), we use At = 1 in our experiment. To constrain the number of parameters, we also use a threshold value (r) of a 2% change, based on the distribution of price changes across our data. In future work, this could be tuned to sector or time. � +1 if |pt(0)+ot−pt(−1) |&gt; r change= pt(−1) −1 otherwise �polarity= +1 +1 if pt(0)+Δt &gt; pt(− 1) and change = +1 −1 if pt(0)+Δt &lt; pt(−1) and change = +1 pt(−1) is the adjusted closing price at the end of the last trading day, and pt(0)+Δt is the price of the end of the trading day after the At day delay. Only the instances with changes are included in the pol</context>
</contexts>
<marker>Tetlock, Saar-Tsechansky, Macskassy, 2008</marker>
<rawString>Paul C. Tetlock, Maytal Saar-Tsechansky, and Sofus Macskassy. 2008. More than Words: Quantifying Language to Measure Firms’ Fundamentals. The Journal of Finance.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul C Tetlock</author>
</authors>
<title>Giving Content to Investor Sentiment: The Role of Media in the Stock Market. The Journal of Finance.</title>
<date>2007</date>
<contexts>
<context position="1249" citStr="Tetlock, 2007" startWordPosition="188" endWordPosition="189">to generalize from specific sentences to scenarios, and to detect the (positive or negative) roles of specific companies. We introduce a novel tree representation, and use it to train predictive models with tree kernels using support vector machines. Our experiments test multiple text representations on two binary classification tasks, change of price and polarity. Experiments show that features derived from semantic frame parsing have significantly better performance across years on the polarity task. 1 Introduction A growing literature evaluates the financial effects of media on the market (Tetlock, 2007; Engelberg and Parsons, 2011). Recent work has applied NLP techniques to various financial media (conventional news, tweets) to detect sentiment in conventional news (Devitt and Ahmad, 2007; Haider and Mehrotra, 2011) or message boards (Chua et al., 2009), or discriminate expert from nonexpert investors in financial tweets (Bar-Haim et al., 2011). With the exception of Bar-Haim et al. (2011), these NLP studies have relied on small corpora of hand-labeled data for training or evaluation, and the connection to market events is done indirectly through sentiment detection. We hypothesize that con</context>
<context position="10688" citStr="Tetlock (2007)" startWordPosition="1685" endWordPosition="1686">re less correlated. Lavrenko et al. (2000) present an approach to identify news stories that influence the behavior of financial markets, and predict trends in stock prices based on the content of news stories that precede the trends. Luss and d’Aspremont (2008) and Lavrenko et al. (2000) both point out the desire for document feature engineering as future research directions. We explore a rich feature space that relies on frame semantic parsing. Sentiment analysis figures strongly in NLP work on news. General Inquirer (GI), a content analysis program, is used to quantify pessimism of news in Tetlock (2007) and Tetlock et al. (2008). Other resources for sentiment detection include the Dictionary of Affect in Language (DAL) to score the prior polarity of words, as in Agarwal et al. (2011) on social media data. Our study incorporates DAL scores along with other features. FrameNet is a rich lexical resource (Fillmore et al., 2003), based on the theory of frame semantics (Fillmore, 1976). There is active research Category Features Value type Frame F, FT, FE N attributes wF, wFT, wFE R&gt;0 BOW UniG, BiG, TriG N wUniG, wBiG, wTriG R&gt;0 pDAL all-Pls, all-Act, all-Img R_µ=0,std=1 VB-Pls, VB-Act, VB-Img R_µ</context>
</contexts>
<marker>Tetlock, 2007</marker>
<rawString>Paul C. Tetlock. 2007. Giving Content to Investor Sentiment: The Role of Media in the Stock Market. The Journal of Finance.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia M Whissel</author>
</authors>
<title>The dictionary of affect in language.</title>
<date>1989</date>
<journal>Emotion: Theory, Research, and Experience,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="14762" citStr="Whissel, 1989" startWordPosition="2346" endWordPosition="2347">. Here we use F for the frame name, FT for the target words, and FE for frame elements. We use both frequency and weighted scores. For example, we define idf-adjusted weighted frame features, such as wF for attribute F in document d as wFF,d = f(F, d) X log |D| |d∈D:F∈d|, where f(F, d) is the frequency of frame F in d, D is the whole document set and |· |is the cardinality operator. Bag-of-Words features include term frequency and tfidf of unigrams, bigrams, and trigrams. DAL (Dictionary of Affect in Language) is a psycholinguistic resource to measure the emotional meaning of words and texts (Whissel, 1989). It includes 8,742 words that were annotated for three dimensions: Pleasantness (Pls), Activation (Act), and Imagery (Img). Agarwal et al. (2009) introduced part-of-speech specific DAL features for sentiment analysis. We follow their approach by averaging the scores for all words, verb only, adjective only, and adverb only words. Feature values are normalized to mean of zero and standard deviation of one. 4.2 SemTree Feature Space and Kernels We propose SemTree as another feature space to encode semantic information in trees. SemTree can distinguish the roles of each company of interest, or d</context>
</contexts>
<marker>Whissel, 1989</marker>
<rawString>Cynthia M. Whissel. 1989. The dictionary of affect in language. Emotion: Theory, Research, and Experience, 39(4):113–131.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>