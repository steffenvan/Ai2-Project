<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996671">
A Hybrid Hierarchical Model for Multi-Document Summarization
</title>
<author confidence="0.9811">
Asli Celikyilmaz Dilek Hakkani-Tur
</author>
<affiliation confidence="0.9989405">
Computer Science Department International Computer Science Institute
University of California, Berkeley Berkeley, CA
</affiliation>
<email confidence="0.996497">
asli@eecs.berkeley.edu dilek@icsi.berkeley.edu
</email>
<sectionHeader confidence="0.994739" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99972775">
Scoring sentences in documents given ab-
stract summaries created by humans is im-
portant in extractive multi-document sum-
marization. In this paper, we formulate ex-
tractive summarization as a two step learn-
ing problem building a generative model
for pattern discovery and a regression
model for inference. We calculate scores
for sentences in document clusters based
on their latent characteristics using a hi-
erarchical topic model. Then, using these
scores, we train a regression model based
on the lexical and structural characteris-
tics of the sentences, and use the model to
score sentences of new documents to form
a summary. Our system advances current
state-of-the-art improving ROUGE scores
by —7%. Generated summaries are less
redundant and more coherent based upon
manual quality evaluations.
</bodyText>
<sectionHeader confidence="0.998785" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999937847457627">
Extractive approach to multi-document summa-
rization (MDS) produces a summary by select-
ing sentences from original documents. Doc-
ument Understanding Conferences (DUC), now
TAC, fosters the effort on building MDS systems,
which take document clusters (documents on a
same topic) and description of the desired sum-
mary focus as input and output a word length lim-
ited summary. Human summaries are provided for
training summarization models and measuring the
performance of machine generated summaries.
Extractive summarization methods can be clas-
sified into two groups: supervised methods that
rely on provided document-summary pairs, and
unsupervised methods based upon properties de-
rived from document clusters. Supervised meth-
ods treat the summarization task as a classifica-
tion/regression problem, e.g., (Shen et al., 2007;
Yeh et al., 2005). Each candidate sentence is
classified as summary or non-summary based on
the features that they pose and those with high-
est scores are selected. Unsupervised methods
aim to score sentences based on semantic group-
ings extracted from documents, e.g., (DaumeIII
and Marcu, 2006; Titov and McDonald, 2008;
Tang et al., 2009; Haghighi and Vanderwende,
2009; Radev et al., 2004; Branavan et al., 2009),
etc. Such models can yield comparable or bet-
ter performance on DUC and other evaluations,
since representing documents as topic distribu-
tions rather than bags of words diminishes the ef-
fect of lexical variability. To the best of our knowl-
edge, there is no previous research which utilizes
the best features of both approaches for MDS as
presented in this paper.
In this paper, we present a novel approach that
formulates MDS as a prediction problem based
on a two-step hybrid model: a generative model
for hierarchical topic discovery and a regression
model for inference. We investigate if a hierarchi-
cal model can be adopted to discover salient char-
acteristics of sentences organized into hierarchies
utilizing human generated summary text.
We present a probabilistic topic model on sen-
tence level building on hierarchical Latent Dirich-
let Allocation (hLDA) (Blei et al., 2003a), which
is a generalization of LDA (Blei et al., 2003b). We
construct a hybrid learning algorithm by extract-
ing salient features to characterize summary sen-
tences, and implement a regression model for in-
ference (Fig.3). Contributions of this work are:
— construction of hierarchical probabilistic model
designed to discover the topic structures of all sen-
tences. Our focus is on identifying similarities of
candidate sentences to summary sentences using a
novel tree based sentence scoring algorithm, con-
cerning topic distributions at different levels of the
discovered hierarchy as described in § 3 and § 4,
— representation of sentences by meta-features to
</bodyText>
<page confidence="0.981053">
815
</page>
<note confidence="0.943438">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 815–824,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999643142857143">
characterize their candidacy for inclusion in sum-
mary text. Our aim is to find features that can best
represent summary sentences as described in § 5,
− implementation of a feasible inference method
based on a regression model to enable scoring of
sentences in test document clusters without re-
training, (which has not been investigated in gen-
erative summarization models) described in § 5.2.
We show in § 6 that our hybrid summarizer
achieves comparable (if not better) ROUGE score
on the challenging task of extracting the sum-
maries of multiple newswire documents. The hu-
man evaluations confirm that our hybrid model can
produce coherent and non-redundant summaries.
</bodyText>
<sectionHeader confidence="0.885218" genericHeader="introduction">
2 Background and Motivation
</sectionHeader>
<bodyText confidence="0.999967033333334">
There are many studies on the principles govern-
ing multi-document summarization to produce co-
herent and semantically relevant summaries. Pre-
vious work (Nenkova and Vanderwende, 2005;
Conroy et al., 2006), focused on the fact that fre-
quency of words plays an important factor. While,
earlier work on summarization depend on a word
score function, which is used to measure sentence
rank scores based on (semi-)supervised learn-
ing methods, recent trend of purely data-driven
methods, (Barzilay and Lee, 2004; DaumeIII and
Marcu, 2006; Tang et al., 2009; Haghighi and
Vanderwende, 2009), have shown remarkable im-
provements. Our work builds on both methods by
constructing a hybrid approach to summarization.
Our objective is to discover from document
clusters, the latent topics that are organized into hi-
erarchies following (Haghighi and Vanderwende,
2009). A hierarchical model is particularly ap-
pealing to summarization than a ”flat” model, e.g.
LDA (Blei et al., 2003b), in that one can discover
”abstract” and ”specific” topics. For instance, dis-
covering that ”baseball” and ”football” are both
contained in an abstract class ”sports” can help to
identify summary sentences. It follows that sum-
mary topics are commonly shared by many docu-
ments, while specific topics are more likely to be
mentioned in rather a small subset of documents.
Feature based learning approaches to summa-
rization methods discover salient features by mea-
suring similarity between candidate sentences and
summary sentences (Nenkova and Vanderwende,
2005; Conroy et al., 2006). While such methods
are effective in extractive summarization, the fact
that some of these methods are based on greedy
algorithms can limit the application areas. More-
over, using information on the hidden semantic
structure of document clusters would improve the
performance of these methods.
Recent studies focused on the discovery of la-
tent topics of document sets in extracting sum-
maries. In these models, the challenges of infer-
ring topics of test documents are not addressed
in detail. One of the challenges of using a pre-
viously trained topic model is that the new docu-
ment might have a totally new vocabulary or may
include many other specific topics, which may or
may not exist in the trained model. A common
method is to re-build a topic model for new sets
of documents (Haghighi and Vanderwende, 2009),
which has proven to produce coherent summaries.
An alternative yet feasible solution, presented in
this work, is building a model that can summa-
rize new document clusters using characteristics
of topic distributions of training documents. Our
approach differs from the early work, in that, we
combine a generative hierarchical model and re-
gression model to score sentences in new docu-
ments, eliminating the need for building a genera-
tive model for new document clusters.
</bodyText>
<sectionHeader confidence="0.988002" genericHeader="method">
3 Summary-Focused Hierarchical Model
</sectionHeader>
<bodyText confidence="0.964329545454546">
Our MDS system, hybrid hierarchical summa-
rizer, HybHSum, is based on an hybrid learn-
ing approach to extract sentences for generating
summary. We discover hidden topic distributions
of sentences in a given document cluster along
with provided summary sentences based on hLDA
described in (Blei et al., 2003a)1. We build a
summary-focused hierarchical probabilistic topic
model, sumHLDA, for each document cluster at
sentence level, because it enables capturing ex-
pected topic distributions in given sentences di-
rectly from the model. Besides, document clusters
contain a relatively small number of documents,
which may limit the variability of topics if they are
evaluated on the document level. As described in §
4, we present a new method for scoring candidate
sentences from this hierarchical structure.
Let a given document cluster D be represented
with sentences O={om}�O�
m�1 and its corresponding
human summary be represented with sentences
S={sn} n�1. All sentences are comprised of words
</bodyText>
<equation confidence="0.475585">
V = {w1, w2, ..w|V  |} in 1O U S}.
</equation>
<footnote confidence="0.983461">
1Please refer to (Blei et al., 2003b) and (Blei et al., 2003a)
for details and demonstrations of topic models.
</footnote>
<page confidence="0.998523">
816
</page>
<bodyText confidence="0.99995252173913">
Summary hLDA (sumHLDA): The hLDA
represents distribution of topics in sentences by
organizing topics into a tree of a fixed depth L
(Fig.1.a). Each candidate sentence om is assigned
to a path co� in the tree and each word wi in a
given sentence is assigned to a hidden topic zo�
at a level l of co�. Each node is associated with a
topic distribution over words. The sampler method
alternates between choosing a new path for each
sentence through the tree and assigning each word
in each sentence to a topic along that path. The
structure of tree is learnt along with the topics us-
ing a nested Chinese restaurant process (nCRP)
(Blei et al., 2003a), which is used as a prior.
The nCRP is a stochastic process, which as-
signs probability distributions to infinitely branch-
ing and infinitely deep trees. In our model, nCRP
specifies a distribution of words into paths in an
L-level tree. The assignments of sentences to
paths are sampled sequentially: The first sentence
takes the initial L-level path, starting with a sin-
gle branch tree. Later, mth subsequent sentence is
assigned to a path drawn from the distribution:
</bodyText>
<equation confidence="0.9968055">
p(pathold, c|m, mc) = m� (1)
γ+m�1
γ
p(pathnew, c|m, mc) = γ+m�1
</equation>
<bodyText confidence="0.999846736842105">
pathold and pathnew represent an existing and
novel (branch) path consecutively, mc is the num-
ber of previous sentences assigned to path c, m is
the total number of sentences seen so far, and -y is
a hyper-parameter which controls the probability
of creating new paths. Based on this probability
each node can branch out a different number of
child nodes proportional to -y. Small values of -y
suppress the number of branches.
Summary sentences generally comprise abstract
concepts of the content. With sumHLDA we want
to capture these abstract concepts in candidate sen-
tences. The idea is to represent each path shared
by similar candidate sentences with representative
summary sentence(s). We let summary sentences
share existing paths generated by similar candi-
date sentences instead of sampling new paths and
influence the tree structure by introducing two sep-
arate hyper-parameters for nCRP prior:
</bodyText>
<listItem confidence="0.9730045">
• if a summary sentence is sampled, use -y = -ys,
• if a candidate sentence is sampled, use -y = -yo.
At each node, we let summary sentences sample
a path by choosing only from the existing children
of that node with a probability proportional to the
number of other sentences assigned to that child.
</listItem>
<bodyText confidence="0.997026166666667">
This can be achieved by using a small value for -ys
(0 &lt; -ys &lt;&lt;&lt; 1). We only let candidate sentences
to have an option of creating a new child node
with a probability proportional to -yo. By choos-
ing -ys &lt;&lt;&lt; -yo we suppress the generation of new
branches for summary sentences and modify the
-y of nCRP prior in Eq.(1) using -ys and -yo hyper-
parameters for different sentence types. In the ex-
periments, we discuss the effects of this modifica-
tion on the hierarchical topic tree.
The following is the generative process for
sumHLDA used in our HybHSum :
</bodyText>
<listItem confidence="0.972828222222222">
(1) For each topic k E T, sample a distribution
Qk — Dirichlet(q).
(2) For each sentence d E {O U 5},
(a) if d E O, draw a path cd — nCRP(-yo),
else if d E 5, draw a path cd — nCRP(-ys).
(b) Sample L-vector Bd mixing weights from
Dirichlet distribution Bd — Dir(α).
(c) For each word n, choose: (i) level zd,n|Bd
and (ii) word wd,n |{zd,n, cd, Q}
</listItem>
<bodyText confidence="0.990168291666667">
Given sentence d, Bd is a vector of topic pro-
portions from L dimensional Dirichlet parameter-
ized by α (distribution over levels in the tree.) The
nth word of d is sampled by first choosing a level
zd,n = l from the discrete distribution Bd with
probability Bd,l. Dirichlet parameter q and -yo con-
trol the size of tree effecting the number of topics.
(Small values of -ys do not effect the tree.) Large
values of q favor more topics (Blei et al., 2003a).
Model Learning: Gibbs sampling is a common
method to fit the hLDA models. The aim is to ob-
tain the following samples from the posterior of:
(i) the latent tree T, (ii) the level assignment z for
all words, (iii) the path assignments c for all sen-
tences conditioned on the observed words w.
Given the assignment of words w to levels z and
assignments of sentences to paths c, the expected
posterior probability of a particular word w at a
given topic z=l of a path c=c is proportional to the
number of times w was generated by that topic:
p(w|z, c, w, q) a n(z=l,c=c,w=w) + q (2)
Similarly, posterior probability of a particular
topic z in a given sentence d is proportional to
number of times z was generated by that sentence:
</bodyText>
<equation confidence="0.744678">
p(z|z, c, α) a n(c=cd,z=l) + α (3)
</equation>
<bodyText confidence="0.998119666666667">
n(.) is the count of elements of an array satisfy-
ing the condition. Note from Eq.(3) that two sen-
tences d1 and d2 on the same path c would have
</bodyText>
<page confidence="0.980927">
817
</page>
<bodyText confidence="0.998986833333333">
different words, and hence different posterior topic
probabilities. Posterior probabilities are normal-
ized with total counts and their hyperparameters.
words wsn in sn of the same topic. The proba-
bility of each word in pom,l and psn,l are obtained
using Eq. (2) and then normalized (see Fig.1.b).
</bodyText>
<sectionHeader confidence="0.928917" genericHeader="method">
4 Tree-Based Sentence Scoring
</sectionHeader>
<bodyText confidence="0.99994045">
The sumHLDA constructs a hierarchical tree
structure of candidate sentences (per document
cluster) by positioning summary sentences on the
tree. Each sentence is represented by a path in the
tree, and each path can be shared by many sen-
tences. The assumption is that sentences sharing
the same path should be more similar to each other
because they share the same topics. Moreover, if
a path includes a summary sentence, then candi-
date sentences on that path are more likely to be
selected for summary text. In particular, the sim-
ilarity of a candidate sentence om to a summary
sentence sn sharing the same path is a measure
of strength, indicating how likely om is to be in-
cluded in the generated summary (Algorithm 1):
Let com be the path for a given om. We find
summary sentences that share the same path with
om via: M = isn E S|csn = com}. The score of
each sentence is calculated by similarity to the best
matching summary sentence in M:
</bodyText>
<equation confidence="0.996745">
score(om) = maxsnEM sim(om, sn) (4)
</equation>
<bodyText confidence="0.984734782608695">
If M=O, then score(om)=0. The efficiency of our
similarity measure in identifying the best match-
ing summary sentence, is tied to how expressive
the extracted topics of our sumHLDA models are.
Given path com, we calculate the similarity of om
to each sn, n=1..|M |by measuring similarities on:
? sparse unigram distributions (sim1) at each
topic l on com: similarity between p(wom,l|zom =
l, com, vl) and p(wsn,l|zsn = l, com, vl)
?? distributions of topic proportions (sim2);
similarity between p(zom|com) and p(zsn|com).
− sim1: We define two sparse (discrete) un-
igram distributions for candidate om and sum-
mary sn at each node l on a vocabulary iden-
tified with words generated by the topic at that
node, vl C V . Given wom = {w1, ..., w|om |},
let wom,l C wom be the set of words in om that
are generated from topic zom at level l on path
com. The discrete unigram distribution poml =
p(wom,l|zom = l, com, vl) represents the probabil-
ity over all words vl assigned to topic zom at level
l, by sampling only for words in wom,l. Similarly,
psn,l = p(wsn,l|zsn, com, vl) is the probability of
</bodyText>
<construct confidence="0.375691">
Algorithm 1 Tree-Based Sentence Scoring
</construct>
<listItem confidence="0.957516307692308">
1: Given tree T from sumHLDA, candidate and summary
sentences: O = {o1,..., om} , S = {s1, ..., sn}
2: for sentences m ← 1, ..., |O |do
3: - Find path co,n on tree T and summary sentences
4: on path co_: M = {sn ∈ S|csn = co,,,}
5: for summary sentences n ← 1, ..., |M |do
6: - Find score(om)=maxsn sim(om, sn),
7: where sim(om, sn) = sim1 ∗ sim2
8: using Eq.(7) and Eq.(8)
9: end for
10: end for
11: Obtain scores Y = {score(om)}|O|
m=1
</listItem>
<bodyText confidence="0.97785575">
The similarity between pom,l and psn,l is
obtained by first calculating the divergence
with information radius- IR based on Kullback-
Liebler(KL) divergence, p=pom,l, q=psn,l :
</bodyText>
<equation confidence="0.996640333333333">
IRcom,l(pom,l,psn,l)=KL(p ||p+q
2 )+KL(q ||p+q
2 ) (5)
</equation>
<bodyText confidence="0.925904714285714">
where, KL(p||q)=Ei pi log pi
qi . Then the divergence
is transformed into a similarity measure (Manning
and Schuetze, 1999):
10_IRcom,l(pom,l,psn,l)
IR is a measure of total divergence from the av-
erage, representing how much information is lost
when two distributions p and q are described in
terms of average distributions. We opted for IR
instead of the commonly used KL because with
IR there is no problem with infinite values since
pi+qi
2 70 if either pi 70 or qi70. Moreover, un-
like KL, IR is symmetric, i.e., KL(p,q)�=KL(q,p).
Finally sim1 is obtained by average similarity of
sentences using Eq.(6) at each level of com by:
sim1 (om, sn) = L �l 1 Wcom ,l (pom,l , psn,l) * l
The similarity between pom,l and psn,l at each level
is weighted proportional to the level l because the
similarity between sentences should be rewarded
if there is a specific word overlap at child nodes.
−sim2: We introduce another measure based
on sentence-topic mixing proportions to calculate
the concept-based similarities between om and sn.
We calculate the topic proportions of om and sn,
represented by pzom = p(zom|com) and pzsn =
p(zsn|com) via Eq.(3). The similarity between the
distributions is then measured with transformed IR
</bodyText>
<equation confidence="0.566224">
Wcom,l(pom,l, psn,l) =
</equation>
<page confidence="0.697838">
818
</page>
<figure confidence="0.985861">
(a) Snapshot of Hierarchical Topic Structure of a (b) Magnified view of sample path c [z1,z2,z3] showing
document cluster on “global warming”. (Duc06) om={w1,w2,w3,w4,w5} and sn={w1,w2,w6,w7,w8}
</figure>
<figureCaption confidence="0.98020075">
Figure 1: (a) A sample 3-level tree using sumHLDA. Each sentence is associated with a path c through the hierarchy, where
each node zl,c is associated with a distribution over terms (Most probable terms are illustrated). (b) magnified view of a path
(darker nodes) in (a). Distribution of words in given two sentences, a candidate (om) and a summary (sn) using sub-vocabulary
of words at each topic vzl. Discrete distributions on the left are topic mixtures for each sentence, pzom and pzsn .
</figureCaption>
<figure confidence="0.758770842105263">
Posterior Topic
Distributions
pomz
z
z1 z2 z3
pzsn
Posterior Topic-Word Distributions
candidate om summary sn
w1
vz1
w1w5w6 w7.... w1w5w6 w7....
w6
. w7
.
.
.
vz2 vz2
w2 w8 ....
.
.
.
z3
.
z2
.
.
.
.
.
..
. .
.
.
.
z1 w5
w8
.
..
</figure>
<equation confidence="0.951903411764706">
w2
p(w |z1, c )
om,1 om
p(w |z2, c ) p(w |z2, c )
om,2 om sn,2 sn
w5 .... w5 ....
vz3
p(w |z1, c )
sn,1 sn
w2 w8 ....
vz3
vz1
om: “Global1 warming2 may rise3 incidence4 of malaria5.”
sn:“Global1 warming2 effects6 human7 health8.”
p(w |z3, c )
om,3 om
p(w |z3, c )
</equation>
<figure confidence="0.971545454545454">
sn,3 sn
.w5 . .
zJ
global
human
incidence
temperature
level:2
warming
disease
z2
health
forecast
predict
zK-J
...
level:3
slow
malaria
sneeze
...
z3
zs
zK
siberia
middle-east
starving
level:J
research
change
.
.
z
</figure>
<equation confidence="0.59182">
z1 z2 z3
as in Eq.(6) by:
sim2 (om, sn) = 10−IRcom (pzom ,pzsn) (8)
</equation>
<bodyText confidence="0.974995066666667">
sim1 provides information about the similarity
between two sentences, om and sn based on topic-
word distributions. Similarly, sim2 provides in-
formation on the similarity between the weights of
the topics in each sentence. They jointly effect the
sentence score and are combined in one measure:
sim(om, sn) = sim1(om, sn) * sim2 (om, sn) (9)
The final score for a given om is calculated from
Eq.(4). Fig.1.b depicts a sample path illustrating
sparse unigram distributions of om and sm at each
level as well as their topic proportions, pzom , and
pzsn . In experiment 3, we discuss the effect of our
tree-based scoring on summarization performance
in comparison to a classical scoring method pre-
sented as our baseline model.
</bodyText>
<sectionHeader confidence="0.999424" genericHeader="method">
5 Regression Model
</sectionHeader>
<bodyText confidence="0.999868">
Each candidate sentence om, m = 1..|O |is rep-
resented with a multi-dimensional vector of q fea-
tures fm = {fm1, ..., fmq}. We build a regression
model using sentence scores as output and selected
salient features as input variables described below:
</bodyText>
<subsectionHeader confidence="0.986305">
5.1 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.859758425">
We compile our training dataset using sentences
from different document clusters, which do not
necessarily share vocabularies. Thus, we create n-
gram meta-features to represent sentences instead
of word n-gram frequencies:
(I) nGram Meta-Features (NMF): For each
document cluster D, we identify most fre-
quent (non-stop word) unigrams, i.e., vfreq =
{wi}ri=1 C V , where r is a model param-
eter of number of most frequent unigram fea-
tures. We measure observed unigram proba-
bilities for each wi E vfreq with pD(wi) =
nD(wi)/�|V |
j=1 nD(wj), where nD(wi) is the
number of times wi appears in D and |V  |is the
total number of unigrams. For any ith feature, the
value is fmi = 0, if given sentence does not con-
tain wi, otherwise fmi = pD(wi). These features
can be extended for any n-grams. We similarly
include bigram features in the experiments.
(II) Document Word Frequency Meta-
Features (DMF): The characteristics of sentences
at the document level can be important in sum-
mary generation. DMF identify whether a word
in a given sentence is specific to the document
in consideration or it is commonly used in the
document cluster. This is important because
summary sentences usually contain abstract terms
rather than specific terms.
To characterize this feature, we re-use the r
most frequent unigrams, i.e., wi E vfreq. Given
sentence om, let d be the document that om be-
longs to, i.e., om E d. We measure unigram prob-
abilities for each wi by p(wi E om) = nd(wi E
om)/nD(wi), where nd(wi E om) is the number
of times wi appears in d and nD(wi) is the number
of times wi appears in D. For any ith feature, the
value is fmi = 0, if given sentence does not con-
tain wi, otherwise fmi = p(wi E om). We also
include bigram extensions of DMF features.
</bodyText>
<page confidence="0.996899">
819
</page>
<bodyText confidence="0.994466636363637">
(III) Other Features (OF): Term frequency of
sentences such as SUMBASTC are proven to be
good predictors in sentence scoring (Nenkova and
Vanderwende, 2005). We measure the average
uunigram probability of a sentence by: p(om) =
FIW2o�joM1jPDM, where PD(w) is the observed
unigram probability in the document collection D
and |om |is the total number of words in om. We
use sentence bigram frequency, sentence rank in
a document, and sentence size as additional fea-
tures.
</bodyText>
<subsectionHeader confidence="0.999572">
5.2 Predicting Scores for New Sentences
</subsectionHeader>
<bodyText confidence="0.997256178571429">
Due to the large feature space to explore, we chose
to work with support vector regression (SVR)
(Drucker et al., 1997) as the learning algorithm
to predict sentence scores. Given training sen-
tences {fm, ym}|O|
m�1, where fm = {fm1, ..., fmQ}
is a multi-dimensional vector of features and
ym=score(om)E R are their scores obtained via
Eq.(4), we train a regression model. In experi-
ments we use non-linear Gaussian kernel for SVR.
Once the SVR model is trained, we use it to predict
the scores of ntest number of sentences in test (un-
seen) document clusters, Otest = {o1, ...o|Ote.t |I.
Our HybHSum captures the sentence character-
istics with a regression model using sentences in
different document clusters. At test time, this valu-
able information is used to score testing sentences.
Redundancy Elimination: To eliminate redun-
dant sentences in the generated summary, we in-
crementally add onto the summary the highest
ranked sentence om and check if om significantly
repeats the information already included in the
summary until the algorithm reaches word count
limit. We use a word overlap measure between
sentences normalized to sentence length. A om is
discarded if its similarity to any of the previously
selected sentences is greater than a threshold iden-
tified by a greedy search on the training dataset.
</bodyText>
<sectionHeader confidence="0.997285" genericHeader="evaluation">
6 Experiments and Discussions
</sectionHeader>
<bodyText confidence="0.997021383333333">
In this section we describe a number of experi-
ments using our hybrid model on 100 document
clusters each containing 25 news articles from
DUC2005-2006 tasks. We evaluate the perfor-
mance of HybHSum using 45 document clusters
each containing 25 news articles from DUC2007
task. From these sets, we collected —80K and
—25K sentences to compile training and testing
data respectively. The task is to create max. 250
word long summary for each document cluster.
We use Gibbs sampling for inference in hLDA
and sumHLDA. The hLDA is used to capture ab-
straction and specificity of words in documents
(Blei et al., 2009). Contrary to typical hLDA mod-
els, to efficiently represent sentences in summa-
rization task, we set ascending values for Dirichlet
hyper-parameter η as the level increases, encour-
aging mid to low level distributions to generate as
many words as in higher levels, e.g., for a tree of
depth=3, η = {0.125, 0.5, 1}. This causes sen-
tences share paths only when they include similar
concepts, starting higher level topics of the tree.
For SVR, we set E = 0.1 using the default choice,
which is the inverse of the average of φ(f)Tφ(f)
(Joachims, 1999), dot product of kernelized input
vectors. We use greedy optimization during train-
ing based on ROUGE scores to find best regular-
izer C = 110−1..1021 using the Gaussian kernel.
We applied feature extraction of § 5.1 to com-
pile the training and testing datasets. ROUGE
is used for performance measure (Lin and Hovy,
2003; Lin, 2004), which evaluates summaries
based on the maxium number of overlapping units
between generated summary text and a set of hu-
man summaries. We use R-1 (recall against uni-
grams), R-2 (recall against bigrams), and R-SU4
(recall against skip-4 bigrams).
Experiment 1: sumHLDA Parameter Analy-
sis: In sumHLDA we introduce a prior different
than the standard nested CRP (nCRP). Here, we
illustrate that this prior is practical in learning hi-
erarchical topics for summarization task.
We use sentences from the human generated
summaries during the discovery of hierarchical
topics of sentences in document clusters. Since
summary sentences generally contain abstract
words, they are indicative of sentences in docu-
ments and should produce minimal amount of new
topics (if not none). To implement this, in nCRP
prior of sumHLDA, we use dual hyper-parameters
and choose a very small value for summary sen-
tences, γs = 10e−4 « γo. We compare the re-
sults to hLDA (Blei et al., 2003a) with nCRP prior
which uses only one free parameter, γ. To ana-
lyze this prior, we generate a corpus of —1300 sen-
tences of a document cluster in DUC2005. We re-
peated the experiment for 9 other clusters of sim-
ilar size and averaged the total number of gener-
ated topics. We show results for different values
of γ and γo hyper-parameters and tree depths.
</bodyText>
<page confidence="0.989031">
820
</page>
<table confidence="0.90443225">
γ = γo 0.1 1 10
depth 3 5 8 3 5 8 3 5 8
hLDA 3 5 8 41 267 1509 1522 4080 8015
sumHLDA 3 5 8 27 162 671 1207 3598 7050
</table>
<tableCaption confidence="0.819015">
Table 1: Average # of topics per document cluster from
sumHLDA and hLDA for different γ and γo and tree depths.
γa = 10e−4 is used for sumHLDA for each depth.
</tableCaption>
<table confidence="0.999922555555556">
Features Baseline HybHSum
R-1 R-2 R-SU4 R-1 R-2 R-SU4
NMF (1) 40.3 7.8 13.7 41.6 8.4 12.3
DMF (2) 41.3 7.5 14.3 41.3 8.0 13.9
OF (3) 40.3 7.4 13.7 42.4 8.0 14.4
(1+2) 41.5 7.9 14.0 41.8 8.5 14.5
(1+3) 40.8 7.5 13.8 41.6 8.2 14.1
(2+3) 40.7 7.4 13.8 42.7 8.7 14.9
(1+2+3) 41.4 8.1 13.7 43.0 9.1 15.1
</table>
<tableCaption confidence="0.915403666666667">
Table 2: ROUGE results (with stop-words) on DUC2006
for different features and methods. Results in bold show sta-
tistical significance over baseline in corresponding metric.
</tableCaption>
<bodyText confidence="0.999937214285714">
As shown in Table 1, the nCRP prior for
sumHLDA is more effective than hLDA prior in
the summarization task. Less number of top-
ics(nodes) in sumHLDA suggests that summary
sentences share pre-existing paths and no new
paths or nodes are sampled for them. We also
observe that using &apos;yo = 0.1 causes the model
to generate minimum number of topics (# of top-
ics=depth), while setting &apos;yo = 10 creates exces-
sive amount of topics. &apos;y0 = 1 gives reasonable
number of topics, thus we use this value for the
rest of the experiments. In experiment 3, we use
both nCRP priors in HybHSum to analyze whether
there is any performance gain with the new prior.
</bodyText>
<subsectionHeader confidence="0.934884">
Experiment 2: Feature Selection Analysis
</subsectionHeader>
<bodyText confidence="0.999843041666667">
Here we test individual contribution of each set
of features on our HybHSum (using sumHLDA).
We use a Baseline by replacing the scoring algo-
rithm of HybHSum with a simple cosine distance
measure. The score of a candidate sentence is the
cosine similarity to the maximum matching sum-
mary sentence. Later, we build a regression model
with the same features as our HybHSum to create
a summary. We train models with DUC2005 and
evaluate performance on DUC2006 documents for
different parameter values as shown in Table 2.
As presented in § 5, NMF is the bundle of fre-
quency based meta-features on document cluster
level, DMF is a bundle of frequency based meta-
features on individual document level and OF rep-
resents sentence term frequency, location, and size
features. In comparison to the baseline, OF has a
significant effect on the ROUGE scores. In addi-
tion, DMF together with OF has shown to improve
all scores, in comparison to baseline, on average
by 10%. Although the NMF have minimal indi-
vidual improvement, all these features can statis-
tically improve R-2 without stop words by 12%
(significance is measured by t-test statistics).
</bodyText>
<subsectionHeader confidence="0.792038">
Experiment 3: ROUGE Evaluations
</subsectionHeader>
<bodyText confidence="0.999852333333333">
We use the following multi-document summariza-
tion models along with the Baseline presented in
Experiment 2 to evaluate HybSumm.
</bodyText>
<listItem confidence="0.66420225">
* PYTHY : (Toutanova et al., 2007) A state-
of-the-art supervised summarization system that
ranked first in overall ROUGE evaluations in
DUC2007. Similar to HybHSum, human gener-
ated summaries are used to train a sentence rank-
ing system using a classifier model.
* HIERSUM : (Haghighi and Vanderwende,
2009) A generative summarization method based
</listItem>
<bodyText confidence="0.973630793103449">
on topic models, which uses sentences as an addi-
tional level. Using an approximation for inference,
sentences are greedily added to a summary so long
as they decrease KL-divergence.
* HybFSum (Hybrid Flat Summarizer): To
investigate the performance of hierarchical topic
model, we build another hybrid model using flat
LDA (Blei et al., 2003b). In LDA each sentence
is a superposition of all K topics with sentence
specific weights, there is no hierarchical relation
between topics. We keep the parameters and the
features of the regression model of hierarchical
HybHSum intact for consistency. We only change
the sentence scoring method. Instead of the new
tree-based sentence scoring (§ 4), we present a
similar method using topics from LDA on sen-
tence level. Note that in LDA the topic-word dis-
tributions 0 are over entire vocabulary, and topic
mixing proportions for sentences 0 are over all
the topics discovered from sentences in a docu-
ment cluster. Hence, we define sim1 and sim2
measures for LDA using topic-word proportions 0
(in place of discrete topic-word distributions from
each level in Eq.2) and topic mixing weights 0 in
sentences (in place of topic proportions in Eq.3)
respectively. Maximum matching score is calcu-
lated as same as in HybHSum.
* HybHSum1 and HybHSum2: To analyze the ef-
fect of the new nCRP prior of sumHLDA on sum-
</bodyText>
<page confidence="0.993039">
821
</page>
<table confidence="0.986037666666667">
ROUGE w/o stop words w/ stop words
R-1 R-2 R-4 R-1 R-2 R-4
Baseline 32.4 7.4 10.6 41.0 9.3 15.2
PYTHY 35.7 8.9 12.1 42.6 11.9 16.8
HIERSUM 33.8 9.3 11.6 42.4 11.8 16.7
HybFSum 34.5 8.6 10.9 43.6 9.5 15.7
HybHSum1 34.0 7.9 11.5 44.8 11.0 16.7
HybHSum2 35.1 8.3 11.8 45.6 11.4 17.2
(a) Ref. Output
</table>
<bodyText confidence="0.615671333333333">
The Agriculture Department
began to propose standards for
all organic foods in the late
1990&apos;s because their sale had
grown more than 20 per cent a
year in that decade. In January
1999 the USDA approved a
&amp;quot;certified organic&amp;quot; label for
meats and poultry that were
raised without growth hormones,
pesticide-treated feed, and
antibiotics.
</bodyText>
<figure confidence="0.425443">
(b) HybHSum2 Output
</figure>
<bodyText confidence="0.974844083333333">
New federal rules for organic
food will assure consumers that
the products are grown and
processed to the same standards
nationwide. But as sales grew
more than 20 percent a year
through the 1990s, organic food
came to account for $1 of every
$100 spent on food, and in 1997
the agency took notice,
proposing national organic
standards for all food.
</bodyText>
<tableCaption confidence="0.82096">
Table 3: ROUGE results of the best systems on
DUC2007 dataset (best results are bolded.)
</tableCaption>
<bodyText confidence="0.9977065625">
marization model performance, we build two dif-
ferent versions of our hybrid model: HybHSum1
using standard hLDA (Blei et al., 2003a) and
HybHSum2 using our sumHLDA.
The ROUGE results are shown in Table 3. The
HybHSum2 achieves the best performance on R-
1 and R-4 and comparable on R-2. When stop
words are used the HybHSum2 outperforms state-
of-the-art by 2.5-7% except R-2 (with statistical
significance). Note that R-2 is a measure of bi-
gram recall and sumHLDA of HybHSum2 is built
on unigrams rather than bigrams. Compared to
the HybFSum built on LDA, both HybHSum1&amp;2
yield better performance indicating the effective-
ness of using hierarchical topic model in summa-
rization task. HybHSum2 appear to be less re-
dundant than HybFSum capturing not only com-
mon terms but also specific words in Fig. 2, due
to the new hierarchical tree-based sentence scor-
ing which characterizes sentences on deeper level.
Similarly, HybHSum1&amp;2 far exceeds baseline built
on simple classifier. The results justify the per-
formance gain by using our novel tree-based scor-
ing method. Although the ROUGE scores for
HybHSum1 and HybHSum2 are not significantly
different, the sumHLDA is more suitable for sum-
marization tasks than hLDA.
HybHSum2 is comparable to (if not better than)
fully generative HIERSUM. This indicates that
with our regression model built on training data,
summaries can be efficiently generated for test
documents (suitable for online systems).
</bodyText>
<subsectionHeader confidence="0.552909">
Experiment 4: Manual Evaluations
</subsectionHeader>
<bodyText confidence="0.999961166666667">
Here, we manually evaluate quality of summaries,
a common DUC task. Human annotators are given
two sets of summary text for each document set,
generated from two approaches: best hierarchi-
cal hybrid HybHSum2 and flat hybrid HybFSum
models, and are asked to mark the better summary
</bodyText>
<figure confidence="0.447909">
(c) HybFSum Output
</figure>
<figureCaption confidence="0.513150538461538">
word
By the year 2001, organic
products are projected to
command 5 percent of total food
sales in the United States. The
sale of organics rose by about 30
percent last year, driven by
concerns over food safety, the
environment and a fear of
genetically engineered food. U.S.
sales of organic foods have
grown by 20 percent annually for
the last seven years.
</figureCaption>
<figure confidence="0.594137">
1 1 0
</figure>
<figureCaption confidence="0.977633666666667">
Figure 2: Example summary text generated by systems
compared in Experiment 3. (Id:D0744 in DUC2007). Ref.
is the human generated summary.
</figureCaption>
<table confidence="0.999777">
Criteria HybFSum HybHSum2 Tie
Non-redundancy 26 44 22
Coherence 24 56 12
Focus 24 56 12
Responsiveness 30 50 12
Overall 24 66 2
</table>
<tableCaption confidence="0.995901333333333">
Table 4: Frequency results of manual quality evaluations.
Results are statistically significant based on t-test. Tie indi-
cates evaluations where two summaries are rated equal.
</tableCaption>
<bodyText confidence="0.9997826">
according to five criteria: non-redundancy (which
summary is less redundant), coherence (which
summary is more coherent), focus and readabil-
ity (content and not include unnecessary details),
responsiveness and overall performance.
We asked 4 annotators to rate DUC2007 pre-
dicted summaries (45 summary pairs per anno-
tator). A total of 92 pairs are judged and eval-
uation results in frequencies are shown in Table
4. The participants rated HybHSum2 generated
summaries more coherent and focused compared
to HybFSum. All results in Table 4 are statis-
tically significant (based on t-test on 95% con-
fidence level.) indicating that HybHSum2 sum-
maries are rated significantly better.
</bodyText>
<figure confidence="0.9711694">
organic
genetic
allow
specific
agriculture
standard
sludge
federal
bar
certified
6 6 6
2 4 3
2 2 1
1 1 1
5 7 0
1 1 0
1 1 0
1 1 0
822
h(f,y) : regression model for sentence ranking
</figure>
<figureCaption confidence="0.963783">
Figure 3: Flow diagram for Hybrid Learning Algorithm for Multi-Document Summarization.
</figureCaption>
<figure confidence="0.999797581395349">
Document Cluster1
Document Cluster2
Document Clustern
sumHLDA
sumHLDA
sumHLDA
candidate sentence scores
candidate sentence scores
candidate sentence scores
...
y-output
0.43
0.20
0.03
.
0.02
0.01
0.0
.
0.35
0.09
0.01
.
y-output
y-output
f-input features
f1 f2 f3 ... fq
f1 f2 f3 ... fq
...
f1 f2 f3 ... fq
f-input features
f-input features
...
...
...
...
...
...
...
...
...
...
...
</figure>
<sectionHeader confidence="0.965274" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999956428571429">
In this paper, we presented a hybrid model for
multi-document summarization. We demonstrated
that implementation of a summary focused hierar-
chical topic model to discover sentence structures
as well as construction of a discriminative method
for inference can benefit summarization quality on
manual and automatic evaluation metrics.
</bodyText>
<sectionHeader confidence="0.988076" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.675788">
Research supported in part by ONR N00014-02-1-
0294, BT Grant CT1080028046, Azerbaijan Min-
istry of Communications and Information Tech-
nology Grant, Azerbaijan University of Azerbai-
jan Republic and the BISC Program of UC Berke-
ley.
</bodyText>
<sectionHeader confidence="0.982696" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999243631578947">
R. Barzilay and L. Lee. Catching the drift: Proba-
bilistic content models with applications to gen-
eration and summarization. In In Proc. HLT-
NAACL’04, 2004.
D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum.
Hierarchical topic models and the nested chi-
nese restaurant process. In In Neural Informa-
tion Processing Systems [NIPS], 2003a.
D. Blei, T. Griffiths, and M. Jordan. The nested
chinese restaurant process and bayesian non-
parametric inference of topic hierarchies. In
Journal of ACM, 2009.
D. M. Blei, A. Ng, and M. Jordan. Latent dirichlet
allocation. In Jrnl. Machine Learning Research,
3:993-1022, 2003b.
S.R.K. Branavan, H. Chen, J. Eisenstein, and
R. Barzilay. Learning document-level seman-
tic properties from free-text annotations. In
Journal of Artificial Intelligence Research, vol-
ume 34, 2009.
J.M. Conroy, J.D. Schlesinger, and D.P. O’Leary.
Topic focused multi-cument summarization us-
ing an approximate oracle score. In In Proc.
ACL’06, 2006.
H. DaumeIII and D. Marcu. Bayesian query fo-
cused summarization. In Proc. ACL-06, 2006.
H. Drucker, C.J.C. Burger, L. Kaufman, A. Smola,
and V. Vapnik. Support vector regression ma-
chines. In NIPS 9, 1997.
A. Haghighi and L. Vanderwende. Exploring con-
tent models for multi-document summarization.
In NAACL HLT-09, 2009.
T. Joachims. Making large-scale svm learning
practical. In In Advances in Kernel Methods -
Support Vector Learning. MIT Press., 1999.
C.-Y. Lin. Rouge: A package for automatic evalu-
ation of summaries. In In Proc. ACL Workshop
on Text Summarization Branches Out, 2004.
</reference>
<page confidence="0.989786">
823
</page>
<reference confidence="0.99267128125">
C.-Y. Lin and E.H. Hovy. Automatic evaluation
of summaries using n-gram co-occurance statis-
tics. In Proc. HLT-NAACL, Edmonton, Canada,
2003.
C. Manning and H. Schuetze. Foundations of sta-
tistical natural language processing. In MIT
Press. Cambridge, MA, 1999.
A. Nenkova and L. Vanderwende. The impact of
frequency on summarization. In Tech. Report
MSR-TR-2005-101, Microsoft Research, Red-
wood, Washington, 2005.
D.R. Radev, H. Jing, M. Stys, and D. Tam.
Centroid-based summarization for multiple
documents. In In Int. Jrnl. Information Process-
ing and Management, 2004.
D. Shen, J.T. Sun, H. Li, Q. Yang, and Z. Chen.
Document summarization using conditional
random fields. In Proc. IJCAI’07, 2007.
J. Tang, L. Yao, and D. Chens. Multi-topic based
query-oriented summarization. In SIAM Inter-
national Conference Data Mining, 2009.
I. Titov and R. McDonald. A joint model of text
and aspect ratings for sentiment summarization.
In ACL-08:HLT, 2008.
K. Toutanova, C. Brockett, M. Gamon, J. Jagarla-
mudi, H. Suzuki, and L. Vanderwende. The ph-
thy summarization system: Microsoft research
at duc 2007. In Proc. DUC, 2007.
J.Y. Yeh, H.-R. Ke, W.P. Yang, and I-H. Meng.
Text summarization using a trainable summa-
rizer and latent semantic analysis. In Informa-
tion Processing and Management, 2005.
</reference>
<page confidence="0.998666">
824
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.851742">
<title confidence="0.999963">A Hybrid Hierarchical Model for Multi-Document Summarization</title>
<author confidence="0.990079">Asli Celikyilmaz Dilek Hakkani-Tur</author>
<affiliation confidence="0.9344215">Computer Science Department International Computer Science Institute University of California, Berkeley Berkeley, CA</affiliation>
<email confidence="0.998219">asli@eecs.berkeley.edudilek@icsi.berkeley.edu</email>
<abstract confidence="0.999616904761905">Scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization. In this paper, we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference. We calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model. Then, using these scores, we train a regression model based on the lexical and structural characteristics of the sentences, and use the model to score sentences of new documents to form a summary. Our system advances current state-of-the-art improving ROUGE scores Generated summaries are less redundant and more coherent based upon manual quality evaluations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>L Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models with applications to generation and summarization. In</title>
<date>2004</date>
<booktitle>In Proc. HLTNAACL’04,</booktitle>
<contexts>
<context position="5254" citStr="Barzilay and Lee, 2004" startWordPosition="794" endWordPosition="797">aluations confirm that our hybrid model can produce coherent and non-redundant summaries. 2 Background and Motivation There are many studies on the principles governing multi-document summarization to produce coherent and semantically relevant summaries. Previous work (Nenkova and Vanderwende, 2005; Conroy et al., 2006), focused on the fact that frequency of words plays an important factor. While, earlier work on summarization depend on a word score function, which is used to measure sentence rank scores based on (semi-)supervised learning methods, recent trend of purely data-driven methods, (Barzilay and Lee, 2004; DaumeIII and Marcu, 2006; Tang et al., 2009; Haghighi and Vanderwende, 2009), have shown remarkable improvements. Our work builds on both methods by constructing a hybrid approach to summarization. Our objective is to discover from document clusters, the latent topics that are organized into hierarchies following (Haghighi and Vanderwende, 2009). A hierarchical model is particularly appealing to summarization than a ”flat” model, e.g. LDA (Blei et al., 2003b), in that one can discover ”abstract” and ”specific” topics. For instance, discovering that ”baseball” and ”football” are both containe</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>R. Barzilay and L. Lee. Catching the drift: Probabilistic content models with applications to generation and summarization. In In Proc. HLTNAACL’04, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>T Griffiths</author>
<author>M Jordan</author>
<author>J Tenenbaum</author>
</authors>
<title>Hierarchical topic models and the nested chinese restaurant process.</title>
<date>2003</date>
<booktitle>In In Neural Information Processing Systems [NIPS],</booktitle>
<contexts>
<context position="3201" citStr="Blei et al., 2003" startWordPosition="477" endWordPosition="480">ous research which utilizes the best features of both approaches for MDS as presented in this paper. In this paper, we present a novel approach that formulates MDS as a prediction problem based on a two-step hybrid model: a generative model for hierarchical topic discovery and a regression model for inference. We investigate if a hierarchical model can be adopted to discover salient characteristics of sentences organized into hierarchies utilizing human generated summary text. We present a probabilistic topic model on sentence level building on hierarchical Latent Dirichlet Allocation (hLDA) (Blei et al., 2003a), which is a generalization of LDA (Blei et al., 2003b). We construct a hybrid learning algorithm by extracting salient features to characterize summary sentences, and implement a regression model for inference (Fig.3). Contributions of this work are: — construction of hierarchical probabilistic model designed to discover the topic structures of all sentences. Our focus is on identifying similarities of candidate sentences to summary sentences using a novel tree based sentence scoring algorithm, concerning topic distributions at different levels of the discovered hierarchy as described in § </context>
<context position="5717" citStr="Blei et al., 2003" startWordPosition="865" endWordPosition="868">h is used to measure sentence rank scores based on (semi-)supervised learning methods, recent trend of purely data-driven methods, (Barzilay and Lee, 2004; DaumeIII and Marcu, 2006; Tang et al., 2009; Haghighi and Vanderwende, 2009), have shown remarkable improvements. Our work builds on both methods by constructing a hybrid approach to summarization. Our objective is to discover from document clusters, the latent topics that are organized into hierarchies following (Haghighi and Vanderwende, 2009). A hierarchical model is particularly appealing to summarization than a ”flat” model, e.g. LDA (Blei et al., 2003b), in that one can discover ”abstract” and ”specific” topics. For instance, discovering that ”baseball” and ”football” are both contained in an abstract class ”sports” can help to identify summary sentences. It follows that summary topics are commonly shared by many documents, while specific topics are more likely to be mentioned in rather a small subset of documents. Feature based learning approaches to summarization methods discover salient features by measuring similarity between candidate sentences and summary sentences (Nenkova and Vanderwende, 2005; Conroy et al., 2006). While such meth</context>
<context position="7927" citStr="Blei et al., 2003" startWordPosition="1217" endWordPosition="1220">topic distributions of training documents. Our approach differs from the early work, in that, we combine a generative hierarchical model and regression model to score sentences in new documents, eliminating the need for building a generative model for new document clusters. 3 Summary-Focused Hierarchical Model Our MDS system, hybrid hierarchical summarizer, HybHSum, is based on an hybrid learning approach to extract sentences for generating summary. We discover hidden topic distributions of sentences in a given document cluster along with provided summary sentences based on hLDA described in (Blei et al., 2003a)1. We build a summary-focused hierarchical probabilistic topic model, sumHLDA, for each document cluster at sentence level, because it enables capturing expected topic distributions in given sentences directly from the model. Besides, document clusters contain a relatively small number of documents, which may limit the variability of topics if they are evaluated on the document level. As described in § 4, we present a new method for scoring candidate sentences from this hierarchical structure. Let a given document cluster D be represented with sentences O={om}�O� m�1 and its corresponding hu</context>
<context position="9412" citStr="Blei et al., 2003" startWordPosition="1468" endWordPosition="1471">hLDA represents distribution of topics in sentences by organizing topics into a tree of a fixed depth L (Fig.1.a). Each candidate sentence om is assigned to a path co� in the tree and each word wi in a given sentence is assigned to a hidden topic zo� at a level l of co�. Each node is associated with a topic distribution over words. The sampler method alternates between choosing a new path for each sentence through the tree and assigning each word in each sentence to a topic along that path. The structure of tree is learnt along with the topics using a nested Chinese restaurant process (nCRP) (Blei et al., 2003a), which is used as a prior. The nCRP is a stochastic process, which assigns probability distributions to infinitely branching and infinitely deep trees. In our model, nCRP specifies a distribution of words into paths in an L-level tree. The assignments of sentences to paths are sampled sequentially: The first sentence takes the initial L-level path, starting with a single branch tree. Later, mth subsequent sentence is assigned to a path drawn from the distribution: p(pathold, c|m, mc) = m� (1) γ+m�1 γ p(pathnew, c|m, mc) = γ+m�1 pathold and pathnew represent an existing and novel (branch) pa</context>
<context position="12503" citStr="Blei et al., 2003" startWordPosition="2015" endWordPosition="2018">(-ys). (b) Sample L-vector Bd mixing weights from Dirichlet distribution Bd — Dir(α). (c) For each word n, choose: (i) level zd,n|Bd and (ii) word wd,n |{zd,n, cd, Q} Given sentence d, Bd is a vector of topic proportions from L dimensional Dirichlet parameterized by α (distribution over levels in the tree.) The nth word of d is sampled by first choosing a level zd,n = l from the discrete distribution Bd with probability Bd,l. Dirichlet parameter q and -yo control the size of tree effecting the number of topics. (Small values of -ys do not effect the tree.) Large values of q favor more topics (Blei et al., 2003a). Model Learning: Gibbs sampling is a common method to fit the hLDA models. The aim is to obtain the following samples from the posterior of: (i) the latent tree T, (ii) the level assignment z for all words, (iii) the path assignments c for all sentences conditioned on the observed words w. Given the assignment of words w to levels z and assignments of sentences to paths c, the expected posterior probability of a particular word w at a given topic z=l of a path c=c is proportional to the number of times w was generated by that topic: p(w|z, c, w, q) a n(z=l,c=c,w=w) + q (2) Similarly, poster</context>
<context position="26299" citStr="Blei et al., 2003" startWordPosition="4415" endWordPosition="4418">d nested CRP (nCRP). Here, we illustrate that this prior is practical in learning hierarchical topics for summarization task. We use sentences from the human generated summaries during the discovery of hierarchical topics of sentences in document clusters. Since summary sentences generally contain abstract words, they are indicative of sentences in documents and should produce minimal amount of new topics (if not none). To implement this, in nCRP prior of sumHLDA, we use dual hyper-parameters and choose a very small value for summary sentences, γs = 10e−4 « γo. We compare the results to hLDA (Blei et al., 2003a) with nCRP prior which uses only one free parameter, γ. To analyze this prior, we generate a corpus of —1300 sentences of a document cluster in DUC2005. We repeated the experiment for 9 other clusters of similar size and averaged the total number of generated topics. We show results for different values of γ and γo hyper-parameters and tree depths. 820 γ = γo 0.1 1 10 depth 3 5 8 3 5 8 3 5 8 hLDA 3 5 8 41 267 1509 1522 4080 8015 sumHLDA 3 5 8 27 162 671 1207 3598 7050 Table 1: Average # of topics per document cluster from sumHLDA and hLDA for different γ and γo and tree depths. γa = 10e−4 is</context>
<context position="30074" citStr="Blei et al., 2003" startWordPosition="5078" endWordPosition="5081">rization system that ranked first in overall ROUGE evaluations in DUC2007. Similar to HybHSum, human generated summaries are used to train a sentence ranking system using a classifier model. * HIERSUM : (Haghighi and Vanderwende, 2009) A generative summarization method based on topic models, which uses sentences as an additional level. Using an approximation for inference, sentences are greedily added to a summary so long as they decrease KL-divergence. * HybFSum (Hybrid Flat Summarizer): To investigate the performance of hierarchical topic model, we build another hybrid model using flat LDA (Blei et al., 2003b). In LDA each sentence is a superposition of all K topics with sentence specific weights, there is no hierarchical relation between topics. We keep the parameters and the features of the regression model of hierarchical HybHSum intact for consistency. We only change the sentence scoring method. Instead of the new tree-based sentence scoring (§ 4), we present a similar method using topics from LDA on sentence level. Note that in LDA the topic-word distributions 0 are over entire vocabulary, and topic mixing proportions for sentences 0 are over all the topics discovered from sentences in a doc</context>
<context position="32305" citStr="Blei et al., 2003" startWordPosition="5460" endWordPosition="5463">ated feed, and antibiotics. (b) HybHSum2 Output New federal rules for organic food will assure consumers that the products are grown and processed to the same standards nationwide. But as sales grew more than 20 percent a year through the 1990s, organic food came to account for $1 of every $100 spent on food, and in 1997 the agency took notice, proposing national organic standards for all food. Table 3: ROUGE results of the best systems on DUC2007 dataset (best results are bolded.) marization model performance, we build two different versions of our hybrid model: HybHSum1 using standard hLDA (Blei et al., 2003a) and HybHSum2 using our sumHLDA. The ROUGE results are shown in Table 3. The HybHSum2 achieves the best performance on R1 and R-4 and comparable on R-2. When stop words are used the HybHSum2 outperforms stateof-the-art by 2.5-7% except R-2 (with statistical significance). Note that R-2 is a measure of bigram recall and sumHLDA of HybHSum2 is built on unigrams rather than bigrams. Compared to the HybFSum built on LDA, both HybHSum1&amp;2 yield better performance indicating the effectiveness of using hierarchical topic model in summarization task. HybHSum2 appear to be less redundant than HybFSum </context>
</contexts>
<marker>Blei, Griffiths, Jordan, Tenenbaum, 2003</marker>
<rawString>D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum. Hierarchical topic models and the nested chinese restaurant process. In In Neural Information Processing Systems [NIPS], 2003a.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>T Griffiths</author>
<author>M Jordan</author>
</authors>
<title>The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies.</title>
<date>2009</date>
<journal>In Journal of ACM,</journal>
<contexts>
<context position="24459" citStr="Blei et al., 2009" startWordPosition="4108" endWordPosition="4111">ssions In this section we describe a number of experiments using our hybrid model on 100 document clusters each containing 25 news articles from DUC2005-2006 tasks. We evaluate the performance of HybHSum using 45 document clusters each containing 25 news articles from DUC2007 task. From these sets, we collected —80K and —25K sentences to compile training and testing data respectively. The task is to create max. 250 word long summary for each document cluster. We use Gibbs sampling for inference in hLDA and sumHLDA. The hLDA is used to capture abstraction and specificity of words in documents (Blei et al., 2009). Contrary to typical hLDA models, to efficiently represent sentences in summarization task, we set ascending values for Dirichlet hyper-parameter η as the level increases, encouraging mid to low level distributions to generate as many words as in higher levels, e.g., for a tree of depth=3, η = {0.125, 0.5, 1}. This causes sentences share paths only when they include similar concepts, starting higher level topics of the tree. For SVR, we set E = 0.1 using the default choice, which is the inverse of the average of φ(f)Tφ(f) (Joachims, 1999), dot product of kernelized input vectors. We use greed</context>
</contexts>
<marker>Blei, Griffiths, Jordan, 2009</marker>
<rawString>D. Blei, T. Griffiths, and M. Jordan. The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies. In Journal of ACM, 2009.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D M Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<booktitle>In Jrnl. Machine Learning Research,</booktitle>
<pages>3--993</pages>
<marker>Blei, Ng, Jordan, </marker>
<rawString>D. M. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. In Jrnl. Machine Learning Research, 3:993-1022, 2003b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>H Chen</author>
<author>J Eisenstein</author>
<author>R Barzilay</author>
</authors>
<title>Learning document-level semantic properties from free-text annotations.</title>
<date>2009</date>
<journal>In Journal of Artificial Intelligence Research,</journal>
<volume>34</volume>
<contexts>
<context position="2321" citStr="Branavan et al., 2009" startWordPosition="335" endWordPosition="338">summary pairs, and unsupervised methods based upon properties derived from document clusters. Supervised methods treat the summarization task as a classification/regression problem, e.g., (Shen et al., 2007; Yeh et al., 2005). Each candidate sentence is classified as summary or non-summary based on the features that they pose and those with highest scores are selected. Unsupervised methods aim to score sentences based on semantic groupings extracted from documents, e.g., (DaumeIII and Marcu, 2006; Titov and McDonald, 2008; Tang et al., 2009; Haghighi and Vanderwende, 2009; Radev et al., 2004; Branavan et al., 2009), etc. Such models can yield comparable or better performance on DUC and other evaluations, since representing documents as topic distributions rather than bags of words diminishes the effect of lexical variability. To the best of our knowledge, there is no previous research which utilizes the best features of both approaches for MDS as presented in this paper. In this paper, we present a novel approach that formulates MDS as a prediction problem based on a two-step hybrid model: a generative model for hierarchical topic discovery and a regression model for inference. We investigate if a hiera</context>
</contexts>
<marker>Branavan, Chen, Eisenstein, Barzilay, 2009</marker>
<rawString>S.R.K. Branavan, H. Chen, J. Eisenstein, and R. Barzilay. Learning document-level semantic properties from free-text annotations. In Journal of Artificial Intelligence Research, volume 34, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Conroy</author>
<author>J D Schlesinger</author>
<author>D P O’Leary</author>
</authors>
<title>Topic focused multi-cument summarization using an approximate oracle score. In</title>
<date>2006</date>
<booktitle>In Proc. ACL’06,</booktitle>
<marker>Conroy, Schlesinger, O’Leary, 2006</marker>
<rawString>J.M. Conroy, J.D. Schlesinger, and D.P. O’Leary. Topic focused multi-cument summarization using an approximate oracle score. In In Proc. ACL’06, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H DaumeIII</author>
<author>D Marcu</author>
</authors>
<title>Bayesian query focused summarization.</title>
<date>2006</date>
<booktitle>In Proc. ACL-06,</booktitle>
<contexts>
<context position="2200" citStr="DaumeIII and Marcu, 2006" startWordPosition="315" endWordPosition="318">ies. Extractive summarization methods can be classified into two groups: supervised methods that rely on provided document-summary pairs, and unsupervised methods based upon properties derived from document clusters. Supervised methods treat the summarization task as a classification/regression problem, e.g., (Shen et al., 2007; Yeh et al., 2005). Each candidate sentence is classified as summary or non-summary based on the features that they pose and those with highest scores are selected. Unsupervised methods aim to score sentences based on semantic groupings extracted from documents, e.g., (DaumeIII and Marcu, 2006; Titov and McDonald, 2008; Tang et al., 2009; Haghighi and Vanderwende, 2009; Radev et al., 2004; Branavan et al., 2009), etc. Such models can yield comparable or better performance on DUC and other evaluations, since representing documents as topic distributions rather than bags of words diminishes the effect of lexical variability. To the best of our knowledge, there is no previous research which utilizes the best features of both approaches for MDS as presented in this paper. In this paper, we present a novel approach that formulates MDS as a prediction problem based on a two-step hybrid m</context>
<context position="5280" citStr="DaumeIII and Marcu, 2006" startWordPosition="798" endWordPosition="801">ur hybrid model can produce coherent and non-redundant summaries. 2 Background and Motivation There are many studies on the principles governing multi-document summarization to produce coherent and semantically relevant summaries. Previous work (Nenkova and Vanderwende, 2005; Conroy et al., 2006), focused on the fact that frequency of words plays an important factor. While, earlier work on summarization depend on a word score function, which is used to measure sentence rank scores based on (semi-)supervised learning methods, recent trend of purely data-driven methods, (Barzilay and Lee, 2004; DaumeIII and Marcu, 2006; Tang et al., 2009; Haghighi and Vanderwende, 2009), have shown remarkable improvements. Our work builds on both methods by constructing a hybrid approach to summarization. Our objective is to discover from document clusters, the latent topics that are organized into hierarchies following (Haghighi and Vanderwende, 2009). A hierarchical model is particularly appealing to summarization than a ”flat” model, e.g. LDA (Blei et al., 2003b), in that one can discover ”abstract” and ”specific” topics. For instance, discovering that ”baseball” and ”football” are both contained in an abstract class ”sp</context>
</contexts>
<marker>DaumeIII, Marcu, 2006</marker>
<rawString>H. DaumeIII and D. Marcu. Bayesian query focused summarization. In Proc. ACL-06, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Drucker</author>
<author>C J C Burger</author>
<author>L Kaufman</author>
<author>A Smola</author>
<author>V Vapnik</author>
</authors>
<title>Support vector regression machines.</title>
<date>1997</date>
<booktitle>In NIPS 9,</booktitle>
<contexts>
<context position="22627" citStr="Drucker et al., 1997" startWordPosition="3807" endWordPosition="3810">er Features (OF): Term frequency of sentences such as SUMBASTC are proven to be good predictors in sentence scoring (Nenkova and Vanderwende, 2005). We measure the average uunigram probability of a sentence by: p(om) = FIW2o�joM1jPDM, where PD(w) is the observed unigram probability in the document collection D and |om |is the total number of words in om. We use sentence bigram frequency, sentence rank in a document, and sentence size as additional features. 5.2 Predicting Scores for New Sentences Due to the large feature space to explore, we chose to work with support vector regression (SVR) (Drucker et al., 1997) as the learning algorithm to predict sentence scores. Given training sentences {fm, ym}|O| m�1, where fm = {fm1, ..., fmQ} is a multi-dimensional vector of features and ym=score(om)E R are their scores obtained via Eq.(4), we train a regression model. In experiments we use non-linear Gaussian kernel for SVR. Once the SVR model is trained, we use it to predict the scores of ntest number of sentences in test (unseen) document clusters, Otest = {o1, ...o|Ote.t |I. Our HybHSum captures the sentence characteristics with a regression model using sentences in different document clusters. At test tim</context>
</contexts>
<marker>Drucker, Burger, Kaufman, Smola, Vapnik, 1997</marker>
<rawString>H. Drucker, C.J.C. Burger, L. Kaufman, A. Smola, and V. Vapnik. Support vector regression machines. In NIPS 9, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>L Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In NAACL HLT-09,</booktitle>
<contexts>
<context position="2277" citStr="Haghighi and Vanderwende, 2009" startWordPosition="327" endWordPosition="330">: supervised methods that rely on provided document-summary pairs, and unsupervised methods based upon properties derived from document clusters. Supervised methods treat the summarization task as a classification/regression problem, e.g., (Shen et al., 2007; Yeh et al., 2005). Each candidate sentence is classified as summary or non-summary based on the features that they pose and those with highest scores are selected. Unsupervised methods aim to score sentences based on semantic groupings extracted from documents, e.g., (DaumeIII and Marcu, 2006; Titov and McDonald, 2008; Tang et al., 2009; Haghighi and Vanderwende, 2009; Radev et al., 2004; Branavan et al., 2009), etc. Such models can yield comparable or better performance on DUC and other evaluations, since representing documents as topic distributions rather than bags of words diminishes the effect of lexical variability. To the best of our knowledge, there is no previous research which utilizes the best features of both approaches for MDS as presented in this paper. In this paper, we present a novel approach that formulates MDS as a prediction problem based on a two-step hybrid model: a generative model for hierarchical topic discovery and a regression mo</context>
<context position="5332" citStr="Haghighi and Vanderwende, 2009" startWordPosition="806" endWordPosition="809">redundant summaries. 2 Background and Motivation There are many studies on the principles governing multi-document summarization to produce coherent and semantically relevant summaries. Previous work (Nenkova and Vanderwende, 2005; Conroy et al., 2006), focused on the fact that frequency of words plays an important factor. While, earlier work on summarization depend on a word score function, which is used to measure sentence rank scores based on (semi-)supervised learning methods, recent trend of purely data-driven methods, (Barzilay and Lee, 2004; DaumeIII and Marcu, 2006; Tang et al., 2009; Haghighi and Vanderwende, 2009), have shown remarkable improvements. Our work builds on both methods by constructing a hybrid approach to summarization. Our objective is to discover from document clusters, the latent topics that are organized into hierarchies following (Haghighi and Vanderwende, 2009). A hierarchical model is particularly appealing to summarization than a ”flat” model, e.g. LDA (Blei et al., 2003b), in that one can discover ”abstract” and ”specific” topics. For instance, discovering that ”baseball” and ”football” are both contained in an abstract class ”sports” can help to identify summary sentences. It fol</context>
<context position="7112" citStr="Haghighi and Vanderwende, 2009" startWordPosition="1091" endWordPosition="1094">nformation on the hidden semantic structure of document clusters would improve the performance of these methods. Recent studies focused on the discovery of latent topics of document sets in extracting summaries. In these models, the challenges of inferring topics of test documents are not addressed in detail. One of the challenges of using a previously trained topic model is that the new document might have a totally new vocabulary or may include many other specific topics, which may or may not exist in the trained model. A common method is to re-build a topic model for new sets of documents (Haghighi and Vanderwende, 2009), which has proven to produce coherent summaries. An alternative yet feasible solution, presented in this work, is building a model that can summarize new document clusters using characteristics of topic distributions of training documents. Our approach differs from the early work, in that, we combine a generative hierarchical model and regression model to score sentences in new documents, eliminating the need for building a generative model for new document clusters. 3 Summary-Focused Hierarchical Model Our MDS system, hybrid hierarchical summarizer, HybHSum, is based on an hybrid learning ap</context>
<context position="29692" citStr="Haghighi and Vanderwende, 2009" startWordPosition="5019" endWordPosition="5022">e NMF have minimal individual improvement, all these features can statistically improve R-2 without stop words by 12% (significance is measured by t-test statistics). Experiment 3: ROUGE Evaluations We use the following multi-document summarization models along with the Baseline presented in Experiment 2 to evaluate HybSumm. * PYTHY : (Toutanova et al., 2007) A stateof-the-art supervised summarization system that ranked first in overall ROUGE evaluations in DUC2007. Similar to HybHSum, human generated summaries are used to train a sentence ranking system using a classifier model. * HIERSUM : (Haghighi and Vanderwende, 2009) A generative summarization method based on topic models, which uses sentences as an additional level. Using an approximation for inference, sentences are greedily added to a summary so long as they decrease KL-divergence. * HybFSum (Hybrid Flat Summarizer): To investigate the performance of hierarchical topic model, we build another hybrid model using flat LDA (Blei et al., 2003b). In LDA each sentence is a superposition of all K topics with sentence specific weights, there is no hierarchical relation between topics. We keep the parameters and the features of the regression model of hierarchi</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>A. Haghighi and L. Vanderwende. Exploring content models for multi-document summarization. In NAACL HLT-09, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale svm learning practical. In</title>
<date>1999</date>
<booktitle>In Advances in Kernel Methods -Support Vector Learning.</booktitle>
<publisher>MIT Press.,</publisher>
<contexts>
<context position="25004" citStr="Joachims, 1999" startWordPosition="4204" endWordPosition="4205"> abstraction and specificity of words in documents (Blei et al., 2009). Contrary to typical hLDA models, to efficiently represent sentences in summarization task, we set ascending values for Dirichlet hyper-parameter η as the level increases, encouraging mid to low level distributions to generate as many words as in higher levels, e.g., for a tree of depth=3, η = {0.125, 0.5, 1}. This causes sentences share paths only when they include similar concepts, starting higher level topics of the tree. For SVR, we set E = 0.1 using the default choice, which is the inverse of the average of φ(f)Tφ(f) (Joachims, 1999), dot product of kernelized input vectors. We use greedy optimization during training based on ROUGE scores to find best regularizer C = 110−1..1021 using the Gaussian kernel. We applied feature extraction of § 5.1 to compile the training and testing datasets. ROUGE is used for performance measure (Lin and Hovy, 2003; Lin, 2004), which evaluates summaries based on the maxium number of overlapping units between generated summary text and a set of human summaries. We use R-1 (recall against unigrams), R-2 (recall against bigrams), and R-SU4 (recall against skip-4 bigrams). Experiment 1: sumHLDA </context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. Making large-scale svm learning practical. In In Advances in Kernel Methods -Support Vector Learning. MIT Press., 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries. In</title>
<date>2004</date>
<booktitle>In Proc. ACL Workshop on Text Summarization Branches Out,</booktitle>
<contexts>
<context position="25334" citStr="Lin, 2004" startWordPosition="4260" endWordPosition="4261">g., for a tree of depth=3, η = {0.125, 0.5, 1}. This causes sentences share paths only when they include similar concepts, starting higher level topics of the tree. For SVR, we set E = 0.1 using the default choice, which is the inverse of the average of φ(f)Tφ(f) (Joachims, 1999), dot product of kernelized input vectors. We use greedy optimization during training based on ROUGE scores to find best regularizer C = 110−1..1021 using the Gaussian kernel. We applied feature extraction of § 5.1 to compile the training and testing datasets. ROUGE is used for performance measure (Lin and Hovy, 2003; Lin, 2004), which evaluates summaries based on the maxium number of overlapping units between generated summary text and a set of human summaries. We use R-1 (recall against unigrams), R-2 (recall against bigrams), and R-SU4 (recall against skip-4 bigrams). Experiment 1: sumHLDA Parameter Analysis: In sumHLDA we introduce a prior different than the standard nested CRP (nCRP). Here, we illustrate that this prior is practical in learning hierarchical topics for summarization task. We use sentences from the human generated summaries during the discovery of hierarchical topics of sentences in document clust</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>C.-Y. Lin. Rouge: A package for automatic evaluation of summaries. In In Proc. ACL Workshop on Text Summarization Branches Out, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>E H Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurance statistics.</title>
<date>2003</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<location>Edmonton, Canada,</location>
<contexts>
<context position="25322" citStr="Lin and Hovy, 2003" startWordPosition="4256" endWordPosition="4259">in higher levels, e.g., for a tree of depth=3, η = {0.125, 0.5, 1}. This causes sentences share paths only when they include similar concepts, starting higher level topics of the tree. For SVR, we set E = 0.1 using the default choice, which is the inverse of the average of φ(f)Tφ(f) (Joachims, 1999), dot product of kernelized input vectors. We use greedy optimization during training based on ROUGE scores to find best regularizer C = 110−1..1021 using the Gaussian kernel. We applied feature extraction of § 5.1 to compile the training and testing datasets. ROUGE is used for performance measure (Lin and Hovy, 2003; Lin, 2004), which evaluates summaries based on the maxium number of overlapping units between generated summary text and a set of human summaries. We use R-1 (recall against unigrams), R-2 (recall against bigrams), and R-SU4 (recall against skip-4 bigrams). Experiment 1: sumHLDA Parameter Analysis: In sumHLDA we introduce a prior different than the standard nested CRP (nCRP). Here, we illustrate that this prior is practical in learning hierarchical topics for summarization task. We use sentences from the human generated summaries during the discovery of hierarchical topics of sentences in do</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>C.-Y. Lin and E.H. Hovy. Automatic evaluation of summaries using n-gram co-occurance statistics. In Proc. HLT-NAACL, Edmonton, Canada, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>H Schuetze</author>
</authors>
<title>Foundations of statistical natural language processing. In</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="16647" citStr="Manning and Schuetze, 1999" startWordPosition="2762" endWordPosition="2765">T and summary sentences 4: on path co_: M = {sn ∈ S|csn = co,,,} 5: for summary sentences n ← 1, ..., |M |do 6: - Find score(om)=maxsn sim(om, sn), 7: where sim(om, sn) = sim1 ∗ sim2 8: using Eq.(7) and Eq.(8) 9: end for 10: end for 11: Obtain scores Y = {score(om)}|O| m=1 The similarity between pom,l and psn,l is obtained by first calculating the divergence with information radius- IR based on KullbackLiebler(KL) divergence, p=pom,l, q=psn,l : IRcom,l(pom,l,psn,l)=KL(p ||p+q 2 )+KL(q ||p+q 2 ) (5) where, KL(p||q)=Ei pi log pi qi . Then the divergence is transformed into a similarity measure (Manning and Schuetze, 1999): 10_IRcom,l(pom,l,psn,l) IR is a measure of total divergence from the average, representing how much information is lost when two distributions p and q are described in terms of average distributions. We opted for IR instead of the commonly used KL because with IR there is no problem with infinite values since pi+qi 2 70 if either pi 70 or qi70. Moreover, unlike KL, IR is symmetric, i.e., KL(p,q)�=KL(q,p). Finally sim1 is obtained by average similarity of sentences using Eq.(6) at each level of com by: sim1 (om, sn) = L �l 1 Wcom ,l (pom,l , psn,l) * l The similarity between pom,l and psn,l a</context>
</contexts>
<marker>Manning, Schuetze, 1999</marker>
<rawString>C. Manning and H. Schuetze. Foundations of statistical natural language processing. In MIT Press. Cambridge, MA, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>L Vanderwende</author>
</authors>
<title>The impact of frequency on summarization. In</title>
<date>2005</date>
<tech>Tech. Report MSR-TR-2005-101,</tech>
<institution>Microsoft Research,</institution>
<location>Redwood, Washington,</location>
<contexts>
<context position="4931" citStr="Nenkova and Vanderwende, 2005" startWordPosition="742" endWordPosition="745">entences in test document clusters without retraining, (which has not been investigated in generative summarization models) described in § 5.2. We show in § 6 that our hybrid summarizer achieves comparable (if not better) ROUGE score on the challenging task of extracting the summaries of multiple newswire documents. The human evaluations confirm that our hybrid model can produce coherent and non-redundant summaries. 2 Background and Motivation There are many studies on the principles governing multi-document summarization to produce coherent and semantically relevant summaries. Previous work (Nenkova and Vanderwende, 2005; Conroy et al., 2006), focused on the fact that frequency of words plays an important factor. While, earlier work on summarization depend on a word score function, which is used to measure sentence rank scores based on (semi-)supervised learning methods, recent trend of purely data-driven methods, (Barzilay and Lee, 2004; DaumeIII and Marcu, 2006; Tang et al., 2009; Haghighi and Vanderwende, 2009), have shown remarkable improvements. Our work builds on both methods by constructing a hybrid approach to summarization. Our objective is to discover from document clusters, the latent topics that a</context>
<context position="6278" citStr="Nenkova and Vanderwende, 2005" startWordPosition="950" endWordPosition="953">ng to summarization than a ”flat” model, e.g. LDA (Blei et al., 2003b), in that one can discover ”abstract” and ”specific” topics. For instance, discovering that ”baseball” and ”football” are both contained in an abstract class ”sports” can help to identify summary sentences. It follows that summary topics are commonly shared by many documents, while specific topics are more likely to be mentioned in rather a small subset of documents. Feature based learning approaches to summarization methods discover salient features by measuring similarity between candidate sentences and summary sentences (Nenkova and Vanderwende, 2005; Conroy et al., 2006). While such methods are effective in extractive summarization, the fact that some of these methods are based on greedy algorithms can limit the application areas. Moreover, using information on the hidden semantic structure of document clusters would improve the performance of these methods. Recent studies focused on the discovery of latent topics of document sets in extracting summaries. In these models, the challenges of inferring topics of test documents are not addressed in detail. One of the challenges of using a previously trained topic model is that the new docume</context>
<context position="22153" citStr="Nenkova and Vanderwende, 2005" startWordPosition="3728" endWordPosition="3731"> r most frequent unigrams, i.e., wi E vfreq. Given sentence om, let d be the document that om belongs to, i.e., om E d. We measure unigram probabilities for each wi by p(wi E om) = nd(wi E om)/nD(wi), where nd(wi E om) is the number of times wi appears in d and nD(wi) is the number of times wi appears in D. For any ith feature, the value is fmi = 0, if given sentence does not contain wi, otherwise fmi = p(wi E om). We also include bigram extensions of DMF features. 819 (III) Other Features (OF): Term frequency of sentences such as SUMBASTC are proven to be good predictors in sentence scoring (Nenkova and Vanderwende, 2005). We measure the average uunigram probability of a sentence by: p(om) = FIW2o�joM1jPDM, where PD(w) is the observed unigram probability in the document collection D and |om |is the total number of words in om. We use sentence bigram frequency, sentence rank in a document, and sentence size as additional features. 5.2 Predicting Scores for New Sentences Due to the large feature space to explore, we chose to work with support vector regression (SVR) (Drucker et al., 1997) as the learning algorithm to predict sentence scores. Given training sentences {fm, ym}|O| m�1, where fm = {fm1, ..., fmQ} is</context>
</contexts>
<marker>Nenkova, Vanderwende, 2005</marker>
<rawString>A. Nenkova and L. Vanderwende. The impact of frequency on summarization. In Tech. Report MSR-TR-2005-101, Microsoft Research, Redwood, Washington, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>H Jing</author>
<author>M Stys</author>
<author>D Tam</author>
</authors>
<title>Centroid-based summarization for multiple documents. In</title>
<date>2004</date>
<booktitle>In Int. Jrnl. Information Processing and Management,</booktitle>
<contexts>
<context position="2297" citStr="Radev et al., 2004" startWordPosition="331" endWordPosition="334">n provided document-summary pairs, and unsupervised methods based upon properties derived from document clusters. Supervised methods treat the summarization task as a classification/regression problem, e.g., (Shen et al., 2007; Yeh et al., 2005). Each candidate sentence is classified as summary or non-summary based on the features that they pose and those with highest scores are selected. Unsupervised methods aim to score sentences based on semantic groupings extracted from documents, e.g., (DaumeIII and Marcu, 2006; Titov and McDonald, 2008; Tang et al., 2009; Haghighi and Vanderwende, 2009; Radev et al., 2004; Branavan et al., 2009), etc. Such models can yield comparable or better performance on DUC and other evaluations, since representing documents as topic distributions rather than bags of words diminishes the effect of lexical variability. To the best of our knowledge, there is no previous research which utilizes the best features of both approaches for MDS as presented in this paper. In this paper, we present a novel approach that formulates MDS as a prediction problem based on a two-step hybrid model: a generative model for hierarchical topic discovery and a regression model for inference. W</context>
</contexts>
<marker>Radev, Jing, Stys, Tam, 2004</marker>
<rawString>D.R. Radev, H. Jing, M. Stys, and D. Tam. Centroid-based summarization for multiple documents. In In Int. Jrnl. Information Processing and Management, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shen</author>
<author>J T Sun</author>
<author>H Li</author>
<author>Q Yang</author>
<author>Z Chen</author>
</authors>
<title>Document summarization using conditional random fields.</title>
<date>2007</date>
<booktitle>In Proc. IJCAI’07,</booktitle>
<contexts>
<context position="1905" citStr="Shen et al., 2007" startWordPosition="268" endWordPosition="271">g MDS systems, which take document clusters (documents on a same topic) and description of the desired summary focus as input and output a word length limited summary. Human summaries are provided for training summarization models and measuring the performance of machine generated summaries. Extractive summarization methods can be classified into two groups: supervised methods that rely on provided document-summary pairs, and unsupervised methods based upon properties derived from document clusters. Supervised methods treat the summarization task as a classification/regression problem, e.g., (Shen et al., 2007; Yeh et al., 2005). Each candidate sentence is classified as summary or non-summary based on the features that they pose and those with highest scores are selected. Unsupervised methods aim to score sentences based on semantic groupings extracted from documents, e.g., (DaumeIII and Marcu, 2006; Titov and McDonald, 2008; Tang et al., 2009; Haghighi and Vanderwende, 2009; Radev et al., 2004; Branavan et al., 2009), etc. Such models can yield comparable or better performance on DUC and other evaluations, since representing documents as topic distributions rather than bags of words diminishes the</context>
</contexts>
<marker>Shen, Sun, Li, Yang, Chen, 2007</marker>
<rawString>D. Shen, J.T. Sun, H. Li, Q. Yang, and Z. Chen. Document summarization using conditional random fields. In Proc. IJCAI’07, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tang</author>
<author>L Yao</author>
<author>D Chens</author>
</authors>
<title>Multi-topic based query-oriented summarization.</title>
<date>2009</date>
<booktitle>In SIAM International Conference Data Mining,</booktitle>
<contexts>
<context position="2245" citStr="Tang et al., 2009" startWordPosition="323" endWordPosition="326">ied into two groups: supervised methods that rely on provided document-summary pairs, and unsupervised methods based upon properties derived from document clusters. Supervised methods treat the summarization task as a classification/regression problem, e.g., (Shen et al., 2007; Yeh et al., 2005). Each candidate sentence is classified as summary or non-summary based on the features that they pose and those with highest scores are selected. Unsupervised methods aim to score sentences based on semantic groupings extracted from documents, e.g., (DaumeIII and Marcu, 2006; Titov and McDonald, 2008; Tang et al., 2009; Haghighi and Vanderwende, 2009; Radev et al., 2004; Branavan et al., 2009), etc. Such models can yield comparable or better performance on DUC and other evaluations, since representing documents as topic distributions rather than bags of words diminishes the effect of lexical variability. To the best of our knowledge, there is no previous research which utilizes the best features of both approaches for MDS as presented in this paper. In this paper, we present a novel approach that formulates MDS as a prediction problem based on a two-step hybrid model: a generative model for hierarchical top</context>
<context position="5299" citStr="Tang et al., 2009" startWordPosition="802" endWordPosition="805">e coherent and non-redundant summaries. 2 Background and Motivation There are many studies on the principles governing multi-document summarization to produce coherent and semantically relevant summaries. Previous work (Nenkova and Vanderwende, 2005; Conroy et al., 2006), focused on the fact that frequency of words plays an important factor. While, earlier work on summarization depend on a word score function, which is used to measure sentence rank scores based on (semi-)supervised learning methods, recent trend of purely data-driven methods, (Barzilay and Lee, 2004; DaumeIII and Marcu, 2006; Tang et al., 2009; Haghighi and Vanderwende, 2009), have shown remarkable improvements. Our work builds on both methods by constructing a hybrid approach to summarization. Our objective is to discover from document clusters, the latent topics that are organized into hierarchies following (Haghighi and Vanderwende, 2009). A hierarchical model is particularly appealing to summarization than a ”flat” model, e.g. LDA (Blei et al., 2003b), in that one can discover ”abstract” and ”specific” topics. For instance, discovering that ”baseball” and ”football” are both contained in an abstract class ”sports” can help to i</context>
</contexts>
<marker>Tang, Yao, Chens, 2009</marker>
<rawString>J. Tang, L. Yao, and D. Chens. Multi-topic based query-oriented summarization. In SIAM International Conference Data Mining, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>R McDonald</author>
</authors>
<title>A joint model of text and aspect ratings for sentiment summarization. In</title>
<date>2008</date>
<booktitle>ACL-08:HLT,</booktitle>
<contexts>
<context position="2226" citStr="Titov and McDonald, 2008" startWordPosition="319" endWordPosition="322">ion methods can be classified into two groups: supervised methods that rely on provided document-summary pairs, and unsupervised methods based upon properties derived from document clusters. Supervised methods treat the summarization task as a classification/regression problem, e.g., (Shen et al., 2007; Yeh et al., 2005). Each candidate sentence is classified as summary or non-summary based on the features that they pose and those with highest scores are selected. Unsupervised methods aim to score sentences based on semantic groupings extracted from documents, e.g., (DaumeIII and Marcu, 2006; Titov and McDonald, 2008; Tang et al., 2009; Haghighi and Vanderwende, 2009; Radev et al., 2004; Branavan et al., 2009), etc. Such models can yield comparable or better performance on DUC and other evaluations, since representing documents as topic distributions rather than bags of words diminishes the effect of lexical variability. To the best of our knowledge, there is no previous research which utilizes the best features of both approaches for MDS as presented in this paper. In this paper, we present a novel approach that formulates MDS as a prediction problem based on a two-step hybrid model: a generative model f</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>I. Titov and R. McDonald. A joint model of text and aspect ratings for sentiment summarization. In ACL-08:HLT, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>C Brockett</author>
<author>M Gamon</author>
<author>J Jagarlamudi</author>
<author>H Suzuki</author>
<author>L Vanderwende</author>
</authors>
<title>The phthy summarization system: Microsoft research at duc</title>
<date>2007</date>
<booktitle>In Proc. DUC,</booktitle>
<contexts>
<context position="29422" citStr="Toutanova et al., 2007" startWordPosition="4977" endWordPosition="4980">nts sentence term frequency, location, and size features. In comparison to the baseline, OF has a significant effect on the ROUGE scores. In addition, DMF together with OF has shown to improve all scores, in comparison to baseline, on average by 10%. Although the NMF have minimal individual improvement, all these features can statistically improve R-2 without stop words by 12% (significance is measured by t-test statistics). Experiment 3: ROUGE Evaluations We use the following multi-document summarization models along with the Baseline presented in Experiment 2 to evaluate HybSumm. * PYTHY : (Toutanova et al., 2007) A stateof-the-art supervised summarization system that ranked first in overall ROUGE evaluations in DUC2007. Similar to HybHSum, human generated summaries are used to train a sentence ranking system using a classifier model. * HIERSUM : (Haghighi and Vanderwende, 2009) A generative summarization method based on topic models, which uses sentences as an additional level. Using an approximation for inference, sentences are greedily added to a summary so long as they decrease KL-divergence. * HybFSum (Hybrid Flat Summarizer): To investigate the performance of hierarchical topic model, we build an</context>
</contexts>
<marker>Toutanova, Brockett, Gamon, Jagarlamudi, Suzuki, Vanderwende, 2007</marker>
<rawString>K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi, H. Suzuki, and L. Vanderwende. The phthy summarization system: Microsoft research at duc 2007. In Proc. DUC, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Y Yeh</author>
<author>H-R Ke</author>
<author>W P Yang</author>
<author>I-H Meng</author>
</authors>
<title>Text summarization using a trainable summarizer and latent semantic analysis.</title>
<date>2005</date>
<booktitle>In Information Processing and Management,</booktitle>
<contexts>
<context position="1924" citStr="Yeh et al., 2005" startWordPosition="272" endWordPosition="275">h take document clusters (documents on a same topic) and description of the desired summary focus as input and output a word length limited summary. Human summaries are provided for training summarization models and measuring the performance of machine generated summaries. Extractive summarization methods can be classified into two groups: supervised methods that rely on provided document-summary pairs, and unsupervised methods based upon properties derived from document clusters. Supervised methods treat the summarization task as a classification/regression problem, e.g., (Shen et al., 2007; Yeh et al., 2005). Each candidate sentence is classified as summary or non-summary based on the features that they pose and those with highest scores are selected. Unsupervised methods aim to score sentences based on semantic groupings extracted from documents, e.g., (DaumeIII and Marcu, 2006; Titov and McDonald, 2008; Tang et al., 2009; Haghighi and Vanderwende, 2009; Radev et al., 2004; Branavan et al., 2009), etc. Such models can yield comparable or better performance on DUC and other evaluations, since representing documents as topic distributions rather than bags of words diminishes the effect of lexical </context>
</contexts>
<marker>Yeh, Ke, Yang, Meng, 2005</marker>
<rawString>J.Y. Yeh, H.-R. Ke, W.P. Yang, and I-H. Meng. Text summarization using a trainable summarizer and latent semantic analysis. In Information Processing and Management, 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>