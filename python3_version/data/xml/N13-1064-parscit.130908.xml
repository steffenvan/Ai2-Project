<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000129">
<title confidence="0.995177">
On Quality Ratings for Spoken Dialogue Systems – Experts vs. Users
</title>
<author confidence="0.996859">
Stefan Ultes, Alexander Schmitt, and Wolfgang Minker
</author>
<affiliation confidence="0.989924">
Ulm University
</affiliation>
<address confidence="0.9311">
Albert-Einstein-Allee 43
89073 Ulm, Germany
</address>
<email confidence="0.967761">
{stefan.ultes,alexander.schmitt,wolfgang.minker}@uni-ulm.de
</email>
<sectionHeader confidence="0.996087" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999927619047619">
In the field of Intelligent User Interfaces, Spo-
ken Dialogue Systems (SDSs) play a key role
as speech represents a true intuitive means
of human communication. Deriving informa-
tion about its quality can help rendering SDSs
more user-adaptive. Work on automatic esti-
mation of subjective quality usually relies on
statistical models. To create those, manual
data annotation is required, which may be per-
formed by actual users or by experts. Here,
both variants have their advantages and draw-
backs. In this paper, we analyze the relation-
ship between user and expert ratings by in-
vestigating models which combine the advan-
tages of both types of ratings. We explore two
novel approaches using statistical classifica-
tion methods and evaluate those with a pre-
existing corpus providing user and expert rat-
ings. After analyzing the results, we eventu-
ally recommend to use expert ratings instead
of user ratings in general.
</bodyText>
<sectionHeader confidence="0.989412" genericHeader="categories and subject descriptors">
1 Introduction and Motivation
</sectionHeader>
<bodyText confidence="0.999847357142857">
In human-machine interaction it is important that
user interfaces can adapt to the specific requirements
of its users. Handicapped persons or angry users, for
example, have specific needs and should be treated
differently than regular users.
Speech is a major component of modern user in-
terfaces as it is the natural means of human com-
munication. Therefore, it seems logical to use Spo-
ken Dialogue Systems (SDS) as part of Intelligent
User Interfaces enabling speech communication of
different complexity reaching from simple spoken
commands up to complex dialogues. Besides the
spoken words, the speech signal also may be used
to acquire information about the user state, e.g.,
about their emotional state (cf., e.g., (Polzehl et
al., 2011))). By additional analysis of the human-
computer-dialogues, even more abstract informa-
tion may be derived, e.g., the quality of the system
(cf., e.g., (Engelbrecht and M¨oller, 2010)). System
quality information may be used to adapt the sys-
tem’s behavior online during the ongoing dialogue
(cf. (Ultes et al., 2012)).
For determining the quality of Spoken Dialogue
Systems, several aspects are of interest. M¨oller et
al. (2009) presented a taxonomy of quality criteria.
They describe quality as a bipartite issue consisting
of Quality of Service (QoS) and Quality of Experi-
ence (QoE). Quality of Service describes objective
criteria like dialogue duration or number of turns.
While these are well-defined items that can be de-
termined easily, Quality of Experience, which de-
scribes the user experience with subjective criteria,
is more vague and without a sound definition, e.g.,
User Satisfaction (US).
Subjective aspects like US are either determined
by using questionnaires like SASSI (Hone and Gra-
ham, 2000) or the ITU-standard augmented frame-
work for questionnaires (M¨oller, 2003), or by us-
ing single-valued ratings, i.e., a rater only applies
one single score. In general, two major categories
of work on determining single-valued User Satisfac-
tion exist. The satisfaction ratings are applied either
</bodyText>
<listItem confidence="0.9998465">
• by users during or right after the dialogue or
• by experts by listening to recorded dialogues.
</listItem>
<page confidence="0.978231">
569
</page>
<note confidence="0.470136">
Proceedings of NAACL-HLT 2013, pages 569–578,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999952425287356">
In this work, users or user raters are people who
actually perform a dialogue with the system and ap-
ply ratings while doing so. There is no constraint
about their expertise in the field of Human Com-
puter Interaction or Spoken Dialogue Systems: They
may be novices or have a high expertise. With ex-
perts or expert raters, we refer to people who are
not participating in the dialogue thus constituting
a completely different set of people. Expert raters
listen to recorded dialogues after the interactions
and rate them by assuming the point of view of the
actual person performing the dialogue. These ex-
perts are supposed to have some experience with di-
alogue systems. In this work, expert raters were “ad-
vanced students of computer science and engineer-
ing” (Schmitt et al., 2011a).
For User Satisfaction, ratings applied by the users
seem to be clearly the better choice over ratings ap-
plied by third persons. However, determining true
User Satisfaction is only possible by asking real
users interacting with the system. Ideally, the ratings
are applied by users talking to a system employed in
the field, e.g., commercial systems, as these users
have real concerns.
For such Spoken Dialogue Systems, though, it
is not easy to get users to apply quality ratings
to the dialogue – especially for each system-user-
exchange. The users would have to rate either by
pressing a button on the phone or by speech, which
would significantly influence the performance of the
dialogue. Longer dialogues imply longer call dura-
tions which cost money. Further, most callers only
want to quickly get some information from the sys-
tem. Therefore, it may be assumed that most users
do not want to engage in dialogues which are ar-
tificially made longer. This also inhabits the risk
that users who participated in long dialogues do
not want to call again. Therefore, collecting rat-
ings applied by users are considered to be expensive.
One possible way of overcoming the problem of rat-
ing input would be to use some special installation
which enables the users to provide ratings more eas-
ily (cf. (Schmitt et al., 2011b)). However, this is also
expensive and the system’s usability would be very
restricted. Further, this setup could most likely only
be used in a lab situation.
Expert raters, on the other hand, are able to simply
listen to the recorded dialogues and to apply ratings,
e.g., by using a specialized rating software. This
process is much easier and does not require the same
amount of effort needed for acquiring user ratings.
Further, as already pointed out, we refer to experts
as people who have some basic understanding of di-
alogue systems but are not required to be high-level
experts in the field. That is why we believe that these
people can be found easily.
As both categories of ratings have their advan-
tages and disadvantages, this contribution aims at
learning about the differences and similarities of
user and expert ratings with the ultimate goal of
either being able to predict user ratings more effi-
ciently or of advocating for replacing the use of user
ratings by using only expert ratings in general.
Therefore, this work analyzes the relation be-
tween quality ratings applied by user and expert
raters by analyzing approaches which take advan-
tage of both categories: Using the less expensive
rating process with expert raters and still predict-
ing real User Satisfaction ratings. Moreover, this
works’ goal is to shed light on the question whether
information about one rating (in this case the less
expensive expert ratings) may be used to predict the
other rating (the more expensive user ratings). For
this, we present two approaches applying two differ-
ent statistical classification methods for a showcase
corpus. Results of both methods are compared to a
given baseline.
The remainder of this paper is organized as fol-
lows. First, we give a brief overview of work done
in both categories (user ratings vs. expert ratings) in
Section 2 and present our choice of data the analy-
sis in this paper is based on in Section 3. Further,
evaluation metrics are illustrated in Section 4 and
approaches on facilitating prediction of user rater
scores by expert rater information are presented in
Section 5 followed by an evaluation and discussion
of the results in Section 6.
</bodyText>
<sectionHeader confidence="0.941622" genericHeader="general terms">
2 Significant Related Work
</sectionHeader>
<bodyText confidence="0.998084166666667">
Predicting User Satisfaction for SDSs has been in
the focus of research for many years, most famously
the PARADISE framework by Walker et al. (1997).
The authors assume a linear dependency between
quantitative parameters derived from the dialogue
and US, modeling this dependency using linear re-
</bodyText>
<page confidence="0.990597">
570
</page>
<bodyText confidence="0.999962117647059">
gression. Unfortunately, for generating the regres-
sion model, weighting factors have to be computed
for each system anew. This generates high costs
as dialogues have to be performed with real users
where each user further has to complete a question-
naire after completing the dialogue. Moreover, in
the PARADISE framework, only quality measure-
ment for the whole dialogue (or system) is allowed.
However, this is not suitable for using quality infor-
mation for online adaption of the dialogue (cf. (Ultes
et al., 2012)). Furthermore, PARADISE relies on
questionnaires while we focus on work using single-
valued ratings.
Numerous work on predicting User Satisfaction
as a single-valued rating task for each system-user-
exchange has been performed in both categories.
This work is briefly presented in the following.
</bodyText>
<subsectionHeader confidence="0.963028">
2.1 Expert Ratings
</subsectionHeader>
<bodyText confidence="0.999985076923077">
Higashinaka et al. (2010a) proposed a model to pre-
dict turn-wise ratings for human-human dialogues
(transcribed conversation) and human-machine di-
alogues (text from chat system). Ratings ranging
from 1-7 were applied by two expert raters label-
ing “Smoothness”, “Closeness”, and “Willingness”
not achieving a Match Rate per Rating (MR/R)1 of
more than 0.2-0.24. This results are only slightly
above the random baseline of 0.14. Further work
by Higashinaka et al. (2010b) uses ratings for over-
all dialogues to predict ratings for each system-
user-exchange. Again, evaluating in three user
satisfaction categories “Smoothness”, “Closeness”,
and “Willingness” with ratings ranging from 1-7
achieved best performance of 0.19 MR/R.
Interaction Quality (IQ) has been introduced by
Schmitt et al. (2011a) as an alternative performance
measure to User Satisfaction. In their terminology,
US ratings are only applied by users. As their pre-
sented measure uses ratings applied by expert raters,
a different term is used. Each system-user exchange
was annotated by three different raters using strict
guidelines. The ratings ranging from 1-5 are used
as target variable for statistical classifiers using a set
of automatically derivable interaction parameters as
input. They achieve a MR/R of 0.58.
</bodyText>
<footnote confidence="0.7953555">
1MR/R is equal to Unweighted Average Recall (UAR)
which is explained in Section 4.
</footnote>
<subsectionHeader confidence="0.99655">
2.2 User Ratings
</subsectionHeader>
<bodyText confidence="0.99996955">
An approach presented by Engelbrecht et al. (2009)
uses Hidden Markov Models (HMMs) to model the
SDS as a process evolving over time. User Satisfac-
tion was predicted at any point within the dialogue
on a 5 point scale. Evaluation was performed based
on labels the users applied themselves during the di-
alogue.
Hara et al. (2010) derived turn level ratings from
an overall score applied by the users after the dia-
logue. Using n-gram models reflecting the dialogue
history, the achieved results for recognizing User
Satisfaction on a 5 point scale showed to be hardly
above chance.
Work by Schmitt et al. (2011b) deals with deter-
mining User Satisfaction from ratings applied by the
users themselves during the dialogues. A statistical
classification model was trained using automatically
derived interaction parameter to predict User Satis-
faction for each system-user-exchange on a 5-point
scale achieving an MR/R of 0.49.
</bodyText>
<sectionHeader confidence="0.991345" genericHeader="keywords">
3 Corpus
</sectionHeader>
<bodyText confidence="0.9999765">
The corpus used by Schmitt et al. (2011b) not only
contains user ratings but also expert ratings which
makes it a perfect candidate for our research pre-
sented in this paper. Adopting the terminology by
Schmitt et al., user ratings are described as User Sat-
isfaction (US) whereas expert ratings are referred to
with the term Interaction Quality (IQ) (cf. (Schmitt
et al., 2011a)). The data used for all experiments
of this work was collected by Schmitt et al. (2011b)
during a lab user study with 38 users in the domain
of the “Let’s Go Bus Information” system (Raux et
al., 2006) of the Carnegie Mellon University in Pitts-
burgh. 128 calls were collected consisting of a total
of 2,897 system-user exchanges. Both ratings, IQ
and US, are at a scale from 1 to 5 where 1 stands for
“extremely unsatisfied” and 5 for “satisfied”. Each
dialogue starts with a rating of 5 as the user is ex-
pected to be satisfied in the beginning because noth-
ing unsatisfying has happened yet.
Further, the corpus also provides interaction pa-
rameters which may be used as input variables
for the IQ and US recognition models. These
parameters have been derived automatically from
three dialogue modules: Automatic Speech Recog-
</bodyText>
<page confidence="0.973172">
571
</page>
<figure confidence="0.9253195">
e1
e2
</figure>
<figureCaption confidence="0.94099025">
Figure 1: The three different modeling levels representing the interaction at exchange e,,,: The most detailed exchange
level, comprising parameters of the current exchange; the window level, capturing important parameters from the
previous n dialog steps (here n = 3); the dialog level, measuring overall performance values from the entire previous
interaction.
</figureCaption>
<bodyText confidence="0.927307">
nition, Spoken Language Understanding, and Dia-
logue Management. Furthermore, the parameters
are modeled on three different levels (see Figure 1):
</bodyText>
<listItem confidence="0.968982916666667">
• Exchange level parameters can be derived di-
rectly from the respective dialogue modules,
e.g., ASRConfidence.
• Dialogue level parameters consist of counts (#),
means (Mean), etc. of the exchange level pa-
rameters calculated from all exchanges of the
whole dialogue up to the current exchange, e.g.,
MeanASRConfidence.
• Window level parameters consist of counts
({#}), means ({Mean}), etc. of the exchange
level parameters calculated from the last three
exchanges, e.g., {Mean}ASRConfidence.
</listItem>
<sectionHeader confidence="0.984098" genericHeader="introduction">
4 Evaluation metrics
</sectionHeader>
<bodyText confidence="0.9700739">
For measuring the performance of the classification
algorithms, we rely on Unweighted Average Recall
(UAR), Cohen’s Kappa and Spearman’s Rho. The
latter two also represent a measure for similarity of
paired data. All measures will be briefly described
in the following:
Unweighted Average Recall The Unweighted Av-
erage Recall (UAR) is defined as the sum of all
class-wise recalls rc divided by the number of
classes |C|:
</bodyText>
<equation confidence="0.992187333333333">
1 �
UAR =
|C|
cEC
Recall rc for class c is defined as
δhiri , (2)
</equation>
<bodyText confidence="0.999513923076923">
where δ is the Kronecker-delta, hi and ri rep-
resent the corresponding hypothesis-reference-
pair of rating i, and |Rc |the total number of
all ratings of class c. In other words, UAR
for multi-class classification problems is the ac-
curacy corrected by the effects of unbalanced
data.
Cohen’s Kappa To measure the relative agreement
between two corresponding sets of ratings, the
number of label agreements corrected by the
chance level of agreement divided by the max-
imum proportion of times the labelers could
agree is computed. κ is defined as
</bodyText>
<equation confidence="0.943707333333333">
κ = p0 − pc (3)
1 − pc
,
</equation>
<bodyText confidence="0.998805833333333">
where p0 is the rate of agreement and pc is the
chance agreement (Cohen, 1960). As US and
IQ are on an ordinal scale, a weighting factor w
is introduced reducing the discount of disagree-
ments the smaller the difference is between two
ratings (Cohen, 1968):
</bodyText>
<equation confidence="0.99741">
|r1 − r2|
w = (4)
|rmax− rmin |.
</equation>
<bodyText confidence="0.999345">
Here, r1 and r2 denote the rating pair and rmax
and rmin the maximal and minimal rating. This
results in w = 0 for agreement and w = 1 if the
ratings have maximal difference.
Spearman’s Rho The correlation of two variables
describes the degree by that one variable can be
expressed by the other. Spearman’s Rank Cor-
relation Coefficient is a non-parametric method
assuming a monotonic function between the
</bodyText>
<equation confidence="0.995723833333333">
rc . (1)
1
� |R.|
i=1
|Rc|
rc =
</equation>
<page confidence="0.890732">
572
</page>
<bodyText confidence="0.893114">
two variables (Spearman, 1904). It is defined
by
</bodyText>
<equation confidence="0.984837666666667">
_
/ Ei(xi − x)(yi − y) (5)
ρ V Ei(xi − x)2 Pi(yi − 9)2
</equation>
<bodyText confidence="0.999946">
where xi and yi are corresponding ranked rat-
ings and x� and y� the mean ranks. Thus, two
sets of ratings can have total correlation even if
they never agree. This would happen if all rat-
ings are shifted by the same value, for example.
</bodyText>
<sectionHeader confidence="0.786915" genericHeader="method">
5 Recognition of US Using IQ Information
</sectionHeader>
<bodyText confidence="0.999952193548387">
As discussed in Section 1, automatic recognition of
ratings applied by users as performed by Schmitt et
al. (2011b) for User Satisfaction is time-consuming
and expensive. Therefore, approaches are presented
which facilitate expert ratings, i.e., Interaction Qual-
ity, with the hope of making US recognition more
feasible. IQ an US are strongly related as both met-
rics represent the same quantity applied by differ-
ent rater groups. Results of the Mann-Whitney U
test, which is used to test for significant difference
between Interaction Quality and User Satisfaction,
show their difference (p &lt; 0.05) but values for Co-
hen’s Kappa (Cohen, 1960) and Spearman’s Rank
Correlation Coefficient (Spearman, 1904) empha-
size the that IQ and US are quite similar. Achieving
κ = 0.5 can be considered as a moderate agreement
according to Landis and Koch’s Kappa Benchmark
Scale (Landis and Koch, 1977). Furthermore, a cor-
relation of ρ = 0.66 (p &lt; 0.01) indicates a strong
relationship between IQ and US (Cohen, 1988).
While it has been shown that user and expert rat-
ings are similar, it is desirable nonetheless to being
able to predict real user ratings. These ratings are the
desired kind of ratings when it comes to subjective
dialogue system assessment. Only users can give a
rating about their satisfaction level, i.e., how they
like the system and the interaction with the system.
However, user ratings are expensive as elaborated in
Section 1. Therefore, we investigate approaches to
recognize US which rely on means of IQ recogni-
tion.
</bodyText>
<subsectionHeader confidence="0.996548">
5.1 Belief-Based Sequential Recognition
</subsectionHeader>
<bodyText confidence="0.999895709677419">
Methods used for IQ and US recognition by Schmitt
et al. (2011b; 2011a) suffer from the fact that the
sequential character of the data is modeled inade-
quately as they assume statistical independence be-
tween the single exchanges (recognition of IQ and
US does not depend on the respective value of the
previous exchange). Hence, we present a Marko-
vian approach overcoming these issues. A probabil-
ity distribution over all US states, called belief state,
is updated after each system-user-exchange taking
also into account the belief state of the previous ex-
change. This belief update2 is equivalent to the For-
ward Algorithm known from Hidden Markov Mod-
els (cf. (Rabiner, 1989)). In doing so, the new US
probabilities also depend on the US values of the
previous exchange. Moreover, a latent variable is
introduced in order to decouple the target variable
US with the variable the observation probability de-
pends on IQ. This results in an indirect approach
for recognizing User Satisfaction that is based on the
more affordable recognition of Interaction Quality
assuming that a universal mapping between IQ and
US exists.
Thus, to determine the probability b(US) of hav-
ing the true User Satisfaction label US after the cur-
rent system-user-exchange, we rely on Interaction
Quality recognition, whose observation probability
is depicted as P(o|IQ). Furthermore, for coupling
both quantities, we introduce a coherence probabil-
ity P(IQ|US). Belief update for estimating the new
values for b&apos;(US&apos;) is as follows:
</bodyText>
<equation confidence="0.9960845">
b&apos;(US&apos;) = α · X P(o&apos;|IQ&apos;) · P(IQ&apos;|US&apos;)
IQ/
X· P(US&apos;|US)b(US) (6)
US
</equation>
<bodyText confidence="0.994022222222222">
The observation probability P(o&apos;|IQ&apos;) is modeled
using confidence scores of classifiers applied for IQ
recognition. Further, we compute the sum over all
previous US beliefs b(US) weighted by the transi-
tion probability P(US&apos;|US). Both, transition and
coherence probability have been computed by tak-
ing the frequency of their occurrences in the training
data. The α factor is used for normalization only.
Since we are aiming at generating an estimate US
</bodyText>
<footnote confidence="0.988177">
2Terminology is taken from Partially Observable Markov
Decision Processes, cf. (Kaelbling et al., 1998)
</footnote>
<page confidence="0.98581">
573
</page>
<equation confidence="0.996856333333333">
at each exchange, it is calculated by
US = arg max
US/ b&apos;(US&apos;) (7)
</equation>
<bodyText confidence="0.9987072">
generating a sequence of estimates for each dia-
logue.
As the action of the system a can be expected to
influence the satisfaction level of the user, action-
dependency is added to Equation 6 resulting in
</bodyText>
<equation confidence="0.9965115">
b&apos;(US&apos;) = α · 1: P(o&apos;|IQ&apos;) · P(IQ&apos;|US&apos;,a)
IQ/
1: · P(US&apos;|US,a)b(US). (8)
US
</equation>
<bodyText confidence="0.967121">
Hence, each system action a influences coherence
and transition probabilities. It should be noted that
action-dependency can only be introduced as in a
SDS each turn a system action is selected and ex-
ecuted by the dialogue manager.
</bodyText>
<subsectionHeader confidence="0.999569">
5.2 Model Exchange
</subsectionHeader>
<bodyText confidence="0.999979416666667">
While in Belief-Based Sequential Recognition, prob-
ability models are used for coupling expert and user
ratings explicitly, a simpler approach has also been
examined. A statistical classifier trained on the tar-
get variable IQ is used to evaluate classification of
the target variable US. This seems to be reasonable
as the set of scores and meaning of the scores of both
metrics are equivalent. Furthermore, necessary pre-
requisites are fulfilled: the sample corpus contains
both labels, the labels for US and IQ correspond, and
both recognition approaches are based on the same
feature set.
</bodyText>
<sectionHeader confidence="0.997133" genericHeader="method">
6 Experiments and Results
</sectionHeader>
<bodyText confidence="0.986236666666667">
For evaluating Belief-Based Sequential Recognition,
not only the absolute performance is of interest but
also how this performance is influenced by the char-
acteristics of the observation probability, i.e., the
performance of the applied statistical classification
approach and the variance of their confidence scores.
In order to obtain different confidence characteris-
tics, multiple classification algorithms, or algorithm
variants respectively, are needed. Hence, five statis-
tical classifiers have been chosen arbitrarily to pro-
duce the observation probabilities for Belief-Based
Sequential Recognition:
</bodyText>
<listItem confidence="0.9997472">
• SVM3 with cubic kernel
• SVM with RBF-kernel
• Naive Bayes
• Naive Bayes with kernel
• Rule Induction
</listItem>
<bodyText confidence="0.847356210526316">
In contrast to Schmitt et al. (2011b; 2011a), a re-
duced feature set was used consisting of 43 parame-
ters as some textual parameters were removed which
are very specific and take many different values, e.g.,
UTTERANCE (the system utterance) or INTERPRE-
TATION (the interpretation of the speech input).
The resulting feature set consists of the following
parameters (parameter names are in accordance with
the parameter names of the LEGO corpus (Schmitt
et al., 2012)):
Exchange Level ACTIVITY, ACTIVITYTYPE,
UTD, BARGED-IN?, ASRCONFIDENCE,
MEANASRCONFIDENCE, TURNNUMBER,
MODALITY, LOOPNAME, ASRRECOGNI-
TIONSTATUS, ROLEINDEX, ROLENAME,
NOISE?, HELPREQUEST?, REPROMPT?,
WPST, WPUT
Dialogue Level #BARGEINS #ASRSUCCESS,
#HELPREQUESTS, #TIMEOUTS, #TIME-
</bodyText>
<sectionHeader confidence="0.543664571428571" genericHeader="method">
OUTS ASRREJECTIONS, #ASRREJEC-
TIONS, #REPROMPTS, #SYSTEMQUES-
TIONS, #SYSTEMTURNS, #USERTURNS,
%BARGEINS, %ASRSUCCESS, %HEL-
PREQUESTS, %TIMEOUTS, %TIME-
OUTS ASRREJECTIONS, %ASRREJEC-
TIONS, %REPROMPTS
</sectionHeader>
<bodyText confidence="0.9847606">
Window Level {#}TIMEOUTS ASRREJCTIONS,
{#}HELPREQUESTS, {#}ASRREJECTIONS,
{MEAN}ASRCONFIDENCE, {#}TIMEOUTS,
{#}REPROMPTS, {#}SYSTEMQUESTIONS,
{#}ASRSUCCESS, {#}BARGEINS
All results are evaluated with respect to the ref-
erence experiment of direct US recognition (US
recognition using models trained on US). This is
performed in accordance to Schmitt et al. (2011b)
using the statistical classification algorithms stated
</bodyText>
<footnote confidence="0.926176">
3Support Vector Machine, cf. (Vapnik, 1995)
</footnote>
<page confidence="0.99737">
574
</page>
<tableCaption confidence="0.975124">
Table 1: Results (UAR, Cohen’s Kappa, and Spearman’s
Rho) of 10-fold cross-validation for US recognition of US
recognition using models trained on US
Table 3: Results (UAR, Cohen’s Kappa, and Spearman’s
</tableCaption>
<table confidence="0.9955145">
Rho) of 10-fold cross-validation for US recognition of
action-independent Belief-Based Sequential Recognition
Classifier UAR r. p
SVM (cubic Kernel) 0.39 0.33 0.48
SVM (RBF-Kernel) 0.39 0.42 0.55
Naive Bayes 0.36 0.40 0.55
Naive Bayes (Kernel) 0.42 0.44 0.59
Rule Induction 0.50 0.51 0.61
Classifier UAR r. p
SVM (cubic Kernel) 0.28 0.36 0.48
SVM (RBF-Kernel) 0.30 0.40 0.54
Naive Bayes 0.32 0.39 0.54
Naive Bayes (Kernel) 0.33 0.45 0.61
Rule Induction 0.33 0.47 0.63
</table>
<tableCaption confidence="0.81188025">
Table 2: Results (UAR, Cohen’s Kappa, and Spearman’s
Rho) of 10-fold cross-validation for US recognition of the
Model Exchange approach (trained on IQ, evaluated on
US)
</tableCaption>
<table confidence="0.999805666666667">
Classifier UAR r. p
SVM (cubic Kernel) 0.34 0.42 0.55
SVM (RBF-Kernel) 0.34 0.42 0.58
Naive Bayes 0.35 0.40 0.57
Naive Bayes (Kernel) 0.34 0.37 0.60
Rule Induction 0.34 0.42 0.59
</table>
<bodyText confidence="0.99532912">
above. The performance of the reference experiment
is shown in Table 1.
Using the same feature set, these classification al-
gorithms are also applied for the evaluation of the
Model Exchange approach using 10-fold cross val-
idation. Note that the parameters of the classifiers
also remained the same. The data was partitioned
randomly on exchange level, i.e., without regarding
their belonging to a specific dialogue. The measured
results of the Model Exchange approach for the five
classification methods can be seen in Table 2.
While the results are significantly above chance4,
comparing them to the reference experiment reveals
that in terms of UAR the reference experiment out-
performs Model Exchange for all five classifiers.
The achieved r. and p values show similar scores
for both the reference experiment and the Model Ex-
change approach. However, in the data used for the
experiments, the amount of occurrences of the rat-
ings was not balanced (equal for all classes) which
has been identified as the most likely reason for this
effect.
Experiments for Belief-Based Sequential Recog-
nition have also been performed using 10-fold cross
validation. As complete dialogues and the order
</bodyText>
<table confidence="0.561288">
4UAR of 0.2 for five classes
</table>
<tableCaption confidence="0.727603666666667">
Table 4: Results (UAR, Cohen’s Kappa, and Spearman’s
Rho) of 10-fold cross-validation for US recognition of
action-dependent Belief-Based Sequential Recognition
</tableCaption>
<table confidence="0.999818166666667">
Classifier UAR r. p
SVM (cubic Kernel) 0.28 0.35 0.48
SVM (RBF-Kernel) 0.29 0.40 0.54
Naive Bayes 0.32 0.40 0.55
Naive Bayes (Kernel) 0.34 0.44 0.60
Rule Induction 0.35 0.47 0.62
</table>
<bodyText confidence="0.942483928571429">
of exchanges within the dialogues are important for
this approach, the data was partitioned randomly on
the dialogue level. As previously explained, for the
probability distributions of the observation proba-
bility model, classification results of IQ recognition
with 10-fold cross validation has been used in order
to get good estimates for the whole data set. Re-
sults for the action-independent version can be seen
in Table 3.
For the action-dependent version, four different
basic actions ANNOUNCEMENT, CONFIRMATION,
QUESTION, and WAIT have been used, generat-
ing results presented in Table 4. The results il-
lustrate that neither action-independent nor action-
dependent Belief-Based Sequential Recognition can
outperform the reference experiment (cf. Table 1).
Still, both variants achieve results clearly above
chance. Again, the unbalanced data causes r. and
p to be similar to the reference experiment.
A comparison of the action-independent with the
action-dependent approach shows almost no differ-
ences in their performances. Only a slight tendency
towards better UARs for action-dependency can be
spotted.
Figure 2 displays the performances of both vari-
ants of Belief-Based Sequential Recognition along
with performance of IQ recognition and the vari-
ance Q2 of the corresponding confidence distribu-
</bodyText>
<page confidence="0.961959">
575
</page>
<figure confidence="0.999611571428571">
0.60
0.50
0.40
0.30
0.20
0.10
0.00
</figure>
<figureCaption confidence="0.871114333333333">
Figure 2: UAR of IQ recognition and Belief-Based Se-
quential Recognition along with σ2 of confidence distri-
butions of IQ recognition
</figureCaption>
<tableCaption confidence="0.9972325">
Table 5: Recognition performance and variance of confi-
dence distributions for IQ recognition
</tableCaption>
<table confidence="0.999729833333333">
Classifier σ2 UAR κ ρ
SVM (cubic Kernel) 0.03 0.38 0.54 0.69
SVM (RBF-Kernel) 0.05 0.48 0.65 0.77
Naive Bayes 0.13 0.49 0.57 0.71
Naive Bayes (Kernel) 0.12 0.52 0.59 0.73
Rule Induction 0.13 0.55 0.68 0.79
</table>
<bodyText confidence="0.999622230769231">
tion (cf. Table 5). It can easily be seen that with
rising UAR for IQ recognition, σ2 also rises. This
directly transfers to the performance of the Belief-
Based Sequential Recognition. The more accu-
rate the observation performance, the more accurate
the belief prediction. Furthermore, when compar-
ing the action-dependent to the action-independent
variant of Belief-Based Sequential Recognition, bet-
ter IQ performance and therefore a higher variance
also causes slightly better results for the action-
dependent variant. These differences, however, are
only marginally. Therefore, they do not allow for
drawing a conclusion.
</bodyText>
<sectionHeader confidence="0.998814" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999933775510204">
For estimating User Satisfaction-like ratings, two
categories exist: work relying on user ratings and
work relying on expert ratings. To learn something
about their differences and similarities, we explored
the possibility of using the information encoded in
the expert ratings to predict user ratings with the
hope to get acceptable user rating prediction results.
Therefore, we investigated if it is possible to de-
termine the preferred true User Satisfaction value
based on less expensive expert ratings. For this, a
corpus containing both kinds of ratings was chosen,
i.e., User Satisfaction (US) and Interaction Qual-
ity (IQ) ratings. Furthermore, interaction parame-
ters were used to create statistical recognition mod-
els for predicting IQ and US, respectively. Two ap-
proaches have been investigated: Belief-Based Se-
quential Recognition, which is based on an HMM-
like structure with IQ as an additional latent variable,
and Model Exchange, which uses statistical models
trained on IQ to recognize US. Unfortunately, nei-
ther Belief-Based Sequential Recognition nor Model
Exchange achieved results with an acceptable UAR.
The high correlation between expert and user rat-
ings, depicted by high values for Cohen’s κ and
Spearman’s ρ, already allow the conclusion that ex-
pert ratings can be used as a good replacement for
user ratings. Moreover, the presented recognition re-
sults of the Model Exchange approach being clearly
above chance underpin the strong similarity of IQ
and US. Furthermore, IQ recognition is much more
reliable and accurate than US recognition (shown by
higher UAR, κ and ρ values).
While the experiments disproved the hope of get-
ting acceptable user rating prediction results, the ob-
tained results confirmed the similarity between both
kinds of ratings. And as it is not necessary to use
user ratings for most applications, e.g., for using the
quality information to automatically improve the in-
teraction (cf. (Ultes et al., 2012)), we believe that it
suffices to use expert ratings as those can be acquired
easier and less expensively and are similar enough
to user ratings. Prompting the user to apply quality
ratings in everyday situations with real-life systems
will always be annoying to the user while recording
of such interactions are always much easier to rate.
By providing a study for determining quality rat-
ings of dialogues, we hope to encourage other re-
searchers to look into this research for other param-
eters, e.g., emotion recognition.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9861465">
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. In Educational and Psychological Mea-
surement, volume 20, pages 37–46, April.
Jacob Cohen. 1968. Weighted kappa: Nominal scale
</reference>
<page confidence="0.993389">
576
</page>
<reference confidence="0.998208814814814">
agreement provision for scaled disagreement or partial
credit. Psychological bulletin, 70(4):213.
Jacob Cohen. 1988. Statistical power analysis for the
behavioral sciences. New Jersey: Lawrence Erlbaum
Associates, July.
Klaus-Peter Engelbrecht and Sebastian M¨oller. 2010. A
User Model to Predict User Satisfaction with Spoken
Dialog Systems. In Gary Geunbae Lee, Joseph Mari-
ani, Wolfgang Minker, and Satoshi Nakamura, editors,
Spoken Dialogue Systems for Ambient Environments.
2nd Int. Workshop on Spoken Dialogue Systems Tech-
nology, Lecture Notes in Artificial Intelligence, pages
150–155. Springer, October.
Klaus-Peter Engelbrecht, Florian G¨odde, Felix Hartard,
Hamed Ketabdar, and Sebastian M¨oller. 2009. Mod-
eling user satisfaction with hidden markov model. In
SIGDIAL ’09: Proceedings of the SIGDIAL 2009 Con-
ference, pages 170–177, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Sunao Hara, Norihide Kitaoka, and Kazuya Takeda.
2010. Estimation method of user satisfaction using
n-gram-based dialog history model for spoken dialog
system. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC’10), Valletta, Malta, May. European Language
Resources Association (ELRA).
Ryuichiro Higashinaka, Yasuhiro Minami, Kohji
Dohsaka, and Toyomi Meguro. 2010a. Issues in
predicting user satisfaction transitions in dialogues:
Individual differences, evaluation criteria, and predic-
tion models. In Gary Lee, Joseph Mariani, Wolfgang
Minker, and Satoshi Nakamura, editors, Spoken
Dialogue Systems for Ambient Environments, volume
6392 of Lecture Notes in Computer Science, pages
48–60. Springer Berlin / Heidelberg.
Ryuichiro Higashinaka, Yasuhiro Minami, Kohji
Dohsaka, and Toyomi Meguro. 2010b. Modeling
user satisfaction transitions in dialogues from over-
all ratings. In Proceedings of the SIGDIAL 2010
Conference, pages 18–27, Tokyo, Japan, September.
Association for Computational Linguistics.
Kate S. Hone and Robert Graham. 2000. Towards a tool
for the subjective assessment of speech system inter-
faces (sassi). Nat. Lang. Eng., 6(3-4):287–303.
L. P. Kaelbling, M. L. Littman, and A. R. Cassandra.
1998. Planning and acting in partially observable
stochastic domains. Artificial Intelligence, 101(1-
2):99–134.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159–174, March.
Sebastian M¨oller, Klaus-Peter Engelbrecht, C. K¨uhnel,
I. Wechsung, and B. Weiss. 2009. A taxonomy of
quality of service and quality of experience of multi-
modal human-machine interaction. In Quality of Mul-
timedia Experience, 2009. QoMEx 2009. International
Workshop on, pages 7–12, July.
Sebastian M¨oller. 2003. Subjective Quality Evalua-
tion of Telephone Services Based on Spoken Dia-
logue Systems. ITU-T Recommendation P.851, Inter-
national Telecommunication Union, Geneva, Switzer-
land, November. Based on ITU-T Contr. COM 12-59
(2003).
Tim Polzehl, Alexander Schmitt, and Florian Metze.
2011. Salient features for anger recognition in german
and english ivr portals. In Wolfgang Minker, Gary Ge-
unbae Lee, Satoshi Nakamura, and Joseph Mariani,
editors, Spoken Dialogue Systems Technology and De-
sign, pages 83–105. Springer New York. 10.1007/978-
1-4419-7934-6 4.
Lawrence R. Rabiner. 1989. A tutorial on hidden Markov
models and selected applications in speech recogni-
tion. Morgan Kaufmann Publishers Inc., San Fran-
cisco, CA, USA.
Antoine Raux, Dan Bohus, Brian Langner, Alan W.
Black, and Maxine Eskenazi. 2006. Doing research
on a deployed spoken dialogue system: One year of
lets go! experience. In Proc. of the International Con-
ference on Speech and Language Processing (ICSLP),
September.
Alexander Schmitt, Benjamin Schatz, and Wolfgang
Minker. 2011a. Modeling and predicting quality in
spoken human-computer interaction. In Proceedings
of the SIGDIAL 2011 Conference, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Alexander Schmitt, Benjamin Schatz, and Wolfgang
Minker. 2011b. A statistical approach for estimat-
ing user satisfaction in spoken human-machine inter-
action. In Proceedings of the IEEE Jordan Confer-
ence on Applied Electrical Engineering and Comput-
ing Technologies (AEECT), Amman, Jordan, Decem-
ber. IEEE.
Alexander Schmitt, Stefan Ultes, and Wolfgang Minker.
2012. A parameterized and annotated corpus of the
cmu let’s go bus information system. In International
Conference on Language Resources and Evaluation
(LREC).
C. Spearman. 1904. The proof and measurement of as-
sociation between two things. American Journal of
Psychology, 15:88–103.
Stefan Ultes, Alexander Schmitt, and Wolfgang Minker.
2012. Towards quality-adaptive spoken dialogue man-
agement. In NAACL-HLT Workshop on Future di-
rections and needs in the Spoken Dialog Commu-
</reference>
<page confidence="0.971326">
577
</page>
<reference confidence="0.999466846153846">
nity: Tools and Data (SDCTD 2012), pages 49–52,
Montr´eal, Canada, June. Association for Computa-
tional Linguistics.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc., New
York, NY, USA.
Marilyn Walker, Diane Litman, Candace A. Kamm, and
Alicia Abella. 1997. Paradise: a framework for eval-
uating spoken dialogue agents. In Proceedings of the
eighth conference on European chapter of the Associ-
ation for Computational Linguistics, pages 271–280,
Morristown, NJ, USA. Association for Computational
Linguistics.
</reference>
<page confidence="0.996679">
578
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.731265">
<title confidence="0.999879">On Quality Ratings for Spoken Dialogue Systems – Experts vs. Users</title>
<author confidence="0.989566">Stefan Ultes</author>
<author confidence="0.989566">Alexander Schmitt</author>
<author confidence="0.989566">Wolfgang</author>
<affiliation confidence="0.843213">Ulm Albert-Einstein-Allee</affiliation>
<address confidence="0.971678">89073 Ulm,</address>
<abstract confidence="0.999430590909091">In the field of Intelligent User Interfaces, Spoken Dialogue Systems (SDSs) play a key role as speech represents a true intuitive means of human communication. Deriving information about its quality can help rendering SDSs more user-adaptive. Work on automatic estimation of subjective quality usually relies on statistical models. To create those, manual data annotation is required, which may be performed by actual users or by experts. Here, both variants have their advantages and drawbacks. In this paper, we analyze the relationship between user and expert ratings by investigating models which combine the advantages of both types of ratings. We explore two novel approaches using statistical classification methods and evaluate those with a preexisting corpus providing user and expert ratings. After analyzing the results, we eventually recommend to use expert ratings instead of user ratings in general.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>In Educational and Psychological Measurement,</booktitle>
<volume>20</volume>
<pages>37--46</pages>
<contexts>
<context position="14589" citStr="Cohen, 1960" startWordPosition="2345" endWordPosition="2346">and ri represent the corresponding hypothesis-referencepair of rating i, and |Rc |the total number of all ratings of class c. In other words, UAR for multi-class classification problems is the accuracy corrected by the effects of unbalanced data. Cohen’s Kappa To measure the relative agreement between two corresponding sets of ratings, the number of label agreements corrected by the chance level of agreement divided by the maximum proportion of times the labelers could agree is computed. κ is defined as κ = p0 − pc (3) 1 − pc , where p0 is the rate of agreement and pc is the chance agreement (Cohen, 1960). As US and IQ are on an ordinal scale, a weighting factor w is introduced reducing the discount of disagreements the smaller the difference is between two ratings (Cohen, 1968): |r1 − r2| w = (4) |rmax− rmin |. Here, r1 and r2 denote the rating pair and rmax and rmin the maximal and minimal rating. This results in w = 0 for agreement and w = 1 if the ratings have maximal difference. Spearman’s Rho The correlation of two variables describes the degree by that one variable can be expressed by the other. Spearman’s Rank Correlation Coefficient is a non-parametric method assuming a monotonic func</context>
<context position="16263" citStr="Cohen, 1960" startWordPosition="2643" endWordPosition="2644">tomatic recognition of ratings applied by users as performed by Schmitt et al. (2011b) for User Satisfaction is time-consuming and expensive. Therefore, approaches are presented which facilitate expert ratings, i.e., Interaction Quality, with the hope of making US recognition more feasible. IQ an US are strongly related as both metrics represent the same quantity applied by different rater groups. Results of the Mann-Whitney U test, which is used to test for significant difference between Interaction Quality and User Satisfaction, show their difference (p &lt; 0.05) but values for Cohen’s Kappa (Cohen, 1960) and Spearman’s Rank Correlation Coefficient (Spearman, 1904) emphasize the that IQ and US are quite similar. Achieving κ = 0.5 can be considered as a moderate agreement according to Landis and Koch’s Kappa Benchmark Scale (Landis and Koch, 1977). Furthermore, a correlation of ρ = 0.66 (p &lt; 0.01) indicates a strong relationship between IQ and US (Cohen, 1988). While it has been shown that user and expert ratings are similar, it is desirable nonetheless to being able to predict real user ratings. These ratings are the desired kind of ratings when it comes to subjective dialogue system assessmen</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. In Educational and Psychological Measurement, volume 20, pages 37–46, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit.</title>
<date>1968</date>
<journal>Psychological bulletin,</journal>
<volume>70</volume>
<issue>4</issue>
<contexts>
<context position="14766" citStr="Cohen, 1968" startWordPosition="2376" endWordPosition="2377">problems is the accuracy corrected by the effects of unbalanced data. Cohen’s Kappa To measure the relative agreement between two corresponding sets of ratings, the number of label agreements corrected by the chance level of agreement divided by the maximum proportion of times the labelers could agree is computed. κ is defined as κ = p0 − pc (3) 1 − pc , where p0 is the rate of agreement and pc is the chance agreement (Cohen, 1960). As US and IQ are on an ordinal scale, a weighting factor w is introduced reducing the discount of disagreements the smaller the difference is between two ratings (Cohen, 1968): |r1 − r2| w = (4) |rmax− rmin |. Here, r1 and r2 denote the rating pair and rmax and rmin the maximal and minimal rating. This results in w = 0 for agreement and w = 1 if the ratings have maximal difference. Spearman’s Rho The correlation of two variables describes the degree by that one variable can be expressed by the other. Spearman’s Rank Correlation Coefficient is a non-parametric method assuming a monotonic function between the rc . (1) 1 � |R.| i=1 |Rc| rc = 572 two variables (Spearman, 1904). It is defined by _ / Ei(xi − x)(yi − y) (5) ρ V Ei(xi − x)2 Pi(yi − 9)2 where xi and yi are </context>
</contexts>
<marker>Cohen, 1968</marker>
<rawString>Jacob Cohen. 1968. Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit. Psychological bulletin, 70(4):213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>Statistical power analysis for the behavioral sciences. New Jersey: Lawrence Erlbaum Associates,</title>
<date>1988</date>
<contexts>
<context position="16624" citStr="Cohen, 1988" startWordPosition="2704" endWordPosition="2705">tity applied by different rater groups. Results of the Mann-Whitney U test, which is used to test for significant difference between Interaction Quality and User Satisfaction, show their difference (p &lt; 0.05) but values for Cohen’s Kappa (Cohen, 1960) and Spearman’s Rank Correlation Coefficient (Spearman, 1904) emphasize the that IQ and US are quite similar. Achieving κ = 0.5 can be considered as a moderate agreement according to Landis and Koch’s Kappa Benchmark Scale (Landis and Koch, 1977). Furthermore, a correlation of ρ = 0.66 (p &lt; 0.01) indicates a strong relationship between IQ and US (Cohen, 1988). While it has been shown that user and expert ratings are similar, it is desirable nonetheless to being able to predict real user ratings. These ratings are the desired kind of ratings when it comes to subjective dialogue system assessment. Only users can give a rating about their satisfaction level, i.e., how they like the system and the interaction with the system. However, user ratings are expensive as elaborated in Section 1. Therefore, we investigate approaches to recognize US which rely on means of IQ recognition. 5.1 Belief-Based Sequential Recognition Methods used for IQ and US recogn</context>
</contexts>
<marker>Cohen, 1988</marker>
<rawString>Jacob Cohen. 1988. Statistical power analysis for the behavioral sciences. New Jersey: Lawrence Erlbaum Associates, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus-Peter Engelbrecht</author>
<author>Sebastian M¨oller</author>
</authors>
<title>A User Model to Predict User Satisfaction with Spoken Dialog Systems.</title>
<date>2010</date>
<booktitle>Spoken Dialogue Systems for Ambient Environments. 2nd Int. Workshop on Spoken Dialogue Systems Technology, Lecture Notes in Artificial Intelligence,</booktitle>
<pages>150--155</pages>
<editor>In Gary Geunbae Lee, Joseph Mariani, Wolfgang Minker, and Satoshi Nakamura, editors,</editor>
<publisher>Springer,</publisher>
<marker>Engelbrecht, M¨oller, 2010</marker>
<rawString>Klaus-Peter Engelbrecht and Sebastian M¨oller. 2010. A User Model to Predict User Satisfaction with Spoken Dialog Systems. In Gary Geunbae Lee, Joseph Mariani, Wolfgang Minker, and Satoshi Nakamura, editors, Spoken Dialogue Systems for Ambient Environments. 2nd Int. Workshop on Spoken Dialogue Systems Technology, Lecture Notes in Artificial Intelligence, pages 150–155. Springer, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus-Peter Engelbrecht</author>
<author>Florian G¨odde</author>
<author>Felix Hartard</author>
<author>Hamed Ketabdar</author>
<author>Sebastian M¨oller</author>
</authors>
<title>Modeling user satisfaction with hidden markov model.</title>
<date>2009</date>
<booktitle>In SIGDIAL ’09: Proceedings of the SIGDIAL 2009 Conference,</booktitle>
<pages>170--177</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Engelbrecht, G¨odde, Hartard, Ketabdar, M¨oller, 2009</marker>
<rawString>Klaus-Peter Engelbrecht, Florian G¨odde, Felix Hartard, Hamed Ketabdar, and Sebastian M¨oller. 2009. Modeling user satisfaction with hidden markov model. In SIGDIAL ’09: Proceedings of the SIGDIAL 2009 Conference, pages 170–177, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Sunao Hara</author>
<author>Norihide Kitaoka</author>
<author>Kazuya Takeda</author>
</authors>
<title>Estimation method of user satisfaction using n-gram-based dialog history model for spoken dialog system.</title>
<date>2010</date>
<booktitle>In Nicoletta Calzolari</booktitle>
<editor>(Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<location>Valletta, Malta,</location>
<contexts>
<context position="10618" citStr="Hara et al. (2010)" startWordPosition="1694" endWordPosition="1697">es. The ratings ranging from 1-5 are used as target variable for statistical classifiers using a set of automatically derivable interaction parameters as input. They achieve a MR/R of 0.58. 1MR/R is equal to Unweighted Average Recall (UAR) which is explained in Section 4. 2.2 User Ratings An approach presented by Engelbrecht et al. (2009) uses Hidden Markov Models (HMMs) to model the SDS as a process evolving over time. User Satisfaction was predicted at any point within the dialogue on a 5 point scale. Evaluation was performed based on labels the users applied themselves during the dialogue. Hara et al. (2010) derived turn level ratings from an overall score applied by the users after the dialogue. Using n-gram models reflecting the dialogue history, the achieved results for recognizing User Satisfaction on a 5 point scale showed to be hardly above chance. Work by Schmitt et al. (2011b) deals with determining User Satisfaction from ratings applied by the users themselves during the dialogues. A statistical classification model was trained using automatically derived interaction parameter to predict User Satisfaction for each system-user-exchange on a 5-point scale achieving an MR/R of 0.49. 3 Corpu</context>
</contexts>
<marker>Hara, Kitaoka, Takeda, 2010</marker>
<rawString>Sunao Hara, Norihide Kitaoka, and Kazuya Takeda. 2010. Estimation method of user satisfaction using n-gram-based dialog history model for spoken dialog system. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10), Valletta, Malta, May. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ryuichiro Higashinaka</author>
</authors>
<title>Yasuhiro Minami, Kohji Dohsaka, and Toyomi Meguro. 2010a. Issues in predicting user satisfaction transitions in dialogues: Individual differences, evaluation criteria, and prediction models.</title>
<booktitle>Spoken Dialogue Systems for Ambient Environments,</booktitle>
<volume>6392</volume>
<pages>48--60</pages>
<editor>In Gary Lee, Joseph Mariani, Wolfgang Minker, and Satoshi Nakamura, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<marker>Higashinaka, </marker>
<rawString>Ryuichiro Higashinaka, Yasuhiro Minami, Kohji Dohsaka, and Toyomi Meguro. 2010a. Issues in predicting user satisfaction transitions in dialogues: Individual differences, evaluation criteria, and prediction models. In Gary Lee, Joseph Mariani, Wolfgang Minker, and Satoshi Nakamura, editors, Spoken Dialogue Systems for Ambient Environments, volume 6392 of Lecture Notes in Computer Science, pages 48–60. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryuichiro Higashinaka</author>
<author>Yasuhiro Minami</author>
<author>Kohji Dohsaka</author>
<author>Toyomi Meguro</author>
</authors>
<title>Modeling user satisfaction transitions in dialogues from overall ratings.</title>
<date>2010</date>
<booktitle>In Proceedings of the SIGDIAL 2010 Conference,</booktitle>
<pages>18--27</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Tokyo, Japan,</location>
<contexts>
<context position="8929" citStr="Higashinaka et al. (2010" startWordPosition="1432" endWordPosition="1435">complete a questionnaire after completing the dialogue. Moreover, in the PARADISE framework, only quality measurement for the whole dialogue (or system) is allowed. However, this is not suitable for using quality information for online adaption of the dialogue (cf. (Ultes et al., 2012)). Furthermore, PARADISE relies on questionnaires while we focus on work using singlevalued ratings. Numerous work on predicting User Satisfaction as a single-valued rating task for each system-userexchange has been performed in both categories. This work is briefly presented in the following. 2.1 Expert Ratings Higashinaka et al. (2010a) proposed a model to predict turn-wise ratings for human-human dialogues (transcribed conversation) and human-machine dialogues (text from chat system). Ratings ranging from 1-7 were applied by two expert raters labeling “Smoothness”, “Closeness”, and “Willingness” not achieving a Match Rate per Rating (MR/R)1 of more than 0.2-0.24. This results are only slightly above the random baseline of 0.14. Further work by Higashinaka et al. (2010b) uses ratings for overall dialogues to predict ratings for each systemuser-exchange. Again, evaluating in three user satisfaction categories “Smoothness”, </context>
</contexts>
<marker>Higashinaka, Minami, Dohsaka, Meguro, 2010</marker>
<rawString>Ryuichiro Higashinaka, Yasuhiro Minami, Kohji Dohsaka, and Toyomi Meguro. 2010b. Modeling user satisfaction transitions in dialogues from overall ratings. In Proceedings of the SIGDIAL 2010 Conference, pages 18–27, Tokyo, Japan, September. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kate S Hone</author>
<author>Robert Graham</author>
</authors>
<title>Towards a tool for the subjective assessment of speech system interfaces (sassi).</title>
<date>2000</date>
<journal>Nat. Lang. Eng.,</journal>
<pages>6--3</pages>
<contexts>
<context position="2943" citStr="Hone and Graham, 2000" startWordPosition="447" endWordPosition="451"> are of interest. M¨oller et al. (2009) presented a taxonomy of quality criteria. They describe quality as a bipartite issue consisting of Quality of Service (QoS) and Quality of Experience (QoE). Quality of Service describes objective criteria like dialogue duration or number of turns. While these are well-defined items that can be determined easily, Quality of Experience, which describes the user experience with subjective criteria, is more vague and without a sound definition, e.g., User Satisfaction (US). Subjective aspects like US are either determined by using questionnaires like SASSI (Hone and Graham, 2000) or the ITU-standard augmented framework for questionnaires (M¨oller, 2003), or by using single-valued ratings, i.e., a rater only applies one single score. In general, two major categories of work on determining single-valued User Satisfaction exist. The satisfaction ratings are applied either • by users during or right after the dialogue or • by experts by listening to recorded dialogues. 569 Proceedings of NAACL-HLT 2013, pages 569–578, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics In this work, users or user raters are people who actually perform a dial</context>
</contexts>
<marker>Hone, Graham, 2000</marker>
<rawString>Kate S. Hone and Robert Graham. 2000. Towards a tool for the subjective assessment of speech system interfaces (sassi). Nat. Lang. Eng., 6(3-4):287–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L P Kaelbling</author>
<author>M L Littman</author>
<author>A R Cassandra</author>
</authors>
<title>Planning and acting in partially observable stochastic domains.</title>
<date>1998</date>
<journal>Artificial Intelligence,</journal>
<pages>101--1</pages>
<contexts>
<context position="19321" citStr="Kaelbling et al., 1998" startWordPosition="3131" endWordPosition="3134">b&apos;(US&apos;) = α · X P(o&apos;|IQ&apos;) · P(IQ&apos;|US&apos;) IQ/ X· P(US&apos;|US)b(US) (6) US The observation probability P(o&apos;|IQ&apos;) is modeled using confidence scores of classifiers applied for IQ recognition. Further, we compute the sum over all previous US beliefs b(US) weighted by the transition probability P(US&apos;|US). Both, transition and coherence probability have been computed by taking the frequency of their occurrences in the training data. The α factor is used for normalization only. Since we are aiming at generating an estimate US 2Terminology is taken from Partially Observable Markov Decision Processes, cf. (Kaelbling et al., 1998) 573 at each exchange, it is calculated by US = arg max US/ b&apos;(US&apos;) (7) generating a sequence of estimates for each dialogue. As the action of the system a can be expected to influence the satisfaction level of the user, actiondependency is added to Equation 6 resulting in b&apos;(US&apos;) = α · 1: P(o&apos;|IQ&apos;) · P(IQ&apos;|US&apos;,a) IQ/ 1: · P(US&apos;|US,a)b(US). (8) US Hence, each system action a influences coherence and transition probabilities. It should be noted that action-dependency can only be introduced as in a SDS each turn a system action is selected and executed by the dialogue manager. 5.2 Model Exchange</context>
</contexts>
<marker>Kaelbling, Littman, Cassandra, 1998</marker>
<rawString>L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. 1998. Planning and acting in partially observable stochastic domains. Artificial Intelligence, 101(1-2):99–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="16509" citStr="Landis and Koch, 1977" startWordPosition="2681" endWordPosition="2684">, with the hope of making US recognition more feasible. IQ an US are strongly related as both metrics represent the same quantity applied by different rater groups. Results of the Mann-Whitney U test, which is used to test for significant difference between Interaction Quality and User Satisfaction, show their difference (p &lt; 0.05) but values for Cohen’s Kappa (Cohen, 1960) and Spearman’s Rank Correlation Coefficient (Spearman, 1904) emphasize the that IQ and US are quite similar. Achieving κ = 0.5 can be considered as a moderate agreement according to Landis and Koch’s Kappa Benchmark Scale (Landis and Koch, 1977). Furthermore, a correlation of ρ = 0.66 (p &lt; 0.01) indicates a strong relationship between IQ and US (Cohen, 1988). While it has been shown that user and expert ratings are similar, it is desirable nonetheless to being able to predict real user ratings. These ratings are the desired kind of ratings when it comes to subjective dialogue system assessment. Only users can give a rating about their satisfaction level, i.e., how they like the system and the interaction with the system. However, user ratings are expensive as elaborated in Section 1. Therefore, we investigate approaches to recognize </context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. R. Landis and G. G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33(1):159–174, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian M¨oller</author>
<author>Klaus-Peter Engelbrecht</author>
<author>C K¨uhnel</author>
<author>I Wechsung</author>
<author>B Weiss</author>
</authors>
<title>A taxonomy of quality of service and quality of experience of multimodal human-machine interaction.</title>
<date>2009</date>
<booktitle>In Quality of Multimedia Experience,</booktitle>
<pages>7--12</pages>
<marker>M¨oller, Engelbrecht, K¨uhnel, Wechsung, Weiss, 2009</marker>
<rawString>Sebastian M¨oller, Klaus-Peter Engelbrecht, C. K¨uhnel, I. Wechsung, and B. Weiss. 2009. A taxonomy of quality of service and quality of experience of multimodal human-machine interaction. In Quality of Multimedia Experience, 2009. QoMEx 2009. International Workshop on, pages 7–12, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian M¨oller</author>
</authors>
<title>Subjective Quality Evaluation of Telephone Services Based on Spoken Dialogue Systems.</title>
<date>2003</date>
<booktitle>ITU-T Recommendation P.851, International Telecommunication</booktitle>
<pages>12--59</pages>
<location>Union, Geneva, Switzerland,</location>
<marker>M¨oller, 2003</marker>
<rawString>Sebastian M¨oller. 2003. Subjective Quality Evaluation of Telephone Services Based on Spoken Dialogue Systems. ITU-T Recommendation P.851, International Telecommunication Union, Geneva, Switzerland, November. Based on ITU-T Contr. COM 12-59 (2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Polzehl</author>
<author>Alexander Schmitt</author>
<author>Florian Metze</author>
</authors>
<title>Salient features for anger recognition in german and english ivr portals.</title>
<date>2011</date>
<booktitle>Spoken Dialogue Systems Technology and Design,</booktitle>
<pages>83--105</pages>
<editor>In Wolfgang Minker, Gary Geunbae Lee, Satoshi Nakamura, and Joseph Mariani, editors,</editor>
<publisher>Springer</publisher>
<location>New York.</location>
<contexts>
<context position="1933" citStr="Polzehl et al., 2011" startWordPosition="292" endWordPosition="295">andicapped persons or angry users, for example, have specific needs and should be treated differently than regular users. Speech is a major component of modern user interfaces as it is the natural means of human communication. Therefore, it seems logical to use Spoken Dialogue Systems (SDS) as part of Intelligent User Interfaces enabling speech communication of different complexity reaching from simple spoken commands up to complex dialogues. Besides the spoken words, the speech signal also may be used to acquire information about the user state, e.g., about their emotional state (cf., e.g., (Polzehl et al., 2011))). By additional analysis of the humancomputer-dialogues, even more abstract information may be derived, e.g., the quality of the system (cf., e.g., (Engelbrecht and M¨oller, 2010)). System quality information may be used to adapt the system’s behavior online during the ongoing dialogue (cf. (Ultes et al., 2012)). For determining the quality of Spoken Dialogue Systems, several aspects are of interest. M¨oller et al. (2009) presented a taxonomy of quality criteria. They describe quality as a bipartite issue consisting of Quality of Service (QoS) and Quality of Experience (QoE). Quality of Serv</context>
</contexts>
<marker>Polzehl, Schmitt, Metze, 2011</marker>
<rawString>Tim Polzehl, Alexander Schmitt, and Florian Metze. 2011. Salient features for anger recognition in german and english ivr portals. In Wolfgang Minker, Gary Geunbae Lee, Satoshi Nakamura, and Joseph Mariani, editors, Spoken Dialogue Systems Technology and Design, pages 83–105. Springer New York. 10.1007/978-1-4419-7934-6 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1989</date>
<publisher>Morgan Kaufmann Publishers Inc.,</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="17866" citStr="Rabiner, 1989" startWordPosition="2907" endWordPosition="2908">1b; 2011a) suffer from the fact that the sequential character of the data is modeled inadequately as they assume statistical independence between the single exchanges (recognition of IQ and US does not depend on the respective value of the previous exchange). Hence, we present a Markovian approach overcoming these issues. A probability distribution over all US states, called belief state, is updated after each system-user-exchange taking also into account the belief state of the previous exchange. This belief update2 is equivalent to the Forward Algorithm known from Hidden Markov Models (cf. (Rabiner, 1989)). In doing so, the new US probabilities also depend on the US values of the previous exchange. Moreover, a latent variable is introduced in order to decouple the target variable US with the variable the observation probability depends on IQ. This results in an indirect approach for recognizing User Satisfaction that is based on the more affordable recognition of Interaction Quality assuming that a universal mapping between IQ and US exists. Thus, to determine the probability b(US) of having the true User Satisfaction label US after the current system-user-exchange, we rely on Interaction Qual</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Lawrence R. Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Raux</author>
<author>Dan Bohus</author>
<author>Brian Langner</author>
<author>Alan W Black</author>
<author>Maxine Eskenazi</author>
</authors>
<title>Doing research on a deployed spoken dialogue system: One year of lets go! experience.</title>
<date>2006</date>
<booktitle>In Proc. of the International Conference on Speech and Language Processing (ICSLP),</booktitle>
<contexts>
<context position="11799" citStr="Raux et al., 2006" startWordPosition="1889" endWordPosition="1892">le achieving an MR/R of 0.49. 3 Corpus The corpus used by Schmitt et al. (2011b) not only contains user ratings but also expert ratings which makes it a perfect candidate for our research presented in this paper. Adopting the terminology by Schmitt et al., user ratings are described as User Satisfaction (US) whereas expert ratings are referred to with the term Interaction Quality (IQ) (cf. (Schmitt et al., 2011a)). The data used for all experiments of this work was collected by Schmitt et al. (2011b) during a lab user study with 38 users in the domain of the “Let’s Go Bus Information” system (Raux et al., 2006) of the Carnegie Mellon University in Pittsburgh. 128 calls were collected consisting of a total of 2,897 system-user exchanges. Both ratings, IQ and US, are at a scale from 1 to 5 where 1 stands for “extremely unsatisfied” and 5 for “satisfied”. Each dialogue starts with a rating of 5 as the user is expected to be satisfied in the beginning because nothing unsatisfying has happened yet. Further, the corpus also provides interaction parameters which may be used as input variables for the IQ and US recognition models. These parameters have been derived automatically from three dialogue modules:</context>
</contexts>
<marker>Raux, Bohus, Langner, Black, Eskenazi, 2006</marker>
<rawString>Antoine Raux, Dan Bohus, Brian Langner, Alan W. Black, and Maxine Eskenazi. 2006. Doing research on a deployed spoken dialogue system: One year of lets go! experience. In Proc. of the International Conference on Speech and Language Processing (ICSLP), September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Schmitt</author>
<author>Benjamin Schatz</author>
<author>Wolfgang Minker</author>
</authors>
<title>Modeling and predicting quality in spoken human-computer interaction.</title>
<date>2011</date>
<booktitle>In Proceedings of the SIGDIAL 2011 Conference,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="4248" citStr="Schmitt et al., 2011" startWordPosition="661" endWordPosition="664">heir expertise in the field of Human Computer Interaction or Spoken Dialogue Systems: They may be novices or have a high expertise. With experts or expert raters, we refer to people who are not participating in the dialogue thus constituting a completely different set of people. Expert raters listen to recorded dialogues after the interactions and rate them by assuming the point of view of the actual person performing the dialogue. These experts are supposed to have some experience with dialogue systems. In this work, expert raters were “advanced students of computer science and engineering” (Schmitt et al., 2011a). For User Satisfaction, ratings applied by the users seem to be clearly the better choice over ratings applied by third persons. However, determining true User Satisfaction is only possible by asking real users interacting with the system. Ideally, the ratings are applied by users talking to a system employed in the field, e.g., commercial systems, as these users have real concerns. For such Spoken Dialogue Systems, though, it is not easy to get users to apply quality ratings to the dialogue – especially for each system-userexchange. The users would have to rate either by pressing a button </context>
<context position="5566" citStr="Schmitt et al., 2011" startWordPosition="883" endWordPosition="886">nger dialogues imply longer call durations which cost money. Further, most callers only want to quickly get some information from the system. Therefore, it may be assumed that most users do not want to engage in dialogues which are artificially made longer. This also inhabits the risk that users who participated in long dialogues do not want to call again. Therefore, collecting ratings applied by users are considered to be expensive. One possible way of overcoming the problem of rating input would be to use some special installation which enables the users to provide ratings more easily (cf. (Schmitt et al., 2011b)). However, this is also expensive and the system’s usability would be very restricted. Further, this setup could most likely only be used in a lab situation. Expert raters, on the other hand, are able to simply listen to the recorded dialogues and to apply ratings, e.g., by using a specialized rating software. This process is much easier and does not require the same amount of effort needed for acquiring user ratings. Further, as already pointed out, we refer to experts as people who have some basic understanding of dialogue systems but are not required to be high-level experts in the field</context>
<context position="9698" citStr="Schmitt et al. (2011" startWordPosition="1545" endWordPosition="1548">tem). Ratings ranging from 1-7 were applied by two expert raters labeling “Smoothness”, “Closeness”, and “Willingness” not achieving a Match Rate per Rating (MR/R)1 of more than 0.2-0.24. This results are only slightly above the random baseline of 0.14. Further work by Higashinaka et al. (2010b) uses ratings for overall dialogues to predict ratings for each systemuser-exchange. Again, evaluating in three user satisfaction categories “Smoothness”, “Closeness”, and “Willingness” with ratings ranging from 1-7 achieved best performance of 0.19 MR/R. Interaction Quality (IQ) has been introduced by Schmitt et al. (2011a) as an alternative performance measure to User Satisfaction. In their terminology, US ratings are only applied by users. As their presented measure uses ratings applied by expert raters, a different term is used. Each system-user exchange was annotated by three different raters using strict guidelines. The ratings ranging from 1-5 are used as target variable for statistical classifiers using a set of automatically derivable interaction parameters as input. They achieve a MR/R of 0.58. 1MR/R is equal to Unweighted Average Recall (UAR) which is explained in Section 4. 2.2 User Ratings An appro</context>
<context position="11259" citStr="Schmitt et al. (2011" startWordPosition="1795" endWordPosition="1798">atings from an overall score applied by the users after the dialogue. Using n-gram models reflecting the dialogue history, the achieved results for recognizing User Satisfaction on a 5 point scale showed to be hardly above chance. Work by Schmitt et al. (2011b) deals with determining User Satisfaction from ratings applied by the users themselves during the dialogues. A statistical classification model was trained using automatically derived interaction parameter to predict User Satisfaction for each system-user-exchange on a 5-point scale achieving an MR/R of 0.49. 3 Corpus The corpus used by Schmitt et al. (2011b) not only contains user ratings but also expert ratings which makes it a perfect candidate for our research presented in this paper. Adopting the terminology by Schmitt et al., user ratings are described as User Satisfaction (US) whereas expert ratings are referred to with the term Interaction Quality (IQ) (cf. (Schmitt et al., 2011a)). The data used for all experiments of this work was collected by Schmitt et al. (2011b) during a lab user study with 38 users in the domain of the “Let’s Go Bus Information” system (Raux et al., 2006) of the Carnegie Mellon University in Pittsburgh. 128 calls </context>
<context position="15735" citStr="Schmitt et al. (2011" startWordPosition="2559" endWordPosition="2562">rrelation Coefficient is a non-parametric method assuming a monotonic function between the rc . (1) 1 � |R.| i=1 |Rc| rc = 572 two variables (Spearman, 1904). It is defined by _ / Ei(xi − x)(yi − y) (5) ρ V Ei(xi − x)2 Pi(yi − 9)2 where xi and yi are corresponding ranked ratings and x� and y� the mean ranks. Thus, two sets of ratings can have total correlation even if they never agree. This would happen if all ratings are shifted by the same value, for example. 5 Recognition of US Using IQ Information As discussed in Section 1, automatic recognition of ratings applied by users as performed by Schmitt et al. (2011b) for User Satisfaction is time-consuming and expensive. Therefore, approaches are presented which facilitate expert ratings, i.e., Interaction Quality, with the hope of making US recognition more feasible. IQ an US are strongly related as both metrics represent the same quantity applied by different rater groups. Results of the Mann-Whitney U test, which is used to test for significant difference between Interaction Quality and User Satisfaction, show their difference (p &lt; 0.05) but values for Cohen’s Kappa (Cohen, 1960) and Spearman’s Rank Correlation Coefficient (Spearman, 1904) emphasize </context>
<context position="17253" citStr="Schmitt et al. (2011" startWordPosition="2806" endWordPosition="2809">t has been shown that user and expert ratings are similar, it is desirable nonetheless to being able to predict real user ratings. These ratings are the desired kind of ratings when it comes to subjective dialogue system assessment. Only users can give a rating about their satisfaction level, i.e., how they like the system and the interaction with the system. However, user ratings are expensive as elaborated in Section 1. Therefore, we investigate approaches to recognize US which rely on means of IQ recognition. 5.1 Belief-Based Sequential Recognition Methods used for IQ and US recognition by Schmitt et al. (2011b; 2011a) suffer from the fact that the sequential character of the data is modeled inadequately as they assume statistical independence between the single exchanges (recognition of IQ and US does not depend on the respective value of the previous exchange). Hence, we present a Markovian approach overcoming these issues. A probability distribution over all US states, called belief state, is updated after each system-user-exchange taking also into account the belief state of the previous exchange. This belief update2 is equivalent to the Forward Algorithm known from Hidden Markov Models (cf. (R</context>
<context position="21283" citStr="Schmitt et al. (2011" startWordPosition="3442" endWordPosition="3445">mance is influenced by the characteristics of the observation probability, i.e., the performance of the applied statistical classification approach and the variance of their confidence scores. In order to obtain different confidence characteristics, multiple classification algorithms, or algorithm variants respectively, are needed. Hence, five statistical classifiers have been chosen arbitrarily to produce the observation probabilities for Belief-Based Sequential Recognition: • SVM3 with cubic kernel • SVM with RBF-kernel • Naive Bayes • Naive Bayes with kernel • Rule Induction In contrast to Schmitt et al. (2011b; 2011a), a reduced feature set was used consisting of 43 parameters as some textual parameters were removed which are very specific and take many different values, e.g., UTTERANCE (the system utterance) or INTERPRETATION (the interpretation of the speech input). The resulting feature set consists of the following parameters (parameter names are in accordance with the parameter names of the LEGO corpus (Schmitt et al., 2012)): Exchange Level ACTIVITY, ACTIVITYTYPE, UTD, BARGED-IN?, ASRCONFIDENCE, MEANASRCONFIDENCE, TURNNUMBER, MODALITY, LOOPNAME, ASRRECOGNITIONSTATUS, ROLEINDEX, ROLENAME, NOI</context>
<context position="22548" citStr="Schmitt et al. (2011" startWordPosition="3601" endWordPosition="3604">ogue Level #BARGEINS #ASRSUCCESS, #HELPREQUESTS, #TIMEOUTS, #TIMEOUTS ASRREJECTIONS, #ASRREJECTIONS, #REPROMPTS, #SYSTEMQUESTIONS, #SYSTEMTURNS, #USERTURNS, %BARGEINS, %ASRSUCCESS, %HELPREQUESTS, %TIMEOUTS, %TIMEOUTS ASRREJECTIONS, %ASRREJECTIONS, %REPROMPTS Window Level {#}TIMEOUTS ASRREJCTIONS, {#}HELPREQUESTS, {#}ASRREJECTIONS, {MEAN}ASRCONFIDENCE, {#}TIMEOUTS, {#}REPROMPTS, {#}SYSTEMQUESTIONS, {#}ASRSUCCESS, {#}BARGEINS All results are evaluated with respect to the reference experiment of direct US recognition (US recognition using models trained on US). This is performed in accordance to Schmitt et al. (2011b) using the statistical classification algorithms stated 3Support Vector Machine, cf. (Vapnik, 1995) 574 Table 1: Results (UAR, Cohen’s Kappa, and Spearman’s Rho) of 10-fold cross-validation for US recognition of US recognition using models trained on US Table 3: Results (UAR, Cohen’s Kappa, and Spearman’s Rho) of 10-fold cross-validation for US recognition of action-independent Belief-Based Sequential Recognition Classifier UAR r. p SVM (cubic Kernel) 0.39 0.33 0.48 SVM (RBF-Kernel) 0.39 0.42 0.55 Naive Bayes 0.36 0.40 0.55 Naive Bayes (Kernel) 0.42 0.44 0.59 Rule Induction 0.50 0.51 0.61 Cl</context>
</contexts>
<marker>Schmitt, Schatz, Minker, 2011</marker>
<rawString>Alexander Schmitt, Benjamin Schatz, and Wolfgang Minker. 2011a. Modeling and predicting quality in spoken human-computer interaction. In Proceedings of the SIGDIAL 2011 Conference, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Schmitt</author>
<author>Benjamin Schatz</author>
<author>Wolfgang Minker</author>
</authors>
<title>A statistical approach for estimating user satisfaction in spoken human-machine interaction.</title>
<date>2011</date>
<booktitle>In Proceedings of the IEEE Jordan Conference on Applied Electrical Engineering and Computing Technologies (AEECT), Amman,</booktitle>
<publisher>IEEE.</publisher>
<location>Jordan,</location>
<contexts>
<context position="4248" citStr="Schmitt et al., 2011" startWordPosition="661" endWordPosition="664">heir expertise in the field of Human Computer Interaction or Spoken Dialogue Systems: They may be novices or have a high expertise. With experts or expert raters, we refer to people who are not participating in the dialogue thus constituting a completely different set of people. Expert raters listen to recorded dialogues after the interactions and rate them by assuming the point of view of the actual person performing the dialogue. These experts are supposed to have some experience with dialogue systems. In this work, expert raters were “advanced students of computer science and engineering” (Schmitt et al., 2011a). For User Satisfaction, ratings applied by the users seem to be clearly the better choice over ratings applied by third persons. However, determining true User Satisfaction is only possible by asking real users interacting with the system. Ideally, the ratings are applied by users talking to a system employed in the field, e.g., commercial systems, as these users have real concerns. For such Spoken Dialogue Systems, though, it is not easy to get users to apply quality ratings to the dialogue – especially for each system-userexchange. The users would have to rate either by pressing a button </context>
<context position="5566" citStr="Schmitt et al., 2011" startWordPosition="883" endWordPosition="886">nger dialogues imply longer call durations which cost money. Further, most callers only want to quickly get some information from the system. Therefore, it may be assumed that most users do not want to engage in dialogues which are artificially made longer. This also inhabits the risk that users who participated in long dialogues do not want to call again. Therefore, collecting ratings applied by users are considered to be expensive. One possible way of overcoming the problem of rating input would be to use some special installation which enables the users to provide ratings more easily (cf. (Schmitt et al., 2011b)). However, this is also expensive and the system’s usability would be very restricted. Further, this setup could most likely only be used in a lab situation. Expert raters, on the other hand, are able to simply listen to the recorded dialogues and to apply ratings, e.g., by using a specialized rating software. This process is much easier and does not require the same amount of effort needed for acquiring user ratings. Further, as already pointed out, we refer to experts as people who have some basic understanding of dialogue systems but are not required to be high-level experts in the field</context>
<context position="9698" citStr="Schmitt et al. (2011" startWordPosition="1545" endWordPosition="1548">tem). Ratings ranging from 1-7 were applied by two expert raters labeling “Smoothness”, “Closeness”, and “Willingness” not achieving a Match Rate per Rating (MR/R)1 of more than 0.2-0.24. This results are only slightly above the random baseline of 0.14. Further work by Higashinaka et al. (2010b) uses ratings for overall dialogues to predict ratings for each systemuser-exchange. Again, evaluating in three user satisfaction categories “Smoothness”, “Closeness”, and “Willingness” with ratings ranging from 1-7 achieved best performance of 0.19 MR/R. Interaction Quality (IQ) has been introduced by Schmitt et al. (2011a) as an alternative performance measure to User Satisfaction. In their terminology, US ratings are only applied by users. As their presented measure uses ratings applied by expert raters, a different term is used. Each system-user exchange was annotated by three different raters using strict guidelines. The ratings ranging from 1-5 are used as target variable for statistical classifiers using a set of automatically derivable interaction parameters as input. They achieve a MR/R of 0.58. 1MR/R is equal to Unweighted Average Recall (UAR) which is explained in Section 4. 2.2 User Ratings An appro</context>
<context position="11259" citStr="Schmitt et al. (2011" startWordPosition="1795" endWordPosition="1798">atings from an overall score applied by the users after the dialogue. Using n-gram models reflecting the dialogue history, the achieved results for recognizing User Satisfaction on a 5 point scale showed to be hardly above chance. Work by Schmitt et al. (2011b) deals with determining User Satisfaction from ratings applied by the users themselves during the dialogues. A statistical classification model was trained using automatically derived interaction parameter to predict User Satisfaction for each system-user-exchange on a 5-point scale achieving an MR/R of 0.49. 3 Corpus The corpus used by Schmitt et al. (2011b) not only contains user ratings but also expert ratings which makes it a perfect candidate for our research presented in this paper. Adopting the terminology by Schmitt et al., user ratings are described as User Satisfaction (US) whereas expert ratings are referred to with the term Interaction Quality (IQ) (cf. (Schmitt et al., 2011a)). The data used for all experiments of this work was collected by Schmitt et al. (2011b) during a lab user study with 38 users in the domain of the “Let’s Go Bus Information” system (Raux et al., 2006) of the Carnegie Mellon University in Pittsburgh. 128 calls </context>
<context position="15735" citStr="Schmitt et al. (2011" startWordPosition="2559" endWordPosition="2562">rrelation Coefficient is a non-parametric method assuming a monotonic function between the rc . (1) 1 � |R.| i=1 |Rc| rc = 572 two variables (Spearman, 1904). It is defined by _ / Ei(xi − x)(yi − y) (5) ρ V Ei(xi − x)2 Pi(yi − 9)2 where xi and yi are corresponding ranked ratings and x� and y� the mean ranks. Thus, two sets of ratings can have total correlation even if they never agree. This would happen if all ratings are shifted by the same value, for example. 5 Recognition of US Using IQ Information As discussed in Section 1, automatic recognition of ratings applied by users as performed by Schmitt et al. (2011b) for User Satisfaction is time-consuming and expensive. Therefore, approaches are presented which facilitate expert ratings, i.e., Interaction Quality, with the hope of making US recognition more feasible. IQ an US are strongly related as both metrics represent the same quantity applied by different rater groups. Results of the Mann-Whitney U test, which is used to test for significant difference between Interaction Quality and User Satisfaction, show their difference (p &lt; 0.05) but values for Cohen’s Kappa (Cohen, 1960) and Spearman’s Rank Correlation Coefficient (Spearman, 1904) emphasize </context>
<context position="17253" citStr="Schmitt et al. (2011" startWordPosition="2806" endWordPosition="2809">t has been shown that user and expert ratings are similar, it is desirable nonetheless to being able to predict real user ratings. These ratings are the desired kind of ratings when it comes to subjective dialogue system assessment. Only users can give a rating about their satisfaction level, i.e., how they like the system and the interaction with the system. However, user ratings are expensive as elaborated in Section 1. Therefore, we investigate approaches to recognize US which rely on means of IQ recognition. 5.1 Belief-Based Sequential Recognition Methods used for IQ and US recognition by Schmitt et al. (2011b; 2011a) suffer from the fact that the sequential character of the data is modeled inadequately as they assume statistical independence between the single exchanges (recognition of IQ and US does not depend on the respective value of the previous exchange). Hence, we present a Markovian approach overcoming these issues. A probability distribution over all US states, called belief state, is updated after each system-user-exchange taking also into account the belief state of the previous exchange. This belief update2 is equivalent to the Forward Algorithm known from Hidden Markov Models (cf. (R</context>
<context position="21283" citStr="Schmitt et al. (2011" startWordPosition="3442" endWordPosition="3445">mance is influenced by the characteristics of the observation probability, i.e., the performance of the applied statistical classification approach and the variance of their confidence scores. In order to obtain different confidence characteristics, multiple classification algorithms, or algorithm variants respectively, are needed. Hence, five statistical classifiers have been chosen arbitrarily to produce the observation probabilities for Belief-Based Sequential Recognition: • SVM3 with cubic kernel • SVM with RBF-kernel • Naive Bayes • Naive Bayes with kernel • Rule Induction In contrast to Schmitt et al. (2011b; 2011a), a reduced feature set was used consisting of 43 parameters as some textual parameters were removed which are very specific and take many different values, e.g., UTTERANCE (the system utterance) or INTERPRETATION (the interpretation of the speech input). The resulting feature set consists of the following parameters (parameter names are in accordance with the parameter names of the LEGO corpus (Schmitt et al., 2012)): Exchange Level ACTIVITY, ACTIVITYTYPE, UTD, BARGED-IN?, ASRCONFIDENCE, MEANASRCONFIDENCE, TURNNUMBER, MODALITY, LOOPNAME, ASRRECOGNITIONSTATUS, ROLEINDEX, ROLENAME, NOI</context>
<context position="22548" citStr="Schmitt et al. (2011" startWordPosition="3601" endWordPosition="3604">ogue Level #BARGEINS #ASRSUCCESS, #HELPREQUESTS, #TIMEOUTS, #TIMEOUTS ASRREJECTIONS, #ASRREJECTIONS, #REPROMPTS, #SYSTEMQUESTIONS, #SYSTEMTURNS, #USERTURNS, %BARGEINS, %ASRSUCCESS, %HELPREQUESTS, %TIMEOUTS, %TIMEOUTS ASRREJECTIONS, %ASRREJECTIONS, %REPROMPTS Window Level {#}TIMEOUTS ASRREJCTIONS, {#}HELPREQUESTS, {#}ASRREJECTIONS, {MEAN}ASRCONFIDENCE, {#}TIMEOUTS, {#}REPROMPTS, {#}SYSTEMQUESTIONS, {#}ASRSUCCESS, {#}BARGEINS All results are evaluated with respect to the reference experiment of direct US recognition (US recognition using models trained on US). This is performed in accordance to Schmitt et al. (2011b) using the statistical classification algorithms stated 3Support Vector Machine, cf. (Vapnik, 1995) 574 Table 1: Results (UAR, Cohen’s Kappa, and Spearman’s Rho) of 10-fold cross-validation for US recognition of US recognition using models trained on US Table 3: Results (UAR, Cohen’s Kappa, and Spearman’s Rho) of 10-fold cross-validation for US recognition of action-independent Belief-Based Sequential Recognition Classifier UAR r. p SVM (cubic Kernel) 0.39 0.33 0.48 SVM (RBF-Kernel) 0.39 0.42 0.55 Naive Bayes 0.36 0.40 0.55 Naive Bayes (Kernel) 0.42 0.44 0.59 Rule Induction 0.50 0.51 0.61 Cl</context>
</contexts>
<marker>Schmitt, Schatz, Minker, 2011</marker>
<rawString>Alexander Schmitt, Benjamin Schatz, and Wolfgang Minker. 2011b. A statistical approach for estimating user satisfaction in spoken human-machine interaction. In Proceedings of the IEEE Jordan Conference on Applied Electrical Engineering and Computing Technologies (AEECT), Amman, Jordan, December. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Schmitt</author>
<author>Stefan Ultes</author>
<author>Wolfgang Minker</author>
</authors>
<title>A parameterized and annotated corpus of the cmu let’s go bus information system.</title>
<date>2012</date>
<booktitle>In International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="21712" citStr="Schmitt et al., 2012" startWordPosition="3510" endWordPosition="3513">obabilities for Belief-Based Sequential Recognition: • SVM3 with cubic kernel • SVM with RBF-kernel • Naive Bayes • Naive Bayes with kernel • Rule Induction In contrast to Schmitt et al. (2011b; 2011a), a reduced feature set was used consisting of 43 parameters as some textual parameters were removed which are very specific and take many different values, e.g., UTTERANCE (the system utterance) or INTERPRETATION (the interpretation of the speech input). The resulting feature set consists of the following parameters (parameter names are in accordance with the parameter names of the LEGO corpus (Schmitt et al., 2012)): Exchange Level ACTIVITY, ACTIVITYTYPE, UTD, BARGED-IN?, ASRCONFIDENCE, MEANASRCONFIDENCE, TURNNUMBER, MODALITY, LOOPNAME, ASRRECOGNITIONSTATUS, ROLEINDEX, ROLENAME, NOISE?, HELPREQUEST?, REPROMPT?, WPST, WPUT Dialogue Level #BARGEINS #ASRSUCCESS, #HELPREQUESTS, #TIMEOUTS, #TIMEOUTS ASRREJECTIONS, #ASRREJECTIONS, #REPROMPTS, #SYSTEMQUESTIONS, #SYSTEMTURNS, #USERTURNS, %BARGEINS, %ASRSUCCESS, %HELPREQUESTS, %TIMEOUTS, %TIMEOUTS ASRREJECTIONS, %ASRREJECTIONS, %REPROMPTS Window Level {#}TIMEOUTS ASRREJCTIONS, {#}HELPREQUESTS, {#}ASRREJECTIONS, {MEAN}ASRCONFIDENCE, {#}TIMEOUTS, {#}REPROMPTS, {#}</context>
</contexts>
<marker>Schmitt, Ultes, Minker, 2012</marker>
<rawString>Alexander Schmitt, Stefan Ultes, and Wolfgang Minker. 2012. A parameterized and annotated corpus of the cmu let’s go bus information system. In International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Spearman</author>
</authors>
<title>The proof and measurement of association between two things.</title>
<date>1904</date>
<journal>American Journal of Psychology,</journal>
<pages>15--88</pages>
<contexts>
<context position="15272" citStr="Spearman, 1904" startWordPosition="2470" endWordPosition="2471">oduced reducing the discount of disagreements the smaller the difference is between two ratings (Cohen, 1968): |r1 − r2| w = (4) |rmax− rmin |. Here, r1 and r2 denote the rating pair and rmax and rmin the maximal and minimal rating. This results in w = 0 for agreement and w = 1 if the ratings have maximal difference. Spearman’s Rho The correlation of two variables describes the degree by that one variable can be expressed by the other. Spearman’s Rank Correlation Coefficient is a non-parametric method assuming a monotonic function between the rc . (1) 1 � |R.| i=1 |Rc| rc = 572 two variables (Spearman, 1904). It is defined by _ / Ei(xi − x)(yi − y) (5) ρ V Ei(xi − x)2 Pi(yi − 9)2 where xi and yi are corresponding ranked ratings and x� and y� the mean ranks. Thus, two sets of ratings can have total correlation even if they never agree. This would happen if all ratings are shifted by the same value, for example. 5 Recognition of US Using IQ Information As discussed in Section 1, automatic recognition of ratings applied by users as performed by Schmitt et al. (2011b) for User Satisfaction is time-consuming and expensive. Therefore, approaches are presented which facilitate expert ratings, i.e., Inte</context>
</contexts>
<marker>Spearman, 1904</marker>
<rawString>C. Spearman. 1904. The proof and measurement of association between two things. American Journal of Psychology, 15:88–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Ultes</author>
<author>Alexander Schmitt</author>
<author>Wolfgang Minker</author>
</authors>
<title>Towards quality-adaptive spoken dialogue management.</title>
<date>2012</date>
<booktitle>In NAACL-HLT Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data (SDCTD 2012),</booktitle>
<pages>49--52</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="2247" citStr="Ultes et al., 2012" startWordPosition="341" endWordPosition="344">ser Interfaces enabling speech communication of different complexity reaching from simple spoken commands up to complex dialogues. Besides the spoken words, the speech signal also may be used to acquire information about the user state, e.g., about their emotional state (cf., e.g., (Polzehl et al., 2011))). By additional analysis of the humancomputer-dialogues, even more abstract information may be derived, e.g., the quality of the system (cf., e.g., (Engelbrecht and M¨oller, 2010)). System quality information may be used to adapt the system’s behavior online during the ongoing dialogue (cf. (Ultes et al., 2012)). For determining the quality of Spoken Dialogue Systems, several aspects are of interest. M¨oller et al. (2009) presented a taxonomy of quality criteria. They describe quality as a bipartite issue consisting of Quality of Service (QoS) and Quality of Experience (QoE). Quality of Service describes objective criteria like dialogue duration or number of turns. While these are well-defined items that can be determined easily, Quality of Experience, which describes the user experience with subjective criteria, is more vague and without a sound definition, e.g., User Satisfaction (US). Subjective </context>
<context position="8591" citStr="Ultes et al., 2012" startWordPosition="1382" endWordPosition="1385">n quantitative parameters derived from the dialogue and US, modeling this dependency using linear re570 gression. Unfortunately, for generating the regression model, weighting factors have to be computed for each system anew. This generates high costs as dialogues have to be performed with real users where each user further has to complete a questionnaire after completing the dialogue. Moreover, in the PARADISE framework, only quality measurement for the whole dialogue (or system) is allowed. However, this is not suitable for using quality information for online adaption of the dialogue (cf. (Ultes et al., 2012)). Furthermore, PARADISE relies on questionnaires while we focus on work using singlevalued ratings. Numerous work on predicting User Satisfaction as a single-valued rating task for each system-userexchange has been performed in both categories. This work is briefly presented in the following. 2.1 Expert Ratings Higashinaka et al. (2010a) proposed a model to predict turn-wise ratings for human-human dialogues (transcribed conversation) and human-machine dialogues (text from chat system). Ratings ranging from 1-7 were applied by two expert raters labeling “Smoothness”, “Closeness”, and “Willing</context>
</contexts>
<marker>Ultes, Schmitt, Minker, 2012</marker>
<rawString>Stefan Ultes, Alexander Schmitt, and Wolfgang Minker. 2012. Towards quality-adaptive spoken dialogue management. In NAACL-HLT Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data (SDCTD 2012), pages 49–52, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The nature of statistical learning theory.</title>
<date>1995</date>
<publisher>Springer-Verlag</publisher>
<location>New York,</location>
<contexts>
<context position="22649" citStr="Vapnik, 1995" startWordPosition="3615" endWordPosition="3616">PTS, #SYSTEMQUESTIONS, #SYSTEMTURNS, #USERTURNS, %BARGEINS, %ASRSUCCESS, %HELPREQUESTS, %TIMEOUTS, %TIMEOUTS ASRREJECTIONS, %ASRREJECTIONS, %REPROMPTS Window Level {#}TIMEOUTS ASRREJCTIONS, {#}HELPREQUESTS, {#}ASRREJECTIONS, {MEAN}ASRCONFIDENCE, {#}TIMEOUTS, {#}REPROMPTS, {#}SYSTEMQUESTIONS, {#}ASRSUCCESS, {#}BARGEINS All results are evaluated with respect to the reference experiment of direct US recognition (US recognition using models trained on US). This is performed in accordance to Schmitt et al. (2011b) using the statistical classification algorithms stated 3Support Vector Machine, cf. (Vapnik, 1995) 574 Table 1: Results (UAR, Cohen’s Kappa, and Spearman’s Rho) of 10-fold cross-validation for US recognition of US recognition using models trained on US Table 3: Results (UAR, Cohen’s Kappa, and Spearman’s Rho) of 10-fold cross-validation for US recognition of action-independent Belief-Based Sequential Recognition Classifier UAR r. p SVM (cubic Kernel) 0.39 0.33 0.48 SVM (RBF-Kernel) 0.39 0.42 0.55 Naive Bayes 0.36 0.40 0.55 Naive Bayes (Kernel) 0.42 0.44 0.59 Rule Induction 0.50 0.51 0.61 Classifier UAR r. p SVM (cubic Kernel) 0.28 0.36 0.48 SVM (RBF-Kernel) 0.30 0.40 0.54 Naive Bayes 0.32 </context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The nature of statistical learning theory. Springer-Verlag New York, Inc., New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Diane Litman</author>
<author>Candace A Kamm</author>
<author>Alicia Abella</author>
</authors>
<title>Paradise: a framework for evaluating spoken dialogue agents.</title>
<date>1997</date>
<booktitle>In Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics,</booktitle>
<pages>271--280</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="7925" citStr="Walker et al. (1997)" startWordPosition="1277" endWordPosition="1280">ows. First, we give a brief overview of work done in both categories (user ratings vs. expert ratings) in Section 2 and present our choice of data the analysis in this paper is based on in Section 3. Further, evaluation metrics are illustrated in Section 4 and approaches on facilitating prediction of user rater scores by expert rater information are presented in Section 5 followed by an evaluation and discussion of the results in Section 6. 2 Significant Related Work Predicting User Satisfaction for SDSs has been in the focus of research for many years, most famously the PARADISE framework by Walker et al. (1997). The authors assume a linear dependency between quantitative parameters derived from the dialogue and US, modeling this dependency using linear re570 gression. Unfortunately, for generating the regression model, weighting factors have to be computed for each system anew. This generates high costs as dialogues have to be performed with real users where each user further has to complete a questionnaire after completing the dialogue. Moreover, in the PARADISE framework, only quality measurement for the whole dialogue (or system) is allowed. However, this is not suitable for using quality informa</context>
</contexts>
<marker>Walker, Litman, Kamm, Abella, 1997</marker>
<rawString>Marilyn Walker, Diane Litman, Candace A. Kamm, and Alicia Abella. 1997. Paradise: a framework for evaluating spoken dialogue agents. In Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics, pages 271–280, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>