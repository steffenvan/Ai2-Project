<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000057">
<title confidence="0.967732">
Manual Annotation of Opinion Categories in Meetings
</title>
<author confidence="0.999542">
Swapna Somasundaran1, Janyce Wiebe1, Paul Hoffmann2, Diane Litman1
</author>
<affiliation confidence="0.964903">
1Department of Computer Science, University of Pittsburgh, Pittsburgh, PA 15260
2Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA 15260
</affiliation>
<email confidence="0.999423">
{swapna,wiebe,hoffmanp,litman}@cs.pitt.edu
</email>
<sectionHeader confidence="0.99565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996452631579">
This paper applies the categories from an
opinion annotation scheme developed for
monologue text to the genre of multiparty
meetings. We describe modifications to
the coding guidelines that were required
to extend the categories to the new type
of data, and present the results of an in-
ter-annotator agreement study. As re-
searchers have found with other types of
annotations in speech data, inter-
annotator agreement is higher when the
annotators both read and listen to the data
than when they only read the transcripts.
Previous work exploited prosodic clues
to perform automatic detection of speaker
emotion (Liscombe et al. 2003). Our
findings suggest that doing so to recog-
nize opinion categories would be a prom-
ising line of work.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999407620689655">
Subjectivity refers to aspects of language that
express opinions, beliefs, evaluations and specu-
lations (Wiebe et al. 2005). Many natural lan-
guage processing applications could benefit from
being able to distinguish between facts and opin-
ions of various types, including speech-oriented
applications such as meeting browsers, meeting
summarizers, and speech-oriented question an-
swering (QA) systems. Meeting browsers could
find instances in meetings where opinions about
key topics are expressed. Summarizers could in-
clude strong arguments for and against issues, to
make the final outcome of the meeting more un-
derstandable. A preliminary user survey
(Lisowska 2003) showed that users would like to
be able to query meeting records with subjective
questions like “Show me the conflicts of opin-
ions between X and Y” , “Who made the highest
number of positive/negative comments” and
“Give me all the contributions of participant X in
favor of alternative A regarding the issue I.” A
QA system with a component to recognize opin-
ions would be able to help find answers to such
questions.
Consider the following example from a meet-
ing about an investment firm choosing which car
to buy1. (In the examples, the words and phrases
describing or expressing the opinion are under-
lined):
</bodyText>
<listItem confidence="0.87966875">
(1)2 OCK: Revenues of less
than a million and losses of
like five million you know
that&apos;s pathetic
</listItem>
<bodyText confidence="0.999013666666667">
Here, the speaker, OCK, shows his strong nega-
tive evaluation by using the expression “That’s
pathetic.”
</bodyText>
<listItem confidence="0.989841">
(2) OCK: No it might just be
a piece of junk cheap piece
of junk that&apos;s not a good
investment
</listItem>
<bodyText confidence="0.99926">
In (2), the speaker uses the term “just a piece of
junk” to express his negative evaluation and uses
this to argue for his belief that it is “not a good
investment.”
</bodyText>
<listItem confidence="0.9054858">
(3) OCK: Yeah I think that&apos;s
the wrong image for an in-
vestment bank he wants sta-
bility and s safety and you
don&apos;t want flashy like zip-
</listItem>
<footnote confidence="0.895319125">
1 Throughout this paper we take examples from a meeting
where a group of people are deciding on a new car for an
investment bank. The management wants to attract younger
investors with a sporty car.
2 We have presented the examples the way they were ut-
tered by the speaker. Hence they may show many false
starts and repetitions. Capitalization was added to improve
readability.
</footnote>
<page confidence="0.976414">
54
</page>
<note confidence="0.6997715">
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 54–61,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999586148148148">
ping around the corner kind
of thing you know
The example above shows that the speaker has a
negative judgment towards the suggestion of a
sports car (that was made in the previous turn)
which is indicated by the words “wrong image.”
The speaker then goes on to positively argue for
what he wants. He further argues against the cur-
rent suggestion by using more negative terms
like “flashy” and “zipping around the corner.”
The speaker believes that “zipping around the
corner” is bad as it would give a wrong impres-
sion of the bank to the customers. In the absence
of such analyses, the decision making process
and rationale behind the outcomes of meetings,
which form an important part of the organiza-
tion’s memory, might remain unavailable.
In this paper, we perform annotation of a
meeting corpus to lay the foundation for research
on opinion detection in speech. We show how
categories from an opinion (subjectivity) annota-
tion scheme, which was developed for news arti-
cles, can be applied to the genre of multi-party
meetings. The new genre poses challenges as it is
significantly different from the text domain,
where opinion analysis has traditionally been
applied. Specifically, differences arise because:
</bodyText>
<listItem confidence="0.96879">
1) There are many participants interacting with
one another, each expressing his or her own
opinion, and eliciting reactions in the process.
2) Social interactions may constrain how openly
people express their opinions; i.e., they are often
indirect in their negative evaluations.
</listItem>
<bodyText confidence="0.999274">
We also explore the influence of speech on hu-
man perception of opinions.
Specifically, we annotated some meeting data
with the opinion categories Sentiment and Argu-
ing as defined in Wilson and Wiebe (2005). In
our annotation we first distinguish whether a
Sentiment or Arguing is being expressed. If one
is, we then mark the polarity (i.e., positive or
negative) and the intensity (i.e., how strong the
opinion is). Annotating the individual opinion
expressions is useful in this genre, because we
see many utterances that have more than one
type of opinion (e.g. (3) above). To investigate
how opinions are expressed in speech, we divide
our annotation into two tasks, one in which the
annotator only reads the raw text, and the other
in which the annotator reads the raw text and also
listens to the speech. We measure inter-annotator
agreement for both tasks.
We found that the opinion categories apply
well to the multi-party meeting data, although
there is some room for improvement: the Kappa
values range from 0.32 to 0.69. As has been
found for other types of annotations in speech,
agreement is higher when the annotators both
read and listen to the data than when they only
read the transcripts. Interestingly, the advantages
are more dramatic for some categories than oth-
ers. And, in both conditions, agreement is higher
for the positive than for the negative categories.
We discuss possible reasons for these disparities.
Prosodic clues have been exploited to perform
automatic detection of speaker emotion (Lis-
combe et al. 2003). Our findings suggest that
doing so to recognize opinion categories is a
promising line of work.
The rest of the paper is organized as follows:
In Section 2 we discuss the data and the annota-
tion scheme and present examples. We then pre-
sent our inter-annotator agreement results in Sec-
tion 3, and in Section 4 we discuss issues and
observations. Related work is described in Sec-
tion 5. Conclusions and Future Work are pre-
sented in Section 6.
</bodyText>
<sectionHeader confidence="0.971704" genericHeader="introduction">
2 Annotation
</sectionHeader>
<subsectionHeader confidence="0.934925">
2.1 Data
</subsectionHeader>
<bodyText confidence="0.999989722222222">
The data is from the ISL meeting corpus (Bur-
ger et al. 2002). We chose task oriented meet-
ings from the games/scenario and discussion
genres, as we felt they would be closest to the
applications for which the opinion analysis will
be useful. The ISL speech is accompanied by
rich transcriptions, which are tagged according to
VERBMOBIL conventions. However, since real-
time applications only have access to ASR out-
put, we gave the annotators raw text, from which
all VERBMOBIL tags, punctuation, and capitali-
zations were removed.
In order to see how annotations would be af-
fected by the presence or absence of speech, we
divided each raw text document into 2 segments.
One part was annotated while reading the raw
text only. For the annotation of the other part,
speech as well as the raw text was provided.
</bodyText>
<subsectionHeader confidence="0.996847">
2.2 Opinion Category Definitions
</subsectionHeader>
<bodyText confidence="0.999822888888889">
We base our annotation definitions on the
scheme developed by Wiebe et al. (2005) for
news articles. That scheme centers on the notion
of subjectivity, the linguistic expression of pri-
vate states. Private states are internal mental
states that cannot be objectively observed or veri-
fied (Quirk et al. 1985) and include opinions,
beliefs, judgments, evaluations, thoughts, and
feelings. Amongst these many forms of subjec-
</bodyText>
<page confidence="0.997493">
55
</page>
<bodyText confidence="0.9991125">
tivity, we focus on the Sentiment and Arguing
categories proposed by Wilson and Wiebe
(2005). The categories are broken down by po-
larity and defined as follows:
</bodyText>
<listItem confidence="0.8060846">
Positive Sentiments: positive emotions,
evaluations, judgments and stances.
(4) TBC: Well ca How about
one of the the newer Cadil-
lac the Lexus is good
</listItem>
<bodyText confidence="0.8287832">
In (4), taken from the discussion of which car to
buy, the speaker uses the term “good” to express
his positive evaluation of the Lexus .
Negative Sentiments: negative emotions,
evaluations, judgments and stances.
</bodyText>
<listItem confidence="0.9943945">
(5) OCK: I think these are
all really bad choices
</listItem>
<bodyText confidence="0.9899285">
In (5), the speaker expresses his negative evalua-
tion of the choices for the company car. Note that
“really” makes the evaluation more intense.
Positive Arguing: arguing for something, ar-
guing that something is true or is so, arguing that
something did happen or will happen, etc.
</bodyText>
<listItem confidence="0.841364230769231">
(6) ZDN: Yeah definitely
moon roof
In (6), the speaker is arguing that whatever car
they get should have a moon roof.
Negative Arguing: arguing against some-
thing, arguing that something is not true or is not
so, arguing that something did not happen or will
not happen, etc.
(7) OCK: Like a Lexus or
perhaps a Stretch Lexus
something like that but that
might be too a little too
luxurious
</listItem>
<bodyText confidence="0.999522">
In the above example, the speaker is using the
term “a little too luxurious” to argue against a
Lexus for the car choice.
In an initial tagging experiment, we applied
the above definitions, without modification, to
some sample meeting data. The definitions cov-
ered much of the arguing and sentiment we ob-
served. However, we felt that some cases of Ar-
guing that are more prevalent in meeting than in
news data needed to be highlighted more, namely
Arguing opinions that are implicit or that under-
lie what is explicitly said. Thus we add the fol-
lowing to the arguing definitions.
Positive Arguing: expressing support for or
backing the acceptance of an object, viewpoint,
idea or stance by providing reasoning, justifica-
tions, judgment, evaluations or beliefs. This sup-
port or backing may be explicit or implicit.
</bodyText>
<listItem confidence="0.9958435">
(8) MHJ: That&apos;s That&apos;s why I
wanna What about the the
</listItem>
<bodyText confidence="0.954808">
child safety locks I think I
think that would be a good
thing because if our custom-
ers happen to have children
Example (8) is marked as both Positive Arguing
and Positive Sentiment. The more explicit one is
the Positive Sentiment that the locks are good.
The underlying Argument is that the company
car they choose should have child safety locks.
Negative Arguing: expressing lack of support
for or attacking the acceptance of an object,
viewpoint, idea or stance by providing reasoning,
justifications, judgment, evaluations or beliefs.
This may be explicit or implicit.
</bodyText>
<listItem confidence="0.9906995">
(9) OCK: Town Car But it&apos;s a
little a It&apos;s a little like
</listItem>
<bodyText confidence="0.910466833333333">
your grandf Yeah your grand-
father would drive that
Example (9) is explicitly stating who would drive
a Town Car, while implicitly arguing against
choosing the Town Car (as they want younger
investors).
</bodyText>
<subsectionHeader confidence="0.999706">
2.3 Annotation Guidelines
</subsectionHeader>
<bodyText confidence="0.999853333333333">
Due to genre differences, we also needed to
modify the annotation guidelines. For each Argu-
ing or Sentiment the annotator perceives, he or
she identifies the words or phrases used to ex-
press it (the text span), and then creates an anno-
tation consisting of the following.
</bodyText>
<listItem confidence="0.999930333333333">
• Opinion Category and Polarity
• Opinion Intensity
• Annotator Certainty
</listItem>
<bodyText confidence="0.99891415">
Opinion Category and Polarity: These are
defined in the previous sub-section. Note that the
target of an opinion is what the opinion is about.
For example, the target of “John loves baseball”
is baseball. An opinion may or may not have a
separate target. For example, “want stability” in
“We want stability” denotes a Positive Senti-
ment, and there is no separate target. In contrast,
“good” in “The Lexus is good” expresses a Posi-
tive Sentiment and there is a separate target,
namely the Lexus.
In addition to Sentiments toward a topic of
discussion, we also mark Sentiments toward
other team members (e.g. “Man you guys
are so limited”). We do not mark
agreements or disagreements as Sentiments, as
these are different dialog acts (though they some-
times co-occur with Sentiments and Arguing).
Intensity: We use a slightly modified version
of Craggs and Wood&apos;s (2004) emotion intensity
</bodyText>
<page confidence="0.980623">
56
</page>
<bodyText confidence="0.999954766666667">
annotation scheme. According to that scheme,
there are 5 levels of intensity. Level “0” denotes
a lack of the emotion (Sentiment or Arguing in
our case), “1” denotes traces of emotion, “2” de-
notes a low level of emotion, “3” denotes a clear
expression while “4” denotes a strong expres-
sion. Our intensity levels mean the same, but we
do not mark intensity level 0 as this level implies
the absence of opinion.
If a turn has multiple, separate expressions
marked with the same opinion tag (category and
polarity), and all expressions refer to the same
target, then the annotators merge all the expres-
sions into a larger text span, including the sepa-
rating text in between the expressions. This re-
sulting text span has the same opinion tag as its
constituents, and it has an intensity that is greater
than or equal to the highest intensity of the con-
stituent expressions that were merged.
Annotator Certainty: The annotators use this
tag if they are not sure that a given opinion is
present, or if, given the context, there are multi-
ple possible interpretations of the utterance and
the annotator is not sure which interpretation is
correct. This attribute is distinct from the Inten-
sity attribute, because the Intensity attribute indi-
cates the strength of the opinion, while the Anno-
tator Certainty attribute indicates whether the
annotator is sure about a given tag (whatever the
intensity is).
</bodyText>
<subsectionHeader confidence="0.945112">
2.4 Examples
</subsectionHeader>
<bodyText confidence="0.998538">
We conclude this section with some examples
of annotations from our corpus.
</bodyText>
<listItem confidence="0.9917115">
(10) OCK: So Lexun had reve-
nues of a hundred and fifty
million last year and prof-
its of like six million.
</listItem>
<table confidence="0.429989333333333">
That&apos;s pretty good
Annotation: Text span=That&apos;s
pretty good Cate-
gory=Positive Sentiment In-
tensity=3 Annotator Cer-
tainty=Certain
</table>
<bodyText confidence="0.998419833333334">
The annotator marked the text span “That’s
pretty good” as Positive Sentiment because this
this expression is used by OCK to show his fa-
vorable judgment towards the company reve-
nues. The intensity is 3, as it is a clear expression
of Sentiment.
</bodyText>
<listItem confidence="0.997539">
(11) OCK: No it might just
be a piece of junk Cheap
piece of junk that’s not a
good investment
</listItem>
<figure confidence="0.613335769230769">
Annotation1: Text span=it
might just be a piece of
junk Cheap piece of junk
that’s not a good investment
Category=Negative Sentiment
Intensity=4 Annotator Cer-
tainty=Certain
Annotation2: Text span=Cheap
piece of junk that’s not a
good investment Category
=Negative Arguing Inten-
sity=3 Annotator Certainty
=Certain
</figure>
<bodyText confidence="0.999944875">
In the above example, there are multiple expres-
sions of opinions. In Annotation1, the expres-
sions “it might just be a piece of junk”, “cheap
piece of junk” and “not a good investment” ex-
press negative evaluations towards the car choice
(suggested by another participant in a previous
turn). Each of these expressions is a clear case of
Negative Sentiment (Intensity=3). As they are all
of the same category and polarity and towards
the same target, they have been merged by the
annotator into one long expression of Inten-
sity=4. In Annotation2, the sub-expression
“cheap piece of junk that is not a good invest-
ment” is also used by the speaker OCK to argue
against the car choice. Hence the annotator has
marked this as Negative Arguing.
</bodyText>
<sectionHeader confidence="0.9922935" genericHeader="method">
3 Guideline Development and Inter-
Annotator Agreement
</sectionHeader>
<subsectionHeader confidence="0.999763">
3.1 Annotator Training
</subsectionHeader>
<bodyText confidence="0.999983857142857">
Two annotators (both co-authors) underwent
three rounds of tagging. After each round, dis-
crepancies were discussed, and the guidelines
were modified to reflect the resolved ambiguities.
A total of 1266 utterances belonging to sections
of four meetings (two of the discussion genre and
two of the game genre) were used in this phase.
</bodyText>
<subsectionHeader confidence="0.998099">
3.2 Agreement
</subsectionHeader>
<bodyText confidence="0.99990825">
The unit for which agreement was calculated
was the turn. The ISL transcript provides demar-
cation of speaker turns along with the speaker ID.
If an expression is marked in a turn, the turn is
assigned the label of that expression. If there are
multiple expressions marked within a turn with
different category tags, the turn is assigned all
those categories. This does not pose a problem
for our evaluation, as we evaluate each category
separately.
A previously unseen section of a meeting con-
taining 639 utterances was selected and divided
</bodyText>
<page confidence="0.992915">
57
</page>
<bodyText confidence="0.999959636363636">
into 2 segments. One part of 319 utterances was
annotated using raw text as the only signal, and
the remaining 320 utterances were annotated us-
ing text and speech. Cohen’s Kappa (1960) was
used to calculate inter-annotator agreement. We
calculated inter-annotator agreement for both
conditions: raw-text-only and raw-text+speech.
This was done for each of the categories: Posi-
tive Sentiment, Positive Arguing, Negative Sen-
timent, and Negative Arguing. To evaluate a
category, we did the following:
</bodyText>
<listItem confidence="0.9075464">
• For each turn, if both annotators tagged
the turn with the given category, or both
did not tag the turn with the category, then
it is a match.
• Otherwise it is a mismatch
</listItem>
<tableCaption confidence="0.9079295">
Table 1 shows the inter-annotator Kappa val-
ues on the test set.
</tableCaption>
<table confidence="0.999874666666667">
Agreement (Kappa) Raw Text Raw Text
only + Speech
Positive Arguing 0.54 0.60
Negative Arguing 0.32 0.65
Positive Sentiment 0.57 0.69
Negative Sentiment 0.41 0.61
</table>
<tableCaption confidence="0.850978">
Table 1 Inter-annotator agreement on different
categories.
</tableCaption>
<bodyText confidence="0.999810619047619">
With raw-text-only annotation, the Kappa
value is in the moderate range according to
Landis and Koch (1977), except for Negative
Arguing for which it is 0.32. Positive Arguing
and Positive Sentiment were more reliably de-
tected than Negative Arguing and Negative Sen-
timent. We believe this is because participants
were more comfortable with directly expressing
their positive sentiments in front of other partici-
pants. Given only the raw text data, inter-
annotator reliability measures for Negative Argu-
ing and Negative Sentiment are the lowest. We
believe this might be due to the fact that partici-
pants in social interactions are not very forthright
with their Negative Sentiments and Arguing.
Negative Sentiments and Arguing towards some-
thing may be expressed by saying that something
else is better. For example, consider the follow-
ing response of one participant to another par-
ticipant’s suggestion of aluminum wheels for the
company car
</bodyText>
<listItem confidence="0.899802">
(12) ZDN: Yeah see what kind
</listItem>
<bodyText confidence="0.998818688888889">
of wheels you know they have
to look dignified to go with
the car
The above example was marked as Negative Ar-
guing by one annotator (i.e., they should not get
aluminum wheels) while the other annotator did
not mark it at all. The implied Negative Arguing
toward getting aluminum wheels can be inferred
from the statement that the wheels should look
dignified. However the annotators were not sure,
as the participant chose to focus on what is desir-
able (i.e., dignified wheels). This utterance is
actually both a general statement of what is de-
sirable, and an implication that aluminum wheels
are not dignified. But this may be difficult to as-
certain with the raw text signal only.
When the annotators had speech to guide their
judgments, the Kappa values go up significantly
for each category. All the agreement numbers for
raw text+speech are in the substantial range ac-
cording to Landis and Koch (1977). We observe
that with speech, Kappa for Negative Arguing
has doubled over the Kappa obtained without
speech. The Kappa for Negative Sentiment
(text+speech) shows a 1.5 times improvement
over the one with only raw text. Both these ob-
servations indicate that speech is able to help the
annotators tag negativity more reliably. It is quite
likely that a seemingly neutral sentence could
sound negative, depending on the way words are
stressed or pauses are inserted. Comparing the
agreement on Positive Sentiment, we get a 1.2
times improvement by using speech. Similarly,
agreement improves by 1.1 times for Positive
Arguing when speech is used. The improvement
with speech for the Positive categories is not as
high as compared to negative categories, which
conforms to our belief that people are more
forthcoming about their positive judgments,
evaluations, and beliefs.
In order to test if the turns where annotators
were uncertain were the places that caused mis-
match, we calculated the Kappa with the annota-
tor-uncertain cases removed. The corresponding
Kappa values are shown in Table 2
</bodyText>
<table confidence="0.998339833333333">
Agreement ( Kappa) Raw Text Raw Text
only + Speech
Positive Arguing 0.52 0.63
Negative Arguing 0.36 0.63
Positive Sentiment 0.60 0.73
Negative Sentiment 0.50 0.61
</table>
<tableCaption confidence="0.675877">
Table-2 Inter-annotator agreement on different
categories, Annotator Uncertain cases removed.
</tableCaption>
<bodyText confidence="0.972357">
The trends observed in Table 1 are seen in Ta-
ble 2 as well, namely annotation reliability im-
proving with speech. Comparing Tables 1 and 2,
</bodyText>
<page confidence="0.99604">
58
</page>
<bodyText confidence="0.9997334">
we see that for the raw text, the inter-annotator
agreement goes up by 0.04 points for Negative
Arguing and goes up by 0.09 points for Negative
Sentiment. However, the agreement for Negative
Arguing and Negative Sentiment on raw-text+
speech between Tables 1 and 2 remains almost
the same. We believe this is because we had
20% fewer Annotator Uncertainty tags in the
raw-text+speech annotation as compared to raw-
text-only, thus indicating that some types of un-
certainties seen in raw-text-only were resolved in
the raw-text+speech due to the speech input. The
remaining cases of Annotator Uncertainty could
have been due to other factors, as discussed in
the next section
Table 3 shows Kappa with the low intensity
tags removed. The hypothesis was that low in-
tensity might be borderline cases, and that re-
moving these might increase inter-annotator reli-
ability.
</bodyText>
<table confidence="0.999120833333333">
Agreement ( Kappa) Raw Text Raw Text
only + Speech
Positive Arguing 0.53 0.66
Negative Arguing 0.26 0.65
Positive Sentiment 0.65 0.74
Negative Sentiment 0.45 0.59
</table>
<bodyText confidence="0.9615206">
Table-3 Inter-annotator agreement on different
categories, Intensity 1, 2 removed.
Comparing Tables 1 and 3 (the raw-text col-
umns), we see that there is an improvement in
the agreement on sentiment (both positive and
negative) if the low intensity cases are removed.
The agreement for Negative Sentiment (raw-text)
goes up marginally by 0.04 points. Surprisingly,
the agreement for Negative Arguing (raw-text)
goes down by 0.06 points. Similarly in raw-
text+speech results, removal of low intensity
cases does not improve the agreement for Nega-
tive Arguing while hurting Negative Sentiment
category (by 0.02 points). One possible explana-
tion is that it may be equally difficult to detect
Negative categories at both low and high intensi-
ties. Recall that in (12) it was difficult to detect if
there is Negative Arguing at all. If the annotator
decided that it is indeed a Negative Arguing, it is
put at intensity level=3 (i.e., a clear case).
</bodyText>
<sectionHeader confidence="0.999117" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.9998866875">
There were a number of interesting subjectiv-
ity related phenomena in meetings that we ob-
served during our annotation. These are issues
that will need to be addressed for improving in-
ter-annotator reliability.
Global and local context for arguing: In the
context of a meeting, participants argue for (posi-
tively) or against (negatively) a topic. This may
become ambiguous when the participant uses an
explicit local Positive Arguing and an implicit
global Negative Arguing. Consider the following
speaker turn, at a point in the meeting when one
participant has suggested that the company car
should have a moon roof and another participant
has opposed it, by saying that a moon roof would
compromise the headroom.
</bodyText>
<listItem confidence="0.918407">
(13) OCK: We wanna make sure
there&apos;s adequate headroom
</listItem>
<bodyText confidence="0.945746470588235">
for all those six foot six
investors
In the above example, the speaker OCK, in the
local context of the turn, is arguing positively
that headroom is important. However, in the
global context of the meeting, he is arguing
against the idea of a moon roof that was sug-
gested by a participant. Such cases occur when
one object (or opinion) is endorsed which auto-
matically precludes another, mutually exclusive
object (or opinion).
Sarcasm/Humor: The meetings we analyzed
had a large amount of sarcasm and humor. Issues
arose with sarcasm due to our approach of mark-
ing opinions towards the content of the meeting
(which forms the target of the opinion). Sarcasm
is difficult to annotate because sarcasm can be
</bodyText>
<listItem confidence="0.986734181818182">
1) On topic: Here the target is the topic of dis-
cussion and hence sarcasm is used as a Negative
Sentiment.
2) Off topic: Here the target is not a topic un-
der discussion, and the aim is to purely elicit
laughter.
3) Allied topic: In this case, the target is re-
lated to the topic in some way, and it’s difficult
to determine if the aim of the sarcasm/humor was
to elicit laughter or to imply something negative
towards the topic.
</listItem>
<bodyText confidence="0.950795818181818">
Multiple modalities: In addition to text and
speech, gestures and visual diagrams play an im-
portant role in some types of meetings. In one
meeting that we analyzed, participants were
working together to figure out how to protect an
egg when it is dropped from a long distance,
given the materials they have. It was evident they
were using some gestures to describe their ideas
(“we can put tape like this”) and that they drew
diagrams to get points across. In the absence of
visual input, annotators would need to guess
</bodyText>
<page confidence="0.996061">
59
</page>
<bodyText confidence="0.999184">
what was happening. This might further hurt the
inter-annotator reliability.
</bodyText>
<sectionHeader confidence="0.999898" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999972607594937">
Our opinion categories are from the subjectiv-
ity schemes described in Wiebe et al. (2005) and
Wilson and Wiebe (2005). Wiebe et al. (2005)
perform expression level annotation of opinions
and subjectivity in text. They define their annota-
tions as an experiencer having some type of atti-
tude (such as Sentiment or Arguing), of a certain
intensity, towards a target. Wilson and Wiebe
(2005) extend this basic annotation scheme to
include different types of subjectivity, including
Positive Sentiment, Negative Sentiment, Positive
Arguing, and Negative Arguing.
Speech was found to improve inter-annotator
agreement in discourse segmentation of mono-
logs (Hirschberg and Nakatani 1996). Acoustic
clues have been successfully employed for the
reliable detection of the speaker’s emotions, in-
cluding frustration, annoyance, anger, happiness,
sadness, and boredom (Liscombe et al. 2003).
Devillers et al. (2003) performed perceptual tests
with and without speech in detecting the
speaker’s fear, anger, satisfaction and embar-
rassment. Though related, our work is not con-
cerned with the speaker’s emotions, but rather
opinions toward the issues and topics addressed
in the meeting.
Most annotation work in multiparty conversa-
tion has focused on exchange structures and dis-
course functional units like common grounding
(Nakatani and Traum, 1998). In common ground-
ing research, the focus is on whether the partici-
pants of the discourse are able to understand each
other, and not their opinions towards the content
of the discourse. Other tagging schemes like the
one proposed by Flammia and Zue (1997) focus
on information seeking and question answering
exchanges where one participant is purely seek-
ing information, while the other is providing it.
The SWBD DAMSL (Jurafsky et al., 1997) an-
notation scheme over the Switchboard telephonic
conversation corpus labels shallow discourse
structures. The SWBD-DAMSL had a label “sv”
for opinions. However, due to poor inter-
annotator agreement, the authors discarded these
annotations. The ICSI MRDA annotation scheme
(Rajdip et al., 2003) adopts the SWBD DAMSL
scheme, but does not distinguish between the
opinionated and objective statements. The ISL
meeting corpus (Burger and Sloane, 2004) is an-
notated with dialog acts and discourse moves like
initiation and response, which in turn consist of
dialog tags such as query, align, and statement.
Their statement dialog category would not only
include Sentiment and Arguing tags discussed in
this paper, but it would also include objective
statements and other types of subjectivity.
“Hot spots” in meetings closely relate to our
work because they find sections in the meeting
where participants are involved in debates or
high arousal activity (Wrede and Shriberg 2003).
While that work distinguishes between high
arousal and low arousal, it does not distinguish
between opinion or non-opinion or the different
types of opinion. However, Janin et al. (2004)
suggest that there is a relationship between dia-
log acts and involvement, and that involved ut-
terances contain significantly more evaluative
and subjective statements as well as extremely
positive or negative answers. Thus we believe it
may be beneficial for such works to make these
distinctions.
Another closely related work that finds par-
ticipants’ positions regarding issues is argument
diagramming (Rienks et al. 2005). This ap-
proach, based on the IBIS system (Kunz and Rit-
tel 1970), divides a discourse into issues, and
finds lines of deliberated arguments. However
they do not distinguish between subjective and
objective contributions towards the meeting.
</bodyText>
<sectionHeader confidence="0.998742" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999756782608696">
In this paper we performed an annotation
study of opinions in meetings, and investigated
the effects of speech. We have shown that it is
possible to reliably detect opinions within multi-
party conversations. Our consistently better
agreement results with text+speech input over
text-only input suggest that speech is a reliable
indicator of opinions. We have also found that
Annotator Uncertainty decreased with speech
input. Our results also show that speech is a more
informative indicator for negative versus positive
categories. We hypothesize that this is due to the
fact the people express their positive attitudes
more explicitly. The speech signal is thus even
more important for discerning negative opinions.
This experience has also helped us gain insights
to the ambiguities that arise due to sarcasm and
humor.
Our promising results open many new avenues
for research. It will be interesting to see how our
categories relate to other discourse structures,
both at the shallow level (agree-
ment/disagreement) as well as at the deeper level
</bodyText>
<page confidence="0.99372">
60
</page>
<bodyText confidence="0.999968125">
(intentions/goals). It will also be interesting to
investigate how other forms of subjectivity like
speculation and intention are expressed in multi-
party discourse. Finding prosodic correlates of
speech as well as lexical clues that help in opin-
ion detection would be useful in building subjec-
tivity detection applications for multiparty meet-
ings.
</bodyText>
<sectionHeader confidence="0.999189" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999839686046512">
Susanne Burger and Zachary A Sloane. 2004. The ISL
Meeting Corpus: Categorical Features of Commu-
nicative Group Interactions. NIST Meeting Recog-
nition Workshop 2004, NIST 2004, Montreal, Can-
ada, 2004-05-17
Susanne Burger, Victoria MacLaren and Hua Yu.
2002. The ISL Meeting Corpus: The Impact of
Meeting Type on Speech Style. ICSLP-2002. Den-
ver, CO: ISCA, 9 2002.
Jacob Cohen. 1960. A coefficient of agreement for
nominal scales. Educational and Psychological
Meas., 20:37–46.
Richard Craggs and Mary McGee Wood. 2004. A
categorical annotation scheme for emotion in the
linguistic content of dialogue. Affective Dialogue
Systems. 2004.
Laurence Devillers, Lori Lamel and Ioana Vasilescu.
2003. Emotion detection in task-oriented spoken
dialogs. IEEE International Conference on Multi-
media and Expo (ICME).
Rajdip Dhillon, Sonali Bhagat, Hannah Carvey and
Elizabeth Shriberg. 2003. “Meeting Recorder Pro-
ject: Dialog Act Labeling Guide,” ICSI Technical
Report TR-04-002, Version 3, October 2003
Giovanni Flammia and Victor Zue. 1997. Learning
The Structure of Mixed Initiative Dialogues Using
A Corpus of Annotated Conversations. Eurospeech
1997, Rhodes, Greece 1997, p1871—1874
Julia Hirschberg and Christine Nakatani. 1996. A Pro-
sodic Analysis of Discourse Segments in Direction-
Giving Monologues Annual Meeting- Association
For Computational Linguistics 1996, VOL 34,
pages 286-293
Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip Dhil-
lon, Jane Edwards, Javier Mac´ıas-Guarasa, Nelson
Morgan, Barbara Peskin, Elizabeth Shriberg, An-
dreas Stolcke, Chuck Wooters and Britta Wrede.
2004. “The ICSI Meeting Project: Resources and
Research,” ICASSP-2004 Meeting Recognition
Workshop. Montreal; Canada: NIST, 5 2004
Daniel Jurafsky, Elizabeth Shriberg and Debra Biasca,
1997. Switchboard-DAMSL Labeling Project
Coder’s Manual.
http://stripe.colorado.edu/˜jurafsky/manual.august1
Werner Kunz and Horst W. J. Rittel. 1970. Issues as
elements of information systems. Working Paper
WP-131, Univ. Stuttgart, Inst. Fuer Grundlagen der
Planung, 1970
Richard Landis and Gary Koch. 1977. The Measure-
ment of Observer Agreement for Categorical Data
Biometrics, Vol. 33, No. 1 (Mar., 1977) , pp. 159-
174
Agnes Lisowska. 2003. Multimodal interface design
for the multimodal meeting domain: Preliminary
indications from a query analysis study. Technical
Report IM2. Technical report, ISSCO/TIM/ETI.
Universit de Genve, Switserland, November 2003.
Jackson Liscombe, Jennifer Venditti and Julia
Hirschberg. 2003. Classifying Subject Ratings of
Emotional Speech Using Acoustic Features. Eu-
rospeech 2003.
Christine Nakatani and David Traum. 1998. Draft:
Discourse Structure Coding Manual version
2/27/98
Randolph Quirk, Sidney Greenbaum, Geoffry Leech
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman, New
York.s
Rutger Rienks, Dirk Heylen and Erik van der Wei-
jden. 2005. Argument diagramming of meeting
conversations. In Vinciarelli, A. and Odobez, J.,
editors, Multimodal Multiparty Meeting Process-
ing, Workshop at the 7th International Conference
on Multimodal Interfaces, pages 85–92, Trento, It-
aly
Janyce Wiebe, Theresa Wilson and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and
Evaluation (formerly Computers and the Humani-
ties), volume 39, issue 2-3, pp. 165-210.
Theresa Wilson and Janyce Wiebe. 2005. Annotating
attributions and private states. ACL Workshop on
Frontiers in Corpus Annotation II: Pie in the Sky.
Britta Wrede and Elizabeth Shriberg. 2003. Spotting
&amp;quot;Hotspots&amp;quot; in Meetings: Human Judgments and
Prosodic Cues. Eurospeech 2003, Geneva
</reference>
<page confidence="0.999269">
61
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.762261">
<title confidence="0.998794">Manual Annotation of Opinion Categories in Meetings</title>
<author confidence="0.988424">Janyce Paul Diane</author>
<address confidence="0.828427">of Computer Science, University of Pittsburgh, Pittsburgh, PA 15260</address>
<note confidence="0.9679655">Systems Program, University of Pittsburgh, Pittsburgh, PA 15260 {swapna,wiebe,hoffmanp,litman}@cs.pitt.edu</note>
<abstract confidence="0.99913875">This paper applies the categories from an opinion annotation scheme developed for monologue text to the genre of multiparty meetings. We describe modifications to the coding guidelines that were required to extend the categories to the new type of data, and present the results of an inter-annotator agreement study. As researchers have found with other types of annotations in speech data, interannotator agreement is higher when the annotators both read and listen to the data than when they only read the transcripts. Previous work exploited prosodic clues to perform automatic detection of speaker emotion (Liscombe et al. 2003). Our findings suggest that doing so to recognize opinion categories would be a promising line of work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Susanne Burger</author>
<author>Zachary A Sloane</author>
</authors>
<title>The ISL Meeting Corpus: Categorical Features of Communicative Group Interactions. NIST Meeting Recognition Workshop</title>
<date>2004</date>
<pages>2004--05</pages>
<location>NIST</location>
<contexts>
<context position="27588" citStr="Burger and Sloane, 2004" startWordPosition="4524" endWordPosition="4527">formation seeking and question answering exchanges where one participant is purely seeking information, while the other is providing it. The SWBD DAMSL (Jurafsky et al., 1997) annotation scheme over the Switchboard telephonic conversation corpus labels shallow discourse structures. The SWBD-DAMSL had a label “sv” for opinions. However, due to poor interannotator agreement, the authors discarded these annotations. The ICSI MRDA annotation scheme (Rajdip et al., 2003) adopts the SWBD DAMSL scheme, but does not distinguish between the opinionated and objective statements. The ISL meeting corpus (Burger and Sloane, 2004) is annotated with dialog acts and discourse moves like initiation and response, which in turn consist of dialog tags such as query, align, and statement. Their statement dialog category would not only include Sentiment and Arguing tags discussed in this paper, but it would also include objective statements and other types of subjectivity. “Hot spots” in meetings closely relate to our work because they find sections in the meeting where participants are involved in debates or high arousal activity (Wrede and Shriberg 2003). While that work distinguishes between high arousal and low arousal, it</context>
</contexts>
<marker>Burger, Sloane, 2004</marker>
<rawString>Susanne Burger and Zachary A Sloane. 2004. The ISL Meeting Corpus: Categorical Features of Communicative Group Interactions. NIST Meeting Recognition Workshop 2004, NIST 2004, Montreal, Canada, 2004-05-17</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susanne Burger</author>
<author>Victoria MacLaren</author>
<author>Hua Yu</author>
</authors>
<date>2002</date>
<journal>ISCA,</journal>
<booktitle>The ISL Meeting Corpus: The Impact of Meeting Type on Speech Style. ICSLP-2002.</booktitle>
<volume>9</volume>
<location>Denver, CO:</location>
<contexts>
<context position="7060" citStr="Burger et al. 2002" startWordPosition="1150" endWordPosition="1154">s have been exploited to perform automatic detection of speaker emotion (Liscombe et al. 2003). Our findings suggest that doing so to recognize opinion categories is a promising line of work. The rest of the paper is organized as follows: In Section 2 we discuss the data and the annotation scheme and present examples. We then present our inter-annotator agreement results in Section 3, and in Section 4 we discuss issues and observations. Related work is described in Section 5. Conclusions and Future Work are presented in Section 6. 2 Annotation 2.1 Data The data is from the ISL meeting corpus (Burger et al. 2002). We chose task oriented meetings from the games/scenario and discussion genres, as we felt they would be closest to the applications for which the opinion analysis will be useful. The ISL speech is accompanied by rich transcriptions, which are tagged according to VERBMOBIL conventions. However, since realtime applications only have access to ASR output, we gave the annotators raw text, from which all VERBMOBIL tags, punctuation, and capitalizations were removed. In order to see how annotations would be affected by the presence or absence of speech, we divided each raw text document into 2 seg</context>
</contexts>
<marker>Burger, MacLaren, Yu, 2002</marker>
<rawString>Susanne Burger, Victoria MacLaren and Hua Yu. 2002. The ISL Meeting Corpus: The Impact of Meeting Type on Speech Style. ICSLP-2002. Denver, CO: ISCA, 9 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Meas.,</booktitle>
<pages>20--37</pages>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Meas., 20:37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Craggs</author>
<author>Mary McGee Wood</author>
</authors>
<title>A categorical annotation scheme for emotion in the linguistic content of dialogue. Affective Dialogue Systems.</title>
<date>2004</date>
<marker>Craggs, Wood, 2004</marker>
<rawString>Richard Craggs and Mary McGee Wood. 2004. A categorical annotation scheme for emotion in the linguistic content of dialogue. Affective Dialogue Systems. 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurence Devillers</author>
<author>Lori Lamel</author>
<author>Ioana Vasilescu</author>
</authors>
<title>Emotion detection in task-oriented spoken dialogs.</title>
<date>2003</date>
<booktitle>IEEE International Conference on Multimedia and Expo (ICME).</booktitle>
<contexts>
<context position="26265" citStr="Devillers et al. (2003)" startWordPosition="4321" endWordPosition="4324"> attitude (such as Sentiment or Arguing), of a certain intensity, towards a target. Wilson and Wiebe (2005) extend this basic annotation scheme to include different types of subjectivity, including Positive Sentiment, Negative Sentiment, Positive Arguing, and Negative Arguing. Speech was found to improve inter-annotator agreement in discourse segmentation of monologs (Hirschberg and Nakatani 1996). Acoustic clues have been successfully employed for the reliable detection of the speaker’s emotions, including frustration, annoyance, anger, happiness, sadness, and boredom (Liscombe et al. 2003). Devillers et al. (2003) performed perceptual tests with and without speech in detecting the speaker’s fear, anger, satisfaction and embarrassment. Though related, our work is not concerned with the speaker’s emotions, but rather opinions toward the issues and topics addressed in the meeting. Most annotation work in multiparty conversation has focused on exchange structures and discourse functional units like common grounding (Nakatani and Traum, 1998). In common grounding research, the focus is on whether the participants of the discourse are able to understand each other, and not their opinions towards the content </context>
</contexts>
<marker>Devillers, Lamel, Vasilescu, 2003</marker>
<rawString>Laurence Devillers, Lori Lamel and Ioana Vasilescu. 2003. Emotion detection in task-oriented spoken dialogs. IEEE International Conference on Multimedia and Expo (ICME).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajdip Dhillon</author>
<author>Sonali Bhagat</author>
<author>Hannah Carvey</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Meeting Recorder Project: Dialog Act Labeling Guide,” ICSI</title>
<date>2003</date>
<tech>Technical Report TR-04-002, Version 3,</tech>
<marker>Dhillon, Bhagat, Carvey, Shriberg, 2003</marker>
<rawString>Rajdip Dhillon, Sonali Bhagat, Hannah Carvey and Elizabeth Shriberg. 2003. “Meeting Recorder Project: Dialog Act Labeling Guide,” ICSI Technical Report TR-04-002, Version 3, October 2003</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giovanni Flammia</author>
<author>Victor Zue</author>
</authors>
<title>Learning The Structure of Mixed Initiative Dialogues Using A Corpus of Annotated Conversations. Eurospeech</title>
<date>1997</date>
<pages>1871--1874</pages>
<location>Rhodes,</location>
<contexts>
<context position="26952" citStr="Flammia and Zue (1997)" startWordPosition="4430" endWordPosition="4433">the speaker’s fear, anger, satisfaction and embarrassment. Though related, our work is not concerned with the speaker’s emotions, but rather opinions toward the issues and topics addressed in the meeting. Most annotation work in multiparty conversation has focused on exchange structures and discourse functional units like common grounding (Nakatani and Traum, 1998). In common grounding research, the focus is on whether the participants of the discourse are able to understand each other, and not their opinions towards the content of the discourse. Other tagging schemes like the one proposed by Flammia and Zue (1997) focus on information seeking and question answering exchanges where one participant is purely seeking information, while the other is providing it. The SWBD DAMSL (Jurafsky et al., 1997) annotation scheme over the Switchboard telephonic conversation corpus labels shallow discourse structures. The SWBD-DAMSL had a label “sv” for opinions. However, due to poor interannotator agreement, the authors discarded these annotations. The ICSI MRDA annotation scheme (Rajdip et al., 2003) adopts the SWBD DAMSL scheme, but does not distinguish between the opinionated and objective statements. The ISL meet</context>
</contexts>
<marker>Flammia, Zue, 1997</marker>
<rawString>Giovanni Flammia and Victor Zue. 1997. Learning The Structure of Mixed Initiative Dialogues Using A Corpus of Annotated Conversations. Eurospeech 1997, Rhodes, Greece 1997, p1871—1874</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
<author>Christine Nakatani</author>
</authors>
<title>A Prosodic Analysis of Discourse Segments</title>
<date>1996</date>
<booktitle>in DirectionGiving Monologues Annual Meeting- Association For Computational Linguistics</booktitle>
<volume>34</volume>
<pages>286--293</pages>
<contexts>
<context position="26042" citStr="Hirschberg and Nakatani 1996" startWordPosition="4290" endWordPosition="4293">hemes described in Wiebe et al. (2005) and Wilson and Wiebe (2005). Wiebe et al. (2005) perform expression level annotation of opinions and subjectivity in text. They define their annotations as an experiencer having some type of attitude (such as Sentiment or Arguing), of a certain intensity, towards a target. Wilson and Wiebe (2005) extend this basic annotation scheme to include different types of subjectivity, including Positive Sentiment, Negative Sentiment, Positive Arguing, and Negative Arguing. Speech was found to improve inter-annotator agreement in discourse segmentation of monologs (Hirschberg and Nakatani 1996). Acoustic clues have been successfully employed for the reliable detection of the speaker’s emotions, including frustration, annoyance, anger, happiness, sadness, and boredom (Liscombe et al. 2003). Devillers et al. (2003) performed perceptual tests with and without speech in detecting the speaker’s fear, anger, satisfaction and embarrassment. Though related, our work is not concerned with the speaker’s emotions, but rather opinions toward the issues and topics addressed in the meeting. Most annotation work in multiparty conversation has focused on exchange structures and discourse functional</context>
</contexts>
<marker>Hirschberg, Nakatani, 1996</marker>
<rawString>Julia Hirschberg and Christine Nakatani. 1996. A Prosodic Analysis of Discourse Segments in DirectionGiving Monologues Annual Meeting- Association For Computational Linguistics 1996, VOL 34, pages 286-293</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Janin</author>
<author>Jeremy Ang</author>
<author>Sonali Bhagat</author>
<author>Rajdip Dhillon</author>
<author>Jane Edwards</author>
<author>Javier Mac´ıas-Guarasa</author>
<author>Nelson Morgan</author>
<author>Barbara Peskin</author>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
<author>Chuck Wooters</author>
<author>Britta Wrede</author>
</authors>
<date>2004</date>
<booktitle>The ICSI Meeting Project: Resources and Research,” ICASSP-2004 Meeting Recognition Workshop.</booktitle>
<location>Montreal; Canada: NIST, 5</location>
<marker>Janin, Ang, Bhagat, Dhillon, Edwards, Mac´ıas-Guarasa, Morgan, Peskin, Shriberg, Stolcke, Wooters, Wrede, 2004</marker>
<rawString>Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip Dhillon, Jane Edwards, Javier Mac´ıas-Guarasa, Nelson Morgan, Barbara Peskin, Elizabeth Shriberg, Andreas Stolcke, Chuck Wooters and Britta Wrede. 2004. “The ICSI Meeting Project: Resources and Research,” ICASSP-2004 Meeting Recognition Workshop. Montreal; Canada: NIST, 5 2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>Elizabeth Shriberg</author>
<author>Debra Biasca</author>
</authors>
<date>1997</date>
<booktitle>Switchboard-DAMSL Labeling Project Coder’s Manual. http://stripe.colorado.edu/˜jurafsky/manual.august1</booktitle>
<contexts>
<context position="27139" citStr="Jurafsky et al., 1997" startWordPosition="4459" endWordPosition="4462">d in the meeting. Most annotation work in multiparty conversation has focused on exchange structures and discourse functional units like common grounding (Nakatani and Traum, 1998). In common grounding research, the focus is on whether the participants of the discourse are able to understand each other, and not their opinions towards the content of the discourse. Other tagging schemes like the one proposed by Flammia and Zue (1997) focus on information seeking and question answering exchanges where one participant is purely seeking information, while the other is providing it. The SWBD DAMSL (Jurafsky et al., 1997) annotation scheme over the Switchboard telephonic conversation corpus labels shallow discourse structures. The SWBD-DAMSL had a label “sv” for opinions. However, due to poor interannotator agreement, the authors discarded these annotations. The ICSI MRDA annotation scheme (Rajdip et al., 2003) adopts the SWBD DAMSL scheme, but does not distinguish between the opinionated and objective statements. The ISL meeting corpus (Burger and Sloane, 2004) is annotated with dialog acts and discourse moves like initiation and response, which in turn consist of dialog tags such as query, align, and stateme</context>
</contexts>
<marker>Jurafsky, Shriberg, Biasca, 1997</marker>
<rawString>Daniel Jurafsky, Elizabeth Shriberg and Debra Biasca, 1997. Switchboard-DAMSL Labeling Project Coder’s Manual. http://stripe.colorado.edu/˜jurafsky/manual.august1</rawString>
</citation>
<citation valid="true">
<authors>
<author>Werner Kunz</author>
<author>Horst W J Rittel</author>
</authors>
<title>Issues as elements of information systems.</title>
<date>1970</date>
<booktitle>Working Paper WP-131, Univ. Stuttgart, Inst. Fuer Grundlagen</booktitle>
<contexts>
<context position="28792" citStr="Kunz and Rittel 1970" startWordPosition="4713" endWordPosition="4717">d low arousal, it does not distinguish between opinion or non-opinion or the different types of opinion. However, Janin et al. (2004) suggest that there is a relationship between dialog acts and involvement, and that involved utterances contain significantly more evaluative and subjective statements as well as extremely positive or negative answers. Thus we believe it may be beneficial for such works to make these distinctions. Another closely related work that finds participants’ positions regarding issues is argument diagramming (Rienks et al. 2005). This approach, based on the IBIS system (Kunz and Rittel 1970), divides a discourse into issues, and finds lines of deliberated arguments. However they do not distinguish between subjective and objective contributions towards the meeting. 6 Conclusions and Future Work In this paper we performed an annotation study of opinions in meetings, and investigated the effects of speech. We have shown that it is possible to reliably detect opinions within multiparty conversations. Our consistently better agreement results with text+speech input over text-only input suggest that speech is a reliable indicator of opinions. We have also found that Annotator Uncertain</context>
</contexts>
<marker>Kunz, Rittel, 1970</marker>
<rawString>Werner Kunz and Horst W. J. Rittel. 1970. Issues as elements of information systems. Working Paper WP-131, Univ. Stuttgart, Inst. Fuer Grundlagen der Planung, 1970</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Landis</author>
<author>Gary Koch</author>
</authors>
<title>The Measurement of Observer Agreement for</title>
<date>1977</date>
<journal>Categorical Data Biometrics,</journal>
<volume>33</volume>
<pages>159--174</pages>
<contexts>
<context position="17605" citStr="Landis and Koch (1977)" startWordPosition="2907" endWordPosition="2910"> Arguing. To evaluate a category, we did the following: • For each turn, if both annotators tagged the turn with the given category, or both did not tag the turn with the category, then it is a match. • Otherwise it is a mismatch Table 1 shows the inter-annotator Kappa values on the test set. Agreement (Kappa) Raw Text Raw Text only + Speech Positive Arguing 0.54 0.60 Negative Arguing 0.32 0.65 Positive Sentiment 0.57 0.69 Negative Sentiment 0.41 0.61 Table 1 Inter-annotator agreement on different categories. With raw-text-only annotation, the Kappa value is in the moderate range according to Landis and Koch (1977), except for Negative Arguing for which it is 0.32. Positive Arguing and Positive Sentiment were more reliably detected than Negative Arguing and Negative Sentiment. We believe this is because participants were more comfortable with directly expressing their positive sentiments in front of other participants. Given only the raw text data, interannotator reliability measures for Negative Arguing and Negative Sentiment are the lowest. We believe this might be due to the fact that participants in social interactions are not very forthright with their Negative Sentiments and Arguing. Negative Sent</context>
<context position="19373" citStr="Landis and Koch (1977)" startWordPosition="3200" endWordPosition="3203">ls can be inferred from the statement that the wheels should look dignified. However the annotators were not sure, as the participant chose to focus on what is desirable (i.e., dignified wheels). This utterance is actually both a general statement of what is desirable, and an implication that aluminum wheels are not dignified. But this may be difficult to ascertain with the raw text signal only. When the annotators had speech to guide their judgments, the Kappa values go up significantly for each category. All the agreement numbers for raw text+speech are in the substantial range according to Landis and Koch (1977). We observe that with speech, Kappa for Negative Arguing has doubled over the Kappa obtained without speech. The Kappa for Negative Sentiment (text+speech) shows a 1.5 times improvement over the one with only raw text. Both these observations indicate that speech is able to help the annotators tag negativity more reliably. It is quite likely that a seemingly neutral sentence could sound negative, depending on the way words are stressed or pauses are inserted. Comparing the agreement on Positive Sentiment, we get a 1.2 times improvement by using speech. Similarly, agreement improves by 1.1 tim</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>Richard Landis and Gary Koch. 1977. The Measurement of Observer Agreement for Categorical Data Biometrics, Vol. 33, No. 1 (Mar., 1977) , pp. 159-174</rawString>
</citation>
<citation valid="true">
<authors>
<author>Agnes Lisowska</author>
</authors>
<title>Multimodal interface design for the multimodal meeting domain: Preliminary indications from a query analysis study.</title>
<date>2003</date>
<tech>Technical Report IM2. Technical report, ISSCO/TIM/ETI. Universit de Genve, Switserland,</tech>
<contexts>
<context position="1746" citStr="Lisowska 2003" startWordPosition="252" endWordPosition="253">ss opinions, beliefs, evaluations and speculations (Wiebe et al. 2005). Many natural language processing applications could benefit from being able to distinguish between facts and opinions of various types, including speech-oriented applications such as meeting browsers, meeting summarizers, and speech-oriented question answering (QA) systems. Meeting browsers could find instances in meetings where opinions about key topics are expressed. Summarizers could include strong arguments for and against issues, to make the final outcome of the meeting more understandable. A preliminary user survey (Lisowska 2003) showed that users would like to be able to query meeting records with subjective questions like “Show me the conflicts of opinions between X and Y” , “Who made the highest number of positive/negative comments” and “Give me all the contributions of participant X in favor of alternative A regarding the issue I.” A QA system with a component to recognize opinions would be able to help find answers to such questions. Consider the following example from a meeting about an investment firm choosing which car to buy1. (In the examples, the words and phrases describing or expressing the opinion are un</context>
</contexts>
<marker>Lisowska, 2003</marker>
<rawString>Agnes Lisowska. 2003. Multimodal interface design for the multimodal meeting domain: Preliminary indications from a query analysis study. Technical Report IM2. Technical report, ISSCO/TIM/ETI. Universit de Genve, Switserland, November 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jackson Liscombe</author>
<author>Jennifer Venditti</author>
<author>Julia Hirschberg</author>
</authors>
<title>Classifying Subject Ratings of Emotional Speech Using Acoustic Features. Eurospeech</title>
<date>2003</date>
<contexts>
<context position="960" citStr="Liscombe et al. 2003" startWordPosition="133" endWordPosition="136">applies the categories from an opinion annotation scheme developed for monologue text to the genre of multiparty meetings. We describe modifications to the coding guidelines that were required to extend the categories to the new type of data, and present the results of an inter-annotator agreement study. As researchers have found with other types of annotations in speech data, interannotator agreement is higher when the annotators both read and listen to the data than when they only read the transcripts. Previous work exploited prosodic clues to perform automatic detection of speaker emotion (Liscombe et al. 2003). Our findings suggest that doing so to recognize opinion categories would be a promising line of work. 1 Introduction Subjectivity refers to aspects of language that express opinions, beliefs, evaluations and speculations (Wiebe et al. 2005). Many natural language processing applications could benefit from being able to distinguish between facts and opinions of various types, including speech-oriented applications such as meeting browsers, meeting summarizers, and speech-oriented question answering (QA) systems. Meeting browsers could find instances in meetings where opinions about key topics</context>
<context position="6535" citStr="Liscombe et al. 2003" startWordPosition="1054" endWordPosition="1058">lti-party meeting data, although there is some room for improvement: the Kappa values range from 0.32 to 0.69. As has been found for other types of annotations in speech, agreement is higher when the annotators both read and listen to the data than when they only read the transcripts. Interestingly, the advantages are more dramatic for some categories than others. And, in both conditions, agreement is higher for the positive than for the negative categories. We discuss possible reasons for these disparities. Prosodic clues have been exploited to perform automatic detection of speaker emotion (Liscombe et al. 2003). Our findings suggest that doing so to recognize opinion categories is a promising line of work. The rest of the paper is organized as follows: In Section 2 we discuss the data and the annotation scheme and present examples. We then present our inter-annotator agreement results in Section 3, and in Section 4 we discuss issues and observations. Related work is described in Section 5. Conclusions and Future Work are presented in Section 6. 2 Annotation 2.1 Data The data is from the ISL meeting corpus (Burger et al. 2002). We chose task oriented meetings from the games/scenario and discussion ge</context>
<context position="26240" citStr="Liscombe et al. 2003" startWordPosition="4317" endWordPosition="4320">cer having some type of attitude (such as Sentiment or Arguing), of a certain intensity, towards a target. Wilson and Wiebe (2005) extend this basic annotation scheme to include different types of subjectivity, including Positive Sentiment, Negative Sentiment, Positive Arguing, and Negative Arguing. Speech was found to improve inter-annotator agreement in discourse segmentation of monologs (Hirschberg and Nakatani 1996). Acoustic clues have been successfully employed for the reliable detection of the speaker’s emotions, including frustration, annoyance, anger, happiness, sadness, and boredom (Liscombe et al. 2003). Devillers et al. (2003) performed perceptual tests with and without speech in detecting the speaker’s fear, anger, satisfaction and embarrassment. Though related, our work is not concerned with the speaker’s emotions, but rather opinions toward the issues and topics addressed in the meeting. Most annotation work in multiparty conversation has focused on exchange structures and discourse functional units like common grounding (Nakatani and Traum, 1998). In common grounding research, the focus is on whether the participants of the discourse are able to understand each other, and not their opin</context>
</contexts>
<marker>Liscombe, Venditti, Hirschberg, 2003</marker>
<rawString>Jackson Liscombe, Jennifer Venditti and Julia Hirschberg. 2003. Classifying Subject Ratings of Emotional Speech Using Acoustic Features. Eurospeech 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Nakatani</author>
<author>David Traum</author>
</authors>
<date>1998</date>
<booktitle>Draft: Discourse Structure Coding Manual version</booktitle>
<pages>2--27</pages>
<contexts>
<context position="26697" citStr="Nakatani and Traum, 1998" startWordPosition="4386" endWordPosition="4389">cessfully employed for the reliable detection of the speaker’s emotions, including frustration, annoyance, anger, happiness, sadness, and boredom (Liscombe et al. 2003). Devillers et al. (2003) performed perceptual tests with and without speech in detecting the speaker’s fear, anger, satisfaction and embarrassment. Though related, our work is not concerned with the speaker’s emotions, but rather opinions toward the issues and topics addressed in the meeting. Most annotation work in multiparty conversation has focused on exchange structures and discourse functional units like common grounding (Nakatani and Traum, 1998). In common grounding research, the focus is on whether the participants of the discourse are able to understand each other, and not their opinions towards the content of the discourse. Other tagging schemes like the one proposed by Flammia and Zue (1997) focus on information seeking and question answering exchanges where one participant is purely seeking information, while the other is providing it. The SWBD DAMSL (Jurafsky et al., 1997) annotation scheme over the Switchboard telephonic conversation corpus labels shallow discourse structures. The SWBD-DAMSL had a label “sv” for opinions. Howe</context>
</contexts>
<marker>Nakatani, Traum, 1998</marker>
<rawString>Christine Nakatani and David Traum. 1998. Draft: Discourse Structure Coding Manual version 2/27/98</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randolph Quirk</author>
<author>Sidney Greenbaum</author>
<author>Geoffry Leech</author>
<author>Jan Svartvik</author>
</authors>
<date>1985</date>
<journal>A Comprehensive Grammar of the English Language. Longman,</journal>
<location>New York.s</location>
<contexts>
<context position="8145" citStr="Quirk et al. 1985" startWordPosition="1331" endWordPosition="1334"> In order to see how annotations would be affected by the presence or absence of speech, we divided each raw text document into 2 segments. One part was annotated while reading the raw text only. For the annotation of the other part, speech as well as the raw text was provided. 2.2 Opinion Category Definitions We base our annotation definitions on the scheme developed by Wiebe et al. (2005) for news articles. That scheme centers on the notion of subjectivity, the linguistic expression of private states. Private states are internal mental states that cannot be objectively observed or verified (Quirk et al. 1985) and include opinions, beliefs, judgments, evaluations, thoughts, and feelings. Amongst these many forms of subjec55 tivity, we focus on the Sentiment and Arguing categories proposed by Wilson and Wiebe (2005). The categories are broken down by polarity and defined as follows: Positive Sentiments: positive emotions, evaluations, judgments and stances. (4) TBC: Well ca How about one of the the newer Cadillac the Lexus is good In (4), taken from the discussion of which car to buy, the speaker uses the term “good” to express his positive evaluation of the Lexus . Negative Sentiments: negative emo</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>Randolph Quirk, Sidney Greenbaum, Geoffry Leech and Jan Svartvik. 1985. A Comprehensive Grammar of the English Language. Longman, New York.s</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rutger Rienks</author>
</authors>
<title>Dirk Heylen and Erik van der Weijden.</title>
<date>2005</date>
<booktitle>Multimodal Multiparty Meeting Processing, Workshop at the 7th International Conference on Multimodal Interfaces,</booktitle>
<pages>85--92</pages>
<editor>In Vinciarelli, A. and Odobez, J., editors,</editor>
<location>Trento, Italy</location>
<marker>Rienks, 2005</marker>
<rawString>Rutger Rienks, Dirk Heylen and Erik van der Weijden. 2005. Argument diagramming of meeting conversations. In Vinciarelli, A. and Odobez, J., editors, Multimodal Multiparty Meeting Processing, Workshop at the 7th International Conference on Multimodal Interfaces, pages 85–92, Trento, Italy</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<booktitle>Language Resources and Evaluation (formerly Computers and the Humanities),</booktitle>
<volume>39</volume>
<pages>2--3</pages>
<contexts>
<context position="1202" citStr="Wiebe et al. 2005" startWordPosition="172" endWordPosition="175">d present the results of an inter-annotator agreement study. As researchers have found with other types of annotations in speech data, interannotator agreement is higher when the annotators both read and listen to the data than when they only read the transcripts. Previous work exploited prosodic clues to perform automatic detection of speaker emotion (Liscombe et al. 2003). Our findings suggest that doing so to recognize opinion categories would be a promising line of work. 1 Introduction Subjectivity refers to aspects of language that express opinions, beliefs, evaluations and speculations (Wiebe et al. 2005). Many natural language processing applications could benefit from being able to distinguish between facts and opinions of various types, including speech-oriented applications such as meeting browsers, meeting summarizers, and speech-oriented question answering (QA) systems. Meeting browsers could find instances in meetings where opinions about key topics are expressed. Summarizers could include strong arguments for and against issues, to make the final outcome of the meeting more understandable. A preliminary user survey (Lisowska 2003) showed that users would like to be able to query meetin</context>
<context position="7920" citStr="Wiebe et al. (2005)" startWordPosition="1295" endWordPosition="1298">are tagged according to VERBMOBIL conventions. However, since realtime applications only have access to ASR output, we gave the annotators raw text, from which all VERBMOBIL tags, punctuation, and capitalizations were removed. In order to see how annotations would be affected by the presence or absence of speech, we divided each raw text document into 2 segments. One part was annotated while reading the raw text only. For the annotation of the other part, speech as well as the raw text was provided. 2.2 Opinion Category Definitions We base our annotation definitions on the scheme developed by Wiebe et al. (2005) for news articles. That scheme centers on the notion of subjectivity, the linguistic expression of private states. Private states are internal mental states that cannot be objectively observed or verified (Quirk et al. 1985) and include opinions, beliefs, judgments, evaluations, thoughts, and feelings. Amongst these many forms of subjec55 tivity, we focus on the Sentiment and Arguing categories proposed by Wilson and Wiebe (2005). The categories are broken down by polarity and defined as follows: Positive Sentiments: positive emotions, evaluations, judgments and stances. (4) TBC: Well ca How </context>
<context position="25451" citStr="Wiebe et al. (2005)" startWordPosition="4203" endWordPosition="4206">y an important role in some types of meetings. In one meeting that we analyzed, participants were working together to figure out how to protect an egg when it is dropped from a long distance, given the materials they have. It was evident they were using some gestures to describe their ideas (“we can put tape like this”) and that they drew diagrams to get points across. In the absence of visual input, annotators would need to guess 59 what was happening. This might further hurt the inter-annotator reliability. 5 Related Work Our opinion categories are from the subjectivity schemes described in Wiebe et al. (2005) and Wilson and Wiebe (2005). Wiebe et al. (2005) perform expression level annotation of opinions and subjectivity in text. They define their annotations as an experiencer having some type of attitude (such as Sentiment or Arguing), of a certain intensity, towards a target. Wilson and Wiebe (2005) extend this basic annotation scheme to include different types of subjectivity, including Positive Sentiment, Negative Sentiment, Positive Arguing, and Negative Arguing. Speech was found to improve inter-annotator agreement in discourse segmentation of monologs (Hirschberg and Nakatani 1996). Acousti</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation (formerly Computers and the Humanities), volume 39, issue 2-3, pp. 165-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
</authors>
<title>Annotating attributions and private states.</title>
<date>2005</date>
<booktitle>ACL Workshop on Frontiers in Corpus Annotation II: Pie in the Sky.</booktitle>
<contexts>
<context position="5199" citStr="Wilson and Wiebe (2005)" startWordPosition="835" endWordPosition="838">ficantly different from the text domain, where opinion analysis has traditionally been applied. Specifically, differences arise because: 1) There are many participants interacting with one another, each expressing his or her own opinion, and eliciting reactions in the process. 2) Social interactions may constrain how openly people express their opinions; i.e., they are often indirect in their negative evaluations. We also explore the influence of speech on human perception of opinions. Specifically, we annotated some meeting data with the opinion categories Sentiment and Arguing as defined in Wilson and Wiebe (2005). In our annotation we first distinguish whether a Sentiment or Arguing is being expressed. If one is, we then mark the polarity (i.e., positive or negative) and the intensity (i.e., how strong the opinion is). Annotating the individual opinion expressions is useful in this genre, because we see many utterances that have more than one type of opinion (e.g. (3) above). To investigate how opinions are expressed in speech, we divide our annotation into two tasks, one in which the annotator only reads the raw text, and the other in which the annotator reads the raw text and also listens to the spe</context>
<context position="8354" citStr="Wilson and Wiebe (2005)" startWordPosition="1362" endWordPosition="1365">annotation of the other part, speech as well as the raw text was provided. 2.2 Opinion Category Definitions We base our annotation definitions on the scheme developed by Wiebe et al. (2005) for news articles. That scheme centers on the notion of subjectivity, the linguistic expression of private states. Private states are internal mental states that cannot be objectively observed or verified (Quirk et al. 1985) and include opinions, beliefs, judgments, evaluations, thoughts, and feelings. Amongst these many forms of subjec55 tivity, we focus on the Sentiment and Arguing categories proposed by Wilson and Wiebe (2005). The categories are broken down by polarity and defined as follows: Positive Sentiments: positive emotions, evaluations, judgments and stances. (4) TBC: Well ca How about one of the the newer Cadillac the Lexus is good In (4), taken from the discussion of which car to buy, the speaker uses the term “good” to express his positive evaluation of the Lexus . Negative Sentiments: negative emotions, evaluations, judgments and stances. (5) OCK: I think these are all really bad choices In (5), the speaker expresses his negative evaluation of the choices for the company car. Note that “really” makes t</context>
<context position="25479" citStr="Wilson and Wiebe (2005)" startWordPosition="4208" endWordPosition="4211">ome types of meetings. In one meeting that we analyzed, participants were working together to figure out how to protect an egg when it is dropped from a long distance, given the materials they have. It was evident they were using some gestures to describe their ideas (“we can put tape like this”) and that they drew diagrams to get points across. In the absence of visual input, annotators would need to guess 59 what was happening. This might further hurt the inter-annotator reliability. 5 Related Work Our opinion categories are from the subjectivity schemes described in Wiebe et al. (2005) and Wilson and Wiebe (2005). Wiebe et al. (2005) perform expression level annotation of opinions and subjectivity in text. They define their annotations as an experiencer having some type of attitude (such as Sentiment or Arguing), of a certain intensity, towards a target. Wilson and Wiebe (2005) extend this basic annotation scheme to include different types of subjectivity, including Positive Sentiment, Negative Sentiment, Positive Arguing, and Negative Arguing. Speech was found to improve inter-annotator agreement in discourse segmentation of monologs (Hirschberg and Nakatani 1996). Acoustic clues have been successful</context>
</contexts>
<marker>Wilson, Wiebe, 2005</marker>
<rawString>Theresa Wilson and Janyce Wiebe. 2005. Annotating attributions and private states. ACL Workshop on Frontiers in Corpus Annotation II: Pie in the Sky.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Britta Wrede</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Spotting &amp;quot;Hotspots&amp;quot; in Meetings: Human Judgments and Prosodic Cues. Eurospeech</title>
<date>2003</date>
<location>Geneva</location>
<contexts>
<context position="28116" citStr="Wrede and Shriberg 2003" startWordPosition="4608" endWordPosition="4611">tween the opinionated and objective statements. The ISL meeting corpus (Burger and Sloane, 2004) is annotated with dialog acts and discourse moves like initiation and response, which in turn consist of dialog tags such as query, align, and statement. Their statement dialog category would not only include Sentiment and Arguing tags discussed in this paper, but it would also include objective statements and other types of subjectivity. “Hot spots” in meetings closely relate to our work because they find sections in the meeting where participants are involved in debates or high arousal activity (Wrede and Shriberg 2003). While that work distinguishes between high arousal and low arousal, it does not distinguish between opinion or non-opinion or the different types of opinion. However, Janin et al. (2004) suggest that there is a relationship between dialog acts and involvement, and that involved utterances contain significantly more evaluative and subjective statements as well as extremely positive or negative answers. Thus we believe it may be beneficial for such works to make these distinctions. Another closely related work that finds participants’ positions regarding issues is argument diagramming (Rienks </context>
</contexts>
<marker>Wrede, Shriberg, 2003</marker>
<rawString>Britta Wrede and Elizabeth Shriberg. 2003. Spotting &amp;quot;Hotspots&amp;quot; in Meetings: Human Judgments and Prosodic Cues. Eurospeech 2003, Geneva</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>