<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011172">
<title confidence="0.997076">
Pattern Learning for Relation Extraction with a Hierarchical Topic Model
</title>
<author confidence="0.958934">
Enrique Alfonseca Katja Filippova Jean-Yves Delort
</author>
<affiliation confidence="0.911121">
Google Research
</affiliation>
<address confidence="0.9709775">
Brandschenkestrasse 110
8002 Zurich, Switzerland
</address>
<email confidence="0.998958">
lealfonseca,katjaf,jydelortl@google.com
</email>
<note confidence="0.591643">
Guillermo Garrido*
NLP &amp; IR Group, UNED
Juan del Rosal, 16.
</note>
<address confidence="0.871079">
28040 Madrid, Spain
</address>
<email confidence="0.995896">
ggarrido@lsi.uned.es
</email>
<sectionHeader confidence="0.99858" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998251">
We describe the use of a hierarchical topic
model for automatically identifying syntactic
and lexical patterns that explicitly state on-
tological relations. We leverage distant su-
pervision using relations from the knowledge
base FreeBase, but do not require any man-
ual heuristic nor manual seed list selections.
Results show that the learned patterns can be
used to extract new relations with good preci-
sion.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998959981481482">
The detection of relations between entities for the
automatic population of knowledge bases is very
useful for solving tasks such as Entity Disambigua-
tion, Information Retrieval and Question Answer-
ing. The availability of high-coverage, general-
purpose knowledge bases enable the automatic iden-
tification and disambiguation of entities in text
and its applications (Bunescu and Pasca, 2006;
Cucerzan, 2007; McNamee and Dang, 2009; Kwok
et al., 2001; Pasca et al., 2006; Weld et al., 2008;
Pereira et al., 2009; Kasneci et al., 2009).
Most early works in this area were designed
for supervised Information Extraction competitions
such as MUC (Sundheim and Chinchor, 1993) and
ACE (ACE, 2004; Doddington et al., 2004; Li et
al., 2011), which rely on the availability of anno-
tated data. Open Information Extraction (Sekine,
2006; Banko et al., 2007; Bollegala et al., 2010)
started as an effort to approach relation extraction in
*Work done during an internship at Google Zurich.
a completely unsupervised way, by learning regular-
ities and patterns from the web. Two example sys-
tems implementing this paradigm are TEXTRUN-
NER (Yates et al., 2007) and REVERB (Fader et al.,
2011). These systems do not need any manual data
or rules, but the relational facts they extract are not
immediately disambiguated to entities and relations
from a knowledge base.
A different family of unsupervised methods for
relation extraction is unsupervised semantic pars-
ing, which aims at clustering entity mentions and
relation surface forms, thus generating a semantic
representation of the texts on which inference may
be used. Some techniques that have been used are
Markov Random Fields (Poon and Domingos, 2009)
and Bayesian generative models (Titov and Klemen-
tiev, 2011). These are quite powerful approaches
but have very high computational requirements (cf.
(Yao et al., 2011)).
A good trade-off between fully supervised and
fully unsupervised approaches is distant supervi-
sion, a semi-supervised procedure consisting of find-
ing sentences that contain two entities whose rela-
tion we know, and using those sentences as train-
ing examples for a supervised classifier (Hoffmann
et al., 2010; Wu and Weld, 2010; Hoffmann et al.,
2011; Wang et al., 2011; Yao et al., 2011). A usual
problem is that two related entities may co-occur in
one sentence for many unrelated reasons. For ex-
ample, Barack Obama is the president of the United
States, but not every sentence including the two en-
tities supports and states this relation. Much of the
previous work uses heuristics, e.g. extracting sen-
tences only from encyclopedic entries (Mintz et al.,
</bodyText>
<page confidence="0.98744">
54
</page>
<note confidence="0.8141845">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 54–59,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figure confidence="0.99935095">
Person-parent
(Liza Minneli, Judy Garland)
...
(Achilles, Peleus)
...
(...)
Person-death place
(Napoleon Bonaparte, Saint
Helena)
...
(Johann Christian Bach, Lon-
don)
...
(...)
Person-birth place
(Charles Darwin, Shrewsbury)
...
(Anthony Daniels, Salisbury)
...
(...)
</figure>
<bodyText confidence="0.9986675625">
2009; Hoffmann et al., 2011; Wang et al., 2011), or
syntactic restrictions on the sentences and the entity
mentions (Wu and Weld, 2010). These are usually
defined manually and may need to be adapted to dif-
ferent languages and domains. Manually selected
seeds can also be used (Ravichandran and Hovy,
2002; Kozareva and Hovy, 2010).
The main contribution of this work is presenting
a variant of distance supervision for relation extrac-
tion where we do not use heuristics in the selection
of the training data. Instead, we use topic models to
discriminate between the patterns that are expressing
the relation and those that are ambiguous and can be
applied across relations. In this way, high-precision
extraction patterns can be learned without the need
of any manual intervention.
</bodyText>
<sectionHeader confidence="0.816011" genericHeader="method">
2 Unsupervised relational pattern learning
</sectionHeader>
<bodyText confidence="0.998656413793103">
Similar to other distant supervision methods, our ap-
proach takes as input an existing knowledge base
containing entities and relations, and a textual cor-
pus. In this work it is not necessary for the corpus
to be related to the knowledge base. In what follows
we assume that all the relations studied are binary
and hold between exactly two entities in the knowl-
edge base. We also assume a dependency parser is
available, and that the entities have been automat-
ically disambiguated using the knowledge base as
sense inventory.
One of the most important problems to solve in
distant supervision approaches is to be able to dis-
tinguish which of the textual examples that include
two related entities, ei and ej, are supporting the re-
lation. This section describes a fully unsupervised
solution to this problem, computing the probability
that a pattern supports a given relation, which will
allow us to determine the most likely relation ex-
pressed in any sentence. Specifically, if a sentence
contains two entities, ei and ej, connected through a
pattern w, our model computes the probability that
the pattern is expressing any relation –P(r|w)– for
any relation r defined in the knowledge base. Note
that we refer to patterns with the symbol w, as they
are the words in our topic models.
Preprocessing As a first step, the textual corpus
is processed and the data is transformed in the fol-
lowing way: (a) the input corpus is parsed and en-
</bodyText>
<table confidence="0.970572810810811">
Author-book
(Mark Twain, Adventures of Huckleberry
Finn)
poss�,
ARG&apos; ARG2
nnnn
�, ,�
ARG&apos; novels ARG2
nsubj
�-�q
ARG&apos; releaseddobj ARG2
conj
�r
ARG2 ARG&apos;
nsubj�, �r
ARG&apos; wrote dobj ARG2
poss�,
ARG&apos; ARG2
���
(Jhumpa Lahiri, The Namesake)
nn�-
ARG&apos; ARG2
prep �q �unn
ARG2 byARG&apos;
nnappos
�, �-
ARG&apos; novel ARG2
prep �qnn
i
ARG2 by ARG&apos;
prep �qnn
i
ARG2 by - ARG&apos;
poss�-
ARG&apos; ARG&apos;
���
(...)
</table>
<figureCaption confidence="0.9947196">
Figure 1: Example of a generated set of document collec-
tions from a news corpus for relation extraction. Larger
boxes are document collections (relations), and inner
boxes are documents (entity pairs). Document contain
dependency patterns, which are words in the topic model.
</figureCaption>
<bodyText confidence="0.997156555555555">
tities are disambiguated; (b) for each relation r in
the knowledge base, a new (initially empty) docu-
ment collection Cr is created; (c) for each entity pair
(ei, ej) which are related in the knowledge base, a
new (initially empty) document Dij is created; (d)
for each sentence in the input corpus containing one
mention of ei and one mention of ej, a new term is
added to Dij consisting of the context in which the
two entities were seen in the document. This context
may be a complex structure, such as the dependency
path joining the two entities, but it is considered for
our purposes as a single term; (e) for each relation r
relating ei with ej, document Dij is added to collec-
tion Cr. Note that if the two entities are related in
different ways at the same time, an identical copy of
the document Dij will be added to the collection for
all those relations.
Figure 1 shows a set of document collections gen-
</bodyText>
<page confidence="0.995929">
55
</page>
<figureCaption confidence="0.999171">
Figure 2: Plate diagram of the generative model used.
</figureCaption>
<bodyText confidence="0.9999861">
erated for three relations using this procedure. Each
relation r has associated a different document col-
lection, which contains one document associated to
each entity pair from the knowledge base which is
in relation r. The words in each document can be,
for example, all the dependency paths that have been
observed in the input textual corpus between the two
related entities. Each document will contain some
very generic paths (e.g. the two entities consecutive
in the text) and some more specific paths.
Generative model Once these collections are
built, we use the generative model from Figure 2
to learn the probability that a dependency path is
conveying some relation between the entities it con-
nects. This model is very similar to the one used
by Haghighi and Vanderwende (2009) in the con-
text of text summarization. w (the observed vari-
able) represents a pattern between two entities. The
topic model OG captures general patterns that appear
for all relations. OD captures patterns that are spe-
cific about a certain entity pair, but which are not
generalizable across all pairs with the same relation.
Finally OA contains the patterns that are observed
across most pairs related with the same relation. OA
is the topic model of interest for us.
We use Gibbs sampling to estimate the different
models from the source data. The topic assignments
(for each pattern) that are the output of this process
are used to estimate P(rlw): when we observe pat-
tern w, the probability that it conveys relation r.
</bodyText>
<sectionHeader confidence="0.995148" genericHeader="evaluation">
3 Experiments and results
</sectionHeader>
<bodyText confidence="0.999966159090909">
Settings We use Freebase as our knowledge base.
It can be freely downloaded1. text corpus used con-
tains 33 million English news articles that we down-
loaded between January 2004 and December 2011.
A random sample of 3M of them is used for building
the document collections on which to train the topic
models, and the remaining 30M is used for testing.
The corpus is preprocessed by identifying Freebase
entity mentions, using an approach similar to (Milne
and Witten, 2008), and parsing it with an inductive
dependency parser (Nivre, 2006).
From the three million training documents, a set
of document collections (one per relation) has been
generated, by considering the sentences that contain
two entities which are related in FreeBase through
any binary relation and restricting to high-frequency
200 relations. Two ways of extracting patterns have
been used: (a) Syntactic, taking the dependency
path between the two entities, and (b) Intertext,
taking the text between the two. In both cases, a
topic model has been trained to learn the probabil-
ity of a relation given a pattern w: p(rlw). For A
we use symmetric Dirichlet priors AG = 0.1 and
AD = AA = 0.001, following the intuition that for
the background the probability mass across patterns
should be more evenly distributed. -y is set as (15,
15, 1), indicating in the prior that we expect more
patterns to belong to the background and entity-pair-
specific distributions due to the very noisy nature of
the input data. These values have not been tuned.
As a baseline, using the same training corpus, we
have calculated p(rlw) using the maximum likeli-
hood estimate: the number of times that a pattern w
has been seen connecting two entities for which r
holds divided by the total frequency of the pattern.
Extractions evaluation The patterns have been
applied to the 30 million documents left for testing.
For each pair of entities disambiguated as FreeBase
entities, if they are connected through a known pat-
tern, they are assigned arg max, p(r1w). We have
randomly sampled 4,000 such extractions and sent
them to raters. An extraction is to be judged cor-
rect if both it is correct in real life and the sentence
from which it was extracted really supports it. We
</bodyText>
<footnote confidence="0.999278">
1http://wiki.freebase.com/wiki/Data dumps
</footnote>
<page confidence="0.997448">
56
</page>
<figureCaption confidence="0.999902">
Figure 3: Evaluation of the extractions. X-axis has the threshold for p(r|w), and Y-axis has the precision of the extractions as a percentage.
</figureCaption>
<bodyText confidence="0.999168222222222">
have collected three ratings per example and taken
the majority decision. There was disagreement for
9.4% of the items on whether the sentence supports
the relation, and for 20% of the items on whether the
relation holds in the real world.
The results for different thresholds of p(rIw) are
shown in Figure 3. As can be seen, the MLE base-
lines (in red with syntactic patterns and green with
intertext) perform consistently worse than the mod-
els learned using the topic models (in pink and blue).
The difference in precision, aggregated across all re-
lations, is statistically significant at 95% confidence
for most of the thresholds.
Extractions aggregation We can take advantage
of redundancy on the web to calculate a support met-
ric for the extractions. In this experiment, for every
extracted relation (r, e1, e2), for every occurrence
of a pattern wi connecting e1 and e2, we add up
p(rlwi). Extractions that are obtained many times
and from high-precision patterns will rank higher.
Table 1 describes the results of this aggregation.
We have considered the top four highest-frequency
relations for people. After aggregating all the ex-
tracted relations and ranking them by support, we
have divided the evaluation set into two parts: (a)
for relations that were not already in FreeBase, we
evaluate the precision; (b) for extractions that were
already in FreeBase, we take the top-confidence sen-
tence identified and evaluate whether the sentence
is providing support to the relation. For each of
these, both syntactic patterns and intermediate-text
patterns have been evaluated.
The results are very interesting: using syntax,
Death place appears easy to extract new relations
and to find support. The patterns obtained are quite
unambiguous, e.g.
</bodyText>
<subsubsectionHeader confidence="0.285638">
pobj
</subsubsectionHeader>
<table confidence="0.94715425">
��
Relation Unknown relations Known relations
Correct relation P@50 Sentence support P@50
Syntax Intertext Syntax Intertext
Parent 0.58 0.38 1.00 1.00
Death place 0.90 0.68 0.98 0.94
Birth place 0.38 0.56 0.54 0.98
Nationality 0.86 0.78 0.34 0.40
</table>
<tableCaption confidence="0.999979">
Table 1: Evaluation on aggregated extractions.
</tableCaption>
<bodyText confidence="0.999954529411765">
On the other hand, birth place and nationality have
very different results for new relation acquisition
vs. finding sentence support for new relations. The
reason is that these relations are very correlated to
other relations that we did not have in our training
set. In the case of birth place, many relations re-
fer to having an official position in the city, such as
mayor; and for nationality, many of the patterns ex-
tract presidents or ministers. Not having mayor or
president in our initial collection (see Figure 1), the
support for these patterns is incorrectly learned. In
the case of nationality, however, even though the ex-
tracted sentences do not support the relation (P@50
= 0.34 for intertext), the new relations extracted are
mostly correct (P@50 = 0.86) as most presidents and
ministers in the real world have the nationality of the
country where they govern.
</bodyText>
<sectionHeader confidence="0.99738" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9991228">
We have described a new distant supervision model
with which to learn patterns for relation extraction
with no manual intervention. Results are promising,
we could obtain new relations that are not in Free-
Base with a high precision for some relation types. It
is also useful to extract support sentences for known
relations. More work is needed in understanding
which relations are compatible or overlapping and
which ones can partially imply each other (such as
president-country or born in-mayor).
</bodyText>
<footnote confidence="0.664546714285714">
subj prep
Z_
prep
��
pobj
i
ARG1 died at home in ARG2
</footnote>
<page confidence="0.994207">
57
</page>
<sectionHeader confidence="0.953244" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997857666666667">
ACE. 2004. The automatic content extraction projects.
http://projects.ldc.upenn.edu/ace.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the web. In IJCAI’07.
D.T. Bollegala, Y. Matsuo, and M. Ishizuka. 2010. Rela-
tional duality: Unsupervised extraction of semantic re-
lations between entities on the web. In Proceedings of
the 19th international conference on World wide web,
pages 151–160. ACM.
R. Bunescu and M. Pasca. 2006. Using encyclopedic
knowledge for named entity disambiguation. In Pro-
ceedings of EACL, volume 6, pages 9–16.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In Proceedings of
EMNLP-CoNLL, volume 2007, pages 708–716.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. The automatic
content extraction (ace) program–tasks, data, and eval-
uation. In Proceedings of LREC, volume 4, pages
837–840. Citeseer.
A. Fader, S. Soderland, and O. Etzioni. 2011. Identify-
ing relations for open information extraction. In Pro-
ceedings of Empirical Methods in Natural Language
Processing.
A. Haghighi and L. Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 362–370. Association for Computational Lin-
guistics.
R. Hoffmann, C. Zhang, and D.S. Weld. 2010. Learning
5000 relational extractors. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 286–295. Association for Computa-
tional Linguistics.
R. Hoffmann, C. Zhang, X. Ling, L. Zettlemoyer, and
D.S. Weld. 2011. Knowledge-based weak supervision
for information extraction of overlapping relations. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 541–550. Asso-
ciation for Computational Linguistics.
G. Kasneci, M. Ramanath, F. Suchanek, and G. Weikum.
2009. The yago-naga approach to knowledge discov-
ery. ACM SIGMOD Record, 37(4):41–47.
Z. Kozareva and E. Hovy. 2010. Learning arguments
and supertypes of semantic relations using recursive
patterns. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 1482–1491. Association for Computational Lin-
guistics.
C. Kwok, O. Etzioni, and D.S. Weld. 2001. Scaling
question answering to the web. ACM Transactions on
Information Systems (TOIS), 19(3):242–262.
D. Li, S. Somasundaran, and A. Chakraborty. 2011. A
combination of topic models with max-margin learn-
ing for relation detection.
P. McNamee and H.T. Dang. 2009. Overview of the tac
2009 knowledge base population track. In Text Analy-
sis Conference (TAC).
D. Milne and I.H. Witten. 2008. Learning to link with
wikipedia. In Proceeding of the 17th ACM conference
on Information and knowledge management, pages
509–518. ACM.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Dis-
tant supervision for relation extraction without labeled
data. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 2-Volume 2, pages 1003–
1011. Association for Computational Linguistics.
J. Nivre. 2006. Inductive dependency parsing. In
Text, Speech and Language Technology, volume 34.
Springer Verlag.
M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Organizing and searching the world wide web
of facts-step one: the one-million fact extraction chal-
lenge. In Proceedings of the National Conference on
Artificial Intelligence, page 1400. Menlo Park, CA;
Cambridge, MA; London; AAAI Press; MIT Press;
1999.
F. Pereira, A. Rajaraman, S. Sarawagi, W. Tunstall-
Pedoe, G. Weikum, and A. Halevy. 2009. An-
swering web questions using structured data: dream
or reality? Proceedings of the VLDB Endowment,
2(2):1646–1646.
H. Poon and P. Domingos. 2009. Unsupervised seman-
tic parsing. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing: Volume 1-Volume 1, pages 1–10. Association for
Computational Linguistics.
D. Ravichandran and E. Hovy. 2002. Learning surface
text patterns for a question answering system. In Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, pages 41–47. Associa-
tion for Computational Linguistics.
S. Sekine. 2006. On-demand information extraction. In
Proceedings of the COLING/ACL on Main conference
poster sessions, pages 731–738. Association for Com-
putational Linguistics.
Beth M. Sundheim and Nancy A. Chinchor. 1993. Sur-
vey of the message understanding conferences. In
HLT’93.
</reference>
<page confidence="0.983642">
58
</page>
<reference confidence="0.999847448275862">
I. Titov and A. Klementiev. 2011. A bayesian model for
unsupervised semantic parsing. In The 49th Annual
Meeting of the Association for Computational Linguis-
tics.
C. Wang, J. Fan, A. Kalyanpur, and D. Gondek. 2011.
Relation extraction with relation topics. In Proceed-
ings of Empirical Methods in Natural Language Pro-
cessing.
Daniel S. Weld, Fei Wu, Eytan Adar, Saleema Amershi,
James Fogarty, Raphael Hoffmann, Kayur Patel, and
Michael Skinner. 2008. Intelligence in wikipedia. In
Proceedings of the 23rd national conference on Artifi-
cial intelligence, pages 1609–1614. AAAI Press.
F. Wu and D.S. Weld. 2010. Open information extraction
using wikipedia. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 118–127. Association for Computational
Linguistics.
L. Yao, A. Haghighi, S. Riedel, and A. McCallum. 2011.
Structured relation discovery using generative models.
In Empirical Methods in Natural Language Process-
ing (EMNLP).
A. Yates, M. Cafarella, M. Banko, O. Etzioni, M. Broad-
head, and S. Soderland. 2007. Textrunner: Open in-
formation extraction on the web. In Proceedings of
Human Language Technologies: The Annual Confer-
ence of the North American Chapter of the Association
for Computational Linguistics: Demonstrations, pages
25–26. Association for Computational Linguistics.
</reference>
<page confidence="0.999262">
59
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.061951">
<title confidence="0.573794">Pattern Learning for Relation Extraction with a Hierarchical Topic Model Enrique Alfonseca Katja Filippova Jean-Yves Google Brandschenkestrasse</title>
<address confidence="0.998502">8002 Zurich, Switzerland</address>
<affiliation confidence="0.910951">NLP &amp; IR Group,</affiliation>
<address confidence="0.692851">Juan del Rosal, 28040 Madrid, Spain</address>
<email confidence="0.991517">ggarrido@lsi.uned.es</email>
<abstract confidence="0.983373272727273">We describe the use of a hierarchical topic model for automatically identifying syntactic and lexical patterns that explicitly state ontological relations. We leverage distant supervision using relations from the knowledge base FreeBase, but do not require any manual heuristic nor manual seed list selections. Results show that the learned patterns can be used to extract new relations with good precision.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ACE</author>
</authors>
<title>The automatic content extraction projects.</title>
<date>2004</date>
<note>http://projects.ldc.upenn.edu/ace.</note>
<contexts>
<context position="1450" citStr="ACE, 2004" startWordPosition="210" endWordPosition="211">wledge bases is very useful for solving tasks such as Entity Disambiguation, Information Retrieval and Question Answering. The availability of high-coverage, generalpurpose knowledge bases enable the automatic identification and disambiguation of entities in text and its applications (Bunescu and Pasca, 2006; Cucerzan, 2007; McNamee and Dang, 2009; Kwok et al., 2001; Pasca et al., 2006; Weld et al., 2008; Pereira et al., 2009; Kasneci et al., 2009). Most early works in this area were designed for supervised Information Extraction competitions such as MUC (Sundheim and Chinchor, 1993) and ACE (ACE, 2004; Doddington et al., 2004; Li et al., 2011), which rely on the availability of annotated data. Open Information Extraction (Sekine, 2006; Banko et al., 2007; Bollegala et al., 2010) started as an effort to approach relation extraction in *Work done during an internship at Google Zurich. a completely unsupervised way, by learning regularities and patterns from the web. Two example systems implementing this paradigm are TEXTRUNNER (Yates et al., 2007) and REVERB (Fader et al., 2011). These systems do not need any manual data or rules, but the relational facts they extract are not immediately dis</context>
</contexts>
<marker>ACE, 2004</marker>
<rawString>ACE. 2004. The automatic content extraction projects. http://projects.ldc.upenn.edu/ace.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matt Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2007</date>
<booktitle>In IJCAI’07.</booktitle>
<contexts>
<context position="1606" citStr="Banko et al., 2007" startWordPosition="234" endWordPosition="237">-coverage, generalpurpose knowledge bases enable the automatic identification and disambiguation of entities in text and its applications (Bunescu and Pasca, 2006; Cucerzan, 2007; McNamee and Dang, 2009; Kwok et al., 2001; Pasca et al., 2006; Weld et al., 2008; Pereira et al., 2009; Kasneci et al., 2009). Most early works in this area were designed for supervised Information Extraction competitions such as MUC (Sundheim and Chinchor, 1993) and ACE (ACE, 2004; Doddington et al., 2004; Li et al., 2011), which rely on the availability of annotated data. Open Information Extraction (Sekine, 2006; Banko et al., 2007; Bollegala et al., 2010) started as an effort to approach relation extraction in *Work done during an internship at Google Zurich. a completely unsupervised way, by learning regularities and patterns from the web. Two example systems implementing this paradigm are TEXTRUNNER (Yates et al., 2007) and REVERB (Fader et al., 2011). These systems do not need any manual data or rules, but the relational facts they extract are not immediately disambiguated to entities and relations from a knowledge base. A different family of unsupervised methods for relation extraction is unsupervised semantic pars</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J. Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In IJCAI’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D T Bollegala</author>
<author>Y Matsuo</author>
<author>M Ishizuka</author>
</authors>
<title>Relational duality: Unsupervised extraction of semantic relations between entities on the web.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th international conference on World wide web,</booktitle>
<pages>151--160</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1631" citStr="Bollegala et al., 2010" startWordPosition="238" endWordPosition="241">rpose knowledge bases enable the automatic identification and disambiguation of entities in text and its applications (Bunescu and Pasca, 2006; Cucerzan, 2007; McNamee and Dang, 2009; Kwok et al., 2001; Pasca et al., 2006; Weld et al., 2008; Pereira et al., 2009; Kasneci et al., 2009). Most early works in this area were designed for supervised Information Extraction competitions such as MUC (Sundheim and Chinchor, 1993) and ACE (ACE, 2004; Doddington et al., 2004; Li et al., 2011), which rely on the availability of annotated data. Open Information Extraction (Sekine, 2006; Banko et al., 2007; Bollegala et al., 2010) started as an effort to approach relation extraction in *Work done during an internship at Google Zurich. a completely unsupervised way, by learning regularities and patterns from the web. Two example systems implementing this paradigm are TEXTRUNNER (Yates et al., 2007) and REVERB (Fader et al., 2011). These systems do not need any manual data or rules, but the relational facts they extract are not immediately disambiguated to entities and relations from a knowledge base. A different family of unsupervised methods for relation extraction is unsupervised semantic parsing, which aims at cluste</context>
</contexts>
<marker>Bollegala, Matsuo, Ishizuka, 2010</marker>
<rawString>D.T. Bollegala, Y. Matsuo, and M. Ishizuka. 2010. Relational duality: Unsupervised extraction of semantic relations between entities on the web. In Proceedings of the 19th international conference on World wide web, pages 151–160. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bunescu</author>
<author>M Pasca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<volume>6</volume>
<pages>9--16</pages>
<contexts>
<context position="1150" citStr="Bunescu and Pasca, 2006" startWordPosition="158" endWordPosition="161">using relations from the knowledge base FreeBase, but do not require any manual heuristic nor manual seed list selections. Results show that the learned patterns can be used to extract new relations with good precision. 1 Introduction The detection of relations between entities for the automatic population of knowledge bases is very useful for solving tasks such as Entity Disambiguation, Information Retrieval and Question Answering. The availability of high-coverage, generalpurpose knowledge bases enable the automatic identification and disambiguation of entities in text and its applications (Bunescu and Pasca, 2006; Cucerzan, 2007; McNamee and Dang, 2009; Kwok et al., 2001; Pasca et al., 2006; Weld et al., 2008; Pereira et al., 2009; Kasneci et al., 2009). Most early works in this area were designed for supervised Information Extraction competitions such as MUC (Sundheim and Chinchor, 1993) and ACE (ACE, 2004; Doddington et al., 2004; Li et al., 2011), which rely on the availability of annotated data. Open Information Extraction (Sekine, 2006; Banko et al., 2007; Bollegala et al., 2010) started as an effort to approach relation extraction in *Work done during an internship at Google Zurich. a completely</context>
</contexts>
<marker>Bunescu, Pasca, 2006</marker>
<rawString>R. Bunescu and M. Pasca. 2006. Using encyclopedic knowledge for named entity disambiguation. In Proceedings of EACL, volume 6, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on wikipedia data.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<volume>volume</volume>
<pages>708--716</pages>
<contexts>
<context position="1166" citStr="Cucerzan, 2007" startWordPosition="162" endWordPosition="163">knowledge base FreeBase, but do not require any manual heuristic nor manual seed list selections. Results show that the learned patterns can be used to extract new relations with good precision. 1 Introduction The detection of relations between entities for the automatic population of knowledge bases is very useful for solving tasks such as Entity Disambiguation, Information Retrieval and Question Answering. The availability of high-coverage, generalpurpose knowledge bases enable the automatic identification and disambiguation of entities in text and its applications (Bunescu and Pasca, 2006; Cucerzan, 2007; McNamee and Dang, 2009; Kwok et al., 2001; Pasca et al., 2006; Weld et al., 2008; Pereira et al., 2009; Kasneci et al., 2009). Most early works in this area were designed for supervised Information Extraction competitions such as MUC (Sundheim and Chinchor, 1993) and ACE (ACE, 2004; Doddington et al., 2004; Li et al., 2011), which rely on the availability of annotated data. Open Information Extraction (Sekine, 2006; Banko et al., 2007; Bollegala et al., 2010) started as an effort to approach relation extraction in *Work done during an internship at Google Zurich. a completely unsupervised wa</context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>S. Cucerzan. 2007. Large-scale named entity disambiguation based on wikipedia data. In Proceedings of EMNLP-CoNLL, volume 2007, pages 708–716.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
<author>A Mitchell</author>
<author>M Przybocki</author>
<author>L Ramshaw</author>
<author>S Strassel</author>
<author>R Weischedel</author>
</authors>
<title>The automatic content extraction (ace) program–tasks, data, and evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>4</volume>
<pages>837--840</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="1475" citStr="Doddington et al., 2004" startWordPosition="212" endWordPosition="215">s is very useful for solving tasks such as Entity Disambiguation, Information Retrieval and Question Answering. The availability of high-coverage, generalpurpose knowledge bases enable the automatic identification and disambiguation of entities in text and its applications (Bunescu and Pasca, 2006; Cucerzan, 2007; McNamee and Dang, 2009; Kwok et al., 2001; Pasca et al., 2006; Weld et al., 2008; Pereira et al., 2009; Kasneci et al., 2009). Most early works in this area were designed for supervised Information Extraction competitions such as MUC (Sundheim and Chinchor, 1993) and ACE (ACE, 2004; Doddington et al., 2004; Li et al., 2011), which rely on the availability of annotated data. Open Information Extraction (Sekine, 2006; Banko et al., 2007; Bollegala et al., 2010) started as an effort to approach relation extraction in *Work done during an internship at Google Zurich. a completely unsupervised way, by learning regularities and patterns from the web. Two example systems implementing this paradigm are TEXTRUNNER (Yates et al., 2007) and REVERB (Fader et al., 2011). These systems do not need any manual data or rules, but the relational facts they extract are not immediately disambiguated to entities an</context>
</contexts>
<marker>Doddington, Mitchell, Przybocki, Ramshaw, Strassel, Weischedel, 2004</marker>
<rawString>G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw, S. Strassel, and R. Weischedel. 2004. The automatic content extraction (ace) program–tasks, data, and evaluation. In Proceedings of LREC, volume 4, pages 837–840. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fader</author>
<author>S Soderland</author>
<author>O Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1935" citStr="Fader et al., 2011" startWordPosition="288" endWordPosition="291">his area were designed for supervised Information Extraction competitions such as MUC (Sundheim and Chinchor, 1993) and ACE (ACE, 2004; Doddington et al., 2004; Li et al., 2011), which rely on the availability of annotated data. Open Information Extraction (Sekine, 2006; Banko et al., 2007; Bollegala et al., 2010) started as an effort to approach relation extraction in *Work done during an internship at Google Zurich. a completely unsupervised way, by learning regularities and patterns from the web. Two example systems implementing this paradigm are TEXTRUNNER (Yates et al., 2007) and REVERB (Fader et al., 2011). These systems do not need any manual data or rules, but the relational facts they extract are not immediately disambiguated to entities and relations from a knowledge base. A different family of unsupervised methods for relation extraction is unsupervised semantic parsing, which aims at clustering entity mentions and relation surface forms, thus generating a semantic representation of the texts on which inference may be used. Some techniques that have been used are Markov Random Fields (Poon and Domingos, 2009) and Bayesian generative models (Titov and Klementiev, 2011). These are quite powe</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>A. Fader, S. Soderland, and O. Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>L Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>362--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8520" citStr="Haghighi and Vanderwende (2009)" startWordPosition="1377" endWordPosition="1380">ach entity pair from the knowledge base which is in relation r. The words in each document can be, for example, all the dependency paths that have been observed in the input textual corpus between the two related entities. Each document will contain some very generic paths (e.g. the two entities consecutive in the text) and some more specific paths. Generative model Once these collections are built, we use the generative model from Figure 2 to learn the probability that a dependency path is conveying some relation between the entities it connects. This model is very similar to the one used by Haghighi and Vanderwende (2009) in the context of text summarization. w (the observed variable) represents a pattern between two entities. The topic model OG captures general patterns that appear for all relations. OD captures patterns that are specific about a certain entity pair, but which are not generalizable across all pairs with the same relation. Finally OA contains the patterns that are observed across most pairs related with the same relation. OA is the topic model of interest for us. We use Gibbs sampling to estimate the different models from the source data. The topic assignments (for each pattern) that are the o</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>A. Haghighi and L. Vanderwende. 2009. Exploring content models for multi-document summarization. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 362–370. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hoffmann</author>
<author>C Zhang</author>
<author>D S Weld</author>
</authors>
<title>Learning 5000 relational extractors.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>286--295</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2930" citStr="Hoffmann et al., 2010" startWordPosition="440" endWordPosition="443">presentation of the texts on which inference may be used. Some techniques that have been used are Markov Random Fields (Poon and Domingos, 2009) and Bayesian generative models (Titov and Klementiev, 2011). These are quite powerful approaches but have very high computational requirements (cf. (Yao et al., 2011)). A good trade-off between fully supervised and fully unsupervised approaches is distant supervision, a semi-supervised procedure consisting of finding sentences that contain two entities whose relation we know, and using those sentences as training examples for a supervised classifier (Hoffmann et al., 2010; Wu and Weld, 2010; Hoffmann et al., 2011; Wang et al., 2011; Yao et al., 2011). A usual problem is that two related entities may co-occur in one sentence for many unrelated reasons. For example, Barack Obama is the president of the United States, but not every sentence including the two entities supports and states this relation. Much of the previous work uses heuristics, e.g. extracting sentences only from encyclopedic entries (Mintz et al., 54 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 54–59, Jeju, Republic of Korea, 8-14 July 2012. c�201</context>
</contexts>
<marker>Hoffmann, Zhang, Weld, 2010</marker>
<rawString>R. Hoffmann, C. Zhang, and D.S. Weld. 2010. Learning 5000 relational extractors. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 286–295. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hoffmann</author>
<author>C Zhang</author>
<author>X Ling</author>
<author>L Zettlemoyer</author>
<author>D S Weld</author>
</authors>
<title>Knowledge-based weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>541--550</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2972" citStr="Hoffmann et al., 2011" startWordPosition="448" endWordPosition="451">ce may be used. Some techniques that have been used are Markov Random Fields (Poon and Domingos, 2009) and Bayesian generative models (Titov and Klementiev, 2011). These are quite powerful approaches but have very high computational requirements (cf. (Yao et al., 2011)). A good trade-off between fully supervised and fully unsupervised approaches is distant supervision, a semi-supervised procedure consisting of finding sentences that contain two entities whose relation we know, and using those sentences as training examples for a supervised classifier (Hoffmann et al., 2010; Wu and Weld, 2010; Hoffmann et al., 2011; Wang et al., 2011; Yao et al., 2011). A usual problem is that two related entities may co-occur in one sentence for many unrelated reasons. For example, Barack Obama is the president of the United States, but not every sentence including the two entities supports and states this relation. Much of the previous work uses heuristics, e.g. extracting sentences only from encyclopedic entries (Mintz et al., 54 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 54–59, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistic</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>R. Hoffmann, C. Zhang, X. Ling, L. Zettlemoyer, and D.S. Weld. 2011. Knowledge-based weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 541–550. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Kasneci</author>
<author>M Ramanath</author>
<author>F Suchanek</author>
<author>G Weikum</author>
</authors>
<title>The yago-naga approach to knowledge discovery.</title>
<date>2009</date>
<journal>ACM SIGMOD Record,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="1293" citStr="Kasneci et al., 2009" startWordPosition="184" endWordPosition="187">learned patterns can be used to extract new relations with good precision. 1 Introduction The detection of relations between entities for the automatic population of knowledge bases is very useful for solving tasks such as Entity Disambiguation, Information Retrieval and Question Answering. The availability of high-coverage, generalpurpose knowledge bases enable the automatic identification and disambiguation of entities in text and its applications (Bunescu and Pasca, 2006; Cucerzan, 2007; McNamee and Dang, 2009; Kwok et al., 2001; Pasca et al., 2006; Weld et al., 2008; Pereira et al., 2009; Kasneci et al., 2009). Most early works in this area were designed for supervised Information Extraction competitions such as MUC (Sundheim and Chinchor, 1993) and ACE (ACE, 2004; Doddington et al., 2004; Li et al., 2011), which rely on the availability of annotated data. Open Information Extraction (Sekine, 2006; Banko et al., 2007; Bollegala et al., 2010) started as an effort to approach relation extraction in *Work done during an internship at Google Zurich. a completely unsupervised way, by learning regularities and patterns from the web. Two example systems implementing this paradigm are TEXTRUNNER (Yates et </context>
</contexts>
<marker>Kasneci, Ramanath, Suchanek, Weikum, 2009</marker>
<rawString>G. Kasneci, M. Ramanath, F. Suchanek, and G. Weikum. 2009. The yago-naga approach to knowledge discovery. ACM SIGMOD Record, 37(4):41–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Kozareva</author>
<author>E Hovy</author>
</authors>
<title>Learning arguments and supertypes of semantic relations using recursive patterns.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1482--1491</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4171" citStr="Kozareva and Hovy, 2010" startWordPosition="634" endWordPosition="637"> Computational Linguistics Person-parent (Liza Minneli, Judy Garland) ... (Achilles, Peleus) ... (...) Person-death place (Napoleon Bonaparte, Saint Helena) ... (Johann Christian Bach, London) ... (...) Person-birth place (Charles Darwin, Shrewsbury) ... (Anthony Daniels, Salisbury) ... (...) 2009; Hoffmann et al., 2011; Wang et al., 2011), or syntactic restrictions on the sentences and the entity mentions (Wu and Weld, 2010). These are usually defined manually and may need to be adapted to different languages and domains. Manually selected seeds can also be used (Ravichandran and Hovy, 2002; Kozareva and Hovy, 2010). The main contribution of this work is presenting a variant of distance supervision for relation extraction where we do not use heuristics in the selection of the training data. Instead, we use topic models to discriminate between the patterns that are expressing the relation and those that are ambiguous and can be applied across relations. In this way, high-precision extraction patterns can be learned without the need of any manual intervention. 2 Unsupervised relational pattern learning Similar to other distant supervision methods, our approach takes as input an existing knowledge base cont</context>
</contexts>
<marker>Kozareva, Hovy, 2010</marker>
<rawString>Z. Kozareva and E. Hovy. 2010. Learning arguments and supertypes of semantic relations using recursive patterns. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1482–1491. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Kwok</author>
<author>O Etzioni</author>
<author>D S Weld</author>
</authors>
<title>Scaling question answering to the web.</title>
<date>2001</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="1209" citStr="Kwok et al., 2001" startWordPosition="168" endWordPosition="171">ire any manual heuristic nor manual seed list selections. Results show that the learned patterns can be used to extract new relations with good precision. 1 Introduction The detection of relations between entities for the automatic population of knowledge bases is very useful for solving tasks such as Entity Disambiguation, Information Retrieval and Question Answering. The availability of high-coverage, generalpurpose knowledge bases enable the automatic identification and disambiguation of entities in text and its applications (Bunescu and Pasca, 2006; Cucerzan, 2007; McNamee and Dang, 2009; Kwok et al., 2001; Pasca et al., 2006; Weld et al., 2008; Pereira et al., 2009; Kasneci et al., 2009). Most early works in this area were designed for supervised Information Extraction competitions such as MUC (Sundheim and Chinchor, 1993) and ACE (ACE, 2004; Doddington et al., 2004; Li et al., 2011), which rely on the availability of annotated data. Open Information Extraction (Sekine, 2006; Banko et al., 2007; Bollegala et al., 2010) started as an effort to approach relation extraction in *Work done during an internship at Google Zurich. a completely unsupervised way, by learning regularities and patterns fr</context>
</contexts>
<marker>Kwok, Etzioni, Weld, 2001</marker>
<rawString>C. Kwok, O. Etzioni, and D.S. Weld. 2001. Scaling question answering to the web. ACM Transactions on Information Systems (TOIS), 19(3):242–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Li</author>
<author>S Somasundaran</author>
<author>A Chakraborty</author>
</authors>
<title>A combination of topic models with max-margin learning for relation detection.</title>
<date>2011</date>
<contexts>
<context position="1493" citStr="Li et al., 2011" startWordPosition="216" endWordPosition="219">ing tasks such as Entity Disambiguation, Information Retrieval and Question Answering. The availability of high-coverage, generalpurpose knowledge bases enable the automatic identification and disambiguation of entities in text and its applications (Bunescu and Pasca, 2006; Cucerzan, 2007; McNamee and Dang, 2009; Kwok et al., 2001; Pasca et al., 2006; Weld et al., 2008; Pereira et al., 2009; Kasneci et al., 2009). Most early works in this area were designed for supervised Information Extraction competitions such as MUC (Sundheim and Chinchor, 1993) and ACE (ACE, 2004; Doddington et al., 2004; Li et al., 2011), which rely on the availability of annotated data. Open Information Extraction (Sekine, 2006; Banko et al., 2007; Bollegala et al., 2010) started as an effort to approach relation extraction in *Work done during an internship at Google Zurich. a completely unsupervised way, by learning regularities and patterns from the web. Two example systems implementing this paradigm are TEXTRUNNER (Yates et al., 2007) and REVERB (Fader et al., 2011). These systems do not need any manual data or rules, but the relational facts they extract are not immediately disambiguated to entities and relations from a</context>
</contexts>
<marker>Li, Somasundaran, Chakraborty, 2011</marker>
<rawString>D. Li, S. Somasundaran, and A. Chakraborty. 2011. A combination of topic models with max-margin learning for relation detection.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P McNamee</author>
<author>H T Dang</author>
</authors>
<title>Overview of the tac 2009 knowledge base population track.</title>
<date>2009</date>
<booktitle>In Text Analysis Conference (TAC).</booktitle>
<contexts>
<context position="1190" citStr="McNamee and Dang, 2009" startWordPosition="164" endWordPosition="167">reeBase, but do not require any manual heuristic nor manual seed list selections. Results show that the learned patterns can be used to extract new relations with good precision. 1 Introduction The detection of relations between entities for the automatic population of knowledge bases is very useful for solving tasks such as Entity Disambiguation, Information Retrieval and Question Answering. The availability of high-coverage, generalpurpose knowledge bases enable the automatic identification and disambiguation of entities in text and its applications (Bunescu and Pasca, 2006; Cucerzan, 2007; McNamee and Dang, 2009; Kwok et al., 2001; Pasca et al., 2006; Weld et al., 2008; Pereira et al., 2009; Kasneci et al., 2009). Most early works in this area were designed for supervised Information Extraction competitions such as MUC (Sundheim and Chinchor, 1993) and ACE (ACE, 2004; Doddington et al., 2004; Li et al., 2011), which rely on the availability of annotated data. Open Information Extraction (Sekine, 2006; Banko et al., 2007; Bollegala et al., 2010) started as an effort to approach relation extraction in *Work done during an internship at Google Zurich. a completely unsupervised way, by learning regularit</context>
</contexts>
<marker>McNamee, Dang, 2009</marker>
<rawString>P. McNamee and H.T. Dang. 2009. Overview of the tac 2009 knowledge base population track. In Text Analysis Conference (TAC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Milne</author>
<author>I H Witten</author>
</authors>
<title>Learning to link with wikipedia.</title>
<date>2008</date>
<booktitle>In Proceeding of the 17th ACM conference on Information and knowledge management,</booktitle>
<pages>509--518</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="9740" citStr="Milne and Witten, 2008" startWordPosition="1584" endWordPosition="1587">e output of this process are used to estimate P(rlw): when we observe pattern w, the probability that it conveys relation r. 3 Experiments and results Settings We use Freebase as our knowledge base. It can be freely downloaded1. text corpus used contains 33 million English news articles that we downloaded between January 2004 and December 2011. A random sample of 3M of them is used for building the document collections on which to train the topic models, and the remaining 30M is used for testing. The corpus is preprocessed by identifying Freebase entity mentions, using an approach similar to (Milne and Witten, 2008), and parsing it with an inductive dependency parser (Nivre, 2006). From the three million training documents, a set of document collections (one per relation) has been generated, by considering the sentences that contain two entities which are related in FreeBase through any binary relation and restricting to high-frequency 200 relations. Two ways of extracting patterns have been used: (a) Syntactic, taking the dependency path between the two entities, and (b) Intertext, taking the text between the two. In both cases, a topic model has been trained to learn the probability of a relation given</context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>D. Milne and I.H. Witten. 2008. Learning to link with wikipedia. In Proceeding of the 17th ACM conference on Information and knowledge management, pages 509–518. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mintz</author>
<author>S Bills</author>
<author>R Snow</author>
<author>D Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>1003--1011</pages>
<institution>Association for Computational Linguistics.</institution>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 1003– 1011. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>Inductive dependency parsing.</title>
<date>2006</date>
<booktitle>In Text, Speech and Language Technology,</booktitle>
<volume>34</volume>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="9806" citStr="Nivre, 2006" startWordPosition="1596" endWordPosition="1597"> w, the probability that it conveys relation r. 3 Experiments and results Settings We use Freebase as our knowledge base. It can be freely downloaded1. text corpus used contains 33 million English news articles that we downloaded between January 2004 and December 2011. A random sample of 3M of them is used for building the document collections on which to train the topic models, and the remaining 30M is used for testing. The corpus is preprocessed by identifying Freebase entity mentions, using an approach similar to (Milne and Witten, 2008), and parsing it with an inductive dependency parser (Nivre, 2006). From the three million training documents, a set of document collections (one per relation) has been generated, by considering the sentences that contain two entities which are related in FreeBase through any binary relation and restricting to high-frequency 200 relations. Two ways of extracting patterns have been used: (a) Syntactic, taking the dependency path between the two entities, and (b) Intertext, taking the text between the two. In both cases, a topic model has been trained to learn the probability of a relation given a pattern w: p(rlw). For A we use symmetric Dirichlet priors AG =</context>
</contexts>
<marker>Nivre, 2006</marker>
<rawString>J. Nivre. 2006. Inductive dependency parsing. In Text, Speech and Language Technology, volume 34. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pasca</author>
<author>D Lin</author>
<author>J Bigham</author>
<author>A Lifchits</author>
<author>A Jain</author>
</authors>
<title>Organizing and searching the world wide web of facts-step one: the one-million fact extraction challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<pages>1400</pages>
<publisher>AAAI Press; MIT Press;</publisher>
<location>Menlo Park, CA; Cambridge, MA; London;</location>
<contexts>
<context position="1229" citStr="Pasca et al., 2006" startWordPosition="172" endWordPosition="175">istic nor manual seed list selections. Results show that the learned patterns can be used to extract new relations with good precision. 1 Introduction The detection of relations between entities for the automatic population of knowledge bases is very useful for solving tasks such as Entity Disambiguation, Information Retrieval and Question Answering. The availability of high-coverage, generalpurpose knowledge bases enable the automatic identification and disambiguation of entities in text and its applications (Bunescu and Pasca, 2006; Cucerzan, 2007; McNamee and Dang, 2009; Kwok et al., 2001; Pasca et al., 2006; Weld et al., 2008; Pereira et al., 2009; Kasneci et al., 2009). Most early works in this area were designed for supervised Information Extraction competitions such as MUC (Sundheim and Chinchor, 1993) and ACE (ACE, 2004; Doddington et al., 2004; Li et al., 2011), which rely on the availability of annotated data. Open Information Extraction (Sekine, 2006; Banko et al., 2007; Bollegala et al., 2010) started as an effort to approach relation extraction in *Work done during an internship at Google Zurich. a completely unsupervised way, by learning regularities and patterns from the web. Two exam</context>
</contexts>
<marker>Pasca, Lin, Bigham, Lifchits, Jain, 2006</marker>
<rawString>M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain. 2006. Organizing and searching the world wide web of facts-step one: the one-million fact extraction challenge. In Proceedings of the National Conference on Artificial Intelligence, page 1400. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>A Rajaraman</author>
<author>S Sarawagi</author>
<author>W TunstallPedoe</author>
<author>G Weikum</author>
<author>A Halevy</author>
</authors>
<title>Answering web questions using structured data: dream or reality?</title>
<date>2009</date>
<booktitle>Proceedings of the VLDB Endowment,</booktitle>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="1270" citStr="Pereira et al., 2009" startWordPosition="180" endWordPosition="183">Results show that the learned patterns can be used to extract new relations with good precision. 1 Introduction The detection of relations between entities for the automatic population of knowledge bases is very useful for solving tasks such as Entity Disambiguation, Information Retrieval and Question Answering. The availability of high-coverage, generalpurpose knowledge bases enable the automatic identification and disambiguation of entities in text and its applications (Bunescu and Pasca, 2006; Cucerzan, 2007; McNamee and Dang, 2009; Kwok et al., 2001; Pasca et al., 2006; Weld et al., 2008; Pereira et al., 2009; Kasneci et al., 2009). Most early works in this area were designed for supervised Information Extraction competitions such as MUC (Sundheim and Chinchor, 1993) and ACE (ACE, 2004; Doddington et al., 2004; Li et al., 2011), which rely on the availability of annotated data. Open Information Extraction (Sekine, 2006; Banko et al., 2007; Bollegala et al., 2010) started as an effort to approach relation extraction in *Work done during an internship at Google Zurich. a completely unsupervised way, by learning regularities and patterns from the web. Two example systems implementing this paradigm ar</context>
</contexts>
<marker>Pereira, Rajaraman, Sarawagi, TunstallPedoe, Weikum, Halevy, 2009</marker>
<rawString>F. Pereira, A. Rajaraman, S. Sarawagi, W. TunstallPedoe, G. Weikum, and A. Halevy. 2009. Answering web questions using structured data: dream or reality? Proceedings of the VLDB Endowment, 2(2):1646–1646.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Poon</author>
<author>P Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>1</volume>
<pages>1--10</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2453" citStr="Poon and Domingos, 2009" startWordPosition="368" endWordPosition="371">ple systems implementing this paradigm are TEXTRUNNER (Yates et al., 2007) and REVERB (Fader et al., 2011). These systems do not need any manual data or rules, but the relational facts they extract are not immediately disambiguated to entities and relations from a knowledge base. A different family of unsupervised methods for relation extraction is unsupervised semantic parsing, which aims at clustering entity mentions and relation surface forms, thus generating a semantic representation of the texts on which inference may be used. Some techniques that have been used are Markov Random Fields (Poon and Domingos, 2009) and Bayesian generative models (Titov and Klementiev, 2011). These are quite powerful approaches but have very high computational requirements (cf. (Yao et al., 2011)). A good trade-off between fully supervised and fully unsupervised approaches is distant supervision, a semi-supervised procedure consisting of finding sentences that contain two entities whose relation we know, and using those sentences as training examples for a supervised classifier (Hoffmann et al., 2010; Wu and Weld, 2010; Hoffmann et al., 2011; Wang et al., 2011; Yao et al., 2011). A usual problem is that two related entit</context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>H. Poon and P. Domingos. 2009. Unsupervised semantic parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 1–10. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>E Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>41--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4145" citStr="Ravichandran and Hovy, 2002" startWordPosition="630" endWordPosition="633"> 2012. c�2012 Association for Computational Linguistics Person-parent (Liza Minneli, Judy Garland) ... (Achilles, Peleus) ... (...) Person-death place (Napoleon Bonaparte, Saint Helena) ... (Johann Christian Bach, London) ... (...) Person-birth place (Charles Darwin, Shrewsbury) ... (Anthony Daniels, Salisbury) ... (...) 2009; Hoffmann et al., 2011; Wang et al., 2011), or syntactic restrictions on the sentences and the entity mentions (Wu and Weld, 2010). These are usually defined manually and may need to be adapted to different languages and domains. Manually selected seeds can also be used (Ravichandran and Hovy, 2002; Kozareva and Hovy, 2010). The main contribution of this work is presenting a variant of distance supervision for relation extraction where we do not use heuristics in the selection of the training data. Instead, we use topic models to discriminate between the patterns that are expressing the relation and those that are ambiguous and can be applied across relations. In this way, high-precision extraction patterns can be learned without the need of any manual intervention. 2 Unsupervised relational pattern learning Similar to other distant supervision methods, our approach takes as input an ex</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>D. Ravichandran and E. Hovy. 2002. Learning surface text patterns for a question answering system. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 41–47. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sekine</author>
</authors>
<title>On-demand information extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions,</booktitle>
<pages>731--738</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1586" citStr="Sekine, 2006" startWordPosition="232" endWordPosition="233">bility of high-coverage, generalpurpose knowledge bases enable the automatic identification and disambiguation of entities in text and its applications (Bunescu and Pasca, 2006; Cucerzan, 2007; McNamee and Dang, 2009; Kwok et al., 2001; Pasca et al., 2006; Weld et al., 2008; Pereira et al., 2009; Kasneci et al., 2009). Most early works in this area were designed for supervised Information Extraction competitions such as MUC (Sundheim and Chinchor, 1993) and ACE (ACE, 2004; Doddington et al., 2004; Li et al., 2011), which rely on the availability of annotated data. Open Information Extraction (Sekine, 2006; Banko et al., 2007; Bollegala et al., 2010) started as an effort to approach relation extraction in *Work done during an internship at Google Zurich. a completely unsupervised way, by learning regularities and patterns from the web. Two example systems implementing this paradigm are TEXTRUNNER (Yates et al., 2007) and REVERB (Fader et al., 2011). These systems do not need any manual data or rules, but the relational facts they extract are not immediately disambiguated to entities and relations from a knowledge base. A different family of unsupervised methods for relation extraction is unsupe</context>
</contexts>
<marker>Sekine, 2006</marker>
<rawString>S. Sekine. 2006. On-demand information extraction. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 731–738. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth M Sundheim</author>
<author>Nancy A Chinchor</author>
</authors>
<title>Survey of the message understanding conferences.</title>
<date>1993</date>
<booktitle>In HLT’93.</booktitle>
<contexts>
<context position="1431" citStr="Sundheim and Chinchor, 1993" startWordPosition="204" endWordPosition="207">es for the automatic population of knowledge bases is very useful for solving tasks such as Entity Disambiguation, Information Retrieval and Question Answering. The availability of high-coverage, generalpurpose knowledge bases enable the automatic identification and disambiguation of entities in text and its applications (Bunescu and Pasca, 2006; Cucerzan, 2007; McNamee and Dang, 2009; Kwok et al., 2001; Pasca et al., 2006; Weld et al., 2008; Pereira et al., 2009; Kasneci et al., 2009). Most early works in this area were designed for supervised Information Extraction competitions such as MUC (Sundheim and Chinchor, 1993) and ACE (ACE, 2004; Doddington et al., 2004; Li et al., 2011), which rely on the availability of annotated data. Open Information Extraction (Sekine, 2006; Banko et al., 2007; Bollegala et al., 2010) started as an effort to approach relation extraction in *Work done during an internship at Google Zurich. a completely unsupervised way, by learning regularities and patterns from the web. Two example systems implementing this paradigm are TEXTRUNNER (Yates et al., 2007) and REVERB (Fader et al., 2011). These systems do not need any manual data or rules, but the relational facts they extract are </context>
</contexts>
<marker>Sundheim, Chinchor, 1993</marker>
<rawString>Beth M. Sundheim and Nancy A. Chinchor. 1993. Survey of the message understanding conferences. In HLT’93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>A Klementiev</author>
</authors>
<title>A bayesian model for unsupervised semantic parsing.</title>
<date>2011</date>
<booktitle>In The 49th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2513" citStr="Titov and Klementiev, 2011" startWordPosition="376" endWordPosition="380">tes et al., 2007) and REVERB (Fader et al., 2011). These systems do not need any manual data or rules, but the relational facts they extract are not immediately disambiguated to entities and relations from a knowledge base. A different family of unsupervised methods for relation extraction is unsupervised semantic parsing, which aims at clustering entity mentions and relation surface forms, thus generating a semantic representation of the texts on which inference may be used. Some techniques that have been used are Markov Random Fields (Poon and Domingos, 2009) and Bayesian generative models (Titov and Klementiev, 2011). These are quite powerful approaches but have very high computational requirements (cf. (Yao et al., 2011)). A good trade-off between fully supervised and fully unsupervised approaches is distant supervision, a semi-supervised procedure consisting of finding sentences that contain two entities whose relation we know, and using those sentences as training examples for a supervised classifier (Hoffmann et al., 2010; Wu and Weld, 2010; Hoffmann et al., 2011; Wang et al., 2011; Yao et al., 2011). A usual problem is that two related entities may co-occur in one sentence for many unrelated reasons.</context>
</contexts>
<marker>Titov, Klementiev, 2011</marker>
<rawString>I. Titov and A. Klementiev. 2011. A bayesian model for unsupervised semantic parsing. In The 49th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wang</author>
<author>J Fan</author>
<author>A Kalyanpur</author>
<author>D Gondek</author>
</authors>
<title>Relation extraction with relation topics.</title>
<date>2011</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2991" citStr="Wang et al., 2011" startWordPosition="452" endWordPosition="455">chniques that have been used are Markov Random Fields (Poon and Domingos, 2009) and Bayesian generative models (Titov and Klementiev, 2011). These are quite powerful approaches but have very high computational requirements (cf. (Yao et al., 2011)). A good trade-off between fully supervised and fully unsupervised approaches is distant supervision, a semi-supervised procedure consisting of finding sentences that contain two entities whose relation we know, and using those sentences as training examples for a supervised classifier (Hoffmann et al., 2010; Wu and Weld, 2010; Hoffmann et al., 2011; Wang et al., 2011; Yao et al., 2011). A usual problem is that two related entities may co-occur in one sentence for many unrelated reasons. For example, Barack Obama is the president of the United States, but not every sentence including the two entities supports and states this relation. Much of the previous work uses heuristics, e.g. extracting sentences only from encyclopedic entries (Mintz et al., 54 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 54–59, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Person-parent (Li</context>
</contexts>
<marker>Wang, Fan, Kalyanpur, Gondek, 2011</marker>
<rawString>C. Wang, J. Fan, A. Kalyanpur, and D. Gondek. 2011. Relation extraction with relation topics. In Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel S Weld</author>
<author>Fei Wu</author>
<author>Eytan Adar</author>
<author>Saleema Amershi</author>
<author>James Fogarty</author>
<author>Raphael Hoffmann</author>
<author>Kayur Patel</author>
<author>Michael Skinner</author>
</authors>
<title>Intelligence in wikipedia.</title>
<date>2008</date>
<booktitle>In Proceedings of the 23rd national conference on Artificial intelligence,</booktitle>
<pages>1609--1614</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="1248" citStr="Weld et al., 2008" startWordPosition="176" endWordPosition="179">d list selections. Results show that the learned patterns can be used to extract new relations with good precision. 1 Introduction The detection of relations between entities for the automatic population of knowledge bases is very useful for solving tasks such as Entity Disambiguation, Information Retrieval and Question Answering. The availability of high-coverage, generalpurpose knowledge bases enable the automatic identification and disambiguation of entities in text and its applications (Bunescu and Pasca, 2006; Cucerzan, 2007; McNamee and Dang, 2009; Kwok et al., 2001; Pasca et al., 2006; Weld et al., 2008; Pereira et al., 2009; Kasneci et al., 2009). Most early works in this area were designed for supervised Information Extraction competitions such as MUC (Sundheim and Chinchor, 1993) and ACE (ACE, 2004; Doddington et al., 2004; Li et al., 2011), which rely on the availability of annotated data. Open Information Extraction (Sekine, 2006; Banko et al., 2007; Bollegala et al., 2010) started as an effort to approach relation extraction in *Work done during an internship at Google Zurich. a completely unsupervised way, by learning regularities and patterns from the web. Two example systems impleme</context>
</contexts>
<marker>Weld, Wu, Adar, Amershi, Fogarty, Hoffmann, Patel, Skinner, 2008</marker>
<rawString>Daniel S. Weld, Fei Wu, Eytan Adar, Saleema Amershi, James Fogarty, Raphael Hoffmann, Kayur Patel, and Michael Skinner. 2008. Intelligence in wikipedia. In Proceedings of the 23rd national conference on Artificial intelligence, pages 1609–1614. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wu</author>
<author>D S Weld</author>
</authors>
<title>Open information extraction using wikipedia.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>118--127</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2949" citStr="Wu and Weld, 2010" startWordPosition="444" endWordPosition="447">ts on which inference may be used. Some techniques that have been used are Markov Random Fields (Poon and Domingos, 2009) and Bayesian generative models (Titov and Klementiev, 2011). These are quite powerful approaches but have very high computational requirements (cf. (Yao et al., 2011)). A good trade-off between fully supervised and fully unsupervised approaches is distant supervision, a semi-supervised procedure consisting of finding sentences that contain two entities whose relation we know, and using those sentences as training examples for a supervised classifier (Hoffmann et al., 2010; Wu and Weld, 2010; Hoffmann et al., 2011; Wang et al., 2011; Yao et al., 2011). A usual problem is that two related entities may co-occur in one sentence for many unrelated reasons. For example, Barack Obama is the president of the United States, but not every sentence including the two entities supports and states this relation. Much of the previous work uses heuristics, e.g. extracting sentences only from encyclopedic entries (Mintz et al., 54 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 54–59, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for C</context>
</contexts>
<marker>Wu, Weld, 2010</marker>
<rawString>F. Wu and D.S. Weld. 2010. Open information extraction using wikipedia. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 118–127. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Yao</author>
<author>A Haghighi</author>
<author>S Riedel</author>
<author>A McCallum</author>
</authors>
<title>Structured relation discovery using generative models.</title>
<date>2011</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="2620" citStr="Yao et al., 2011" startWordPosition="393" endWordPosition="396">onal facts they extract are not immediately disambiguated to entities and relations from a knowledge base. A different family of unsupervised methods for relation extraction is unsupervised semantic parsing, which aims at clustering entity mentions and relation surface forms, thus generating a semantic representation of the texts on which inference may be used. Some techniques that have been used are Markov Random Fields (Poon and Domingos, 2009) and Bayesian generative models (Titov and Klementiev, 2011). These are quite powerful approaches but have very high computational requirements (cf. (Yao et al., 2011)). A good trade-off between fully supervised and fully unsupervised approaches is distant supervision, a semi-supervised procedure consisting of finding sentences that contain two entities whose relation we know, and using those sentences as training examples for a supervised classifier (Hoffmann et al., 2010; Wu and Weld, 2010; Hoffmann et al., 2011; Wang et al., 2011; Yao et al., 2011). A usual problem is that two related entities may co-occur in one sentence for many unrelated reasons. For example, Barack Obama is the president of the United States, but not every sentence including the two </context>
</contexts>
<marker>Yao, Haghighi, Riedel, McCallum, 2011</marker>
<rawString>L. Yao, A. Haghighi, S. Riedel, and A. McCallum. 2011. Structured relation discovery using generative models. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yates</author>
<author>M Cafarella</author>
<author>M Banko</author>
<author>O Etzioni</author>
<author>M Broadhead</author>
<author>S Soderland</author>
</authors>
<title>Textrunner: Open information extraction on the web.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,</booktitle>
<pages>25--26</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1903" citStr="Yates et al., 2007" startWordPosition="282" endWordPosition="285">l., 2009). Most early works in this area were designed for supervised Information Extraction competitions such as MUC (Sundheim and Chinchor, 1993) and ACE (ACE, 2004; Doddington et al., 2004; Li et al., 2011), which rely on the availability of annotated data. Open Information Extraction (Sekine, 2006; Banko et al., 2007; Bollegala et al., 2010) started as an effort to approach relation extraction in *Work done during an internship at Google Zurich. a completely unsupervised way, by learning regularities and patterns from the web. Two example systems implementing this paradigm are TEXTRUNNER (Yates et al., 2007) and REVERB (Fader et al., 2011). These systems do not need any manual data or rules, but the relational facts they extract are not immediately disambiguated to entities and relations from a knowledge base. A different family of unsupervised methods for relation extraction is unsupervised semantic parsing, which aims at clustering entity mentions and relation surface forms, thus generating a semantic representation of the texts on which inference may be used. Some techniques that have been used are Markov Random Fields (Poon and Domingos, 2009) and Bayesian generative models (Titov and Klement</context>
</contexts>
<marker>Yates, Cafarella, Banko, Etzioni, Broadhead, Soderland, 2007</marker>
<rawString>A. Yates, M. Cafarella, M. Banko, O. Etzioni, M. Broadhead, and S. Soderland. 2007. Textrunner: Open information extraction on the web. In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, pages 25–26. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>