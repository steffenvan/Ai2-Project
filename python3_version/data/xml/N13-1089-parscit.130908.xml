<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000351">
<title confidence="0.9987475">
Improving Lexical Semantics for Sentential Semantics: Modeling Selectional
Preference and Similar Words in a Latent Variable Model
</title>
<author confidence="0.998049">
Weiwei Guo
</author>
<affiliation confidence="0.9981975">
Department of Computer Science
Columbia University
</affiliation>
<email confidence="0.992674">
weiwei@cs.columbia.edu
</email>
<author confidence="0.992715">
Mona Diab
</author>
<affiliation confidence="0.9879855">
Department of Computer Science
George Washington University
</affiliation>
<email confidence="0.997279">
mtdiab@gwu.edu
</email>
<sectionHeader confidence="0.997446" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996419294117647">
Sentence Similarity [SS] computes a similar-
ity score between two sentences. The SS task
differs from document level semantics tasks
in that it features the sparsity of words in a
data unit, i.e. a sentence. Accordingly it is
crucial to robustly model each word in a sen-
tence to capture the complete semantic picture
of the sentence. In this paper, we hypoth-
esize that by better modeling lexical seman-
tics we can obtain better sentential semantics.
We incorporate both corpus-based (selectional
preference information) and knowledge-based
(similar words extracted in a dictionary) lex-
ical semantics into a latent variable model.
The experiments show state-of-the-art perfor-
mance among unsupervised systems on two
SS datasets.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999609403846154">
Sentence Similarity [SS] is emerging as a crucial
step in many NLP tasks that focus on sentence level
semantics such as word sense disambiguation (Guo
and Diab, 2010; Guo and Diab, 2012a), summariza-
tion (Zhou et al., 2006), text coherence (Lapata and
Barzilay, 2005), tweet clustering (Sankaranarayanan
et al., 2009; Jin et al., 2011), etc. SS operates in a
very small context, on average 11 words per sen-
tence in Semeval-2012 dataset (Agirre et al., 2012),
resulting in inadequate evidence to generalize to ro-
bust sentential semantics.
Weighted Textual Matrix Factorization [WTMF]
(Guo and Diab, 2012b) is a latent variable model that
outperforms Latent Semantic Analysis [LSA] (Deer-
wester et al., 1990) and Latent Dirichelet Allocation
[LDA] (Blei et al., 2003) models by a large margin in
the SS task, yielding state-of-the-art performance on
the LI06 (Li et al., 2006) SS dataset. However, all of
these models make harsh simplifying assumptions
on how a token is generated: (1) in LSA/WTMF, a
token is generated by the inner product of the word
latent vector and the document latent vector; (2) in
LDA, all the tokens in a document are sampled from
the same document level topic distribution. Under
this framework, they ignore rich linguistic phenom-
ena such as inter-word dependency, semantic scope
of words, etc. This is a result of simply using docu-
ment IDs as features to represent a word.
Modeling quality lexical semantics in latent vari-
able models does not draw enough attention in the
community, since people usually apply dimension
reduction techniques for documents, which have
abundant words for extracting the document level
semantics. However, in the SS setting, it is crucial to
make good use of each word, given the limited num-
ber of words in a sentence. We believe a reasonable
word generation story will avoid introducing noise
in sentential semantics, encouraging robust lexical
semantics which can further boost the sentential se-
mantics. In this paper, we explicitly encode lexical
semantics, both corpus-based and knowledge-based
information, in the WTMF model, by which we are
able to achieve even better results in SS task.
The additional corpus-based information we ex-
ploit is selectional preference semantics (Resnik,
1997), a feature already existing in the data yet ig-
nored by most latent variable models. Selectional
preference focuses on the admissible arguments for
a word, thus capturing more nuanced semantics than
the sentence IDs (when applied to a corpus of sen-
tences as opposed to documents). Consider the fol-
lowing example:
</bodyText>
<page confidence="0.983661">
739
</page>
<affiliation confidence="0.234063">
Proceedings of NAACL-HLT 2013, pages 739–745,
</affiliation>
<page confidence="0.321242">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</page>
<figureCaption confidence="0.99941">
Figure 1: matrix factorization
</figureCaption>
<bodyText confidence="0.979979054054054">
Many analysts say the global Brent crude oil bench-
mark price, currently around $111 a barrel ...
In WTMF/LSA/LDA, a word will receive semantics
from all the other words in a sentence, hence, the
word oil, in the above example, will be assigned the
incorrect finance topic that reflects the sentence level
semantics. Moreover, the problem worsens for ad-
jectives, adverbs and verbs, which have a much nar-
rower semantic scope than the whole sentence. For
example, the verb say should only be associated with
analyst (only receiving semantics from analyst), as
it is not related to other words in the sentence. In
contrast, oil, according to its selectional preference,
should be associated with crude indicating the re-
source topic. We believe modeling selectional pref-
erence capturing local evidence completes the se-
mantic picture for words, hence further rendering
better sentential semantics. To our best knowledge,
this is the first work to model selectional preference
for sentence/document semantics.
We also integrate knowledge-based semantics
in the WTMF framework. Knowledge-based se-
mantics, a human-annotated clean resource, is an
important complement to corpus-based noisy co-
occurrence information. We extract similar word
pairs from Wordnet (Fellbaum, 1998). Leveraging
these pairs, an infrequent word such as purchase
can exploit robust latent vectors from its synonyms
such as buy. Similar words pairs can be seamlessly
modeled in WTMF, since in the matrix factorization
framework a latent vector profile is explicitly created
for each word, while in LDA all the data structures
are designed for documents/sentences. We construct
a graph to connect words according to the extracted
similar word pairs, to encourage similar words to
share similar latent vector profiles. We will refer to
our proposed novel model as WTMF+PK.
</bodyText>
<sectionHeader confidence="0.96327" genericHeader="introduction">
2 Weighted Textual Matrix Factorization
</sectionHeader>
<bodyText confidence="0.9912435">
Our previous work (Guo and Diab, 2012b) models
the sentences in the weighted matrix factorization
</bodyText>
<listItem confidence="0.6393756">
framework (Figure 1). The corpus is stored in an
M x N matrix X, with each cell containing the TF-
IDF values of words. The rows of X are M distinct
words and columns are N sentences. As in Figure
1, X is approximated by the product of a K x M
matrix P and a K x N matrix Q. Accordingly, each
sentence sj is represented by a K dimensional la-
tent vector Q·,j. Similarly a word wi is generalized
by P·,i. P and Q is optimized by minimize the ob-
jective function:
</listItem>
<equation confidence="0.9892925">
� EWij (P·,i · ` ·,j − Xij)2 + A||P||22 + A||` ||22
i j
Wi,j = J 1, if Xij # 0 (1)
l wM, if Xij = 0
</equation>
<bodyText confidence="0.993971">
where A is a regularization term. Missing tokens are
modeled by assigning a different weight wm for each
0 cell in the matrix X. We can see the inner product
of a word vector P·,i and a sentence vector Q·,j is
used to approximate the cell Xij.
The graphical model of WTMF is illustrated in
Figure 2a. A wi/sj node is a latent vector P·,i/Q·,j,
corresponding to a word/sentence, respectively. A
shaded node is a non-zero cell in X, representing
an observed token in a sentence. For simplicity, the
missing tokens and weights are not shown in the
graph.
</bodyText>
<sectionHeader confidence="0.9315905" genericHeader="method">
3 Corpus-based Semantics: Selectional
Preference
</sectionHeader>
<bodyText confidence="0.9999813125">
In this paper, we focus on selectional preference that
reflects the association of two words: if two words
form a bigram, then the two words should share
similar latent dimensions. In the previous example,
crude and oil form a bigram, and they share the re-
source topic. In our framework, this is implemented
by adding extra columns in X, so that each addi-
tional column corresponds to a bigram, treating each
bigram as a pseudo-sentence for the two words. The
graphical model is illustrated in Figure 2b. There-
fore, oil will receive more resource topic from crude
through the bigram crude oil, instead of only finance
topic from the sentence as a whole.
Each non-zero cell in the new columns of X, i.e.
an observed token in a bigram (pseudo-sentence), is
given a different weight:
</bodyText>
<equation confidence="0.966067714285714">
1, if Xij =6 0 and j is a sentence index
-y · freq(j), if Xij =6 0 and j is a bigram index
WM, if Xij = 0
⎧
⎨
⎩
Wi,j =
</equation>
<page confidence="0.975874">
740
</page>
<figureCaption confidence="0.987482">
Figure 2: WTMF+PK model (WTMF + corpus-based Selectional [P]references semantics + [K]nowledge-based
semantics): a w/s/b node represents a word/sentence/bigram, respectively
</figureCaption>
<figure confidence="0.995835333333333">
S, S2
S, S2
W, W2 W3 WO W�
W, W2 W3 WO W�
b,
(a) (b)
b2
W, W2 W3 WO W�
b,
S, S2
(C)
b2
</figure>
<bodyText confidence="0.999597714285714">
freq(j) denotes the frequency of bigram j appear-
ing in the corpus, hence the strength of association is
differentiated such that higher weights are assigned
on the more probable bigrams. The coefficient -y is
the importance of selectional preference. A larger
-y indicates that we trust the selectional preference
over the global sentential semantics.
</bodyText>
<sectionHeader confidence="0.976257" genericHeader="method">
4 Knowledge-based Semantics: Similar
</sectionHeader>
<subsectionHeader confidence="0.552044">
Word Pairs
</subsectionHeader>
<bodyText confidence="0.9999901875">
We first extract synonym pairs from WordNet, which
are words associated with the same sense, synset.
We further expand the set by exploiting the relations
defined in WordNet. For the extracted words, we
consider the first sense of each word, and if it is con-
nected to other senses by any of the WordNet defined
relations (hypernym, similar words, etc.), then we
treat the words associated with the other senses as
similar words. In total, we are able to discover 80K
pairs of similar words for the 46K distinct words in
our corpus.
Given a pair of similar words wi,/wi2, we want
the two corresponding latent vectors P·,i,/P·,i2 to be
as close as possible, namely the cosine similarity to
be close to 1. Accordingly, a term is added in equa-
tion 1 for each similar word pair wi,/wi2:
</bodyText>
<equation confidence="0.989685">
δ · (P·,i1 · P·,i2 − |P·,i1||P·,i2|)2 (2)
</equation>
<bodyText confidence="0.99995875">
|P·,i |denotes the length of the vector P·,i. The co-
efficient 6, analogous to -y, denotes the importance
of the knowledge-based evidence. The Figure 2c
shows the final WTMF+PK model.
</bodyText>
<sectionHeader confidence="0.999921" genericHeader="method">
5 Inference
</sectionHeader>
<bodyText confidence="0.999913111111111">
In (Guo and Diab, 2012b) we use Alternating Least
Square [ALS] for inference, which is to set the
derivative of equation 1 for P/Q to 0 and iteratively
compute P/Q by fixing the other matrix (Srebro and
Jaakkola, 2003). However, it is no longer applicable
with the new term (equation 2) involving the length
of word vectors |P·,i|. Therefore we approximate the
objective function by treating the vector length |P·,i|
as fixed values during the ALS iterations:
</bodyText>
<equation confidence="0.9980616">
( )−,
Q·,j = P W� (j)P&gt; + λI P W�(j)X·,j
P·,i = (Q W�(i)Q&gt; + λI + δP·,s(i)P· y(i))−
( )
Q W� (i)X&gt; i,· + δLiP·,s(i)Ls(i)
</equation>
<bodyText confidence="0.9994626">
where P·,s(i) are the latent vectors of similar words
of word i; the length of these vectors in the current
iteration are stored in Ls(i) (similarly Li is the cur-
rent length of P·,i) (cf. (Steck, 2010; Guo and Diab,
2012b) for optimization details).
</bodyText>
<sectionHeader confidence="0.998576" genericHeader="method">
6 Experimental Setting
</sectionHeader>
<bodyText confidence="0.999887133333333">
We build the model WTMF+PK on the same cor-
pora as used in our previous work (Guo and Diab,
2012b), comprising the following: Brown corpus
(each sentence is treated as a document), sense def-
initions from Wiktionary and Wordnet (only defini-
tions without target words and usage examples). We
follow the preprocessing steps in (Guo and Diab,
2012c): tokenization, pos-tagging, lemmatization
and further merge lemmas. The corpus is used for
building matrix X.
The evaluation datasets are LI06 dataset and
Semeval-2012 STS [STS12] (Agirre et al., 2012)
dataset. LI06 consists of 30 sentence pairs (dic-
tionary definitions). For STS12,1 the training data
(2000 pairs) are used as the tuning set for setting the
</bodyText>
<footnote confidence="0.9214695">
1A detailed description of the data sets is provided in (Agirre
et al., 2012).
</footnote>
<figure confidence="0.335861">
(3)
</figure>
<page confidence="0.97707">
741
</page>
<bodyText confidence="0.999975888888889">
parameters of our models. This data comprises msr-
par, msr-vid, smt-eur. Once the models are tuned,
we evaluate them on the STS12 test data that com-
prises 3150 sentence pairs from msr-par, msr-vid,
smt-eur, smt-news, On-WN. It is worth noting that
smt-news and On-WN are not part of the tuning data.
We use cosine similarity to measure the similarity
scores between two sentences. Pearson correlation
between the system’s answer and gold standard sim-
ilarity scores is used as the evaluation metric.
We include three baselines LSA, LDA and
WTMF using the setting described in (Guo and
Diab, 2012b). We run Gibbs Sampling based LDA
for 2000 iterations and average the model over the
last 10 iterations. For WTMF, we run 20 iterations
and fix the missing words weight at wm = 0.01 with
a regularization coefficient set at A = 20, which is
the best condition found in (Guo and Diab, 2012b).
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.999915076923077">
Table 1 summarizes the results at dimension K =
100 (the dimension of latent vectors). To remove
randomness, each reported number is the averaged
results of 10 runs. Based on the STS tuning set,
we experiment with different values for the selec-
tional preference weight (&apos;y = {0, 1, 2}), and like-
wise for the similar word pairs weight varying the 6
value as follows 6 = {0, 0.1, 0.3, 0.5, 0.7}. The per-
formance on STS12 tuning and test dataset as well
as on the LI06 dataset are illustrated in Figures 3a,
3b and 3d. The parameters of model 6 in Table 1
(&apos;y = 2, 6 = 0.3) are the chosen values based on
tuning set performance.
</bodyText>
<subsectionHeader confidence="0.996512">
7.1 Evaluation on the STS12 datasets
</subsectionHeader>
<bodyText confidence="0.979547696428572">
Table 1 shows WTMF is already a very strong base-
line: it outperforms LSA and LDA by a large mar-
gin. Same as in (Guo and Diab, 2012b), LSA per-
formance degrades dramatically when trained on a
corpus of sentence sized documents, yielding results
worse than the surface words baseline 31% (Agirre
et al., 2012). Using corpus-based selectional prefer-
ence semantics alone (model 4 WTMF+P in Table
1) boosts the performance of WTMF by +1.17% on
the test set, while using knowledge-based semantics
alone (model 5 WTMF+K) improves the over the
WTMF results by an absolute +2.31%. Combining
them (model 6 WTMF+PK) yields the best results,
with an absolute increase of +3.39%, which sug-
gests that the two sources of semantic evidence are
useful, but more importantly, they are complemen-
tary for each other.
Table 1 also presents the performance on each in-
dividual dataset. The gain on each individual source
is not as much as the overall gain, which suggests
part of the overall gain comes from the correct rank-
ing of intra-source pairs. Note that WTMF+PK im-
proves all individual datasets except smt-eur. This
may be caused by too many overlapping words in
the sentence pairs in smt-eur, while our approach
focuses on extracting similarity between different
words.
Observing the performance using different values
of weights in figure 3a and 3b, we can conclude
that the selectional preference and similar word pairs
yield very promising results. The trends hold in
different parameter conditions with a consistent im-
provement. Figure 3c illustrates the impact of di-
mension K = {50, 75,100,125,150} on WTMF
and WTMF+PK. Generally a larger K leads to a
higher Pearson correlation, but the improvement is
tiny when K ≥ 100 (0.1% increase).
Compared to all the unsupervised systems that
participated in Semeval STS 2012 task, WTMF+PK
yields state-of-the-art performance (70.70%).2 In
(Guo and Diab, 2012c) we also apply WTMF (K =
100) on STS12, achieving a correlation of 69.5%.
However, additional data is incorporated in the train-
ing corpora: (1) STS12 tuning set; (2) for WordNet
and Wiktionary data, the target words are also in-
cluded in the definitions (hence synonym pairs were
used); (3) the usage examples of target words were
also appended to the definitions.3 While trained with
this experimental setting, our model WTMF+PK
(&apos;y = 2, 6 = 0.3, K = 100) is able to reach an even
higher correlation of 72.0%.
2WTMF+PK is an unsupervised system, since the gold stan-
dard similarly scores are never used in the objective function.
Moreover, even without a tuning set, a non-zero value of -y or S
will always improve the baseline WTMF according to figure 3a
and 3b.
</bodyText>
<footnote confidence="0.84658275">
3We do not adopt this corpora schema, since some defini-
tions are test set sentences in On-WN, thereby adding target
words and usage examples introduces additional information
for some of the test set sentences
</footnote>
<page confidence="0.986061">
742
</page>
<listItem confidence="0.860760571428571">
Models Parameters STS12 tune STS12 test msr-par msr-vid On-WN smt-eur smt-news LI06
1. LSA - 21.67% 24.41% 27.18% 9.91% 50.93% 27.86% 19.73% 63.77%
2. LDA α = 0.05, C3 = 0.05 71.10% 63.18% 29.15% 76.73% 62.81% 47.81% 27.2% 83.71%
3. WTMF - 71.41% 67.31% 44.00% 82.59% 70.78% 50.89% 37.77% 89.81%
4. WTMF+P -y = 2, 6 = 0 72.94% 68.48% 46.21% 83.29% 70.61% 49.54% 39.50% 90.16%
5. WTMF+K -y = 0, 6 = 0.3 73.84% 69.64% 45.04% 83.04% 70.40% 49.88% 41.66% 90.11%
6. WTMF+PK -y = 2, 6 = 0.3 75.29% 70.70% 46.77% 83.90% 71.03% 49.77% 40.48% 90.17%
</listItem>
<tableCaption confidence="0.998571">
Table 1: Evaluation Results using Pearson Correlation on STS12 and LI06
</tableCaption>
<figure confidence="0.993044052631579">
75
74
γ=0
γ=1
γ=2
73
72
71
0 0.1 0.3 0.5 0.7
71
68
67
0 0.1 0.3 0.5 0.7
70
69
71
70
69
WFMF
WFMF+PK, γ=2, δ=0.3
67
66
50 75 100 125 150
91
r=0
r=1
r=2
90.5
90
89.5
0 0.1 0.3 0.5 0.7
correlation%
68
γt0
γt1
γt2
S S K S
(a) STS12 tuning set (K = 100) (b) STS12 test set (K = 100) (c) STS12 test set (d) LI06 (K = 100)
</figure>
<figureCaption confidence="0.999545">
Figure 3: Pearson correlation at different parameter settings
</figureCaption>
<subsectionHeader confidence="0.978888">
7.2 Evaluation on the LI06 dataset
</subsectionHeader>
<bodyText confidence="0.999066153846154">
Figure 3d presents the results obtained on the LI06
data set at different weight values for the corpus-
based selectional preference semantics -y and for the
knowledge-based semantics S. Our previous exper-
iments (Guo and Diab, 2012b) show that WTMF
is the state-of-the-art model on LI06. With lexi-
cal semantics explicitly modeled, WTMF+PK yields
better results than WTMF (see Table 1). It should
be noted that LI06 prefers a smaller similar word
pair weight ( a 6 = 0.1 yields the best perfor-
mance around of 90.75%), yet in almost all condi-
tions WTMF+PK outperforms WTMF as shown in
Figure 3d.
</bodyText>
<sectionHeader confidence="0.999842" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.999934107142857">
SS has progressed immensely in recent years, espe-
cially with the establishment of the Semantic Tex-
tual Similarity task in SEMEVAL 2012. Early work
in SS focused on word pair similarity in the high di-
mensional space (Li et al., 2006; Liu et al., 2007;
Islam and Inkpen, 2008; Tsatsaronis et al., 2010; Ho
et al., 2010), where co-occurrence information was
not efficiently exploited. Researchers (O’Shea et al.,
2008) find LSA does not yield good performance. In
(Guo and Diab, 2012b; Guo and Diab, 2012c), we
show the superiority of the latent space approach in
WTMF. In this paper, we improve the WTMF model
and achieve state-of-the-art Pearson correlation on
two standard SS datasets.
There are latent variable models designed for lex-
ical semantics, such as word senses (Boyd-Graber
et al., 2007; Guo and Diab, 2011), function words
(Griffiths et al., 2005), selectional preference (Ritter
et al., 2010), synonyms and antonyms (Yih et al.,
2012), etc. However little improvement is shown
on document/sentence level semantics: (Ritter et al.,
2010) and (Yih et al., 2012) focus on selectional
preference and antonym identification, respectively;
in (Griffiths et al., 2005) the LDA performance de-
grades in the text categorization task including the
modeling of function words. Rather, we concentrate
on nuanced lexical semantics phenomena that could
benefit sentential semantics.
</bodyText>
<sectionHeader confidence="0.996802" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.9999345">
We incorporate corpus-based (selectional prefer-
ence) and knowledge-based (similar word pairs) lex-
ical semantics into a latent variable model. Our
system yields state-of-the-art unsupervised perfor-
mance on two most popular and standard SS
datasets.
</bodyText>
<sectionHeader confidence="0.990023" genericHeader="acknowledgments">
10 Acknowledgment
</sectionHeader>
<bodyText confidence="0.9557825">
This work is supported by the IARPA SCIL pro-
gram.
</bodyText>
<page confidence="0.998045">
743
</page>
<sectionHeader confidence="0.995598" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.989177358490567">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In First Joint
Conference on Lexical and Computational Semantics
(*SEM).
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Machine
Learning Research, 3.
Jordan Boyd-Graber, David Blei, and Xiaojin Zhu. 2007.
A topic model for word sense disambiguation. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning.
Scott Deerwester, Susan T Dumais, George W Furnas,
Thomas K Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and
syntax. In Advances in Neural Information Processing
Systems.
Weiwei Guo and Mona Diab. 2010. Combining orthogo-
nal monolingual and multilingual sources of evidence
for all words wsd. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics.
Weiwei Guo and Mona Diab. 2011. Semantic topic mod-
els: Combining word distributional statistics and dic-
tionary definitions. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing.
Weiwei Guo and Mona Diab. 2012a. Learning the latent
semantics of a concept by its definition. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics.
Weiwei Guo and Mona Diab. 2012b. Modeling sen-
tences in the latent space. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics.
Weiwei Guo and Mona Diab. 2012c. Weiwei: A simple
unsupervised latent semantics based approach for sen-
tence similarity. In First Joint Conference on Lexical
and Computational Semantics (*SEM).
Chukfong Ho, Masrah Azrifah Azmi Murad, Rabiah Ab-
dul Kadir, and Shyamala C. Doraisamy. 2010. Word
sense disambiguation-based sentence similarity. In
Proceedings of the 23rd International Conference on
Computational Linguistics.
Aminul Islam and Diana Inkpen. 2008. Semantic
text similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data, 2.
Ou Jin, Nathan N. Liu, Kai Zhao, Yong Yu, and Qiang
Yang. 2011. Transferring topical knowledge from
auxiliary long texts for short text clustering. In Pro-
ceedings of the 20th ACM international conference on
Information and knowledge management.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In Proceedings of the 19th International Joint
Conference on Artificial Intelligence.
Yuhua Li, Davi d McLean, Zuhair A. Bandar, James D.
O’Shea, and Keeley Crockett. 2006. Sentence similar-
ity based on semantic nets and corpus statistics. IEEE
Transaction on Knowledge and Data Engineering, 18.
Xiao-Ying Liu, Yi-Ming Zhou, and Ruo-Shi Zheng.
2007. Sentence similarity based on dynamic time
warping. In The International Conference on Seman-
tic Computing.
James O’Shea, Zuhair Bandar, Keeley Crockett, and
David McLean. 2008. A comparative study of two
short text semantic similarity measures. In Proceed-
ings of the Agent and Multi-Agent Systems: Technolo-
gies and Applications, Second KES International Sym-
posium (KES-AMSTA).
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why, What, and How?
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet allocation method for selectional preferences.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics.
Jagan Sankaranarayanan, Hanan Samet, Benjamin E.
Teitler, Michael D. Lieberman, and Jon Sperling.
2009. Twitterstand: news in tweets. In Proceedings of
the 17th ACM SIGSPATIAL International Conference
on Advances in Geographic Information Systems.
Nathan Srebro and Tommi Jaakkola. 2003. Weighted
low-rank approximations. In Proceedings of the Twen-
tieth International Conference on Machine Learning.
Harald Steck. 2010. Training and testing of recom-
mender systems on data missing not at random. In
Proceedings of the 16th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing.
George Tsatsaronis, Iraklis Varlamis, and Michalis Vazir-
giannis. 2010. Text relatedness based on a word the-
saurus. Journal of Articial Intelligence Research, 37.
Wentau Yih, Geoffrey Zweig, and John C. Platt. 2012.
Polarity inducing latent semantic analysis. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning.
</reference>
<page confidence="0.982816">
744
</page>
<reference confidence="0.9143624">
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. Paraeval: Using paraphrases
to evaluate summaries automatically. In Proceedings
of Human Language Technology Conference of the
North American Chapter of the ACL,.
</reference>
<page confidence="0.998472">
745
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.564375">
<title confidence="0.998687">Improving Lexical Semantics for Sentential Semantics: Modeling Selectional Preference and Similar Words in a Latent Variable Model</title>
<author confidence="0.974409">Weiwei</author>
<affiliation confidence="0.893231">Department of Computer Columbia</affiliation>
<email confidence="0.999461">weiwei@cs.columbia.edu</email>
<author confidence="0.988509">Mona</author>
<affiliation confidence="0.994447">Department of Computer</affiliation>
<author confidence="0.84285">George Washington</author>
<email confidence="0.999793">mtdiab@gwu.edu</email>
<abstract confidence="0.993930722222222">Sentence Similarity [SS] computes a similarity score between two sentences. The SS task differs from document level semantics tasks in that it features the sparsity of words in a data unit, i.e. a sentence. Accordingly it is crucial to robustly model each word in a sentence to capture the complete semantic picture of the sentence. In this paper, we hypothesize that by better modeling lexical semantics we can obtain better sentential semantics. We incorporate both corpus-based (selectional preference information) and knowledge-based (similar words extracted in a dictionary) lexical semantics into a latent variable model. The experiments show state-of-the-art performance among unsupervised systems on two SS datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In First Joint Conference on Lexical and Computational Semantics (*SEM).</booktitle>
<contexts>
<context position="1506" citStr="Agirre et al., 2012" startWordPosition="223" endWordPosition="226">ry) lexical semantics into a latent variable model. The experiments show state-of-the-art performance among unsupervised systems on two SS datasets. 1 Introduction Sentence Similarity [SS] is emerging as a crucial step in many NLP tasks that focus on sentence level semantics such as word sense disambiguation (Guo and Diab, 2010; Guo and Diab, 2012a), summarization (Zhou et al., 2006), text coherence (Lapata and Barzilay, 2005), tweet clustering (Sankaranarayanan et al., 2009; Jin et al., 2011), etc. SS operates in a very small context, on average 11 words per sentence in Semeval-2012 dataset (Agirre et al., 2012), resulting in inadequate evidence to generalize to robust sentential semantics. Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) models by a large margin in the SS task, yielding state-of-the-art performance on the LI06 (Li et al., 2006) SS dataset. However, all of these models make harsh simplifying assumptions on how a token is generated: (1) in LSA/WTMF, a token is generated by the inner product of the word latent </context>
<context position="10891" citStr="Agirre et al., 2012" startWordPosition="1805" endWordPosition="1808">0; Guo and Diab, 2012b) for optimization details). 6 Experimental Setting We build the model WTMF+PK on the same corpora as used in our previous work (Guo and Diab, 2012b), comprising the following: Brown corpus (each sentence is treated as a document), sense definitions from Wiktionary and Wordnet (only definitions without target words and usage examples). We follow the preprocessing steps in (Guo and Diab, 2012c): tokenization, pos-tagging, lemmatization and further merge lemmas. The corpus is used for building matrix X. The evaluation datasets are LI06 dataset and Semeval-2012 STS [STS12] (Agirre et al., 2012) dataset. LI06 consists of 30 sentence pairs (dictionary definitions). For STS12,1 the training data (2000 pairs) are used as the tuning set for setting the 1A detailed description of the data sets is provided in (Agirre et al., 2012). (3) 741 parameters of our models. This data comprises msrpar, msr-vid, smt-eur. Once the models are tuned, we evaluate them on the STS12 test data that comprises 3150 sentence pairs from msr-par, msr-vid, smt-eur, smt-news, On-WN. It is worth noting that smt-news and On-WN are not part of the tuning data. We use cosine similarity to measure the similarity scores</context>
<context position="13004" citStr="Agirre et al., 2012" startWordPosition="2181" endWordPosition="2184">alue as follows 6 = {0, 0.1, 0.3, 0.5, 0.7}. The performance on STS12 tuning and test dataset as well as on the LI06 dataset are illustrated in Figures 3a, 3b and 3d. The parameters of model 6 in Table 1 (&apos;y = 2, 6 = 0.3) are the chosen values based on tuning set performance. 7.1 Evaluation on the STS12 datasets Table 1 shows WTMF is already a very strong baseline: it outperforms LSA and LDA by a large margin. Same as in (Guo and Diab, 2012b), LSA performance degrades dramatically when trained on a corpus of sentence sized documents, yielding results worse than the surface words baseline 31% (Agirre et al., 2012). Using corpus-based selectional preference semantics alone (model 4 WTMF+P in Table 1) boosts the performance of WTMF by +1.17% on the test set, while using knowledge-based semantics alone (model 5 WTMF+K) improves the over the WTMF results by an absolute +2.31%. Combining them (model 6 WTMF+PK) yields the best results, with an absolute increase of +3.39%, which suggests that the two sources of semantic evidence are useful, but more importantly, they are complementary for each other. Table 1 also presents the performance on each individual dataset. The gain on each individual source is not as</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In First Joint Conference on Lexical and Computational Semantics (*SEM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<contexts>
<context position="1813" citStr="Blei et al., 2003" startWordPosition="268" endWordPosition="271"> (Guo and Diab, 2010; Guo and Diab, 2012a), summarization (Zhou et al., 2006), text coherence (Lapata and Barzilay, 2005), tweet clustering (Sankaranarayanan et al., 2009; Jin et al., 2011), etc. SS operates in a very small context, on average 11 words per sentence in Semeval-2012 dataset (Agirre et al., 2012), resulting in inadequate evidence to generalize to robust sentential semantics. Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) models by a large margin in the SS task, yielding state-of-the-art performance on the LI06 (Li et al., 2006) SS dataset. However, all of these models make harsh simplifying assumptions on how a token is generated: (1) in LSA/WTMF, a token is generated by the inner product of the word latent vector and the document latent vector; (2) in LDA, all the tokens in a document are sampled from the same document level topic distribution. Under this framework, they ignore rich linguistic phenomena such as inter-word dependency, semantic scope of words, etc. This is a result of simply using document IDs</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David Blei</author>
<author>Xiaojin Zhu</author>
</authors>
<title>A topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="17995" citStr="Boyd-Graber et al., 2007" startWordPosition="3045" endWordPosition="3048">r similarity in the high dimensional space (Li et al., 2006; Liu et al., 2007; Islam and Inkpen, 2008; Tsatsaronis et al., 2010; Ho et al., 2010), where co-occurrence information was not efficiently exploited. Researchers (O’Shea et al., 2008) find LSA does not yield good performance. In (Guo and Diab, 2012b; Guo and Diab, 2012c), we show the superiority of the latent space approach in WTMF. In this paper, we improve the WTMF model and achieve state-of-the-art Pearson correlation on two standard SS datasets. There are latent variable models designed for lexical semantics, such as word senses (Boyd-Graber et al., 2007; Guo and Diab, 2011), function words (Griffiths et al., 2005), selectional preference (Ritter et al., 2010), synonyms and antonyms (Yih et al., 2012), etc. However little improvement is shown on document/sentence level semantics: (Ritter et al., 2010) and (Yih et al., 2012) focus on selectional preference and antonym identification, respectively; in (Griffiths et al., 2005) the LDA performance degrades in the text categorization task including the modeling of function words. Rather, we concentrate on nuanced lexical semantics phenomena that could benefit sentential semantics. 9 Conclusion We </context>
</contexts>
<marker>Boyd-Graber, Blei, Zhu, 2007</marker>
<rawString>Jordan Boyd-Graber, David Blei, and Xiaojin Zhu. 2007. A topic model for word sense disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science.</journal>
<contexts>
<context position="1754" citStr="Deerwester et al., 1990" startWordPosition="258" endWordPosition="262">cus on sentence level semantics such as word sense disambiguation (Guo and Diab, 2010; Guo and Diab, 2012a), summarization (Zhou et al., 2006), text coherence (Lapata and Barzilay, 2005), tweet clustering (Sankaranarayanan et al., 2009; Jin et al., 2011), etc. SS operates in a very small context, on average 11 words per sentence in Semeval-2012 dataset (Agirre et al., 2012), resulting in inadequate evidence to generalize to robust sentential semantics. Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) models by a large margin in the SS task, yielding state-of-the-art performance on the LI06 (Li et al., 2006) SS dataset. However, all of these models make harsh simplifying assumptions on how a token is generated: (1) in LSA/WTMF, a token is generated by the inner product of the word latent vector and the document latent vector; (2) in LDA, all the tokens in a document are sampled from the same document level topic distribution. Under this framework, they ignore rich linguistic phenomena such as inter-word dependency, semantic scope o</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5042" citStr="Fellbaum, 1998" startWordPosition="774" endWordPosition="775">nal preference, should be associated with crude indicating the resource topic. We believe modeling selectional preference capturing local evidence completes the semantic picture for words, hence further rendering better sentential semantics. To our best knowledge, this is the first work to model selectional preference for sentence/document semantics. We also integrate knowledge-based semantics in the WTMF framework. Knowledge-based semantics, a human-annotated clean resource, is an important complement to corpus-based noisy cooccurrence information. We extract similar word pairs from Wordnet (Fellbaum, 1998). Leveraging these pairs, an infrequent word such as purchase can exploit robust latent vectors from its synonyms such as buy. Similar words pairs can be seamlessly modeled in WTMF, since in the matrix factorization framework a latent vector profile is explicitly created for each word, while in LDA all the data structures are designed for documents/sentences. We construct a graph to connect words according to the extracted similar word pairs, to encourage similar words to share similar latent vector profiles. We will refer to our proposed novel model as WTMF+PK. 2 Weighted Textual Matrix Facto</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>David M Blei</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Integrating topics and syntax.</title>
<date>2005</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="18057" citStr="Griffiths et al., 2005" startWordPosition="3055" endWordPosition="3058">u et al., 2007; Islam and Inkpen, 2008; Tsatsaronis et al., 2010; Ho et al., 2010), where co-occurrence information was not efficiently exploited. Researchers (O’Shea et al., 2008) find LSA does not yield good performance. In (Guo and Diab, 2012b; Guo and Diab, 2012c), we show the superiority of the latent space approach in WTMF. In this paper, we improve the WTMF model and achieve state-of-the-art Pearson correlation on two standard SS datasets. There are latent variable models designed for lexical semantics, such as word senses (Boyd-Graber et al., 2007; Guo and Diab, 2011), function words (Griffiths et al., 2005), selectional preference (Ritter et al., 2010), synonyms and antonyms (Yih et al., 2012), etc. However little improvement is shown on document/sentence level semantics: (Ritter et al., 2010) and (Yih et al., 2012) focus on selectional preference and antonym identification, respectively; in (Griffiths et al., 2005) the LDA performance degrades in the text categorization task including the modeling of function words. Rather, we concentrate on nuanced lexical semantics phenomena that could benefit sentential semantics. 9 Conclusion We incorporate corpus-based (selectional preference) and knowledg</context>
</contexts>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2005</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, David M. Blei, and Joshua B. Tenenbaum. 2005. Integrating topics and syntax. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Combining orthogonal monolingual and multilingual sources of evidence for all words wsd.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1215" citStr="Guo and Diab, 2010" startWordPosition="175" endWordPosition="178">he complete semantic picture of the sentence. In this paper, we hypothesize that by better modeling lexical semantics we can obtain better sentential semantics. We incorporate both corpus-based (selectional preference information) and knowledge-based (similar words extracted in a dictionary) lexical semantics into a latent variable model. The experiments show state-of-the-art performance among unsupervised systems on two SS datasets. 1 Introduction Sentence Similarity [SS] is emerging as a crucial step in many NLP tasks that focus on sentence level semantics such as word sense disambiguation (Guo and Diab, 2010; Guo and Diab, 2012a), summarization (Zhou et al., 2006), text coherence (Lapata and Barzilay, 2005), tweet clustering (Sankaranarayanan et al., 2009; Jin et al., 2011), etc. SS operates in a very small context, on average 11 words per sentence in Semeval-2012 dataset (Agirre et al., 2012), resulting in inadequate evidence to generalize to robust sentential semantics. Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) m</context>
</contexts>
<marker>Guo, Diab, 2010</marker>
<rawString>Weiwei Guo and Mona Diab. 2010. Combining orthogonal monolingual and multilingual sources of evidence for all words wsd. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Semantic topic models: Combining word distributional statistics and dictionary definitions.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="18016" citStr="Guo and Diab, 2011" startWordPosition="3049" endWordPosition="3052">imensional space (Li et al., 2006; Liu et al., 2007; Islam and Inkpen, 2008; Tsatsaronis et al., 2010; Ho et al., 2010), where co-occurrence information was not efficiently exploited. Researchers (O’Shea et al., 2008) find LSA does not yield good performance. In (Guo and Diab, 2012b; Guo and Diab, 2012c), we show the superiority of the latent space approach in WTMF. In this paper, we improve the WTMF model and achieve state-of-the-art Pearson correlation on two standard SS datasets. There are latent variable models designed for lexical semantics, such as word senses (Boyd-Graber et al., 2007; Guo and Diab, 2011), function words (Griffiths et al., 2005), selectional preference (Ritter et al., 2010), synonyms and antonyms (Yih et al., 2012), etc. However little improvement is shown on document/sentence level semantics: (Ritter et al., 2010) and (Yih et al., 2012) focus on selectional preference and antonym identification, respectively; in (Griffiths et al., 2005) the LDA performance degrades in the text categorization task including the modeling of function words. Rather, we concentrate on nuanced lexical semantics phenomena that could benefit sentential semantics. 9 Conclusion We incorporate corpus-ba</context>
</contexts>
<marker>Guo, Diab, 2011</marker>
<rawString>Weiwei Guo and Mona Diab. 2011. Semantic topic models: Combining word distributional statistics and dictionary definitions. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Learning the latent semantics of a concept by its definition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1235" citStr="Guo and Diab, 2012" startWordPosition="179" endWordPosition="182"> picture of the sentence. In this paper, we hypothesize that by better modeling lexical semantics we can obtain better sentential semantics. We incorporate both corpus-based (selectional preference information) and knowledge-based (similar words extracted in a dictionary) lexical semantics into a latent variable model. The experiments show state-of-the-art performance among unsupervised systems on two SS datasets. 1 Introduction Sentence Similarity [SS] is emerging as a crucial step in many NLP tasks that focus on sentence level semantics such as word sense disambiguation (Guo and Diab, 2010; Guo and Diab, 2012a), summarization (Zhou et al., 2006), text coherence (Lapata and Barzilay, 2005), tweet clustering (Sankaranarayanan et al., 2009; Jin et al., 2011), etc. SS operates in a very small context, on average 11 words per sentence in Semeval-2012 dataset (Agirre et al., 2012), resulting in inadequate evidence to generalize to robust sentential semantics. Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) models by a large mar</context>
<context position="5688" citStr="Guo and Diab, 2012" startWordPosition="874" endWordPosition="877"> infrequent word such as purchase can exploit robust latent vectors from its synonyms such as buy. Similar words pairs can be seamlessly modeled in WTMF, since in the matrix factorization framework a latent vector profile is explicitly created for each word, while in LDA all the data structures are designed for documents/sentences. We construct a graph to connect words according to the extracted similar word pairs, to encourage similar words to share similar latent vector profiles. We will refer to our proposed novel model as WTMF+PK. 2 Weighted Textual Matrix Factorization Our previous work (Guo and Diab, 2012b) models the sentences in the weighted matrix factorization framework (Figure 1). The corpus is stored in an M x N matrix X, with each cell containing the TFIDF values of words. The rows of X are M distinct words and columns are N sentences. As in Figure 1, X is approximated by the product of a K x M matrix P and a K x N matrix Q. Accordingly, each sentence sj is represented by a K dimensional latent vector Q·,j. Similarly a word wi is generalized by P·,i. P and Q is optimized by minimize the objective function: � EWij (P·,i · ` ·,j − Xij)2 + A||P||22 + A||` ||22 i j Wi,j = J 1, if Xij # 0 (1</context>
<context position="9513" citStr="Guo and Diab, 2012" startWordPosition="1571" endWordPosition="1574">ords. In total, we are able to discover 80K pairs of similar words for the 46K distinct words in our corpus. Given a pair of similar words wi,/wi2, we want the two corresponding latent vectors P·,i,/P·,i2 to be as close as possible, namely the cosine similarity to be close to 1. Accordingly, a term is added in equation 1 for each similar word pair wi,/wi2: δ · (P·,i1 · P·,i2 − |P·,i1||P·,i2|)2 (2) |P·,i |denotes the length of the vector P·,i. The coefficient 6, analogous to -y, denotes the importance of the knowledge-based evidence. The Figure 2c shows the final WTMF+PK model. 5 Inference In (Guo and Diab, 2012b) we use Alternating Least Square [ALS] for inference, which is to set the derivative of equation 1 for P/Q to 0 and iteratively compute P/Q by fixing the other matrix (Srebro and Jaakkola, 2003). However, it is no longer applicable with the new term (equation 2) involving the length of word vectors |P·,i|. Therefore we approximate the objective function by treating the vector length |P·,i| as fixed values during the ALS iterations: ( )−, Q·,j = P W� (j)P&gt; + λI P W�(j)X·,j P·,i = (Q W�(i)Q&gt; + λI + δP·,s(i)P· y(i))− ( ) Q W� (i)X&gt; i,· + δLiP·,s(i)Ls(i) where P·,s(i) are the latent vectors of s</context>
<context position="11728" citStr="Guo and Diab, 2012" startWordPosition="1945" endWordPosition="1948">e et al., 2012). (3) 741 parameters of our models. This data comprises msrpar, msr-vid, smt-eur. Once the models are tuned, we evaluate them on the STS12 test data that comprises 3150 sentence pairs from msr-par, msr-vid, smt-eur, smt-news, On-WN. It is worth noting that smt-news and On-WN are not part of the tuning data. We use cosine similarity to measure the similarity scores between two sentences. Pearson correlation between the system’s answer and gold standard similarity scores is used as the evaluation metric. We include three baselines LSA, LDA and WTMF using the setting described in (Guo and Diab, 2012b). We run Gibbs Sampling based LDA for 2000 iterations and average the model over the last 10 iterations. For WTMF, we run 20 iterations and fix the missing words weight at wm = 0.01 with a regularization coefficient set at A = 20, which is the best condition found in (Guo and Diab, 2012b). 7 Experiments Table 1 summarizes the results at dimension K = 100 (the dimension of latent vectors). To remove randomness, each reported number is the averaged results of 10 runs. Based on the STS tuning set, we experiment with different values for the selectional preference weight (&apos;y = {0, 1, 2}), and li</context>
<context position="14587" citStr="Guo and Diab, 2012" startWordPosition="2436" endWordPosition="2439">formance using different values of weights in figure 3a and 3b, we can conclude that the selectional preference and similar word pairs yield very promising results. The trends hold in different parameter conditions with a consistent improvement. Figure 3c illustrates the impact of dimension K = {50, 75,100,125,150} on WTMF and WTMF+PK. Generally a larger K leads to a higher Pearson correlation, but the improvement is tiny when K ≥ 100 (0.1% increase). Compared to all the unsupervised systems that participated in Semeval STS 2012 task, WTMF+PK yields state-of-the-art performance (70.70%).2 In (Guo and Diab, 2012c) we also apply WTMF (K = 100) on STS12, achieving a correlation of 69.5%. However, additional data is incorporated in the training corpora: (1) STS12 tuning set; (2) for WordNet and Wiktionary data, the target words are also included in the definitions (hence synonym pairs were used); (3) the usage examples of target words were also appended to the definitions.3 While trained with this experimental setting, our model WTMF+PK (&apos;y = 2, 6 = 0.3, K = 100) is able to reach an even higher correlation of 72.0%. 2WTMF+PK is an unsupervised system, since the gold standard similarly scores are never u</context>
<context position="16821" citStr="Guo and Diab, 2012" startWordPosition="2846" endWordPosition="2849">71 0 0.1 0.3 0.5 0.7 71 68 67 0 0.1 0.3 0.5 0.7 70 69 71 70 69 WFMF WFMF+PK, γ=2, δ=0.3 67 66 50 75 100 125 150 91 r=0 r=1 r=2 90.5 90 89.5 0 0.1 0.3 0.5 0.7 correlation% 68 γt0 γt1 γt2 S S K S (a) STS12 tuning set (K = 100) (b) STS12 test set (K = 100) (c) STS12 test set (d) LI06 (K = 100) Figure 3: Pearson correlation at different parameter settings 7.2 Evaluation on the LI06 dataset Figure 3d presents the results obtained on the LI06 data set at different weight values for the corpusbased selectional preference semantics -y and for the knowledge-based semantics S. Our previous experiments (Guo and Diab, 2012b) show that WTMF is the state-of-the-art model on LI06. With lexical semantics explicitly modeled, WTMF+PK yields better results than WTMF (see Table 1). It should be noted that LI06 prefers a smaller similar word pair weight ( a 6 = 0.1 yields the best performance around of 90.75%), yet in almost all conditions WTMF+PK outperforms WTMF as shown in Figure 3d. 8 Related Work SS has progressed immensely in recent years, especially with the establishment of the Semantic Textual Similarity task in SEMEVAL 2012. Early work in SS focused on word pair similarity in the high dimensional space (Li et </context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Weiwei Guo and Mona Diab. 2012a. Learning the latent semantics of a concept by its definition. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Modeling sentences in the latent space.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1235" citStr="Guo and Diab, 2012" startWordPosition="179" endWordPosition="182"> picture of the sentence. In this paper, we hypothesize that by better modeling lexical semantics we can obtain better sentential semantics. We incorporate both corpus-based (selectional preference information) and knowledge-based (similar words extracted in a dictionary) lexical semantics into a latent variable model. The experiments show state-of-the-art performance among unsupervised systems on two SS datasets. 1 Introduction Sentence Similarity [SS] is emerging as a crucial step in many NLP tasks that focus on sentence level semantics such as word sense disambiguation (Guo and Diab, 2010; Guo and Diab, 2012a), summarization (Zhou et al., 2006), text coherence (Lapata and Barzilay, 2005), tweet clustering (Sankaranarayanan et al., 2009; Jin et al., 2011), etc. SS operates in a very small context, on average 11 words per sentence in Semeval-2012 dataset (Agirre et al., 2012), resulting in inadequate evidence to generalize to robust sentential semantics. Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) models by a large mar</context>
<context position="5688" citStr="Guo and Diab, 2012" startWordPosition="874" endWordPosition="877"> infrequent word such as purchase can exploit robust latent vectors from its synonyms such as buy. Similar words pairs can be seamlessly modeled in WTMF, since in the matrix factorization framework a latent vector profile is explicitly created for each word, while in LDA all the data structures are designed for documents/sentences. We construct a graph to connect words according to the extracted similar word pairs, to encourage similar words to share similar latent vector profiles. We will refer to our proposed novel model as WTMF+PK. 2 Weighted Textual Matrix Factorization Our previous work (Guo and Diab, 2012b) models the sentences in the weighted matrix factorization framework (Figure 1). The corpus is stored in an M x N matrix X, with each cell containing the TFIDF values of words. The rows of X are M distinct words and columns are N sentences. As in Figure 1, X is approximated by the product of a K x M matrix P and a K x N matrix Q. Accordingly, each sentence sj is represented by a K dimensional latent vector Q·,j. Similarly a word wi is generalized by P·,i. P and Q is optimized by minimize the objective function: � EWij (P·,i · ` ·,j − Xij)2 + A||P||22 + A||` ||22 i j Wi,j = J 1, if Xij # 0 (1</context>
<context position="9513" citStr="Guo and Diab, 2012" startWordPosition="1571" endWordPosition="1574">ords. In total, we are able to discover 80K pairs of similar words for the 46K distinct words in our corpus. Given a pair of similar words wi,/wi2, we want the two corresponding latent vectors P·,i,/P·,i2 to be as close as possible, namely the cosine similarity to be close to 1. Accordingly, a term is added in equation 1 for each similar word pair wi,/wi2: δ · (P·,i1 · P·,i2 − |P·,i1||P·,i2|)2 (2) |P·,i |denotes the length of the vector P·,i. The coefficient 6, analogous to -y, denotes the importance of the knowledge-based evidence. The Figure 2c shows the final WTMF+PK model. 5 Inference In (Guo and Diab, 2012b) we use Alternating Least Square [ALS] for inference, which is to set the derivative of equation 1 for P/Q to 0 and iteratively compute P/Q by fixing the other matrix (Srebro and Jaakkola, 2003). However, it is no longer applicable with the new term (equation 2) involving the length of word vectors |P·,i|. Therefore we approximate the objective function by treating the vector length |P·,i| as fixed values during the ALS iterations: ( )−, Q·,j = P W� (j)P&gt; + λI P W�(j)X·,j P·,i = (Q W�(i)Q&gt; + λI + δP·,s(i)P· y(i))− ( ) Q W� (i)X&gt; i,· + δLiP·,s(i)Ls(i) where P·,s(i) are the latent vectors of s</context>
<context position="11728" citStr="Guo and Diab, 2012" startWordPosition="1945" endWordPosition="1948">e et al., 2012). (3) 741 parameters of our models. This data comprises msrpar, msr-vid, smt-eur. Once the models are tuned, we evaluate them on the STS12 test data that comprises 3150 sentence pairs from msr-par, msr-vid, smt-eur, smt-news, On-WN. It is worth noting that smt-news and On-WN are not part of the tuning data. We use cosine similarity to measure the similarity scores between two sentences. Pearson correlation between the system’s answer and gold standard similarity scores is used as the evaluation metric. We include three baselines LSA, LDA and WTMF using the setting described in (Guo and Diab, 2012b). We run Gibbs Sampling based LDA for 2000 iterations and average the model over the last 10 iterations. For WTMF, we run 20 iterations and fix the missing words weight at wm = 0.01 with a regularization coefficient set at A = 20, which is the best condition found in (Guo and Diab, 2012b). 7 Experiments Table 1 summarizes the results at dimension K = 100 (the dimension of latent vectors). To remove randomness, each reported number is the averaged results of 10 runs. Based on the STS tuning set, we experiment with different values for the selectional preference weight (&apos;y = {0, 1, 2}), and li</context>
<context position="14587" citStr="Guo and Diab, 2012" startWordPosition="2436" endWordPosition="2439">formance using different values of weights in figure 3a and 3b, we can conclude that the selectional preference and similar word pairs yield very promising results. The trends hold in different parameter conditions with a consistent improvement. Figure 3c illustrates the impact of dimension K = {50, 75,100,125,150} on WTMF and WTMF+PK. Generally a larger K leads to a higher Pearson correlation, but the improvement is tiny when K ≥ 100 (0.1% increase). Compared to all the unsupervised systems that participated in Semeval STS 2012 task, WTMF+PK yields state-of-the-art performance (70.70%).2 In (Guo and Diab, 2012c) we also apply WTMF (K = 100) on STS12, achieving a correlation of 69.5%. However, additional data is incorporated in the training corpora: (1) STS12 tuning set; (2) for WordNet and Wiktionary data, the target words are also included in the definitions (hence synonym pairs were used); (3) the usage examples of target words were also appended to the definitions.3 While trained with this experimental setting, our model WTMF+PK (&apos;y = 2, 6 = 0.3, K = 100) is able to reach an even higher correlation of 72.0%. 2WTMF+PK is an unsupervised system, since the gold standard similarly scores are never u</context>
<context position="16821" citStr="Guo and Diab, 2012" startWordPosition="2846" endWordPosition="2849">71 0 0.1 0.3 0.5 0.7 71 68 67 0 0.1 0.3 0.5 0.7 70 69 71 70 69 WFMF WFMF+PK, γ=2, δ=0.3 67 66 50 75 100 125 150 91 r=0 r=1 r=2 90.5 90 89.5 0 0.1 0.3 0.5 0.7 correlation% 68 γt0 γt1 γt2 S S K S (a) STS12 tuning set (K = 100) (b) STS12 test set (K = 100) (c) STS12 test set (d) LI06 (K = 100) Figure 3: Pearson correlation at different parameter settings 7.2 Evaluation on the LI06 dataset Figure 3d presents the results obtained on the LI06 data set at different weight values for the corpusbased selectional preference semantics -y and for the knowledge-based semantics S. Our previous experiments (Guo and Diab, 2012b) show that WTMF is the state-of-the-art model on LI06. With lexical semantics explicitly modeled, WTMF+PK yields better results than WTMF (see Table 1). It should be noted that LI06 prefers a smaller similar word pair weight ( a 6 = 0.1 yields the best performance around of 90.75%), yet in almost all conditions WTMF+PK outperforms WTMF as shown in Figure 3d. 8 Related Work SS has progressed immensely in recent years, especially with the establishment of the Semantic Textual Similarity task in SEMEVAL 2012. Early work in SS focused on word pair similarity in the high dimensional space (Li et </context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Weiwei Guo and Mona Diab. 2012b. Modeling sentences in the latent space. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Weiwei: A simple unsupervised latent semantics based approach for sentence similarity.</title>
<date>2012</date>
<booktitle>In First Joint Conference on Lexical and Computational Semantics (*SEM).</booktitle>
<contexts>
<context position="1235" citStr="Guo and Diab, 2012" startWordPosition="179" endWordPosition="182"> picture of the sentence. In this paper, we hypothesize that by better modeling lexical semantics we can obtain better sentential semantics. We incorporate both corpus-based (selectional preference information) and knowledge-based (similar words extracted in a dictionary) lexical semantics into a latent variable model. The experiments show state-of-the-art performance among unsupervised systems on two SS datasets. 1 Introduction Sentence Similarity [SS] is emerging as a crucial step in many NLP tasks that focus on sentence level semantics such as word sense disambiguation (Guo and Diab, 2010; Guo and Diab, 2012a), summarization (Zhou et al., 2006), text coherence (Lapata and Barzilay, 2005), tweet clustering (Sankaranarayanan et al., 2009; Jin et al., 2011), etc. SS operates in a very small context, on average 11 words per sentence in Semeval-2012 dataset (Agirre et al., 2012), resulting in inadequate evidence to generalize to robust sentential semantics. Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) models by a large mar</context>
<context position="5688" citStr="Guo and Diab, 2012" startWordPosition="874" endWordPosition="877"> infrequent word such as purchase can exploit robust latent vectors from its synonyms such as buy. Similar words pairs can be seamlessly modeled in WTMF, since in the matrix factorization framework a latent vector profile is explicitly created for each word, while in LDA all the data structures are designed for documents/sentences. We construct a graph to connect words according to the extracted similar word pairs, to encourage similar words to share similar latent vector profiles. We will refer to our proposed novel model as WTMF+PK. 2 Weighted Textual Matrix Factorization Our previous work (Guo and Diab, 2012b) models the sentences in the weighted matrix factorization framework (Figure 1). The corpus is stored in an M x N matrix X, with each cell containing the TFIDF values of words. The rows of X are M distinct words and columns are N sentences. As in Figure 1, X is approximated by the product of a K x M matrix P and a K x N matrix Q. Accordingly, each sentence sj is represented by a K dimensional latent vector Q·,j. Similarly a word wi is generalized by P·,i. P and Q is optimized by minimize the objective function: � EWij (P·,i · ` ·,j − Xij)2 + A||P||22 + A||` ||22 i j Wi,j = J 1, if Xij # 0 (1</context>
<context position="9513" citStr="Guo and Diab, 2012" startWordPosition="1571" endWordPosition="1574">ords. In total, we are able to discover 80K pairs of similar words for the 46K distinct words in our corpus. Given a pair of similar words wi,/wi2, we want the two corresponding latent vectors P·,i,/P·,i2 to be as close as possible, namely the cosine similarity to be close to 1. Accordingly, a term is added in equation 1 for each similar word pair wi,/wi2: δ · (P·,i1 · P·,i2 − |P·,i1||P·,i2|)2 (2) |P·,i |denotes the length of the vector P·,i. The coefficient 6, analogous to -y, denotes the importance of the knowledge-based evidence. The Figure 2c shows the final WTMF+PK model. 5 Inference In (Guo and Diab, 2012b) we use Alternating Least Square [ALS] for inference, which is to set the derivative of equation 1 for P/Q to 0 and iteratively compute P/Q by fixing the other matrix (Srebro and Jaakkola, 2003). However, it is no longer applicable with the new term (equation 2) involving the length of word vectors |P·,i|. Therefore we approximate the objective function by treating the vector length |P·,i| as fixed values during the ALS iterations: ( )−, Q·,j = P W� (j)P&gt; + λI P W�(j)X·,j P·,i = (Q W�(i)Q&gt; + λI + δP·,s(i)P· y(i))− ( ) Q W� (i)X&gt; i,· + δLiP·,s(i)Ls(i) where P·,s(i) are the latent vectors of s</context>
<context position="11728" citStr="Guo and Diab, 2012" startWordPosition="1945" endWordPosition="1948">e et al., 2012). (3) 741 parameters of our models. This data comprises msrpar, msr-vid, smt-eur. Once the models are tuned, we evaluate them on the STS12 test data that comprises 3150 sentence pairs from msr-par, msr-vid, smt-eur, smt-news, On-WN. It is worth noting that smt-news and On-WN are not part of the tuning data. We use cosine similarity to measure the similarity scores between two sentences. Pearson correlation between the system’s answer and gold standard similarity scores is used as the evaluation metric. We include three baselines LSA, LDA and WTMF using the setting described in (Guo and Diab, 2012b). We run Gibbs Sampling based LDA for 2000 iterations and average the model over the last 10 iterations. For WTMF, we run 20 iterations and fix the missing words weight at wm = 0.01 with a regularization coefficient set at A = 20, which is the best condition found in (Guo and Diab, 2012b). 7 Experiments Table 1 summarizes the results at dimension K = 100 (the dimension of latent vectors). To remove randomness, each reported number is the averaged results of 10 runs. Based on the STS tuning set, we experiment with different values for the selectional preference weight (&apos;y = {0, 1, 2}), and li</context>
<context position="14587" citStr="Guo and Diab, 2012" startWordPosition="2436" endWordPosition="2439">formance using different values of weights in figure 3a and 3b, we can conclude that the selectional preference and similar word pairs yield very promising results. The trends hold in different parameter conditions with a consistent improvement. Figure 3c illustrates the impact of dimension K = {50, 75,100,125,150} on WTMF and WTMF+PK. Generally a larger K leads to a higher Pearson correlation, but the improvement is tiny when K ≥ 100 (0.1% increase). Compared to all the unsupervised systems that participated in Semeval STS 2012 task, WTMF+PK yields state-of-the-art performance (70.70%).2 In (Guo and Diab, 2012c) we also apply WTMF (K = 100) on STS12, achieving a correlation of 69.5%. However, additional data is incorporated in the training corpora: (1) STS12 tuning set; (2) for WordNet and Wiktionary data, the target words are also included in the definitions (hence synonym pairs were used); (3) the usage examples of target words were also appended to the definitions.3 While trained with this experimental setting, our model WTMF+PK (&apos;y = 2, 6 = 0.3, K = 100) is able to reach an even higher correlation of 72.0%. 2WTMF+PK is an unsupervised system, since the gold standard similarly scores are never u</context>
<context position="16821" citStr="Guo and Diab, 2012" startWordPosition="2846" endWordPosition="2849">71 0 0.1 0.3 0.5 0.7 71 68 67 0 0.1 0.3 0.5 0.7 70 69 71 70 69 WFMF WFMF+PK, γ=2, δ=0.3 67 66 50 75 100 125 150 91 r=0 r=1 r=2 90.5 90 89.5 0 0.1 0.3 0.5 0.7 correlation% 68 γt0 γt1 γt2 S S K S (a) STS12 tuning set (K = 100) (b) STS12 test set (K = 100) (c) STS12 test set (d) LI06 (K = 100) Figure 3: Pearson correlation at different parameter settings 7.2 Evaluation on the LI06 dataset Figure 3d presents the results obtained on the LI06 data set at different weight values for the corpusbased selectional preference semantics -y and for the knowledge-based semantics S. Our previous experiments (Guo and Diab, 2012b) show that WTMF is the state-of-the-art model on LI06. With lexical semantics explicitly modeled, WTMF+PK yields better results than WTMF (see Table 1). It should be noted that LI06 prefers a smaller similar word pair weight ( a 6 = 0.1 yields the best performance around of 90.75%), yet in almost all conditions WTMF+PK outperforms WTMF as shown in Figure 3d. 8 Related Work SS has progressed immensely in recent years, especially with the establishment of the Semantic Textual Similarity task in SEMEVAL 2012. Early work in SS focused on word pair similarity in the high dimensional space (Li et </context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Weiwei Guo and Mona Diab. 2012c. Weiwei: A simple unsupervised latent semantics based approach for sentence similarity. In First Joint Conference on Lexical and Computational Semantics (*SEM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chukfong Ho</author>
</authors>
<title>Masrah Azrifah Azmi Murad, Rabiah Abdul Kadir, and Shyamala</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics.</booktitle>
<marker>Ho, 2010</marker>
<rawString>Chukfong Ho, Masrah Azrifah Azmi Murad, Rabiah Abdul Kadir, and Shyamala C. Doraisamy. 2010. Word sense disambiguation-based sentence similarity. In Proceedings of the 23rd International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aminul Islam</author>
<author>Diana Inkpen</author>
</authors>
<title>Semantic text similarity using corpus-based word similarity and string similarity.</title>
<date>2008</date>
<journal>ACM Transactions on Knowledge Discovery from Data,</journal>
<volume>2</volume>
<contexts>
<context position="17472" citStr="Islam and Inkpen, 2008" startWordPosition="2962" endWordPosition="2965">e-of-the-art model on LI06. With lexical semantics explicitly modeled, WTMF+PK yields better results than WTMF (see Table 1). It should be noted that LI06 prefers a smaller similar word pair weight ( a 6 = 0.1 yields the best performance around of 90.75%), yet in almost all conditions WTMF+PK outperforms WTMF as shown in Figure 3d. 8 Related Work SS has progressed immensely in recent years, especially with the establishment of the Semantic Textual Similarity task in SEMEVAL 2012. Early work in SS focused on word pair similarity in the high dimensional space (Li et al., 2006; Liu et al., 2007; Islam and Inkpen, 2008; Tsatsaronis et al., 2010; Ho et al., 2010), where co-occurrence information was not efficiently exploited. Researchers (O’Shea et al., 2008) find LSA does not yield good performance. In (Guo and Diab, 2012b; Guo and Diab, 2012c), we show the superiority of the latent space approach in WTMF. In this paper, we improve the WTMF model and achieve state-of-the-art Pearson correlation on two standard SS datasets. There are latent variable models designed for lexical semantics, such as word senses (Boyd-Graber et al., 2007; Guo and Diab, 2011), function words (Griffiths et al., 2005), selectional p</context>
</contexts>
<marker>Islam, Inkpen, 2008</marker>
<rawString>Aminul Islam and Diana Inkpen. 2008. Semantic text similarity using corpus-based word similarity and string similarity. ACM Transactions on Knowledge Discovery from Data, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ou Jin</author>
<author>Nathan N Liu</author>
<author>Kai Zhao</author>
<author>Yong Yu</author>
<author>Qiang Yang</author>
</authors>
<title>Transferring topical knowledge from auxiliary long texts for short text clustering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th ACM international conference on Information and knowledge management.</booktitle>
<contexts>
<context position="1384" citStr="Jin et al., 2011" startWordPosition="201" endWordPosition="204">orate both corpus-based (selectional preference information) and knowledge-based (similar words extracted in a dictionary) lexical semantics into a latent variable model. The experiments show state-of-the-art performance among unsupervised systems on two SS datasets. 1 Introduction Sentence Similarity [SS] is emerging as a crucial step in many NLP tasks that focus on sentence level semantics such as word sense disambiguation (Guo and Diab, 2010; Guo and Diab, 2012a), summarization (Zhou et al., 2006), text coherence (Lapata and Barzilay, 2005), tweet clustering (Sankaranarayanan et al., 2009; Jin et al., 2011), etc. SS operates in a very small context, on average 11 words per sentence in Semeval-2012 dataset (Agirre et al., 2012), resulting in inadequate evidence to generalize to robust sentential semantics. Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) models by a large margin in the SS task, yielding state-of-the-art performance on the LI06 (Li et al., 2006) SS dataset. However, all of these models make harsh simplifyi</context>
</contexts>
<marker>Jin, Liu, Zhao, Yu, Yang, 2011</marker>
<rawString>Ou Jin, Nathan N. Liu, Kai Zhao, Yong Yu, and Qiang Yang. 2011. Transferring topical knowledge from auxiliary long texts for short text clustering. In Proceedings of the 20th ACM international conference on Information and knowledge management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Regina Barzilay</author>
</authors>
<title>Automatic evaluation of text coherence: Models and representations.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="1316" citStr="Lapata and Barzilay, 2005" startWordPosition="191" endWordPosition="194">deling lexical semantics we can obtain better sentential semantics. We incorporate both corpus-based (selectional preference information) and knowledge-based (similar words extracted in a dictionary) lexical semantics into a latent variable model. The experiments show state-of-the-art performance among unsupervised systems on two SS datasets. 1 Introduction Sentence Similarity [SS] is emerging as a crucial step in many NLP tasks that focus on sentence level semantics such as word sense disambiguation (Guo and Diab, 2010; Guo and Diab, 2012a), summarization (Zhou et al., 2006), text coherence (Lapata and Barzilay, 2005), tweet clustering (Sankaranarayanan et al., 2009; Jin et al., 2011), etc. SS operates in a very small context, on average 11 words per sentence in Semeval-2012 dataset (Agirre et al., 2012), resulting in inadequate evidence to generalize to robust sentential semantics. Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) models by a large margin in the SS task, yielding state-of-the-art performance on the LI06 (Li et al.,</context>
</contexts>
<marker>Lapata, Barzilay, 2005</marker>
<rawString>Mirella Lapata and Regina Barzilay. 2005. Automatic evaluation of text coherence: Models and representations. In Proceedings of the 19th International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhua Li</author>
<author>Davi d McLean</author>
<author>Zuhair A Bandar</author>
<author>James D O’Shea</author>
<author>Keeley Crockett</author>
</authors>
<title>Sentence similarity based on semantic nets and corpus statistics.</title>
<date>2006</date>
<journal>IEEE Transaction on Knowledge and Data Engineering,</journal>
<volume>18</volume>
<marker>Li, McLean, Bandar, O’Shea, Crockett, 2006</marker>
<rawString>Yuhua Li, Davi d McLean, Zuhair A. Bandar, James D. O’Shea, and Keeley Crockett. 2006. Sentence similarity based on semantic nets and corpus statistics. IEEE Transaction on Knowledge and Data Engineering, 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao-Ying Liu</author>
<author>Yi-Ming Zhou</author>
<author>Ruo-Shi Zheng</author>
</authors>
<title>Sentence similarity based on dynamic time warping.</title>
<date>2007</date>
<booktitle>In The International Conference on Semantic Computing.</booktitle>
<contexts>
<context position="17448" citStr="Liu et al., 2007" startWordPosition="2958" endWordPosition="2961">t WTMF is the state-of-the-art model on LI06. With lexical semantics explicitly modeled, WTMF+PK yields better results than WTMF (see Table 1). It should be noted that LI06 prefers a smaller similar word pair weight ( a 6 = 0.1 yields the best performance around of 90.75%), yet in almost all conditions WTMF+PK outperforms WTMF as shown in Figure 3d. 8 Related Work SS has progressed immensely in recent years, especially with the establishment of the Semantic Textual Similarity task in SEMEVAL 2012. Early work in SS focused on word pair similarity in the high dimensional space (Li et al., 2006; Liu et al., 2007; Islam and Inkpen, 2008; Tsatsaronis et al., 2010; Ho et al., 2010), where co-occurrence information was not efficiently exploited. Researchers (O’Shea et al., 2008) find LSA does not yield good performance. In (Guo and Diab, 2012b; Guo and Diab, 2012c), we show the superiority of the latent space approach in WTMF. In this paper, we improve the WTMF model and achieve state-of-the-art Pearson correlation on two standard SS datasets. There are latent variable models designed for lexical semantics, such as word senses (Boyd-Graber et al., 2007; Guo and Diab, 2011), function words (Griffiths et a</context>
</contexts>
<marker>Liu, Zhou, Zheng, 2007</marker>
<rawString>Xiao-Ying Liu, Yi-Ming Zhou, and Ruo-Shi Zheng. 2007. Sentence similarity based on dynamic time warping. In The International Conference on Semantic Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James O’Shea</author>
<author>Zuhair Bandar</author>
<author>Keeley Crockett</author>
<author>David McLean</author>
</authors>
<title>A comparative study of two short text semantic similarity measures.</title>
<date>2008</date>
<booktitle>In Proceedings of the Agent and Multi-Agent Systems: Technologies and Applications, Second KES International Symposium (KES-AMSTA).</booktitle>
<marker>O’Shea, Bandar, Crockett, McLean, 2008</marker>
<rawString>James O’Shea, Zuhair Bandar, Keeley Crockett, and David McLean. 2008. A comparative study of two short text semantic similarity measures. In Proceedings of the Agent and Multi-Agent Systems: Technologies and Applications, Second KES International Symposium (KES-AMSTA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional preference and sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why,</booktitle>
<location>What, and How?</location>
<contexts>
<context position="3300" citStr="Resnik, 1997" startWordPosition="509" endWordPosition="510">el semantics. However, in the SS setting, it is crucial to make good use of each word, given the limited number of words in a sentence. We believe a reasonable word generation story will avoid introducing noise in sentential semantics, encouraging robust lexical semantics which can further boost the sentential semantics. In this paper, we explicitly encode lexical semantics, both corpus-based and knowledge-based information, in the WTMF model, by which we are able to achieve even better results in SS task. The additional corpus-based information we exploit is selectional preference semantics (Resnik, 1997), a feature already existing in the data yet ignored by most latent variable models. Selectional preference focuses on the admissible arguments for a word, thus capturing more nuanced semantics than the sentence IDs (when applied to a corpus of sentences as opposed to documents). Consider the following example: 739 Proceedings of NAACL-HLT 2013, pages 739–745, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics Figure 1: matrix factorization Many analysts say the global Brent crude oil benchmark price, currently around $111 a barrel ... In WTMF/LSA/LDA, a word wi</context>
</contexts>
<marker>Resnik, 1997</marker>
<rawString>Philip Resnik. 1997. Selectional preference and sense disambiguation. In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How?</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>A latent dirichlet allocation method for selectional preferences.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="18103" citStr="Ritter et al., 2010" startWordPosition="3061" endWordPosition="3064">is et al., 2010; Ho et al., 2010), where co-occurrence information was not efficiently exploited. Researchers (O’Shea et al., 2008) find LSA does not yield good performance. In (Guo and Diab, 2012b; Guo and Diab, 2012c), we show the superiority of the latent space approach in WTMF. In this paper, we improve the WTMF model and achieve state-of-the-art Pearson correlation on two standard SS datasets. There are latent variable models designed for lexical semantics, such as word senses (Boyd-Graber et al., 2007; Guo and Diab, 2011), function words (Griffiths et al., 2005), selectional preference (Ritter et al., 2010), synonyms and antonyms (Yih et al., 2012), etc. However little improvement is shown on document/sentence level semantics: (Ritter et al., 2010) and (Yih et al., 2012) focus on selectional preference and antonym identification, respectively; in (Griffiths et al., 2005) the LDA performance degrades in the text categorization task including the modeling of function words. Rather, we concentrate on nuanced lexical semantics phenomena that could benefit sentential semantics. 9 Conclusion We incorporate corpus-based (selectional preference) and knowledge-based (similar word pairs) lexical semantics</context>
</contexts>
<marker>Ritter, Mausam, Etzioni, 2010</marker>
<rawString>Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent dirichlet allocation method for selectional preferences. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jagan Sankaranarayanan</author>
<author>Hanan Samet</author>
<author>Benjamin E Teitler</author>
<author>Michael D Lieberman</author>
<author>Jon Sperling</author>
</authors>
<title>Twitterstand: news in tweets.</title>
<date>2009</date>
<booktitle>In Proceedings of the 17th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems.</booktitle>
<contexts>
<context position="1365" citStr="Sankaranarayanan et al., 2009" startWordPosition="197" endWordPosition="200">sentential semantics. We incorporate both corpus-based (selectional preference information) and knowledge-based (similar words extracted in a dictionary) lexical semantics into a latent variable model. The experiments show state-of-the-art performance among unsupervised systems on two SS datasets. 1 Introduction Sentence Similarity [SS] is emerging as a crucial step in many NLP tasks that focus on sentence level semantics such as word sense disambiguation (Guo and Diab, 2010; Guo and Diab, 2012a), summarization (Zhou et al., 2006), text coherence (Lapata and Barzilay, 2005), tweet clustering (Sankaranarayanan et al., 2009; Jin et al., 2011), etc. SS operates in a very small context, on average 11 words per sentence in Semeval-2012 dataset (Agirre et al., 2012), resulting in inadequate evidence to generalize to robust sentential semantics. Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) models by a large margin in the SS task, yielding state-of-the-art performance on the LI06 (Li et al., 2006) SS dataset. However, all of these models m</context>
</contexts>
<marker>Sankaranarayanan, Samet, Teitler, Lieberman, Sperling, 2009</marker>
<rawString>Jagan Sankaranarayanan, Hanan Samet, Benjamin E. Teitler, Michael D. Lieberman, and Jon Sperling. 2009. Twitterstand: news in tweets. In Proceedings of the 17th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Srebro</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Weighted low-rank approximations.</title>
<date>2003</date>
<booktitle>In Proceedings of the Twentieth International Conference on Machine Learning.</booktitle>
<contexts>
<context position="9709" citStr="Srebro and Jaakkola, 2003" startWordPosition="1605" endWordPosition="1608">tors P·,i,/P·,i2 to be as close as possible, namely the cosine similarity to be close to 1. Accordingly, a term is added in equation 1 for each similar word pair wi,/wi2: δ · (P·,i1 · P·,i2 − |P·,i1||P·,i2|)2 (2) |P·,i |denotes the length of the vector P·,i. The coefficient 6, analogous to -y, denotes the importance of the knowledge-based evidence. The Figure 2c shows the final WTMF+PK model. 5 Inference In (Guo and Diab, 2012b) we use Alternating Least Square [ALS] for inference, which is to set the derivative of equation 1 for P/Q to 0 and iteratively compute P/Q by fixing the other matrix (Srebro and Jaakkola, 2003). However, it is no longer applicable with the new term (equation 2) involving the length of word vectors |P·,i|. Therefore we approximate the objective function by treating the vector length |P·,i| as fixed values during the ALS iterations: ( )−, Q·,j = P W� (j)P&gt; + λI P W�(j)X·,j P·,i = (Q W�(i)Q&gt; + λI + δP·,s(i)P· y(i))− ( ) Q W� (i)X&gt; i,· + δLiP·,s(i)Ls(i) where P·,s(i) are the latent vectors of similar words of word i; the length of these vectors in the current iteration are stored in Ls(i) (similarly Li is the current length of P·,i) (cf. (Steck, 2010; Guo and Diab, 2012b) for optimizati</context>
</contexts>
<marker>Srebro, Jaakkola, 2003</marker>
<rawString>Nathan Srebro and Tommi Jaakkola. 2003. Weighted low-rank approximations. In Proceedings of the Twentieth International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Steck</author>
</authors>
<title>Training and testing of recommender systems on data missing not at random.</title>
<date>2010</date>
<booktitle>In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="10272" citStr="Steck, 2010" startWordPosition="1710" endWordPosition="1711">ing the other matrix (Srebro and Jaakkola, 2003). However, it is no longer applicable with the new term (equation 2) involving the length of word vectors |P·,i|. Therefore we approximate the objective function by treating the vector length |P·,i| as fixed values during the ALS iterations: ( )−, Q·,j = P W� (j)P&gt; + λI P W�(j)X·,j P·,i = (Q W�(i)Q&gt; + λI + δP·,s(i)P· y(i))− ( ) Q W� (i)X&gt; i,· + δLiP·,s(i)Ls(i) where P·,s(i) are the latent vectors of similar words of word i; the length of these vectors in the current iteration are stored in Ls(i) (similarly Li is the current length of P·,i) (cf. (Steck, 2010; Guo and Diab, 2012b) for optimization details). 6 Experimental Setting We build the model WTMF+PK on the same corpora as used in our previous work (Guo and Diab, 2012b), comprising the following: Brown corpus (each sentence is treated as a document), sense definitions from Wiktionary and Wordnet (only definitions without target words and usage examples). We follow the preprocessing steps in (Guo and Diab, 2012c): tokenization, pos-tagging, lemmatization and further merge lemmas. The corpus is used for building matrix X. The evaluation datasets are LI06 dataset and Semeval-2012 STS [STS12] (A</context>
</contexts>
<marker>Steck, 2010</marker>
<rawString>Harald Steck. 2010. Training and testing of recommender systems on data missing not at random. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Tsatsaronis</author>
</authors>
<title>Iraklis Varlamis, and Michalis Vazirgiannis.</title>
<date>2010</date>
<journal>Journal of Articial Intelligence Research,</journal>
<volume>37</volume>
<marker>Tsatsaronis, 2010</marker>
<rawString>George Tsatsaronis, Iraklis Varlamis, and Michalis Vazirgiannis. 2010. Text relatedness based on a word thesaurus. Journal of Articial Intelligence Research, 37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wentau Yih</author>
<author>Geoffrey Zweig</author>
<author>John C Platt</author>
</authors>
<title>Polarity inducing latent semantic analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="18145" citStr="Yih et al., 2012" startWordPosition="3068" endWordPosition="3071">ccurrence information was not efficiently exploited. Researchers (O’Shea et al., 2008) find LSA does not yield good performance. In (Guo and Diab, 2012b; Guo and Diab, 2012c), we show the superiority of the latent space approach in WTMF. In this paper, we improve the WTMF model and achieve state-of-the-art Pearson correlation on two standard SS datasets. There are latent variable models designed for lexical semantics, such as word senses (Boyd-Graber et al., 2007; Guo and Diab, 2011), function words (Griffiths et al., 2005), selectional preference (Ritter et al., 2010), synonyms and antonyms (Yih et al., 2012), etc. However little improvement is shown on document/sentence level semantics: (Ritter et al., 2010) and (Yih et al., 2012) focus on selectional preference and antonym identification, respectively; in (Griffiths et al., 2005) the LDA performance degrades in the text categorization task including the modeling of function words. Rather, we concentrate on nuanced lexical semantics phenomena that could benefit sentential semantics. 9 Conclusion We incorporate corpus-based (selectional preference) and knowledge-based (similar word pairs) lexical semantics into a latent variable model. Our system </context>
</contexts>
<marker>Yih, Zweig, Platt, 2012</marker>
<rawString>Wentau Yih, Geoffrey Zweig, and John C. Platt. 2012. Polarity inducing latent semantic analysis. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Zhou</author>
<author>Chin-Yew Lin</author>
<author>Dragos Stefan Munteanu</author>
<author>Eduard Hovy</author>
</authors>
<title>Paraeval: Using paraphrases to evaluate summaries automatically.</title>
<date>2006</date>
<booktitle>In Proceedings of Human Language Technology Conference of the North American Chapter of the ACL,.</booktitle>
<contexts>
<context position="1272" citStr="Zhou et al., 2006" startWordPosition="185" endWordPosition="188">er, we hypothesize that by better modeling lexical semantics we can obtain better sentential semantics. We incorporate both corpus-based (selectional preference information) and knowledge-based (similar words extracted in a dictionary) lexical semantics into a latent variable model. The experiments show state-of-the-art performance among unsupervised systems on two SS datasets. 1 Introduction Sentence Similarity [SS] is emerging as a crucial step in many NLP tasks that focus on sentence level semantics such as word sense disambiguation (Guo and Diab, 2010; Guo and Diab, 2012a), summarization (Zhou et al., 2006), text coherence (Lapata and Barzilay, 2005), tweet clustering (Sankaranarayanan et al., 2009; Jin et al., 2011), etc. SS operates in a very small context, on average 11 words per sentence in Semeval-2012 dataset (Agirre et al., 2012), resulting in inadequate evidence to generalize to robust sentential semantics. Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) models by a large margin in the SS task, yielding state-of</context>
</contexts>
<marker>Zhou, Lin, Munteanu, Hovy, 2006</marker>
<rawString>Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu, and Eduard Hovy. 2006. Paraeval: Using paraphrases to evaluate summaries automatically. In Proceedings of Human Language Technology Conference of the North American Chapter of the ACL,.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>