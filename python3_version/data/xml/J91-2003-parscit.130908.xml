<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.986282">
Semantics of Paragraphs
</title>
<author confidence="0.974228">
Wlodek Zadrozny*
Karen Jensen*
</author>
<bodyText confidence="0.9816275">
IBM T. J. Watson Research Center
We present a computational theory of the paragraph. Within it we formally define coherence,
give semantics to the adversative conjunction &amp;quot;but&amp;quot; and to the Gricean maxim of quantity,
and present some new methods for anaphora resolution.
The theory precisely characterizes the relationship between the content of the paragraph
and background knowledge needed for its understanding. This is achieved by introducing a
new type of logical theory consisting of an object level, corresponding to the content of the
paragraph, a referential level, which is a new logical level encoding background knowledge,
and a metalevel containing constraints on models of discourse (e.g. a formal version of Gricean
maxims). We propose also specific mechanisms of interaction between these levels, resembling
both classical provability and abduction. Paragraphs are then represented by a class of structures
called p-models.
</bodyText>
<sectionHeader confidence="0.989761" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999904391304348">
Logic and knowledge have been often discussed by linguists. Anaphora is another
prominent subject in linguistic analyses. Not so frequently examined are different types
of cohesion. And it is quite rare to find the word &amp;quot;paragraph&amp;quot; in articles or books
about natural language understanding, although paragraphs are grammatical units
and units of discourse. But it is possible to speak formally about the role of background
knowledge, cohesion, coherence and anaphora—all within one, flexible and natural,
logical system—if one examines the semantic role of the linguistic construct called a
paragraph.
Paragraphs have been sometimes described, rather loosely, as &amp;quot;units of thought.&amp;quot;
We establish a correspondence between them and certain types of logical models,
thereby making the characterization of paragraphs more precise. The correspondence
gives us also an opportunity to identify and attack—with some success, we believe—
three interesting and important problems: (1) how to define formally coherence and
topic, (2) how to resolve anaphora, and (3) what is the formal meaning of linkages (con-
nectives) such as but, however, and, certainly, usually, because, then, etc. These questions
are central from our point of view because: (1) the &amp;quot;unity&amp;quot; of a paragraph stems from
its coherence, while the &amp;quot;aboutness&amp;quot; of thought can be, at least to some extent, de-
scribed as existence of a topic; (2) without determining reference of pronouns and
phrases, the universes of the models are undefined; and (3) the linkages, which make
sentences into paragraphs, have semantic roles that must be accounted for. We can ex-
plain then the process of building a computational model of a paragraph (a p-model) as
an interaction between its sentences, background knowledge to which these sentences
refer, and metatheoretical operators that indicate types of permitted models.
</bodyText>
<note confidence="0.838088333333333">
* P.O. Box 704, Yorktown Heights, NY 10598
© 1991 Association for Computational Linguistics
Computational Linguistics Volume 17, Number 2
</note>
<bodyText confidence="0.999990823529412">
At this point the reader may ask: what is so special about paragraphs; does all
this mean that a chapter, a book or a letter do not have any formal counterparts? We
believe they do. But we simply do not yet know how corresponding formal structures
would be created from models of paragraphs. To answer this question we may need
more advanced theories of categorization and learning than exist today. On the other
hand, the paragraph is the right place to begin: it is the next grammatical unit after
the sentence; connectives providing cohesion operate here, not at the level of an entire
discourse; and it is the smallest reasonable domain of anaphora resolution. Further-
more, we will argue, it is the smallest domain in which topic and coherence can be
defined.
The formalization of paragraph structure requires the introduction of a new type
of logical theory and a corresponding class of models. As we know, the usual log-
ical structures consist of an object level theory T and provability relation I-; within
the context of the semantics of natural language, the object theory contains a logical
translation of the surface form of sentences, and [-- is the standard provability relation
(logical consequence). In mathematical logic, this scheme is sometimes extended by
adding a metalevel assumption, for instance postulating the standardness of natural
numbers. In artificial intelligence, a metarule—typically, the closed world assumption
of circumscription—can be used in dealing with theoretical questions, like the frame
problem. But a formal account of natural language understanding requires more. It
requires at least a description (a) of how background knowledge about objects and
relations that the sentences describe is used in the process of understanding, and (b) of
general constraints on linguistic communications, as expressed for instance in Gricean
maxims. It is well known that without the former it is impossible to find references of
pronouns or attachments of prepositional phrases; background knowledge, as it turns
out, is also indispensable in establishing coherence. We have then reasons for introduc-
ing a new logical level—a referential level R, which codes the background knowledge.
As for Gricean maxims, we show that they can be expressed formally and can be used
in a computational model of communication. We include them in a metalevel M, which
contains global constraints on models of a text and definitions of meta-operators such
as the conjunction but. We end up with three-level logical theories (M, T, R, FR + m),
where a provability relation FR + m, based on R and M, can be used, for example, to
establish the reference of pronouns.
This work is addressed primarily to our colleagues working on computational
models of natural language; but it should be also of interest to linguists, logicians,
and philosophers. It should be of interest to linguists because the notion that a para-
graph is equivalent to a model is something concrete to discuss; because p-models
are as formal as formal languages (and therefore something satisfyingly theoretical
to argue about); and because new directions for analysis are opened beyond the sen-
tence. The work should be of interest to logicians because it introduces a new type of
three-level theory and corresponding models. The theory of these structures, which
are based on linguistic constructs, will differ from classical model theory—for instance,
by the fact that names of predicates of an object theory matter, because they connect
the object theory with the referential level. This work should be of interest to philoso-
phers for many of the same reasons: it makes more sense to talk about the meaning
of a paragraph than about the meaning of a sentence. The following parallel can be
drawn: a sentence is meaningful only with respect to a model of a paragraph, exactly
as the truth value of a formula can be computed only with respect to a given model.
Moreover, it is possible in this framework to talk about meaning without mentioning
the idea of possible worlds. However, we do not identify meaning with truth condi-
tions; in this paper, the meaning of a sentence is its role in the model of the paragraph
</bodyText>
<page confidence="0.990527">
172
</page>
<note confidence="0.974183">
Zadrozny and Jensen Semantics of Paragraphs
</note>
<bodyText confidence="0.9990790625">
in which this sentence occurs. Our intuitive concept of meaning is similar to Lakoff&apos;s
(1987) Idealized Cognitive Model (ICM). Needless to say, we believe in the possibil-
ity of formalizing ICMs, although in this paper we will not try to express, in logic,
prototype effects, metaphors, or formal links with vision.
The paper is presented in six sections. In Section 2, we discuss the grammatical
function of the paragraph and we show, informally, how a formal model of a para-
graph might actually be built. In Section 3 we give the logical preliminaries to our
analysis. We discuss a three-part logical structure that includes a referential level, and
we introduce a model for plausible meaning. Section 4 discusses paragraph coherence,
and Section 5 constructs a model of a paragraph, a p-model, based on the information
contained in the paragraph itself and background information contained in the ref-
erential level R. Section 5 further motivates the use of the referential level, showing
how it contributes to the resolution of anaphoric reference. In Section 6, we broaden
our presentation of the metalevel, introducing some metalevel axioms, and sketching
ways by which they can be used to reduce ambiguity and construct new models. We
also show metalevel rules for interpreting &amp;quot;but.&amp;quot;
</bodyText>
<sectionHeader confidence="0.559487" genericHeader="method">
2. The Paragraph as a Discourse Unit
</sectionHeader>
<subsectionHeader confidence="0.841004">
2.1 Approaches to Paragraph Analysis
</subsectionHeader>
<bodyText confidence="0.999882064516129">
Recent syntactic theory—that is, in the last 30 years—has been preoccupied with
sentence-level analysis. Within discourse theory, however, some significant work has
been done on the analysis of written paragraphs. We can identify four different linguis-
tic approaches to paragraphs: prescriptivist, psycholinguist, textualist, and discourse-
oriented.
The prescriptivist approach is typified in standard English grammar textbooks,
such as Warriner (1963). In these sources, a paragraph is notionally defined as some-
thing like a series of sentences that develop one single topic, and rules are laid down
for the construction of an ideal (or at least an acceptable) paragraph. Although these
dictates are fairly clear, the underlying notion of topic is not.
An example of psycholinguistically oriented research work can be found in Bond
and Hayes (1983). These authors take the position that a paragraph is a psychologically
real unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found
three major formal devices that are used, by readers, to identify a paragraph: (1) the
repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference;
and (3) paragraph length, as determined by spatial and/or sentence-count information.
Other psycholing-uistic studies that confirm the validity of paragraph units can be
found in Black and Bower (1979) and Haberlandt et al. (1980).
The textualist approach to paragraph analysis is exemplified by E. J. Crothers. His
work is taxonomic, in that he performs detailed descriptive analyses of paragraphs. He
lists, classifies, and discusses various types of inference, by which he means, generally,
&amp;quot;the linguistic-logical notions of consequent and presupposition&amp;quot; Crothers (1979:112)
have collected convincing evidence of the existence of language chunks—real struc-
tures, not just orthographic conventions—that are smaller than a discourse, larger than
a sentence, generally composed of sentences, and recursive in nature (like sentences).
These chunks are sometimes called &amp;quot;episodes,&amp;quot; and sometimes &amp;quot;paragraphs.&amp;quot; Accord-
ing to Hinds (1979), paragraphs are made up of segments, which in turn are made up of
sentences or clauses, which in turn are made up of phrases. Paragraphs therefore give
hierarchical structure to sentences. Hinds discusses three major types of paragraphs,
and their corresponding segment types. The three types are procedural (how-to), ex-
pository (essay), and narrative (in this case, spontaneous conversation). For each type,
</bodyText>
<page confidence="0.99729">
173
</page>
<note confidence="0.800943">
Computational Linguistics Volume 17, Number 2
</note>
<bodyText confidence="0.99975025">
its segments are distinguished by bearing distinct relationships to the paragraph topic
(which is central, but nowhere clearly defined). Segments themselves are composed
of clauses and regulated by &amp;quot;switching&amp;quot; patterns, such as the question-answer pattern
and the remark-reply pattern.
</bodyText>
<subsectionHeader confidence="0.999431">
2.2 Our View of Paragraphs: An Informal Sketch
</subsectionHeader>
<bodyText confidence="0.999878738095238">
Although there are other discussions of the paragraph as a central element of discourse
(e.g. Chafe 1979, Halliday and Hasan 1976, Longacre 1979, Haberlandt et al. 1980), all
of them share a certain limitation in their formal techniques for analyzing paragraph
structure. Discourse linguists show little interest in making the structural descriptions
precise enough so that a computational grammar of text could adapt them and use
them. Our interest, however, lies precisely in that area.
We suggest that the paragraph is a grammatical and logical unit. It is the small-
est linguistic representation of what, in logic, is called a &amp;quot;model,&amp;quot; and it is the first
reasonable domain of anaphora resolution, and of coherent thought about a central
topic.
A paragraph can be thought of as a grammatical unit in the following sense: it
is the discourse unit in which a functional (or a predicate-argument) structure can be
definitely assigned to sentences/strings. For instance, Sells (1985, p. 8) says that the
sentence &amp;quot;Reagan thinks bananas,&amp;quot; which is otherwise strange, is in fact acceptable if
it occurs as an answer to the question &amp;quot;What is Kissinger&apos;s favorite fruit?&amp;quot; The pairing
of these two sentences may be said to create a small paragraph. Our point is that an
acceptable structure can be assigned to the utterance &amp;quot;Reagan thinks bananas&amp;quot; only
within the paragraph in which this utterance occurs. We believe that, in general, no unit
larger than a paragraph is necessary to assign a functional structure to a sentence, and
that no smaller discourse fragment, such as two (or one) neighboring sentences, will
be sufficient for this task. That is, we can ask in the first sentence of a paragraph about
Kissinger&apos;s favorite fruit, elaborate the question and the circumstances in the next few
sentences, and give the above answer at the end. We do not claim that a paragraph is
necessarily described by a set of grammar rules in some grammar formalism (although
it may be); rather, it has the grammatical role of providing functional structures that
can be assigned to strings.
The logical structure of paragraphs will be analyzed in the next sections. At this
point we would like to present some intuitions that led to this analysis. But first we
want to identify our point of departure. In order to resolve anaphora and to establish
the coherence or incoherence of a text, one must appeal to the necessary background
knowledge. Hence, a formal analysis of paragraphs must include a formal description
of background knowledge and its usage. Furthermore, this background knowledge
cannot be treated as a collection of facts or formulas in some formal language, because
that would preclude dealing with contradictory word senses, or multiple meanings.
Secondly, this background knowledge is not infinite and esoteric. In fact, to a large ex-
tent it can be found in standard reference works such as dictionaries and encyclopedias.
To argue for these points, we can consider the following paragraph:1
In the summer of 1347 a merchant ship returning from the Black Sea entered the
Sicilian port of Messina bringing with it the horrifying disease that came to be
known as the Black Death. It struck rapidly. Within twenty-four hours of
infection and the appearance of the first small black pustule came an agonizing
death. The effect of the Black Death was appalling. In less than twenty years half
</bodyText>
<page confidence="0.7638215">
1 J. Burke, The Day the Universe Changed. 1986. Little, Brown &amp; Co., Boston, Massachusetts, p. 55.
174
</page>
<note confidence="0.979722">
Zadrozny and Jensen Semantics of Paragraphs
</note>
<bodyText confidence="0.98605066">
the population of Europe had been killed, the countryside devastated, and a
period of optimism and growing economic welfare had been brought to a
sudden and catastrophic end.
The sentences that compose a paragraph must stick together; to put it more tech-
nically, they must cohere. This means very often that they show cohesion in the sense
of Halliday (1976)—semantic links between elements. Crucially, also, the sentences of
a paragraph must all be related to a topic.
However, in the example paragraph, very few instances can be found here of the
formal grammatical devices for paragraph cohesion. There are no connectives, and
there are only two anaphoric pronouns (both occurrences of &amp;quot;it&amp;quot;). In each case, there
are multiple possible referents for the pronoun. The paragraph is coherent because it
has a topic: &amp;quot;Black Death&amp;quot;; all sentences mention it, explicitly or implicitly.
Notice that resolving anaphora precedes the discovery of a topic. A few words
about this will illustrate the usage of background knowledge. By parsing with syntactic
information alone, we show that resolution of the first &amp;quot;it&amp;quot; reference hinges on the
proper attachment of the participial clause &amp;quot;bringing within it... &amp;quot;. If the &amp;quot;bringing&amp;quot;
clause modifies &amp;quot;Messina,&amp;quot; then &amp;quot;Messina&amp;quot; is the subject of &amp;quot;bringing&amp;quot; and must be
the referent for &amp;quot;it.&amp;quot; If the clause modifies &amp;quot;port,&amp;quot; then &amp;quot;port&amp;quot; is the desired referent;
if the clause is attached at the level of the main verb of the sentence, then &amp;quot;ship&amp;quot; is
the referent.
But syntactic relations do not suffice to resolve anaphora: Hobbs&apos; (1976) algorithm
for resolving the reference of pronouns, depending only on the surface syntax of
sentences in the text, when applied to &amp;quot;it&amp;quot; in the example paragraph, fails in both
cases to identify the most likely referent NP.
Adding selectional restrictions (semantic feature information, Hobbs 1977) does
not solve the problem, because isolated features offer only part of the background
knowledge necessary for reference disambiguation. Later, Hobbs (1979, 1982) proposed
a knowledge base in which information about language and the world would be
encoded, and he emphasized the need for using &amp;quot;salience&amp;quot; in choosing facts from this
knowledge base.
We will investigate the possibility that the structure of this knowledge base can
actually resemble the structure of, for example, natural language dictionaries. The
process of finding referents could then be automated.
Determining that the most likely subject for &amp;quot;bringing,&amp;quot; in the first sentence, is the
noun &amp;quot;ship&amp;quot; is done in the following fashion. The first definition for &amp;quot;bring&amp;quot; in W7
(Webster&apos;s Seventh Collegiate Dictionary) is &amp;quot;to convey, lead, carry, or cause to come along
with one... &amp;quot;The available possible subjects for &amp;quot;bringing&amp;quot; are &amp;quot;Messina,&amp;quot; &amp;quot;port,&amp;quot; and
&amp;quot;ship.&amp;quot; &amp;quot;Messina&amp;quot; is listed in the Pronouncing Gazetteer of W7, which means that it
is a place (and is so identified in the subtitle of the Gazetteer). So we can substitute
the word &amp;quot;place&amp;quot; for the word &amp;quot;Messina.&amp;quot; Then we check the given definitions for the
words &amp;quot;place,&amp;quot; &amp;quot;port,&amp;quot; and &amp;quot;ship&amp;quot; in both dictionaries. LDOCE (Longman Dictionary of
Contemporary English) proves particularly useful at this point. Definitions for &amp;quot;place&amp;quot;
begin: &amp;quot;a particular part of space... &amp;quot;. Definitions for &amp;quot;port&amp;quot; include: &amp;quot;harbour... &amp;quot;; &amp;quot;an
opening in the side of a ship... &amp;quot;. But the first entry for &amp;quot;ship&amp;quot; in LDOCE reads &amp;quot;a large
boat for carrying people or goods...... This demonstrates a very quick connection with
the definition for the verb &amp;quot;bring,&amp;quot; since the word &amp;quot;carry&amp;quot; occurs in both definitions.
It requires much more time and effort to find a connection between &amp;quot;bring&amp;quot; and either
of the other two candidate subject words &amp;quot;place&amp;quot; or &amp;quot;port.&amp;quot; Similar techniques can be
used to assign &amp;quot;disease&amp;quot; as the most probable referent for the second &amp;quot;it&amp;quot; anaphor in
our example paragraph.
</bodyText>
<page confidence="0.993819">
175
</page>
<note confidence="0.300184">
Computational Linguistics Volume 17, Number 2
</note>
<bodyText confidence="0.999774476190476">
Equally significant in this instance is the realization that a dictionary points to
synonym and paraphrase relations, and thereby verifies the cohesiveness of the pas-
sage. Through the dictionary (LDOCE again), we establish shared-word relationships
between and among the words &amp;quot;disease,&amp;quot; &amp;quot;Black death,&amp;quot; &amp;quot;infection,&amp;quot; &amp;quot;death,&amp;quot; &amp;quot;killed,&amp;quot;
and &amp;quot;end.&amp;quot; Note that there is no other means, short of appealing to human under-
standing or to some hand-coded body of predicate assertions, for making these rela-
tionships.
This demonstrates that information needed to identify and resolve anaphoric ref-
erences can be found, to an interesting extent, in standard dictionaries and thesauri.
(Other reference works could be treated as additional sources of world knowledge.)
This type of consultation uses existing natural language texts as a referential level for
processing purposes. It is the lack of exactly this notion of referential level that has
stood in the way of other linguists who have been interested in the paragraph as a
unit. Crothers (1979, p. 112), for example, bemoans the fact that his &amp;quot;theory lacks a
world knowledge component, a mental &apos;encyclopedia,&apos; which could be invoked to gen-
erate inferences... &amp;quot;. With respect to that independent source of knowledge, our main
contributions are two. First, we identify its possible structure (a collection of partially
ordered theories) and make formal the choice of a most plausible interpretation. In
other words, we recognize it as a separate logical level—the referential level. Second,
we suggest that natural language reference works, like dictionaries and thesauri, can
quite often fill the role of the referential level.
</bodyText>
<sectionHeader confidence="0.574566" genericHeader="method">
3. The Logic of Reference
</sectionHeader>
<bodyText confidence="0.9998952">
The goal of this section is to introduce a formalism describing how background knowl-
edge is used in understanding text. The term &amp;quot;logic of reference&amp;quot; denotes a formal
description of this process of consulting various sources of information in order to
produce an interpretation of a text. The formalist will be presented in a number of
steps in which we will elaborate one simple example:
</bodyText>
<subsectionHeader confidence="0.837369">
Example 1
</subsectionHeader>
<bodyText confidence="0.969993333333333">
Entering the port, a ship brought a disease.
This sentence can be translated into the logical formula (ignoring only the past
tense of &amp;quot;bring&amp;quot; and the progressive of &amp;quot;enter&amp;quot;):
</bodyText>
<construct confidence="0.7428115">
Definition S: enter(xl , x2) &amp; ship(xl) &amp; port(x2) &amp; bring(x3, x4) &amp; disease(x4) &amp; xl = s
&amp; x2 == m &amp; x3 =s &amp; x4 = d,
</construct>
<bodyText confidence="0.978752555555556">
where s, m, d, are constants.
We adopt the three-level semantics as a formal tool for the analysis of paragraphs.
This semantics was constructed (Zadrozny 1987a, 1987b) as a formal framework for
default and commonsense reasoning. It should not come as a surprise that we can
now use this apparatus for text/discourse analysis; after all, many natural language
inferences are based on defaults, and quite often they can be reduced to choosing most
plausible interpretations of predicates. For instance, relating &amp;quot;they&amp;quot; to &amp;quot;apples&amp;quot; in the
sentence (cf. Haugeland 1985 p. 195; Zadrozny 1987a):
We bought the boys apples because they were so cheap
</bodyText>
<page confidence="0.996882">
176
</page>
<note confidence="0.825162">
Zadrozny and Jensen Semantics of Paragraphs
</note>
<bodyText confidence="0.994763">
can be an example of such a most plausible choice.
The main ideas of the three-level semantics can be stated as follows:
</bodyText>
<listItem confidence="0.999202625">
1. Reasoning takes place in a three-level structure consisting of an object
level, a referential level, and a metalevel.
2. The object level is used to describe the current situation, and in our case
is reserved for the formal representation of paragraph sentences. For the
sake of simplicity, the object level will consist of a first order theory.
3. The referential level, denoted by R, consists of theories representing
background knowledge, from which information relevant to the
understanding of a given piece of text has to be extracted. It constrains
interpretations of the predicates of an object theory. Its structure and the
extraction methods will be discussed below.
4. Understanding has as its goal construction of an interpretation of the
text, i.e. building some kind of model. Since not all logically permissible
models are linguistically appropriate, one needs a place, namely the
metalevel, to put constraints on types of models. Gricean maxims belong
there; Section 6 will be devoted to a presentation of the metalevel rules
corresponding to them.
</listItem>
<bodyText confidence="0.999748">
We have shown elsewhere (Jensen and Binot 1988; Zadrozny 1987a, 1987b) that natural
language programs, such as on-line grammars and dictionaries, can be used as referen-
tial levels for commonsense reasoning—for example, to disambiguate PP attachment.
This means that information contained in grammars and dictionaries can be used to
constrain possible interpretations of the logical predicates of an object-level theory.
The referential structures we are going to use are collections of logical theories,
but the concept of reference is more general. Some of the intuitions we associate with
this notion have been very well expressed by Turner (1987, pp. 7-8):
... Semantics is constrained by our models of ourselves and our worlds. We have
models of up and down that are based by the way our bodies actually function.
Once the word &amp;quot;up&amp;quot; is given its meaning relative to our experience with gravity,
it is not free to &amp;quot;slip&amp;quot; into its opposite. &amp;quot;Up&amp;quot; means up and not down. ... We
have a model that men and women couple to produce offspring who are similar
to their parents, and this model is grounded in genetics, and the semantics of
kinship metaphor is grounded in this model. Mothers have a different role than
fathers in this model, and thus there is a reason why &amp;quot;Death is the father of
beauty&amp;quot; fails poetically while &amp;quot;Death is the mother of beauty&amp;quot; succeeds....
It is precisely this &amp;quot;grounding&amp;quot; of logical predicates in other conceptual structures
that we would like to capture. We investigate here only the &amp;quot;grounding&amp;quot; in logical the-
ories. However, it is possible to think about constraining linguistic or logical predicates
by simulating physical experiences (cf. Woods 1987).
We assume here that a translation of the surface forms of sentences into a logical
formalism is possible. Its details are not important for our aim of giving a semantic
interpretation of paragraphs; the main theses of our theory do not depend on a logical
notation. So we will use a very simple formalism, like the one above, resembling the
standard first order language. But, obviously, there are other possibilities—for instance,
the discourse representation structures (DRS&apos;s) of Kamp (1981), which have been used
to translate a subset of English into logical formulas, to model text (identified with a list
of sentences), to analyze a fragment of English, and to deal with anaphora. The logical
</bodyText>
<page confidence="0.987239">
177
</page>
<note confidence="0.298803">
Computational Linguistics Volume 17, Number 2
</note>
<bodyText confidence="0.999726875">
notation of Montague (1970) is more sophisticated, and may be considered another pos-
sibility. Jackendoff&apos;s (1983) formalism is richer and resembles more closely an English
grammar. Jackendoff (1983, p. 14) writes &amp;quot;it would be perverse not to take as a working
assumption that language is a relatively efficient and accurate encoding of the infor-
mation it conveys.&amp;quot; Therefore a formalism of the kind he advocates would probably be
most suitable for an implementation of our semantics. It will also be a model for our
simplified logical notation (cf. Section 5). We can envision a system that uses data struc-
tures produced by a computational grammar to obtain the logical form of sentences.
</bodyText>
<subsectionHeader confidence="0.999824">
3.1 Finite Representations, Finite Theories
</subsectionHeader>
<bodyText confidence="0.999960828571429">
Unless explicitly stated otherwise, we assume that formulas are expressed in a certain
(formal) language L without equality; the extension L(=) of L is going to be used only
in Section 5 for dealing with noun phrase references. This means that natural language
expressions such as &amp;quot;A is B,&amp;quot; &amp;quot;A is the same as B,&amp;quot; etc. are not directly represented by
logical equality; similarly, &amp;quot;not&amp;quot; is often not treated as logical negation; cf. Hintikka
(1985).
All logical notions that we are going to consider, such as theory or model, will
be finitary. For example, a model would typically contain fewer than a hundred ele-
ments of different logical sorts. Therefore these notions, and all other constructs we
are going to define (axioms, metarules, definitions etc.) are computational, although
usually we will not provide explicit algorithms for computing them. The issues of
control are not so important for us at this point; we restrict ourselves to describing
the logic. This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff
(1983), Kamp (1981), and implicitly or explicitly by almost all researchers in compu-
tational linguistics. As a logical postulate it is not very radical; it is possible within a
finitary framework to develop that part of mathematics that is used or has potential
applications in natural science, such as mathematical analysis (cf. Mycielski 1981).
On the other hand, a possible obstacle to our strategy of using only finite objects is
the fact that the deductive closure of any set of formulas is not finite in standard logic,
while, clearly, we will have to deduce new facts from formal representations of text
and background knowledge. But there are several ways to avoid this obstruction. For
example, consider theories consisting of universal formulas without function symbols.
Let Th(T) of such a theory T be defined as T plus ground clauses/sets of literals prov-
able from T in standard logic. It is easily seen that it is a closure, i.e. Th(Th(T)) = Th(T);
and obviously, it is finite, for finite T. It makes sense then to require that logical conse-
quences of paragraph sentences have similar finite representations. However, in order
not to limit the expressive power of the formal language, we should proceed in a
slightly different manner. The easiest way to achieve the above requirement is by pos-
tulating that all universes of discourse are always finite, and therefore all quantifiers
actually range over finite domains. In practice, we would use those two and other
tricks: we could forbid more than three quantifier changes, because even in mathe-
matics more than three are rare; we could restrict the size of universes of discourse to
some large number such as 1001; we could allow only a fixed finite nesting of func-
tion symbols (or operators) in formulas; etc. The intention of this illustration was to
convince the reader that we now can introduce the following set of definitions.
</bodyText>
<subsectionHeader confidence="0.72944">
Definitions
</subsectionHeader>
<listItem confidence="0.9954305">
• A theory is a finite set of sentences Sent (formulas without free variables
in some formal language).
</listItem>
<page confidence="0.965567">
178
</page>
<bodyText confidence="0.430505">
Zadrozny and Jensen Semantics of Paragraphs
</bodyText>
<listItem confidence="0.966854272727273">
• A deductive closure operator is a function Th :P(Sent) P(Sent)
(a) T c Th(T), for any T,
(b) Th(Th(T)) = Th(T),
(c) Th(T) is finite, for finite T; additionally, we require it to be
ground, for ground T.
• A theory T is consistent if there is no formula cb such that both 0 and
belong to Th(T) (and inconsistent otherwise).
• A model of a theory T is defined, as usual, as an interpretation defined on
a certain domain, which satisfies all formulas of T. The collection of all
(finite) models of a theory T will be denoted by Mods(T).
• The set of all subformulas of a collection of formulas F is denoted by
</listItem>
<bodyText confidence="0.990984571428571">
Form(F). 0 is a ground instance of a formula 0, if 0 contains no variables,
and 0 = 00, for some substitution O.
Thus, we do not require Th(T) to be closed under substitution instances of tautologies.
Although in this paper we take modus ponens as the main rule of inference, in general
one can consider deductive closures with respect to weaker, nonstandard logics, (cf.
Levesque 1984; Frisch 1987; Patel-Schneider 1985). But we won&apos;t pursue this topic
further here.
</bodyText>
<subsectionHeader confidence="0.999929">
3.2 The Structure of Background Knowledge
</subsectionHeader>
<bodyText confidence="0.999896">
Background knowledge is not a simple list of meaning postulates—it has a structure
and it may contain contradictions and ambiguities. These actualities have to be taken
into account in any realistic model of natural language understanding. For instance, the
verb &amp;quot;enter&amp;quot; is polysemous. But, unless context specifies otherwise, &amp;quot;to come in&amp;quot; is a
more plausible meaning than &amp;quot;to join a group.&amp;quot; Assuming some logical representation
of this knowledge, we can write that
</bodyText>
<equation confidence="0.900575">
enter(x, y) –4 {come _in(x y); place(y)} (e1)
enter(x, y) {join (x , y)&amp;group(y); .1 (e2)
and e2 &lt;enter el.
</equation>
<bodyText confidence="0.96245">
Two things should be explained now about this notation:
</bodyText>
<listItem confidence="0.996483333333333">
• Meanings of predicates/words are represented on the right-hand sides of
the arrows as collections of formulas—i.e., theories. The main idea is that
these mini-theories of predicates appearing in a paragraph will jointly
provide enough constraints to exclude implausible interpretations. (One
can think of meanings as regions in space, and of constraints as sets of
linear inequalities approximating these regions). How this can be done,
we will show in a moment.
• These theories are partially ordered; and their partial orders are written
as &lt;enter(x,y), or &lt;enter, or &lt;,, or simply &lt;, depending on context. This is
our way of making formal the asymmetry of plausibility of different
meanings of a predicate. Again, a way of exploiting it will be shown
below.
</listItem>
<page confidence="0.995749">
179
</page>
<figure confidence="0.394009">
Computational Linguistics Volume 17, Number 2
Definition
</figure>
<listItem confidence="0.910824">
• A referential level R is a structure
</listItem>
<equation confidence="0.953082">
R = {(0 , &lt;0 :LJ Formulae}
</equation>
<bodyText confidence="0.658376333333333">
where—for each &apos;0— &lt;0 is a partially ordered (by a relation of
plausibility) collection of implications &apos;0 -4 T.
The term 0 —&gt; T stands for the theory {&apos;0 —&gt; r:r E T}, and
</bodyText>
<listItem confidence="0.90963275">
1,2, • • •I abbreviates CO -4
• It is convenient to assume also that all formulas, except &amp;quot;laws&amp;quot;—which
are supposed to be always true—have the least preferred empty
interpretation a.
• We suppose also that interpretations are additionally ranked according
to the canonical partial ordering on subformulas. The ranking provides a
natural method of dealing with exceptions, as in the case of finding an
interpretation of a &amp; p &amp; 0 with R containing (a —&gt; y), (a &amp;
</listItem>
<bodyText confidence="0.999204666666667">
where would be preferred to y—if both are consistent, and both
defaults are equally preferred. This means that preference is given to
more specific information. For instance, the sentence The officer went out
and struck the flag will get the reading &amp;quot;lowered the flag,&amp;quot; if the
appropriate theory of strike(x,y)&amp;flag(y) is part of background
knowledge; if not, it will be understood as &amp;quot;hit the flag.&amp;quot;
The referential level (R, &lt;) may contain the theories listed below. Since we view
a dictionary as an (imperfect) embodiment of a referential level, we have derived the
formulas in every theory To from a dictionary definition of the term We believe that
even such a crude model can be useful in practice, but a refinement of this model will
be needed to have a sophisticated theory of a working natural language understanding
system.
</bodyText>
<equation confidence="0.89217875">
enter(x, y) {come _in (x , y); place (y); ...} (el)
/* enter—to come into a place */
enter (x, y) join(x , y)&amp;group(y); typically : professionals (y)} (e2)
/* enter—to join a group; typically of professionals */
ship(x) flarge_boat(x); 3y carry (x , y)&amp;(people(y) V goods(y)); .} (shl)
/* ship—a large boat for carrying people or goods on the sea */
ship(x) Warge_aircraft(x) V space_vehicle(x));...} (sh2)
bring(x , y) -4 {carry (x , y); .} (b1)
/* bring—to carry */
bring(x, y) {cause(x, y); ...} (b2)
/* bring—to cause */
disease(y) {illness(y);...} (del)
/* disease—illness caused by an infection */
180
Zadrozny and Jensen Semantics of Paragraphs
disaster(y) {. ;ax cause(y, x)&amp;harm(x) .} (drl)
/* disaster—causes a harm */
port(x) fharbor(x); .1 (pi)
/* port—harbor */
(. .)
</equation>
<bodyText confidence="0.999943305555556">
Note: We leave undefined the semantics of adverbs such as typically in (e2). This ad-
verb appears in the formula as an operator; our purpose in choosing this representation
is to call the reader&apos;s attention to the fact that for any real applications the theories
will have to be more complex and very likely written in a higher order language (cf.
Section 4).
The theories, which we describe here only partially, restricting ourselves to their
relevant parts, represent the meanings of concepts. We assume as before that (el) is
more plausible than (e2), i.e. e2 &lt;
-enter el; similarly, for (sh1), (sh2) and &lt;ship, etc. This
particular ordering of theories is based on the ordering of meanings of the correspond-
ing words in dictionaries (derived and less frequent meanings have lower priority).
But one can imagine deriving such orderings by other means, such as statistics.
The partial order &lt;enter has the theories {el, e2, a} as its domain; a is the least
preferred empty interpretation corresponding to our lack of knowledge about the
predicate; it is used when both (el) and (e2) are inconsistent with a current object
theory. The domain is ordered by the relation of preference a &lt;
-enter e2 &lt;enter el. The
theory (el) will always be used in constructing theories and models of paragraphs
in which the expression &amp;quot;enter&amp;quot; (in any grammatical form) appears, unless assuming
it would lead to an inconsistency. In such a case, the meaning of &amp;quot;to enter&amp;quot; would
change to (e2), or some other theory belonging to R.
We would like to stress three points now: (1) the above implications are based
on the definitions that actually occur in dictionaries; (2) the ordering can actually be
found in some dictionaries—it is not our own arbitrary invention; (3) it is natural to
treat a dictionary definition as a theory, since it expresses &amp;quot;the analysis of a set of
facts in their relation to one another,&amp;quot; different definitions corresponding to possible
different analyses. (Encyclopedia articles are even more theory-like.) In this sense, the
notion of a referential level is a formalization of a real phenomenon.
Obviously, dictionaries or encyclopedias do not include all knowledge an agent
must have to function in the world, or a program should possess in order to under-
stand any kind of discourse. Although the exact boundary between world knowledge
and lexical knowledge is impossible to draw, we do know that lexicons usually contain
very little information about human behavior or temporal relations among objects of
the world. Despite all these differences, we may assume that world knowledge and
lexical knowledge (its proper subset) have a similar formal structure. And in the ex-
amples that we present, it is the structure that matters.
</bodyText>
<subsectionHeader confidence="0.999869">
3.3 How to Use Background Knowledge
</subsectionHeader>
<bodyText confidence="0.999928">
The next few pages will be devoted to an analysis of the interaction of background
knowledge with a logical representation of a text. We will describe two modes of such
an interaction; both seem to be present in our understanding of language. One exploits
differences in plausibility of the meanings of words and phrases, in the absence of
context (e.g., the difference between a central and a peripheral sense, or between a
frequent and a rare meaning). The other one takes advantage of connections between
those meanings. We do not claim that this is the only possible such analysis; rather,
</bodyText>
<page confidence="0.974624">
181
</page>
<note confidence="0.274255">
Computational Linguistics Volume 17, Number 2
</note>
<bodyText confidence="0.994349">
we present a formal model which can perhaps be eventually disproved and replaced
by a better one. As far as we know, this is the first such formal proposal.
3.3.1 Dominance. In Figure 1 the theories of &amp;quot;enter,&amp;quot; &amp;quot;ship,&amp;quot; etc. and the partial orders
are represented graphically; more plausible theories are positioned higher. A path
through this graph chooses an interpretation of the sentence S. For instance, the path
fint = {el, shl, pl, b1, d1} and S say together that
A large boat (ship) that carries people or goods came into the harbor and carried a
disease (illness).
Since it is the &amp;quot;highest&amp;quot; path, flit is the most plausible (relative to R) interpretation of
the words that appear in the sentence. Because it is also consistent, it will be chosen
as a best interpretation of S, (cf. Zadrozny 1987a, 1987b). Another theory, consisting
of f&apos; = {el , sh2, pl , b2, dl } and S, saying that
A space vehicle came into the harbor and caused a disease/illness
is less plausible according to that ordering. As it turns out, f&apos; is never constructed in
the process of building an interpretation of a paragraph containing the sentence S,
unless assuming fmt would lead to a contradiction, for instance within the higher level
context of a science fiction story.
The collection of these most plausible consistent interpretations of a given theory
T is denoted by PT &lt; (T). Thenf int belongs to PT &lt;(Th({S})), but this is not true for f&apos;.
Note: One should remember that, in general, because all our orderings are partial,
there can be more than one most plausible interpretation of a sentence or a paragraph,
and more that one &amp;quot;next best&amp;quot; interpretation. Moreover, to try to impose a total order
on all the paths (i.e. the cartesian product defined in Section 3.3.2) would be a mistake;
it would mean that ambiguities, represented in our formalism by existence of more
than one (&amp;quot;best&amp;quot;) interpretation of a text, are outlawed.
</bodyText>
<subsubsectionHeader confidence="0.48474">
3.3.2 Cartesian Products. Formalization of the interpretation produced in 3.3.1. is pre-
</subsubsectionHeader>
<bodyText confidence="0.81368175">
sented below. Any path through the graph of Figure 1 is an element of the cartesian
product TT xpEsubformulas(S) &lt;4, of the partial orderings.
Figure 2 explains the geometric intuitions we associate with the product and the
ordering. The product itself is given by the following definition:
</bodyText>
<subsectionHeader confidence="0.922817">
Definition
</subsectionHeader>
<bodyText confidence="0.998791">
Let F be a collection of formulas 0„ e &lt; m, for some natural number m; and let, for
each e, &lt;e be a partial ordering on the collection theories
</bodyText>
<equation confidence="0.88189475">
TO Tel • • • 7-Pe —4 Terie }
of tpe. Define:
II(F) H &lt;e= : (Ve m)(31 &lt; ne)[f(e) 111e 1711
e&lt;m
</equation>
<bodyText confidence="0.9999525">
We denote by &lt; the partial order induced on II(F) by the orderings &lt;, and the canoni-
cal ordering of subformulas (a formula is &amp;quot;greater&amp;quot; than its subformulas). The geomet-
rical meaning of this ordering can be expressed as &amp;quot;higher paths are more important
provided they pass through the most specific nodes.&amp;quot;
</bodyText>
<page confidence="0.969649">
182
</page>
<figure confidence="0.970918444444445">
Zadrozny and Jensen Semantics of Paragraphs
/
fint ,.....„ //1
„—
,e1, shl hi—di / PI ---&amp;quot;
fi-- I I I ,1 ---- I
e2 sh2 b2&apos; ta ii
I I I
ii a CI
</figure>
<figureCaption confidence="0.990362">
Figure 1
</figureCaption>
<bodyText confidence="0.752623333333333">
The partial ordering of theories of the referential level R and the ordering of interpretations.
Since (shl) and (b1) dominate (respectively) (sh2) and (b2), the path fnt represents a more
plausible interpretation than f&apos;.
</bodyText>
<figure confidence="0.976193142857143">
&lt;I &lt;2 &lt;3
&amp;quot;floe &amp;quot;strike&amp;quot; &amp;quot;strike a flog&amp;quot;
&amp;quot;cloth&amp;quot; &amp;quot;hit/anger&amp;quot; &amp;quot;lower&amp;quot; (I)
(4)
&amp;quot;toil&amp;quot;
• • •
d
</figure>
<figureCaption confidence="0.993325">
Figure 2
</figureCaption>
<bodyText confidence="0.995179083333333">
The cartesian product n &lt;,.=&lt;1 X &lt;2 X &lt;3 can be depicted as a collection of all paths through
the graphs representing the partial orderings; a path chooses only one element from each
ordering—thus (1) and (2) are &amp;quot;legal&amp;quot; paths, while (4) is not. Also, more plausible theories
appear higher: &amp;quot;cloth&amp;quot; &gt; &amp;quot;music&amp;quot; &gt; a. More specific paths are preferred: Assuming that all
higher paths, like path (1), are excluded by inconsistency, path (2) is the most plausible
interpretation of oz&amp;O, and it is preferred to (3). (More explanations in the text).
To make Figure 2 more intuitive we assigned some meanings to the partial orders.
Thus, &lt;1 represents some possible meanings of &amp;quot;flag,&amp;quot; shown with the help of the &amp;quot;key
words;&amp;quot; the meaning &amp;quot;piece of cloth&amp;quot; preferred to &amp;quot;deer&apos;s tail.&amp;quot; The word &amp;quot;strike&amp;quot; has
dozens of meanings, and we can imagine that the meaning of the transitive verb being
represented by &lt;2, with &amp;quot;hit in anger&amp;quot; at the top, then &amp;quot;hit, e.g. a ball&amp;quot; and &amp;quot;discover&amp;quot;
equally preferred, and then all other possible meanings. The trivial &lt;3 representing
</bodyText>
<page confidence="0.985653">
183
</page>
<note confidence="0.277053">
Computational Linguistics Volume 17, Number 2
</note>
<bodyText confidence="0.999146956521739">
&amp;quot;strike a flag&amp;quot; should remind us that we already know all that from Section 3.2. Notice
that path (2) does not give us the correct interpretation of &amp;quot;strike the flag,&amp;quot; which is
created from &amp;quot;cloth&amp;quot;-a-&amp;quot;lower.&amp;quot;
Each element of the cartesian product n &lt;i represents a set of possible meanings.
These meanings can be combined in various ways, the simplest of which consists of
taking their union as we did in 3.3.1. But a paragraph isn&apos;t just a sum of its sentences,
as a sentence isn&apos;t simply a concatenation of its phrases. The cohesion devices—such
as &amp;quot;but,&amp;quot; &amp;quot;unless,&amp;quot; &amp;quot;since&amp;quot;—arrange sentences together, and they also have semantic
functions. This is reflected, for instance, in the way various pieces of background
knowledge are pasted together. Fortunately, at this point we can abstract from this by
introducing an operator variable e whose meaning will be, as a default, that of a set
theoretic union, U; but, as we describe it in Section 6.2, it can sometimes be changed
to a more sophisticated join operator. There, when considering the semantics of &amp;quot;but,&amp;quot;
we&apos;ll see that referential level theories can be combined in slightly more complicated
ways. In other words, a partial theory corresponding to a paragraph cannot be just a
sum of the theories of its sentences—the arrangement of those theories should obey
the metalevel composition rules, which give the semantics of connectives. However,
from a purely formal point of view, ED can be any function producing a theory from a
collection of theories.
The cartesian product represents all possible amalgamations of these elementary
theories. In other words, this product is the space of possible combinations of mean-
ings, some of which will be inconsistent with the object level theory T. We can imme-
diately exclude the inconsistent combinations, eliminating at least some nonsense:
</bodyText>
<equation confidence="0.800016">
I-1(F) = If E II(F) : ef is consistent with T}
</equation>
<bodyText confidence="0.955559">
It remains now to fill in the details of the construction of PT&lt;. We assume that a text
P can be translated into a (ground) theory .15 (a set of logical sentences); T = Th(P) is
the set of logical consequences of P. We denote by F the set Form(Th(15))—the set of all
subformulas of Th(/5), about which we shall seek information at the referential level
R. If F =-- {01(6), ... ,on(en)} ce, is a collection of constants that are arguments of 0,),
is this theory, we have to describe a method of augmenting it with the background
knowledge. We can assume without loss of generality that each 0,(-e,) in F has, in R, a
corresponding partial order &lt;, of theories of 0,(54). We now substitute the constants ei
for the variables xj inside the theories of &lt;,. With a slight abuse of notation, we will use
the same symbol &lt;, for the new ordering. The product spaces II(F) and H(F) can then
be defined as before, with the new orderings in place of the ones with variables. Notice
that if only some of the variables of &apos;t/),(Y,) were bound by e„ the same construction
would work. We have arrived then at a general method of linking object level formulas
with their theories from R.
Now we can define PT&lt;(T) of the theory T as the set of most likely consistent
theories of T given by (II(F), &lt;), where F = Form(T):
PT&lt;(T) =-- {T UT&apos; : T&apos; = GI and f is a maximal element of (II(F), &lt;)}
Notice that PT&lt;(T) can contain more than one theory, meaning that T is ambiguous.
This is a consequence of the fact that the cartesian product is only partially ordered
by &lt;. The main reason for using ground instances V),(-6) in modifying the orderings
is the need to deal with multiple occurrences of the same predicate, as in
John went to the bank by the bank.
</bodyText>
<page confidence="0.986803">
184
</page>
<note confidence="0.766081">
Zadrozny and Jensen Semantics of Paragraphs
</note>
<bodyText confidence="0.975494142857143">
The above construction is also very close in spirit to Poole&apos;s (1988) method for default
reasoning, where object theories are augmented by ground instances of defaults.
3.3.3 Coherence Links. The reasoning that led to the intended interpretation fint in our
discussion of dominance was based on the partial ordering of the theories of R. We
want to exploit now another property of the theories of R—their coherence. Finding
an interpretation for a natural language text or sentence typically involves an appeal
to coherence. Consider
S2: Entering the port, a ship brought a disaster.
Using the coherence link between (b2) and (dr1) (cf. Section 3.2)—the presence of
cause(*,*) in the theories of &amp;quot;bring&amp;quot; and &amp;quot;disaster&amp;quot;—we can find a partial coherent
interpretation T E PT,(Th({S2})) of S2. In this interpretation, theories explaining the
meanings of terms are chosen on the basis of shared terms. This makes (b2) (&amp;quot;to bring&amp;quot;
means &amp;quot;to cause&amp;quot;) plausible and therefore it would be included in T. The formalization
of all this is given below:
</bodyText>
<subsectionHeader confidence="0.902161">
Definitions
</subsectionHeader>
<bodyText confidence="0.975332666666667">
• The set of all theories about the formulas of T is defined as:
Here, we ignore the ordering, because we are interested only in
connections between concepts (represented by words).
</bodyText>
<listItem confidence="0.773406333333333">
• If t, t&apos; E E(T), t t&apos;, share a predicate, we say that there is a c-link
between t and t&apos;. A c-path is defined as a chain of c-links; i.e. if
t = T and t&apos; = --&gt; T&apos; belong to a c-path, then V)
</listItem>
<bodyText confidence="0.972506666666667">
Under this condition, for any predicate, only one of its theories will
belong to a c-path. A c-path therefore chooses only one meaning for each
term.
</bodyText>
<listItem confidence="0.780276">
• C(T) will denote the set of all c-paths in E(T) consistent with T, i.e. for
each p e C(T), ep U T is consistent.
</listItem>
<bodyText confidence="0.987467">
This construction is like the one we have encountered when defining
ii(T). The details should be filled out exactly as before; we leave this to
the reader.
</bodyText>
<listItem confidence="0.99606">
• We define PTC (T) of a theory T as the set of most coherent consistent
theories of T given by C(T):
</listItem>
<equation confidence="0.945776">
PTc(T) = {T u T&apos; : = ep and p is a c — maximal element of C(T)}
</equation>
<bodyText confidence="0.997276857142857">
Going back to S2, PTc(Th(S2)) contains also the interpretation based on
the coherence link between &amp;quot;ship&amp;quot; and &amp;quot;bring,&amp;quot; which involves &amp;quot;carry.&amp;quot;
Based on the just-described coherence relations, we conclude that
sentence S2 is ambiguous; it has two interpretations, based on the two
senses of &amp;quot;bring.&amp;quot; Resolution of ambiguities involves factors beyond the
scope of this section—for instance, Gricean maxims and topic (Section 6),
or various notions of context (cf. Small et al. 1988). We will continue the
</bodyText>
<page confidence="0.981946">
185
</page>
<note confidence="0.268243">
Computational Linguistics Volume 17, Number 2
</note>
<bodyText confidence="0.919008333333333">
topic of the interaction of object level theories with background
knowledge by showing how the two methods of using background
knowledge can be combined.
</bodyText>
<equation confidence="0.501441">
3.3.4 Partial Theories. A finer interpretation of an object level theory T—its partial
theory—is obtained by the iteration:
PT(T) = PT&lt;(PTc(Th(T)))
PT is well defined after we specify that PT of a set of theories is the set of the PTs
(for both &lt; and C):
PT,({Ti, T2, . . .}) = PT Q(Ti) U PT,,(T2) U . . .
</equation>
<bodyText confidence="0.9895732">
Notice that coherence does not decide between (el) and (e2) given the above R, but
the iteration produces two theories of S2, both of which assert that the meaning of
&amp;quot;ship entered&amp;quot; is &amp;quot;ship came.&amp;quot;
A ship/boat came into the harbor/port and caused/brought a disaster.
A ship/boat came into the harbor/port and carried/brought a disaster.
</bodyText>
<equation confidence="0.691879">
PT({S1}) contains only one interpretation based on fint:
</equation>
<bodyText confidence="0.97776384">
A ship/boat came into the harbor/port and carried/brought a disease.
Partial theories will be the main syntactic constructs in the subsequent sections. In
particular, the p-models will be defined as some special models or partial theories of
paragraphs.
3.4 Summary and Discussion
We have shown that finding an interpretation of a sentence depends on two graph-
theoretical properties—coherence and dominance. Coherence is a purely &amp;quot;associative&amp;quot;
property; we are interested only in the existence of links between represented con-
cepts/theories. Dominance uses the directionality of the partial orders.
A partial theory PT(T) of an object theory T corresponding to a paragraph is
obtained by joining most plausible theories or sentences, collocations, and words of
the paragraph. However, this simple picture must be slightly retouched to account
for semantic roles of inter- and intra-sentential connectives such as &amp;quot;but,&amp;quot; and to
assure consistency of the partial theory. These modifications have complicated the
definitions a little bit. The above definitions capture the fact that even if, in principle,
any consistent combination of the mini-theories about predicates can be extended to
an interpretation, we are really interested only in the most plausible ones. The theory
PT(T) is called &amp;quot;partial&amp;quot; because it does not contain all knowledge about predicates—
less plausible properties are excluded from consideration, although they are accessible
should an inconsistency appear. Moreover, the partiality is related to the unutilized
possibility of iterating the operator PT (cf. Section 4).
How can we now summarize what we have learned about the three logical levels?
To begin with, one should notice that they are syntactically distinct. If object level
theories are expressed by collections of first order formulas, metalevel definitions—
e.g., to express as a default that ED is a set theoretical union—require another language,
</bodyText>
<page confidence="0.993579">
186
</page>
<note confidence="0.910907">
Zadrozny and Jensen Semantics of Paragraphs
</note>
<bodyText confidence="0.99997435483871">
such as higher order logic or set theory where one can define predicates dealing
with models, consistency, and provability. Even if all background knowledge were
described, as in our examples, by sets of first order theories, because of the preferences
and inconsistencies of meanings, we could not treat R as a flat database of facts—such
a model simply would not be realistic. Rather, R must be treated as a separate logical
level for these syntactic reasons, and because of its function—being a pool of possibly
conflicting semantic constraints.
The last point may be seen better if we look at some differences between our
system and KRYPTON, which also distinguishes between an object theory and back-
ground knowledge (cf. Brachman et al. 1985). KRYPTON&apos;s A-box, encoding the object
theory as a set of assertions, uses standard first order logic; the T-box contains informa-
tion expressed in a frame-based language equivalent to a fragment of FOL. However,
the distinction between the two parts is purely functional—that is, characterized in
terms of the system&apos;s behavior. From the logical point of view, the knowledge base
is the union of the two boxes, i.e. a theory, and the entailment is standard. In our
system, we also distinguish between the &amp;quot;definitional&amp;quot; and factual information, but
the &amp;quot;definitional&amp;quot; part contains collections of mutually excluding theories, not just
of formulas describing a semantic network. Moreover, in addition to proposing this
structure of R, we have described the two mechanisms for exploiting it, &amp;quot;coherence&amp;quot;
and &amp;quot;dominance,&amp;quot; which are not variants of the standard first order entailment, but
abduction.
The idea of using preferences among theories is new, hence it was described in
more detail. &amp;quot;Coherence,&amp;quot; as outlined above, can be understood as a declarative (or
static) version of marker passing (Hirst 1987; Charniak 1983), with one difference: the
activation spreads to theories that share a predicate, not through the IS-A hierarchy,
and is limited to elementary facts about predicates appearing in the text.
The metalevel rules we are going to discuss in Section 6, and that deal with the
Gricean maxims and the meaning of &amp;quot;but,&amp;quot; can be easily expressed in the languages
of set theory or higher order logic, but not everything expressible in those languages
makes sense in natural language. Hence, putting limitations on the expressive power
of the language of the metalevel will remain as one of many open problems.
</bodyText>
<subsectionHeader confidence="0.711735">
4. Coherence of Paragraphs
</subsectionHeader>
<bodyText confidence="0.999933647058823">
We are now in a position to use the notion of the referential level in a formal definition
of coherence and topic. Having done that, we will turn our attention to the resolution
of anaphora, linking it with the provability relation (abduction) 1-R±A4 and a metarule
postulating that a most plausible model of a paragraph is one in which anaphors have
references. Since the example paragraph we analyze has only one connective (&amp;quot;and&amp;quot;),
we can postpone a discussion of connectives until Section 6.
Building an interpretation of a paragraph does not mean finding all of its possible
meanings; the implausible ones should not be computed at all. This viewpoint has
been reflected in the definition of a partial theory as a most plausible interpretation
of a sequence of predicates. Now we want to restrict the notion of a partial theory by
introducing the formal notions of topic and coherence. We can then later (Section 5.2)
define p-models—a category of models corresponding to paragraphs—as models of
coherent theories that satisfy all metalevel conditions.
The partial theories pick up from the referential level the most obvious or the
most important information about a formula. This immediate information may be
insufficient to decide the truth of certain predicates. It would seem therefore that
the iteration of the PT operation to form a closure is needed (cf. Zadrozny 1987b).
</bodyText>
<page confidence="0.973566">
187
</page>
<note confidence="0.475553">
Computational Linguistics Volume 17, Number 2
</note>
<bodyText confidence="0.9997873125">
However, there are at least three arguments against iterating PT. First of all, iteration
would increase the complexity of building a model of a paragraph; infinite iteration
would almost certainly make impossible such a construction in real time. Secondly, the
cooperative principle of Grice (1975, 1978), under the assumption that referential levels
of a writer and a reader are quite similar, implies that the writer should structure the
text in a way that makes the construction of his intended model easy for the reader;
and this seems to imply that he should appeal only to the most direct knowledge of the
reader. Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit
information necessary for understanding a piece of text is about 8:1; furthermore, our
reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that
only the most direct or obvious inferences are being made in the process of building a
model or constructing a theory of a paragraph. Thus, for example, we can expect that
in the worst case only one or two steps of such an iteration would be needed to find
answers to wh-questions.
Let P be a paragraph, let .15 = (s1,... , S) be its translation into a sequence of
logical formulas. The set of all predicates appearing in X will be denoted by Pred(X).
</bodyText>
<sectionHeader confidence="0.674784" genericHeader="method">
Definition
</sectionHeader>
<bodyText confidence="0.895097666666667">
Let T be a partial theory of a paragraph P. A sequence of predicates appearing in 15,
denoted by Tp, is called a topic of the paragraph P, if it is a longest sequence satisfying
the conditions (1) and (2) below:
</bodyText>
<figure confidence="0.880872571428571">
1. For all &amp;quot;sentences&amp;quot; S„ (a) or (b) or (c) holds:
(a) Direct reference to the topic:
Tp C Pred(S,)
(b) Indirect reference to the topic:
If lp c Pred(S,) &amp; (V) E T, then Tp C Pred(To)
(c) Direct reference to a previous sentence:
If 1/) e Pred(S,)&amp; (V) To) c T then Pred(S,i)nPred(cP -4 To)
</figure>
<listItem confidence="0.981438">
2. Either (i) or (ii) is satisfied:
(i) Existence of a topic sentence: Tp C Pred(S,), for some sentence S,;
(ii) Existence of a topic sentence: a theory of Tp belongs to R, i.e. if 0 is
the conjunction of predicates of Tp then 0 -÷ To E R, for
some To.
</listItem>
<bodyText confidence="0.999771857142857">
The last two conditions say that either the discussed concept (topic) already exists in
the background knowledge or it must be introduced in a sentence. For instance, we
can see that the sentence The effect of the Black Death was appalling can be assumed to
be a topic sentence.
The first three conditions make the requirements for a collection of sentences to
have a topic. Either every sentence talks about the topic (as, for instance, the first two
sentences of the paragraph about the Black Death), or a sentence refers to the topic
</bodyText>
<page confidence="0.993356">
188
</page>
<note confidence="0.91043">
Zadrozny and Jensen Semantics of Paragraphs
</note>
<bodyText confidence="0.954140689655172">
through background knowledge—the topic appears in a theory about an entity or a
relation of the sentence (in the case of Within twenty-four hours of infection ..., &amp;quot;infec-
tion&amp;quot; can be linked to &amp;quot;disease&amp;quot;—cf. Sections 2 and 4.2), or else a sentence elaborates
a fragment of the previous sentence (the theme The effect of... being developed in In
less than...).
The definition allows a paragraph to have more than one topic. For instance, a
paragraph consisting of
John thinks Mary is pretty. John thinks Mary is intelligent. John wants to marry
her.
can be either about {John(j), Mary (m), think(j , m, pretty (m))1 , or about John, Mary and
marrying. (Notice that the condition 2 (i) forbids us merging the two topics into a
larger one). Thus paragraphs can be ambiguous about what constitutes their topics.
The point is that they should have one.
It is also clear that what constitutes a topic depends on the way the content of
paragraph sentences is represented. In the last case, if &amp;quot;pretty&amp;quot; were translated into a
predicate, and not into a modifier of m (i.e. an operator), &amp;quot;John thinking about Mary&amp;quot;
could not be a topic, for it wouldn&apos;t be the longest sequence of predicates satisfying
the conditions (1) and (2).
We&apos;d like to put forward a hypothesis that this relationship between topics and
representations can actually be useful: Because the requirement that a well-formed
paragraph should have a topic is a very natural one (and we can judge pretty well
what can be a topic and what can&apos;t), we can obtain a new method for judging semantic
representations. Thus, if the naive first order representation containing pretty(m) as
one of the formulas gives a wrong answer as to what is the topic of the above, or
another, paragraph, we can reject it in favor of a (higher order) representation in
which adjectives and adverbs are operators, not predicates, and which provides us
with an intuitively correct topic. Such a method can be used in addition to the standard
criteria for judging representations, such as elegance and ability to express semantic
generalizations.
</bodyText>
<subsectionHeader confidence="0.823203">
Definition
</subsectionHeader>
<bodyText confidence="0.999505142857143">
A partial theory T E PT(P) of the paragraph P is coherent iff the paragraph P has a
topic.
A random permutation of just any sentences about a disease wouldn&apos;t be coher-
ent. But it would be premature to jump to the conclusion that we need more than just
existence of a topic as a condition for coherence. Although it may be the case that it
will be necessary in the future to introduce notions like &amp;quot;temporal coherence,&amp;quot; &amp;quot;deictic
coherence,&amp;quot; or &amp;quot;causal coherence,&amp;quot; there is no need to start multiplying beings now.
We can surmise that the random permutations we talk about would produce an in-
consistent theory; hence, the temporal, causal, and other aspects would be dealt with
by consistency. But of course at this point it is just a hypothesis.
An important aspect of the definition is that coherence has been defined as a prop-
erty of representation—in our case, it is a property of a formal theory. The existence of
the topic, the direct or indirect allusion to it, and anaphora (which will be addressed
below) take up the issue of formal criteria for a paragraph definition, which was raised
</bodyText>
<page confidence="0.987935">
189
</page>
<note confidence="0.480892">
Computational Linguistics Volume 17, Number 2
</note>
<bodyText confidence="0.99980721875">
by Bond and Hayes (1983) (cf. also Section 2.1). The question of paragraph length can
probably be attended to by limiting the size of p-models, perhaps after introducing
some kind of metric on logical data structures.
Still, our definition of coherence may not be restrictive enough: two collections
of sentences, one referring to &amp;quot;black&amp;quot; (about black pencils, black pullovers, and black
poodles), the other one about &amp;quot;death&amp;quot; (war, cancer, etc.), connected by a sentence
referring to both of these, could be interpreted as one paragraph about the new, broader
topic &amp;quot;black + death.&amp;quot; This problem may be similar to the situation in which current
formal grammars allow nonsensical but parsable collections of words (e.g., &amp;quot;colorless
green ideas... &amp;quot;), while before the advent of Chomskyan formalisms, a sentence was
defined as the smallest meaningful collection of words; Fowler (1965, p. 546) gives 10
definitions of a sentence.
It then seems worth differentiating between the creation of a new concept like
&amp;quot;black + death,&amp;quot; with a meaning given by a paraphrase of the example collection of
sentences, and the acceptance of the new concept—storing it in R. In our case the
concept &amp;quot;black + death,&amp;quot; which does not refer to any normal experiences, would be
discarded as useless, although the collection of sentences would be recognized as a
strange, even if coherent, paragraph.
We can also hope for some fine-tuning of the notion of topic, which would prevent
many offensive examples. This approach is taken in computational syntactic grammars
(e.g. Jensen 1986); the number of unlikely parses is severely reduced whenever pos-
sible, but no attempt is made to define only the so-called grammatical strings of a
language.
Finally, as the paragraph is a natural domain in which word senses can be reliably
assigned to words or sentences can be syntactically disambiguated, larger chunks of
discourse may be needed for precise assignment of topics, which we view as another
type of disambiguation. Notice also that for coherence, as defined above, it does not
matter whether the topic is defined as a longest, a shortest, or—simply—a sequence of
predicates satisfying the conditions (1) and (2); the existence of a sequence is equivalent
with the existence of a shortest and a longest sequence. The reason for choosing a
longest sequence as the topic is our belief that the topic should rather contain more
information about a paragraph than less.
</bodyText>
<subsectionHeader confidence="0.999919">
4.1 Comparison with Other Approaches
</subsectionHeader>
<bodyText confidence="0.9999588125">
At this point it may be proper to comment on the relationship between our theory of
coherence and theories advocated by others. We are going to make such a comparison
with the theories proposed by J. Hobbs (1979, 1982) that represent a more computa-
tionally oriented approach to coherence, and those of T.A. van Dijk and W. Kintch
(1983), who are more interested in addressing psychological and cognitive aspects of
discourse coherence. The quoted works seem to be good representatives for each of
the directions; they also point to related literature.
The approach we advocate is compatible with the work of these researchers, we
believe. There are, however, some interesting differences: first of all, we emphasize the
role of paragraphs; second, we talk about formal principles regulating the organization
and use of knowledge in language understanding; and third, we realize that natural
language text (such as an on-line dictionary) can, in many cases, provide the type of
commonsense background information that Hobbs (for example) advocated but didn&apos;t
know how to access. (There are also some other, minor, differences. For instance, our
three-level semantics does not appeal to possible worlds, as van Dijk and Kintch do;
neither is it objectivist, as Hobbs&apos; semantics seems to be.)
</bodyText>
<page confidence="0.988396">
190
</page>
<note confidence="0.905424">
Zadrozny and Jensen Semantics of Paragraphs
</note>
<bodyText confidence="0.988256979166667">
We shall discuss only the first two points, since the third one has already been
explained.
The chief difference between our approach and the other two lies in identifying the
paragraph as a domain of coherence. Hobbs, van Dijk, and Kintch distinguish between
&amp;quot;local&amp;quot; coherence—a property of subsequent sentences—and &amp;quot;global&amp;quot; coherence—a
property of discourse as a whole. Hobbs explains coherence in terms of an inventory
of &amp;quot;local,&amp;quot; possibly computable, coherence relations, like &amp;quot;elaboration,&amp;quot; &amp;quot;occasion,&amp;quot;
etc. (Mann and Thompson 1983 give an even more detailed list of coherence relations
than Hobbs.) Van Dijk and Kintch do this too, but they also describe &amp;quot;macrostructures&amp;quot;
representing the global content of discourse, and they emphasize psychological and
cognitive strategies used by people in establishing discourse coherence. Since we have
linked coherence to models of paragraphs, we can talk simply about &amp;quot;coherence&amp;quot;—
without adjectives—as a property of these models. To us the first &amp;quot;local&amp;quot; domain seems
to be too small, and the second &amp;quot;global&amp;quot; one too large, for constructing meaningful
computational models. To be sure, we believe relations between pairs of sentences are
worth investigating, especially in dialogs. However, in written discourse, the smallest
domain of coherence is a paragraph, very much as the sentence is the basic domain
of grammaticality (although one can also judge the correctness of phrases).
To see the advantage of assuming that coherence is a property of a fragment of a
text/discourse, and not a relation between subsequent sentences, let us consider for
instance the text
John took a train from Paris to Istanbul. He likes spinach.
According to Hobbs (1979, p. 67), these two sentences are incoherent. However, the
same fragment, augmented with the third sentence Mary told him yesterday that the
French spinach crop failed and Turkey is the only country. . . (ibid.) suddenly (for Hobbs)
becomes coherent. It seems that any analysis of coherence in terms of the relation
between subsequent sentences cannot explain this sudden change; after all, the first
two sentences didn&apos;t change when the third one was added. On the other hand, this
change is easily explained when we treat the first two sentences as a paragraph:
if the third sentence is not a part of the background knowledge, the paragraph is
incoherent. And the paragraph obtained by adding the third sentence is coherent.
Moreover, coherence here is clearly the result of the existence of the topic &amp;quot;John likes
spinach.&amp;quot;
We derive coherence from formal principles regulating the organization and use
of knowledge in language understanding. Although, like the authors discussed above,
we stress the importance of inferencing and background knowledge in determining
coherence, we also address the problem of knowledge organization; for us the cen-
tral problem is how a model emerges from such an organization. Hobbs sets forth
hypotheses about the interaction of background knowledge with sentences that are
examined at a given moment; van Dijk and Kintch provide a wealth of psychological
information on that topic. But their analyses of how such knowledge could be used
are quasi-formal. Our point of departure is different: we assume a certain simple struc-
ture of the referential level (partial orders) and a natural way of using the knowledge
contained there (&amp;quot;coherence links&amp;quot; + &amp;quot;most plausible = first&amp;quot;). Then we examine what
corresponds to &amp;quot;topic&amp;quot; and &amp;quot;coherence&amp;quot;—they become mathematical concepts. In this
sense our work refines these concepts, changes the way of looking at them by linking
them to the notion of paragraph, and puts the findings of the other researchers into a
new context.
</bodyText>
<page confidence="0.984545">
191
</page>
<note confidence="0.508018">
Computational Linguistics Volume 17, Number 2
</note>
<subsectionHeader confidence="0.400744">
5. Models of Paragraphs
</subsectionHeader>
<bodyText confidence="0.999987944444444">
We argue below that paragraphs can be mapped into models with small, finite uni-
verses. We could have chosen another, more abstract semantics, with infinite models,
but in this and all cases below we have in mind computational reasons for this enter-
prise. Thus, as in the case of Kamp&apos;s (1981) DRS, we shall construct a kind of Herbrand
model of texts, with common and proper names translated into unary predicates, in-
transitive verbs into unary predicates, and transitive verbs into binary predicates. In
building the logical model M of a collection of formulas S. corresponding to the sen-
tences of a paragraph, we assume that the universe of M contains constants introduced
by elements of S, usually by ones corresponding to NPs, and possibly by some formu-
las picked by the construction from the referential level. However, we are interested
not in the relationship between truth conditions and representations of a sentence,
but in a formalization of the way knowledge is used to produce a representation of a
section of text. Therefore we need not only a logical description of the truth conditions
of sentences, as presented by Kamp, but also a formal analysis of how background
knowledge and metalevel operations are used in the construction of models. This
extension is important and nontrivial; we doubt that one can deal effectively with
coherence, anaphora, presuppositions or the semantics of connectives without it. We
have begun presenting such an analysis in Section 3, and we continue now.
</bodyText>
<subsectionHeader confidence="0.99817">
5.1 The Example Revisited: Preparation for Building a Model
</subsectionHeader>
<bodyText confidence="0.999900666666667">
We return now to the example paragraph, to illustrate how the interaction between
an object theory and a referential level produces a coherent interpretation of the text
(i.e., a p-model) and resolves the anaphoric references. The method will be similar
to, but more formal than, what was presented in Section 2. In order not to bore the
reader with the same details all over again, we will use a shorter version of the same
text.
</bodyText>
<subsectionHeader confidence="0.65733">
Example 2
</subsectionHeader>
<bodyText confidence="0.957585421052632">
Pi: In 1347 a ship entered the port of Messina bringing with it the disease
that came to be known as the Black Death.
P2: It struck rapidly.
P3: Within twenty-four hours of infection came an agonizing death.
5.1.1 Translation to Logic. The text concerns events happening in time. Naturally, we
will use a logical notation in which formulas may have temporal and event compo-
nents. We assume that any formal interpretation of time will agree with the intuitive
one. So it is not necessary now to present a formal semantics here. The reader may
consult recent papers on this subject (e.g. Moens and Steedman 1987; Webber 1987) to
see what a formal interpretation of events in time might look like. Since sentences can
refer to events described by other sentences, we may need also a quotation operator;
Perlis (1985) describes how first order logic can be augmented with such an operator.
Extending and revising Jackendoff&apos;s (1983) formalism seems to us a correct method
to achieve the correspondence between syntax and semantics expressed in the gram-
matical constraint (&amp;quot;that one should prefer a semantic theory that explains otherwise
arbitrary generalizations about the syntax and the lexicon&amp;quot;—ibid.).
However, as noted before, we will use a simplified version of such a logical no-
tation; we will have only time, event, result, and property as primitives. After these
remarks we can begin constructing the model of the example paragraph. We assume
</bodyText>
<page confidence="0.993164">
192
</page>
<note confidence="0.413538">
Zadrozny and Jensen Semantics of Paragraphs
</note>
<bodyText confidence="0.507601">
that constants are introduced by NPs. We have then
</bodyText>
<listItem confidence="0.475468714285714">
(i) Constants s, m, d, i, b, 1347 satisfying: ship(s), Messina(m), disease(d),
infection(i), death(b), year(1347).
(ii) Formulae
S1: [time: year(1347); event: enter (s , m) &amp; ship (s) &amp; port (m) &amp;
bring(xo,d) &amp; disease(d) &amp; name(d,BlackDeath) &amp; (xo -= s V
xo d V xo m)]
S2: time : past; event: rapidly: strike(yo) &amp; (yo = s V yo m V
</listItem>
<equation confidence="0.755179">
Yo d)]
S3: 3t, t&apos;lltime : t; infection(i)] &amp; [time: t&apos; E (t,t + 24h);
event : come(b) &amp; death(b) &amp; agonizing(b)]}
</equation>
<bodyText confidence="0.999667285714286">
The notation time: a(t); event : /3 should be understood as meaning
that the event described by the formula took place in (or during) the
time period described by the formula a(t), t ranges over instants of time
(not intervals).
Note. We assume that &amp;quot;strike&amp;quot; is used intransitively. But our construction of the
p-models of the paragraph would look exactly the same for the transitive meaning,
except that we would be expected to infer that people were harmed by the illness.
</bodyText>
<subsubsectionHeader confidence="0.615776">
5.1.2 Referential Level. We use only standard dictionaries as a source of background
</subsubsectionHeader>
<bodyText confidence="0.991308833333333">
information. The content of this information is of secondary importance—we want to
stress the formal, logical side of the interaction between the referential level and the
object theory. Therefore we represent both in our simplified logical notation, and not
in English. All formulas at the referential level below have been obtained by a direct
translation of appropriate entries in Webster&apos;s and Longman. The translation in this
case was manual, but could be automated.
</bodyText>
<listItem confidence="0.859234">
• Referential level (a fragment):
</listItem>
<equation confidence="0.996397875">
ship(x) --+ {large: boat(x); 3y carry(x, y) &amp; (people(y) v goods(y)) &amp; agent(x); ...I (sh1)
/* ship—a large boat for carrying people or goods on the sea */
bring(x, y) {carry(x, y); .} (b1)
/* bring—to carry */
strike(x, y) –4 {hit(x , y); agent (x) &amp; patient (y); .} (s1a)
strike(x) {hit(x); agent(x);...} (sib)
/* strike—to hit */
strike(x) --4 {illness(x) &amp; 3y suddenly : harm (x , y); . . .} (s2_ex)
</equation>
<bodyText confidence="0.66547">
/* strike—to harm suddenly; &amp;quot;they were struck by illness */
</bodyText>
<page confidence="0.979434">
193
</page>
<note confidence="0.394874">
Computational Linguistics Volume 17, Number 2
</note>
<equation confidence="0.939425166666667">
disease(y) {illness(y) &amp; 3z (infection(z) &amp; causes (z , y)); ...} (d1)
/* disease—illness caused by an infection */
death(x) -4 {]t, y[x [time: t; event : die(y) &amp; (creature(y) V plant (y))]1} (de_1)
/* death—an event in which a creature or a plant dies */
come(x) {3t[time : t; event : arrive(x)]} (ct_1)
/* to come—to arrive (... ) in the course of time */
infection(x) {3e, y, z[e = event : infect (y , z) &amp; person(y) &amp; disease (z))1
&amp; x = result (e)} (i_1)
/* infection—the result of being infected by a disease */
agonizing(x) causes(x,y) &amp; pain(y)} (a_1)
/* agonizing—causing great pain */
enter (x , y) --&gt; {come _in(x , y); place(y)} (e_1)
</equation>
<bodyText confidence="0.980405619047619">
/* enter—to come into a place */
We have shown, in Section 3, the role of preferences in building the model of a para-
graph. Therefore, to make our exposition clearer, we assume that all the above theories
are equally preferred. Still, some interesting things will happen before we arrive at our
intended model.
5.1.3 Provability and Anaphora Resolution. To formalize a part of the process for
finding antecedents of anaphors, we have to introduce a new logical notion—the re-
lation of weak R + M-abduction. This relation would hold, for instance, between the
object theory of our example paragraph and a formula expressing the equality of two
constants, i and i&apos;, denoting (respectively) the &amp;quot;infection&amp;quot; in the sentence Within twenty-
four hours of infection..., and the &amp;quot;infection&amp;quot; of the theory (d1)—a disease is an illness
caused by an infection. This equality i = i&apos; cannot be proven, but it may be reasonably
assumed—we know that in this case the infection i&apos; caused the illness, which, in turn,
caused the death.
The necessity of this kind of merging of arguments has been recognized before:
Charniak and McDermott (1985) call it abductive unification/matching, Hobbs (1978, 1979)
refers to such operations using the terms knitting or petty conversational implicature.
Neither Hobbs nor Charniak and McDermott tried then to make this notion precise,
but the paper by Hobbs et al. (1988) moves in that direction. The purpose of this
subsection is to formalize and explain how assumptions like that one above can be
made.
</bodyText>
<sectionHeader confidence="0.61611" genericHeader="method">
Definition
</sectionHeader>
<bodyText confidence="0.999929333333333">
A formula 0 is weakly provable from an object theory T, expressed as T HR 0, iff there
exists a partial theory T E PT(T) such that T 0, i.e. T proves 0 in logic. (We call HR
&amp;quot;weak&amp;quot; because it is enough to find one partial theory proving a given formula.)
As an example, in the case of the three-sentence paragraph, we have a partial
theory Ti based on (sib) saying that &amp;quot; &apos;it&apos; hits rapidly,&amp;quot; and T2 saying that &amp;quot;an illness
(&apos;it&apos;) harms rapidly&amp;quot; (s2_ex). Thus both statements are weakly provable.
</bodyText>
<page confidence="0.997337">
194
</page>
<note confidence="0.904558">
Zadrozny and Jensen Semantics of Paragraphs
</note>
<bodyText confidence="0.999917">
Since we view the metalevel constraints M rather as rules for choosing models than
as special inference rules, the definition of the R+M-abduction is model-theoretic, not
proof-theoretic:
</bodyText>
<subsectionHeader confidence="0.870622">
Definition
</subsectionHeader>
<bodyText confidence="0.999776153846154">
A preferred model of a theory T is an element of Mods(T) that satisfies metalevel con-
straints contained in M. The set of all preferred models of T is denoted by PM(T).
A formula q5 of L(=), the language with equality, is weakly R+ M-abductible from
an object theory T, denoted by T I-R+m 0, iff there exists a partial theory T e PT(T) and
a preferred model M E PM(T) such that M =0, i.e. 0 is true in at least one preferred
model of the partial theory T.
Note: The notions of strong provability and strong R + M-abduction can be in-
troduced by replacing &amp;quot;there exists&amp;quot; by &amp;quot;all&amp;quot; in the above definitions (cf. Zadrozny
1987b). We will have, however, no need for &amp;quot;strong&amp;quot; notions in this paper. Also, in a
practical system, &amp;quot;satisfies&amp;quot; should be probably replaced by &amp;quot;violates fewest.&amp;quot;
Obviously, it is better to have references of pronouns resolved than not. After
all, we assume that texts make sense, and that authors know these references. That
applies to references of noun phrases too. On the other hand, there must be some
restrictions on possible references; we would rather assume that &amp;quot;spinach&amp;quot; $ &amp;quot;train&amp;quot;
(i.e. V x,y)(spinach(x) &amp; train(y) --* x y)), or &amp;quot;ship&amp;quot; &amp;quot;disease.&amp;quot; Two elementary
conditions limiting the number of equalities are: an equality Ni .-- N2 may be assumed
only if either Ni and N2 are listed as synonyms (or paraphrases) or their equality is
explicitly asserted by the partial theory T. Of course there are other conditions, like
&amp;quot;typically, the determiner &apos;a&apos; introduces a new entity, while &apos;the&apos; refers to an already
introduced constant.&amp;quot; (But notice that in our example paragraph &amp;quot;infection&amp;quot; appears
without an article.) All these, and other, guidelines can be articulated in the form of
metarules.
We define another partial order, this time on models Mods(T) of a partial theory
T of a paragraph: Mi &gt;= M2, if M1, satisfies more R + M-abductible equalities than
M2. The principle articulating preference for having the references resolved can now
be expressed as
</bodyText>
<subsectionHeader confidence="0.793091">
Metarule 1
</subsectionHeader>
<bodyText confidence="0.9985919375">
Assume that T E PT(i5) is a partial theory of a paragraph P. Every preferred model
M E PM(T) is a maximal element of the ordering &gt;= of Mods(T).
To explain the meaning of the metarule, let us analyze the paragraph (131, P2, P3)
and the background knowledge needed for some kind of rudimentary understanding
of that text. The rule (i_1) (infection is a result of being infected by a disease...), dealing
with the infection i, introduces a disease di; we also know about the existence of the
disease d in 1347. Now, notice that there may be many models satisfying the object
theory of the paragraph P augmented by the background knowledge. But we can find
two among them: in one, call it M1, d and di are identical; in the other one, M2, they
are distinct. The rule says that only the first one has a chance to be a preferred model
of the paragraph; it has more noun phrase references resolved than the other model,
or—formally—it satisfies more R + M-abductible equalities, and therefore Mi &gt;= M2.
This reasoning, as the reader surely has noticed, resembles the example about
infections from the beginning of this section. The difference between the cases lies in
the equality d = d1 being the result of a formal choice of a model, while i = i&apos; wasn&apos;t
proved, just &amp;quot;reasonably&amp;quot; assumed.
</bodyText>
<page confidence="0.994824">
195
</page>
<note confidence="0.487935">
Computational Linguistics Volume 17, Number 2
</note>
<bodyText confidence="0.989323625">
In interpreting texts, knowledge of typical subjects and typical objects of verbs
helps in anaphora resolution (cf. Braden-Harder &amp; Zadrozny 1990). Thus if we know
that A farmer grows vegetables, either having obtained this information directly from
a text, or from R, we can reasonably assume that He also grows some cotton refers to
the farmer, and not to a policeman mentioned in the same paragraph. Of course, this
should be only a defeasible assumption, if nothing indicates otherwise. We now want
to express this strategy as a metarule:
Metarule 2
Let us assume that it is known that P (a , b) &amp; Q(a) &amp; R(b), and it is not known that
P (a&apos; , X), for any X. Then models in which P(a , c) &amp; R&apos; (c) holds are preferred to models
in which P (a&apos; , c) &amp; R&apos; (c) is true.
One can think of this rule as a model-theoretic version of Ockham&apos;s razor or
abduction; it says &amp;quot;minimize the number of things that have the property P(*,*),&amp;quot;
and it allows us to draw certain conclusions on the basis of partial information. We
shall see it in action in Section 5.2.
We have no doubts that various other metarules will be necessary; clearly, our
two metarules cannot constitute the whole theory of anaphora resolution. They are
intended as an illustration of the power of abduction, which in this framework helps
determine the universe of the model (that is the set of entities that appear in it). Other
factors, such as the role of focus (Grosz 1977, 1978; Sidner 1983) or quantifier scoping
(Webber 1983) must play a role, too. Determining the relative importance of those
factors, the above metarules, and syntactic clues, appears to be an interesting topic in
itself.
Note: In our translation from English to logic we are assuming that &amp;quot;it&amp;quot; is anaphoric
(with the pronoun following the element that it refers to), not cataphoric (the other
way around). This means that the &amp;quot;it&amp;quot; that brought the disease in P1 will not be con-
sidered to refer to the infection &amp;quot;i&amp;quot; or the death &amp;quot;d&amp;quot; in P3. This strategy is certainly
the right one to start out with, since anaphora is always the more typical direction of
reference in English prose (Halliday and Hasan 1976, p. 329).
Since techniques developed elsewhere may prove useful, at least for comparison,
it is worth mentioning at this point that the proposed metarules are distant cousins
of &amp;quot;unique-name assumption&amp;quot; (Genesereth and Nilsson 1987), &amp;quot;domain closure as-
sumption&amp;quot; (ibid.), &amp;quot;domain circumscription&amp;quot; (cf. Etherington and Mercer 1987), and
their kin. Similarly, the notion of R + M-abduction is spiritually related to the &amp;quot;abduc-
tive inference&amp;quot; of Reggia (1985), the &amp;quot;diagnosis from first principles&amp;quot; of Reiter (1987),
&amp;quot;explainability&amp;quot; of Poole (1988), and the subset principle of Berwick (1986). But, ob-
viously, trying to establish precise connections for the metarules or the provability
and the R + M-abduction would go much beyond the scope of an argument for the
correspondence of paragraphs and models. These connections are being examined
elsewhere (Zadrozny forthcoming).
</bodyText>
<subsectionHeader confidence="0.998701">
5.2 p-Models
</subsectionHeader>
<bodyText confidence="0.999900333333333">
The construction of a model of a paragraph, a p-model, must be based on the in-
formation contained in the paragraph itself (the object theory) and in the referential
level, while the metalevel restricts ways that the model can be constructed, or, in other
words, provides criteria for choosing a most plausible model(s), if a partial theory is
ambiguous. This role of the metarules will be clearly visible in finding references of
pronouns in a simple case requiring only a rule postulating that these references be
</bodyText>
<page confidence="0.997708">
196
</page>
<note confidence="0.908779">
Zadrozny and Jensen Semantics of Paragraphs
</note>
<bodyText confidence="0.968834782608696">
searched for, and in a more complex case (in Section 5) when they can be found only
by an interplay of background knowledge and (a formalization of) Gricean maxims.
Definition
M is a p-model of a paragraph P iff there exists a coherent partial theory T E PT(i5)
such that M E PM(T).
Having defined the notion of a p-model, we can mimic now, in logic, the rea-
soning presented in Section 2.2. Using background information and the translation of
sentences, we build a p-model of the paragraph. This involves determining the ref-
erences of the pronoun &amp;quot;it,&amp;quot; and deciding whether &amp;quot;struck&amp;quot; in the sentence It struck
rapidly means &amp;quot;hit&amp;quot; (sib) or &amp;quot;harmed&amp;quot; (s2_ex). We have then two meanings of &amp;quot;strike&amp;quot;
and a number of possibilities for the pronouns.
We begin by constructing the two classes of preferred models given by (sib) and
(s2_ex), respectively. It is easily seen that, in the models of the first class, based on
{S2, slb}, (that is {rapidly : strike(yo) and strike(x) hit(x)...}), together with all other
available information, do not let us R+M-abduct anything about yo, i.e., the referent for
the subject pronoun &amp;quot;it&amp;quot; in P2 (it struck rapidly). On the other hand, from {S2, s2_ex, dl}
we R M-abduct that yo = d, i.e. the disease struck rapidly. That is the case because
s2_ex implies that the agent that &amp;quot;struck rapidly&amp;quot; is actually an illness. From rapidly:
strike(yo), strike(x) illness(x) &amp; disease(y) illness(y) &amp; ..., and disease(d) we can
infer illness(yo) and illness(d); by the Metarule (1) we conclude that yo = d. In other
words, the referent for the subject &amp;quot;it&amp;quot; is &amp;quot;disease.&amp;quot; Thus the Metarule (1) immediately
eliminates all the models from the first class given by (sib), in which &amp;quot;struck&amp;quot; means
&amp;quot;hit.&amp;quot;
Notice that we cannot prove in classical logic that the ship has brought the disease.
But we are allowed to assume it by the above formal rule as the most plausible state
of affairs, or—in other words—we prove it in our three-level logic.
We are left then with models of the three sentences (S1, S2, 53) that contain {S2,
s2_ex, dl}; they all satisfy yo d. We now use {Si , ski , bl} (enter (s , m) &amp; ship(s) &amp;
bring(xo, d) &amp; . . .; ship(s) (4)carry(s, y) &amp; . . .; V z[bring (xo, Z) carry(xo, z)]). From
these facts we can conclude by Metarule (1) that xo -= s: a &amp;quot;ship&amp;quot; is an agent that carries
goods; to &amp;quot;bring&amp;quot; means to &amp;quot;carry&amp;quot;; and the disease has been brought by something—
we obtain carry (xo , d) and carry (s, y); and then by Metarule (2), carry (s , d). That is, the
referent for the pronoun &amp;quot;it&amp;quot; in P1 (...bringing with it the disease...) should be &amp;quot;ship.&amp;quot;
Observe that we do not assert about the disease that it is a kind of goods or people;
the line of reasoning goes as follows: since ships are known to carry people or goods,
and ports are not known to carry anything, we may assume that the ship carried the
disease along with its standard cargo.
Having resolved all pronoun references, with no ambiguity left, we conclude that
the class PM(P) consists of only one model, based on the the partial theory
{S1, S2, S3, ski., bl, e_1, s2_ex, dl, del, il, al, ct1}.
The model describes a situation in which the ship came into the port/harbor; the
ship brought the disease; the disease was caused by an infection; the disease harmed
rapidly, causing a painful death; and so on.
The topic Tp of (Pl. P2/ P3) is the disease(x). The first sentence talks about it; the
second one refers to it using the pronoun &amp;quot;it,&amp;quot; and the third one extends our knowl-
edge about the topic, since &amp;quot;disease&apos; is linked to &amp;quot;infection&amp;quot; through dl. Furthermore,
</bodyText>
<page confidence="0.976553">
197
</page>
<figure confidence="0.9983898125">
Computational Linguistics Volume 17, Number 2
i i
place (m)1
I
L
I so{ come )-0
m
I ./4harbor (m)1
j
r /
ship (s) i port (m)
time :1347
enter
s , d /--- &amp;quot;.&amp;quot;*.] name : Black Death
1 disease (d) - --As_
n
I s d i &apos;
i
carry H ..)-4 -(causes)40
L
I I infection (i) illness (d)I
L _.i —J
d _ i
•{ strike )- -.1 14 harm ) i
&apos;..I rapidly I
P2 1 illness(d) 3...-Isuddenly I
L _J
o4 bring )-•
name: Messina
P3
i
b t
o{ come )-
- -death (b)
agonizing (b)
I
i
I
i
1
L.
o event (e) -1 I
i
P , , d I
0-k infect )-• 1
person (p) disease (d)I
J
/
r i
, 1
I 0 event (es) I
Ie&apos; i
I
1
1 ( die )-o I
P
I
L _ person (p)j
I-
...).1 in-time (24+ 0
t LI i
0 infection (i)
i
time : t
</figure>
<figureCaption confidence="0.8319098">
Figure 3
The p-model for the example paragraph
&amp;quot;disease&amp;quot; is the only noun phrase mentioned or referred to in all three sentences; the
sentence Si is the topic sentence. The p-model of the paragraph is represented by
Figure 3.
</figureCaption>
<bodyText confidence="0.999847">
But notice that our definition of topic licences also other analyses, for example, one
in which all the predicates of the first sentence constitute the topic of the paragraph, S2
elaborates Si (in the sense of condition 1 (c) of the definition of topic), and S3 elaborates
S2. Based on the larger context, we prefer the first analysis; however, a computational
criterion for such a preference remains as an open problem.
</bodyText>
<sectionHeader confidence="0.798285" genericHeader="method">
6. On the Role of the Metalevel
</sectionHeader>
<bodyText confidence="0.96784425">
We have already seen examples of the application of metalevel rules. In the analysis
of the paragraph, we applied one such rule expressing our commonsense knowledge
about the usage of pronouns. In this section we discuss two other sources of met-
alevel axioms: Gricean cooperative principles, which reduce the number of possible
</bodyText>
<page confidence="0.996865">
198
</page>
<note confidence="0.938255">
Zadrozny and Jensen Semantics of Paragraphs
</note>
<bodyText confidence="0.998826413793104">
interpretations of a text or an utterance; and connectives and modalities—such as
&amp;quot;but,&amp;quot; &amp;quot;unless,&amp;quot; or &amp;quot;maybe&amp;quot;—which refer to the process of constructing the models
or partial theories, and to some operations on them (see Figure 4).
We can see then two applications of metarules: in constructing models of a text
from representations of sentences, and in reducing, or constraining, the ambiguity
of the obtained structure. We begin by showing how to formalize the latter. In the
next subsection (6.1), assuming the Gricean maxims to be constraints on language
communication, either spoken or written, we use their formal versions in building
partial theories. A specific instance of the rule of &amp;quot;quantity&amp;quot; turns out to be applicable
to anaphora resolution. That example will end our discussion of anaphora in this
article.
The last topic we intend to tackle is the semantic role of conjunctions. In subsection
6.2 we present a metalevel axiom dealing with the semantic role of the adversative con-
junction &amp;quot;but;&amp;quot; then we talk about some of its consequences for constructing models
of text. This will complete our investigation of the most important issues concerning
paragraph structure: coherence (how one can determine that a paragraph expresses
a &amp;quot;thought&amp;quot;), anaphora (how one can compute &amp;quot;links&amp;quot; between entities that a para-
graph talks about), and cohesion (what makes a paragraph more than just a sum of
sentences). Of course, we will not have final answers to any of these problems, but
we do believe that the/a direction of search for computational models of text will be
visible at that point.
We assume a flat structure of the metalevel, envisioning it as a collection of (closed)
formulas written in the language of set theory or higher order logic. In either of the two
theories it is possible to define the notions of a model, satisfiability, provability, etc. for
any first order language (cf. e.g. Shoenfield 1967); therefore the metalevel formulas can
say how partial theories should be constructed (specifying for instance the meaning of
e) and what kinds of models are admissible. The metarules thus form a logical theory
in a special language, such as the language of ZF-set theory. However, for the sake of
readability, we express all of them in English.
</bodyText>
<subsectionHeader confidence="0.804383">
6.1 A Formalization of Gricean Maxims
</subsectionHeader>
<bodyText confidence="0.999834909090909">
A Gricean Cooperative Principle applies to text, too. For instance, in normal writing
people do not express common knowledge about typical functions of objects. In fact,
as the reader may check for himself, there is nothing in Gricean maxims that does not
apply to written language. That the maxims play a semantic role is hardly surprising.
But that they can be axiomatized and used in building formal models of texts is new.
We present in the next couple of paragraphs our formalization of the first maxim, and
sketch axiomatizations of the others. Then we will apply the formal rule in an example.
Gricean maxims, after formalization, belong to the metalevel. This can be seen from
our formalization of the rule &amp;quot;don&apos;t say too much.&amp;quot; To this end we define redundancy
of a partial theory T of a paragraph as the situation in which some sentences can be
logically derived from other sentences and from the theory T in a direct manner:
</bodyText>
<equation confidence="0.592327">
(35 E P) (2o- E R) [o- E T &amp; a -=- V) -4 q5 &amp; P h tp &amp; {b} u (P — {S}) h 5]
</equation>
<bodyText confidence="0.998084833333333">
The meaning of this formula can be explained as follows: a paragraph P has been
translated into its formal version P and is to be examined for redundancy. Its partial
theory PT(P) has also been computed. The test will turn positive if, for some sentence
S. we can find a rule/theorem a = IP --+ 0 in PT(P) such that the sentence S is implied
(in a classical logic) by the other sentences and a. For example, if the paragraph
about Black Death were to contain also the sentence The ship carried people or goods, or
</bodyText>
<page confidence="0.993901">
199
</page>
<note confidence="0.424443">
Computational Linguistics Volume 17, Number 2
</note>
<listItem confidence="0.99992875">
• Quantity. Say neither too much nor too little.
• Quality. Try to make your contribution one that is true.
• Relation. Be relevant.
• Manner. Avoid obscurity and ambiguity; be brief and orderly.
</listItem>
<figureCaption confidence="0.913408">
Figure 4
</figureCaption>
<subsectionHeader confidence="0.406828">
The Gricean maxims
</subsectionHeader>
<bodyText confidence="0.9997755">
both, which (in its logical form) belongs to R, it would be redundant: a- = (shl), there.
Similarly, the definition takes care of the redundancy resulting from a simple repetition.
</bodyText>
<subsectionHeader confidence="0.990744">
Metarule Gla
</subsectionHeader>
<bodyText confidence="0.9999413125">
(nonredundancy) If T1, T2 G PTA and 1&apos;1 is less redundant than T2, then the theory
T1 is preferred to T2. (Where &amp;quot;less redundant&amp;quot; means that the number of redundant
sentences in T1 is smaller than in T2)
The relevant half of the Maxim of Quantity has been expressed by Gla. How
would we express the other maxims? The &amp;quot;too little&amp;quot; part of the first maxim might
be represented as a preference for unambiguous partial theories. The second maxim
has been assumed all the time—when constructing partial theories or models, the
sentences of a paragraph are assumed to be true. The Maxim of Manner seems to us
to be more relevant for critiquing the style of a written passage or for natural language
generation; in the case of text generation, it can be construed as a requirement that
the produced text be coherent and cohesive.
We do not claim that Gla is the best or unique way of expressing the rule &amp;quot;assume
that the writer did not say too much.&amp;quot; Rather, we stress the possibility that one can
axiomatize and productively use such a rule. We shall see this in the next example:
two sentences, regarded as a fragment of paragraph, are a variation on a theme by
Hobbs (1979).
</bodyText>
<subsectionHeader confidence="0.841739">
Example 3
</subsectionHeader>
<bodyText confidence="0.954643">
The captain is worried because the third officer can open his safe. He knows the combination.
The above metarule postulating &amp;quot;nonredundancy&amp;quot; implies that &amp;quot;he&amp;quot; = &amp;quot;the third
officer,&amp;quot; &amp;quot;his&amp;quot; = &amp;quot;the captain&apos;s&amp;quot; are the referents of the pronouns. This is because the
formula
</bodyText>
<equation confidence="0.612472">
safe(x) (owns(y , x) &amp; cmbntn(z, x) knows(y , z) &amp; can_open(y , x)) G Tsafe,
</equation>
<bodyText confidence="0.8900584">
belongs to R, since it is common knowledge about safes that they have owners, and
also combinations that are known to the owners. Therefore &amp;quot;his&amp;quot; = &amp;quot;the third officer&apos;s&amp;quot;
would produce a redundant formula, corresponding to the sentence The third officer
can open the third officer&apos;s safe. By the same token, The captain knows the combination
would be redundant too.
</bodyText>
<page confidence="0.98909">
200
</page>
<note confidence="0.932983">
Zadrozny and Jensen Semantics of Paragraphs
</note>
<bodyText confidence="0.9269336">
We now explain the details of this reasoning. One first proves that &amp;quot;his&amp;quot; = &amp;quot;the
captain&apos;s.&amp;quot; Indeed, if &amp;quot;his&amp;quot; = &amp;quot;the third officer&apos;s,&amp;quot; then our example sentence would
mean
? The captain is worried because the third officer can open the third officer&apos;s safe;
in logic:
</bodyText>
<equation confidence="0.974742">
captain(x) &amp; worry(x,$) &amp; sentence(s)
</equation>
<bodyText confidence="0.944109724137931">
&amp; s = &apos;the third officer can open the third officer&apos;s safe.&apos;
We assume also, based on common knowledge about worrying, that worry(x&apos; , s&apos;)
S. That is, one worries about things that might possibly be or become true (S denotes
the logical formula corresponding to the sentence s, cf. Section 3); but one doesn&apos;t
worry about things that are accepted as (almost) always true (such as the law of
gravity), so that worry(x&apos;, &apos;s&apos;) E T f), where f ranges over subformulas of S.
In our case, S immediately follows from Tsafe and X, where X = safe(sf) &amp; third_offi-
cer(o) &amp; owns(o, sf)—the fact that &amp;quot;the third officer can open the third officer&apos;s safe&amp;quot; is
a consequence of general knowledge about the ownership of safes. And therefore the
interpretation with &amp;quot;his&amp;quot; = &amp;quot;the captain&apos;s&amp;quot; is preferred as less redundant by the rule
Gla. This theory contains representations of the two sentences, the theory of safes, a
theory of worrying, and the equality &amp;quot;his&amp;quot; = &amp;quot;captain&apos;s.&amp;quot;
It remains to prove that &amp;quot;he&amp;quot; = &amp;quot;the third officer.&amp;quot; Otherwise we have
P1. The captain is worried because the third officer can open the captain&apos;s safe.
P2. ? The captain knows the combination.
Clearly, the last sentence is true but redundant—the theory of &amp;quot;safe&amp;quot; and P1 entail P2:
{P1} U Tsafe I- P2
We are left with the combination
Ql. The captain is worried because the third officer can open the captain&apos;s safe.
Q2. ? The third officer knows the combination.
In this case, Q2 does not follow from {Q1} U T5a1e and therefore Ql, Q2 is preferred to
P1, P2 (by Gla). We obtain then
The captain is worried because the third officer can open the captain&apos;s safe.
The third officer knows the combination
as the most plausible interpretation of our example sentences.
Note: The reader must have noticed that we did not bother to distinguish the
sentences Pl, P2, Q1 and Q2 from their logical forms. Representing &amp;quot;because&amp;quot; and
&amp;quot;know&amp;quot; adequately should be considered a separate topic; representing the rest (in
the first order convention of this paper) is trivial.
</bodyText>
<footnote confidence="0.7553235">
6.1.1 Was the Use of a Gricean Maxim Necessary? Can one deal effectively with the
problem of reference without axiomatized Gricean maxims, for instance by using only
</footnote>
<page confidence="0.990657">
201
</page>
<note confidence="0.592187">
Computational Linguistics Volume 17, Number 2
</note>
<bodyText confidence="0.9983945">
&amp;quot;petty conversational implicature&amp;quot; (Hobbs 1979), or the metarules of Section 5.2? It
seems to us that the answer is no.
As a case in point, consider the process of finding the antecedent of the anaphor
&amp;quot;he&amp;quot; in the sentences
</bodyText>
<subsectionHeader confidence="0.884001">
John can open Bill&apos;s safe. He knows the combination.
</subsectionHeader>
<bodyText confidence="0.994751">
Hobbs (1979, 1982) proves &amp;quot;he&amp;quot; = &amp;quot;John&amp;quot; by assuming the relation of &amp;quot;elaboration&amp;quot;
between the sentences. (Elaboration is a relation between two segments of a text. It
intuitively means &amp;quot;expressing the same thought from a different perspective,&amp;quot; but has
been defined formally as the existence of a proposition implied by both segments—
here the proposition is &amp;quot;John can open the safe&amp;quot;) However, if we change the pair to
the triple
Bill has a safe under the painting of his yacht. John can open Bill&apos;s safe. He knows
the combination
the relation of elaboration holds between the segment consisting of the first two sen-
tences of the triple and each of the two possible readings: John knows the combination
and Bill knows the combination. In this case, elaboration cannot choose the correct ref-
erent, but the rule Gla can and does. Clearly, an elaboration should not degenerate
into redundancy; the Gricean maxims are to keep it fresh.
As we have observed, correct interpretations cannot be chosen by an interaction of
an object level theory and a referential level alone, because coherence, plausibility and
consistency are too weak to weed out wrong partial theories. Metarules are necessary.
True, the captain knew the combination, but it was consistent that &amp;quot;his&amp;quot; might have
referred to &amp;quot;the third officer&apos;s.&amp;quot;
</bodyText>
<subsectionHeader confidence="0.99964">
6.2 Semantics of the Conjunction &amp;quot;But&amp;quot;
</subsectionHeader>
<bodyText confidence="0.999776692307692">
Any analysis of natural language text, to be useful for a computational system, will
have to deal with coherence, anaphora, and connectives. We have examined so far
the first two concepts; we shall present now our view of connectives to complete the
argument about paragraphs being counterparts of models. We present a metalevel rule
that governs the behavior of the conjunction &amp;quot;but&amp;quot; we formalize the manner in which
&amp;quot;but&amp;quot; carries out the contradiction. Then we derive from it two rules that prevent
infelicitous uses of &amp;quot;but.&amp;quot;
Connectives are function words—like conjunctions and some adverbs—that are
responsible simultaneously for maintaining cohesiveness within the text and for sig-
naling the nature of the relationships that hold between and among various text units.
&amp;quot;And,&amp;quot; &amp;quot;or,&amp;quot; and &amp;quot;but&amp;quot; are the three main coordinating connectives in English. How-
ever, &amp;quot;but&amp;quot; does not behave quite like the other two—semantically, &amp;quot;but&amp;quot; signals a
contradiction, and in this role it seems to have three subfunctions:
</bodyText>
<listItem confidence="0.659784">
1. Opposition (called &amp;quot;adversative&amp;quot; or &amp;quot;contrary-to-expectation&amp;quot; by
Halliday and Hasan 1976; cf. also Quirk et al. 1972, p. 672).
</listItem>
<bodyText confidence="0.6598855">
The ship arrived but the passengers could not get off.
The yacht is cheap but elegant.
</bodyText>
<listItem confidence="0.685171">
2. Comparison. In this function, the first conjunct is not so directly
contradicted by the second. A contradiction exists, but we may have to
</listItem>
<page confidence="0.992532">
202
</page>
<note confidence="0.908716">
Zadrozny and Jensen Semantics of Paragraphs
</note>
<bodyText confidence="0.833445117647059">
go through additional levels of implication to find it. Consider the
sentence:
That basketball player is short, but he&apos;s very quick.
3. Affirmation. This use of &amp;quot;but&amp;quot; always follows a negative clause, and
actually augments the meaning of the preceding clause by adding
supporting information:
The disease not only killed thousands of people, but also ended a period
of economic welfare.
In this section we consider only the first, or adversative, function of the coordinating
conjunction &amp;quot;but.&amp;quot;
6.2.1 The Semantic Function of &amp;quot;But&amp;quot;. &amp;quot;But&amp;quot; introduces an element of surprise into
discourse. Because it expresses some kind of contradiction, &amp;quot;but&amp;quot; has no role in the
propositional calculus equivalent to the roles filled by &amp;quot;and&amp;quot; and &amp;quot;or.&amp;quot; Although there
are logical formation rules using the conjunction operator (&amp;quot;and&amp;quot;) and the disjunction
operator (&amp;quot;or&amp;quot;), there is no &amp;quot;but&amp;quot; operator. What, then, is the semantic role of &amp;quot;but&amp;quot;?
We believe that its function should be described at the metalevel as one of many rules
guiding the construction of partial theories. This is expressed below.
</bodyText>
<subsectionHeader confidence="0.935268">
Metarule (BUT)
</subsectionHeader>
<bodyText confidence="0.99958047368421">
The formulas 43 but xli, 43&apos; but kV, ... of a (formal representation of) paragraph P are
to be interpreted as follows:
In the construction of any T E MI5) instead of taking ef to be the union U
of a —+ T,,, take the union of a -- T,/{41, tIf&apos;, ...}.
The symbol a --4 Tc,/{41, tIf&apos;, ...} denotes a maximal consistent with {a-, 4i, 11/&apos;, ...} sub-
theory of a -4 T,, and in general T/T&apos; will be a maximal consistent with T&apos; subtheory
of T.
&amp;quot;But&amp;quot; is then an order to delete from background information everything contra-
dicting 4f, but to use what remains. Notice that &amp;quot;and&amp;quot; does not have this meaning; a
model for 43 and W will not contain any part of a theory that contradicts either of the
clauses 43 or W.
Typically this rule will be used to override defaults, to say that the expected
consequences of the first conjunct hold except for the fact expressed by the second
conjunct; for instance: We were coming to see you, but it rained (so we didn&apos;t). The rule
BUT is supposed to capture the &amp;quot;contrary-to-expectation&amp;quot; function of &amp;quot;but.&amp;quot;
We present now a simple example of building a model of a one-sentence paragraph
containing &amp;quot;but.&amp;quot; We will use this example to explain how the rule BUT can be used.
Using background information presented below, we will construct a partial model for
this one-sentence paragraph.
</bodyText>
<page confidence="0.99461">
203
</page>
<figure confidence="0.7762">
Computational Linguistics Volume 17, Number 2
Example 4
This yacht is cheap, but it is elegant.
Referential level (a fragment)
</figure>
<equation confidence="0.9363556">
cheap(x) {-elegant(x); poor _quality (x); -expensive} (c1)
expensive(x) ---cheap(x) (end)
yacht(x) {ship(x) &amp; small(x)} (y1)
elegant(x) {-cheap(x),...} (el)
elegant(x) &amp; yacht(x) {status_symbol(x)} (e_y1)
</equation>
<bodyText confidence="0.95326825">
Note: Compare (y1) with (cl); in (yl) smallness is a property of a ship; this would
be more precisely expressed as yacht(x) [ship(x); property: small(x)]. This trick al-
lows us not to conclude that &amp;quot;a big ant is big,&amp;quot; or &amp;quot;a small elephant is small.&amp;quot;
We ignore the problem of multiple meanings (theories) of predicates, and assume
the trivial ordering in which all formulas are equally preferred. (But note that (e_y1)
is still preferred to (el) as a more specific theory of &amp;quot;elegant;&amp;quot; cf. Section 3.)
Construction of the model: In our case 4) E yacht(yo) &amp; cheap(yo) and W a.--
elegant(y0). Then
</bodyText>
<equation confidence="0.997848285714286">
Tcheap --= {-elegant(x); poor _quality(x) ; -expensive(x)}
Tyacht = {ship(x) &amp; small(x)}
Telegant = {expensive(x)}
Tyacht &amp; elegant = {Status_symbol(x)}
Tcheap =- {poor_quality(x) ; -expensive(x)}
Tyacht/111 = Tyacht
Tyacht&amp;elegant /41 = Tyacht&amp;elegant
</equation>
<bodyText confidence="0.996192">
We can now use the Metarule (BUT) and construct the partial theories of the sentence.
In this case, there is only one:
</bodyText>
<equation confidence="0.979999666666667">
PT(1, but T) = {T}, where
T = {yacht(y0), elegant(y0),cheap(yo),ship(yo) &amp; small (yo),
status_symbol (y0), poor_quality(Yo)}
</equation>
<bodyText confidence="0.9998526">
In other words, the yacht in question is a poor quality small and elegant ship serving
as an inexpensive status symbol.
The partial model of the theory T is quite trivial: it consists of one entity repre-
senting the yacht and of a bunch of its attributes. However, the size of the model is
not important here; what counts is the method of derivation of the partial theory.
</bodyText>
<footnote confidence="0.6285095">
6.2.2 Confirming the Analysis. The Metarule (BUT) is supposed to capture the &amp;quot;con-
trary-to-expectation&amp;quot; function of &amp;quot;but.&amp;quot; The next two rules follow from our formaliza-
</footnote>
<page confidence="0.994239">
204
</page>
<note confidence="0.921777">
Zadrozny and Jensen Semantics of Paragraphs
</note>
<bodyText confidence="0.977901794871795">
tion; their validity indirectly supports the plausibility of our analysis of &amp;quot;but.&amp;quot;
BUT_C1 : (I) but -ill&apos; is incorrect, if 4&apos; -- W is a &amp;quot;law.&amp;quot;
e.g. Henry was murdered but not killed.
Our referential level is a collection of partially ordered theories; we have expressed
the fact that a theory of some cb is a &amp;quot;law&amp;quot; is by deleting the empty interpretation of 0
from the partial order. If we accept the definition of a concept as given by necessary
and sufficient conditions, the theories would all appear as laws. If we subscribe to a
more realistic view where definitions are given by a collection of central/prototypical
and peripheral conditions, only the peripheral ones can be contradicted by &amp;quot;but.&amp;quot; In
either formalization we get BUT_Cl as a consequence: Since &amp;quot;laws&amp;quot; cannot be deleted,
BUT can&apos;t be applied, and hence its use in those kinds of sentences would be incorrect.
W. Labov (1973) discussed sentences of the form
*This is a chair but you can sit on it.
The sentence is incorrect, since the function &amp;quot;one can sit on it&amp;quot; belongs to the core of
the concept &amp;quot;chair&amp;quot;; so—contrary to the role of &amp;quot;but&amp;quot;—the sentence does not contain
any surprising new elements. Using the Metarule (BUT) and the cooperative principle
of Grice, we get
BUT_C2: 4) but IF is incorrect, if 4) ---4 kIf is a &amp;quot;law.&amp;quot;
The Metarule (BUT) gives the semantics of &amp;quot;but;&amp;quot; the rules BUT_Cl and BUT_C2 follow
from it (after formalization in a sufficiently strong metalanguage such as type theory
or set theory). We can link all of them to procedures for constructing and evaluating
models of text. Are they sufficient? Certainly not. We have not dealt with the other
usages of &amp;quot;but;&amp;quot; neither have we shown how to deal with the apparent asymmetry
of conclusions: cheap but elegant seems to imply &amp;quot;worth buying,&amp;quot; but elegant but cheap
doesn&apos;t; we have ignored possible prototypical effects in our semantics. However, we
do believe that other rules, dealing with &amp;quot;but&amp;quot; or with other connectives, can be conve-
niently expressed in our framework. (The main idea is that one should talk explicitly
and formally about relations between text and background knowledge, and that this
knowledge is more than just a list of facts—it has structure, and it is ambiguous.)
Furthermore, the semantics of &amp;quot;but&amp;quot; as described above is computationally tractable.
We also believe that one could not present a similarly natural account of the se-
mantics of &amp;quot;but&amp;quot; in traditional logics, because classical logics withstand contradictions
with great difficulty. Contradiction, however, is precisely what &amp;quot;but&amp;quot; expresses. No-
tice that certain types of coordinating conjunctions often have their counterparts in
classical logic: copulative (and, or, neither-nor, besides, sometimes etc.), disjunctive (like
either-or), illative (hence, for that reason). Others, such as explanatory (namely or viz.)
or causal (for) conjunctions can probably be expressed somehow, for better or worse,
within a classical framework. Thus the class of adversative conjunctions (but, still, and
yet, nevertheless) is in that sense unique.
</bodyText>
<sectionHeader confidence="0.945841" genericHeader="conclusions">
7. Conclusions
</sectionHeader>
<bodyText confidence="0.996581">
We hope that the reader has found this paper coherent, and its topic—the correspon-
dence between paragraphs and models—interesting. Our strategy was to divide the
</bodyText>
<page confidence="0.99096">
205
</page>
<note confidence="0.552027">
Computational Linguistics Volume 17, Number 2
</note>
<bodyText confidence="0.998132315789474">
subject into three subtopics: a theory of anaphora, which corresponds to the logical
theory of equality in p-models; a theory of background knowledge, expressed as the
logical theory of reference in the three-level semantics; and principles of communication
encoded in metarules. These principles include Gricean maxims and the semantics of
cohesion, and specify a model theory for the three-level semantics. The framework re-
sulting from putting these theories together is computational, empirical, and verifiable
(even if incomplete); furthermore, it has strong links to already existing natural lan-
guage processing systems. In particular, the new logical level—the referential level—is
exemplified by on-line dictionaries and other reference works, from which we extract
background information about defaults and plausibility rankings.
We also hope that the reader would be inclined to share our belief that natural
language text can be properly and usefully analyzed by means of a three-level seman-
tics that includes an object level, a metalevel, and a referential level. We believe that
the coherence of an essay, a paper, or a book can be described by an extension of our
theory. The work of van Dijk and Kintch (1983) on &amp;quot;macrostructures&amp;quot; could probably
form the basis for such an expansion. Similarly, much of the abovementioned work
by Hobbs, Webber, Grosz, and Sidner can be incorporated into this framework.
Our main intention was to demonstrate that formal notions of background knowl-
edge can be used to
</bodyText>
<listItem confidence="0.986717857142857">
• define coherence, make it semantically distinct from mere consistency,
and link it formally with the notion of a topic;
• define a class of p-models—logical models of paragraphs;
• provide a semantics for &amp;quot;but&amp;quot; (which exemplifies our understanding of
grammatical cohesion);
• express the Gricean maxims formally, and use them in a computational
model of communication (which seems to contradict Allen 1988, p. 464).
</listItem>
<bodyText confidence="0.999959636363636">
Moreover, we tried to convince the reader that paragraph is an important linguistic
unit, not only because of its pragmatic importance exemplified by coherence and links
to background knowledge, but also because of its role in assignment of syntactic
structures (viz, ellipsis) and in semantics (viz, its possible role in evaluating semantic
representations).
A great many issues have been omitted from our analysis. Thus, although we are
aware that anaphora resolution and consistency depend on previously processed text,
the problem of connecting a paragraph to such text has been conveniently ignored.
Notice that this doesn&apos;t make our thesis about paragraphs being units of semantic
processing any weaker, we have not claimed that paragraphs are independent. The
questions of how to translate from natural language to a logical notation needs a lot
of attention; we have merely assumed that this can be done. Continuing this list, we
have accepted a very classical theory of meaning, given by Tarski: the truth is what
is satisfied in a model. This theory should be refined, for instance by formalizing
Lakoff&apos;s (1987) concept of radial categories, and proposing mechanisms for exploiting
it. By the same token, the concept of reference has to be broadened to include iconic
(e.g., visual and tactile) information. And certainly it would be nice to have a more
detailed theory describing the role of the metalevel. In particular, we can imagine
that the simple structure of a collection of set theoretic formulas can be replaced by
something more interesting. We leave this as another open problem.
We have shown that it is possible to develop a formal system with an explicit
relationship between background knowledge and text, showing mechanisms that take
</bodyText>
<page confidence="0.994964">
206
</page>
<note confidence="0.924477">
Zadrozny and Jensen Semantics of Paragraphs
</note>
<bodyText confidence="0.999685272727273">
advantage of preference, coherence, and contradiction (the reality of these phenomena
has never been disputed, but their semantic functions had not been investigated). We
should also mention that we have also begun some work on actually checking the
empirical validity of this model (cf. e.g. Braden-Harder and Zadrozny 1990), using on-
line dictionaries (LDOCE and Webster&apos;s) as the referential level. We know, of course,
that existing dictionaries are very imperfect, but (1) they can be accessed and used
by computer programs; (2) they are getting better, as they are very systematically
created with help of computers (see Sinclair 1987 for an account of how COBUILD
was constructed); (3) obviously, we can imagine useful new ones, like a Dictionary of
Pragmatics; (4) we believe that we can explore the new inference mechanisms even in
such unrefined environments.
Although the scheme we have proposed certainly needs further refinement, we
believe that it is correct in two of its most important aspects: first, in the separation
of current paragraph analysis (the object theory) from background information (the
referential level); and second, in asserting that the function of a paragraph is to allow
the building of a model of the situation described in the paragraph. This model can be
stored, maybe modified, and subsequently used as a reference for processing following
paragraphs.
Finally, the paper can be viewed as an argument that the meaning of a sentence
cannot be defined outside of a context, just as the truth value of a formula cannot be
computed in a vacuum. A paragraph is the smallest example of such a context—it is
&amp;quot;a unit of thought.&amp;quot;
</bodyText>
<sectionHeader confidence="0.985379" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999637571428572">
We gratefully acknowledge the help
provided by our anonymous but diligent
referees, by Graeme Hirst and Susan
McRoy, and by our many colleagues at IBM
Research, all for whom helped us to avoid
worst consequences of Murphy&apos;s Law
during the writing of this paper.
</bodyText>
<sectionHeader confidence="0.992125" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999678927272727">
Allen, J. (1987). Natural Language
Understanding. Benjamin/Cummings.
Menlo Park, California.
Berwick, R.C. (1986). &amp;quot;Learning from
Positive-Only Examples: The Subset
Principle and Three Case Studies.&amp;quot; In
Michalski, R.S. et al., eds. Machine
Learning Vol. II. Morgan Kaufmann. Los
Altos, California: 625-645.
Black, J.B., and Bower, G.H. (1979).
&amp;quot;Episodes as Chunks in Narrative
Memory.&amp;quot; Journal of Verbal Learning and
Verbal Behavior 18: 309-318.
Bond, S.J., and Hayes, J.R. (1983). Cues
People Use to Paragraph Text. manuscript.
Dept. of Psychology, Carnegie-Mellon
University.
Brachman, R.J., Fikes, R.E., and Levesque,
H.J. (1985). &amp;quot;Krypton: A Functional
Approach to Knowledge Representation.&amp;quot;
In Brachman, R.J., and Levesque, H.J.,
eds. Readings in Knowledge Representation.
Morgan Kaufmann. Los Altos, California:
411-430.
Braden-Harder, L., and Zadrozny, W. (1989).
Lexicons for Broad Coverage Semantics. IBM
Research Division Report, RC 15568.
Chafe, W.L. (1979). &amp;quot;The Flow of Thought
and the Flow of Language.&amp;quot; In Givon, T.,
ed., Syntax and Semantics, Vol. 12.
Academic Press. New York, New York.
Charniak, E. (1983). &amp;quot;Passing Markers: A
Theory of Contextual Influence in
Language Comprehension.&amp;quot; Cognitive
Science, 7(3): 171-190.
Charniak, E., and McDermott, D. (1985).
Introduction to Artificial Intelligence.
Addison-Wesley. Reading, Massachusetts.
Crothers, E.J. (1979). Paragraph Structure
Inference. Ablex Publishing Corp.,
Norwood, New Jersey.
Etherington, D.W., and Mercer, R.E. (1987).
&amp;quot;Domain Circumscription: A
Reevaluation.&amp;quot; Computational Intelligence
3: 94-99.
Frisch, A. (1987). &amp;quot;Inference without
Chaining.&amp;quot; Proc. IJCAI-87. Milan, Italy:
515-519.
Fowler, H.W. (1965). A Dictionary of Modern
English Usage. Oxford University Press.
New York, New York.
Genesereth, M.R., and Nilsson, N.J. (1987).
Logical Foundations of Artificial Intelligence.
Morgan Kaufmann. Los Altos, California.
Grice, H.P. (1978). &amp;quot;Further Notes on Logic
</reference>
<page confidence="0.968759">
207
</page>
<reference confidence="0.999727162601626">
Computational Linguistics Volume 17, Number 2
and Conversation.&amp;quot; In Cole, P., ed. Syntax
and Semantics 9: Pragmatics. Academic
Press. New York, New York: 113-128.
Grice, H.P. (1975). &amp;quot;Logic and
Conversation.&amp;quot; In Cole, R, and Morgan,
J.L., eds., Syntax and Semantics 3: Speech
Acts. Academic Press. New York, New
York: 41-58.
Groesser, A.C. (1981). Prose Comprehension
Beyond the Word. Springer. New York,
New York.
Grosz, B.J. (1978). DISCOURSE
KNOWLEDGE-Section 4 of Walker, D.E.,
ed., Understanding Spoken Language.
North-Holland. New York, New York:
229-344.
Grosz, B.J. (1977). &amp;quot;The Representation and
Use of Focus in a System for
Understanding Dialogs.&amp;quot; Proc. IJCAI-77.
W. Kaufmann. Los Altos, California:
67-76.
Haberlandt, K., Berian, C., and Sandson, J.
(1980). &amp;quot;The Episode Schema in Story
Processing.&amp;quot; Journal of Verbal Learning and
Verbal Behavior 19: 635-650.
Halliday, M.A.K., and Hasan, R. (1976).
Cohesion in English. Longman Group Ltd.
London.
Haugeland, J. (1985). Artificial Intelligence:
The Very Idea. MIT Press. Cambridge,
Massachusetts.
Hinds, J. (1979). &amp;quot;Organizational Patterns in
Discourse.&amp;quot; In Givon, T., ed., Syntax and
Semantics, Vol. 12. Academic Press. New
York, New York.
Hintikka, J. (1985). The Game of Language.
D. Reidel. Dordrecht.
Hirst, G. (1987). Semantic Interpretation and
the Resolution of Ambiguity. Cambridge
University Press. Cambridge.
Hobbs, J.R. (1982). &amp;quot;Towards an
Understanding of Coherence in
Discourse.&amp;quot; In Lehnert, W.G., and Ringle,
M.H., eds. Strategies for Natural Language
Processing. Lawrence Erlbaum. Hillsdale,
New Jersey: 223-244.
Hobbs, J.R. (1979). &amp;quot;Coherence and
Coreference.&amp;quot; Cognitive Science 3: 67-90.
Hobbs, J.R. (1978). &amp;quot;Resolving Pronoun
References.&amp;quot; Lingua 44: 311-338.
Hobbs, J.R. (1977). 38 Examples of Elusive
Antecedents from Published Texts.
Research Report 77-2. Department of
Computer Science, City College, CUNY,
New York.
Hobbs, J.R. (1976). Pronoun Resolution.
Research Report 76-1. Dept. of Computer
Science. City College, CUNY, New York.
Hobbs, J., Stickel, M., Martin, P., and
Edwards, D. (1988). &amp;quot;Interpretation as
Abduction.&amp;quot; In Proc. of 26th Annual
Meeting of the Association for Computational
Linguistics, ACL: 95-103.
Jackendoff, R. (1983). Semantics and
Cognition. The MIT Press. Cambridge,
Massachusetts.
Jensen, K. (1988). &amp;quot;Issues in Parsing.&amp;quot; In
A. Blaser, ed., Natural Language at the
Computer. Springer-Verlag, Berlin,
Germany: 65-83.
Jensen, K. (1986). Parsing Strategies in a
Broad-Coverage Grammar of English.
Research Report RC 12147. IBM
T.J. Watson Research Center, Yorktown
Heights, New York.
Jensen, K., and Binot, J.-L. (1988).
&amp;quot;Disambiguating Prepositional Phrase
Attachments by Using On-line Dictionary
Definitions.&amp;quot; Computational Linguistics
13.3-4.251-260 (special issue on the
lexicon).
Johnson-Laird, P.N. (1983). Mental Models.
Harvard University Press. Cambridge,
Massachusetts.
Kamp, H. (1981). &amp;quot;A Theory of Truth and
Semantic Representation.&amp;quot; In Groenendijk,
J.A.G., et al. eds., Formal Methods in the
Study of Language, I. Mathematisch
Centrum, Amsterdam: 277-322,
Labov, W. (1973). &amp;quot;The Boundaries of Words
and Their Meanings.&amp;quot; In Fishman, J., New
Ways of Analyzing Variation in English.
Georgetown U. Press. Washington, D.C.:
340-373.
Lakoff, G. (1987). Women, Fire and Dangerous
Things. The University of Chicago Press.
Chicago, Illinois.
Levesque, H.J. (1984). &amp;quot;A Logic of Implicit
and Explicit Beliefs.&amp;quot; Proc. AAAI-84.
AAAI: 198-202.
Longacre, R.E. (1979). The Paragraph as a
Grammatical Unit. In Givon, T., ed.,
Syntax and Semantics, Vol. 12. Academic
Press. New York, New York.
Longman Dictionary of Contemporary English.
(1978). Longman Group Ltd., London.
Lyons, J. (1969). Introduction to Theoretical
Linguistics. Cambridge University Press.
Cambridge, England.
Mann, W.C., and Thompson, S.A. (1983).
Relational Propositions in Discourse.
Information Sciences Institute Research
Report 83-115.
Moens, M., and Steedman, M. (1987).
&amp;quot;Temporal Ontology in Natural
Language.&amp;quot; Proc. 25th Annual Meeting of
the ACL. Stanford, California: 1-7.
Montague, R. (1970). &amp;quot;Universal Grammar.&amp;quot;
Theoria 36: 373-398.
Mycielski, J. (1981). &amp;quot;Analysis without
Actual Infinity.&amp;quot; Journal of Symbolic Logic,
46(3): 625-633.
</reference>
<page confidence="0.976013">
208
</page>
<reference confidence="0.99855947826087">
Zadrozny and Jensen Semantics of Paragraphs
Patel-Schneider, P.S. (1985). &amp;quot;A Decidable
First Order Logic for Knowledge
Representation.&amp;quot; Proc. IJCAI-85. AAAI:
455-458.
Perlis, D. (1985). &amp;quot;Languages with Self
Reference I: Foundations.&amp;quot; Artificial
Intelligence 25: 301-322.
Poole, D. (1988). &amp;quot;A Logical Framework for
Default Reasoning.&amp;quot; Artificial Intelligence
36(1): 27-47.
Quirk, R., Greenbaum, S., Leech, G., and
Svartvik, J. (1972). A Grammar of
Contemporary English. Longman Group
Ltd. London.
Reggia, J.A. (1985). &amp;quot;Abductive Inference.&amp;quot;
In Karma, K.N., ed., Expert Systems in
Government Symposium. IEEE: 484-489.
Reiter, R. (1987). &amp;quot;A Theory of Diagnosis
from First Principles.&amp;quot; Artificial
Intelligence, 32(1): 57-95.
Sidner, C. (1983). &amp;quot;Focusing in the
Comprehension of Definite Anaphora.&amp;quot; In
Brady, M., and Berwick, R., eds.,
Computational Models of Discourse, MIT
Press. Cambridge, Massachusetts:
330-367.
Sells, P. (1985). Lectures on Contemporary
Syntactic Theories. CSLI lecture notes;
no. 3.
Shoenfield, J.R. (1987). Mathematical Logic.
Addison-Wesley. New York, New York.
Sinclair, J.M., ed. (1987). Looking Up. An
account of the COBUILD project. Collins
ELT, London.
Small, S.L., Cottrell, G.W., and Tanenhaus,
M.K., eds. (1988). Lexical Ambiguity
Resolution. Morgan Kaufmann. San Mateo,
California.
Turner, M. (1987). Death is the Mother of
Beauty. The University of Chicago Press.
Chicago, Illinois.
van Dijk, T.A., and Kintch, W. (1983).
Strategies of Discourse Comprehension.
Academic Press. Orlando, Florida.
Warriner, J.E. (1963). English Grammar and
Composition. Harcourt, Brace &amp; World,
Inc., New York, New York.
Webber, B. (1983). &amp;quot;So What Can We Talk
About Now?&amp;quot; In Brady, M., and Berwick,
R., eds., Computational Models of Discourse.
MIT Press. Cambridge, Massachusetts:
147-154.
Webber, B. (1987). &amp;quot;The Interpretation of
Tense in Discourse.&amp;quot; Proc, 25th Annual
Meeting of the ACL, ACL: 147-154.
Webster&apos;s Seventh New Collegiate Dictionary.
(1963). Merriam-Webster, Inc. Springfield,
Massachusetts.
Woods, W. (1987). &amp;quot;Don&apos;t Blame the Tool.&amp;quot;
Computational Intelligence 3(3): 228-237.
Zadrozny, W. (1987a). &amp;quot;Intended Models,
Circumscription and Commonsense
Reasoning.&amp;quot; Proc. IJCAI-87: 909-916.
Zadrozny, W. (1987b). &amp;quot;A Theory of Default
Reasoning.&amp;quot; Proc. AAAI-87. Seattle,
Washington: 385-390.
Zadrozny, W. (unpublished). The Logic of
Abduction.
</reference>
<page confidence="0.998951">
209
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.421689">
<title confidence="0.769525">Semantics of Paragraphs Wlodek Zadrozny*</title>
<author confidence="0.999745">Karen Jensen</author>
<affiliation confidence="0.992109">IBM T. J. Watson Research Center</affiliation>
<abstract confidence="0.9780868">present a computational theory of the paragraph. Within it we formally define give semantics to the adversative conjunction &amp;quot;but&amp;quot; and to the Gricean maxim of quantity, and present some new methods for anaphora resolution. The theory precisely characterizes the relationship between the content of the paragraph and background knowledge needed for its understanding. This is achieved by introducing a type of logical theory consisting of an level, to the content of the a level, is a new logical level encoding background knowledge, a constraints on models of discourse (e.g. a formal version of Gricean maxims). We propose also specific mechanisms of interaction between these levels, resembling both classical provability and abduction. Paragraphs are then represented by a class of structures</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allen</author>
</authors>
<title>Natural Language Understanding.</title>
<date>1987</date>
<location>Benjamin/Cummings. Menlo Park, California.</location>
<marker>Allen, 1987</marker>
<rawString>Allen, J. (1987). Natural Language Understanding. Benjamin/Cummings. Menlo Park, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Berwick</author>
</authors>
<title>Learning from Positive-Only Examples: The Subset Principle and Three Case Studies.&amp;quot;</title>
<date>1986</date>
<booktitle>Machine Learning Vol. II.</booktitle>
<pages>625--645</pages>
<editor>In Michalski, R.S. et al., eds.</editor>
<publisher>Morgan Kaufmann.</publisher>
<location>Los Altos, California:</location>
<contexts>
<context position="83347" citStr="Berwick (1986)" startWordPosition="13752" endWordPosition="13753">(Halliday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &amp;quot;unique-name assumption&amp;quot; (Genesereth and Nilsson 1987), &amp;quot;domain closure assumption&amp;quot; (ibid.), &amp;quot;domain circumscription&amp;quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &amp;quot;abductive inference&amp;quot; of Reggia (1985), the &amp;quot;diagnosis from first principles&amp;quot; of Reiter (1987), &amp;quot;explainability&amp;quot; of Poole (1988), and the subset principle of Berwick (1986). But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming). 5.2 p-Models The construction of a model of a paragraph, a p-model, must be based on the information contained in the paragraph itself (the object theory) and in the referential level, while the metalevel restricts ways that the model can be constructed, or, in other words, provides criteria for choosing a most pl</context>
</contexts>
<marker>Berwick, 1986</marker>
<rawString>Berwick, R.C. (1986). &amp;quot;Learning from Positive-Only Examples: The Subset Principle and Three Case Studies.&amp;quot; In Michalski, R.S. et al., eds. Machine Learning Vol. II. Morgan Kaufmann. Los Altos, California: 625-645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Black</author>
<author>G H Bower</author>
</authors>
<title>Episodes as Chunks in Narrative Memory.&amp;quot;</title>
<date>1979</date>
<journal>Journal of Verbal Learning and Verbal Behavior</journal>
<volume>18</volume>
<pages>309--318</pages>
<contexts>
<context position="9999" citStr="Black and Bower (1979)" startWordPosition="1569" endWordPosition="1572">ple of psycholinguistically oriented research work can be found in Bond and Hayes (1983). These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information. Other psycholing-uistic studies that confirm the validity of paragraph units can be found in Black and Bower (1979) and Haberlandt et al. (1980). The textualist approach to paragraph analysis is exemplified by E. J. Crothers. His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs. He lists, classifies, and discusses various types of inference, by which he means, generally, &amp;quot;the linguistic-logical notions of consequent and presupposition&amp;quot; Crothers (1979:112) have collected convincing evidence of the existence of language chunks—real structures, not just orthographic conventions—that are smaller than a discourse, larger than a sentence, generally composed of sentences, and rec</context>
</contexts>
<marker>Black, Bower, 1979</marker>
<rawString>Black, J.B., and Bower, G.H. (1979). &amp;quot;Episodes as Chunks in Narrative Memory.&amp;quot; Journal of Verbal Learning and Verbal Behavior 18: 309-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Bond</author>
<author>J R Hayes</author>
</authors>
<title>Cues People Use to Paragraph Text.</title>
<date>1983</date>
<tech>manuscript.</tech>
<institution>Dept. of Psychology, Carnegie-Mellon University.</institution>
<contexts>
<context position="9465" citStr="Bond and Hayes (1983)" startWordPosition="1488" endWordPosition="1491">fy four different linguistic approaches to paragraphs: prescriptivist, psycholinguist, textualist, and discourseoriented. The prescriptivist approach is typified in standard English grammar textbooks, such as Warriner (1963). In these sources, a paragraph is notionally defined as something like a series of sentences that develop one single topic, and rules are laid down for the construction of an ideal (or at least an acceptable) paragraph. Although these dictates are fairly clear, the underlying notion of topic is not. An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983). These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information. Other psycholing-uistic studies that confirm the validity of paragraph units can be found in Black and Bower (1979) and Haberlandt et al. (1980). The textualist approach to paragrap</context>
<context position="61205" citStr="Bond and Hayes (1983)" startWordPosition="10070" endWordPosition="10073">mutations we talk about would produce an inconsistent theory; hence, the temporal, causal, and other aspects would be dealt with by consistency. But of course at this point it is just a hypothesis. An important aspect of the definition is that coherence has been defined as a property of representation—in our case, it is a property of a formal theory. The existence of the topic, the direct or indirect allusion to it, and anaphora (which will be addressed below) take up the issue of formal criteria for a paragraph definition, which was raised 189 Computational Linguistics Volume 17, Number 2 by Bond and Hayes (1983) (cf. also Section 2.1). The question of paragraph length can probably be attended to by limiting the size of p-models, perhaps after introducing some kind of metric on logical data structures. Still, our definition of coherence may not be restrictive enough: two collections of sentences, one referring to &amp;quot;black&amp;quot; (about black pencils, black pullovers, and black poodles), the other one about &amp;quot;death&amp;quot; (war, cancer, etc.), connected by a sentence referring to both of these, could be interpreted as one paragraph about the new, broader topic &amp;quot;black + death.&amp;quot; This problem may be similar to the situat</context>
</contexts>
<marker>Bond, Hayes, 1983</marker>
<rawString>Bond, S.J., and Hayes, J.R. (1983). Cues People Use to Paragraph Text. manuscript. Dept. of Psychology, Carnegie-Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Brachman</author>
<author>R E Fikes</author>
<author>H J Levesque</author>
</authors>
<title>Krypton: A Functional Approach to Knowledge Representation.&amp;quot;</title>
<date>1985</date>
<booktitle>Readings in Knowledge Representation.</booktitle>
<editor>In Brachman, R.J., and Levesque, H.J., eds.</editor>
<contexts>
<context position="52088" citStr="Brachman et al. 1985" startWordPosition="8526" endWordPosition="8529">en if all background knowledge were described, as in our examples, by sets of first order theories, because of the preferences and inconsistencies of meanings, we could not treat R as a flat database of facts—such a model simply would not be realistic. Rather, R must be treated as a separate logical level for these syntactic reasons, and because of its function—being a pool of possibly conflicting semantic constraints. The last point may be seen better if we look at some differences between our system and KRYPTON, which also distinguishes between an object theory and background knowledge (cf. Brachman et al. 1985). KRYPTON&apos;s A-box, encoding the object theory as a set of assertions, uses standard first order logic; the T-box contains information expressed in a frame-based language equivalent to a fragment of FOL. However, the distinction between the two parts is purely functional—that is, characterized in terms of the system&apos;s behavior. From the logical point of view, the knowledge base is the union of the two boxes, i.e. a theory, and the entailment is standard. In our system, we also distinguish between the &amp;quot;definitional&amp;quot; and factual information, but the &amp;quot;definitional&amp;quot; part contains collections of mut</context>
</contexts>
<marker>Brachman, Fikes, Levesque, 1985</marker>
<rawString>Brachman, R.J., Fikes, R.E., and Levesque, H.J. (1985). &amp;quot;Krypton: A Functional Approach to Knowledge Representation.&amp;quot; In Brachman, R.J., and Levesque, H.J., eds. Readings in Knowledge Representation.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Morgan Kaufmann</author>
</authors>
<pages>411--430</pages>
<location>Los Altos, California:</location>
<marker>Kaufmann, </marker>
<rawString>Morgan Kaufmann. Los Altos, California: 411-430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Braden-Harder</author>
<author>W Zadrozny</author>
</authors>
<title>Lexicons for Broad Coverage Semantics.</title>
<date>1989</date>
<journal>IBM Research Division Report, RC</journal>
<pages>15568</pages>
<marker>Braden-Harder, Zadrozny, 1989</marker>
<rawString>Braden-Harder, L., and Zadrozny, W. (1989). Lexicons for Broad Coverage Semantics. IBM Research Division Report, RC 15568.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W L Chafe</author>
</authors>
<title>The Flow of Thought and the Flow of Language.&amp;quot;</title>
<date>1979</date>
<booktitle>Syntax and Semantics,</booktitle>
<volume>12</volume>
<editor>In Givon, T., ed.,</editor>
<publisher>Academic Press.</publisher>
<location>New York, New York.</location>
<contexts>
<context position="11630" citStr="Chafe 1979" startWordPosition="1806" endWordPosition="1807"> types are procedural (how-to), expository (essay), and narrative (in this case, spontaneous conversation). For each type, 173 Computational Linguistics Volume 17, Number 2 its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined). Segments themselves are composed of clauses and regulated by &amp;quot;switching&amp;quot; patterns, such as the question-answer pattern and the remark-reply pattern. 2.2 Our View of Paragraphs: An Informal Sketch Although there are other discussions of the paragraph as a central element of discourse (e.g. Chafe 1979, Halliday and Hasan 1976, Longacre 1979, Haberlandt et al. 1980), all of them share a certain limitation in their formal techniques for analyzing paragraph structure. Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area. We suggest that the paragraph is a grammatical and logical unit. It is the smallest linguistic representation of what, in logic, is called a &amp;quot;model,&amp;quot; and it is the first reasonable domain of anaphora resolution, and</context>
</contexts>
<marker>Chafe, 1979</marker>
<rawString>Chafe, W.L. (1979). &amp;quot;The Flow of Thought and the Flow of Language.&amp;quot; In Givon, T., ed., Syntax and Semantics, Vol. 12. Academic Press. New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Passing Markers: A Theory of Contextual Influence in Language Comprehension.&amp;quot;</title>
<date>1983</date>
<journal>Cognitive Science,</journal>
<volume>7</volume>
<issue>3</issue>
<pages>171--190</pages>
<contexts>
<context position="53209" citStr="Charniak 1983" startWordPosition="8701" endWordPosition="8702">efinitional&amp;quot; and factual information, but the &amp;quot;definitional&amp;quot; part contains collections of mutually excluding theories, not just of formulas describing a semantic network. Moreover, in addition to proposing this structure of R, we have described the two mechanisms for exploiting it, &amp;quot;coherence&amp;quot; and &amp;quot;dominance,&amp;quot; which are not variants of the standard first order entailment, but abduction. The idea of using preferences among theories is new, hence it was described in more detail. &amp;quot;Coherence,&amp;quot; as outlined above, can be understood as a declarative (or static) version of marker passing (Hirst 1987; Charniak 1983), with one difference: the activation spreads to theories that share a predicate, not through the IS-A hierarchy, and is limited to elementary facts about predicates appearing in the text. The metalevel rules we are going to discuss in Section 6, and that deal with the Gricean maxims and the meaning of &amp;quot;but,&amp;quot; can be easily expressed in the languages of set theory or higher order logic, but not everything expressible in those languages makes sense in natural language. Hence, putting limitations on the expressive power of the language of the metalevel will remain as one of many open problems. 4.</context>
</contexts>
<marker>Charniak, 1983</marker>
<rawString>Charniak, E. (1983). &amp;quot;Passing Markers: A Theory of Contextual Influence in Language Comprehension.&amp;quot; Cognitive Science, 7(3): 171-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>D McDermott</author>
</authors>
<title>Introduction to Artificial Intelligence.</title>
<date>1985</date>
<publisher>Addison-Wesley.</publisher>
<location>Reading, Massachusetts.</location>
<contexts>
<context position="76124" citStr="Charniak and McDermott (1985)" startWordPosition="12510" endWordPosition="12513">abduction. This relation would hold, for instance, between the object theory of our example paragraph and a formula expressing the equality of two constants, i and i&apos;, denoting (respectively) the &amp;quot;infection&amp;quot; in the sentence Within twentyfour hours of infection..., and the &amp;quot;infection&amp;quot; of the theory (d1)—a disease is an illness caused by an infection. This equality i = i&apos; cannot be proven, but it may be reasonably assumed—we know that in this case the infection i&apos; caused the illness, which, in turn, caused the death. The necessity of this kind of merging of arguments has been recognized before: Charniak and McDermott (1985) call it abductive unification/matching, Hobbs (1978, 1979) refers to such operations using the terms knitting or petty conversational implicature. Neither Hobbs nor Charniak and McDermott tried then to make this notion precise, but the paper by Hobbs et al. (1988) moves in that direction. The purpose of this subsection is to formalize and explain how assumptions like that one above can be made. Definition A formula 0 is weakly provable from an object theory T, expressed as T HR 0, iff there exists a partial theory T E PT(T) such that T 0, i.e. T proves 0 in logic. (We call HR &amp;quot;weak&amp;quot; because i</context>
</contexts>
<marker>Charniak, McDermott, 1985</marker>
<rawString>Charniak, E., and McDermott, D. (1985). Introduction to Artificial Intelligence. Addison-Wesley. Reading, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Crothers</author>
</authors>
<title>Paragraph Structure Inference.</title>
<date>1979</date>
<publisher>Ablex Publishing Corp.,</publisher>
<location>Norwood, New Jersey.</location>
<contexts>
<context position="10372" citStr="Crothers (1979" startWordPosition="1624" endWordPosition="1625">tives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information. Other psycholing-uistic studies that confirm the validity of paragraph units can be found in Black and Bower (1979) and Haberlandt et al. (1980). The textualist approach to paragraph analysis is exemplified by E. J. Crothers. His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs. He lists, classifies, and discusses various types of inference, by which he means, generally, &amp;quot;the linguistic-logical notions of consequent and presupposition&amp;quot; Crothers (1979:112) have collected convincing evidence of the existence of language chunks—real structures, not just orthographic conventions—that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sentences). These chunks are sometimes called &amp;quot;episodes,&amp;quot; and sometimes &amp;quot;paragraphs.&amp;quot; According to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases. Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, </context>
<context position="20013" citStr="Crothers (1979" startWordPosition="3150" endWordPosition="3151">r to some hand-coded body of predicate assertions, for making these relationships. This demonstrates that information needed to identify and resolve anaphoric references can be found, to an interesting extent, in standard dictionaries and thesauri. (Other reference works could be treated as additional sources of world knowledge.) This type of consultation uses existing natural language texts as a referential level for processing purposes. It is the lack of exactly this notion of referential level that has stood in the way of other linguists who have been interested in the paragraph as a unit. Crothers (1979, p. 112), for example, bemoans the fact that his &amp;quot;theory lacks a world knowledge component, a mental &apos;encyclopedia,&apos; which could be invoked to generate inferences... &amp;quot;. With respect to that independent source of knowledge, our main contributions are two. First, we identify its possible structure (a collection of partially ordered theories) and make formal the choice of a most plausible interpretation. In other words, we recognize it as a separate logical level—the referential level. Second, we suggest that natural language reference works, like dictionaries and thesauri, can quite often fill </context>
<context position="56122" citStr="Crothers (1979)" startWordPosition="9176" endWordPosition="9177">eal time. Secondly, the cooperative principle of Grice (1975, 1978), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader. Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph. Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions. Let P be a paragraph, let .15 = (s1,... , S) be its translation into a sequence of logical formulas. The set of all predicates appearing in X will be denoted by Pred(X). Definition Let T be a partial theory of a paragraph P. A sequence of predicates appearing in 15, denoted by Tp, is called a </context>
</contexts>
<marker>Crothers, 1979</marker>
<rawString>Crothers, E.J. (1979). Paragraph Structure Inference. Ablex Publishing Corp., Norwood, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D W Etherington</author>
<author>R E Mercer</author>
</authors>
<title>Domain Circumscription: A Reevaluation.&amp;quot;</title>
<date>1987</date>
<journal>Computational Intelligence</journal>
<volume>3</volume>
<pages>94--99</pages>
<contexts>
<context position="83087" citStr="Etherington and Mercer 1987" startWordPosition="13710" endWordPosition="13713">his means that the &amp;quot;it&amp;quot; that brought the disease in P1 will not be considered to refer to the infection &amp;quot;i&amp;quot; or the death &amp;quot;d&amp;quot; in P3. This strategy is certainly the right one to start out with, since anaphora is always the more typical direction of reference in English prose (Halliday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &amp;quot;unique-name assumption&amp;quot; (Genesereth and Nilsson 1987), &amp;quot;domain closure assumption&amp;quot; (ibid.), &amp;quot;domain circumscription&amp;quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &amp;quot;abductive inference&amp;quot; of Reggia (1985), the &amp;quot;diagnosis from first principles&amp;quot; of Reiter (1987), &amp;quot;explainability&amp;quot; of Poole (1988), and the subset principle of Berwick (1986). But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming). 5.2 p-Models The construction of a model of a paragraph</context>
</contexts>
<marker>Etherington, Mercer, 1987</marker>
<rawString>Etherington, D.W., and Mercer, R.E. (1987). &amp;quot;Domain Circumscription: A Reevaluation.&amp;quot; Computational Intelligence 3: 94-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Frisch</author>
</authors>
<title>Inference without Chaining.&amp;quot;</title>
<date>1987</date>
<booktitle>Proc. IJCAI-87.</booktitle>
<pages>515--519</pages>
<location>Milan, Italy:</location>
<contexts>
<context position="30524" citStr="Frisch 1987" startWordPosition="4888" endWordPosition="4889">ned on a certain domain, which satisfies all formulas of T. The collection of all (finite) models of a theory T will be denoted by Mods(T). • The set of all subformulas of a collection of formulas F is denoted by Form(F). 0 is a ground instance of a formula 0, if 0 contains no variables, and 0 = 00, for some substitution O. Thus, we do not require Th(T) to be closed under substitution instances of tautologies. Although in this paper we take modus ponens as the main rule of inference, in general one can consider deductive closures with respect to weaker, nonstandard logics, (cf. Levesque 1984; Frisch 1987; Patel-Schneider 1985). But we won&apos;t pursue this topic further here. 3.2 The Structure of Background Knowledge Background knowledge is not a simple list of meaning postulates—it has a structure and it may contain contradictions and ambiguities. These actualities have to be taken into account in any realistic model of natural language understanding. For instance, the verb &amp;quot;enter&amp;quot; is polysemous. But, unless context specifies otherwise, &amp;quot;to come in&amp;quot; is a more plausible meaning than &amp;quot;to join a group.&amp;quot; Assuming some logical representation of this knowledge, we can write that enter(x, y) –4 {come _</context>
</contexts>
<marker>Frisch, 1987</marker>
<rawString>Frisch, A. (1987). &amp;quot;Inference without Chaining.&amp;quot; Proc. IJCAI-87. Milan, Italy: 515-519.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H W Fowler</author>
</authors>
<title>A Dictionary of Modern English Usage.</title>
<date>1965</date>
<publisher>Oxford University Press.</publisher>
<location>New York, New York.</location>
<contexts>
<context position="62063" citStr="Fowler (1965" startWordPosition="10205" endWordPosition="10206">ctive enough: two collections of sentences, one referring to &amp;quot;black&amp;quot; (about black pencils, black pullovers, and black poodles), the other one about &amp;quot;death&amp;quot; (war, cancer, etc.), connected by a sentence referring to both of these, could be interpreted as one paragraph about the new, broader topic &amp;quot;black + death.&amp;quot; This problem may be similar to the situation in which current formal grammars allow nonsensical but parsable collections of words (e.g., &amp;quot;colorless green ideas... &amp;quot;), while before the advent of Chomskyan formalisms, a sentence was defined as the smallest meaningful collection of words; Fowler (1965, p. 546) gives 10 definitions of a sentence. It then seems worth differentiating between the creation of a new concept like &amp;quot;black + death,&amp;quot; with a meaning given by a paraphrase of the example collection of sentences, and the acceptance of the new concept—storing it in R. In our case the concept &amp;quot;black + death,&amp;quot; which does not refer to any normal experiences, would be discarded as useless, although the collection of sentences would be recognized as a strange, even if coherent, paragraph. We can also hope for some fine-tuning of the notion of topic, which would prevent many offensive examples.</context>
</contexts>
<marker>Fowler, 1965</marker>
<rawString>Fowler, H.W. (1965). A Dictionary of Modern English Usage. Oxford University Press. New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Genesereth</author>
<author>N J Nilsson</author>
</authors>
<date>1987</date>
<booktitle>Logical Foundations of Artificial Intelligence.</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>Los Altos, California.</location>
<contexts>
<context position="82990" citStr="Genesereth and Nilsson 1987" startWordPosition="13698" endWordPosition="13701">th the pronoun following the element that it refers to), not cataphoric (the other way around). This means that the &amp;quot;it&amp;quot; that brought the disease in P1 will not be considered to refer to the infection &amp;quot;i&amp;quot; or the death &amp;quot;d&amp;quot; in P3. This strategy is certainly the right one to start out with, since anaphora is always the more typical direction of reference in English prose (Halliday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &amp;quot;unique-name assumption&amp;quot; (Genesereth and Nilsson 1987), &amp;quot;domain closure assumption&amp;quot; (ibid.), &amp;quot;domain circumscription&amp;quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &amp;quot;abductive inference&amp;quot; of Reggia (1985), the &amp;quot;diagnosis from first principles&amp;quot; of Reiter (1987), &amp;quot;explainability&amp;quot; of Poole (1988), and the subset principle of Berwick (1986). But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being e</context>
</contexts>
<marker>Genesereth, Nilsson, 1987</marker>
<rawString>Genesereth, M.R., and Nilsson, N.J. (1987). Logical Foundations of Artificial Intelligence. Morgan Kaufmann. Los Altos, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Grice</author>
</authors>
<title>and Conversation.&amp;quot;</title>
<date>1978</date>
<booktitle>Further Notes on Logic Computational Linguistics Volume 17, Number</booktitle>
<volume>2</volume>
<pages>113--128</pages>
<editor>In Cole, P., ed.</editor>
<publisher>Academic Press.</publisher>
<location>New York, New York:</location>
<marker>Grice, 1978</marker>
<rawString>Grice, H.P. (1978). &amp;quot;Further Notes on Logic Computational Linguistics Volume 17, Number 2 and Conversation.&amp;quot; In Cole, P., ed. Syntax and Semantics 9: Pragmatics. Academic Press. New York, New York: 113-128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Grice</author>
</authors>
<title>Logic and Conversation.&amp;quot;</title>
<date>1975</date>
<booktitle>Syntax and Semantics 3: Speech Acts.</booktitle>
<pages>41--58</pages>
<editor>In Cole, R, and Morgan, J.L., eds.,</editor>
<publisher>Academic Press.</publisher>
<location>New York, New York:</location>
<contexts>
<context position="55567" citStr="Grice (1975" startWordPosition="9081" endWordPosition="9082"> obvious or the most important information about a formula. This immediate information may be insufficient to decide the truth of certain predicates. It would seem therefore that the iteration of the PT operation to form a closure is needed (cf. Zadrozny 1987b). 187 Computational Linguistics Volume 17, Number 2 However, there are at least three arguments against iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time. Secondly, the cooperative principle of Grice (1975, 1978), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader. Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct </context>
</contexts>
<marker>Grice, 1975</marker>
<rawString>Grice, H.P. (1975). &amp;quot;Logic and Conversation.&amp;quot; In Cole, R, and Morgan, J.L., eds., Syntax and Semantics 3: Speech Acts. Academic Press. New York, New York: 41-58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A C Groesser</author>
</authors>
<date>1981</date>
<booktitle>Prose Comprehension Beyond the Word.</booktitle>
<publisher>Springer.</publisher>
<location>New York, New York.</location>
<contexts>
<context position="55935" citStr="Groesser (1981)" startWordPosition="9146" endWordPosition="9147"> iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time. Secondly, the cooperative principle of Grice (1975, 1978), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader. Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph. Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions. Let P be a paragraph, let .15 = (s1,... , S) be its translation into a sequence of logical formulas. The se</context>
</contexts>
<marker>Groesser, 1981</marker>
<rawString>Groesser, A.C. (1981). Prose Comprehension Beyond the Word. Springer. New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
</authors>
<date>1978</date>
<journal>DISCOURSE KNOWLEDGE-Section</journal>
<volume>4</volume>
<pages>229--344</pages>
<editor>of Walker, D.E., ed., Understanding Spoken Language.</editor>
<publisher>North-Holland.</publisher>
<location>New York, New York:</location>
<marker>Grosz, 1978</marker>
<rawString>Grosz, B.J. (1978). DISCOURSE KNOWLEDGE-Section 4 of Walker, D.E., ed., Understanding Spoken Language. North-Holland. New York, New York: 229-344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
</authors>
<title>The Representation and Use of Focus in a System for Understanding Dialogs.&amp;quot;</title>
<date>1977</date>
<booktitle>Proc. IJCAI-77.</booktitle>
<pages>67--76</pages>
<publisher>W. Kaufmann.</publisher>
<location>Los Altos, California:</location>
<contexts>
<context position="82051" citStr="Grosz 1977" startWordPosition="13545" endWordPosition="13546">of Ockham&apos;s razor or abduction; it says &amp;quot;minimize the number of things that have the property P(*,*),&amp;quot; and it allows us to draw certain conclusions on the basis of partial information. We shall see it in action in Section 5.2. We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution. They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it). Other factors, such as the role of focus (Grosz 1977, 1978; Sidner 1983) or quantifier scoping (Webber 1983) must play a role, too. Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself. Note: In our translation from English to logic we are assuming that &amp;quot;it&amp;quot; is anaphoric (with the pronoun following the element that it refers to), not cataphoric (the other way around). This means that the &amp;quot;it&amp;quot; that brought the disease in P1 will not be considered to refer to the infection &amp;quot;i&amp;quot; or the death &amp;quot;d&amp;quot; in P3. This strategy is certainly the right one to start out with, </context>
</contexts>
<marker>Grosz, 1977</marker>
<rawString>Grosz, B.J. (1977). &amp;quot;The Representation and Use of Focus in a System for Understanding Dialogs.&amp;quot; Proc. IJCAI-77. W. Kaufmann. Los Altos, California: 67-76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Haberlandt</author>
<author>C Berian</author>
<author>J Sandson</author>
</authors>
<title>The Episode Schema in Story Processing.&amp;quot;</title>
<date>1980</date>
<journal>Journal of Verbal Learning and Verbal Behavior</journal>
<volume>19</volume>
<pages>635--650</pages>
<contexts>
<context position="10028" citStr="Haberlandt et al. (1980)" startWordPosition="1574" endWordPosition="1577"> oriented research work can be found in Bond and Hayes (1983). These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found three major formal devices that are used, by readers, to identify a paragraph: (1) the repetition of content words (nouns, verbs, adjectives, adverbs); (2) pronoun reference; and (3) paragraph length, as determined by spatial and/or sentence-count information. Other psycholing-uistic studies that confirm the validity of paragraph units can be found in Black and Bower (1979) and Haberlandt et al. (1980). The textualist approach to paragraph analysis is exemplified by E. J. Crothers. His work is taxonomic, in that he performs detailed descriptive analyses of paragraphs. He lists, classifies, and discusses various types of inference, by which he means, generally, &amp;quot;the linguistic-logical notions of consequent and presupposition&amp;quot; Crothers (1979:112) have collected convincing evidence of the existence of language chunks—real structures, not just orthographic conventions—that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like senten</context>
<context position="11695" citStr="Haberlandt et al. 1980" startWordPosition="1814" endWordPosition="1817">and narrative (in this case, spontaneous conversation). For each type, 173 Computational Linguistics Volume 17, Number 2 its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined). Segments themselves are composed of clauses and regulated by &amp;quot;switching&amp;quot; patterns, such as the question-answer pattern and the remark-reply pattern. 2.2 Our View of Paragraphs: An Informal Sketch Although there are other discussions of the paragraph as a central element of discourse (e.g. Chafe 1979, Halliday and Hasan 1976, Longacre 1979, Haberlandt et al. 1980), all of them share a certain limitation in their formal techniques for analyzing paragraph structure. Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area. We suggest that the paragraph is a grammatical and logical unit. It is the smallest linguistic representation of what, in logic, is called a &amp;quot;model,&amp;quot; and it is the first reasonable domain of anaphora resolution, and of coherent thought about a central topic. A paragraph can be th</context>
</contexts>
<marker>Haberlandt, Berian, Sandson, 1980</marker>
<rawString>Haberlandt, K., Berian, C., and Sandson, J. (1980). &amp;quot;The Episode Schema in Story Processing.&amp;quot; Journal of Verbal Learning and Verbal Behavior 19: 635-650.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>R Hasan</author>
</authors>
<title>Cohesion in English.</title>
<date>1976</date>
<publisher>Longman Group Ltd.</publisher>
<location>London.</location>
<contexts>
<context position="11655" citStr="Halliday and Hasan 1976" startWordPosition="1808" endWordPosition="1811">rocedural (how-to), expository (essay), and narrative (in this case, spontaneous conversation). For each type, 173 Computational Linguistics Volume 17, Number 2 its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined). Segments themselves are composed of clauses and regulated by &amp;quot;switching&amp;quot; patterns, such as the question-answer pattern and the remark-reply pattern. 2.2 Our View of Paragraphs: An Informal Sketch Although there are other discussions of the paragraph as a central element of discourse (e.g. Chafe 1979, Halliday and Hasan 1976, Longacre 1979, Haberlandt et al. 1980), all of them share a certain limitation in their formal techniques for analyzing paragraph structure. Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area. We suggest that the paragraph is a grammatical and logical unit. It is the smallest linguistic representation of what, in logic, is called a &amp;quot;model,&amp;quot; and it is the first reasonable domain of anaphora resolution, and of coherent thought abou</context>
<context position="82757" citStr="Halliday and Hasan 1976" startWordPosition="13664" endWordPosition="13667">ermining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself. Note: In our translation from English to logic we are assuming that &amp;quot;it&amp;quot; is anaphoric (with the pronoun following the element that it refers to), not cataphoric (the other way around). This means that the &amp;quot;it&amp;quot; that brought the disease in P1 will not be considered to refer to the infection &amp;quot;i&amp;quot; or the death &amp;quot;d&amp;quot; in P3. This strategy is certainly the right one to start out with, since anaphora is always the more typical direction of reference in English prose (Halliday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &amp;quot;unique-name assumption&amp;quot; (Genesereth and Nilsson 1987), &amp;quot;domain closure assumption&amp;quot; (ibid.), &amp;quot;domain circumscription&amp;quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &amp;quot;abductive inference&amp;quot; of Reggia (1985), the &amp;quot;diagnosis from first principles&amp;quot; of Reiter (1987), &amp;quot;explainability&amp;quot; of Poole (1988), and the subset principle of Berwick (1986). But, obv</context>
<context position="101000" citStr="Halliday and Hasan 1976" startWordPosition="16822" endWordPosition="16825">t two rules that prevent infelicitous uses of &amp;quot;but.&amp;quot; Connectives are function words—like conjunctions and some adverbs—that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units. &amp;quot;And,&amp;quot; &amp;quot;or,&amp;quot; and &amp;quot;but&amp;quot; are the three main coordinating connectives in English. However, &amp;quot;but&amp;quot; does not behave quite like the other two—semantically, &amp;quot;but&amp;quot; signals a contradiction, and in this role it seems to have three subfunctions: 1. Opposition (called &amp;quot;adversative&amp;quot; or &amp;quot;contrary-to-expectation&amp;quot; by Halliday and Hasan 1976; cf. also Quirk et al. 1972, p. 672). The ship arrived but the passengers could not get off. The yacht is cheap but elegant. 2. Comparison. In this function, the first conjunct is not so directly contradicted by the second. A contradiction exists, but we may have to 202 Zadrozny and Jensen Semantics of Paragraphs go through additional levels of implication to find it. Consider the sentence: That basketball player is short, but he&apos;s very quick. 3. Affirmation. This use of &amp;quot;but&amp;quot; always follows a negative clause, and actually augments the meaning of the preceding clause by adding supporting info</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Halliday, M.A.K., and Hasan, R. (1976). Cohesion in English. Longman Group Ltd. London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Haugeland</author>
</authors>
<title>Artificial Intelligence: The Very Idea.</title>
<date>1985</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="21932" citStr="Haugeland 1985" startWordPosition="3466" endWordPosition="3467">disease(x4) &amp; xl = s &amp; x2 == m &amp; x3 =s &amp; x4 = d, where s, m, d, are constants. We adopt the three-level semantics as a formal tool for the analysis of paragraphs. This semantics was constructed (Zadrozny 1987a, 1987b) as a formal framework for default and commonsense reasoning. It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates. For instance, relating &amp;quot;they&amp;quot; to &amp;quot;apples&amp;quot; in the sentence (cf. Haugeland 1985 p. 195; Zadrozny 1987a): We bought the boys apples because they were so cheap 176 Zadrozny and Jensen Semantics of Paragraphs can be an example of such a most plausible choice. The main ideas of the three-level semantics can be stated as follows: 1. Reasoning takes place in a three-level structure consisting of an object level, a referential level, and a metalevel. 2. The object level is used to describe the current situation, and in our case is reserved for the formal representation of paragraph sentences. For the sake of simplicity, the object level will consist of a first order theory. 3. </context>
</contexts>
<marker>Haugeland, 1985</marker>
<rawString>Haugeland, J. (1985). Artificial Intelligence: The Very Idea. MIT Press. Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hinds</author>
</authors>
<title>Organizational Patterns in Discourse.&amp;quot;</title>
<date>1979</date>
<booktitle>Syntax and Semantics,</booktitle>
<volume>12</volume>
<editor>In Givon, T., ed.,</editor>
<publisher>Academic Press.</publisher>
<location>New York, New York.</location>
<contexts>
<context position="10733" citStr="Hinds (1979)" startWordPosition="1673" endWordPosition="1674"> taxonomic, in that he performs detailed descriptive analyses of paragraphs. He lists, classifies, and discusses various types of inference, by which he means, generally, &amp;quot;the linguistic-logical notions of consequent and presupposition&amp;quot; Crothers (1979:112) have collected convincing evidence of the existence of language chunks—real structures, not just orthographic conventions—that are smaller than a discourse, larger than a sentence, generally composed of sentences, and recursive in nature (like sentences). These chunks are sometimes called &amp;quot;episodes,&amp;quot; and sometimes &amp;quot;paragraphs.&amp;quot; According to Hinds (1979), paragraphs are made up of segments, which in turn are made up of sentences or clauses, which in turn are made up of phrases. Paragraphs therefore give hierarchical structure to sentences. Hinds discusses three major types of paragraphs, and their corresponding segment types. The three types are procedural (how-to), expository (essay), and narrative (in this case, spontaneous conversation). For each type, 173 Computational Linguistics Volume 17, Number 2 its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined). Seg</context>
</contexts>
<marker>Hinds, 1979</marker>
<rawString>Hinds, J. (1979). &amp;quot;Organizational Patterns in Discourse.&amp;quot; In Givon, T., ed., Syntax and Semantics, Vol. 12. Academic Press. New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hintikka</author>
</authors>
<title>The Game of Language.</title>
<date>1985</date>
<location>D. Reidel. Dordrecht.</location>
<contexts>
<context position="26858" citStr="Hintikka (1985)" startWordPosition="4266" endWordPosition="4267">can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences. 3.1 Finite Representations, Finite Theories Unless explicitly stated otherwise, we assume that formulas are expressed in a certain (formal) language L without equality; the extension L(=) of L is going to be used only in Section 5 for dealing with noun phrase references. This means that natural language expressions such as &amp;quot;A is B,&amp;quot; &amp;quot;A is the same as B,&amp;quot; etc. are not directly represented by logical equality; similarly, &amp;quot;not&amp;quot; is often not treated as logical negation; cf. Hintikka (1985). All logical notions that we are going to consider, such as theory or model, will be finitary. For example, a model would typically contain fewer than a hundred elements of different logical sorts. Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff (19</context>
</contexts>
<marker>Hintikka, 1985</marker>
<rawString>Hintikka, J. (1985). The Game of Language. D. Reidel. Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
</authors>
<title>Semantic Interpretation and the Resolution of Ambiguity.</title>
<date>1987</date>
<publisher>Cambridge University Press. Cambridge.</publisher>
<contexts>
<context position="53193" citStr="Hirst 1987" startWordPosition="8699" endWordPosition="8700">tween the &amp;quot;definitional&amp;quot; and factual information, but the &amp;quot;definitional&amp;quot; part contains collections of mutually excluding theories, not just of formulas describing a semantic network. Moreover, in addition to proposing this structure of R, we have described the two mechanisms for exploiting it, &amp;quot;coherence&amp;quot; and &amp;quot;dominance,&amp;quot; which are not variants of the standard first order entailment, but abduction. The idea of using preferences among theories is new, hence it was described in more detail. &amp;quot;Coherence,&amp;quot; as outlined above, can be understood as a declarative (or static) version of marker passing (Hirst 1987; Charniak 1983), with one difference: the activation spreads to theories that share a predicate, not through the IS-A hierarchy, and is limited to elementary facts about predicates appearing in the text. The metalevel rules we are going to discuss in Section 6, and that deal with the Gricean maxims and the meaning of &amp;quot;but,&amp;quot; can be easily expressed in the languages of set theory or higher order logic, but not everything expressible in those languages makes sense in natural language. Hence, putting limitations on the expressive power of the language of the metalevel will remain as one of many o</context>
</contexts>
<marker>Hirst, 1987</marker>
<rawString>Hirst, G. (1987). Semantic Interpretation and the Resolution of Ambiguity. Cambridge University Press. Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
</authors>
<title>Towards an Understanding of Coherence in Discourse.&amp;quot;</title>
<date>1982</date>
<booktitle>Strategies for Natural Language Processing. Lawrence Erlbaum.</booktitle>
<pages>223--244</pages>
<editor>In Lehnert, W.G., and Ringle, M.H., eds.</editor>
<location>Hillsdale, New Jersey:</location>
<marker>Hobbs, 1982</marker>
<rawString>Hobbs, J.R. (1982). &amp;quot;Towards an Understanding of Coherence in Discourse.&amp;quot; In Lehnert, W.G., and Ringle, M.H., eds. Strategies for Natural Language Processing. Lawrence Erlbaum. Hillsdale, New Jersey: 223-244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
</authors>
<title>Coherence and Coreference.&amp;quot;</title>
<date>1979</date>
<journal>Cognitive Science</journal>
<volume>3</volume>
<pages>67--90</pages>
<contexts>
<context position="17089" citStr="Hobbs (1979" startWordPosition="2690" endWordPosition="2691">ttached at the level of the main verb of the sentence, then &amp;quot;ship&amp;quot; is the referent. But syntactic relations do not suffice to resolve anaphora: Hobbs&apos; (1976) algorithm for resolving the reference of pronouns, depending only on the surface syntax of sentences in the text, when applied to &amp;quot;it&amp;quot; in the example paragraph, fails in both cases to identify the most likely referent NP. Adding selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation. Later, Hobbs (1979, 1982) proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using &amp;quot;salience&amp;quot; in choosing facts from this knowledge base. We will investigate the possibility that the structure of this knowledge base can actually resemble the structure of, for example, natural language dictionaries. The process of finding referents could then be automated. Determining that the most likely subject for &amp;quot;bringing,&amp;quot; in the first sentence, is the noun &amp;quot;ship&amp;quot; is done in the following fashion. The first definition for &amp;quot;bring&amp;quot; in W7 (Webster&apos;s Sev</context>
<context position="63871" citStr="Hobbs (1979" startWordPosition="10505" endWordPosition="10506">is defined as a longest, a shortest, or—simply—a sequence of predicates satisfying the conditions (1) and (2); the existence of a sequence is equivalent with the existence of a shortest and a longest sequence. The reason for choosing a longest sequence as the topic is our belief that the topic should rather contain more information about a paragraph than less. 4.1 Comparison with Other Approaches At this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others. We are going to make such a comparison with the theories proposed by J. Hobbs (1979, 1982) that represent a more computationally oriented approach to coherence, and those of T.A. van Dijk and W. Kintch (1983), who are more interested in addressing psychological and cognitive aspects of discourse coherence. The quoted works seem to be good representatives for each of the directions; they also point to related literature. The approach we advocate is compatible with the work of these researchers, we believe. There are, however, some interesting differences: first of all, we emphasize the role of paragraphs; second, we talk about formal principles regulating the organization and</context>
<context position="66698" citStr="Hobbs (1979" startWordPosition="10940" endWordPosition="10941">meaningful computational models. To be sure, we believe relations between pairs of sentences are worth investigating, especially in dialogs. However, in written discourse, the smallest domain of coherence is a paragraph, very much as the sentence is the basic domain of grammaticality (although one can also judge the correctness of phrases). To see the advantage of assuming that coherence is a property of a fragment of a text/discourse, and not a relation between subsequent sentences, let us consider for instance the text John took a train from Paris to Istanbul. He likes spinach. According to Hobbs (1979, p. 67), these two sentences are incoherent. However, the same fragment, augmented with the third sentence Mary told him yesterday that the French spinach crop failed and Turkey is the only country. . . (ibid.) suddenly (for Hobbs) becomes coherent. It seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change; after all, the first two sentences didn&apos;t change when the third one was added. On the other hand, this change is easily explained when we treat the first two sentences as a paragraph: if the third sentence is not a part </context>
<context position="94962" citStr="Hobbs (1979)" startWordPosition="15825" endWordPosition="15826">d to be true. The Maxim of Manner seems to us to be more relevant for critiquing the style of a written passage or for natural language generation; in the case of text generation, it can be construed as a requirement that the produced text be coherent and cohesive. We do not claim that Gla is the best or unique way of expressing the rule &amp;quot;assume that the writer did not say too much.&amp;quot; Rather, we stress the possibility that one can axiomatize and productively use such a rule. We shall see this in the next example: two sentences, regarded as a fragment of paragraph, are a variation on a theme by Hobbs (1979). Example 3 The captain is worried because the third officer can open his safe. He knows the combination. The above metarule postulating &amp;quot;nonredundancy&amp;quot; implies that &amp;quot;he&amp;quot; = &amp;quot;the third officer,&amp;quot; &amp;quot;his&amp;quot; = &amp;quot;the captain&apos;s&amp;quot; are the referents of the pronouns. This is because the formula safe(x) (owns(y , x) &amp; cmbntn(z, x) knows(y , z) &amp; can_open(y , x)) G Tsafe, belongs to R, since it is common knowledge about safes that they have owners, and also combinations that are known to the owners. Therefore &amp;quot;his&amp;quot; = &amp;quot;the third officer&apos;s&amp;quot; would produce a redundant formula, corresponding to the sentence The thi</context>
<context position="98322" citStr="Hobbs 1979" startWordPosition="16393" endWordPosition="16394">t plausible interpretation of our example sentences. Note: The reader must have noticed that we did not bother to distinguish the sentences Pl, P2, Q1 and Q2 from their logical forms. Representing &amp;quot;because&amp;quot; and &amp;quot;know&amp;quot; adequately should be considered a separate topic; representing the rest (in the first order convention of this paper) is trivial. 6.1.1 Was the Use of a Gricean Maxim Necessary? Can one deal effectively with the problem of reference without axiomatized Gricean maxims, for instance by using only 201 Computational Linguistics Volume 17, Number 2 &amp;quot;petty conversational implicature&amp;quot; (Hobbs 1979), or the metarules of Section 5.2? It seems to us that the answer is no. As a case in point, consider the process of finding the antecedent of the anaphor &amp;quot;he&amp;quot; in the sentences John can open Bill&apos;s safe. He knows the combination. Hobbs (1979, 1982) proves &amp;quot;he&amp;quot; = &amp;quot;John&amp;quot; by assuming the relation of &amp;quot;elaboration&amp;quot; between the sentences. (Elaboration is a relation between two segments of a text. It intuitively means &amp;quot;expressing the same thought from a different perspective,&amp;quot; but has been defined formally as the existence of a proposition implied by both segments— here the proposition is &amp;quot;John can o</context>
</contexts>
<marker>Hobbs, 1979</marker>
<rawString>Hobbs, J.R. (1979). &amp;quot;Coherence and Coreference.&amp;quot; Cognitive Science 3: 67-90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
</authors>
<title>Resolving Pronoun References.&amp;quot;</title>
<date>1978</date>
<journal>Lingua</journal>
<volume>44</volume>
<pages>311--338</pages>
<contexts>
<context position="76176" citStr="Hobbs (1978" startWordPosition="12518" endWordPosition="12519">theory of our example paragraph and a formula expressing the equality of two constants, i and i&apos;, denoting (respectively) the &amp;quot;infection&amp;quot; in the sentence Within twentyfour hours of infection..., and the &amp;quot;infection&amp;quot; of the theory (d1)—a disease is an illness caused by an infection. This equality i = i&apos; cannot be proven, but it may be reasonably assumed—we know that in this case the infection i&apos; caused the illness, which, in turn, caused the death. The necessity of this kind of merging of arguments has been recognized before: Charniak and McDermott (1985) call it abductive unification/matching, Hobbs (1978, 1979) refers to such operations using the terms knitting or petty conversational implicature. Neither Hobbs nor Charniak and McDermott tried then to make this notion precise, but the paper by Hobbs et al. (1988) moves in that direction. The purpose of this subsection is to formalize and explain how assumptions like that one above can be made. Definition A formula 0 is weakly provable from an object theory T, expressed as T HR 0, iff there exists a partial theory T E PT(T) such that T 0, i.e. T proves 0 in logic. (We call HR &amp;quot;weak&amp;quot; because it is enough to find one partial theory proving a giv</context>
</contexts>
<marker>Hobbs, 1978</marker>
<rawString>Hobbs, J.R. (1978). &amp;quot;Resolving Pronoun References.&amp;quot; Lingua 44: 311-338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
</authors>
<title>38 Examples of Elusive Antecedents from Published Texts.</title>
<date>1977</date>
<tech>Research Report 77-2.</tech>
<institution>Department of Computer Science,</institution>
<location>City College, CUNY, New York.</location>
<contexts>
<context position="16932" citStr="Hobbs 1977" startWordPosition="2668" endWordPosition="2669">ssina&amp;quot; is the subject of &amp;quot;bringing&amp;quot; and must be the referent for &amp;quot;it.&amp;quot; If the clause modifies &amp;quot;port,&amp;quot; then &amp;quot;port&amp;quot; is the desired referent; if the clause is attached at the level of the main verb of the sentence, then &amp;quot;ship&amp;quot; is the referent. But syntactic relations do not suffice to resolve anaphora: Hobbs&apos; (1976) algorithm for resolving the reference of pronouns, depending only on the surface syntax of sentences in the text, when applied to &amp;quot;it&amp;quot; in the example paragraph, fails in both cases to identify the most likely referent NP. Adding selectional restrictions (semantic feature information, Hobbs 1977) does not solve the problem, because isolated features offer only part of the background knowledge necessary for reference disambiguation. Later, Hobbs (1979, 1982) proposed a knowledge base in which information about language and the world would be encoded, and he emphasized the need for using &amp;quot;salience&amp;quot; in choosing facts from this knowledge base. We will investigate the possibility that the structure of this knowledge base can actually resemble the structure of, for example, natural language dictionaries. The process of finding referents could then be automated. Determining that the most lik</context>
</contexts>
<marker>Hobbs, 1977</marker>
<rawString>Hobbs, J.R. (1977). 38 Examples of Elusive Antecedents from Published Texts. Research Report 77-2. Department of Computer Science, City College, CUNY, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
</authors>
<title>Pronoun Resolution.</title>
<date>1976</date>
<tech>Research Report 76-1.</tech>
<institution>Dept. of Computer Science.</institution>
<location>City College, CUNY, New York.</location>
<marker>Hobbs, 1976</marker>
<rawString>Hobbs, J.R. (1976). Pronoun Resolution. Research Report 76-1. Dept. of Computer Science. City College, CUNY, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
<author>M Stickel</author>
<author>P Martin</author>
<author>D Edwards</author>
</authors>
<title>Interpretation as Abduction.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proc. of 26th Annual Meeting of the Association for Computational Linguistics, ACL:</booktitle>
<pages>95--103</pages>
<contexts>
<context position="76389" citStr="Hobbs et al. (1988)" startWordPosition="12550" endWordPosition="12553">nfection&amp;quot; of the theory (d1)—a disease is an illness caused by an infection. This equality i = i&apos; cannot be proven, but it may be reasonably assumed—we know that in this case the infection i&apos; caused the illness, which, in turn, caused the death. The necessity of this kind of merging of arguments has been recognized before: Charniak and McDermott (1985) call it abductive unification/matching, Hobbs (1978, 1979) refers to such operations using the terms knitting or petty conversational implicature. Neither Hobbs nor Charniak and McDermott tried then to make this notion precise, but the paper by Hobbs et al. (1988) moves in that direction. The purpose of this subsection is to formalize and explain how assumptions like that one above can be made. Definition A formula 0 is weakly provable from an object theory T, expressed as T HR 0, iff there exists a partial theory T E PT(T) such that T 0, i.e. T proves 0 in logic. (We call HR &amp;quot;weak&amp;quot; because it is enough to find one partial theory proving a given formula.) As an example, in the case of the three-sentence paragraph, we have a partial theory Ti based on (sib) saying that &amp;quot; &apos;it&apos; hits rapidly,&amp;quot; and T2 saying that &amp;quot;an illness (&apos;it&apos;) harms rapidly&amp;quot; (s2_ex). T</context>
</contexts>
<marker>Hobbs, Stickel, Martin, Edwards, 1988</marker>
<rawString>Hobbs, J., Stickel, M., Martin, P., and Edwards, D. (1988). &amp;quot;Interpretation as Abduction.&amp;quot; In Proc. of 26th Annual Meeting of the Association for Computational Linguistics, ACL: 95-103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jackendoff</author>
</authors>
<title>Semantics and Cognition.</title>
<date>1983</date>
<publisher>The MIT Press.</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="25876" citStr="Jackendoff (1983" startWordPosition="4102" endWordPosition="4103">mbling the standard first order language. But, obviously, there are other possibilities—for instance, the discourse representation structures (DRS&apos;s) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical 177 Computational Linguistics Volume 17, Number 2 notation of Montague (1970) is more sophisticated, and may be considered another possibility. Jackendoff&apos;s (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes &amp;quot;it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.&amp;quot; Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics. It will also be a model for our simplified logical notation (cf. Section 5). We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences. 3.1 Finite Representations, Finite Theories Unless explicitly stated otherwise, we assume that formulas are </context>
<context position="27461" citStr="Jackendoff (1983)" startWordPosition="4361" endWordPosition="4362">intikka (1985). All logical notions that we are going to consider, such as theory or model, will be finitary. For example, a model would typically contain fewer than a hundred elements of different logical sorts. Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff (1983), Kamp (1981), and implicitly or explicitly by almost all researchers in computational linguistics. As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. Mycielski 1981). On the other hand, a possible obstacle to our strategy of using only finite objects is the fact that the deductive closure of any set of formulas is not finite in standard logic, while, clearly, we will have to deduce new facts from formal representations of </context>
</contexts>
<marker>Jackendoff, 1983</marker>
<rawString>Jackendoff, R. (1983). Semantics and Cognition. The MIT Press. Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jensen</author>
</authors>
<title>Issues in Parsing.&amp;quot;</title>
<date>1988</date>
<booktitle>Natural Language at the Computer.</booktitle>
<pages>65--83</pages>
<editor>In A. Blaser, ed.,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin, Germany:</location>
<marker>Jensen, 1988</marker>
<rawString>Jensen, K. (1988). &amp;quot;Issues in Parsing.&amp;quot; In A. Blaser, ed., Natural Language at the Computer. Springer-Verlag, Berlin, Germany: 65-83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jensen</author>
</authors>
<title>Parsing Strategies in a Broad-Coverage Grammar of English. Research Report RC 12147. IBM T.J. Watson Research Center,</title>
<date>1986</date>
<location>Yorktown Heights, New York.</location>
<contexts>
<context position="62741" citStr="Jensen 1986" startWordPosition="10317" endWordPosition="10318">differentiating between the creation of a new concept like &amp;quot;black + death,&amp;quot; with a meaning given by a paraphrase of the example collection of sentences, and the acceptance of the new concept—storing it in R. In our case the concept &amp;quot;black + death,&amp;quot; which does not refer to any normal experiences, would be discarded as useless, although the collection of sentences would be recognized as a strange, even if coherent, paragraph. We can also hope for some fine-tuning of the notion of topic, which would prevent many offensive examples. This approach is taken in computational syntactic grammars (e.g. Jensen 1986); the number of unlikely parses is severely reduced whenever possible, but no attempt is made to define only the so-called grammatical strings of a language. Finally, as the paragraph is a natural domain in which word senses can be reliably assigned to words or sentences can be syntactically disambiguated, larger chunks of discourse may be needed for precise assignment of topics, which we view as another type of disambiguation. Notice also that for coherence, as defined above, it does not matter whether the topic is defined as a longest, a shortest, or—simply—a sequence of predicates satisfyin</context>
</contexts>
<marker>Jensen, 1986</marker>
<rawString>Jensen, K. (1986). Parsing Strategies in a Broad-Coverage Grammar of English. Research Report RC 12147. IBM T.J. Watson Research Center, Yorktown Heights, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jensen</author>
<author>J-L Binot</author>
</authors>
<title>Disambiguating Prepositional Phrase Attachments by Using On-line Dictionary Definitions.&amp;quot; Computational Linguistics 13.3-4.251-260 (special issue on the lexicon).</title>
<date>1988</date>
<contexts>
<context position="23293" citStr="Jensen and Binot 1988" startWordPosition="3684" endWordPosition="3687">nding of a given piece of text has to be extracted. It constrains interpretations of the predicates of an object theory. Its structure and the extraction methods will be discussed below. 4. Understanding has as its goal construction of an interpretation of the text, i.e. building some kind of model. Since not all logically permissible models are linguistically appropriate, one needs a place, namely the metalevel, to put constraints on types of models. Gricean maxims belong there; Section 6 will be devoted to a presentation of the metalevel rules corresponding to them. We have shown elsewhere (Jensen and Binot 1988; Zadrozny 1987a, 1987b) that natural language programs, such as on-line grammars and dictionaries, can be used as referential levels for commonsense reasoning—for example, to disambiguate PP attachment. This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory. The referential structures we are going to use are collections of logical theories, but the concept of reference is more general. Some of the intuitions we associate with this notion have been very well expressed by Turner (19</context>
</contexts>
<marker>Jensen, Binot, 1988</marker>
<rawString>Jensen, K., and Binot, J.-L. (1988). &amp;quot;Disambiguating Prepositional Phrase Attachments by Using On-line Dictionary Definitions.&amp;quot; Computational Linguistics 13.3-4.251-260 (special issue on the lexicon).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P N Johnson-Laird</author>
</authors>
<title>Mental Models.</title>
<date>1983</date>
<publisher>Harvard University Press.</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="27442" citStr="Johnson-Laird (1983)" startWordPosition="4359" endWordPosition="4360">ogical negation; cf. Hintikka (1985). All logical notions that we are going to consider, such as theory or model, will be finitary. For example, a model would typically contain fewer than a hundred elements of different logical sorts. Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff (1983), Kamp (1981), and implicitly or explicitly by almost all researchers in computational linguistics. As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. Mycielski 1981). On the other hand, a possible obstacle to our strategy of using only finite objects is the fact that the deductive closure of any set of formulas is not finite in standard logic, while, clearly, we will have to deduce new facts from formal </context>
</contexts>
<marker>Johnson-Laird, 1983</marker>
<rawString>Johnson-Laird, P.N. (1983). Mental Models. Harvard University Press. Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kamp</author>
</authors>
<title>A Theory of Truth and Semantic Representation.&amp;quot;</title>
<date>1981</date>
<booktitle>Formal Methods in the Study of Language, I. Mathematisch Centrum,</booktitle>
<pages>277--322</pages>
<editor>In Groenendijk, J.A.G., et al. eds.,</editor>
<location>Amsterdam:</location>
<contexts>
<context position="25424" citStr="Kamp (1981)" startWordPosition="4032" endWordPosition="4033">le to think about constraining linguistic or logical predicates by simulating physical experiences (cf. Woods 1987). We assume here that a translation of the surface forms of sentences into a logical formalism is possible. Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But, obviously, there are other possibilities—for instance, the discourse representation structures (DRS&apos;s) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical 177 Computational Linguistics Volume 17, Number 2 notation of Montague (1970) is more sophisticated, and may be considered another possibility. Jackendoff&apos;s (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes &amp;quot;it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the infor</context>
<context position="27474" citStr="Kamp (1981)" startWordPosition="4363" endWordPosition="4364"> logical notions that we are going to consider, such as theory or model, will be finitary. For example, a model would typically contain fewer than a hundred elements of different logical sorts. Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff (1983), Kamp (1981), and implicitly or explicitly by almost all researchers in computational linguistics. As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. Mycielski 1981). On the other hand, a possible obstacle to our strategy of using only finite objects is the fact that the deductive closure of any set of formulas is not finite in standard logic, while, clearly, we will have to deduce new facts from formal representations of text and back</context>
</contexts>
<marker>Kamp, 1981</marker>
<rawString>Kamp, H. (1981). &amp;quot;A Theory of Truth and Semantic Representation.&amp;quot; In Groenendijk, J.A.G., et al. eds., Formal Methods in the Study of Language, I. Mathematisch Centrum, Amsterdam: 277-322,</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Labov</author>
</authors>
<title>The Boundaries of Words and Their Meanings.&amp;quot;</title>
<date>1973</date>
<journal>In Fishman, J., New Ways of Analyzing Variation in English. Georgetown</journal>
<pages>340--373</pages>
<location>Washington, D.C.:</location>
<contexts>
<context position="106636" citStr="Labov (1973)" startWordPosition="17759" endWordPosition="17760"> a theory of some cb is a &amp;quot;law&amp;quot; is by deleting the empty interpretation of 0 from the partial order. If we accept the definition of a concept as given by necessary and sufficient conditions, the theories would all appear as laws. If we subscribe to a more realistic view where definitions are given by a collection of central/prototypical and peripheral conditions, only the peripheral ones can be contradicted by &amp;quot;but.&amp;quot; In either formalization we get BUT_Cl as a consequence: Since &amp;quot;laws&amp;quot; cannot be deleted, BUT can&apos;t be applied, and hence its use in those kinds of sentences would be incorrect. W. Labov (1973) discussed sentences of the form *This is a chair but you can sit on it. The sentence is incorrect, since the function &amp;quot;one can sit on it&amp;quot; belongs to the core of the concept &amp;quot;chair&amp;quot;; so—contrary to the role of &amp;quot;but&amp;quot;—the sentence does not contain any surprising new elements. Using the Metarule (BUT) and the cooperative principle of Grice, we get BUT_C2: 4) but IF is incorrect, if 4) ---4 kIf is a &amp;quot;law.&amp;quot; The Metarule (BUT) gives the semantics of &amp;quot;but;&amp;quot; the rules BUT_Cl and BUT_C2 follow from it (after formalization in a sufficiently strong metalanguage such as type theory or set theory). We can </context>
</contexts>
<marker>Labov, 1973</marker>
<rawString>Labov, W. (1973). &amp;quot;The Boundaries of Words and Their Meanings.&amp;quot; In Fishman, J., New Ways of Analyzing Variation in English. Georgetown U. Press. Washington, D.C.: 340-373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Lakoff</author>
</authors>
<title>Women, Fire and Dangerous Things. The University of Chicago Press.</title>
<date>1987</date>
<location>Chicago, Illinois.</location>
<marker>Lakoff, 1987</marker>
<rawString>Lakoff, G. (1987). Women, Fire and Dangerous Things. The University of Chicago Press. Chicago, Illinois.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H J Levesque</author>
</authors>
<title>A Logic of Implicit and Explicit Beliefs.&amp;quot;</title>
<date>1984</date>
<booktitle>Proc. AAAI-84. AAAI:</booktitle>
<pages>198--202</pages>
<contexts>
<context position="30511" citStr="Levesque 1984" startWordPosition="4886" endWordPosition="4887">rpretation defined on a certain domain, which satisfies all formulas of T. The collection of all (finite) models of a theory T will be denoted by Mods(T). • The set of all subformulas of a collection of formulas F is denoted by Form(F). 0 is a ground instance of a formula 0, if 0 contains no variables, and 0 = 00, for some substitution O. Thus, we do not require Th(T) to be closed under substitution instances of tautologies. Although in this paper we take modus ponens as the main rule of inference, in general one can consider deductive closures with respect to weaker, nonstandard logics, (cf. Levesque 1984; Frisch 1987; Patel-Schneider 1985). But we won&apos;t pursue this topic further here. 3.2 The Structure of Background Knowledge Background knowledge is not a simple list of meaning postulates—it has a structure and it may contain contradictions and ambiguities. These actualities have to be taken into account in any realistic model of natural language understanding. For instance, the verb &amp;quot;enter&amp;quot; is polysemous. But, unless context specifies otherwise, &amp;quot;to come in&amp;quot; is a more plausible meaning than &amp;quot;to join a group.&amp;quot; Assuming some logical representation of this knowledge, we can write that enter(x, </context>
</contexts>
<marker>Levesque, 1984</marker>
<rawString>Levesque, H.J. (1984). &amp;quot;A Logic of Implicit and Explicit Beliefs.&amp;quot; Proc. AAAI-84. AAAI: 198-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Longacre</author>
</authors>
<title>The Paragraph as a Grammatical Unit.</title>
<date>1979</date>
<booktitle>Syntax and Semantics,</booktitle>
<volume>12</volume>
<editor>In Givon, T., ed.,</editor>
<publisher>Academic Press.</publisher>
<location>New York, New York.</location>
<contexts>
<context position="11670" citStr="Longacre 1979" startWordPosition="1812" endWordPosition="1813">itory (essay), and narrative (in this case, spontaneous conversation). For each type, 173 Computational Linguistics Volume 17, Number 2 its segments are distinguished by bearing distinct relationships to the paragraph topic (which is central, but nowhere clearly defined). Segments themselves are composed of clauses and regulated by &amp;quot;switching&amp;quot; patterns, such as the question-answer pattern and the remark-reply pattern. 2.2 Our View of Paragraphs: An Informal Sketch Although there are other discussions of the paragraph as a central element of discourse (e.g. Chafe 1979, Halliday and Hasan 1976, Longacre 1979, Haberlandt et al. 1980), all of them share a certain limitation in their formal techniques for analyzing paragraph structure. Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area. We suggest that the paragraph is a grammatical and logical unit. It is the smallest linguistic representation of what, in logic, is called a &amp;quot;model,&amp;quot; and it is the first reasonable domain of anaphora resolution, and of coherent thought about a central top</context>
</contexts>
<marker>Longacre, 1979</marker>
<rawString>Longacre, R.E. (1979). The Paragraph as a Grammatical Unit. In Givon, T., ed., Syntax and Semantics, Vol. 12. Academic Press. New York, New York.</rawString>
</citation>
<citation valid="true">
<title>Longman Dictionary of Contemporary English.</title>
<date>1978</date>
<publisher>Longman Group Ltd.,</publisher>
<location>London.</location>
<contexts>
<context position="55574" citStr="(1975, 1978)" startWordPosition="9082" endWordPosition="9083">us or the most important information about a formula. This immediate information may be insufficient to decide the truth of certain predicates. It would seem therefore that the iteration of the PT operation to form a closure is needed (cf. Zadrozny 1987b). 187 Computational Linguistics Volume 17, Number 2 However, there are at least three arguments against iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time. Secondly, the cooperative principle of Grice (1975, 1978), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader. Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvi</context>
<context position="76183" citStr="(1978, 1979)" startWordPosition="12519" endWordPosition="12520"> of our example paragraph and a formula expressing the equality of two constants, i and i&apos;, denoting (respectively) the &amp;quot;infection&amp;quot; in the sentence Within twentyfour hours of infection..., and the &amp;quot;infection&amp;quot; of the theory (d1)—a disease is an illness caused by an infection. This equality i = i&apos; cannot be proven, but it may be reasonably assumed—we know that in this case the infection i&apos; caused the illness, which, in turn, caused the death. The necessity of this kind of merging of arguments has been recognized before: Charniak and McDermott (1985) call it abductive unification/matching, Hobbs (1978, 1979) refers to such operations using the terms knitting or petty conversational implicature. Neither Hobbs nor Charniak and McDermott tried then to make this notion precise, but the paper by Hobbs et al. (1988) moves in that direction. The purpose of this subsection is to formalize and explain how assumptions like that one above can be made. Definition A formula 0 is weakly provable from an object theory T, expressed as T HR 0, iff there exists a partial theory T E PT(T) such that T 0, i.e. T proves 0 in logic. (We call HR &amp;quot;weak&amp;quot; because it is enough to find one partial theory proving a given form</context>
</contexts>
<marker>1978</marker>
<rawString>Longman Dictionary of Contemporary English. (1978). Longman Group Ltd., London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lyons</author>
</authors>
<title>Introduction to Theoretical Linguistics.</title>
<date>1969</date>
<publisher>Cambridge University Press.</publisher>
<location>Cambridge, England.</location>
<marker>Lyons, 1969</marker>
<rawString>Lyons, J. (1969). Introduction to Theoretical Linguistics. Cambridge University Press. Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Mann</author>
<author>S A Thompson</author>
</authors>
<date>1983</date>
<booktitle>Relational Propositions in Discourse. Information Sciences Institute Research Report</booktitle>
<pages>83--115</pages>
<contexts>
<context position="65534" citStr="Mann and Thompson 1983" startWordPosition="10756" endWordPosition="10759">st, as Hobbs&apos; semantics seems to be.) 190 Zadrozny and Jensen Semantics of Paragraphs We shall discuss only the first two points, since the third one has already been explained. The chief difference between our approach and the other two lies in identifying the paragraph as a domain of coherence. Hobbs, van Dijk, and Kintch distinguish between &amp;quot;local&amp;quot; coherence—a property of subsequent sentences—and &amp;quot;global&amp;quot; coherence—a property of discourse as a whole. Hobbs explains coherence in terms of an inventory of &amp;quot;local,&amp;quot; possibly computable, coherence relations, like &amp;quot;elaboration,&amp;quot; &amp;quot;occasion,&amp;quot; etc. (Mann and Thompson 1983 give an even more detailed list of coherence relations than Hobbs.) Van Dijk and Kintch do this too, but they also describe &amp;quot;macrostructures&amp;quot; representing the global content of discourse, and they emphasize psychological and cognitive strategies used by people in establishing discourse coherence. Since we have linked coherence to models of paragraphs, we can talk simply about &amp;quot;coherence&amp;quot;— without adjectives—as a property of these models. To us the first &amp;quot;local&amp;quot; domain seems to be too small, and the second &amp;quot;global&amp;quot; one too large, for constructing meaningful computational models. To be sure, we</context>
</contexts>
<marker>Mann, Thompson, 1983</marker>
<rawString>Mann, W.C., and Thompson, S.A. (1983). Relational Propositions in Discourse. Information Sciences Institute Research Report 83-115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Moens</author>
<author>M Steedman</author>
</authors>
<title>Temporal Ontology in Natural Language.&amp;quot;</title>
<date>1987</date>
<booktitle>Proc. 25th Annual Meeting of the ACL.</booktitle>
<pages>1--7</pages>
<location>Stanford, California:</location>
<contexts>
<context position="71387" citStr="Moens and Steedman 1987" startWordPosition="11708" endWordPosition="11711">xt. Example 2 Pi: In 1347 a ship entered the port of Messina bringing with it the disease that came to be known as the Black Death. P2: It struck rapidly. P3: Within twenty-four hours of infection came an agonizing death. 5.1.1 Translation to Logic. The text concerns events happening in time. Naturally, we will use a logical notation in which formulas may have temporal and event components. We assume that any formal interpretation of time will agree with the intuitive one. So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject (e.g. Moens and Steedman 1987; Webber 1987) to see what a formal interpretation of events in time might look like. Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator. Extending and revising Jackendoff&apos;s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (&amp;quot;that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon&amp;quot;—ibid.). However</context>
</contexts>
<marker>Moens, Steedman, 1987</marker>
<rawString>Moens, M., and Steedman, M. (1987). &amp;quot;Temporal Ontology in Natural Language.&amp;quot; Proc. 25th Annual Meeting of the ACL. Stanford, California: 1-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Montague</author>
</authors>
<title>Universal Grammar.&amp;quot;</title>
<date>1970</date>
<journal>Theoria</journal>
<volume>36</volume>
<pages>373--398</pages>
<contexts>
<context position="25706" citStr="Montague (1970)" startWordPosition="4078" endWordPosition="4079">emantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But, obviously, there are other possibilities—for instance, the discourse representation structures (DRS&apos;s) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical 177 Computational Linguistics Volume 17, Number 2 notation of Montague (1970) is more sophisticated, and may be considered another possibility. Jackendoff&apos;s (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes &amp;quot;it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.&amp;quot; Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics. It will also be a model for our simplified logical notation (cf. Section 5). We can envision a system that uses data structures produced by a c</context>
</contexts>
<marker>Montague, 1970</marker>
<rawString>Montague, R. (1970). &amp;quot;Universal Grammar.&amp;quot; Theoria 36: 373-398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mycielski</author>
</authors>
<title>Analysis without Actual Infinity.&amp;quot;</title>
<date>1981</date>
<journal>Journal of Symbolic Logic,</journal>
<volume>46</volume>
<issue>3</issue>
<pages>625--633</pages>
<contexts>
<context position="27800" citStr="Mycielski 1981" startWordPosition="4414" endWordPosition="4415">, although usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by Johnson-Laird (1983), Jackendoff (1983), Kamp (1981), and implicitly or explicitly by almost all researchers in computational linguistics. As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. Mycielski 1981). On the other hand, a possible obstacle to our strategy of using only finite objects is the fact that the deductive closure of any set of formulas is not finite in standard logic, while, clearly, we will have to deduce new facts from formal representations of text and background knowledge. But there are several ways to avoid this obstruction. For example, consider theories consisting of universal formulas without function symbols. Let Th(T) of such a theory T be defined as T plus ground clauses/sets of literals provable from T in standard logic. It is easily seen that it is a closure, i.e. Th</context>
</contexts>
<marker>Mycielski, 1981</marker>
<rawString>Mycielski, J. (1981). &amp;quot;Analysis without Actual Infinity.&amp;quot; Journal of Symbolic Logic, 46(3): 625-633.</rawString>
</citation>
<citation valid="false">
<institution>Zadrozny and Jensen Semantics of Paragraphs</institution>
<marker></marker>
<rawString>Zadrozny and Jensen Semantics of Paragraphs</rawString>
</citation>
<citation valid="true">
<authors>
<author>P S Patel-Schneider</author>
</authors>
<title>A Decidable First Order Logic for Knowledge Representation.&amp;quot;</title>
<date>1985</date>
<booktitle>Proc. IJCAI-85. AAAI:</booktitle>
<pages>455--458</pages>
<contexts>
<context position="30547" citStr="Patel-Schneider 1985" startWordPosition="4890" endWordPosition="4891">ain domain, which satisfies all formulas of T. The collection of all (finite) models of a theory T will be denoted by Mods(T). • The set of all subformulas of a collection of formulas F is denoted by Form(F). 0 is a ground instance of a formula 0, if 0 contains no variables, and 0 = 00, for some substitution O. Thus, we do not require Th(T) to be closed under substitution instances of tautologies. Although in this paper we take modus ponens as the main rule of inference, in general one can consider deductive closures with respect to weaker, nonstandard logics, (cf. Levesque 1984; Frisch 1987; Patel-Schneider 1985). But we won&apos;t pursue this topic further here. 3.2 The Structure of Background Knowledge Background knowledge is not a simple list of meaning postulates—it has a structure and it may contain contradictions and ambiguities. These actualities have to be taken into account in any realistic model of natural language understanding. For instance, the verb &amp;quot;enter&amp;quot; is polysemous. But, unless context specifies otherwise, &amp;quot;to come in&amp;quot; is a more plausible meaning than &amp;quot;to join a group.&amp;quot; Assuming some logical representation of this knowledge, we can write that enter(x, y) –4 {come _in(x y); place(y)} (e1)</context>
</contexts>
<marker>Patel-Schneider, 1985</marker>
<rawString>Patel-Schneider, P.S. (1985). &amp;quot;A Decidable First Order Logic for Knowledge Representation.&amp;quot; Proc. IJCAI-85. AAAI: 455-458.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Perlis</author>
</authors>
<title>Languages with Self Reference I: Foundations.&amp;quot;</title>
<date>1985</date>
<journal>Artificial Intelligence</journal>
<volume>25</volume>
<pages>301--322</pages>
<contexts>
<context position="71591" citStr="Perlis (1985)" startWordPosition="11744" endWordPosition="11745"> death. 5.1.1 Translation to Logic. The text concerns events happening in time. Naturally, we will use a logical notation in which formulas may have temporal and event components. We assume that any formal interpretation of time will agree with the intuitive one. So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject (e.g. Moens and Steedman 1987; Webber 1987) to see what a formal interpretation of events in time might look like. Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator. Extending and revising Jackendoff&apos;s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (&amp;quot;that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon&amp;quot;—ibid.). However, as noted before, we will use a simplified version of such a logical notation; we will have only time, event, result, and property as primitives. After these remarks we can begin constructing the model o</context>
</contexts>
<marker>Perlis, 1985</marker>
<rawString>Perlis, D. (1985). &amp;quot;Languages with Self Reference I: Foundations.&amp;quot; Artificial Intelligence 25: 301-322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Poole</author>
</authors>
<title>A Logical Framework for Default Reasoning.&amp;quot;</title>
<date>1988</date>
<journal>Artificial Intelligence</journal>
<volume>36</volume>
<issue>1</issue>
<pages>27--47</pages>
<contexts>
<context position="83303" citStr="Poole (1988)" startWordPosition="13745" endWordPosition="13746">l direction of reference in English prose (Halliday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &amp;quot;unique-name assumption&amp;quot; (Genesereth and Nilsson 1987), &amp;quot;domain closure assumption&amp;quot; (ibid.), &amp;quot;domain circumscription&amp;quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &amp;quot;abductive inference&amp;quot; of Reggia (1985), the &amp;quot;diagnosis from first principles&amp;quot; of Reiter (1987), &amp;quot;explainability&amp;quot; of Poole (1988), and the subset principle of Berwick (1986). But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming). 5.2 p-Models The construction of a model of a paragraph, a p-model, must be based on the information contained in the paragraph itself (the object theory) and in the referential level, while the metalevel restricts ways that the model can be constructed, or, in other wor</context>
</contexts>
<marker>Poole, 1988</marker>
<rawString>Poole, D. (1988). &amp;quot;A Logical Framework for Default Reasoning.&amp;quot; Artificial Intelligence 36(1): 27-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quirk</author>
<author>S Greenbaum</author>
<author>G Leech</author>
<author>J Svartvik</author>
</authors>
<title>A Grammar of Contemporary English.</title>
<date>1972</date>
<publisher>Longman Group Ltd.</publisher>
<location>London.</location>
<contexts>
<context position="101028" citStr="Quirk et al. 1972" startWordPosition="16828" endWordPosition="16831">ous uses of &amp;quot;but.&amp;quot; Connectives are function words—like conjunctions and some adverbs—that are responsible simultaneously for maintaining cohesiveness within the text and for signaling the nature of the relationships that hold between and among various text units. &amp;quot;And,&amp;quot; &amp;quot;or,&amp;quot; and &amp;quot;but&amp;quot; are the three main coordinating connectives in English. However, &amp;quot;but&amp;quot; does not behave quite like the other two—semantically, &amp;quot;but&amp;quot; signals a contradiction, and in this role it seems to have three subfunctions: 1. Opposition (called &amp;quot;adversative&amp;quot; or &amp;quot;contrary-to-expectation&amp;quot; by Halliday and Hasan 1976; cf. also Quirk et al. 1972, p. 672). The ship arrived but the passengers could not get off. The yacht is cheap but elegant. 2. Comparison. In this function, the first conjunct is not so directly contradicted by the second. A contradiction exists, but we may have to 202 Zadrozny and Jensen Semantics of Paragraphs go through additional levels of implication to find it. Consider the sentence: That basketball player is short, but he&apos;s very quick. 3. Affirmation. This use of &amp;quot;but&amp;quot; always follows a negative clause, and actually augments the meaning of the preceding clause by adding supporting information: The disease not onl</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1972</marker>
<rawString>Quirk, R., Greenbaum, S., Leech, G., and Svartvik, J. (1972). A Grammar of Contemporary English. Longman Group Ltd. London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Reggia</author>
</authors>
<title>Abductive Inference.&amp;quot;</title>
<date>1985</date>
<booktitle>Expert Systems in Government Symposium. IEEE:</booktitle>
<pages>484--489</pages>
<editor>In Karma, K.N., ed.,</editor>
<contexts>
<context position="83213" citStr="Reggia (1985)" startWordPosition="13733" endWordPosition="13734">tegy is certainly the right one to start out with, since anaphora is always the more typical direction of reference in English prose (Halliday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &amp;quot;unique-name assumption&amp;quot; (Genesereth and Nilsson 1987), &amp;quot;domain closure assumption&amp;quot; (ibid.), &amp;quot;domain circumscription&amp;quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &amp;quot;abductive inference&amp;quot; of Reggia (1985), the &amp;quot;diagnosis from first principles&amp;quot; of Reiter (1987), &amp;quot;explainability&amp;quot; of Poole (1988), and the subset principle of Berwick (1986). But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming). 5.2 p-Models The construction of a model of a paragraph, a p-model, must be based on the information contained in the paragraph itself (the object theory) and in the referential lev</context>
</contexts>
<marker>Reggia, 1985</marker>
<rawString>Reggia, J.A. (1985). &amp;quot;Abductive Inference.&amp;quot; In Karma, K.N., ed., Expert Systems in Government Symposium. IEEE: 484-489.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Reiter</author>
</authors>
<title>A Theory of Diagnosis from First Principles.&amp;quot;</title>
<date>1987</date>
<journal>Artificial Intelligence,</journal>
<volume>32</volume>
<issue>1</issue>
<pages>57--95</pages>
<contexts>
<context position="83269" citStr="Reiter (1987)" startWordPosition="13741" endWordPosition="13742"> anaphora is always the more typical direction of reference in English prose (Halliday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &amp;quot;unique-name assumption&amp;quot; (Genesereth and Nilsson 1987), &amp;quot;domain closure assumption&amp;quot; (ibid.), &amp;quot;domain circumscription&amp;quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &amp;quot;abductive inference&amp;quot; of Reggia (1985), the &amp;quot;diagnosis from first principles&amp;quot; of Reiter (1987), &amp;quot;explainability&amp;quot; of Poole (1988), and the subset principle of Berwick (1986). But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming). 5.2 p-Models The construction of a model of a paragraph, a p-model, must be based on the information contained in the paragraph itself (the object theory) and in the referential level, while the metalevel restricts ways that the model ca</context>
</contexts>
<marker>Reiter, 1987</marker>
<rawString>Reiter, R. (1987). &amp;quot;A Theory of Diagnosis from First Principles.&amp;quot; Artificial Intelligence, 32(1): 57-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sidner</author>
</authors>
<title>Focusing in the Comprehension of Definite Anaphora.&amp;quot;</title>
<date>1983</date>
<booktitle>Computational Models of Discourse,</booktitle>
<pages>330--367</pages>
<editor>In Brady, M., and Berwick, R., eds.,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachusetts:</location>
<contexts>
<context position="82071" citStr="Sidner 1983" startWordPosition="13548" endWordPosition="13549">or abduction; it says &amp;quot;minimize the number of things that have the property P(*,*),&amp;quot; and it allows us to draw certain conclusions on the basis of partial information. We shall see it in action in Section 5.2. We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution. They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it). Other factors, such as the role of focus (Grosz 1977, 1978; Sidner 1983) or quantifier scoping (Webber 1983) must play a role, too. Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself. Note: In our translation from English to logic we are assuming that &amp;quot;it&amp;quot; is anaphoric (with the pronoun following the element that it refers to), not cataphoric (the other way around). This means that the &amp;quot;it&amp;quot; that brought the disease in P1 will not be considered to refer to the infection &amp;quot;i&amp;quot; or the death &amp;quot;d&amp;quot; in P3. This strategy is certainly the right one to start out with, since anaphora is al</context>
</contexts>
<marker>Sidner, 1983</marker>
<rawString>Sidner, C. (1983). &amp;quot;Focusing in the Comprehension of Definite Anaphora.&amp;quot; In Brady, M., and Berwick, R., eds., Computational Models of Discourse, MIT Press. Cambridge, Massachusetts: 330-367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Sells</author>
</authors>
<title>Lectures on Contemporary Syntactic Theories. CSLI lecture notes;</title>
<date>1985</date>
<pages>3</pages>
<contexts>
<context position="12507" citStr="Sells (1985" startWordPosition="1945" endWordPosition="1946">hat a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area. We suggest that the paragraph is a grammatical and logical unit. It is the smallest linguistic representation of what, in logic, is called a &amp;quot;model,&amp;quot; and it is the first reasonable domain of anaphora resolution, and of coherent thought about a central topic. A paragraph can be thought of as a grammatical unit in the following sense: it is the discourse unit in which a functional (or a predicate-argument) structure can be definitely assigned to sentences/strings. For instance, Sells (1985, p. 8) says that the sentence &amp;quot;Reagan thinks bananas,&amp;quot; which is otherwise strange, is in fact acceptable if it occurs as an answer to the question &amp;quot;What is Kissinger&apos;s favorite fruit?&amp;quot; The pairing of these two sentences may be said to create a small paragraph. Our point is that an acceptable structure can be assigned to the utterance &amp;quot;Reagan thinks bananas&amp;quot; only within the paragraph in which this utterance occurs. We believe that, in general, no unit larger than a paragraph is necessary to assign a functional structure to a sentence, and that no smaller discourse fragment, such as two (or one</context>
</contexts>
<marker>Sells, 1985</marker>
<rawString>Sells, P. (1985). Lectures on Contemporary Syntactic Theories. CSLI lecture notes; no. 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Shoenfield</author>
</authors>
<date>1987</date>
<booktitle>Mathematical Logic.</booktitle>
<publisher>Addison-Wesley.</publisher>
<location>New York, New York.</location>
<marker>Shoenfield, 1987</marker>
<rawString>Shoenfield, J.R. (1987). Mathematical Logic. Addison-Wesley. New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Sinclair</author>
<author>ed</author>
</authors>
<date>1987</date>
<booktitle>Looking Up. An account of the COBUILD project. Collins ELT,</booktitle>
<location>London.</location>
<marker>Sinclair, ed, 1987</marker>
<rawString>Sinclair, J.M., ed. (1987). Looking Up. An account of the COBUILD project. Collins ELT, London.</rawString>
</citation>
<citation valid="true">
<title>Lexical Ambiguity Resolution.</title>
<date>1988</date>
<editor>Small, S.L., Cottrell, G.W., and Tanenhaus, M.K., eds.</editor>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, California.</location>
<contexts>
<context position="46006" citStr="(1988)" startWordPosition="7528" endWordPosition="7528">t theories of T given by (II(F), &lt;), where F = Form(T): PT&lt;(T) =-- {T UT&apos; : T&apos; = GI and f is a maximal element of (II(F), &lt;)} Notice that PT&lt;(T) can contain more than one theory, meaning that T is ambiguous. This is a consequence of the fact that the cartesian product is only partially ordered by &lt;. The main reason for using ground instances V),(-6) in modifying the orderings is the need to deal with multiple occurrences of the same predicate, as in John went to the bank by the bank. 184 Zadrozny and Jensen Semantics of Paragraphs The above construction is also very close in spirit to Poole&apos;s (1988) method for default reasoning, where object theories are augmented by ground instances of defaults. 3.3.3 Coherence Links. The reasoning that led to the intended interpretation fint in our discussion of dominance was based on the partial ordering of the theories of R. We want to exploit now another property of the theories of R—their coherence. Finding an interpretation for a natural language text or sentence typically involves an appeal to coherence. Consider S2: Entering the port, a ship brought a disaster. Using the coherence link between (b2) and (dr1) (cf. Section 3.2)—the presence of cau</context>
<context position="76389" citStr="(1988)" startWordPosition="12553" endWordPosition="12553">the theory (d1)—a disease is an illness caused by an infection. This equality i = i&apos; cannot be proven, but it may be reasonably assumed—we know that in this case the infection i&apos; caused the illness, which, in turn, caused the death. The necessity of this kind of merging of arguments has been recognized before: Charniak and McDermott (1985) call it abductive unification/matching, Hobbs (1978, 1979) refers to such operations using the terms knitting or petty conversational implicature. Neither Hobbs nor Charniak and McDermott tried then to make this notion precise, but the paper by Hobbs et al. (1988) moves in that direction. The purpose of this subsection is to formalize and explain how assumptions like that one above can be made. Definition A formula 0 is weakly provable from an object theory T, expressed as T HR 0, iff there exists a partial theory T E PT(T) such that T 0, i.e. T proves 0 in logic. (We call HR &amp;quot;weak&amp;quot; because it is enough to find one partial theory proving a given formula.) As an example, in the case of the three-sentence paragraph, we have a partial theory Ti based on (sib) saying that &amp;quot; &apos;it&apos; hits rapidly,&amp;quot; and T2 saying that &amp;quot;an illness (&apos;it&apos;) harms rapidly&amp;quot; (s2_ex). T</context>
<context position="83303" citStr="(1988)" startWordPosition="13746" endWordPosition="13746">ction of reference in English prose (Halliday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &amp;quot;unique-name assumption&amp;quot; (Genesereth and Nilsson 1987), &amp;quot;domain closure assumption&amp;quot; (ibid.), &amp;quot;domain circumscription&amp;quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &amp;quot;abductive inference&amp;quot; of Reggia (1985), the &amp;quot;diagnosis from first principles&amp;quot; of Reiter (1987), &amp;quot;explainability&amp;quot; of Poole (1988), and the subset principle of Berwick (1986). But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming). 5.2 p-Models The construction of a model of a paragraph, a p-model, must be based on the information contained in the paragraph itself (the object theory) and in the referential level, while the metalevel restricts ways that the model can be constructed, or, in other wor</context>
</contexts>
<marker>1988</marker>
<rawString>Small, S.L., Cottrell, G.W., and Tanenhaus, M.K., eds. (1988). Lexical Ambiguity Resolution. Morgan Kaufmann. San Mateo, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Turner</author>
</authors>
<title>Death is the Mother of Beauty.</title>
<date>1987</date>
<publisher>The University of Chicago Press.</publisher>
<location>Chicago, Illinois.</location>
<contexts>
<context position="23895" citStr="Turner (1987" startWordPosition="3776" endWordPosition="3777">Binot 1988; Zadrozny 1987a, 1987b) that natural language programs, such as on-line grammars and dictionaries, can be used as referential levels for commonsense reasoning—for example, to disambiguate PP attachment. This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory. The referential structures we are going to use are collections of logical theories, but the concept of reference is more general. Some of the intuitions we associate with this notion have been very well expressed by Turner (1987, pp. 7-8): ... Semantics is constrained by our models of ourselves and our worlds. We have models of up and down that are based by the way our bodies actually function. Once the word &amp;quot;up&amp;quot; is given its meaning relative to our experience with gravity, it is not free to &amp;quot;slip&amp;quot; into its opposite. &amp;quot;Up&amp;quot; means up and not down. ... We have a model that men and women couple to produce offspring who are similar to their parents, and this model is grounded in genetics, and the semantics of kinship metaphor is grounded in this model. Mothers have a different role than fathers in this model, and thus ther</context>
</contexts>
<marker>Turner, 1987</marker>
<rawString>Turner, M. (1987). Death is the Mother of Beauty. The University of Chicago Press. Chicago, Illinois.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T A van Dijk</author>
<author>W Kintch</author>
</authors>
<date>1983</date>
<booktitle>Strategies of Discourse Comprehension.</booktitle>
<publisher>Academic Press.</publisher>
<location>Orlando, Florida.</location>
<marker>van Dijk, Kintch, 1983</marker>
<rawString>van Dijk, T.A., and Kintch, W. (1983). Strategies of Discourse Comprehension. Academic Press. Orlando, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Warriner</author>
</authors>
<title>English Grammar and Composition.</title>
<date>1963</date>
<publisher>Harcourt, Brace &amp; World, Inc.,</publisher>
<location>New York, New York.</location>
<contexts>
<context position="9068" citStr="Warriner (1963)" startWordPosition="1424" endWordPosition="1425"> ambiguity and construct new models. We also show metalevel rules for interpreting &amp;quot;but.&amp;quot; 2. The Paragraph as a Discourse Unit 2.1 Approaches to Paragraph Analysis Recent syntactic theory—that is, in the last 30 years—has been preoccupied with sentence-level analysis. Within discourse theory, however, some significant work has been done on the analysis of written paragraphs. We can identify four different linguistic approaches to paragraphs: prescriptivist, psycholinguist, textualist, and discourseoriented. The prescriptivist approach is typified in standard English grammar textbooks, such as Warriner (1963). In these sources, a paragraph is notionally defined as something like a series of sentences that develop one single topic, and rules are laid down for the construction of an ideal (or at least an acceptable) paragraph. Although these dictates are fairly clear, the underlying notion of topic is not. An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983). These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found three major formal devices that are used, by </context>
</contexts>
<marker>Warriner, 1963</marker>
<rawString>Warriner, J.E. (1963). English Grammar and Composition. Harcourt, Brace &amp; World, Inc., New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Webber</author>
</authors>
<title>So What Can We Talk About Now?&amp;quot;</title>
<date>1983</date>
<booktitle>Computational Models of Discourse.</booktitle>
<pages>147--154</pages>
<editor>In Brady, M., and Berwick, R., eds.,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachusetts:</location>
<contexts>
<context position="82107" citStr="Webber 1983" startWordPosition="13553" endWordPosition="13554">number of things that have the property P(*,*),&amp;quot; and it allows us to draw certain conclusions on the basis of partial information. We shall see it in action in Section 5.2. We have no doubts that various other metarules will be necessary; clearly, our two metarules cannot constitute the whole theory of anaphora resolution. They are intended as an illustration of the power of abduction, which in this framework helps determine the universe of the model (that is the set of entities that appear in it). Other factors, such as the role of focus (Grosz 1977, 1978; Sidner 1983) or quantifier scoping (Webber 1983) must play a role, too. Determining the relative importance of those factors, the above metarules, and syntactic clues, appears to be an interesting topic in itself. Note: In our translation from English to logic we are assuming that &amp;quot;it&amp;quot; is anaphoric (with the pronoun following the element that it refers to), not cataphoric (the other way around). This means that the &amp;quot;it&amp;quot; that brought the disease in P1 will not be considered to refer to the infection &amp;quot;i&amp;quot; or the death &amp;quot;d&amp;quot; in P3. This strategy is certainly the right one to start out with, since anaphora is always the more typical direction of r</context>
</contexts>
<marker>Webber, 1983</marker>
<rawString>Webber, B. (1983). &amp;quot;So What Can We Talk About Now?&amp;quot; In Brady, M., and Berwick, R., eds., Computational Models of Discourse. MIT Press. Cambridge, Massachusetts: 147-154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Webber</author>
</authors>
<title>The Interpretation of Tense in Discourse.&amp;quot;</title>
<date>1987</date>
<booktitle>Proc, 25th Annual Meeting of the ACL, ACL:</booktitle>
<pages>147--154</pages>
<contexts>
<context position="71401" citStr="Webber 1987" startWordPosition="11712" endWordPosition="11713"> a ship entered the port of Messina bringing with it the disease that came to be known as the Black Death. P2: It struck rapidly. P3: Within twenty-four hours of infection came an agonizing death. 5.1.1 Translation to Logic. The text concerns events happening in time. Naturally, we will use a logical notation in which formulas may have temporal and event components. We assume that any formal interpretation of time will agree with the intuitive one. So it is not necessary now to present a formal semantics here. The reader may consult recent papers on this subject (e.g. Moens and Steedman 1987; Webber 1987) to see what a formal interpretation of events in time might look like. Since sentences can refer to events described by other sentences, we may need also a quotation operator; Perlis (1985) describes how first order logic can be augmented with such an operator. Extending and revising Jackendoff&apos;s (1983) formalism seems to us a correct method to achieve the correspondence between syntax and semantics expressed in the grammatical constraint (&amp;quot;that one should prefer a semantic theory that explains otherwise arbitrary generalizations about the syntax and the lexicon&amp;quot;—ibid.). However, as noted bef</context>
</contexts>
<marker>Webber, 1987</marker>
<rawString>Webber, B. (1987). &amp;quot;The Interpretation of Tense in Discourse.&amp;quot; Proc, 25th Annual Meeting of the ACL, ACL: 147-154.</rawString>
</citation>
<citation valid="true">
<title>Webster&apos;s Seventh New Collegiate Dictionary.</title>
<date>1963</date>
<publisher>Merriam-Webster, Inc. Springfield,</publisher>
<location>Massachusetts.</location>
<contexts>
<context position="9068" citStr="(1963)" startWordPosition="1425" endWordPosition="1425">y and construct new models. We also show metalevel rules for interpreting &amp;quot;but.&amp;quot; 2. The Paragraph as a Discourse Unit 2.1 Approaches to Paragraph Analysis Recent syntactic theory—that is, in the last 30 years—has been preoccupied with sentence-level analysis. Within discourse theory, however, some significant work has been done on the analysis of written paragraphs. We can identify four different linguistic approaches to paragraphs: prescriptivist, psycholinguist, textualist, and discourseoriented. The prescriptivist approach is typified in standard English grammar textbooks, such as Warriner (1963). In these sources, a paragraph is notionally defined as something like a series of sentences that develop one single topic, and rules are laid down for the construction of an ideal (or at least an acceptable) paragraph. Although these dictates are fairly clear, the underlying notion of topic is not. An example of psycholinguistically oriented research work can be found in Bond and Hayes (1983). These authors take the position that a paragraph is a psychologically real unit of discourse, and, in fact, a formal grammatical unit. Bond and Hayes found three major formal devices that are used, by </context>
</contexts>
<marker>1963</marker>
<rawString>Webster&apos;s Seventh New Collegiate Dictionary. (1963). Merriam-Webster, Inc. Springfield, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Woods</author>
</authors>
<title>Don&apos;t Blame the Tool.&amp;quot;</title>
<date>1987</date>
<journal>Computational Intelligence</journal>
<volume>3</volume>
<issue>3</issue>
<pages>228--237</pages>
<contexts>
<context position="24928" citStr="Woods 1987" startWordPosition="3953" endWordPosition="3954">, and this model is grounded in genetics, and the semantics of kinship metaphor is grounded in this model. Mothers have a different role than fathers in this model, and thus there is a reason why &amp;quot;Death is the father of beauty&amp;quot; fails poetically while &amp;quot;Death is the mother of beauty&amp;quot; succeeds.... It is precisely this &amp;quot;grounding&amp;quot; of logical predicates in other conceptual structures that we would like to capture. We investigate here only the &amp;quot;grounding&amp;quot; in logical theories. However, it is possible to think about constraining linguistic or logical predicates by simulating physical experiences (cf. Woods 1987). We assume here that a translation of the surface forms of sentences into a logical formalism is possible. Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But, obviously, there are other possibilities—for instance, the discourse representation structures (DRS&apos;s) of Kamp (1981), which have been used to translate a subset of English into logical formulas, to model text (identified</context>
</contexts>
<marker>Woods, 1987</marker>
<rawString>Woods, W. (1987). &amp;quot;Don&apos;t Blame the Tool.&amp;quot; Computational Intelligence 3(3): 228-237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Zadrozny</author>
</authors>
<title>Intended Models, Circumscription and Commonsense Reasoning.&amp;quot;</title>
<date>1987</date>
<booktitle>Proc. IJCAI-87:</booktitle>
<pages>909--916</pages>
<contexts>
<context position="21526" citStr="Zadrozny 1987" startWordPosition="3403" endWordPosition="3404"> in order to produce an interpretation of a text. The formalist will be presented in a number of steps in which we will elaborate one simple example: Example 1 Entering the port, a ship brought a disease. This sentence can be translated into the logical formula (ignoring only the past tense of &amp;quot;bring&amp;quot; and the progressive of &amp;quot;enter&amp;quot;): Definition S: enter(xl , x2) &amp; ship(xl) &amp; port(x2) &amp; bring(x3, x4) &amp; disease(x4) &amp; xl = s &amp; x2 == m &amp; x3 =s &amp; x4 = d, where s, m, d, are constants. We adopt the three-level semantics as a formal tool for the analysis of paragraphs. This semantics was constructed (Zadrozny 1987a, 1987b) as a formal framework for default and commonsense reasoning. It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates. For instance, relating &amp;quot;they&amp;quot; to &amp;quot;apples&amp;quot; in the sentence (cf. Haugeland 1985 p. 195; Zadrozny 1987a): We bought the boys apples because they were so cheap 176 Zadrozny and Jensen Semantics of Paragraphs can be an example of such a most plausible choice. The main ideas o</context>
<context position="23308" citStr="Zadrozny 1987" startWordPosition="3688" endWordPosition="3689">of text has to be extracted. It constrains interpretations of the predicates of an object theory. Its structure and the extraction methods will be discussed below. 4. Understanding has as its goal construction of an interpretation of the text, i.e. building some kind of model. Since not all logically permissible models are linguistically appropriate, one needs a place, namely the metalevel, to put constraints on types of models. Gricean maxims belong there; Section 6 will be devoted to a presentation of the metalevel rules corresponding to them. We have shown elsewhere (Jensen and Binot 1988; Zadrozny 1987a, 1987b) that natural language programs, such as on-line grammars and dictionaries, can be used as referential levels for commonsense reasoning—for example, to disambiguate PP attachment. This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory. The referential structures we are going to use are collections of logical theories, but the concept of reference is more general. Some of the intuitions we associate with this notion have been very well expressed by Turner (1987, pp. 7-8): .</context>
<context position="38628" citStr="Zadrozny 1987" startWordPosition="6238" endWordPosition="6239">theories of &amp;quot;enter,&amp;quot; &amp;quot;ship,&amp;quot; etc. and the partial orders are represented graphically; more plausible theories are positioned higher. A path through this graph chooses an interpretation of the sentence S. For instance, the path fint = {el, shl, pl, b1, d1} and S say together that A large boat (ship) that carries people or goods came into the harbor and carried a disease (illness). Since it is the &amp;quot;highest&amp;quot; path, flit is the most plausible (relative to R) interpretation of the words that appear in the sentence. Because it is also consistent, it will be chosen as a best interpretation of S, (cf. Zadrozny 1987a, 1987b). Another theory, consisting of f&apos; = {el , sh2, pl , b2, dl } and S, saying that A space vehicle came into the harbor and caused a disease/illness is less plausible according to that ordering. As it turns out, f&apos; is never constructed in the process of building an interpretation of a paragraph containing the sentence S, unless assuming fmt would lead to a contradiction, for instance within the higher level context of a science fiction story. The collection of these most plausible consistent interpretations of a given theory T is denoted by PT &lt; (T). Thenf int belongs to PT &lt;(Th({S})), </context>
<context position="55215" citStr="Zadrozny 1987" startWordPosition="9029" endWordPosition="9030">edicates. Now we want to restrict the notion of a partial theory by introducing the formal notions of topic and coherence. We can then later (Section 5.2) define p-models—a category of models corresponding to paragraphs—as models of coherent theories that satisfy all metalevel conditions. The partial theories pick up from the referential level the most obvious or the most important information about a formula. This immediate information may be insufficient to decide the truth of certain predicates. It would seem therefore that the iteration of the PT operation to form a closure is needed (cf. Zadrozny 1987b). 187 Computational Linguistics Volume 17, Number 2 However, there are at least three arguments against iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time. Secondly, the cooperative principle of Grice (1975, 1978), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to im</context>
<context position="77897" citStr="Zadrozny 1987" startWordPosition="12822" endWordPosition="12823">rred model of a theory T is an element of Mods(T) that satisfies metalevel constraints contained in M. The set of all preferred models of T is denoted by PM(T). A formula q5 of L(=), the language with equality, is weakly R+ M-abductible from an object theory T, denoted by T I-R+m 0, iff there exists a partial theory T e PT(T) and a preferred model M E PM(T) such that M =0, i.e. 0 is true in at least one preferred model of the partial theory T. Note: The notions of strong provability and strong R + M-abduction can be introduced by replacing &amp;quot;there exists&amp;quot; by &amp;quot;all&amp;quot; in the above definitions (cf. Zadrozny 1987b). We will have, however, no need for &amp;quot;strong&amp;quot; notions in this paper. Also, in a practical system, &amp;quot;satisfies&amp;quot; should be probably replaced by &amp;quot;violates fewest.&amp;quot; Obviously, it is better to have references of pronouns resolved than not. After all, we assume that texts make sense, and that authors know these references. That applies to references of noun phrases too. On the other hand, there must be some restrictions on possible references; we would rather assume that &amp;quot;spinach&amp;quot; $ &amp;quot;train&amp;quot; (i.e. V x,y)(spinach(x) &amp; train(y) --* x y)), or &amp;quot;ship&amp;quot; &amp;quot;disease.&amp;quot; Two elementary conditions limiting the num</context>
</contexts>
<marker>Zadrozny, 1987</marker>
<rawString>Zadrozny, W. (1987a). &amp;quot;Intended Models, Circumscription and Commonsense Reasoning.&amp;quot; Proc. IJCAI-87: 909-916.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Zadrozny</author>
</authors>
<title>A Theory of Default Reasoning.&amp;quot;</title>
<date>1987</date>
<booktitle>Proc. AAAI-87.</booktitle>
<pages>385--390</pages>
<location>Seattle, Washington:</location>
<contexts>
<context position="21526" citStr="Zadrozny 1987" startWordPosition="3403" endWordPosition="3404"> in order to produce an interpretation of a text. The formalist will be presented in a number of steps in which we will elaborate one simple example: Example 1 Entering the port, a ship brought a disease. This sentence can be translated into the logical formula (ignoring only the past tense of &amp;quot;bring&amp;quot; and the progressive of &amp;quot;enter&amp;quot;): Definition S: enter(xl , x2) &amp; ship(xl) &amp; port(x2) &amp; bring(x3, x4) &amp; disease(x4) &amp; xl = s &amp; x2 == m &amp; x3 =s &amp; x4 = d, where s, m, d, are constants. We adopt the three-level semantics as a formal tool for the analysis of paragraphs. This semantics was constructed (Zadrozny 1987a, 1987b) as a formal framework for default and commonsense reasoning. It should not come as a surprise that we can now use this apparatus for text/discourse analysis; after all, many natural language inferences are based on defaults, and quite often they can be reduced to choosing most plausible interpretations of predicates. For instance, relating &amp;quot;they&amp;quot; to &amp;quot;apples&amp;quot; in the sentence (cf. Haugeland 1985 p. 195; Zadrozny 1987a): We bought the boys apples because they were so cheap 176 Zadrozny and Jensen Semantics of Paragraphs can be an example of such a most plausible choice. The main ideas o</context>
<context position="23308" citStr="Zadrozny 1987" startWordPosition="3688" endWordPosition="3689">of text has to be extracted. It constrains interpretations of the predicates of an object theory. Its structure and the extraction methods will be discussed below. 4. Understanding has as its goal construction of an interpretation of the text, i.e. building some kind of model. Since not all logically permissible models are linguistically appropriate, one needs a place, namely the metalevel, to put constraints on types of models. Gricean maxims belong there; Section 6 will be devoted to a presentation of the metalevel rules corresponding to them. We have shown elsewhere (Jensen and Binot 1988; Zadrozny 1987a, 1987b) that natural language programs, such as on-line grammars and dictionaries, can be used as referential levels for commonsense reasoning—for example, to disambiguate PP attachment. This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory. The referential structures we are going to use are collections of logical theories, but the concept of reference is more general. Some of the intuitions we associate with this notion have been very well expressed by Turner (1987, pp. 7-8): .</context>
<context position="38628" citStr="Zadrozny 1987" startWordPosition="6238" endWordPosition="6239">theories of &amp;quot;enter,&amp;quot; &amp;quot;ship,&amp;quot; etc. and the partial orders are represented graphically; more plausible theories are positioned higher. A path through this graph chooses an interpretation of the sentence S. For instance, the path fint = {el, shl, pl, b1, d1} and S say together that A large boat (ship) that carries people or goods came into the harbor and carried a disease (illness). Since it is the &amp;quot;highest&amp;quot; path, flit is the most plausible (relative to R) interpretation of the words that appear in the sentence. Because it is also consistent, it will be chosen as a best interpretation of S, (cf. Zadrozny 1987a, 1987b). Another theory, consisting of f&apos; = {el , sh2, pl , b2, dl } and S, saying that A space vehicle came into the harbor and caused a disease/illness is less plausible according to that ordering. As it turns out, f&apos; is never constructed in the process of building an interpretation of a paragraph containing the sentence S, unless assuming fmt would lead to a contradiction, for instance within the higher level context of a science fiction story. The collection of these most plausible consistent interpretations of a given theory T is denoted by PT &lt; (T). Thenf int belongs to PT &lt;(Th({S})), </context>
<context position="55215" citStr="Zadrozny 1987" startWordPosition="9029" endWordPosition="9030">edicates. Now we want to restrict the notion of a partial theory by introducing the formal notions of topic and coherence. We can then later (Section 5.2) define p-models—a category of models corresponding to paragraphs—as models of coherent theories that satisfy all metalevel conditions. The partial theories pick up from the referential level the most obvious or the most important information about a formula. This immediate information may be insufficient to decide the truth of certain predicates. It would seem therefore that the iteration of the PT operation to form a closure is needed (cf. Zadrozny 1987b). 187 Computational Linguistics Volume 17, Number 2 However, there are at least three arguments against iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time. Secondly, the cooperative principle of Grice (1975, 1978), under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to im</context>
<context position="77897" citStr="Zadrozny 1987" startWordPosition="12822" endWordPosition="12823">rred model of a theory T is an element of Mods(T) that satisfies metalevel constraints contained in M. The set of all preferred models of T is denoted by PM(T). A formula q5 of L(=), the language with equality, is weakly R+ M-abductible from an object theory T, denoted by T I-R+m 0, iff there exists a partial theory T e PT(T) and a preferred model M E PM(T) such that M =0, i.e. 0 is true in at least one preferred model of the partial theory T. Note: The notions of strong provability and strong R + M-abduction can be introduced by replacing &amp;quot;there exists&amp;quot; by &amp;quot;all&amp;quot; in the above definitions (cf. Zadrozny 1987b). We will have, however, no need for &amp;quot;strong&amp;quot; notions in this paper. Also, in a practical system, &amp;quot;satisfies&amp;quot; should be probably replaced by &amp;quot;violates fewest.&amp;quot; Obviously, it is better to have references of pronouns resolved than not. After all, we assume that texts make sense, and that authors know these references. That applies to references of noun phrases too. On the other hand, there must be some restrictions on possible references; we would rather assume that &amp;quot;spinach&amp;quot; $ &amp;quot;train&amp;quot; (i.e. V x,y)(spinach(x) &amp; train(y) --* x y)), or &amp;quot;ship&amp;quot; &amp;quot;disease.&amp;quot; Two elementary conditions limiting the num</context>
</contexts>
<marker>Zadrozny, 1987</marker>
<rawString>Zadrozny, W. (1987b). &amp;quot;A Theory of Default Reasoning.&amp;quot; Proc. AAAI-87. Seattle, Washington: 385-390.</rawString>
</citation>
<citation valid="false">
<authors>
<author>W Zadrozny</author>
</authors>
<title>The Logic of Abduction.</title>
<marker>Zadrozny, </marker>
<rawString>Zadrozny, W. (unpublished). The Logic of Abduction.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>