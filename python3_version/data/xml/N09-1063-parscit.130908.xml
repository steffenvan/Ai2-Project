<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000040">
<title confidence="0.998117">
Hierarchical Search for Parsing
</title>
<author confidence="0.998844">
Adam Pauls Dan Klein
</author>
<affiliation confidence="0.844874">
Computer Science Division
University of California at Berkeley
Berkeley, CA 94720, USA
</affiliation>
<email confidence="0.999123">
{adpauls,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.995645" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999799533333333">
Both coarse-to-fine and A* parsing use simple
grammars to guide search in complex ones.
We compare the two approaches in a com-
mon, agenda-based framework, demonstrat-
ing the tradeoffs and relative strengths of each
method. Overall, coarse-to-fine is much faster
for moderate levels of search errors, but be-
low a certain threshold A* is superior. In addi-
tion, we present the first experiments on hier-
archical A* parsing, in which computation of
heuristics is itself guided by meta-heuristics.
Multi-level hierarchies are helpful in both ap-
proaches, but are more effective in the coarse-
to-fine case because of accumulated slack in
A* heuristics.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961306122449">
The grammars used by modern parsers are ex-
tremely large, rendering exhaustive parsing imprac-
tical. For example, the lexicalized grammars of
Collins (1997) and Charniak (1997) and the state-
split grammars of Petrov et al. (2006) are all
too large to construct unpruned charts in memory.
One effective approach is coarse-to-fine pruning, in
which a small, coarse grammar is used to prune
edges in a large, refined grammar (Charniak et al.,
2006). Indeed, coarse-to-fine is even more effective
when a hierarchy of successive approximations is
used (Charniak et al., 2006; Petrov and Klein, 2007).
In particular, Petrov and Klein (2007) generate a se-
quence of approximations to a highly subcategorized
grammar, parsing with each in turn.
Despite its practical success, coarse-to-fine prun-
ing is approximate, with no theoretical guarantees
on optimality. Another line of work has explored
A* search methods, in which simpler problems are
used not for pruning, but for prioritizing work in
the full search space (Klein and Manning, 2003a;
Haghighi et al., 2007). In particular, Klein and Man-
ning (2003a) investigated A* for lexicalized parsing
in a factored model. In that case, A* vastly im-
proved the search in the lexicalized grammar, with
provable optimality. However, their bottleneck was
clearly shown to be the exhaustive parsing used to
compute the A* heuristic itself. It is not obvious,
however, how A* can be stacked in a hierarchical or
multi-pass way to speed up the computation of such
complex heuristics.
In this paper, we address three open questions
regarding efficient hierarchical search. First, can
a hierarchy of A* bounds be used, analogously to
hierarchical coarse-to-fine pruning? We show that
recent work in hierarchical A* (Felzenszwalb and
McAllester, 2007) can naturally be applied to both
the hierarchically refined grammars of Petrov and
Klein (2007) as well as the lexicalized grammars
of Klein and Manning (2003a). Second, what are
the tradeoffs between coarse-to-fine pruning and A*
methods? We show that coarse-to-fine is generally
much faster, but at the cost of search errors.1 Below
a certain search error rate, A* is faster and, of course,
optimal. Finally, when and how, qualitatively, do
these methods fail? A* search’s work grows quickly
as the slack increases between the heuristic bounds
and the true costs. On the other hand, coarse-to-fine
prunes unreliably when the approximating grammar
</bodyText>
<footnote confidence="0.9743625">
1In this paper, we consider only errors made by the search
procedure, not modeling errors.
</footnote>
<page confidence="0.881824">
557
</page>
<note confidence="0.87698975">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 557–565,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
Name Rule Priority
IN r:wr I(Bt, i, k) : βB I(Ct, k, j) : βC =�- I(At, i, j) : βA = βB + βC + wr βA + h(A, i, j)
</note>
<tableCaption confidence="0.851500333333333">
Table 1: Deduction rule for A* parsing. The items on the left of the =&gt; indicate what edges must be present on the
chart and what rule can be used to combine them, and the item on the right is the edge that may be added to the agenda.
The weight of each edge appears after the colon. The rule r is A → B C.
</tableCaption>
<bodyText confidence="0.9689315">
is very different from the target grammar. We em-
pirically demonstrate both failure modes.
</bodyText>
<sectionHeader confidence="0.939278" genericHeader="method">
2 Parsing algorithms
</sectionHeader>
<bodyText confidence="0.999905217391305">
Our primary goal in this paper is to compare hi-
erarchical A* (HA*) and hierarchical coarse-to-fine
(CTF) pruning methods. Unfortunately, these two
algorithms are generally deployed in different archi-
tectures: CTF is most naturally implemented using
a dynamic program like CKY, while best-first al-
gorithms like A* are necessarily implemented with
agenda-based parsers. To facilitate comparison, we
would like to implement them in a common architec-
ture. We therefore work entirely in an agenda-based
setting, noting that the crucial property of CTF is
not the CKY order of exploration, but the pruning
of unlikely edges, which can be equally well done
in an agenda-based parser. In fact, it is possible to
closely mimic dynamic programs like CKY using a
best-first algorithm with a particular choice of prior-
ities; we discuss this in Section 2.3.
While a general HA* framework is presented in
Felzenszwalb and McAllester (2007), we present
here a specialization to the parsing problem. We first
review the standard agenda-driven search frame-
work and basic A* parsing before generalizing to
HA*.
</bodyText>
<subsectionHeader confidence="0.989605">
2.1 Agenda-Driven Parsing
</subsectionHeader>
<bodyText confidence="0.999978083333333">
A non-hierarchical, best-first parser takes as input a
PCFG !9 (with root symbol R), a priority function
p(·) and a sentence consisting of terminals (words)
T0 ... T&amp;quot;. The parser’s task is to find the best
scoring (Viterbi) tree structure which is rooted at R
and spans the input sentence. Without loss of gen-
erality, we consider grammars in Chomsky normal
form, so that each non-terminal rule in the grammar
has the form r = A → B C with weight wr. We
assume that weights are non-negative (e.g. negative
log probabilities) and that we wish to minimize the
sum of the rule weights.
</bodyText>
<figureCaption confidence="0.847824857142857">
Figure 1: Deduction rule for A* depicted graphically.
Items to the left of the arrow indicate edges and rules that
can be combined to produce the edge to the right of the ar-
row. Edges are depicted as complete triangles. The value
inside an edge represents the weight of that edge. Each
new edge is assigned the priority written above the arrow
when added to the agenda.
</figureCaption>
<bodyText confidence="0.99993996">
The objects in an agenda-based parser are edges
e = I(X, i, j), also called items, which represent
parses spanning i to j and rooted at symbol X. We
denote edges as triangles, as in Figure 1. At all
times, edges have scores Qe, which are estimates
of their Viterbi inside probabilities (also called path
costs). These estimates improve over time as new
derivations are considered, and may or may not be
correct at termination, depending on the properties
of p. The parser maintains an agenda (a priority
queue of edges), as well as a chart (or closed list
in search terminology) of edges already processed.
The fundamental operation of the algorithm is to pop
the best (lowest) priority edge e from the agenda,
put it into the chart, and enqueue any edges which
can be built by combining e with other edges in the
chart. The combination of two adjacent edges into
a larger edge is shown graphically in Figure 1 and
as a weighted deduction rule in Table 1 (Shieber et
al., 1995; Nederhof, 2003). When an edge a is built
from adjacent edges b and c and a rule r, its cur-
rent score Qa is compared to Qb + Q, + wr and up-
dated if necessary. To allow reconstruction of best
parses, backpointers are maintained in the standard
way. The agenda is initialized with I(Ti, i, i + 1)
</bodyText>
<figure confidence="0.9987525">
A
wr
B
C
B C
p=!A+h(A,i,j)
A
!C
i k k j i j
!A=
!B+!C+wr
!B
</figure>
<page confidence="0.992328">
558
</page>
<bodyText confidence="0.998631695652174">
for i = 0 ... n − 1. The algorithm terminates when
I(R, 0, n) is popped off the queue.
Priorities are in general different than weights.
Whenever an edge e’s score changes, its priority
p(e), which may or may not depend on its score,
may improve. Edges are promoted accordingly in
the agenda if their priorities improve. In the sim-
plest case, the priorities are simply the Qe estimates,
which gives a correct uniform cost search wherein
the root edge is guaranteed to have its correct inside
score estimate at termination (Caraballo and Char-
niak, 1996).
A∗ parsing (Klein and Manning, 2003b) is a spe-
cial case of such an agenda-driven parser in which
the priority function p takes the form p(e) = Qe +
h(e), where e = I(X, i, j) and h(·) is some approx-
imation of e’s Viterbi outside cost (its completion
cost). If h is consistent, then the A∗ algorithm guar-
antees that whenever an edge comes off the agenda,
its weight is its true Viterbi inside cost. In particular,
this guarantee implies that the first edge represent-
ing the root I(R, 0, n) will be scored with the true
Viterbi score for the sentence.
</bodyText>
<subsectionHeader confidence="0.996511">
2.2 Hierarchical A∗
</subsectionHeader>
<bodyText confidence="0.999940304347826">
In the standard A∗ case the heuristics are assumed
to come from a black box. For example, Klein and
Manning (2003b) precomputes most heuristics of-
fline, while Klein and Manning (2003a) solves sim-
pler parsing problems for each sentence. In such
cases, the time spent to compute heuristics is often
non-trivial. Indeed, it is typical that effective heuris-
tics are themselves expensive search problems. We
would therefore like to apply A∗ methods to the
computation of the heuristics themselves. Hierar-
chical A∗ allows us to do exactly that.
Formally, HA∗ takes as input a sentence and a se-
quence (or hierarchy) of m + 1 PCFGs G0 ... Gm,
where Gm is the target grammar and G0 ... Gm−1
are auxiliary grammars. Each grammar Gt has an in-
ventory of symbols Et, hereafter denoted with capi-
tal letters. In particular, each grammar has a distin-
guished terminal symbol TZt for each word TZ in the
input and a root symbol Rt.
The grammars G0 ... Gm must form a hierarchy in
which Gt is a relaxed projection of Gt+1. A grammar
Gt−1 is a projection of Gt if there exists some onto
function 7rt : Et H Et−1 defined for all symbols in
</bodyText>
<figure confidence="0.9908668">
Agenda Chart
go I O
91 I O
92 O
I
</figure>
<figureCaption confidence="0.682744166666667">
Figure 3: Operation of hierarchical A* parsing. An edge
comes off the agenda and is added to the chart (solid line).
From this edge, multiple new edges can be constructed
and added to the agenda (dashed lines). The chart is com-
posed of two subcharts for each grammar in the hierar-
chy: an inside chart (I) and an outside chart (O).
</figureCaption>
<bodyText confidence="0.999456285714286">
Gt; hereafter, we will use A$t to represent 7rt(At). A
projection is a relaxation if, for every rule r = At �
Bt Ct with weight wr the projection r$ = 7rt(r) =
A$t —* B$tC$ t has weight wry &lt; wr in Gt−1. Given
a target grammar Gm and a projection function 7rm,
it is easy to construct a relaxed projection Gm−1 by
minimizing over rules collapsed by 7rm:
</bodyText>
<equation confidence="0.7299905">
wry = min
r∈ggm:Irm(r)=r&apos;
</equation>
<bodyText confidence="0.90792148">
Given a series of projection functions 7r1 ... 7rm,
we can construct relaxed projections by projecting
Gm to Gm−1, then Gm−1 to Gm−2 and so on. Note
that by construction, parses in a relaxed projection
give lower bounds on parses in the target grammar
(Klein and Manning, 2003b).
HA∗ differs from standard A∗ in two ways.
First, it tracks not only standard inside edges
e = I(X, i, j) which represent derivations of
X —* TZ ... Tj, but also outside edges o =
O(X, i, j) which represent derivations of R �
T0 ... TZ−1 X Tj+1 ... T&apos;. For example, where
I(VP, 0, 3) denotes trees rooted at VP covering the
span [0, 3], O(VP, 0, 3) denotes the derivation of the
“rest” of the structure to the root. Where inside
edges e have scores Qe which represent (approxima-
tions of) their Viterbi inside scores, outside edges o
have scores αo which are (approximations of) their
Viterbi outside scores. When we need to denote the
inside version of an outside edge, or the reverse, we
write o = e, etc.
I(NP, 3,5)
O(VP, 4,8)
I(NN, 2,3)
wr
</bodyText>
<page confidence="0.971453">
559
</page>
<table confidence="0.996439833333333">
Name Rule Priority
IN-BASE O(T�� t , i, i + 1) : αT I(Tt, i, i + 1) : 0 αT
IN r : wr O(At, i, j) : αA1 I(Bt, i, k) : βB I(Ct, k, j) : βC I(At, i, j) : βA = βB + βC + wr βA + αA1
OUT-BASE I(Rt, 0, n) : βR O(Rt, 0, n) : 0 βR
OUT-L r:wr O(At, i, j) : αA I(Bt, i, k) : βB I(Ct, k, j) : βC O(Bt,i,k) : αB = αA + βC + wr βB + αB
OUT-R r:wr O(At, i, j) : αA I(Bt, i, k) : βB I(Ct, k, j) : βC O(Ct, k, j) : αC = αA + βB + wr βC + αC
</table>
<tableCaption confidence="0.995357">
Table 2: Deduction rules for HA*. The rule r is in all cases At → Bt Ct.
</tableCaption>
<figureCaption confidence="0.988247">
Figure 2: Non-base case deduction rules for HA* depicted graphically. (a) shows the rule used to build inside edges
and (b) shows the rules to build outside edges. Inside edges are depicted as complete triangles, while outside edges
are depicted as chevrons. An edge from a previous level in the hierarchy is denoted with dashed lines.
</figureCaption>
<figure confidence="0.998647297297297">
aA
A
i k
&amp;quot;B=
&amp;quot;A+!C+wr
B
n
0
0 i j n
(a) (b)
&amp;quot;A&apos;
A&apos;
0 i j n
A
B
B C
P=!A+&amp;quot;A&apos;
IN
wr
C
A
!A=
!B+!C+wr
i k k j i j
!B
!C
i k k j
!B !C
B
B C
wr
A
C
0 k j n
&amp;quot;C=
&amp;quot;A+!B+wr
C
</figure>
<bodyText confidence="0.999389261904762">
The second difference is that HA* tracks items
from all levels of the hierarchy on a single, shared
agenda, so that all items compete (see Figure 3).
While there is only one agenda, it is useful to imag-
ine several charts, one for each type of edge and each
grammar level. In particular, outside edges from one
level of the hierarchy are the source of completion
costs (heuristics) for inside edges at the next level.
The deduction rules for HA* are given in Table 2
and represented graphically in Figure 2. The IN rule
(a) is the familiar deduction rule from standard A*:
we can combine two adjacent inside edges using a
binary rule to form a new inside edge. The new twist
is that because heuristics (scores of outside edges
from the previous level) are also computed on the
fly, they may not be ready yet. Therefore, we cannot
carry out this deduction until the required outside
edge is present in the previous level’s chart. That
is, fine inside deductions wait for the relevant coarse
outside edges to be popped. While coarse outside
edges contribute to priorities of refined inside scores
(as heuristic values), they do not actually affect the
inside scores of edges (again just like basic A*).
In standard A*, we begin with all terminal edges
on the agenda. However, in HA*, we cannot en-
queue refined terminal edges until their outside
scores are ready. The IN-BASE rule specifies the
base case for a grammar fit: we cannot begin un-
til the outside score for the terminal symbol T is
ready in the coarser grammar !9t_1. The initial queue
contains only the most abstract level’s terminals,
I(Ti0, i, i + 1). The entire search terminates when
the inside edge I(Rm, 0, n), represting root deriva-
tions in the target grammar, is dequeued.
The deductions which assemble outside edges are
less familiar from the standard A* algorithm. These
deductions take larger outside edges and produce
smaller sub-edges by linking up with inside edges,
as shown in Figure 2(b). The OUT-BASE rule states
that an outside pass for !9t can be started if the in-
side score of the root symbol for that level Rt has
been computed. The OUT-L and OUT-R rules are
</bodyText>
<page confidence="0.976022">
560
</page>
<bodyText confidence="0.998925806451613">
the deduction rules for building outside edges. OUT-
L states that, given an outside edge over the span
[i, j] and some inside edge over [i, k], we may con-
struct an outside edge over [k, j]. For outside edges,
the score reflects an estimate of the Viterbi outside
score.
As in standard A*, inside edges are placed on the
agenda with a priority equal to their path cost (inside
score) and some estimate of their completion cost
(outside score), now taken from the previous projec-
tion rather than a black box. Specifically, the priority
function takes the form p(e) = Qe + αe, where e&apos;
is the outside version of e one level previous in the
hierarchy.
Outside edges also have priorities which combine
path costs with a completion estimate, except that
the roles of inside and outside scores are reversed:
the path cost for an outside edge o is its (outside)
score αo, while the completion cost is some estimate
of the inside score, which is the weight Qe of o’s
complementary edge e = 6. Therefore, p(o) = αo+
Qs.
Note that inside edges combine their inside score
estimates with outside scores from a previous level
(a lower bound), while outside edges combine their
outside score estimates with inside scores from the
same level, which are already available. Felzen-
szwalb and McAllester (2007) show that these
choices of priorities have the same guarantee as stan-
dard A*: whenever an inside or outside edge comes
off the queue, its path cost is optimal.
</bodyText>
<subsectionHeader confidence="0.997877">
2.3 Agenda-driven Coarse-to-Fine Parsing
</subsectionHeader>
<bodyText confidence="0.999388772727273">
We can always replace the HA* priority function
with an alternate priority function of our choosing.
In doing so, we may lose the optimality guarantees
of HA*, but we may also be able to achieve sig-
nificant increases in performance. We do exactly
this in order to put CTF pruning in an agenda-based
framework. An agenda-based implementation al-
lows us to put CTF on a level playing field with HA*,
highlighting the effectiveness of the various parsing
strategies and normalizing their implementations.
First, we define coarse-to-fine pruning. In stan-
dard CTF, we exhaustively parse in each projection
level, but skip edges whose projections in the previ-
ous level had sufficiently low scores. In particular,
an edge e in the grammar !9t will be skipped entirely
if its projection e&apos; in gt_1 had a low max marginal:
αe + Qe!, that is, the score of the best tree contain-
ing e&apos; was low compared to the score best overall
root derivation QR!. Formally, we prune all e where
αe + Qe! &gt; QR! + T for some threshold T.
The priority function we use to implement CTF in
our agenda-based framework is:
</bodyText>
<equation confidence="0.999194666666667">
p(e) = ,3e
0o αo/ +,3a &gt;
,3Ry + &apos;rt
</equation>
<bodyText confidence="0.967296923076923">
αo + ,35 otherwise
Here, Tt &gt; 0 is a user-defined threshold for level
t and QRt is the inside score of the root for gram-
mar fit. These priorities lead to uniform-cost explo-
ration for inside edges and completely suppress out-
side edges which would have been pruned in stan-
dard CTF. Note that, by the construction of the IN
rule, pruning an outside edge also prunes all inside
edges in the next level that depend on it; we there-
fore prune slightly earlier than in standard CTF. In
any case, this priority function maintains the set of
states explored in CKY-based CTF, but does not nec-
essarily explore those states in the same order.
</bodyText>
<sectionHeader confidence="0.999537" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.975368">
3.1 Evaluation
</subsectionHeader>
<bodyText confidence="0.99893975">
Our focus is parsing speed. Thus, we would ideally
evaluate our algorithms in terms of CPU time. How-
ever, this measure is problematic: CPU time is influ-
enced by a variety of factors, including the architec-
ture of the hardware, low-level implementation de-
tails, and other running processes, all of which are
hard to normalize.
It is common to evaluate best-first parsers in terms
of edges popped off the agenda. This measure is
used by Charniak et al. (1998) and Klein and Man-
ning (2003b). However, when edges from grammars
of varying size are processed on the same agenda,
the number of successor edges per edge popped
changes depending on what grammar the edge was
constructed from. In particular, edges in more re-
fined grammars are more expensive than edges in
coarser grammars. Thus, our basic unit of measure-
ment will be edges pushed onto the agenda. We
found in our experiments that this was well corre-
lated with CPU time.
</bodyText>
<equation confidence="0.877412">
p(o) =
</equation>
<page confidence="0.919139">
561
</page>
<figure confidence="0.92023175">
424
Edges pushed (billions)
0 100 200 300 400
86.6 78.2 58.8 60.1
</figure>
<figureCaption confidence="0.998359">
Figure 4: Efficiency of several hierarchical parsing algo-
rithms, across the test set. UCS and all A* variants are
optimal and thus make no search errors. The CTF vari-
ants all make search errors on about 2% of sentences.
</figureCaption>
<subsectionHeader confidence="0.999134">
3.2 State-Split Grammars
</subsectionHeader>
<bodyText confidence="0.999998380952381">
We first experimented with the grammars described
in Petrov et al. (2006). Starting with an X-Bar gram-
mar, they iteratively refine each symbol in the gram-
mar by adding latent substates via a split-merge pro-
cedure. This training procedure creates a natural hi-
erarchy of grammars, and is thus ideal for our pur-
poses. We used the Berkeley Parser2 to train such
grammars on sections 2-21 of the Penn Treebank
(Marcus et al., 1993). We ran 6 split-merge cycles,
producing a total of 7 grammars. These grammars
range in size from 98 symbols and 8773 rules in the
unsplit X-Bar grammar to 1139 symbols and 973696
rules in the 6-split grammar. We then parsed all sen-
tences of length G 30 of section 23 of the Treebank
with these grammars. Our “target grammar” was in
all cases the largest (most split) grammar. Our pars-
ing objective was to find the Viterbi derivation (i.e.
fully refined structure) in this grammar. Note that
this differs from the objective used by Petrov and
Klein (2007), who use a variational approximation
to the most probable parse.
</bodyText>
<subsectionHeader confidence="0.80711">
3.2.1 A* versus HA*
</subsectionHeader>
<bodyText confidence="0.999964454545454">
We first compare HA* with standard A*. In A* as
presented by Klein and Manning (2003b), an aux-
iliary grammar can be used, but we are restricted
to only one and we must compute inside and out-
side estimates for that grammar exhaustively. For
our single auxiliary grammar, we chose the 3-split
grammar; we found that this grammar provided the
best overall speed.
For HA*, we can include as many or as few
auxiliary grammars from the hierarchy as desired.
Ideally, we would find that each auxiliary gram-
</bodyText>
<footnote confidence="0.924112">
2http://berkeleyparser.googlecode.com
</footnote>
<bodyText confidence="0.999950704545455">
mar increases performance. To check this, we per-
formed experiments with all 6 auxiliary grammars
(0-5 split); the largest 3 grammars (3-5 split); and
only the 3-split grammar.
Figure 4 shows the results of these experiments.
As a baseline, we also compare with uniform cost
search (UCS) (A* with h = 0 ). A* provides a
speed-up of about a factor of 5 over this UCS base-
line. Interestingly, HA* using only the 3-split gram-
mar is faster than A* by about 10% despite using the
same grammars. This is because, unlike A*, HA*
need not exhaustively parse the 3-split grammar be-
fore beginning to search in the target grammar.
When we add the 4- and 5-split grammars to HA*,
it increases performance by another 25%. However,
we can also see an important failure case of HA*:
using all 6 auxiliary grammars actually decreases
performance compared to using only 3-5. This is be-
cause HA* requires that auxiliary grammars are all
relaxed projections of the target grammar. Since the
weights of the rules in the smaller grammars are the
minimum of a large set of rules in the target gram-
mar, these grammars have costs that are so cheap
that all edges in those grammars will be processed
long before much progress is made in the refined,
more expensive levels. The time spent parsing in
the smaller grammars is thus entirely wasted. This
is in sharp contrast to hierarchical CTF (see below)
where adding levels is always beneficial.
To quantify the effect of optimistically cheap
costs in the coarsest projections, we can look at the
degree to which the outside costs in auxiliary gram-
mars underestimate the true outside cost in the target
grammar (the “slack”). In Figure 5, we plot the aver-
age slack as a function of outside context size (num-
ber of unincorporated words) for each of the auxil-
iary grammars. The slack for large outside contexts
gets very large for the smaller, coarser grammars. In
Figure 6, we plot the number of edges pushed when
bounding with each auxiliary grammar individually,
against the average slack in that grammar. This plot
shows that greater slack leads to more work, reflect-
ing the theoretical property of A* that the work done
can be exponential in the slack of the heuristic.
</bodyText>
<subsectionHeader confidence="0.815162">
3.2.2 HA* versus CTF
</subsectionHeader>
<bodyText confidence="0.987248">
In this section, we compare HA* to CTF, again
using the grammars of Petrov et al. (2006). It is
</bodyText>
<figure confidence="0.949259111111111">
HA* CTF CTF
0-5 3 3-5
UCS A* HA* HA*
3 3 3-5
CTF
0-5
8.83 7.12 1.98
562
Number of words in outside context
</figure>
<figureCaption confidence="0.994001166666667">
Figure 5: Average slack (difference between estimated
outside cost and true outside cost) at each level of ab-
straction as a function of the size of the outside context.
The average is over edges in the Viterbi tree. The lower
and upper dashed lines represent the slack of the exact
and uniformly zero heuristics.
</figureCaption>
<figure confidence="0.978247666666667">
Edges pushed (millions) 0 500 1500 2500 3500
5 10 15 20 25 30 35
Slack for span length 10
</figure>
<figureCaption confidence="0.998963">
Figure 6: Edges pushed as a function of the average slack
for spans of length 10 when parsing with each auxiliary
grammar individually.
</figureCaption>
<bodyText confidence="0.964405823529412">
important to note, however, that we do not use the
same grammars when parsing with these two al-
gorithms. While we use the same projections to
coarsen the target grammar, the scores in the CTF
case need not be lower bounds. Instead, we fol-
low Petrov and Klein (2007) in taking coarse gram-
mar weights which make the induced distribution
over trees as close as possible to the target in KL-
divergence. These grammars represent not a mini-
mum projection, but more of an average.3
The performance of CTF as compared to HA*
is shown in Figure 4. CTF represents a significant
speed up over HA*. The key advantage of CTF, as
shown here, is that, where the work saved by us-
3We tried using these average projections as heuristics in
HA*, but doing so violates consistency, causes many search er-
rors, and does not substantially speed up the search.
</bodyText>
<subsectionHeader confidence="0.282464">
Length of sentence
</subsectionHeader>
<figureCaption confidence="0.9925545">
Figure 7: Edges pushed as function of sentence length for
HA* 3-5 and CTF 0-5.
</figureCaption>
<bodyText confidence="0.99932912">
ing coarser projections falls off for HA*, the work
saved with CTF increases with the addition of highly
coarse grammars. Adding the 0- through 2-split
grammars to CTF was responsible for a factor of 8
speed-up with no additional search errors.
Another important property of CTF is that it
scales far better with sentence length than does HA*.
Figure 7 shows a plot of edges pushed against sen-
tence length. This is not surprising in light of the in-
crease in slack that comes with parsing longer sen-
tences. The more words in an outside context, the
more slack there will generally be in the outside es-
timate, which triggers the time explosion.
Since we prune based on thresholds rt in CTF,
we can explore the relationship between the number
of search errors made and the speed of the parser.
While it is possible to tune thresholds for each gram-
mar individually, we use a single threshold for sim-
plicity. In Figure 8, we plot the performance of CTF
using all 6 auxiliary grammars for various values of
,r. For a moderate number of search errors (&lt; 5%),
CTF parses more than 10 times faster than HA* and
nearly 100 times faster than UCS. However, below a
certain tolerance for search errors (&lt; 1%) on these
grammars, HA* is the faster option.4
</bodyText>
<subsectionHeader confidence="0.998456">
3.3 Lexicalized parsing experiments
</subsectionHeader>
<bodyText confidence="0.999918">
We also experimented with the lexicalized parsing
model described in Klein and Manning (2003a).
This lexicalized parsing model is constructed as the
product of a dependency model and the unlexical-
</bodyText>
<footnote confidence="0.956733">
4In Petrov and Klein (2007), fewer search errors are re-
ported; this difference is because their search objective is more
closely aligned to the CTF pruning criterion.
</footnote>
<figure confidence="0.974765555555555">
5 10 15 20
Average slack
0 20 40 60 80 100
0 split
1 split
2 split
3 split
4 split
5 split
5 10 15 20 25 30
Edges pushed per sentence (millions)
0 20 40 60 80 120
HA* 3-5
CTF 0-5
563
Edges pushed (billions) OS 2.0 5.0 20.0 100.0 500.0 HA* 3-5 UCS
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
Fraction of sentences without search errors
</figure>
<figureCaption confidence="0.999019">
Figure 8: Performance of CTF as a function of search er-
</figureCaption>
<bodyText confidence="0.981698727272727">
rors for state split grammars. The dashed lines represent
the time taken by UCS and HA* which make no search
errors. As search accuracy increases, the time taken by
CTF increases until it eventually becomes slower than
HA*. The y-axis is a log scale.
ized PCFG model in Klein and Manning (2003c).
We constructed these grammars using the Stanford
Parser.5 The PCFG has 19054 symbols 36078 rules.
The combined (sentence-specific) grammar has n
times as many symbols and 2n2 times as many rules,
where n is the length of an input sentence. This
model was trained on sections 2-20 of the Penn Tree-
bank and tested on section 21.
For these lexicalized grammars, we did not per-
form experiments with UCS or more than one level
of HA*. We used only the single PCFG projection
used in Klein and Manning (2003a). This grammar
differs from the state split grammars in that it factors
into two separate projections, a dependency projec-
tion and a PCFG. Klein and Manning (2003a) show
that one can use the sum of outside scores computed
in these two projections as a heuristic in the com-
bined lexicalized grammar. The generalization of
HA* to the factored case is straightforward but not
effective. We therefore treated the dependency pro-
jection as a black box and used only the PCFG pro-
jection inside the HA* framework. When comput-
ing A* outside estimates in the combined space, we
use the sum of the two projections’ outside scores as
our completion costs. This is the same procedure as
Klein and Manning (2003a). For CTF, we carry out
a uniform cost search in the combined space where
we have pruned items based on their max-marginals
</bodyText>
<footnote confidence="0.864677">
5http://nlp.stanford.edu/software/
</footnote>
<figureCaption confidence="0.8693624">
Fraction of sentences without search errors
Figure 9: Performance of CTF for lexicalized parsing as
a function of search errors. The dashed line represents
the time taken by A*, which makes no search errors. The
y-axis is a log scale.
</figureCaption>
<bodyText confidence="0.9998608">
in the PCFG model only.
In Figure 9, we examine the speed/accuracy trade
off for the lexicalized grammar. The trend here is
the reverse of the result for the state split grammars:
HA* is always faster than posterior pruning, even for
thresholds which produce many search errors. This
is because the heuristic used in this model is actu-
ally an extraordinarily tight bound – on average, the
slack even for spans of length 1 was less than 1% of
the overall model cost.
</bodyText>
<sectionHeader confidence="0.999815" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9999792">
We have a presented an empirical comparison of
hierarchical A* search and coarse-to-fine pruning.
While HA* does provide benefits over flat A*
search, the extra levels of the hierarchy are dramat-
ically more beneficial for CTF. This is because, in
CTF, pruning choices cascade and even very coarse
projections can prune many highly unlikely edges.
However, in HA*, overly coarse projections become
so loose as to not rule out anything of substance. In
addition, we experimentally characterized the fail-
ure cases of A* and CTF in a way which matches
the formal results on A*: A* does vastly more work
as heuristics loosen and only outperforms CTF when
either near-optimality is desired or heuristics are ex-
tremely tight.
</bodyText>
<sectionHeader confidence="0.997515" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9950785">
This work was partially supported by an NSERC Post-Graduate
Scholarship awarded to the first author.
</bodyText>
<figure confidence="0.991301">
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
A*
Edges pushed (billions)
3 4 5 6 7 8
</figure>
<page confidence="0.992987">
564
</page>
<sectionHeader confidence="0.995303" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999944301587301">
Sharon Caraballo and Eugene Charniak. 1996. Figures
of Merit for Best-First Probabalistic Parsing. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Eugene Charniak. 1997 Statistical Parsing with a
Context-Free Grammar and Word Statistics. In Pro-
ceedings of the Fourteenth National Conference on Ar-
tificial Intelligence.
Eugene Charniak, Sharon Goldwater and Mark Johnson.
1998. Edge-based Best First Parsing. In Proceedings
of the Sixth Workshop on Very Large Corpora.
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph
Austerweil, David Ellis, Isaac Haxton, Catherine Hill,
R. Shrivaths, Jeremy Moore, Michael Pozar, and
Theresa Vu. 2006. Multilevel Coarse-to-fine PCFG
Parsing. In Proceedings of the North American Chap-
ter of the Association for Computational Linguistics.
Michael Collins. 1997. Three Generative, Lexicalised
Models for Statistical Parsing. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics.
P. Felzenszwalb and D. McAllester. 2007. The General-
ized A* Architecture. In Journal of Artificial Intelli-
gence Research.
Aria Haghighi, John DeNero, and Dan Klein. 2007. Ap-
proximate Factoring for A* Search. In Proceedings
of the North American Chapter of the Association for
Computational Linguistics.
Dan Klein and Chris Manning. 2002. Fast Exact In-
ference with a Factored Model for Natural Language
Processing. In Advances in Neural Information Pro-
cessing Systems.
Dan Klein and Chris Manning. 2003. Factored A*
Search for Models over Sequences and Trees. In Pro-
ceedings of the International Joint Conference on Ar-
tificial Intelligence.
Dan Klein and Chris Manning. 2003. A* Parsing: Fast
Exact Viterbi Parse Selection. In Proceedings of the
North American Chapter of the Association for Com-
putational Linguistics
Dan Klein and Chris Manning. 2003. Accurate Unlexi-
calized Parsing. In Proceedings of the North American
Chapter of the Association for Computational Linguis-
tics.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
Mark-Jan Nederhof. 2003. Weighted deductive parsing
and Knuth’s algorithm. In Computational Linguistics,
29(1):135–143.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2003. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of de-
ductive parsing. In Journal of Logic Programming,
24:3–36.
</reference>
<page confidence="0.998492">
565
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.562795">
<title confidence="0.999984">Hierarchical Search for Parsing</title>
<author confidence="0.999474">Adam Pauls Dan</author>
<affiliation confidence="0.998994">Computer Science University of California at</affiliation>
<address confidence="0.994328">Berkeley, CA 94720,</address>
<abstract confidence="0.998931714285714">coarse-to-fine and parsing use simple grammars to guide search in complex ones. We compare the two approaches in a common, agenda-based framework, demonstrating the tradeoffs and relative strengths of each method. Overall, coarse-to-fine is much faster for moderate levels of search errors, but bea certain threshold is superior. In addition, we present the first experiments on hierparsing, in which computation of heuristics is itself guided by meta-heuristics. Multi-level hierarchies are helpful in both approaches, but are more effective in the coarse-</abstract>
<intro confidence="0.572496">to-fine case because of accumulated slack in</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sharon Caraballo</author>
<author>Eugene Charniak</author>
</authors>
<title>Figures of Merit for Best-First Probabalistic Parsing.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="8042" citStr="Caraballo and Charniak, 1996" startWordPosition="1355" endWordPosition="1359">i, i, i + 1) A wr B C B C p=!A+h(A,i,j) A !C i k k j i j !A= !B+!C+wr !B 558 for i = 0 ... n − 1. The algorithm terminates when I(R, 0, n) is popped off the queue. Priorities are in general different than weights. Whenever an edge e’s score changes, its priority p(e), which may or may not depend on its score, may improve. Edges are promoted accordingly in the agenda if their priorities improve. In the simplest case, the priorities are simply the Qe estimates, which gives a correct uniform cost search wherein the root edge is guaranteed to have its correct inside score estimate at termination (Caraballo and Charniak, 1996). A∗ parsing (Klein and Manning, 2003b) is a special case of such an agenda-driven parser in which the priority function p takes the form p(e) = Qe + h(e), where e = I(X, i, j) and h(·) is some approximation of e’s Viterbi outside cost (its completion cost). If h is consistent, then the A∗ algorithm guarantees that whenever an edge comes off the agenda, its weight is its true Viterbi inside cost. In particular, this guarantee implies that the first edge representing the root I(R, 0, n) will be scored with the true Viterbi score for the sentence. 2.2 Hierarchical A∗ In the standard A∗ case the </context>
</contexts>
<marker>Caraballo, Charniak, 1996</marker>
<rawString>Sharon Caraballo and Eugene Charniak. 1996. Figures of Merit for Best-First Probabalistic Parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical Parsing with a Context-Free Grammar and Word Statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="1013" citStr="Charniak (1997)" startWordPosition="150" endWordPosition="151">hod. Overall, coarse-to-fine is much faster for moderate levels of search errors, but below a certain threshold A* is superior. In addition, we present the first experiments on hierarchical A* parsing, in which computation of heuristics is itself guided by meta-heuristics. Multi-level hierarchies are helpful in both approaches, but are more effective in the coarseto-fine case because of accumulated slack in A* heuristics. 1 Introduction The grammars used by modern parsers are extremely large, rendering exhaustive parsing impractical. For example, the lexicalized grammars of Collins (1997) and Charniak (1997) and the statesplit grammars of Petrov et al. (2006) are all too large to construct unpruned charts in memory. One effective approach is coarse-to-fine pruning, in which a small, coarse grammar is used to prune edges in a large, refined grammar (Charniak et al., 2006). Indeed, coarse-to-fine is even more effective when a hierarchy of successive approximations is used (Charniak et al., 2006; Petrov and Klein, 2007). In particular, Petrov and Klein (2007) generate a sequence of approximations to a highly subcategorized grammar, parsing with each in turn. Despite its practical success, coarse-to-</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997 Statistical Parsing with a Context-Free Grammar and Word Statistics. In Proceedings of the Fourteenth National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Sharon Goldwater</author>
<author>Mark Johnson</author>
</authors>
<title>Edge-based Best First Parsing.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="18381" citStr="Charniak et al. (1998)" startWordPosition="3294" endWordPosition="3297">iority function maintains the set of states explored in CKY-based CTF, but does not necessarily explore those states in the same order. 3 Experiments 3.1 Evaluation Our focus is parsing speed. Thus, we would ideally evaluate our algorithms in terms of CPU time. However, this measure is problematic: CPU time is influenced by a variety of factors, including the architecture of the hardware, low-level implementation details, and other running processes, all of which are hard to normalize. It is common to evaluate best-first parsers in terms of edges popped off the agenda. This measure is used by Charniak et al. (1998) and Klein and Manning (2003b). However, when edges from grammars of varying size are processed on the same agenda, the number of successor edges per edge popped changes depending on what grammar the edge was constructed from. In particular, edges in more refined grammars are more expensive than edges in coarser grammars. Thus, our basic unit of measurement will be edges pushed onto the agenda. We found in our experiments that this was well correlated with CPU time. p(o) = 561 424 Edges pushed (billions) 0 100 200 300 400 86.6 78.2 58.8 60.1 Figure 4: Efficiency of several hierarchical parsing</context>
</contexts>
<marker>Charniak, Goldwater, Johnson, 1998</marker>
<rawString>Eugene Charniak, Sharon Goldwater and Mark Johnson. 1998. Edge-based Best First Parsing. In Proceedings of the Sixth Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
<author>Micha Elsner</author>
<author>Joseph Austerweil</author>
<author>David Ellis</author>
<author>Isaac Haxton</author>
<author>Catherine Hill</author>
<author>R Shrivaths</author>
<author>Jeremy Moore</author>
<author>Michael Pozar</author>
<author>Theresa Vu</author>
</authors>
<title>Multilevel Coarse-to-fine PCFG Parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1281" citStr="Charniak et al., 2006" startWordPosition="194" endWordPosition="197">-heuristics. Multi-level hierarchies are helpful in both approaches, but are more effective in the coarseto-fine case because of accumulated slack in A* heuristics. 1 Introduction The grammars used by modern parsers are extremely large, rendering exhaustive parsing impractical. For example, the lexicalized grammars of Collins (1997) and Charniak (1997) and the statesplit grammars of Petrov et al. (2006) are all too large to construct unpruned charts in memory. One effective approach is coarse-to-fine pruning, in which a small, coarse grammar is used to prune edges in a large, refined grammar (Charniak et al., 2006). Indeed, coarse-to-fine is even more effective when a hierarchy of successive approximations is used (Charniak et al., 2006; Petrov and Klein, 2007). In particular, Petrov and Klein (2007) generate a sequence of approximations to a highly subcategorized grammar, parsing with each in turn. Despite its practical success, coarse-to-fine pruning is approximate, with no theoretical guarantees on optimality. Another line of work has explored A* search methods, in which simpler problems are used not for pruning, but for prioritizing work in the full search space (Klein and Manning, 2003a; Haghighi e</context>
</contexts>
<marker>Charniak, Johnson, Elsner, Austerweil, Ellis, Haxton, Hill, Shrivaths, Moore, Pozar, Vu, 2006</marker>
<rawString>Eugene Charniak, Mark Johnson, Micha Elsner, Joseph Austerweil, David Ellis, Isaac Haxton, Catherine Hill, R. Shrivaths, Jeremy Moore, Michael Pozar, and Theresa Vu. 2006. Multilevel Coarse-to-fine PCFG Parsing. In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three Generative, Lexicalised Models for Statistical Parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="993" citStr="Collins (1997)" startWordPosition="147" endWordPosition="148">rengths of each method. Overall, coarse-to-fine is much faster for moderate levels of search errors, but below a certain threshold A* is superior. In addition, we present the first experiments on hierarchical A* parsing, in which computation of heuristics is itself guided by meta-heuristics. Multi-level hierarchies are helpful in both approaches, but are more effective in the coarseto-fine case because of accumulated slack in A* heuristics. 1 Introduction The grammars used by modern parsers are extremely large, rendering exhaustive parsing impractical. For example, the lexicalized grammars of Collins (1997) and Charniak (1997) and the statesplit grammars of Petrov et al. (2006) are all too large to construct unpruned charts in memory. One effective approach is coarse-to-fine pruning, in which a small, coarse grammar is used to prune edges in a large, refined grammar (Charniak et al., 2006). Indeed, coarse-to-fine is even more effective when a hierarchy of successive approximations is used (Charniak et al., 2006; Petrov and Klein, 2007). In particular, Petrov and Klein (2007) generate a sequence of approximations to a highly subcategorized grammar, parsing with each in turn. Despite its practical</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three Generative, Lexicalised Models for Statistical Parsing. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Felzenszwalb</author>
<author>D McAllester</author>
</authors>
<title>The Generalized A* Architecture.</title>
<date>2007</date>
<journal>In Journal of Artificial Intelligence Research.</journal>
<contexts>
<context position="2615" citStr="Felzenszwalb and McAllester, 2007" startWordPosition="401" endWordPosition="404">factored model. In that case, A* vastly improved the search in the lexicalized grammar, with provable optimality. However, their bottleneck was clearly shown to be the exhaustive parsing used to compute the A* heuristic itself. It is not obvious, however, how A* can be stacked in a hierarchical or multi-pass way to speed up the computation of such complex heuristics. In this paper, we address three open questions regarding efficient hierarchical search. First, can a hierarchy of A* bounds be used, analogously to hierarchical coarse-to-fine pruning? We show that recent work in hierarchical A* (Felzenszwalb and McAllester, 2007) can naturally be applied to both the hierarchically refined grammars of Petrov and Klein (2007) as well as the lexicalized grammars of Klein and Manning (2003a). Second, what are the tradeoffs between coarse-to-fine pruning and A* methods? We show that coarse-to-fine is generally much faster, but at the cost of search errors.1 Below a certain search error rate, A* is faster and, of course, optimal. Finally, when and how, qualitatively, do these methods fail? A* search’s work grows quickly as the slack increases between the heuristic bounds and the true costs. On the other hand, coarse-to-fine</context>
<context position="5008" citStr="Felzenszwalb and McAllester (2007)" startWordPosition="806" endWordPosition="809">st algorithms like A* are necessarily implemented with agenda-based parsers. To facilitate comparison, we would like to implement them in a common architecture. We therefore work entirely in an agenda-based setting, noting that the crucial property of CTF is not the CKY order of exploration, but the pruning of unlikely edges, which can be equally well done in an agenda-based parser. In fact, it is possible to closely mimic dynamic programs like CKY using a best-first algorithm with a particular choice of priorities; we discuss this in Section 2.3. While a general HA* framework is presented in Felzenszwalb and McAllester (2007), we present here a specialization to the parsing problem. We first review the standard agenda-driven search framework and basic A* parsing before generalizing to HA*. 2.1 Agenda-Driven Parsing A non-hierarchical, best-first parser takes as input a PCFG !9 (with root symbol R), a priority function p(·) and a sentence consisting of terminals (words) T0 ... T&amp;quot;. The parser’s task is to find the best scoring (Viterbi) tree structure which is rooted at R and spans the input sentence. Without loss of generality, we consider grammars in Chomsky normal form, so that each non-terminal rule in the gramm</context>
<context position="15937" citStr="Felzenszwalb and McAllester (2007)" startWordPosition="2860" endWordPosition="2864">edges also have priorities which combine path costs with a completion estimate, except that the roles of inside and outside scores are reversed: the path cost for an outside edge o is its (outside) score αo, while the completion cost is some estimate of the inside score, which is the weight Qe of o’s complementary edge e = 6. Therefore, p(o) = αo+ Qs. Note that inside edges combine their inside score estimates with outside scores from a previous level (a lower bound), while outside edges combine their outside score estimates with inside scores from the same level, which are already available. Felzenszwalb and McAllester (2007) show that these choices of priorities have the same guarantee as standard A*: whenever an inside or outside edge comes off the queue, its path cost is optimal. 2.3 Agenda-driven Coarse-to-Fine Parsing We can always replace the HA* priority function with an alternate priority function of our choosing. In doing so, we may lose the optimality guarantees of HA*, but we may also be able to achieve significant increases in performance. We do exactly this in order to put CTF pruning in an agenda-based framework. An agenda-based implementation allows us to put CTF on a level playing field with HA*, h</context>
</contexts>
<marker>Felzenszwalb, McAllester, 2007</marker>
<rawString>P. Felzenszwalb and D. McAllester. 2007. The Generalized A* Architecture. In Journal of Artificial Intelligence Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Approximate Factoring for A* Search.</title>
<date>2007</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1893" citStr="Haghighi et al., 2007" startWordPosition="288" endWordPosition="291">al., 2006). Indeed, coarse-to-fine is even more effective when a hierarchy of successive approximations is used (Charniak et al., 2006; Petrov and Klein, 2007). In particular, Petrov and Klein (2007) generate a sequence of approximations to a highly subcategorized grammar, parsing with each in turn. Despite its practical success, coarse-to-fine pruning is approximate, with no theoretical guarantees on optimality. Another line of work has explored A* search methods, in which simpler problems are used not for pruning, but for prioritizing work in the full search space (Klein and Manning, 2003a; Haghighi et al., 2007). In particular, Klein and Manning (2003a) investigated A* for lexicalized parsing in a factored model. In that case, A* vastly improved the search in the lexicalized grammar, with provable optimality. However, their bottleneck was clearly shown to be the exhaustive parsing used to compute the A* heuristic itself. It is not obvious, however, how A* can be stacked in a hierarchical or multi-pass way to speed up the computation of such complex heuristics. In this paper, we address three open questions regarding efficient hierarchical search. First, can a hierarchy of A* bounds be used, analogous</context>
</contexts>
<marker>Haghighi, DeNero, Klein, 2007</marker>
<rawString>Aria Haghighi, John DeNero, and Dan Klein. 2007. Approximate Factoring for A* Search. In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Chris Manning</author>
</authors>
<title>Fast Exact Inference with a Factored Model for Natural Language Processing.</title>
<date>2002</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Chris Manning. 2002. Fast Exact Inference with a Factored Model for Natural Language Processing. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Chris Manning</author>
</authors>
<title>Factored A* Search for Models over Sequences and Trees.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="1868" citStr="Klein and Manning, 2003" startWordPosition="284" endWordPosition="287">ined grammar (Charniak et al., 2006). Indeed, coarse-to-fine is even more effective when a hierarchy of successive approximations is used (Charniak et al., 2006; Petrov and Klein, 2007). In particular, Petrov and Klein (2007) generate a sequence of approximations to a highly subcategorized grammar, parsing with each in turn. Despite its practical success, coarse-to-fine pruning is approximate, with no theoretical guarantees on optimality. Another line of work has explored A* search methods, in which simpler problems are used not for pruning, but for prioritizing work in the full search space (Klein and Manning, 2003a; Haghighi et al., 2007). In particular, Klein and Manning (2003a) investigated A* for lexicalized parsing in a factored model. In that case, A* vastly improved the search in the lexicalized grammar, with provable optimality. However, their bottleneck was clearly shown to be the exhaustive parsing used to compute the A* heuristic itself. It is not obvious, however, how A* can be stacked in a hierarchical or multi-pass way to speed up the computation of such complex heuristics. In this paper, we address three open questions regarding efficient hierarchical search. First, can a hierarchy of A* </context>
<context position="8079" citStr="Klein and Manning, 2003" startWordPosition="1362" endWordPosition="1365">C i k k j i j !A= !B+!C+wr !B 558 for i = 0 ... n − 1. The algorithm terminates when I(R, 0, n) is popped off the queue. Priorities are in general different than weights. Whenever an edge e’s score changes, its priority p(e), which may or may not depend on its score, may improve. Edges are promoted accordingly in the agenda if their priorities improve. In the simplest case, the priorities are simply the Qe estimates, which gives a correct uniform cost search wherein the root edge is guaranteed to have its correct inside score estimate at termination (Caraballo and Charniak, 1996). A∗ parsing (Klein and Manning, 2003b) is a special case of such an agenda-driven parser in which the priority function p takes the form p(e) = Qe + h(e), where e = I(X, i, j) and h(·) is some approximation of e’s Viterbi outside cost (its completion cost). If h is consistent, then the A∗ algorithm guarantees that whenever an edge comes off the agenda, its weight is its true Viterbi inside cost. In particular, this guarantee implies that the first edge representing the root I(R, 0, n) will be scored with the true Viterbi score for the sentence. 2.2 Hierarchical A∗ In the standard A∗ case the heuristics are assumed to come from a</context>
<context position="10754" citStr="Klein and Manning, 2003" startWordPosition="1859" endWordPosition="1862"> represent 7rt(At). A projection is a relaxation if, for every rule r = At � Bt Ct with weight wr the projection r$ = 7rt(r) = A$t —* B$tC$ t has weight wry &lt; wr in Gt−1. Given a target grammar Gm and a projection function 7rm, it is easy to construct a relaxed projection Gm−1 by minimizing over rules collapsed by 7rm: wry = min r∈ggm:Irm(r)=r&apos; Given a series of projection functions 7r1 ... 7rm, we can construct relaxed projections by projecting Gm to Gm−1, then Gm−1 to Gm−2 and so on. Note that by construction, parses in a relaxed projection give lower bounds on parses in the target grammar (Klein and Manning, 2003b). HA∗ differs from standard A∗ in two ways. First, it tracks not only standard inside edges e = I(X, i, j) which represent derivations of X —* TZ ... Tj, but also outside edges o = O(X, i, j) which represent derivations of R � T0 ... TZ−1 X Tj+1 ... T&apos;. For example, where I(VP, 0, 3) denotes trees rooted at VP covering the span [0, 3], O(VP, 0, 3) denotes the derivation of the “rest” of the structure to the root. Where inside edges e have scores Qe which represent (approximations of) their Viterbi inside scores, outside edges o have scores αo which are (approximations of) their Viterbi outsi</context>
<context position="18409" citStr="Klein and Manning (2003" startWordPosition="3299" endWordPosition="3303">he set of states explored in CKY-based CTF, but does not necessarily explore those states in the same order. 3 Experiments 3.1 Evaluation Our focus is parsing speed. Thus, we would ideally evaluate our algorithms in terms of CPU time. However, this measure is problematic: CPU time is influenced by a variety of factors, including the architecture of the hardware, low-level implementation details, and other running processes, all of which are hard to normalize. It is common to evaluate best-first parsers in terms of edges popped off the agenda. This measure is used by Charniak et al. (1998) and Klein and Manning (2003b). However, when edges from grammars of varying size are processed on the same agenda, the number of successor edges per edge popped changes depending on what grammar the edge was constructed from. In particular, edges in more refined grammars are more expensive than edges in coarser grammars. Thus, our basic unit of measurement will be edges pushed onto the agenda. We found in our experiments that this was well correlated with CPU time. p(o) = 561 424 Edges pushed (billions) 0 100 200 300 400 86.6 78.2 58.8 60.1 Figure 4: Efficiency of several hierarchical parsing algorithms, across the test</context>
<context position="20325" citStr="Klein and Manning (2003" startWordPosition="3636" endWordPosition="3639">ymbols and 8773 rules in the unsplit X-Bar grammar to 1139 symbols and 973696 rules in the 6-split grammar. We then parsed all sentences of length G 30 of section 23 of the Treebank with these grammars. Our “target grammar” was in all cases the largest (most split) grammar. Our parsing objective was to find the Viterbi derivation (i.e. fully refined structure) in this grammar. Note that this differs from the objective used by Petrov and Klein (2007), who use a variational approximation to the most probable parse. 3.2.1 A* versus HA* We first compare HA* with standard A*. In A* as presented by Klein and Manning (2003b), an auxiliary grammar can be used, but we are restricted to only one and we must compute inside and outside estimates for that grammar exhaustively. For our single auxiliary grammar, we chose the 3-split grammar; we found that this grammar provided the best overall speed. For HA*, we can include as many or as few auxiliary grammars from the hierarchy as desired. Ideally, we would find that each auxiliary gram2http://berkeleyparser.googlecode.com mar increases performance. To check this, we performed experiments with all 6 auxiliary grammars (0-5 split); the largest 3 grammars (3-5 split); a</context>
<context position="26041" citStr="Klein and Manning (2003" startWordPosition="4644" endWordPosition="4647">ch errors made and the speed of the parser. While it is possible to tune thresholds for each grammar individually, we use a single threshold for simplicity. In Figure 8, we plot the performance of CTF using all 6 auxiliary grammars for various values of ,r. For a moderate number of search errors (&lt; 5%), CTF parses more than 10 times faster than HA* and nearly 100 times faster than UCS. However, below a certain tolerance for search errors (&lt; 1%) on these grammars, HA* is the faster option.4 3.3 Lexicalized parsing experiments We also experimented with the lexicalized parsing model described in Klein and Manning (2003a). This lexicalized parsing model is constructed as the product of a dependency model and the unlexical4In Petrov and Klein (2007), fewer search errors are reported; this difference is because their search objective is more closely aligned to the CTF pruning criterion. 5 10 15 20 Average slack 0 20 40 60 80 100 0 split 1 split 2 split 3 split 4 split 5 split 5 10 15 20 25 30 Edges pushed per sentence (millions) 0 20 40 60 80 120 HA* 3-5 CTF 0-5 563 Edges pushed (billions) OS 2.0 5.0 20.0 100.0 500.0 HA* 3-5 UCS 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 Fraction of sentences without search error</context>
<context position="27496" citStr="Klein and Manning (2003" startWordPosition="4911" endWordPosition="4914">ses until it eventually becomes slower than HA*. The y-axis is a log scale. ized PCFG model in Klein and Manning (2003c). We constructed these grammars using the Stanford Parser.5 The PCFG has 19054 symbols 36078 rules. The combined (sentence-specific) grammar has n times as many symbols and 2n2 times as many rules, where n is the length of an input sentence. This model was trained on sections 2-20 of the Penn Treebank and tested on section 21. For these lexicalized grammars, we did not perform experiments with UCS or more than one level of HA*. We used only the single PCFG projection used in Klein and Manning (2003a). This grammar differs from the state split grammars in that it factors into two separate projections, a dependency projection and a PCFG. Klein and Manning (2003a) show that one can use the sum of outside scores computed in these two projections as a heuristic in the combined lexicalized grammar. The generalization of HA* to the factored case is straightforward but not effective. We therefore treated the dependency projection as a black box and used only the PCFG projection inside the HA* framework. When computing A* outside estimates in the combined space, we use the sum of the two project</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Chris Manning. 2003. Factored A* Search for Models over Sequences and Trees. In Proceedings of the International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Chris Manning</author>
</authors>
<title>A* Parsing: Fast Exact Viterbi Parse Selection.</title>
<date>2003</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics</booktitle>
<contexts>
<context position="1868" citStr="Klein and Manning, 2003" startWordPosition="284" endWordPosition="287">ined grammar (Charniak et al., 2006). Indeed, coarse-to-fine is even more effective when a hierarchy of successive approximations is used (Charniak et al., 2006; Petrov and Klein, 2007). In particular, Petrov and Klein (2007) generate a sequence of approximations to a highly subcategorized grammar, parsing with each in turn. Despite its practical success, coarse-to-fine pruning is approximate, with no theoretical guarantees on optimality. Another line of work has explored A* search methods, in which simpler problems are used not for pruning, but for prioritizing work in the full search space (Klein and Manning, 2003a; Haghighi et al., 2007). In particular, Klein and Manning (2003a) investigated A* for lexicalized parsing in a factored model. In that case, A* vastly improved the search in the lexicalized grammar, with provable optimality. However, their bottleneck was clearly shown to be the exhaustive parsing used to compute the A* heuristic itself. It is not obvious, however, how A* can be stacked in a hierarchical or multi-pass way to speed up the computation of such complex heuristics. In this paper, we address three open questions regarding efficient hierarchical search. First, can a hierarchy of A* </context>
<context position="8079" citStr="Klein and Manning, 2003" startWordPosition="1362" endWordPosition="1365">C i k k j i j !A= !B+!C+wr !B 558 for i = 0 ... n − 1. The algorithm terminates when I(R, 0, n) is popped off the queue. Priorities are in general different than weights. Whenever an edge e’s score changes, its priority p(e), which may or may not depend on its score, may improve. Edges are promoted accordingly in the agenda if their priorities improve. In the simplest case, the priorities are simply the Qe estimates, which gives a correct uniform cost search wherein the root edge is guaranteed to have its correct inside score estimate at termination (Caraballo and Charniak, 1996). A∗ parsing (Klein and Manning, 2003b) is a special case of such an agenda-driven parser in which the priority function p takes the form p(e) = Qe + h(e), where e = I(X, i, j) and h(·) is some approximation of e’s Viterbi outside cost (its completion cost). If h is consistent, then the A∗ algorithm guarantees that whenever an edge comes off the agenda, its weight is its true Viterbi inside cost. In particular, this guarantee implies that the first edge representing the root I(R, 0, n) will be scored with the true Viterbi score for the sentence. 2.2 Hierarchical A∗ In the standard A∗ case the heuristics are assumed to come from a</context>
<context position="10754" citStr="Klein and Manning, 2003" startWordPosition="1859" endWordPosition="1862"> represent 7rt(At). A projection is a relaxation if, for every rule r = At � Bt Ct with weight wr the projection r$ = 7rt(r) = A$t —* B$tC$ t has weight wry &lt; wr in Gt−1. Given a target grammar Gm and a projection function 7rm, it is easy to construct a relaxed projection Gm−1 by minimizing over rules collapsed by 7rm: wry = min r∈ggm:Irm(r)=r&apos; Given a series of projection functions 7r1 ... 7rm, we can construct relaxed projections by projecting Gm to Gm−1, then Gm−1 to Gm−2 and so on. Note that by construction, parses in a relaxed projection give lower bounds on parses in the target grammar (Klein and Manning, 2003b). HA∗ differs from standard A∗ in two ways. First, it tracks not only standard inside edges e = I(X, i, j) which represent derivations of X —* TZ ... Tj, but also outside edges o = O(X, i, j) which represent derivations of R � T0 ... TZ−1 X Tj+1 ... T&apos;. For example, where I(VP, 0, 3) denotes trees rooted at VP covering the span [0, 3], O(VP, 0, 3) denotes the derivation of the “rest” of the structure to the root. Where inside edges e have scores Qe which represent (approximations of) their Viterbi inside scores, outside edges o have scores αo which are (approximations of) their Viterbi outsi</context>
<context position="18409" citStr="Klein and Manning (2003" startWordPosition="3299" endWordPosition="3303">he set of states explored in CKY-based CTF, but does not necessarily explore those states in the same order. 3 Experiments 3.1 Evaluation Our focus is parsing speed. Thus, we would ideally evaluate our algorithms in terms of CPU time. However, this measure is problematic: CPU time is influenced by a variety of factors, including the architecture of the hardware, low-level implementation details, and other running processes, all of which are hard to normalize. It is common to evaluate best-first parsers in terms of edges popped off the agenda. This measure is used by Charniak et al. (1998) and Klein and Manning (2003b). However, when edges from grammars of varying size are processed on the same agenda, the number of successor edges per edge popped changes depending on what grammar the edge was constructed from. In particular, edges in more refined grammars are more expensive than edges in coarser grammars. Thus, our basic unit of measurement will be edges pushed onto the agenda. We found in our experiments that this was well correlated with CPU time. p(o) = 561 424 Edges pushed (billions) 0 100 200 300 400 86.6 78.2 58.8 60.1 Figure 4: Efficiency of several hierarchical parsing algorithms, across the test</context>
<context position="20325" citStr="Klein and Manning (2003" startWordPosition="3636" endWordPosition="3639">ymbols and 8773 rules in the unsplit X-Bar grammar to 1139 symbols and 973696 rules in the 6-split grammar. We then parsed all sentences of length G 30 of section 23 of the Treebank with these grammars. Our “target grammar” was in all cases the largest (most split) grammar. Our parsing objective was to find the Viterbi derivation (i.e. fully refined structure) in this grammar. Note that this differs from the objective used by Petrov and Klein (2007), who use a variational approximation to the most probable parse. 3.2.1 A* versus HA* We first compare HA* with standard A*. In A* as presented by Klein and Manning (2003b), an auxiliary grammar can be used, but we are restricted to only one and we must compute inside and outside estimates for that grammar exhaustively. For our single auxiliary grammar, we chose the 3-split grammar; we found that this grammar provided the best overall speed. For HA*, we can include as many or as few auxiliary grammars from the hierarchy as desired. Ideally, we would find that each auxiliary gram2http://berkeleyparser.googlecode.com mar increases performance. To check this, we performed experiments with all 6 auxiliary grammars (0-5 split); the largest 3 grammars (3-5 split); a</context>
<context position="26041" citStr="Klein and Manning (2003" startWordPosition="4644" endWordPosition="4647">ch errors made and the speed of the parser. While it is possible to tune thresholds for each grammar individually, we use a single threshold for simplicity. In Figure 8, we plot the performance of CTF using all 6 auxiliary grammars for various values of ,r. For a moderate number of search errors (&lt; 5%), CTF parses more than 10 times faster than HA* and nearly 100 times faster than UCS. However, below a certain tolerance for search errors (&lt; 1%) on these grammars, HA* is the faster option.4 3.3 Lexicalized parsing experiments We also experimented with the lexicalized parsing model described in Klein and Manning (2003a). This lexicalized parsing model is constructed as the product of a dependency model and the unlexical4In Petrov and Klein (2007), fewer search errors are reported; this difference is because their search objective is more closely aligned to the CTF pruning criterion. 5 10 15 20 Average slack 0 20 40 60 80 100 0 split 1 split 2 split 3 split 4 split 5 split 5 10 15 20 25 30 Edges pushed per sentence (millions) 0 20 40 60 80 120 HA* 3-5 CTF 0-5 563 Edges pushed (billions) OS 2.0 5.0 20.0 100.0 500.0 HA* 3-5 UCS 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 Fraction of sentences without search error</context>
<context position="27496" citStr="Klein and Manning (2003" startWordPosition="4911" endWordPosition="4914">ses until it eventually becomes slower than HA*. The y-axis is a log scale. ized PCFG model in Klein and Manning (2003c). We constructed these grammars using the Stanford Parser.5 The PCFG has 19054 symbols 36078 rules. The combined (sentence-specific) grammar has n times as many symbols and 2n2 times as many rules, where n is the length of an input sentence. This model was trained on sections 2-20 of the Penn Treebank and tested on section 21. For these lexicalized grammars, we did not perform experiments with UCS or more than one level of HA*. We used only the single PCFG projection used in Klein and Manning (2003a). This grammar differs from the state split grammars in that it factors into two separate projections, a dependency projection and a PCFG. Klein and Manning (2003a) show that one can use the sum of outside scores computed in these two projections as a heuristic in the combined lexicalized grammar. The generalization of HA* to the factored case is straightforward but not effective. We therefore treated the dependency projection as a black box and used only the PCFG projection inside the HA* framework. When computing A* outside estimates in the combined space, we use the sum of the two project</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Chris Manning. 2003. A* Parsing: Fast Exact Viterbi Parse Selection. In Proceedings of the North American Chapter of the Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Chris Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1868" citStr="Klein and Manning, 2003" startWordPosition="284" endWordPosition="287">ined grammar (Charniak et al., 2006). Indeed, coarse-to-fine is even more effective when a hierarchy of successive approximations is used (Charniak et al., 2006; Petrov and Klein, 2007). In particular, Petrov and Klein (2007) generate a sequence of approximations to a highly subcategorized grammar, parsing with each in turn. Despite its practical success, coarse-to-fine pruning is approximate, with no theoretical guarantees on optimality. Another line of work has explored A* search methods, in which simpler problems are used not for pruning, but for prioritizing work in the full search space (Klein and Manning, 2003a; Haghighi et al., 2007). In particular, Klein and Manning (2003a) investigated A* for lexicalized parsing in a factored model. In that case, A* vastly improved the search in the lexicalized grammar, with provable optimality. However, their bottleneck was clearly shown to be the exhaustive parsing used to compute the A* heuristic itself. It is not obvious, however, how A* can be stacked in a hierarchical or multi-pass way to speed up the computation of such complex heuristics. In this paper, we address three open questions regarding efficient hierarchical search. First, can a hierarchy of A* </context>
<context position="8079" citStr="Klein and Manning, 2003" startWordPosition="1362" endWordPosition="1365">C i k k j i j !A= !B+!C+wr !B 558 for i = 0 ... n − 1. The algorithm terminates when I(R, 0, n) is popped off the queue. Priorities are in general different than weights. Whenever an edge e’s score changes, its priority p(e), which may or may not depend on its score, may improve. Edges are promoted accordingly in the agenda if their priorities improve. In the simplest case, the priorities are simply the Qe estimates, which gives a correct uniform cost search wherein the root edge is guaranteed to have its correct inside score estimate at termination (Caraballo and Charniak, 1996). A∗ parsing (Klein and Manning, 2003b) is a special case of such an agenda-driven parser in which the priority function p takes the form p(e) = Qe + h(e), where e = I(X, i, j) and h(·) is some approximation of e’s Viterbi outside cost (its completion cost). If h is consistent, then the A∗ algorithm guarantees that whenever an edge comes off the agenda, its weight is its true Viterbi inside cost. In particular, this guarantee implies that the first edge representing the root I(R, 0, n) will be scored with the true Viterbi score for the sentence. 2.2 Hierarchical A∗ In the standard A∗ case the heuristics are assumed to come from a</context>
<context position="10754" citStr="Klein and Manning, 2003" startWordPosition="1859" endWordPosition="1862"> represent 7rt(At). A projection is a relaxation if, for every rule r = At � Bt Ct with weight wr the projection r$ = 7rt(r) = A$t —* B$tC$ t has weight wry &lt; wr in Gt−1. Given a target grammar Gm and a projection function 7rm, it is easy to construct a relaxed projection Gm−1 by minimizing over rules collapsed by 7rm: wry = min r∈ggm:Irm(r)=r&apos; Given a series of projection functions 7r1 ... 7rm, we can construct relaxed projections by projecting Gm to Gm−1, then Gm−1 to Gm−2 and so on. Note that by construction, parses in a relaxed projection give lower bounds on parses in the target grammar (Klein and Manning, 2003b). HA∗ differs from standard A∗ in two ways. First, it tracks not only standard inside edges e = I(X, i, j) which represent derivations of X —* TZ ... Tj, but also outside edges o = O(X, i, j) which represent derivations of R � T0 ... TZ−1 X Tj+1 ... T&apos;. For example, where I(VP, 0, 3) denotes trees rooted at VP covering the span [0, 3], O(VP, 0, 3) denotes the derivation of the “rest” of the structure to the root. Where inside edges e have scores Qe which represent (approximations of) their Viterbi inside scores, outside edges o have scores αo which are (approximations of) their Viterbi outsi</context>
<context position="18409" citStr="Klein and Manning (2003" startWordPosition="3299" endWordPosition="3303">he set of states explored in CKY-based CTF, but does not necessarily explore those states in the same order. 3 Experiments 3.1 Evaluation Our focus is parsing speed. Thus, we would ideally evaluate our algorithms in terms of CPU time. However, this measure is problematic: CPU time is influenced by a variety of factors, including the architecture of the hardware, low-level implementation details, and other running processes, all of which are hard to normalize. It is common to evaluate best-first parsers in terms of edges popped off the agenda. This measure is used by Charniak et al. (1998) and Klein and Manning (2003b). However, when edges from grammars of varying size are processed on the same agenda, the number of successor edges per edge popped changes depending on what grammar the edge was constructed from. In particular, edges in more refined grammars are more expensive than edges in coarser grammars. Thus, our basic unit of measurement will be edges pushed onto the agenda. We found in our experiments that this was well correlated with CPU time. p(o) = 561 424 Edges pushed (billions) 0 100 200 300 400 86.6 78.2 58.8 60.1 Figure 4: Efficiency of several hierarchical parsing algorithms, across the test</context>
<context position="20325" citStr="Klein and Manning (2003" startWordPosition="3636" endWordPosition="3639">ymbols and 8773 rules in the unsplit X-Bar grammar to 1139 symbols and 973696 rules in the 6-split grammar. We then parsed all sentences of length G 30 of section 23 of the Treebank with these grammars. Our “target grammar” was in all cases the largest (most split) grammar. Our parsing objective was to find the Viterbi derivation (i.e. fully refined structure) in this grammar. Note that this differs from the objective used by Petrov and Klein (2007), who use a variational approximation to the most probable parse. 3.2.1 A* versus HA* We first compare HA* with standard A*. In A* as presented by Klein and Manning (2003b), an auxiliary grammar can be used, but we are restricted to only one and we must compute inside and outside estimates for that grammar exhaustively. For our single auxiliary grammar, we chose the 3-split grammar; we found that this grammar provided the best overall speed. For HA*, we can include as many or as few auxiliary grammars from the hierarchy as desired. Ideally, we would find that each auxiliary gram2http://berkeleyparser.googlecode.com mar increases performance. To check this, we performed experiments with all 6 auxiliary grammars (0-5 split); the largest 3 grammars (3-5 split); a</context>
<context position="26041" citStr="Klein and Manning (2003" startWordPosition="4644" endWordPosition="4647">ch errors made and the speed of the parser. While it is possible to tune thresholds for each grammar individually, we use a single threshold for simplicity. In Figure 8, we plot the performance of CTF using all 6 auxiliary grammars for various values of ,r. For a moderate number of search errors (&lt; 5%), CTF parses more than 10 times faster than HA* and nearly 100 times faster than UCS. However, below a certain tolerance for search errors (&lt; 1%) on these grammars, HA* is the faster option.4 3.3 Lexicalized parsing experiments We also experimented with the lexicalized parsing model described in Klein and Manning (2003a). This lexicalized parsing model is constructed as the product of a dependency model and the unlexical4In Petrov and Klein (2007), fewer search errors are reported; this difference is because their search objective is more closely aligned to the CTF pruning criterion. 5 10 15 20 Average slack 0 20 40 60 80 100 0 split 1 split 2 split 3 split 4 split 5 split 5 10 15 20 25 30 Edges pushed per sentence (millions) 0 20 40 60 80 120 HA* 3-5 CTF 0-5 563 Edges pushed (billions) OS 2.0 5.0 20.0 100.0 500.0 HA* 3-5 UCS 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 Fraction of sentences without search error</context>
<context position="27496" citStr="Klein and Manning (2003" startWordPosition="4911" endWordPosition="4914">ses until it eventually becomes slower than HA*. The y-axis is a log scale. ized PCFG model in Klein and Manning (2003c). We constructed these grammars using the Stanford Parser.5 The PCFG has 19054 symbols 36078 rules. The combined (sentence-specific) grammar has n times as many symbols and 2n2 times as many rules, where n is the length of an input sentence. This model was trained on sections 2-20 of the Penn Treebank and tested on section 21. For these lexicalized grammars, we did not perform experiments with UCS or more than one level of HA*. We used only the single PCFG projection used in Klein and Manning (2003a). This grammar differs from the state split grammars in that it factors into two separate projections, a dependency projection and a PCFG. Klein and Manning (2003a) show that one can use the sum of outside scores computed in these two projections as a heuristic in the combined lexicalized grammar. The generalization of HA* to the factored case is straightforward but not effective. We therefore treated the dependency projection as a black box and used only the PCFG projection inside the HA* framework. When computing A* outside estimates in the combined space, we use the sum of the two project</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Chris Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. In Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="19600" citStr="Marcus et al., 1993" startWordPosition="3508" endWordPosition="3511">g algorithms, across the test set. UCS and all A* variants are optimal and thus make no search errors. The CTF variants all make search errors on about 2% of sentences. 3.2 State-Split Grammars We first experimented with the grammars described in Petrov et al. (2006). Starting with an X-Bar grammar, they iteratively refine each symbol in the grammar by adding latent substates via a split-merge procedure. This training procedure creates a natural hierarchy of grammars, and is thus ideal for our purposes. We used the Berkeley Parser2 to train such grammars on sections 2-21 of the Penn Treebank (Marcus et al., 1993). We ran 6 split-merge cycles, producing a total of 7 grammars. These grammars range in size from 98 symbols and 8773 rules in the unsplit X-Bar grammar to 1139 symbols and 973696 rules in the 6-split grammar. We then parsed all sentences of length G 30 of section 23 of the Treebank with these grammars. Our “target grammar” was in all cases the largest (most split) grammar. Our parsing objective was to find the Viterbi derivation (i.e. fully refined structure) in this grammar. Note that this differs from the objective used by Petrov and Klein (2007), who use a variational approximation to the </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>Weighted deductive parsing and Knuth’s algorithm.</title>
<date>2003</date>
<booktitle>In Computational Linguistics,</booktitle>
<pages>29--1</pages>
<contexts>
<context position="7147" citStr="Nederhof, 2003" startWordPosition="1185" endWordPosition="1186">d, and may or may not be correct at termination, depending on the properties of p. The parser maintains an agenda (a priority queue of edges), as well as a chart (or closed list in search terminology) of edges already processed. The fundamental operation of the algorithm is to pop the best (lowest) priority edge e from the agenda, put it into the chart, and enqueue any edges which can be built by combining e with other edges in the chart. The combination of two adjacent edges into a larger edge is shown graphically in Figure 1 and as a weighted deduction rule in Table 1 (Shieber et al., 1995; Nederhof, 2003). When an edge a is built from adjacent edges b and c and a rule r, its current score Qa is compared to Qb + Q, + wr and updated if necessary. To allow reconstruction of best parses, backpointers are maintained in the standard way. The agenda is initialized with I(Ti, i, i + 1) A wr B C B C p=!A+h(A,i,j) A !C i k k j i j !A= !B+!C+wr !B 558 for i = 0 ... n − 1. The algorithm terminates when I(R, 0, n) is popped off the queue. Priorities are in general different than weights. Whenever an edge e’s score changes, its priority p(e), which may or may not depend on its score, may improve. Edges are </context>
</contexts>
<marker>Nederhof, 2003</marker>
<rawString>Mark-Jan Nederhof. 2003. Weighted deductive parsing and Knuth’s algorithm. In Computational Linguistics, 29(1):135–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning Accurate, Compact, and Interpretable Tree Annotation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Petrov, Barrett, Thibaux, Klein, 2003</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2003. Learning Accurate, Compact, and Interpretable Tree Annotation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved Inference for Unlexicalized Parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1430" citStr="Petrov and Klein, 2007" startWordPosition="216" endWordPosition="219">n A* heuristics. 1 Introduction The grammars used by modern parsers are extremely large, rendering exhaustive parsing impractical. For example, the lexicalized grammars of Collins (1997) and Charniak (1997) and the statesplit grammars of Petrov et al. (2006) are all too large to construct unpruned charts in memory. One effective approach is coarse-to-fine pruning, in which a small, coarse grammar is used to prune edges in a large, refined grammar (Charniak et al., 2006). Indeed, coarse-to-fine is even more effective when a hierarchy of successive approximations is used (Charniak et al., 2006; Petrov and Klein, 2007). In particular, Petrov and Klein (2007) generate a sequence of approximations to a highly subcategorized grammar, parsing with each in turn. Despite its practical success, coarse-to-fine pruning is approximate, with no theoretical guarantees on optimality. Another line of work has explored A* search methods, in which simpler problems are used not for pruning, but for prioritizing work in the full search space (Klein and Manning, 2003a; Haghighi et al., 2007). In particular, Klein and Manning (2003a) investigated A* for lexicalized parsing in a factored model. In that case, A* vastly improved </context>
<context position="2711" citStr="Petrov and Klein (2007)" startWordPosition="416" endWordPosition="419">ality. However, their bottleneck was clearly shown to be the exhaustive parsing used to compute the A* heuristic itself. It is not obvious, however, how A* can be stacked in a hierarchical or multi-pass way to speed up the computation of such complex heuristics. In this paper, we address three open questions regarding efficient hierarchical search. First, can a hierarchy of A* bounds be used, analogously to hierarchical coarse-to-fine pruning? We show that recent work in hierarchical A* (Felzenszwalb and McAllester, 2007) can naturally be applied to both the hierarchically refined grammars of Petrov and Klein (2007) as well as the lexicalized grammars of Klein and Manning (2003a). Second, what are the tradeoffs between coarse-to-fine pruning and A* methods? We show that coarse-to-fine is generally much faster, but at the cost of search errors.1 Below a certain search error rate, A* is faster and, of course, optimal. Finally, when and how, qualitatively, do these methods fail? A* search’s work grows quickly as the slack increases between the heuristic bounds and the true costs. On the other hand, coarse-to-fine prunes unreliably when the approximating grammar 1In this paper, we consider only errors made b</context>
<context position="20155" citStr="Petrov and Klein (2007)" startWordPosition="3606" endWordPosition="3609">grammars on sections 2-21 of the Penn Treebank (Marcus et al., 1993). We ran 6 split-merge cycles, producing a total of 7 grammars. These grammars range in size from 98 symbols and 8773 rules in the unsplit X-Bar grammar to 1139 symbols and 973696 rules in the 6-split grammar. We then parsed all sentences of length G 30 of section 23 of the Treebank with these grammars. Our “target grammar” was in all cases the largest (most split) grammar. Our parsing objective was to find the Viterbi derivation (i.e. fully refined structure) in this grammar. Note that this differs from the objective used by Petrov and Klein (2007), who use a variational approximation to the most probable parse. 3.2.1 A* versus HA* We first compare HA* with standard A*. In A* as presented by Klein and Manning (2003b), an auxiliary grammar can be used, but we are restricted to only one and we must compute inside and outside estimates for that grammar exhaustively. For our single auxiliary grammar, we chose the 3-split grammar; we found that this grammar provided the best overall speed. For HA*, we can include as many or as few auxiliary grammars from the hierarchy as desired. Ideally, we would find that each auxiliary gram2http://berkele</context>
<context position="24002" citStr="Petrov and Klein (2007)" startWordPosition="4285" endWordPosition="4288">is over edges in the Viterbi tree. The lower and upper dashed lines represent the slack of the exact and uniformly zero heuristics. Edges pushed (millions) 0 500 1500 2500 3500 5 10 15 20 25 30 35 Slack for span length 10 Figure 6: Edges pushed as a function of the average slack for spans of length 10 when parsing with each auxiliary grammar individually. important to note, however, that we do not use the same grammars when parsing with these two algorithms. While we use the same projections to coarsen the target grammar, the scores in the CTF case need not be lower bounds. Instead, we follow Petrov and Klein (2007) in taking coarse grammar weights which make the induced distribution over trees as close as possible to the target in KLdivergence. These grammars represent not a minimum projection, but more of an average.3 The performance of CTF as compared to HA* is shown in Figure 4. CTF represents a significant speed up over HA*. The key advantage of CTF, as shown here, is that, where the work saved by us3We tried using these average projections as heuristics in HA*, but doing so violates consistency, causes many search errors, and does not substantially speed up the search. Length of sentence Figure 7: </context>
<context position="26172" citStr="Petrov and Klein (2007)" startWordPosition="4665" endWordPosition="4668">threshold for simplicity. In Figure 8, we plot the performance of CTF using all 6 auxiliary grammars for various values of ,r. For a moderate number of search errors (&lt; 5%), CTF parses more than 10 times faster than HA* and nearly 100 times faster than UCS. However, below a certain tolerance for search errors (&lt; 1%) on these grammars, HA* is the faster option.4 3.3 Lexicalized parsing experiments We also experimented with the lexicalized parsing model described in Klein and Manning (2003a). This lexicalized parsing model is constructed as the product of a dependency model and the unlexical4In Petrov and Klein (2007), fewer search errors are reported; this difference is because their search objective is more closely aligned to the CTF pruning criterion. 5 10 15 20 Average slack 0 20 40 60 80 100 0 split 1 split 2 split 3 split 4 split 5 split 5 10 15 20 25 30 Edges pushed per sentence (millions) 0 20 40 60 80 120 HA* 3-5 CTF 0-5 563 Edges pushed (billions) OS 2.0 5.0 20.0 100.0 500.0 HA* 3-5 UCS 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 Fraction of sentences without search errors Figure 8: Performance of CTF as a function of search errors for state split grammars. The dashed lines represent the time taken b</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved Inference for Unlexicalized Parsing. In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<booktitle>In Journal of Logic Programming,</booktitle>
<pages>24--3</pages>
<contexts>
<context position="7130" citStr="Shieber et al., 1995" startWordPosition="1181" endWordPosition="1184">ivations are considered, and may or may not be correct at termination, depending on the properties of p. The parser maintains an agenda (a priority queue of edges), as well as a chart (or closed list in search terminology) of edges already processed. The fundamental operation of the algorithm is to pop the best (lowest) priority edge e from the agenda, put it into the chart, and enqueue any edges which can be built by combining e with other edges in the chart. The combination of two adjacent edges into a larger edge is shown graphically in Figure 1 and as a weighted deduction rule in Table 1 (Shieber et al., 1995; Nederhof, 2003). When an edge a is built from adjacent edges b and c and a rule r, its current score Qa is compared to Qb + Q, + wr and updated if necessary. To allow reconstruction of best parses, backpointers are maintained in the standard way. The agenda is initialized with I(Ti, i, i + 1) A wr B C B C p=!A+h(A,i,j) A !C i k k j i j !A= !B+!C+wr !B 558 for i = 0 ... n − 1. The algorithm terminates when I(R, 0, n) is popped off the queue. Priorities are in general different than weights. Whenever an edge e’s score changes, its priority p(e), which may or may not depend on its score, may im</context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Stuart M. Shieber, Yves Schabes, and Fernando C. N. Pereira. 1995. Principles and implementation of deductive parsing. In Journal of Logic Programming, 24:3–36.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>