<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9982715">
A Class-based Approach to Word
Alignment
</title>
<author confidence="0.999382">
Sue J. Ker* Jason S. Chang*
</author>
<affiliation confidence="0.961433">
National Tsing Hua University National Tsing Hua University
</affiliation>
<bodyText confidence="0.992349571428571">
This paper presents an algorithm capable of identifying the translation for each word in a bilingual
corpus. Previously proposed methods rely heavily on word-based statistics. Under a word-based
approach, frequent words with a consistent translation can be aligned at a high rate of precision.
However, words that are less frequent or exhibit diverse translations generally do not have sta-
tistically significant evidence for confident alignment, thereby leading to incomplete or incorrect
alignments. The algorithm proposed herein attempts to broaden coverage by exploiting lexico-
graphic resources. To this end, we draw on the two classification systems of words in Longman
Lexicon of Contemporary English (LLOCE) and Tongyici Cilin (Synonym Forest, CILIN). Au-
tomatically acquired class-based alignment rules are used to compensate for what is lacking in a
bilingual dictionary such as the English-Chinese version of the Longman Dictionary of Contem-
porary English (LecDOCE). In addition, this alignment method is implemented using LecDOCE
examples and their translations for training and testing, while further examples from a technical
manual in both English and Chinese are used for an open test. Quantitative results of the closed
and open tests are also summarized.
</bodyText>
<sectionHeader confidence="0.996006" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.994801947368422">
Brown, Cocke, Della Pietra, Della Pietra, Jelinek, Lafferty, Mercer, and Roosin (1990)
advocate a statistical approach to machine translation (MT) and initiate much of the
recent interest in bilingual corpora. Statistical machine translation (SMT) can be un-
derstood as a word-by-word model consisting of two submodels: a language model
for generating a source text segment S and a translation model for mapping S to its
translation T. They recommend using a bilingual corpus to train the parameters of
translation probability, Pr(S j T) in the translation model. For MT and other pur-
poses, many methods have been proposed for sentence alignment of the Hansards,
an English-French corpus of Canadian parliamentary debates (Brown, Lai, and Mer-
cer 1991; Gale and Church 1991a; Simard, Foster, and Isabelle 1992; Chen 1993; Gale
and Church 1993), and for other language pairs, including English-German, English-
Chinese, and English-Japanese (Kay and Roscheisen 1993; Church, Dagan, Gale, Fung,
Helfman, and Satish 1993; Fung and McKeown 1994; Wu 1994). Alignment at other
levels of resolution is obviously useful. A section, paragraph, sentence, phrase, col-
location, or word can be aligned to its translation (Kupiec 1993; Smadja, McKeown,
and Hatzivassiloglou 1996). Other logical approaches involve aligning parse trees of
a sentence and its translation (Matsumoto, Ishimoto, and Utsuro 1993; Meyers, Yan-
garber, and Grishman 1996), or simultaneously generating parse trees and alignment
arrangements (Wu 1995).
</bodyText>
<affiliation confidence="0.542652">
* Department of Computer Science, National Tsing Hua University, Hsinchu, 30043, Taiwan, ROC.
</affiliation>
<email confidence="0.979888">
E-mail: ksj@volans.cs.scu.edu.tw; jschang@cs.nthu.edu.tw
</email>
<note confidence="0.9079205">
Â© 1997 Association for Computational Linguistics
Computational Linguistics Volume 23, Number 2
</note>
<bodyText confidence="0.999920653846154">
In addition to machine translation, many applications for aligned corpora have
been suggested, including machine-aided translation (Shemtov 1993), translation as-
sessment and critiquing tools (Isabelle 1992; des Tombe and Armstrong-Warwick 1993;
Macklovitch 1994), text generation (Smadja 1992; Smadja, McKeown, and Hatzivas-
siloglou 1996), bilingual lexicography (Klavans and Tzoukermann 1990; Church and
Gale 1991; Daille, Gaussier, and Lange 1994; Kupiec 1993; van der Eijk 1993; Li 1994;
Wu and Xia 1994), and word-sense disambiguation (Gale, Church, and Yarowsky 1992;
Chang, Chen, Sheng, and Ker 1996). For these applications, we must go one step fur-
ther from sentence alignment and identify alignment at the word level. In the process
of word alignment, the translation of each source word is identified. This study con-
centrates primarily on identifying alignment at the word level for a given sentence
and its translation.
In the context of SMT, Brown et al. (1993) present a series of five models of Pr(S I T)
for word alignment. Model 1 assumes that Pr(S I T) depends only on lexical transla-
tion probability (LTP) t(s I t), that is, the probability that the ith word s in S translates
into the jth word t in T. The pair of words (s, t), or more precisely (s, t,i,j) since there
could be more than one instance of s or t, is called a connection. Model 2 enhances
Model 1 by considering the dependence of Pr(S I T) on the distortion probability (DP)
d(i 11,1, m) where 1 and m are the respective lengths of S and T measured in number of
words. Brown et al. (1990) propose using an adaptive Expectation and Maximization
(EM) algorithm to estimate the parameters for LTP and DP from a bilingual corpus.
The EM algorithm iterates between two phases to estimate LTP and DP until both
functions converge. In the expectation phase, the parameters t(s I t) and d(i Ii,!, m) in
the SMT model for all possible values of s, t, j, 1, and m are estimated from the sample
of an aligned bilingual corpus. In the maximization phase, each sentence-translation
pair in the corpus is aligned by maximizing the translation probability, Pr(S 1 T). They
examine the feasibility of aligning the English-French Hansards corpus using the SMT
model, on both the sentence level and the word level. The SMT model is then tested
for the task of machine translation. The model produces 35 acceptable translations for
73 sentences. However, to our knowledge, the degree of success of word alignment
has not yet been explored.
Dagan, Church, and Gale (1993) observe that reliably distinguishing sentence
boundaries for a noisy bilingual text scanned by an OCR device is quite difficult. In
such a circumstance, they recommend aligning words directly without the preprocess-
ing phase of sentence alignment. Under that proposal, a rough character-by-character
alignment is first performed. Based on the character alignment, words are subsequently
aligned based on a modified version of Brown et al.&apos;s Model 2. The authors report
that 60.5% of 65,000 words in a noisy document are correctly aligned. For 84% of the
words, the offset from correct alignment is at most 3.
Gale and Church (1991b) present an alternative algorithm that does not estimate
and store probabilities for all word pairs to reduce memory requirement and to ensure
robustness of probability estimation. Instead, for each source word s, only a handful of
target words strongly associated with s are found and stored. Such a task is achieved
by applying a x2-like statistic. They report that the method produces highly precise
(95%) alignment for 61% of the words in the 800 sentences tested.
This paper is motivated by the following observations: First, the above survey
clearly reveals that word-based methods offer only limited coverage even after they
are trained with an extremely large bilingual corpus. Second, we believe that for most
applications, low coverage is just as serious as low precision. For aligned corpora to
be useful for NLP tasks such as machine translation and word-sense disambiguation,
a coverage rate higher than 60% is desirable, even at the expense of a slightly lower
precision rate.
</bodyText>
<page confidence="0.995817">
314
</page>
<note confidence="0.986875">
Sue J. Ker and Jason S. Chang Word Alignment
</note>
<bodyText confidence="0.999883">
This paper presents a word alignment algorithm based on classification in existing
thesauri. The proposed algorithm, called ClassAlign, relies on an automatic procedure
to acquire class-based alignment rules; it does not employ word-by-word translation
probabilities, nor does it use an iterative EM algorithm for estimating such proba-
bilities. Experimental results indicate that classification based on existing thesauri is
highly effective in broadening coverage while maintaining a high precision rate.
The rest of this paper is organized as follows: In Section 2 we briefly discuss the
nature of text and translation that justifies a class-based approach. A set of three al-
gorithms leading to class-based alignment are outlined in Section 3. The algorithms&apos;
effectiveness is demonstrated through examples and their translations in the LecDOCE
(Longman Group 1992), a bilingual version of the Longman Dictionary of Contempo-
rary English (LDOCE, Proctor 1988), as well as sentences from bilingual texts in the
LightShip User&apos;s Guide (Pilot Software Inc. 1993; Galaxy Software Services 1994). The
experiments we undertook to assess the performance of these algorithms are the topic
of Section 4. Quantitative experimental results are also summarized. In Section 5, we
analyze the experimental results and consider ways in which the proposed algorithms
might be extended and improved. Concluding remarks are made in Section 6.
</bodyText>
<sectionHeader confidence="0.799955" genericHeader="categories and subject descriptors">
2. Text and Translation as a Class-to-Class Mapping
</sectionHeader>
<bodyText confidence="0.999944636363637">
The discussion in Section 1 indicates the limitations of statistical methods. As an
alternative, we examine the feasibility of using an everyday bilingual dictionary in
machine-readable form for word alignment. With tens of thousands of headword-and-
translation pairs that can be used to propose high-precision connections, a bilingual
machine-readable dictionary (MRD) surprisingly leads to even lower coverage than a
statistically-derived lexicon. Below, observations are made to account for the reason
why a substantial portion of translations deviate from what is listed in the bilingual
MRD or what is statistically probable. Such deviations inhibit word-based methods
from achieving broad coverage. We contend that a word&apos;s translational deviation is
mostly bounded within the relevant semantic classes, thus justifying a class-based
approach to word alignment.
</bodyText>
<subsectionHeader confidence="0.845471">
2.1 Diverse In-Context Translations
</subsectionHeader>
<bodyText confidence="0.999954555555555">
Given that the translations for a headword (dictionary translations, DTs for short) can
be extracted from a bilingual MRD such as the LecDOCE, a word in S can be aligned
at a high precision rate with its DTs found in T. Headword-and-translation pairs are
a reliable knowledge source for word alignment. However, they cover only a small
part of the connections in an average sentence and its translation. Our experiments
reveal that the translations of a word in context (in-context translations, ICTs for short)
are frequently more diversified than the offerings in an everyday bilingual dictionary.
More specifically, less than 30% of the English words in the context of LecDOCE
examples translate into one of the relevant DTs in the same dictionary.
Translations in an everyday dictionary are meant to provide the reader with the
idea of what is implied by the headword out of context; they are frequently more of
an explanation than a translation. For instance, one LecDOCE sense entry defines the
word boy as &apos;infml esp. AmE a male person, of any age, from a given place&apos; and gives
M 93.A. (modi lai zhi nanren) as the translation relevant to this particular sense.
Such a &apos;translation&apos; per se seems unlikely to appear as the ICT of boy. Aside from this
fundamental difference, behind the disparity between DT and ICT are a plethora of
factors. These include (1) a failure on the dictionary&apos;s (or the statistically derived lexi-
con&apos;s) part to cover a needed word sense, (2) mismatches in sense specificity between
</bodyText>
<page confidence="0.997087">
315
</page>
<note confidence="0.883368">
Computational Linguistics Volume 23, Number 2
</note>
<bodyText confidence="0.951563608695652">
the two languages, (3) collocation pattern, and (4) frequent use of interchangeable syn-
onymous translation not covered in the dictionary These factors are the reasons why
many translations are statistically unlikely, thereby leading to a low coverage rate for
word-based methods.
Sense Gaps. Dictionary and statistically derived translations might not cover the word
sense appropriate to a given word in context. This is particularly true when using an
everyday dictionary for aligning bilingual technical manuals. For instance, the Lec-
DOCE lists four senses and relevant translations for the word click, including (1) Offi
entffiklignigial &apos;to make a slight and short sound&apos;, (2) fitli &apos;to succeed&apos;, (3) {VA
ft tit, &apos;to fall into place&apos;, (4) MRS-A- &apos;to be a quick success, esp. with members of
the opposite sex&apos;, none of which is the right sense for J &apos;to press&apos;, the translation of
click in the context of (El):
(El) Click anywhere else on the screen background or press ESC.
(Cl) /fIffil/AnIgtithltRâTRR ESC.
Mismatch in Sense Specificity. Dictionary treatment of word senses in the source lan-
guage might not correspond to the level of specificity for the relevant concepts in the
target language. For instance, the LecDOCE differentiates two word senses for the
word news by the means in which it is reported: whether it is via electronic (radio or
television) or non-electronic (newspaper) media. In Chinese, the relevant concept is
also differentiated according to how it is reported; however, the difference is between
mass media (translated as NI) and personal communication (translated as Al).
The following examples (E2, C2) and (E3, C3) demonstrate this particular instance of
mismatch in sense specificity.
</bodyText>
<listItem confidence="0.760694">
(E2) to listen to the 7 o&apos;clock news broadcast.
(C2) IBCM-015ffi AMIE
(E3) Our latest news of our son was a letter a month ago.
(C3) V11192,7419 a In el a la tam â414 Mill10110945t4 PM
</listItem>
<bodyText confidence="0.937055428571429">
Collocation Pattern. The collocation pattern often forces the choice of an ICT quite dif-
ferent from the DTs. For instance, the LecDOCE lists MN &apos;news&apos; and WIEN*. &apos;news
reportage&apos; as the translations for news. However, the translation for news modified
by bad is usually FAS &apos;message&apos;. Similarly, lady modified by old almost surely trans-
lates into tt &apos;wife&apos; rather than the DTs, tc-Â± &apos;lady&apos; or ktt &apos;woman&apos; given in the
LecDOCE. The following examples provide further details.
(E4) Nothing but bad news in the newspaper today.
</bodyText>
<equation confidence="0.419952">
(C4) -,gptiitartÂ±-w-ing..
(C4&apos;) )4ei-RIEfit_EPA&apos;Niffill0
(E5) He was very attentive to the old lady and did everything for her.
</equation>
<page confidence="0.994105">
316
</page>
<note confidence="0.931031">
Sue J. Ker and Jason S. Chang Word Alignment
(C5) fttilligtÂ±4MILE,N, 4-tft*NAlittheZ0
(C5&apos;) witIciAli42tcÂ±4MOULI,,
</note>
<figure confidence="0.2747815">
(C5&amp;quot;) 0( ft 1411342ttrf14M616, 4-11f*MAntgo
(E6) He abdicated all responsibility for the care of the child.
(C6) itiAMTIMA A/MVOâW*410
(C6&apos;) oftlik*7HRP4A4NAMâVJAll0
</figure>
<bodyText confidence="0.989154">
Interchangeable Synonymous Translation. Disparity might arise simply because an in-
terchangeable synonym of a DT is used. For instance, for the following LecDOCE
examples, the synonyms jam, Am, and ON are present in the translations, instead
of the respective DTs, rJtJ, AN, and ga. If these ICTs in (C7, C8, C9) are replaced
by the DTs, the subsequent translations (C7&apos;, C8&apos;, C9&apos;) remain correct.
</bodyText>
<figure confidence="0.870186222222222">
(E7) I caught a fish yesterday.
(C7) irpf-A#FONJ
(C7&apos;) IfFfilf-T11
(E8) I have never met so nice a girl.
(C8) fttt*A.R. lariefffi`gtcfA-T-
(C8&apos;) flItt*ILINJAVIVfffrgtcfAT.
(E9) He abandoned himself to grief.
(C9) ItTNIILINO
(C9&apos;) 11TifÂ§e65.
</figure>
<bodyText confidence="0.993032">
A statistically-derived lexicon generally fares better than MRDs in covering such syn-
onymous translations. However, this is limited to synonyms that appear as an alter-
native translation frequently and consistently in a bilingual corpus.
Bounding the ICTs. An ICT may deviate from the relevant DTs for a variety of reasons,
but the deviation is not without constraints. Table 1 lists some examples of deviating
translations taken from the LecDOCE. Examples include the words news, meet, lady,
grief, care, and child and their respective translations irat , ttAgg,
and /JNIA . Notice that most in-context and dictionary translations of source words
are bounded within the same category in a typical thesaurus such as the LLOCE
(McArthur 1992) and CILIN (Mei et al. 1993). For instance, Da/9-class words in CILIN
(news and messages), MS, aria, tiM appear as the translations (DTs and ICTs) for
Ge194-words in LLOCE (information and news) such as news and report. Similarly,
1d18-class words (hitting, touching, meeting, and missing), gx, AN, gig, iNg
appear as the translations for Mc072-words (meeting people and things) such as meet
and encounter. This finding suggests that LTP can be estimated more robustly via class-
to-class mapping. Furthermore, such ICTs and DTs are often synonymous compounds
</bodyText>
<page confidence="0.998707">
317
</page>
<tableCaption confidence="0.9175815">
Computational Linguistics Volume 23, Number 2
Table 1
</tableCaption>
<table confidence="0.98339975">
Disparity between ICT and DT is bounded within thesaurus categories.
Example Sentences Word ICT DT
What wonderful news: the painting on my wall is a news &apos;Ala ER
Rembrandt!
Altff41101;6Â±AlirlifilMittRAAM. (Ge194) (Da19) (Da19)
Reports that the general is to be dismissed are gaining report 41,E1 ittPi
currency among government ministers.
4M11411 IlleAfraigriVRâ¬33-KrEM4RiiVREM. (Ge194) (Da19) (Da19)
I have never met so nice a girl. meet Mi
fil#t*Ellf42.Ceti`gtcAT. (Mc072) (Id18) (Id18)
He encountered many difficulties. encounter Mil ME; 24E
1tM11111X11111. (Mc072) (Id18) (Id18)
He was very attentive to the old lady and did everything lady tt tcÂ±
for her. (Ca005) (Ab01) (Ab01)
ittAligtttIMIN6, itffStliNiktglo
She&apos;s a very wealthy woman, and moves in the highest woman tCÂ± Mk;
circles of society. tCM; tCA
kt11211AlltritcÂ±, MEWAVI-15-zi, 0 (Ca002) (Ab01) (Ab01)
He abandoned himself to grief. grief ita Pgai
e*Icergo (Fd082) (Ga01) (Ga01)
The sad man was in an abyss of hopelessness. sad Siib; ,ES;
mirstrgATa/fotrgivals0 (Fd080) (Ga01) Algi;
(Ga01)
He abdicated all responsibility for the care of the child. care PRIA .44
ftbk*T âlil-A/NA --C1(4-10 (N1366) (Hi37) (Hi37)
We should advertise for someone to look after the look after .Y4 (Hi37)
garden. (Nf162) (Hi37)
fliMPERWMIBMA*110$44TEIZI.
He abdicated all responsibility for the care of the child. child %HA
IthMT âfaAikfAtrgâtihtil. (Ca003) (Ab04) (Ab04)
John Smith? Yes - he&apos;s a local boy, I believe. boy A 93A
Zigln? (Ca002) (Ab02) (Ab01)
</table>
<bodyText confidence="0.976132666666667">
that share a common morpheme. For instance, the (ICT, DT) pairs, MN and RES) and
(tcÂ± and tct) share a common morpheme Xt &apos;sad&apos; and tc &apos;female&apos;, respectively. Fujii
and Croft (1993) also point out a similar thesaurus effect of Mandarin morphemes in
Japanese information retrieval (IR).1
Dictionary-based Alignment. The above observations suggest that a DT-based algorithm,
coupled with morpheme-level partial matching, can be adopted to obtain a substantial
</bodyText>
<footnote confidence="0.99853625">
1 Fujii and Croft observe that a document is likely to be relevant if it contains an index term that has a
morpheme (kanji) in common with a query term. More often than not, the index term and the query
term are synonyms that might appear under the same category in a thesaurus. The authors call this
phenomenon the thesaurus effect of kanji.
</footnote>
<page confidence="0.995016">
318
</page>
<note confidence="0.983966">
Sue J. Ker and Jason S. Chang Word Alignment
</note>
<tableCaption confidence="0.991319">
Table 2
</tableCaption>
<figure confidence="0.281293">
Complete and partial matches against dictionary translations.
Example Sentences and Translations
I only know it was a dog and
not a cat that bit me.
WEVOI Ergl
JMO
I have made you an absolute
promise that I will help you.
</figure>
<sectionHeader confidence="0.573424" genericHeader="method">
VIE fittftititSPER:4111t4.
</sectionHeader>
<bodyText confidence="0.987857">
There was an acute lack of food.
He added the wood to the fire.
</bodyText>
<equation confidence="0.981551227272727">
itftgeigT2M.
Complete Matches
(Headword, DT=ICT)
(know, ,;112),
(bit, 05),
(dog, ff), (cat, M)
(help, Xiij ),
(you, 4),
(will, g )
(lack,
(food, *It)
(he, {E),
(fire, 9()
Partial Matches
(Headword, DT, ICT)
(only, .c-M, p)
(have, E,
(absolute,
fakflftErg, figfAitith)
(acute, ME BJ, km)
(wood, ?KU, *M)
(wood, MA, *V)
</equation>
<bodyText confidence="0.99956725">
number of high-precision connections. Experimental results indicate that a DT-based
method connects over 40% of words in LecDOCE examples with their ICTs using this
rudimentary method. Table 2 presents some examples from the experiments, indicating
the connections that are attributed to a complete or partial match using headword-and-
DT pairs extracted from the LecDOCE. For instance, partial match enables the method
to pair up only and 14 according to its DT R. These connections can be subsequently
used as the basis for generalizing to a class-based alignment rule in the form of (X, Y),
which stipulate the connection between an X-class word and a Y-class word.
</bodyText>
<subsectionHeader confidence="0.997887">
2.2 Class-based Word Alignment
</subsectionHeader>
<bodyText confidence="0.996608142857143">
To ensure broad coverage, the class-based approach seems to be a promising alter-
native to word-based methods. Classes can be formed from words in more than one
way. Automatic statistical methods for derived classes (Brown, Della Pietra, deSouza,
Lai, and Mercer 1992) are not appropriate, since they also suffer low coverage due to
data sparseness. Classes formed from morphologically related words are easy to de-
rive and apply. Morphological classes can be formed, either from words that start with
the same five-character prefix as in Gale and Church (1991b), or rigorous analysis as
suggested in Brown, Della Pietra, Della Pietra, Lafferty, and Mercer (1992). Although
easily applicable, morphological classes are not particularly effective in broadening
coverage of word alignment. Chang and Chen (1994) also examine the feasibility of
using part-of-speech classes. A potential alternative involves adopting categories avail-
able in machine-readable lexicographic resources such as Roget&apos;s thesaurus (Chapman
1977) or hand-crafted computer lexicons (Miller, Beckwith, Fellbaum, Gross, and Miller
1990; McRoy 1992).
</bodyText>
<sectionHeader confidence="0.771389" genericHeader="method">
3. Algorithms Leading to Class-based Word Alignment
</sectionHeader>
<bodyText confidence="0.999968833333333">
This section describes a series of three algorithms leading to a class-based system
for word alignment. The first algorithm attempts to obtain reliable connections. The
second algorithm generalizes the connections into a list of class-based rules, which
stipulate that a pair of classes of words in the source and target languages are likely
mutual translations. The third algorithm performs the actual word alignment based
on the acquired rules, in addition to DTs.
</bodyText>
<page confidence="0.997672">
319
</page>
<note confidence="0.46221">
Computational Linguistics Volume 23, Number 2
</note>
<subsectionHeader confidence="0.995387">
3.1 Dictionary-based Word Alignment
</subsectionHeader>
<bodyText confidence="0.984721583333333">
This section describes a rudimentary word-alignment algorithm, DietAlign, based on
the DTs from a bilingual MRD such as the LecDOCE. Consider a text and translation
pair (S, T), a word s in S. and its ICT, t in T. Let DT, denote the set of translations
listed in the LecDOCE for the headword s. Recall that if for a word tin T, there is a dt
in DT, such that t matches dt completely or partially, then, t is likely to be the ICT of
s. Taking advantage of this phenomenon, DictAlign computes the set WT = ft I t is a
word in T} and calculates the similarity between each t and the DT, relevant to S. A
similarity measure based on the unweighted Dice coefficient (Dice 1945) can be given
as follows:
Based on this similarity measure, the likelihood of a connection can be associated with
the following formulation that links the likelihood of a connection to similarity with
a DT:
</bodyText>
<equation confidence="0.952655">
DTSim(s, t) = max Sim(d, t)
dEDT,
</equation>
<bodyText confidence="0.7439575">
For instance, consider the following sentence and its Mandarin translation, focusing
on the word encounter:
</bodyText>
<equation confidence="0.953776625">
S = He encountered many difficulties.
T = itANItliX1111.
where d, t =
Idl =
Iti =
Idntl =
2 x Idfltl
Sim(d, t) = (1)
</equation>
<figure confidence="0.6166252">
idi iti
Mandarin morpheme strings,
the number of the morphemes in d,
the number of the morphemes in t,
the number of the morphemes in the intersection of d and t.
(2)
We will have the following:
Ws =
WT =
DTencounter =
</figure>
<construct confidence="0.2985">
{he, encounter, many, difficulty}
ft, A, AK ill, 111X, X, W, ON, NO
{NM OM, agl.
</construct>
<bodyText confidence="0.90498">
Therefore, the connections relevant to encounter with nonzero DTSim values based on
unweighted Dice coefficient are as follows:
</bodyText>
<construct confidence="0.3445484">
DTSim(encounter, A) = max{Sim(RA, A),Sim(, ),Sim(RA, A)}
DTSim(encounter,ANJ) max f 2 x 1 0 2 x 1
= 0.67
1 1+ 2&apos; 1+ 2&apos; 1+ 2
max{Sim(INA, ),Sim(, lff ), Sim(, lJ )}
</construct>
<equation confidence="0.9962925">
max2 2&apos;2 2&apos;22
i 2 x 1 0 2 x 1 1
= =0.5
1 + + +
</equation>
<bodyText confidence="0.996083">
The head morpheme in a word is usually more relevant in determining a word&apos;s
meaning, just as content words carry more meaning than function words. Matching
such a morpheme often implies a higher likelihood of finding the ICT. For instance,
IA is the head morpheme of the DT APJ, and should be given a heavier weight. Our
</bodyText>
<page confidence="0.995812">
320
</page>
<note confidence="0.904082">
Sue J. Ker and Jason S. Chang Word Alignment
</note>
<bodyText confidence="0.942193833333333">
experiments indicate that by weighting morphemes, ICT ambiguity can be resolved
more successfully. Assuming that such weights can be obtained in a manner similar
to what is done in IR when assigning weights to index terms, the weighted Dice
coefficient can be used by substituting weights for counts in equation (1) to arrive at
the following:
where d,t =
</bodyText>
<equation confidence="0.7990742">
idl =
It =
Id n ti =
2 x ldn tl
Sim(d, t) = Id, iti
</equation>
<bodyText confidence="0.98959225">
the Mandarin morpheme strings,
total weights for the morphemes in d,
total weights for the morphemes in t,
total weights for the morphemes in the intersection of d and t.
</bodyText>
<equation confidence="0.541409">
(3)
</equation>
<bodyText confidence="0.435813">
The above descriptions are summarized as the DictAlign Algorithm:
</bodyText>
<construct confidence="0.437104888888889">
Algorithm 1 (DictAlign) Align each word s in S with the ICT t in T based on DTs.
Step 1: Remove all stop words in S to obtain a list of keywords, Ws.
Step 2: Lookup all possible words WT of T in a dictionary
Step 3: For each s in Ws, look up the root of s in a bilingual dictionary to obtain DTs.
Step 4: For all d E DTs and all t E WT, calculate Sim(d, t) according to equation (3).
Step 5: For each (s, t) E Ws X WT, calculate DTSim(s, t) according to equation (2).
Step 6: For each word s, produce a connection (s, t), if DTSim(s, t) is maximized over
t E WT and DTSim(s, t) &gt; h1 where h1 is a preset threshold.
Step 7: Compile the list of connections and denote the list as CONN.
</construct>
<bodyText confidence="0.962543111111111">
To illustrate how DictAlign works, consider the sentence pair (E10, C10). After the
stopwords are removed, we obtain Ws = {old, lady, clad Jur, coat} . The list of words
in T is also obtained by consulting a Chinese dictionary.&apos; Subsequently, for each s in
Ws, we lookup s in the LecDOCE to obtain DT,. Table 3 shows dictionary translations
relevant to (E10). Table 4 lists all Ws words along with the relevant DTâ possible
translation t, as well as the values of Sim(d, t) and DTSim(s, t). Table 5 displays the
result CONN for various values of threshold hi.
(E10) The old lady was clad in a fur coat.
(do)
</bodyText>
<subsectionHeader confidence="0.999982">
3.2 Acquisition of Mutually Translatable Class Pairs
</subsectionHeader>
<bodyText confidence="0.999469875">
ClassAlign is conceived to capture the diversity of translations for broad-coverage
alignment. One way to do so is via the classification of words in thesauri. More
specifically, one can generalize from a connection (s, t) to a class-to-class mapping
(X, Y) where X and Y are thesaurus classes containing s and t respectively. However,
this simple intuition is complicated by the fact that a word might belong to more
than one class, that is, if the classification is based on a thesaurus that allows for
word-sense ambiguity. For a word in a particular context, if one considers classes that
are not intended for the context, noise can be introduced. For instance, consider the
</bodyText>
<footnote confidence="0.9463485">
2 The dictionary used in this study is a combination of CHAN and an on-line dictionary developed by
the CKIP group, Academy Sinica, Nankang, Taiwan.
</footnote>
<page confidence="0.994571">
321
</page>
<note confidence="0.392464">
Computational Linguistics Volume 23, Number 2
</note>
<tableCaption confidence="0.763317333333333">
Table 3
The DT, for each word s in Ws for example (E10, CM).
Word Root Stopword Dictionary Translations of Words in S
</tableCaption>
<bodyText confidence="0.817913272727273">
the the yes None (all sense entries have an explanation in brackets;
not a translation)
old old no kifteErg, JAMBI ffigErg, ifftrtg, *an, tot],
reammti, ...
lady lady no tt Â±, tt&apos;, *A, tdt, MIZIEfill, EA, ...
was be yes 1, 411, tf ... ZÂ±, a&apos; ...
clad clad no
in in yes /1 . . . 2134 , /1 . . .
a a yes -IN, -ft, -41., -41, - , 44110/9, vi- 40 , ...
fur fur no
coat coat no tA, ,FA, 3$A, *V, thit2WVZ
</bodyText>
<tableCaption confidence="0.993899">
Table 4
</tableCaption>
<table confidence="0.993773384615385">
All connection candidates and DTSim values in (E10, C10).
s E Ws t E WT dt E DTs Sim(dt,t) DTSim(s,t)
old t iff g tr9 0.54 0.74
old t gErg 0.74 0.74
old tn *tErtj 0.41 0.51
old to tErg 0.51 0.51
old tOSA Ifftitj 0.35 0.42
old VGA t Er) 0.42 0.42
lady VO *NZ fk 01 0.30 0.30
lady VOA imii vat- VI 0.27 0.31
lady EA 0.30 0.31
lady to A A 0.31 0.31
lady NA NM Zit Vi 0.31 0.39
lady NA EA 0.37 0.39
lady NA 01A 0.39 0.39
lady o MI Z al VI 0.34 0.34
lady A A 0.52 0.56
lady A *A 0.56 0.56
clad V 0* 0.71 0.71
clad V* V* 1.00 1.00
clad * 0 * 0.62 0.62
fur &amp; E2.+Se 0.31 0.67
fur &amp; ki&amp; 0.53 0.67
fur &amp; aA 0.67 0.67
fur &amp; 0.67 0.67
coat &amp; fiMZWA 0.31 0.31
</table>
<page confidence="0.985778">
322
</page>
<note confidence="0.972284">
Sue J. Ker and Jason S. Chang Word Alignment
</note>
<tableCaption confidence="0.977381">
Table 5
</tableCaption>
<figure confidence="0.765717882352941">
The results of running DictAlign on
(E10, C10) under various thresholds.
Threshold = 0.7
Word s Translation t DTSim(s, t)
old 0.74
clad 5r* 1.00
Threshold = 0.67
Word s Translation t DTSim(s, t)
old g 0.74
clad V* 1.00
fur 0.67
Threshold = 0.5
Word s Translation t DTSim(s, t)
old g 0.74
lady A 0.56
clad gr* 1.00
fur &amp; 0.67
</figure>
<figureCaption confidence="0.38013">
connection (have, found in the context of Example (Ell, C11). According to the
</figureCaption>
<bodyText confidence="0.947760909090909">
LLOCE, have belongs to the following topical sets: Cb024 (relating to sex), De081 (hav-
ing and owning), Ea003 (eating and drinking), Li273 (auxiliary related to time), and
Nf159 (making necessary). The LLOCE class Ea003 and CILIN class Fc06 (to eat, chew,
suck, and drink) are intended for this context. However, without that information, the
following noisy rules might be introduced: (Cb024, Fc06), (De081, Fc06), (Li273, Fc06),
(Nf159, Fc06), along with the signal (Ea003, Fc06).
(Ell) Let&apos;s have breakfast early for a change.
(C11) flifIVINW4L,
The noise is usually distributed randomly while the signal tends to repeat itself.
Nevertheless, connections (s, t) related to some ambiguous words s or t may cause
noise to accumulate, leading to erroneous generalization. Therefore, one should try to
throw away such noise. Moreover, any signal that gets thrown away by not considering
(s, t) is often filled by a connection (s&apos;, t) where s&apos; is a synonym of s. For instance, get is
many ways ambiguous, as indicated in diversified ICTs in the LecDOCE examples in
Table 6. However, each of these ICTs seems to form a connection with a less-ambiguous
synonym of get such as receive, reach, and understand in LecDOCE examples. Table 6
provides further details.
As is typical in IR research, highly frequent and ambiguous words (known as
stopwords in the IR literature) can be thrown out to reduce such noise. A list of
stopwords used in the experiments includes the following:
a, at, be, drive, eye, field, fix, for, from, function, get, go, have, head, idea, in, into,
lot, of, on, place, the, to, up, with, .
</bodyText>
<page confidence="0.7785835">
3 Stopwords include eye, field, and fix, which are not usually member of stopword lists in IR. They are
323
</page>
<figure confidence="0.62670945">
Computational Linguistics Volume 23, Number 2
Table 6
Some sentence-translation pairs containing the word get.
Example and Translation Source/Class
I got a letter today. get
fil-i.113t01 (De083, Getting and earning)
It is unpleasant to receive anonymous letters. receive
fft ii54-AT15k (De083, Getting and earning)
We got there at 8 o&apos;clock.
filflAIMMUJIM
His hunger was not appeased until he
reached the hotel.
lJiStaZikfthrititt t#41-14NIAR
get
(Ma005, Arriving and reaching)
reach
(Ma005, Arriving and reaching)
I don&apos;t get you; what do you mean? get
RTC n,tesz? (Gb031, Understand and realize)
I understood that it was time to leave. understand
</figure>
<figureCaption confidence="0.408211">
uturginfilAtTo (Gb031, Understand and realize)
</figureCaption>
<bodyText confidence="0.999980466666667">
With the difficulties of finding appropriate classification systems and suppress-
ing noise now resolved, the question remains: How can class-to-class mapping be
acquired? Just as with the derivation of a bilingual lexicon from a corpus, acquisition
of such mapping requires a statistical measure. The Dice coefficient (Dice 1945) is a
similarity measure that gauges the ratio of the members in one collection being iden-
tical to those of another collection. Smadja, McKeown, and Hatzivassiloglou (1996)
propose to link co-occurrence to the Dice coefficient in their study of bilingual collo-
cations. They observe that, unlike statistical measures related to mutual information,
the Dice coefficient is insensitive to sample size and, thus, more effective for acquiring
bilingual collocations from a bilingual corpus. Our experimental results confirm their
observation. Under a formulation linking translation to conceptual similarity, the Dice
coefficient is a very useful estimator of the class-to-class mapping.
Therefore, in this work, we measure the likelihood of class-to-class translation
mapping in terms of the ratio of member pairs that are connections observed in a
bilingual corpus. This ratio can be easily measured using the Dice coefficient as follows:
</bodyText>
<equation confidence="0.9979885">
E From(a, Y) E To (X, b)
ClassSim(X, Y) = â¬X (4)
IXI -I- WI
b E Y
</equation>
<bodyText confidence="0.9986045">
where I XI = the total number of the words in X,
Yi = the total number of the words in Y,
From(a, Y) = 1, if (ay E Y)(a, y) E ALLCONN,
= 0, otherwise,
treated as such because of their high frequency in the LDOCE and their involvement in diverse LLOCE
topics and CILIN categories. For instance, the LLOCE lists eye under the following topical sets: Ac051
(part of an animal), Aj151 (part of a plant), Bc023 (part of human body), Dg152 (part of a shoe), Hd126
(part of a needle), etc.
</bodyText>
<page confidence="0.993445">
324
</page>
<note confidence="0.888627">
Sue J. Ker and Jason S. Chang Word Alignment
</note>
<equation confidence="0.988661">
To(X,b) = 1, if (3x E X)(x,b) E ALLCONN,
</equation>
<bodyText confidence="0.933164923076923">
= 0, otherwise,
ALLCONN = the word-translation pairs compiled from the results of running
DictAlign on all sentence-translation pairs of a bilingual corpus.
This naive estimator works efficiently for classes of compatible sizes. Occasion-
ally, for extremely small or large classes, the coefficient does not accurately reflect
how likely words in one class are to translate to words in another class. To remedy
this problem, we explore the feasibility of weighting the member words. According
to our result, weighting eradicates most instances of the problem caused by uneven
classification. The weight assigned to each word should positively correlate to the
frequency of the word so it reflects the expected ratio of word-translation pairs. As-
suming that such weights can be obtained on the basis of each word&apos;s frequency in
a bilingual corpus, weights can substitute for counts in equation (4) to arrive at the
weighted version of the Dice coefficient shown below:
</bodyText>
<equation confidence="0.993378666666667">
E From(a, Y) + E To(X, b)
ClassSim(X, Y) = 41EX bEY
IXI+IYI
</equation>
<bodyText confidence="0.708095">
where IN = the total weights of the words in X,
</bodyText>
<equation confidence="0.738449666666667">
I YI = the total weights of the words in Y,
From (a, Y) = the weight of a, if (3y E Y)(a,y) E ALLCONN,
= 0, otherwise,
To(X, b) = the weight of b, if (3x E X)(x,b) E ALLCONN,
= 0, otherwise,
ALLCONN = the source-translation pairs obtained for all sentences and
</equation>
<bodyText confidence="0.98453975">
translations in the training corpus using Algorithm 1.
Algorithm 2 summarizes the ClassRule algorithm for acquiring class-based rules.
Table 7 presents a random sample of class-based rules acquired from connections
found in LecDOCE examples and translations.
</bodyText>
<construct confidence="0.713093333333333">
Algorithm 2. (ClassRule) Acquisition of pairs of mutually translatable classes (X, Y).
Step 1: Run DictAlign on the sentences in a bilingual corpus to obtain a list of initial
connections ALLCONN.
Step 2: For all X E CX and all Y E CY, compute similarity ClassSim(X, Y) based on
weighted Dice coefficient given in (5), where CX and CY are some classifica-
tion of words in the source and target languages, respectively.
Step 3: Produce an alignment rule (X, Y),
if ClassSim(X, Y) &gt; h1, a preset threshold,
or if ClassSim(X, Y) is maximized over all X in CX or all Y in CY.
</construct>
<bodyText confidence="0.7489525">
Step 4: Compile the list of such class pairs satisfying the conditions in Step 3 and
denote the list as RULES.
</bodyText>
<figure confidence="0.868397607843137">
(5)
325
Computational Linguistics Volume 23, Number 2
m-Ri 0
o ;0â, a
41112
tu&apos; cu e 20
z 0. a)
tIO
c)
TS
^CS
g
0
L.; -6
as
o.) g
g uQ.) it
O &apos;65. 8 ;-â 
o 6 co
3 li w &apos;Jo) 1 g
CA&apos;1âI a.,â; â
0 0 ed
0
âª xi
(4v)
,-â 
nj bp U a) cu j4) .. 4
z LO .--1 cj
Â° â¢ 5 Z &gt;i 7:5 4 0 Z CI) a)
&apos;d .8 E at
Cll 0
PT-I U 4 Eââ  &lt; 124 Cr) v. &gt; u-, cn
C,)
(,
a) rn
a,
:d
o .4-.
-,-.
a) ,0
a) as u
,1-1).cu a)
0 Ts
z .__,
ec ;... --.
u a:
o TA
a4 at ca ....
...., -ti
as -a .)
</figure>
<equation confidence="0.973428615384616">
En tn 0
Cl) ()N 1W
-0 ..- 0) t u
) .
its -d&apos;t
s.., o CU 1-1 4
.Â°.&apos; _ z :El Fk
Oâ¢ cd ,
a cb 4 bi, 0, â
-0 iâ - &lt;i. ,c) 5 6,3 &apos;-b. 6
z 0 03 ,4-1
bt &amp;quot;a&amp;quot; 4 az -I-. &lt;10
vi a; (t
,P ., .
Cl)Ts (4.â¢ 0 a) g
Cl) &gt; &amp;quot;0 g 1),,, -85 &apos;4&amp;quot;
.. b.0
0 .... bo 1:1) Z
â â¢â¢â¢â  â¢-â  Ts a) g at
&gt; ...J
Lr) N. co
2
aT 751 go
Ln .1) es&apos; Ln NI
(NI en N M .71 m &amp;quot;
2 2 cs,
</equation>
<table confidence="0.940513285714286">
LC) â¢41
Gloss for CILIN Classes
CILIN Class Gloss for LLOCE Classes
LLOCE Class
oo
o
go al
</table>
<page confidence="0.987017">
326
</page>
<note confidence="0.785037">
Sue J. Ker and Jason S. Chang Word Alignment
</note>
<equation confidence="0.9638622">
Class (Ca005) = {Mrs, Ms, broad, dame, female, girl, lady, Madam, missis, miss,
missus}
Class (Ab01) = Y Â±ft , A ,ftÂ± , tCP, tcd,
/JAI A A , tt , , 14303, MI, !UM, Elk ,
5M, 93A, 93k, 937-, %TM, 92,k, tt-M, L492.tc,
From (x, Ab01) = 1, for all x E {dame, female, girl, lady, Madam, miss}
To (Ca005, x) = 1, for all XE {kA, k, tcÂ±, NE, AA}
7 + 6
ClassSim(Ca005, Ab01) = = 0.13
11 + 92
</equation>
<figureCaption confidence="0.93276">
Figure 1
</figureCaption>
<bodyText confidence="0.9964827">
The word classes Ca005 and AbOl and their conceptual similarity.
The main step in ClassRule is illustrated through the calculation of the ClassSim
value between LLOCE topical set Ca005 (kinds of woman) and CILIN category Ab01
(man and woman, Y3A , tc A, 93k). For simplicity, the unweighted value of Class-
Sim(Ca005, Ab01) is calculated. Ca005 and Ab01 contain 11 and 92 words, respectively.
In ALLCONN, six words in Ca005, i.e. dame, female, girl, lady, Madam, and miss, trans-
late into words listed under Ab01. In the other direction, seven words listed under
Ab01, i.e. kA, k, aft tcÂ±, Ng, and are the translations from words
listed under Ca005. Thus, the ClassSim value between Ca005 and Ab01 can be valued
at (7 + 6)/(11 + 92) = 0.13. Figure 1 presents further details.
</bodyText>
<subsectionHeader confidence="0.994028">
3.3 Class-based Word Alignment
</subsectionHeader>
<bodyText confidence="0.9999926">
The proposed alignment algorithm ClassAlign is based on the following observations:
First, dictionary translations can be used to produce high-precision connections. Thus,
DictAlign should be employed to produce initial connections whose translations ex-
hibit a high similarity to a DT. That is, a relatively high threshold, 0.7 should be used.
Second, the class-based rules acquired through the ClassRule Algorithm should cap-
ture the diversity of translations to a large extent. According to the observations in
Section 2, the rules should stipulate most of the connections left out in the DictAlign
step. Nevertheless, conflicting connections do occasionally arise. Such conflicts can be
resolved according to an additional consideration of distortion mentioned in Section 1
Estimating the Likelihood of a Connection Candidate. The above observations can be stated
formally from the perspective of Brown et al.&apos;s (1993) Model 2. As mentioned earlier,
the model stipulates that a connection be given a probability value Pr(s, t), the product
of lexical translation probability t(s I t) and distortion probability, d(i I j, 1, m). Also
according to the model, we give each connection candidate a probabilistic value based
on lexical and positional considerations:
</bodyText>
<equation confidence="0.891936">
Pr(s, t) = t(s, t) x d(i, j) (6)
</equation>
<bodyText confidence="0.996883666666667">
We argue, however, that it is difficult to robustly estimate t(s, t) and d(i, j) for all the
values of s, t, i, and j. Therefore, the two functions are defined and estimated by a
limited number of cases, according to lexical, conceptual, and positional conditions.
</bodyText>
<page confidence="0.980522">
327
</page>
<note confidence="0.468363">
Computational Linguistics Volume 23, Number 2
</note>
<bodyText confidence="0.630578">
For this purpose, we define conceptual similarity between s and t as follows:
</bodyText>
<equation confidence="0.7743545">
ConceptSim(s, t) = max ClassSim(X, Y)
sEX,tEY
</equation>
<bodyText confidence="0.9412248">
Lexical and conceptual conditions are set up based on DTSirn and ConceptSim, while
positional conditions are set up based on dislocation, a distortion measure relative to
both left and right context.
Estimation of LTP Based on Lexical and Conceptual Conditions. The LTP t(s, t) is defined by
the following cases:
</bodyText>
<tableCaption confidence="0.781809">
Case 1. Connection (s, t) exhibits high lexical and conceptual similarity,
i.e., ConceptSim (s, t) &gt; h1 and DTSim(s, t) &gt; h2.
Case 2. Connection (s, t) exhibits high conceptual similarity i.e.,
ConceptSim (s, t) &gt; h1 and DTSim(s, t) &lt;h2.
Case 3. Connection (s, t) exhibits high lexical similarity i.e.,
ConceptSim (s, t) &lt;h1 and DTSim(s, t) &gt; h2.
Case 4. Otherwise, ConceptSim (s, t) &lt;h1 and DTSim(s, t) &lt;h2.
</tableCaption>
<bodyText confidence="0.9938706">
The connections satisfying each condition are given the same probability value
determined by maximal likelihood estimation (MLE). For instance, if there are k true
connections in a sample of n candidates (s, t) such that ConceptSim(s, t) &gt; h1 and
DTSim(s, t) &gt; h2, then all these candidates are given the same MLE value for LTP, i.e.,
t(s, t) = t1 = k/n. Equation (8) sums up the above discussion:
</bodyText>
<table confidence="0.906292">
1 tl if ConceptSim (s, t) &gt; hl and DtSim(s, 0 &gt; hz, (8)
t2 if ConceptSim (s, t) &gt; h1 and DtSim(s, t) &lt;h2,
t(s&apos; t) = t3 if ConceptSim (s, t) &lt;h1 and DtSim(s, t) &gt; hz,
t4 if ConceptSim (s, t) &lt;h1 and DtSim(s, 0 &lt;h2.
</table>
<bodyText confidence="0.981772125">
Estimation of Distortion Probability (DP). In a similar fashion, we formulate the distor-
tion function by cases related to the monotonicity of translational position with respect
to context. Such a formulation is inspired by Gale and Church&apos;s (1991b) treatment of
distortion. In their study, the authors replace the distortion probability with a proba-
bility function defined by different values of slope, a measure of the position of t with
respect to the left context of s. This measure is generally quite accurate, leading to
a distribution function concentrating at slope 1. Nevertheless, room for improvement
still exists, as can be illustrated using the concept of a binary inversion transduction
tree (ITT) proposed by Wu (1995). The ITT is a shared parse tree depicting the struc-
tural difference between a sentence S and its translation T. Figure 2 presents the ITT
of (E12, C12). The horizontal bar denotes that the noun phrase such a lazy mortal and
the prepositional phrase as you are inverted when translated into Mandarin. The slope
of the first word in such an inverted structure is typically quite large, making the dis-
tribution of the slope function slightly flat. If multiword structural inversion occurs,
as it frequently does, then the slope of the first word according to the right context is
still very small.
</bodyText>
<equation confidence="0.914759">
(E12) Ii &apos;ve2 never3 known4 such5 a6 lazy7 mortal8 as9 youio .11
(C12) i ttEV2 NA4 5 16 ZIP7 Vi8 irg9 A10 0 11
(7)
</equation>
<page confidence="0.992856">
328
</page>
<note confidence="0.959907">
Sue J. Ker and Jason S. Chang Word Alignment
</note>
<tableCaption confidence="0.997272">
Table 8
</tableCaption>
<table confidence="0.891190428571429">
Alignment connections for example (E12, C12).
i 0 1 2 3 4 5 6 7 8 9 10 11
s $ I &apos;ve never known such a lazy mortal as you .
t $ fil tT3K RI&apos; RA Ziff VA Erg A
j 1 2 3 4 7 8 9 10 5 6 11
&apos;ye never known such a lazy mortal as you
It*- 5iA&amp;quot; ki41 im(v3) A.fg
</table>
<figureCaption confidence="0.875945">
Figure 2
</figureCaption>
<bodyText confidence="0.986956857142857">
ITT of the example-translation pair (E12, C12).
For instance, the first word as in the inverted structure as you has a high slope value
with respect to its left context such a lazy mortal. However, since the words as and you
translate into the fifth and sixth words in (C12), the word as has a slope value of 1
with respect to its right context, you.
We believe that by considering the translational position relative to both the left
and right contexts, one obtains a distribution function with a smaller deviation, thereby
making a tighter estimation possible for d(i,j). To this end, we define dislocation, dis,
for the connection (s, t) of the ith and jth words in S and T to denote 1(1âi&apos;) â (i â
where i&apos; is the position of a word s&apos; sharing the minimum syntactic structure with
s, and s&apos; translates into t&apos;, the j&apos;th word in T. Short of syntactic analysis, dis(i, j) can
be calculated with respect to a nearby connection in CONN, the initial connections
established by DictAlign. Such treatment closely approximates the dislocation value.
In light of this, dislocation can be defined as follows:
</bodyText>
<equation confidence="0.786742">
t min( IA I, IdR I) otherwise,
- if 3(f)(i, j&apos;) E CONN, (9)
</equation>
<bodyText confidence="0.924554333333333">
where i = the sequence number of s in S,
the sequence number of t in T,4
dt, =
</bodyText>
<equation confidence="0.995082666666667">
dR =
argmax(P,P)ECONN&lt;;
(iRjR) =
ar-g
min(,p)E CONN,i
CONN&lt; = {(k, 1) I kth and /th word in (S, T) form a connection in CONN, k &lt;
</equation>
<page confidence="0.995442">
329
</page>
<figure confidence="0.99927">
Volume 23, Number 2
Computational Linguistics
&apos;ve
never j known such
a
lazy
mortal
as
y
2
6
5
4
3
11
5
10
8
1
3
2
4
7
5
4
1
10
.1;t4E
2
1 :
0
1
2
3
4
6
8
3
Nâ¢
1St
4
3
5
2
0
1
7.
6
5â
2
1
0
fB
6
5
3
2
1
0
0
2:
1
6 4 3 2
0
1
3
2
7 5 4 3
8 6 5 4
3
2
1:
3
2
2 1
10
4
7
6
5
1
3 2
10 8
</figure>
<figureCaption confidence="0.961888">
Figure 3
</figureCaption>
<bodyText confidence="0.918713166666667">
Dislocation values for the example-translation pair (E12, C12). Each connection candidate in
(E12, C12) is represented as a cell. The connections in CONN are shown as a bold face 0. Each
of these zero dislocation values extends vertically, incrementing by one for each upward or
downward move, resulting in a shaded vertical bar of dislocation values. All other
connections take their dislocation values from the minimum diagonal projection of the related
cell on the two bounding bars. For instance, the projections of the connection (such, aff
(shaded in figure) on the left and right bounding bars (/-connections and lazy-connections) are
2 and 1, respectively. Therefore, the dislocation value is 1, the minimum of 2 and 1.
CONN&gt;, = {(k, 1) I kth and /th word in (S, T) form a connection in CONN, k&gt; ,
CONN = the initial connections established according to DT.
The distortion function defined by cases can now be given according to dislocation
values.
</bodyText>
<equation confidence="0.9978346">
d1 if dis(i, j) = 0,
d( = d2 if dis(i, j) = 1, (10)
i,j)
d3 if dis(i, j) = 2,
d4 if dis(i, j) &gt; 3.
</equation>
<bodyText confidence="0.999929727272727">
The connection candidates with small dislocation values tend to be true connec-
tions. For instance, 8 out of 15 zero-dislocation connections for (E12, C12) are correct,
while only 1 out of 20 candidates with a dislocation of 1 is a true connection. All
candidates with dislocation values greater than 1 are false. Figure 3 provides further
details. Again, all connections satisfying a certain case in equation (10) are given the
same MLE value. For instance, if there are k true connections in a sample of n can-
didates (i,j) with 0 dislocation, then all of these candidates are given the same MLE
value for DP, i.e., d(i, j) = d1 = k/n for all i and j such that dis(i, j) = 0.
By using a small sample of 200 sentences from the LecDOCE, the LTP and DP
values ti and d, for 1 &lt; i &lt; 4 can be estimated by the maximum likelihood princi-
ple. Tables 9 and 10 summarizes the MLE probabilistic values associated with lexical,
</bodyText>
<footnote confidence="0.4690865">
4 The sequence number of Mandarin words is assigned according to the segmentation that satisfies the
long-word-first heuristic and is consistent with the established connections in CONN.
</footnote>
<page confidence="0.977708">
330
</page>
<figure confidence="0.968408428571429">
11
7
6
5
4 3
1
0
</figure>
<tableCaption confidence="0.6367825">
Sue J. Ker and Jason S. Chang Word Alignment
Table 9
</tableCaption>
<table confidence="0.889122071428571">
Maximum likelihood estimation (MLE) of LTP.
Conceptual and Lexical Condictions # Candidates # True MLE of t(s, t)
Connections
ConceptSim(s, t) &gt; 0.05 and DTSim(s, t) &gt; 0.3 508 481 ti 0.947
ConceptSim(s, t) &gt; 0.05 and DTSim(s, t) &lt;0.3 167 84 t2 0.503
ConceptSim(s, t) &lt;0.05 and DTSim(s, t) &gt; 0.3 1,589 499 t3 0.193
ConceptSim(s, t) &lt;0.05 and DTSim(s, t) &lt;0.3 14,687 165 Li 0.011
Table 10
Maximum likelihood estimation (MLE) of DP.
Dislocation # Candidates # True Connections MLE of d(i, j)
0 2,158 893 d1 0.414
1 3,445 210 d2 0.061
2 2,805 31 d3 0.011
3 9,543 95 d4 0.010
</table>
<bodyText confidence="0.813058">
conceptual, and positional factors. The above description of word alignment is sum-
marized as the ClassAlign algorithm.
</bodyText>
<figureCaption confidence="0.827110153846154">
Algorithm 3. (ClassAlign) Class-based word alignment for a pair of sentences (S. T).
Step 1: Tag each word in S with POS information and convert each word to the
root form to obtain the set Ws of words in S.
Step 2: Initialize the result ANS to an empty list. Run DictAlign on (S, T) to obtain
a list of initial connections, CONN.
Step 3: Look up the dictionary to obtain the set WT of possible words in T.
Step 4: For each connection candidate (s, t) E Ws X WT, compute Pr(s, t) according
to equations (6) through (10).
Step 5: Add to ANS the connection (s*, t* ) that maximizes Pr(s, t) over all
s, t E Ws X WT with a value greater than h3.5 Remove all conflicting
candidates involving s* and t* from subsequent consideration. This step
repeats itself until the candidates run out or every remaining candidate
(s, t) is associated with a Pr(s, t) value lower than h3.
</figureCaption>
<bodyText confidence="0.678213">
Step 6: Output ANS as the final result of word alignment.
</bodyText>
<subsectionHeader confidence="0.999108">
3.4 An Illustrative Example
</subsectionHeader>
<bodyText confidence="0.999685">
In the following, we demonstrate how ClassAlign works using example (E10, C10),
reproduced below with the sequence number of each word denoted by a subscript
number.
</bodyText>
<equation confidence="0.7277045">
(E10) Thei old2 lady3 was4 clads in6 a7 fur8 coat9 .10
(C10).-2 a- A4 6 5 o 8
</equation>
<bodyText confidence="0.862464666666667">
As demonstrated earlier in Section 3.1, DictAlign produces connections (old, and
(clad, 0*) from (E10, C10) using a threshold value of 0.7 for DTSim. Table 11 lists
5 Ties are resolved in favor of the longer, leftmost Mandarin word.
</bodyText>
<equation confidence="0.98108575">
dis --=
dis =
dis =
dis &gt;
</equation>
<page confidence="0.993677">
331
</page>
<figure confidence="0.668227433333333">
Computational Linguistics Volume 23, Number 2
Table 11
Classes listed in LLOCE for Ws in (E10).
Word POS Classes in LLOCE
the det Gh285
old adj Lg200, Lg208, Lh241
lady n Ci157, Ci158, Ci160, Ca005
be v Aa001, De080, Li273, Na001, Nf159
clad adj Dg136
in prep Li272, Mh204
a det Nd098
fur n Hc088
coat n Dg142, Hc093
Table 12
Classes listed in CILIN for WT in (C10).
Word POS Classes in LLOCE
ra det Ed61
n Ca31, Ka08
42 n Cb01, Di15, Di17
42 cl Dn08
t n Ab02
â¢ adj Eb15, Eb24, Eb29, Eb36, Ec05, Ed51, Ee21
WA n AbOl
a n Ab01, Ah15
A n Aa01, Dd17, De01, Dn03
0* v Fa18
n Bb04, Bc02, Bk10, Bm10, Bm13
â¢ adj Ee09
n Bq03
1 cl Dn08
</figure>
<bodyText confidence="0.967629">
the Ws words and their relevant POS and topical sets in the LLOCE. Table 12 displays
the WT words and their relevant CILIN categories. Table 13 presents the dislocation
values for all connection candidates in Ws x WT. The cells with a boldface 0 in Table 13
represent the initial connections in CONN and two dummy connections placed at the
beginning and end of both sentences. Table 14 lists the connection candidates with
higher Pr(s, t) values.
After executing Step 5, ClassAlign selects the candidates, (lady, A),6 (fur,
), both with Pr(s, t) value of 0.392, in terms at Step 6. These connections are added
to ANS and the conflicting candidates such as (old, VA), (old, A), (old, NA),
(lady, N), (lady, A), (in, (coat, &amp;), (fur, I), etc. are removed. In the subsequent
iterations, connections (coat, 1) (Pr(s, t) = 0.208), (old, (Pr(s, t) = 0.080), (clad, V*)
(Pr(s, t) = 0.080), and (The, 2)7 (Pr(s, t) 0.005), are selected. ClassAlign stops after
running out of connections with a probabilistic value greater than h3, 0.005. Table 15
summarizes the connections chosen to form the solution. The success rate is evaluated
</bodyText>
<footnote confidence="0.860415">
6 This candidate ties with (lady, NV). The conflict is resolved in favor of the longer Mandarin word.
7 This candidate ties with (The, 42). The conflict is resolved in favor of the leftmost Mandarin word.
</footnote>
<page confidence="0.997487">
332
</page>
<bodyText confidence="0.9990558">
according to how many English words are correctly aligned.&apos; Evaluation is based on
A word not given a connection is considered a failure if it should be connected to a
100% coverage, i.e., each word in the English sentence is checked for correct alignment.
Mandarin word; otherwise it is considered a success. For this example, all nine words
are aligned correctly. Therefore, the success rate is 9/9 = 100%.
</bodyText>
<sectionHeader confidence="0.830134" genericHeader="method">
4. Experiments with ClassAlign
</sectionHeader>
<bodyText confidence="0.99953525">
To assess the proposed method&apos;s effectiveness, we have implemented the algorithms
described in Section 3 and conducted a series of experiments. Tests are performed on
the sentences found in the LecDOCE and a user&apos;s manual available in both languages
to assess the method&apos;s robustness and generality. The similarities and differences be-
tween English and Mandarin texts are briefly reviewed, since our experiments involve
the alignment of English-Mandarin parallel corpora. A general description of the ma-
terials used in the experiments follows. Finally, the success rates are quantitatively
evaluated.
</bodyText>
<subsectionHeader confidence="0.999799">
4.1 Contrastive Analysis of English and Mandarin Chinese
</subsectionHeader>
<bodyText confidence="0.953661583333333">
Language typology is the study of similarities and differences between languages,
formalized in terms of parameters such as word order and morphological structure.
Li and Thompson (1981) examine Mandarin Chinese according to four typological
parameters that reveal the basic structure of Mandarin Chinese as compared to those
of other languages, English in particular. These four parameters are the morphological
structure of words, the number of syllables per word, topic prominence, and word
order. Li and Thompson&apos;s typological description of Mandarin is described below,
from the perspective of the task of word alignment.
8 A small percentage of connections (7.8%) in our evaluation are incomplete ones and are considered to
be correct. Melamed (1996) takes the same stance in his study of deriving a probabilistic lexicon. He
observes that even incomplete entries are useful for many applications and there are ways of expanding
incomplete morphemes or words in a connection, so that they become complete (Smadja 1992).
</bodyText>
<page confidence="0.990818">
333
</page>
<note confidence="0.880218">
Sue J. Ker and Jason S. Chang
</note>
<figure confidence="0.8765794">
Word Alignment
dad
in
Table 13
Dislocation values for connection candidates (s, t) in (E10, 00).
The
old:
lady
a
coat
fur
was
1
3
4
4
5
6
7
0
2
3
3
4
5
6
0
2
2
3
4
5
1
0
1
2
3
4
2
0
0
0
2
3
V*
3
0
1
0
2
1
4
2
0
0
1
A
5
3
2
0
0
6
1
0
1
2
3
4
Computational Linguistics Volume 23, Number 2
Table 14
The connection candidates (s, t) in (E10, C10) with higher Pr(s, t) values.
Connection Candidates Lexical Translation Probability Distortion Prob TP
i j s t ConceptSim(i, j) DTSim(i, j) t(s, t) dis(i, j) d(i, j) Pr (s, t)
1 1 The tri o o 0.011 0 0.414 0.005
1 2 The 42 0 0 0.011 0 0.414 0.005
2 3 old 0 0.74 0.193 0 0.414 0.080
2 3 old VS 0 0.51 0.193 0 0.414 0.080
2 3 old tW A 0 0.42 0.193 0 0.414 0.080
3 3 lady VG 0 0.30 0.193 0 0.414 0.080
3 4 lady 44A. 0.21 0.39 0.947 0 0.414 0.392
3 3 lady tO A 0 0.31 0.193 0 0.414 0.080
3 4 lady a 0.21 0.34 0.947 0 0.414 0.392
3 4 lady A. 0 0.56 0.193 0 0.414 0.080
4 3 was g 0 0 0.011 0 0.414 0.005
4 4 was NA o o 0.011 0 0.414 0.005
5 5 clad 0 0 0.71 0.193 0 0.414 0.080
5 5 clad OM 0 1.00 0.193 0 0.414 0.080
5 5 clad V 0 0.62 0.193 0 0.414 0.080
6 6 in A o o lion 0 0.414 0.005
7 7 a 1 0 0 0.011 0 0.414 0.005
8 6 fur a 0.28 0.67 0.947 0 0.414 0.392
9 6 coat a 0 0.31 0.193 1 0.061 0.012
9 7 coat I 0.14 0 0.503 0 0.414 0.208
Table 15
Final alignment of example (E10, C10). Initial alignment connections are shown in shaded cells.
0 1 2 3 4 5 6 7 8 9 :10:
The old lady was clad in a fur coat
A
0 1 2 3 4 5 6 7
</figure>
<bodyText confidence="0.996295076923077">
Morphological Structure of Words. The most striking feature of Mandarin as compared to
English is the relative simplicity of word structure. That is, most Mandarin words are
comprised of a single morpheme rather than a stem morpheme and a suffix serving
grammatical functions such as case (as in Turkish and Japanese), number, agreement, or
tense (as in many other languages including English). Mandarin verbs do have aspect
morphemes, including 7 (-le, perfective), (-guo, experienced action) and (-zhe,
durative). Other grammatical functions are either non-existent or expressed through
an additional function word. In contrast to this lack of inflectional morphological
complexity, Mandarin is relatively rich in other types of morphological combinations,
including compounding.
These morphological differences result in a difference in the number of words in
an English sentence and its Mandarin translation. In terms of alignment, this word-
number difference means that multiword connections must be considered, a task which
</bodyText>
<page confidence="0.995934">
334
</page>
<note confidence="0.824327">
Sue J. Ker and Jason S. Chang Word Alignment
</note>
<bodyText confidence="0.997560727272727">
is beyond the reach of methods proposed in recent alignment works based on Brown
et al.&apos;s (1993) Model 1 and 2.
Basic Orientation of the Sentence: Topic vs. Subject. Another feature distinguishing Man-
darin from other languages is topic prominence. In addition to the grammatical rela-
tion of subject, a description of Mandarin must include the topic element, which can
be characterized as follows: First, a topic always comes first in the sentence and is op-
tionally followed by a pause in speech. Second, a topic is the old information of which
both the speaker and listener have some knowledge. Third, what distinguishes a topic
from a subject is that the subject must always have a direct syntactic and semantic
relation with the verb, but the topic does not need to. For instance, in the sentence
(E13, C13), the first word (daxiang, &apos;elephant&apos;) is the topic and the second word
07- (bizi, &apos;nose&apos;) is the subject; t* &apos;elephant&apos; is the focus of the discourse, but it is
the subject *7- &apos;nose&apos; that is very long; not t91 &apos;elephant&apos;.
(E13) The elephant has a very long nose.
(C13) tiP. -no
Daxiang bizi hen chang
Elephant nose very long
The topic prominence of Mandarin sentences represents alignment connections
with a large distortion in position, leading to difficulty in estimating the likelihood of
a connection according to translational position.
Word Order. Greenberg (1963) stated that the world&apos;s languages fall into three word
order groups according to the order of the subject (S), verb (V), and object (0) in a
simple transitive sentence. A language, in general, belongs to one of three basic word
order types, SVO, SOV, and VSO. By this notion, English is an SVO language in which
the verb typically follows the subject and precedes the object. For most languages,
other aspects of word order, such as that of modifier and modified elements, correlate
with the order of V and 0. However, Mandarin is not an easy language to classify
according to this typology for a number of reasons. First, the notion of subject is not
well-defined. Second, unlike in English, word order in Mandarin is not determined
solely on grammatical grounds but rather depends on semantics. For instance, whether
an adverbial expression appears in pre- or postverbal position depends on subtle
semantic differences. More specifically, a time phrase in preverbal position tends to
denote punctual time, while that in postverbal position signals durative time, as in:
</bodyText>
<equation confidence="0.741483">
(E14) I have a meeting at three o&apos;clock.
(C14) ft El. FM
I three o&apos;clock have-a-meeting
(E15) I slept for three hours.
(C15) Tffig.
I sleep ASPECT three o&apos;clock
(C15&apos;) fil .5-61 IM To
I three o&apos;clock sleep ASPECT
</equation>
<bodyText confidence="0.964927333333333">
In contrast, both kinds of time phrase appear in postverbal position in English. As a
result of facts such as these, many linguists contend that Mandarin is a language in
transition from SVO to SOV. Further details can be found in Li and Thompson (1981).
</bodyText>
<page confidence="0.996684">
335
</page>
<note confidence="0.739388">
Computational Linguistics Volume 23, Number 2
</note>
<bodyText confidence="0.99948325">
Similar to the situation created for topic prominent sentences, the SOV features
of Mandarin represent a deviation from the SVO order of English. Such a deviation
further worsens our ability to estimate the likelihood of a connection according to
translational position.
</bodyText>
<subsectionHeader confidence="0.998374">
4.2 The Experimental Setup
</subsectionHeader>
<bodyText confidence="0.99994475">
The experimental results obtained from the proposed algorithm with respect to word
alignment are presented in this section. Nearly 42,000 example sentences and their
translations from the LecDOCE were used as training data, primarily to acquire rules
and to determine MLE estimates for the cases of LTP and DP. The algorithm&apos;s per-
formance was evaluated using the two sets of data. The closed test set consists of
200 examples and their Mandarin translations randomly selected from the LecDOCE.
The English examples range from 8 to 23 words long; average example length is 11.5
words. There are, on average, 1.56 inversions per example-translation pair. The open
test set consists of 200 sentences randomly drawn from the English and Chinese ver-
sions of the LightShip User&apos;s Guide. The English sentences in this test set range from
4 to 34 words long; average sentence length is 11.8 words. There are, on average, 1.60
inversions per sentence pair. Table 16 provides some examples from the LightShip
User&apos;s Guides.
The two thesauri, LLOCE and CILIN, are used as the classification systems of
source and target words. The LLOCE contains 23,769 entries and CILIN contains 63,754
entries. Both thesauri cover just over 90% of the words in the test sets.
</bodyText>
<subsectionHeader confidence="0.997215">
4.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.999986428571429">
The first three experiments were designed to demonstrate the effectiveness of the
naive DictAlign algorithm based on a bilingual MRD. According to the experimental
results, although DictAlign produces high-precision alignment, the coverage for both
test sets is below 30%. However, if the thesaurus effect is exploited, the coverage can
be increased considerably, at the cost of a decrease of less than 4% in precision. Table 17
provides further details.
In the fourth experiment, the ClassAlign algorithm is employed to align both sets
of test data again. Table 18 reveals that the acquired conceptual information compen-
sates for what is lacking in the LecDOCE to yield optimum alignment results. The
ClassAlign algorithm expands coverage almost twofold to over 80%, while maintain-
ing the same level of precision. The generality of the approach is evident from the
open test&apos;s comparably high coverage and precision rates. As shown in Table 18, over
80% of the source words in both test sets are connected to a target and over 90% of
the connections are true ones.
</bodyText>
<sectionHeader confidence="0.995062" genericHeader="method">
5. Discussion
</sectionHeader>
<bodyText confidence="0.999682">
This section thoroughly analyzes the alignment results from the experiments described
in Section 4 and, in particular, the data relating to cases where the algorithms failed.
Analytical results demonstrate the strengths and limitations of the methods and sug-
gest possible improvements to the algorithms.
</bodyText>
<subsectionHeader confidence="0.999295">
5.1 Compounding in Mandarin
</subsectionHeader>
<bodyText confidence="0.997393">
As stated earlier, the compounding effect in Mandarin frequently results in a change
in the number of words between an English sentence and its Mandarin translation.
The correct alignment decision for a Mandarin compound frequently involves more
than one English word. ClassAlign often fails under such circumstances. For instance,
</bodyText>
<page confidence="0.99597">
336
</page>
<note confidence="0.824508">
Sue J. Ker and Jason S. Chang Word Alignment
</note>
<figure confidence="0.77858925">
C
0
a)
Mandarin Sentence
English Sentence
1--1 â¢ 1-1 1-1 â ââ  1-1 0 Cr)
o
ifq
2
ig
1 IV
1* IX
4T-1, tat
tg
EEE
-3th g
</figure>
<equation confidence="0.8983705">
711N *( LC 41-
ot ifigÂ°
tni z,k,u
292 Lts +1-+Eigt
fp, tio.&amp;quot;4,40014g eE .1,&lt;
W, {Mt `;1%all Tiki .2d-.&lt; tat .1X
54140:0S 0.144V cut3E ;47. 1
Sit a; 1.1E1= tialaleniqth te JAE gPica g
*ROUT 112&amp;quot;11EVIC r.Jg RE
-1410[141r-CiffMN4n4liamitt 11#14
5 o 0 z cu c.&apos; z
pox 4-. 0 .a.) s..) 0
</equation>
<bodyText confidence="0.920876">
O 0 F. .0
o &amp;quot;0 C, o r., ccA o w
,i, 0, 7:,. bo cz a, .5 a) as
</bodyText>
<figure confidence="0.773962102564103">
5 â¢E u - ,5 a&gt;
Sc.1. sy ,1 &amp;quot;Iii.I CZ &apos;1) Z 0 CY, â  .15. 1 , Z , 5
5 &apos;7.)
0
â &lt; ,- 04 0
,-P .ao-tiu ma,.-,-..
2;-. CI) 4-&apos; Q) 6)(1) -6, 0 CL) 0 2 , M LO
Â° &amp;quot;0 Ez4 to 7) u&apos;.â¢ ;!.. a) co 4 ca ;-
0
Q) 0 5 75
.5 &apos;t 2,15otdcD&amp;quot;&apos; t5o 4 (1) c&apos; .-. U Q.) â¢ c.,
2 sc; 0 24 , )9:I) it t);
t) cu2 a, ;... ,, ),:s ms 7 o â¬tstuu-2,4-c7, 4
cz
4 0 â .5
&apos;..&apos; &apos;,... U Cl) uâ _110&amp;quot;--. tar .-; FID Z iiir-i-&apos; -,-. aJ 03 0 -6-.
0 â u.,_,â¢,-. c.., c...) ,, &gt; 0 0 a) a)
.. 0 --. 0 0 0 0 a) .co ti) 0.a) a)
Cl)
(6 âº, ,-. z 4 0 Cao ,_i 6 &apos;
,. â&apos;,;, ,
.
Q) Z cy,
-
9, âª .5 ;YE .4, `&apos;&amp;quot;
&lt;a âª a) 8 .R. c, Ow
a J:7o
,s 0 â¢ 0., bio 0 z o
1-â; .,... t)
&amp;quot;i5 a. w .-
zâ¢
4., . â¢ ..1 Z ..0 CI) 71
JP..
z 4 g&apos;,0 ;4 -0 Â° Zi __,,a Â§ &gt;
o a.) -5 0 5
-0
Cl) . :1) id â¢ Â° â¢
a râ¢ .), L&apos; .0 -4-. &lt;
r..., au ,...
</figure>
<page confidence="0.735769">
337
</page>
<note confidence="0.43665">
Computational Linguistics Volume 23, Number 2
</note>
<tableCaption confidence="0.979438">
Table 17
</tableCaption>
<table confidence="0.956633444444444">
Experimental results for DictAlign.
Test Set #1: LecDOCE Examples Test Set #2: LightShip Manual
# Matched # Correct Coverage Precision # Matched # Correct Coverage Precision
DictAlign 525 505 28.8% 96.2% 604 576 25.6% 95.4%
(DTSim = 1.0) 808 755 44.3% 93.4% 767 705 32.5% 91.9%
DictAlign 937 822 51.4% 87.7% 1023 825 43.4% 80.7%
(DTSim &gt; 0.7)
DictAlign
(DTSim &gt; 0.5)
</table>
<tableCaption confidence="0.991695">
Table 18
</tableCaption>
<table confidence="0.9379956">
Experimental results for ClassAlign.
Test Set #1: LecDOCE Examples Test Set #2: LightShip Manual
# of Words # Correct Coverage Precision # of Words # Correct Coverage Precisic
All words 1,823 1,460 100% 80.1% 2,359 1,800 100% 76.3%
Matched words 1,561 1,460 85.6% 93.5% 1,965 1,800 83.3% 91.6%
</table>
<bodyText confidence="0.9206595">
ClassAlign incorrectly connects the compound gni in (C16) to a single English word
company according to the alignment rule (Co292, Dm07).
</bodyText>
<equation confidence="0.837529">
(E16) She is a star with the theatre company.
(C16)kt1161.11111ErgarM0
</equation>
<bodyText confidence="0.9995995">
Other methods for aligning English and Mandarin texts in the literature also fall
prey to the problem of Mandarin compounds. For instance, the following partially
correct connections complicated by compounding are reported in a recent study on
alignment of Hong Kong Basic Law (Fung and McKeown 1994).
</bodyText>
<equation confidence="0.810081666666667">
(E17) monoxide
(C17) ---xtg (&apos;carbon monoxide&apos;)
(E18) Basic
(C18) M2MA (&apos;Basic Law&apos;)
(E19) second
(C19) .7_1411 (&apos;second reading&apos;)
</equation>
<bodyText confidence="0.9738825">
Because it is not limited to the connections involved in a presegmented target sen-
tence (Fung and McKeown 1994; Wu and Xai 1994), ClassAlign avoids most instances of
these errors. In addition, with elaborate preprocessing such as parsing, phrase group-
ing, and collocation analysis (Smadja 1992), the problem of word-number difference
</bodyText>
<page confidence="0.993037">
338
</page>
<note confidence="0.972001">
Sue J. Ker and Jason S. Chang Word Alignment
</note>
<tableCaption confidence="0.997175">
Table 19
</tableCaption>
<table confidence="0.9784648">
The final alignment of example (E20, C20).
1 2 3 4 5 6 7 8 9 10
He abdicated all responsibility for the care of the child
fft lil* ----EZ *11 7
1 2 8 9 3 4 5 â¢7â¢ 6
</table>
<bodyText confidence="0.926871666666667">
can be averted by performing alignment at various levels: parse tree (Matsumoto,
Ishimoto, and Utsuro 1993; Meyers, Yangarber, and Grishman 1996), phrase (Kupiec
1993), and collocation (Smadja, McKeown, and Hatzivassiloglou 1996).
</bodyText>
<subsectionHeader confidence="0.99697">
5.2 Function Words, Collocation, and Free Translation
</subsectionHeader>
<bodyText confidence="0.996868333333333">
Language-Specific Function Words. The morphological differences between English and
Mandarin give rise to many language-specific function words. Such Mandarin function
words are often quite ambiguous in part of speech as well as in word sense, leading to
numerous alignment errors. For instance, ClassAlign connects the words for and of in
(E20) erroneously to the morphemes 7 and Erg in (C20), respectively. Table 19 presents
further details.
</bodyText>
<equation confidence="0.5962095">
(E20) He abdicated all responsibility for the care of the child.
(C20) ittik*TRINAMNAVâVA4-10
</equation>
<bodyText confidence="0.998308">
Collocation. As mentioned in the previous section, collocation is one of the reasons
why in-context translation usually deviates from the dictionary translation. However,
unlike other deviations, bilingual collocation is not easily bounded within a couple
of classes. For instance, the translation for take (Mb051, carrying, taking and bring) in
the collocation take effect is usually Si (&apos;see&apos;) (Fc04, seeing and looking), as in example
(E21, C21). However, there is insufficient evidence to support a class-to-class mapping
from Mb051 to Fc04. In any case, deriving the Mb051-to-Fc04 mapping would be an
overgeneralization.
</bodyText>
<equation confidence="0.418488">
(E21) How soon does the medicine take effect?
(C21) At01,Zt?
</equation>
<bodyText confidence="0.9994378">
Paraphrased and Free Translations. For various reasons, such as language typology, style,
and cultural differences, a translator does not always translate literally on a word-
by-word basis. Adding and deleting words is commonplace, sometimes resulting in a
paraphrased or free translation. Such translations obviously create problems for word
alignment. For instance, in example (E24, C24), only one word, I, is translated literally,
into ft . The main verb angle in example (E25) is given a paraphrased translation
&amp;Wit (&apos;to change the angle&apos;). The noun phrase the people she is speaking to in (E25)
is paraphrased as E* &apos;audience.&apos; A significant amount of free translation arises due
to the use of four-morpheme Mandarin idioms for stylistic reasons. For instance, the
clause as long as I breathe in (E22) translates into an idiom and the sentence
</bodyText>
<page confidence="0.99512">
339
</page>
<note confidence="0.6524">
Computational Linguistics Volume 23, Number 2
</note>
<bodyText confidence="0.895818">
(E23) translates into Again. Such free or paraphrased translations are beyond the
reach of the proposed method.
</bodyText>
<equation confidence="0.677102714285714">
(E22) I shall love you as long as I breathe!
(C22) 1iâ¢Z*filidiX11z.
(E23) When in Rome, do as the Romans do.
(C23) k&apos;
(E24)
(E24) I don&apos;t care who wins.
(C24) fvfm-rang_tivgipAo
</equation>
<listItem confidence="0.5812125">
(E25) She angles her reports to suit the people she is speaking to.
(C25) kteizWAI*AktfrgiM, AtitiatErgEW.
</listItem>
<subsectionHeader confidence="0.979877">
5.3 Class-based versus Word-based Models
</subsectionHeader>
<bodyText confidence="0.995129833333333">
ClassAlign achieves a degree of generality in the sense that a true connection can
be identified, even when it occurs only rarely, or not at all, in the training corpus.
This kind of generality is unattainable with statistically trained word-based models.
Moreover, class-based models offer the advantages of a smaller storage requirement
and higher system efficiency. Unfortunately, they have the disadvantage of erroneous
overgeneralization from word-specific connections. For instance, due to the acquired
mapping from Gg273 (element of sound in language) to Bg07 (sound, tone, etc.), the
verb accent in (E26) is connected erroneously to MD (&apos;syllable&apos;) in (C26).
(E26) The accent in the word &amp;quot;important&amp;quot; is on the second syllable.
(C26) Important 1.-.14--priigt-gly-fm.. o
Nevertheless, our experiment has shown that the advantages outweigh the disadvan-
tages, at least for this particular formulation of a class-based approach to alignment.
</bodyText>
<sectionHeader confidence="0.976544" genericHeader="method">
6. Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999848090909091">
In this paper, we have presented an algorithm capable of identifying words and their
in-context translations in a bilingual corpus. The algorithm is effective for specific
linguistic reasons. First, a significant majority of words have diversified translations
that are not found in a bilingual dictionary or statistically-derived lexicon but that are
largely bounded within the word classes in thesauri. Therefore, we contend that a more
successful alignment can be achieved using a class-based approach. Our assumption
seems to hold, for the experiments in this study demonstrate that the method provides
broad-coverage alignment with almost no loss in precision.
In a broader sense, we have shown that thesauri and corpora can be used in
combination to address the critical issues of generality and efficiency. The thesaurus
provides classification that can be used to generalize the empirical knowledge gleaned
</bodyText>
<page confidence="0.993485">
340
</page>
<note confidence="0.961445">
Sue J. Ker and Jason S. Chang Word Alignment
</note>
<bodyText confidence="0.998802454545454">
from a corpus. The corpus provides training and testing materials, thereby allowing
knowledge to be derived and evaluated objectively.
The algorithm&apos;s performance could definitely be improved by enhanci-ig the var-
ious modules of the algorithms, e.g., morphological analyses, bilingual dictionary,
monolingual thesauri, and rule acquisition. Nevertheless, this work presents a func-
tional core for processing bilingual corpora at lexical and conceptual levels.
While this paper has specifically addressed English-Chinese corpora, the linguistic
issues motivating the algorithms seem to be quite general and are, to a large extent,
language independent, which means that the algorithm presented here should be
adaptable to other language pairs. The prospects for English-Japanese or Chinese-
Japanese, in particular, seem highly promising.
</bodyText>
<sectionHeader confidence="0.9922" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99984325">
This work was partially supported by ROC
NSC grants 82-0408-E-007-195,
83-0408-E-007-010,84-2213-E-007-023. We
are grateful to Keh-Yih Su for his
suggestions and comments at the early
stage in the development of this work. We
are also thankful to J-P Chanod and the
anonymous reviewers for many useful
suggestions. We would like to thank Liming
Yu from Zebra English Service Union, Betty
Teng and Nora Liu from Longman Asia
Limited, Keh-Jiann Chen and Chu-Ren
Huang from Academy Sirtica, and Perry
Chang from Galaxy Software Services for
making the dictionaries, thesauri, and
corpora available to us.
</bodyText>
<sectionHeader confidence="0.991824" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99932575">
Brown, P. F., J. Cocke, S. A. Della Pietra,
V. J. Della Pietra, F. Jelinek, J. D. Lafferty,
R. L. Mercer, and P. S. Roosin. 1990. A
statistical approach to machine
translation. Computational Linguistics,
16(2):79-85.
Brown, P. F., S. A. Della Pietra, V. J. Della
Pietra, J. D. Lafferty, and R. L. Mercer.
1992. Analysis, statistical transfer, and
synthesis in machine translation. In
Proceedings of the Fourth International
Conference on Theoretic and Methodological
Issues in Machine Translation, pages 83-100.
Brown, P. F., S. A. Della Pietra, V. J. Della
Pietra, and R. L. Mercer. 1993. The
mathematics of statistical machine
translation: Parameter estimation.
Computational Linguistics, 19(2):263-311.
Brown, P. F., V. J. Della Pietra, P. V. deSouza,
J. C. Lai, and R. L. Mercer. 1992.
Class-based n-gram models of natural
language. Computational Linguistics,
18(4):467-479.
Brown, P. F., J. C. Lai, and R. L. Mercer.
1991. Aligning sentences in parallel
corpora. In Proceedings of the 29th Annual
Meeting, pages 169-176, Berkeley, CA.
Association for Computational
Linguistics.
Chang, J. S. and M. H. C. Chen. 1994. Using
partial aligned parallel text and
part-of-speech information in word
alignment. In Proceedings of the First
Conference of the Association for Machine
Translation in the Americas (AMTA),
pages 16-23, Columbia, MD.
Chang, J. S., J. N. Chen, H. H. Sheng and
S. J. Ker. 1996. Combining machine
readable lexical resources and bilingual
corpora for broad word sense
disambiguation. In Proceedings of the
Second Conference of the Association for
Machine Translation in the Americas (AMTA),
pages 115-124, Montreal, Canada.
Chapman, R. 1977. Roget&apos;s International
Thesaurus. Harper and Row, New York.
Chen, Stanley F. 1993. Aligning sentences in
bilingual corpora using lexical
information. In Proceedings of the 31st
Annual Meeting, pages 9-16, Ohio.
Association for Computational
Linguistics.
Church, K. W., I. Dagan, W. A. Gale,
P. Fung, J. Helfman, and B. Satish. 1993.
Aligning parallel texts: Do methods
developed for English-French generalize
to Asian languages? In Proceedings of the
First Pacific Asia Conference on Formal and
Computational Linguistics, pages 1-12.
Church, K. W. and W. A. Gale. 1991.
Concordances for parallel text. In
Proceedings of the 7th Annual Conference of
the UW Centre for the New OED and Text
Research, pages 40-62, St. Catherine&apos;s
College, Oxford, England.
Dagan, I., K. W. Church, and W. A. Gale.
1993. Robust bilingual word alignment
for machine aided translation. In
Proceedings of the Workshop on Very Large
Corpora: Academic and Industrial
Perspectives, pages 1-8, Columbus, OH.
Daille, B., E. Gaussier, and J.-M. Lange.
</reference>
<page confidence="0.96314">
341
</page>
<note confidence="0.360234">
Computational Linguistics Volume 23, Number 2
</note>
<reference confidence="0.999764516393443">
1994. Towards automatic extraction of
monolingual and bilingual terminology.
In Proceedings of the 15th International
Conference on Computational Linguistics,
pages 515-521, Kyoto, Japan.
Dice, L. R. 1945. Measures of the amount of
ecologic association between species.
Journal of Ecology, 26:297-302.
van der Eijk, P. 1993. Automating the
acquisition of bilingual terminology. In
Proceedings of the Sixth Conference of the
European Chapter of the Association for
Computational Linguistics, pages 113-119,
Utrecht, The Netherlands.
Fujii, H. and W. Bruce Croft. 1993. A
comparison of indexing techniques for
Japanese text retrieval. In Proceedings of the
16th International ACM SIGIR Conference on
Research and Development in Information
Retrieval, pages 237-246.
Fung, P. and K. McKeown. 1994. Aligning
noisy parallel corpora across language
groups: Word pair feature matching by
dynamic time warping. In Proceedings of
the First Conference of the Association for
Machine Translation in the Americas
(AMTA-94), pages 81-88, Columbia, MD.
Galaxy Software Services. 1994. Lightship
User&apos;s Guide (in Chinese). Galaxy Software
Services, Taiwan.
Gale, W. A. and K. W. Church. 1991a. A
program for aligning sentences in
bilingual corpora. In Proceedings of the 29th
Annual Meeting, pages 177-184, Berkeley,
CA. Association for Computational
Linguistics.
Gale, W. A. and K. W. Church. 1991b.
Identifying word correspondences in
parallel texts. In Proceedings of the Fourth
DARPA Speech and Natural Language
Workshop, pages 152-157, Pacific Grove,
CA.
Gale, W. A. and K. W. Church. 1993. A
program for aligning sentences in
bilingual corpora. Computational
Linguistics, 19(1):75-102.
Gale, W. A., K. W. Church, and D. Yarowsky.
1992. Using bilingual materials to develop
word sense disambiguation methods. In
Proceedings of the Fourth International
Conference on Theoretical and Methodological
Issues in Machine Translation,
pages 101-112, Montreal, Canada.
Greenberg, J. H. 1963. Universals of Language.
MIT Press, Cambridge, MA.
Isabelle, P. 1992. Bi-textual aids for
translators. In Proceedings of the Eighth
Annual Conference of the UW Centre for the
New OED and Text Research, pages 76-89,
Waterloo, Canada.
Kay, M. and M. Roscheisen. 1993.
Text-translation alignment. Computational
Linguistics, 19(1):121-142.
Klavans, J. L. and E. Tzoukermann. 1990.
The BICORD system. In Proceedings of the
13th International Conference on
Computational Linguistics, pages 174-179,
Helsinki, Finland.
Kupiec, J. M. 1993. An algorithm for finding
noun phrase correspondences in bilingual
corpora. In Proceedings of the 31st Annual
Meeting, pages 17-22, Columbus, OH.
Association for Computational
Linguistics.
Li, C. N. and S. A. Thompson. 1981.
Mandarin Chinese-A Functional Reference
Grammar. University of California Press,
Los Angeles, CA.
Li, Hung-Wen. 1994. Word Alignment and
Refinement of Transfer Dictionary. Master
thesis, Institute of Computer Science and
Information Engineering, National Chiao
Tung University, Taiwan, R.O.C.
Longman Group. 1992. Longman
English-Chinese Dictionary of Contemporary
English. Longman Group (Far East) Ltd.,
Hong Kong.
Macklovitch, E. 1994. Using bi-textual
alignment for translation validation: The
TransCheck system. In Proceedings of the
First Conference of the Association for
Machine Translation in the Americas (AMTA),
pages 157-168, Columbia, MD.
Matsumoto, Y., H. Ishimoto, and T. Utsuro.
1993. Structural matching of parallel texts.
In Proceedings of the 31st Annual Meeting,
pages 22-30, Columbus, OH. Association
for Computational Linguistics.
McArthur, T. 1992. Longman Lexicon of
Contemporary English. Longman Group
(Far East) Ltd., Hong Kong.
McRoy, Susan W. 1992. Using multiple
knowledge sources for word sense
discrimination. Computational Linguistics,
18(1):1-30.
Mei, J. J., I. M. Zhu, Y. C. Gao, and H. S. Yin.
1993. Tongyici Cilin (Word Forest of
Synonyms). Tong Hua, Taipei. (Traditional
Chinese edition of a simplified Chinese
edition published in 1984.)
Melamed, I. Dan. 1996. Automatic
construction of clean broad-coverage
translation lexicons. In Proceedings of the
Second Conference of the Association for
Machine Translation in the Americas (AMTA),
pages 125-134, Montreal, Canada.
Meyers, A., R. Yangarber, and R. Grishman.
1996. Alignment of shared forests for
bilingual corpora. In Proceedings of the 16th
International Conference on Computational
Linguistics, pages 460-465, Copenhagen,
Denmark. COLING-96.
</reference>
<page confidence="0.996678">
342
</page>
<note confidence="0.971399">
Sue J. Ker and Jason S. Chang Word Alignment
</note>
<reference confidence="0.999840438596491">
Miller, G. A., R. Beckwith, C. Fellbaum,
D. Gross, and K. J. Miller. 1990.
Introduction to Wordnet: An on-line
lexical database. Journal of Lexicography,
3(4):235-244.
Pilot Software Inc. 1993. LightShip User&apos;s
Guide, Pilot Software Inc., Boston.
Proctor, P. 1988. Longman English-Chinese
Dictionary of Contemporary English.
Longman Group (Far East), Hong Kong.
Shemtov, H. 1993. Text alignment in a tool
for translating revised documents. In
Proceedings of the Sixth Conference of the
European Chapter of the Association for
Computational Linguistics, pages 449-453,
Utrecht, The Netherlands.
Simard, M., G. F. Foster, and P. Isabelle.
1992. Using cognates to align sentences in
bilingual corpora. In Proceedings of the
Fourth International Conference on Theoretical
and Methodological Issues in Machine
Translation (TMI-92), pages 67-81,
Montreal, Canada.
Smadja, F. 1992. How to compile a bilingual
collocation lexicon automatically. In
Proceedings of the AAAI-92 Workshop on
Statistically-Based NLP Techniques,
pages 65-71, San Jose, CA. American
Association for Artificial Intelligence.
Smadja, F., K. R. McKeown, and
V. Hatzivassiloglou. 1996. Translating
collocations for bilingual lexicons: A
statistical approach. Computational
Linguistics, 22(1):1-38.
des Tombe, L. and S. Armstrong-Warwick.
1993. Using function words to measure
translation quality. In Proceedings of the
Ninth Annual Conference of the UW Centre for
the New OED and Text Research, pages 1-17.
Wu, D. 1994. Aligning a parallel
English-Chinese corpus statistically with
lexical criteria. In Proceedings of the 32nd
Annual Meeting, pages 80-87, Las Cruces,
NM. Association for Computational
Linguistics.
Wu, D. 1995. Grammarless extraction of
phrasal translation examples from parallel
texts. In Proceedings of the Sixth
International Conference on Theoretical and
Methodological Issues in Machine Translation,
pages 354-372, Belgium.
Wu, D. and X. Xia. 1994. Learning an
English-Chinese lexicon from a parallel
corpus. In Proceedings of the First Conference
of the Association for Machine Translation in
the Americas (AMTA), pages 206-213,
Columbia, MD.
</reference>
<page confidence="0.999365">
343
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.838624">
<title confidence="0.9779205">A Class-based Approach to Word Alignment</title>
<author confidence="0.999559">Sue J Ker Jason S Chang</author>
<affiliation confidence="0.996326">National Tsing Hua University National Tsing Hua University</affiliation>
<abstract confidence="0.990679142857143">This paper presents an algorithm capable of identifying the translation for each word in a bilingual corpus. Previously proposed methods rely heavily on word-based statistics. Under a word-based approach, frequent words with a consistent translation can be aligned at a high rate of precision. However, words that are less frequent or exhibit diverse translations generally do not have statistically significant evidence for confident alignment, thereby leading to incomplete or incorrect alignments. The algorithm proposed herein attempts to broaden coverage by exploiting lexicographic resources. To this end, we draw on the two classification systems of words in Longman Lexicon of Contemporary English (LLOCE) and Tongyici Cilin (Synonym Forest, CILIN). Automatically acquired class-based alignment rules are used to compensate for what is lacking in a bilingual dictionary such as the English-Chinese version of the Longman Dictionary of Contemporary English (LecDOCE). In addition, this alignment method is implemented using LecDOCE examples and their translations for training and testing, while further examples from a technical manual in both English and Chinese are used for an open test. Quantitative results of the closed and open tests are also summarized.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>J Cocke</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>F Jelinek</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
<author>P S Roosin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--2</pages>
<contexts>
<context position="4738" citStr="Brown et al. (1990)" startWordPosition="725" endWordPosition="728">, Brown et al. (1993) present a series of five models of Pr(S I T) for word alignment. Model 1 assumes that Pr(S I T) depends only on lexical translation probability (LTP) t(s I t), that is, the probability that the ith word s in S translates into the jth word t in T. The pair of words (s, t), or more precisely (s, t,i,j) since there could be more than one instance of s or t, is called a connection. Model 2 enhances Model 1 by considering the dependence of Pr(S I T) on the distortion probability (DP) d(i 11,1, m) where 1 and m are the respective lengths of S and T measured in number of words. Brown et al. (1990) propose using an adaptive Expectation and Maximization (EM) algorithm to estimate the parameters for LTP and DP from a bilingual corpus. The EM algorithm iterates between two phases to estimate LTP and DP until both functions converge. In the expectation phase, the parameters t(s I t) and d(i Ii,!, m) in the SMT model for all possible values of s, t, j, 1, and m are estimated from the sample of an aligned bilingual corpus. In the maximization phase, each sentence-translation pair in the corpus is aligned by maximizing the translation probability, Pr(S 1 T). They examine the feasibility of ali</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roosin, 1990</marker>
<rawString>Brown, P. F., J. Cocke, S. A. Della Pietra, V. J. Della Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roosin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
</authors>
<title>Analysis, statistical transfer, and synthesis in machine translation.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourth International Conference on Theoretic and Methodological Issues in Machine Translation,</booktitle>
<pages>83--100</pages>
<marker>Brown, Pietra, Pietra, Lafferty, Mercer, 1992</marker>
<rawString>Brown, P. F., S. A. Della Pietra, V. J. Della Pietra, J. D. Lafferty, and R. L. Mercer. 1992. Analysis, statistical transfer, and synthesis in machine translation. In Proceedings of the Fourth International Conference on Theoretic and Methodological Issues in Machine Translation, pages 83-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="4140" citStr="Brown et al. (1993)" startWordPosition="606" endWordPosition="609">icography (Klavans and Tzoukermann 1990; Church and Gale 1991; Daille, Gaussier, and Lange 1994; Kupiec 1993; van der Eijk 1993; Li 1994; Wu and Xia 1994), and word-sense disambiguation (Gale, Church, and Yarowsky 1992; Chang, Chen, Sheng, and Ker 1996). For these applications, we must go one step further from sentence alignment and identify alignment at the word level. In the process of word alignment, the translation of each source word is identified. This study concentrates primarily on identifying alignment at the word level for a given sentence and its translation. In the context of SMT, Brown et al. (1993) present a series of five models of Pr(S I T) for word alignment. Model 1 assumes that Pr(S I T) depends only on lexical translation probability (LTP) t(s I t), that is, the probability that the ith word s in S translates into the jth word t in T. The pair of words (s, t), or more precisely (s, t,i,j) since there could be more than one instance of s or t, is called a connection. Model 2 enhances Model 1 by considering the dependence of Pr(S I T) on the distortion probability (DP) d(i 11,1, m) where 1 and m are the respective lengths of S and T measured in number of words. Brown et al. (1990) p</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Brown, P. F., S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della Pietra</author>
<author>P V deSouza</author>
<author>J C Lai</author>
<author>R L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--4</pages>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Brown, P. F., V. J. Della Pietra, P. V. deSouza, J. C. Lai, and R. L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>J C Lai</author>
<author>R L Mercer</author>
</authors>
<title>Aligning sentences in parallel corpora.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting,</booktitle>
<pages>169--176</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Berkeley, CA.</location>
<marker>Brown, Lai, Mercer, 1991</marker>
<rawString>Brown, P. F., J. C. Lai, and R. L. Mercer. 1991. Aligning sentences in parallel corpora. In Proceedings of the 29th Annual Meeting, pages 169-176, Berkeley, CA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Chang</author>
<author>M H C Chen</author>
</authors>
<title>Using partial aligned parallel text and part-of-speech information in word alignment.</title>
<date>1994</date>
<booktitle>In Proceedings of the First Conference of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<pages>16--23</pages>
<location>Columbia, MD.</location>
<contexts>
<context position="20652" citStr="Chang and Chen (1994)" startWordPosition="3223" endWordPosition="3226"> methods for derived classes (Brown, Della Pietra, deSouza, Lai, and Mercer 1992) are not appropriate, since they also suffer low coverage due to data sparseness. Classes formed from morphologically related words are easy to derive and apply. Morphological classes can be formed, either from words that start with the same five-character prefix as in Gale and Church (1991b), or rigorous analysis as suggested in Brown, Della Pietra, Della Pietra, Lafferty, and Mercer (1992). Although easily applicable, morphological classes are not particularly effective in broadening coverage of word alignment. Chang and Chen (1994) also examine the feasibility of using part-of-speech classes. A potential alternative involves adopting categories available in machine-readable lexicographic resources such as Roget&apos;s thesaurus (Chapman 1977) or hand-crafted computer lexicons (Miller, Beckwith, Fellbaum, Gross, and Miller 1990; McRoy 1992). 3. Algorithms Leading to Class-based Word Alignment This section describes a series of three algorithms leading to a class-based system for word alignment. The first algorithm attempts to obtain reliable connections. The second algorithm generalizes the connections into a list of class-ba</context>
</contexts>
<marker>Chang, Chen, 1994</marker>
<rawString>Chang, J. S. and M. H. C. Chen. 1994. Using partial aligned parallel text and part-of-speech information in word alignment. In Proceedings of the First Conference of the Association for Machine Translation in the Americas (AMTA), pages 16-23, Columbia, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Chang</author>
<author>J N Chen</author>
<author>H H Sheng</author>
<author>S J Ker</author>
</authors>
<title>Combining machine readable lexical resources and bilingual corpora for broad word sense disambiguation.</title>
<date>1996</date>
<booktitle>In Proceedings of the Second Conference of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<pages>115--124</pages>
<location>Montreal, Canada.</location>
<marker>Chang, Chen, Sheng, Ker, 1996</marker>
<rawString>Chang, J. S., J. N. Chen, H. H. Sheng and S. J. Ker. 1996. Combining machine readable lexical resources and bilingual corpora for broad word sense disambiguation. In Proceedings of the Second Conference of the Association for Machine Translation in the Americas (AMTA), pages 115-124, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Chapman</author>
</authors>
<title>Roget&apos;s International Thesaurus. Harper and Row,</title>
<date>1977</date>
<location>New York.</location>
<contexts>
<context position="20862" citStr="Chapman 1977" startWordPosition="3251" endWordPosition="3252">asy to derive and apply. Morphological classes can be formed, either from words that start with the same five-character prefix as in Gale and Church (1991b), or rigorous analysis as suggested in Brown, Della Pietra, Della Pietra, Lafferty, and Mercer (1992). Although easily applicable, morphological classes are not particularly effective in broadening coverage of word alignment. Chang and Chen (1994) also examine the feasibility of using part-of-speech classes. A potential alternative involves adopting categories available in machine-readable lexicographic resources such as Roget&apos;s thesaurus (Chapman 1977) or hand-crafted computer lexicons (Miller, Beckwith, Fellbaum, Gross, and Miller 1990; McRoy 1992). 3. Algorithms Leading to Class-based Word Alignment This section describes a series of three algorithms leading to a class-based system for word alignment. The first algorithm attempts to obtain reliable connections. The second algorithm generalizes the connections into a list of class-based rules, which stipulate that a pair of classes of words in the source and target languages are likely mutual translations. The third algorithm performs the actual word alignment based on the acquired rules, </context>
</contexts>
<marker>Chapman, 1977</marker>
<rawString>Chapman, R. 1977. Roget&apos;s International Thesaurus. Harper and Row, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
</authors>
<title>Aligning sentences in bilingual corpora using lexical information.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting,</booktitle>
<pages>9--16</pages>
<institution>Ohio. Association for Computational Linguistics.</institution>
<contexts>
<context position="2233" citStr="Chen 1993" startWordPosition="336" endWordPosition="337"> Statistical machine translation (SMT) can be understood as a word-by-word model consisting of two submodels: a language model for generating a source text segment S and a translation model for mapping S to its translation T. They recommend using a bilingual corpus to train the parameters of translation probability, Pr(S j T) in the translation model. For MT and other purposes, many methods have been proposed for sentence alignment of the Hansards, an English-French corpus of Canadian parliamentary debates (Brown, Lai, and Mercer 1991; Gale and Church 1991a; Simard, Foster, and Isabelle 1992; Chen 1993; Gale and Church 1993), and for other language pairs, including English-German, EnglishChinese, and English-Japanese (Kay and Roscheisen 1993; Church, Dagan, Gale, Fung, Helfman, and Satish 1993; Fung and McKeown 1994; Wu 1994). Alignment at other levels of resolution is obviously useful. A section, paragraph, sentence, phrase, collocation, or word can be aligned to its translation (Kupiec 1993; Smadja, McKeown, and Hatzivassiloglou 1996). Other logical approaches involve aligning parse trees of a sentence and its translation (Matsumoto, Ishimoto, and Utsuro 1993; Meyers, Yangarber, and Grish</context>
</contexts>
<marker>Chen, 1993</marker>
<rawString>Chen, Stanley F. 1993. Aligning sentences in bilingual corpora using lexical information. In Proceedings of the 31st Annual Meeting, pages 9-16, Ohio. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>I Dagan</author>
<author>W A Gale</author>
<author>P Fung</author>
<author>J Helfman</author>
<author>B Satish</author>
</authors>
<title>Aligning parallel texts: Do methods developed for English-French generalize to Asian languages?</title>
<date>1993</date>
<booktitle>In Proceedings of the First Pacific Asia Conference on Formal and Computational Linguistics,</booktitle>
<pages>1--12</pages>
<marker>Church, Dagan, Gale, Fung, Helfman, Satish, 1993</marker>
<rawString>Church, K. W., I. Dagan, W. A. Gale, P. Fung, J. Helfman, and B. Satish. 1993. Aligning parallel texts: Do methods developed for English-French generalize to Asian languages? In Proceedings of the First Pacific Asia Conference on Formal and Computational Linguistics, pages 1-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>W A Gale</author>
</authors>
<title>Concordances for parallel text.</title>
<date>1991</date>
<booktitle>In Proceedings of the 7th Annual Conference of the UW Centre for the New OED and Text Research,</booktitle>
<pages>40--62</pages>
<institution>St. Catherine&apos;s College,</institution>
<location>Oxford, England.</location>
<contexts>
<context position="3582" citStr="Church and Gale 1991" startWordPosition="513" endWordPosition="516">nal Tsing Hua University, Hsinchu, 30043, Taiwan, ROC. E-mail: ksj@volans.cs.scu.edu.tw; jschang@cs.nthu.edu.tw Â© 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 2 In addition to machine translation, many applications for aligned corpora have been suggested, including machine-aided translation (Shemtov 1993), translation assessment and critiquing tools (Isabelle 1992; des Tombe and Armstrong-Warwick 1993; Macklovitch 1994), text generation (Smadja 1992; Smadja, McKeown, and Hatzivassiloglou 1996), bilingual lexicography (Klavans and Tzoukermann 1990; Church and Gale 1991; Daille, Gaussier, and Lange 1994; Kupiec 1993; van der Eijk 1993; Li 1994; Wu and Xia 1994), and word-sense disambiguation (Gale, Church, and Yarowsky 1992; Chang, Chen, Sheng, and Ker 1996). For these applications, we must go one step further from sentence alignment and identify alignment at the word level. In the process of word alignment, the translation of each source word is identified. This study concentrates primarily on identifying alignment at the word level for a given sentence and its translation. In the context of SMT, Brown et al. (1993) present a series of five models of Pr(S I</context>
</contexts>
<marker>Church, Gale, 1991</marker>
<rawString>Church, K. W. and W. A. Gale. 1991. Concordances for parallel text. In Proceedings of the 7th Annual Conference of the UW Centre for the New OED and Text Research, pages 40-62, St. Catherine&apos;s College, Oxford, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>K W Church</author>
<author>W A Gale</author>
</authors>
<title>Robust bilingual word alignment for machine aided translation.</title>
<date>1993</date>
<booktitle>In Proceedings of the Workshop on Very Large Corpora: Academic and Industrial Perspectives,</booktitle>
<pages>1--8</pages>
<location>Columbus, OH.</location>
<marker>Dagan, Church, Gale, 1993</marker>
<rawString>Dagan, I., K. W. Church, and W. A. Gale. 1993. Robust bilingual word alignment for machine aided translation. In Proceedings of the Workshop on Very Large Corpora: Academic and Industrial Perspectives, pages 1-8, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Daille</author>
<author>E Gaussier</author>
<author>J-M Lange</author>
</authors>
<title>Towards automatic extraction of monolingual and bilingual terminology.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>515--521</pages>
<location>Kyoto, Japan.</location>
<marker>Daille, Gaussier, Lange, 1994</marker>
<rawString>Daille, B., E. Gaussier, and J.-M. Lange. 1994. Towards automatic extraction of monolingual and bilingual terminology. In Proceedings of the 15th International Conference on Computational Linguistics, pages 515-521, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Dice</author>
</authors>
<title>Measures of the amount of ecologic association between species.</title>
<date>1945</date>
<journal>Journal of Ecology,</journal>
<pages>26--297</pages>
<contexts>
<context position="22240" citStr="Dice 1945" startWordPosition="3483" endWordPosition="3484"> DietAlign, based on the DTs from a bilingual MRD such as the LecDOCE. Consider a text and translation pair (S, T), a word s in S. and its ICT, t in T. Let DT, denote the set of translations listed in the LecDOCE for the headword s. Recall that if for a word tin T, there is a dt in DT, such that t matches dt completely or partially, then, t is likely to be the ICT of s. Taking advantage of this phenomenon, DictAlign computes the set WT = ft I t is a word in T} and calculates the similarity between each t and the DT, relevant to S. A similarity measure based on the unweighted Dice coefficient (Dice 1945) can be given as follows: Based on this similarity measure, the likelihood of a connection can be associated with the following formulation that links the likelihood of a connection to similarity with a DT: DTSim(s, t) = max Sim(d, t) dEDT, For instance, consider the following sentence and its Mandarin translation, focusing on the word encounter: S = He encountered many difficulties. T = itANItliX1111. where d, t = Idl = Iti = Idntl = 2 x Idfltl Sim(d, t) = (1) idi iti Mandarin morpheme strings, the number of the morphemes in d, the number of the morphemes in t, the number of the morphemes in </context>
<context position="30962" citStr="Dice 1945" startWordPosition="5073" endWordPosition="5074">iStaZikfthrititt t#41-14NIAR get (Ma005, Arriving and reaching) reach (Ma005, Arriving and reaching) I don&apos;t get you; what do you mean? get RTC n,tesz? (Gb031, Understand and realize) I understood that it was time to leave. understand uturginfilAtTo (Gb031, Understand and realize) With the difficulties of finding appropriate classification systems and suppressing noise now resolved, the question remains: How can class-to-class mapping be acquired? Just as with the derivation of a bilingual lexicon from a corpus, acquisition of such mapping requires a statistical measure. The Dice coefficient (Dice 1945) is a similarity measure that gauges the ratio of the members in one collection being identical to those of another collection. Smadja, McKeown, and Hatzivassiloglou (1996) propose to link co-occurrence to the Dice coefficient in their study of bilingual collocations. They observe that, unlike statistical measures related to mutual information, the Dice coefficient is insensitive to sample size and, thus, more effective for acquiring bilingual collocations from a bilingual corpus. Our experimental results confirm their observation. Under a formulation linking translation to conceptual similari</context>
</contexts>
<marker>Dice, 1945</marker>
<rawString>Dice, L. R. 1945. Measures of the amount of ecologic association between species. Journal of Ecology, 26:297-302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P van der Eijk</author>
</authors>
<title>Automating the acquisition of bilingual terminology.</title>
<date>1993</date>
<booktitle>In Proceedings of the Sixth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>113--119</pages>
<location>Utrecht, The Netherlands.</location>
<marker>van der Eijk, 1993</marker>
<rawString>van der Eijk, P. 1993. Automating the acquisition of bilingual terminology. In Proceedings of the Sixth Conference of the European Chapter of the Association for Computational Linguistics, pages 113-119, Utrecht, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Fujii</author>
<author>W Bruce Croft</author>
</authors>
<title>A comparison of indexing techniques for Japanese text retrieval.</title>
<date>1993</date>
<booktitle>In Proceedings of the 16th International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>237--246</pages>
<contexts>
<context position="17839" citStr="Fujii and Croft (1993)" startWordPosition="2771" endWordPosition="2774">icated all responsibility for the care of the child. care PRIA .44 ftbk*T âlil-A/NA --C1(4-10 (N1366) (Hi37) (Hi37) We should advertise for someone to look after the look after .Y4 (Hi37) garden. (Nf162) (Hi37) fliMPERWMIBMA*110$44TEIZI. He abdicated all responsibility for the care of the child. child %HA IthMT âfaAikfAtrgâtihtil. (Ca003) (Ab04) (Ab04) John Smith? Yes - he&apos;s a local boy, I believe. boy A 93A Zigln? (Ca002) (Ab02) (Ab01) that share a common morpheme. For instance, the (ICT, DT) pairs, MN and RES) and (tcÂ± and tct) share a common morpheme Xt &apos;sad&apos; and tc &apos;female&apos;, respectively. Fujii and Croft (1993) also point out a similar thesaurus effect of Mandarin morphemes in Japanese information retrieval (IR).1 Dictionary-based Alignment. The above observations suggest that a DT-based algorithm, coupled with morpheme-level partial matching, can be adopted to obtain a substantial 1 Fujii and Croft observe that a document is likely to be relevant if it contains an index term that has a morpheme (kanji) in common with a query term. More often than not, the index term and the query term are synonyms that might appear under the same category in a thesaurus. The authors call this phenomenon the thesaur</context>
</contexts>
<marker>Fujii, Croft, 1993</marker>
<rawString>Fujii, H. and W. Bruce Croft. 1993. A comparison of indexing techniques for Japanese text retrieval. In Proceedings of the 16th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 237-246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
<author>K McKeown</author>
</authors>
<title>Aligning noisy parallel corpora across language groups: Word pair feature matching by dynamic time warping.</title>
<date>1994</date>
<booktitle>In Proceedings of the First Conference of the Association for Machine Translation in the Americas (AMTA-94),</booktitle>
<pages>81--88</pages>
<location>Columbia, MD.</location>
<contexts>
<context position="2451" citStr="Fung and McKeown 1994" startWordPosition="365" endWordPosition="368"> its translation T. They recommend using a bilingual corpus to train the parameters of translation probability, Pr(S j T) in the translation model. For MT and other purposes, many methods have been proposed for sentence alignment of the Hansards, an English-French corpus of Canadian parliamentary debates (Brown, Lai, and Mercer 1991; Gale and Church 1991a; Simard, Foster, and Isabelle 1992; Chen 1993; Gale and Church 1993), and for other language pairs, including English-German, EnglishChinese, and English-Japanese (Kay and Roscheisen 1993; Church, Dagan, Gale, Fung, Helfman, and Satish 1993; Fung and McKeown 1994; Wu 1994). Alignment at other levels of resolution is obviously useful. A section, paragraph, sentence, phrase, collocation, or word can be aligned to its translation (Kupiec 1993; Smadja, McKeown, and Hatzivassiloglou 1996). Other logical approaches involve aligning parse trees of a sentence and its translation (Matsumoto, Ishimoto, and Utsuro 1993; Meyers, Yangarber, and Grishman 1996), or simultaneously generating parse trees and alignment arrangements (Wu 1995). * Department of Computer Science, National Tsing Hua University, Hsinchu, 30043, Taiwan, ROC. E-mail: ksj@volans.cs.scu.edu.tw; </context>
<context position="63272" citStr="Fung and McKeown 1994" startWordPosition="10999" endWordPosition="11002">ords 1,823 1,460 100% 80.1% 2,359 1,800 100% 76.3% Matched words 1,561 1,460 85.6% 93.5% 1,965 1,800 83.3% 91.6% ClassAlign incorrectly connects the compound gni in (C16) to a single English word company according to the alignment rule (Co292, Dm07). (E16) She is a star with the theatre company. (C16)kt1161.11111ErgarM0 Other methods for aligning English and Mandarin texts in the literature also fall prey to the problem of Mandarin compounds. For instance, the following partially correct connections complicated by compounding are reported in a recent study on alignment of Hong Kong Basic Law (Fung and McKeown 1994). (E17) monoxide (C17) ---xtg (&apos;carbon monoxide&apos;) (E18) Basic (C18) M2MA (&apos;Basic Law&apos;) (E19) second (C19) .7_1411 (&apos;second reading&apos;) Because it is not limited to the connections involved in a presegmented target sentence (Fung and McKeown 1994; Wu and Xai 1994), ClassAlign avoids most instances of these errors. In addition, with elaborate preprocessing such as parsing, phrase grouping, and collocation analysis (Smadja 1992), the problem of word-number difference 338 Sue J. Ker and Jason S. Chang Word Alignment Table 19 The final alignment of example (E20, C20). 1 2 3 4 5 6 7 8 9 10 He abdicate</context>
</contexts>
<marker>Fung, McKeown, 1994</marker>
<rawString>Fung, P. and K. McKeown. 1994. Aligning noisy parallel corpora across language groups: Word pair feature matching by dynamic time warping. In Proceedings of the First Conference of the Association for Machine Translation in the Americas (AMTA-94), pages 81-88, Columbia, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Galaxy Software Services</author>
</authors>
<date>1994</date>
<booktitle>Lightship User&apos;s Guide (in Chinese). Galaxy Software Services,</booktitle>
<location>Taiwan.</location>
<contexts>
<context position="8464" citStr="Services 1994" startWordPosition="1314" endWordPosition="1315">ining a high precision rate. The rest of this paper is organized as follows: In Section 2 we briefly discuss the nature of text and translation that justifies a class-based approach. A set of three algorithms leading to class-based alignment are outlined in Section 3. The algorithms&apos; effectiveness is demonstrated through examples and their translations in the LecDOCE (Longman Group 1992), a bilingual version of the Longman Dictionary of Contemporary English (LDOCE, Proctor 1988), as well as sentences from bilingual texts in the LightShip User&apos;s Guide (Pilot Software Inc. 1993; Galaxy Software Services 1994). The experiments we undertook to assess the performance of these algorithms are the topic of Section 4. Quantitative experimental results are also summarized. In Section 5, we analyze the experimental results and consider ways in which the proposed algorithms might be extended and improved. Concluding remarks are made in Section 6. 2. Text and Translation as a Class-to-Class Mapping The discussion in Section 1 indicates the limitations of statistical methods. As an alternative, we examine the feasibility of using an everyday bilingual dictionary in machine-readable form for word alignment. Wi</context>
</contexts>
<marker>Services, 1994</marker>
<rawString>Galaxy Software Services. 1994. Lightship User&apos;s Guide (in Chinese). Galaxy Software Services, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting,</booktitle>
<pages>177--184</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Berkeley, CA.</location>
<contexts>
<context position="2186" citStr="Gale and Church 1991" startWordPosition="327" endWordPosition="330">initiate much of the recent interest in bilingual corpora. Statistical machine translation (SMT) can be understood as a word-by-word model consisting of two submodels: a language model for generating a source text segment S and a translation model for mapping S to its translation T. They recommend using a bilingual corpus to train the parameters of translation probability, Pr(S j T) in the translation model. For MT and other purposes, many methods have been proposed for sentence alignment of the Hansards, an English-French corpus of Canadian parliamentary debates (Brown, Lai, and Mercer 1991; Gale and Church 1991a; Simard, Foster, and Isabelle 1992; Chen 1993; Gale and Church 1993), and for other language pairs, including English-German, EnglishChinese, and English-Japanese (Kay and Roscheisen 1993; Church, Dagan, Gale, Fung, Helfman, and Satish 1993; Fung and McKeown 1994; Wu 1994). Alignment at other levels of resolution is obviously useful. A section, paragraph, sentence, phrase, collocation, or word can be aligned to its translation (Kupiec 1993; Smadja, McKeown, and Hatzivassiloglou 1996). Other logical approaches involve aligning parse trees of a sentence and its translation (Matsumoto, Ishimoto</context>
<context position="6331" citStr="Gale and Church (1991" startWordPosition="984" endWordPosition="987">that reliably distinguishing sentence boundaries for a noisy bilingual text scanned by an OCR device is quite difficult. In such a circumstance, they recommend aligning words directly without the preprocessing phase of sentence alignment. Under that proposal, a rough character-by-character alignment is first performed. Based on the character alignment, words are subsequently aligned based on a modified version of Brown et al.&apos;s Model 2. The authors report that 60.5% of 65,000 words in a noisy document are correctly aligned. For 84% of the words, the offset from correct alignment is at most 3. Gale and Church (1991b) present an alternative algorithm that does not estimate and store probabilities for all word pairs to reduce memory requirement and to ensure robustness of probability estimation. Instead, for each source word s, only a handful of target words strongly associated with s are found and stored. Such a task is achieved by applying a x2-like statistic. They report that the method produces highly precise (95%) alignment for 61% of the words in the 800 sentences tested. This paper is motivated by the following observations: First, the above survey clearly reveals that word-based methods offer only</context>
<context position="20403" citStr="Gale and Church (1991" startWordPosition="3189" endWordPosition="3192">-class word and a Y-class word. 2.2 Class-based Word Alignment To ensure broad coverage, the class-based approach seems to be a promising alternative to word-based methods. Classes can be formed from words in more than one way. Automatic statistical methods for derived classes (Brown, Della Pietra, deSouza, Lai, and Mercer 1992) are not appropriate, since they also suffer low coverage due to data sparseness. Classes formed from morphologically related words are easy to derive and apply. Morphological classes can be formed, either from words that start with the same five-character prefix as in Gale and Church (1991b), or rigorous analysis as suggested in Brown, Della Pietra, Della Pietra, Lafferty, and Mercer (1992). Although easily applicable, morphological classes are not particularly effective in broadening coverage of word alignment. Chang and Chen (1994) also examine the feasibility of using part-of-speech classes. A potential alternative involves adopting categories available in machine-readable lexicographic resources such as Roget&apos;s thesaurus (Chapman 1977) or hand-crafted computer lexicons (Miller, Beckwith, Fellbaum, Gross, and Miller 1990; McRoy 1992). 3. Algorithms Leading to Class-based Wor</context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>Gale, W. A. and K. W. Church. 1991a. A program for aligning sentences in bilingual corpora. In Proceedings of the 29th Annual Meeting, pages 177-184, Berkeley, CA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
</authors>
<title>Identifying word correspondences in parallel texts.</title>
<date>1991</date>
<booktitle>In Proceedings of the Fourth DARPA Speech and Natural Language Workshop,</booktitle>
<pages>152--157</pages>
<location>Pacific Grove, CA.</location>
<contexts>
<context position="2186" citStr="Gale and Church 1991" startWordPosition="327" endWordPosition="330">initiate much of the recent interest in bilingual corpora. Statistical machine translation (SMT) can be understood as a word-by-word model consisting of two submodels: a language model for generating a source text segment S and a translation model for mapping S to its translation T. They recommend using a bilingual corpus to train the parameters of translation probability, Pr(S j T) in the translation model. For MT and other purposes, many methods have been proposed for sentence alignment of the Hansards, an English-French corpus of Canadian parliamentary debates (Brown, Lai, and Mercer 1991; Gale and Church 1991a; Simard, Foster, and Isabelle 1992; Chen 1993; Gale and Church 1993), and for other language pairs, including English-German, EnglishChinese, and English-Japanese (Kay and Roscheisen 1993; Church, Dagan, Gale, Fung, Helfman, and Satish 1993; Fung and McKeown 1994; Wu 1994). Alignment at other levels of resolution is obviously useful. A section, paragraph, sentence, phrase, collocation, or word can be aligned to its translation (Kupiec 1993; Smadja, McKeown, and Hatzivassiloglou 1996). Other logical approaches involve aligning parse trees of a sentence and its translation (Matsumoto, Ishimoto</context>
<context position="6331" citStr="Gale and Church (1991" startWordPosition="984" endWordPosition="987">that reliably distinguishing sentence boundaries for a noisy bilingual text scanned by an OCR device is quite difficult. In such a circumstance, they recommend aligning words directly without the preprocessing phase of sentence alignment. Under that proposal, a rough character-by-character alignment is first performed. Based on the character alignment, words are subsequently aligned based on a modified version of Brown et al.&apos;s Model 2. The authors report that 60.5% of 65,000 words in a noisy document are correctly aligned. For 84% of the words, the offset from correct alignment is at most 3. Gale and Church (1991b) present an alternative algorithm that does not estimate and store probabilities for all word pairs to reduce memory requirement and to ensure robustness of probability estimation. Instead, for each source word s, only a handful of target words strongly associated with s are found and stored. Such a task is achieved by applying a x2-like statistic. They report that the method produces highly precise (95%) alignment for 61% of the words in the 800 sentences tested. This paper is motivated by the following observations: First, the above survey clearly reveals that word-based methods offer only</context>
<context position="20403" citStr="Gale and Church (1991" startWordPosition="3189" endWordPosition="3192">-class word and a Y-class word. 2.2 Class-based Word Alignment To ensure broad coverage, the class-based approach seems to be a promising alternative to word-based methods. Classes can be formed from words in more than one way. Automatic statistical methods for derived classes (Brown, Della Pietra, deSouza, Lai, and Mercer 1992) are not appropriate, since they also suffer low coverage due to data sparseness. Classes formed from morphologically related words are easy to derive and apply. Morphological classes can be formed, either from words that start with the same five-character prefix as in Gale and Church (1991b), or rigorous analysis as suggested in Brown, Della Pietra, Della Pietra, Lafferty, and Mercer (1992). Although easily applicable, morphological classes are not particularly effective in broadening coverage of word alignment. Chang and Chen (1994) also examine the feasibility of using part-of-speech classes. A potential alternative involves adopting categories available in machine-readable lexicographic resources such as Roget&apos;s thesaurus (Chapman 1977) or hand-crafted computer lexicons (Miller, Beckwith, Fellbaum, Gross, and Miller 1990; McRoy 1992). 3. Algorithms Leading to Class-based Wor</context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>Gale, W. A. and K. W. Church. 1991b. Identifying word correspondences in parallel texts. In Proceedings of the Fourth DARPA Speech and Natural Language Workshop, pages 152-157, Pacific Grove, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="2256" citStr="Gale and Church 1993" startWordPosition="338" endWordPosition="341">l machine translation (SMT) can be understood as a word-by-word model consisting of two submodels: a language model for generating a source text segment S and a translation model for mapping S to its translation T. They recommend using a bilingual corpus to train the parameters of translation probability, Pr(S j T) in the translation model. For MT and other purposes, many methods have been proposed for sentence alignment of the Hansards, an English-French corpus of Canadian parliamentary debates (Brown, Lai, and Mercer 1991; Gale and Church 1991a; Simard, Foster, and Isabelle 1992; Chen 1993; Gale and Church 1993), and for other language pairs, including English-German, EnglishChinese, and English-Japanese (Kay and Roscheisen 1993; Church, Dagan, Gale, Fung, Helfman, and Satish 1993; Fung and McKeown 1994; Wu 1994). Alignment at other levels of resolution is obviously useful. A section, paragraph, sentence, phrase, collocation, or word can be aligned to its translation (Kupiec 1993; Smadja, McKeown, and Hatzivassiloglou 1996). Other logical approaches involve aligning parse trees of a sentence and its translation (Matsumoto, Ishimoto, and Utsuro 1993; Meyers, Yangarber, and Grishman 1996), or simultane</context>
</contexts>
<marker>Gale, Church, 1993</marker>
<rawString>Gale, W. A. and K. W. Church. 1993. A program for aligning sentences in bilingual corpora. Computational Linguistics, 19(1):75-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
<author>D Yarowsky</author>
</authors>
<title>Using bilingual materials to develop word sense disambiguation methods.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>101--112</pages>
<location>Montreal, Canada.</location>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>Gale, W. A., K. W. Church, and D. Yarowsky. 1992. Using bilingual materials to develop word sense disambiguation methods. In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation, pages 101-112, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Greenberg</author>
</authors>
<title>Universals of Language.</title>
<date>1963</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="55862" citStr="Greenberg (1963)" startWordPosition="9669" endWordPosition="9670">to. For instance, in the sentence (E13, C13), the first word (daxiang, &apos;elephant&apos;) is the topic and the second word 07- (bizi, &apos;nose&apos;) is the subject; t* &apos;elephant&apos; is the focus of the discourse, but it is the subject *7- &apos;nose&apos; that is very long; not t91 &apos;elephant&apos;. (E13) The elephant has a very long nose. (C13) tiP. -no Daxiang bizi hen chang Elephant nose very long The topic prominence of Mandarin sentences represents alignment connections with a large distortion in position, leading to difficulty in estimating the likelihood of a connection according to translational position. Word Order. Greenberg (1963) stated that the world&apos;s languages fall into three word order groups according to the order of the subject (S), verb (V), and object (0) in a simple transitive sentence. A language, in general, belongs to one of three basic word order types, SVO, SOV, and VSO. By this notion, English is an SVO language in which the verb typically follows the subject and precedes the object. For most languages, other aspects of word order, such as that of modifier and modified elements, correlate with the order of V and 0. However, Mandarin is not an easy language to classify according to this typology for a nu</context>
</contexts>
<marker>Greenberg, 1963</marker>
<rawString>Greenberg, J. H. 1963. Universals of Language. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Isabelle</author>
</authors>
<title>Bi-textual aids for translators.</title>
<date>1992</date>
<booktitle>In Proceedings of the Eighth Annual Conference of the UW Centre for the New OED and Text Research,</booktitle>
<pages>76--89</pages>
<location>Waterloo, Canada.</location>
<contexts>
<context position="2222" citStr="Isabelle 1992" startWordPosition="334" endWordPosition="335">ingual corpora. Statistical machine translation (SMT) can be understood as a word-by-word model consisting of two submodels: a language model for generating a source text segment S and a translation model for mapping S to its translation T. They recommend using a bilingual corpus to train the parameters of translation probability, Pr(S j T) in the translation model. For MT and other purposes, many methods have been proposed for sentence alignment of the Hansards, an English-French corpus of Canadian parliamentary debates (Brown, Lai, and Mercer 1991; Gale and Church 1991a; Simard, Foster, and Isabelle 1992; Chen 1993; Gale and Church 1993), and for other language pairs, including English-German, EnglishChinese, and English-Japanese (Kay and Roscheisen 1993; Church, Dagan, Gale, Fung, Helfman, and Satish 1993; Fung and McKeown 1994; Wu 1994). Alignment at other levels of resolution is obviously useful. A section, paragraph, sentence, phrase, collocation, or word can be aligned to its translation (Kupiec 1993; Smadja, McKeown, and Hatzivassiloglou 1996). Other logical approaches involve aligning parse trees of a sentence and its translation (Matsumoto, Ishimoto, and Utsuro 1993; Meyers, Yangarber</context>
</contexts>
<marker>Isabelle, 1992</marker>
<rawString>Isabelle, P. 1992. Bi-textual aids for translators. In Proceedings of the Eighth Annual Conference of the UW Centre for the New OED and Text Research, pages 76-89, Waterloo, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
<author>M Roscheisen</author>
</authors>
<date>1993</date>
<booktitle>Text-translation alignment. Computational Linguistics,</booktitle>
<pages>19--1</pages>
<contexts>
<context position="2375" citStr="Kay and Roscheisen 1993" startWordPosition="353" endWordPosition="356">or generating a source text segment S and a translation model for mapping S to its translation T. They recommend using a bilingual corpus to train the parameters of translation probability, Pr(S j T) in the translation model. For MT and other purposes, many methods have been proposed for sentence alignment of the Hansards, an English-French corpus of Canadian parliamentary debates (Brown, Lai, and Mercer 1991; Gale and Church 1991a; Simard, Foster, and Isabelle 1992; Chen 1993; Gale and Church 1993), and for other language pairs, including English-German, EnglishChinese, and English-Japanese (Kay and Roscheisen 1993; Church, Dagan, Gale, Fung, Helfman, and Satish 1993; Fung and McKeown 1994; Wu 1994). Alignment at other levels of resolution is obviously useful. A section, paragraph, sentence, phrase, collocation, or word can be aligned to its translation (Kupiec 1993; Smadja, McKeown, and Hatzivassiloglou 1996). Other logical approaches involve aligning parse trees of a sentence and its translation (Matsumoto, Ishimoto, and Utsuro 1993; Meyers, Yangarber, and Grishman 1996), or simultaneously generating parse trees and alignment arrangements (Wu 1995). * Department of Computer Science, National Tsing Hua</context>
</contexts>
<marker>Kay, Roscheisen, 1993</marker>
<rawString>Kay, M. and M. Roscheisen. 1993. Text-translation alignment. Computational Linguistics, 19(1):121-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Klavans</author>
<author>E Tzoukermann</author>
</authors>
<title>The BICORD system.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics,</booktitle>
<pages>174--179</pages>
<location>Helsinki, Finland.</location>
<contexts>
<context position="3560" citStr="Klavans and Tzoukermann 1990" startWordPosition="509" endWordPosition="512">ent of Computer Science, National Tsing Hua University, Hsinchu, 30043, Taiwan, ROC. E-mail: ksj@volans.cs.scu.edu.tw; jschang@cs.nthu.edu.tw Â© 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 2 In addition to machine translation, many applications for aligned corpora have been suggested, including machine-aided translation (Shemtov 1993), translation assessment and critiquing tools (Isabelle 1992; des Tombe and Armstrong-Warwick 1993; Macklovitch 1994), text generation (Smadja 1992; Smadja, McKeown, and Hatzivassiloglou 1996), bilingual lexicography (Klavans and Tzoukermann 1990; Church and Gale 1991; Daille, Gaussier, and Lange 1994; Kupiec 1993; van der Eijk 1993; Li 1994; Wu and Xia 1994), and word-sense disambiguation (Gale, Church, and Yarowsky 1992; Chang, Chen, Sheng, and Ker 1996). For these applications, we must go one step further from sentence alignment and identify alignment at the word level. In the process of word alignment, the translation of each source word is identified. This study concentrates primarily on identifying alignment at the word level for a given sentence and its translation. In the context of SMT, Brown et al. (1993) present a series of</context>
</contexts>
<marker>Klavans, Tzoukermann, 1990</marker>
<rawString>Klavans, J. L. and E. Tzoukermann. 1990. The BICORD system. In Proceedings of the 13th International Conference on Computational Linguistics, pages 174-179, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Kupiec</author>
</authors>
<title>An algorithm for finding noun phrase correspondences in bilingual corpora.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting,</booktitle>
<pages>17--22</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, OH.</location>
<contexts>
<context position="2631" citStr="Kupiec 1993" startWordPosition="395" endWordPosition="396">ve been proposed for sentence alignment of the Hansards, an English-French corpus of Canadian parliamentary debates (Brown, Lai, and Mercer 1991; Gale and Church 1991a; Simard, Foster, and Isabelle 1992; Chen 1993; Gale and Church 1993), and for other language pairs, including English-German, EnglishChinese, and English-Japanese (Kay and Roscheisen 1993; Church, Dagan, Gale, Fung, Helfman, and Satish 1993; Fung and McKeown 1994; Wu 1994). Alignment at other levels of resolution is obviously useful. A section, paragraph, sentence, phrase, collocation, or word can be aligned to its translation (Kupiec 1993; Smadja, McKeown, and Hatzivassiloglou 1996). Other logical approaches involve aligning parse trees of a sentence and its translation (Matsumoto, Ishimoto, and Utsuro 1993; Meyers, Yangarber, and Grishman 1996), or simultaneously generating parse trees and alignment arrangements (Wu 1995). * Department of Computer Science, National Tsing Hua University, Hsinchu, 30043, Taiwan, ROC. E-mail: ksj@volans.cs.scu.edu.tw; jschang@cs.nthu.edu.tw Â© 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 2 In addition to machine translation, many applications for alig</context>
<context position="64128" citStr="Kupiec 1993" startWordPosition="11147" endWordPosition="11148"> Wu and Xai 1994), ClassAlign avoids most instances of these errors. In addition, with elaborate preprocessing such as parsing, phrase grouping, and collocation analysis (Smadja 1992), the problem of word-number difference 338 Sue J. Ker and Jason S. Chang Word Alignment Table 19 The final alignment of example (E20, C20). 1 2 3 4 5 6 7 8 9 10 He abdicated all responsibility for the care of the child fft lil* ----EZ *11 7 1 2 8 9 3 4 5 â¢7â¢ 6 can be averted by performing alignment at various levels: parse tree (Matsumoto, Ishimoto, and Utsuro 1993; Meyers, Yangarber, and Grishman 1996), phrase (Kupiec 1993), and collocation (Smadja, McKeown, and Hatzivassiloglou 1996). 5.2 Function Words, Collocation, and Free Translation Language-Specific Function Words. The morphological differences between English and Mandarin give rise to many language-specific function words. Such Mandarin function words are often quite ambiguous in part of speech as well as in word sense, leading to numerous alignment errors. For instance, ClassAlign connects the words for and of in (E20) erroneously to the morphemes 7 and Erg in (C20), respectively. Table 19 presents further details. (E20) He abdicated all responsibility </context>
</contexts>
<marker>Kupiec, 1993</marker>
<rawString>Kupiec, J. M. 1993. An algorithm for finding noun phrase correspondences in bilingual corpora. In Proceedings of the 31st Annual Meeting, pages 17-22, Columbus, OH. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C N Li</author>
<author>S A Thompson</author>
</authors>
<title>Mandarin Chinese-A Functional Reference Grammar.</title>
<date>1981</date>
<publisher>University of California Press,</publisher>
<location>Los Angeles, CA.</location>
<contexts>
<context position="51075" citStr="Li and Thompson (1981)" startWordPosition="8768" endWordPosition="8771">available in both languages to assess the method&apos;s robustness and generality. The similarities and differences between English and Mandarin texts are briefly reviewed, since our experiments involve the alignment of English-Mandarin parallel corpora. A general description of the materials used in the experiments follows. Finally, the success rates are quantitatively evaluated. 4.1 Contrastive Analysis of English and Mandarin Chinese Language typology is the study of similarities and differences between languages, formalized in terms of parameters such as word order and morphological structure. Li and Thompson (1981) examine Mandarin Chinese according to four typological parameters that reveal the basic structure of Mandarin Chinese as compared to those of other languages, English in particular. These four parameters are the morphological structure of words, the number of syllables per word, topic prominence, and word order. Li and Thompson&apos;s typological description of Mandarin is described below, from the perspective of the task of word alignment. 8 A small percentage of connections (7.8%) in our evaluation are incomplete ones and are considered to be correct. Melamed (1996) takes the same stance in his </context>
<context position="57401" citStr="Li and Thompson (1981)" startWordPosition="9927" endWordPosition="9930">nces. More specifically, a time phrase in preverbal position tends to denote punctual time, while that in postverbal position signals durative time, as in: (E14) I have a meeting at three o&apos;clock. (C14) ft El. FM I three o&apos;clock have-a-meeting (E15) I slept for three hours. (C15) Tffig. I sleep ASPECT three o&apos;clock (C15&apos;) fil .5-61 IM To I three o&apos;clock sleep ASPECT In contrast, both kinds of time phrase appear in postverbal position in English. As a result of facts such as these, many linguists contend that Mandarin is a language in transition from SVO to SOV. Further details can be found in Li and Thompson (1981). 335 Computational Linguistics Volume 23, Number 2 Similar to the situation created for topic prominent sentences, the SOV features of Mandarin represent a deviation from the SVO order of English. Such a deviation further worsens our ability to estimate the likelihood of a connection according to translational position. 4.2 The Experimental Setup The experimental results obtained from the proposed algorithm with respect to word alignment are presented in this section. Nearly 42,000 example sentences and their translations from the LecDOCE were used as training data, primarily to acquire rules</context>
</contexts>
<marker>Li, Thompson, 1981</marker>
<rawString>Li, C. N. and S. A. Thompson. 1981. Mandarin Chinese-A Functional Reference Grammar. University of California Press, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hung-Wen Li</author>
</authors>
<title>Word Alignment and Refinement of Transfer Dictionary. Master thesis,</title>
<date>1994</date>
<institution>Institute of Computer Science and Information Engineering, National Chiao Tung University,</institution>
<location>Taiwan, R.O.C.</location>
<contexts>
<context position="3657" citStr="Li 1994" startWordPosition="528" endWordPosition="529"> jschang@cs.nthu.edu.tw Â© 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 2 In addition to machine translation, many applications for aligned corpora have been suggested, including machine-aided translation (Shemtov 1993), translation assessment and critiquing tools (Isabelle 1992; des Tombe and Armstrong-Warwick 1993; Macklovitch 1994), text generation (Smadja 1992; Smadja, McKeown, and Hatzivassiloglou 1996), bilingual lexicography (Klavans and Tzoukermann 1990; Church and Gale 1991; Daille, Gaussier, and Lange 1994; Kupiec 1993; van der Eijk 1993; Li 1994; Wu and Xia 1994), and word-sense disambiguation (Gale, Church, and Yarowsky 1992; Chang, Chen, Sheng, and Ker 1996). For these applications, we must go one step further from sentence alignment and identify alignment at the word level. In the process of word alignment, the translation of each source word is identified. This study concentrates primarily on identifying alignment at the word level for a given sentence and its translation. In the context of SMT, Brown et al. (1993) present a series of five models of Pr(S I T) for word alignment. Model 1 assumes that Pr(S I T) depends only on lexi</context>
</contexts>
<marker>Li, 1994</marker>
<rawString>Li, Hung-Wen. 1994. Word Alignment and Refinement of Transfer Dictionary. Master thesis, Institute of Computer Science and Information Engineering, National Chiao Tung University, Taiwan, R.O.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longman Group</author>
</authors>
<title>Longman English-Chinese Dictionary of Contemporary English.</title>
<date>1992</date>
<publisher>Longman Group (Far East) Ltd.,</publisher>
<location>Hong Kong.</location>
<contexts>
<context position="8240" citStr="Group 1992" startWordPosition="1280" endWordPosition="1281">babilities, nor does it use an iterative EM algorithm for estimating such probabilities. Experimental results indicate that classification based on existing thesauri is highly effective in broadening coverage while maintaining a high precision rate. The rest of this paper is organized as follows: In Section 2 we briefly discuss the nature of text and translation that justifies a class-based approach. A set of three algorithms leading to class-based alignment are outlined in Section 3. The algorithms&apos; effectiveness is demonstrated through examples and their translations in the LecDOCE (Longman Group 1992), a bilingual version of the Longman Dictionary of Contemporary English (LDOCE, Proctor 1988), as well as sentences from bilingual texts in the LightShip User&apos;s Guide (Pilot Software Inc. 1993; Galaxy Software Services 1994). The experiments we undertook to assess the performance of these algorithms are the topic of Section 4. Quantitative experimental results are also summarized. In Section 5, we analyze the experimental results and consider ways in which the proposed algorithms might be extended and improved. Concluding remarks are made in Section 6. 2. Text and Translation as a Class-to-Cla</context>
</contexts>
<marker>Group, 1992</marker>
<rawString>Longman Group. 1992. Longman English-Chinese Dictionary of Contemporary English. Longman Group (Far East) Ltd., Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Macklovitch</author>
</authors>
<title>Using bi-textual alignment for translation validation: The TransCheck system.</title>
<date>1994</date>
<booktitle>In Proceedings of the First Conference of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<pages>157--168</pages>
<location>Columbia, MD.</location>
<contexts>
<context position="3431" citStr="Macklovitch 1994" startWordPosition="495" endWordPosition="496">angarber, and Grishman 1996), or simultaneously generating parse trees and alignment arrangements (Wu 1995). * Department of Computer Science, National Tsing Hua University, Hsinchu, 30043, Taiwan, ROC. E-mail: ksj@volans.cs.scu.edu.tw; jschang@cs.nthu.edu.tw Â© 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 2 In addition to machine translation, many applications for aligned corpora have been suggested, including machine-aided translation (Shemtov 1993), translation assessment and critiquing tools (Isabelle 1992; des Tombe and Armstrong-Warwick 1993; Macklovitch 1994), text generation (Smadja 1992; Smadja, McKeown, and Hatzivassiloglou 1996), bilingual lexicography (Klavans and Tzoukermann 1990; Church and Gale 1991; Daille, Gaussier, and Lange 1994; Kupiec 1993; van der Eijk 1993; Li 1994; Wu and Xia 1994), and word-sense disambiguation (Gale, Church, and Yarowsky 1992; Chang, Chen, Sheng, and Ker 1996). For these applications, we must go one step further from sentence alignment and identify alignment at the word level. In the process of word alignment, the translation of each source word is identified. This study concentrates primarily on identifying ali</context>
</contexts>
<marker>Macklovitch, 1994</marker>
<rawString>Macklovitch, E. 1994. Using bi-textual alignment for translation validation: The TransCheck system. In Proceedings of the First Conference of the Association for Machine Translation in the Americas (AMTA), pages 157-168, Columbia, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsumoto</author>
<author>H Ishimoto</author>
<author>T Utsuro</author>
</authors>
<title>Structural matching of parallel texts.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting,</booktitle>
<pages>22--30</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, OH.</location>
<marker>Matsumoto, Ishimoto, Utsuro, 1993</marker>
<rawString>Matsumoto, Y., H. Ishimoto, and T. Utsuro. 1993. Structural matching of parallel texts. In Proceedings of the 31st Annual Meeting, pages 22-30, Columbus, OH. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T McArthur</author>
</authors>
<title>Longman Lexicon of Contemporary English.</title>
<date>1992</date>
<publisher>Longman Group (Far East) Ltd.,</publisher>
<location>Hong Kong.</location>
<contexts>
<context position="15539" citStr="McArthur 1992" startWordPosition="2420" endWordPosition="2421">mited to synonyms that appear as an alternative translation frequently and consistently in a bilingual corpus. Bounding the ICTs. An ICT may deviate from the relevant DTs for a variety of reasons, but the deviation is not without constraints. Table 1 lists some examples of deviating translations taken from the LecDOCE. Examples include the words news, meet, lady, grief, care, and child and their respective translations irat , ttAgg, and /JNIA . Notice that most in-context and dictionary translations of source words are bounded within the same category in a typical thesaurus such as the LLOCE (McArthur 1992) and CILIN (Mei et al. 1993). For instance, Da/9-class words in CILIN (news and messages), MS, aria, tiM appear as the translations (DTs and ICTs) for Ge194-words in LLOCE (information and news) such as news and report. Similarly, 1d18-class words (hitting, touching, meeting, and missing), gx, AN, gig, iNg appear as the translations for Mc072-words (meeting people and things) such as meet and encounter. This finding suggests that LTP can be estimated more robustly via classto-class mapping. Furthermore, such ICTs and DTs are often synonymous compounds 317 Computational Linguistics Volume 23, N</context>
</contexts>
<marker>McArthur, 1992</marker>
<rawString>McArthur, T. 1992. Longman Lexicon of Contemporary English. Longman Group (Far East) Ltd., Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan W McRoy</author>
</authors>
<title>Using multiple knowledge sources for word sense discrimination.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--1</pages>
<contexts>
<context position="20961" citStr="McRoy 1992" startWordPosition="3264" endWordPosition="3265">ame five-character prefix as in Gale and Church (1991b), or rigorous analysis as suggested in Brown, Della Pietra, Della Pietra, Lafferty, and Mercer (1992). Although easily applicable, morphological classes are not particularly effective in broadening coverage of word alignment. Chang and Chen (1994) also examine the feasibility of using part-of-speech classes. A potential alternative involves adopting categories available in machine-readable lexicographic resources such as Roget&apos;s thesaurus (Chapman 1977) or hand-crafted computer lexicons (Miller, Beckwith, Fellbaum, Gross, and Miller 1990; McRoy 1992). 3. Algorithms Leading to Class-based Word Alignment This section describes a series of three algorithms leading to a class-based system for word alignment. The first algorithm attempts to obtain reliable connections. The second algorithm generalizes the connections into a list of class-based rules, which stipulate that a pair of classes of words in the source and target languages are likely mutual translations. The third algorithm performs the actual word alignment based on the acquired rules, in addition to DTs. 319 Computational Linguistics Volume 23, Number 2 3.1 Dictionary-based Word Ali</context>
</contexts>
<marker>McRoy, 1992</marker>
<rawString>McRoy, Susan W. 1992. Using multiple knowledge sources for word sense discrimination. Computational Linguistics, 18(1):1-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Mei</author>
<author>I M Zhu</author>
<author>Y C Gao</author>
<author>H S Yin</author>
</authors>
<title>Tongyici Cilin (Word Forest of Synonyms). Tong Hua, Taipei. (Traditional Chinese edition of a simplified Chinese edition published in</title>
<date>1993</date>
<contexts>
<context position="15567" citStr="Mei et al. 1993" startWordPosition="2424" endWordPosition="2427">ear as an alternative translation frequently and consistently in a bilingual corpus. Bounding the ICTs. An ICT may deviate from the relevant DTs for a variety of reasons, but the deviation is not without constraints. Table 1 lists some examples of deviating translations taken from the LecDOCE. Examples include the words news, meet, lady, grief, care, and child and their respective translations irat , ttAgg, and /JNIA . Notice that most in-context and dictionary translations of source words are bounded within the same category in a typical thesaurus such as the LLOCE (McArthur 1992) and CILIN (Mei et al. 1993). For instance, Da/9-class words in CILIN (news and messages), MS, aria, tiM appear as the translations (DTs and ICTs) for Ge194-words in LLOCE (information and news) such as news and report. Similarly, 1d18-class words (hitting, touching, meeting, and missing), gx, AN, gig, iNg appear as the translations for Mc072-words (meeting people and things) such as meet and encounter. This finding suggests that LTP can be estimated more robustly via classto-class mapping. Furthermore, such ICTs and DTs are often synonymous compounds 317 Computational Linguistics Volume 23, Number 2 Table 1 Disparity be</context>
</contexts>
<marker>Mei, Zhu, Gao, Yin, 1993</marker>
<rawString>Mei, J. J., I. M. Zhu, Y. C. Gao, and H. S. Yin. 1993. Tongyici Cilin (Word Forest of Synonyms). Tong Hua, Taipei. (Traditional Chinese edition of a simplified Chinese edition published in 1984.)</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Automatic construction of clean broad-coverage translation lexicons.</title>
<date>1996</date>
<booktitle>In Proceedings of the Second Conference of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<pages>125--134</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="51645" citStr="Melamed (1996)" startWordPosition="8856" endWordPosition="8857">phological structure. Li and Thompson (1981) examine Mandarin Chinese according to four typological parameters that reveal the basic structure of Mandarin Chinese as compared to those of other languages, English in particular. These four parameters are the morphological structure of words, the number of syllables per word, topic prominence, and word order. Li and Thompson&apos;s typological description of Mandarin is described below, from the perspective of the task of word alignment. 8 A small percentage of connections (7.8%) in our evaluation are incomplete ones and are considered to be correct. Melamed (1996) takes the same stance in his study of deriving a probabilistic lexicon. He observes that even incomplete entries are useful for many applications and there are ways of expanding incomplete morphemes or words in a connection, so that they become complete (Smadja 1992). 333 Sue J. Ker and Jason S. Chang Word Alignment dad in Table 13 Dislocation values for connection candidates (s, t) in (E10, 00). The old: lady a coat fur was 1 3 4 4 5 6 7 0 2 3 3 4 5 6 0 2 2 3 4 5 1 0 1 2 3 4 2 0 0 0 2 3 V* 3 0 1 0 2 1 4 2 0 0 1 A 5 3 2 0 0 6 1 0 1 2 3 4 Computational Linguistics Volume 23, Number 2 Table 14 </context>
</contexts>
<marker>Melamed, 1996</marker>
<rawString>Melamed, I. Dan. 1996. Automatic construction of clean broad-coverage translation lexicons. In Proceedings of the Second Conference of the Association for Machine Translation in the Americas (AMTA), pages 125-134, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>R Yangarber</author>
<author>R Grishman</author>
</authors>
<title>Alignment of shared forests for bilingual corpora.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics,</booktitle>
<pages>460--465</pages>
<location>Copenhagen,</location>
<marker>Meyers, Yangarber, Grishman, 1996</marker>
<rawString>Meyers, A., R. Yangarber, and R. Grishman. 1996. Alignment of shared forests for bilingual corpora. In Proceedings of the 16th International Conference on Computational Linguistics, pages 460-465, Copenhagen, Denmark. COLING-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K J Miller</author>
</authors>
<title>Introduction to Wordnet: An on-line lexical database.</title>
<date>1990</date>
<journal>Journal of Lexicography,</journal>
<pages>3--4</pages>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>Miller, G. A., R. Beckwith, C. Fellbaum, D. Gross, and K. J. Miller. 1990. Introduction to Wordnet: An on-line lexical database. Journal of Lexicography, 3(4):235-244.</rawString>
</citation>
<citation valid="true">
<title>LightShip User&apos;s Guide,</title>
<date>1993</date>
<publisher>Pilot Software Inc.,</publisher>
<institution>Pilot Software Inc.</institution>
<location>Boston.</location>
<contexts>
<context position="4140" citStr="(1993)" startWordPosition="609" endWordPosition="609">avans and Tzoukermann 1990; Church and Gale 1991; Daille, Gaussier, and Lange 1994; Kupiec 1993; van der Eijk 1993; Li 1994; Wu and Xia 1994), and word-sense disambiguation (Gale, Church, and Yarowsky 1992; Chang, Chen, Sheng, and Ker 1996). For these applications, we must go one step further from sentence alignment and identify alignment at the word level. In the process of word alignment, the translation of each source word is identified. This study concentrates primarily on identifying alignment at the word level for a given sentence and its translation. In the context of SMT, Brown et al. (1993) present a series of five models of Pr(S I T) for word alignment. Model 1 assumes that Pr(S I T) depends only on lexical translation probability (LTP) t(s I t), that is, the probability that the ith word s in S translates into the jth word t in T. The pair of words (s, t), or more precisely (s, t,i,j) since there could be more than one instance of s or t, is called a connection. Model 2 enhances Model 1 by considering the dependence of Pr(S I T) on the distortion probability (DP) d(i 11,1, m) where 1 and m are the respective lengths of S and T measured in number of words. Brown et al. (1990) p</context>
<context position="5701" citStr="(1993)" startWordPosition="888" endWordPosition="888">1, and m are estimated from the sample of an aligned bilingual corpus. In the maximization phase, each sentence-translation pair in the corpus is aligned by maximizing the translation probability, Pr(S 1 T). They examine the feasibility of aligning the English-French Hansards corpus using the SMT model, on both the sentence level and the word level. The SMT model is then tested for the task of machine translation. The model produces 35 acceptable translations for 73 sentences. However, to our knowledge, the degree of success of word alignment has not yet been explored. Dagan, Church, and Gale (1993) observe that reliably distinguishing sentence boundaries for a noisy bilingual text scanned by an OCR device is quite difficult. In such a circumstance, they recommend aligning words directly without the preprocessing phase of sentence alignment. Under that proposal, a rough character-by-character alignment is first performed. Based on the character alignment, words are subsequently aligned based on a modified version of Brown et al.&apos;s Model 2. The authors report that 60.5% of 65,000 words in a noisy document are correctly aligned. For 84% of the words, the offset from correct alignment is at</context>
<context position="17839" citStr="(1993)" startWordPosition="2774" endWordPosition="2774">nsibility for the care of the child. care PRIA .44 ftbk*T âlil-A/NA --C1(4-10 (N1366) (Hi37) (Hi37) We should advertise for someone to look after the look after .Y4 (Hi37) garden. (Nf162) (Hi37) fliMPERWMIBMA*110$44TEIZI. He abdicated all responsibility for the care of the child. child %HA IthMT âfaAikfAtrgâtihtil. (Ca003) (Ab04) (Ab04) John Smith? Yes - he&apos;s a local boy, I believe. boy A 93A Zigln? (Ca002) (Ab02) (Ab01) that share a common morpheme. For instance, the (ICT, DT) pairs, MN and RES) and (tcÂ± and tct) share a common morpheme Xt &apos;sad&apos; and tc &apos;female&apos;, respectively. Fujii and Croft (1993) also point out a similar thesaurus effect of Mandarin morphemes in Japanese information retrieval (IR).1 Dictionary-based Alignment. The above observations suggest that a DT-based algorithm, coupled with morpheme-level partial matching, can be adopted to obtain a substantial 1 Fujii and Croft observe that a document is likely to be relevant if it contains an index term that has a morpheme (kanji) in common with a query term. More often than not, the index term and the query term are synonyms that might appear under the same category in a thesaurus. The authors call this phenomenon the thesaur</context>
<context position="37866" citStr="(1993)" startWordPosition="6363" endWordPosition="6363">hreshold, 0.7 should be used. Second, the class-based rules acquired through the ClassRule Algorithm should capture the diversity of translations to a large extent. According to the observations in Section 2, the rules should stipulate most of the connections left out in the DictAlign step. Nevertheless, conflicting connections do occasionally arise. Such conflicts can be resolved according to an additional consideration of distortion mentioned in Section 1 Estimating the Likelihood of a Connection Candidate. The above observations can be stated formally from the perspective of Brown et al.&apos;s (1993) Model 2. As mentioned earlier, the model stipulates that a connection be given a probability value Pr(s, t), the product of lexical translation probability t(s I t) and distortion probability, d(i I j, 1, m). Also according to the model, we give each connection candidate a probabilistic value based on lexical and positional considerations: Pr(s, t) = t(s, t) x d(i, j) (6) We argue, however, that it is difficult to robustly estimate t(s, t) and d(i, j) for all the values of s, t, i, and j. Therefore, the two functions are defined and estimated by a limited number of cases, according to lexical</context>
<context position="54577" citStr="(1993)" startWordPosition="9457" endWordPosition="9457">or expressed through an additional function word. In contrast to this lack of inflectional morphological complexity, Mandarin is relatively rich in other types of morphological combinations, including compounding. These morphological differences result in a difference in the number of words in an English sentence and its Mandarin translation. In terms of alignment, this wordnumber difference means that multiword connections must be considered, a task which 334 Sue J. Ker and Jason S. Chang Word Alignment is beyond the reach of methods proposed in recent alignment works based on Brown et al.&apos;s (1993) Model 1 and 2. Basic Orientation of the Sentence: Topic vs. Subject. Another feature distinguishing Mandarin from other languages is topic prominence. In addition to the grammatical relation of subject, a description of Mandarin must include the topic element, which can be characterized as follows: First, a topic always comes first in the sentence and is optionally followed by a pause in speech. Second, a topic is the old information of which both the speaker and listener have some knowledge. Third, what distinguishes a topic from a subject is that the subject must always have a direct syntac</context>
</contexts>
<marker>1993</marker>
<rawString>Pilot Software Inc. 1993. LightShip User&apos;s Guide, Pilot Software Inc., Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Proctor</author>
</authors>
<title>Longman English-Chinese Dictionary of Contemporary English. Longman Group (Far East),</title>
<date>1988</date>
<location>Hong Kong.</location>
<contexts>
<context position="8333" citStr="Proctor 1988" startWordPosition="1294" endWordPosition="1295">perimental results indicate that classification based on existing thesauri is highly effective in broadening coverage while maintaining a high precision rate. The rest of this paper is organized as follows: In Section 2 we briefly discuss the nature of text and translation that justifies a class-based approach. A set of three algorithms leading to class-based alignment are outlined in Section 3. The algorithms&apos; effectiveness is demonstrated through examples and their translations in the LecDOCE (Longman Group 1992), a bilingual version of the Longman Dictionary of Contemporary English (LDOCE, Proctor 1988), as well as sentences from bilingual texts in the LightShip User&apos;s Guide (Pilot Software Inc. 1993; Galaxy Software Services 1994). The experiments we undertook to assess the performance of these algorithms are the topic of Section 4. Quantitative experimental results are also summarized. In Section 5, we analyze the experimental results and consider ways in which the proposed algorithms might be extended and improved. Concluding remarks are made in Section 6. 2. Text and Translation as a Class-to-Class Mapping The discussion in Section 1 indicates the limitations of statistical methods. As a</context>
</contexts>
<marker>Proctor, 1988</marker>
<rawString>Proctor, P. 1988. Longman English-Chinese Dictionary of Contemporary English. Longman Group (Far East), Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Shemtov</author>
</authors>
<title>Text alignment in a tool for translating revised documents.</title>
<date>1993</date>
<booktitle>In Proceedings of the Sixth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>449--453</pages>
<location>Utrecht, The Netherlands.</location>
<contexts>
<context position="3314" citStr="Shemtov 1993" startWordPosition="480" endWordPosition="481">s involve aligning parse trees of a sentence and its translation (Matsumoto, Ishimoto, and Utsuro 1993; Meyers, Yangarber, and Grishman 1996), or simultaneously generating parse trees and alignment arrangements (Wu 1995). * Department of Computer Science, National Tsing Hua University, Hsinchu, 30043, Taiwan, ROC. E-mail: ksj@volans.cs.scu.edu.tw; jschang@cs.nthu.edu.tw Â© 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 2 In addition to machine translation, many applications for aligned corpora have been suggested, including machine-aided translation (Shemtov 1993), translation assessment and critiquing tools (Isabelle 1992; des Tombe and Armstrong-Warwick 1993; Macklovitch 1994), text generation (Smadja 1992; Smadja, McKeown, and Hatzivassiloglou 1996), bilingual lexicography (Klavans and Tzoukermann 1990; Church and Gale 1991; Daille, Gaussier, and Lange 1994; Kupiec 1993; van der Eijk 1993; Li 1994; Wu and Xia 1994), and word-sense disambiguation (Gale, Church, and Yarowsky 1992; Chang, Chen, Sheng, and Ker 1996). For these applications, we must go one step further from sentence alignment and identify alignment at the word level. In the process of wo</context>
</contexts>
<marker>Shemtov, 1993</marker>
<rawString>Shemtov, H. 1993. Text alignment in a tool for translating revised documents. In Proceedings of the Sixth Conference of the European Chapter of the Association for Computational Linguistics, pages 449-453, Utrecht, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Simard</author>
<author>G F Foster</author>
<author>P Isabelle</author>
</authors>
<title>Using cognates to align sentences in bilingual corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-92),</booktitle>
<pages>67--81</pages>
<location>Montreal, Canada.</location>
<marker>Simard, Foster, Isabelle, 1992</marker>
<rawString>Simard, M., G. F. Foster, and P. Isabelle. 1992. Using cognates to align sentences in bilingual corpora. In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-92), pages 67-81, Montreal, Canada.</rawString>
</citation>
<citation valid="false">
<authors>
<author>F Smadja</author>
</authors>
<title>How to compile a bilingual collocation lexicon automatically.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<booktitle>In Proceedings of the AAAI-92 Workshop on Statistically-Based NLP Techniques,</booktitle>
<pages>65--71</pages>
<publisher>American Association for</publisher>
<location>San Jose, CA.</location>
<contexts>
<context position="3461" citStr="Smadja 1992" startWordPosition="499" endWordPosition="500">ultaneously generating parse trees and alignment arrangements (Wu 1995). * Department of Computer Science, National Tsing Hua University, Hsinchu, 30043, Taiwan, ROC. E-mail: ksj@volans.cs.scu.edu.tw; jschang@cs.nthu.edu.tw Â© 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 2 In addition to machine translation, many applications for aligned corpora have been suggested, including machine-aided translation (Shemtov 1993), translation assessment and critiquing tools (Isabelle 1992; des Tombe and Armstrong-Warwick 1993; Macklovitch 1994), text generation (Smadja 1992; Smadja, McKeown, and Hatzivassiloglou 1996), bilingual lexicography (Klavans and Tzoukermann 1990; Church and Gale 1991; Daille, Gaussier, and Lange 1994; Kupiec 1993; van der Eijk 1993; Li 1994; Wu and Xia 1994), and word-sense disambiguation (Gale, Church, and Yarowsky 1992; Chang, Chen, Sheng, and Ker 1996). For these applications, we must go one step further from sentence alignment and identify alignment at the word level. In the process of word alignment, the translation of each source word is identified. This study concentrates primarily on identifying alignment at the word level for a</context>
<context position="51913" citStr="Smadja 1992" startWordPosition="8899" endWordPosition="8900"> structure of words, the number of syllables per word, topic prominence, and word order. Li and Thompson&apos;s typological description of Mandarin is described below, from the perspective of the task of word alignment. 8 A small percentage of connections (7.8%) in our evaluation are incomplete ones and are considered to be correct. Melamed (1996) takes the same stance in his study of deriving a probabilistic lexicon. He observes that even incomplete entries are useful for many applications and there are ways of expanding incomplete morphemes or words in a connection, so that they become complete (Smadja 1992). 333 Sue J. Ker and Jason S. Chang Word Alignment dad in Table 13 Dislocation values for connection candidates (s, t) in (E10, 00). The old: lady a coat fur was 1 3 4 4 5 6 7 0 2 3 3 4 5 6 0 2 2 3 4 5 1 0 1 2 3 4 2 0 0 0 2 3 V* 3 0 1 0 2 1 4 2 0 0 1 A 5 3 2 0 0 6 1 0 1 2 3 4 Computational Linguistics Volume 23, Number 2 Table 14 The connection candidates (s, t) in (E10, C10) with higher Pr(s, t) values. Connection Candidates Lexical Translation Probability Distortion Prob TP i j s t ConceptSim(i, j) DTSim(i, j) t(s, t) dis(i, j) d(i, j) Pr (s, t) 1 1 The tri o o 0.011 0 0.414 0.005 1 2 The 42</context>
<context position="63699" citStr="Smadja 1992" startWordPosition="11065" endWordPosition="11066">ompounds. For instance, the following partially correct connections complicated by compounding are reported in a recent study on alignment of Hong Kong Basic Law (Fung and McKeown 1994). (E17) monoxide (C17) ---xtg (&apos;carbon monoxide&apos;) (E18) Basic (C18) M2MA (&apos;Basic Law&apos;) (E19) second (C19) .7_1411 (&apos;second reading&apos;) Because it is not limited to the connections involved in a presegmented target sentence (Fung and McKeown 1994; Wu and Xai 1994), ClassAlign avoids most instances of these errors. In addition, with elaborate preprocessing such as parsing, phrase grouping, and collocation analysis (Smadja 1992), the problem of word-number difference 338 Sue J. Ker and Jason S. Chang Word Alignment Table 19 The final alignment of example (E20, C20). 1 2 3 4 5 6 7 8 9 10 He abdicated all responsibility for the care of the child fft lil* ----EZ *11 7 1 2 8 9 3 4 5 â¢7â¢ 6 can be averted by performing alignment at various levels: parse tree (Matsumoto, Ishimoto, and Utsuro 1993; Meyers, Yangarber, and Grishman 1996), phrase (Kupiec 1993), and collocation (Smadja, McKeown, and Hatzivassiloglou 1996). 5.2 Function Words, Collocation, and Free Translation Language-Specific Function Words. The morphological d</context>
</contexts>
<marker>Smadja, 1992</marker>
<rawString>Smadja, F. 1992. How to compile a bilingual collocation lexicon automatically. In Proceedings of the AAAI-92 Workshop on Statistically-Based NLP Techniques, pages 65-71, San Jose, CA. American Association for Artificial Intelligence. Smadja, F., K. R. McKeown, and V. Hatzivassiloglou. 1996. Translating collocations for bilingual lexicons: A statistical approach. Computational Linguistics, 22(1):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>des Tombe</author>
<author>L</author>
<author>S Armstrong-Warwick</author>
</authors>
<title>Using function words to measure translation quality.</title>
<date>1993</date>
<booktitle>In Proceedings of the Ninth Annual Conference of the UW Centre for the New OED and Text Research,</booktitle>
<pages>1--17</pages>
<marker>Tombe, L, Armstrong-Warwick, 1993</marker>
<rawString>des Tombe, L. and S. Armstrong-Warwick. 1993. Using function words to measure translation quality. In Proceedings of the Ninth Annual Conference of the UW Centre for the New OED and Text Research, pages 1-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Aligning a parallel English-Chinese corpus statistically with lexical criteria.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting,</booktitle>
<pages>80--87</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Las Cruces, NM.</location>
<contexts>
<context position="2461" citStr="Wu 1994" startWordPosition="369" endWordPosition="370">y recommend using a bilingual corpus to train the parameters of translation probability, Pr(S j T) in the translation model. For MT and other purposes, many methods have been proposed for sentence alignment of the Hansards, an English-French corpus of Canadian parliamentary debates (Brown, Lai, and Mercer 1991; Gale and Church 1991a; Simard, Foster, and Isabelle 1992; Chen 1993; Gale and Church 1993), and for other language pairs, including English-German, EnglishChinese, and English-Japanese (Kay and Roscheisen 1993; Church, Dagan, Gale, Fung, Helfman, and Satish 1993; Fung and McKeown 1994; Wu 1994). Alignment at other levels of resolution is obviously useful. A section, paragraph, sentence, phrase, collocation, or word can be aligned to its translation (Kupiec 1993; Smadja, McKeown, and Hatzivassiloglou 1996). Other logical approaches involve aligning parse trees of a sentence and its translation (Matsumoto, Ishimoto, and Utsuro 1993; Meyers, Yangarber, and Grishman 1996), or simultaneously generating parse trees and alignment arrangements (Wu 1995). * Department of Computer Science, National Tsing Hua University, Hsinchu, 30043, Taiwan, ROC. E-mail: ksj@volans.cs.scu.edu.tw; jschang@cs</context>
</contexts>
<marker>Wu, 1994</marker>
<rawString>Wu, D. 1994. Aligning a parallel English-Chinese corpus statistically with lexical criteria. In Proceedings of the 32nd Annual Meeting, pages 80-87, Las Cruces, NM. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Grammarless extraction of phrasal translation examples from parallel texts.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>354--372</pages>
<contexts>
<context position="2921" citStr="Wu 1995" startWordPosition="434" endWordPosition="435"> EnglishChinese, and English-Japanese (Kay and Roscheisen 1993; Church, Dagan, Gale, Fung, Helfman, and Satish 1993; Fung and McKeown 1994; Wu 1994). Alignment at other levels of resolution is obviously useful. A section, paragraph, sentence, phrase, collocation, or word can be aligned to its translation (Kupiec 1993; Smadja, McKeown, and Hatzivassiloglou 1996). Other logical approaches involve aligning parse trees of a sentence and its translation (Matsumoto, Ishimoto, and Utsuro 1993; Meyers, Yangarber, and Grishman 1996), or simultaneously generating parse trees and alignment arrangements (Wu 1995). * Department of Computer Science, National Tsing Hua University, Hsinchu, 30043, Taiwan, ROC. E-mail: ksj@volans.cs.scu.edu.tw; jschang@cs.nthu.edu.tw Â© 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 2 In addition to machine translation, many applications for aligned corpora have been suggested, including machine-aided translation (Shemtov 1993), translation assessment and critiquing tools (Isabelle 1992; des Tombe and Armstrong-Warwick 1993; Macklovitch 1994), text generation (Smadja 1992; Smadja, McKeown, and Hatzivassiloglou 1996), bilingual lex</context>
<context position="40748" citStr="Wu (1995)" startWordPosition="6853" endWordPosition="6854">e monotonicity of translational position with respect to context. Such a formulation is inspired by Gale and Church&apos;s (1991b) treatment of distortion. In their study, the authors replace the distortion probability with a probability function defined by different values of slope, a measure of the position of t with respect to the left context of s. This measure is generally quite accurate, leading to a distribution function concentrating at slope 1. Nevertheless, room for improvement still exists, as can be illustrated using the concept of a binary inversion transduction tree (ITT) proposed by Wu (1995). The ITT is a shared parse tree depicting the structural difference between a sentence S and its translation T. Figure 2 presents the ITT of (E12, C12). The horizontal bar denotes that the noun phrase such a lazy mortal and the prepositional phrase as you are inverted when translated into Mandarin. The slope of the first word in such an inverted structure is typically quite large, making the distribution of the slope function slightly flat. If multiword structural inversion occurs, as it frequently does, then the slope of the first word according to the right context is still very small. (E12</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>Wu, D. 1995. Grammarless extraction of phrasal translation examples from parallel texts. In Proceedings of the Sixth International Conference on Theoretical and Methodological Issues in Machine Translation, pages 354-372, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
<author>X Xia</author>
</authors>
<title>Learning an English-Chinese lexicon from a parallel corpus.</title>
<date>1994</date>
<booktitle>In Proceedings of the First Conference of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<pages>206--213</pages>
<location>Columbia, MD.</location>
<contexts>
<context position="3675" citStr="Wu and Xia 1994" startWordPosition="530" endWordPosition="533">cs.nthu.edu.tw Â© 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 2 In addition to machine translation, many applications for aligned corpora have been suggested, including machine-aided translation (Shemtov 1993), translation assessment and critiquing tools (Isabelle 1992; des Tombe and Armstrong-Warwick 1993; Macklovitch 1994), text generation (Smadja 1992; Smadja, McKeown, and Hatzivassiloglou 1996), bilingual lexicography (Klavans and Tzoukermann 1990; Church and Gale 1991; Daille, Gaussier, and Lange 1994; Kupiec 1993; van der Eijk 1993; Li 1994; Wu and Xia 1994), and word-sense disambiguation (Gale, Church, and Yarowsky 1992; Chang, Chen, Sheng, and Ker 1996). For these applications, we must go one step further from sentence alignment and identify alignment at the word level. In the process of word alignment, the translation of each source word is identified. This study concentrates primarily on identifying alignment at the word level for a given sentence and its translation. In the context of SMT, Brown et al. (1993) present a series of five models of Pr(S I T) for word alignment. Model 1 assumes that Pr(S I T) depends only on lexical translation pr</context>
</contexts>
<marker>Wu, Xia, 1994</marker>
<rawString>Wu, D. and X. Xia. 1994. Learning an English-Chinese lexicon from a parallel corpus. In Proceedings of the First Conference of the Association for Machine Translation in the Americas (AMTA), pages 206-213, Columbia, MD.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>