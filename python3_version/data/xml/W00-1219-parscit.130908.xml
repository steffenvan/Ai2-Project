<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000021">
<title confidence="0.9986795">
Extraction of Chinese Compound Words —
An Experimental Study on a Very Large Corpus
</title>
<author confidence="0.99925">
Jian Zhang Jianfeng Gao, Ming Thou
</author>
<affiliation confidence="0.999756">
Department of Computer Science and Microsoft Research China
Technology of Tsinghua University, China { jfgao, mingzhou}@microsoft.com
</affiliation>
<email confidence="0.982293">
ajian@s1000e.cs.tsinghua.edu.cn
</email>
<sectionHeader confidence="0.97158" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999895583333333">
This paper is to introduce a statistical
method to extract Chinese compound words
from a very large corpus&apos;. This method is
based on mutual information and context
dependency. Experimental results show that
this method is efficient and robust
compared with other approaches. We also
examined the impact of different parameter
settings, corpus size and heterogeneousness
on the extraction results. We finally present
results on information retrieval to show the
usefulness of extracted compounds.
</bodyText>
<sectionHeader confidence="0.998484" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997334672727273">
Almost all techniques to statistical language
processing, including speech recognition, machine
translation and information retrieval, are based on
words. Although word-based approaches work
very well for western languages, where words are
well defined, it is difficult to apply to Chinese.
Chinese sentences are written as characters strings
with no spaces between words. Therefore, words
in Chinese are actually not well marked in
sentences, and there does not exist a commonly
accepted Chinese lexicon.
Furthermore, since new compounds (words
formed with at least two characters) are constantly
created, it is impossible to list them exhaustively
in a lexicon. Therefore, automatic extraction of
compounds is an important issue. Traditional
extraction approaches used rules. However,
compounds extracted in this way are not always
desirable. So, human effort is still required to find
the preferred compounds from a large compound
This work was done while the author worked for
Microsoft Research China as a visiting student.
candidate list. Some statistical approaches to
extract Chinese compounds from corpus have
been proposed (Lee-Feng Chien 1997, WU Dekai
and Xuanyin XIA 1995, Ming-Wen Wu and Keh-
Yih Su 1993) as well, but almost all experiments
are based on relatively small corpus, it is not clear
whether these methods still work well with large
corpus.
In this paper, we investigate statistical
approaches to Chinese compound extraction from
very large corpus by using statistical features,
namely mutual information and context
dependency. There are three main contributions in
this paper. First, we apply our procedure on a very
large corpus while other experiments were based
on small or medium size corpora. We show that
better results can be obtained with a large corpus.
Second, we examine how the results can be
influenced by parameter settings including mutual
information and context dependency restrictions.
It turns out that mutual information mainly affects
precision while context dependency affects the
count of extracted items. Third, we test the
usefulness of the extracted compounds for
information retrieval. Our experimental results on
IR show that the new compounds have a positive
effect on IR.
The rest of this paper is structured as follows. In
section 2, we describe the techniques we used. In
section 3, we present several sets of experimental
results. In section 4, we outline the related works
as well as their results. Finally, we give our
conclusions in section 5.
</bodyText>
<sectionHeader confidence="0.714827" genericHeader="method">
2 Technique description
</sectionHeader>
<bodyText confidence="0.9966445">
Statistical extraction of Chinese compounds has
been used in (Lee-Feng Chien 1997)(WU Dekai
and Xuanyin XIA 1995) and (Ming-Wen Wu and
Keh-Yih Su 1993). The basic idea is that a
</bodyText>
<page confidence="0.996793">
132
</page>
<bodyText confidence="0.999753545454545">
Chinese compound should appear as a stable
sequence in corpus. That is, the components in the
compound are strongly correlated, while the
components lie at both ends should have low
correlations with miter words.
The method consists of two steps. At first, a list
of candidate compounds is extracted from a very
large corpus by using mutual information. Then,
context dependency is used to remove undesirable
compounds. In what follows, we will describe
them in more detail.
</bodyText>
<subsectionHeader confidence="0.998228">
2.1 Mutual Information
</subsectionHeader>
<bodyText confidence="0.975825636363636">
According to our study on Chinese corpora,
most compounds are of length less than 5
characters. The average length of words in the
segmented-corpus is of approximately 1.6
characters. Therefore, only word bi-gram, tri-
gram, and quad-gram in the corpus are of interest
to us in compound extraction.
We use a criterion, called mutual information,
to evaluate the correlation of different components
in the compound. Mutual information MI(x,y) of a
bi-gram (x, y) is estimated by:
</bodyText>
<equation confidence="0.9956715">
MI (x, y) — f (x, y)
M/(x, y,z) = f (x)+ f (y) + f(z)— f(x, y,z)
</equation>
<bodyText confidence="0.99968925">
The estimation of mutual information of quad-
grams is similar to that of tri-grams. The extracted
compounds should be of higher value of MI than a
pre-set threshold.
</bodyText>
<subsectionHeader confidence="0.745274">
2.2 Context Dependency
clikg
</subsectionHeader>
<bodyText confidence="0.948646615384615">
Figure 1
The extracted Chinese compounds should be
complete. That is, we should generate a whole
word, not a part of it. For example,
egMf41-41,11(missile defense plan) is a
complete word, and 43M41411 (missile defense) is
not, although both have relatively high value of
mutual information.
Therefore, we use another feature, called
context dependency. The contexts of the word
Mf4l(defense) are illustrated by figure 1.
A compound X has NO left context dependency
if
</bodyText>
<equation confidence="0.8155715">
L.Size =I L I&gt; tl or
f (aX) &lt; t2
MaxL = MAX
f (X)
Where ti, t2 are threshold value, fr.) is
frequency, L is the set of left adjacent strings of X,
(XEL and ILI means the number of unique left
adjacent strings. Similarly, a compound X has NO
right context dependency if
RSize =R I&gt; t3 or
MaxR = MAX A f (151V) &lt; t4
f (X)
</equation>
<bodyText confidence="0.931292833333333">
Where ti, t2, t3, t4 are threshold value, fr.) is
frequency, R is the set of right adjacent strings of
X, I3ER and IRI means the number of unique left
adjacent strings.
The extracted complete compounds should have
neither left nor right context dependency.
</bodyText>
<sectionHeader confidence="0.964206" genericHeader="method">
3 Experimental results
</sectionHeader>
<bodyText confidence="0.999530636363636">
In our experiments, three corpora were used to
test the performance of the presented approach.
These corpora are described in table 1. Corpus A
consists of local news with more than 325 million
characters. Corpus B consists of documents from
different domains of novel, news, technique
report, etc., with approximately 650 million
characters. Corpus C consists of People&apos;s Daily
news and Xinhua news from TREC5 and TREC6
(Harman and Voorhees, 1996) with 75 million
characters.
</bodyText>
<tableCaption confidence="0.994313">
Table 1: Characteristics of Corpora
</tableCaption>
<figure confidence="0.736373833333333">
Corpus Source Size(char#)
Corpus political, economic news 325 M
A
Corpus Corpus A + novels + 650 M
B technique reports, etc.
f (x) + f (y) - f (x, Y)
</figure>
<bodyText confidence="0.996644428571429">
Where f(x) is the occurrence frequency of word
x in the corpus, and f(x,y) is the occurrence
frequency of the word pair (x,y) in the corpus. The
higher the value of MI is, the more likely x and y
are to form a compound.
The mutual information M1(x,y,z) of tri-gram
(x,y,z) is estimated by:
</bodyText>
<page confidence="0.982505">
133
</page>
<note confidence="0.442637">
Corpus
</note>
<bodyText confidence="0.957196592592593">
TREC 5/6 Chinese
corpus
In the first experiment, we test the performance
of our method on corpus A, which is homogeneity
in style. We then use corpus B in the second
experiment to test if the method works as well on
the corpus that is heterogeneity in style. We also
use different parameter settings in order to figure
out the best combination of the two statistical
features, i.e. mutual information and context
dependency. In the third experiment, we apply the
results of the method to information retrieval
system. We extract new compounds on corpus C,
and add them to the indexing lexicon, and we
achieve a higher average precision-recall. In all
experiments, corpora are segmented automatically
into words using a lexicon consisting of 65,502
entries.
3.1 Compounds Extraction from Homogeneous
Corpus
Corpus A contains political and economic news.
In this series of tests, we gradually loosen the
conditions to form a compound, i.e. MI threshold
becomes smaller and MaxL/MaxR becomes larger.
Results for quad-grams, tri-grams and bi-grams
are shown in tables 2,3,4. Some compounds
extracted are listed in table 5.
</bodyText>
<page confidence="0.162107">
75 M
</page>
<tableCaption confidence="0.588443">
Table2: Performance of quad-gram compounds extraction
</tableCaption>
<table confidence="0.998135714285714">
Parameter setting Number of New Precision (correct
(MI, LSize, MaxL, RSize, MaxR) compounds found compounds/compounds checked)
1 0.01 1 0.75 1 0.75 27 100% (27/27)
2 0.005 1 0.85 1 0.85 92 98.9% (91/92)
3 0.002 1 0.90 1 0.90 513 95.8% (113/118)
4 0.001 1 0.95 1 0.95 1648 96.2% (179/186)
5 0.0005 1 0.95 1 0.95 4707 96.7% (206/213)
</table>
<tableCaption confidence="0.807685">
Table3: Performance of tri- ram compounds extraction
</tableCaption>
<table confidence="0.999645285714286">
Parameter setting Number of New Precision (correct
(MI, LSize, MaxL, RSize, MaxR) compounds found compounds/compounds checked)
1 0.02 2 0.70 2 0.70 167 100% (167/167)
2 0.01 2 0.75 2 0.75 538 100% (205/205)
3 0.005 2 0.80 2 0.80 1607 100% (262/262)
4 0.003 2 0.80 2 0.80 3532 98.3% (341/347)
5 0.001 2 0.80 2 0.80 16849 96.6% (488/501)
</table>
<tableCaption confidence="0.88606">
Table4: Performance of bi- ram compounds extraction
</tableCaption>
<table confidence="0.999676">
Parameter setting Number of New Precision (correct
(MI, LSize, MaxL, RSize, MaxR) compounds found compounds/compounds checked)
1 0.05 3 0.5 3 0.5 1622 98.9% (184/186)
2 0.05 3 0.6 3 0.6 1904 98.6% (309/212)
3 0.03 3 0.6 3 0.6 3938 97.8% (218/223)
4 0.01 3 0.5 3 0.5 14666 97.5% (354/363)
5 0.005 3 0.5 3 0.5 32899 97.3% (404/415)
</table>
<tableCaption confidence="0.993297">
Table 5: Some N- ram compounds found by our method
</tableCaption>
<table confidence="0.9980232">
N-gram Extracted Compounds
N=2 OW (grain depot).. YL jEgbii (CD-ROM Dr i v er) . AR (Bill Gates)
N=3 Amin (XuanWu Gate) . RPIIMIAA (asynchronous transfer model) . 3E1413a
(Amazon)
N=4 IlTiltIV; (Elysee) . fRIAlifti3t1 (Ohio).. 21311*I5t (Mr. Dong Jianhua)
</table>
<page confidence="0.994142">
134
</page>
<bodyText confidence="0.999972473684211">
It turns out that our algorithm successfully
extracted a large number of new compounds
(&gt;50000) from raw texts. Compared with previous
methods described in the next section, the
precision is very high. We can also find that there
is little precision loss when we loose restriction.
The result may be due to three reasons. First, the
two statistical features really characterize the
nature of compounds, and provide a simple and
efficient way to estimate the possibility of a word
sequence being a compound. Second, the corpus
we use is very large. It is always true that more
data leads to better results. Third, the corpus we
used in this experiment is homogeneity in style.
The raw corpus is composed of news on politics,
economy, science and technology. These are
formal articles, and the sentences and compounds
are well normalized and strict. This is very helpful
for compound extraction.
</bodyText>
<subsectionHeader confidence="0.9975215">
3.2 Compounds Extraction from Heterogeneous
Corpus
</subsectionHeader>
<bodyText confidence="0.9515353">
In this experiment, we use a heterogeneous
corpus. It is a combination of corpus A, and some
other novels, technique reports, etc. For
simplicity, we discuss the extraction of bi-gram
compounds only. In comparison with the first
experiment, we find that the precision is strongly
affected by the corpus we used. As shown in table
6, for each corpus, we use the same parameter
setting, say MI &gt;0.005, LSize &gt;3, MaxL &lt;0.5,
R5ize&gt;3 and MaxR&lt;0.5.
</bodyText>
<tableCaption confidence="0.985859">
Table 6: Im act of heterogeneousness of co ora
</tableCaption>
<table confidence="0.997948333333333">
Corpus Compounds Extract
extracted precision
Corpus A 32899 97.3%
(404/415)
Corpus B 36383 88.3%
(362/410)
</table>
<bodyText confidence="0.999822238095238">
As we mentioned early, the larger the corpus we
use, the better results we obtain. Therefore, we
intuitively expect better result on corpus B, which
is larger than corpus A. But, the result shown in
table 6 is just the opposite.
There are mainly two reasons for this. The first
one is that our method works better on
homogeneous corpus than on heterogeneous
corpus. The second one is that it might not be
suitable to use the same parameter settings on two
different corpora. We then try different parameter
settings on corpus B.
There are two groups of parameters. MI
measures the correlation between adjacent words,
and other four parameters, namely LSize, RSize,
MaxL, and MaxR, measure the context
dependency. Therefore, each time, we fix one
parameter, and relax another from tight to loose to
see what happens. The Number of extracted
compounds and precision of each parameter
setting are shown in table 7.
</bodyText>
<tableCaption confidence="0.98038">
Table 7: Extraction results with different parameter settings
</tableCaption>
<table confidence="0.968889411764706">
(MI=Mutual information, CD = Context De enden =(LSize, MaxL, RSize, MaxR)
(2,0.8, 2, (6,0.7, 6, (10,0.6, (14, 0.5, 4, (18, 0.4, (22, 0.3, (26, 0.2, 6,
MI\CD 0.8) 0.7) 10,0.6) 0.5) 18,0.4) 22,0.3) 0.2)
0.0002 1457781 809502 570601 426223 314810 209910 96383
(39.06%) (42.24%) (43.98%) (44.67%) (43.96%) (43.38%) (40.93%)
0.0004 784082 485143 359499 277673 209634 141215 63907
(48.98%) (46.84%) (52.53%) (49.25%) (53.92%) (49.55%) (52.53%)
0.0006 530723 349882 266068 208921 159363 108120 48683
(51.28%) (53.96%) (60.39%) (52.48%) (49.49%) (63.35%) (61.65%)
0.0008 396602 273231 211044 167660 128819 87869 39502
(54.63%) (58.00%) (55.19%) (65.24%) (60.54%) (64.40%) (54.86%)
0.0010 313868 223827 175050 140197 108322 74104 33354
(59.11%) (66.51%) (61.14%) (57.66%) (67.38%) (63.08%) (67.50%)
0.0012 257990 189014 149315 120312 93323 64079 28879
(58.94%) (59.50%) (60.98%) (65.28%) (70.47%) (65.32%) (64.65%)
0.0014 217766 163189 129978 105334 82083 56582 25486
(58.93%) (67.91%) (60.19%) (65.84%) (66.83%) (67.50%) (65.46%)
</table>
<page confidence="0.998023">
135
</page>
<bodyText confidence="0.996666181818182">
Table 7 shows the extraction results with
different parameters. These results fit our
intuition. While parameters become more and
more strict, less and less compounds are found
and precisions become higher. This phenomena is
also illustrated in figure 2 and 3. in which the
&amp;quot;correct compounds extracted&amp;quot; is an estimation
from table7, i.e. number of compounds found X
precision. (These two figures are very useful for
one who wants to automatically extract a new
lexicon with pre-defined size from a large corpus.)
</bodyText>
<figureCaption confidence="0.993948">
Figure 2 Impact of Parameter Mutual Information
</figureCaption>
<figure confidence="0.993703454545455">
(2 0. 8 2 0.8)
(6 0. 7 6 0.7)
600
c) 200
a9
• 100
0
1 2 3 4 5
mutual information
-- (10 0.6 10 0.6)
(14 0.5 14 0.5)
(18 0.4 18 0. 4)
(22 0. 3 22 0. 3)
(26 0. 2 26 O. 2)
6 7
—0— raj x=0. 0002
mi x=0. 0004
mix=0. 0006
mix=0. 0008
—31E— m i x=0. 0010
—e— mi x=0. 0012
- mi x=0. 0014
1 2 3 4 5 6 7
context dependency
7, 600
• 500
• 400
&amp;quot;CS
300
Ei
8 200
c1.9 100
(.9 0
</figure>
<figureCaption confidence="0.99678">
Figure 3 Impact of Parameter Context Dependency
</figureCaption>
<page confidence="0.996943">
136
</page>
<bodyText confidence="0.9995677">
The precision of extraction is estimated in the
following way. We extract a set of compounds
based on a series of pre-defined parameter set. For
each set of compounds, we randomly select 200
compounds. Then we merge those selected
compounds to a new file for manually check. This
file consists of about 9,800 new compounds
because there are 49 compounds lists. One person
will group these &apos;compounds&apos; into two sets, say
set A and set B. Set A contains the items that are
considered to be correct, and set B contains
incorrect ones. Then for each original group of
about 200 compounds we select in the first step,
we check how many items that also appear in set
A and how many items in set B. Suppose these
two values are al and bl, then we estimate the
precision as al/(al+b1).
So, there are two important points in our
evaluation process. First, it is difficult to give a
definition of the term &amp;quot;compound&amp;quot; to be accepted
popularly. Different people may have different
judgement. Only one person takes part in the
evaluation in our experiment. This can eliminate
the effect of divergence among different persons.
Second, we merge those items together. This can
eliminate the effect of different time period. One
may feel tired after checked too many items. If he
checks those 49 files one by one, the latter results
are incomparable with the previous one.
The precisions estimated by the above method
are not exactly correct. However, as described
above, the precisions of different parameter
settings are comparable. In this experiment, what
we want to show is how the parameter settings
affect the results.
Both MI and CD can affect number of extracted
compounds, as shown in table 7. Compared with
MI, CD has stronger effect in this aspect. For each
row in table 7, numbers of extracted compounds
finally decrease to 10% of that showed in the first
column. For each column, while MI changes from
0.0002 to 0.0014, the number is decreased of
about 20%. This may be explained by the fact that
it is difficult for candidate to fulfill all four
restrictions in CD simultaneously. Many
disqualified candidates are cut off. Table 7 lists
the precisions of extracted results. It shows that
there is no clear increasing/decreasing pattern in
each row. That is to say, CD doesn&apos;t strongly
affect the precision. When we check each column,
we can see that precision is in a growing progress.
As we defined above, MI and CD are two
different measurements. What role they play in
our extraction procedure? Our conclusion is that
mutual information mainly affects the precision
while context dependency mainly affects the count
of extracted items. This conclusion is also
confirmed by Fig2 and Fig3. That is, the curves in
Fig2 are more flat than corresponding curves in
Fig3.
</bodyText>
<subsectionHeader confidence="0.9983385">
33 Testing the Extracted Compounds in
Information Retrieval
</subsectionHeader>
<bodyText confidence="0.999580029411765">
In this experiment, we apply our method to
improve information retrieval results. We use
SMART system (Buckley 1985) for our
experiments. SMART is a robust, efficient and
flexible information retrieval system. The corpus
used in this experiment is TREC Chinese corpus
(Harman and Voorhees, 1996). The corpus
contains about 160,000 articles, including articles
published in the People&apos;s Daily from 1991 to
1993, and a part of the news released by the
Xinhua News Agency in 1994 and 1995. A set of
54 queries has been set up and evaluated by
people in NIST(National Institute of Standards
and Technology).
We first use an initial lexicon consisting of
65,502 entries to segment the corpus. When
running SMART on the segmented corpus, we
obtain an average precision of 42.90%.
Then we extract new compounds from the
segmented corpus, and add them into the initial
lexicon. With the new lexicon, the TREC Chinese
corpus is re-segmented. When running SMART
on this re-segmented corpus, we obtain an average
precision of 43.42%, which shows a slight
improvement of 1.2%.
Further analysis shows that the new lexicon
brings positive effect to 10 queries and negative
effect to 4 queries. For other 40 queries, there is
no obvious effect. Some improved queries are
listed in table 8 as well as new compounds being
contained.
As an example, we give the segmentation
results with the two lexicons for query 23 in table
9.
</bodyText>
<page confidence="0.998263">
137
</page>
<tableCaption confidence="0.997547">
Table 8: Improved Query Samples
</tableCaption>
<table confidence="0.999895">
Query Base line New Improvement Extracted compounds
ID precision precision
9 0.3648 0.4173 14.4% 44733-c Mdrugs
sale), 41 lila A (ij a(Drug Problems
in China)
23 0.3940 0.5154 30.8% V*KII-4311*(the UN Security
Council),fIlTga(peace proposal)
30 0.3457 0.3639 5.3%
46 0.3483 0.4192 20.4% 41A(China and Vietnam)
47 0.5369 0.5847 8.9% AO] 111.114)( di (Mount
Minatubo),AVM(ozone layer),
Lt A(Subic)
</table>
<tableCaption confidence="0.994046">
Table 9: Segmented Corpus with the Two Lexicons for Query 23
Query 23 segment with small lexicon
</tableCaption>
<figure confidence="0.513006571428571">
4-11) 3c4t VCI 44MR41- rI3 friS , &amp;quot;a4t 4 111-tfLA IYJ
, 3:1CA [a-g-m] [gzoi-A-] 113 1143 4)(tira ,kbk3
[VT] Mal
Query 23 segment with new lexicon
tElA c&apos;ft Etm vrt *m&amp;t4i- tui14 4:13_41-1111fflM frit , &apos;g16 4 VilkA tiq
4g3i1 , VFA [v*Ema-A-1 Affl •fA-14-ia , MOM 1)3-tkA #fiCtW
fi&lt;J [4l-T3ta]
</figure>
<bodyText confidence="0.997453166666667">
Another interesting example is query 30. There
is no new compound extracted from that query. Its
result is also improved significantly because its
relevant documents are segmented better than
before.
Because the compounds extracted from the
corpus are not exactly correct, the new lexicon
will bring negative effect to some queries, such as
query 10. The retrieval precision changes from
0.3086 to 0.1359. The main reason is that
-&apos;1 iff&amp;quot;(Chinese Xinliang) is taken as a new
compound in the query.
</bodyText>
<sectionHeader confidence="0.999342" genericHeader="method">
4 Related works
</sectionHeader>
<bodyText confidence="0.929119842105263">
Several methods have been proposed for
extracting compounds from corpus by statistical
approaches. In this section, we will briefly
describe some of them.
(Lee-Feng Chien 1997) proposed an approach
based on PAT-Tree to automatically extracting
domain specific terms from online text
collections. Our method is primary derived from
(Lee-Feng Chien 1997), and use the similar
statistical features, i.e. mutual information and
context dependency. The difference is that we use
n-gram instead of PAT-Tree, due to the efficiency
issue. Another difference lies in the expenments.
In Chien&apos;s work, only domain specific terms are
extracted from domain specific corpus, and the
size of the corpus is relatively small, namely
1,872 political news abstracts.
(Cheng-Huang Tung and His-Jian Lee 1994)
also presented an efficient method for identifying
unknown words from a large corpus. The
statistical features used consist of string (character
sequence) frequency and entropy of left/right
neighboring characters (similar to left/right
context dependency). The corpus consists of
178,027 sentences, representing a total of more
than 2 million Chinese characters. 8327 unknown
words were identified and 5366 items of them
were confirmed manually.
(Ming-Wen Wu and Keh-Yih Su 1993)
presented a method using mutual information and
relative frequency. 9,124 compounds are extracted
from the corpus consists of 74,404 words, with the
precision of 47.43%. In this method, the
compound extraction problem is formulated as
classification problem. Each bi-gram (tri-gram) is
assigned to one of those two clusters. It also needs
a training corpus to estimate parameters for
classification model. In our method, we didn&apos;t
</bodyText>
<page confidence="0.994626">
138
</page>
<bodyText confidence="0.999936777777778">
make use of any training corpus. Another
difference is that they use the method for English
compounds extraction while we extract Chinese
compounds in our experiments.
(Pascale Fung 1998) presented two simple
systems for Chinese compound extraction—
CXtract. CXtract uses predominantly statistical
lexical information to find term boundaries in
large text. Evaluations on the corpus consisting of
2 million characters show that the average
precision is 54.09%.
We should note that since the experiment setup
and evaluation systems of the methods mentioned
above are not identical, the results are not
comparable. However, by showing our
experimental results on much larger and
heterogenous corpus, we can say that our method
is an efficient and robust one.
</bodyText>
<sectionHeader confidence="0.995907" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999494">
In this paper, we investigate a statistical
approach to Chinese compounds extraction from
very large corpora using mutual information and
context dependency.
We explained how the performance can be
influenced by different parameter settings, corpus
size, and corpus heterogeneousness. We also
refine the lexicon with information retrieval
system by adding compounds obtained by our
methods, and achieve 1.2% improvements on
precision of IR.
Through our experiments, we conclude that
statistical method based on mutual information
and context dependency is efficient and robust for
Chinese compounds extraction. And, mutual
information mainly affects the precision while
context dependency mainly affects the count of
extracted items.
</bodyText>
<sectionHeader confidence="0.973553" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.999385827586207">
Lee-Feng Chien, (1997) &amp;quot;PAT-tree-based keyword
extraction for Chinese Information retrieval&amp;quot;, ACM
SIGIR&apos;97, Phliadelphia, USA, 50-58
WU, Dekai and Xuanyin XIA. (1995). &amp;quot;Large-scale
automatic extraction of an English-Chinese lexicon&amp;quot;.
Machine Translation 9(3-4), pp.285-313.
Ming-Wen Wu and Keh-Yih Su. (1993). &amp;quot;Corpus-
based Automatic Compound Extraction with Mutual
Information and Relative Frequency Count,&amp;quot;
Proceedings of R. 0. C. Computational Linguistics
Conference VI . Nantou, Taiwan, R. 0. C., pp.207-
216.
Pascale Fung. (1998). &amp;quot;Extracting Key Terms from
Chinese and Japanese texts &amp;quot;. The International
Journal on Computer Processing of Oriental
Language, Special Issue on Information Retrieval on
Oriental Languages, pp.99-121.
Cheng-Huang Tung and His-Jian Lee. (1994).
&amp;quot;Identification of Unknown Words From a Corpus&amp;quot;.
Compouter Processing of Chinese and Oriental
Languages Vol.8, pp.131-145.
Buckley, C. (1985). Implementation of the SMART
information retrieval system, Technical report, #85-
686, Cornell University.
Harman, D. K. and Voorhees, E. M., Eds. (1996).
Information Technology: The Fifth Text Retrieval
Conference(TREC5), NIST SP 500-238.
Gaithersburg, National Institute fo standards and
Technology.
</reference>
<page confidence="0.998846">
139
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.593977">
<title confidence="0.998816">Extraction of Chinese Compound Words An Experimental Study on a Very Large Corpus</title>
<author confidence="0.999646">Jian Zhang Jianfeng Gao</author>
<author confidence="0.999646">Ming Thou</author>
<affiliation confidence="0.9893825">Department of Computer Science and Microsoft Research China Technology of Tsinghua University, China { jfgao,</affiliation>
<email confidence="0.606081">ajian@s1000e.cs.tsinghua.edu.cn</email>
<abstract confidence="0.999237692307692">This paper is to introduce a statistical method to extract Chinese compound words from a very large corpus&apos;. This method is on information results show that this method is efficient and robust compared with other approaches. We also examined the impact of different parameter settings, corpus size and heterogeneousness on the extraction results. We finally present results on information retrieval to show the usefulness of extracted compounds.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lee-Feng Chien</author>
</authors>
<title>PAT-tree-based keyword extraction for Chinese Information retrieval&amp;quot;,</title>
<date>1997</date>
<booktitle>ACM SIGIR&apos;97,</booktitle>
<pages>50--58</pages>
<location>Phliadelphia, USA,</location>
<contexts>
<context position="1958" citStr="Chien 1997" startWordPosition="280" endWordPosition="281">ds formed with at least two characters) are constantly created, it is impossible to list them exhaustively in a lexicon. Therefore, automatic extraction of compounds is an important issue. Traditional extraction approaches used rules. However, compounds extracted in this way are not always desirable. So, human effort is still required to find the preferred compounds from a large compound This work was done while the author worked for Microsoft Research China as a visiting student. candidate list. Some statistical approaches to extract Chinese compounds from corpus have been proposed (Lee-Feng Chien 1997, WU Dekai and Xuanyin XIA 1995, Ming-Wen Wu and KehYih Su 1993) as well, but almost all experiments are based on relatively small corpus, it is not clear whether these methods still work well with large corpus. In this paper, we investigate statistical approaches to Chinese compound extraction from very large corpus by using statistical features, namely mutual information and context dependency. There are three main contributions in this paper. First, we apply our procedure on a very large corpus while other experiments were based on small or medium size corpora. We show that better results c</context>
<context position="3417" citStr="Chien 1997" startWordPosition="513" endWordPosition="514">text dependency affects the count of extracted items. Third, we test the usefulness of the extracted compounds for information retrieval. Our experimental results on IR show that the new compounds have a positive effect on IR. The rest of this paper is structured as follows. In section 2, we describe the techniques we used. In section 3, we present several sets of experimental results. In section 4, we outline the related works as well as their results. Finally, we give our conclusions in section 5. 2 Technique description Statistical extraction of Chinese compounds has been used in (Lee-Feng Chien 1997)(WU Dekai and Xuanyin XIA 1995) and (Ming-Wen Wu and Keh-Yih Su 1993). The basic idea is that a 132 Chinese compound should appear as a stable sequence in corpus. That is, the components in the compound are strongly correlated, while the components lie at both ends should have low correlations with miter words. The method consists of two steps. At first, a list of candidate compounds is extracted from a very large corpus by using mutual information. Then, context dependency is used to remove undesirable compounds. In what follows, we will describe them in more detail. 2.1 Mutual Information Ac</context>
<context position="19645" citStr="Chien 1997" startWordPosition="3239" endWordPosition="3240">m that query. Its result is also improved significantly because its relevant documents are segmented better than before. Because the compounds extracted from the corpus are not exactly correct, the new lexicon will bring negative effect to some queries, such as query 10. The retrieval precision changes from 0.3086 to 0.1359. The main reason is that -&apos;1 iff&amp;quot;(Chinese Xinliang) is taken as a new compound in the query. 4 Related works Several methods have been proposed for extracting compounds from corpus by statistical approaches. In this section, we will briefly describe some of them. (Lee-Feng Chien 1997) proposed an approach based on PAT-Tree to automatically extracting domain specific terms from online text collections. Our method is primary derived from (Lee-Feng Chien 1997), and use the similar statistical features, i.e. mutual information and context dependency. The difference is that we use n-gram instead of PAT-Tree, due to the efficiency issue. Another difference lies in the expenments. In Chien&apos;s work, only domain specific terms are extracted from domain specific corpus, and the size of the corpus is relatively small, namely 1,872 political news abstracts. (Cheng-Huang Tung and His-Ji</context>
</contexts>
<marker>Chien, 1997</marker>
<rawString>Lee-Feng Chien, (1997) &amp;quot;PAT-tree-based keyword extraction for Chinese Information retrieval&amp;quot;, ACM SIGIR&apos;97, Phliadelphia, USA, 50-58</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai WU</author>
<author>Xuanyin XIA</author>
</authors>
<title>Large-scale automatic extraction of an English-Chinese lexicon&amp;quot;.</title>
<date>1995</date>
<journal>Machine Translation</journal>
<volume>9</volume>
<issue>3</issue>
<pages>285--313</pages>
<marker>WU, XIA, 1995</marker>
<rawString>WU, Dekai and Xuanyin XIA. (1995). &amp;quot;Large-scale automatic extraction of an English-Chinese lexicon&amp;quot;. Machine Translation 9(3-4), pp.285-313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Wen Wu</author>
<author>Keh-Yih Su</author>
</authors>
<title>Corpusbased Automatic Compound Extraction with Mutual Information and Relative Frequency Count,&amp;quot;</title>
<date>1993</date>
<booktitle>Proceedings of R. 0. C. Computational Linguistics Conference</booktitle>
<pages>207--216</pages>
<marker>Wu, Su, 1993</marker>
<rawString>Ming-Wen Wu and Keh-Yih Su. (1993). &amp;quot;Corpusbased Automatic Compound Extraction with Mutual Information and Relative Frequency Count,&amp;quot; Proceedings of R. 0. C. Computational Linguistics Conference VI . Nantou, Taiwan, R. 0. C., pp.207-216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
</authors>
<title>Extracting Key Terms from Chinese and Japanese texts &amp;quot;.</title>
<date>1998</date>
<booktitle>The International Journal on Computer Processing of Oriental Language, Special Issue on Information Retrieval on Oriental Languages,</booktitle>
<pages>99--121</pages>
<contexts>
<context position="21361" citStr="Fung 1998" startWordPosition="3494" endWordPosition="3495">method using mutual information and relative frequency. 9,124 compounds are extracted from the corpus consists of 74,404 words, with the precision of 47.43%. In this method, the compound extraction problem is formulated as classification problem. Each bi-gram (tri-gram) is assigned to one of those two clusters. It also needs a training corpus to estimate parameters for classification model. In our method, we didn&apos;t 138 make use of any training corpus. Another difference is that they use the method for English compounds extraction while we extract Chinese compounds in our experiments. (Pascale Fung 1998) presented two simple systems for Chinese compound extraction— CXtract. CXtract uses predominantly statistical lexical information to find term boundaries in large text. Evaluations on the corpus consisting of 2 million characters show that the average precision is 54.09%. We should note that since the experiment setup and evaluation systems of the methods mentioned above are not identical, the results are not comparable. However, by showing our experimental results on much larger and heterogenous corpus, we can say that our method is an efficient and robust one. 5 Conclusion In this paper, we</context>
</contexts>
<marker>Fung, 1998</marker>
<rawString>Pascale Fung. (1998). &amp;quot;Extracting Key Terms from Chinese and Japanese texts &amp;quot;. The International Journal on Computer Processing of Oriental Language, Special Issue on Information Retrieval on Oriental Languages, pp.99-121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cheng-Huang Tung</author>
<author>His-Jian Lee</author>
</authors>
<title>Identification of Unknown Words From a Corpus&amp;quot;.</title>
<date>1994</date>
<booktitle>Compouter Processing of Chinese and Oriental Languages Vol.8,</booktitle>
<pages>131--145</pages>
<marker>Tung, Lee, 1994</marker>
<rawString>Cheng-Huang Tung and His-Jian Lee. (1994). &amp;quot;Identification of Unknown Words From a Corpus&amp;quot;. Compouter Processing of Chinese and Oriental Languages Vol.8, pp.131-145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Buckley</author>
</authors>
<title>Implementation of the SMART information retrieval system,</title>
<date>1985</date>
<tech>Technical report, #85-686,</tech>
<institution>Cornell University.</institution>
<contexts>
<context position="16838" citStr="Buckley 1985" startWordPosition="2782" endWordPosition="2783">n, we can see that precision is in a growing progress. As we defined above, MI and CD are two different measurements. What role they play in our extraction procedure? Our conclusion is that mutual information mainly affects the precision while context dependency mainly affects the count of extracted items. This conclusion is also confirmed by Fig2 and Fig3. That is, the curves in Fig2 are more flat than corresponding curves in Fig3. 33 Testing the Extracted Compounds in Information Retrieval In this experiment, we apply our method to improve information retrieval results. We use SMART system (Buckley 1985) for our experiments. SMART is a robust, efficient and flexible information retrieval system. The corpus used in this experiment is TREC Chinese corpus (Harman and Voorhees, 1996). The corpus contains about 160,000 articles, including articles published in the People&apos;s Daily from 1991 to 1993, and a part of the news released by the Xinhua News Agency in 1994 and 1995. A set of 54 queries has been set up and evaluated by people in NIST(National Institute of Standards and Technology). We first use an initial lexicon consisting of 65,502 entries to segment the corpus. When running SMART on the se</context>
</contexts>
<marker>Buckley, 1985</marker>
<rawString>Buckley, C. (1985). Implementation of the SMART information retrieval system, Technical report, #85-686, Cornell University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D K Harman</author>
<author>E M Voorhees</author>
<author>Eds</author>
</authors>
<date>1996</date>
<booktitle>Information Technology: The Fifth Text Retrieval Conference(TREC5), NIST SP 500-238. Gaithersburg, National Institute fo standards and Technology.</booktitle>
<marker>Harman, Voorhees, Eds, 1996</marker>
<rawString>Harman, D. K. and Voorhees, E. M., Eds. (1996). Information Technology: The Fifth Text Retrieval Conference(TREC5), NIST SP 500-238. Gaithersburg, National Institute fo standards and Technology.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>