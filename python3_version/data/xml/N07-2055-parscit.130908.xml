<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.040416">
<title confidence="0.9980115">
A Semi-Automatic Evaluation Scheme: Automated Nuggetization for
Manual Annotation
</title>
<author confidence="0.999633">
Liang Zhou, Namhee Kwon, and Eduard Hovy
</author>
<affiliation confidence="0.997685">
Information Sciences Institute
University of Southern California
</affiliation>
<address confidence="0.897038">
4676 Admiralty Way
Marina del Rey, CA 90292
</address>
<email confidence="0.996719">
{liangz, nkwon, hovy}@isi.edu
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999313777777778">
In this paper we describe automatic in-
formation nuggetization and its applica-
tion to text comparison. More
specifically, we take a close look at how
machine-generated nuggets can be used to
create evaluation material. A semi-
automatic annotation scheme is designed
to produce gold-standard data with excep-
tionally high inter-human agreement.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99843195">
In many natural language processing (NLP) tasks,
we are faced with the problem of determining the
appropriate granularity level for information units.
Most commonly, we use sentences to model indi-
vidual pieces of information. However, more NLP
applications require us to define text units smaller
than sentences, essentially decomposing sentences
into a collection of phrases. Each phrase carries an
independent piece of information that can be used
as a standalone unit. These finer-grained informa-
tion units are usually referred to as nuggets.
When performing within-sentence comparison
for redundancy and/or relevancy judgments, with-
out a precise and consistent breakdown of nuggets
we can only rely on rudimentary n-gram segmenta-
tions of sentences to form nuggets and perform
subsequent n-gram-wise text comparison. This is
not satisfactory for a variety of reasons. For exam-
ple, one n-gram window may contain several sepa-
rate pieces of information, while another of the
same length may not contain even one complete
piece of information.
Previous work shows that humans can create
nuggets in a relatively straightforward fashion. In
the PYRAMID scheme for manual evaluation of
summaries (Nenkova and Passonneau, 2004), ma-
chine-generated summaries were compared with
human-written ones at the nugget level. However,
automatic creation of the nuggets is not trivial.
Hamly et al. (2005) explore the enumeration and
combination of all words in a sentence to create the
set of all possible nuggets. Their automation proc-
ess still requires nuggets to be manually created a
priori for reference summaries before any sum-
mary comparison takes place. This human in-
volvement allows a much smaller subset of phrase
segments, resulting from word enumeration, to be
matched in summary comparisons. Without the
human-created nuggets, text comparison falls back
to its dependency on n-grams. Similarly, in ques-
tion-answering (QA) evaluations, gold-standard
answers use manually created nuggets and com-
pare them against system-produced answers bro-
ken down into n-gram pieces, as shown in
POURPRE (Lin and Demner-Fushman, 2005) and
NUGGETEER (Marton and Radul, 2006).
A serious problem in manual nugget creation is
the inconsistency in human decisions (Lin and
Hovy, 2003). The same nugget will not be marked
consistently with the same words when sentences
containing multiple instances of it are presented to
human annotators. And if the annotation is per-
formed over an extended period of time, the con-
sistency is even lower. In recent exercises of the
PYRAMID evaluation, inconsistent nuggets are
flagged by a tracking program and returned back to
the annotators, and resolved manually.
Given these issues, we address two questions in
this paper: First, how do we define nuggets so that
they are consistent in definition? Secondly, how do
</bodyText>
<page confidence="0.974731">
217
</page>
<note confidence="0.431211">
Proceedings of NAACL HLT 2007, Companion Volume, pages 217–220,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.993446">
we utilize automatically extracted nuggets for vari-
ous evaluation purposes?
</bodyText>
<sectionHeader confidence="0.982554" genericHeader="method">
2 Nugget Definition
</sectionHeader>
<bodyText confidence="0.990014666666667">
Based on our manual analysis and computational
modeling of nuggets, we define them as follows:
Definition:
</bodyText>
<listItem confidence="0.99115775">
• A nugget is predicated on either an event or
an entity.
• Each nugget consists of two parts: the an-
chor and the content.
</listItem>
<bodyText confidence="0.687008">
The anchor is either:
</bodyText>
<listItem confidence="0.8548008">
• the head noun of the entity, or
• the head verb of the event, plus the head
noun of its associated entity (if more than
one entity is attached to the verb, then its
subject).
</listItem>
<bodyText confidence="0.999954764705882">
The content is a coherent single piece of infor-
mation associated with the anchor. Each anchor
may have several separate contents.
When a nugget contains nested sentences, this
definition is applied recursively. Figure 1 shows an
example. Anchors are marked with square brack-
ets. If the anchor is a verb, then its entity attach-
ment is marked with curly brackets. If the sentence
in question is a compound and/or complex sen-
tence, then this definition is applied recursively to
allow decomposition. For example, in Figure 1,
without recursive decomposition, only two nuggets
are formed: 1) “[girl] working at the bookstore in
Hollywood”, and 2) “{girl} [talked] to the diplo-
mat living in Britain”. In this example, recursive
decomposition produces nuggets with labels 1-a, 1-
b, 2-a, and 2-b.
</bodyText>
<subsectionHeader confidence="0.990938">
2.1 Nugget Extraction
</subsectionHeader>
<bodyText confidence="0.948077461538462">
We use syntactic parse trees produced by the
Collins parser (Collins, 1999) to obtain the struc-
tural representation of sentences. Nuggets are ex-
tracted by identifying subtrees that are descriptions
for entities and events. For entity nuggets, we ex-
amine subtrees headed by “NP”; for event nuggets,
subtrees headed by “VP” are examined and their
corresponding subjects (siblings headed by “NP”)
are treated as entity attachments for the verb
phrases.
Sentence:
The girl working at the bookstore in Hollywood
talked to the diplomat living in Britain.
</bodyText>
<figure confidence="0.996133882352941">
Nuggets are:
1) [girl] working at the bookstore in Holly-
wood
a. [girl] working at the bookstore
b. [bookstore] in Hollywood
2) {girl} [talked] to the diplomat living in
Britain
a. {girl} [talked] to the diplomat
b. [diplomat] living in Britian
Anchors:
1) [girl]
a. [girl]
b. [bookstore]
2) {girl} [talked]: talked is the anchor verb
and girl is its entity attachment.
a. {girl} [talked]
b. [diplomat]
</figure>
<figureCaption confidence="0.999555">
Figure 1. Nugget definition examples.
</figureCaption>
<sectionHeader confidence="0.939574" genericHeader="method">
3 Utilizing Nuggets in Evaluations
</sectionHeader>
<bodyText confidence="0.9999303">
In recent QA and summarization evaluation exer-
cises, manually created nuggets play a determinate
role in judging system qualities. Although the two
task evaluations are similar, the text comparison
task in summarization evaluation is more complex
because systems are required to produce long re-
sponses and thus it is hard to yield high agreement
if manual annotations are performed. The follow-
ing experiments are conducted in the realm of
summarization evaluation.
</bodyText>
<subsectionHeader confidence="0.998191">
3.1 Manually Created Nuggets
</subsectionHeader>
<bodyText confidence="0.999812642857143">
During the recent two Document Understanding
Confereces (DUC-05 and DUC-06) (NIST, 2002–
2007), the PYRAMID framework (Nenkova and
Passonneau, 2004) was used for manual summary
evaluations. In this framework, human annotators
select and highlight portions of reference summa-
ries to form a pyramid of summary content units
(SCUs) for each docset. A pyramid is constructed
from SCUs and their corresponding popularity
scores—the number of reference summaries they
appeared in individually. SCUs carrying the same
information do not necessarily have the same sur-
face-level words. Annotators need to make the de-
cisions based on semantic equivalence among
</bodyText>
<page confidence="0.996829">
218
</page>
<bodyText confidence="0.999889">
various SCUs. To evaluate a peer summary from a
particular docset, annotators highlight portions of
text in the peer summary that convey the same in-
formation as those SCUs in previously constructed
pyramids.
</bodyText>
<subsectionHeader confidence="0.999395">
3.2 Automatically Created Nuggets
</subsectionHeader>
<bodyText confidence="0.998653">
We envisage the nuggetization process being
automated and nugget comparison and aggregation
being performed by humans. It is crucial to involve
humans in the evaluation process because recog-
nizing semantically equivalent units is not a trivial
task computationally. In addition, since nuggets are
system-produced and can be imperfect, annotators
are allowed to reject and re-create them. We per-
form record-keeping in the background on which
nugget or nugget groups are edited so that further
improvements can be made for nuggetization.
The evaluation scheme is designed as follows:
For reference summaries (per docset):
</bodyText>
<listItem confidence="0.991107777777778">
• Nuggets are created for all sentences;
• Annotators will group equivalent nuggets.
• Popularity scores are automatically assigned
to nugget groups.
For peer summaries:
• Nuggets are created for all sentences;
• Annotators will match/align peer’s nuggets
with reference nugget groups.
• Recall scores are to be computed.
</listItem>
<subsectionHeader confidence="0.849381">
3.3 Consistency in Human Involvement
</subsectionHeader>
<bodyText confidence="0.991721733333333">
The process of creating nuggets has been auto-
mated and we can assume a certain level of consis-
tency based on the usage of the syntactic parser.
However, a more important issue emerges. When
given the same set of nuggets, would human anno-
tators agree on nugget group selections and their
corresponding contributing nuggets? What levels
of agreement and disagreement should be ex-
pected? Two annotators, one familiar with the no-
tion of nuggetization (C1) and one not (C2),
participated in the following experiments.
Figure 2 shows the annotation procedure for
reference summaries. After two rounds of individ-
ual annotations and consolidations and one final
round of conflict resolution, a set of gold-standard
</bodyText>
<figureCaption confidence="0.997314">
Figure 2. Reference annotation and gold-standard
data creation.
</figureCaption>
<bodyText confidence="0.999961933333333">
nugget groups is created for each docset and will
be subsequently used in peer summary annotations.
The first round of annotation is needed since one
of the annotators, C2, is not familiar with nuggeti-
zation. After the initial introduction of the task,
concerns and questions arisen can be addressed.
Then the annotators proceed to the second round of
annotation. Naturally, some differences and con-
flicts remain. Annotators must resolve these prob-
lems during the final round of conflict resolution
and create the agreed-upon gold-standard data.
Previous manual nugget annotation has used one
annotator as the primary nugget creator and an-
other annotator as an inspector (Nenkova and Pas-
sonneau, 2004). In our annotation experiment, we
encourage both annotators to play equally active
roles. Conflicts between annotators resulting from
ideology, comprehension, and interpretation differ-
ences helped us to understand that complete
agreement between annotators is not realistic and
not achievable, unless one annotator is dominant
over the other. We should expect a 5-10% annota-
tion variation.
In Figure 3, we show annotation comparisons
from first to second round. The x-axis shows the
nugget groups that C1 and C2 have agreed on. The
y-axis shows the popularity score a particular nug-
get group received. Selecting from three reference
summaries, a score of three for a nugget group in-
dicates it was created from nuggets in all three
</bodyText>
<page confidence="0.998675">
219
</page>
<figureCaption confidence="0.9984945">
Figure 3. Annotation comparisons from 1st to
2nd round.
</figureCaption>
<bodyText confidence="0.999987151515151">
summaries. The first round initially appears suc-
cessful because the two annotators had 100%
agreement on nugget groups and their correspond-
ing scores. However, C2, the novice nuggetizer,
was much more conservative than C1, because
only 10 nugget groups were created. The geometric
mean of agreement on all nugget group assignment
is merely 0.4786. During the second round, differ-
ences in group-score allocations emerge, 0.9192,
because C2 is creating more nugget groups. The
geometric mean of agreement on all nugget group
assignment has been improved to 0.7465.
After the final round of conflict resolution,
gold-standard data was created. Since all conflicts
must be resolved, annotators have to either con-
vince or be convinced by the other. How much
change is there between an annotator’s second-
round annotation and the gold-standard? Geomet-
ric mean of agreement on all nugget group assign-
ment for C1 is 0.7543 and for C2 is 0.8099.
Agreement on nugget group score allocation for
C1 is 0.9681 and for C2 is 0.9333. From these fig-
ures, we see that while C2 contributed more to the
gold-standard’s nugget group creations, C1 had
more accuracy in finding the correct number of
nugget occurrences in reference summaries. This
confirms that both annotators played an active role.
Using the gold-standard nugget groups, the annota-
tors performed 4 peer summary annotations. The
agreement among peer summary annotations is
quite high, at approximately 0.95. Among the four,
annotations on one peer summary from the two
annotators are completely identical.
</bodyText>
<sectionHeader confidence="0.999716" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999436357142857">
In this paper we have given a concrete definition
for information nuggets and provided a systematic
implementation of them. Our main goal is to use
these machine-generated nuggets in a semi-
automatic evaluation environment for various NLP
applications. We took a close look at how this can
be accomplished for summary evaluation, using
nuggets created from reference summaries to grade
peer summaries. Inter-annotator agreements are
measured to insure the quality of the gold-standard
data created. And the agreements are very high by
following a meticulous procedure. We are cur-
rently preparing to deploy our design into full-
scale evaluation exercises.
</bodyText>
<sectionHeader confidence="0.99909" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9995294">
Collins, M. 1999. Head-driven statistical models for
natural language processing. PhD Dissertation, Uni-
versity of Pennsylvania.
Hamly, A., A. Nenkova, R. Passonneau, and O. Ram-
bow. 2005. Automation of summary evaluation by
the pyramid method. In Proceedings of RANLP.
Lin, C.Y. and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of NAACL-HLT.
Lin, J. and D. Demner-Fushman. 2005. Automatically
evaluating answers to definition questions. In Pro-
ceedings of HLT-EMNLP.
Marton, G. and A. Radul. 2006. Nuggeteer: automatic
nugget-based evaluation using description and judg-
ments. In Proceedings NAACL-HLT.
Nenkova, A. and R. Passonneau. 2004. Evaluating con-
tent selection in summarization: the pyramid method.
In Proceedings NAACL-HLT.
NIST. 2001–2007. Document Understanding Confer-
ence. www-nlpir.nist.gov/projects/duc/index.html.
</reference>
<page confidence="0.99756">
220
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.861179">
<title confidence="0.9641805">A Semi-Automatic Evaluation Scheme: Automated Nuggetization Manual Annotation</title>
<author confidence="0.966099">Liang Zhou</author>
<author confidence="0.966099">Namhee Kwon</author>
<author confidence="0.966099">Eduard</author>
<affiliation confidence="0.9964035">Information Sciences University of Southern</affiliation>
<address confidence="0.9932085">4676 Admiralty Marina del Rey, CA 90292</address>
<email confidence="0.997844">liangz@isi.edu</email>
<email confidence="0.997844">nkwon@isi.edu</email>
<email confidence="0.997844">hovy@isi.edu</email>
<abstract confidence="0.9973703">In this paper we describe automatic information nuggetization and its application to text comparison. More specifically, we take a close look at how machine-generated nuggets can be used to create evaluation material. A semiautomatic annotation scheme is designed to produce gold-standard data with exceptionally high inter-human agreement.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language processing.</title>
<date>1999</date>
<institution>PhD Dissertation, University of Pennsylvania.</institution>
<contexts>
<context position="5013" citStr="Collins, 1999" startWordPosition="777" endWordPosition="778">ets. If the anchor is a verb, then its entity attachment is marked with curly brackets. If the sentence in question is a compound and/or complex sentence, then this definition is applied recursively to allow decomposition. For example, in Figure 1, without recursive decomposition, only two nuggets are formed: 1) “[girl] working at the bookstore in Hollywood”, and 2) “{girl} [talked] to the diplomat living in Britain”. In this example, recursive decomposition produces nuggets with labels 1-a, 1- b, 2-a, and 2-b. 2.1 Nugget Extraction We use syntactic parse trees produced by the Collins parser (Collins, 1999) to obtain the structural representation of sentences. Nuggets are extracted by identifying subtrees that are descriptions for entities and events. For entity nuggets, we examine subtrees headed by “NP”; for event nuggets, subtrees headed by “VP” are examined and their corresponding subjects (siblings headed by “NP”) are treated as entity attachments for the verb phrases. Sentence: The girl working at the bookstore in Hollywood talked to the diplomat living in Britain. Nuggets are: 1) [girl] working at the bookstore in Hollywood a. [girl] working at the bookstore b. [bookstore] in Hollywood 2)</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Collins, M. 1999. Head-driven statistical models for natural language processing. PhD Dissertation, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hamly</author>
<author>A Nenkova</author>
<author>R Passonneau</author>
<author>O Rambow</author>
</authors>
<title>Automation of summary evaluation by the pyramid method.</title>
<date>2005</date>
<booktitle>In Proceedings of RANLP.</booktitle>
<contexts>
<context position="2016" citStr="Hamly et al. (2005)" startWordPosition="293" endWordPosition="296">d perform subsequent n-gram-wise text comparison. This is not satisfactory for a variety of reasons. For example, one n-gram window may contain several separate pieces of information, while another of the same length may not contain even one complete piece of information. Previous work shows that humans can create nuggets in a relatively straightforward fashion. In the PYRAMID scheme for manual evaluation of summaries (Nenkova and Passonneau, 2004), machine-generated summaries were compared with human-written ones at the nugget level. However, automatic creation of the nuggets is not trivial. Hamly et al. (2005) explore the enumeration and combination of all words in a sentence to create the set of all possible nuggets. Their automation process still requires nuggets to be manually created a priori for reference summaries before any summary comparison takes place. This human involvement allows a much smaller subset of phrase segments, resulting from word enumeration, to be matched in summary comparisons. Without the human-created nuggets, text comparison falls back to its dependency on n-grams. Similarly, in question-answering (QA) evaluations, gold-standard answers use manually created nuggets and c</context>
</contexts>
<marker>Hamly, Nenkova, Passonneau, Rambow, 2005</marker>
<rawString>Hamly, A., A. Nenkova, R. Passonneau, and O. Rambow. 2005. Automation of summary evaluation by the pyramid method. In Proceedings of RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
<author>E Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="2887" citStr="Lin and Hovy, 2003" startWordPosition="426" endWordPosition="429">lace. This human involvement allows a much smaller subset of phrase segments, resulting from word enumeration, to be matched in summary comparisons. Without the human-created nuggets, text comparison falls back to its dependency on n-grams. Similarly, in question-answering (QA) evaluations, gold-standard answers use manually created nuggets and compare them against system-produced answers broken down into n-gram pieces, as shown in POURPRE (Lin and Demner-Fushman, 2005) and NUGGETEER (Marton and Radul, 2006). A serious problem in manual nugget creation is the inconsistency in human decisions (Lin and Hovy, 2003). The same nugget will not be marked consistently with the same words when sentences containing multiple instances of it are presented to human annotators. And if the annotation is performed over an extended period of time, the consistency is even lower. In recent exercises of the PYRAMID evaluation, inconsistent nuggets are flagged by a tracking program and returned back to the annotators, and resolved manually. Given these issues, we address two questions in this paper: First, how do we define nuggets so that they are consistent in definition? Secondly, how do 217 Proceedings of NAACL HLT 20</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Lin, C.Y. and E. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
<author>D Demner-Fushman</author>
</authors>
<title>Automatically evaluating answers to definition questions.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP.</booktitle>
<contexts>
<context position="2742" citStr="Lin and Demner-Fushman, 2005" startWordPosition="403" endWordPosition="406">ible nuggets. Their automation process still requires nuggets to be manually created a priori for reference summaries before any summary comparison takes place. This human involvement allows a much smaller subset of phrase segments, resulting from word enumeration, to be matched in summary comparisons. Without the human-created nuggets, text comparison falls back to its dependency on n-grams. Similarly, in question-answering (QA) evaluations, gold-standard answers use manually created nuggets and compare them against system-produced answers broken down into n-gram pieces, as shown in POURPRE (Lin and Demner-Fushman, 2005) and NUGGETEER (Marton and Radul, 2006). A serious problem in manual nugget creation is the inconsistency in human decisions (Lin and Hovy, 2003). The same nugget will not be marked consistently with the same words when sentences containing multiple instances of it are presented to human annotators. And if the annotation is performed over an extended period of time, the consistency is even lower. In recent exercises of the PYRAMID evaluation, inconsistent nuggets are flagged by a tracking program and returned back to the annotators, and resolved manually. Given these issues, we address two que</context>
</contexts>
<marker>Lin, Demner-Fushman, 2005</marker>
<rawString>Lin, J. and D. Demner-Fushman. 2005. Automatically evaluating answers to definition questions. In Proceedings of HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Marton</author>
<author>A Radul</author>
</authors>
<title>Nuggeteer: automatic nugget-based evaluation using description and judgments.</title>
<date>2006</date>
<booktitle>In Proceedings NAACL-HLT.</booktitle>
<contexts>
<context position="2781" citStr="Marton and Radul, 2006" startWordPosition="409" endWordPosition="412">requires nuggets to be manually created a priori for reference summaries before any summary comparison takes place. This human involvement allows a much smaller subset of phrase segments, resulting from word enumeration, to be matched in summary comparisons. Without the human-created nuggets, text comparison falls back to its dependency on n-grams. Similarly, in question-answering (QA) evaluations, gold-standard answers use manually created nuggets and compare them against system-produced answers broken down into n-gram pieces, as shown in POURPRE (Lin and Demner-Fushman, 2005) and NUGGETEER (Marton and Radul, 2006). A serious problem in manual nugget creation is the inconsistency in human decisions (Lin and Hovy, 2003). The same nugget will not be marked consistently with the same words when sentences containing multiple instances of it are presented to human annotators. And if the annotation is performed over an extended period of time, the consistency is even lower. In recent exercises of the PYRAMID evaluation, inconsistent nuggets are flagged by a tracking program and returned back to the annotators, and resolved manually. Given these issues, we address two questions in this paper: First, how do we </context>
</contexts>
<marker>Marton, Radul, 2006</marker>
<rawString>Marton, G. and A. Radul. 2006. Nuggeteer: automatic nugget-based evaluation using description and judgments. In Proceedings NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>R Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: the pyramid method.</title>
<date>2004</date>
<booktitle>In Proceedings NAACL-HLT.</booktitle>
<contexts>
<context position="1849" citStr="Nenkova and Passonneau, 2004" startWordPosition="268" endWordPosition="271">redundancy and/or relevancy judgments, without a precise and consistent breakdown of nuggets we can only rely on rudimentary n-gram segmentations of sentences to form nuggets and perform subsequent n-gram-wise text comparison. This is not satisfactory for a variety of reasons. For example, one n-gram window may contain several separate pieces of information, while another of the same length may not contain even one complete piece of information. Previous work shows that humans can create nuggets in a relatively straightforward fashion. In the PYRAMID scheme for manual evaluation of summaries (Nenkova and Passonneau, 2004), machine-generated summaries were compared with human-written ones at the nugget level. However, automatic creation of the nuggets is not trivial. Hamly et al. (2005) explore the enumeration and combination of all words in a sentence to create the set of all possible nuggets. Their automation process still requires nuggets to be manually created a priori for reference summaries before any summary comparison takes place. This human involvement allows a much smaller subset of phrase segments, resulting from word enumeration, to be matched in summary comparisons. Without the human-created nugget</context>
<context position="6604" citStr="Nenkova and Passonneau, 2004" startWordPosition="1021" endWordPosition="1024">and summarization evaluation exercises, manually created nuggets play a determinate role in judging system qualities. Although the two task evaluations are similar, the text comparison task in summarization evaluation is more complex because systems are required to produce long responses and thus it is hard to yield high agreement if manual annotations are performed. The following experiments are conducted in the realm of summarization evaluation. 3.1 Manually Created Nuggets During the recent two Document Understanding Confereces (DUC-05 and DUC-06) (NIST, 2002– 2007), the PYRAMID framework (Nenkova and Passonneau, 2004) was used for manual summary evaluations. In this framework, human annotators select and highlight portions of reference summaries to form a pyramid of summary content units (SCUs) for each docset. A pyramid is constructed from SCUs and their corresponding popularity scores—the number of reference summaries they appeared in individually. SCUs carrying the same information do not necessarily have the same surface-level words. Annotators need to make the decisions based on semantic equivalence among 218 various SCUs. To evaluate a peer summary from a particular docset, annotators highlight porti</context>
<context position="9804" citStr="Nenkova and Passonneau, 2004" startWordPosition="1511" endWordPosition="1515">n peer summary annotations. The first round of annotation is needed since one of the annotators, C2, is not familiar with nuggetization. After the initial introduction of the task, concerns and questions arisen can be addressed. Then the annotators proceed to the second round of annotation. Naturally, some differences and conflicts remain. Annotators must resolve these problems during the final round of conflict resolution and create the agreed-upon gold-standard data. Previous manual nugget annotation has used one annotator as the primary nugget creator and another annotator as an inspector (Nenkova and Passonneau, 2004). In our annotation experiment, we encourage both annotators to play equally active roles. Conflicts between annotators resulting from ideology, comprehension, and interpretation differences helped us to understand that complete agreement between annotators is not realistic and not achievable, unless one annotator is dominant over the other. We should expect a 5-10% annotation variation. In Figure 3, we show annotation comparisons from first to second round. The x-axis shows the nugget groups that C1 and C2 have agreed on. The y-axis shows the popularity score a particular nugget group receive</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>Nenkova, A. and R. Passonneau. 2004. Evaluating content selection in summarization: the pyramid method. In Proceedings NAACL-HLT.</rawString>
</citation>
<citation valid="false">
<title>Document Understanding Conference.</title>
<note>www-nlpir.nist.gov/projects/duc/index.html.</note>
<marker></marker>
<rawString>NIST. 2001–2007. Document Understanding Conference. www-nlpir.nist.gov/projects/duc/index.html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>