<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000073">
<title confidence="0.9944335">
FSA: An Efficient and Flexible C++ Toolkit for Finite State Automata
Using On-Demand Computation
</title>
<author confidence="0.996959">
Stephan Kanthak and Hermann Ney
</author>
<affiliation confidence="0.969978">
Lehrstuhl f¨ur Informatik VI, Computer Science Department
RWTH Aachen – University of Technology
</affiliation>
<address confidence="0.660431">
52056 Aachen, Germany
</address>
<email confidence="0.997574">
{kanthak,ney}@informatik.rwth-aachen.de
</email>
<sectionHeader confidence="0.993907" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999366894736842">
In this paper we present the RWTH FSA toolkit – an
efficient implementation of algorithms for creating
and manipulating weighted finite-state automata.
The toolkit has been designed using the principle
of on-demand computation and offers a large range
of widely used algorithms. To prove the superior
efficiency of the toolkit, we compare the implemen-
tation to that of other publically available toolkits.
We also show that on-demand computations help to
reduce memory requirements significantly without
any loss in speed. To increase its flexibility, the
RWTH FSA toolkit supports high-level interfaces
to the programming language Python as well as a
command-line tool for interactive manipulation of
FSAs. Furthermore, we show how to utilize the
toolkit to rapidly build a fast and accurate statisti-
cal machine translation system. Future extensibility
of the toolkit is ensured as it will be publically avail-
able as open source software.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.979038695652174">
Finite-state automata (FSA) methods proved to el-
egantly solve many difficult problems in the field
of natural language processing. Among the most
recent ones are full and lazy compilation of the
search network for speech recognition (Mohri et
al., 2000a), integrated speech translation (Vidal,
1997; Bangalore and Riccardi, 2000), speech sum-
marization (Hori et al., 2003), language modelling
(Allauzen et al., 2003) and parameter estimation
through EM (Eisner, 2001) to mention only a few.
From this list of different applications it is clear that
there is a high demand for generic tools to create
and manipulate FSAs.
In the past, a number of toolkits have been pub-
lished, all with different design principles. Here, we
give a short overview of toolkits that offer an almost
complete set of algorithms:
• The FSM LibraryTM from AT&amp;T (Mohri et
al., 2000b) is judged the most efficient im-
plementation, offers various semirings, on-
demand computation and many algorithms, but
is available only in binary form with a propri-
etary, non commercial license.
</bodyText>
<listItem confidence="0.732555">
• FSA6.1 from (van Noord, 2000) is imple-
mented in Prolog. It is licensed under the terms
of the (GPL, 1991).
• The WFST toolkit from (Adant, 2000) is built
on top of the Automaton Standard Template
Library (LeMaout, 1998) and uses C++ tem-
plate mechanisms for efficiency and flexibil-
ity, but lacks on-demand computation. Also
licensed under the terms of the (GPL, 1991).
</listItem>
<bodyText confidence="0.999509148148148">
This paper describes a highly efficient new im-
plementation of a finite-state automata toolkit that
uses on-demand computation. Currently, it is
being used at the Lehrstuhl f¨ur Informatik VI,
RWTH Aachen in different speech recognition
and translation research applications. The toolkit
will be available under an open source license
(GPL, 1991) and can be obtained from our website
http://www-i6.informatik.rwth-aachen.de.
The remaining part of the paper is organized
as follows: Section 2 will give a short introduc-
tion to the theory of finite-state automata to re-
call part of the terminology and notation. We will
also give a short explanation of composition which
we use as an exemplary object of study in the fol-
lowing sections. In Section 2.3 we will discuss
the locality of algorithms defined on finite-state au-
tomata. This forms the basis for implementations
using on-demand computations. Then the RWTH
FSA toolkit implementation is detailed in Section
3. In Section 4.1 we will compare the efficiency of
different toolkits. As a showcase for the flexibility
we show how to use the toolkit to build a statistical
machine translation system in Section 4.2. We con-
clude the paper with a short summary in Section 5
and discuss some possible future extensions in Sec-
tion 6.
</bodyText>
<sectionHeader confidence="0.994231" genericHeader="method">
2 Finite-State Automata
</sectionHeader>
<subsectionHeader confidence="0.992677">
2.1 Weighted Finite-State Transducer
</subsectionHeader>
<bodyText confidence="0.996707129032258">
The basic theory of weighted finite-state automata
has been reviewed in numerous papers (Mohri,
1997; Allauzen et al., 2003). We will introduce the
notation briefly.
A semiring (K, ⊕, ⊗, 0,1) is a structure with a
set K and two binary operations ⊕ and ⊗ such
that (K, ⊕, 0) is a commutative monoid, (K, ⊗,1)
is a monoid and ⊗ distributes over ⊕ and 0 ⊗
x = x ⊗ 0 = 0 for any x ∈ K. We will
also associate the term weights with the elements
of a semiring. Semirings that are frequently used
in speech recognition are the positive real semir-
ing (IR ∪ {−∞, +∞}, ⊕log, +, +∞, 0) with a ⊕log
b = −log(e−a + e−b) and the tropical semiring
(IR∪{−∞, +∞}, min, +, +∞, 0) representing the
well-known sum and maximum weighted path cri-
teria.
A weighted finite-state transducer (Q, E ∪
{E}, Q ∪ {E}, K, E, i, F, λ, p) is a structure with a
set Q of states1, an alphabet E of input symbols,
an alphabet Q of output symbols, a weight semir-
ing K (we assume it k-closed here for some algo-
rithms as described in (Mohri and Riley, 2001)), a
set E ⊆ Q × (E ∪ {E}) × (Q ∪ {E}) × K × Q of
arcs, a single initial state i with weight λ and a set of
final states F weighted by the function p : F → K.
To simplify the notation we will also denote with
QT and ET the set of states and arcs of a trans-
ducer T. A weighted finite-state acceptor is simply
a weighted finite-state transducer without the output
alphabet.
</bodyText>
<subsectionHeader confidence="0.998136">
2.2 Composition
</subsectionHeader>
<bodyText confidence="0.998824">
As we will refer to this example throughout the pa-
per we shortly review the composition algorithm
here. Let T1 : E*×Q* → K and T2 : Q*×I&apos;* → K
be two transducers defined over the same semiring
K. Their composition T1 ◦ T2 realizes the function
T : E*×I&apos;* → K and the theory has been described
in detail in (Pereira and Riley, 1996).
For simplification purposes, let us assume that the
input automata are 2-free and S = (Q1 × Q2, ←, →
, empty) is a stack of state tuples of T1 and T2 with
push, pop and empty test operations. A non lazy
version of composition is shown in Figure 1.
Composition of automata containing 2 labels is
more complex and can be solved by using an in-
termediate filter transducer that also has been de-
scribed in (Pereira and Riley, 1996).
</bodyText>
<footnote confidence="0.931233">
1we do not restrict this to be a finite set as most algorithms
of the lazy implementation presented in this paper also support
a virtually infinite set
</footnote>
<equation confidence="0.9437314">
T=T1oT2:
i = (i1, i2)
S +— (i1, i2)
while not S empty
(s1, s2) +— S
QT = QT U (s1, s2)
foreach (s1, i1, o1, w1, t1) E ET,
foreach (s2, i2, o2, w2, t2) E ET, with o1 = i2
ET = ET U ((s1, s2), i1, o2, w1 (9 w2, (t1, t2))
if (t1, t2) E� QT then S +— (t1, t2)
</equation>
<figureCaption confidence="0.999379">
Figure 1: Simplified version of composition (as-
sumes 2-free input transducers).
</figureCaption>
<bodyText confidence="0.9998120625">
What we can see from the pseudo-code above is
that composition uses tuples of states of the two in-
put transducers to describe states of the target trans-
ducer. Other operations defined on weighted finite-
state automata use different abstract states. For
example transducer determinization (Mohri, 1997)
uses a set of pairs of states and weights. However,
it is more convenient to use integers as state indices
for an implementation. Therefore algorithms usu-
ally maintain a mapping from abstract states to in-
teger state indices. This mapping has linear mem-
ory requirements of O(|QT |) which is quite attrac-
tive, but that depends on the structure of the abstract
states. Especially in case of determinization where
the size of an abstract state may vary, the complex-
ity is no longer linear in general.
</bodyText>
<subsectionHeader confidence="0.998507">
2.3 Local Algorithms
</subsectionHeader>
<bodyText confidence="0.995347611111111">
Mohri and colleagues pointed out (Mohri et al.,
2000b) that a special class of transducer algorithms
can be computed on demand. We will give a more
detailed analysis here. We focus on algorithms that
produce a single transducer and refer to them as al-
gorithmic transducers.
Definition: Let 0 be the input configuration of
an algorithm A(0) that outputs a single finite-state
transducer T. Additionally, let M : S → QT be
a one-to-one mapping from the set of abstract state
descriptions S that A generates onto the set of states
of T. We call A local iff for all states s ∈ QT A
can generate a state s of T and all outgoing arcs
(s, i, o, w, s&apos;) ∈ ET, depending only on its abstract
state M−1(s) and the input configuration 0.
With the preceding definition it is quite easy to
prove the following lemma:
Lemma: An algorithm A that has the local prop-
erty can be built on demand starting with the ini-
tial state iTA of its associated algorithmic transducer
TA.
Proof: For the proof it is sufficient to show that
we can generate and therefore reach all states of TA.
Let S be a stack of states of TA that we still have
to process. Due to the one-to-one mapping M we
can map each state of TA back to an abstract state
of A. By definition the abstract state is sufficient to
generate the complete state and its outgoing arcs.
We then push those target states of all outgoing arcs
onto the stack S that have not yet been processed.
As TA is finite the traversal ends after all states of
TA as been processed exactly once. ✷
Algorithmic transducers that can be computed
on-demand are also called lazy or virtual transduc-
ers. Note, that due to the local property the set of
states does not necessarily be finite anymore.
</bodyText>
<sectionHeader confidence="0.980069" genericHeader="method">
3 The Toolkit
</sectionHeader>
<bodyText confidence="0.999985454545455">
The current implementation is the second version of
this toolkit. For the first version – which was called
FSM – we opted for using C++ templates to gain ef-
ficiency, but algorithms were not lazy. It turned out
that the implementation was fast, but many opera-
tions wasted a lot of memory as their resulting trans-
ducer had been fully expanded in memory. How-
ever, we plan to also make this initial version publi-
cally available.
The design principles of the second version of the
toolkit, which we will call FSA, are:
</bodyText>
<listItem confidence="0.999568">
• decoupling of data structures and algorithms,
• on-demand computation for increased memory
efficiency,
• low computational costs,
• an abstract interface to alphabets to support
lazy mappings from strings to indices for arc
labels,
• an abstract interface to semirings (should be k-
closed for at least some algorithms),
• implementation in C++, as it is fast, ubiquitous
and well-known by many other researchers,
• easy to use interfaces.
</listItem>
<subsectionHeader confidence="0.99864">
3.1 The C++ Library Implementation
</subsectionHeader>
<bodyText confidence="0.9997521">
We use the lemma from Section 2.3 to specify an
interface for lazy algorithmic transducers directly.
The code written in pseudo-C++ is given in Figure
2. Note that all lazy algorithmic transducers are de-
rived from the class Automaton.
The lazy interface also has disadvantages. The
virtual access to the data structure might slow com-
putations down, and obtaining global information
about the automaton becomes more complicated.
For example the size of an automaton can only be
</bodyText>
<figure confidence="0.995460894736842">
class Automaton {
public:
struct Arc {
StateId target();
Weight weight();
LabelId input();
LabelId output();
};
struct State {
StateId id();
Weight weight();
ConstArcIterator arcsBegin();
ConstArcIterator arcsEnd();
};
virtual R&lt;Alphabet&gt; inputAlphabet();
virtual R&lt;Alphabet&gt; outputAlphabet();
virtual StateId initialState();
virtual R&lt;State&gt; getState(StateId);
};
</figure>
<figureCaption confidence="0.939177">
Figure 2: Pseudo-C++ code fragment for the ab-
stract datatype of transducers. Note that R&lt;T&gt;
refers to a smart pointer of T.
</figureCaption>
<bodyText confidence="0.998898411764706">
computed by traversing it. Therefore central al-
gorithms of the RWTH FSA toolkit are the depth-
first search (DFS) and the computation of strongly
connected components (SCC). Efficient versions of
these algorithms are described in (Mehlhorn, 1984)
and (Cormen et al., 1990).
It is very costly to store arbitrary types as arc la-
bels within the arcs itself. Therefore the RWTH
FSA toolkit offers alphabets that define mappings
between strings and label indices. Alphabets are
implemented using the abstract interface shown in
Figure 4. With alphabets arcs only need to store
the abstract label indices. The interface for alpha-
bets is defined using a single constant: for each la-
bel index an alphabet reports it must ensure to al-
ways deliver the same symbol on request through
getSymbol().
</bodyText>
<figure confidence="0.594882571428571">
class Alphabet {
public:
virtual LabelId begin();
virtual LabelId end();
virtual LabelId next(LabelId);
virtual string getSymbol(LabelId);
};
</figure>
<figureCaption confidence="0.9744615">
Figure 4: Pseudo-C++ code fragment for the ab-
stract datatype of alphabets.
</figureCaption>
<subsectionHeader confidence="0.996787">
3.2 Algorithms
</subsectionHeader>
<bodyText confidence="0.990509333333333">
The current implementation of the toolkit offers a
wide range of well-known algorithms defined on
weighted finite-state transducers:
</bodyText>
<listItem confidence="0.949112">
• basic operations
</listItem>
<bodyText confidence="0.682428">
sort (by input labels, output labels or by to-
</bodyText>
<equation confidence="0.9563875">
compose(T1,T2) = simple-compose( cache(sort-output(map-output(T1, AT2,I))),
cache(sort-input(T2)))
</equation>
<figureCaption confidence="0.65775">
Figure 3: Optimized composition where AT2,I denotes the input alphabet of T2. Six algorithmic transducers
are used to gain maximum efficiency. Mapping of arc labels is necessary as symbol indices may differ
between alphabets.
</figureCaption>
<bodyText confidence="0.999675">
tal arc), map-input and -output labels sym-
bolically (as the user expects that two alpha-
bets match symbolically, but their mapping
to label indices may differ), cache (helps to
reduce computations with lazy implementa-
tions), topologically-sort states
</bodyText>
<listItem confidence="0.757519">
• rational operations
</listItem>
<bodyText confidence="0.742784">
project-input, project-output, transpose (also
known as reversal: calculates an equivalent au-
tomaton with the adjacency matrix being trans-
posed), union, concat, invert
</bodyText>
<listItem confidence="0.939172416666667">
• classical graph operations
depth-first search (DFS), single-source short-
est path (SSSP), connect (only keep accessi-
ble and coaccessible state), strongly connected
components (SCCs)
• operations on relations of sets
compose (filtered), intersect, complement
• equivalence transformations
determinize, minimize, remove-epsilons
• search algorithms
best, n-best
• weight/probability-based algorithms
</listItem>
<bodyText confidence="0.999828">
prune (based on forward/backward state po-
tentials), posterior, push (push weights toward
initial/final states), failure (given an accep-
tor/transducer defined over the tropical semir-
ing converts 2-transitions to failure transitions)
</bodyText>
<listItem confidence="0.95500725">
• diagnostic operations
count (counts states, final states, different arc
types, SCCs, alphabet sizes, ...)
• input/output operations
</listItem>
<bodyText confidence="0.998002732394366">
supported input and/or output formats are:
AT&amp;T (currently, ASCII only), binary (fast,
uses fixed byte-order), XML (slower, any en-
coding, fully portable), memory-mapped
(also on-demand), dot (AT&amp;T graphviz)
We will discuss some details and refer to the pub-
lication of the algorithms briefly. Most of the basic
operations have a straigthforward implementation.
As arc labels are integers in the implementation
and their meaning is bound to an appropriate sym-
bolic alphabet, there is the need for symbolic map-
ping between different alphabets. Therefore the
toolkit provides the lazy map-input and map-output
transducers, which map the input and output arc in-
dices of an automaton to be compatible with the in-
dices of another given alphabet.
The implementations of all classical graph algo-
rithms are based on the descriptions of (Mehlhorn,
1984) and (Cormen et al., 1990) and (Mohri and Ri-
ley, 2001) for SSSP. The general graph algorithms
DFS and SCC are helpful in the realisation of many
other operations, examples are: transpose, connect
and count. However, counting the number of states
of an automaton or the number of symbols of an al-
phabet is not well-defined in case of an infinite set
of states or symbols.
SSSP and transpose are the only two algorithms
without a lazy implementation. The result of SSSP
is a list of state potentials (see also (Mohri and Ri-
ley, 2001)). And a lazy implementation for trans-
pose would be possible if the data structures provide
lists of both successor and predecessor arcs at each
state. This needs either more memory or more com-
putations and increases the size of the abstract inter-
face for the lazy algorithms, so as a compromise we
omitted this.
The implementations of compose (Pereira and
Riley, 1996), determinize (Mohri, 1997), minimize
(Mohri, 1997) and remove-epsilons (Mohri, 2001)
use more refined methods to gain efficiency. All
use at least the lazy cache transducer as they re-
fer to states of the input transducer(s) more than
once. With respect to the number of lazy trans-
ducers involved in computing the result, compose
has the most complicated implementation. Given
the implementations for the algorithmic transduc-
ers cache, map-output, sort-input, sort-output and
simple-compose that assumes arc labels to be com-
patible and sorted in order to perform matching as
fast as possible, the final implementation of com-
pose in the RWTH FSA toolkit is given in figure 3.
So, the current implementation of compose uses 6
algorithmic transducers in addition to the two input
automata. Determinize additionally uses lazy cache
and sort-input transducers.
The search algorithms best and n-best are based
on (Mohri and Riley, 2002), push is based on (Mohri
and Riley, 2001) and failure mainly uses ideas from
(Allauzen et al., 2003). The algorithms posterior
and prune compute arc posterior probabilities and
prune arcs with respect to them. We believe they
are standard algorithms defined on probabilistic net-
works and they were simply ported to the frame-
work of weighted finite-state automata.
Finally, the RWTH FSA toolkit can be loosely
interfaced to the AT&amp;T FSM LibraryTM through
its ASCII-based input/output format. In addition,
a new XML-based file format primarly designed as
being human readable and a fast binary file format
are also supported. All file formats support optional
on-the-fly compression using gzip.
</bodyText>
<subsectionHeader confidence="0.978143">
3.3 High-Level Interfaces
</subsectionHeader>
<bodyText confidence="0.988211368421053">
In addition to the C++ library level interface the
toolkit also offers two high-level interfaces: a
Python interface, and an interactive command-line
interface.
The Python interface has been built using the
SWIG interface generator (Beazley et al., 1996)
and enables rapid development of larger applica-
tions without lengthy compilation of C++ code. The
command-line interface comes handy for quickly
applying various combinations of algorithms to
transducers without writing any line of code at all.
As the Python interface is mainly identical to the
C++ interface we will only give a short impression
of how to use the command-line interface.
The command-line interface is a single exe-
cutable and uses a stack-based execution model
(postfix notation) for the application of operations.
This is different from the pipe model that AT&amp;T
command-line tools use. The disadvantage of us-
ing pipes is that automata must be serialized and
get fully expanded by the next executable in chain.
However, an advantage of multiple executables is
that memory does not get fragmented through the
interaction of different algorithms.
With the command-line interface, operations are
applied to the topmost transducers of the stack and
the results are pushed back onto the stack again. For
example,
&gt; fsa A B compose determinize draw -
reads A and B from files, calculates the determinized
composition and writes the resulting automaton to
the terminal in dot format (which may be piped to
dot directly). As you can see from the examples
some operations like write or draw take additional
arguments that must follow the name of the opera-
tion. Although this does not follow the strict postfix
design, we found it more convenient as these param-
eters are not automata.
</bodyText>
<sectionHeader confidence="0.994524" genericHeader="method">
4 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.999206">
4.1 Comparison of Toolkits
</subsectionHeader>
<bodyText confidence="0.9997745">
A crucial aspect of an FSA toolkit is its computa-
tional and memory efficiency. In this section we will
compare the efficiency of four different implemen-
tations of weighted-finite state toolkits, namely:
</bodyText>
<listItem confidence="0.9997488">
• RWTH FSA,
• RWTH FSM (predecessor of RWTH FSA),
• AT&amp;T FSM LibraryTM 4.0 (Mohri et al.,
2000b),
• WFST (Adant, 2000).
</listItem>
<bodyText confidence="0.999478310344827">
We opted to not evaluate the FSA6.1 from (van
Noord, 2000) as we found that it is not easy to in-
stall and it seemed to be significantly slower than
any of the other implementations. RWTH FSA and
the AT&amp;T FSM LibraryTM use on-demand com-
putations whereas FSM and WFST do not. As the
algorithmic code between RWTH FSA and its pre-
decessor RWTH FSM has not changed much ex-
cept for the interface of lazy transducers, we can
also compare lazy versus non lazy implementation.
Nevertheless, this direct comparison is also possible
with RWTH FSA as it provides a static storage class
transducer and a traversing deep copy operation.
Table 1 summarizes the tasks used for the eval-
uation of efficiency together with the sizes of the
resulting transducers. The exact meaning of the dif-
ferent transducers is out of scope of this compari-
son. We simply focus on measuring the efficiency of
the algorithms. Experiment 1 is the full expansion
of the static part of a speech recognition search net-
work. Experiment 2 deals with a translation prob-
lem and splits words of a “bilanguage” into single
words. The meaning of the transducers used for
Experiment 2 will be described in detail in Section
4.2. Experiment 3 is similar to Experiment 1 except
for that the grammar transducer is exchanged with
a translation transducer and the result represents the
static network for a speech-to-text translation sys-
tem.
</bodyText>
<tableCaption confidence="0.906330333333333">
Table 1: Tasks used for measuring the efficiency of
the toolkits. Sizes are given for the resulting trans-
ducers (VM = Verbmobil).
</tableCaption>
<table confidence="0.855400666666667">
Experiment states arcs
1 VM, HCL o G 12,203,420 37,174,684
2 VM, C1 o A o C2 341,614 832,225
</table>
<page confidence="0.905881">
3 Eutrans, HCL o T 1,201,718 3,572,601
</page>
<bodyText confidence="0.9982235625">
All experiments were performed on a PC with a
1.2GHz AMD Athlon processor and 2 GB of mem-
ory using Linux as operating system. Table 2 sum-
marizes the peak memory usage of the different
toolkit implementations for the given tasks and Ta-
ble 3 shows the CPU usage accordingly.
As can be seen from Tables 2 and 3 for all given
tasks the RWTH FSA toolkit uses less memory and
computational power than any of the other toolk-
its. However, it is unclear to the authors why the
AT&amp;T LibraryTM is a factor of 1800 slower for ex-
periment 2. The numbers also do not change much
after additionally connecting the composition result
(as in RWTH FSA compose does not connect the
result by default): memory usage rises to 62 MB
and execution time increases to 9.7 seconds. How-
ever, a detailed analysis for the RWTH FSA toolkit
has shown that the composition task of experiment
2 makes intense use of the lazy cache transducer
due to the loop character of the two transducers C1
and C2.
It can also be seen from the two tables that
the lazy implementation RWTH FSA uses signif-
icantly less memory than the non lazy implemen-
tation RWTH FSM and less than half of the CPU
time. One explanation for this is the poor mem-
ory management of RWTH FSM as all interme-
diate results need to be fully expanded in mem-
ory. In contrast, due to its lazy transducer inter-
face, RWTH FSA may allocate memory for a state
only once and reuse it for all subsequent calls to the
getState() method.
</bodyText>
<tableCaption confidence="0.8102925">
Table 2: Comparison of peak memory usage in MB
(* aborted due to exceeded memory limits).
</tableCaption>
<table confidence="0.99968">
Exp. FSA FSM AT&amp;T WFST
1 360 1700 1500 &gt; 1850*
2 59 310 69 &gt; 1850*
3 48 230 176 550
</table>
<tableCaption confidence="0.8701165">
Table 3: Comparison of CPU time in seconds in-
cluding I/O using a 1.2GHz AMD Athlon proces-
sor (* exceeded memory limits: given time indicates
point of abortion).
</tableCaption>
<table confidence="0.9980955">
Exp. FSA FSM AT&amp;T WFST
1 105 203 515 &gt; 40*
2 6.5 182 11760 &gt; 64*
3 6.6 21 28 3840
</table>
<subsectionHeader confidence="0.992288">
4.2 Statistical Machine Translation
</subsectionHeader>
<bodyText confidence="0.962691470588235">
Statistical machine translation may be viewed as
a weighted language transduction problem (Vidal,
1997). Therefore it is fairly easy to build a machine
translation system with the use of weighted finite-
state transducers.
Let fJ1 and eIi be two sentences from a source
and target language respectively. Also assume that
we have word level alignments A of all sentences
from a bilingual training corpus. We denote with
epJ
p1 the segmentation of a target sentence eI1 into
phrases such that fJ1 and epJ
p1 can be aligned mono-
toneously. This segmentation can be directly calcu-
lated from the alignments A. Then we can formu-
late the problem of finding the best translation ˆeI1 of
a source sentence as follows:
</bodyText>
<equation confidence="0.999209411764706">
ˆeI1 = argmax
eI
1
Pr(fJ1 , epJ
p1 )
�
= argmax Pr(fj, epj|fj−1
1 , epj−1
p1 )
A,epJ
p1 fj:j=1..J
�
≈ argmax Pr(fj, epj|fj−1
j−n, epj−1
pj−n)
A,epJ
p1 fj:j=1..J
</equation>
<bodyText confidence="0.999590933333334">
The last line suggests to solve the translation
problem by estimating a language model on a bi-
language (see also (Bangalore and Riccardi, 2000;
Casacuberta et al., 2001)). An example of sentences
from this bilanguage is given in Figure 5 for the
translation task Vermobil (German → English). For
technical reasons, 2-labels are represented by a $
symbol. Note, that due to the fixed segmentation
given by the alignments, phrases in the target lan-
guage are moved to the last source word of an align-
ment block.
So, given an appropriate alignment which can
be obtained by means of the pubically available
GIZA++ toolkit (Och and Ney, 2000), the approach
is very easy in practice:
</bodyText>
<listItem confidence="0.9990518">
1. Transform the training corpus with a given
alignment into the corresponding bilingual cor-
pus
2. Train a language model on the bilingual corpus
3. Build an acceptor A from the language model
</listItem>
<bodyText confidence="0.999908625">
The symbols of the resulting acceptor are still a mix-
ture of words from the source language and phrases
from the target language. So, we additionally use
two simple transducers to split these bilingual words
(C1 maps source words fj to bilingual words that
start with fj and C2 maps bilingual words with the
target sequence epj to the sequences of target words
the phrase was made of):
</bodyText>
<listItem confidence="0.9437405">
4. Split the bilingual phrases of A into single
words:
</listItem>
<page confidence="0.833181">
T=C1oAoC2
</page>
<bodyText confidence="0.986077">
Then the translation problem from above can be
rewritten using finite-state terminology:
</bodyText>
<figure confidence="0.981081142857143">
Pr(fJ1 , eI1)
≈ argmax
A,epJ
p1
dann|$ melde|$ ich|I_am_calling mich|$ noch|$ einmal|once_more .|.
11U|eleven Uhr|o’clock ist|is hervorragend|excellent .|.
ich|I bin|have da|$ relativ|quite_a_lot_of frei|free_days_then .|.
</figure>
<figureCaption confidence="0.996788">
Figure 5: Example corpus for the bilanguage (Verbmobil, German → English).
</figureCaption>
<tableCaption confidence="0.9028585">
Table 4: Translation results for different tasks compared to similar systems using the alignment template
(AT) approach (Tests were performed on a 1.2GHz AMD Athlon).
</tableCaption>
<table confidence="0.9993353">
Task System Translation WER PER 100-BLEU Memory Time/Sentence
[%] [%] [MB] [ms]
Eutrans FSA Spanish → English 8.12 7.64 10.7 6-8 20
AT 8.25 - - - -
FUB FSA Italian → English 27.0 21.5 37.7 3-5 22
AT 23.7 18.1 36.0 - -
Verbmobil FSA German → English 48.3 41.6 69.8 65-90 460
AT 40.5 30.1 62.2 - -
PF-Star FSA Italian → English 39.8 34.1 58.4 12-15 35
AT 36.8 29.1 54.3 - -
</table>
<equation confidence="0.586644">
e&apos; = project-output(best(f ◦ T))
</equation>
<bodyText confidence="0.999638151515152">
Translation results using this approach are summa-
rized in Table 4 and are being compared with results
obtained using the alignment template approach
(Och and Ney, 2000). Results for both approaches
were obtaining using the same training corpus align-
ments. Detailed task descriptions for Eutrans/FUB
and Verbmobil can be found in (Casacuberta et al.,
2001) and (Zens et al., 2002) respectively. We use
the usual definitions for word error rate (WER), po-
sition independent word error rate (PER) and BLEU
statistics here.
For the simpler tasks Eutrans, FUB and PF-Star,
the WER, PER and the inverted BLEU statistics
are close for both approaches. On the German-to-
English Verbmobil task the FSA approach suffers
from long distance reorderings (captured through
the fixed training corpus segmentation), which is not
very surprising.
Although we do not have comparable numbers of
the memory usage and the translation times for the
alignment template approach, resource usage of the
finite-state approach is quite remarkable as we only
use generic methods from the RWTH FSA toolkit
and full search (i.e. we do not prune the search
space). However, informal tests have shown that
the finite-state approach uses much less memory
and computations than the current implementation
of the alignment template approach.
Two additional advantages of finite-state methods
for translation in general are: the input to the search
algorithm may also be a word lattice and it is easy
to combine speech recognition with translation in
order to do speech-to-speech translation.
</bodyText>
<sectionHeader confidence="0.99914" genericHeader="method">
5 Summary
</sectionHeader>
<bodyText confidence="0.999615761904762">
In this paper we have given a characterization of al-
gorithms that produce a single finite-state automa-
ton and bear an on-demand implementation. For
this purpose we formally introduced the local prop-
erty of such an algorithm.
We have described the efficient implementation
of a finite-state toolkit that uses the principle of
lazy algorithmic transducers for almost all algo-
rithms. Among several publically available toolkits,
the RWTH FSA toolkit presented here turned out to
be the most efficient one, as several tests showed.
Additionally, with lazy algorithmic transducers we
have reduced the memory requirements and even in-
creased the speed significantly compared to a non
lazy implementation.
We have also shown that a finite-state automata
toolkit supports rapid solutions to problems from
the field of natural language processing such as sta-
tistical machine translation. Despite the genericity
of the methods, statistical machine translation can
be done very efficiently.
</bodyText>
<sectionHeader confidence="0.996805" genericHeader="evaluation">
6 Shortcomings and Future Extensions
</sectionHeader>
<bodyText confidence="0.999811125">
There is still room to improve the RWTH FSA
toolkit. For example, the current implementation
of determinization is not as general as described in
(Allauzen and Mohri, 2003). In case of ambiguous
input the algorithm still produces an infinite trans-
ducer. At the moment this can be solved in many
cases by adding disambiguation symbols to the in-
put transducer manually.
As the implementation model is based on virtual
C++ methods for all types of objects in use (semir-
ings, alphabets, transducers and algorithmic trans-
ducers) it should also be fairly easy to add support
for dynamically loadable objects to the toolkit.
Other semirings like the expectation semiring de-
scribed in (Eisner, 2001) are supported but not yet
implemented.
</bodyText>
<sectionHeader confidence="0.996709" genericHeader="conclusions">
7 Acknowledgment
</sectionHeader>
<bodyText confidence="0.998935">
The authors would like to thank Andre Altmann for
his help with the translation experiments.
</bodyText>
<sectionHeader confidence="0.998502" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999606792079208">
Alfred V. Aho and Jeffrey D. Ullman, 1972, The The-
ory of Parsing, Translation and Compiling, volume 1,
Prentice-Hall, Englewood Cliffs, NJ, 1972.
Arnaud Adant, 2000, WFST: A Finite-State Template Li-
brary in C++, http://membres.lycos.fr/adant/tfe/.
Cyril Allauzen, Mehryar Mohri, and Brian Roark, 2003,
Generalized Algorithms for Constructing Statistical
Language Models, In Proc. of the 41st Meeting of the
Association for Computational Linguistics, Sapporo,
Japan, July 2003.
Cyril Allauzen and Mehryar Mohri, 2003, General-
ized Optimization Algorithm for Speech Recognition
Transducers, In Proc. of the IEEE Int. Conf. on
Acoustics, Speech, and Signal Processing, pp. , Hong
Kong, China, April 2003.
Srinivas Bangalore and Giuseppe Riccardi, 2000,
Stochastic Finite-State models for Spoken Language
Machine Translation, In Proc. of the Workshop on
Embedded Machine Translation Systems, pp. 52–59,
2000.
David Beazley, William Fulton, Matthias K¨oppe, Lyle
Johnson, Richard Palmer, 1996, SWIG - Simplified
Wrapper and Interface Generator, Electronic Docu-
ment, http://www.swig.org, February 1996.
F. Casacuberta, D. Llorens, C. Martinez, S. Molau, F.
Nevado, H. Ney, M. Pasto, D. Pico, A. Sanchis, E. Vi-
dal and J.M. Vilar, 2001, Speech-to-Speech Transla-
tion based on Finite-State Transducer, In Proc. IEEE
Int. Conf. on Acoustics, Speech and Signal Process-
ing, pp. 613-616, Salt Lake City, Utah, May 2001.
Thomas H. Cormen, Charles E. Leiserson and Ronald L.
Rivest, 1990, Introductions to Algorithms, The MIT
Press, Cambridge, MA, 1990.
Jason Eisner, 2001, Expectation Semirings: Flexible
EM for Finite-State Transducers, In Proc. of the
ESSLLI Workshop on Finite-State Methods in NLP
(FSMNLP), Helsinki, August 2001.
Free Software Foundation, 1991, GNU General
Public License, Version 2, Electronic Document,
http://www.gnu.org/copyleft/gpl.html, June 1991.
Takaaki Hori, Chiori Hori and Yasuhiro Minami, 2003,
Speech Summarization using Weighted Finite-State
Transducers, In Proc. of the European Conf. on
Speech Communication and Technology, Geneva,
Switzerland, September 2003.
Vincent Le Maout, 1998, ASTL: Automaton Stan-
dardTemplate Library, http://www-igm.univ-
mlv.fr/˜lemaout/.
Kurt Mehlhorn, 1984, Data Structures and Efficient Al-
gorithms, Chapter 4, Springer Verlag, EATCS Mono-
graphs, 1984, also available from http://www.mpi-
sb.mpg.de/˜mehlhorn/DatAlgbooks.html.
Mehryar Mohri, 1997, Finite-State Transducers in Lan-
guage and Speech Processing, Computational Lin-
guistics, 23:2, 1997.
Mehryar Mohri, Fernando C.N. Pereira, and Michael
Riley, 2000, Weighted Finite-State Transducers in
Speech Recognition, In Proc. of the ISCA Tutorial and
Research Workshop, Automatic Speech Recognition:
Challenges for the new Millenium (ASR2000), Paris,
France, September 2000.
Mehryar Mohri, Fernando C.N. Pereira, and Michael Ri-
ley, 2000, The Design Principles ofa Weighted Finite-
State Transducer Library, Theoretical Computer Sci-
ence, 231:17-32, January 2000.
Mehryar Mohri and Michael Riley, 2000, A Weight
Pushing Algorithm for Large Vocabulary Speech
Recognition, In Proc. of the European Conf. on
Speech Communication and Technology, pp. 1603–
1606, ˚Aalborg, Denmark, September 2001.
Mehryar Mohri, 2001, Generic Epsilon-Removal Algo-
rithm for Weighted Automata, In Sheng Yu and An-
drei Paun, editor, 5th Int. Conf., CIAA 2000, London
Ontario, Canada. volume 2088 of Lecture Notes in
Computer Science, pages 230-242. Springer-Verlag,
Berlin-NY, 2001.
Mehryar Mohri and Michael Riley, 2002, An Efficient
Algorithm for the N-Best-Strings Problem, In Proc.
of the Int. Conf. on Spoken Language Processing, pp.
1313–1316, Denver, Colorado, September 2002.
Franz J. Och and Hermann Ney, 2000, Improved Sta-
tistical Alignment Models, In Proc. of the 38th An-
nual Meeting of the Association for Computational
Linguistics, pp. 440-447, Hongkong, China, October
2000.
Fernando C.N. Pereira and Michael Riley, 1996, Speech
Recognition by Composition of Weighted Finite
Automata, Available from http://xxx.lanl.gov/cmp-
lg/9603001, Computation and Language, 1996.
Gertjan van Noord, 2000, FSA6 Reference Manual,
http://odur.let.rug.nl/˜vannoord/Fsa/.
Enrique Vidal, 1997, Finite-State Speech-to-Speech
Translation, In Proc. of the IEEE Int. Conf. on Acous-
tics, Speech and Signal Processing, pp. 111–114, Mu-
nich, Germany, 1997.
Richard Zens, Franz J. Och and H. Ney, 2002, Phrase-
Based Statistical Machine Translation, In: M. Jarke,
J. Koehler, G. Lakemeyer (Eds.) : KI - 2002: Ad-
vances in artificial intelligence. 25. Annual German
Conference on AI, KI 2002, Vol. LNAI 2479, pp. 18-
32, Springer Verlag, September 2002.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.966195">
<title confidence="0.998568">FSA: An Efficient and Flexible C++ Toolkit for Finite State Automata Using On-Demand Computation</title>
<author confidence="0.999982">Stephan Kanthak</author>
<author confidence="0.999982">Hermann Ney</author>
<affiliation confidence="0.994739">Lehrstuhl f¨ur Informatik VI, Computer Science Department RWTH Aachen – University of Technology</affiliation>
<address confidence="0.99993">52056 Aachen, Germany</address>
<abstract confidence="0.99894145">In this paper we present the RWTH FSA toolkit – an efficient implementation of algorithms for creating and manipulating weighted finite-state automata. The toolkit has been designed using the principle of on-demand computation and offers a large range of widely used algorithms. To prove the superior efficiency of the toolkit, we compare the implementation to that of other publically available toolkits. We also show that on-demand computations help to reduce memory requirements significantly without any loss in speed. To increase its flexibility, the RWTH FSA toolkit supports high-level interfaces to the programming language Python as well as a command-line tool for interactive manipulation of FSAs. Furthermore, we show how to utilize the toolkit to rapidly build a fast and accurate statistical machine translation system. Future extensibility of the toolkit is ensured as it will be publically available as open source software.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>The Theory of Parsing,</title>
<date>1972</date>
<journal>Translation and Compiling,</journal>
<volume>1</volume>
<location>Prentice-Hall, Englewood Cliffs, NJ,</location>
<marker>Aho, Ullman, 1972</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman, 1972, The Theory of Parsing, Translation and Compiling, volume 1, Prentice-Hall, Englewood Cliffs, NJ, 1972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arnaud Adant</author>
</authors>
<title>WFST: A Finite-State Template Library</title>
<date>2000</date>
<note>in C++, http://membres.lycos.fr/adant/tfe/.</note>
<contexts>
<context position="2449" citStr="Adant, 2000" startWordPosition="378" endWordPosition="379">ols to create and manipulate FSAs. In the past, a number of toolkits have been published, all with different design principles. Here, we give a short overview of toolkits that offer an almost complete set of algorithms: • The FSM LibraryTM from AT&amp;T (Mohri et al., 2000b) is judged the most efficient implementation, offers various semirings, ondemand computation and many algorithms, but is available only in binary form with a proprietary, non commercial license. • FSA6.1 from (van Noord, 2000) is implemented in Prolog. It is licensed under the terms of the (GPL, 1991). • The WFST toolkit from (Adant, 2000) is built on top of the Automaton Standard Template Library (LeMaout, 1998) and uses C++ template mechanisms for efficiency and flexibility, but lacks on-demand computation. Also licensed under the terms of the (GPL, 1991). This paper describes a highly efficient new implementation of a finite-state automata toolkit that uses on-demand computation. Currently, it is being used at the Lehrstuhl f¨ur Informatik VI, RWTH Aachen in different speech recognition and translation research applications. The toolkit will be available under an open source license (GPL, 1991) and can be obtained from our w</context>
<context position="19418" citStr="Adant, 2000" startWordPosition="3197" endWordPosition="3198">the examples some operations like write or draw take additional arguments that must follow the name of the operation. Although this does not follow the strict postfix design, we found it more convenient as these parameters are not automata. 4 Experimental Results 4.1 Comparison of Toolkits A crucial aspect of an FSA toolkit is its computational and memory efficiency. In this section we will compare the efficiency of four different implementations of weighted-finite state toolkits, namely: • RWTH FSA, • RWTH FSM (predecessor of RWTH FSA), • AT&amp;T FSM LibraryTM 4.0 (Mohri et al., 2000b), • WFST (Adant, 2000). We opted to not evaluate the FSA6.1 from (van Noord, 2000) as we found that it is not easy to install and it seemed to be significantly slower than any of the other implementations. RWTH FSA and the AT&amp;T FSM LibraryTM use on-demand computations whereas FSM and WFST do not. As the algorithmic code between RWTH FSA and its predecessor RWTH FSM has not changed much except for the interface of lazy transducers, we can also compare lazy versus non lazy implementation. Nevertheless, this direct comparison is also possible with RWTH FSA as it provides a static storage class transducer and a travers</context>
</contexts>
<marker>Adant, 2000</marker>
<rawString>Arnaud Adant, 2000, WFST: A Finite-State Template Library in C++, http://membres.lycos.fr/adant/tfe/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Mehryar Mohri</author>
<author>Brian Roark</author>
</authors>
<title>Generalized Algorithms for Constructing Statistical Language Models,</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Meeting of the Association for Computational Linguistics,</booktitle>
<location>Sapporo, Japan,</location>
<contexts>
<context position="1667" citStr="Allauzen et al., 2003" startWordPosition="241" endWordPosition="244">toolkit to rapidly build a fast and accurate statistical machine translation system. Future extensibility of the toolkit is ensured as it will be publically available as open source software. 1 Introduction Finite-state automata (FSA) methods proved to elegantly solve many difficult problems in the field of natural language processing. Among the most recent ones are full and lazy compilation of the search network for speech recognition (Mohri et al., 2000a), integrated speech translation (Vidal, 1997; Bangalore and Riccardi, 2000), speech summarization (Hori et al., 2003), language modelling (Allauzen et al., 2003) and parameter estimation through EM (Eisner, 2001) to mention only a few. From this list of different applications it is clear that there is a high demand for generic tools to create and manipulate FSAs. In the past, a number of toolkits have been published, all with different design principles. Here, we give a short overview of toolkits that offer an almost complete set of algorithms: • The FSM LibraryTM from AT&amp;T (Mohri et al., 2000b) is judged the most efficient implementation, offers various semirings, ondemand computation and many algorithms, but is available only in binary form with a p</context>
<context position="4136" citStr="Allauzen et al., 2003" startWordPosition="644" endWordPosition="647">basis for implementations using on-demand computations. Then the RWTH FSA toolkit implementation is detailed in Section 3. In Section 4.1 we will compare the efficiency of different toolkits. As a showcase for the flexibility we show how to use the toolkit to build a statistical machine translation system in Section 4.2. We conclude the paper with a short summary in Section 5 and discuss some possible future extensions in Section 6. 2 Finite-State Automata 2.1 Weighted Finite-State Transducer The basic theory of weighted finite-state automata has been reviewed in numerous papers (Mohri, 1997; Allauzen et al., 2003). We will introduce the notation briefly. A semiring (K, ⊕, ⊗, 0,1) is a structure with a set K and two binary operations ⊕ and ⊗ such that (K, ⊕, 0) is a commutative monoid, (K, ⊗,1) is a monoid and ⊗ distributes over ⊕ and 0 ⊗ x = x ⊗ 0 = 0 for any x ∈ K. We will also associate the term weights with the elements of a semiring. Semirings that are frequently used in speech recognition are the positive real semiring (IR ∪ {−∞, +∞}, ⊕log, +, +∞, 0) with a ⊕log b = −log(e−a + e−b) and the tropical semiring (IR∪{−∞, +∞}, min, +, +∞, 0) representing the well-known sum and maximum weighted path crit</context>
<context position="16681" citStr="Allauzen et al., 2003" startWordPosition="2759" endWordPosition="2762">rithmic transducers cache, map-output, sort-input, sort-output and simple-compose that assumes arc labels to be compatible and sorted in order to perform matching as fast as possible, the final implementation of compose in the RWTH FSA toolkit is given in figure 3. So, the current implementation of compose uses 6 algorithmic transducers in addition to the two input automata. Determinize additionally uses lazy cache and sort-input transducers. The search algorithms best and n-best are based on (Mohri and Riley, 2002), push is based on (Mohri and Riley, 2001) and failure mainly uses ideas from (Allauzen et al., 2003). The algorithms posterior and prune compute arc posterior probabilities and prune arcs with respect to them. We believe they are standard algorithms defined on probabilistic networks and they were simply ported to the framework of weighted finite-state automata. Finally, the RWTH FSA toolkit can be loosely interfaced to the AT&amp;T FSM LibraryTM through its ASCII-based input/output format. In addition, a new XML-based file format primarly designed as being human readable and a fast binary file format are also supported. All file formats support optional on-the-fly compression using gzip. 3.3 Hig</context>
</contexts>
<marker>Allauzen, Mohri, Roark, 2003</marker>
<rawString>Cyril Allauzen, Mehryar Mohri, and Brian Roark, 2003, Generalized Algorithms for Constructing Statistical Language Models, In Proc. of the 41st Meeting of the Association for Computational Linguistics, Sapporo, Japan, July 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Mehryar Mohri</author>
</authors>
<title>Generalized Optimization Algorithm for Speech Recognition Transducers,</title>
<date>2003</date>
<booktitle>In Proc. of the IEEE Int. Conf. on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>pp.</pages>
<location>Hong Kong, China,</location>
<contexts>
<context position="28877" citStr="Allauzen and Mohri, 2003" startWordPosition="4804" endWordPosition="4807"> transducers we have reduced the memory requirements and even increased the speed significantly compared to a non lazy implementation. We have also shown that a finite-state automata toolkit supports rapid solutions to problems from the field of natural language processing such as statistical machine translation. Despite the genericity of the methods, statistical machine translation can be done very efficiently. 6 Shortcomings and Future Extensions There is still room to improve the RWTH FSA toolkit. For example, the current implementation of determinization is not as general as described in (Allauzen and Mohri, 2003). In case of ambiguous input the algorithm still produces an infinite transducer. At the moment this can be solved in many cases by adding disambiguation symbols to the input transducer manually. As the implementation model is based on virtual C++ methods for all types of objects in use (semirings, alphabets, transducers and algorithmic transducers) it should also be fairly easy to add support for dynamically loadable objects to the toolkit. Other semirings like the expectation semiring described in (Eisner, 2001) are supported but not yet implemented. 7 Acknowledgment The authors would like t</context>
</contexts>
<marker>Allauzen, Mohri, 2003</marker>
<rawString>Cyril Allauzen and Mehryar Mohri, 2003, Generalized Optimization Algorithm for Speech Recognition Transducers, In Proc. of the IEEE Int. Conf. on Acoustics, Speech, and Signal Processing, pp. , Hong Kong, China, April 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Stochastic Finite-State models for Spoken Language Machine Translation,</title>
<date>2000</date>
<booktitle>In Proc. of the Workshop on Embedded Machine Translation Systems,</booktitle>
<pages>52--59</pages>
<contexts>
<context position="1581" citStr="Bangalore and Riccardi, 2000" startWordPosition="228" endWordPosition="231">mand-line tool for interactive manipulation of FSAs. Furthermore, we show how to utilize the toolkit to rapidly build a fast and accurate statistical machine translation system. Future extensibility of the toolkit is ensured as it will be publically available as open source software. 1 Introduction Finite-state automata (FSA) methods proved to elegantly solve many difficult problems in the field of natural language processing. Among the most recent ones are full and lazy compilation of the search network for speech recognition (Mohri et al., 2000a), integrated speech translation (Vidal, 1997; Bangalore and Riccardi, 2000), speech summarization (Hori et al., 2003), language modelling (Allauzen et al., 2003) and parameter estimation through EM (Eisner, 2001) to mention only a few. From this list of different applications it is clear that there is a high demand for generic tools to create and manipulate FSAs. In the past, a number of toolkits have been published, all with different design principles. Here, we give a short overview of toolkits that offer an almost complete set of algorithms: • The FSM LibraryTM from AT&amp;T (Mohri et al., 2000b) is judged the most efficient implementation, offers various semirings, o</context>
<context position="23986" citStr="Bangalore and Riccardi, 2000" startWordPosition="4021" endWordPosition="4024">ual training corpus. We denote with epJ p1 the segmentation of a target sentence eI1 into phrases such that fJ1 and epJ p1 can be aligned monotoneously. This segmentation can be directly calculated from the alignments A. Then we can formulate the problem of finding the best translation ˆeI1 of a source sentence as follows: ˆeI1 = argmax eI 1 Pr(fJ1 , epJ p1 ) � = argmax Pr(fj, epj|fj−1 1 , epj−1 p1 ) A,epJ p1 fj:j=1..J � ≈ argmax Pr(fj, epj|fj−1 j−n, epj−1 pj−n) A,epJ p1 fj:j=1..J The last line suggests to solve the translation problem by estimating a language model on a bilanguage (see also (Bangalore and Riccardi, 2000; Casacuberta et al., 2001)). An example of sentences from this bilanguage is given in Figure 5 for the translation task Vermobil (German → English). For technical reasons, 2-labels are represented by a $ symbol. Note, that due to the fixed segmentation given by the alignments, phrases in the target language are moved to the last source word of an alignment block. So, given an appropriate alignment which can be obtained by means of the pubically available GIZA++ toolkit (Och and Ney, 2000), the approach is very easy in practice: 1. Transform the training corpus with a given alignment into the </context>
</contexts>
<marker>Bangalore, Riccardi, 2000</marker>
<rawString>Srinivas Bangalore and Giuseppe Riccardi, 2000, Stochastic Finite-State models for Spoken Language Machine Translation, In Proc. of the Workshop on Embedded Machine Translation Systems, pp. 52–59, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Beazley</author>
<author>William Fulton</author>
<author>Matthias K¨oppe</author>
<author>Lyle Johnson</author>
<author>Richard Palmer</author>
</authors>
<date>1996</date>
<booktitle>SWIG - Simplified Wrapper and Interface Generator, Electronic Document,</booktitle>
<location>http://www.swig.org,</location>
<marker>Beazley, Fulton, K¨oppe, Johnson, Palmer, 1996</marker>
<rawString>David Beazley, William Fulton, Matthias K¨oppe, Lyle Johnson, Richard Palmer, 1996, SWIG - Simplified Wrapper and Interface Generator, Electronic Document, http://www.swig.org, February 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Casacuberta</author>
<author>D Llorens</author>
<author>C Martinez</author>
<author>S Molau</author>
<author>F Nevado</author>
<author>H Ney</author>
<author>M Pasto</author>
<author>D Pico</author>
<author>A Sanchis</author>
<author>E Vidal</author>
<author>J M Vilar</author>
</authors>
<title>Speech-to-Speech Translation based on Finite-State Transducer,</title>
<date>2001</date>
<booktitle>In Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing,</booktitle>
<pages>613--616</pages>
<location>Salt Lake City, Utah,</location>
<contexts>
<context position="24013" citStr="Casacuberta et al., 2001" startWordPosition="4025" endWordPosition="4028"> with epJ p1 the segmentation of a target sentence eI1 into phrases such that fJ1 and epJ p1 can be aligned monotoneously. This segmentation can be directly calculated from the alignments A. Then we can formulate the problem of finding the best translation ˆeI1 of a source sentence as follows: ˆeI1 = argmax eI 1 Pr(fJ1 , epJ p1 ) � = argmax Pr(fj, epj|fj−1 1 , epj−1 p1 ) A,epJ p1 fj:j=1..J � ≈ argmax Pr(fj, epj|fj−1 j−n, epj−1 pj−n) A,epJ p1 fj:j=1..J The last line suggests to solve the translation problem by estimating a language model on a bilanguage (see also (Bangalore and Riccardi, 2000; Casacuberta et al., 2001)). An example of sentences from this bilanguage is given in Figure 5 for the translation task Vermobil (German → English). For technical reasons, 2-labels are represented by a $ symbol. Note, that due to the fixed segmentation given by the alignments, phrases in the target language are moved to the last source word of an alignment block. So, given an appropriate alignment which can be obtained by means of the pubically available GIZA++ toolkit (Och and Ney, 2000), the approach is very easy in practice: 1. Transform the training corpus with a given alignment into the corresponding bilingual cor</context>
<context position="26479" citStr="Casacuberta et al., 2001" startWordPosition="4433" endWordPosition="4436">8.25 - - - - FUB FSA Italian → English 27.0 21.5 37.7 3-5 22 AT 23.7 18.1 36.0 - - Verbmobil FSA German → English 48.3 41.6 69.8 65-90 460 AT 40.5 30.1 62.2 - - PF-Star FSA Italian → English 39.8 34.1 58.4 12-15 35 AT 36.8 29.1 54.3 - - e&apos; = project-output(best(f ◦ T)) Translation results using this approach are summarized in Table 4 and are being compared with results obtained using the alignment template approach (Och and Ney, 2000). Results for both approaches were obtaining using the same training corpus alignments. Detailed task descriptions for Eutrans/FUB and Verbmobil can be found in (Casacuberta et al., 2001) and (Zens et al., 2002) respectively. We use the usual definitions for word error rate (WER), position independent word error rate (PER) and BLEU statistics here. For the simpler tasks Eutrans, FUB and PF-Star, the WER, PER and the inverted BLEU statistics are close for both approaches. On the German-toEnglish Verbmobil task the FSA approach suffers from long distance reorderings (captured through the fixed training corpus segmentation), which is not very surprising. Although we do not have comparable numbers of the memory usage and the translation times for the alignment template approach, r</context>
</contexts>
<marker>Casacuberta, Llorens, Martinez, Molau, Nevado, Ney, Pasto, Pico, Sanchis, Vidal, Vilar, 2001</marker>
<rawString>F. Casacuberta, D. Llorens, C. Martinez, S. Molau, F. Nevado, H. Ney, M. Pasto, D. Pico, A. Sanchis, E. Vidal and J.M. Vilar, 2001, Speech-to-Speech Translation based on Finite-State Transducer, In Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing, pp. 613-616, Salt Lake City, Utah, May 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Charles E Leiserson</author>
<author>Ronald L Rivest</author>
</authors>
<title>Introductions to Algorithms,</title>
<date>1990</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="11435" citStr="Cormen et al., 1990" startWordPosition="1963" endWordPosition="1966">; Weight weight(); ConstArcIterator arcsBegin(); ConstArcIterator arcsEnd(); }; virtual R&lt;Alphabet&gt; inputAlphabet(); virtual R&lt;Alphabet&gt; outputAlphabet(); virtual StateId initialState(); virtual R&lt;State&gt; getState(StateId); }; Figure 2: Pseudo-C++ code fragment for the abstract datatype of transducers. Note that R&lt;T&gt; refers to a smart pointer of T. computed by traversing it. Therefore central algorithms of the RWTH FSA toolkit are the depthfirst search (DFS) and the computation of strongly connected components (SCC). Efficient versions of these algorithms are described in (Mehlhorn, 1984) and (Cormen et al., 1990). It is very costly to store arbitrary types as arc labels within the arcs itself. Therefore the RWTH FSA toolkit offers alphabets that define mappings between strings and label indices. Alphabets are implemented using the abstract interface shown in Figure 4. With alphabets arcs only need to store the abstract label indices. The interface for alphabets is defined using a single constant: for each label index an alphabet reports it must ensure to always deliver the same symbol on request through getSymbol(). class Alphabet { public: virtual LabelId begin(); virtual LabelId end(); virtual Label</context>
<context position="14776" citStr="Cormen et al., 1990" startWordPosition="2444" endWordPosition="2447"> the publication of the algorithms briefly. Most of the basic operations have a straigthforward implementation. As arc labels are integers in the implementation and their meaning is bound to an appropriate symbolic alphabet, there is the need for symbolic mapping between different alphabets. Therefore the toolkit provides the lazy map-input and map-output transducers, which map the input and output arc indices of an automaton to be compatible with the indices of another given alphabet. The implementations of all classical graph algorithms are based on the descriptions of (Mehlhorn, 1984) and (Cormen et al., 1990) and (Mohri and Riley, 2001) for SSSP. The general graph algorithms DFS and SCC are helpful in the realisation of many other operations, examples are: transpose, connect and count. However, counting the number of states of an automaton or the number of symbols of an alphabet is not well-defined in case of an infinite set of states or symbols. SSSP and transpose are the only two algorithms without a lazy implementation. The result of SSSP is a list of state potentials (see also (Mohri and Riley, 2001)). And a lazy implementation for transpose would be possible if the data structures provide lis</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, 1990</marker>
<rawString>Thomas H. Cormen, Charles E. Leiserson and Ronald L. Rivest, 1990, Introductions to Algorithms, The MIT Press, Cambridge, MA, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Expectation Semirings: Flexible EM for Finite-State Transducers,</title>
<date>2001</date>
<booktitle>In Proc. of the ESSLLI Workshop on Finite-State Methods in NLP (FSMNLP),</booktitle>
<location>Helsinki,</location>
<contexts>
<context position="1718" citStr="Eisner, 2001" startWordPosition="250" endWordPosition="251">hine translation system. Future extensibility of the toolkit is ensured as it will be publically available as open source software. 1 Introduction Finite-state automata (FSA) methods proved to elegantly solve many difficult problems in the field of natural language processing. Among the most recent ones are full and lazy compilation of the search network for speech recognition (Mohri et al., 2000a), integrated speech translation (Vidal, 1997; Bangalore and Riccardi, 2000), speech summarization (Hori et al., 2003), language modelling (Allauzen et al., 2003) and parameter estimation through EM (Eisner, 2001) to mention only a few. From this list of different applications it is clear that there is a high demand for generic tools to create and manipulate FSAs. In the past, a number of toolkits have been published, all with different design principles. Here, we give a short overview of toolkits that offer an almost complete set of algorithms: • The FSM LibraryTM from AT&amp;T (Mohri et al., 2000b) is judged the most efficient implementation, offers various semirings, ondemand computation and many algorithms, but is available only in binary form with a proprietary, non commercial license. • FSA6.1 from (</context>
</contexts>
<marker>Eisner, 2001</marker>
<rawString>Jason Eisner, 2001, Expectation Semirings: Flexible EM for Finite-State Transducers, In Proc. of the ESSLLI Workshop on Finite-State Methods in NLP (FSMNLP), Helsinki, August 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Free Software Foundation</author>
</authors>
<date>1991</date>
<booktitle>GNU General Public License, Version 2, Electronic Document,</booktitle>
<location>http://www.gnu.org/copyleft/gpl.html,</location>
<marker>Foundation, 1991</marker>
<rawString>Free Software Foundation, 1991, GNU General Public License, Version 2, Electronic Document, http://www.gnu.org/copyleft/gpl.html, June 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Hori</author>
<author>Chiori Hori</author>
<author>Yasuhiro Minami</author>
</authors>
<title>Speech Summarization using Weighted Finite-State Transducers,</title>
<date>2003</date>
<booktitle>In Proc. of the European Conf. on Speech Communication and Technology,</booktitle>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="1623" citStr="Hori et al., 2003" startWordPosition="235" endWordPosition="238">Furthermore, we show how to utilize the toolkit to rapidly build a fast and accurate statistical machine translation system. Future extensibility of the toolkit is ensured as it will be publically available as open source software. 1 Introduction Finite-state automata (FSA) methods proved to elegantly solve many difficult problems in the field of natural language processing. Among the most recent ones are full and lazy compilation of the search network for speech recognition (Mohri et al., 2000a), integrated speech translation (Vidal, 1997; Bangalore and Riccardi, 2000), speech summarization (Hori et al., 2003), language modelling (Allauzen et al., 2003) and parameter estimation through EM (Eisner, 2001) to mention only a few. From this list of different applications it is clear that there is a high demand for generic tools to create and manipulate FSAs. In the past, a number of toolkits have been published, all with different design principles. Here, we give a short overview of toolkits that offer an almost complete set of algorithms: • The FSM LibraryTM from AT&amp;T (Mohri et al., 2000b) is judged the most efficient implementation, offers various semirings, ondemand computation and many algorithms, b</context>
</contexts>
<marker>Hori, Hori, Minami, 2003</marker>
<rawString>Takaaki Hori, Chiori Hori and Yasuhiro Minami, 2003, Speech Summarization using Weighted Finite-State Transducers, In Proc. of the European Conf. on Speech Communication and Technology, Geneva, Switzerland, September 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Le Maout</author>
</authors>
<date>1998</date>
<booktitle>ASTL: Automaton StandardTemplate Library,</booktitle>
<location>http://www-igm.univmlv.fr/˜lemaout/.</location>
<marker>Le Maout, 1998</marker>
<rawString>Vincent Le Maout, 1998, ASTL: Automaton StandardTemplate Library, http://www-igm.univmlv.fr/˜lemaout/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Mehlhorn</author>
</authors>
<title>Data Structures and Efficient Algorithms, Chapter 4,</title>
<date>1984</date>
<publisher>Springer Verlag, EATCS Monographs,</publisher>
<note>also available from http://www.mpisb.mpg.de/˜mehlhorn/DatAlgbooks.html.</note>
<contexts>
<context position="11409" citStr="Mehlhorn, 1984" startWordPosition="1960" endWordPosition="1961"> State { StateId id(); Weight weight(); ConstArcIterator arcsBegin(); ConstArcIterator arcsEnd(); }; virtual R&lt;Alphabet&gt; inputAlphabet(); virtual R&lt;Alphabet&gt; outputAlphabet(); virtual StateId initialState(); virtual R&lt;State&gt; getState(StateId); }; Figure 2: Pseudo-C++ code fragment for the abstract datatype of transducers. Note that R&lt;T&gt; refers to a smart pointer of T. computed by traversing it. Therefore central algorithms of the RWTH FSA toolkit are the depthfirst search (DFS) and the computation of strongly connected components (SCC). Efficient versions of these algorithms are described in (Mehlhorn, 1984) and (Cormen et al., 1990). It is very costly to store arbitrary types as arc labels within the arcs itself. Therefore the RWTH FSA toolkit offers alphabets that define mappings between strings and label indices. Alphabets are implemented using the abstract interface shown in Figure 4. With alphabets arcs only need to store the abstract label indices. The interface for alphabets is defined using a single constant: for each label index an alphabet reports it must ensure to always deliver the same symbol on request through getSymbol(). class Alphabet { public: virtual LabelId begin(); virtual La</context>
<context position="14750" citStr="Mehlhorn, 1984" startWordPosition="2441" endWordPosition="2442"> details and refer to the publication of the algorithms briefly. Most of the basic operations have a straigthforward implementation. As arc labels are integers in the implementation and their meaning is bound to an appropriate symbolic alphabet, there is the need for symbolic mapping between different alphabets. Therefore the toolkit provides the lazy map-input and map-output transducers, which map the input and output arc indices of an automaton to be compatible with the indices of another given alphabet. The implementations of all classical graph algorithms are based on the descriptions of (Mehlhorn, 1984) and (Cormen et al., 1990) and (Mohri and Riley, 2001) for SSSP. The general graph algorithms DFS and SCC are helpful in the realisation of many other operations, examples are: transpose, connect and count. However, counting the number of states of an automaton or the number of symbols of an alphabet is not well-defined in case of an infinite set of states or symbols. SSSP and transpose are the only two algorithms without a lazy implementation. The result of SSSP is a list of state potentials (see also (Mohri and Riley, 2001)). And a lazy implementation for transpose would be possible if the d</context>
</contexts>
<marker>Mehlhorn, 1984</marker>
<rawString>Kurt Mehlhorn, 1984, Data Structures and Efficient Algorithms, Chapter 4, Springer Verlag, EATCS Monographs, 1984, also available from http://www.mpisb.mpg.de/˜mehlhorn/DatAlgbooks.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<date>1997</date>
<booktitle>Finite-State Transducers in Language and Speech Processing, Computational Linguistics, 23:2,</booktitle>
<contexts>
<context position="4112" citStr="Mohri, 1997" startWordPosition="642" endWordPosition="643">is forms the basis for implementations using on-demand computations. Then the RWTH FSA toolkit implementation is detailed in Section 3. In Section 4.1 we will compare the efficiency of different toolkits. As a showcase for the flexibility we show how to use the toolkit to build a statistical machine translation system in Section 4.2. We conclude the paper with a short summary in Section 5 and discuss some possible future extensions in Section 6. 2 Finite-State Automata 2.1 Weighted Finite-State Transducer The basic theory of weighted finite-state automata has been reviewed in numerous papers (Mohri, 1997; Allauzen et al., 2003). We will introduce the notation briefly. A semiring (K, ⊕, ⊗, 0,1) is a structure with a set K and two binary operations ⊕ and ⊗ such that (K, ⊕, 0) is a commutative monoid, (K, ⊗,1) is a monoid and ⊗ distributes over ⊕ and 0 ⊗ x = x ⊗ 0 = 0 for any x ∈ K. We will also associate the term weights with the elements of a semiring. Semirings that are frequently used in speech recognition are the positive real semiring (IR ∪ {−∞, +∞}, ⊕log, +, +∞, 0) with a ⊕log b = −log(e−a + e−b) and the tropical semiring (IR∪{−∞, +∞}, min, +, +∞, 0) representing the well-known sum and ma</context>
<context position="6966" citStr="Mohri, 1997" startWordPosition="1207" endWordPosition="1208">+— (i1, i2) while not S empty (s1, s2) +— S QT = QT U (s1, s2) foreach (s1, i1, o1, w1, t1) E ET, foreach (s2, i2, o2, w2, t2) E ET, with o1 = i2 ET = ET U ((s1, s2), i1, o2, w1 (9 w2, (t1, t2)) if (t1, t2) E� QT then S +— (t1, t2) Figure 1: Simplified version of composition (assumes 2-free input transducers). What we can see from the pseudo-code above is that composition uses tuples of states of the two input transducers to describe states of the target transducer. Other operations defined on weighted finitestate automata use different abstract states. For example transducer determinization (Mohri, 1997) uses a set of pairs of states and weights. However, it is more convenient to use integers as state indices for an implementation. Therefore algorithms usually maintain a mapping from abstract states to integer state indices. This mapping has linear memory requirements of O(|QT |) which is quite attractive, but that depends on the structure of the abstract states. Especially in case of determinization where the size of an abstract state may vary, the complexity is no longer linear in general. 2.3 Local Algorithms Mohri and colleagues pointed out (Mohri et al., 2000b) that a special class of tr</context>
<context position="15677" citStr="Mohri, 1997" startWordPosition="2601" endWordPosition="2602">efined in case of an infinite set of states or symbols. SSSP and transpose are the only two algorithms without a lazy implementation. The result of SSSP is a list of state potentials (see also (Mohri and Riley, 2001)). And a lazy implementation for transpose would be possible if the data structures provide lists of both successor and predecessor arcs at each state. This needs either more memory or more computations and increases the size of the abstract interface for the lazy algorithms, so as a compromise we omitted this. The implementations of compose (Pereira and Riley, 1996), determinize (Mohri, 1997), minimize (Mohri, 1997) and remove-epsilons (Mohri, 2001) use more refined methods to gain efficiency. All use at least the lazy cache transducer as they refer to states of the input transducer(s) more than once. With respect to the number of lazy transducers involved in computing the result, compose has the most complicated implementation. Given the implementations for the algorithmic transducers cache, map-output, sort-input, sort-output and simple-compose that assumes arc labels to be compatible and sorted in order to perform matching as fast as possible, the final implementation of compos</context>
</contexts>
<marker>Mohri, 1997</marker>
<rawString>Mehryar Mohri, 1997, Finite-State Transducers in Language and Speech Processing, Computational Linguistics, 23:2, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando C N Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Weighted Finite-State Transducers in Speech Recognition,</title>
<date>2000</date>
<booktitle>In Proc. of the ISCA Tutorial and Research Workshop, Automatic Speech Recognition: Challenges for the new Millenium (ASR2000),</booktitle>
<location>Paris, France,</location>
<contexts>
<context position="1504" citStr="Mohri et al., 2000" startWordPosition="219" endWordPosition="222">vel interfaces to the programming language Python as well as a command-line tool for interactive manipulation of FSAs. Furthermore, we show how to utilize the toolkit to rapidly build a fast and accurate statistical machine translation system. Future extensibility of the toolkit is ensured as it will be publically available as open source software. 1 Introduction Finite-state automata (FSA) methods proved to elegantly solve many difficult problems in the field of natural language processing. Among the most recent ones are full and lazy compilation of the search network for speech recognition (Mohri et al., 2000a), integrated speech translation (Vidal, 1997; Bangalore and Riccardi, 2000), speech summarization (Hori et al., 2003), language modelling (Allauzen et al., 2003) and parameter estimation through EM (Eisner, 2001) to mention only a few. From this list of different applications it is clear that there is a high demand for generic tools to create and manipulate FSAs. In the past, a number of toolkits have been published, all with different design principles. Here, we give a short overview of toolkits that offer an almost complete set of algorithms: • The FSM LibraryTM from AT&amp;T (Mohri et al., 20</context>
<context position="7537" citStr="Mohri et al., 2000" startWordPosition="1303" endWordPosition="1306">xample transducer determinization (Mohri, 1997) uses a set of pairs of states and weights. However, it is more convenient to use integers as state indices for an implementation. Therefore algorithms usually maintain a mapping from abstract states to integer state indices. This mapping has linear memory requirements of O(|QT |) which is quite attractive, but that depends on the structure of the abstract states. Especially in case of determinization where the size of an abstract state may vary, the complexity is no longer linear in general. 2.3 Local Algorithms Mohri and colleagues pointed out (Mohri et al., 2000b) that a special class of transducer algorithms can be computed on demand. We will give a more detailed analysis here. We focus on algorithms that produce a single transducer and refer to them as algorithmic transducers. Definition: Let 0 be the input configuration of an algorithm A(0) that outputs a single finite-state transducer T. Additionally, let M : S → QT be a one-to-one mapping from the set of abstract state descriptions S that A generates onto the set of states of T. We call A local iff for all states s ∈ QT A can generate a state s of T and all outgoing arcs (s, i, o, w, s&apos;) ∈ ET, d</context>
<context position="19394" citStr="Mohri et al., 2000" startWordPosition="3191" endWordPosition="3194">irectly). As you can see from the examples some operations like write or draw take additional arguments that must follow the name of the operation. Although this does not follow the strict postfix design, we found it more convenient as these parameters are not automata. 4 Experimental Results 4.1 Comparison of Toolkits A crucial aspect of an FSA toolkit is its computational and memory efficiency. In this section we will compare the efficiency of four different implementations of weighted-finite state toolkits, namely: • RWTH FSA, • RWTH FSM (predecessor of RWTH FSA), • AT&amp;T FSM LibraryTM 4.0 (Mohri et al., 2000b), • WFST (Adant, 2000). We opted to not evaluate the FSA6.1 from (van Noord, 2000) as we found that it is not easy to install and it seemed to be significantly slower than any of the other implementations. RWTH FSA and the AT&amp;T FSM LibraryTM use on-demand computations whereas FSM and WFST do not. As the algorithmic code between RWTH FSA and its predecessor RWTH FSM has not changed much except for the interface of lazy transducers, we can also compare lazy versus non lazy implementation. Nevertheless, this direct comparison is also possible with RWTH FSA as it provides a static storage class </context>
</contexts>
<marker>Mohri, Pereira, Riley, 2000</marker>
<rawString>Mehryar Mohri, Fernando C.N. Pereira, and Michael Riley, 2000, Weighted Finite-State Transducers in Speech Recognition, In Proc. of the ISCA Tutorial and Research Workshop, Automatic Speech Recognition: Challenges for the new Millenium (ASR2000), Paris, France, September 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando C N Pereira</author>
<author>Michael Riley</author>
</authors>
<date>2000</date>
<booktitle>The Design Principles ofa Weighted FiniteState Transducer Library, Theoretical Computer Science,</booktitle>
<pages>231--17</pages>
<contexts>
<context position="1504" citStr="Mohri et al., 2000" startWordPosition="219" endWordPosition="222">vel interfaces to the programming language Python as well as a command-line tool for interactive manipulation of FSAs. Furthermore, we show how to utilize the toolkit to rapidly build a fast and accurate statistical machine translation system. Future extensibility of the toolkit is ensured as it will be publically available as open source software. 1 Introduction Finite-state automata (FSA) methods proved to elegantly solve many difficult problems in the field of natural language processing. Among the most recent ones are full and lazy compilation of the search network for speech recognition (Mohri et al., 2000a), integrated speech translation (Vidal, 1997; Bangalore and Riccardi, 2000), speech summarization (Hori et al., 2003), language modelling (Allauzen et al., 2003) and parameter estimation through EM (Eisner, 2001) to mention only a few. From this list of different applications it is clear that there is a high demand for generic tools to create and manipulate FSAs. In the past, a number of toolkits have been published, all with different design principles. Here, we give a short overview of toolkits that offer an almost complete set of algorithms: • The FSM LibraryTM from AT&amp;T (Mohri et al., 20</context>
<context position="7537" citStr="Mohri et al., 2000" startWordPosition="1303" endWordPosition="1306">xample transducer determinization (Mohri, 1997) uses a set of pairs of states and weights. However, it is more convenient to use integers as state indices for an implementation. Therefore algorithms usually maintain a mapping from abstract states to integer state indices. This mapping has linear memory requirements of O(|QT |) which is quite attractive, but that depends on the structure of the abstract states. Especially in case of determinization where the size of an abstract state may vary, the complexity is no longer linear in general. 2.3 Local Algorithms Mohri and colleagues pointed out (Mohri et al., 2000b) that a special class of transducer algorithms can be computed on demand. We will give a more detailed analysis here. We focus on algorithms that produce a single transducer and refer to them as algorithmic transducers. Definition: Let 0 be the input configuration of an algorithm A(0) that outputs a single finite-state transducer T. Additionally, let M : S → QT be a one-to-one mapping from the set of abstract state descriptions S that A generates onto the set of states of T. We call A local iff for all states s ∈ QT A can generate a state s of T and all outgoing arcs (s, i, o, w, s&apos;) ∈ ET, d</context>
<context position="19394" citStr="Mohri et al., 2000" startWordPosition="3191" endWordPosition="3194">irectly). As you can see from the examples some operations like write or draw take additional arguments that must follow the name of the operation. Although this does not follow the strict postfix design, we found it more convenient as these parameters are not automata. 4 Experimental Results 4.1 Comparison of Toolkits A crucial aspect of an FSA toolkit is its computational and memory efficiency. In this section we will compare the efficiency of four different implementations of weighted-finite state toolkits, namely: • RWTH FSA, • RWTH FSM (predecessor of RWTH FSA), • AT&amp;T FSM LibraryTM 4.0 (Mohri et al., 2000b), • WFST (Adant, 2000). We opted to not evaluate the FSA6.1 from (van Noord, 2000) as we found that it is not easy to install and it seemed to be significantly slower than any of the other implementations. RWTH FSA and the AT&amp;T FSM LibraryTM use on-demand computations whereas FSM and WFST do not. As the algorithmic code between RWTH FSA and its predecessor RWTH FSM has not changed much except for the interface of lazy transducers, we can also compare lazy versus non lazy implementation. Nevertheless, this direct comparison is also possible with RWTH FSA as it provides a static storage class </context>
</contexts>
<marker>Mohri, Pereira, Riley, 2000</marker>
<rawString>Mehryar Mohri, Fernando C.N. Pereira, and Michael Riley, 2000, The Design Principles ofa Weighted FiniteState Transducer Library, Theoretical Computer Science, 231:17-32, January 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Michael Riley</author>
</authors>
<title>A Weight Pushing Algorithm for Large Vocabulary Speech Recognition,</title>
<date>2000</date>
<booktitle>In Proc. of the European Conf. on Speech Communication and Technology,</booktitle>
<pages>1603--1606</pages>
<location>Aalborg, Denmark,</location>
<marker>Mohri, Riley, 2000</marker>
<rawString>Mehryar Mohri and Michael Riley, 2000, A Weight Pushing Algorithm for Large Vocabulary Speech Recognition, In Proc. of the European Conf. on Speech Communication and Technology, pp. 1603– 1606, ˚Aalborg, Denmark, September 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Generic Epsilon-Removal Algorithm for Weighted Automata,</title>
<date>2001</date>
<booktitle>In Sheng Yu and Andrei Paun, editor, 5th Int. Conf., CIAA 2000,</booktitle>
<volume>2088</volume>
<pages>230--242</pages>
<publisher>Springer-Verlag,</publisher>
<location>London Ontario,</location>
<contexts>
<context position="15735" citStr="Mohri, 2001" startWordPosition="2608" endWordPosition="2609">SP and transpose are the only two algorithms without a lazy implementation. The result of SSSP is a list of state potentials (see also (Mohri and Riley, 2001)). And a lazy implementation for transpose would be possible if the data structures provide lists of both successor and predecessor arcs at each state. This needs either more memory or more computations and increases the size of the abstract interface for the lazy algorithms, so as a compromise we omitted this. The implementations of compose (Pereira and Riley, 1996), determinize (Mohri, 1997), minimize (Mohri, 1997) and remove-epsilons (Mohri, 2001) use more refined methods to gain efficiency. All use at least the lazy cache transducer as they refer to states of the input transducer(s) more than once. With respect to the number of lazy transducers involved in computing the result, compose has the most complicated implementation. Given the implementations for the algorithmic transducers cache, map-output, sort-input, sort-output and simple-compose that assumes arc labels to be compatible and sorted in order to perform matching as fast as possible, the final implementation of compose in the RWTH FSA toolkit is given in figure 3. So, the cu</context>
</contexts>
<marker>Mohri, 2001</marker>
<rawString>Mehryar Mohri, 2001, Generic Epsilon-Removal Algorithm for Weighted Automata, In Sheng Yu and Andrei Paun, editor, 5th Int. Conf., CIAA 2000, London Ontario, Canada. volume 2088 of Lecture Notes in Computer Science, pages 230-242. Springer-Verlag, Berlin-NY, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Michael Riley</author>
</authors>
<title>An Efficient Algorithm for the N-Best-Strings Problem,</title>
<date>2002</date>
<booktitle>In Proc. of the Int. Conf. on Spoken Language Processing,</booktitle>
<pages>1313--1316</pages>
<location>Denver, Colorado,</location>
<contexts>
<context position="16580" citStr="Mohri and Riley, 2002" startWordPosition="2741" endWordPosition="2744">g the result, compose has the most complicated implementation. Given the implementations for the algorithmic transducers cache, map-output, sort-input, sort-output and simple-compose that assumes arc labels to be compatible and sorted in order to perform matching as fast as possible, the final implementation of compose in the RWTH FSA toolkit is given in figure 3. So, the current implementation of compose uses 6 algorithmic transducers in addition to the two input automata. Determinize additionally uses lazy cache and sort-input transducers. The search algorithms best and n-best are based on (Mohri and Riley, 2002), push is based on (Mohri and Riley, 2001) and failure mainly uses ideas from (Allauzen et al., 2003). The algorithms posterior and prune compute arc posterior probabilities and prune arcs with respect to them. We believe they are standard algorithms defined on probabilistic networks and they were simply ported to the framework of weighted finite-state automata. Finally, the RWTH FSA toolkit can be loosely interfaced to the AT&amp;T FSM LibraryTM through its ASCII-based input/output format. In addition, a new XML-based file format primarly designed as being human readable and a fast binary file fo</context>
</contexts>
<marker>Mohri, Riley, 2002</marker>
<rawString>Mehryar Mohri and Michael Riley, 2002, An Efficient Algorithm for the N-Best-Strings Problem, In Proc. of the Int. Conf. on Spoken Language Processing, pp. 1313–1316, Denver, Colorado, September 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Statistical Alignment Models,</title>
<date>2000</date>
<booktitle>In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<location>Hongkong, China,</location>
<contexts>
<context position="24480" citStr="Och and Ney, 2000" startWordPosition="4105" endWordPosition="4108">ests to solve the translation problem by estimating a language model on a bilanguage (see also (Bangalore and Riccardi, 2000; Casacuberta et al., 2001)). An example of sentences from this bilanguage is given in Figure 5 for the translation task Vermobil (German → English). For technical reasons, 2-labels are represented by a $ symbol. Note, that due to the fixed segmentation given by the alignments, phrases in the target language are moved to the last source word of an alignment block. So, given an appropriate alignment which can be obtained by means of the pubically available GIZA++ toolkit (Och and Ney, 2000), the approach is very easy in practice: 1. Transform the training corpus with a given alignment into the corresponding bilingual corpus 2. Train a language model on the bilingual corpus 3. Build an acceptor A from the language model The symbols of the resulting acceptor are still a mixture of words from the source language and phrases from the target language. So, we additionally use two simple transducers to split these bilingual words (C1 maps source words fj to bilingual words that start with fj and C2 maps bilingual words with the target sequence epj to the sequences of target words the p</context>
<context position="26292" citStr="Och and Ney, 2000" startWordPosition="4405" endWordPosition="4408">ests were performed on a 1.2GHz AMD Athlon). Task System Translation WER PER 100-BLEU Memory Time/Sentence [%] [%] [MB] [ms] Eutrans FSA Spanish → English 8.12 7.64 10.7 6-8 20 AT 8.25 - - - - FUB FSA Italian → English 27.0 21.5 37.7 3-5 22 AT 23.7 18.1 36.0 - - Verbmobil FSA German → English 48.3 41.6 69.8 65-90 460 AT 40.5 30.1 62.2 - - PF-Star FSA Italian → English 39.8 34.1 58.4 12-15 35 AT 36.8 29.1 54.3 - - e&apos; = project-output(best(f ◦ T)) Translation results using this approach are summarized in Table 4 and are being compared with results obtained using the alignment template approach (Och and Ney, 2000). Results for both approaches were obtaining using the same training corpus alignments. Detailed task descriptions for Eutrans/FUB and Verbmobil can be found in (Casacuberta et al., 2001) and (Zens et al., 2002) respectively. We use the usual definitions for word error rate (WER), position independent word error rate (PER) and BLEU statistics here. For the simpler tasks Eutrans, FUB and PF-Star, the WER, PER and the inverted BLEU statistics are close for both approaches. On the German-toEnglish Verbmobil task the FSA approach suffers from long distance reorderings (captured through the fixed t</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz J. Och and Hermann Ney, 2000, Improved Statistical Alignment Models, In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics, pp. 440-447, Hongkong, China, October 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Speech Recognition by Composition of Weighted Finite Automata,</title>
<date>1996</date>
<booktitle>Available from http://xxx.lanl.gov/cmplg/9603001, Computation and Language,</booktitle>
<contexts>
<context position="5748" citStr="Pereira and Riley, 1996" startWordPosition="973" endWordPosition="976">and a set of final states F weighted by the function p : F → K. To simplify the notation we will also denote with QT and ET the set of states and arcs of a transducer T. A weighted finite-state acceptor is simply a weighted finite-state transducer without the output alphabet. 2.2 Composition As we will refer to this example throughout the paper we shortly review the composition algorithm here. Let T1 : E*×Q* → K and T2 : Q*×I&apos;* → K be two transducers defined over the same semiring K. Their composition T1 ◦ T2 realizes the function T : E*×I&apos;* → K and the theory has been described in detail in (Pereira and Riley, 1996). For simplification purposes, let us assume that the input automata are 2-free and S = (Q1 × Q2, ←, → , empty) is a stack of state tuples of T1 and T2 with push, pop and empty test operations. A non lazy version of composition is shown in Figure 1. Composition of automata containing 2 labels is more complex and can be solved by using an intermediate filter transducer that also has been described in (Pereira and Riley, 1996). 1we do not restrict this to be a finite set as most algorithms of the lazy implementation presented in this paper also support a virtually infinite set T=T1oT2: i = (i1, </context>
<context position="15650" citStr="Pereira and Riley, 1996" startWordPosition="2596" endWordPosition="2599">of symbols of an alphabet is not well-defined in case of an infinite set of states or symbols. SSSP and transpose are the only two algorithms without a lazy implementation. The result of SSSP is a list of state potentials (see also (Mohri and Riley, 2001)). And a lazy implementation for transpose would be possible if the data structures provide lists of both successor and predecessor arcs at each state. This needs either more memory or more computations and increases the size of the abstract interface for the lazy algorithms, so as a compromise we omitted this. The implementations of compose (Pereira and Riley, 1996), determinize (Mohri, 1997), minimize (Mohri, 1997) and remove-epsilons (Mohri, 2001) use more refined methods to gain efficiency. All use at least the lazy cache transducer as they refer to states of the input transducer(s) more than once. With respect to the number of lazy transducers involved in computing the result, compose has the most complicated implementation. Given the implementations for the algorithmic transducers cache, map-output, sort-input, sort-output and simple-compose that assumes arc labels to be compatible and sorted in order to perform matching as fast as possible, the fin</context>
</contexts>
<marker>Pereira, Riley, 1996</marker>
<rawString>Fernando C.N. Pereira and Michael Riley, 1996, Speech Recognition by Composition of Weighted Finite Automata, Available from http://xxx.lanl.gov/cmplg/9603001, Computation and Language, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<date>2000</date>
<note>FSA6 Reference Manual, http://odur.let.rug.nl/˜vannoord/Fsa/.</note>
<marker>van Noord, 2000</marker>
<rawString>Gertjan van Noord, 2000, FSA6 Reference Manual, http://odur.let.rug.nl/˜vannoord/Fsa/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Vidal</author>
</authors>
<title>Finite-State Speech-to-Speech Translation,</title>
<date>1997</date>
<booktitle>In Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing,</booktitle>
<pages>111--114</pages>
<location>Munich, Germany,</location>
<contexts>
<context position="1550" citStr="Vidal, 1997" startWordPosition="226" endWordPosition="227">well as a command-line tool for interactive manipulation of FSAs. Furthermore, we show how to utilize the toolkit to rapidly build a fast and accurate statistical machine translation system. Future extensibility of the toolkit is ensured as it will be publically available as open source software. 1 Introduction Finite-state automata (FSA) methods proved to elegantly solve many difficult problems in the field of natural language processing. Among the most recent ones are full and lazy compilation of the search network for speech recognition (Mohri et al., 2000a), integrated speech translation (Vidal, 1997; Bangalore and Riccardi, 2000), speech summarization (Hori et al., 2003), language modelling (Allauzen et al., 2003) and parameter estimation through EM (Eisner, 2001) to mention only a few. From this list of different applications it is clear that there is a high demand for generic tools to create and manipulate FSAs. In the past, a number of toolkits have been published, all with different design principles. Here, we give a short overview of toolkits that offer an almost complete set of algorithms: • The FSM LibraryTM from AT&amp;T (Mohri et al., 2000b) is judged the most efficient implementati</context>
<context position="23080" citStr="Vidal, 1997" startWordPosition="3860" endWordPosition="3861">e it for all subsequent calls to the getState() method. Table 2: Comparison of peak memory usage in MB (* aborted due to exceeded memory limits). Exp. FSA FSM AT&amp;T WFST 1 360 1700 1500 &gt; 1850* 2 59 310 69 &gt; 1850* 3 48 230 176 550 Table 3: Comparison of CPU time in seconds including I/O using a 1.2GHz AMD Athlon processor (* exceeded memory limits: given time indicates point of abortion). Exp. FSA FSM AT&amp;T WFST 1 105 203 515 &gt; 40* 2 6.5 182 11760 &gt; 64* 3 6.6 21 28 3840 4.2 Statistical Machine Translation Statistical machine translation may be viewed as a weighted language transduction problem (Vidal, 1997). Therefore it is fairly easy to build a machine translation system with the use of weighted finitestate transducers. Let fJ1 and eIi be two sentences from a source and target language respectively. Also assume that we have word level alignments A of all sentences from a bilingual training corpus. We denote with epJ p1 the segmentation of a target sentence eI1 into phrases such that fJ1 and epJ p1 can be aligned monotoneously. This segmentation can be directly calculated from the alignments A. Then we can formulate the problem of finding the best translation ˆeI1 of a source sentence as follow</context>
</contexts>
<marker>Vidal, 1997</marker>
<rawString>Enrique Vidal, 1997, Finite-State Speech-to-Speech Translation, In Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing, pp. 111–114, Munich, Germany, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Franz J Och</author>
<author>H Ney</author>
</authors>
<title>PhraseBased Statistical Machine Translation, In:</title>
<date>2002</date>
<booktitle>Advances in artificial intelligence. 25. Annual German Conference on AI, KI 2002, Vol. LNAI 2479,</booktitle>
<pages>18--32</pages>
<publisher>Springer Verlag,</publisher>
<contexts>
<context position="26503" citStr="Zens et al., 2002" startWordPosition="4438" endWordPosition="4441">English 27.0 21.5 37.7 3-5 22 AT 23.7 18.1 36.0 - - Verbmobil FSA German → English 48.3 41.6 69.8 65-90 460 AT 40.5 30.1 62.2 - - PF-Star FSA Italian → English 39.8 34.1 58.4 12-15 35 AT 36.8 29.1 54.3 - - e&apos; = project-output(best(f ◦ T)) Translation results using this approach are summarized in Table 4 and are being compared with results obtained using the alignment template approach (Och and Ney, 2000). Results for both approaches were obtaining using the same training corpus alignments. Detailed task descriptions for Eutrans/FUB and Verbmobil can be found in (Casacuberta et al., 2001) and (Zens et al., 2002) respectively. We use the usual definitions for word error rate (WER), position independent word error rate (PER) and BLEU statistics here. For the simpler tasks Eutrans, FUB and PF-Star, the WER, PER and the inverted BLEU statistics are close for both approaches. On the German-toEnglish Verbmobil task the FSA approach suffers from long distance reorderings (captured through the fixed training corpus segmentation), which is not very surprising. Although we do not have comparable numbers of the memory usage and the translation times for the alignment template approach, resource usage of the fin</context>
</contexts>
<marker>Zens, Och, Ney, 2002</marker>
<rawString>Richard Zens, Franz J. Och and H. Ney, 2002, PhraseBased Statistical Machine Translation, In: M. Jarke, J. Koehler, G. Lakemeyer (Eds.) : KI - 2002: Advances in artificial intelligence. 25. Annual German Conference on AI, KI 2002, Vol. LNAI 2479, pp. 18-32, Springer Verlag, September 2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>