<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000159">
<title confidence="0.99945">
A Comparison and Semi-Quantitative Analysis of Words and
Character-Bigrams as Features in Chinese Text Categorization
</title>
<author confidence="0.997458">
Jingyang Li Maosong Sun Xian Zhang
</author>
<affiliation confidence="0.9159475">
National Lab. of Intelligent Technology &amp; Systems, Department of Computer Sci. &amp; Tech.
Tsinghua University, Beijing 100084, China
</affiliation>
<email confidence="0.998611">
lijingyang@gmail.com sms@tsinghua.edu.cn kevinn9@gmail.com
</email>
<sectionHeader confidence="0.998596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99977">
Words and character-bigrams are both
used as features in Chinese text process-
ing tasks, but no systematic comparison
or analysis of their values as features for
Chinese text categorization has been re-
ported heretofore. We carry out here a
full performance comparison between
them by experiments on various docu-
ment collections (including a manually
word-segmented corpus as a golden stan-
dard), and a semi-quantitative analysis to
elucidate the characteristics of their be-
havior; and try to provide some prelimi-
nary clue for feature term choice (in most
cases, character-bigrams are better than
words) and dimensionality setting in text
categorization systems.
</bodyText>
<sectionHeader confidence="0.982402" genericHeader="keywords">
1 Introduction1
</sectionHeader>
<bodyText confidence="0.992551407407407">
Because of the popularity of the Vector Space
Model (VSM) in text information processing,
document indexing (term extraction) acts as a
pre-requisite step in most text information proc-
essing tasks such as Information Retrieval
(Baeza-Yates and Ribeiro-Neto, 1999) and Text
Categorization (Sebastiani, 2002). It is empiri-
cally known that the indexing scheme is a non-
trivial complication to system performance, es-
pecially for some Asian languages in which there
are no explicit word margins and even no natural
semantic unit. Concretely, in Chinese Text Cate-
gorization tasks, the two most important index-
1 This research is supported by the National Natural Science
Foundation of China under grant number 60573187 and
60321002, and the Tsinghua-ALVIS Project co-sponsored
by the National Natural Science Foundation of China under
grant number 60520130299 and EU FP6.
ing units (feature terms) are word and character-
bigram, so the problem is: which kind of terms2
should be chosen as the feature terms, words or
character-bigrams?
To obtain an all-sided idea about feature
choice beforehand, we review here the possible
feature variants (or, options). First, at the word
level, we can do stemming, do stop-word prun-
ing, include POS (Part of Speech) information,
etc. Second, term combinations (such as “word-
bigram”, “word + word-bigram”, “character-
bigram + character-trigram”3, etc.) can also be
used as features (Nie et al., 2000). But, for Chi-
nese Text Categorization, the “word or bigram”
question is fundamental. They have quite differ-
ent characteristics (e.g. bigrams overlap each
other in text, but words do not) and influence the
classification performance in different ways.
In Information Retrieval, it is reported that bi-
gram indexing schemes outperforms word
schemes to some or little extent (Luk and Kwok,
1997; Leong and Zhou 1998; Nie et al., 2000).
Few similar comparative studies have been re-
ported for Text Categorization (Li et al., 2003) so
far in literature.
Text categorization and Information Retrieval
are tasks that sometimes share identical aspects
(Sebastiani, 2002) apart from term extraction
(document indexing), such as tfidf term weight-
ing and performance evaluation. Nevertheless,
they are different tasks. One of the generally ac-
cepted connections between Information Re-
trieval and Text Categorization is that an infor-
mation retrieval task could be partially taken as a
binary classification problem with the query as
the only positive training document. From this
</bodyText>
<footnote confidence="0.9978566">
2 The terminology “term” stands for both word and charac-
ter-bigram. Term or combination of terms (in word-bigram
or other forms) might be chosen as “feature”.
3 The terminology “character” stands for Chinese character,
and “bigram” stands for character-bigram in this paper.
</footnote>
<page confidence="0.953876">
545
</page>
<note confidence="0.5490645">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 545–552,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999929">
viewpoint, an IR task and a general TC task have
a large difference in granularity. To better illus-
trate this difference, an example is present here.
The words “制片人(film producer)” and “译制
片(dubbed film)” should be taken as different
terms in an IR task because a document with one
would not necessarily be a good match for a
query with the other, so the bigram “制片(film
production)” is semantically not a shared part of
these two words, i.e. not an appropriate feature
term. But in a Text Categorization task, both
words might have a similar meaning at the cate-
gory level (“film” category, generally), which
enables us to regard the bigram “制片” as a se-
mantically acceptable representative word snip-
pet for them, or for the category.
There are also differences in some other as-
pects of IR and TC. So it is significant to make a
detailed comparison and analysis here on the
relative value of words and bigrams as features
in Text Categorization. The organization of this
paper is as follows: Section 2 shows some ex-
periments on different document collections to
observe the common trends in the performance
curves of the word-scheme and bigram-scheme;
Section 3 qualitatively analyses these trends;
Section 4 makes some statistical analysis to cor-
roborate the issues addressed in Section 3; Sec-
tion 5 summarizes the results and concludes.
</bodyText>
<sectionHeader confidence="0.993118" genericHeader="introduction">
2 Performance Comparison
</sectionHeader>
<bodyText confidence="0.998577611111111">
Three document collections in Chinese language
are used in this study.
The electronic version of Chinese Encyclo-
pedia (“CE”): It has 55 subject categories and
71674 single-labeled documents (entries). It is
randomly split by a proportion of 9:1 into a train-
ing set with 64533 documents and a test set with
7141 documents. Every document has the full-
text. This data collection does not have much of
a sparseness problem.
The training data from a national Chinese
text categorization evaluation4 (“CTC”): It has
36 subject categories and 3600 single-labeled5
documents. It is randomly split by a proportion
of 4:1 into a training set with 2800 documents
and a test set with 720 documents. Documents in
this data collection are from various sources in-
cluding news websites, and some documents
</bodyText>
<footnote confidence="0.9846904">
4 The Annual Evaluation of Chinese Text Categorization
2004, by 863 National Natural Science Foundation.
5 In the original document collection, a document might
have a secondary category label. In this study, only the pri-
mary category label is reserved.
</footnote>
<bodyText confidence="0.998939571428571">
may be very short. This data collection has a
moderate sparseness problem.
A manually word-segmented corpus from
the State Language Affairs Commission
(“LC”): It has more than 100 categories and
more than 20000 single-labeled documents6. In
this study, we choose a subset of 12 categories
with the most documents (totally 2022 docu-
ments). It is randomly split by a proportion of 2:1
into a training set and a test set. Every document
has the full-text and has been entirely word-
segmented7 by hand (which could be regarded as
a golden standard of segmentation).
All experiments in this study are carried out at
various feature space dimensionalities to show
the scalability. Classifiers used in this study are
Rocchio and SVM. All experiments here are
multi-class tasks and each document is assigned
a single category label.
The outline of this section is as follows: Sub-
section 2.1 shows experiments based on the Roc-
chio classifier, feature selection schemes besides
Chi and term weighting schemes besides tfidf to
compare the automatic segmented word features
with bigram features on CE and CTC, and both
document collections lead to similar behaviors;
Subsection 2.2 shows experiments on CE by a
SVM classifier, in which, unlike with the Roc-
chio method, Chi feature selection scheme and
tfidf term weighting scheme outperform other
schemes; Subsection 2.3 shows experiments by a
SVM classifier with Chi feature selection and
tfidf term weighting on LC (manual word seg-
mentation) to compare the best word features
with bigram features.
</bodyText>
<subsectionHeader confidence="0.9096765">
2.1 The Rocchio Method and Various Set-
tings
</subsectionHeader>
<bodyText confidence="0.992206538461538">
The Rocchio method is rooted in the IR tradition,
and is very different from machine learning ones
(such as SVM) (Joachims, 1997; Sebastiani,
2002). Therefore, we choose it here as one of the
representative classifiers to be examined. In the
experiment, the control parameter of negative
examples is set to 0, so this Rocchio based classi-
fier is in fact a centroid-based classifier.
Chimax is a state-of-the-art feature selection
criterion for dimensionality reduction (Yang and
Peterson, 1997; Rogati and Yang, 2002). Chi-
max*CIG (Xue and Sun, 2003a) is reported to be
better in Chinese text categorization by a cen-
</bodyText>
<footnote confidence="0.916351333333333">
6 Not completed.
7 And POS (part-of-speech) tagged as well. But POS tags
are not used in this study.
</footnote>
<page confidence="0.996436">
546
</page>
<figureCaption confidence="0.999878">
Figure 2. chi-tfidf and chicig-tfidfcig on CTC
</figureCaption>
<bodyText confidence="0.891669666666666">
Figure 2 shows the same group of curves for
the CTC document collection. The curves fluctu-
ate more than the curves for the CE collection
because of sparseness; The CE collection is more
sensitive to the additions of terms that come with
the increase of dimensionality. The CE curves in
the following figures show similar fluctuations
for the same reason.
For a parallel comparison among mmword,
lqword and bigram schemes, the curves in Fig-
ure 1 and Figure 2 are regrouped and shown in
Figure 3 and Figure 4.
</bodyText>
<figureCaption confidence="0.854009">
dimensionality x 104
Figure 1. chi-tfidf and chicig-tfidfcig on CE
</figureCaption>
<bodyText confidence="0.888256111111111">
Figure 1 shows the performance-
dimensionality curves of the chi-tfidf approach
and the approach with CIG, by mmword, lqword
and bigram document indexing, on the CE
document collection. We can see that the original
chi-tfidf approach is better at low dimensional-
ities (less than 10000 dimensions), while the CIG
version is better at high dimensionalities and
reaches a higher limit.10
</bodyText>
<footnote confidence="0.9872392">
8 http://www.nlp.org.cn/project/project.php?proj_id=6
9 Microaveraging is more prefered in most cases than
macroaveraging (Sebastiani 2002).
10 In all figures in this paper, curves might be truncated due
to the large scale of dimensionality, especially the curves of
</footnote>
<bodyText confidence="0.980961708333334">
troid based classifier, so we choose it as another
representative feature selection criterion besides
Chimax.
Likewise, as for term weighting schemes, in
addition to tfidf, the state of the art (Baeza-Yates
and Ribeiro-Neto, 1999), we also choose
tfidf*CIG (Xue and Sun, 2003b).
Two word segmentation schemes are used for
the word-indexing of documents. One is the
maximum match algorithm (“mmword” in the
figures), which is a representative of simple and
fast word segmentation algorithms. The other is
ICTCLAS8 (“lqword” in the figures). ICTCLAS
is one of the best word segmentation systems
(SIGHAN 2003) and reaches a segmentation
precision of more than 97%, so we choose it as a
representative of state-of-the-art schemes for
automatic word-indexing of document).
For evaluation of single-label classifications,
F1-measure, precision, recall and accuracy
(Baeza-Yates and Ribeiro-Neto, 1999; Sebastiani,
2002) have the same value by microaveraging9,
and are labeled with “performance” in the fol-
lowing figures.
</bodyText>
<figure confidence="0.999346701492537">
mmword
chi-tfidf
chicig-tfidfcig
1 2 3 4 5 6 7 8
lqword x 104
1 2 3 4 5 6 7 8
chi-tfidf
chicig-tfidfcig
x 104
chi-tfidf
chicig-tfidfcig
1 2 3 4 5 6 7 8
performance
0.8
0.7
0.6
0.5
performance
0.8
0.7
0.6
0.5
performance
0.8
0.7
0.6
0.5
bigram
mmword
1 2 3 4 5 6 7 8
chi-tfidf
chicig-tfidfcig
x 104
chi-tfidf
chicig-tfidfcig
1 2 3 4 5 6 7 8
chi-tfidf
chicig-tfidfcig
1 2 3 4 5 6 7 8
dimensionality x 104
0.8
0.7
0.6
0.5
0.8
0.7
0.6
0.5
0.8
0.7
0.6
0.5
lqword
performance
performance
performance
x 104
bigram
0.85
0.8
0.75
performance
0.7
0.65
0.6
0.55
0.5
</figure>
<figureCaption confidence="0.829756">
Figure 3. mmword, lqword and bigram on CE
</figureCaption>
<figure confidence="0.999371959183673">
chi-tfidf
1 2 3 4 5
dimensionality x 104
chicig-tfidfcig
1 2 3 4 5
dimensionality x 104
chicig-tfidfcig
chi-tfidf
0.85
0.85
0.8
0.8
0.75
0.75
performance
0.7
0.7
0.65
0.65
0.6
0.6
mmword
lqword
bigram
mmword
lqword
bigram
0.55
0.55
0.5
0.5
2 4 6 8
dimensionality x 104
2 4 6 8
dimensionality x 104
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
mmword
lqword
bigram
mmword
lqword
bigram
</figure>
<figureCaption confidence="0.834958833333333">
Figure 4. mmword, lqword and bigram on CTC
bigram scheme. For these kinds of figures, at least one of
the following is satisfied: (a) every curve has shown its
zenith; (b) only one curve is not complete and has shown a
higher zenith than other curves; (c) a margin line is shown
to indicate the limit of the incomplete curve.
</figureCaption>
<page confidence="0.96139">
547
</page>
<bodyText confidence="0.999877866666667">
We can see that the lqword scheme outper-
forms the mmword scheme at almost any dimen-
sionality, which means the more precise the word
segmentation the better the classification per-
formance. At the same time, the bigram scheme
outperforms both of the word schemes on a high
dimensionality, wherea the word schemes might
outperform the bigram scheme on a low dimen-
sionality.
Till now, the experiments on CE and CTC
show the same characteristics despite the per-
formance fluctuation on CTC caused by sparse-
ness. Hence in the next subsections CE is used
instead of both of them because its curves are
smoother.
</bodyText>
<subsectionHeader confidence="0.994606">
2.2 SVM on Words and Bigrams
</subsectionHeader>
<bodyText confidence="0.999683034482759">
As stated in the previous subsection, the lqword
scheme always outperforms the mmword scheme;
we compare here only the lqword scheme with
the bigram scheme.
Support Vector Machine (SVM) is one of the
best classifiers at present (Vapnik, 1995;
Joachims, 1998), so we choose it as the main
classifier in this study. The SVM implementation
used here is LIBSVM (Chang, 2001); the type of
SVM is set to “C-SVC” and the kernel type is set
to linear, which means a one-with-one scheme is
used in the multi-class classification.
Because the CIG’s effectiveness on a SVM
classifier is not examined in Xue and Sun (2003a,
2003b)’s report, we make here the four combina-
tions of schemes with and without CIG in feature
selection and term weighting. The experiment
results are shown in Figure 5. The collection
used is CE.
weighting scheme are related to the classifier,
which is worth noting. In other words, no feature
selection scheme or term weighting scheme is
absolutely the best for all classifiers. Therefore, a
reasonable choice is to select the best performing
combination of feature selection scheme, term
weighting scheme and classifier, i.e. chi-tfidf and
SVM. The curves for the lqword scheme and the
bigram scheme are redrawn in Figure 6 to make
them clearer.
</bodyText>
<figureCaption confidence="0.91943">
Figure 6. lqword and bigram on CE
</figureCaption>
<bodyText confidence="0.999612666666667">
The curves shown in Figure 6 are similar to
those in Figure 3. The differences are: (a) a lar-
ger dimensionality is needed for the bigram
scheme to start outperforming the lqword scheme;
(b) the two schemes have a smaller performance
gap.
The lqword scheme reaches its top perform-
ance at a dimensionality of around 40000, and
the bigram scheme reaches its top performance
at a dimensionality of around 60000 to 70000,
after which both schemes’ performances slowly
decrease. The reason is that the low ranked terms
in feature selection are in fact noise and do not
help to classification, which is why the feature
selection phase is necessary.
</bodyText>
<figure confidence="0.999553175">
1 2 3 4 5 6 7
dimensionality x 104
performance
0.85
0.75
0.9
0.8
lqword
bigram
0.9
0.85
0.8
performance
0.75
0.7
0.65
0.6
lqword
chi-tfidf
chi-tfidfcig
chicig-tfidf
chicig-tfidfcig
1 2 3 4 5 6 7
dimensionality x 104
bigram
chi-tfidf
chi-tfidfcig
chicig-tfidf
chicig-tfidfcig
1 2 3 4 5 6 7
dimensionality x 104
0.85
0.75
0.65
0.9
0.8
0.7
0.6
2.3 Comparing Manually Segmented
Words and Bigrams
</figure>
<figureCaption confidence="0.9858785">
Figure 5. chi-tfidf and cig-involved approaches
on lqword and bigram
</figureCaption>
<bodyText confidence="0.999628833333333">
Here we find that the chi-tfidf combination
outperforms any approach with CIG, which is the
opposite of the results with the Rocchio method.
And the results with SVM are all better than the
results with the Rocchio method. So we find that
the feature selection scheme and the term
</bodyText>
<figure confidence="0.525322">
0 1 2 3 4 5 6 7 8 9 10
dimansionality x 104
</figure>
<figureCaption confidence="0.999453">
Figure 7. word and bigram on LC
</figureCaption>
<figure confidence="0.996237846153846">
performance
88
86
84
82
80
78
76
74
72
word
bigram
bigram limit
</figure>
<page confidence="0.99086">
548
</page>
<bodyText confidence="0.984474875">
Up to now, bigram features seem to be better
than word ones for fairly large dimensionalities.
But it appears that word segmentation precision
impacts classification performance. So we
choose here a fully manually segmented docu-
ment collection to detect the best performance a
word scheme could reach and compare it with
the bigram scheme.
Figure 7 shows such an experiment result on
the LC document collection (the circles indicate
the maximums and the dash-dot lines indicate the
superior limit and the asymptotic interior limit of
the bigram scheme). The word scheme reaches a
top performance around the dimensionality of
20000, which is a little higher than the bigram
scheme’s zenith around 70000.
Besides this experiment on 12 categories of
the LC document collection, some experiments
on fewer (2 to 6) categories of this subset were
also done, and showed similar behaviors. The
word scheme shows a better performance than
the bigram scheme and needs a much lower di-
mensionality. The simpler the classification task
is, the more distinct this behavior is.
</bodyText>
<sectionHeader confidence="0.990134" genericHeader="method">
3 Qualitative Analysis
</sectionHeader>
<bodyText confidence="0.99986125">
To analyze the performance of words and bi-
grams as feature terms in Chinese text categori-
zation, we need to investigate two aspects as fol-
lows.
</bodyText>
<subsectionHeader confidence="0.998401">
3.1 An Individual Feature Perspective
</subsectionHeader>
<bodyText confidence="0.999981851851852">
The word is a natural semantic unit in Chinese
language and expresses a complete meaning in
text. The bigram is not a natural semantic unit
and might not express a complete meaning in
text, but there are also reasons for the bigram to
be a good feature term.
First, two-character words and three-character
words account for most of all multi-character
Chinese words (Liu and Liang, 1986). A two-
character word can be substituted by the same
bigram. At the granularity of most categorization
tasks, a three-character words can often be sub-
stituted by one of its sub-bigrams (namely the
“intraword bigram” in the next section) without
a change of meaning. For instance, “标赛” is a
sub-bigram of the word “锦标赛(tournament)”
and could represent it without ambiguity.
Second, a bigram may overlap on two succes-
sive words (namely the “interword bigram” in
the next section), and thus to some extent fills the
role of a word-bigram. The word-bigram as a
more definite (although more sparse) feature
surely helps the classification. For instance, “气
预” is a bigram overlapping on the two succes-
sive words “ 天 气 (weather)” and “ 预 报
(forecast)”, and could almost replace the word-
bigram (also a phrase) “天气预报(weather fore-
cast)”, which is more likely to be a representative
feature of the category “气象学(meteorology)”
than either word.
Third, due to the first issue, bigram features
have some capability of identifying OOV (out-
of-vocabulary) words11, and help improve the
recall of classification.
The above issues state the advantages of bi-
grams compared with words. But in the first and
second issue, the equivalence between bigram
and word or word-bigram is not perfect. For in-
stance, the word “文学(literature)” is a also sub-
bigram of the word “天文学(astronomy)”, but
their meanings are completely different. So the
loss and distortion of semantic information is a
disadvantage of bigram features over word fea-
tures.
Furthermore, one-character words cover about
7% of words and more than 30% of word occur-
rences in the Chinese language; they are effev-
tive in the word scheme and are not involved in
the above issues. Note that the impact of effec-
tive one-character words on the classification is
not as large as their total frequency, because the
high frequency ones are often too common to
have a good classification power, for instance,
the word “的 (of, ‘s)”.
</bodyText>
<subsectionHeader confidence="0.992167">
3.2 A Mass Feature Perspective
</subsectionHeader>
<bodyText confidence="0.999933058823529">
Features are not independently acting in text
classification. They are assembled together to
constitute a feature space. Except for a few mod-
els such as Latent Semantic Indexing (LSI)
(Deerwester et al., 1990), most models assume
the feature space to be orthogonal. This assump-
tion might not affect the effectiveness of the
models, but the semantic redundancy and com-
plementation among the feature terms do impact
on the classification efficiency at a given dimen-
sionality.
According to the first issue addressed in the
previous subsection, a bigram might cover for
more than one word. For instance, the bigram
“织物” is a sub-bigram of the words “织物
(fabric)”, “ 棉 织 物 (cotton fabric)”, “针 织 物
(knitted fabric)”, and also a good substitute of
</bodyText>
<footnote confidence="0.9002535">
11 The “OOV words” in this paper stand for the words that
occur in the test documents but not in the training document.
</footnote>
<page confidence="0.997628">
549
</page>
<bodyText confidence="0.998691923076923">
them. So, to a certain extent, word features are
redundant with regard to the bigram features as-
sociated to them. Similarly, according to the sec-
ond issue addressed, a bigram might cover for
more than one word-bigram. For instance, the
bigram “篇小” is a sub-bigram of the word-
bigrams (phrases) “短篇小说(short story)”, “中
篇小说(novelette)”, “长篇小说(novel)” and also
a good substitute for them. So, as an addition to
the second issue stated in the previous subsection,
a bigram feature might even cover for more than
one word-bigram.
On the other hand, bigrams features are also
redundant with regard to word features associ-
ated with them. For instance, the “锦标” and “标
赛” are both sub-bigrams of the previously men-
tioned word “锦标赛”. In some cases, more than
one sub-bigram can be a good representative of a
word.
We make a word list and a bigram list sorted
by the feature selection criterion in a descending
order. We now try to find how the relative re-
dundancy degrees of the word list and the bigram
list vary with the dimensionality. Following is-
sues are elicited by an observation on the two
lists (not shown here due to space limitations).
The relative redundancy rate in the word list
keeps even while the dimensionality varies to a
certain extent, because words that share a com-
mon sub-bigram might not have similar statistics
and thus be scattered in the word feature list.
Note that these words are possibly ranked lower
in the list than the sub-bigram because feature
selection criteria (such as Chi) often prefer
higher frequency terms to lower frequency ones,
and every word containing the bigram certainly
has a lower frequency than the bigram itself.
The relative redundancy in the bigram list
might be not as even as in the word list. Good
(representative) sub-bigrams of a word are quite
likely to be ranked close to the word itself. For
instance, “作曲” and “曲家” are sub-bigrams of
the word “作曲家(music composer)”, both the
bigrams and the word are on the top of the lists.
Theretofore, the bigram list has a relatively large
redundancy rate at low dimensionalities. The
redundancy rate should decrease along with the
increas of dimensionality for: (a) the relative re-
dundancy in the word list counteracts the redun-
dancy in the bigram list, because the words that
contain a same bigram are gradually included as
the dimensionality increases; (b) the proportion
of interword bigrams increases in the bigram list
and there is generally no redundancy between
interword bigrams and intraword bigrams.
Last, there are more bigram features than word
features because bigrams can overlap each other
in the text but words can not. Thus the bigrams
as a whole should theoretically contain more in-
formation than the words as a whole.
From the above analysis and observations, bi-
gram features are expected to outperform word
features at high dimensionalities. And word fea-
tures are expected to outperform bigram features
at low dimensionalities.
</bodyText>
<sectionHeader confidence="0.991945" genericHeader="method">
4 Semi-Quantitative Analysis
</sectionHeader>
<bodyText confidence="0.999850666666667">
In this section, a preliminary statistical analysis
is presented to corroborate the statements in the
above qualitative analysis and expected to be
identical with the experiment results shown in
Section 1. All statistics in this section are based
on the CE document collection and the lqword
segmentation scheme (because the CE document
collection is large enough to provide good statis-
tical characteristics).
</bodyText>
<subsectionHeader confidence="0.953524">
4.1 Intraword Bigrams and Interword Bi-
grams
</subsectionHeader>
<bodyText confidence="0.999711666666667">
In the previous section, only the intraword bi-
grams were discussed together with the words.
But every bigram may have both intraword oc-
currences and interword occurrences. Therefore
we need to distinguish these two kinds of bi-
grams at a statistical level. For every bigram, the
number of intraword occurrences and the number
of interword occurrences are counted and we can
use
</bodyText>
<equation confidence="0.952231333333333">
⎛loginterword# + 1 ⎜ ⎝
intraword# +⎠⎟
1
</equation>
<bodyText confidence="0.99559425">
as a metric to indicate its natual propensity to be
a intraword bigram. The probability density of
bigrams about on this metric is shown in Figure
8.
</bodyText>
<figure confidence="0.983850375">
0.25
0.2
0.15
0.1
0.05
0
-12 -10 -8 -6 -4 -2 0 2 4 6 8 10
log(intraword#/interword#)
</figure>
<figureCaption confidence="0.960316">
Figure 8. Bigram Probability Density on
log(intraword#/interword#)
</figureCaption>
<figure confidence="0.447609">
probability density
</figure>
<page confidence="0.946483">
550
</page>
<bodyText confidence="0.9998426">
The figure shows a mixture of two Gaussian
distributions, the left one for “natural interword
bigrams” and the right one for “natural intraword
bigrams”. We can moderately distinguish these
two kinds of bigrams by a division at -1.4.
</bodyText>
<subsectionHeader confidence="0.38153">
4.2 Overall Information Quantity of a Fea-
ture Space
</subsectionHeader>
<bodyText confidence="0.999653944444444">
The performance limit of a classification is re-
lated to the quantity of information used. So a
quantitative metric of the information a feature
space can provide is need. Feature Quantity (Ai-
zawa, 2000) is suitable for this purpose because
it comes from information theory and is additive;
tfidf was also reported as an appropriate metric of
feature quantity (defined as “probability • infor-
mation”). Because of the probability involved as
a factor, the overall information provided by a
feature space can be calculated on training data
by summation.
The redundancy and complementation men-
tioned in Subsection 3.2 must be taken into ac-
count in the calculation of overall information
quantity. For bigrams, the redundancy with re-
gard to words associated with them between two
intraword bigrams is given by
</bodyText>
<equation confidence="0.674599">
{idf b idf b
( 1), ( 2 ))
</equation>
<bodyText confidence="0.9999773">
in which b1 and b2 stand for the two bigrams and
w stands for any word containing both of them.
The overall information quantity is obtained by
subtracting the redundancy between each pair of
bigrams from the sum of all features’ feature
quantity (tfidf). Redundancy among more than
two bigrams is ignored. For words, there is only
complementation among words but not redun-
dancy, the complementation with regard to bi-
grams associated with them is given by
</bodyText>
<equation confidence="0.608428">
{idf (b)) , if b exists;
tf (w) • idf (w), if does not exists.
b
</equation>
<bodyText confidence="0.999847">
in which b is an intraword bigram contained by
w. The overall information is calculated by
summing the complementations of all words.
</bodyText>
<subsectionHeader confidence="0.9973">
4.3 Statistics and Discussion
</subsectionHeader>
<bodyText confidence="0.980735">
Figure 9 shows the variation of these overall in-
formation metrics on the CE document collection.
It corroborates the characteristics analyzed in
Section 3 and corresponds with the performance
curves in Section 2.
Figure 10 shows the proportion of interword
bigrams at different dimensionalities, which also
corresponds with the analysis in Section 3.
</bodyText>
<figure confidence="0.994983571428572">
x 107
16
14
12
overall information quantity 10
8
6
4
2
word
bigram
0
0 2 4 6 8 10 12 14 16
dimensionality x 104
</figure>
<figureCaption confidence="0.999951">
Figure 9. Overall Information Quantity on CE
</figureCaption>
<bodyText confidence="0.999976076923077">
The curves do not cross at exactly the same
dimensionality as in the figures in Section 1, be-
cause other complications impact on the classifi-
cation performance: (a) OOV word identifying
capability, as stated in Subsection 3.1; (b) word
segmentation precision; (c) granularity of the
categories (words have more definite semantic
meaning than bigrams and lead to a better per-
formance for small category granularities); (d)
noise terms, introduced in the feature space dur-
ing the increase of dimensionality. With these
factors, the actual curves would not keep increas-
ing as they do in Figure 9.
</bodyText>
<figure confidence="0.965676">
0 2 4 6 8 10 12 14 16
dimensionality x 104
</figure>
<figureCaption confidence="0.996605">
Figure 10. Interword Bigram Proportion on CE
</figureCaption>
<sectionHeader confidence="0.999185" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999838090909091">
In this paper, we aimed to thoroughly compare
the value of words and bigrams as feature terms
in text categorization, and make the implicit
mechanism explicit.
Experimental comparison showed that the Chi
feature selection scheme and the tfidf term
weighting scheme are still the best choices for
(Chinese) text categorization on a SVM classifier.
In most cases, the bigram scheme outperforms
the word scheme at high dimensionalities and
usually reaches its top performance at a dimen-
</bodyText>
<figure confidence="0.993876142857143">
interword bigram proportion
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
E
tf (w ) min
•
b w
1,2 c
t
w •
tf ( ) min
bcw
</figure>
<page confidence="0.993253">
551
</page>
<bodyText confidence="0.999979516129032">
sionality of around 70000. The word scheme of-
ten outperforms the bigram scheme at low di-
mensionalities and reaches its top performance at
a dimensionality of less than 40000.
Whether the best performance of the word
scheme is higher than the best performance
scheme depends considerably on the word seg-
mentation precision and the number of categories.
The word scheme performs better with a higher
word segmentation precision and fewer (&lt;10)
categories.
A word scheme costs more document indexing
time than a bigram scheme does; however a bi-
gram scheme costs more training time and classi-
fication time than a word scheme does at the
same performance level due to its higher dimen-
sionality. Considering that the document index-
ing is needed in both the training phase and the
classification phase, a high precision word
scheme is more time consuming as a whole than
a bigram scheme.
As a concluding suggestion: a word scheme is
more fit for small-scale tasks (with no more than
10 categories and no strict classification speed
requirements) and needs a high precision word
segmentation system; a bigram scheme is more
fit for large-scale tasks (with dozens of catego-
ries or even more) without too strict training
speed requirements (because a high dimensional-
ity and a large number of categories lead to a
long training time).
</bodyText>
<sectionHeader confidence="0.996587" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.999841972972973">
Akiko Aizawa. 2000. The Feature Quantity: An In-
formation Theoretic Perspective of Tfidf-like
Measures, Proceedings of ACM SIGIR 2000, 104-
111.
Ricardo Baeza-Yates, Berthier Ribeiro-Neto. 1999.
Modern Information Retrieval, Addison-Wesley
Chih-Chung Chang, Chih-Jen Lin. 2001. LIBSVM: A
Library for Support Vector Machines, Software
available at http://www.csie.ntu.edu.tw/~cjlin/
libsvm
Steve Deerwester, Sue T. Dumais, George W. Furnas,
Richard Harshman. 1990. Indexing by Latent Se-
mantic Analysis, Journal of the American Society
for Information Science, 41:391-407.
Thorsten Joachims. 1997. A Probabilistic Analysis of
the Rocchio Algorithm with TFIDF for Text Cate-
gorization, Proceedings of 14th International Con-
ference on Machine Learning (Nashville, TN,
1997), 143-151.
Thorsten Joachims. 1998. Text Categorization with
Support Vector Machine: Learning with Many
Relevant Features, Proceedings of the 10th Euro-
pean Conference on Machine Learning, 137-142.
Mun-Kew Leong, Hong Zhou. 1998. Preliminary
Qualitative Analysis of Segmented vs. Bigram In-
dexing in Chinese, The 6th Text Retrieval Confer-
ence (TREC-6), NIST Special Publication 500-240,
551-557.
Baoli Li, Yuzhong Chen, Xiaojing Bai, Shiwen Yu.
2003. Experimental Study on Representing Units in
Chinese Text Categorization, Proceedings of the
4th International Conference on Computational
Linguistics and Intelligent Text Processing (CI-
CLing 2003), 602-614.
Yuan Liu, Nanyuan Liang. 1986. Basic Engineering
for Chinese Processing – Contemporary Chinese
Words Frequency Count, Journal of Chinese In-
formation Processing, 1(1):17-25.
Robert W.P. Luk, K.L. Kwok. 1997. Comparing rep-
resentations in Chinese information retrieval. Pro-
ceedings of ACM SIGIR 1997, 34-41.
Jianyun Nie, Fuji Ren. 1999. Chinese Information
Retrieval: Using Characters or Words? Informa-
tion Processing and Management, 35:443-462.
Jianyun Nie, Jianfeng Gao, Jian Zhang, Ming Zhou.
2000. On the Use of Words and N-grams for Chi-
nese Information Retrieval, Proceedings of 5th In-
ternational Workshop on Information Retrieval
with Asian Languages
Monica Rogati, Yiming Yang. 2002. High-performing
Feature Selection for Text Classification, Proceed-
ings of ACM Conference on Information and
Knowledge Management 2002, 659-661.
Gerard Salton, Christopher Buckley. 1988. Term
Weighting Approaches in Automatic Text Retrieval,
Information Processing and Management,
24(5):513-523.
Fabrizio Sebastiani. 2002. Machine Learning in
Automated Text Categorization, ACM Computing
Surveys, 34(1):1-47
Dejun Xue, Maosong Sun. 2003a. Select Strong In-
formation Features to Improve Text Categorization
Effectiveness, Journal of Intelligent Systems, Spe-
cial Issue.
Dejun Xue, Maosong Sun. 2003b. A Study on Feature
Weighting in Chinese Text Categorization, Pro-
ceedings of the 4th International Conference on
Computational Linguistics and Intelligent Text
Processing (CICLing 2003), 594-604.
Vladimir Vapnik. 1995. The Nature of Statistical
Learning Theory, Springer.
Yiming Yang, Jan O. Pederson. 1997. A Comparative
Study on Feature Selection in Text Categorization,
Proceedings of ICML 1997, 412-420.
</reference>
<page confidence="0.99764">
552
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9993795">A Comparison and Semi-Quantitative Analysis of Words and Character-Bigrams as Features in Chinese Text Categorization</title>
<author confidence="0.998435">Jingyang Li Maosong Sun Xian Zhang</author>
<affiliation confidence="0.992532">National Lab. of Intelligent Technology &amp; Systems, Department of Computer Sci. &amp; Tech.</affiliation>
<address confidence="0.680182">Tsinghua University, Beijing 100084, China</address>
<email confidence="0.992755">lijingyang@gmail.comsms@tsinghua.edu.cnkevinn9@gmail.com</email>
<abstract confidence="0.973731961117864">Words and character-bigrams are both used as features in Chinese text processing tasks, but no systematic comparison or analysis of their values as features for Chinese text categorization has been reported heretofore. We carry out here a full performance comparison between them by experiments on various document collections (including a manually word-segmented corpus as a golden standard), and a semi-quantitative analysis to elucidate the characteristics of their behavior; and try to provide some preliminary clue for feature term choice (in most cases, character-bigrams are better than words) and dimensionality setting in text categorization systems. Because of the popularity of the Vector Space Model (VSM) in text information processing, document indexing (term extraction) acts as a pre-requisite step in most text information processing tasks such as Information Retrieval (Baeza-Yates and Ribeiro-Neto, 1999) and Text Categorization (Sebastiani, 2002). It is empirically known that the indexing scheme is a nontrivial complication to system performance, especially for some Asian languages in which there are no explicit word margins and even no natural semantic unit. Concretely, in Chinese Text Catetasks, the two most important indexresearch is supported by the National Natural Science Foundation of China under grant number 60573187 and 60321002, and the Tsinghua-ALVIS Project co-sponsored by the National Natural Science Foundation of China under grant number 60520130299 and EU FP6. ing units (feature terms) are word and characterso the problem is: which kind of should be chosen as the feature terms, words or character-bigrams? To obtain an all-sided idea about feature choice beforehand, we review here the possible feature variants (or, options). First, at the word level, we can do stemming, do stop-word pruning, include POS (Part of Speech) information, etc. Second, term combinations (such as “wordbigram”, “word + word-bigram”, “character- + etc.) can also be used as features (Nie et al., 2000). But, for Chinese Text Categorization, the “word or bigram” question is fundamental. They have quite different characteristics (e.g. bigrams overlap each other in text, but words do not) and influence the classification performance in different ways. In Information Retrieval, it is reported that bigram indexing schemes outperforms word schemes to some or little extent (Luk and Kwok, 1997; Leong and Zhou 1998; Nie et al., 2000). Few similar comparative studies have been reported for Text Categorization (Li et al., 2003) so far in literature. Text categorization and Information Retrieval are tasks that sometimes share identical aspects (Sebastiani, 2002) apart from term extraction indexing), such as weighting and performance evaluation. Nevertheless, they are different tasks. One of the generally accepted connections between Information Retrieval and Text Categorization is that an information retrieval task could be partially taken as a binary classification problem with the query as the only positive training document. From this terminology “term” stands for both word and character-bigram. Term or combination of terms (in word-bigram or other forms) might be chosen as “feature”. terminology “character” stands for Chinese character, and “bigram” stands for character-bigram in this paper. 545 of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the pages 545–552, July 2006. Association for Computational Linguistics viewpoint, an IR task and a general TC task have a large difference in granularity. To better illustrate this difference, an example is present here. words producer)” and film)” should be taken as different terms in an IR task because a document with one would not necessarily be a good match for a with the other, so the bigram production)” is semantically not a shared part of these two words, i.e. not an appropriate feature term. But in a Text Categorization task, both words might have a similar meaning at the category level (“film” category, generally), which us to regard the bigram as a semantically acceptable representative word snippet for them, or for the category. There are also differences in some other aspects of IR and TC. So it is significant to make a detailed comparison and analysis here on the relative value of words and bigrams as features in Text Categorization. The organization of this paper is as follows: Section 2 shows some experiments on different document collections to observe the common trends in the performance curves of the word-scheme and bigram-scheme; Section 3 qualitatively analyses these trends; Section 4 makes some statistical analysis to corroborate the issues addressed in Section 3; Section 5 summarizes the results and concludes. 2 Performance Comparison Three document collections in Chinese language are used in this study. electronic version of Encyclohas 55 subject categories and 71674 single-labeled documents (entries). It is randomly split by a proportion of 9:1 into a training set with 64533 documents and a test set with 7141 documents. Every document has the fulltext. This data collection does not have much of a sparseness problem. The training data from a national Chinese categorization (“CTC”): has subject categories and 3600 documents. It is randomly split by a proportion of 4:1 into a training set with 2800 documents and a test set with 720 documents. Documents in this data collection are from various sources including news websites, and some documents Annual Evaluation of Chinese Text Categorization 2004, by 863 National Natural Science Foundation. the original document collection, a document might have a secondary category label. In this study, only the primary category label is reserved. may be very short. This data collection has a moderate sparseness problem. A manually word-segmented corpus from the State Language Affairs Commission has more than 100 categories and than 20000 single-labeled In this study, we choose a subset of 12 categories with the most documents (totally 2022 documents). It is randomly split by a proportion of 2:1 into a training set and a test set. Every document has the full-text and has been entirely wordby hand (which could be regarded as a golden standard of segmentation). All experiments in this study are carried out at various feature space dimensionalities to show the scalability. Classifiers used in this study are Rocchio and SVM. All experiments here are multi-class tasks and each document is assigned a single category label. The outline of this section is as follows: Subsection 2.1 shows experiments based on the Rocchio classifier, feature selection schemes besides term weighting schemes besides compare the automatic segmented word features with bigram features on CE and CTC, and both document collections lead to similar behaviors; Subsection 2.2 shows experiments on CE by a SVM classifier, in which, unlike with the Rocmethod, selection scheme and weighting scheme outperform other schemes; Subsection 2.3 shows experiments by a classifier with selection and weighting on LC (manual word segmentation) to compare the best word features with bigram features. 2.1 The Rocchio Method and Various Settings The Rocchio method is rooted in the IR tradition, and is very different from machine learning ones (such as SVM) (Joachims, 1997; Sebastiani, 2002). Therefore, we choose it here as one of the representative classifiers to be examined. In the experiment, the control parameter of negative examples is set to 0, so this Rocchio based classifier is in fact a centroid-based classifier. a state-of-the-art feature selection criterion for dimensionality reduction (Yang and 1997; Rogati and Yang, 2002). Chiand Sun, 2003a) is reported to be in Chinese text categorization by a cencompleted. POS (part-of-speech) tagged as well. But POS tags are not used in this study. 546 2. CTC Figure 2 shows the same group of curves for the CTC document collection. The curves fluctuate more than the curves for the CE collection because of sparseness; The CE collection is more sensitive to the additions of terms that come with the increase of dimensionality. The CE curves in the following figures show similar fluctuations for the same reason. a parallel comparison among the curves in Figure 1 and Figure 2 are regrouped and shown in Figure 3 and Figure 4. 1. CE Figure 1 shows the performancecurves of the the approach with by indexing, on the CE document collection. We can see that the original is better at low dimensional- (less than 10000 dimensions), while the version is better at high dimensionalities and a higher is more prefered in most cases than macroaveraging (Sebastiani 2002). all figures in this paper, curves might be truncated due to the large scale of dimensionality, especially the curves of troid based classifier, so we choose it as another representative feature selection criterion besides Likewise, as for term weighting schemes, in to the state of the art (Baeza-Yates and Ribeiro-Neto, 1999), we also choose and Sun, 2003b). Two word segmentation schemes are used for the word-indexing of documents. One is the match (“mmword” in the figures), which is a representative of simple and fast word segmentation algorithms. The other is (“lqword” in the figures). ICTCLAS is one of the best word segmentation systems (SIGHAN 2003) and reaches a segmentation precision of more than 97%, so we choose it as a representative of state-of-the-art schemes for automatic word-indexing of document). For evaluation of single-label classifications, (Baeza-Yates and Ribeiro-Neto, 1999; Sebastiani, have the same value by and are labeled with “performance” in the following figures. mmword chi-tfidf chicig-tfidfcig 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 chi-tfidf chicig-tfidfcig chi-tfidf chicig-tfidfcig 1 2 3 4 5 6 7 8 performance 0.8 0.7 0.6 0.5 performance 0.8 0.7 0.6 0.5 performance 0.8 0.7 0.6 0.5 bigram mmword 1 2 3 4 5 6 7 8 chi-tfidf chicig-tfidfcig chi-tfidf chicig-tfidfcig 1 2 3 4 5 6 7 8 chi-tfidf chicig-tfidfcig 1 2 3 4 5 6 7 8 0.8 0.7 0.6 0.5 0.8 0.7 0.6 0.5 0.8 0.7 0.6 0.5 lqword performance performance performance bigram 0.85 0.8 0.75 performance 0.7 0.65 0.6 0.55 0.5 3. CE chi-tfidf 1 2 3 4 5 chicig-tfidfcig 1 2 3 4 5 chicig-tfidfcig chi-tfidf 0.85 0.85 0.8 0.8 0.75 0.75 performance 0.7 0.7 0.65 0.65 0.6 0.6 mmword lqword bigram mmword lqword bigram 0.55 0.55 0.5 0.5 2 4 6 8 2 4 6 8 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 mmword lqword mmword lqword bigram 4. CTC bigram scheme. For these kinds of figures, at least one of the following is satisfied: (a) every curve has shown its zenith; (b) only one curve is not complete and has shown a higher zenith than other curves; (c) a margin line is shown to indicate the limit of the incomplete curve. 547 can see that the outperthe at almost any dimensionality, which means the more precise the word segmentation the better the classification per- At the same time, the outperforms both of the word schemes on a high dimensionality, wherea the word schemes might the on a low dimensionality. Till now, the experiments on CE and CTC show the same characteristics despite the performance fluctuation on CTC caused by sparseness. Hence in the next subsections CE is used instead of both of them because its curves are smoother. 2.2 SVM on Words and Bigrams stated in the previous subsection, the always outperforms the compare here only the with Support Vector Machine (SVM) is one of the best classifiers at present (Vapnik, 1995; Joachims, 1998), so we choose it as the main classifier in this study. The SVM implementation used here is LIBSVM (Chang, 2001); the type of SVM is set to “C-SVC” and the kernel type is set to linear, which means a one-with-one scheme is used in the multi-class classification. the effectiveness on a SVM classifier is not examined in Xue and Sun (2003a, 2003b)’s report, we make here the four combinaof schemes with and without feature selection and term weighting. The experiment results are shown in Figure 5. The collection used is CE. weighting scheme are related to the classifier, which is worth noting. In other words, no feature selection scheme or term weighting scheme is absolutely the best for all classifiers. Therefore, a reasonable choice is to select the best performing combination of feature selection scheme, term scheme and classifier, i.e. The curves for the and the are redrawn in Figure 6 to make them clearer. 6. CE The curves shown in Figure 6 are similar to those in Figure 3. The differences are: (a) a lardimensionality is needed for the to start outperforming the (b) the two schemes have a smaller performance gap. reaches its top performance at a dimensionality of around 40000, and reaches its top performance at a dimensionality of around 60000 to 70000, after which both schemes’ performances slowly decrease. The reason is that the low ranked terms in feature selection are in fact noise and do not help to classification, which is why the feature selection phase is necessary. 1 2 3 4 5 6 7 x performance 0.85 0.75 0.9 0.8 lqword bigram 0.9 0.85 0.8 performance 0.75 0.7 0.65 0.6 lqword chi-tfidf chi-tfidfcig chicig-tfidf chicig-tfidfcig 1 2 3 4 5 6 7 bigram chi-tfidf chi-tfidfcig chicig-tfidf chicig-tfidfcig 1 2 3 4 5 6 7 0.85 0.75 0.65 0.9 0.8 0.7 0.6 2.3 Comparing Manually Segmented Words and Bigrams 5. approaches we find that the any approach with which is the opposite of the results with the Rocchio method. And the results with SVM are all better than the results with the Rocchio method. So we find that the feature selection scheme and the term 0 1 2 3 4 5 6 7 8 9 10 x 7. LC performance 88 86 84 82 80 78 76 74 72 word bigram bigram limit 548 Up to now, bigram features seem to be better than word ones for fairly large dimensionalities. But it appears that word segmentation precision impacts classification performance. So we choose here a fully manually segmented document collection to detect the best performance a word scheme could reach and compare it with the bigram scheme. Figure 7 shows such an experiment result on the LC document collection (the circles indicate the maximums and the dash-dot lines indicate the superior limit and the asymptotic interior limit of the bigram scheme). The word scheme reaches a top performance around the dimensionality of 20000, which is a little higher than the bigram scheme’s zenith around 70000. Besides this experiment on 12 categories of the LC document collection, some experiments on fewer (2 to 6) categories of this subset were also done, and showed similar behaviors. The word scheme shows a better performance than the bigram scheme and needs a much lower dimensionality. The simpler the classification task is, the more distinct this behavior is. 3 Qualitative Analysis To analyze the performance of words and bigrams as feature terms in Chinese text categorization, we need to investigate two aspects as follows. 3.1 An Individual Feature Perspective The word is a natural semantic unit in Chinese language and expresses a complete meaning in text. The bigram is not a natural semantic unit and might not express a complete meaning in text, but there are also reasons for the bigram to be a good feature term. First, two-character words and three-character words account for most of all multi-character Chinese words (Liu and Liang, 1986). A twocharacter word can be substituted by the same bigram. At the granularity of most categorization tasks, a three-character words can often be substituted by one of its sub-bigrams (namely the “intraword bigram” in the next section) without change of meaning. For instance, is a of the word and could represent it without ambiguity. Second, a bigram may overlap on two successive words (namely the “interword bigram” in the next section), and thus to some extent fills the role of a word-bigram. The word-bigram as a more definite (although more sparse) feature helps the classification. For instance, is a bigram overlapping on the two succeswords “ 气 and “ 报 (forecast)”, and could almost replace the word- (also a phrase) forecast)”, which is more likely to be a representative of the category than either word. Third, due to the first issue, bigram features have some capability of identifying OOV (outand help improve the classification. The above issues state the advantages of bigrams compared with words. But in the first and second issue, the equivalence between bigram and word or word-bigram is not perfect. For inthe word is a also subof the word but their meanings are completely different. So the loss and distortion of semantic information is a disadvantage of bigram features over word features. Furthermore, one-character words cover about 7% of words and more than 30% of word occurrences in the Chinese language; they are effevtive in the word scheme and are not involved in the above issues. Note that the impact of effective one-character words on the classification is not as large as their total frequency, because the high frequency ones are often too common to have a good classification power, for instance, word ‘s)”. 3.2 A Mass Feature Perspective Features are not independently acting in text classification. They are assembled together to constitute a feature space. Except for a few models such as Latent Semantic Indexing (LSI) (Deerwester et al., 1990), most models assume the feature space to be orthogonal. This assumption might not affect the effectiveness of the models, but the semantic redundancy and complementation among the feature terms do impact on the classification efficiency at a given dimensionality. According to the first issue addressed in the previous subsection, a bigram might cover for more than one word. For instance, the bigram is a sub-bigram of the words 棉 织 物 fabric)”, 织 物 (knitted fabric)”, and also a good substitute of “OOV words” in this paper stand for the words that occur in the test documents but not in the training document. 549 them. So, to a certain extent, word features are with regard to the bigram features associated to them. Similarly, according to the second issue addressed, a bigram might cover for more than one word-bigram. For instance, the is a sub-bigram of the word- (phrases) story)”, and also a good substitute for them. So, as an addition to the second issue stated in the previous subsection, a bigram feature might even cover for more than one word-bigram. On the other hand, bigrams features are also with regard to word features associwith them. For instance, the and are both sub-bigrams of the previously menword In some cases, more than one sub-bigram can be a good representative of a word. We make a word list and a bigram list sorted by the feature selection criterion in a descending order. We now try to find how the relative redundancy degrees of the word list and the bigram list vary with the dimensionality. Following issues are elicited by an observation on the two lists (not shown here due to space limitations). The relative redundancy rate in the word list keeps even while the dimensionality varies to a extent, because words that share a common sub-bigram might not have similar statistics and thus be scattered in the word feature list. Note that these words are possibly ranked lower in the list than the sub-bigram because feature criteria (such as often prefer higher frequency terms to lower frequency ones, and every word containing the bigram certainly has a lower frequency than the bigram itself. The relative redundancy in the bigram list might be not as even as in the word list. Good (representative) sub-bigrams of a word are quite likely to be ranked close to the word itself. For and are sub-bigrams of word composer)”, both the bigrams and the word are on the top of the lists. Theretofore, the bigram list has a relatively large redundancy rate at low dimensionalities. The redundancy rate should decrease along with the of dimensionality for: (a) the relative rein the word list counteracts the redundancy in the bigram list, because the words that contain a same bigram are gradually included as the dimensionality increases; (b) the proportion of interword bigrams increases in the bigram list and there is generally no redundancy between interword bigrams and intraword bigrams. Last, there are more bigram features than word features because bigrams can overlap each other in the text but words can not. Thus the bigrams as a whole should theoretically contain more information than the words as a whole. From the above analysis and observations, bigram features are expected to outperform word features at high dimensionalities. And word features are expected to outperform bigram features at low dimensionalities. 4 Semi-Quantitative Analysis In this section, a preliminary statistical analysis is presented to corroborate the statements in the above qualitative analysis and expected to be identical with the experiment results shown in Section 1. All statistics in this section are based the CE document collection and the segmentation scheme (because the CE document collection is large enough to provide good statistical characteristics). 4.1 Intraword Bigrams and Interword Bigrams In the previous section, only the intraword bigrams were discussed together with the words. But every bigram may have both intraword occurrences and interword occurrences. Therefore we need to distinguish these two kinds of bigrams at a statistical level. For every bigram, the number of intraword occurrences and the number of interword occurrences are counted and we can use ⎝ 1 as a metric to indicate its natual propensity to be a intraword bigram. The probability density of bigrams about on this metric is shown in Figure 8. 0.25 0.2 0.15 0.1 0.05 0 -12 -10 -8 -6 -4 -2 0 2 4 6 8 10 log(intraword#/interword#) Figure 8. Bigram Probability Density on probability density 550 The figure shows a mixture of two Gaussian distributions, the left one for “natural interword bigrams” and the right one for “natural intraword bigrams”. We can moderately distinguish these two kinds of bigrams by a division at -1.4. 4.2 Overall Information Quantity of a Feature Space The performance limit of a classification is related to the quantity of information used. So a quantitative metric of the information a feature can provide is need. Quantity (Aizawa, 2000) is suitable for this purpose because it comes from information theory and is additive; also reported as an appropriate metric of quantity (defined as infor- Because of the probability involved as a factor, the overall information provided by a feature space can be calculated on training data by summation. The redundancy and complementation mentioned in Subsection 3.2 must be taken into account in the calculation of overall information quantity. For bigrams, the redundancy with regard to words associated with them between two intraword bigrams is given by b idf b ( 2 which for the two bigrams and for any word containing both of them. The overall information quantity is obtained by subtracting the redundancy between each pair of from the sum of all features’ Redundancy among more than two bigrams is ignored. For words, there is only complementation among words but not redundancy, the complementation with regard to bigrams associated with them is given by does not exists. b which an intraword bigram contained by The overall information is calculated by summing the complementations of all words. 4.3 Statistics and Discussion Figure 9 shows the variation of these overall information metrics on the CE document collection. It corroborates the characteristics analyzed in Section 3 and corresponds with the performance curves in Section 2. Figure 10 shows the proportion of interword bigrams at different dimensionalities, which also corresponds with the analysis in Section 3. 16 14 12 overall information quantity 10 8 6 4 2 word bigram 0 0 2 4 6 8 10 12 14 16 x Figure 9. Overall Information Quantity on CE The curves do not cross at exactly the same dimensionality as in the figures in Section 1, because other complications impact on the classification performance: (a) OOV word identifying capability, as stated in Subsection 3.1; (b) word segmentation precision; (c) granularity of the categories (words have more definite semantic meaning than bigrams and lead to a better performance for small category granularities); (d) noise terms, introduced in the feature space during the increase of dimensionality. With these factors, the actual curves would not keep increasing as they do in Figure 9. 0 2 4 6 8 10 12 14 16 x Figure 10. Interword Bigram Proportion on CE 5 Conclusion In this paper, we aimed to thoroughly compare the value of words and bigrams as feature terms in text categorization, and make the implicit mechanism explicit. comparison showed that the selection scheme and the weighting scheme are still the best choices for (Chinese) text categorization on a SVM classifier. In most cases, the bigram scheme outperforms the word scheme at high dimensionalities and reaches its top performance at a dimeninterword bigram proportion 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 1 E min • b w t ) min 551 sionality of around 70000. The word scheme often outperforms the bigram scheme at low dimensionalities and reaches its top performance at a dimensionality of less than 40000. Whether the best performance of the word scheme is higher than the best performance scheme depends considerably on the word segmentation precision and the number of categories. The word scheme performs better with a higher word segmentation precision and fewer (&lt;10) categories. A word scheme costs more document indexing time than a bigram scheme does; however a bigram scheme costs more training time and classification time than a word scheme does at the same performance level due to its higher dimensionality. Considering that the document indexing is needed in both the training phase and the classification phase, a high precision word scheme is more time consuming as a whole than a bigram scheme. As a concluding suggestion: a word scheme is more fit for small-scale tasks (with no more than 10 categories and no strict classification speed requirements) and needs a high precision word segmentation system; a bigram scheme is more fit for large-scale tasks (with dozens of categories or even more) without too strict training speed requirements (because a high dimensionality and a large number of categories lead to a long training time).</abstract>
<note confidence="0.775116">Reference Aizawa. 2000. Feature Quantity: An Information Theoretic Perspective of Tfidf-like Proceedings of ACM SIGIR 2000, 111. Ricardo Baeza-Yates, Berthier Ribeiro-Neto. 1999. Information Retrieval, Chang, Chih-Jen Lin. 2001. A for Support Vector Software available at http://www.csie.ntu.edu.tw/~cjlin/</note>
<email confidence="0.423883">libsvm</email>
<author confidence="0.975498">Steve Deerwester</author>
<author confidence="0.975498">Sue T Dumais</author>
<author confidence="0.975498">George W Furnas</author>
<note confidence="0.843749219512195">Harshman. 1990. by Latent Semantic Analysis, Journal of the American Society Information 41:391-407. Joachims. 1997. Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Cate- Proceedings of International Conon Machine Learning TN, Joachims. 1998. Categorization with Support Vector Machine: Learning with Many Features, Proceedings of the Euro- Conference on Machine 137-142. Leong, Hong Zhou. 1998. Qualitative Analysis of Segmented vs. Bigram Inin Chinese, The Text Retrieval Confer- (TREC-6), NIST Special Publication Baoli Li, Yuzhong Chen, Xiaojing Bai, Shiwen Yu. Study on Representing Units in Text of the Conference on Computational Linguistics and Intelligent Text Processing (CI- 602-614. Liu, Nanyuan Liang. 1986. Engineering Chinese Processing Chinese Words Frequency Count, Journal of Chinese In- 1(1):17-25. W.P. Luk, K.L. Kwok. 1997. representations in Chinese information retrieval. Proof ACM SIGIR Nie, Fuji Ren. 1999. Information Retrieval: Using Characters or Words? Informa- Processing and 35:443-462. Jianyun Nie, Jianfeng Gao, Jian Zhang, Ming Zhou. the Use of Words and N-grams for Chi- Information Retrieval, Proceedings of International Workshop on Information Retrieval with Asian Languages Rogati, Yiming Yang. 2002. Feature Selection for Text Classification, Proceedings of ACM Conference on Information and Management 659-661. Salton, Christopher Buckley. 1988.</note>
<title confidence="0.588675">Weighting Approaches in Automatic Text Retrieval, Processing and</title>
<note confidence="0.884037368421053">24(5):513-523. Sebastiani. 2002. Learning in Automated Text Categorization, ACM Computing 34(1):1-47 Xue, Maosong Sun. 2003a. Strong Information Features to Improve Text Categorization Journal of Intelligent Systems, Special Issue. Xue, Maosong Sun. 2003b. Study on Feature Weighting in Chinese Text Categorization, Proof the International Conference on Computational Linguistics and Intelligent Text (CICLing 594-604. Vapnik. 1995. Nature of Statistical Theory, Yang, Jan O. Pederson. 1997. Comparative Study on Feature Selection in Text Categorization, of ICML 412-420. 552</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Akiko Aizawa</author>
</authors>
<title>The Feature Quantity: An Information Theoretic Perspective of Tfidf-like Measures,</title>
<date>2000</date>
<booktitle>Proceedings of ACM SIGIR</booktitle>
<pages>104--111</pages>
<contexts>
<context position="24918" citStr="Aizawa, 2000" startWordPosition="4094" endWordPosition="4096">6 8 10 log(intraword#/interword#) Figure 8. Bigram Probability Density on log(intraword#/interword#) probability density 550 The figure shows a mixture of two Gaussian distributions, the left one for “natural interword bigrams” and the right one for “natural intraword bigrams”. We can moderately distinguish these two kinds of bigrams by a division at -1.4. 4.2 Overall Information Quantity of a Feature Space The performance limit of a classification is related to the quantity of information used. So a quantitative metric of the information a feature space can provide is need. Feature Quantity (Aizawa, 2000) is suitable for this purpose because it comes from information theory and is additive; tfidf was also reported as an appropriate metric of feature quantity (defined as “probability • information”). Because of the probability involved as a factor, the overall information provided by a feature space can be calculated on training data by summation. The redundancy and complementation mentioned in Subsection 3.2 must be taken into account in the calculation of overall information quantity. For bigrams, the redundancy with regard to words associated with them between two intraword bigrams is given </context>
</contexts>
<marker>Aizawa, 2000</marker>
<rawString>Akiko Aizawa. 2000. The Feature Quantity: An Information Theoretic Perspective of Tfidf-like Measures, Proceedings of ACM SIGIR 2000, 104-111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ricardo Baeza-Yates</author>
<author>Berthier Ribeiro-Neto</author>
</authors>
<title>Modern Information Retrieval,</title>
<date>1999</date>
<publisher>Addison-Wesley</publisher>
<contexts>
<context position="1290" citStr="Baeza-Yates and Ribeiro-Neto, 1999" startWordPosition="179" endWordPosition="182">s document collections (including a manually word-segmented corpus as a golden standard), and a semi-quantitative analysis to elucidate the characteristics of their behavior; and try to provide some preliminary clue for feature term choice (in most cases, character-bigrams are better than words) and dimensionality setting in text categorization systems. 1 Introduction1 Because of the popularity of the Vector Space Model (VSM) in text information processing, document indexing (term extraction) acts as a pre-requisite step in most text information processing tasks such as Information Retrieval (Baeza-Yates and Ribeiro-Neto, 1999) and Text Categorization (Sebastiani, 2002). It is empirically known that the indexing scheme is a nontrivial complication to system performance, especially for some Asian languages in which there are no explicit word margins and even no natural semantic unit. Concretely, in Chinese Text Categorization tasks, the two most important index1 This research is supported by the National Natural Science Foundation of China under grant number 60573187 and 60321002, and the Tsinghua-ALVIS Project co-sponsored by the National Natural Science Foundation of China under grant number 60520130299 and EU FP6.</context>
<context position="10206" citStr="Baeza-Yates and Ribeiro-Neto, 1999" startWordPosition="1602" endWordPosition="1605">low dimensionalities (less than 10000 dimensions), while the CIG version is better at high dimensionalities and reaches a higher limit.10 8 http://www.nlp.org.cn/project/project.php?proj_id=6 9 Microaveraging is more prefered in most cases than macroaveraging (Sebastiani 2002). 10 In all figures in this paper, curves might be truncated due to the large scale of dimensionality, especially the curves of troid based classifier, so we choose it as another representative feature selection criterion besides Chimax. Likewise, as for term weighting schemes, in addition to tfidf, the state of the art (Baeza-Yates and Ribeiro-Neto, 1999), we also choose tfidf*CIG (Xue and Sun, 2003b). Two word segmentation schemes are used for the word-indexing of documents. One is the maximum match algorithm (“mmword” in the figures), which is a representative of simple and fast word segmentation algorithms. The other is ICTCLAS8 (“lqword” in the figures). ICTCLAS is one of the best word segmentation systems (SIGHAN 2003) and reaches a segmentation precision of more than 97%, so we choose it as a representative of state-of-the-art schemes for automatic word-indexing of document). For evaluation of single-label classifications, F1-measure, pr</context>
</contexts>
<marker>Baeza-Yates, Ribeiro-Neto, 1999</marker>
<rawString>Ricardo Baeza-Yates, Berthier Ribeiro-Neto. 1999. Modern Information Retrieval, Addison-Wesley</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A Library for Support Vector Machines, Software available at http://www.csie.ntu.edu.tw/~cjlin/ libsvm</title>
<date>2001</date>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang, Chih-Jen Lin. 2001. LIBSVM: A Library for Support Vector Machines, Software available at http://www.csie.ntu.edu.tw/~cjlin/ libsvm</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Deerwester</author>
<author>Sue T Dumais</author>
<author>George W Furnas</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis,</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>41--391</pages>
<contexts>
<context position="19606" citStr="Deerwester et al., 1990" startWordPosition="3207" endWordPosition="3210">more than 30% of word occurrences in the Chinese language; they are effevtive in the word scheme and are not involved in the above issues. Note that the impact of effective one-character words on the classification is not as large as their total frequency, because the high frequency ones are often too common to have a good classification power, for instance, the word “的 (of, ‘s)”. 3.2 A Mass Feature Perspective Features are not independently acting in text classification. They are assembled together to constitute a feature space. Except for a few models such as Latent Semantic Indexing (LSI) (Deerwester et al., 1990), most models assume the feature space to be orthogonal. This assumption might not affect the effectiveness of the models, but the semantic redundancy and complementation among the feature terms do impact on the classification efficiency at a given dimensionality. According to the first issue addressed in the previous subsection, a bigram might cover for more than one word. For instance, the bigram “织物” is a sub-bigram of the words “织物 (fabric)”, “ 棉 织 物 (cotton fabric)”, “针 织 物 (knitted fabric)”, and also a good substitute of 11 The “OOV words” in this paper stand for the words that occur in </context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Harshman, 1990</marker>
<rawString>Steve Deerwester, Sue T. Dumais, George W. Furnas, Richard Harshman. 1990. Indexing by Latent Semantic Analysis, Journal of the American Society for Information Science, 41:391-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization,</title>
<date>1997</date>
<booktitle>Proceedings of 14th International Conference on Machine Learning</booktitle>
<pages>143--151</pages>
<location>Nashville, TN,</location>
<contexts>
<context position="8112" citStr="Joachims, 1997" startWordPosition="1271" endWordPosition="1272">, and both document collections lead to similar behaviors; Subsection 2.2 shows experiments on CE by a SVM classifier, in which, unlike with the Rocchio method, Chi feature selection scheme and tfidf term weighting scheme outperform other schemes; Subsection 2.3 shows experiments by a SVM classifier with Chi feature selection and tfidf term weighting on LC (manual word segmentation) to compare the best word features with bigram features. 2.1 The Rocchio Method and Various Settings The Rocchio method is rooted in the IR tradition, and is very different from machine learning ones (such as SVM) (Joachims, 1997; Sebastiani, 2002). Therefore, we choose it here as one of the representative classifiers to be examined. In the experiment, the control parameter of negative examples is set to 0, so this Rocchio based classifier is in fact a centroid-based classifier. Chimax is a state-of-the-art feature selection criterion for dimensionality reduction (Yang and Peterson, 1997; Rogati and Yang, 2002). Chimax*CIG (Xue and Sun, 2003a) is reported to be better in Chinese text categorization by a cen6 Not completed. 7 And POS (part-of-speech) tagged as well. But POS tags are not used in this study. 546 Figure 2</context>
</contexts>
<marker>Joachims, 1997</marker>
<rawString>Thorsten Joachims. 1997. A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization, Proceedings of 14th International Conference on Machine Learning (Nashville, TN, 1997), 143-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text Categorization with Support Vector Machine: Learning with Many Relevant Features,</title>
<date>1998</date>
<booktitle>Proceedings of the 10th European Conference on Machine Learning,</booktitle>
<pages>137--142</pages>
<contexts>
<context position="13185" citStr="Joachims, 1998" startWordPosition="2125" endWordPosition="2126">sionality, wherea the word schemes might outperform the bigram scheme on a low dimensionality. Till now, the experiments on CE and CTC show the same characteristics despite the performance fluctuation on CTC caused by sparseness. Hence in the next subsections CE is used instead of both of them because its curves are smoother. 2.2 SVM on Words and Bigrams As stated in the previous subsection, the lqword scheme always outperforms the mmword scheme; we compare here only the lqword scheme with the bigram scheme. Support Vector Machine (SVM) is one of the best classifiers at present (Vapnik, 1995; Joachims, 1998), so we choose it as the main classifier in this study. The SVM implementation used here is LIBSVM (Chang, 2001); the type of SVM is set to “C-SVC” and the kernel type is set to linear, which means a one-with-one scheme is used in the multi-class classification. Because the CIG’s effectiveness on a SVM classifier is not examined in Xue and Sun (2003a, 2003b)’s report, we make here the four combinations of schemes with and without CIG in feature selection and term weighting. The experiment results are shown in Figure 5. The collection used is CE. weighting scheme are related to the classifier, </context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text Categorization with Support Vector Machine: Learning with Many Relevant Features, Proceedings of the 10th European Conference on Machine Learning, 137-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mun-Kew Leong</author>
<author>Hong Zhou</author>
</authors>
<title>Preliminary Qualitative Analysis of Segmented vs. Bigram Indexing</title>
<date>1998</date>
<booktitle>in Chinese, The 6th Text Retrieval Conference (TREC-6), NIST Special Publication 500-240,</booktitle>
<pages>551--557</pages>
<contexts>
<context position="2866" citStr="Leong and Zhou 1998" startWordPosition="427" endWordPosition="430">ude POS (Part of Speech) information, etc. Second, term combinations (such as “wordbigram”, “word + word-bigram”, “characterbigram + character-trigram”3, etc.) can also be used as features (Nie et al., 2000). But, for Chinese Text Categorization, the “word or bigram” question is fundamental. They have quite different characteristics (e.g. bigrams overlap each other in text, but words do not) and influence the classification performance in different ways. In Information Retrieval, it is reported that bigram indexing schemes outperforms word schemes to some or little extent (Luk and Kwok, 1997; Leong and Zhou 1998; Nie et al., 2000). Few similar comparative studies have been reported for Text Categorization (Li et al., 2003) so far in literature. Text categorization and Information Retrieval are tasks that sometimes share identical aspects (Sebastiani, 2002) apart from term extraction (document indexing), such as tfidf term weighting and performance evaluation. Nevertheless, they are different tasks. One of the generally accepted connections between Information Retrieval and Text Categorization is that an information retrieval task could be partially taken as a binary classification problem with the qu</context>
</contexts>
<marker>Leong, Zhou, 1998</marker>
<rawString>Mun-Kew Leong, Hong Zhou. 1998. Preliminary Qualitative Analysis of Segmented vs. Bigram Indexing in Chinese, The 6th Text Retrieval Conference (TREC-6), NIST Special Publication 500-240, 551-557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baoli Li</author>
<author>Yuzhong Chen</author>
<author>Xiaojing Bai</author>
<author>Shiwen Yu</author>
</authors>
<title>Experimental Study on Representing Units in Chinese Text Categorization,</title>
<date>2003</date>
<booktitle>Proceedings of the 4th International Conference on Computational Linguistics and Intelligent Text Processing</booktitle>
<pages>602--614</pages>
<contexts>
<context position="2979" citStr="Li et al., 2003" startWordPosition="446" endWordPosition="449">haracterbigram + character-trigram”3, etc.) can also be used as features (Nie et al., 2000). But, for Chinese Text Categorization, the “word or bigram” question is fundamental. They have quite different characteristics (e.g. bigrams overlap each other in text, but words do not) and influence the classification performance in different ways. In Information Retrieval, it is reported that bigram indexing schemes outperforms word schemes to some or little extent (Luk and Kwok, 1997; Leong and Zhou 1998; Nie et al., 2000). Few similar comparative studies have been reported for Text Categorization (Li et al., 2003) so far in literature. Text categorization and Information Retrieval are tasks that sometimes share identical aspects (Sebastiani, 2002) apart from term extraction (document indexing), such as tfidf term weighting and performance evaluation. Nevertheless, they are different tasks. One of the generally accepted connections between Information Retrieval and Text Categorization is that an information retrieval task could be partially taken as a binary classification problem with the query as the only positive training document. From this 2 The terminology “term” stands for both word and character</context>
</contexts>
<marker>Li, Chen, Bai, Yu, 2003</marker>
<rawString>Baoli Li, Yuzhong Chen, Xiaojing Bai, Shiwen Yu. 2003. Experimental Study on Representing Units in Chinese Text Categorization, Proceedings of the 4th International Conference on Computational Linguistics and Intelligent Text Processing (CICLing 2003), 602-614.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Liu</author>
<author>Nanyuan Liang</author>
</authors>
<title>Basic Engineering for Chinese Processing – Contemporary Chinese Words Frequency Count,</title>
<date>1986</date>
<journal>Journal of Chinese Information Processing,</journal>
<pages>1--1</pages>
<contexts>
<context position="17406" citStr="Liu and Liang, 1986" startWordPosition="2843" endWordPosition="2846"> distinct this behavior is. 3 Qualitative Analysis To analyze the performance of words and bigrams as feature terms in Chinese text categorization, we need to investigate two aspects as follows. 3.1 An Individual Feature Perspective The word is a natural semantic unit in Chinese language and expresses a complete meaning in text. The bigram is not a natural semantic unit and might not express a complete meaning in text, but there are also reasons for the bigram to be a good feature term. First, two-character words and three-character words account for most of all multi-character Chinese words (Liu and Liang, 1986). A twocharacter word can be substituted by the same bigram. At the granularity of most categorization tasks, a three-character words can often be substituted by one of its sub-bigrams (namely the “intraword bigram” in the next section) without a change of meaning. For instance, “标赛” is a sub-bigram of the word “锦标赛(tournament)” and could represent it without ambiguity. Second, a bigram may overlap on two successive words (namely the “interword bigram” in the next section), and thus to some extent fills the role of a word-bigram. The word-bigram as a more definite (although more sparse) featur</context>
</contexts>
<marker>Liu, Liang, 1986</marker>
<rawString>Yuan Liu, Nanyuan Liang. 1986. Basic Engineering for Chinese Processing – Contemporary Chinese Words Frequency Count, Journal of Chinese Information Processing, 1(1):17-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert W P Luk</author>
<author>K L Kwok</author>
</authors>
<title>Comparing representations in Chinese information retrieval.</title>
<date>1997</date>
<booktitle>Proceedings of ACM SIGIR</booktitle>
<pages>34--41</pages>
<contexts>
<context position="2845" citStr="Luk and Kwok, 1997" startWordPosition="423" endWordPosition="426">p-word pruning, include POS (Part of Speech) information, etc. Second, term combinations (such as “wordbigram”, “word + word-bigram”, “characterbigram + character-trigram”3, etc.) can also be used as features (Nie et al., 2000). But, for Chinese Text Categorization, the “word or bigram” question is fundamental. They have quite different characteristics (e.g. bigrams overlap each other in text, but words do not) and influence the classification performance in different ways. In Information Retrieval, it is reported that bigram indexing schemes outperforms word schemes to some or little extent (Luk and Kwok, 1997; Leong and Zhou 1998; Nie et al., 2000). Few similar comparative studies have been reported for Text Categorization (Li et al., 2003) so far in literature. Text categorization and Information Retrieval are tasks that sometimes share identical aspects (Sebastiani, 2002) apart from term extraction (document indexing), such as tfidf term weighting and performance evaluation. Nevertheless, they are different tasks. One of the generally accepted connections between Information Retrieval and Text Categorization is that an information retrieval task could be partially taken as a binary classificatio</context>
</contexts>
<marker>Luk, Kwok, 1997</marker>
<rawString>Robert W.P. Luk, K.L. Kwok. 1997. Comparing representations in Chinese information retrieval. Proceedings of ACM SIGIR 1997, 34-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianyun Nie</author>
<author>Fuji Ren</author>
</authors>
<title>Chinese Information Retrieval: Using Characters or Words? Information Processing and Management,</title>
<date>1999</date>
<pages>35--443</pages>
<marker>Nie, Ren, 1999</marker>
<rawString>Jianyun Nie, Fuji Ren. 1999. Chinese Information Retrieval: Using Characters or Words? Information Processing and Management, 35:443-462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianyun Nie</author>
<author>Jianfeng Gao</author>
<author>Jian Zhang</author>
<author>Ming Zhou</author>
</authors>
<title>On the Use of Words and N-grams for Chinese Information Retrieval,</title>
<date>2000</date>
<booktitle>Proceedings of 5th International Workshop on Information Retrieval with Asian Languages</booktitle>
<contexts>
<context position="2454" citStr="Nie et al., 2000" startWordPosition="362" endWordPosition="365">n of China under grant number 60520130299 and EU FP6. ing units (feature terms) are word and characterbigram, so the problem is: which kind of terms2 should be chosen as the feature terms, words or character-bigrams? To obtain an all-sided idea about feature choice beforehand, we review here the possible feature variants (or, options). First, at the word level, we can do stemming, do stop-word pruning, include POS (Part of Speech) information, etc. Second, term combinations (such as “wordbigram”, “word + word-bigram”, “characterbigram + character-trigram”3, etc.) can also be used as features (Nie et al., 2000). But, for Chinese Text Categorization, the “word or bigram” question is fundamental. They have quite different characteristics (e.g. bigrams overlap each other in text, but words do not) and influence the classification performance in different ways. In Information Retrieval, it is reported that bigram indexing schemes outperforms word schemes to some or little extent (Luk and Kwok, 1997; Leong and Zhou 1998; Nie et al., 2000). Few similar comparative studies have been reported for Text Categorization (Li et al., 2003) so far in literature. Text categorization and Information Retrieval are ta</context>
</contexts>
<marker>Nie, Gao, Zhang, Zhou, 2000</marker>
<rawString>Jianyun Nie, Jianfeng Gao, Jian Zhang, Ming Zhou. 2000. On the Use of Words and N-grams for Chinese Information Retrieval, Proceedings of 5th International Workshop on Information Retrieval with Asian Languages</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monica Rogati</author>
<author>Yiming Yang</author>
</authors>
<title>High-performing Feature Selection for Text Classification,</title>
<date>2002</date>
<booktitle>Proceedings of ACM Conference on Information and Knowledge Management</booktitle>
<pages>659--661</pages>
<contexts>
<context position="8501" citStr="Rogati and Yang, 2002" startWordPosition="1328" endWordPosition="1331">ion) to compare the best word features with bigram features. 2.1 The Rocchio Method and Various Settings The Rocchio method is rooted in the IR tradition, and is very different from machine learning ones (such as SVM) (Joachims, 1997; Sebastiani, 2002). Therefore, we choose it here as one of the representative classifiers to be examined. In the experiment, the control parameter of negative examples is set to 0, so this Rocchio based classifier is in fact a centroid-based classifier. Chimax is a state-of-the-art feature selection criterion for dimensionality reduction (Yang and Peterson, 1997; Rogati and Yang, 2002). Chimax*CIG (Xue and Sun, 2003a) is reported to be better in Chinese text categorization by a cen6 Not completed. 7 And POS (part-of-speech) tagged as well. But POS tags are not used in this study. 546 Figure 2. chi-tfidf and chicig-tfidfcig on CTC Figure 2 shows the same group of curves for the CTC document collection. The curves fluctuate more than the curves for the CE collection because of sparseness; The CE collection is more sensitive to the additions of terms that come with the increase of dimensionality. The CE curves in the following figures show similar fluctuations for the same rea</context>
</contexts>
<marker>Rogati, Yang, 2002</marker>
<rawString>Monica Rogati, Yiming Yang. 2002. High-performing Feature Selection for Text Classification, Proceedings of ACM Conference on Information and Knowledge Management 2002, 659-661.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Christopher Buckley</author>
</authors>
<title>Term Weighting Approaches</title>
<date>1988</date>
<booktitle>in Automatic Text Retrieval, Information Processing and Management,</booktitle>
<pages>24--5</pages>
<marker>Salton, Buckley, 1988</marker>
<rawString>Gerard Salton, Christopher Buckley. 1988. Term Weighting Approaches in Automatic Text Retrieval, Information Processing and Management, 24(5):513-523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Sebastiani</author>
</authors>
<date>2002</date>
<journal>Machine Learning in Automated Text Categorization, ACM Computing Surveys,</journal>
<pages>34--1</pages>
<contexts>
<context position="1333" citStr="Sebastiani, 2002" startWordPosition="186" endWordPosition="187">orpus as a golden standard), and a semi-quantitative analysis to elucidate the characteristics of their behavior; and try to provide some preliminary clue for feature term choice (in most cases, character-bigrams are better than words) and dimensionality setting in text categorization systems. 1 Introduction1 Because of the popularity of the Vector Space Model (VSM) in text information processing, document indexing (term extraction) acts as a pre-requisite step in most text information processing tasks such as Information Retrieval (Baeza-Yates and Ribeiro-Neto, 1999) and Text Categorization (Sebastiani, 2002). It is empirically known that the indexing scheme is a nontrivial complication to system performance, especially for some Asian languages in which there are no explicit word margins and even no natural semantic unit. Concretely, in Chinese Text Categorization tasks, the two most important index1 This research is supported by the National Natural Science Foundation of China under grant number 60573187 and 60321002, and the Tsinghua-ALVIS Project co-sponsored by the National Natural Science Foundation of China under grant number 60520130299 and EU FP6. ing units (feature terms) are word and cha</context>
<context position="3115" citStr="Sebastiani, 2002" startWordPosition="466" endWordPosition="467">“word or bigram” question is fundamental. They have quite different characteristics (e.g. bigrams overlap each other in text, but words do not) and influence the classification performance in different ways. In Information Retrieval, it is reported that bigram indexing schemes outperforms word schemes to some or little extent (Luk and Kwok, 1997; Leong and Zhou 1998; Nie et al., 2000). Few similar comparative studies have been reported for Text Categorization (Li et al., 2003) so far in literature. Text categorization and Information Retrieval are tasks that sometimes share identical aspects (Sebastiani, 2002) apart from term extraction (document indexing), such as tfidf term weighting and performance evaluation. Nevertheless, they are different tasks. One of the generally accepted connections between Information Retrieval and Text Categorization is that an information retrieval task could be partially taken as a binary classification problem with the query as the only positive training document. From this 2 The terminology “term” stands for both word and character-bigram. Term or combination of terms (in word-bigram or other forms) might be chosen as “feature”. 3 The terminology “character” stands</context>
<context position="8131" citStr="Sebastiani, 2002" startWordPosition="1273" endWordPosition="1274">ent collections lead to similar behaviors; Subsection 2.2 shows experiments on CE by a SVM classifier, in which, unlike with the Rocchio method, Chi feature selection scheme and tfidf term weighting scheme outperform other schemes; Subsection 2.3 shows experiments by a SVM classifier with Chi feature selection and tfidf term weighting on LC (manual word segmentation) to compare the best word features with bigram features. 2.1 The Rocchio Method and Various Settings The Rocchio method is rooted in the IR tradition, and is very different from machine learning ones (such as SVM) (Joachims, 1997; Sebastiani, 2002). Therefore, we choose it here as one of the representative classifiers to be examined. In the experiment, the control parameter of negative examples is set to 0, so this Rocchio based classifier is in fact a centroid-based classifier. Chimax is a state-of-the-art feature selection criterion for dimensionality reduction (Yang and Peterson, 1997; Rogati and Yang, 2002). Chimax*CIG (Xue and Sun, 2003a) is reported to be better in Chinese text categorization by a cen6 Not completed. 7 And POS (part-of-speech) tagged as well. But POS tags are not used in this study. 546 Figure 2. chi-tfidf and chi</context>
<context position="9848" citStr="Sebastiani 2002" startWordPosition="1548" endWordPosition="1549"> in Figure 3 and Figure 4. dimensionality x 104 Figure 1. chi-tfidf and chicig-tfidfcig on CE Figure 1 shows the performancedimensionality curves of the chi-tfidf approach and the approach with CIG, by mmword, lqword and bigram document indexing, on the CE document collection. We can see that the original chi-tfidf approach is better at low dimensionalities (less than 10000 dimensions), while the CIG version is better at high dimensionalities and reaches a higher limit.10 8 http://www.nlp.org.cn/project/project.php?proj_id=6 9 Microaveraging is more prefered in most cases than macroaveraging (Sebastiani 2002). 10 In all figures in this paper, curves might be truncated due to the large scale of dimensionality, especially the curves of troid based classifier, so we choose it as another representative feature selection criterion besides Chimax. Likewise, as for term weighting schemes, in addition to tfidf, the state of the art (Baeza-Yates and Ribeiro-Neto, 1999), we also choose tfidf*CIG (Xue and Sun, 2003b). Two word segmentation schemes are used for the word-indexing of documents. One is the maximum match algorithm (“mmword” in the figures), which is a representative of simple and fast word segmen</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Fabrizio Sebastiani. 2002. Machine Learning in Automated Text Categorization, ACM Computing Surveys, 34(1):1-47</rawString>
</citation>
<citation valid="false">
<authors>
<author>Dejun Xue</author>
</authors>
<title>Maosong Sun. 2003a. Select Strong Information Features to Improve Text Categorization Effectiveness,</title>
<journal>Journal of Intelligent Systems, Special Issue.</journal>
<marker>Xue, </marker>
<rawString>Dejun Xue, Maosong Sun. 2003a. Select Strong Information Features to Improve Text Categorization Effectiveness, Journal of Intelligent Systems, Special Issue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dejun Xue</author>
<author>Maosong Sun</author>
</authors>
<title>A Study on Feature Weighting in Chinese Text Categorization,</title>
<date>2003</date>
<booktitle>Proceedings of the 4th International Conference on Computational Linguistics and Intelligent Text Processing</booktitle>
<pages>594--604</pages>
<contexts>
<context position="8532" citStr="Xue and Sun, 2003" startWordPosition="1334" endWordPosition="1337">es with bigram features. 2.1 The Rocchio Method and Various Settings The Rocchio method is rooted in the IR tradition, and is very different from machine learning ones (such as SVM) (Joachims, 1997; Sebastiani, 2002). Therefore, we choose it here as one of the representative classifiers to be examined. In the experiment, the control parameter of negative examples is set to 0, so this Rocchio based classifier is in fact a centroid-based classifier. Chimax is a state-of-the-art feature selection criterion for dimensionality reduction (Yang and Peterson, 1997; Rogati and Yang, 2002). Chimax*CIG (Xue and Sun, 2003a) is reported to be better in Chinese text categorization by a cen6 Not completed. 7 And POS (part-of-speech) tagged as well. But POS tags are not used in this study. 546 Figure 2. chi-tfidf and chicig-tfidfcig on CTC Figure 2 shows the same group of curves for the CTC document collection. The curves fluctuate more than the curves for the CE collection because of sparseness; The CE collection is more sensitive to the additions of terms that come with the increase of dimensionality. The CE curves in the following figures show similar fluctuations for the same reason. For a parallel comparison </context>
<context position="10251" citStr="Xue and Sun, 2003" startWordPosition="1610" endWordPosition="1613">G version is better at high dimensionalities and reaches a higher limit.10 8 http://www.nlp.org.cn/project/project.php?proj_id=6 9 Microaveraging is more prefered in most cases than macroaveraging (Sebastiani 2002). 10 In all figures in this paper, curves might be truncated due to the large scale of dimensionality, especially the curves of troid based classifier, so we choose it as another representative feature selection criterion besides Chimax. Likewise, as for term weighting schemes, in addition to tfidf, the state of the art (Baeza-Yates and Ribeiro-Neto, 1999), we also choose tfidf*CIG (Xue and Sun, 2003b). Two word segmentation schemes are used for the word-indexing of documents. One is the maximum match algorithm (“mmword” in the figures), which is a representative of simple and fast word segmentation algorithms. The other is ICTCLAS8 (“lqword” in the figures). ICTCLAS is one of the best word segmentation systems (SIGHAN 2003) and reaches a segmentation precision of more than 97%, so we choose it as a representative of state-of-the-art schemes for automatic word-indexing of document). For evaluation of single-label classifications, F1-measure, precision, recall and accuracy (Baeza-Yates and</context>
<context position="13536" citStr="Xue and Sun (2003" startWordPosition="2186" endWordPosition="2189">Bigrams As stated in the previous subsection, the lqword scheme always outperforms the mmword scheme; we compare here only the lqword scheme with the bigram scheme. Support Vector Machine (SVM) is one of the best classifiers at present (Vapnik, 1995; Joachims, 1998), so we choose it as the main classifier in this study. The SVM implementation used here is LIBSVM (Chang, 2001); the type of SVM is set to “C-SVC” and the kernel type is set to linear, which means a one-with-one scheme is used in the multi-class classification. Because the CIG’s effectiveness on a SVM classifier is not examined in Xue and Sun (2003a, 2003b)’s report, we make here the four combinations of schemes with and without CIG in feature selection and term weighting. The experiment results are shown in Figure 5. The collection used is CE. weighting scheme are related to the classifier, which is worth noting. In other words, no feature selection scheme or term weighting scheme is absolutely the best for all classifiers. Therefore, a reasonable choice is to select the best performing combination of feature selection scheme, term weighting scheme and classifier, i.e. chi-tfidf and SVM. The curves for the lqword scheme and the bigram </context>
</contexts>
<marker>Xue, Sun, 2003</marker>
<rawString>Dejun Xue, Maosong Sun. 2003b. A Study on Feature Weighting in Chinese Text Categorization, Proceedings of the 4th International Conference on Computational Linguistics and Intelligent Text Processing (CICLing 2003), 594-604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
</authors>
<date>1995</date>
<booktitle>The Nature of Statistical Learning Theory,</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="13168" citStr="Vapnik, 1995" startWordPosition="2123" endWordPosition="2124">n a high dimensionality, wherea the word schemes might outperform the bigram scheme on a low dimensionality. Till now, the experiments on CE and CTC show the same characteristics despite the performance fluctuation on CTC caused by sparseness. Hence in the next subsections CE is used instead of both of them because its curves are smoother. 2.2 SVM on Words and Bigrams As stated in the previous subsection, the lqword scheme always outperforms the mmword scheme; we compare here only the lqword scheme with the bigram scheme. Support Vector Machine (SVM) is one of the best classifiers at present (Vapnik, 1995; Joachims, 1998), so we choose it as the main classifier in this study. The SVM implementation used here is LIBSVM (Chang, 2001); the type of SVM is set to “C-SVC” and the kernel type is set to linear, which means a one-with-one scheme is used in the multi-class classification. Because the CIG’s effectiveness on a SVM classifier is not examined in Xue and Sun (2003a, 2003b)’s report, we make here the four combinations of schemes with and without CIG in feature selection and term weighting. The experiment results are shown in Figure 5. The collection used is CE. weighting scheme are related to</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir Vapnik. 1995. The Nature of Statistical Learning Theory, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Jan O Pederson</author>
</authors>
<title>A Comparative Study on Feature Selection in Text Categorization,</title>
<date>1997</date>
<booktitle>Proceedings of ICML</booktitle>
<pages>412--420</pages>
<marker>Yang, Pederson, 1997</marker>
<rawString>Yiming Yang, Jan O. Pederson. 1997. A Comparative Study on Feature Selection in Text Categorization, Proceedings of ICML 1997, 412-420.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>