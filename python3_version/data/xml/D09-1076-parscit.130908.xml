<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000016">
<title confidence="0.987203">
Synchronous Tree Adjoining Machine Translation
</title>
<author confidence="0.879521">
Steve DeNeefe and Kevin Knight
</author>
<affiliation confidence="0.700743">
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
</affiliation>
<address confidence="0.626773">
Marina del Rey, CA 90292 USA
</address>
<email confidence="0.998862">
{sdeneefe,knight}@isi.edu
</email>
<sectionHeader confidence="0.993324" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.884306727272727">
Tree Adjoining Grammars have well-known
advantages, but are typically considered too
difficult for practical systems. We demon-
strate that, when done right, adjoining im-
proves translation quality without becoming
computationally intractable. Using adjoining
to model optionality allows general translation
patterns to be learned without the clutter of
endless variations of optional material. The
appropriate modifiers can later be spliced in as
needed.
In this paper, we describe a novel method
for learning a type of Synchronous Tree Ad-
joining Grammar and associated probabilities
from aligned tree/string training data. We in-
troduce a method of converting these gram-
mars to a weakly equivalent tree transducer
for decoding. Finally, we show that adjoining
results in an end-to-end improvement of +0.8
BLEU over a baseline statistical syntax-based
MT model on a large-scale Arabic/English MT
task.
</bodyText>
<sectionHeader confidence="0.998785" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947893617022">
Statistical MT has changed a lot in recent years.
We have seen quick progress from manually
crafted linguistic models to empirically learned
statistical models, from word-based models to
phrase-based models, and from string-based mod-
els to tree-based models. Recently there is a swing
back to incorporating more linguistic information
again, but this time linguistic insight carefully
guides the setup of empirically learned models.
Shieber (2007) recently argued that proba-
bilistic Synchronous Tree Adjoining Grammars
(Shieber and Schabes, 1990) have the right com-
bination of properties that satisfy both linguists
and empirical MT practitioners. So far, though,
most work in this area has been either more lin-
guistic than statistical (Abeille et al., 1990) or
statistically-based, but linguistically light (Nesson
et al., 2006).
Current tree-based models that integrate lin-
guistics and statistics, such as GE KM (Galley et
al., 2004), are not able to generalize well from
a single phrase pair. For example, from the data
in Figure 1, GE KM can learn rule (a) to translate
nouns with two pre-modifiers, but does not gener-
alize to learn translation rules (b) - (d) without the
optional adjective or noun modifiers. Likewise,
none of these rules allow extra material to be intro-
duced, e.g. “Pakistan’s national defense minister”.
In large enough training data sets, we see many
examples of all the common patterns, but the rarer
patterns have sparse statistics or poor coverage.
To mitigate this problem, the parse trees used
as training data for these systems can be binarized
(Wang et al., 2007). Binarization allows rules with
partial constituents to be learned, resulting in more
general rules, richer statistics, and better phrasal
coverage (DeNeefe et al., 2007), but no principled
required vs. optional decision has been made. This
method’s key weakness is that binarization always
keeps adjacent siblings together, so there is no way
to group the head with a required complement if
optional information intervenes between the two.
Furthermore, if all kinds of children are consid-
ered equally optional, then we have removed im-
portant syntactic constraints, which may end up
permitting too much freedom. In addition, spu-
rious alignments may limit the binarization tech-
</bodyText>
<figure confidence="0.998251333333333">
H NN3 NN2 JJ1
NP
(a)
NN
NN
defense
JJ
national
minister
wzyr AldfAE AlwTnY
JJ1 NN2 NN3
NP
NN1 NN2
NP
JJ1 NN2
NP
H NN1
NN1
</figure>
<figureCaption confidence="0.990842666666667">
Figure 1: Rule (a) can be learned from this training
example. Arguably, the more general rules (b) -
(d) should also be learnable.
</figureCaption>
<equation confidence="0.829377">
H NN2 NN1
H NN2 JJ1
NP
</equation>
<page confidence="0.949748">
727
</page>
<note confidence="0.9326965">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 727–736,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.997548875">
nique’s effectiveness.
In this paper, we present a method of learning
a type of probabilistic Synchronous Tree Adjoin-
ing Grammar (STAG) automatically from a cor-
pus of word-aligned tree/string pairs. To learn this
grammar we use linguistic resources to make the
required vs. optional decision. We then directly
model the optionality in the translation rules by
learning statistics for the required parts of the rule
independently from the optional parts. We also
present a method of converting these rules into a
well-studied tree transducer formalism for decod-
ing purposes. We then show that modeling option-
ality using adjoining results in a statistically sig-
nificant BLEU gain over our baseline syntax-based
model with no adjoining.
</bodyText>
<sectionHeader confidence="0.999633" genericHeader="method">
2 Translation Model
</sectionHeader>
<subsectionHeader confidence="0.999413">
2.1 Synchronous Tree Insertion Grammars
</subsectionHeader>
<bodyText confidence="0.99996575">
Tree Adjoining Grammars (TAG), introduced by
Joshi et al. (1975) and Joshi (1985), allow inser-
tion of unbounded amounts of material into the
structure of an existing tree using an adjunction
operation. Usually they also include a substitution
operation, which has a ‘fill in the blank’ seman-
tics, replacing a substitution leaf node with a tree.
Figure 2 visually demonstrates TAG operations.
Shieber and Schabes (1990) offer a synchronous
version of TAG (STAG), allowing the construc-
tion of a pair of trees in lockstep fashion using the
TAG operations of substitution and adjunction on
tree pairs. To facilitate this synchronous behav-
ior, links between pairs of nodes in each tree pair
define the possible sites for substitution and ad-
junction to happen. One application of STAG is
machine translation (Abeille et al., 1990).
One negative aspect of TAG is the compu-
tational complexity: O(n6) time is required
for monolingual parsing (and thus decoding),
and STAG requires O(n12) for bilingual parsing
(which might be used for training the model di-
rectly on bilingual data). Tree Insertion Grammars
(TIG) are a restricted form of TAG that was in-
troduced (Schabes and Waters, 1995) to keep the
same benefits as TAG (adjoining of unbounded
material) without the computational complexity—
TIG parsing is O(n3). This reduction is due to a
limitation on adjoining: auxiliary trees can only
introduce tree material to the left or the right of
the node adjoined to. Thus an auxiliary tree can
be classified by direction as left or right adjoining.
</bodyText>
<figure confidence="0.822786333333333">
the
NN
minister
</figure>
<figureCaption confidence="0.955276">
Figure 2: TAG grammars use substitution and ad-
</figureCaption>
<bodyText confidence="0.911074947368422">
junction operations to construct trees. Substitu-
tion replaces the substitution node (marked with
1) with another tree. Adjunction inserts an aux-
iliary tree—a special kind of tree fragment with a
foot node (marked with *)—into an existing tree at
a permitted non-terminal node. Note that in TAG,
adjunctions are permitted at any non-terminal with
the same label as the root and foot node of the
auxiliary tree, while in STAG adjunctions are re-
stricted to linked sites.
Nesson et al. (2006) introduce a probabilis-
tic, synchronous variant of TIG and demonstrate
its use for machine translation, showing results
that beat both word-based and phrase-based MT
models on a limited-vocabulary, small-scale train-
ing and test set. Training the model uses an
O(n6) bilingual parsing algorithm, and decoding
is O(n3). Though this model uses trees in the for-
mal sense, it does not create Penn Treebank (Mar-
cus et al., 1993) style linguistic trees, but uses only
one non-terminal label (X) to create those trees us-
ing six simple rule structures.
The grammars we use in this paper share some
properties in common with those of Nesson et al.
(2006) in that they are of the probabilistic, syn-
chronous tree-insertion variety. All pairs of sites
(both adjunction and substitution in our case) are
explicitly linked. Adjunction sites are restricted by
direction: at each linked site, the source and target
side each specify one allowed direction. The re-
sult is that each synchronous adjunction site can be
classified into one of four direction classes: {LR,
LL, RR, RL}. For example, LR means the source
side site only allows left adjoining trees and the
target side site only allows right adjoining trees.
There are several important differences between
our grammars and the ones of Nesson et al. (2006):
Richer, Linguistic Trees: Our grammars have a
</bodyText>
<figure confidence="0.998807529411765">
the
NN1
NP
adjunction
NP
substitution substitution
NP
DT
JJ1 NP*
NN JJ
minister defense
��
NP
DT NP
JJ
defense
NP
</figure>
<page confidence="0.981616">
728
</page>
<bodyText confidence="0.999609666666667">
Penn Treebank-style linguistic tree on the En-
glish (target) side, and a hierarchical structure
using only a single non-terminal symbol (X)
on the source side. We believe this provides
the rich information needed in the target lan-
guage without over-constraining the model.
</bodyText>
<subsectionHeader confidence="0.895228">
Substitution Sites/Non-lexical trees: We use
</subsectionHeader>
<bodyText confidence="0.999702357142857">
both substitution and adjunction (Nesson
et al. (2006) only used adjunction) and do
not require all trees to contain lexical items
as is commonly done in TIG (Schabes and
Waters, 1995).
Single Adjunction/Multiple Sites: Each non-
terminal node in a tree may allow multiple
adjunction sites, but every site only allows at
most one adjunction,1 a common assumption
for TAG as specified in the Vijay-Shanker
(1987) definition.
Here are some examples of automatically
learned translation rules with interpretations of
how they work:
</bodyText>
<listItem confidence="0.984697">
1. simple lexical rules for translating words or
phrases:
</listItem>
<sectionHeader confidence="0.602316" genericHeader="method">
IN
</sectionHeader>
<bodyText confidence="0.7747725">
��
without
interpretation: translate the Arabic word
“AlA” as the preposition “without”
2. rules with substitution for translating phrases
with holes (substitution sites are designated
by an arrow and numeric subscript, e.g.
NP11):
</bodyText>
<equation confidence="0.956525">
PP
PP NP11 X
��
IN X11
</equation>
<bodyText confidence="0.904231">
of
interpretation: insert “of” to turn a noun
phrase into a prepositional phrase
</bodyText>
<listItem confidence="0.6452515">
3. simple adjoining rules for inserting optional
modifiers (adjoining sites are designated by
</listItem>
<footnote confidence="0.5434365">
1An adjoined rule may itself have adjoining sites allowing
further adjunction.
</footnote>
<bodyText confidence="0.983411666666667">
an alphabetic subscript before or after a non-
terminal to indicate direction of adjoining,
e.g. aNP):
</bodyText>
<equation confidence="0.961537666666667">
X
�� X* Xa
X11
</equation>
<bodyText confidence="0.9682495">
interpretation: adjoin an adjective before a
noun in English but after in Arabic, and al-
lowing further adjoinings in those same di-
rections afterward
4. rules with multiple adjunction and substitu-
tion sites:
</bodyText>
<equation confidence="0.9891266">
aS
NP11 6Sc
VPd
VPe NP13
VBD12
</equation>
<bodyText confidence="0.999560333333333">
interpretation: translate an Arabic sentence in
VSO form into an English sentence in SVO
form, with multiple adjoining options
</bodyText>
<subsectionHeader confidence="0.995923">
2.2 Generative Story
</subsectionHeader>
<bodyText confidence="0.999942">
When we use these rules to translate from a for-
eign sentence f into an English sentence e, we
use several models together in a log-linear fash-
ion, but our primary model is a joint model of
P(etree, ftree), which is our surrogate for directly
modeling P(elf). This can be justified because
</bodyText>
<equation confidence="0.966341555555556">
P(e|f) = � �e,f)
� �f) , and P(f) is fixed for a given
foreign sentence. Therefore:
argmax P(elf) = argmax P(e, f)
e e
� yield(argmax
etree
� yield(argmax
etree
</equation>
<bodyText confidence="0.9998347">
where detree,ftree is a derivation tree of rules that
generates etree and ftree. In other words, e, the
highest probability translation of f, can be approx-
imated by taking the yield of the highest proba-
bility tree etree that is a translation of the high-
est probability tree of f. This can further be ap-
proximated by the highest probability derivation
of rules translating between f and e via trees.
Now we define the probability of generating
detree,ftree. Starting with an initial symbol pair
</bodyText>
<figure confidence="0.97853184">
X
AlA
aNP
JJ11 NP*
X
��
aX
X12
X
X11
e,6Xd,c
X13
P(etree, ftree))
P(detree,ftree))
729
S
ADVP , NP VP .
S
ADVPO ,O NPR VPH .O
S
ADVP , NP VP .
(a)
��
(b)
��
</figure>
<figureCaption confidence="0.977315">
Figure 3: Parse tree to TIG transformation: (a) mark constituent children with (H)ead, (R)equired, and
(O)ptional, then (b) restructure the tree so that head and required elements are substitutions, while op-
tional elements are adjoined (shown with dotted lines).
</figureCaption>
<figure confidence="0.9961675">
XYZ
(a) excising one optional child (XYZ) (b) excising a series of optional children (DEF, then XYZ)
</figure>
<figureCaption confidence="0.994718666666667">
Figure 4: Two examples of excising auxiliary trees from a head-out binarized parse tree: (a) excising one
optional left branch, (b) excising a chain of optional branches in the same (right) direction into a series
of adjunctions. In both examples, the ‘ABC’ child is the head, while the other children are optional.
</figureCaption>
<figure confidence="0.992596178571428">
NT1
NT1
NT1
NT1
ABC
NT1
NT3
��
��
NT2
NT1
XYZ
NT2
NT1
XYZ
ABC
DEF
NT1*
NT3
DEF
ABC
NT1
NT1
NT2
NT1*
ABC NT1
NT1* NT2
XYZ
</figure>
<figureCaption confidence="0.432532">
adjoined to the previous child, as in Figure 4(b).
</figureCaption>
<bodyText confidence="0.99321941509434">
C. Extracting rules and derivation trees. We
now have a TIG derivation tree, with each elemen-
tary tree attached to its parent by a substitution or
adjunction link. We can now extract synchronous
rules allowed by the alignments and syntactic con-
stituents. This can be done using a method in-
spired by the rule-extraction approach of Galley et
al. (2004), but instead of directly operating on the
parse tree we process the English TIG derivation
tree. In bottom-up fashion, we visit each elemen-
tary tree in the derivation, allowing a rule rooted
at this tree to be extracted if its words or those
of its descendants are aligned such that they are
the English side of a self-contained parallel phrase
(i.e., the foreign text of this phrase is not aligned to
English leaves outside of the set of descendants).
Otherwise, this elementary tree is rejoined with its
parent to form a larger elementary tree. At the end
of this process we have a new set of linked ele-
mentary trees which make up the English side of
the grammar, where each substitution or adjunc-
tion link becomes a substitution or adjunction site
in the synchronous grammar.
On the foreign side we start with the foreign text
of the self-contained parallel phrase and replace
any parts of this phrase covered by substituted or
adjoined children of the English side tree with sub-
stitution sites or adjunction site markers. From
this, we produce a tree with a simple, regular form
by placing all items under a root node labeled X.
In the case of more than one foreign word or sub-
stitution site, we introduce an intermediate level of
X-labeled non-terminals to allow for possible ad-
junction between elements, otherwise the adjoin-
ing sites attach to the single root node. We attach
all foreign-side adjoining sites to be left adjoining,
except on the right side of the right-hand child.
It is possible to have the head child tree on the
English side not aligned to anything, while the ad-
joined children are. This may lead to rules with no
foreign non-terminal from which to anchor the ad-
junctions, so in this case, we attach adjoined child
elementary trees starting from the head and mov-
ing out until we attach a some child with a non-
empty foreign side.
D. Generalizing rules. We need to clarify
what makes one rule distinct from another. Con-
sider the example in Figure 5, which shows se-
lected rules learned in the case of two different
noun phrases. If the noun phrase consists of just
a single noun, we learn rule (a), while if the noun
phrase also has an adjective, we learn rules (b) and
(c). Since adjoining the adjective is optional, we
</bodyText>
<page confidence="0.995158">
731
</page>
<bodyText confidence="0.996118666666667">
consider rules (a) and (c) to be the same rule, the
latter with an adjoining seen, and the former with
the same adjoining not seen.
</bodyText>
<subsectionHeader confidence="0.998448">
3.2 Statistical Models
</subsectionHeader>
<bodyText confidence="0.99583519047619">
Once we have the derivation trees and list of rules,
we learn our statistical models using maximum
likelihood estimation. By counting and normal-
izing appropriately over the entire corpus, we can
straightforwardly learn the Psub and Padj distribu-
tions. However, recall that in our model Pifadj is a
rule-specific probability, which makes it more dif-
ficult to estimate accurately. For common rules,
we see plenty of examples of adjoining, while for
other rules, we need to learn from only a handful
of examples. Smoothing and generalization are es-
pecially important for these low frequency cases.
Two options present themselves for how to esti-
mate adjoining:
(a) A joint model of adjoining. We assume that
adjoining decisions are made in combination
with each other, and so learn non-zero proba-
bilities only for adjoining combinations seen
in data
(b) An independent model of adjoining. We as-
sume adjoining decisions are made indepen-
dently, and learn a model for each adjoining
site separately
Option (a) may be sufficient for frequent rules,
and will accurately model dependencies between
different kinds of adjoining. However, it does not
allow us to generalize to unseen patterns of adjoin-
ing. Consider the low frequency situation depicted
in Figure 6, rules (d)-(f). We may have seen this
rule four times, once with adjoining site a, twice
with adjoining sites a and b, and once with a third
adjoining site c. The joint model will give a zero
probability to unseen patterns of adjoining, e.g. no
adjoining at any site or adjoining at site b alone.
Even if we use a discounting method to give a non-
zero probability to unseen cases, we still have no
way to distinguish one from another.
Option (b) allows us to learn reasonable esti-
mates for these missing cases by separating out
adjoining decisions and letting each speak for it-
self. To properly learn non-zero probabilities for
unseen cases5 we use add k smoothing (k = 2).
</bodyText>
<footnote confidence="0.983211666666667">
5For example, low frequency rules may have always been
observed with a single adjoining pattern, and never without
adjoining.
</footnote>
<bodyText confidence="0.998444238095238">
A weakness of this approach still remains: ad-
joining is not a truly independent process, as we
observe empirically in the data. In real data, fre-
quent rules have many different observed adjoin-
ing sites (10 or 20 in some cases), many of which
represent already infrequent sites in combinations
never seen together. To reduce the number of in-
valid combinations produced, we only allow ad-
joinings to be used at the same time if they have
occurred together in the training data. This restric-
tion makes it possible to do less adjoining than ob-
served, but not more. For the example in Figure 6,
in addition to the observed patterns, we would also
allow site b to be used alone, and we would allow
no adjoinings, but we would not allow combina-
tions of site c with either a or b. Later, we will
see that this makes the decoding process more ef-
ficient.
Because both option (a) and (b) above have
strengths and weaknesses, we also explore a third
option which builds upon the strengths of each:
</bodyText>
<listItem confidence="0.98736">
(c) A log-linear combination of the joint model
and independent model. We assume the prob-
ability has both a dependent and indepen-
dent element, and learn the relative weight
between them automatically
</listItem>
<bodyText confidence="0.99729275">
To help smooth this model we add two addi-
tional binary features: one indicating adjoining
patterns seen in data and one indicating previously
unseen patterns.
</bodyText>
<sectionHeader confidence="0.989546" genericHeader="method">
4 Decoding
</sectionHeader>
<bodyText confidence="0.9997143125">
To translate with these rules, we do a monolingual
parse using the foreign side of the rules (constrain-
ing the search using non-terminal labels from both
sides), while keeping track of the English side
string and structure for language modeling pur-
poses. This produces all valid derivations of rules
whose foreign side yield is the input string, from
which we simply choose the one with the high-
est log-linear model score. Though this process
could be done directly using a specialized parsing
algorithm, we note that these rules have weakly
equivalent counterparts in the Synchronous Tree
Substitution Grammar (STSG) and Tree-to-string
transducer (xLNTs6) worlds, such that each STIG
rule can be translated into one equivalent rule, plus
some helper rules to model the adjoin/no-adjoin
</bodyText>
<footnote confidence="0.9982255">
6xLNTs is shorthand for extended linear non-deleting top-
down tree-to-string transducer.
</footnote>
<page confidence="0.982116">
732
</page>
<figure confidence="0.978643967741936">
Case 1: Case 2:
NP
NP
NN
health
→ (a)
NP
⇐⇒
X
X↓1
NN↓1
X
⇐⇒ X* X
X↓1
Xa
X↓1
→
JJ↓1 NP*
defense
(c)
aNP
NN↓1
⇐⇒
NP
JJ
national
NN
NP
(b)
AlSHp
AldfAE AlwTnY
</figure>
<figureCaption confidence="0.998855">
Figure 5: Selected rules learned in two cases. Rule (a) and (c) are considered the same rule, where (c)
</figureCaption>
<bodyText confidence="0.490930666666667">
has the optional synchronous adjoining site marked with a. From these (limited) examples alone we
would infer that adjective adjoining happens half the time, and is positioned before the noun in English,
but after the noun in Arabic (thus the positioning of site a).
</bodyText>
<equation confidence="0.726632333333333">
(d) aQPb ⇐⇒ aXb (e) aQP ⇐⇒ aX (f) cQP ⇐⇒ Xc
IN↓1 X↓1 IN↓1 X↓1 IN↓1 X↓1
(seen once) (seen twice) (seen once)
</equation>
<bodyText confidence="0.986496340909091">
Figure 6: For a low frequency rule, we may see only a few different adjoining patterns, but we want to
infer more.
decision. Conversion to a better known and ex-
plored formalism allows us to take advantage of
existing code and algorithms. Here we describe
the conversion process to xLNTs rules, though
conversion to STSG is similar.
Algorithm 1 describes the process of converting
one of our automatically learned STIG rules. On
each side of the rule, we traverse the tree in a top-
down, left-to-right order, recording words, substi-
tution sites, and adjoining sites in the order en-
countered (left adjoinings before the node’s chil-
dren and right adjoinings after). We make these
words and sites as the children under a single root
node. The substitution sites are given states made
up of a combination of their source and target la-
bels as are the roots of non-adjoining rules. Ad-
joining sites are labeled with a combination of the
rule id and a site id. Adjoining rule roots are la-
beled with a combination of the source and target
root labels and the direction class. To allow for the
adjoining/no-adjoining decision, two helper rules
are created for each adjoining site, their root state
a combination of the rule and site ids. One of these
rules has only epsilon leaf nodes (representing no
adjoining), while the other has leaf nodes and a
state that match with the corresponding adjoining
rule root (labeled with the site’s source and target
labels and the direction class).
For each rule, the algorithm generates one
main rule and pairs of helper rules to facilitate
adjoining/non-adjoining. For computational efÞ-
ciency reasons, our decoder supports neither ep-
silon rules nor non-binary rules. So we remove ep-
silons using an exponential expansion of the rules:
combine each main rule with an adjoining or non-
adjoining helper rule for each adjunction site, then
remove epsilon-only branches. For k adjunction
sites this could possibly results in 2k rules. But as
discussed previously (at the end of Section 3.2),
we only allow subsets of adjoining combinations
seen in training data, so this number is substan-
tially lower for large values of k.
</bodyText>
<sectionHeader confidence="0.999706" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999926">
All experiments are trained with a subset (171,000
sentences or 4 million words) of the Arabic-
English training data from the constrained data
track of the NIST 2008 MT Evaluation, leav-
ing out LDC2004T18, LDC2007E07, and the UN
data. The training data is aligned using the LEAF
technique (Fraser and Marcu, 2007). The English
side of the training data is parsed with an imple-
mentation of Collins Model 2 (Collins, 2003)
then head-out binarized. The tuning data (1,178
sentences) and devtest data (1,298 sentences) are
</bodyText>
<page confidence="0.995386">
733
</page>
<bodyText confidence="0.999083387755102">
made up of newswire documents drawn from the
NIST MT evaluation data from 2004, 2005, and
2006 (GALE part). We use the newswire docu-
ments from the NIST part of the 2006 evaluation
data (765 sentences) as a held-out test set.
We train our feature weights using max-BLEU
(Och, 2003) and decode with a CKY-based de-
coder that supports language model scoring di-
rectly integrated into the search.
In addition to Pub, Padj, and Pifadj, we
use several other features in our log-linear
model during decoding, including: lexical and
phrase-based translation probabilities, a model
similar to conditional probability on the trees
(P(ftree(rule)|etree(rule))), a probability model
for generating the top tree non-terminal, a 5-gram
language model7, and target length bonus. We
also have several binary features—lexical rule,
rule with missing or spurious content words—and
several binary indicator features for specialized
rules: unknown word rules; name, number, and
date translation rules; and special fail-safe mono-
tone translation rules in case of parse failures and
extremely long sentences.
Table 1 shows the comparison between our
baseline model (minimal GHKM on head-out bi-
narized parse trees) and different models of ad-
joining, measured with case-insensitive, NIST-
tokenized BLEU (IBM definition). The top section
(lines 1–4) compares the joint adjoining probabil-
ity model to the independent adjoining probabil-
ity model and seen vs. unseen adjoining combi-
nations. While the joint model results in a BLEU
score at the same level as our baseline (line 2),
the independent model (line 4) improves BLEU by
+0.5 and +0.6, which are significant differences
at the 95% confidence level. Since with the in-
dependent model we introduce both new adjoin-
ing patterns and a different probability model for
adjoining (each site is independent), we also use
the independent model with only previously seen
adjoining patterns (line 3). The insignificant dif-
ference in BLEU between lines 2 and 3 leads us
to think that the new adjoining patterns are where
the improvement comes from, rather than the in-
dependent probability model alone.
We also test several other features and combi-
nations. First, we add binary features to indicate
a new adjoining combination vs. one previously
</bodyText>
<footnote confidence="0.909102">
7The 5-gram LM was trained on 2 billion words of auto-
matically selected collections taken from the NIST 08 allow-
able data.
</footnote>
<bodyText confidence="0.999107">
seen in data. We also add features to indicate the
direction class of adjoining to test if there is a sys-
tematic bias toward particular directions. These
features cause no significant difference in score
(line 5). We also add the joint-adjoining proba-
bility as a feature, allowing it to be combined in a
log-linear fashion with the independent probabil-
ity (line 6). This results in our best BLEU gain:
+0.7 and +0.8 over our non-adjoining baseline.
</bodyText>
<sectionHeader confidence="0.998907" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999966416666667">
We have presented a novel method for learning
the rules and probabilities for a new statistical,
linguistically-informed, syntax-based MT model
that allows for adjoining. We have described a
method to translate using this model. And we have
demonstrated that linguistically-motivated adjoin-
ing improves the end-to-end MT results.
There are many potential directions for research
to proceed. One possibility is to investigate other
methods of making the required vs. optional de-
cision, either using linguistic resources such as
COMLEX or automatically learning the distinc-
tion using EM (as done for tree binarization by
Wang et al. (2007)). In addition, most ideas pre-
sented here are extendable to rules with linguistic
trees on both sides (using insights from Lavie et
al. (2008)). Also worth investigating is the direct
integration of bilingual dictionaries into the gram-
mar (as suggested by Shieber (2007)). Lastly, rule
composition and different amounts of lexicaliza-
tion (Galley et al., 2006; Marcu et al., 2006; De-
Neefe et al., 2007) or context modeling (Mari÷no et
al., 2006) have been successful with other mod-
els.
</bodyText>
<sectionHeader confidence="0.998236" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998940333333333">
We thank David Chiang for suggestions about
adjoining models, Michael Pust and Jens-S¬onke
V¬ockler for developing parts of the experimen-
tal framework, and other colleagues at ISI for
their helpful input. We also thank the anony-
mous reviewers for insightful comments and sug-
gestions. This research is financially supported
under DARPA Contract No. HR0011-06-C-0022,
BBN subcontract 9500008412.
</bodyText>
<sectionHeader confidence="0.99914" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996821333333333">
Anne Abeille, Yves Schabes, and Aravind K. Joshi.
1990. Using lexicalized TAGs for machine trans-
lation. In Proc. COLING, volume 3.
</reference>
<page confidence="0.98107">
735
</page>
<reference confidence="0.999787768115942">
David Chiang. 2003. Statistical parsing with an auto-
matically extracted tree adjoining grammar. Data-
Oriented Parsing.
Alexander Fraser and Daniel Marcu. 2007. Getting the
structure right for word alignment: LEAF. In Proc.
EMNLP-CoNLL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Lin-
guistics, 29(4).
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proc. SSST.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proc. HLT-NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
ACL.
Aravind K. Joshi, L. S. Levy, and M. Takahashi. 1975.
Tree adjunct grammars. Journal of Computer and
System Sciences, 10(1).
Aravind K. Joshi. 1985. How much context-
sensitivity is necessary for characterizing structural
descriptions—tree adjoining grammars. Natural
Language Processing—Theoretical, Computational,
and Psychological Perspectives.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional Linguistics,19(2).
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactiÞed target language phrases.
In Proc. EMNLP.
Jos«e B. Mari÷no, Rafael E. Banchs, Josep M. Crego,
Adri`a de Gispert, Patrik Lambert, Jos«e A. R. Fonol-
losa, and Marta R. Costa-juss`a. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4).
Rebecca Nesson, Stuart M. Shieber, and Alexander
Rush. 2006. Induction of probabilistic synchronous
tree-insertion grammars for machine translation. In
Proc. AMTA.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn
from phrase-based MT? In Proc. EMNLP-CoNLL.
Yves Schabes and Richard C. Waters. 1995. Tree
insertion grammar: A cubic-time, parsable formal-
ism that lexicalizes context-free grammar without
changing the trees produced. Computational Lin-
guistics, 21(4).
Stuart M. Shieber and Yves Schabes. 1990. Syn-
chronous tree-adjoining grammars. In Proc. COL-
ING.
Kumar Vijay-Shanker. 1987. A study of tree adjoining
grammars. Ph.D. thesis.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based ma-
chine translation accuracy. In Proc. EMNLP and
CoNLL.
Stuart M. Shieber. 2007. Probabilistic synchronous
tree-adjoining grammars for machine translation:
The argument from bilingual dictionaries. In Proc.
SSST Wkshp., NAACL-HLT.
</reference>
<page confidence="0.998546">
736
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.883102">
<title confidence="0.999833">Synchronous Tree Adjoining Machine Translation</title>
<author confidence="0.973149">DeNeefe Knight</author>
<affiliation confidence="0.999967">USC Information Sciences Institute</affiliation>
<address confidence="0.993743">4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 USA</address>
<abstract confidence="0.996165043478261">Tree Adjoining Grammars have well-known advantages, but are typically considered too for practical systems. We demonstrate that, when done right, adjoining improves translation quality without becoming computationally intractable. Using adjoining to model optionality allows general translation patterns to be learned without the clutter of endless variations of optional material. The can later be spliced in as needed. In this paper, we describe a novel method for learning a type of Synchronous Tree Adjoining Grammar and associated probabilities from aligned tree/string training data. We introduce a method of converting these grammars to a weakly equivalent tree transducer for decoding. Finally, we show that adjoining results in an end-to-end improvement of +0.8 a baseline statistical syntax-based MT model on a large-scale Arabic/English MT task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne Abeille</author>
<author>Yves Schabes</author>
<author>Aravind K Joshi</author>
</authors>
<title>Using lexicalized TAGs for machine translation.</title>
<date>1990</date>
<booktitle>In Proc. COLING,</booktitle>
<volume>3</volume>
<contexts>
<context position="1880" citStr="Abeille et al., 1990" startWordPosition="270" endWordPosition="273">ned statistical models, from word-based models to phrase-based models, and from string-based models to tree-based models. Recently there is a swing back to incorporating more linguistic information again, but this time linguistic insight carefully guides the setup of empirically learned models. Shieber (2007) recently argued that probabilistic Synchronous Tree Adjoining Grammars (Shieber and Schabes, 1990) have the right combination of properties that satisfy both linguists and empirical MT practitioners. So far, though, most work in this area has been either more linguistic than statistical (Abeille et al., 1990) or statistically-based, but linguistically light (Nesson et al., 2006). Current tree-based models that integrate linguistics and statistics, such as GE KM (Galley et al., 2004), are not able to generalize well from a single phrase pair. For example, from the data in Figure 1, GE KM can learn rule (a) to translate nouns with two pre-modifiers, but does not generalize to learn translation rules (b) - (d) without the optional adjective or noun modifiers. Likewise, none of these rules allow extra material to be introduced, e.g. “Pakistan’s national defense minister”. In large enough training data</context>
<context position="5445" citStr="Abeille et al., 1990" startWordPosition="848" endWordPosition="851">sually they also include a substitution operation, which has a ‘fill in the blank’ semantics, replacing a substitution leaf node with a tree. Figure 2 visually demonstrates TAG operations. Shieber and Schabes (1990) offer a synchronous version of TAG (STAG), allowing the construction of a pair of trees in lockstep fashion using the TAG operations of substitution and adjunction on tree pairs. To facilitate this synchronous behavior, links between pairs of nodes in each tree pair define the possible sites for substitution and adjunction to happen. One application of STAG is machine translation (Abeille et al., 1990). One negative aspect of TAG is the computational complexity: O(n6) time is required for monolingual parsing (and thus decoding), and STAG requires O(n12) for bilingual parsing (which might be used for training the model directly on bilingual data). Tree Insertion Grammars (TIG) are a restricted form of TAG that was introduced (Schabes and Waters, 1995) to keep the same benefits as TAG (adjoining of unbounded material) without the computational complexity— TIG parsing is O(n3). This reduction is due to a limitation on adjoining: auxiliary trees can only introduce tree material to the left or t</context>
</contexts>
<marker>Abeille, Schabes, Joshi, 1990</marker>
<rawString>Anne Abeille, Yves Schabes, and Aravind K. Joshi. 1990. Using lexicalized TAGs for machine translation. In Proc. COLING, volume 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Statistical parsing with an automatically extracted tree adjoining grammar. DataOriented Parsing.</title>
<date>2003</date>
<marker>Chiang, 2003</marker>
<rawString>David Chiang. 2003. Statistical parsing with an automatically extracted tree adjoining grammar. DataOriented Parsing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Getting the structure right for word alignment: LEAF.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP-CoNLL.</booktitle>
<contexts>
<context position="22189" citStr="Fraser and Marcu, 2007" startWordPosition="3696" endWordPosition="3699">ion site, then remove epsilon-only branches. For k adjunction sites this could possibly results in 2k rules. But as discussed previously (at the end of Section 3.2), we only allow subsets of adjoining combinations seen in training data, so this number is substantially lower for large values of k. 5 Experiments All experiments are trained with a subset (171,000 sentences or 4 million words) of the ArabicEnglish training data from the constrained data track of the NIST 2008 MT Evaluation, leaving out LDC2004T18, LDC2007E07, and the UN data. The training data is aligned using the LEAF technique (Fraser and Marcu, 2007). The English side of the training data is parsed with an implementation of Collins Model 2 (Collins, 2003) then head-out binarized. The tuning data (1,178 sentences) and devtest data (1,298 sentences) are 733 made up of newswire documents drawn from the NIST MT evaluation data from 2004, 2005, and 2006 (GALE part). We use the newswire documents from the NIST part of the 2006 evaluation data (765 sentences) as a held-out test set. We train our feature weights using max-BLEU (Och, 2003) and decode with a CKY-based decoder that supports language model scoring directly integrated into the search.</context>
</contexts>
<marker>Fraser, Marcu, 2007</marker>
<rawString>Alexander Fraser and Daniel Marcu. 2007. Getting the structure right for word alignment: LEAF. In Proc. EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="22679" citStr="Och, 2003" startWordPosition="3782" endWordPosition="3783"> LDC2004T18, LDC2007E07, and the UN data. The training data is aligned using the LEAF technique (Fraser and Marcu, 2007). The English side of the training data is parsed with an implementation of Collins Model 2 (Collins, 2003) then head-out binarized. The tuning data (1,178 sentences) and devtest data (1,298 sentences) are 733 made up of newswire documents drawn from the NIST MT evaluation data from 2004, 2005, and 2006 (GALE part). We use the newswire documents from the NIST part of the 2006 evaluation data (765 sentences) as a held-out test set. We train our feature weights using max-BLEU (Och, 2003) and decode with a CKY-based decoder that supports language model scoring directly integrated into the search. In addition to Pub, Padj, and Pifadj, we use several other features in our log-linear model during decoding, including: lexical and phrase-based translation probabilities, a model similar to conditional probability on the trees (P(ftree(rule)|etree(rule))), a probability model for generating the top tree non-terminal, a 5-gram language model7, and target length bonus. We also have several binary features—lexical rule, rule with missing or spurious content words—and several binary indi</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="22296" citStr="Collins, 2003" startWordPosition="3717" endWordPosition="3718">discussed previously (at the end of Section 3.2), we only allow subsets of adjoining combinations seen in training data, so this number is substantially lower for large values of k. 5 Experiments All experiments are trained with a subset (171,000 sentences or 4 million words) of the ArabicEnglish training data from the constrained data track of the NIST 2008 MT Evaluation, leaving out LDC2004T18, LDC2007E07, and the UN data. The training data is aligned using the LEAF technique (Fraser and Marcu, 2007). The English side of the training data is parsed with an implementation of Collins Model 2 (Collins, 2003) then head-out binarized. The tuning data (1,178 sentences) and devtest data (1,298 sentences) are 733 made up of newswire documents drawn from the NIST MT evaluation data from 2004, 2005, and 2006 (GALE part). We use the newswire documents from the NIST part of the 2006 evaluation data (765 sentences) as a held-out test set. We train our feature weights using max-BLEU (Och, 2003) and decode with a CKY-based decoder that supports language model scoring directly integrated into the search. In addition to Pub, Padj, and Pifadj, we use several other features in our log-linear model during decodin</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Alok Parlikar</author>
<author>Vamshi Ambati</author>
</authors>
<title>Syntax-driven learning of sub-sentential translation equivalents and translation rules from parsed parallel corpora.</title>
<date>2008</date>
<booktitle>In Proc. SSST.</booktitle>
<contexts>
<context position="26021" citStr="Lavie et al. (2008)" startWordPosition="4305" endWordPosition="4308">adjoining. We have described a method to translate using this model. And we have demonstrated that linguistically-motivated adjoining improves the end-to-end MT results. There are many potential directions for research to proceed. One possibility is to investigate other methods of making the required vs. optional decision, either using linguistic resources such as COMLEX or automatically learning the distinction using EM (as done for tree binarization by Wang et al. (2007)). In addition, most ideas presented here are extendable to rules with linguistic trees on both sides (using insights from Lavie et al. (2008)). Also worth investigating is the direct integration of bilingual dictionaries into the grammar (as suggested by Shieber (2007)). Lastly, rule composition and different amounts of lexicalization (Galley et al., 2006; Marcu et al., 2006; DeNeefe et al., 2007) or context modeling (Mari÷no et al., 2006) have been successful with other models. Acknowledgments We thank David Chiang for suggestions about adjoining models, Michael Pust and Jens-S¬onke V¬ockler for developing parts of the experimental framework, and other colleagues at ISI for their helpful input. We also thank the anonymous reviewer</context>
</contexts>
<marker>Lavie, Parlikar, Ambati, 2008</marker>
<rawString>Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008. Syntax-driven learning of sub-sentential translation equivalents and translation rules from parsed parallel corpora. In Proc. SSST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule? In</title>
<date>2004</date>
<booktitle>Proc. HLT-NAACL.</booktitle>
<contexts>
<context position="2057" citStr="Galley et al., 2004" startWordPosition="296" endWordPosition="299">guistic information again, but this time linguistic insight carefully guides the setup of empirically learned models. Shieber (2007) recently argued that probabilistic Synchronous Tree Adjoining Grammars (Shieber and Schabes, 1990) have the right combination of properties that satisfy both linguists and empirical MT practitioners. So far, though, most work in this area has been either more linguistic than statistical (Abeille et al., 1990) or statistically-based, but linguistically light (Nesson et al., 2006). Current tree-based models that integrate linguistics and statistics, such as GE KM (Galley et al., 2004), are not able to generalize well from a single phrase pair. For example, from the data in Figure 1, GE KM can learn rule (a) to translate nouns with two pre-modifiers, but does not generalize to learn translation rules (b) - (d) without the optional adjective or noun modifiers. Likewise, none of these rules allow extra material to be introduced, e.g. “Pakistan’s national defense minister”. In large enough training data sets, we see many examples of all the common patterns, but the rarer patterns have sparse statistics or poor coverage. To mitigate this problem, the parse trees used as trainin</context>
<context position="12393" citStr="Galley et al. (2004)" startWordPosition="2013" endWordPosition="2016">junctions. In both examples, the ‘ABC’ child is the head, while the other children are optional. NT1 NT1 NT1 NT1 ABC NT1 NT3 �� �� NT2 NT1 XYZ NT2 NT1 XYZ ABC DEF NT1* NT3 DEF ABC NT1 NT1 NT2 NT1* ABC NT1 NT1* NT2 XYZ adjoined to the previous child, as in Figure 4(b). C. Extracting rules and derivation trees. We now have a TIG derivation tree, with each elementary tree attached to its parent by a substitution or adjunction link. We can now extract synchronous rules allowed by the alignments and syntactic constituents. This can be done using a method inspired by the rule-extraction approach of Galley et al. (2004), but instead of directly operating on the parse tree we process the English TIG derivation tree. In bottom-up fashion, we visit each elementary tree in the derivation, allowing a rule rooted at this tree to be extracted if its words or those of its descendants are aligned such that they are the English side of a self-contained parallel phrase (i.e., the foreign text of this phrase is not aligned to English leaves outside of the set of descendants). Otherwise, this elementary tree is rejoined with its parent to form a larger elementary tree. At the end of this process we have a new set of link</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proc. HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proc. ACL.</booktitle>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>L S Levy</author>
<author>M Takahashi</author>
</authors>
<title>Tree adjunct grammars.</title>
<date>1975</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>10</volume>
<issue>1</issue>
<contexts>
<context position="4685" citStr="Joshi et al. (1975)" startWordPosition="725" endWordPosition="728">resources to make the required vs. optional decision. We then directly model the optionality in the translation rules by learning statistics for the required parts of the rule independently from the optional parts. We also present a method of converting these rules into a well-studied tree transducer formalism for decoding purposes. We then show that modeling optionality using adjoining results in a statistically significant BLEU gain over our baseline syntax-based model with no adjoining. 2 Translation Model 2.1 Synchronous Tree Insertion Grammars Tree Adjoining Grammars (TAG), introduced by Joshi et al. (1975) and Joshi (1985), allow insertion of unbounded amounts of material into the structure of an existing tree using an adjunction operation. Usually they also include a substitution operation, which has a ‘fill in the blank’ semantics, replacing a substitution leaf node with a tree. Figure 2 visually demonstrates TAG operations. Shieber and Schabes (1990) offer a synchronous version of TAG (STAG), allowing the construction of a pair of trees in lockstep fashion using the TAG operations of substitution and adjunction on tree pairs. To facilitate this synchronous behavior, links between pairs of no</context>
</contexts>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>Aravind K. Joshi, L. S. Levy, and M. Takahashi. 1975. Tree adjunct grammars. Journal of Computer and System Sciences, 10(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>How much contextsensitivity is necessary for characterizing structural descriptions—tree adjoining grammars. Natural Language Processing—Theoretical, Computational, and Psychological Perspectives.</title>
<date>1985</date>
<contexts>
<context position="4702" citStr="Joshi (1985)" startWordPosition="730" endWordPosition="731">quired vs. optional decision. We then directly model the optionality in the translation rules by learning statistics for the required parts of the rule independently from the optional parts. We also present a method of converting these rules into a well-studied tree transducer formalism for decoding purposes. We then show that modeling optionality using adjoining results in a statistically significant BLEU gain over our baseline syntax-based model with no adjoining. 2 Translation Model 2.1 Synchronous Tree Insertion Grammars Tree Adjoining Grammars (TAG), introduced by Joshi et al. (1975) and Joshi (1985), allow insertion of unbounded amounts of material into the structure of an existing tree using an adjunction operation. Usually they also include a substitution operation, which has a ‘fill in the blank’ semantics, replacing a substitution leaf node with a tree. Figure 2 visually demonstrates TAG operations. Shieber and Schabes (1990) offer a synchronous version of TAG (STAG), allowing the construction of a pair of trees in lockstep fashion using the TAG operations of substitution and adjunction on tree pairs. To facilitate this synchronous behavior, links between pairs of nodes in each tree </context>
</contexts>
<marker>Joshi, 1985</marker>
<rawString>Aravind K. Joshi. 1985. How much contextsensitivity is necessary for characterizing structural descriptions—tree adjoining grammars. Natural Language Processing—Theoretical, Computational, and Psychological Perspectives.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn treebank.</title>
<date>1993</date>
<journal>Computational</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="7133" citStr="Marcus et al., 1993" startWordPosition="1125" endWordPosition="1129">TAG, adjunctions are permitted at any non-terminal with the same label as the root and foot node of the auxiliary tree, while in STAG adjunctions are restricted to linked sites. Nesson et al. (2006) introduce a probabilistic, synchronous variant of TIG and demonstrate its use for machine translation, showing results that beat both word-based and phrase-based MT models on a limited-vocabulary, small-scale training and test set. Training the model uses an O(n6) bilingual parsing algorithm, and decoding is O(n3). Though this model uses trees in the formal sense, it does not create Penn Treebank (Marcus et al., 1993) style linguistic trees, but uses only one non-terminal label (X) to create those trees using six simple rule structures. The grammars we use in this paper share some properties in common with those of Nesson et al. (2006) in that they are of the probabilistic, synchronous tree-insertion variety. All pairs of sites (both adjunction and substitution in our case) are explicitly linked. Adjunction sites are restricted by direction: at each linked site, the source and target side each specify one allowed direction. The result is that each synchronous adjunction site can be classified into one of f</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics,19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactiÞed target language phrases.</title>
<date>2006</date>
<booktitle>In Proc. EMNLP.</booktitle>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical machine translation with syntactiÞed target language phrases. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos«e B Mari÷no</author>
<author>Rafael E Banchs</author>
<author>Josep M Crego</author>
<author>Adri`a de Gispert</author>
<author>Patrik Lambert</author>
<author>Jos«e A R Fonollosa</author>
<author>Marta R Costa-juss`a</author>
</authors>
<title>N-grambased machine translation.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<marker>Mari÷no, Banchs, Crego, de Gispert, Lambert, Fonollosa, Costa-juss`a, 2006</marker>
<rawString>Jos«e B. Mari÷no, Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert, Patrik Lambert, Jos«e A. R. Fonollosa, and Marta R. Costa-juss`a. 2006. N-grambased machine translation. Computational Linguistics, 32(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Nesson</author>
<author>Stuart M Shieber</author>
<author>Alexander Rush</author>
</authors>
<title>Induction of probabilistic synchronous tree-insertion grammars for machine translation. In</title>
<date>2006</date>
<booktitle>Proc. AMTA.</booktitle>
<contexts>
<context position="1951" citStr="Nesson et al., 2006" startWordPosition="279" endWordPosition="282">nd from string-based models to tree-based models. Recently there is a swing back to incorporating more linguistic information again, but this time linguistic insight carefully guides the setup of empirically learned models. Shieber (2007) recently argued that probabilistic Synchronous Tree Adjoining Grammars (Shieber and Schabes, 1990) have the right combination of properties that satisfy both linguists and empirical MT practitioners. So far, though, most work in this area has been either more linguistic than statistical (Abeille et al., 1990) or statistically-based, but linguistically light (Nesson et al., 2006). Current tree-based models that integrate linguistics and statistics, such as GE KM (Galley et al., 2004), are not able to generalize well from a single phrase pair. For example, from the data in Figure 1, GE KM can learn rule (a) to translate nouns with two pre-modifiers, but does not generalize to learn translation rules (b) - (d) without the optional adjective or noun modifiers. Likewise, none of these rules allow extra material to be introduced, e.g. “Pakistan’s national defense minister”. In large enough training data sets, we see many examples of all the common patterns, but the rarer p</context>
<context position="6711" citStr="Nesson et al. (2006)" startWordPosition="1058" endWordPosition="1061">liary tree can be classified by direction as left or right adjoining. the NN minister Figure 2: TAG grammars use substitution and adjunction operations to construct trees. Substitution replaces the substitution node (marked with 1) with another tree. Adjunction inserts an auxiliary tree—a special kind of tree fragment with a foot node (marked with *)—into an existing tree at a permitted non-terminal node. Note that in TAG, adjunctions are permitted at any non-terminal with the same label as the root and foot node of the auxiliary tree, while in STAG adjunctions are restricted to linked sites. Nesson et al. (2006) introduce a probabilistic, synchronous variant of TIG and demonstrate its use for machine translation, showing results that beat both word-based and phrase-based MT models on a limited-vocabulary, small-scale training and test set. Training the model uses an O(n6) bilingual parsing algorithm, and decoding is O(n3). Though this model uses trees in the formal sense, it does not create Penn Treebank (Marcus et al., 1993) style linguistic trees, but uses only one non-terminal label (X) to create those trees using six simple rule structures. The grammars we use in this paper share some properties </context>
<context position="8007" citStr="Nesson et al. (2006)" startWordPosition="1271" endWordPosition="1274">listic, synchronous tree-insertion variety. All pairs of sites (both adjunction and substitution in our case) are explicitly linked. Adjunction sites are restricted by direction: at each linked site, the source and target side each specify one allowed direction. The result is that each synchronous adjunction site can be classified into one of four direction classes: {LR, LL, RR, RL}. For example, LR means the source side site only allows left adjoining trees and the target side site only allows right adjoining trees. There are several important differences between our grammars and the ones of Nesson et al. (2006): Richer, Linguistic Trees: Our grammars have a the NN1 NP adjunction NP substitution substitution NP DT JJ1 NP* NN JJ minister defense �� NP DT NP JJ defense NP 728 Penn Treebank-style linguistic tree on the English (target) side, and a hierarchical structure using only a single non-terminal symbol (X) on the source side. We believe this provides the rich information needed in the target language without over-constraining the model. Substitution Sites/Non-lexical trees: We use both substitution and adjunction (Nesson et al. (2006) only used adjunction) and do not require all trees to contain </context>
</contexts>
<marker>Nesson, Shieber, Rush, 2006</marker>
<rawString>Rebecca Nesson, Stuart M. Shieber, and Alexander Rush. 2006. Induction of probabilistic synchronous tree-insertion grammars for machine translation. In Proc. AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve DeNeefe</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
<author>Daniel Marcu</author>
</authors>
<title>What can syntax-based MT learn from phrase-based MT? In</title>
<date>2007</date>
<booktitle>Proc. EMNLP-CoNLL.</booktitle>
<contexts>
<context position="2889" citStr="DeNeefe et al., 2007" startWordPosition="433" endWordPosition="436">on rules (b) - (d) without the optional adjective or noun modifiers. Likewise, none of these rules allow extra material to be introduced, e.g. “Pakistan’s national defense minister”. In large enough training data sets, we see many examples of all the common patterns, but the rarer patterns have sparse statistics or poor coverage. To mitigate this problem, the parse trees used as training data for these systems can be binarized (Wang et al., 2007). Binarization allows rules with partial constituents to be learned, resulting in more general rules, richer statistics, and better phrasal coverage (DeNeefe et al., 2007), but no principled required vs. optional decision has been made. This method’s key weakness is that binarization always keeps adjacent siblings together, so there is no way to group the head with a required complement if optional information intervenes between the two. Furthermore, if all kinds of children are considered equally optional, then we have removed important syntactic constraints, which may end up permitting too much freedom. In addition, spurious alignments may limit the binarization techH NN3 NN2 JJ1 NP (a) NN NN defense JJ national minister wzyr AldfAE AlwTnY JJ1 NN2 NN3 NP NN1 </context>
</contexts>
<marker>DeNeefe, Knight, Wang, Marcu, 2007</marker>
<rawString>Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel Marcu. 2007. What can syntax-based MT learn from phrase-based MT? In Proc. EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Richard C Waters</author>
</authors>
<title>Tree insertion grammar: A cubic-time, parsable formalism that lexicalizes context-free grammar without changing the trees produced.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="5800" citStr="Schabes and Waters, 1995" startWordPosition="906" endWordPosition="909"> substitution and adjunction on tree pairs. To facilitate this synchronous behavior, links between pairs of nodes in each tree pair define the possible sites for substitution and adjunction to happen. One application of STAG is machine translation (Abeille et al., 1990). One negative aspect of TAG is the computational complexity: O(n6) time is required for monolingual parsing (and thus decoding), and STAG requires O(n12) for bilingual parsing (which might be used for training the model directly on bilingual data). Tree Insertion Grammars (TIG) are a restricted form of TAG that was introduced (Schabes and Waters, 1995) to keep the same benefits as TAG (adjoining of unbounded material) without the computational complexity— TIG parsing is O(n3). This reduction is due to a limitation on adjoining: auxiliary trees can only introduce tree material to the left or the right of the node adjoined to. Thus an auxiliary tree can be classified by direction as left or right adjoining. the NN minister Figure 2: TAG grammars use substitution and adjunction operations to construct trees. Substitution replaces the substitution node (marked with 1) with another tree. Adjunction inserts an auxiliary tree—a special kind of tre</context>
<context position="8674" citStr="Schabes and Waters, 1995" startWordPosition="1379" endWordPosition="1382">ave a the NN1 NP adjunction NP substitution substitution NP DT JJ1 NP* NN JJ minister defense �� NP DT NP JJ defense NP 728 Penn Treebank-style linguistic tree on the English (target) side, and a hierarchical structure using only a single non-terminal symbol (X) on the source side. We believe this provides the rich information needed in the target language without over-constraining the model. Substitution Sites/Non-lexical trees: We use both substitution and adjunction (Nesson et al. (2006) only used adjunction) and do not require all trees to contain lexical items as is commonly done in TIG (Schabes and Waters, 1995). Single Adjunction/Multiple Sites: Each nonterminal node in a tree may allow multiple adjunction sites, but every site only allows at most one adjunction,1 a common assumption for TAG as specified in the Vijay-Shanker (1987) definition. Here are some examples of automatically learned translation rules with interpretations of how they work: 1. simple lexical rules for translating words or phrases: IN �� without interpretation: translate the Arabic word “AlA” as the preposition “without” 2. rules with substitution for translating phrases with holes (substitution sites are designated by an arrow</context>
</contexts>
<marker>Schabes, Waters, 1995</marker>
<rawString>Yves Schabes and Richard C. Waters. 1995. Tree insertion grammar: A cubic-time, parsable formalism that lexicalizes context-free grammar without changing the trees produced. Computational Linguistics, 21(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
</authors>
<title>Synchronous tree-adjoining grammars.</title>
<date>1990</date>
<booktitle>In Proc. COLING.</booktitle>
<contexts>
<context position="1668" citStr="Shieber and Schabes, 1990" startWordPosition="235" endWordPosition="238">l syntax-based MT model on a large-scale Arabic/English MT task. 1 Introduction Statistical MT has changed a lot in recent years. We have seen quick progress from manually crafted linguistic models to empirically learned statistical models, from word-based models to phrase-based models, and from string-based models to tree-based models. Recently there is a swing back to incorporating more linguistic information again, but this time linguistic insight carefully guides the setup of empirically learned models. Shieber (2007) recently argued that probabilistic Synchronous Tree Adjoining Grammars (Shieber and Schabes, 1990) have the right combination of properties that satisfy both linguists and empirical MT practitioners. So far, though, most work in this area has been either more linguistic than statistical (Abeille et al., 1990) or statistically-based, but linguistically light (Nesson et al., 2006). Current tree-based models that integrate linguistics and statistics, such as GE KM (Galley et al., 2004), are not able to generalize well from a single phrase pair. For example, from the data in Figure 1, GE KM can learn rule (a) to translate nouns with two pre-modifiers, but does not generalize to learn translati</context>
<context position="5039" citStr="Shieber and Schabes (1990)" startWordPosition="781" endWordPosition="784"> that modeling optionality using adjoining results in a statistically significant BLEU gain over our baseline syntax-based model with no adjoining. 2 Translation Model 2.1 Synchronous Tree Insertion Grammars Tree Adjoining Grammars (TAG), introduced by Joshi et al. (1975) and Joshi (1985), allow insertion of unbounded amounts of material into the structure of an existing tree using an adjunction operation. Usually they also include a substitution operation, which has a ‘fill in the blank’ semantics, replacing a substitution leaf node with a tree. Figure 2 visually demonstrates TAG operations. Shieber and Schabes (1990) offer a synchronous version of TAG (STAG), allowing the construction of a pair of trees in lockstep fashion using the TAG operations of substitution and adjunction on tree pairs. To facilitate this synchronous behavior, links between pairs of nodes in each tree pair define the possible sites for substitution and adjunction to happen. One application of STAG is machine translation (Abeille et al., 1990). One negative aspect of TAG is the computational complexity: O(n6) time is required for monolingual parsing (and thus decoding), and STAG requires O(n12) for bilingual parsing (which might be u</context>
</contexts>
<marker>Shieber, Schabes, 1990</marker>
<rawString>Stuart M. Shieber and Yves Schabes. 1990. Synchronous tree-adjoining grammars. In Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kumar Vijay-Shanker</author>
</authors>
<title>A study of tree adjoining grammars.</title>
<date>1987</date>
<tech>Ph.D. thesis.</tech>
<contexts>
<context position="8899" citStr="Vijay-Shanker (1987)" startWordPosition="1416" endWordPosition="1417">a single non-terminal symbol (X) on the source side. We believe this provides the rich information needed in the target language without over-constraining the model. Substitution Sites/Non-lexical trees: We use both substitution and adjunction (Nesson et al. (2006) only used adjunction) and do not require all trees to contain lexical items as is commonly done in TIG (Schabes and Waters, 1995). Single Adjunction/Multiple Sites: Each nonterminal node in a tree may allow multiple adjunction sites, but every site only allows at most one adjunction,1 a common assumption for TAG as specified in the Vijay-Shanker (1987) definition. Here are some examples of automatically learned translation rules with interpretations of how they work: 1. simple lexical rules for translating words or phrases: IN �� without interpretation: translate the Arabic word “AlA” as the preposition “without” 2. rules with substitution for translating phrases with holes (substitution sites are designated by an arrow and numeric subscript, e.g. NP11): PP PP NP11 X �� IN X11 of interpretation: insert “of” to turn a noun phrase into a prepositional phrase 3. simple adjoining rules for inserting optional modifiers (adjoining sites are desig</context>
</contexts>
<marker>Vijay-Shanker, 1987</marker>
<rawString>Kumar Vijay-Shanker. 1987. A study of tree adjoining grammars. Ph.D. thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Binarizing syntax trees to improve syntax-based machine translation accuracy.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP and CoNLL.</booktitle>
<contexts>
<context position="2718" citStr="Wang et al., 2007" startWordPosition="409" endWordPosition="412">ngle phrase pair. For example, from the data in Figure 1, GE KM can learn rule (a) to translate nouns with two pre-modifiers, but does not generalize to learn translation rules (b) - (d) without the optional adjective or noun modifiers. Likewise, none of these rules allow extra material to be introduced, e.g. “Pakistan’s national defense minister”. In large enough training data sets, we see many examples of all the common patterns, but the rarer patterns have sparse statistics or poor coverage. To mitigate this problem, the parse trees used as training data for these systems can be binarized (Wang et al., 2007). Binarization allows rules with partial constituents to be learned, resulting in more general rules, richer statistics, and better phrasal coverage (DeNeefe et al., 2007), but no principled required vs. optional decision has been made. This method’s key weakness is that binarization always keeps adjacent siblings together, so there is no way to group the head with a required complement if optional information intervenes between the two. Furthermore, if all kinds of children are considered equally optional, then we have removed important syntactic constraints, which may end up permitting too m</context>
<context position="25879" citStr="Wang et al. (2007)" startWordPosition="4281" endWordPosition="4284"> novel method for learning the rules and probabilities for a new statistical, linguistically-informed, syntax-based MT model that allows for adjoining. We have described a method to translate using this model. And we have demonstrated that linguistically-motivated adjoining improves the end-to-end MT results. There are many potential directions for research to proceed. One possibility is to investigate other methods of making the required vs. optional decision, either using linguistic resources such as COMLEX or automatically learning the distinction using EM (as done for tree binarization by Wang et al. (2007)). In addition, most ideas presented here are extendable to rules with linguistic trees on both sides (using insights from Lavie et al. (2008)). Also worth investigating is the direct integration of bilingual dictionaries into the grammar (as suggested by Shieber (2007)). Lastly, rule composition and different amounts of lexicalization (Galley et al., 2006; Marcu et al., 2006; DeNeefe et al., 2007) or context modeling (Mari÷no et al., 2006) have been successful with other models. Acknowledgments We thank David Chiang for suggestions about adjoining models, Michael Pust and Jens-S¬onke V¬ockler</context>
</contexts>
<marker>Wang, Knight, Marcu, 2007</marker>
<rawString>Wei Wang, Kevin Knight, and Daniel Marcu. 2007. Binarizing syntax trees to improve syntax-based machine translation accuracy. In Proc. EMNLP and CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Probabilistic synchronous tree-adjoining grammars for machine translation: The argument from bilingual dictionaries.</title>
<date>2007</date>
<booktitle>In Proc. SSST Wkshp.,</booktitle>
<location>NAACL-HLT.</location>
<contexts>
<context position="1569" citStr="Shieber (2007)" startWordPosition="224" endWordPosition="225"> adjoining results in an end-to-end improvement of +0.8 BLEU over a baseline statistical syntax-based MT model on a large-scale Arabic/English MT task. 1 Introduction Statistical MT has changed a lot in recent years. We have seen quick progress from manually crafted linguistic models to empirically learned statistical models, from word-based models to phrase-based models, and from string-based models to tree-based models. Recently there is a swing back to incorporating more linguistic information again, but this time linguistic insight carefully guides the setup of empirically learned models. Shieber (2007) recently argued that probabilistic Synchronous Tree Adjoining Grammars (Shieber and Schabes, 1990) have the right combination of properties that satisfy both linguists and empirical MT practitioners. So far, though, most work in this area has been either more linguistic than statistical (Abeille et al., 1990) or statistically-based, but linguistically light (Nesson et al., 2006). Current tree-based models that integrate linguistics and statistics, such as GE KM (Galley et al., 2004), are not able to generalize well from a single phrase pair. For example, from the data in Figure 1, GE KM can l</context>
<context position="26149" citStr="Shieber (2007)" startWordPosition="4326" endWordPosition="4327"> improves the end-to-end MT results. There are many potential directions for research to proceed. One possibility is to investigate other methods of making the required vs. optional decision, either using linguistic resources such as COMLEX or automatically learning the distinction using EM (as done for tree binarization by Wang et al. (2007)). In addition, most ideas presented here are extendable to rules with linguistic trees on both sides (using insights from Lavie et al. (2008)). Also worth investigating is the direct integration of bilingual dictionaries into the grammar (as suggested by Shieber (2007)). Lastly, rule composition and different amounts of lexicalization (Galley et al., 2006; Marcu et al., 2006; DeNeefe et al., 2007) or context modeling (Mari÷no et al., 2006) have been successful with other models. Acknowledgments We thank David Chiang for suggestions about adjoining models, Michael Pust and Jens-S¬onke V¬ockler for developing parts of the experimental framework, and other colleagues at ISI for their helpful input. We also thank the anonymous reviewers for insightful comments and suggestions. This research is financially supported under DARPA Contract No. HR0011-06-C-0022, BBN</context>
</contexts>
<marker>Shieber, 2007</marker>
<rawString>Stuart M. Shieber. 2007. Probabilistic synchronous tree-adjoining grammars for machine translation: The argument from bilingual dictionaries. In Proc. SSST Wkshp., NAACL-HLT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>