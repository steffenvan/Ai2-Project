<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000014">
<note confidence="0.9521015">
Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 489-496.
</note>
<title confidence="0.998286">
Ranking Algorithms for Named–Entity Extraction:
Boosting and the Voted Perceptron
</title>
<author confidence="0.71236">
Michael Collins
</author>
<affiliation confidence="0.43102">
AT&amp;T Labs-Research, Florham Park, New Jersey.
</affiliation>
<email confidence="0.995866">
mcollins@research.att.com
</email>
<sectionHeader confidence="0.99384" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999835714285714">
This paper describes algorithms which
rerank the top N hypotheses from a
maximum-entropy tagger, the applica-
tion being the recovery of named-entity
boundaries in a corpus of web data. The
first approach uses a boosting algorithm
for ranking problems. The second ap-
proach uses the voted perceptron algo-
rithm. Both algorithms give compara-
ble, significant improvements over the
maximum-entropy baseline. The voted
perceptron algorithm can be considerably
more efficient to train, at some cost in
computation on test examples.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999930309090909">
Recent work in statistical approaches to parsing and
tagging has begun to consider methods which in-
corporate global features of candidate structures.
Examples of such techniques are Markov Random
Fields (Abney 1997; Della Pietra et al. 1997; John-
son et al. 1999), and boosting algorithms (Freund et
al. 1998; Collins 2000; Walker et al. 2001). One
appeal of these methods is their flexibility in incor-
porating features into a model: essentially any fea-
tures which might be useful in discriminating good
from bad structures can be included. A second ap-
peal of these methods is that their training criterion
is often discriminative, attempting to explicitly push
the score or probability of the correct structure for
each training sentence above the score of competing
structures. This discriminative property is shared by
the methods of (Johnson et al. 1999; Collins 2000),
and also the Conditional Random Field methods of
(Lafferty et al. 2001).
In a previous paper (Collins 2000), a boosting al-
gorithm was used to rerank the output from an ex-
isting statistical parser, giving significant improve-
ments in parsing accuracy on Wall Street Journal
data. Similar boosting algorithms have been applied
to natural language generation, with good results, in
(Walker et al. 2001). In this paper we apply rerank-
ing methods to named-entity extraction. A state-of-
the-art (maximum-entropy) tagger is used to gener-
ate 20 possible segmentations for each input sen-
tence, along with their probabilities. We describe
a number of additional global features of these can-
didate segmentations. These additional features are
used as evidence in reranking the hypotheses from
the max-ent tagger. We describe two learning algo-
rithms: the boosting method of (Collins 2000), and a
variant of the voted perceptron algorithm, which was
initially described in (Freund &amp; Schapire 1999). We
applied the methods to a corpus of over one million
words of tagged web data. The methods give signif-
icant improvements over the maximum-entropy tag-
ger (a 17.7% relative reduction in error-rate for the
voted perceptron, and a 15.6% relative improvement
for the boosting method).
One contribution of this paper is to show that ex-
isting reranking methods are useful for a new do-
main, named-entity tagging, and to suggest global
features which give improvements on this task. We
should stress that another contribution is to show
that a new algorithm, the voted perceptron, gives
very credible results on a natural language task. It is
an extremely simple algorithm to implement, and is
very fast to train (the testing phase is slower, but by
no means sluggish). It should be a viable alternative
to methods such as the boosting or Markov Random
Field algorithms described in previous work.
</bodyText>
<sectionHeader confidence="0.992089" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.912891">
2.1 The data
</subsectionHeader>
<bodyText confidence="0.999227045454546">
Over a period of a year or so we have had over one
million words of named-entity data annotated. The
data is drawn from web pages, the aim being to sup-
port a question-answering system over web data. A
number of categories are annotated: the usual peo-
ple, organization and location categories, as well as
less frequent categories such as brand-names, scien-
tific terms, event titles (such as concerts) and so on.
From this data we created a training set of 53,609
sentences (1,047,491 words), and a test set of 14,717
sentences (291,898 words).
The task we consider is to recover named-entity
boundaries. We leave the recovery of the categories
of entities to a separate stage of processing.1 We
evaluate different methods on the task through pre-
cision and recall. If a method proposes entities on
the test set, and of these are correct (i.e., an entity is
marked by the annotator with exactly the same span
as that proposed) then the precision of a method is
. Similarly, if is the total number of en-
tities in the human annotated version of the test set,
then the recall is .
</bodyText>
<subsectionHeader confidence="0.998946">
2.2 The baseline tagger
</subsectionHeader>
<bodyText confidence="0.999959772727273">
The problem can be framed as a tagging task – to
tag each word as being either the start of an entity,
a continuation of an entity, or not to be part of an
entity at all (we will use the tags S, C and N respec-
tively for these three cases). As a baseline model
we used a maximum entropy tagger, very similar to
the ones described in (Ratnaparkhi 1996; Borthwick
et. al 1998; McCallum et al. 2000). Max-ent tag-
gers have been shown to be highly competitive on a
number of tagging tasks, such as part-of-speech tag-
ging (Ratnaparkhi 1996), named-entity recognition
(Borthwick et. al 1998), and information extraction
tasks (McCallum et al. 2000). Thus the maximum-
entropy tagger we used represents a serious baseline
for the task. We used the following features (sev-
eral of the features were inspired by the approach
of (Bikel et. al 1999), an HMM model which gives
excellent results on named entity extraction):
The word being tagged, the previous word, and
the next word.
The previous tag, and the previous two tags (bi-
gram and trigram features).
</bodyText>
<footnote confidence="0.9122865">
1In initial experiments, we found that forcing the tagger to
recover categories as well as the segmentation, by exploding the
number of tags, reduced performance on the segmentation task,
presumably due to sparse data problems.
</footnote>
<bodyText confidence="0.996082657894737">
A compound feature of three fields: (a) Is the
word at the start of a sentence?; (b) does the word
occur in a list of words which occur more frequently
as lower case rather than upper case words in a large
corpus of text? (c) the type of the first letter of
the word, where is defined as ‘A’ if is a
capitalized letter, ‘a’ if is a lower-case letter, ‘0’
if is a digit, and otherwise. For example, if the
word Animal is seen at the start of a sentence, and
it occurs in the list of frequent lower-cased words,
then it would be mapped to the feature 1-1-A.
The word with each character mapped to its
. For example, G.M. would be mapped to
A.A., and Animal would be mapped to Aaaaaa.
The word with each character mapped to its
type, but repeated consecutive character types are
not repeated in the mapped string. For example, An-
imal would be mapped to Aa, G.M. would again be
mapped to A.A..
The tagger was applied and trained in the same
way as described in (Ratnaparkhi 1996). The feature
templates described above are used to create a set of
binary features , where is the tag, and
is the “history”, or context. An example is
if t = S and the
word being tagged = “Mr.”
otherwise
,
defining a conditional distribution over the tags
given a history as
The parameters are trained using Generalized Iter-
ative Scaling. Following (Ratnaparkhi 1996), we
only include features which occur 5 times or more
in training data. In decoding, we use a beam search
to recover 20 candidate tag sequences for each sen-
tence (the sentence is decoded from left to right,
with the top 20 most probable hypotheses being
stored at each point).
</bodyText>
<subsectionHeader confidence="0.999442">
2.3 Applying the baseline tagger
</subsectionHeader>
<bodyText confidence="0.99839078125">
As a baseline we trained a model on the full 53,609
sentences of training data, and decoded the 14,717
sentences of test data. This gave 20 candidates per
The parameters of the model are for
test sentence, along with their probabilities. The
baseline method is to take the most probable candi-
date for each test data sentence, and then to calculate
precision and recall figures. Our aim is to come up
with strategies for reranking the test data candidates,
in such a way that precision and recall is improved.
In developing a reranking strategy, the 53,609
sentences of training data were split into a 41,992
sentence training portion, and a 11,617 sentence de-
velopment set. The training portion was split into
5 sections, and in each case the maximum-entropy
tagger was trained on 4/5 of the data, then used to
decode the remaining 1/5. The top 20 hypotheses
under a beam search, together with their log prob-
abilities, were recovered for each training sentence.
In a similar way, a model trained on the 41,992 sen-
tence set was used to produce 20 hypotheses for each
sentence in the development set.
generator, and hashes them to integers. For ex-
ample, suppose the three strings WE=Gen Xer,
WE=The Day They Shot John Lennon,
WE=Dougherty Arts Center were hashed
to 100, 250, and 500 respectively. Conceptually,
the candidate is represented by a large number
of features for where is the
number of distinct feature strings in training data.
In this example, only
take the value, all other features being zero.
</bodyText>
<subsectionHeader confidence="0.996758">
3.2 Feature templates
</subsectionHeader>
<bodyText confidence="0.9373292">
We now introduce some notation with which to de-
scribe the full set of global features. First, we as-
sume the following primitives of an input candidate:
for is the’th tag in the tagged
sequence.
</bodyText>
<figure confidence="0.615146666666667">
, and
3 Global features
for is the’th word.
</figure>
<subsectionHeader confidence="0.999028">
3.1 The global-feature generator
</subsectionHeader>
<bodyText confidence="0.999880714285714">
The module we describe in this section generates
global features for each candidate tagged sequence.
As input it takes a sentence, along with a proposed
segmentation (i.e., an assignment of a tag for each
word in the sentence). As output, it produces a set
of feature strings. We will use the following tagged
sentence as a running example in this section:
</bodyText>
<construct confidence="0.7159422">
Whether/N you/N ’/N re/N an/N aging/N flower/N child/N
or/N a/N clueless/N Gen/S Xer/C ,/N “/N The/S Day/C
They/C Shot/C John/C Lennon/C ,/N ”/N playing/N at/N the/N
Dougherty/S Arts/C Center/C ,/N entertains/N the/N imagi-
nation/N ./N
</construct>
<bodyText confidence="0.983023615384616">
An example feature type is simply to list the full
strings of entities that appear in the tagged input. In
this example, this would give the three features
WE=Gen Xer
WE=The Day They Shot John Lennon
WE=Dougherty Arts Center
Here WE stands for “whole entity”. Throughout
this section, we will write the features in this format.
The start of the feature string indicates the feature
type (in this case WE), followed by =. Following the
type, there are generally 1 or more words or other
symbols, which we will separate with the symbol .
A seperate module in our implementation
takes the strings produced by the global-feature
for is if begins with a lower-
case letter, otherwise.
for is a transformation of ,
where the transformation is applied in the same
way as the final feature type in the maximum
entropy tagger. Each character in the word is
mapped to its , but repeated consecutive
character types are not repeated in the mapped
string. For example, Animal would be mapped
to Aa in this feature, G.M. would again be
mapped to A.A..
for is the same as , but has
an additional flag appended. The flag indi-
cates whether or not the word appears in a dic-
tionary of words which appeared more often
lower-cased than capitalized in a large corpus
of text. In our example, Animal appears in the
lexicon, but G.M. does not, so the two values
for would be Aa1 and A.A.0 respectively.
In addition, and are all defined to be
NULL if or .
Most of the features we describe are anchored on
entity boundaries in the candidate segmentation. We
will use “feature templates” to describe the features
that we used. As an example, suppose that an entity
</bodyText>
<table confidence="0.998086555555556">
Description Feature Template
The whole entity string WE=
Thefeatures within the entity FF=
Thefeatures within the entity GF=
The last word in the entity LW=
Indicates whether the last word is lower-cased LWLC=
Bigram boundary features of the words before/after the start BO00= BO01= BO10=
of the entity BO11=
Bigram boundary features of the words before/after the end BE00= BE01= BE10=
of the entity BE11=
Trigram boundary features of the words before/after the start TO000= TO111=
of the entity (16 features total, only 4 shown) TO2000= TO2111=
Trigram boundary features of the words before/after the end TE000= TE111=
of the entity (16 features total, only 4 shown) TE2000= TE2111=
Prefix features PF= PF2= PF= PF2=
PF= PF2=
Suffix features SF= SF2= SF= SF2=
SF= SF2=
</table>
<figureCaption confidence="0.999341">
Figure 1: The full set of entity-anchored feature templates. One of these features is generated for each entity
</figureCaption>
<bodyText confidence="0.981984114754098">
seen in a candidate. We take the entity to span words inclusive in the candidate.
is seen from words to inclusive in a segmenta-
tion. Then the WE feature described in the previous
section can be generated by the template
WE=
Applying this template to the three entities in the
running example generates the three feature strings
described in the previous section. As another exam-
ple, consider the template FF= . This
will generate a feature string for each of the entities
in a candidate, this time using the values
rather than . For the full set of feature tem-
plates that are anchored around entities, see figure 1.
A second set of feature templates is anchored
around quotation marks. In our corpus, entities (typ-
ically with long names) are often seen surrounded
by quotes. For example, “The Day They Shot John
Lennon”, the name of a band, appears in the running
example. Define to be the index of any double quo-
tation marks in the candidate, to be the index of the
next (matching) double quotation marks if they ap-
pear in the candidate. Additionally, define to be
the index of the last word beginning with a lower
case letter, upper case letter, or digit within the quo-
tation marks. The first set of feature templates tracks
the values of for the words within quotes:2
Q=
Q2=
2We only included these features if , to prevent
an explosion in the length of feature strings.
, , and . The val-
ues for and would be and (these
features are derived from The and Lennon, which re-
spectively do and don’t appear in the capitalization
lexicon). This would give QF= and
QF2= .
At this point, we have fully described the repre-
sentation used as input to the reranking algorithms.
The maximum-entropy tagger gives 20 proposed
segmentations for each input sentence. Each can-
didate is represented by the log probability
from the tagger, as well as the values of the global
features for . In the next sec-
tion we describe algorithms which blend these two
sources of information, the aim being to improve
upon a strategy which just takes the candidate from
The next set of feature templates are sensitive
to whether the entire sequence between quotes is
tagged as a named entity. Define to be if
S, and =C for (i.e.,
if the sequence of words within the quotes is tagged
as a single entity). Also define to be the number
of upper cased words within the quotes, to be the
number of lower case words, and to be if ,
otherwise. Then two other templates are:
QF=
QF2=
In the “The Day They Shot John Lennon” example
we would have provided that the entire se-
quence within quotes was tagged as an entity. Ad-
ditionally,
</bodyText>
<sectionHeader confidence="0.998008" genericHeader="method">
4 Ranking Algorithms
</sectionHeader>
<subsectionHeader confidence="0.987392">
4.1 Notation
</subsectionHeader>
<bodyText confidence="0.980647632653061">
This section introduces notation for the reranking
task. The framework is derived by the transforma-
tion from ranking problems to a margin-based clas-
sification problem in (Freund et al. 1998). It is also
related to the Markov Random Field methods for
parsing suggested in (Johnson et al. 1999), and the
boosting methods for parsing in (Collins 2000). We
consider the following set-up:
Training data is a set of example input/output
pairs. In tagging we would have training examples
where each is a sentence and each is the
correct sequence of tags for that sentence.
We assume some way of enumerating a set of
candidates for a particular sentence. We use to
denote the’th candidate for the’th sentence in
training data, and to denote
the set of candidates for . In this paper, the top
outputs from a maximum entropy tagger are used as
the set of candidates.
Without loss of generality we take to be the
candidate for which has the most correct tags, i.e.,
is closest to being correct.3
is the probability that the base model
assigns to . We define
We assume a set of additional features,
for . The features could be arbitrary
functions of the candidates; our hope is to include
features which help in discriminating good candi-
dates from bad ones.
Finally, the parameters of the model are a vector
of parameters,
ranking function is defined as
This function assigns a real-valued number to a can-
didate . It will be taken to be a measure of the
plausibility of a candidate, higher scores meaning
higher plausibility. As such, it assigns a ranking to
different candidate structures for the same sentence,
3In the event that multiple candidates get the same, highest
score, the candidate with the highest value of log-likelihood
under the baseline model is taken as .
and in particular the output on a training or test ex-
ample is . In this paper we
take the features to be fixed, the learning problem
being to choose a good setting for the parameters .
In some parts of this paper we will use vec-
tor notation. Define to be the vector
. Then the ranking score
can also be written as where
is the dot product between vectors and .
</bodyText>
<subsectionHeader confidence="0.996085">
4.2 The boosting algorithm
</subsectionHeader>
<bodyText confidence="0.972616676470588">
The first algorithm we consider is the boosting algo-
rithm for ranking described in (Collins 2000). The
algorithm is a modification of the method in (Freund
et al. 1998). The method can be considered to be a
greedy algorithm for finding the parameters that
minimize the loss function
where as before, . The theo-
retical motivation for this algorithm goes back to the
PAC model of learning. Intuitively, it is useful to
note that this loss function is an upper bound on the
number of “ranking errors”, a ranking error being a
case where an incorrect candidate gets a higher value
for than a correct candidate. This follows because
for all , , where we define to be
for , and otherwise. Hence
where . Note that
the number of ranking errors is .
As an initial step, is set to be
and all other parameters for are set
to be zero. The algorithm then proceeds for iter-
ations ( is usually chosen by cross validation on a
development set). At each iteration, a single feature
is chosen, and its weight is updated. Suppose the
current parameter values are , and a single feature
is chosen, its weight being updated through an in-
crement, i.e., . Then the new loss,
after this parameter update, will be
the tagger with the highest score for .
.
. The
where . The boost-
ing algorithm chooses the feature/update pair
which is optimal in terms of minimizing the loss
function, i.e.,
</bodyText>
<equation confidence="0.873355">
(1)
</equation>
<bodyText confidence="0.997702428571429">
and then makes the update .
Figure 2 shows an algorithm which implements
this greedy procedure. See (Collins 2000) for a
full description of the method, including justifica-
tion that the algorithm does in fact implement the
update in Eq. 1 at each iteration.4 The algorithm re-
lies on the following arrays:
Thus is an index from features to cor-
rect/incorrect candidate pairs where the ’th feature
takes value on the correct candidate, and value
on the incorrect candidate. The array is a simi-
lar index from features to examples. The arrays
and are reverse indices from training examples
to features.
</bodyText>
<subsectionHeader confidence="0.998419">
4.3 The voted perceptron
</subsectionHeader>
<bodyText confidence="0.994481555555555">
Figure 3 shows the training phase of the percep-
tron algorithm, originally introduced in (Rosenblatt
1958). The algorithm maintains a parameter vector
, which is initially set to be all zeros. The algo-
rithm then makes a pass over the training set, at each
training example storing a parameter vector for
. The parameter vector is only modified
when a mistake is made on an example. In this case
the update is very simple, involving adding the dif-
ference of the offending examples’ representations
( in the figure). See
(Cristianini and Shawe-Taylor 2000) chapter 2 for
discussion of the perceptron algorithm, and theory
justifying this method for setting the parameters.
In the most basic form of the perceptron, the pa-
rameter values are taken as the final parame-
ter settings, and the output on a new test exam-
ple with for is simply the highest
</bodyText>
<figure confidence="0.902657176470588">
Input
Initialize
Set
Set
For all , set .
Set
For , calculate
–
–
–
Repeat for = 1 to
Choose
Set
Update one parameter,
for
–
–
</figure>
<table confidence="0.810747470588235">
– for ,
– for ,
–
for
–
–
– for ,
– for ,
–
For all features whose values of
and/or have changed, recalculate
Output Final parameter setting
Examples with initial scores
Arrays , ,and as described in
section 4.2.
Parameters are number of rounds of boosting
, a smoothing parameter.
</table>
<footnote confidence="0.9873855">
4Strictly speaking, this is only the case if the smoothing pa-
rameter is.
</footnote>
<figureCaption confidence="0.999396">
Figure 2: The boosting algorithm.
</figureCaption>
<figure confidence="0.970680571428572">
Define: .
Input: Examples with feature vectors .
Initialization: Set parameters
For
If Then
Else
Output: Parameter vectors for
</figure>
<figureCaption confidence="0.9898825">
Figure 3: The perceptron training algorithm for
ranking problems.
</figureCaption>
<bodyText confidence="0.9990195">
.
Input: A set of candidates for ,
A sequence of parameter vectors for
Initialization: Set for
( stores the number of votes for )
scoring candidate under these parameter values, i.e.,
where .
(Freund &amp; Schapire 1999) describe a refinement
of the perceptron, the voted perceptron. The train-
ing phase is identical to that in figure 3. Note, how-
ever, that all parameter vectors for
are stored. Thus the training phase can be thought
of as a way of constructing different parame-
ter settings. Each of these parameter settings will
have its own highest ranking candidate, where
. The idea behind the voted
perceptron is to take each of the parameter set-
tings to “vote” for a candidate, and the candidate
which gets the most votes is returned as the most
likely candidate. See figure 4 for the algorithm.5
</bodyText>
<sectionHeader confidence="0.999818" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99961925">
We applied the voted perceptron and boosting algo-
rithms to the data described in section 2.3. Only fea-
tures occurring on 5 or more distinct training sen-
tences were included in the model. This resulted
</bodyText>
<footnote confidence="0.719592">
5Note that, for reasons of explication, the decoding algo-
rithm we present is less efficient than necessary. For example,
when it is preferable to use some book-keeping to
avoid recalculation of and .
</footnote>
<table confidence="0.9975628">
P R F
Max-Ent 84.4 86.3 85.3
Boosting 87.3(18.6) 87.9(11.6) 87.6(15.6)
Voted 87.3(18.6) 88.6(16.8) 87.9(17.7)
Perceptron
</table>
<figureCaption confidence="0.992039">
Figure 5: Results for the three tagging methods.
</figureCaption>
<bodyText confidence="0.997829897435898">
precision, recall, F-measure. Fig-
ures in parantheses are relative improvements in er-
ror rate over the maximum-entropy model. All fig-
ures are percentages.
in 93,777 distinct features. The two methods were
trained on the training portion (41,992 sentences) of
the training set. We used the development set to pick
the best values for tunable parameters in each algo-
rithm. For boosting, the main parameter to pick is
the number of rounds, . We ran the algorithm for
a total of 300,000 rounds, and found that the op-
timal value for F-measure on the development set
occurred after 83,233 rounds. For the voted per-
ceptron, the representation was taken to be a
vector where is a pa-
rameter that influences the relative contribution of
the log-likelihood term versus the other features. A
value of was found to give the best re-
sults on the development set. Figure 5 shows the
results for the three methods on the test set. Both of
the reranking algorithms show significant improve-
ments over the baseline: a 15.6% relative reduction
in error for boosting, and a 17.7% relative error re-
duction for the voted perceptron.
In our experiments we found the voted percep-
tron algorithm to be considerably more efficient in
training, at some cost in computation on test exam-
ples. Another attractive property of the voted per-
ceptron is that it can be used with kernels, for exam-
ple the kernels over parse trees described in (Collins
and Duffy 2001; Collins and Duffy 2002). (Collins
and Duffy 2002) describe the voted perceptron ap-
plied to the named-entity data in this paper, but us-
ing kernel-based features rather than the explicit fea-
tures described in this paper. See (Collins 2002) for
additional work using perceptron algorithms to train
tagging models, and a more thorough description of
the theory underlying the perceptron algorithm ap-
plied to ranking problems.
</bodyText>
<figure confidence="0.954099">
For
Output: where
</figure>
<figureCaption confidence="0.978465">
Figure 4: Applying the voted perceptron to a test
example.
</figureCaption>
<figure confidence="0.48144">
Define:
</figure>
<sectionHeader confidence="0.977453" genericHeader="evaluation">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999636">
A question regarding the approaches in this paper
is whether the features we have described could be
incorporated in a maximum-entropy tagger, giving
similar improvements in accuracy. This section dis-
cusses why this is unlikely to be the case. The prob-
lem described here is closely related to the label bias
problem described in (Lafferty et al. 2001).
One straightforward way to incorporate global
features into the maximum-entropy model would be
to introduce new features which indicated
whether the tagging decision in the history cre-
ates a particular global feature. For example, we
could introduce a feature
if t = N and this decision
creates an LWLC=1 feature
otherwise
As an example, this would take the value if its was
tagged as N in the following context,
</bodyText>
<sectionHeader confidence="0.441687" genericHeader="conclusions">
She/N praised/N the/N University/S for/C its/? efforts to
</sectionHeader>
<bodyText confidence="0.999929642857143">
because tagging its as N in this context would create
an entity whose last word was not capitalized, i.e.,
University for. Similar features could be created for
all of the global features introduced in this paper.
This example also illustrates why this approach
is unlikely to improve the performance of the
maximum-entropy tagger. The parameter as-
sociated with this new feature can only affect the
score for a proposed sequence by modifying
at the point at which . In the exam-
ple, this means that the LWLC=1 feature can only
lower the score for the segmentation by lowering the
probability of tagging its as N. But its has almost
probably of not appearing as part of an entity, so
should be almost whether is or
in this context! The decision which effectively cre-
ated the entity University for was the decision to tag
for as C, and this has already been made. The inde-
pendence assumptions in maximum-entropy taggers
of this form often lead points of local ambiguity (in
this example the tag for the word for) to create glob-
ally implausible structures with unreasonably high
scores. See (Collins 1999) section 8.4.2 for a dis-
cussion of this problem in the context of parsing.
Acknowledgements Many thanks to Jack Minisi for
annotating the named-entity data used in the exper-
iments. Thanks also to Nigel Duffy, Rob Schapire
and Yoram Singer for several useful discussions.
</bodyText>
<sectionHeader confidence="0.99645" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999566709090909">
Abney, S. 1997. Stochastic Attribute-Value Grammars. Compu-
tational Linguistics, 23(4):597-618.
Bikel, D., Schwartz, R., and Weischedel, R. (1999). An Algo-
rithm that Learns What’s in a Name. In Machine Learning:
Special Issue on Natural Language Learning, 34(1-3).
Borthwick, A., Sterling, J., Agichtein, E., and Grishman, R.
(1998). Exploiting Diverse Knowledge Sources via Maxi-
mum Entropy in Named Entity Recognition. Proc. of the
Sixth Workshop on Very Large Corpora.
Collins, M. (1999). Head-Driven Statistical Models for Natural
Language Parsing. PhD Thesis, University of Pennsylvania.
Collins, M. (2000). Discriminative Reranking for Natural Lan-
guage Parsing. Proceedings of the Seventeenth International
Conference on Machine Learning (ICML 2000).
Collins, M., and Duffy, N. (2001). Convolution Kernels for Nat-
ural Language. In Proceedings ofNIPS 14.
Collins, M., and Duffy, N. (2002). New Ranking Algorithms for
Parsing and Tagging: Kernels over Discrete Structures, and
the Voted Perceptron. In Proceedings ofACL 2002.
Collins, M. (2002). Discriminative Training Methods for Hid-
den Markov Models: Theory and Experiments with the Per-
ceptron Algorithm. In Proceedings of EMNLP 2002.
Cristianini, N., and Shawe-Tayor, J. (2000). An introduction to
Support Vector Machines and other kernel-based learning
methods. Cambridge University Press.
Della Pietra, S., Della Pietra, V., and Lafferty, J. (1997). Induc-
ing Features of Random Fields. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 19(4), pp. 380-393.
Freund, Y. &amp; Schapire, R. (1999). Large Margin Classifica-
tion using the Perceptron Algorithm. In Machine Learning,
37(3):277–296.
Freund, Y., Iyer, R.,Schapire, R.E., &amp; Singer, Y. (1998). An effi-
cient boosting algorithm for combining preferences. In Ma-
chine Learning: Proceedings of the Fifteenth International
Conference.
Johnson, M., Geman, S., Canon, S., Chi, Z. and Riezler, S.
(1999). Estimators for Stochastic “Unification-based” Gram-
mars. Proceedings of the ACL 1999.
Lafferty, J., McCallum, A., and Pereira, F. (2001). Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proceedings ofICML 2001.
McCallum, A., Freitag, D., and Pereira, F. (2000) Maximum
entropy markov models for information extraction and seg-
mentation. In Proceedings ofICML 2000.
Ratnaparkhi, A. (1996). A maximum entropy part-of-speech
tagger. In Proceedings of the empirical methods in natural
language processing conference.
Rosenblatt, F. (1958). The Perceptron: A Probabilistic Model
for Information Storage and Organization in the Brain. Psy-
chological Review, 65, 386–408. (Reprinted in Neurocom-
puting (MIT Press, 1998).)
Walker, M., Rambow, O., and Rogati, M. (2001). SPoT: a train-
able sentence planner. In Proceedings of the 2nd Meeting of
the North American Chapter of the Association for Compu-
tational Linguistics (NAACL 2001).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.959366">
<note confidence="0.99818">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 489-496.</note>
<title confidence="0.9884115">Ranking Algorithms for Named–Entity Extraction: Boosting and the Voted Perceptron</title>
<author confidence="0.999751">Michael Collins</author>
<affiliation confidence="0.999231">AT&amp;T Labs-Research, Florham Park, New Jersey.</affiliation>
<email confidence="0.999892">mcollins@research.att.com</email>
<abstract confidence="0.999079666666666">This paper describes algorithms which rerank the top N hypotheses from a maximum-entropy tagger, the application being the recovery of named-entity boundaries in a corpus of web data. The first approach uses a boosting algorithm for ranking problems. The second approach uses the voted perceptron algorithm. Both algorithms give comparable, significant improvements over the maximum-entropy baseline. The voted perceptron algorithm can be considerably more efficient to train, at some cost in computation on test examples.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Stochastic Attribute-Value Grammars.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--4</pages>
<contexts>
<context position="1064" citStr="Abney 1997" startWordPosition="149" endWordPosition="150">ntity boundaries in a corpus of web data. The first approach uses a boosting algorithm for ranking problems. The second approach uses the voted perceptron algorithm. Both algorithms give comparable, significant improvements over the maximum-entropy baseline. The voted perceptron algorithm can be considerably more efficient to train, at some cost in computation on test examples. 1 Introduction Recent work in statistical approaches to parsing and tagging has begun to consider methods which incorporate global features of candidate structures. Examples of such techniques are Markov Random Fields (Abney 1997; Della Pietra et al. 1997; Johnson et al. 1999), and boosting algorithms (Freund et al. 1998; Collins 2000; Walker et al. 2001). One appeal of these methods is their flexibility in incorporating features into a model: essentially any features which might be useful in discriminating good from bad structures can be included. A second appeal of these methods is that their training criterion is often discriminative, attempting to explicitly push the score or probability of the correct structure for each training sentence above the score of competing structures. This discriminative property is sha</context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>Abney, S. 1997. Stochastic Attribute-Value Grammars. Computational Linguistics, 23(4):597-618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bikel</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>An Algorithm that Learns What’s in a Name.</title>
<date>1999</date>
<booktitle>In Machine Learning: Special Issue on Natural Language Learning,</booktitle>
<pages>34--1</pages>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>Bikel, D., Schwartz, R., and Weischedel, R. (1999). An Algorithm that Learns What’s in a Name. In Machine Learning: Special Issue on Natural Language Learning, 34(1-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Borthwick</author>
<author>J Sterling</author>
<author>E Agichtein</author>
<author>R Grishman</author>
</authors>
<title>Exploiting Diverse Knowledge Sources via Maximum Entropy in Named Entity Recognition.</title>
<date>1998</date>
<booktitle>Proc. of the Sixth Workshop on Very Large Corpora.</booktitle>
<marker>Borthwick, Sterling, Agichtein, Grishman, 1998</marker>
<rawString>Borthwick, A., Sterling, J., Agichtein, E., and Grishman, R. (1998). Exploiting Diverse Knowledge Sources via Maximum Entropy in Named Entity Recognition. Proc. of the Sixth Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>PhD Thesis,</tech>
<institution>University of Pennsylvania.</institution>
<marker>Collins, 1999</marker>
<rawString>Collins, M. (1999). Head-Driven Statistical Models for Natural Language Parsing. PhD Thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative Reranking for Natural Language Parsing.</title>
<date>2000</date>
<booktitle>Proceedings of the Seventeenth International Conference on Machine Learning (ICML</booktitle>
<contexts>
<context position="1171" citStr="Collins 2000" startWordPosition="168" endWordPosition="169">ms. The second approach uses the voted perceptron algorithm. Both algorithms give comparable, significant improvements over the maximum-entropy baseline. The voted perceptron algorithm can be considerably more efficient to train, at some cost in computation on test examples. 1 Introduction Recent work in statistical approaches to parsing and tagging has begun to consider methods which incorporate global features of candidate structures. Examples of such techniques are Markov Random Fields (Abney 1997; Della Pietra et al. 1997; Johnson et al. 1999), and boosting algorithms (Freund et al. 1998; Collins 2000; Walker et al. 2001). One appeal of these methods is their flexibility in incorporating features into a model: essentially any features which might be useful in discriminating good from bad structures can be included. A second appeal of these methods is that their training criterion is often discriminative, attempting to explicitly push the score or probability of the correct structure for each training sentence above the score of competing structures. This discriminative property is shared by the methods of (Johnson et al. 1999; Collins 2000), and also the Conditional Random Field methods of</context>
<context position="2597" citStr="Collins 2000" startWordPosition="393" endWordPosition="394">et Journal data. Similar boosting algorithms have been applied to natural language generation, with good results, in (Walker et al. 2001). In this paper we apply reranking methods to named-entity extraction. A state-ofthe-art (maximum-entropy) tagger is used to generate 20 possible segmentations for each input sentence, along with their probabilities. We describe a number of additional global features of these candidate segmentations. These additional features are used as evidence in reranking the hypotheses from the max-ent tagger. We describe two learning algorithms: the boosting method of (Collins 2000), and a variant of the voted perceptron algorithm, which was initially described in (Freund &amp; Schapire 1999). We applied the methods to a corpus of over one million words of tagged web data. The methods give significant improvements over the maximum-entropy tagger (a 17.7% relative reduction in error-rate for the voted perceptron, and a 15.6% relative improvement for the boosting method). One contribution of this paper is to show that existing reranking methods are useful for a new domain, named-entity tagging, and to suggest global features which give improvements on this task. We should stre</context>
<context position="15535" citStr="Collins 2000" startWordPosition="2653" endWordPosition="2654">lower case words, and to be if , otherwise. Then two other templates are: QF= QF2= In the “The Day They Shot John Lennon” example we would have provided that the entire sequence within quotes was tagged as an entity. Additionally, 4 Ranking Algorithms 4.1 Notation This section introduces notation for the reranking task. The framework is derived by the transformation from ranking problems to a margin-based classification problem in (Freund et al. 1998). It is also related to the Markov Random Field methods for parsing suggested in (Johnson et al. 1999), and the boosting methods for parsing in (Collins 2000). We consider the following set-up: Training data is a set of example input/output pairs. In tagging we would have training examples where each is a sentence and each is the correct sequence of tags for that sentence. We assume some way of enumerating a set of candidates for a particular sentence. We use to denote the’th candidate for the’th sentence in training data, and to denote the set of candidates for . In this paper, the top outputs from a maximum entropy tagger are used as the set of candidates. Without loss of generality we take to be the candidate for which has the most correct tags,</context>
<context position="17435" citStr="Collins 2000" startWordPosition="2992" endWordPosition="2993">ates get the same, highest score, the candidate with the highest value of log-likelihood under the baseline model is taken as . and in particular the output on a training or test example is . In this paper we take the features to be fixed, the learning problem being to choose a good setting for the parameters . In some parts of this paper we will use vector notation. Define to be the vector . Then the ranking score can also be written as where is the dot product between vectors and . 4.2 The boosting algorithm The first algorithm we consider is the boosting algorithm for ranking described in (Collins 2000). The algorithm is a modification of the method in (Freund et al. 1998). The method can be considered to be a greedy algorithm for finding the parameters that minimize the loss function where as before, . The theoretical motivation for this algorithm goes back to the PAC model of learning. Intuitively, it is useful to note that this loss function is an upper bound on the number of “ranking errors”, a ranking error being a case where an incorrect candidate gets a higher value for than a correct candidate. This follows because for all , , where we define to be for , and otherwise. Hence where . </context>
<context position="18821" citStr="Collins 2000" startWordPosition="3244" endWordPosition="3245"> usually chosen by cross validation on a development set). At each iteration, a single feature is chosen, and its weight is updated. Suppose the current parameter values are , and a single feature is chosen, its weight being updated through an increment, i.e., . Then the new loss, after this parameter update, will be the tagger with the highest score for . . . The where . The boosting algorithm chooses the feature/update pair which is optimal in terms of minimizing the loss function, i.e., (1) and then makes the update . Figure 2 shows an algorithm which implements this greedy procedure. See (Collins 2000) for a full description of the method, including justification that the algorithm does in fact implement the update in Eq. 1 at each iteration.4 The algorithm relies on the following arrays: Thus is an index from features to correct/incorrect candidate pairs where the ’th feature takes value on the correct candidate, and value on the incorrect candidate. The array is a similar index from features to examples. The arrays and are reverse indices from training examples to features. 4.3 The voted perceptron Figure 3 shows the training phase of the perceptron algorithm, originally introduced in (Ro</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Collins, M. (2000). Discriminative Reranking for Natural Language Parsing. Proceedings of the Seventeenth International Conference on Machine Learning (ICML 2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>Convolution Kernels for Natural Language.</title>
<date>2001</date>
<booktitle>In Proceedings ofNIPS 14.</booktitle>
<contexts>
<context position="23692" citStr="Collins and Duffy 2001" startWordPosition="4081" endWordPosition="4084">e the best results on the development set. Figure 5 shows the results for the three methods on the test set. Both of the reranking algorithms show significant improvements over the baseline: a 15.6% relative reduction in error for boosting, and a 17.7% relative error reduction for the voted perceptron. In our experiments we found the voted perceptron algorithm to be considerably more efficient in training, at some cost in computation on test examples. Another attractive property of the voted perceptron is that it can be used with kernels, for example the kernels over parse trees described in (Collins and Duffy 2001; Collins and Duffy 2002). (Collins and Duffy 2002) describe the voted perceptron applied to the named-entity data in this paper, but using kernel-based features rather than the explicit features described in this paper. See (Collins 2002) for additional work using perceptron algorithms to train tagging models, and a more thorough description of the theory underlying the perceptron algorithm applied to ranking problems. For Output: where Figure 4: Applying the voted perceptron to a test example. Define: 6 Discussion A question regarding the approaches in this paper is whether the features we h</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Collins, M., and Duffy, N. (2001). Convolution Kernels for Natural Language. In Proceedings ofNIPS 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="23717" citStr="Collins and Duffy 2002" startWordPosition="4085" endWordPosition="4088">e development set. Figure 5 shows the results for the three methods on the test set. Both of the reranking algorithms show significant improvements over the baseline: a 15.6% relative reduction in error for boosting, and a 17.7% relative error reduction for the voted perceptron. In our experiments we found the voted perceptron algorithm to be considerably more efficient in training, at some cost in computation on test examples. Another attractive property of the voted perceptron is that it can be used with kernels, for example the kernels over parse trees described in (Collins and Duffy 2001; Collins and Duffy 2002). (Collins and Duffy 2002) describe the voted perceptron applied to the named-entity data in this paper, but using kernel-based features rather than the explicit features described in this paper. See (Collins 2002) for additional work using perceptron algorithms to train tagging models, and a more thorough description of the theory underlying the perceptron algorithm applied to ranking problems. For Output: where Figure 4: Applying the voted perceptron to a test example. Define: 6 Discussion A question regarding the approaches in this paper is whether the features we have described could be in</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Collins, M., and Duffy, N. (2002). New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron. In Proceedings ofACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with the Perceptron Algorithm.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="23931" citStr="Collins 2002" startWordPosition="4122" endWordPosition="4123">7.7% relative error reduction for the voted perceptron. In our experiments we found the voted perceptron algorithm to be considerably more efficient in training, at some cost in computation on test examples. Another attractive property of the voted perceptron is that it can be used with kernels, for example the kernels over parse trees described in (Collins and Duffy 2001; Collins and Duffy 2002). (Collins and Duffy 2002) describe the voted perceptron applied to the named-entity data in this paper, but using kernel-based features rather than the explicit features described in this paper. See (Collins 2002) for additional work using perceptron algorithms to train tagging models, and a more thorough description of the theory underlying the perceptron algorithm applied to ranking problems. For Output: where Figure 4: Applying the voted perceptron to a test example. Define: 6 Discussion A question regarding the approaches in this paper is whether the features we have described could be incorporated in a maximum-entropy tagger, giving similar improvements in accuracy. This section discusses why this is unlikely to be the case. The problem described here is closely related to the label bias problem d</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Collins, M. (2002). Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with the Perceptron Algorithm. In Proceedings of EMNLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cristianini</author>
<author>J Shawe-Tayor</author>
</authors>
<title>An introduction to Support Vector Machines and other kernel-based learning methods.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<marker>Cristianini, Shawe-Tayor, 2000</marker>
<rawString>Cristianini, N., and Shawe-Tayor, J. (2000). An introduction to Support Vector Machines and other kernel-based learning methods. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Della Pietra</author>
<author>Della Pietra S</author>
<author>V</author>
<author>J Lafferty</author>
</authors>
<title>Inducing Features of Random Fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>4</issue>
<pages>380--393</pages>
<contexts>
<context position="1090" citStr="Pietra et al. 1997" startWordPosition="152" endWordPosition="155">n a corpus of web data. The first approach uses a boosting algorithm for ranking problems. The second approach uses the voted perceptron algorithm. Both algorithms give comparable, significant improvements over the maximum-entropy baseline. The voted perceptron algorithm can be considerably more efficient to train, at some cost in computation on test examples. 1 Introduction Recent work in statistical approaches to parsing and tagging has begun to consider methods which incorporate global features of candidate structures. Examples of such techniques are Markov Random Fields (Abney 1997; Della Pietra et al. 1997; Johnson et al. 1999), and boosting algorithms (Freund et al. 1998; Collins 2000; Walker et al. 2001). One appeal of these methods is their flexibility in incorporating features into a model: essentially any features which might be useful in discriminating good from bad structures can be included. A second appeal of these methods is that their training criterion is often discriminative, attempting to explicitly push the score or probability of the correct structure for each training sentence above the score of competing structures. This discriminative property is shared by the methods of (Joh</context>
</contexts>
<marker>Pietra, S, V, Lafferty, 1997</marker>
<rawString>Della Pietra, S., Della Pietra, V., and Lafferty, J. (1997). Inducing Features of Random Fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4), pp. 380-393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R Schapire</author>
</authors>
<title>Large Margin Classification using the Perceptron Algorithm.</title>
<date>1999</date>
<booktitle>In Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="2705" citStr="Freund &amp; Schapire 1999" startWordPosition="408" endWordPosition="411">h good results, in (Walker et al. 2001). In this paper we apply reranking methods to named-entity extraction. A state-ofthe-art (maximum-entropy) tagger is used to generate 20 possible segmentations for each input sentence, along with their probabilities. We describe a number of additional global features of these candidate segmentations. These additional features are used as evidence in reranking the hypotheses from the max-ent tagger. We describe two learning algorithms: the boosting method of (Collins 2000), and a variant of the voted perceptron algorithm, which was initially described in (Freund &amp; Schapire 1999). We applied the methods to a corpus of over one million words of tagged web data. The methods give significant improvements over the maximum-entropy tagger (a 17.7% relative reduction in error-rate for the voted perceptron, and a 15.6% relative improvement for the boosting method). One contribution of this paper is to show that existing reranking methods are useful for a new domain, named-entity tagging, and to suggest global features which give improvements on this task. We should stress that another contribution is to show that a new algorithm, the voted perceptron, gives very credible resu</context>
<context position="21098" citStr="Freund &amp; Schapire 1999" startWordPosition="3640" endWordPosition="3643">s , ,and as described in section 4.2. Parameters are number of rounds of boosting , a smoothing parameter. 4Strictly speaking, this is only the case if the smoothing parameter is. Figure 2: The boosting algorithm. Define: . Input: Examples with feature vectors . Initialization: Set parameters For If Then Else Output: Parameter vectors for Figure 3: The perceptron training algorithm for ranking problems. . Input: A set of candidates for , A sequence of parameter vectors for Initialization: Set for ( stores the number of votes for ) scoring candidate under these parameter values, i.e., where . (Freund &amp; Schapire 1999) describe a refinement of the perceptron, the voted perceptron. The training phase is identical to that in figure 3. Note, however, that all parameter vectors for are stored. Thus the training phase can be thought of as a way of constructing different parameter settings. Each of these parameter settings will have its own highest ranking candidate, where . The idea behind the voted perceptron is to take each of the parameter settings to “vote” for a candidate, and the candidate which gets the most votes is returned as the most likely candidate. See figure 4 for the algorithm.5 5 Experiments We </context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Freund, Y. &amp; Schapire, R. (1999). Large Margin Classification using the Perceptron Algorithm. In Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R Iyer</author>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>An efficient boosting algorithm for combining preferences.</title>
<date>1998</date>
<booktitle>In Machine Learning: Proceedings of the Fifteenth International Conference.</booktitle>
<contexts>
<context position="1157" citStr="Freund et al. 1998" startWordPosition="164" endWordPosition="167">m for ranking problems. The second approach uses the voted perceptron algorithm. Both algorithms give comparable, significant improvements over the maximum-entropy baseline. The voted perceptron algorithm can be considerably more efficient to train, at some cost in computation on test examples. 1 Introduction Recent work in statistical approaches to parsing and tagging has begun to consider methods which incorporate global features of candidate structures. Examples of such techniques are Markov Random Fields (Abney 1997; Della Pietra et al. 1997; Johnson et al. 1999), and boosting algorithms (Freund et al. 1998; Collins 2000; Walker et al. 2001). One appeal of these methods is their flexibility in incorporating features into a model: essentially any features which might be useful in discriminating good from bad structures can be included. A second appeal of these methods is that their training criterion is often discriminative, attempting to explicitly push the score or probability of the correct structure for each training sentence above the score of competing structures. This discriminative property is shared by the methods of (Johnson et al. 1999; Collins 2000), and also the Conditional Random Fi</context>
<context position="15377" citStr="Freund et al. 1998" startWordPosition="2624" endWordPosition="2627"> if the sequence of words within the quotes is tagged as a single entity). Also define to be the number of upper cased words within the quotes, to be the number of lower case words, and to be if , otherwise. Then two other templates are: QF= QF2= In the “The Day They Shot John Lennon” example we would have provided that the entire sequence within quotes was tagged as an entity. Additionally, 4 Ranking Algorithms 4.1 Notation This section introduces notation for the reranking task. The framework is derived by the transformation from ranking problems to a margin-based classification problem in (Freund et al. 1998). It is also related to the Markov Random Field methods for parsing suggested in (Johnson et al. 1999), and the boosting methods for parsing in (Collins 2000). We consider the following set-up: Training data is a set of example input/output pairs. In tagging we would have training examples where each is a sentence and each is the correct sequence of tags for that sentence. We assume some way of enumerating a set of candidates for a particular sentence. We use to denote the’th candidate for the’th sentence in training data, and to denote the set of candidates for . In this paper, the top output</context>
<context position="17506" citStr="Freund et al. 1998" startWordPosition="3003" endWordPosition="3006">value of log-likelihood under the baseline model is taken as . and in particular the output on a training or test example is . In this paper we take the features to be fixed, the learning problem being to choose a good setting for the parameters . In some parts of this paper we will use vector notation. Define to be the vector . Then the ranking score can also be written as where is the dot product between vectors and . 4.2 The boosting algorithm The first algorithm we consider is the boosting algorithm for ranking described in (Collins 2000). The algorithm is a modification of the method in (Freund et al. 1998). The method can be considered to be a greedy algorithm for finding the parameters that minimize the loss function where as before, . The theoretical motivation for this algorithm goes back to the PAC model of learning. Intuitively, it is useful to note that this loss function is an upper bound on the number of “ranking errors”, a ranking error being a case where an incorrect candidate gets a higher value for than a correct candidate. This follows because for all , , where we define to be for , and otherwise. Hence where . Note that the number of ranking errors is . As an initial step, is set </context>
</contexts>
<marker>Freund, Iyer, Schapire, Singer, 1998</marker>
<rawString>Freund, Y., Iyer, R.,Schapire, R.E., &amp; Singer, Y. (1998). An efficient boosting algorithm for combining preferences. In Machine Learning: Proceedings of the Fifteenth International Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>S Geman</author>
<author>S Canon</author>
<author>Z Chi</author>
<author>S Riezler</author>
</authors>
<title>Estimators for Stochastic “Unification-based” Grammars.</title>
<date>1999</date>
<booktitle>Proceedings of the ACL</booktitle>
<contexts>
<context position="1112" citStr="Johnson et al. 1999" startWordPosition="156" endWordPosition="160">ta. The first approach uses a boosting algorithm for ranking problems. The second approach uses the voted perceptron algorithm. Both algorithms give comparable, significant improvements over the maximum-entropy baseline. The voted perceptron algorithm can be considerably more efficient to train, at some cost in computation on test examples. 1 Introduction Recent work in statistical approaches to parsing and tagging has begun to consider methods which incorporate global features of candidate structures. Examples of such techniques are Markov Random Fields (Abney 1997; Della Pietra et al. 1997; Johnson et al. 1999), and boosting algorithms (Freund et al. 1998; Collins 2000; Walker et al. 2001). One appeal of these methods is their flexibility in incorporating features into a model: essentially any features which might be useful in discriminating good from bad structures can be included. A second appeal of these methods is that their training criterion is often discriminative, attempting to explicitly push the score or probability of the correct structure for each training sentence above the score of competing structures. This discriminative property is shared by the methods of (Johnson et al. 1999; Coll</context>
<context position="15479" citStr="Johnson et al. 1999" startWordPosition="2642" endWordPosition="2645">er of upper cased words within the quotes, to be the number of lower case words, and to be if , otherwise. Then two other templates are: QF= QF2= In the “The Day They Shot John Lennon” example we would have provided that the entire sequence within quotes was tagged as an entity. Additionally, 4 Ranking Algorithms 4.1 Notation This section introduces notation for the reranking task. The framework is derived by the transformation from ranking problems to a margin-based classification problem in (Freund et al. 1998). It is also related to the Markov Random Field methods for parsing suggested in (Johnson et al. 1999), and the boosting methods for parsing in (Collins 2000). We consider the following set-up: Training data is a set of example input/output pairs. In tagging we would have training examples where each is a sentence and each is the correct sequence of tags for that sentence. We assume some way of enumerating a set of candidates for a particular sentence. We use to denote the’th candidate for the’th sentence in training data, and to denote the set of candidates for . In this paper, the top outputs from a maximum entropy tagger are used as the set of candidates. Without loss of generality we take </context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Johnson, M., Geman, S., Canon, S., Chi, Z. and Riezler, S. (1999). Estimators for Stochastic “Unification-based” Grammars. Proceedings of the ACL 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings ofICML</booktitle>
<contexts>
<context position="1794" citStr="Lafferty et al. 2001" startWordPosition="266" endWordPosition="269">Walker et al. 2001). One appeal of these methods is their flexibility in incorporating features into a model: essentially any features which might be useful in discriminating good from bad structures can be included. A second appeal of these methods is that their training criterion is often discriminative, attempting to explicitly push the score or probability of the correct structure for each training sentence above the score of competing structures. This discriminative property is shared by the methods of (Johnson et al. 1999; Collins 2000), and also the Conditional Random Field methods of (Lafferty et al. 2001). In a previous paper (Collins 2000), a boosting algorithm was used to rerank the output from an existing statistical parser, giving significant improvements in parsing accuracy on Wall Street Journal data. Similar boosting algorithms have been applied to natural language generation, with good results, in (Walker et al. 2001). In this paper we apply reranking methods to named-entity extraction. A state-ofthe-art (maximum-entropy) tagger is used to generate 20 possible segmentations for each input sentence, along with their probabilities. We describe a number of additional global features of th</context>
<context position="24565" citStr="Lafferty et al. 2001" startWordPosition="4221" endWordPosition="4224">al work using perceptron algorithms to train tagging models, and a more thorough description of the theory underlying the perceptron algorithm applied to ranking problems. For Output: where Figure 4: Applying the voted perceptron to a test example. Define: 6 Discussion A question regarding the approaches in this paper is whether the features we have described could be incorporated in a maximum-entropy tagger, giving similar improvements in accuracy. This section discusses why this is unlikely to be the case. The problem described here is closely related to the label bias problem described in (Lafferty et al. 2001). One straightforward way to incorporate global features into the maximum-entropy model would be to introduce new features which indicated whether the tagging decision in the history creates a particular global feature. For example, we could introduce a feature if t = N and this decision creates an LWLC=1 feature otherwise As an example, this would take the value if its was tagged as N in the following context, She/N praised/N the/N University/S for/C its/? efforts to because tagging its as N in this context would create an entity whose last word was not capitalized, i.e., University for. Simi</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, J., McCallum, A., and Pereira, F. (2001). Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings ofICML 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>D Freitag</author>
<author>F Pereira</author>
</authors>
<title>Maximum entropy markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings ofICML</booktitle>
<contexts>
<context position="5118" citStr="McCallum et al. 2000" startWordPosition="836" endWordPosition="839">tator with exactly the same span as that proposed) then the precision of a method is . Similarly, if is the total number of entities in the human annotated version of the test set, then the recall is . 2.2 The baseline tagger The problem can be framed as a tagging task – to tag each word as being either the start of an entity, a continuation of an entity, or not to be part of an entity at all (we will use the tags S, C and N respectively for these three cases). As a baseline model we used a maximum entropy tagger, very similar to the ones described in (Ratnaparkhi 1996; Borthwick et. al 1998; McCallum et al. 2000). Max-ent taggers have been shown to be highly competitive on a number of tagging tasks, such as part-of-speech tagging (Ratnaparkhi 1996), named-entity recognition (Borthwick et. al 1998), and information extraction tasks (McCallum et al. 2000). Thus the maximumentropy tagger we used represents a serious baseline for the task. We used the following features (several of the features were inspired by the approach of (Bikel et. al 1999), an HMM model which gives excellent results on named entity extraction): The word being tagged, the previous word, and the next word. The previous tag, and the p</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>McCallum, A., Freitag, D., and Pereira, F. (2000) Maximum entropy markov models for information extraction and segmentation. In Proceedings ofICML 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy part-of-speech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the empirical methods in natural language processing conference.</booktitle>
<contexts>
<context position="5072" citStr="Ratnaparkhi 1996" startWordPosition="830" endWordPosition="831">ct (i.e., an entity is marked by the annotator with exactly the same span as that proposed) then the precision of a method is . Similarly, if is the total number of entities in the human annotated version of the test set, then the recall is . 2.2 The baseline tagger The problem can be framed as a tagging task – to tag each word as being either the start of an entity, a continuation of an entity, or not to be part of an entity at all (we will use the tags S, C and N respectively for these three cases). As a baseline model we used a maximum entropy tagger, very similar to the ones described in (Ratnaparkhi 1996; Borthwick et. al 1998; McCallum et al. 2000). Max-ent taggers have been shown to be highly competitive on a number of tagging tasks, such as part-of-speech tagging (Ratnaparkhi 1996), named-entity recognition (Borthwick et. al 1998), and information extraction tasks (McCallum et al. 2000). Thus the maximumentropy tagger we used represents a serious baseline for the task. We used the following features (several of the features were inspired by the approach of (Bikel et. al 1999), an HMM model which gives excellent results on named entity extraction): The word being tagged, the previous word, </context>
<context position="6969" citStr="Ratnaparkhi 1996" startWordPosition="1165" endWordPosition="1166">is a digit, and otherwise. For example, if the word Animal is seen at the start of a sentence, and it occurs in the list of frequent lower-cased words, then it would be mapped to the feature 1-1-A. The word with each character mapped to its . For example, G.M. would be mapped to A.A., and Animal would be mapped to Aaaaaa. The word with each character mapped to its type, but repeated consecutive character types are not repeated in the mapped string. For example, Animal would be mapped to Aa, G.M. would again be mapped to A.A.. The tagger was applied and trained in the same way as described in (Ratnaparkhi 1996). The feature templates described above are used to create a set of binary features , where is the tag, and is the “history”, or context. An example is if t = S and the word being tagged = “Mr.” otherwise , defining a conditional distribution over the tags given a history as The parameters are trained using Generalized Iterative Scaling. Following (Ratnaparkhi 1996), we only include features which occur 5 times or more in training data. In decoding, we use a beam search to recover 20 candidate tag sequences for each sentence (the sentence is decoded from left to right, with the top 20 most pro</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Ratnaparkhi, A. (1996). A maximum entropy part-of-speech tagger. In Proceedings of the empirical methods in natural language processing conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Rosenblatt</author>
</authors>
<title>The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.</title>
<date>1958</date>
<journal>Psychological Review,</journal>
<volume>65</volume>
<pages>386--408</pages>
<publisher>MIT Press,</publisher>
<note>Reprinted in Neurocomputing</note>
<contexts>
<context position="19435" citStr="Rosenblatt 1958" startWordPosition="3346" endWordPosition="3347">0) for a full description of the method, including justification that the algorithm does in fact implement the update in Eq. 1 at each iteration.4 The algorithm relies on the following arrays: Thus is an index from features to correct/incorrect candidate pairs where the ’th feature takes value on the correct candidate, and value on the incorrect candidate. The array is a similar index from features to examples. The arrays and are reverse indices from training examples to features. 4.3 The voted perceptron Figure 3 shows the training phase of the perceptron algorithm, originally introduced in (Rosenblatt 1958). The algorithm maintains a parameter vector , which is initially set to be all zeros. The algorithm then makes a pass over the training set, at each training example storing a parameter vector for . The parameter vector is only modified when a mistake is made on an example. In this case the update is very simple, involving adding the difference of the offending examples’ representations ( in the figure). See (Cristianini and Shawe-Taylor 2000) chapter 2 for discussion of the perceptron algorithm, and theory justifying this method for setting the parameters. In the most basic form of the perce</context>
</contexts>
<marker>Rosenblatt, 1958</marker>
<rawString>Rosenblatt, F. (1958). The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain. Psychological Review, 65, 386–408. (Reprinted in Neurocomputing (MIT Press, 1998).)</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>O Rambow</author>
<author>M Rogati</author>
</authors>
<title>SPoT: a trainable sentence planner.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL</booktitle>
<contexts>
<context position="1192" citStr="Walker et al. 2001" startWordPosition="170" endWordPosition="173"> approach uses the voted perceptron algorithm. Both algorithms give comparable, significant improvements over the maximum-entropy baseline. The voted perceptron algorithm can be considerably more efficient to train, at some cost in computation on test examples. 1 Introduction Recent work in statistical approaches to parsing and tagging has begun to consider methods which incorporate global features of candidate structures. Examples of such techniques are Markov Random Fields (Abney 1997; Della Pietra et al. 1997; Johnson et al. 1999), and boosting algorithms (Freund et al. 1998; Collins 2000; Walker et al. 2001). One appeal of these methods is their flexibility in incorporating features into a model: essentially any features which might be useful in discriminating good from bad structures can be included. A second appeal of these methods is that their training criterion is often discriminative, attempting to explicitly push the score or probability of the correct structure for each training sentence above the score of competing structures. This discriminative property is shared by the methods of (Johnson et al. 1999; Collins 2000), and also the Conditional Random Field methods of (Lafferty et al. 200</context>
</contexts>
<marker>Walker, Rambow, Rogati, 2001</marker>
<rawString>Walker, M., Rambow, O., and Rogati, M. (2001). SPoT: a trainable sentence planner. In Proceedings of the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL 2001).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>