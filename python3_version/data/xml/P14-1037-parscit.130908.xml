<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.988647">
Zero-shot Entity Extraction from Web Pages
</title>
<author confidence="0.998021">
Panupong Pasupat Percy Liang
</author>
<affiliation confidence="0.9994815">
Computer Science Department Computer Science Department
Stanford University Stanford University
</affiliation>
<email confidence="0.99156">
ppasupat@cs.stanford.edu pliang@cs.stanford.edu
</email>
<sectionHeader confidence="0.997291" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99987775">
In order to extract entities of a fine-grained
category from semi-structured data in web
pages, existing information extraction sys-
tems rely on seed examples or redundancy
across multiple web pages. In this paper,
we consider a new zero-shot learning task
of extracting entities specified by a natural
language query (in place of seeds) given
only a single web page. Our approach de-
fines a log-linear model over latent extrac-
tion predicates, which select lists of enti-
ties from the web page. The main chal-
lenge is to define features on widely vary-
ing candidate entity lists. We tackle this by
abstracting list elements and using aggre-
gate statistics to define features. Finally,
we created a new dataset of diverse queries
and web pages, and show that our system
achieves significantly better accuracy than
a natural baseline.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978647058824">
We consider the task of extracting entities of
a given category (e.g., hiking trails) from web
pages. Previous approaches either (i) assume that
the same entities appear on multiple web pages,
or (ii) require information such as seed examples
(Etzioni et al., 2005; Wang and Cohen, 2009;
Dalvi et al., 2012). These approaches work well
for common categories but encounter data sparsity
problems for more specific categories, such as the
products of a small company or the dishes at a lo-
cal restaurant. In this context, we may have only a
single web page that contains the information we
need and no seed examples.
In this paper, we propose a novel task, zero-
shot entity extraction, where the specification
of the desired entities is provided as a natural
language query. Given a query (e.g., hiking
</bodyText>
<figure confidence="0.969329083333334">
webweb wepagespage pg
answers
Avalon Super Loop
Hilton Area
Wildlands Loop
...
web page
answers
Avalon Super Loop
Hilton Area
Wildlands Loop
...
</figure>
<figureCaption confidence="0.999361">
Figure 1: Entity extraction typically requires ad-
</figureCaption>
<bodyText confidence="0.987907346153846">
ditional knowledge such as a small set of seed ex-
amples or depends on multiple web pages. In our
setting, we take as input a natural language query
and extract entities from a single web page.
trails near Baltimore) and a web page (e.g.,
http://www.everytrail.com/best/
hiking-baltimore-maryland), the goal is
to extract all entities corresponding to the query
on that page (e.g., Avalon Super Loop, etc.).
Figure 1 summarizes the task setup.
The task introduces two challenges. Given a
single web page to extract entities from, we can
no longer rely on the redundancy of entities across
multiple web pages. Furthermore, in the zero-shot
learning paradigm (Larochelle et al., 2008), where
entire categories might be unseen during training,
the system must generalize to new queries and web
pages without the additional aid of seed examples.
To tackle these challenges, we cast the task as
a structured prediction problem where the input
is the query and the web page, and the output is
a list of entities, mediated by a latent extraction
predicate. To generalize across different inputs,
we rely on two types of features: structural fea-
tures, which look at the layout and placement of
the entities being extracted; and denotation fea-
</bodyText>
<figure confidence="0.994871375">
seeds
Avalon Super Loop
Hilton Area
traditional
query
hiking trails
near Baltimore
our system
</figure>
<page confidence="0.981488">
391
</page>
<note confidence="0.8303035">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 391–401,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999727928571428">
tures, which look at the list of entities as a whole
and assess their linguistic coherence. When defin-
ing features on lists, one technical challenge is be-
ing robust to widely varying list sizes. We ap-
proach this challenge by defining features over a
histogram of abstract tokens derived from the list
elements.
For evaluation, we created the OPENWEB
dataset comprising natural language queries from
the Google Suggest API and diverse web pages re-
turned from web search. Despite the variety of
queries and web pages, our system still achieves
a test accuracy of 40.5% and an accuracy at 5 of
55.8%.
</bodyText>
<sectionHeader confidence="0.987273" genericHeader="method">
2 Problem statement
</sectionHeader>
<bodyText confidence="0.999966352941176">
We define the zero-shot entity extraction task as
follows: let x be a natural language query (e.g.,
hiking trails near Baltimore), and w be a web
page. Our goal is to construct a mapping from
(x, w) to a list of entities y (e.g., [Avalon Super
Loop, Patapsco Valley State Park, ... ]) which are
extracted from the web page.
Ideally, we would want our data to be anno-
tated with the correct entity lists y, but this would
be very expensive to obtain. We instead define
each training and test example as a triple (x, w, c),
where the compatibility function c maps each y to
c(y) E 10, 11 denoting the (approximate) correct-
ness of the list y. In this paper, an entity list y is
compatible (c(y) = 1) when the first, second, and
last elements of y match the annotation; otherwise,
it is incompatible (c(y) = 0).
</bodyText>
<subsectionHeader confidence="0.959832">
2.1 Dataset
</subsectionHeader>
<bodyText confidence="0.995143153846154">
To experiment with a diverse set of queries and
web pages, we created a new dataset, OPENWEB,
using web pages from Google search results.1 We
use the method from Berant et al. (2013) to gen-
erate search queries by performing a breadth-first
search over the query space. Specifically, we
use the Google Suggest API, which takes a par-
tial query (e.g., “list of movies”) and out-
puts several complete queries (e.g., “list of hor-
ror movies”). We start with seed partial queries
“list of • ” where • is one or two initial let-
ters. In each step, we call the Google Suggest API
on the partial queries to obtain complete queries,
</bodyText>
<footnote confidence="0.996782666666667">
1The OPENWEB dataset and our code base are available
for download at http://www-nlp.stanford.edu/
software/web-entity-extractor-ACL2014.
</footnote>
<table confidence="0.996879235294117">
Full query New partial queries
list of X IN Y list of X
where IN is a preposition
(list of [hotels]X in [Guam]Y)
list of X
list of X IN
list of IN Y
list of X CC Y list of X
where CC is a conjunction
(list of [food]X and [drink]Y )
list of X
list of Y
list of Y
list of X w list of w
(list of [good 2012]X [movies],,,)
list of w
list of X
</table>
<tableCaption confidence="0.992981">
Table 1: Rules for generating new partial queries
</tableCaption>
<bodyText confidence="0.979363">
from complete queries. (X and Y are sequences
of words; w is a single word.)
and then apply the transformation rules in Table 1
to generate more partial queries from complete
queries. We run the procedure until we obtained
100K queries.
Afterwards, we downloaded the top 2–3 Google
search results of each query, sanitized the web
pages, and randomly submitted 8000 query / web
page pairs to Amazon Mechanical Turk (AMT).
Each AMT worker must either mark the web page
as irrelevant or extract the first, second, and last
entities from the page. We only included exam-
ples where at least two AMT workers agreed on
the answer.
The resulting OPENWEB dataset consists of
2773 examples from 2269 distinct queries.
Among these queries, there are 894 headwords
ranging from common categories (e.g., movies,
companies, characters) to more specific ones (e.g.,
enzymes, proverbs, headgears). The dataset con-
tains web pages from 1438 web domains, of which
83% appear only once in our dataset.
Figure 2 shows some queries and web pages
from the OPENWEB dataset. Besides the wide
range of queries, another main challenge of the
dataset comes from the diverse data representa-
tion formats, including complex tables, grids, lists,
headings, and paragraphs.
</bodyText>
<sectionHeader confidence="0.996647" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.998729875">
Figure 3 shows the framework of our system.
Given a query x and a web page w, the system
generates a set i(w) of extraction predicates z
which can extract entities from semi-structured
data in w. Section 3.1 describes extraction pred-
icates in more detail. Afterwards, the system
chooses z E i(w) that maximizes the model
probability pθ(z  |x, w), and then executes z on
</bodyText>
<page confidence="0.997309">
392
</page>
<table confidence="0.981362230769231">
Queries Examples (web page, query)
airlines of italy
natural causes of global warming
lsu football coaches
bf3 submachine guns
badminton tournaments
foods high in dha
technical colleges in south carolina
songs on glee season 5
singers who use auto tune
san francisco radio stations
actors from boston
airlines of italy natural causes of global warming lsu football coaches
</table>
<figureCaption confidence="0.9898775">
Figure 2: Some examples illustrating the diversity of queries and web pages from the OPENWEB dataset.
Figure 3: An overview of our system. The system
uses the input query x and web page w to produce
a list of entities y via an extraction predicate z.
</figureCaption>
<bodyText confidence="0.90413075">
w to get the list of entities y = Qz�,,,. Section 3.2
describes the model and the training procedure,
while Section 3.3 presents the features used in our
model.
</bodyText>
<subsectionHeader confidence="0.999724">
3.1 Extraction predicates
</subsectionHeader>
<bodyText confidence="0.999905857142857">
We represent each web page w as a DOM tree, a
common representation among wrapper induction
and web information extraction systems (Sahuguet
and Azavant, 1999; Liu et al., 2000; Crescenzi et
al., 2001). The text of any DOM tree node that is
shorter than 140 characters is a candidate entity.
However, without further restrictions, the number
of possible entity lists grows exponentially with
the number of candidate entities.
To make the problem tractable, we introduce an
extraction predicate z as an intermediate represen-
tation for extracting entities from w. In our sys-
tem, we let an extraction predicate be a simplified
XML path (XPath) such as
</bodyText>
<equation confidence="0.302021">
/html[1]/body[1]/table[2]/tr/td[1]
</equation>
<bodyText confidence="0.999600564102564">
Informally, an extraction predicate is a list of
path entries. Each path entry is either a tag (e.g.,
tr), which selects all children with that tag; or a
tag and an index i (e.g., td[1]), which selects
only the ith child with that tag. The denotation
y = Qz�,,, of an extraction predicate z is the list of
entities selected by the XPath. Figure 4 illustrates
the execution of the extraction predicate above on
a DOM tree.
In the literature, many information extraction
systems employ more versatile extraction predi-
cates (Wang and Cohen, 2009; Fumarola et al.,
2011). However, despite the simplicity, we are
able to find an extraction predicate that extracts
a compatible entity list in 69.7% of the develop-
ment examples. In some examples, we cannot ex-
tract a compatible list due to unrecoverable issues
such as incorrect annotation. Section 4.4 provides
a detailed analysis of these issues. Additionally,
extraction predicates can be easily extended to in-
crease the coverage. For example, by introduc-
ing new index types [1:] (selects all but the first
node) and [:-1] (selects all but the last node),
we can increase the coverage to 76.2%.
Extraction predicate generation. We generate
a set Z(w) of extraction predicates for a given
web page w as follows. For each node in
the DOM tree, we find an extraction predicate
which selects only that node, and then gener-
alizes the predicate by removing any subset of
the indices of the last k path entries. For in-
stance, when k = 2, an extraction predicate
ending in .../tr[5]/td[2] will be general-
ized to .../tr[5]/td[2], .../tr/td[2],
.../tr[5]/td, and .../tr/td. In all ex-
periments, we use k = 8, which gives at most 28
generalized predicates for each original predicate.
This generalization step allows the system to se-
lect multiple nodes with the same structure (e.g.,
</bodyText>
<figure confidence="0.9862025">
hiking trails
near Baltimore
Model
[Avalon Super Loop, Hilton Area, ...]
y
x
Generation
Z
w
head
...
html
body
...
/html[1]/body[1]/table[2]/tr/td[1]
Execution
z
393
DOM tree w
Extraction predicate z
/html[1]/body[1]/table[2]/tr/td[1]
Rendered web page
h1
table
table
html
head body
Home..
td
Expl..
td
tr
Mobi..
td
Crea..
td
Hiki..
Name..
th
tr
Loca..
th
Aval..
td
tr
12.7..
td
... tr
td
Gove..
3.1 ..
td
Home Explore Mobile Apps Create Trip
Hiking near Baltimore, Maryland
Name Length
Avalon Super Loop 12.7 miles
Hilton Area 7.8 miles
Avalon Loop 9.4 miles
Wildlands Loop 4.4 miles
Mckeldin Area 16.7 miles
Greenbury Point 3.7 miles
Governer Bridge Natural Area 3.1 miles
</figure>
<figureCaption confidence="0.7567935">
Figure 4: A simplified example of a DOM tree w and an extraction predicate z, which selects a list of
entity strings y = JzKw from the page (highlighted in red).
</figureCaption>
<bodyText confidence="0.999343">
table cells from the same column or list items from
the same section of the page).
Out of all generalized extraction predicates, we
retain the ones that extract at least two entities
from w. Note that several extraction predicates
may select the same list of nodes and thus produce
the same list of entities.
The procedure above gives a manageable num-
ber of extraction predicates. Among the devel-
opment examples of the OPENWEB dataset, we
generate an average of 8449 extraction predicates
per example, which evaluate to an average of 1209
unique entity lists.
</bodyText>
<subsectionHeader confidence="0.999784">
3.2 Modeling
</subsectionHeader>
<bodyText confidence="0.956588333333333">
Given a query x and a web page w, we define
a log-linear distribution over all extraction predi-
cates z E i(w) as
</bodyText>
<equation confidence="0.934236">
pθ(z  |x, w) a exp{θTφ(x, w, z)J, (1)
</equation>
<bodyText confidence="0.9925405">
where θ E Rd is the parameter vector and
φ(x, w, z) is the feature vector, which will be de-
fined in Section 3.3.
To train the model, we find a parameter vec-
tor θ that maximizes the regularized log marginal
probability of the compatibility function being sat-
isfied. In other words, given training data D =
{(x(i), w(i), c(i))Jni=1, we find θ that maximizes
</bodyText>
<equation confidence="0.848644">
log pθ(c(i) = 1  |x(i),w(i)) - λ211θ1122
</equation>
<bodyText confidence="0.871141">
where
</bodyText>
<equation confidence="0.945727">
�pθ(c = 1  |x, w) = pθ(z  |x, w) - c(JzKw).
z∈Z(w)
</equation>
<bodyText confidence="0.91825175">
Note that c(JzKw) = 1 when the entity list y =
JzKw selected by z is compatible with the annota-
tion; otherwise, c(JzKw) = 0.
We use AdaGrad, an online gradient descent
with an adaptive per-feature step size (Duchi et al.,
2010), making 5 passes over the training data. We
use λ = 0.01 obtained from cross-validation for
all experiments.
</bodyText>
<subsectionHeader confidence="0.960565">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.999970454545455">
To construct the log-linear model, we define a fea-
ture vector φ(x, w, z) for each query x, web page
w, and extraction predicate z. The final feature
vector is the concatenation of structural features
φs(w, z), which consider the selected nodes in
the DOM tree, and denotation features φd(x, y),
which look at the extracted entities.
We will use the query hiking trails near Balti-
more and the web page in Figure 4 as a running
example. Figure 5 lists some features extracted
from the example.
</bodyText>
<sectionHeader confidence="0.546909" genericHeader="method">
3.3.1 Recipe for defining features on lists
</sectionHeader>
<bodyText confidence="0.999941538461538">
One main focus of our work is finding good fea-
ture representations for a list of objects (DOM tree
nodes for structural features and entity strings for
denotation features). One approach is to define the
feature vector of a list to be the sum of the feature
vectors of individual elements. This is commonly
done in structured prediction, where the elements
are local configurations (e.g., rule applications in
parsing). However, this approach raises a normal-
ization issue when we have to compare and rank
lists of drastically different sizes.
As an alternative, we propose a recipe for gen-
erating features from a list as follows:
</bodyText>
<equation confidence="0.625672">
n
i=1
</equation>
<page confidence="0.949696">
394
</page>
<figure confidence="0.977042142857143">
head body
h1 table
tr
... tr
tr
tr
td td
th th
td td
td td
table
...
Structural feature Value
htinl
</figure>
<table confidence="0.931423178571429">
Features on selected nodes:
TAG-MAJORITY = td 1
INDEX-ENTROPY 0.0
Features on parent nodes:
CHILDRENCOUNT-MAJORITY = 2 1
PARENT-SINGLE 1
INDEX-ENTROPY 1.0
HEADHOLE (The first node is skipped) 1
Features on grandparent nodes:
PAGECOVERAGE 0.6
Selected entities
Avalon Super Loop
Hilton Area
Avalon Loop
Wildlands Loop
Mckeldin Area
Greenbury Point
Governer Bridge Natural Area
Denotation feature Value
WORDSCOUNT-MEAN 2.42
PHRASESHAPE-MAJORITY = Aa Aa 1
PHRASESHAPE-MAJORITYRATIO 0.71
WORDSHAPE-MAJORITY = Aa 1
PHRASEPOS-MAJORITY = NNP NN 1
LASTWORD-ENTROPY 0.74
WORDPOS = NN (normalized count) 0.53
. . . . . .
. . . . . .
</table>
<figureCaption confidence="0.928353833333333">
Figure 5: A small subset of features from the example hiking trails near Baltimore in Figure 4.
Figure 6: The recipe for defining features on a
list of objects: (i) the abstraction step converts list
elements into abstract tokens; (ii) the aggregation
step defines features using the histogram of the ab-
stract tokens.
</figureCaption>
<bodyText confidence="0.981392782608695">
Step 1: Abstraction. We map each list element
into an abstract token. For example, we can map
each DOM tree node onto an integer equal to the
number of children, or map each entity string onto
its part-of-speech tag sequence.
Step 2: Aggregation. We create a histogram of
the abstract tokens and define features on proper-
ties of the histogram. Generally, we use ENTROPY
(entropy normalized to the maximum value of 1),
MAJORITY (mode), MAJORITYRATIO (percent-
age of tokens sharing the majority value), and
SINGLE (whether all tokens are identical). For
abstract tokens with finitely many possible values
(e.g., part-of-speech), we also use the normalized
histogram count of each possible value as a fea-
ture. And for real-valued abstract tokens, we also
use the mean and the standard deviation. In the
actual system, we convert real-valued features (en-
tropy, histogram count, mean, and standard devia-
tion) into indicator features by binning.
Figure 6 summarizes the steps explained above.
We use this recipe for defining both structural and
denotation features, which are discussed below.
</bodyText>
<subsectionHeader confidence="0.602478">
3.3.2 Structural features
</subsectionHeader>
<bodyText confidence="0.987942666666667">
Although different web pages represent data in
different formats, they still share some common
hierarchical structures in the DOM tree. To cap-
ture this, we define structural features φs(w, z),
which consider the properties of the selected nodes
in the DOM tree, as follows:
Features on selected nodes. We apply our
recipe on the list of nodes in w selected by z using
the following abstract tokens:
</bodyText>
<listItem confidence="0.9884005">
• TAG, ID, CLASS, etc. (HTML attributes)
• CHILDRENCOUNT and SIBLINGSCOUNT
(number of children and siblings)
• INDEX (position among its siblings)
• PARENT (parent node; e.g., PARENT-SINGLE
means that all nodes share the same parent.)
</listItem>
<bodyText confidence="0.7409515">
Additionally, we define the following features
based on the coverage of all selected nodes:
</bodyText>
<listItem confidence="0.97735925">
• NOHOLE, HEADHOLE, etc. (node coverage
in the same DOM tree level; e.g., HEAD-
HOLE activates when the first sibling of the
selected nodes is not selected.)
</listItem>
<figure confidence="0.97874525">
2
1
1
0
0
A B C D E
Abstraction
Aggregation
0 1 2
histogram
ENTRoPY
MAJoRITY
MAJoRITYRATIo
SINGLE
(MEAN)
(VARIANCE)
</figure>
<page confidence="0.99336">
395
</page>
<bodyText confidence="0.886183888888889">
• PAGECOVERAGE (node coverage relative to
the entire tree; we use depth-first traversal
timestamps to estimate the fraction of nodes
in the subtrees of the selected nodes.)
Features on ancestor nodes. We also define the
same feature set on the list of ancestors of the se-
lected nodes in the DOM tree. In our experiments,
we traverse up to 5 levels of ancestors and define
features from the nodes in each level.
</bodyText>
<subsectionHeader confidence="0.735771">
3.3.3 Denotation features
</subsectionHeader>
<bodyText confidence="0.9999700625">
Structural features are not powerful enough to dis-
tinguish between entity lists appearing in similar
structures such as columns of the same table or
fields of the same record. To solve this ambiguity,
we introduce denotation features od(x, y) which
considers the coherence or appropriateness of the
selected entity strings y = H-.
We observe that the correct entities often share
some linguistic statistics. For instance, entities in
many categories (e.g., people and place names)
usually have only 2–3 word tokens, most of which
are proper nouns. On the other hand, random
words on the web page tend to have more diverse
lengths and part-of-speech tags.
We apply our recipe on the list of selected enti-
ties using the following abstract tokens:
</bodyText>
<listItem confidence="0.9986472">
• WORDSCOUNT (number of words)
• PHRASESHAPE (abstract shape of the phrase;
e.g., Barack Obama becomes Aa Aa)
• WORDSHAPE (abstract shape of each word;
the number of abstract tokens will be the total
number of words over all selected entities)
• FIRSTWORD and LASTWORD
• PHRASEPOS and WORDPOS (part-of-
speech tags for whole phrases and individual
words)
</listItem>
<sectionHeader confidence="0.999223" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9961285">
In this section we evaluate our system on the
OPENWEB dataset.
</bodyText>
<subsectionHeader confidence="0.987065">
4.1 Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.9961105">
Accuracy. As the main metric, we use a notion
of accuracy based on compatibility; specifically,
we define the accuracy as the fraction of examples
where the system predicts a compatible entity list
as defined in Section 2. We also report accuracy
at 5, the fraction of examples where the top five
predictions contain a compatible entity list.
Path suffix pattern (multiset) Count
{a, table, tbody, td[*], tr} 1792
{a, tbody, td[*], text, tr} 1591
{a, table[*], tbody, td[*], tr} 1325
{div, table, tbody, td[*], tr} 1259
{b, div, div, div, div[*]} 1156
{div[*], table, tbody, td[*], tr} 1059
{div, table[*], tbody, td[*], tr} 844
{table, tbody, td[*], text, tr} 828
{div[*], table[*], tbody, td[*],tr} 793
{a, table, tbody, td, tr} 743
Table 2: Top 10 path suffix patterns found by the
baseline learner in the development data. Since
we allow path entries to be permuted, each suffix
pattern is represented by a multiset of path entries.
The notation [*] denotes any path entry index.
To see how our compatibility-based accuracy
tracks exact correctness, we sampled 100 web
pages which have at least one valid extraction
predicate and manually annotated the full list of
entities. We found that in 85% of the examples,
the longest compatible list y is the correct list of
entities, and many lists in the remaining 15% miss
the correct list by only a few entities.
Oracle. In some examples, our system cannot
find any list of entities that is compatible with the
gold annotation. The oracle score is the fraction
of examples in which the system can find at least
one compatible list.
</bodyText>
<subsectionHeader confidence="0.95111">
4.2 Baseline
</subsectionHeader>
<bodyText confidence="0.999994727272727">
As a baseline, we list the suffixes of the cor-
rect extraction predicates in the training data, and
then sort the resulting suffix patterns by frequency.
To improve generalization, we treat path entries
with different indices (e.g., td[1] vs. td[2]) as
equivalent and allow path entries to be permuted.
Table 2 lists the top 10 suffix patterns from the de-
velopment data. At test time, we choose an extrac-
tion predicate with the most frequent suffix pat-
tern. The baseline should work considerably well
if the web pages were relatively homogeneous.
</bodyText>
<subsectionHeader confidence="0.999612">
4.3 Main results
</subsectionHeader>
<bodyText confidence="0.9999895">
We held out 30% of the dataset as test data. For the
results on development data, we report the average
across 10 random 80-20 splits. Table 3 shows the
results. The system gets an accuracy of 41.1% and
40.5% for the development and test data, respec-
tively. If we consider the top 5 lists of entities, the
accuracy increases to 58.4% on the development
data and 55.8% on the test data.
</bodyText>
<page confidence="0.9982">
396
</page>
<table confidence="0.9790164">
Development data Test data
Acc A@5 Acc A@5
Baseline 10.8 f 1.3 25.6 f 2.0 10.3 20.9
Our system 41.1 f 3.4 58.4 f 2.7 40.5 55.8
Oracle 68.7 f 2.4 68.7 f 2.4 66.6 66.6
</table>
<tableCaption confidence="0.874906666666667">
Table 3: Main results on the OPENWEB dataset
using the default set of features. (Acc = accuracy,
A@5 = accuracy at 5)
</tableCaption>
<subsectionHeader confidence="0.994732">
4.4 Error analysis
</subsectionHeader>
<bodyText confidence="0.999989195121951">
We now investigate the errors made by our system
using the development data. We classify the er-
rors into two types: (i) coverage errors, which are
when the system cannot find any entity list satis-
fying the compatibility function; and (ii) ranking
errors, which are when a compatible list of entities
exists, but the system outputs an incompatible list.
Tables 4 and 5 show the breakdown of cover-
age and ranking errors from an experiment on the
development data.
Analysis of coverage errors. From Table 4,
about 36% of coverage errors happen when the
extraction predicate for the correct entities also
captures unrelated parts of the web page (Reason
C1). For example, many Wikipedia articles have
the See Also section that lists related articles in an
unordered list (/ul/li/a), which causes a prob-
lem when the entities are also represented in the
same format.
Another main source of errors is the in-
consistency in HTML tag usage (Reason C2).
For instance, some web pages use &lt;b&gt; and
&lt;strong&gt; tags for bold texts interchangeably,
or switch between &lt;b&gt;&lt;a&gt;...&lt;/a&gt;&lt;/b&gt; and
&lt;a&gt;&lt;b&gt;...&lt;/b&gt;&lt;/a&gt; across entities. We ex-
pect that this problem can be solved by normaliz-
ing the web page, using an alternative web page
representation (Cohen et al., 2002; Wang and Co-
hen, 2009; Fumarola et al., 2011), or leveraging
more expressive extraction predicates (Dalvi et al.,
2011).
One interesting source of errors is Reason C3,
where we need to filter the selected entities to
match the complex requirement in the query. For
example, the query tech companies in China re-
quires the system to select only the company
names with China in the corresponding location
column. To handle such queries, we need a deeper
understanding of the relation between the linguis-
tic structure of the query and the hierarchical
structure of the web page. Tackling this error re-
</bodyText>
<table confidence="0.9996518">
Setting Acc A@5
All features 41.1 f 3.4 58.4 f 2.7
Oracle 68.7 f 2.4 68.7 f 2.4
(Section 4.5)
Structural features only 36.2 f 1.9 54.5 f 2.5
Denotation features only 19.8 f 2.5 41.7 f 2.7
(Section 4.6)
Structural + query-denotation 41.7 f 2.5 58.1 f 2.4
Query-denotation features only 25.0 f 2.3 48.0 f 2.7
Concat. a random web page + 19.3 f 2.6 41.2 f 2.3
structural + denotation
Concat. a random web page + 29.2 f 1.7 49.2 f 2.2
structural + query-denotation
(Section 4.7)
Add 1 seed entity 52.9 f 3.0 66.5 f 2.5
</table>
<tableCaption confidence="0.969329">
Table 6: System accuracy with different feature
</tableCaption>
<bodyText confidence="0.9936898">
and input settings on the development data. (Acc
= accuracy, A@5 = accuracy at 5)
quires compositionality and is critical to general-
ize to more complex queries.
Analysis of ranking errors. From Table 5, a
large number of errors are attributed to the system
selecting non-content elements such as navigation
links and content headings (Reason R1). Feature
analysis reveals that both structural and linguis-
tic statistics of these non-content elements can be
more coherent than those of the correct entities.
We suspect that since many of our features try to
capture the coherence of entities, the system some-
times erroneously favors the more homonogenous
non-content parts of the page. To disfavor these
parts, One possible solution is to add visual fea-
tures that capture how the web page is rendered
and favor more salient parts of the page. (Liu et al.,
2003; Song et al., 2004; Zhu et al., 2005; Zheng et
al., 2007).
</bodyText>
<subsectionHeader confidence="0.995058">
4.5 Feature variations
</subsectionHeader>
<bodyText confidence="0.999924538461539">
We now investigate the contribution of each fea-
ture type. The ablation results on the development
set over 10 random splits are shown in Table 6.
We observe that denotation features improves ac-
curacy on top of structural features.
Table 7 shows an example of an error that is
eliminated by each feature type. Generally, if
the entities are represented as records (e.g., rows
of a table), then denotation features will help the
system select the correct field from each record.
On the other hand, structural features prevent the
system from selecting random entities outside the
main part of the page.
</bodyText>
<page confidence="0.995571">
397
</page>
<note confidence="0.925355363636364">
Reason Short example Count
C1 Answers and contextual elements are selected Select entries in See Also section in addition to the con- 48
by the same extraction predicate. tent because they are all list entries.
C2 HTML tag usage is inconsistent. The page uses both b and strong for headers. 16
C3 The query applies to only some sections of the Need to select only companies in China from the table 20
matching entities. of all Asian companies.
C4 Answers are embedded in running text. Answers are in a comma-separated list. 13
C5 Text normalization issues. Selected Silent Night Lyrics instead of Silent Night. 19
C6 Other issues. Incorrect annotation. / Entities are permuted when the 18
web page is rendered. / etc.
Total 134
</note>
<tableCaption confidence="0.998161">
Table 4: Breakdown of coverage errors from the development data.
</tableCaption>
<table confidence="0.999128416666667">
Reason Short example Count
R1 Select non-content strings. Select navigation links, headers, footers, or sidebars. 25
R2 Select entities from a wrong field. Select book authors instead of book names. 22
R3 Select entities from the wrong section(s). For the query schools in Texas, select all schools on the 19
page, or select the schools in Alabama instead.
R4 Also select headers or footers. Select the table header in addition to the answers. 7
R5 Select only entities with a particular formatting. From a list of answers, select only anchored (a) entities. 4
R6 Select headings instead of the contents or vice Select the categories of rums in h2 tags instead of the 2
versa. rum names in the tables.
R7 Other issues. Incorrect annotation. / Multiple sets of answers appear 9
on the same page. / etc.
Total 88
</table>
<tableCaption confidence="0.984288">
Table 5: Breakdown of ranking errors from the development data.
</tableCaption>
<table confidence="0.999132">
All features Structural only Denotation only
The Sun CIRC: 2,279,492 Paperboy Australia
Daily Mail CIRC: 1,821,684 Paperboy UK
Daily Mirror CIRC: 1,032,144 Paperboy Home Page
. . . . . . . . .
</table>
<tableCaption confidence="0.975196">
Table 7: System outputs for the query UK news-
</tableCaption>
<bodyText confidence="0.98073">
papers with different feature sets. Without deno-
tation features, the system selects the daily circu-
lation of each newspaper instead of the newspaper
names. And without structural features, the sys-
tem selects the hidden navigation links from the
top of the page.
</bodyText>
<subsectionHeader confidence="0.987604">
4.6 Incorporating query information
</subsectionHeader>
<bodyText confidence="0.999889761904762">
So far, note that all our features depend only on
the extraction predicate z and not the input query
x. Remarkably, we were still able to obtain rea-
sonable results. One explanation is that since we
obtained the web pages from a search engine, the
most prominent entities on the web pages, such as
entities in table cells in the middle of the page, are
likely to be good independent of the query.
However, different queries often denote enti-
ties with different linguistic properties. For exam-
ple, queries mayors of Chicago and universities in
Chicago will produce entities of different lengths,
part-of-speech sequences, and word distributions.
This suggests incorporating features that depend
on the query.
To explore the potential of query informa-
tion, we conduct the following oracle experi-
ment. We replace each denotation feature f(y)
with a corresponding query-denotation feature
(f(y), g(x)), where g(x) is the category of the
query x. We manually classified all queries in our
dataset into 7 categories: person, media title, loca-
tion/organization, abtract entity, word/phrase, ob-
ject name, and miscellaneous.
Table 8 shows some examples where adding
these query-denotation features improves the se-
lected entity lists by favoring answers that are
more suitable to the query category. However, Ta-
ble 6 shows that these new features do not signifi-
cantly improve the accuracy of our original system
on the development data.
We suspect that any gains offered by the query-
denotation features are subsumed by the structural
features. To test this hypothesis, we conducted
two experiments, the results of which are shown
in Table 6. First, we removed structural features
and found that using query-denotation features im-
proves accuracy significantly over using denota-
tion features alone from 19.8% to 25.0%. Second,
we created a modified dataset where the web page
in each example is a concatenation of the orig-
inal web page and an unrelated web page. On
</bodyText>
<page confidence="0.994055">
398
</page>
<table confidence="0.999213428571428">
Query euclid’s elements book titles soft drugs professional athletes with concussions
Default “Prematter”, “Book I.”, “Hard drugs”, “Soft drugs”, “Pistons-Knicks Game Becomes Site
features “Book II.”, “Book III.”, ... “Some drugs cannot be of Incredible Dance Battle”, “Toronto
classified that way”, ... Mayor Rob Ford Attends ... ”, . . .
Structural (category = media title) (category = object name) (category = person)
+ Query- “Book I. The fundamentals ... ”, “methamphetamine”, “Mike Richter”, “Stu Grimson”,
Denotation “Book II. Geometric algebra”, ... “psilocybin”, “caffeine” “Geoff Courtnall”, .. .
</table>
<tableCaption confidence="0.999014">
Table 8: System outputs after changing denotation features into query-denotation features.
</tableCaption>
<bodyText confidence="0.99986625">
this modified dataset, the prominent entities may
not be the answers to the query. Here, query-
denotation features improves accuracy over deno-
tation features alone from 19.3% to 29.2%.
</bodyText>
<subsectionHeader confidence="0.998564">
4.7 Comparison with other problem settings
</subsectionHeader>
<bodyText confidence="0.999939090909091">
Since zero-shot entity extraction is a new task,
we cannot directly compare our system with other
systems. However, we can mimic the settings of
other tasks. In one experiment, we augment each
input query with a single seed entity (the second
annotated entity in our experiments); this setting
is suggestive of Wang and Cohen (2009). Table 6
shows that this augmentation increases accuracy
from 41.1% to 52.9%, suggesting that our sys-
tem can perform substantially better with a small
amount of additional supervision.
</bodyText>
<sectionHeader confidence="0.999799" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999996362068966">
Our work shares a base with the wrapper induc-
tion literature (Kushmerick, 1997) in that it lever-
ages regularities of web page structures. However,
wrapper induction usually focuses on a small set
of web domains, where the web pages in each do-
main follow a fixed template (Muslea et al., 2001;
Crescenzi et al., 2001; Cohen et al., 2002; Arasu
and Garcia-Molina, 2003). Later work in web data
extraction attempts to generalize across different
web pages, but relies on either restricted data for-
mats (Wong et al., 2009) or prior knowledge of
web page structures with respect to the type of data
to extract (Zhang et al., 2013).
In our case, we only have the natural language
query, which presents the more difficult problem
of associating the entity class in the query (e.g.,
hiking trails) to concrete entities (e.g., Avalon Su-
per Loop). In contrast to information extraction
systems that extract homogeneous records from
web pages (Liu et al., 2003; Zheng et al., 2009),
our system must choose the correct field from each
record and also identify the relevant part of the
page based on the query.
Another related line of work is information ex-
traction from text, which relies on natural lan-
guage patterns to extract categories and relations
of entities. One classic example is Hearst pat-
terns (Hearst, 1992; Etzioni et al., 2005), which
can learn new entities and extraction patterns from
seed examples. More recent approaches also
leverage semi-structured data to obtain more ro-
bust extraction patterns (Mintz et al., 2009; Hoff-
mann et al., 2011; Surdeanu et al., 2012; Riedel
et al., 2013). Although our work focuses on semi-
structured web pages rather than raw text, we use
linguistic patterns of queries and entities as a sig-
nal for extracting appropriate answers.
Additionally, our efforts can be viewed as build-
ing a lexicon on the fly. In recent years, there
has been a drive to scale semantic parsing to large
databases such as Freebase (Cai and Yates, 2013;
Berant et al., 2013; Kwiatkowski et al., 2013).
However, despite the best efforts of information
extraction, such databases will always lag behind
the open web. For example, Berant et al. (2013)
found that less than 10% of naturally occurring
questions are answerable by a simple Freebase
query. By using the semi-structured data from the
web as a knowledge base, we hope to increase fact
coverage for semantic parsing.
Finally, as pointed out in the error analysis, we
need to filter or aggregate the selected entities for
complex queries (e.g., tech companies in China for
a web page with all Asian tech companies). In fu-
ture work, we would like to explore the issue of
compositionality in queries by aligning linguistic
structures in natural language with the relative po-
sition of entities on web pages.
</bodyText>
<sectionHeader confidence="0.99873" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.96081875">
We gratefully acknowledge the support of the
Google Natural Language Understanding Focused
Program. In addition, we would like to thank
anonymous reviewers for their helpful comments.
</bodyText>
<page confidence="0.998753">
399
</page>
<sectionHeader confidence="0.998332" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999858067307692">
A. Arasu and H. Garcia-Molina. 2003. Extracting
structured data from web pages. In ACM SIGMOD
international conference on Management of data,
pages 337–348.
J. Berant, A. Chou, R. Frostig, and P. Liang. 2013.
Semantic parsing on Freebase from question-answer
pairs. In Empirical Methods in Natural Language
Processing (EMNLP).
Q. Cai and A. Yates. 2013. Large-scale semantic pars-
ing via schema matching and lexicon extension. In
Association for Computational Linguistics (ACL).
W. W. Cohen, M. Hurst, and L. S. Jensen. 2002. A
flexible learning system for wrapping tables and lists
in HTML documents. In World Wide Web (WWW),
pages 232–241.
V. Crescenzi, G. Mecca, P. Merialdo, et al. 2001.
Roadrunner: Towards automatic data extraction
from large web sites. In VLDB, volume 1, pages
109–118.
N. Dalvi, R. Kumar, and M. Soliman. 2011. Auto-
matic wrappers for large scale web extraction. Pro-
ceedings of the VLDB Endowment, 4(4):219–230.
B. Dalvi, W. Cohen, and J. Callan. 2012. Websets:
Extracting sets of entities from the web using unsu-
pervised information extraction. In Web Search and
Data Mining (WSDM), pages 243–252.
J. Duchi, E. Hazan, and Y. Singer. 2010. Adaptive sub-
gradient methods for online learning and stochastic
optimization. In Conference on Learning Theory
(COLT).
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. S. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from
the web: An experimental study. Artificial Intelli-
gence, 165(1):91–134.
F. Fumarola, T. Weninger, R. Barber, D. Malerba, and
J. Han. 2011. Extracting general lists from web doc-
uments: A hybrid approach. Modern Approaches in
Applied Intelligence Springer.
M. A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Interational
Conference on Computational linguistics, pages
539–545.
R. Hoffmann, C. Zhang, X. Ling, L. S. Zettlemoyer,
and D. S. Weld. 2011. Knowledge-based weak su-
pervision for information extraction of overlapping
relations. In Association for Computational Lin-
guistics (ACL), pages 541–550.
N. Kushmerick. 1997. Wrapper induction for informa-
tion extraction. Ph.D. thesis, University of Washing-
ton.
T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer.
2013. Scaling semantic parsers with on-the-fly on-
tology matching. In Empirical Methods in Natural
Language Processing (EMNLP).
H. Larochelle, D. Erhan, and Y. Bengio. 2008. Zero-
data learning of new tasks. In AAAI, volume 8,
pages 646–651.
L. Liu, C. Pu, and W. Han. 2000. XWRAP: An XML-
enabled wrapper construction system for web infor-
mation sources. In Data Engineering, 2000. Pro-
ceedings. 16th International Conference on, pages
611–621.
B. Liu, R. Grossman, and Y. Zhai. 2003. Mining data
records in web pages. In Proceedings of the ninth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 601–606.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009.
Distant supervision for relation extraction without
labeled data. In Association for Computational Lin-
guistics (ACL), pages 1003–1011.
I. Muslea, S. Minton, and C. A. Knoblock. 2001. Hi-
erarchical wrapper induction for semistructured in-
formation sources. Autonomous Agents and Multi-
Agent Systems, 4(1):93–114.
S. Riedel, L. Yao, and A. McCallum. 2013. Re-
lation extraction with matrix factorization and uni-
versal schemas. In North American Association for
Computational Linguistics (NAACL).
A. Sahuguet and F. Azavant. 1999. WysiWyg web
wrapper factory (W4F). In WWW Conference.
R. Song, H. Liu, J. Wen, and W. Ma. 2004. Learning
block importance models for web pages. In World
Wide Web (WWW), pages 203–211.
M. Surdeanu, J. Tibshirani, R. Nallapati, and C. D.
Manning. 2012. Multi-instance multi-label learning
for relation extraction. In Empirical Methods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning (EMNLP/CoNLL), pages
455–465.
R. C. Wang and W. W. Cohen. 2009. Character-level
analysis of semi-structured documents for set expan-
sion. In Empirical Methods in Natural Language
Processing (EMNLP), pages 1503–1512.
Y. W. Wong, D. Widdows, T. Lokovic, and K. Nigam.
2009. Scalable attribute-value extraction from semi-
structured text. In IEEE International Conference
on Data Mining Workshops, pages 302–307.
Z. Zhang, K. Q. Zhu, H. Wang, and H. Li. 2013. Au-
tomatic extraction of top-k lists from the web. In
International Conference on Data Engineering.
S. Zheng, R. Song, and J. Wen. 2007. Template-
independent news extraction based on visual consis-
tency. In AAAI, volume 7, pages 1507–1513.
</reference>
<page confidence="0.963504">
400
</page>
<reference confidence="0.998235">
S. Zheng, R. Song, J. Wen, and C. L. Giles. 2009. Ef-
ficient record-level wrapper induction. In Proceed-
ings of the 18th ACM conference on Information and
knowledge management, pages 47–56.
J. Zhu, Z. Nie, J. Wen, B. Zhang, and W. Ma. 2005.
2D conditional random fields for web information
extraction. In International Conference on Machine
Learning (ICML), pages 1044–1051.
</reference>
<page confidence="0.998641">
401
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.962183">
<title confidence="0.996558">Zero-shot Entity Extraction from Web Pages</title>
<author confidence="0.996375">Panupong Pasupat Percy Liang</author>
<affiliation confidence="0.999862">Computer Science Department Computer Science Department Stanford University Stanford University</affiliation>
<email confidence="0.998264">ppasupat@cs.stanford.edupliang@cs.stanford.edu</email>
<abstract confidence="0.998605428571429">In order to extract entities of a fine-grained category from semi-structured data in web pages, existing information extraction systems rely on seed examples or redundancy across multiple web pages. In this paper, we consider a new zero-shot learning task of extracting entities specified by a natural language query (in place of seeds) given only a single web page. Our approach defines a log-linear model over latent extraction predicates, which select lists of entities from the web page. The main challenge is to define features on widely varying candidate entity lists. We tackle this by abstracting list elements and using aggregate statistics to define features. Finally, we created a new dataset of diverse queries and web pages, and show that our system achieves significantly better accuracy than a natural baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Arasu</author>
<author>H Garcia-Molina</author>
</authors>
<title>Extracting structured data from web pages.</title>
<date>2003</date>
<booktitle>In ACM SIGMOD international conference on Management of data,</booktitle>
<pages>337--348</pages>
<contexts>
<context position="32216" citStr="Arasu and Garcia-Molina, 2003" startWordPosition="5396" endWordPosition="5399">xperiments); this setting is suggestive of Wang and Cohen (2009). Table 6 shows that this augmentation increases accuracy from 41.1% to 52.9%, suggesting that our system can perform substantially better with a small amount of additional supervision. 5 Discussion Our work shares a base with the wrapper induction literature (Kushmerick, 1997) in that it leverages regularities of web page structures. However, wrapper induction usually focuses on a small set of web domains, where the web pages in each domain follow a fixed template (Muslea et al., 2001; Crescenzi et al., 2001; Cohen et al., 2002; Arasu and Garcia-Molina, 2003). Later work in web data extraction attempts to generalize across different web pages, but relies on either restricted data formats (Wong et al., 2009) or prior knowledge of web page structures with respect to the type of data to extract (Zhang et al., 2013). In our case, we only have the natural language query, which presents the more difficult problem of associating the entity class in the query (e.g., hiking trails) to concrete entities (e.g., Avalon Super Loop). In contrast to information extraction systems that extract homogeneous records from web pages (Liu et al., 2003; Zheng et al., 20</context>
</contexts>
<marker>Arasu, Garcia-Molina, 2003</marker>
<rawString>A. Arasu and H. Garcia-Molina. 2003. Extracting structured data from web pages. In ACM SIGMOD international conference on Management of data, pages 337–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Berant</author>
<author>A Chou</author>
<author>R Frostig</author>
<author>P Liang</author>
</authors>
<title>Semantic parsing on Freebase from question-answer pairs.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="5206" citStr="Berant et al. (2013)" startWordPosition="854" endWordPosition="857">correct entity lists y, but this would be very expensive to obtain. We instead define each training and test example as a triple (x, w, c), where the compatibility function c maps each y to c(y) E 10, 11 denoting the (approximate) correctness of the list y. In this paper, an entity list y is compatible (c(y) = 1) when the first, second, and last elements of y match the annotation; otherwise, it is incompatible (c(y) = 0). 2.1 Dataset To experiment with a diverse set of queries and web pages, we created a new dataset, OPENWEB, using web pages from Google search results.1 We use the method from Berant et al. (2013) to generate search queries by performing a breadth-first search over the query space. Specifically, we use the Google Suggest API, which takes a partial query (e.g., “list of movies”) and outputs several complete queries (e.g., “list of horror movies”). We start with seed partial queries “list of • ” where • is one or two initial letters. In each step, we call the Google Suggest API on the partial queries to obtain complete queries, 1The OPENWEB dataset and our code base are available for download at http://www-nlp.stanford.edu/ software/web-entity-extractor-ACL2014. Full query New partial qu</context>
<context position="33835" citStr="Berant et al., 2013" startWordPosition="5669" endWordPosition="5672">traction patterns from seed examples. More recent approaches also leverage semi-structured data to obtain more robust extraction patterns (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2013). Although our work focuses on semistructured web pages rather than raw text, we use linguistic patterns of queries and entities as a signal for extracting appropriate answers. Additionally, our efforts can be viewed as building a lexicon on the fly. In recent years, there has been a drive to scale semantic parsing to large databases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013). However, despite the best efforts of information extraction, such databases will always lag behind the open web. For example, Berant et al. (2013) found that less than 10% of naturally occurring questions are answerable by a simple Freebase query. By using the semi-structured data from the web as a knowledge base, we hope to increase fact coverage for semantic parsing. Finally, as pointed out in the error analysis, we need to filter or aggregate the selected entities for complex queries (e.g., tech companies in China for a web page with all Asian tech companies). I</context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>J. Berant, A. Chou, R. Frostig, and P. Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Cai</author>
<author>A Yates</author>
</authors>
<title>Large-scale semantic parsing via schema matching and lexicon extension.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="33814" citStr="Cai and Yates, 2013" startWordPosition="5665" endWordPosition="5668">n new entities and extraction patterns from seed examples. More recent approaches also leverage semi-structured data to obtain more robust extraction patterns (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2013). Although our work focuses on semistructured web pages rather than raw text, we use linguistic patterns of queries and entities as a signal for extracting appropriate answers. Additionally, our efforts can be viewed as building a lexicon on the fly. In recent years, there has been a drive to scale semantic parsing to large databases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013). However, despite the best efforts of information extraction, such databases will always lag behind the open web. For example, Berant et al. (2013) found that less than 10% of naturally occurring questions are answerable by a simple Freebase query. By using the semi-structured data from the web as a knowledge base, we hope to increase fact coverage for semantic parsing. Finally, as pointed out in the error analysis, we need to filter or aggregate the selected entities for complex queries (e.g., tech companies in China for a web page with all Asi</context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Q. Cai and A. Yates. 2013. Large-scale semantic parsing via schema matching and lexicon extension. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W W Cohen</author>
<author>M Hurst</author>
<author>L S Jensen</author>
</authors>
<title>A flexible learning system for wrapping tables and lists in HTML documents. In World Wide Web (WWW),</title>
<date>2002</date>
<pages>232--241</pages>
<contexts>
<context position="23525" citStr="Cohen et al., 2002" startWordPosition="3959" endWordPosition="3962">rts of the web page (Reason C1). For example, many Wikipedia articles have the See Also section that lists related articles in an unordered list (/ul/li/a), which causes a problem when the entities are also represented in the same format. Another main source of errors is the inconsistency in HTML tag usage (Reason C2). For instance, some web pages use &lt;b&gt; and &lt;strong&gt; tags for bold texts interchangeably, or switch between &lt;b&gt;&lt;a&gt;...&lt;/a&gt;&lt;/b&gt; and &lt;a&gt;&lt;b&gt;...&lt;/b&gt;&lt;/a&gt; across entities. We expect that this problem can be solved by normalizing the web page, using an alternative web page representation (Cohen et al., 2002; Wang and Cohen, 2009; Fumarola et al., 2011), or leveraging more expressive extraction predicates (Dalvi et al., 2011). One interesting source of errors is Reason C3, where we need to filter the selected entities to match the complex requirement in the query. For example, the query tech companies in China requires the system to select only the company names with China in the corresponding location column. To handle such queries, we need a deeper understanding of the relation between the linguistic structure of the query and the hierarchical structure of the web page. Tackling this error reSe</context>
<context position="32184" citStr="Cohen et al., 2002" startWordPosition="5392" endWordPosition="5395">ated entity in our experiments); this setting is suggestive of Wang and Cohen (2009). Table 6 shows that this augmentation increases accuracy from 41.1% to 52.9%, suggesting that our system can perform substantially better with a small amount of additional supervision. 5 Discussion Our work shares a base with the wrapper induction literature (Kushmerick, 1997) in that it leverages regularities of web page structures. However, wrapper induction usually focuses on a small set of web domains, where the web pages in each domain follow a fixed template (Muslea et al., 2001; Crescenzi et al., 2001; Cohen et al., 2002; Arasu and Garcia-Molina, 2003). Later work in web data extraction attempts to generalize across different web pages, but relies on either restricted data formats (Wong et al., 2009) or prior knowledge of web page structures with respect to the type of data to extract (Zhang et al., 2013). In our case, we only have the natural language query, which presents the more difficult problem of associating the entity class in the query (e.g., hiking trails) to concrete entities (e.g., Avalon Super Loop). In contrast to information extraction systems that extract homogeneous records from web pages (Li</context>
</contexts>
<marker>Cohen, Hurst, Jensen, 2002</marker>
<rawString>W. W. Cohen, M. Hurst, and L. S. Jensen. 2002. A flexible learning system for wrapping tables and lists in HTML documents. In World Wide Web (WWW), pages 232–241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Crescenzi</author>
<author>G Mecca</author>
<author>P Merialdo</author>
</authors>
<title>Roadrunner: Towards automatic data extraction from large web sites.</title>
<date>2001</date>
<booktitle>In VLDB,</booktitle>
<volume>1</volume>
<pages>109--118</pages>
<contexts>
<context position="8806" citStr="Crescenzi et al., 2001" startWordPosition="1474" endWordPosition="1477">xamples illustrating the diversity of queries and web pages from the OPENWEB dataset. Figure 3: An overview of our system. The system uses the input query x and web page w to produce a list of entities y via an extraction predicate z. w to get the list of entities y = Qz�,,,. Section 3.2 describes the model and the training procedure, while Section 3.3 presents the features used in our model. 3.1 Extraction predicates We represent each web page w as a DOM tree, a common representation among wrapper induction and web information extraction systems (Sahuguet and Azavant, 1999; Liu et al., 2000; Crescenzi et al., 2001). The text of any DOM tree node that is shorter than 140 characters is a candidate entity. However, without further restrictions, the number of possible entity lists grows exponentially with the number of candidate entities. To make the problem tractable, we introduce an extraction predicate z as an intermediate representation for extracting entities from w. In our system, we let an extraction predicate be a simplified XML path (XPath) such as /html[1]/body[1]/table[2]/tr/td[1] Informally, an extraction predicate is a list of path entries. Each path entry is either a tag (e.g., tr), which sele</context>
<context position="32164" citStr="Crescenzi et al., 2001" startWordPosition="5388" endWordPosition="5391">entity (the second annotated entity in our experiments); this setting is suggestive of Wang and Cohen (2009). Table 6 shows that this augmentation increases accuracy from 41.1% to 52.9%, suggesting that our system can perform substantially better with a small amount of additional supervision. 5 Discussion Our work shares a base with the wrapper induction literature (Kushmerick, 1997) in that it leverages regularities of web page structures. However, wrapper induction usually focuses on a small set of web domains, where the web pages in each domain follow a fixed template (Muslea et al., 2001; Crescenzi et al., 2001; Cohen et al., 2002; Arasu and Garcia-Molina, 2003). Later work in web data extraction attempts to generalize across different web pages, but relies on either restricted data formats (Wong et al., 2009) or prior knowledge of web page structures with respect to the type of data to extract (Zhang et al., 2013). In our case, we only have the natural language query, which presents the more difficult problem of associating the entity class in the query (e.g., hiking trails) to concrete entities (e.g., Avalon Super Loop). In contrast to information extraction systems that extract homogeneous record</context>
</contexts>
<marker>Crescenzi, Mecca, Merialdo, 2001</marker>
<rawString>V. Crescenzi, G. Mecca, P. Merialdo, et al. 2001. Roadrunner: Towards automatic data extraction from large web sites. In VLDB, volume 1, pages 109–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Dalvi</author>
<author>R Kumar</author>
<author>M Soliman</author>
</authors>
<title>Automatic wrappers for large scale web extraction.</title>
<date>2011</date>
<booktitle>Proceedings of the VLDB Endowment,</booktitle>
<volume>4</volume>
<issue>4</issue>
<contexts>
<context position="23645" citStr="Dalvi et al., 2011" startWordPosition="3978" endWordPosition="3981">cles in an unordered list (/ul/li/a), which causes a problem when the entities are also represented in the same format. Another main source of errors is the inconsistency in HTML tag usage (Reason C2). For instance, some web pages use &lt;b&gt; and &lt;strong&gt; tags for bold texts interchangeably, or switch between &lt;b&gt;&lt;a&gt;...&lt;/a&gt;&lt;/b&gt; and &lt;a&gt;&lt;b&gt;...&lt;/b&gt;&lt;/a&gt; across entities. We expect that this problem can be solved by normalizing the web page, using an alternative web page representation (Cohen et al., 2002; Wang and Cohen, 2009; Fumarola et al., 2011), or leveraging more expressive extraction predicates (Dalvi et al., 2011). One interesting source of errors is Reason C3, where we need to filter the selected entities to match the complex requirement in the query. For example, the query tech companies in China requires the system to select only the company names with China in the corresponding location column. To handle such queries, we need a deeper understanding of the relation between the linguistic structure of the query and the hierarchical structure of the web page. Tackling this error reSetting Acc A@5 All features 41.1 f 3.4 58.4 f 2.7 Oracle 68.7 f 2.4 68.7 f 2.4 (Section 4.5) Structural features only 36.</context>
</contexts>
<marker>Dalvi, Kumar, Soliman, 2011</marker>
<rawString>N. Dalvi, R. Kumar, and M. Soliman. 2011. Automatic wrappers for large scale web extraction. Proceedings of the VLDB Endowment, 4(4):219–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dalvi</author>
<author>W Cohen</author>
<author>J Callan</author>
</authors>
<title>Websets: Extracting sets of entities from the web using unsupervised information extraction.</title>
<date>2012</date>
<booktitle>In Web Search and Data Mining (WSDM),</booktitle>
<pages>243--252</pages>
<contexts>
<context position="1374" citStr="Dalvi et al., 2012" startWordPosition="208" endWordPosition="211">es on widely varying candidate entity lists. We tackle this by abstracting list elements and using aggregate statistics to define features. Finally, we created a new dataset of diverse queries and web pages, and show that our system achieves significantly better accuracy than a natural baseline. 1 Introduction We consider the task of extracting entities of a given category (e.g., hiking trails) from web pages. Previous approaches either (i) assume that the same entities appear on multiple web pages, or (ii) require information such as seed examples (Etzioni et al., 2005; Wang and Cohen, 2009; Dalvi et al., 2012). These approaches work well for common categories but encounter data sparsity problems for more specific categories, such as the products of a small company or the dishes at a local restaurant. In this context, we may have only a single web page that contains the information we need and no seed examples. In this paper, we propose a novel task, zeroshot entity extraction, where the specification of the desired entities is provided as a natural language query. Given a query (e.g., hiking webweb wepagespage pg answers Avalon Super Loop Hilton Area Wildlands Loop ... web page answers Avalon Super</context>
</contexts>
<marker>Dalvi, Cohen, Callan, 2012</marker>
<rawString>B. Dalvi, W. Cohen, and J. Callan. 2012. Websets: Extracting sets of entities from the web using unsupervised information extraction. In Web Search and Data Mining (WSDM), pages 243–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>E Hazan</author>
<author>Y Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2010</date>
<booktitle>In Conference on Learning Theory (COLT).</booktitle>
<contexts>
<context position="13345" citStr="Duchi et al., 2010" startWordPosition="2250" endWordPosition="2253">feature vector, which will be defined in Section 3.3. To train the model, we find a parameter vector θ that maximizes the regularized log marginal probability of the compatibility function being satisfied. In other words, given training data D = {(x(i), w(i), c(i))Jni=1, we find θ that maximizes log pθ(c(i) = 1 |x(i),w(i)) - λ211θ1122 where �pθ(c = 1 |x, w) = pθ(z |x, w) - c(JzKw). z∈Z(w) Note that c(JzKw) = 1 when the entity list y = JzKw selected by z is compatible with the annotation; otherwise, c(JzKw) = 0. We use AdaGrad, an online gradient descent with an adaptive per-feature step size (Duchi et al., 2010), making 5 passes over the training data. We use λ = 0.01 obtained from cross-validation for all experiments. 3.3 Features To construct the log-linear model, we define a feature vector φ(x, w, z) for each query x, web page w, and extraction predicate z. The final feature vector is the concatenation of structural features φs(w, z), which consider the selected nodes in the DOM tree, and denotation features φd(x, y), which look at the extracted entities. We will use the query hiking trails near Baltimore and the web page in Figure 4 as a running example. Figure 5 lists some features extracted fro</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2010</marker>
<rawString>J. Duchi, E. Hazan, and Y. Singer. 2010. Adaptive subgradient methods for online learning and stochastic optimization. In Conference on Learning Theory (COLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M Cafarella</author>
<author>D Downey</author>
<author>A Popescu</author>
<author>T Shaked</author>
<author>S Soderland</author>
<author>D S Weld</author>
<author>A Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: An experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="1331" citStr="Etzioni et al., 2005" startWordPosition="200" endWordPosition="203">page. The main challenge is to define features on widely varying candidate entity lists. We tackle this by abstracting list elements and using aggregate statistics to define features. Finally, we created a new dataset of diverse queries and web pages, and show that our system achieves significantly better accuracy than a natural baseline. 1 Introduction We consider the task of extracting entities of a given category (e.g., hiking trails) from web pages. Previous approaches either (i) assume that the same entities appear on multiple web pages, or (ii) require information such as seed examples (Etzioni et al., 2005; Wang and Cohen, 2009; Dalvi et al., 2012). These approaches work well for common categories but encounter data sparsity problems for more specific categories, such as the products of a small company or the dishes at a local restaurant. In this context, we may have only a single web page that contains the information we need and no seed examples. In this paper, we propose a novel task, zeroshot entity extraction, where the specification of the desired entities is provided as a natural language query. Given a query (e.g., hiking webweb wepagespage pg answers Avalon Super Loop Hilton Area Wildl</context>
<context position="33179" citStr="Etzioni et al., 2005" startWordPosition="5558" endWordPosition="5561">icult problem of associating the entity class in the query (e.g., hiking trails) to concrete entities (e.g., Avalon Super Loop). In contrast to information extraction systems that extract homogeneous records from web pages (Liu et al., 2003; Zheng et al., 2009), our system must choose the correct field from each record and also identify the relevant part of the page based on the query. Another related line of work is information extraction from text, which relies on natural language patterns to extract categories and relations of entities. One classic example is Hearst patterns (Hearst, 1992; Etzioni et al., 2005), which can learn new entities and extraction patterns from seed examples. More recent approaches also leverage semi-structured data to obtain more robust extraction patterns (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2013). Although our work focuses on semistructured web pages rather than raw text, we use linguistic patterns of queries and entities as a signal for extracting appropriate answers. Additionally, our efforts can be viewed as building a lexicon on the fly. In recent years, there has been a drive to scale semantic parsing to large databases su</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. Shaked, S. Soderland, D. S. Weld, and A. Yates. 2005. Unsupervised named-entity extraction from the web: An experimental study. Artificial Intelligence, 165(1):91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Fumarola</author>
<author>T Weninger</author>
<author>R Barber</author>
<author>D Malerba</author>
<author>J Han</author>
</authors>
<title>Extracting general lists from web documents: A hybrid approach. Modern Approaches in Applied Intelligence</title>
<date>2011</date>
<publisher>Springer.</publisher>
<contexts>
<context position="9855" citStr="Fumarola et al., 2011" startWordPosition="1646" endWordPosition="1649">path (XPath) such as /html[1]/body[1]/table[2]/tr/td[1] Informally, an extraction predicate is a list of path entries. Each path entry is either a tag (e.g., tr), which selects all children with that tag; or a tag and an index i (e.g., td[1]), which selects only the ith child with that tag. The denotation y = Qz�,,, of an extraction predicate z is the list of entities selected by the XPath. Figure 4 illustrates the execution of the extraction predicate above on a DOM tree. In the literature, many information extraction systems employ more versatile extraction predicates (Wang and Cohen, 2009; Fumarola et al., 2011). However, despite the simplicity, we are able to find an extraction predicate that extracts a compatible entity list in 69.7% of the development examples. In some examples, we cannot extract a compatible list due to unrecoverable issues such as incorrect annotation. Section 4.4 provides a detailed analysis of these issues. Additionally, extraction predicates can be easily extended to increase the coverage. For example, by introducing new index types [1:] (selects all but the first node) and [:-1] (selects all but the last node), we can increase the coverage to 76.2%. Extraction predicate gene</context>
<context position="23571" citStr="Fumarola et al., 2011" startWordPosition="3968" endWordPosition="3971">le, many Wikipedia articles have the See Also section that lists related articles in an unordered list (/ul/li/a), which causes a problem when the entities are also represented in the same format. Another main source of errors is the inconsistency in HTML tag usage (Reason C2). For instance, some web pages use &lt;b&gt; and &lt;strong&gt; tags for bold texts interchangeably, or switch between &lt;b&gt;&lt;a&gt;...&lt;/a&gt;&lt;/b&gt; and &lt;a&gt;&lt;b&gt;...&lt;/b&gt;&lt;/a&gt; across entities. We expect that this problem can be solved by normalizing the web page, using an alternative web page representation (Cohen et al., 2002; Wang and Cohen, 2009; Fumarola et al., 2011), or leveraging more expressive extraction predicates (Dalvi et al., 2011). One interesting source of errors is Reason C3, where we need to filter the selected entities to match the complex requirement in the query. For example, the query tech companies in China requires the system to select only the company names with China in the corresponding location column. To handle such queries, we need a deeper understanding of the relation between the linguistic structure of the query and the hierarchical structure of the web page. Tackling this error reSetting Acc A@5 All features 41.1 f 3.4 58.4 f 2</context>
</contexts>
<marker>Fumarola, Weninger, Barber, Malerba, Han, 2011</marker>
<rawString>F. Fumarola, T. Weninger, R. Barber, D. Malerba, and J. Han. 2011. Extracting general lists from web documents: A hybrid approach. Modern Approaches in Applied Intelligence Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Interational Conference on Computational linguistics,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="33156" citStr="Hearst, 1992" startWordPosition="5556" endWordPosition="5557"> the more difficult problem of associating the entity class in the query (e.g., hiking trails) to concrete entities (e.g., Avalon Super Loop). In contrast to information extraction systems that extract homogeneous records from web pages (Liu et al., 2003; Zheng et al., 2009), our system must choose the correct field from each record and also identify the relevant part of the page based on the query. Another related line of work is information extraction from text, which relies on natural language patterns to extract categories and relations of entities. One classic example is Hearst patterns (Hearst, 1992; Etzioni et al., 2005), which can learn new entities and extraction patterns from seed examples. More recent approaches also leverage semi-structured data to obtain more robust extraction patterns (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2013). Although our work focuses on semistructured web pages rather than raw text, we use linguistic patterns of queries and entities as a signal for extracting appropriate answers. Additionally, our efforts can be viewed as building a lexicon on the fly. In recent years, there has been a drive to scale semantic parsin</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Interational Conference on Computational linguistics, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hoffmann</author>
<author>C Zhang</author>
<author>X Ling</author>
<author>L S Zettlemoyer</author>
<author>D S Weld</author>
</authors>
<title>Knowledge-based weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>541--550</pages>
<contexts>
<context position="33396" citStr="Hoffmann et al., 2011" startWordPosition="5591" endWordPosition="5595">pages (Liu et al., 2003; Zheng et al., 2009), our system must choose the correct field from each record and also identify the relevant part of the page based on the query. Another related line of work is information extraction from text, which relies on natural language patterns to extract categories and relations of entities. One classic example is Hearst patterns (Hearst, 1992; Etzioni et al., 2005), which can learn new entities and extraction patterns from seed examples. More recent approaches also leverage semi-structured data to obtain more robust extraction patterns (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2013). Although our work focuses on semistructured web pages rather than raw text, we use linguistic patterns of queries and entities as a signal for extracting appropriate answers. Additionally, our efforts can be viewed as building a lexicon on the fly. In recent years, there has been a drive to scale semantic parsing to large databases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013). However, despite the best efforts of information extraction, such databases will always lag behind the open web. For example, Berant</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>R. Hoffmann, C. Zhang, X. Ling, L. S. Zettlemoyer, and D. S. Weld. 2011. Knowledge-based weak supervision for information extraction of overlapping relations. In Association for Computational Linguistics (ACL), pages 541–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kushmerick</author>
</authors>
<title>Wrapper induction for information extraction.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Washington.</institution>
<contexts>
<context position="31928" citStr="Kushmerick, 1997" startWordPosition="5348" endWordPosition="5349">ttings Since zero-shot entity extraction is a new task, we cannot directly compare our system with other systems. However, we can mimic the settings of other tasks. In one experiment, we augment each input query with a single seed entity (the second annotated entity in our experiments); this setting is suggestive of Wang and Cohen (2009). Table 6 shows that this augmentation increases accuracy from 41.1% to 52.9%, suggesting that our system can perform substantially better with a small amount of additional supervision. 5 Discussion Our work shares a base with the wrapper induction literature (Kushmerick, 1997) in that it leverages regularities of web page structures. However, wrapper induction usually focuses on a small set of web domains, where the web pages in each domain follow a fixed template (Muslea et al., 2001; Crescenzi et al., 2001; Cohen et al., 2002; Arasu and Garcia-Molina, 2003). Later work in web data extraction attempts to generalize across different web pages, but relies on either restricted data formats (Wong et al., 2009) or prior knowledge of web page structures with respect to the type of data to extract (Zhang et al., 2013). In our case, we only have the natural language query</context>
</contexts>
<marker>Kushmerick, 1997</marker>
<rawString>N. Kushmerick. 1997. Wrapper induction for information extraction. Ph.D. thesis, University of Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>E Choi</author>
<author>Y Artzi</author>
<author>L Zettlemoyer</author>
</authors>
<title>Scaling semantic parsers with on-the-fly ontology matching.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="33862" citStr="Kwiatkowski et al., 2013" startWordPosition="5673" endWordPosition="5676">m seed examples. More recent approaches also leverage semi-structured data to obtain more robust extraction patterns (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2013). Although our work focuses on semistructured web pages rather than raw text, we use linguistic patterns of queries and entities as a signal for extracting appropriate answers. Additionally, our efforts can be viewed as building a lexicon on the fly. In recent years, there has been a drive to scale semantic parsing to large databases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013). However, despite the best efforts of information extraction, such databases will always lag behind the open web. For example, Berant et al. (2013) found that less than 10% of naturally occurring questions are answerable by a simple Freebase query. By using the semi-structured data from the web as a knowledge base, we hope to increase fact coverage for semantic parsing. Finally, as pointed out in the error analysis, we need to filter or aggregate the selected entities for complex queries (e.g., tech companies in China for a web page with all Asian tech companies). In future work, we would lik</context>
</contexts>
<marker>Kwiatkowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Larochelle</author>
<author>D Erhan</author>
<author>Y Bengio</author>
</authors>
<title>Zerodata learning of new tasks.</title>
<date>2008</date>
<booktitle>In AAAI,</booktitle>
<volume>8</volume>
<pages>646--651</pages>
<contexts>
<context position="2741" citStr="Larochelle et al., 2008" startWordPosition="431" endWordPosition="434">r depends on multiple web pages. In our setting, we take as input a natural language query and extract entities from a single web page. trails near Baltimore) and a web page (e.g., http://www.everytrail.com/best/ hiking-baltimore-maryland), the goal is to extract all entities corresponding to the query on that page (e.g., Avalon Super Loop, etc.). Figure 1 summarizes the task setup. The task introduces two challenges. Given a single web page to extract entities from, we can no longer rely on the redundancy of entities across multiple web pages. Furthermore, in the zero-shot learning paradigm (Larochelle et al., 2008), where entire categories might be unseen during training, the system must generalize to new queries and web pages without the additional aid of seed examples. To tackle these challenges, we cast the task as a structured prediction problem where the input is the query and the web page, and the output is a list of entities, mediated by a latent extraction predicate. To generalize across different inputs, we rely on two types of features: structural features, which look at the layout and placement of the entities being extracted; and denotation feaseeds Avalon Super Loop Hilton Area traditional </context>
</contexts>
<marker>Larochelle, Erhan, Bengio, 2008</marker>
<rawString>H. Larochelle, D. Erhan, and Y. Bengio. 2008. Zerodata learning of new tasks. In AAAI, volume 8, pages 646–651.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Liu</author>
<author>C Pu</author>
<author>W Han</author>
</authors>
<title>XWRAP: An XMLenabled wrapper construction system for web information sources.</title>
<date>2000</date>
<booktitle>In Data Engineering,</booktitle>
<pages>611--621</pages>
<contexts>
<context position="8781" citStr="Liu et al., 2000" startWordPosition="1470" endWordPosition="1473">s Figure 2: Some examples illustrating the diversity of queries and web pages from the OPENWEB dataset. Figure 3: An overview of our system. The system uses the input query x and web page w to produce a list of entities y via an extraction predicate z. w to get the list of entities y = Qz�,,,. Section 3.2 describes the model and the training procedure, while Section 3.3 presents the features used in our model. 3.1 Extraction predicates We represent each web page w as a DOM tree, a common representation among wrapper induction and web information extraction systems (Sahuguet and Azavant, 1999; Liu et al., 2000; Crescenzi et al., 2001). The text of any DOM tree node that is shorter than 140 characters is a candidate entity. However, without further restrictions, the number of possible entity lists grows exponentially with the number of candidate entities. To make the problem tractable, we introduce an extraction predicate z as an intermediate representation for extracting entities from w. In our system, we let an extraction predicate be a simplified XML path (XPath) such as /html[1]/body[1]/table[2]/tr/td[1] Informally, an extraction predicate is a list of path entries. Each path entry is either a t</context>
</contexts>
<marker>Liu, Pu, Han, 2000</marker>
<rawString>L. Liu, C. Pu, and W. Han. 2000. XWRAP: An XMLenabled wrapper construction system for web information sources. In Data Engineering, 2000. Proceedings. 16th International Conference on, pages 611–621.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Liu</author>
<author>R Grossman</author>
<author>Y Zhai</author>
</authors>
<title>Mining data records in web pages.</title>
<date>2003</date>
<booktitle>In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>601--606</pages>
<contexts>
<context position="25544" citStr="Liu et al., 2003" startWordPosition="4308" endWordPosition="4311"> to the system selecting non-content elements such as navigation links and content headings (Reason R1). Feature analysis reveals that both structural and linguistic statistics of these non-content elements can be more coherent than those of the correct entities. We suspect that since many of our features try to capture the coherence of entities, the system sometimes erroneously favors the more homonogenous non-content parts of the page. To disfavor these parts, One possible solution is to add visual features that capture how the web page is rendered and favor more salient parts of the page. (Liu et al., 2003; Song et al., 2004; Zhu et al., 2005; Zheng et al., 2007). 4.5 Feature variations We now investigate the contribution of each feature type. The ablation results on the development set over 10 random splits are shown in Table 6. We observe that denotation features improves accuracy on top of structural features. Table 7 shows an example of an error that is eliminated by each feature type. Generally, if the entities are represented as records (e.g., rows of a table), then denotation features will help the system select the correct field from each record. On the other hand, structural features p</context>
<context position="32798" citStr="Liu et al., 2003" startWordPosition="5493" endWordPosition="5496">02; Arasu and Garcia-Molina, 2003). Later work in web data extraction attempts to generalize across different web pages, but relies on either restricted data formats (Wong et al., 2009) or prior knowledge of web page structures with respect to the type of data to extract (Zhang et al., 2013). In our case, we only have the natural language query, which presents the more difficult problem of associating the entity class in the query (e.g., hiking trails) to concrete entities (e.g., Avalon Super Loop). In contrast to information extraction systems that extract homogeneous records from web pages (Liu et al., 2003; Zheng et al., 2009), our system must choose the correct field from each record and also identify the relevant part of the page based on the query. Another related line of work is information extraction from text, which relies on natural language patterns to extract categories and relations of entities. One classic example is Hearst patterns (Hearst, 1992; Etzioni et al., 2005), which can learn new entities and extraction patterns from seed examples. More recent approaches also leverage semi-structured data to obtain more robust extraction patterns (Mintz et al., 2009; Hoffmann et al., 2011; </context>
</contexts>
<marker>Liu, Grossman, Zhai, 2003</marker>
<rawString>B. Liu, R. Grossman, and Y. Zhai. 2003. Mining data records in web pages. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 601–606.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mintz</author>
<author>S Bills</author>
<author>R Snow</author>
<author>D Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="33373" citStr="Mintz et al., 2009" startWordPosition="5587" endWordPosition="5590">us records from web pages (Liu et al., 2003; Zheng et al., 2009), our system must choose the correct field from each record and also identify the relevant part of the page based on the query. Another related line of work is information extraction from text, which relies on natural language patterns to extract categories and relations of entities. One classic example is Hearst patterns (Hearst, 1992; Etzioni et al., 2005), which can learn new entities and extraction patterns from seed examples. More recent approaches also leverage semi-structured data to obtain more robust extraction patterns (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2013). Although our work focuses on semistructured web pages rather than raw text, we use linguistic patterns of queries and entities as a signal for extracting appropriate answers. Additionally, our efforts can be viewed as building a lexicon on the fly. In recent years, there has been a drive to scale semantic parsing to large databases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013). However, despite the best efforts of information extraction, such databases will always lag behind the open w</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Association for Computational Linguistics (ACL), pages 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Muslea</author>
<author>S Minton</author>
<author>C A Knoblock</author>
</authors>
<title>Hierarchical wrapper induction for semistructured information sources.</title>
<date>2001</date>
<booktitle>Autonomous Agents and MultiAgent Systems,</booktitle>
<pages>4--1</pages>
<contexts>
<context position="32140" citStr="Muslea et al., 2001" startWordPosition="5384" endWordPosition="5387">y with a single seed entity (the second annotated entity in our experiments); this setting is suggestive of Wang and Cohen (2009). Table 6 shows that this augmentation increases accuracy from 41.1% to 52.9%, suggesting that our system can perform substantially better with a small amount of additional supervision. 5 Discussion Our work shares a base with the wrapper induction literature (Kushmerick, 1997) in that it leverages regularities of web page structures. However, wrapper induction usually focuses on a small set of web domains, where the web pages in each domain follow a fixed template (Muslea et al., 2001; Crescenzi et al., 2001; Cohen et al., 2002; Arasu and Garcia-Molina, 2003). Later work in web data extraction attempts to generalize across different web pages, but relies on either restricted data formats (Wong et al., 2009) or prior knowledge of web page structures with respect to the type of data to extract (Zhang et al., 2013). In our case, we only have the natural language query, which presents the more difficult problem of associating the entity class in the query (e.g., hiking trails) to concrete entities (e.g., Avalon Super Loop). In contrast to information extraction systems that ex</context>
</contexts>
<marker>Muslea, Minton, Knoblock, 2001</marker>
<rawString>I. Muslea, S. Minton, and C. A. Knoblock. 2001. Hierarchical wrapper induction for semistructured information sources. Autonomous Agents and MultiAgent Systems, 4(1):93–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riedel</author>
<author>L Yao</author>
<author>A McCallum</author>
</authors>
<title>Relation extraction with matrix factorization and universal schemas.</title>
<date>2013</date>
<booktitle>In North American Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="33441" citStr="Riedel et al., 2013" startWordPosition="5600" endWordPosition="5603">our system must choose the correct field from each record and also identify the relevant part of the page based on the query. Another related line of work is information extraction from text, which relies on natural language patterns to extract categories and relations of entities. One classic example is Hearst patterns (Hearst, 1992; Etzioni et al., 2005), which can learn new entities and extraction patterns from seed examples. More recent approaches also leverage semi-structured data to obtain more robust extraction patterns (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2013). Although our work focuses on semistructured web pages rather than raw text, we use linguistic patterns of queries and entities as a signal for extracting appropriate answers. Additionally, our efforts can be viewed as building a lexicon on the fly. In recent years, there has been a drive to scale semantic parsing to large databases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013). However, despite the best efforts of information extraction, such databases will always lag behind the open web. For example, Berant et al. (2013) found that less than 10% of na</context>
</contexts>
<marker>Riedel, Yao, McCallum, 2013</marker>
<rawString>S. Riedel, L. Yao, and A. McCallum. 2013. Relation extraction with matrix factorization and universal schemas. In North American Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sahuguet</author>
<author>F Azavant</author>
</authors>
<title>WysiWyg web wrapper factory (W4F).</title>
<date>1999</date>
<booktitle>In WWW Conference.</booktitle>
<contexts>
<context position="8763" citStr="Sahuguet and Azavant, 1999" startWordPosition="1466" endWordPosition="1469"> warming lsu football coaches Figure 2: Some examples illustrating the diversity of queries and web pages from the OPENWEB dataset. Figure 3: An overview of our system. The system uses the input query x and web page w to produce a list of entities y via an extraction predicate z. w to get the list of entities y = Qz�,,,. Section 3.2 describes the model and the training procedure, while Section 3.3 presents the features used in our model. 3.1 Extraction predicates We represent each web page w as a DOM tree, a common representation among wrapper induction and web information extraction systems (Sahuguet and Azavant, 1999; Liu et al., 2000; Crescenzi et al., 2001). The text of any DOM tree node that is shorter than 140 characters is a candidate entity. However, without further restrictions, the number of possible entity lists grows exponentially with the number of candidate entities. To make the problem tractable, we introduce an extraction predicate z as an intermediate representation for extracting entities from w. In our system, we let an extraction predicate be a simplified XML path (XPath) such as /html[1]/body[1]/table[2]/tr/td[1] Informally, an extraction predicate is a list of path entries. Each path e</context>
</contexts>
<marker>Sahuguet, Azavant, 1999</marker>
<rawString>A. Sahuguet and F. Azavant. 1999. WysiWyg web wrapper factory (W4F). In WWW Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Song</author>
<author>H Liu</author>
<author>J Wen</author>
<author>W Ma</author>
</authors>
<title>Learning block importance models for web pages. In World Wide Web (WWW),</title>
<date>2004</date>
<pages>203--211</pages>
<contexts>
<context position="25563" citStr="Song et al., 2004" startWordPosition="4312" endWordPosition="4315">ecting non-content elements such as navigation links and content headings (Reason R1). Feature analysis reveals that both structural and linguistic statistics of these non-content elements can be more coherent than those of the correct entities. We suspect that since many of our features try to capture the coherence of entities, the system sometimes erroneously favors the more homonogenous non-content parts of the page. To disfavor these parts, One possible solution is to add visual features that capture how the web page is rendered and favor more salient parts of the page. (Liu et al., 2003; Song et al., 2004; Zhu et al., 2005; Zheng et al., 2007). 4.5 Feature variations We now investigate the contribution of each feature type. The ablation results on the development set over 10 random splits are shown in Table 6. We observe that denotation features improves accuracy on top of structural features. Table 7 shows an example of an error that is eliminated by each feature type. Generally, if the entities are represented as records (e.g., rows of a table), then denotation features will help the system select the correct field from each record. On the other hand, structural features prevent the system f</context>
</contexts>
<marker>Song, Liu, Wen, Ma, 2004</marker>
<rawString>R. Song, H. Liu, J. Wen, and W. Ma. 2004. Learning block importance models for web pages. In World Wide Web (WWW), pages 203–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>J Tibshirani</author>
<author>R Nallapati</author>
<author>C D Manning</author>
</authors>
<title>Multi-instance multi-label learning for relation extraction.</title>
<date>2012</date>
<booktitle>In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL),</booktitle>
<pages>455--465</pages>
<contexts>
<context position="33419" citStr="Surdeanu et al., 2012" startWordPosition="5596" endWordPosition="5599">; Zheng et al., 2009), our system must choose the correct field from each record and also identify the relevant part of the page based on the query. Another related line of work is information extraction from text, which relies on natural language patterns to extract categories and relations of entities. One classic example is Hearst patterns (Hearst, 1992; Etzioni et al., 2005), which can learn new entities and extraction patterns from seed examples. More recent approaches also leverage semi-structured data to obtain more robust extraction patterns (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2013). Although our work focuses on semistructured web pages rather than raw text, we use linguistic patterns of queries and entities as a signal for extracting appropriate answers. Additionally, our efforts can be viewed as building a lexicon on the fly. In recent years, there has been a drive to scale semantic parsing to large databases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013). However, despite the best efforts of information extraction, such databases will always lag behind the open web. For example, Berant et al. (2013) found th</context>
</contexts>
<marker>Surdeanu, Tibshirani, Nallapati, Manning, 2012</marker>
<rawString>M. Surdeanu, J. Tibshirani, R. Nallapati, and C. D. Manning. 2012. Multi-instance multi-label learning for relation extraction. In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL), pages 455–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Wang</author>
<author>W W Cohen</author>
</authors>
<title>Character-level analysis of semi-structured documents for set expansion.</title>
<date>2009</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1503--1512</pages>
<contexts>
<context position="1353" citStr="Wang and Cohen, 2009" startWordPosition="204" endWordPosition="207">ge is to define features on widely varying candidate entity lists. We tackle this by abstracting list elements and using aggregate statistics to define features. Finally, we created a new dataset of diverse queries and web pages, and show that our system achieves significantly better accuracy than a natural baseline. 1 Introduction We consider the task of extracting entities of a given category (e.g., hiking trails) from web pages. Previous approaches either (i) assume that the same entities appear on multiple web pages, or (ii) require information such as seed examples (Etzioni et al., 2005; Wang and Cohen, 2009; Dalvi et al., 2012). These approaches work well for common categories but encounter data sparsity problems for more specific categories, such as the products of a small company or the dishes at a local restaurant. In this context, we may have only a single web page that contains the information we need and no seed examples. In this paper, we propose a novel task, zeroshot entity extraction, where the specification of the desired entities is provided as a natural language query. Given a query (e.g., hiking webweb wepagespage pg answers Avalon Super Loop Hilton Area Wildlands Loop ... web page</context>
<context position="9831" citStr="Wang and Cohen, 2009" startWordPosition="1642" endWordPosition="1645">e be a simplified XML path (XPath) such as /html[1]/body[1]/table[2]/tr/td[1] Informally, an extraction predicate is a list of path entries. Each path entry is either a tag (e.g., tr), which selects all children with that tag; or a tag and an index i (e.g., td[1]), which selects only the ith child with that tag. The denotation y = Qz�,,, of an extraction predicate z is the list of entities selected by the XPath. Figure 4 illustrates the execution of the extraction predicate above on a DOM tree. In the literature, many information extraction systems employ more versatile extraction predicates (Wang and Cohen, 2009; Fumarola et al., 2011). However, despite the simplicity, we are able to find an extraction predicate that extracts a compatible entity list in 69.7% of the development examples. In some examples, we cannot extract a compatible list due to unrecoverable issues such as incorrect annotation. Section 4.4 provides a detailed analysis of these issues. Additionally, extraction predicates can be easily extended to increase the coverage. For example, by introducing new index types [1:] (selects all but the first node) and [:-1] (selects all but the last node), we can increase the coverage to 76.2%. E</context>
<context position="23547" citStr="Wang and Cohen, 2009" startWordPosition="3963" endWordPosition="3967">(Reason C1). For example, many Wikipedia articles have the See Also section that lists related articles in an unordered list (/ul/li/a), which causes a problem when the entities are also represented in the same format. Another main source of errors is the inconsistency in HTML tag usage (Reason C2). For instance, some web pages use &lt;b&gt; and &lt;strong&gt; tags for bold texts interchangeably, or switch between &lt;b&gt;&lt;a&gt;...&lt;/a&gt;&lt;/b&gt; and &lt;a&gt;&lt;b&gt;...&lt;/b&gt;&lt;/a&gt; across entities. We expect that this problem can be solved by normalizing the web page, using an alternative web page representation (Cohen et al., 2002; Wang and Cohen, 2009; Fumarola et al., 2011), or leveraging more expressive extraction predicates (Dalvi et al., 2011). One interesting source of errors is Reason C3, where we need to filter the selected entities to match the complex requirement in the query. For example, the query tech companies in China requires the system to select only the company names with China in the corresponding location column. To handle such queries, we need a deeper understanding of the relation between the linguistic structure of the query and the hierarchical structure of the web page. Tackling this error reSetting Acc A@5 All feat</context>
<context position="31650" citStr="Wang and Cohen (2009)" startWordPosition="5303" endWordPosition="5306"> changing denotation features into query-denotation features. this modified dataset, the prominent entities may not be the answers to the query. Here, querydenotation features improves accuracy over denotation features alone from 19.3% to 29.2%. 4.7 Comparison with other problem settings Since zero-shot entity extraction is a new task, we cannot directly compare our system with other systems. However, we can mimic the settings of other tasks. In one experiment, we augment each input query with a single seed entity (the second annotated entity in our experiments); this setting is suggestive of Wang and Cohen (2009). Table 6 shows that this augmentation increases accuracy from 41.1% to 52.9%, suggesting that our system can perform substantially better with a small amount of additional supervision. 5 Discussion Our work shares a base with the wrapper induction literature (Kushmerick, 1997) in that it leverages regularities of web page structures. However, wrapper induction usually focuses on a small set of web domains, where the web pages in each domain follow a fixed template (Muslea et al., 2001; Crescenzi et al., 2001; Cohen et al., 2002; Arasu and Garcia-Molina, 2003). Later work in web data extractio</context>
</contexts>
<marker>Wang, Cohen, 2009</marker>
<rawString>R. C. Wang and W. W. Cohen. 2009. Character-level analysis of semi-structured documents for set expansion. In Empirical Methods in Natural Language Processing (EMNLP), pages 1503–1512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>D Widdows</author>
<author>T Lokovic</author>
<author>K Nigam</author>
</authors>
<title>Scalable attribute-value extraction from semistructured text.</title>
<date>2009</date>
<booktitle>In IEEE International Conference on Data Mining Workshops,</booktitle>
<pages>302--307</pages>
<contexts>
<context position="32367" citStr="Wong et al., 2009" startWordPosition="5421" endWordPosition="5424">system can perform substantially better with a small amount of additional supervision. 5 Discussion Our work shares a base with the wrapper induction literature (Kushmerick, 1997) in that it leverages regularities of web page structures. However, wrapper induction usually focuses on a small set of web domains, where the web pages in each domain follow a fixed template (Muslea et al., 2001; Crescenzi et al., 2001; Cohen et al., 2002; Arasu and Garcia-Molina, 2003). Later work in web data extraction attempts to generalize across different web pages, but relies on either restricted data formats (Wong et al., 2009) or prior knowledge of web page structures with respect to the type of data to extract (Zhang et al., 2013). In our case, we only have the natural language query, which presents the more difficult problem of associating the entity class in the query (e.g., hiking trails) to concrete entities (e.g., Avalon Super Loop). In contrast to information extraction systems that extract homogeneous records from web pages (Liu et al., 2003; Zheng et al., 2009), our system must choose the correct field from each record and also identify the relevant part of the page based on the query. Another related line</context>
</contexts>
<marker>Wong, Widdows, Lokovic, Nigam, 2009</marker>
<rawString>Y. W. Wong, D. Widdows, T. Lokovic, and K. Nigam. 2009. Scalable attribute-value extraction from semistructured text. In IEEE International Conference on Data Mining Workshops, pages 302–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhang</author>
<author>K Q Zhu</author>
<author>H Wang</author>
<author>H Li</author>
</authors>
<title>Automatic extraction of top-k lists from the web.</title>
<date>2013</date>
<booktitle>In International Conference on Data Engineering.</booktitle>
<contexts>
<context position="32474" citStr="Zhang et al., 2013" startWordPosition="5441" endWordPosition="5444">rk shares a base with the wrapper induction literature (Kushmerick, 1997) in that it leverages regularities of web page structures. However, wrapper induction usually focuses on a small set of web domains, where the web pages in each domain follow a fixed template (Muslea et al., 2001; Crescenzi et al., 2001; Cohen et al., 2002; Arasu and Garcia-Molina, 2003). Later work in web data extraction attempts to generalize across different web pages, but relies on either restricted data formats (Wong et al., 2009) or prior knowledge of web page structures with respect to the type of data to extract (Zhang et al., 2013). In our case, we only have the natural language query, which presents the more difficult problem of associating the entity class in the query (e.g., hiking trails) to concrete entities (e.g., Avalon Super Loop). In contrast to information extraction systems that extract homogeneous records from web pages (Liu et al., 2003; Zheng et al., 2009), our system must choose the correct field from each record and also identify the relevant part of the page based on the query. Another related line of work is information extraction from text, which relies on natural language patterns to extract categori</context>
</contexts>
<marker>Zhang, Zhu, Wang, Li, 2013</marker>
<rawString>Z. Zhang, K. Q. Zhu, H. Wang, and H. Li. 2013. Automatic extraction of top-k lists from the web. In International Conference on Data Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Zheng</author>
<author>R Song</author>
<author>J Wen</author>
</authors>
<title>Templateindependent news extraction based on visual consistency.</title>
<date>2007</date>
<booktitle>In AAAI,</booktitle>
<volume>7</volume>
<pages>1507--1513</pages>
<contexts>
<context position="25602" citStr="Zheng et al., 2007" startWordPosition="4320" endWordPosition="4323">avigation links and content headings (Reason R1). Feature analysis reveals that both structural and linguistic statistics of these non-content elements can be more coherent than those of the correct entities. We suspect that since many of our features try to capture the coherence of entities, the system sometimes erroneously favors the more homonogenous non-content parts of the page. To disfavor these parts, One possible solution is to add visual features that capture how the web page is rendered and favor more salient parts of the page. (Liu et al., 2003; Song et al., 2004; Zhu et al., 2005; Zheng et al., 2007). 4.5 Feature variations We now investigate the contribution of each feature type. The ablation results on the development set over 10 random splits are shown in Table 6. We observe that denotation features improves accuracy on top of structural features. Table 7 shows an example of an error that is eliminated by each feature type. Generally, if the entities are represented as records (e.g., rows of a table), then denotation features will help the system select the correct field from each record. On the other hand, structural features prevent the system from selecting random entities outside t</context>
</contexts>
<marker>Zheng, Song, Wen, 2007</marker>
<rawString>S. Zheng, R. Song, and J. Wen. 2007. Templateindependent news extraction based on visual consistency. In AAAI, volume 7, pages 1507–1513.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Zheng</author>
<author>R Song</author>
<author>J Wen</author>
<author>C L Giles</author>
</authors>
<title>Efficient record-level wrapper induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM conference on Information and knowledge management,</booktitle>
<pages>47--56</pages>
<contexts>
<context position="32819" citStr="Zheng et al., 2009" startWordPosition="5497" endWordPosition="5500">ia-Molina, 2003). Later work in web data extraction attempts to generalize across different web pages, but relies on either restricted data formats (Wong et al., 2009) or prior knowledge of web page structures with respect to the type of data to extract (Zhang et al., 2013). In our case, we only have the natural language query, which presents the more difficult problem of associating the entity class in the query (e.g., hiking trails) to concrete entities (e.g., Avalon Super Loop). In contrast to information extraction systems that extract homogeneous records from web pages (Liu et al., 2003; Zheng et al., 2009), our system must choose the correct field from each record and also identify the relevant part of the page based on the query. Another related line of work is information extraction from text, which relies on natural language patterns to extract categories and relations of entities. One classic example is Hearst patterns (Hearst, 1992; Etzioni et al., 2005), which can learn new entities and extraction patterns from seed examples. More recent approaches also leverage semi-structured data to obtain more robust extraction patterns (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012</context>
</contexts>
<marker>Zheng, Song, Wen, Giles, 2009</marker>
<rawString>S. Zheng, R. Song, J. Wen, and C. L. Giles. 2009. Efficient record-level wrapper induction. In Proceedings of the 18th ACM conference on Information and knowledge management, pages 47–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zhu</author>
<author>Z Nie</author>
<author>J Wen</author>
<author>B Zhang</author>
<author>W Ma</author>
</authors>
<title>2D conditional random fields for web information extraction.</title>
<date>2005</date>
<booktitle>In International Conference on Machine Learning (ICML),</booktitle>
<pages>1044--1051</pages>
<contexts>
<context position="25581" citStr="Zhu et al., 2005" startWordPosition="4316" endWordPosition="4319">elements such as navigation links and content headings (Reason R1). Feature analysis reveals that both structural and linguistic statistics of these non-content elements can be more coherent than those of the correct entities. We suspect that since many of our features try to capture the coherence of entities, the system sometimes erroneously favors the more homonogenous non-content parts of the page. To disfavor these parts, One possible solution is to add visual features that capture how the web page is rendered and favor more salient parts of the page. (Liu et al., 2003; Song et al., 2004; Zhu et al., 2005; Zheng et al., 2007). 4.5 Feature variations We now investigate the contribution of each feature type. The ablation results on the development set over 10 random splits are shown in Table 6. We observe that denotation features improves accuracy on top of structural features. Table 7 shows an example of an error that is eliminated by each feature type. Generally, if the entities are represented as records (e.g., rows of a table), then denotation features will help the system select the correct field from each record. On the other hand, structural features prevent the system from selecting rand</context>
</contexts>
<marker>Zhu, Nie, Wen, Zhang, Ma, 2005</marker>
<rawString>J. Zhu, Z. Nie, J. Wen, B. Zhang, and W. Ma. 2005. 2D conditional random fields for web information extraction. In International Conference on Machine Learning (ICML), pages 1044–1051.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>