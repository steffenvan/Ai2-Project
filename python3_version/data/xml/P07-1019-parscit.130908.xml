<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.944932">
Forest Rescoring: Faster Decoding with Integrated Language Models ∗
</title>
<author confidence="0.999571">
Liang Huang David Chiang
</author>
<affiliation confidence="0.998649">
University of Pennsylvania USC Information Sciences Institute
</affiliation>
<address confidence="0.859531">
Philadelphia, PA 19104 Marina del Rey, CA 90292
</address>
<email confidence="0.999422">
lhuang3@cis.upenn.edu chiang@isi.edu
</email>
<sectionHeader confidence="0.993903" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999961384615385">
Efficient decoding has been a fundamental
problem in machine translation, especially
with an integrated language model which
is essential for achieving good translation
quality. We develop faster approaches for
this problem based on k-best parsing algo-
rithms and demonstrate their effectiveness
on both phrase-based and syntax-based MT
systems. In both cases, our methods achieve
significant speed improvements, often by
more than a factor of ten, over the conven-
tional beam-search method at the same lev-
els of search error and translation accuracy.
</bodyText>
<sectionHeader confidence="0.998975" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9994915">
Recent efforts in statistical machine translation
(MT) have seen promising improvements in out-
put quality, especially the phrase-based models (Och
and Ney, 2004) and syntax-based models (Chiang,
2005; Galley et al., 2006). However, efficient de-
coding under these paradigms, especially with inte-
grated language models (LMs), remains a difficult
problem. Part of the complexity arises from the ex-
pressive power of the translation model: for exam-
ple, a phrase- or word-based model with full reorder-
ing has exponential complexity (Knight, 1999). The
language model also, if fully integrated into the de-
coder, introduces an expensive overhead for main-
taining target-language boundary words for dynamic
</bodyText>
<note confidence="0.884186166666666">
∗ The authors would like to thank Dan Gildea, Jonathan
Graehl, Mark Johnson, Kevin Knight, Daniel Marcu, Bob
Moore and Hao Zhang. L. H. was partially supported by
NSF ITR grants IIS-0428020 while visiting USC/ISI and EIA-
0205456 at UPenn. D. C. was partially supported under the
GALE/DARPA program, contract HR0011-06-C-0022.
</note>
<page confidence="0.993084">
144
</page>
<bodyText confidence="0.99983295">
programming (Wu, 1996; Och and Ney, 2004). In
practice, one must prune the search space aggres-
sively to reduce it to a reasonable size.
A much simpler alternative method to incorporate
the LM is rescoring: we first decode without the LM
(henceforth −LM decoding) to produce a k-best list
of candidate translations, and then rerank the k-best
list using the LM. This method runs much faster in
practice but often produces a considerable number
of search errors since the true best translation (taking
LM into account) is often outside of the k-best list.
Cube pruning (Chiang, 2007) is a compromise be-
tween rescoring and full-integration: it rescores k
subtranslations at each node of the forest, rather than
only at the root node as in pure rescoring. By adapt-
ing the k-best parsing Algorithm 2 of Huang and
Chiang (2005), it achieves significant speed-up over
full-integration on Chiang’s Hiero system.
We push the idea behind this method further and
make the following contributions in this paper:
</bodyText>
<listItem confidence="0.9559531">
• We generalize cube pruning and adapt it to two
systems very different from Hiero: a phrase-
based system similar to Pharaoh (Koehn, 2004)
and a tree-to-string system (Huang et al., 2006).
• We also devise a faster variant of cube pruning,
called cube growing, which uses a lazy version
of k-best parsing (Huang and Chiang, 2005)
that tries to reduce k to the minimum needed
at each node to obtain the desired number of
hypotheses at the root.
</listItem>
<bodyText confidence="0.8876795">
Cube pruning and cube growing are collectively
called forest rescoring since they both approxi-
mately rescore the packed forest of derivations from
−LM decoding. In practice they run an order of
</bodyText>
<note confidence="0.90005">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144–151,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.998885333333333">
magnitude faster than full-integration with beam
search, at the same level of search errors and trans-
lation accuracy as measured by BLEU.
</bodyText>
<sectionHeader confidence="0.97432" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.999966833333333">
We establish in this section a unified framework
for translation with an integrated n-gram language
model in both phrase-based systems and syntax-
based systems based on synchronous context-free
grammars (SCFGs). An SCFG (Lewis and Stearns,
1968) is a context-free rewriting system for generat-
ing string pairs. Each rule A → α, Q rewrites a pair
of nonterminals in both languages, where α and Q
are the source and target side components, and there
is a one-to-one correspondence between the nonter-
minal occurrences in α and the nonterminal occur-
rences in Q. For example, the following rule
</bodyText>
<equation confidence="0.991374">
VP → PP (1) VP (2), VP (2) PP (1)
</equation>
<bodyText confidence="0.976075">
captures the swapping of VP and PP between Chi-
nese (source) and English (target).
</bodyText>
<subsectionHeader confidence="0.995516">
2.1 Translation as Deduction
</subsectionHeader>
<bodyText confidence="0.988604125">
We will use the following example from Chinese to
English for both systems described in this section:
yˇu Sh¯al´ong jˇuxing le huit´an
with Sharon hold [past] meeting
‘held a meeting with Sharon’
A typical phrase-based decoder generates partial
target-language outputs in left-to-right order in the
form of hypotheses (Koehn, 2004). Each hypothesis
has a coverage vector capturing the source-language
words translated so far, and can be extended into a
longer hypothesis by a phrase-pair translating an un-
covered segment.
This process can be formalized as a deduc-
tive system. For example, the following deduc-
tion step grows a hypothesis by the phrase-pair
hyˇu Sh¯al´ong, with Sharoni:
</bodyText>
<equation confidence="0.98186">
( •••) : (w, “held a talk”)
(•••••) : (w + c, “held a talk with Sharon”) (1)
</equation>
<bodyText confidence="0.999218666666667">
where a • in the coverage vector indicates the source
word at this position is “covered” (for simplicity
we omit here the ending position of the last phrase
which is needed for distortion costs), and where w
and w + c are the weights of the two hypotheses,
respectively, with c being the cost of the phrase-pair.
Similarly, the decoding problem with SCFGs can
also be cast as a deductive (parsing) system (Shieber
et al., 1995). Basically, we parse the input string us-
ing the source projection of the SCFG while build-
ing the corresponding subtranslations in parallel. A
possible deduction of the above example is notated:
</bodyText>
<equation confidence="0.999956">
(PP1,3) : (w1, t1) (VP3,6) : (w2, t2)
(VP1,6) : (w1 + w2 + c′, t2t1) (2)
</equation>
<bodyText confidence="0.9999385">
where the subscripts denote indices in the input sen-
tence just as in CKY parsing, w1, w2 are the scores
of the two antecedent items, and t1 and t2 are the
corresponding subtranslations. The resulting trans-
lation t2t1 is the inverted concatenation as specified
by the target-side of the SCFG rule with the addi-
tional cost c′ being the cost of this rule.
These two deductive systems represent the search
space of decoding without a language model. When
one is instantiated for a particular input string, it de-
fines a set of derivations, called a forest, represented
in a compact structure that has a structure of a graph
in the phrase-based case, or more generally, a hyper-
graph in both cases. Accordingly we call items like
(•••••) and (VP1,6) nodes in the forest, and instan-
tiated deductions like
</bodyText>
<equation confidence="0.962735">
(•••••) → ( •••) with Sharon,
(VP1,6) → (VP3,6) (PP1,3)
</equation>
<bodyText confidence="0.9917645">
we call hyperedges that connect one or more an-
tecedent nodes to a consequent node.
</bodyText>
<subsectionHeader confidence="0.99988">
2.2 Adding a Language Model
</subsectionHeader>
<bodyText confidence="0.999813363636364">
To integrate with a bigram language model, we can
use the dynamic-programming algorithms of Och
and Ney (2004) and Wu (1996) for phrase-based
and SCFG-based systems, respectively, which we
may think of as doing a finer-grained version of the
deductions above. Each node v in the forest will
be split into a set of augmented items, which we
call +LM items. For phrase-based decoding, a +LM
item has the form (v a) where a is the last word
of the hypothesis. Thus a +LM version of Deduc-
tion (1) might be:
</bodyText>
<equation confidence="0.614068">
( ••• talk) : (w, “held a talk”)
(••••• Sharon) : (w′, “held a talk with Sharon”)
</equation>
<page confidence="0.977765">
145
</page>
<figure confidence="0.998476352941177">
1.0 4.0 7.0 1.0 4.0 7.0 1.0 4.0 7.0 1.0 4.0 7.0
8.5
8.3
2.5
2.5
2.4
1.0
1.1
3.5
2.4
9.2
9.2
2.5
2.4
9.2 17.0
9.5
8.3
15.2
8.5
8.4
9.5
8.3
8.3
2.5
2.4
9.5
/ held * meeting
1� )
3,6
(VP held * talk)
3,6
(VP hold * conference)
3,6
(a) (b) (c) (d)
</figure>
<figureCaption confidence="0.938996">
Figure 1: Cube pruning along one hyperedge. (a): the numbers in the grid denote the score of the resulting
+LM item, including the combination cost; (b)-(d): the best-first enumeration of the top three items. Notice
that the items popped in (b) and (c) are out of order due to the non-monotonicity of the combination cost.
</figureCaption>
<bodyText confidence="0.998459545454546">
where the score of the resulting +LM item
w′ = w + c − log Plm(with  |talk)
now includes a combination cost due to the bigrams
formed when applying the phrase-pair.
Similarly, a +LM item in SCFG-based models
has the form (va*b), where a and b are boundary
words of the hypothesis string, and ⋆ is a placeholder
symbol for an elided part of that string, indicating
that a possible translation of the part of the input
spanned by v starts with a and ends with b. An ex-
ample +LM version of Deduction (2) is:
</bodyText>
<equation confidence="0.986948666666667">
(PP with* Sharon
1,3 ): (w1,t1) (VP
held * talk
3,6 ): (w2, t2)
( held * Sharon
l� 1,6 ): (w, t2t1)
</equation>
<bodyText confidence="0.999660444444444">
where w = w1 +w2 +c′−log Plm(with  |talk) with
a similar combination cost formed in combining ad-
jacent boundary words of antecedents. This scheme
can be easily extended to work with a general n-
gram model (Chiang, 2007). The experiments in this
paper use trigram models.
The conventional full-integration approach tra-
verses the forest bottom-up and explores all pos-
sible +LM deductions along each hyperedge.
The theoretical running time of this algorithm
is O(|F||T|(m−1)) for phrase-based models, and
O(|F||T|4(m−1)) for binary-branching SCFG-based
models, where |F |is the size of the forest, and |T|
is the number of possible target-side words. Even
if we assume a constant number of translations for
each word in the input, with a trigram model, this
still amounts to O(n11) for SCFG-based models and
O(2nn2) for phrase-based models.
</bodyText>
<sectionHeader confidence="0.98759" genericHeader="method">
3 Cube Pruning
</sectionHeader>
<bodyText confidence="0.999954645161291">
Cube pruning (Chiang, 2007) reduces the search
space significantly based on the observation that
when the above method is combined with beam
search, only a small fraction of the possible +LM
items at a node will escape being pruned, and more-
over we can select with reasonable accuracy those
top-k items without computing all possible items
first. In a nutshell, cube pruning works on the −LM
forest, keeping at most k +LM items at each node,
and uses the k-best parsing Algorithm 2 of Huang
and Chiang (2005) to speed up the computation.
For simplicity of presentation, we will use concrete
SCFG-based examples, but the method applies to the
general hypergraph framework in Section 2.
Consider Figure 1(a). Here k = 3 and we use
D(v) to denote the top-k +LM items (in sorted or-
der) of node v. Suppose we have computed D(u1)
and D(u2) for the two antecedent nodes u1 =
(VP3,6) and u2 = (PP1,3) respectively. Then for
the consequent node v = (VP1,6) we just need
to derive the top-3 from the 9 combinations of
(Di(u1), Dj(u2)) with i, j E [1, 3]. Since the an-
tecedent items are sorted, it is very likely that the
best consequent items in this grid lie towards the
upper-left corner. This situation is very similar to k-
best parsing and we can adapt the Algorithm 2 of
Huang and Chiang (2005) here to explore this grid
in a best-first order.
Suppose that the combination costs are negligible,
and therefore the weight of a consequent item is just
the product of the weights of the antecedent items.
</bodyText>
<page confidence="0.989736">
146
</page>
<listItem confidence="0.877372421052631">
1: function CUBE(F) ⊲ the input is a forest F
2: for v E F in (bottom-up) topological order do
3: KBEST(v)
4: return D1(TOP)
5: procedure KBEST(v)
6: cand +— {(e, 1)  |e E IN(v)} ⊲ for each incoming e
7: HEAPIFY(cand) ⊲ a priority queue of candidates
8: buf +— 0
9: while |cand |&gt; 0 and |buf  |&lt; k do
10: item +— POP-MIN(cand)
11: append item to buf
12: PUSHSUCC(item, cand)
13: sort buf to D(v)
14: procedure PUSHSUCC((e, j), cand)
15: e is v --+ ul ... u|e|
16: for i in 1 ... |e |do
17: j′ +— j + bi
18: if |D(ui) |&gt; j′i then
19: PUSH((e, j′), cand)
</listItem>
<figureCaption confidence="0.995811">
Figure 2: Pseudocode for cube pruning.
</figureCaption>
<bodyText confidence="0.999269269230769">
Then we know that D1(v) = (D1(u1),D1(u2)),
the upper-left corner of the grid. Moreover, we
know that D2(v) is the better of (D1(u1), D2(u2))
and (D2(u1),D1(u2)), the two neighbors of the
upper-left corner. We continue in this way (see Fig-
ure 1(b)–(d)), enumerating the consequent items
best-first while keeping track of a relatively small
number of candidates (shaded cells in Figure 1(b),
cand in Figure 2) for the next-best item.
However, when we take into account the combi-
nation costs, this grid is no longer monotonic in gen-
eral, and the above algorithm will not always enu-
merate items in best-first order. We can see this in
the first iteration in Figure 1(b), where an item with
score 2.5 has been enumerated even though there is
an item with score 2.4 still to come. Thus we risk
making more search errors than the full-integration
method, but in practice the loss is much less signif-
icant than the speedup. Because of this disordering,
we do not put the enumerated items directly into
D(v); instead, we collect items in a buffer (buf in
Figure 2) and re-sort the buffer into D(v) after it has
accumulated k items.1
In general the grammar may have multiple rules
that share the same source side but have different
target sides, which we have treated here as separate
</bodyText>
<footnote confidence="0.93289975">
1Notice that different combinations might have the same re-
sulting item, in which case we only keep the one with the better
score (sometimes called hypothesis recombination in MT liter-
ature), so the number of items in D(v) might be less than k.
</footnote>
<bodyText confidence="0.62859925">
k-best +LM rescoring.. .
only at the root node
on-the-fly at each node
on-the-fly at each node
</bodyText>
<tableCaption confidence="0.997677">
Table 1: Comparison of the three methods.
</tableCaption>
<bodyText confidence="0.999040545454546">
hyperedges in the −LM forest. In Hiero, these hy-
peredges are processed as a single unit which we
call a hyperedge bundle. The different target sides
then constitute a third dimension of the grid, form-
ing a cube of possible combinations (Chiang, 2007).
Now consider that there are many hyperedges that
derive v, and we are only interested the top +LM
items of v over all incoming hyperedges. Following
Algorithm 2, we initialize the priority queue cand
with the upper-left corner item from each hyper-
edge, and proceed as above. See Figure 2 for the
pseudocode for cube pruning. We use the notation
(e, j) to identify the derivation of v via the hyper-
edge e and the jith best subderivation of antecedent
ui (1 &lt; i &lt; |j|). Also, we let 1 stand for a vec-
tor whose elements are all 1, and W for the vector
whose members are all 0 except for the ith whose
value is 1 (the dimensionality of either should be ev-
ident from the context). The heart of the algorithm
is lines 10–12. Lines 10–11 move the best deriva-
tion (e, j) from cand to buf , and then line 12 pushes
its successors {(e, j + W)  |i E 1... |e|I into cand.
</bodyText>
<sectionHeader confidence="0.988927" genericHeader="method">
4 Cube Growing
</sectionHeader>
<bodyText confidence="0.9999835625">
Although much faster than full-integration, cube
pruning still computes a fixed amount of +LM items
at each node, many of which will not be useful for
arriving at the 1-best hypothesis at the root. It would
be more efficient to compute as few +LM items at
each node as are needed to obtain the 1-best hypoth-
esis at the root. This new method, called cube grow-
ing, is a lazy version of cube pruning just as Algo-
rithm 3 of Huang and Chiang (2005), is a lazy ver-
sion of Algorithm 2 (see Table 1).
Instead of traversing the forest bottom-up, cube
growing visits nodes recursively in depth-first or-
der from the root node (Figure 4). First we call
LAZYJTHBEST(TOP,1), which uses the same al-
gorithm as cube pruning to find the 1-best +LM
item of the root node using the best +LM items of
</bodyText>
<figure confidence="0.990111857142857">
method
rescoring
cube pruning
cube growing
Alg. 3
Alg. 2
Alg. 3
</figure>
<page confidence="0.550623">
147
</page>
<figureCaption confidence="0.9417215">
Figure 3: Example of cube growing along one hyper-
edge. (a): the h(x) scores for the grid in Figure 1(a),
</figureCaption>
<bodyText confidence="0.974672117647059">
assuming hcombo(e) = 0.1 for this hyperedge; (b)
cube growing prevents early ranking of the top-left
cell (2.5) as the best item in this grid.
the antecedent nodes. However, in this case the best
+LM items of the antecedent nodes are not known,
because we have not visited them yet. So we re-
cursively invoke LAZYJTHBEST on the antecedent
nodes to obtain them as needed. Each invocation of
LAZYJTHBEST(v, j) will recursively call itself on
the antecedents of v until it is confident that the jth
best +LM item for node v has been found.
Consider again the case of one hyperedge e. Be-
cause of the nonmonotonicity caused by combina-
tion costs, the first +LM item ((e,1)) popped from
cand is not guaranteed to be the best of all combina-
tions along this hyperedge (for example, the top-left
cell of 2.5 in Figure 1 is not the best in the grid). So
we cannot simply enumerate items just as they come
off of cand.2 Instead, we need to store up popped
items in a buffer buf , just as in cube pruning, and
enumerate an item only when we are confident that it
will never be surpassed in the future. In other words,
we would like to have an estimate of the best item
not explored yet (analogous to the heuristic func-
tion in A* search). If we can establish a lower bound
hcombo(e) on the combination cost of any +LM de-
duction via hyperedge e, then we can form a mono-
tonic grid (see Figure 3(a)) of lower bounds on the
grid of combinations, by using hcombo(e) in place of
the true combination cost for each +LM item x in
the grid; call this lower bound h(x).
Now suppose that the gray-shaded cells in Fig-
ure 3(a) are the members of cand. Then the min-
imum of h(x) over the items in cand, in this ex-
</bodyText>
<footnote confidence="0.6400585">
2If we did, then the out-of-order enumeration of +LM items
at an antecedent node would cause an entire row or column in
the grid to be disordered at the consequent node, potentially
leading to a multiplication of search errors.
</footnote>
<listItem confidence="0.791604444444445">
1: procedure LAZYJTHBEST(v, j)
2: if cand[v] is undefined then
3: cand[v] +— 0
4: FIRE(e, 1, cand) foreach e E IN(v)
5: buf [v] +— 0
6: while |D(v) |&lt; j and |buf [v] |+ |D(v) |&lt; k and
|cand[v] |&gt; 0 do
7: item +— POP-MIN(cand[v])
8: PUSH(item, buf [v])
</listItem>
<figure confidence="0.936122066666667">
9: PUSHSUCC(item, cand[v])
10: bound +— min{h(x)  |x E cand[v]}
11: ENUM(buf [v], D(v), bound)
12: ENUM(buf [v], D(v), +oo)
13: procedure FIRE(e, j, cand)
14: e is v --+ u1 ... u|e|
15: for i in 1 ... |e |do
16: LAZYJTHBEST(ui, ji)
17: if |D(ui) |&lt; ji then return
18: PUSH((e, j), cand)
19: procedure PUSHSUCC((e, j), cand)
20: FIRE(e, j + bi, cand) foreach i in 1 ... |e|
21: procedure ENUM(buf , D, bound)
22: while |buf  |&gt; 0 and MIN(buf ) &lt; bound do
23: append POP-MIN(buf ) to D
</figure>
<figureCaption confidence="0.999987">
Figure 4: Pseudocode of cube growing.
</figureCaption>
<bodyText confidence="0.905930166666667">
ample, min{2.2, 5.1} = 2.2 is a lower bound on
the cost of any item in the future for the hyperedge
e. Indeed, if cand contains items from multiple hy-
peredges for a single consequent node, this is still a
valid lower bound. More formally:
Lemma 1. For each node v in the forest, the term
</bodyText>
<equation confidence="0.7839205">
bound = min h(x) (3)
x∈cared[v]
</equation>
<bodyText confidence="0.990512933333333">
is a lower bound on the true cost of any future item
that is yet to be explored for v.
Proof. For any item x that is not explored yet, the
true cost c(x) &gt; h(x), by the definition of h. And
there exists an item y E cand[v] along the same hy-
peredge such that h(x) &gt; h(y), due to the mono-
tonicity of h within the grid along one hyperedge.
We also have h(y) &gt; bound by the definition of
bound. Therefore c(x) &gt; bound.
Now we can safely pop the best item from buf if
its true cost MIN(buf ) is better than bound and pass
it up to the consequent node (lines 21–23); but other-
wise, we have to wait for more items to accumulate
in buf to prevent a potential search error, for exam-
ple, in the case of Figure 3(b), where the top-left cell
</bodyText>
<figure confidence="0.9878149">
1.0 4.0 7.0
1.0 4.0 7.0
(a) h-values (b) true costs
1.0
1.1
3.5
2.1
2.2
4.6
5.1
5.2
8.1
8.2
2.5
2.4
8.3
7.6 10.6
148
1 2 3 4 5
1 2 3 4 5
</figure>
<figureCaption confidence="0.995076">
Figure 5: (a) Pharaoh expands the hypotheses in the
</figureCaption>
<bodyText confidence="0.972346375">
current bin (#2) into longer ones. (b) In Cubit, hy-
potheses in previous bins are fed via hyperedge bun-
dles (solid arrows) into a priority queue (shaded tri-
angle), which empties into the current bin (#5).
(2.5) is worse than the current bound of 2.2. The up-
date of bound in each iteration (line 10) can be effi-
ciently implemented by using another heap with the
same contents as cand but prioritized by h instead.
In practice this is a negligible overhead on top of
cube pruning.
We now turn to the problem of estimating the
heuristic function h,o,,,bo. In practice, computing
true lower bounds of the combination costs is too
slow and would compromise the speed up gained
from cube growing. So we instead use a much sim-
pler method that just calculates the minimum com-
bination cost of each hyperedge in the top-i deriva-
tions of the root node in −LM decoding. This is
just an approximation of the true lower bound, and
bad estimates can lead to search errors. However, the
hope is that by choosing the right value of i, these es-
timates will be accurate enough to affect the search
quality only slightly, which is analogous to “almost
admissible” heuristics in A* search (Soricut, 2006).
</bodyText>
<sectionHeader confidence="0.999011" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.998996333333333">
We test our methods on two large-scale English-to-
Chinese translation systems: a phrase-based system
and our tree-to-string system (Huang et al., 2006).
</bodyText>
<figureCaption confidence="0.643457">
Figure 6: A hyperedge bundle represents all +LM
deductions that derives an item in the current bin
from the same coverage vector (see Figure 5). The
phrases on the top denote the target-sides of appli-
cable phrase-pairs sharing the same source-side.
</figureCaption>
<subsectionHeader confidence="0.968702">
5.1 Phrase-based Decoding
</subsectionHeader>
<bodyText confidence="0.99989575">
We implemented Cubit, a Python clone of the
Pharaoh decoder (Koehn, 2004),3 and adapted cube
pruning to it as follows. As in Pharaoh, each bin
i contains hypotheses (i.e., +LM items) covering i
words on the source-side. But at each bin (see Fig-
ure 5), all +LM items from previous bins are first
partitioned into −LM items; then the hyperedges
leading from those −LM items are further grouped
into hyperedge bundles (Figure 6), which are placed
into the priority queue of the current bin.
Our data preparation follows Huang et al. (2006):
the training data is a parallel corpus of 28.3M words
on the English side, and a trigram language model is
trained on the Chinese side. We use the same test set
as (Huang et al., 2006), which is a 140-sentence sub-
set of the NIST 2003 test set with 9–36 words on the
English side. The weights for the log-linear model
are tuned on a separate development set. We set the
decoder phrase-table limit to 100 as suggested in
(Koehn, 2004) and the distortion limit to 4.
Figure 7(a) compares cube pruning against full-
integration in terms of search quality vs. search ef-
ficiency, under various pruning settings (threshold
beam set to 0.0001, stack size varying from 1 to
200). Search quality is measured by average model
cost per sentence (lower is better), and search effi-
ciency is measured by the average number of hy-
potheses generated (smaller is faster). At each level
</bodyText>
<footnote confidence="0.997807666666667">
3In our tests, Cubit always obtains a BLEU score within
0.004 of Pharaoh’s (Figure 7(b)). Source code available at
http://www.cis.upenn.edu/˜lhuang3/cubit/
</footnote>
<figure confidence="0.991137409090909">
1.0
1.1
3.5
� •••meeting)
� ••• talk)
� •••conference)
1.0 4.0 7.0
2.5
2.4
9.2 17.0
...
8.3
8.5
9.5
8.4
15.2
149
average model cost
BLEU score
92
88
84
80
76
102 103 104 105 106
average number of hypotheses per sentence
0.245
0.240
0.235
0.230
0.225
0.220
0.215
0.210
0.205
0.200
102 103 104 105 106
average number of hypotheses per sentence
full-integration (Cubit)
cube pruning (Cubit)
Pharaoh
full-integration (Cubit)
cube pruning (Cubit)
(a) (b)
</figure>
<figureCaption confidence="0.999989">
Figure 7: Cube pruning vs. full-integration (with beam search) on phrase-based decoding.
</figureCaption>
<bodyText confidence="0.999973">
of search quality, the speed-up is always better than
a factor of 10. The speed-up at the lowest search-
error level is a factor of 32. Figure 7(b) makes a
similar comparison but measures search quality by
BLEU, which shows an even larger relative speed-up
for a given BLEU score, because translations with
very different model costs might have similar BLEU
scores. It also shows that our full-integration imple-
mentation in Cubit faithfully reproduces Pharaoh’s
performance. Fixing the stack size to 100 and vary-
ing the threshold yielded a similar result.
</bodyText>
<subsectionHeader confidence="0.993131">
5.2 Tree-to-string Decoding
</subsectionHeader>
<bodyText confidence="0.999932571428571">
In tree-to-string (also called syntax-directed) decod-
ing (Huang et al., 2006; Liu et al., 2006), the source
string is first parsed into a tree, which is then re-
cursively converted into a target string according to
transfer rules in a synchronous grammar (Galley et
al., 2006). For instance, the following rule translates
an English passive construction into Chinese:
</bodyText>
<equation confidence="0.440833">
VP
→ b`ei x2 x1
was x1:VBN PP
IN x2:NP-C
by
</equation>
<bodyText confidence="0.978454972972973">
Our tree-to-string system performs slightly bet-
ter than the state-of-the-art phrase-based system
Pharaoh on the above data set. Although differ-
ent from the SCFG-based systems in Section 2, its
derivation trees remain context-free and the search
space is still a hypergraph, where we can adapt the
methods presented in Sections 3 and 4.
The data set is same as in Section 5.1, except that
we also parsed the English-side using a variant of
the Collins (1997) parser, and then extracted 24.7M
tree-to-string rules using the algorithm of (Galley et
al., 2006). Since our tree-to-string rules may have
many variables, we first binarize each hyperedge in
the forest on the target projection (Huang, 2007).
All the three +LM decoding methods to be com-
pared below take these binarized forests as input. For
cube growing, we use a non-duplicate k-best method
(Huang et al., 2006) to get 100-best unique transla-
tions according to −LM to estimate the lower-bound
heuristics.4 This preprocessing step takes on aver-
age 0.12 seconds per sentence, which is negligible
in comparison to the +LM decoding time.
Figure 8(a) compares cube growing and cube
pruning against full-integration under various beam
settings in the same fashion of Figure 7(a). At the
lowest level of search error, the relative speed-up
from cube growing and cube pruning compared with
full-integration is by a factor of 9.8 and 4.1, respec-
tively. Figure 8(b) is a similar comparison in terms
of BLEU scores and shows an even bigger advantage
of cube growing and cube pruning over the baseline.
4If a hyperedge is not represented at all in the 100-best −LM
derivations at the root node, we use the 1-best −LM derivation
of this hyperedge instead. Here, rules that share the same source
side but have different target sides are treated as separate hy-
peredges, not collected into hyperedge bundles, since grouping
becomes difficult after binarization.
</bodyText>
<sectionHeader confidence="0.613329" genericHeader="method">
VBD VP-C
</sectionHeader>
<page confidence="0.706609">
150
</page>
<figure confidence="0.999328727272727">
BLEU score
103 104 105
average number of +LM items explored per sentence
103 104 105
average number of +LM items explored per sentence
full-integration
cube pruning
cube growing
full-integration
cube pruning
cube growing
average model cost 219.0
218.8
218.6
218.4
218.2
0.262
0.260
0.258
0.256
0.254
(a) (b)
</figure>
<figureCaption confidence="0.999854">
Figure 8: Cube growing vs. cube pruning vs. full-integration (with beam search) on tree-to-string decoding.
</figureCaption>
<sectionHeader confidence="0.997892" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999993">
We have presented a novel extension of cube prun-
ing called cube growing, and shown how both can be
seen as general forest rescoring techniques applica-
ble to both phrase-based and syntax-based decoding.
We evaluated these methods on large-scale transla-
tion tasks and observed considerable speed improve-
ments, often by more than a factor of ten. We plan
to investigate how to adapt cube growing to phrase-
based and hierarchical phrase-based systems.
These forest rescoring algorithms have potential
applications to other computationally intensive tasks
involving combinations of different models, for
example, head-lexicalized parsing (Collins, 1997);
joint parsing and semantic role labeling (Sutton and
McCallum, 2005); or tagging and parsing with non-
local features. Thus we envision forest rescoring as
being of general applicability for reducing compli-
cated search spaces, as an alternative to simulated
annealing methods (Kirkpatrick et al., 1983).
</bodyText>
<sectionHeader confidence="0.999481" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999936454545455">
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. ACL.
David Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2). To appear.
Michael Collins. 1997. Three generative lexicalised models for
statistical parsing. In Proc. ACL.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable inference and
training of context-rich syntactic translation models. In
Proc. COLING-ACL.
Liang Huang and David Chiang. 2005. Better k-best parsing.
In Proc. IWPT.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Sta-
tistical syntax-directed translation with extended domain of
locality. In Proc. AMTA.
Liang Huang. 2007. Binarization, synchronous binarization,
and target-side binarization. In Proc. NAACL Workshop on
Syntax and Structure in Statistical Translation.
S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. 1983. Optimiza-
tion by simulated annealing. Science, 220(4598):671–680.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Linguistics,
25(4):607–615.
Philipp Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In
Proc. AMTA, pages 115–124.
P. M. Lewis and R. E. Stearns. 1968. Syntax-directed transduc-
tion. J. ACM, 15:465–488.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string
alignment template for statistical machine translation. In
Proc. COLING-ACL, pages 609–616.
Franz Joseph Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation. Com-
putational Linguistics, 30:417–449.
Stuart Shieber, Yves Schabes, and Fernando Pereira. 1995.
Principles and implementation of deductive parsing. J. Logic
Programming, 24:3–36.
Radu Soricut. 2006. Natural Language Generation using an
Information-Slim Representation. Ph.D. thesis, University
of Southern California.
Charles Sutton and Andrew McCallum. 2005. Joint parsing
and semantic role labeling. In Proc. CoNLL 2005.
Dekai Wu. 1996. A polynomial-time algorithm for statistical
machine translation. In Proc. ACL.
</reference>
<page confidence="0.998351">
151
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.970053">
<title confidence="0.999378">Rescoring: Faster Decoding with Integrated Language Models</title>
<author confidence="0.999965">Liang Huang David Chiang</author>
<affiliation confidence="0.999998">University of Pennsylvania USC Information Sciences Institute</affiliation>
<address confidence="0.997111">Philadelphia, PA 19104 Marina del Rey, CA 90292</address>
<email confidence="0.993706">lhuang3@cis.upenn.educhiang@isi.edu</email>
<abstract confidence="0.998514142857143">Efficient decoding has been a fundamental problem in machine translation, especially with an integrated language model which is essential for achieving good translation quality. We develop faster approaches for problem based on parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems. In both cases, our methods achieve significant speed improvements, often by more than a factor of ten, over the conventional beam-search method at the same levels of search error and translation accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="1013" citStr="Chiang, 2005" startWordPosition="141" endWordPosition="142">good translation quality. We develop faster approaches for this problem based on k-best parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems. In both cases, our methods achieve significant speed improvements, often by more than a factor of ten, over the conventional beam-search method at the same levels of search error and translation accuracy. 1 Introduction Recent efforts in statistical machine translation (MT) have seen promising improvements in output quality, especially the phrase-based models (Och and Ney, 2004) and syntax-based models (Chiang, 2005; Galley et al., 2006). However, efficient decoding under these paradigms, especially with integrated language models (LMs), remains a difficult problem. Part of the complexity arises from the expressive power of the translation model: for example, a phrase- or word-based model with full reordering has exponential complexity (Knight, 1999). The language model also, if fully integrated into the decoder, introduces an expensive overhead for maintaining target-language boundary words for dynamic ∗ The authors would like to thank Dan Gildea, Jonathan Graehl, Mark Johnson, Kevin Knight, Daniel Marc</context>
<context position="2661" citStr="Chiang (2005)" startWordPosition="408" endWordPosition="409"> first decode without the LM (henceforth −LM decoding) to produce a k-best list of candidate translations, and then rerank the k-best list using the LM. This method runs much faster in practice but often produces a considerable number of search errors since the true best translation (taking LM into account) is often outside of the k-best list. Cube pruning (Chiang, 2007) is a compromise between rescoring and full-integration: it rescores k subtranslations at each node of the forest, rather than only at the root node as in pure rescoring. By adapting the k-best parsing Algorithm 2 of Huang and Chiang (2005), it achieves significant speed-up over full-integration on Chiang’s Hiero system. We push the idea behind this method further and make the following contributions in this paper: • We generalize cube pruning and adapt it to two systems very different from Hiero: a phrasebased system similar to Pharaoh (Koehn, 2004) and a tree-to-string system (Huang et al., 2006). • We also devise a faster variant of cube pruning, called cube growing, which uses a lazy version of k-best parsing (Huang and Chiang, 2005) that tries to reduce k to the minimum needed at each node to obtain the desired number of hy</context>
<context position="10092" citStr="Chiang (2005)" startWordPosition="1697" endWordPosition="1698">, this still amounts to O(n11) for SCFG-based models and O(2nn2) for phrase-based models. 3 Cube Pruning Cube pruning (Chiang, 2007) reduces the search space significantly based on the observation that when the above method is combined with beam search, only a small fraction of the possible +LM items at a node will escape being pruned, and moreover we can select with reasonable accuracy those top-k items without computing all possible items first. In a nutshell, cube pruning works on the −LM forest, keeping at most k +LM items at each node, and uses the k-best parsing Algorithm 2 of Huang and Chiang (2005) to speed up the computation. For simplicity of presentation, we will use concrete SCFG-based examples, but the method applies to the general hypergraph framework in Section 2. Consider Figure 1(a). Here k = 3 and we use D(v) to denote the top-k +LM items (in sorted order) of node v. Suppose we have computed D(u1) and D(u2) for the two antecedent nodes u1 = (VP3,6) and u2 = (PP1,3) respectively. Then for the consequent node v = (VP1,6) we just need to derive the top-3 from the 9 combinations of (Di(u1), Dj(u2)) with i, j E [1, 3]. Since the antecedent items are sorted, it is very likely that t</context>
<context position="14898" citStr="Chiang (2005)" startWordPosition="2586" endWordPosition="2587">s lines 10–12. Lines 10–11 move the best derivation (e, j) from cand to buf , and then line 12 pushes its successors {(e, j + W) |i E 1... |e|I into cand. 4 Cube Growing Although much faster than full-integration, cube pruning still computes a fixed amount of +LM items at each node, many of which will not be useful for arriving at the 1-best hypothesis at the root. It would be more efficient to compute as few +LM items at each node as are needed to obtain the 1-best hypothesis at the root. This new method, called cube growing, is a lazy version of cube pruning just as Algorithm 3 of Huang and Chiang (2005), is a lazy version of Algorithm 2 (see Table 1). Instead of traversing the forest bottom-up, cube growing visits nodes recursively in depth-first order from the root node (Figure 4). First we call LAZYJTHBEST(TOP,1), which uses the same algorithm as cube pruning to find the 1-best +LM item of the root node using the best +LM items of method rescoring cube pruning cube growing Alg. 3 Alg. 2 Alg. 3 147 Figure 3: Example of cube growing along one hyperedge. (a): the h(x) scores for the grid in Figure 1(a), assuming hcombo(e) = 0.1 for this hyperedge; (b) cube growing prevents early ranking of th</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<note>To appear.</note>
<contexts>
<context position="2421" citStr="Chiang, 2007" startWordPosition="366" endWordPosition="367"> contract HR0011-06-C-0022. 144 programming (Wu, 1996; Och and Ney, 2004). In practice, one must prune the search space aggressively to reduce it to a reasonable size. A much simpler alternative method to incorporate the LM is rescoring: we first decode without the LM (henceforth −LM decoding) to produce a k-best list of candidate translations, and then rerank the k-best list using the LM. This method runs much faster in practice but often produces a considerable number of search errors since the true best translation (taking LM into account) is often outside of the k-best list. Cube pruning (Chiang, 2007) is a compromise between rescoring and full-integration: it rescores k subtranslations at each node of the forest, rather than only at the root node as in pure rescoring. By adapting the k-best parsing Algorithm 2 of Huang and Chiang (2005), it achieves significant speed-up over full-integration on Chiang’s Hiero system. We push the idea behind this method further and make the following contributions in this paper: • We generalize cube pruning and adapt it to two systems very different from Hiero: a phrasebased system similar to Pharaoh (Koehn, 2004) and a tree-to-string system (Huang et al., </context>
<context position="8950" citStr="Chiang, 2007" startWordPosition="1512" endWordPosition="1513">as the form (va*b), where a and b are boundary words of the hypothesis string, and ⋆ is a placeholder symbol for an elided part of that string, indicating that a possible translation of the part of the input spanned by v starts with a and ends with b. An example +LM version of Deduction (2) is: (PP with* Sharon 1,3 ): (w1,t1) (VP held * talk 3,6 ): (w2, t2) ( held * Sharon l� 1,6 ): (w, t2t1) where w = w1 +w2 +c′−log Plm(with |talk) with a similar combination cost formed in combining adjacent boundary words of antecedents. This scheme can be easily extended to work with a general ngram model (Chiang, 2007). The experiments in this paper use trigram models. The conventional full-integration approach traverses the forest bottom-up and explores all possible +LM deductions along each hyperedge. The theoretical running time of this algorithm is O(|F||T|(m−1)) for phrase-based models, and O(|F||T|4(m−1)) for binary-branching SCFG-based models, where |F |is the size of the forest, and |T| is the number of possible target-side words. Even if we assume a constant number of translations for each word in the input, with a trigram model, this still amounts to O(n11) for SCFG-based models and O(2nn2) for ph</context>
<context position="13579" citStr="Chiang, 2007" startWordPosition="2331" endWordPosition="2332">rent combinations might have the same resulting item, in which case we only keep the one with the better score (sometimes called hypothesis recombination in MT literature), so the number of items in D(v) might be less than k. k-best +LM rescoring.. . only at the root node on-the-fly at each node on-the-fly at each node Table 1: Comparison of the three methods. hyperedges in the −LM forest. In Hiero, these hyperedges are processed as a single unit which we call a hyperedge bundle. The different target sides then constitute a third dimension of the grid, forming a cube of possible combinations (Chiang, 2007). Now consider that there are many hyperedges that derive v, and we are only interested the top +LM items of v over all incoming hyperedges. Following Algorithm 2, we initialize the priority queue cand with the upper-left corner item from each hyperedge, and proceed as above. See Figure 2 for the pseudocode for cube pruning. We use the notation (e, j) to identify the derivation of v via the hyperedge e and the jith best subderivation of antecedent ui (1 &lt; i &lt; |j|). Also, we let 1 stand for a vector whose elements are all 1, and W for the vector whose members are all 0 except for the ith whose </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2). To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="24503" citStr="Collins (1997)" startWordPosition="4321" endWordPosition="4322">(Galley et al., 2006). For instance, the following rule translates an English passive construction into Chinese: VP → b`ei x2 x1 was x1:VBN PP IN x2:NP-C by Our tree-to-string system performs slightly better than the state-of-the-art phrase-based system Pharaoh on the above data set. Although different from the SCFG-based systems in Section 2, its derivation trees remain context-free and the search space is still a hypergraph, where we can adapt the methods presented in Sections 3 and 4. The data set is same as in Section 5.1, except that we also parsed the English-side using a variant of the Collins (1997) parser, and then extracted 24.7M tree-to-string rules using the algorithm of (Galley et al., 2006). Since our tree-to-string rules may have many variables, we first binarize each hyperedge in the forest on the target projection (Huang, 2007). All the three +LM decoding methods to be compared below take these binarized forests as input. For cube growing, we use a non-duplicate k-best method (Huang et al., 2006) to get 100-best unique translations according to −LM to estimate the lower-bound heuristics.4 This preprocessing step takes on average 0.12 seconds per sentence, which is negligible in </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative lexicalised models for statistical parsing. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>J Graehl</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>S DeNeefe</author>
<author>W Wang</author>
<author>I Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proc. COLING-ACL.</booktitle>
<contexts>
<context position="1035" citStr="Galley et al., 2006" startWordPosition="143" endWordPosition="146">on quality. We develop faster approaches for this problem based on k-best parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems. In both cases, our methods achieve significant speed improvements, often by more than a factor of ten, over the conventional beam-search method at the same levels of search error and translation accuracy. 1 Introduction Recent efforts in statistical machine translation (MT) have seen promising improvements in output quality, especially the phrase-based models (Och and Ney, 2004) and syntax-based models (Chiang, 2005; Galley et al., 2006). However, efficient decoding under these paradigms, especially with integrated language models (LMs), remains a difficult problem. Part of the complexity arises from the expressive power of the translation model: for example, a phrase- or word-based model with full reordering has exponential complexity (Knight, 1999). The language model also, if fully integrated into the decoder, introduces an expensive overhead for maintaining target-language boundary words for dynamic ∗ The authors would like to thank Dan Gildea, Jonathan Graehl, Mark Johnson, Kevin Knight, Daniel Marcu, Bob Moore and Hao Z</context>
<context position="23910" citStr="Galley et al., 2006" startWordPosition="4220" endWordPosition="4223">relative speed-up for a given BLEU score, because translations with very different model costs might have similar BLEU scores. It also shows that our full-integration implementation in Cubit faithfully reproduces Pharaoh’s performance. Fixing the stack size to 100 and varying the threshold yielded a similar result. 5.2 Tree-to-string Decoding In tree-to-string (also called syntax-directed) decoding (Huang et al., 2006; Liu et al., 2006), the source string is first parsed into a tree, which is then recursively converted into a target string according to transfer rules in a synchronous grammar (Galley et al., 2006). For instance, the following rule translates an English passive construction into Chinese: VP → b`ei x2 x1 was x1:VBN PP IN x2:NP-C by Our tree-to-string system performs slightly better than the state-of-the-art phrase-based system Pharaoh on the above data set. Although different from the SCFG-based systems in Section 2, its derivation trees remain context-free and the search space is still a hypergraph, where we can adapt the methods presented in Sections 3 and 4. The data set is same as in Section 5.1, except that we also parsed the English-side using a variant of the Collins (1997) parser</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe, W. Wang, and I. Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proc. COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proc. IWPT.</booktitle>
<contexts>
<context position="2661" citStr="Huang and Chiang (2005)" startWordPosition="406" endWordPosition="409">coring: we first decode without the LM (henceforth −LM decoding) to produce a k-best list of candidate translations, and then rerank the k-best list using the LM. This method runs much faster in practice but often produces a considerable number of search errors since the true best translation (taking LM into account) is often outside of the k-best list. Cube pruning (Chiang, 2007) is a compromise between rescoring and full-integration: it rescores k subtranslations at each node of the forest, rather than only at the root node as in pure rescoring. By adapting the k-best parsing Algorithm 2 of Huang and Chiang (2005), it achieves significant speed-up over full-integration on Chiang’s Hiero system. We push the idea behind this method further and make the following contributions in this paper: • We generalize cube pruning and adapt it to two systems very different from Hiero: a phrasebased system similar to Pharaoh (Koehn, 2004) and a tree-to-string system (Huang et al., 2006). • We also devise a faster variant of cube pruning, called cube growing, which uses a lazy version of k-best parsing (Huang and Chiang, 2005) that tries to reduce k to the minimum needed at each node to obtain the desired number of hy</context>
<context position="10092" citStr="Huang and Chiang (2005)" startWordPosition="1695" endWordPosition="1698">gram model, this still amounts to O(n11) for SCFG-based models and O(2nn2) for phrase-based models. 3 Cube Pruning Cube pruning (Chiang, 2007) reduces the search space significantly based on the observation that when the above method is combined with beam search, only a small fraction of the possible +LM items at a node will escape being pruned, and moreover we can select with reasonable accuracy those top-k items without computing all possible items first. In a nutshell, cube pruning works on the −LM forest, keeping at most k +LM items at each node, and uses the k-best parsing Algorithm 2 of Huang and Chiang (2005) to speed up the computation. For simplicity of presentation, we will use concrete SCFG-based examples, but the method applies to the general hypergraph framework in Section 2. Consider Figure 1(a). Here k = 3 and we use D(v) to denote the top-k +LM items (in sorted order) of node v. Suppose we have computed D(u1) and D(u2) for the two antecedent nodes u1 = (VP3,6) and u2 = (PP1,3) respectively. Then for the consequent node v = (VP1,6) we just need to derive the top-3 from the 9 combinations of (Di(u1), Dj(u2)) with i, j E [1, 3]. Since the antecedent items are sorted, it is very likely that t</context>
<context position="14898" citStr="Huang and Chiang (2005)" startWordPosition="2584" endWordPosition="2587">lgorithm is lines 10–12. Lines 10–11 move the best derivation (e, j) from cand to buf , and then line 12 pushes its successors {(e, j + W) |i E 1... |e|I into cand. 4 Cube Growing Although much faster than full-integration, cube pruning still computes a fixed amount of +LM items at each node, many of which will not be useful for arriving at the 1-best hypothesis at the root. It would be more efficient to compute as few +LM items at each node as are needed to obtain the 1-best hypothesis at the root. This new method, called cube growing, is a lazy version of cube pruning just as Algorithm 3 of Huang and Chiang (2005), is a lazy version of Algorithm 2 (see Table 1). Instead of traversing the forest bottom-up, cube growing visits nodes recursively in depth-first order from the root node (Figure 4). First we call LAZYJTHBEST(TOP,1), which uses the same algorithm as cube pruning to find the 1-best +LM item of the root node using the best +LM items of method rescoring cube pruning cube growing Alg. 3 Alg. 2 Alg. 3 147 Figure 3: Example of cube growing along one hyperedge. (a): the h(x) scores for the grid in Figure 1(a), assuming hcombo(e) = 0.1 for this hyperedge; (b) cube growing prevents early ranking of th</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proc. IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proc. AMTA.</booktitle>
<contexts>
<context position="3026" citStr="Huang et al., 2006" startWordPosition="464" endWordPosition="467">(Chiang, 2007) is a compromise between rescoring and full-integration: it rescores k subtranslations at each node of the forest, rather than only at the root node as in pure rescoring. By adapting the k-best parsing Algorithm 2 of Huang and Chiang (2005), it achieves significant speed-up over full-integration on Chiang’s Hiero system. We push the idea behind this method further and make the following contributions in this paper: • We generalize cube pruning and adapt it to two systems very different from Hiero: a phrasebased system similar to Pharaoh (Koehn, 2004) and a tree-to-string system (Huang et al., 2006). • We also devise a faster variant of cube pruning, called cube growing, which uses a lazy version of k-best parsing (Huang and Chiang, 2005) that tries to reduce k to the minimum needed at each node to obtain the desired number of hypotheses at the root. Cube pruning and cube growing are collectively called forest rescoring since they both approximately rescore the packed forest of derivations from −LM decoding. In practice they run an order of Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144–151, Prague, Czech Republic, June 2007. c�2007 Asso</context>
<context position="20678" citStr="Huang et al., 2006" startWordPosition="3680" endWordPosition="3683"> that just calculates the minimum combination cost of each hyperedge in the top-i derivations of the root node in −LM decoding. This is just an approximation of the true lower bound, and bad estimates can lead to search errors. However, the hope is that by choosing the right value of i, these estimates will be accurate enough to affect the search quality only slightly, which is analogous to “almost admissible” heuristics in A* search (Soricut, 2006). 5 Experiments We test our methods on two large-scale English-toChinese translation systems: a phrase-based system and our tree-to-string system (Huang et al., 2006). Figure 6: A hyperedge bundle represents all +LM deductions that derives an item in the current bin from the same coverage vector (see Figure 5). The phrases on the top denote the target-sides of applicable phrase-pairs sharing the same source-side. 5.1 Phrase-based Decoding We implemented Cubit, a Python clone of the Pharaoh decoder (Koehn, 2004),3 and adapted cube pruning to it as follows. As in Pharaoh, each bin i contains hypotheses (i.e., +LM items) covering i words on the source-side. But at each bin (see Figure 5), all +LM items from previous bins are first partitioned into −LM items; </context>
<context position="23711" citStr="Huang et al., 2006" startWordPosition="4185" endWordPosition="4188">s better than a factor of 10. The speed-up at the lowest searcherror level is a factor of 32. Figure 7(b) makes a similar comparison but measures search quality by BLEU, which shows an even larger relative speed-up for a given BLEU score, because translations with very different model costs might have similar BLEU scores. It also shows that our full-integration implementation in Cubit faithfully reproduces Pharaoh’s performance. Fixing the stack size to 100 and varying the threshold yielded a similar result. 5.2 Tree-to-string Decoding In tree-to-string (also called syntax-directed) decoding (Huang et al., 2006; Liu et al., 2006), the source string is first parsed into a tree, which is then recursively converted into a target string according to transfer rules in a synchronous grammar (Galley et al., 2006). For instance, the following rule translates an English passive construction into Chinese: VP → b`ei x2 x1 was x1:VBN PP IN x2:NP-C by Our tree-to-string system performs slightly better than the state-of-the-art phrase-based system Pharaoh on the above data set. Although different from the SCFG-based systems in Section 2, its derivation trees remain context-free and the search space is still a hyp</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proc. AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Binarization, synchronous binarization, and target-side binarization.</title>
<date>2007</date>
<booktitle>In Proc. NAACL Workshop on Syntax and Structure in Statistical Translation.</booktitle>
<contexts>
<context position="24745" citStr="Huang, 2007" startWordPosition="4358" endWordPosition="4359">ed system Pharaoh on the above data set. Although different from the SCFG-based systems in Section 2, its derivation trees remain context-free and the search space is still a hypergraph, where we can adapt the methods presented in Sections 3 and 4. The data set is same as in Section 5.1, except that we also parsed the English-side using a variant of the Collins (1997) parser, and then extracted 24.7M tree-to-string rules using the algorithm of (Galley et al., 2006). Since our tree-to-string rules may have many variables, we first binarize each hyperedge in the forest on the target projection (Huang, 2007). All the three +LM decoding methods to be compared below take these binarized forests as input. For cube growing, we use a non-duplicate k-best method (Huang et al., 2006) to get 100-best unique translations according to −LM to estimate the lower-bound heuristics.4 This preprocessing step takes on average 0.12 seconds per sentence, which is negligible in comparison to the +LM decoding time. Figure 8(a) compares cube growing and cube pruning against full-integration under various beam settings in the same fashion of Figure 7(a). At the lowest level of search error, the relative speed-up from c</context>
</contexts>
<marker>Huang, 2007</marker>
<rawString>Liang Huang. 2007. Binarization, synchronous binarization, and target-side binarization. In Proc. NAACL Workshop on Syntax and Structure in Statistical Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kirkpatrick</author>
<author>C D Gelatt</author>
<author>M P Vecchi</author>
</authors>
<title>Optimization by simulated annealing.</title>
<date>1983</date>
<journal>Science,</journal>
<volume>220</volume>
<issue>4598</issue>
<marker>Kirkpatrick, Gelatt, Vecchi, 1983</marker>
<rawString>S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. 1983. Optimization by simulated annealing. Science, 220(4598):671–680.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>Decoding complexity in wordreplacement translation models.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="1354" citStr="Knight, 1999" startWordPosition="194" endWordPosition="195">the same levels of search error and translation accuracy. 1 Introduction Recent efforts in statistical machine translation (MT) have seen promising improvements in output quality, especially the phrase-based models (Och and Ney, 2004) and syntax-based models (Chiang, 2005; Galley et al., 2006). However, efficient decoding under these paradigms, especially with integrated language models (LMs), remains a difficult problem. Part of the complexity arises from the expressive power of the translation model: for example, a phrase- or word-based model with full reordering has exponential complexity (Knight, 1999). The language model also, if fully integrated into the decoder, introduces an expensive overhead for maintaining target-language boundary words for dynamic ∗ The authors would like to thank Dan Gildea, Jonathan Graehl, Mark Johnson, Kevin Knight, Daniel Marcu, Bob Moore and Hao Zhang. L. H. was partially supported by NSF ITR grants IIS-0428020 while visiting USC/ISI and EIA0205456 at UPenn. D. C. was partially supported under the GALE/DARPA program, contract HR0011-06-C-0022. 144 programming (Wu, 1996; Och and Ney, 2004). In practice, one must prune the search space aggressively to reduce it </context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>Kevin Knight. 1999. Decoding complexity in wordreplacement translation models. Computational Linguistics, 25(4):607–615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proc. AMTA,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="2977" citStr="Koehn, 2004" startWordPosition="458" endWordPosition="459"> outside of the k-best list. Cube pruning (Chiang, 2007) is a compromise between rescoring and full-integration: it rescores k subtranslations at each node of the forest, rather than only at the root node as in pure rescoring. By adapting the k-best parsing Algorithm 2 of Huang and Chiang (2005), it achieves significant speed-up over full-integration on Chiang’s Hiero system. We push the idea behind this method further and make the following contributions in this paper: • We generalize cube pruning and adapt it to two systems very different from Hiero: a phrasebased system similar to Pharaoh (Koehn, 2004) and a tree-to-string system (Huang et al., 2006). • We also devise a faster variant of cube pruning, called cube growing, which uses a lazy version of k-best parsing (Huang and Chiang, 2005) that tries to reduce k to the minimum needed at each node to obtain the desired number of hypotheses at the root. Cube pruning and cube growing are collectively called forest rescoring since they both approximately rescore the packed forest of derivations from −LM decoding. In practice they run an order of Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144–15</context>
<context position="4882" citStr="Koehn, 2004" startWordPosition="766" endWordPosition="767">ne-to-one correspondence between the nonterminal occurrences in α and the nonterminal occurrences in Q. For example, the following rule VP → PP (1) VP (2), VP (2) PP (1) captures the swapping of VP and PP between Chinese (source) and English (target). 2.1 Translation as Deduction We will use the following example from Chinese to English for both systems described in this section: yˇu Sh¯al´ong jˇuxing le huit´an with Sharon hold [past] meeting ‘held a meeting with Sharon’ A typical phrase-based decoder generates partial target-language outputs in left-to-right order in the form of hypotheses (Koehn, 2004). Each hypothesis has a coverage vector capturing the source-language words translated so far, and can be extended into a longer hypothesis by a phrase-pair translating an uncovered segment. This process can be formalized as a deductive system. For example, the following deduction step grows a hypothesis by the phrase-pair hyˇu Sh¯al´ong, with Sharoni: ( •••) : (w, “held a talk”) (•••••) : (w + c, “held a talk with Sharon”) (1) where a • in the coverage vector indicates the source word at this position is “covered” (for simplicity we omit here the ending position of the last phrase which is ne</context>
<context position="21028" citStr="Koehn, 2004" startWordPosition="3738" endWordPosition="3739">only slightly, which is analogous to “almost admissible” heuristics in A* search (Soricut, 2006). 5 Experiments We test our methods on two large-scale English-toChinese translation systems: a phrase-based system and our tree-to-string system (Huang et al., 2006). Figure 6: A hyperedge bundle represents all +LM deductions that derives an item in the current bin from the same coverage vector (see Figure 5). The phrases on the top denote the target-sides of applicable phrase-pairs sharing the same source-side. 5.1 Phrase-based Decoding We implemented Cubit, a Python clone of the Pharaoh decoder (Koehn, 2004),3 and adapted cube pruning to it as follows. As in Pharaoh, each bin i contains hypotheses (i.e., +LM items) covering i words on the source-side. But at each bin (see Figure 5), all +LM items from previous bins are first partitioned into −LM items; then the hyperedges leading from those −LM items are further grouped into hyperedge bundles (Figure 6), which are placed into the priority queue of the current bin. Our data preparation follows Huang et al. (2006): the training data is a parallel corpus of 28.3M words on the English side, and a trigram language model is trained on the Chinese side.</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Proc. AMTA, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P M Lewis</author>
<author>R E Stearns</author>
</authors>
<title>Syntax-directed transduction.</title>
<date>1968</date>
<journal>J. ACM,</journal>
<pages>15--465</pages>
<contexts>
<context position="4062" citStr="Lewis and Stearns, 1968" startWordPosition="626" endWordPosition="629">ding. In practice they run an order of Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144–151, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics magnitude faster than full-integration with beam search, at the same level of search errors and translation accuracy as measured by BLEU. 2 Preliminaries We establish in this section a unified framework for translation with an integrated n-gram language model in both phrase-based systems and syntaxbased systems based on synchronous context-free grammars (SCFGs). An SCFG (Lewis and Stearns, 1968) is a context-free rewriting system for generating string pairs. Each rule A → α, Q rewrites a pair of nonterminals in both languages, where α and Q are the source and target side components, and there is a one-to-one correspondence between the nonterminal occurrences in α and the nonterminal occurrences in Q. For example, the following rule VP → PP (1) VP (2), VP (2) PP (1) captures the swapping of VP and PP between Chinese (source) and English (target). 2.1 Translation as Deduction We will use the following example from Chinese to English for both systems described in this section: yˇu Sh¯al</context>
</contexts>
<marker>Lewis, Stearns, 1968</marker>
<rawString>P. M. Lewis and R. E. Stearns. 1968. Syntax-directed transduction. J. ACM, 15:465–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-to-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. COLING-ACL,</booktitle>
<pages>609--616</pages>
<contexts>
<context position="23730" citStr="Liu et al., 2006" startWordPosition="4189" endWordPosition="4192">or of 10. The speed-up at the lowest searcherror level is a factor of 32. Figure 7(b) makes a similar comparison but measures search quality by BLEU, which shows an even larger relative speed-up for a given BLEU score, because translations with very different model costs might have similar BLEU scores. It also shows that our full-integration implementation in Cubit faithfully reproduces Pharaoh’s performance. Fixing the stack size to 100 and varying the threshold yielded a similar result. 5.2 Tree-to-string Decoding In tree-to-string (also called syntax-directed) decoding (Huang et al., 2006; Liu et al., 2006), the source string is first parsed into a tree, which is then recursively converted into a target string according to transfer rules in a synchronous grammar (Galley et al., 2006). For instance, the following rule translates an English passive construction into Chinese: VP → b`ei x2 x1 was x1:VBN PP IN x2:NP-C by Our tree-to-string system performs slightly better than the state-of-the-art phrase-based system Pharaoh on the above data set. Although different from the SCFG-based systems in Section 2, its derivation trees remain context-free and the search space is still a hypergraph, where we c</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation. In Proc. COLING-ACL, pages 609–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--417</pages>
<contexts>
<context position="975" citStr="Och and Ney, 2004" startWordPosition="134" endWordPosition="137">uage model which is essential for achieving good translation quality. We develop faster approaches for this problem based on k-best parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems. In both cases, our methods achieve significant speed improvements, often by more than a factor of ten, over the conventional beam-search method at the same levels of search error and translation accuracy. 1 Introduction Recent efforts in statistical machine translation (MT) have seen promising improvements in output quality, especially the phrase-based models (Och and Ney, 2004) and syntax-based models (Chiang, 2005; Galley et al., 2006). However, efficient decoding under these paradigms, especially with integrated language models (LMs), remains a difficult problem. Part of the complexity arises from the expressive power of the translation model: for example, a phrase- or word-based model with full reordering has exponential complexity (Knight, 1999). The language model also, if fully integrated into the decoder, introduces an expensive overhead for maintaining target-language boundary words for dynamic ∗ The authors would like to thank Dan Gildea, Jonathan Graehl, M</context>
<context position="7083" citStr="Och and Ney (2004)" startWordPosition="1148" endWordPosition="1151">s instantiated for a particular input string, it defines a set of derivations, called a forest, represented in a compact structure that has a structure of a graph in the phrase-based case, or more generally, a hypergraph in both cases. Accordingly we call items like (•••••) and (VP1,6) nodes in the forest, and instantiated deductions like (•••••) → ( •••) with Sharon, (VP1,6) → (VP3,6) (PP1,3) we call hyperedges that connect one or more antecedent nodes to a consequent node. 2.2 Adding a Language Model To integrate with a bigram language model, we can use the dynamic-programming algorithms of Och and Ney (2004) and Wu (1996) for phrase-based and SCFG-based systems, respectively, which we may think of as doing a finer-grained version of the deductions above. Each node v in the forest will be split into a set of augmented items, which we call +LM items. For phrase-based decoding, a +LM item has the form (v a) where a is the last word of the hypothesis. Thus a +LM version of Deduction (1) might be: ( ••• talk) : (w, “held a talk”) (••••• Sharon) : (w′, “held a talk with Sharon”) 145 1.0 4.0 7.0 1.0 4.0 7.0 1.0 4.0 7.0 1.0 4.0 7.0 8.5 8.3 2.5 2.5 2.4 1.0 1.1 3.5 2.4 9.2 9.2 2.5 2.4 9.2 17.0 9.5 8.3 15.2</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Joseph Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30:417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
<author>Yves Schabes</author>
<author>Fernando Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>J. Logic Programming,</journal>
<pages>24--3</pages>
<contexts>
<context position="5741" citStr="Shieber et al., 1995" startWordPosition="914" endWordPosition="917">ystem. For example, the following deduction step grows a hypothesis by the phrase-pair hyˇu Sh¯al´ong, with Sharoni: ( •••) : (w, “held a talk”) (•••••) : (w + c, “held a talk with Sharon”) (1) where a • in the coverage vector indicates the source word at this position is “covered” (for simplicity we omit here the ending position of the last phrase which is needed for distortion costs), and where w and w + c are the weights of the two hypotheses, respectively, with c being the cost of the phrase-pair. Similarly, the decoding problem with SCFGs can also be cast as a deductive (parsing) system (Shieber et al., 1995). Basically, we parse the input string using the source projection of the SCFG while building the corresponding subtranslations in parallel. A possible deduction of the above example is notated: (PP1,3) : (w1, t1) (VP3,6) : (w2, t2) (VP1,6) : (w1 + w2 + c′, t2t1) (2) where the subscripts denote indices in the input sentence just as in CKY parsing, w1, w2 are the scores of the two antecedent items, and t1 and t2 are the corresponding subtranslations. The resulting translation t2t1 is the inverted concatenation as specified by the target-side of the SCFG rule with the additional cost c′ being th</context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Stuart Shieber, Yves Schabes, and Fernando Pereira. 1995. Principles and implementation of deductive parsing. J. Logic Programming, 24:3–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
</authors>
<title>Natural Language Generation using an Information-Slim Representation.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Southern California.</institution>
<contexts>
<context position="20512" citStr="Soricut, 2006" startWordPosition="3658" endWordPosition="3659">puting true lower bounds of the combination costs is too slow and would compromise the speed up gained from cube growing. So we instead use a much simpler method that just calculates the minimum combination cost of each hyperedge in the top-i derivations of the root node in −LM decoding. This is just an approximation of the true lower bound, and bad estimates can lead to search errors. However, the hope is that by choosing the right value of i, these estimates will be accurate enough to affect the search quality only slightly, which is analogous to “almost admissible” heuristics in A* search (Soricut, 2006). 5 Experiments We test our methods on two large-scale English-toChinese translation systems: a phrase-based system and our tree-to-string system (Huang et al., 2006). Figure 6: A hyperedge bundle represents all +LM deductions that derives an item in the current bin from the same coverage vector (see Figure 5). The phrases on the top denote the target-sides of applicable phrase-pairs sharing the same source-side. 5.1 Phrase-based Decoding We implemented Cubit, a Python clone of the Pharaoh decoder (Koehn, 2004),3 and adapted cube pruning to it as follows. As in Pharaoh, each bin i contains hyp</context>
</contexts>
<marker>Soricut, 2006</marker>
<rawString>Radu Soricut. 2006. Natural Language Generation using an Information-Slim Representation. Ph.D. thesis, University of Southern California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>Joint parsing and semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proc. CoNLL</booktitle>
<marker>Sutton, McCallum, 2005</marker>
<rawString>Charles Sutton and Andrew McCallum. 2005. Joint parsing and semantic role labeling. In Proc. CoNLL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>A polynomial-time algorithm for statistical machine translation.</title>
<date>1996</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="1861" citStr="Wu, 1996" startWordPosition="272" endWordPosition="273">example, a phrase- or word-based model with full reordering has exponential complexity (Knight, 1999). The language model also, if fully integrated into the decoder, introduces an expensive overhead for maintaining target-language boundary words for dynamic ∗ The authors would like to thank Dan Gildea, Jonathan Graehl, Mark Johnson, Kevin Knight, Daniel Marcu, Bob Moore and Hao Zhang. L. H. was partially supported by NSF ITR grants IIS-0428020 while visiting USC/ISI and EIA0205456 at UPenn. D. C. was partially supported under the GALE/DARPA program, contract HR0011-06-C-0022. 144 programming (Wu, 1996; Och and Ney, 2004). In practice, one must prune the search space aggressively to reduce it to a reasonable size. A much simpler alternative method to incorporate the LM is rescoring: we first decode without the LM (henceforth −LM decoding) to produce a k-best list of candidate translations, and then rerank the k-best list using the LM. This method runs much faster in practice but often produces a considerable number of search errors since the true best translation (taking LM into account) is often outside of the k-best list. Cube pruning (Chiang, 2007) is a compromise between rescoring and f</context>
<context position="7097" citStr="Wu (1996)" startWordPosition="1153" endWordPosition="1154">rticular input string, it defines a set of derivations, called a forest, represented in a compact structure that has a structure of a graph in the phrase-based case, or more generally, a hypergraph in both cases. Accordingly we call items like (•••••) and (VP1,6) nodes in the forest, and instantiated deductions like (•••••) → ( •••) with Sharon, (VP1,6) → (VP3,6) (PP1,3) we call hyperedges that connect one or more antecedent nodes to a consequent node. 2.2 Adding a Language Model To integrate with a bigram language model, we can use the dynamic-programming algorithms of Och and Ney (2004) and Wu (1996) for phrase-based and SCFG-based systems, respectively, which we may think of as doing a finer-grained version of the deductions above. Each node v in the forest will be split into a set of augmented items, which we call +LM items. For phrase-based decoding, a +LM item has the form (v a) where a is the last word of the hypothesis. Thus a +LM version of Deduction (1) might be: ( ••• talk) : (w, “held a talk”) (••••• Sharon) : (w′, “held a talk with Sharon”) 145 1.0 4.0 7.0 1.0 4.0 7.0 1.0 4.0 7.0 1.0 4.0 7.0 8.5 8.3 2.5 2.5 2.4 1.0 1.1 3.5 2.4 9.2 9.2 2.5 2.4 9.2 17.0 9.5 8.3 15.2 8.5 8.4 9.5 8</context>
</contexts>
<marker>Wu, 1996</marker>
<rawString>Dekai Wu. 1996. A polynomial-time algorithm for statistical machine translation. In Proc. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>