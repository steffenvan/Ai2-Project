<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<sectionHeader confidence="0.833901333333333" genericHeader="abstract">
GENERALIZED CHART ALGORITHM:
AN EFFICIENT PROCEDURE FOR
COST-BASED ABDUCTION
</sectionHeader>
<author confidence="0.514506">
Yasuharu Den
</author>
<affiliation confidence="0.491746">
ATR Interpreting Telecommunications Research Laboratories
</affiliation>
<address confidence="0.95688">
2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-02, JAPAN
</address>
<email confidence="0.838264">
Tel: +81-7749-5-1328, Fax: +81-7749-5-1308, e-mail: den@itl.atr.co.jp
</email>
<sectionHeader confidence="0.994662" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999894666666667">
We present an efficient procedure for cost-based ab-
duction, which is based on the idea of using chart
parsers as proof procedures. We discuss in de-
tail three features of our algorithm — goal-driven
bottom-up derivation, tabulation of the partial re-
sults, and agenda control mechanism — and report
the results of the preliminary experiments, which
show how these features improve the computational
efficiency of cost-based abduction.
</bodyText>
<sectionHeader confidence="0.975488" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.998328909090909">
Spoken language understanding is one of the most
challenging research areas in natural language pro-
cessing. Since spoken language is incomplete in var-
ious ways, i.e., containing speech errors, ellipsis,
metonymy, etc., spoken language understanding
systems should have the ability to process incom-
plete inputs by hypothesizing the underlying infor-
mation. The abduction-based approach (Hobbs et
al., 1988) has provided a simple and elegant way to
realize such a task.
Consider the following Japanese sentence:
</bodyText>
<listItem confidence="0.547225">
(1) SOseki kat- ta
</listItem>
<bodyText confidence="0.983792125">
(a famous writer) buy PAST
This sentence contains two typical phenomena aris-
ing in spoken language, i.e., metonymy and the el-
lipsis of a particle. When this sentence is uttered
under the situation where the speaker reports his
experience, its natural interpretation is the speaker
bought a Soseki novel. To derive this interpreta-
tion, we need to resolve the following problems:
</bodyText>
<listItem confidence="0.9235864">
• The metonymy implied by the noun phrase
56seki is expanded to a Soseki novel, based on
the pragmatic knowledge that the name of a
writer is sometimes used to refer to his novel.
• The particle-less thematic relation between the
verb katta and the noun phrase Soseki is deter-
mined to be the object case relation, based on the
semantic knowledge that the object case relation
between a trading action and a commodity can
be linguistically expressed as a thematic relation.
</listItem>
<bodyText confidence="0.997930666666667">
This interpretation is made by abduction. For
instance, the above semantic knowledge is stated,
in terms of the predicate logic, as follows:
</bodyText>
<listItem confidence="0.455633">
(2) sem(e,x) C trade(e) A commodity(x) A obj(e,x)
</listItem>
<bodyText confidence="0.999544130434783">
Then, the inference process derives the consequent
sem(e,x) by hypothesizing an antecedent obj(e,x),
which is never proved from the observed facts. This
process is called abduction.
Of course, there may be several other possibili-
ties that support the thematic relation sem(e,x).
For instance, the thematic relation being deter-
mined to be the agent case relation, sentence (1)
can have another interpretation, i.e., Soseki bought
something, which, under some other situations,
might be more feasible than the first interpretation.
To cope with feasibility, the abduction-based model
usually manages the mechanism for evaluating the
goodness of the interpretation. This is known as
cost-based abduction (Hobbs et al., 1988).
In cost-based abduction, each assumption
bears a certain cost. For instance, the assump-
tion obj(e,x), introduced by applying rule (2), is
specified to have a cost of, say, $2. The goodness of
the interpretation is evaluated by accumulating the
costs of all the assumptions involved. The whole
process of interpreting an utterance is depicted in
the following schema:
</bodyText>
<listItem confidence="0.993489">
1. Find all possible interpretations, and
2. Select the one that has the lowest cost.
</listItem>
<bodyText confidence="0.999976230769231">
In our example, the interpretation that as-
sumes the thematic relation to be the object case
relation, with the metonymy being expanded to
a Soseki novel, is cheaper than the interpretation
that assumes the thematic relation to be the agent
case relation; hence, the former is selected.
An apparent problem here is the high compu-
tational cost; because abduction allows many pos-
sibilities, the schema involves very heavy compu-
tation. Particularly in the spoken language under-
standing task, we need to consider a great number
of possibilities when hypothesizing various underly-
ing information. This makes the abduction process
</bodyText>
<page confidence="0.997405">
218
</page>
<bodyText confidence="0.999417633333333">
computationally demanding, and reduces the prac-
ticality of abduction-based systems. The existing
models do not provide any basic solution to this
problem. Charniak (Charniak and Husain, 1991;
Charniak and Santos Jr., 1992) dealt with the prob-
lem, but those solutions are applicable only to the
propositional case, where the search space is rep-
resented as a directed graph over ground formulas.
In other words, they did not provide a way to build
such graphs from rules, which, in general, contain
variables and can be recursive.
This paper provides a basic and practical so-
lution to the computation problem of cost-based
abduction. The basic idea comes from the natural
language parsing literature. As Pereira and War-
ren (1983) pointed out, there is a strong connec-
tion between parsing and deduction. They showed
that parsing of DCG can be seen as a special case
of deduction of Horn clauses; conversely, deduction
can be seen as a generalization of parsing. Their
idea of using chart parsers as deductive-proof pro-
cedures can easily be extended to the idea of using
chart parsers as abductive-proof procedures. Be-
cause chart parsers have many advantages from the
viewpoint of computational efficiency, chart-based
abductive-proof procedures are expected to nicely
solve the computation problem. Our algorithm,
proposed in this paper, has the following features,
which considerably enhance the computational ef-
ficiency of cost-based abduction:
</bodyText>
<listItem confidence="0.9921792">
1. Goal-driven bottom-up derivation, which reduces
the search space.
2. Tabulation of the partial results, which avoids the
recomputation of the same goal.
3. Agenda control mechanism, which realizes var-
</listItem>
<bodyText confidence="0.937487636363636">
ious search strategies to find the best solution
efficiently.
The rest of the paper is organized as follows.
First, we explain the basic idea of our algorithm,
and then present the details of the algorithm along
with simple examples. Next, we report the results
of the preliminary experiments, which clearly show
how the above features of our algorithm improve
the computational efficiency. Then, we compare
our algorithm with Pereira and Warren&apos;s algorithm,
and finally conclude the paper.
</bodyText>
<subsectionHeader confidence="0.542001">
Head-driven Derivation
</subsectionHeader>
<bodyText confidence="0.99955075">
Pereira and Warren showed that chart parsers
can be used as proof procedures; they presented the
Earley deduction proof procedure, that is a gener-
alization of top-down chart parsers. However, they
mentioned only top-down chart parsers, which is
not always very efficient compared to bottom-up
(left-corner) chart parsers. It seems that using left-
corner parsers as proof procedures is not so easy,
</bodyText>
<figureCaption confidence="0.999749">
Figure 1: Concept of Head-driven Derivation
</figureCaption>
<bodyText confidence="0.986651833333333">
unless the rules given to the provers have a certain
property. Here, we describe under what conditions
left-corner parsers can be used as proof procedures.
Let us begin with the general problems of Horn
clause deduction with naive top-down and bottom-
up derivations:
</bodyText>
<listItem confidence="0.986794833333333">
• Deduction with top-down derivation is affected
by the frequent backtracking necessitated by the
inadequate selection of rules to be applied.
• Deduction with bottom-up derivation is affected
by the extensive vacuous computation, which
never contributes to the proof of the initial goal.
</listItem>
<bodyText confidence="0.998705966666667">
These are similar to the problems that typi-
cally arise in natural language parsing with naive
top-down and bottom-up parsers. In natural lan-
guage parsing, these problems are resolved by intro-
ducing a more sophisticated derivation mechanism,
i.e., left-corner parsing. We have attempted to ap-
ply such a sophisticated mechanism to deduction.
Suppose that the proof of a goal g(x,y) can
be represented in the manner in Figure 1; the first
argument x of the goal g(x,y) is shared by all the
formulas along the path from the goal g(x,y) to
the left corner arn(x,z,,). In such a case, we can
think of a derivation process that is similar to left-
corner parsing. We call this derivation head-driven
derivation, which is depicted as follows:
Step 1 Find a fact a(w,z) whose first argument
w unifies with the first argument x of the goal
g(x,y), and place it on the left corner.
Step 2 Find a rule am_1(w,zm— ) C a(w,zm ) A
B1 A . . . A Bn whose leftmost antecedent
a(w,zm) unifies with the left-corner key a(x,z),
and introduce the new goals B1, ..., and B. If
all these goals are recursively derived, then cre-
ate the consequent am_1(x,z„), which domi-
nates a(x,zm), B1, ..., and 13,, and place it on
the left corner instead of a(x,z).
Step 3 If the consequent am_i(x,zm_i) unifies
with the goal g(x,y), then finish the pro-
cess. Otherwise, go back to step 2 with
am_i(x,zm_,) being the new left-corner key.
</bodyText>
<page confidence="0.998255">
219
</page>
<bodyText confidence="0.999915484848485">
Left-corner parsing of DCG is just a special
case of head-driven derivation, in which the in-
put string is shared along the left border, i.e., the
path from a nonterminal to the leftmost word in
the string that is dominated by that nonterminal.
Also, semantic-head-driven generation (Shieber el
al., 1989; van Noord, 1990) and head-corner pars-
ing (van Noord, 1991; Sikkel and op den Akker,
1993) can be seen as head-driven derivation, when
the semantic-head/syntactic-head is moved to the
leftmost position in the body of each rule and the
argument representing the semantic-feature/head-
feature is moved to the first position in the argu-
ment list of each formula.
To apply the above procedures, all rules must
be in chain form arr,_1(w,zm--,.)Cam(w,zm) AB].
A . A kin; that is, in every rule, the first argu-
ment of the leftmost antecedent must be equal to
the first argument of the consequent. This is the
condition under which left-corner parsers can be
used as proof procedures. Because this condition is
overly restrictive, we extend the procedures so that
they allow non-chain rules, i.e., rules not in chain
form. Step 1 is replaced by the following:
Step 1 Find a non-chain rule a(w,z) C B1 A . A
Br, such that the first argument w of the con-
sequent a(w,z) unifies with the first argument
x of the goal g(x,y), and introduce the new
goals B1, ..., and B. A fact is regarded as
a non-chain rule with an empty antecedent. If
all these goals are recursively derived, then cre-
ate the consequent a(x,z), which dominates Bi,
..., and B„, and place it on the left corner.
</bodyText>
<subsectionHeader confidence="0.905526">
Generalized Chart Algorithm
</subsectionHeader>
<bodyText confidence="0.9999806">
The idea given in the previous section realizes the
goal-driven bottom-up derivation, which is the first
feature of our algorithm. Then, we present a more
refined algorithm based upon the idea, which real-
izes the other two features as well as the first one.
</bodyText>
<subsectionHeader confidence="0.985482">
Chart Parsing and its Generalization
</subsectionHeader>
<bodyText confidence="0.998468461538461">
Like left-corner parsing, which has the drawback of
repeatedly recomputing partial results, head-driven
derivation will face the same problem when it is
executed in a depth-first manner with backtrack-
ing. In the case of left-corner parsing, the prob-
lem is resolved by using the tabulation method,
known as chart parsing (Kay, 1980). A recent
study by Haruno et al. (1993) has shown that
the same method is applicable to semantic-head-
driven generation. The method is also applicable
to head-driven derivation, which is more general
than semantic-head-driven generation.
To generalize charts to use in proof procedures,
</bodyText>
<figure confidence="0.619235666666667">
m(&lt;[A],[Bk JA,131)
..............
ng&lt;11.01.&gt;:q1)
(Some labels m(&lt;IAMI4IA1)
are omitted)
m(&lt;[AMBJ&gt;JB,A.1)
</figure>
<figureCaption confidence="0.999453">
Figure 2: Example of Generalized Charts
</figureCaption>
<bodyText confidence="0.999883166666667">
we first define the chart lexicons. In chart pars-
ing, lexicons are the words in the input string,
each of which is used as the index for a subset
of the edges in the chart; each edge incident from
(the start-point of) lexicon w represents the sub-
structure dominating the sub-string starting from
w. In our case, from the similarity between left-
corner parsing and head-driven derivation, lexicons
are the terms that occur in the first argument po-
sition of any formula; each edge incident from (the
start-point of) lexicon x represents the substruc-
ture dominating the successive sequence of the de-
rived formulas starting from the fact in which x
occupies the first argument position. For example,
in the chart representing the proof in Figure 1, all
the edges corresponding to the formulas on the left
border, i.e. am(x,z,,),arn_1(x,z„,_,), , ai(z,z,)
and g(x,y), are incident from (the start-point of)
lexicon x, and, hence, x is the index for these edges.
Following this definition of the chart lexicons,
there are two major differences between chart
parsing and proof procedures, which Haruno also
showed to be the differences between chart parsing
and semantic-head-driven generation.
</bodyText>
<listItem confidence="0.864232875">
1. In contrast to chart parsing, where lexicons are
determined immediately upon input, in proof
procedures lexicons should be incrementally in-
troduced.
2. In contrast to chart parsing, where lexicons are
connected one by one in a linear sequence, in
proof procedures lexicons should be connected in
many-to-many fashion.
</listItem>
<bodyText confidence="0.981121">
In proof procedures, the chart lexicons are not
determined at the beginning of the proof (because
</bodyText>
<equation confidence="0.9676625">
&lt;111,1BI&gt;•• m(&lt;11.:.(i..ii)
&lt; I &gt;1)
h(‹[A],[B]&gt;,13:...1] I&gt;)
h(&lt;1A0 LH&gt;)
</equation>
<page confidence="0.970224">
220
</page>
<bodyText confidence="0.997726727272727">
we don&apos;t know which formulas are actually used in
the proof), rather they are dynamically extracted
from the subgoals as the process goes. In addi-
tion, if the rules are nondeterministic, it sometimes
happens that there are introduced, from one left-
corner key, a(x,z), two or more distinct succes-
sive subgoals, b1(0 b2(w2,ya
,y1), ) etc., that have
different first arguments, w1, w2, etc. In such a
case, one lexicon x should be connected to two or
more distinct lexicons, w1, w2, etc. Furthermore,
it can happen that two or more distinct left-corner
(xi ,zi), a2(x2,z2
keys, al ) etc., incidentally intro-
duce the successive subgoals, 1)1(w,yi), b2(w,y2),
etc., with the same first argument w. In such a
case, two or more distinct lexicons, x1 , x2, etc.,
should be connected to one lexicon w. Therefore,
the connections among lexicons should be many-
to-many. Figure 2 shows an example of charts with
many-to-many connections, where the connections
are represented by pointers A, B, etc.
</bodyText>
<subsectionHeader confidence="0.711713">
The Algorithm
</subsectionHeader>
<bodyText confidence="0.999895133333333">
We, so far, have considered deduction but not ab-
duction. Here, we extend our idea to apply to ab-
duction, and present the definition of the algorithm.
The extension for abduction is very simple.
First, we add a new procedure, which introduces
an assumption G for a given goal G. An assump-
tion is treated as if it were a fact. This means that
an assumption, as well as a fact, is represented as a
passive edge in terms of the chart algorithm. Sec-
ond, we associate a set S of assumptions with each
edge e in the chart; S consists of all the assump-
tions that are contained in the completed part of
the (partial) proof represented by the edge e. More
formally, the assumption set S associated with an
edge e is determined as follows:
</bodyText>
<listItem confidence="0.907718833333333">
1. If e is a passive edge representing an assumption
A, then S = {A}.
2. If e is a passive/active edge introduced from a
non-chain rule, including fact, then S is empty.
3. If e is a passive/active edge predicted from a
chain rule with a passive edge e&apos; being the left-
corner key, then S is equal to the assumption set
S&apos; of e&apos;.
4. If e is a passive/active edge created by combining
an active edge el and a passive edge e2, then
= S1uS2 where Si and 82 are the assumption
sets of el and e2, respectively.
</listItem>
<bodyText confidence="0.996975">
Taking these into account, the definition of our
algorithm is as follows. f is a function that assigns
a unique vertex to each chart lexicon. The notation
A:S stands for the label of an edge e, where A is
the label of e in an ordinary sense and S is the
assumption set associated with e.
Initialization Add an active edge [[1G] F:0 to
the chart, looping at vertex 0, where G is the
initial goal.
Apply the following procedures repeatedly until
no procedures are applicable.
Introduction Let e be an active edge labeled
[- }A:S incident from vertex s to t,
where Bi = bi(xi,yi) is the first open box in e.
</bodyText>
<listItem confidence="0.884125769230769">
1. If the lexicon xi is never introduced in the
chart, then introduce it and run a pointer
from t to f(xj). Then, do the following:
(a) For every non-chain rule a(w,z) C Bi A
... A Bn, including fact, such that w uni-
fies with xi, create an active edge la-
beled [mB1. • .mBnia(xi,z):0 between ver-
tex f(xj) and f(xj) + 1. (Create, instead,
a passive edge labeled a(xi,z):0 when the
rule is a fact, i.e. n = 0.)
(b) Create a passive edge labeled Bi:{Bi} be-
tween vertex f(xj) and f(xj) + 1.
2. If the lexicon xi was previously introduced in
</listItem>
<bodyText confidence="0.973059636363636">
the chart, then run a pointer from t to
In addition, if the passive edge Bj:{Bj} never
exists in the chart, create it between vertex
f(r) and f(xj)+ I.
Prediction Let e be a passive edge labeled C:S
incident from vertex s to t. For every chain
rule A&apos; C A A Bi A ... A Br, such that A
unifies with C, create an active edge labeled
[AmBi. • -FjB,JA&apos;:S between vertex s and t.
(Create, instead, a passive edge labeled A&apos;S
when A is the single antecedent, i.e., n = 0.)
Combination Let el be an active edge labeled
[• • •11BirlBj+i • •FlEirjA:Si incident from ver-
tex s tot, where Bi is the first open box in el,
and let e2 be a passive edge labeled C:S2 inci-
dent from vertex u to v. If Bi and C unify and
there is a pointer from t to u, then create an ac-
tive edge labeled [- • ..Bi[iBi+i • .NB„]A:Si US2
between vertex s and v. (Create, instead, a pas-
sive edge labeled A:Si U S2 when Bi is the last
element, i.e., j = n.)
Each passive edge T:S represents an answer.
</bodyText>
<subsectionHeader confidence="0.429697">
Examples
</subsectionHeader>
<bodyText confidence="0.999055666666667">
Here, we present a simple example of the appli-
cation of our algorithm to spoken language un-
derstanding. Figure 3 provides the rules for spo-
ken Japanese understanding, with which the sen-
tence (1) is parsed and interpreted. They include
the pragmatic, semantic and knowledge rules as
well as the syntactic and lexical rules.
The syntactic rules allow the connection be-
tween a verb and a noun phrase with or with-
</bodyText>
<page confidence="0.985134">
221
</page>
<figure confidence="0.985147884615384">
Syntactic Rules
s(i ,k ,e)Cvp(i,k ,e)
vp(i,k,e)Cnp(i,j,c,x) A vp(j,k,e) A depend((c,e,x)d)
vp(i,k,e)Cnp(i,i,x) A vp(j,k,e) A depend((c,e,x)a)
np(i,k,c,x)Cnp(i,j,x) A p(j,k,c)
depend((c,e,$)d)CPrag((x)p,y) A sein((c,e,Y)s)
Lexical Rules
np( [Sosekilk],k,x)Csoseki(x)S1
vp([kaitalk],k,e)c buy(e)81
p([galk],k,c)Cga(c)$1
p({wojk],k,c)cwo(c)&amp;quot;
Pragmatic Rules
prag((x)p,x)
prag((x)p,y)Cwriter(x) A write((x,y)p)$1° A novel(y)$1
Semantic Rules
sem(s)cga(s,c) A ga(c)&amp;quot;
senz(s)Cwo(s,c) A wo(c)&amp;quot;
ga((c,e,x),,c)cintend(e) A person(s) A agt((e,x),)$&apos;
wo((c,e,x),,c)ctrade(e) A commodity(x) A obj((e,x),)&amp;quot;
Knowledge Rules
person(x)Csoseki(x)
writer(x)Csoseki(x)
book(x)c novel(x)
commodity(x)Cbook(x)
trade(e)Cbuy(e)
intend(e)Ctrade(e)
</figure>
<figureCaption confidence="0.99999">
Figure 3: Example of Rules
</figureCaption>
<bodyText confidence="0.992132454545454">
action and a third person as the agent case rela-
tion is high, say $20. This assignment of costs is
suitable for a situation in which the speaker re-
ports his experience. In spite of the difficulty of
assigning suitable costs in general, the cost-based
interpretation is valuable, because it provides a uni-
form criteria for syntax, semantics and pragmat-
ics. Hopefully, several techniques, independently
developed in these areas, e.g., stochastic parsing,
example-based/corpus-based techniques for word
sense/structural disambiguation, etc., will be us-
able for better cost assignment. Probability will
also be a key technique for the cost assignment
(Charniak and Shimony, 1990).
Figure 4 and Table 1 show the chart that is
created when a sentence (1) is parsed and inter-
preted using our algorithm. Although the diagram
seems complicated, it is easy to understand if we
break down the diagram. Included are the syntac-
tic parsing of the sentence (indicated by edges 2, 6,
7, 14, 52 and 53), the pragmatic interpretation of
the metonymy by Soseki S (indicated by edges 17,
18, 20 and 24), the semantic interpretation of the
thematic relation between a buying event B and a
novel N written by Sosekz (indicated by edges 42,
44, 45, 47, 48 and 50), and so on. In the pragmatic
interpretation, assumption novel(N) (edge 21) is
introduced, which is reused in the semantic inter-
pretation. In other words, a single assumption is
used more than once. Such a tricky job is naturally
realized by the nature of the chart algorithm.
out a particle, which permit structures like
[vp[NpSosekiNvpkatiall. Such a structure is evalu-
ated by the pragmatic and semantic criteria. That
is, the dependency between a verbal concept e and a
nominal concept x is supported if there is an entity
y such that x and y have a pragmatic relation, i.e.,
a metonymy relation, and e and y have a semantic
relation, i.e., a thematic relation. The metonymy
relation is defined by the pragmatic rules, based on
certain knowledge, such as that the name of a writer
is sometimes used to refer to his novel. Also, the
thematic relation is defined by the semantic rules,
based on certain knowledge, such as that the object
case relation between a trading action and a com-
modity can be linguistically expressed as a thematic
relation.
The subscript $c of a formula A represents
the cost of assuming formula A. A is easy to as-
sume when c is small, while A is difficult to as-
sume when c is large. For instance, the cost of
interpreting the thematic relation between a trad-
ing action and a commodity as the object case re-
lation is low, say $2, while the cost of interpret-
ing the thematic relation between an intentional
</bodyText>
<subsectionHeader confidence="0.836836">
Agenda Control
</subsectionHeader>
<bodyText confidence="0.992938739130435">
Since the aim of cost-based abduction is to find
out the best solution, not all solutions, it is reason-
able to consider combining heuristic search strate-
gies with our algorithm to find the best solution
efficiently. Our algorithm facilitates such an exten-
sion by using the agenda control mechanism, which
is broadly used in advanced chart parsing systems.
The agenda is a storage for the edges created by
any of the three procedures of the chart algorithm,
out of which edges to be added to the chart are
selected, one by one, by a certain criterion. The
simplest strategy is to select the edge which has
the minimal cost at that time, i.e., ordered search.
Although ordered search guarantees that the
first solution is the best one, it is not always very ef-
ficient. We can think of other search strategies, like
best first search, beam search, etc., which are more
practical than ordered search. To date, we have not
investigated any of these practical search strategies.
However, it is apparent that our chart algorithm,
together with the agenda control mechanism, will
provide a good way to realize these practical search
strategies.
</bodyText>
<page confidence="0.960803">
222
</page>
<figureCaption confidence="0.999556">
Figure 4: Chart Diagram for Sosekt katta
</figureCaption>
<figure confidence="0.96673275">
[Soseki,kattal
passive edge
active edge
25
</figure>
<subsectionHeader confidence="0.440934">
Preliminary Experiments
</subsectionHeader>
<bodyText confidence="0.999840375">
We conducted preliminary experiments to compare
four methods of cost-based abduction: top-down al-
gorithm (TD), head-driven algorithm (HD), gener-
alized chart algorithm with full-search (GCF), and
generalized chart algorithm with ordered search
(GCO). The rules used for the experiments are in
the spoken language understanding task, and they
are rather small (51 chain rules + 35 non-chain
rules). The test sentences include one verb and 1-4
noun phrases, e.g., sentence (1).
Table 2 shows the results. The performance of
each method is measured by the number of compu-
tation steps, i.e., the number of derivation steps
in TD and HD, and the number of passive and
active edges in GCF and GCO. The decimals in
parentheses show the ratio of the performance of
each method to the performance of TD. The table
clearly shows how the three features of our algo-
rithm improve the computational efficiency. The
improvement from TD to HD is due to the first fea-
ture, i.e., goal-driven bottom-up derivation, which
eliminates about 50% of the computation steps; the
improvement from HD to GCF is due to the sec-
ond feature, i.e., tabulation of the partial results,
</bodyText>
<tableCaption confidence="0.991639">
Table 2: Comp. among TD, HD, GCF, and GCO
</tableCaption>
<table confidence="0.9276664">
Ns TD HD GCF GCO
1 215 112 (0.52) 83 (0.39) 75 (0.35)
2 432 218 (0.50) 148 0.34) 113 (0.26)
3 654 330 (0.50) 193 0.30) 160 (0.24)
4 876 442 (0.50) 238 0.27) 203 (0.23)
</table>
<bodyText confidence="0.996771833333333">
which decreases the number of steps another 13%-
23%; the improvement from GCF to GCO is due to
the last feature, i.e., the agenda control mechanism,
which decreases the number of steps another 4%-
8%. In short, the efficiency is improved, maximally,
about four times.
</bodyText>
<subsectionHeader confidence="0.904665">
Comparison with Earley Deduction
</subsectionHeader>
<bodyText confidence="0.999861285714286">
We describe, here, some differences between our al-
gorithm and Earley deduction presented by Pereira
and Warren. First, as we mentioned before, our al-
gorithm is mainly based on bottom-up (left-corner)
derivation rather than top-down derivation, that
Earley deduction is based on. Our experiments
showed the superiority of this approach in our par-
</bodyText>
<page confidence="0.996928">
223
</page>
<bodyText confidence="0.999766128205128">
ticular, though not farfetched, example.
Second, our algorithm does not use sub-
surnption-checking of edges, which causes a serious
computation problem in Earley deduction. Our al-
gorithm needs subsumption-checking only when a
new edge is introduced by the combination proce-
dure. In the parsing of augmented grammars, even
when two edges have the same nonterminal symbol,
they are different in the annotated structures asso-
ciated with those edges, e.g., feature structures; in
such a case, we cannot use one edge in place of
another. Likewise, in our algorithm, edges are al-
ways annotated by the assumption sets, which, in
most cases, prevent those edges from being reused.
Therefore, in this case, subsumption-checking is not
effective. In our algorithm, reuse of edges only be-
comes possible when a new edge is introduced by
the introduction procedure. However, this is done
only by adding a pointer to the edge to be reused,
and, to invoke this operation, equality-checking of
lexicons, not edges, is sufficient.
Finally, our algorithm has a stronger connec-
tion with chart parsing than Earley deduction does.
Pereira and Warren noted that the indexing of for-
mulas is just an implementation technique to in-
crease efficiency. However, indexing plays a con-
siderable role in chart parsing, and how to index
formulas in the case of proof procedures is not so
obvious. In our algorithm, from the consideration
of head-driven derivation, the index of a formula
is determined to be the first argument of that for-
mula. All formulas with the same index are derived
the first time that index is introduced in the chart.
Pointers among lexicons are also helpful in avoiding
nonproductive attempts at applying the combina-
tion procedure. All the devices that were originally
used in chart parsers in a restricted way are in-
cluded in the formalism, not in the implementation,
of our algorithm.
</bodyText>
<subsectionHeader confidence="0.988637">
Concluding Remarks
</subsectionHeader>
<bodyText confidence="0.9999449375">
In this paper, we provided a basic and practi-
cal solution to the computation problem of cost-
based abduction. We explained the basic concept
of our algorithm and presented the details of the
algorithm along with simple examples. We also
showed how our algorithm improves computational
efficiency on the basis of the results of the prelimi-
nary experiments.
We are now developing an abduction-based
spoken language understanding system using our
algorithm. The main problem is how to find a good
search strategy that can be implemented with the
agenda control mechanism. We are investigating
this issue using both theoretical and empirical ap-
proaches. We hope to report good results along
these lines in the future.
</bodyText>
<sectionHeader confidence="0.974375" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99981625">
The author would like to thank Prof. Yuji Matsu-
moto of Nara Institute of Science and Technology
and Masahiko Haruno of NTT Communication Sci-
ence Laboratories for their helpful discussions.
</bodyText>
<sectionHeader confidence="0.723931" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.987493829787235">
[Charniak and Husain, 1991] Eugene Charniak
and Saadia Husain. A new admissible heuristic
for minimal-cost proofs. Proceedings of the 12th
IJCAI, pages 446-451, 1991.
[Charniak and Santos Jr., 1992] Eugene Charniak
and Eugene Santos Jr. Dynamic MAP calcu-
lations for abduction. Proceedings of the 10th
AAAI, pages 552-557, 1992.
[Charniak and Shimony, 1990] Eugene Charniak
and Solomon E. Shimony. Probabilistic seman-
tics for cost based abduction. Proceedings of the
8th AAAI, pages 106-111, 1990.
[Haruno et al., 1993] Masahiko Haruno, Yasuharu
Den, Yuji Matsumoto, and Makoto Nagao. Bidi-
rectional chart generation of natural language
texts. Proceedings of the 11th AAAI, pages 350-
356, 1993.
[Hobbs et al., 1988] Jerry R. Hobbs, Mark Stickel,
Paul Martin, and Douglas Edwards. Interpreta-
tion as abduction. Proceedings of the 26th An-
nual Meeting of ACL, pages 95-103, 1988.
[Kay, 1980] Martin Kay. Algorithm schemata and
data structures in syntactic processing. Technical
Report CSL-80-12, XEROX Palo Alto Research
Center, 1980.
[Pereira and Warren, 1983] Fernando C.N. Pereira
and David H.D. Warren. Parsing as deduction.
Proceedings of the 21st Annual Meeting of ACL,
pages 137-144, 1983.
[Shieber et al., 1989] Stuart M. Shieber, Gertjan
van Noord, Robert C. Moore, and Fernando C.N.
Pereira. A semantic-head-driven generation al-
gorithm for unification-based formalisms. Pro-
ceedings of the 27th Annual Meeting of ACL,
pages 7-17, 1989.
[Sikkel and op den Akker, 1993] Klaas Sikkel and
Rieks op den Akker. Predictive head-corner chart
parsing. The 3rd International Workshop on
Parsing Technologies, pages 267-276, 1993.
[van Noord, 1990] Gertjan van Noord. An over-
view of head-driven bottom-up generation. Cur-
rent Research in Natural Language Generation,
chapter 6, pages 141-165. Academic Press, 1990.
[van Noord, 1991] Gertjan van Noord. Head cor-
ner parsing for discontinuous constituency. Pro-
ceedings of the 29th Annual Meeting of ACL,
pages 114-121, 1991.
</bodyText>
<page confidence="0.997802">
224
</page>
<tableCaption confidence="0.965924">
Table 1: Table Representation of the Chart
</tableCaption>
<table confidence="0.561069333333333">
# Arc A-Set From
1 [Ms (4),[],e)1T 0 —
2 alsoseki(S)&amp;quot;] 0 1
</table>
<figure confidence="0.986071489583334">
np(41,111,,S)
3 soseki(S)$1 {a} 2
4 person(S) {a} 3
5 writer(S) {a} 3
6 np(4.,T,S) {a} 2+B+3
7 [np(.1),W ,S) {a} 6
Mvp(T ,k ,e)
Mdepend((c,e,S)d)]
vp (f, ,k ,e)
8 [np(4.,T,S) {a} 6
[119(1 i ,k ,c)]
np(4 &gt; ,k ,c,S)
9 [MbuyEC)sl] 0 7
vp(W„B)
10 buy(B) 1 Ifil 9
11 trade(B) (/3) 10
12 intend(B) PI 11
13 vp(T,D,B) {0} 9+D+10
14 [np(4),W,S) fa, fil 7+C+13
vp(xlf,D,B)
rldepend((P,B,S)d)]
vp(4 ),III,B
15 [Mprag((S),,x) 0 14
[?}sem( P,B,x),)]
depend (P,B ,S)d)
16 prag((S p ,S) 0 15
17 [Mwriter(S) 0 15
Mwrite((S,x)p)*1°
[1novelx)$1]
prag((S)&apos; ,x)
18 [writer(S) {a} 17+G+5
Mwrite((S,N)p)&amp;quot;°
Mnovel(N)sl]
prag((S),,N)
19 write((S,N)p)&apos;&amp;quot; {7} 18
20 [writer(S) (a, 7 } 18+H+19
write((S ,N)p)&apos;&amp;quot;
[?]novel(N)1]
prag((S),,,N)
21 novel(N)11 {6} 20
22 book(N) (6) 21
23 commodity(N) 01 22
24 prag((S)p,N) {a, y, 6) 20+1+21
25 [prag((S)p,S) 0 15+F+16
[lsem((P,B ,S),)]
depend((P ,B ,S)d)
26 {[?]intend(B) 0 25
[?]person(S)
[lagt((B ,S),)$2°]
ga((P,B,S),,P)
27 [Mtrade(B) 0 25
Mcomrnodity(S)
[lobj((B ,S),)$2]
wo((P ,B ,S),,P)
. ,
Arc
A-Set
From
[intend(B)
[?]person(S)
Magt((B,S),)$20]
ga((P,B,S),,P)
[trade(B)
Mcommodity(S)
Mobj((B ,S),)$2]
woUP
[intend(B)
person(S)
[7]agt((B,S),)$20]
ga((P,B,S),,P)
agt((B,S)3)82°
28
{fl}
26+K+12
29
(fl)
27+K+11
30
{a, g}
28+L+4
31
{e}
30
32
ga((P,B,S),,P)
, e}
30-f M+31
[ga((P ,B ,S),,P)
riga(P)s3]
sem(V,B,S),)
ga(P)s3
33
{a, #, €}
32
34
{C}
33
sem ((P ,B ,S),)
35
{a, /3, e,
33+N+34
36
depend((P ,B,S)d)
vp(4 ),D,B)
{a , fl , e, CI
25+,1+35
37
{a, 0, e, C}
14+E+36
38
s(c1),1],B)
{a, )3, E, C}
37
39
{a, )8, f, C}
{a, 7,6}
1+A+38
[prag((S) ,N)
risem((P,B ,N),)]
depend((P ,B ,S)d)
alintend(13)
[?]person (N)
Magt((B,N)3)$20]
ga((P,B,N),,P))
[[?]trade(B)
[lcommodity(N)
Mobj((B ,N),)$2]
wo((P ,B,N),,P)
[intend(B)
[lperson(N)
Magt((B,N)3)$20]
_ga((P ,B ,N) ,,P)
[trade(B)
[lcomm odity(N)
rjobj ((B ,N) 5)$2]
wo((P,B,N),,P)
40
15+F+24
41
40
42
40
43
41+P+12
44
{S}
42+P+11
[trade(B)
commodity(N)
Mobj ((B,Ni„)$2]
wo((P,B,N ,,P)
45
44+Q+23
obj((B,N)s) 2
46
45
47
wo((P,B ,N),,P)
(/3, 8,77)
45+11+46
[wo((P,B,N),,P)
Mwo(P)*3]
sem((P,B ,N),)
48
(/3,6,77)
47
wo(P) 3
49
(9)
48
50
sem((P,B ,N),)
77, 01
48+S+49
depend((P ,B,S)d)
vp(c ,O,B)
51
{a, 0,7,601,9)
40+0+50
52
(a, 7, 6,77,9)
14+E+51
s (4),111,B)
53
{ /3,7, 6, n, 0)
52
= oseIs,kattaj, 111 = atta
a = soseki(S)&amp;quot;, 13 = buy(B)81, -y = write((S,N)p)$1°, 6 = novel(N)$1,
c = agt((B,S)3)$20 , = ga(P)83, ij = obj((B,N)3)$2, 9 = wo(P)&amp;quot;
54
a, 0,7,6,701
1+A+53
</figure>
<page confidence="0.991596">
225
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.689675">
<title confidence="0.998255333333333">GENERALIZED CHART ALGORITHM: AN EFFICIENT PROCEDURE FOR COST-BASED ABDUCTION</title>
<author confidence="0.997232">Yasuharu Den</author>
<affiliation confidence="0.999604">ATR Interpreting Telecommunications Research Laboratories</affiliation>
<address confidence="0.999352">2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-02, JAPAN</address>
<phone confidence="0.700524">Tel: +81-7749-5-1328, Fax: +81-7749-5-1308, e-mail: den@itl.atr.co.jp</phone>
<abstract confidence="0.9992393">We present an efficient procedure for cost-based abduction, which is based on the idea of using chart parsers as proof procedures. We discuss in dethree features of our algorithm — bottom-up derivation, tabulation of the partial recontrol mechanism — report the results of the preliminary experiments, which show how these features improve the computational efficiency of cost-based abduction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>