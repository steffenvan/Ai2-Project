<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.108504">
<title confidence="0.999332">
Predicting Cognitively Salient Modifiers
of the Constitutive Parts of Concepts
</title>
<author confidence="0.995583">
Gerhard Kremer and Marco Baroni
</author>
<affiliation confidence="0.995916">
CIMeC, University of Trento, Italy
</affiliation>
<email confidence="0.988246">
(gerhard.kremer|marco.baroni)@unitn.it
</email>
<sectionHeader confidence="0.993649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999902666666667">
When subjects describe concepts in terms
of their characteristic properties, they often
produce composite properties, e. g., rabbits
are said to have long ears, not just ears. We
present a set of simple methods to extract
the modifiers of composite properties (in
particular: parts) from corpora. We achieve
our best performance by combining evi-
dence about the association between the
modifier and the part both within the con-
text of the target concept and independently
of it. We show that this performance is rel-
atively stable across languages (Italian and
German) and for production vs. perception
of properties.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999743587301587">
Subject-generated concept descriptions in terms of
properties of different kinds (category: rabbits are
mammals, parts: they have long ears, behaviour:
they jump, ... ) are widely used in cognitive sci-
ence as proxies to feature-based representations of
concepts in the mind (Garrard et al., 2001; McRae
et al., 2005; Vinson and Vigliocco, 2008). These
feature norms (as collections of subject-elicited
properties are called in the relevant literature) are
used in simulations of cognitive tasks and experi-
mental design. Moreover, vector spaces that have
subject-generated properties as dimensions have
been shown to be a good complement or alternative
to traditional semantic models based on corpus col-
locates (Andrews et al., 2009; Baroni et al., 2010).
Since the concept–property pairs in feature
norms resemble the tuples that relation extraction
algorithms extract from corpora (Hearst, 1992; Pan-
tel and Pennacchiotti, 2006), recent research has
attempted to extract feature-norm-like concept de-
scriptions from corpora (Almuhareb, 2006; Baroni
et al., 2010; Shaoul and Westbury, 2008). From
a practical point of view, the success of this en-
terprise would mean being able to produce much
larger norms without the need to resort to expensive
and time-consuming elicitation experiments, lead-
ing to wider cognitive simulations and possibly bet-
ter vector space models of semantics. From a the-
oretical point of view, a corpus-based system that
produces human-like concept descriptions might
provide cues of how humans themselves come up
with such descriptions.
However, the corpus-based models proposed for
this task up to this point overlook the fact that sub-
jects very often produce composite properties: Sub-
jects state that rabbits have long ears, not just ears;
cars have four wheels; a calf is a baby cow, etc.
Composite properties are not multi-word expres-
sions in the usual sense. There is nothing special
or idiomatic about long ears. It is just that we
find it to be a remarkable fact about rabbits, worth
stating in their description, that their ears are long.
In the norms described in section 3, around one
third of the part descriptions are composite. Note
that while our focus is on feature norms, a similar
point about the importance of composite properties
could be made for other knowledge repositories of
importance to computational linguistics, such as
WordNet (Fellbaum, 1998) and ConceptNet (Liu
and Singh, 2004), approximately 68,000 (36%) of
the entries and 1,300 (32%) of the part entries being
composites, respectively.
In this paper, we tackle the problem of gener-
ating composite properties from corpus data by
simplifying it in various ways. First, we focus
on part properties only, because they are com-
monly encountered in feature norms, and because
they are are commonly composite (cf. section 3).
Second, we assume that an early step in the pro-
cess of property extraction has already generated
a list of simple parts, perhaps using an existing
whole–part relation extraction algorithm (Girju et
al., 2006). Finally, we focus on composite parts
</bodyText>
<page confidence="0.984809">
54
</page>
<note confidence="0.9792115">
Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 54–62,
Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999875028571428">
with an adjective–noun structure – together with
numeral–noun cases, these constitute the near to-
tality of composite parts in the norms described
in section 3. Having thus delimited the scope of
our exploration, we will adopt the following ter-
minology: concept for the target nominal concept
(rabbit), part for the (nominal) part property (ear)
and modifier for the adjective that makes the part
composite (long).
We present simple methods that, given a list of
concept–part pairs and a POS-tagged and lemma-
tised corpus, rank and extract candidate modifiers
for the parts when predicated of the concepts. We
exploit the co-occurrence patterns of the part with
the modifier both near the concept and in other con-
texts (both kinds of co-occurrences turn out to be
helpful). We first test our methods on German fea-
ture norms, and then we show that they generalise
well by applying them to similar data in Italian, and
to the same set of German concept–part pairs when
evaluated by asking new subjects to rate the top
ranked modifiers generated by the ranking meth-
ods. This also leads to a more general discussion
of differences between modifiers produced by sub-
jects in the elicitation experiment and those that are
rated acceptable in perception, and the significance
of this for corpus-based property generation.
The paper is structured as follows. After shortly
reviewing some related work in section 2, in sec-
tion 3, we describe our feature norms focusing in
particular on composite properties. In section 4,
we describe our methods to harvest modifiers from
a corpus and report the extraction experiments,
whereas section 5 concludes by discussing direc-
tions for further work.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999836476923077">
We are not aware of other attempts to extract
concept-dependent modifiers of properties. We
review instead related work in feature norm col-
lection and prediction, and mention some rele-
vant literature on the extraction of significant co-
occurrences from corpora.
Feature-based concept description norms have
been collected in psychology for decades. Among
the more recent publicly available norms of this
sort, there are those collected by Garrard et al.
(2001), Vinson and Vigliocco (2008) and McRae
et al. (2005). The latter was the main methodologi-
cal inspiration for the bilingual norms we rely on
(see section 3 below). The norms of McRae and
colleagues include descriptions of 541 concrete
concepts corresponding to English nouns. The 725
subjects that rated these concepts had to list their
features on a paper questionnaire. The produced
features were then normalised and classified into
categories such as part and function by the exper-
imenters. The published norms include, among
other kinds of information, the frequency of pro-
duction of each feature for a concept by the sub-
jects.
Almuhareb (2006) was the first to attempt to
reproduce subject-generated features with text min-
ing techniques. He computed precision and re-
call measures of various pattern-based feature ex-
traction methods using Vinson and Vigliocco’s
norms for 35 concepts as a gold standard. The
best precision was around 16% at about 11% re-
call; maximum recall was around 60% with less
than 2% precision, confirming how difficult the
task is. Importantly for our purposes, Almuhareb
removed the modifier from composite features be-
fore running the experiments (1 wheel converted
to wheel), thus eschewing the main characteris-
tic of subject-generated concept descriptions that
we tackle here. Shaoul and Westbury (2008) and
Baroni et al. (2010) used corpus-based semantic
space models to predict the top 10 features of 44
concepts from the McRae norms. The best model
(Baroni et al.’s Strudel) guesses on average 24% of
the human-produced features, again confirming the
difficulty of the task. And, again, the test set was
pre-processed to remove modifiers of composite
features, thus sidestepping the problem we want
to deal with. It is worth remarking that, by remov-
ing modifiers, previous authors are making the task
easier in terms of feature extraction procedure (be-
cause the algorithms only need to look for single
words), but they also create artificial “salient” fea-
tures that, once the modifier has been stripped of,
are not that salient anymore (what distinguishes a
monocycle from a tricycle is that one has 1 wheel,
the other 3, not simply having wheels). It is con-
ceivable that a method to assign sensible modifiers
to features might actually improve the overall qual-
ity of feature extraction algorithms.
Following a very long tradition in computational
linguistics (Church and Hanks, 1990), we use co-
occurrence statistics for words in certain contexts
to hypothesise a meaningful connection between
the words. In this respect, what we propose is not
different from common methods to extract and rank
</bodyText>
<page confidence="0.997493">
55
</page>
<bodyText confidence="0.999861625">
collocations, multi-word expressions or semanti-
cally related terms (Evert, 2008). From a technical
point of view, the innovative aspect of our task is
that we do not just look for co-occurrences between
two items, but for co-occurrences in the context of
a third element, i. e., we are interested in modifier–
part pairs that are related when predicated of a
certain concept. The method we apply to the ex-
traction of modifier–part pairs when they co-occur
with the target concept in a large window is similar
to the idea of looking for partially untethered con-
textual patterns proposed by Garera and Yarowsky
(2009), that extract name–pattern–property tuples
where the pattern and the property must be adja-
cent, but the target name is only required to occur
in the same sentence.
</bodyText>
<sectionHeader confidence="0.97147" genericHeader="method">
3 Composite Parts in Feature Norms
</sectionHeader>
<bodyText confidence="0.999961608695652">
Our empirical starting point are the feature norms
collected in parallel from 73 German and 69 Ital-
ian subjects by Kremer et al. (2008), following a
methodology similar to that of McRae et al. (2005).
The norms pertain to 50 concrete concepts from 10
classes such as mammals (e. g., dog), manipulable
tools (e. g., comb), etc. The concept–part pairs in
these norms served on the one hand as input to our
algorithm – on the other hand, its output (the set of
selected modifiers from the corpus) could be evalu-
ated against those modifiers that were produced by
the subjects. Furthermore, the bilingual nature of
the norms allows us to tune our algorithm on one
language (German), and evaluate its performance
on the other (Italian), to assess its cross-lingual
generalisation capability.
To confirm that speakers actually frequently pro-
duce properties composed of part and modifier, ob-
serve that in the German data (10,010 descriptive
phrases in total), of the 1,667 parts produced, 625
(more than one third) were composite parts, and
404 were composed of an adjective and a noun, the
target of this research work. Looking at the distinct
parts that were elicited, 92 were always produced
with a modifier, 280 only without modifier, and 122
both with and without modifier. That is, for about
43% of the parts at least some speakers used a com-
posite expression of adjective and noun. This high
proportion motivates our work and is not surpris-
ing, given that, for describing a specific concept,
one will tend to come up with whatever makes this
concept special and distinguishes it from other con-
cepts – which (considering parts) sometimes is the
part itself (elephant: trunk) and sometimes some-
thing special about the shape, colour, size, or other
attributes of the part (elephant: big ears).
The data set for modifier extraction and subse-
quent method evaluation comprises all the concept–
modifier–part triples (e. g., onion: brown peel) pro-
duced by at least one subject, taken from the Ger-
man and the Italian norms. The German (Italian)
speakers described 41 (30) different concepts by
using at least one out of 80 (45) different parts in
combination with one out of 62 (50) different mod-
ifiers, totalling to 229 (127) differently combined
triples.
</bodyText>
<sectionHeader confidence="0.999547" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999988666666667">
This section describes the approach we explored for
ranking and extracting modifiers of composite parts
and evaluates the performance of 6 different extrac-
tion methods in terms of the production norms.
Acceptance rate data from a follow-up judgement
experiment complete the evaluation.
</bodyText>
<subsectionHeader confidence="0.996599">
4.1 Ranked Modifier Lists
</subsectionHeader>
<bodyText confidence="0.999967">
Based on the idea that the co-occurrence of words
in a text corpus reflects to some extent how strong
these words are associated in speakers’ minds
(Spence and Owens, 1990), our extraction approach
works on the lemmatised and POS-tagged German
WaCky1 web corpus of about 1.2 billion tokens.
</bodyText>
<subsectionHeader confidence="0.627708">
Modifier–Part Frequencies
</subsectionHeader>
<bodyText confidence="0.999972615384615">
Using the CQP2 tool, corpus frequencies were col-
lected for all co-occurrences of adjectives with
those part nouns that were produced in the exper-
iment described above. A possible gap of up to
3 tokens between the pair of adjective and noun
allowed to extract also adjectives that are not di-
rectly adjacent to the nouns in the corpus (but in a
sequence of adjectives, for example). For each part
noun, the 5 most frequent adjective modifiers from
the ranked modifier–part list were selected under
the assumption that the preferred usage of these
modifiers with the specific part indicates the most
common attributes which that part typically has.
</bodyText>
<footnote confidence="0.99486975">
1See the WaCky project at http://wacky.sslmit.
unibo.it
2Corpus Query Processor (part of the IMS Open Corpus
Workbench, see http://cwb.sourceforge.net)
</footnote>
<page confidence="0.995673">
56
</page>
<subsectionHeader confidence="0.937054">
Log-Likelihood Values of Frequencies
</subsectionHeader>
<bodyText confidence="0.999966181818182">
An attempt to improve the performance of the first
method is to calculate3 the log-likelihood associ-
ation value for each modifier–part pair instead of
keeping the raw co-occurrence frequency, and se-
lect the 5 highest ranked modifiers for each part
from this list. Log-likelihood weighting should
account for typical modifiers which have a low fre-
quency but do generally not occur often in the cor-
pus, and with not many other parts – their log-likeli-
hood value will be higher, and so will be their rank
(e. g., two-sided blade in contrast to long blade).
</bodyText>
<subsectionHeader confidence="0.870458">
Modifier–Part Frequencies in Concept Context
</subsectionHeader>
<bodyText confidence="0.9998844">
However, both of these methods do not necessarily
yield generally atypical modifiers that are however
typical of a part when it is attributed to a specific
concept. For example, birds’ beaks are typically
brown, orange or yellow, but aiming to extract mod-
ifiers for a crow’s beak, black would be one of the
desired modifiers – which does not appear at a high
frequency rank as a generic beak modifier. The
methods described so far did not take the concept
into account when generating the modifier–part
pairs, i. e., for all concepts with a specific part the
same set of modifiers would be extracted.
To address this issue, a second frequency rank
list was prepared in the same manner – with the
only difference that the part noun had to appear
within the context of the concept noun. That way,
also modifiers for specific concepts’ parts that devi-
ate from the most typical part modifiers appear at a
high rank. However, these data are sparser, which
is why we used a wide context of 40 sentences (20
sentences before and after the part) within which
the concept had to occur (i. e., a paragraph-like con-
text size in which the topic, presumably, comprises
the concept). We refer to ranked lists of modifier–
part pairs that do not take the target concept into
account as contextless lists, and to lists within the
span of a context as in-context lists.
Due to the already mentioned data sparseness
problem, not all modifiers used for a part noun in
the production norms could be extracted with the
latter method, as some of the obvious modifiers for
specific parts are just not written about. For these,
there is a higher chance that they appear, if at all, in
the contextless rank list. For example, thin bristles
does not appear in the context of broom. In the in-
</bodyText>
<footnote confidence="0.940163">
3Using the UCS toolkit, described at http://www.
collocations.de/software.html#UCS
</footnote>
<table confidence="0.999627285714286">
contextless concept context
rank freq modifier freq modifier
1 507 thick 16 thick
2 209 dense 14 white
3 204 soft 11 small
4 185 black 11 soft
5 175 long 9 dense
</table>
<tableCaption confidence="0.997457">
Table 1: Top 5 modifiers from frequency rank lists
</tableCaption>
<bodyText confidence="0.990107423076923">
for part fur and concept bear
context list, 33% of the 229 triples extracted from
the German norms were not found (in the context-
less list, only 9% modifier–part pairs are missing).
Additionally, particular concepts, parts, or concept–
part pairs (within the 40 sentence span) might be
missing from the corpus, as well. From the Ger-
man norms collection, all concepts appeared in the
corpus, but one part (a noun–noun compound), and
6 concept–part pairs (rare, colloquial part nouns)
were missing. In the evaluation to follow, all the
modifiers pertaining to these missing data from the
corpus will be counted as positives not found by
the algorithm.
The example excerpt in table 1 shows modifiers
that were selected for bear and fur, using the two
frequency rank lists described above. Although in
this example most of the modifiers (thick, dense,
soft) are found in both lists, two arguably reason-
able modifiers are just in the contextless set (black,
long), and one only in the in-context set (white).
A disadvantage of selecting modifiers from the in-
context rank list is that many modifiers have the
same low frequency, but they should nevertheless
have differing ranks. In such cases, we assigned
ranks according to alphabetic order of modifiers.
</bodyText>
<subsectionHeader confidence="0.888341">
Summed Log-Rescaled Frequencies
</subsectionHeader>
<bodyText confidence="0.999983333333333">
Next, to improve performance and profit from both
information sources the above methods provide,
the in-context and contextless rank lists were com-
bined. In one variant, the scaled frequencies for
the concept–modifier–part triples appearing in both
lists were added. Scaling was necessary because
the frequencies in the contextless list are in general
much higher than in the in-context list. Further-
more, to account for the fact that at high ranks
the difference in frequency between subsequent
ranks is much higher than at lower ranks, scaling
was done by using the logarithmic values of the fre-
</bodyText>
<page confidence="0.992901">
57
</page>
<bodyText confidence="0.998050166666667">
quencies: For each concept–modifier–part triple, its
logarithmic frequency value was divided by the log-
arithmic value of the maximum corpus frequency
of all parts in the corpus (in the contextless list)
or of all concept–part pairs co-occurring within 40
sentences (in the case of the in-context list).
</bodyText>
<subsectionHeader confidence="0.957415">
Productwise Combination of Frequencies
</subsectionHeader>
<bodyText confidence="0.999925230769231">
As an alternative back-off approach, the raw fre-
quencies were combined productwise into a new
list (for those modifier–part pairs missing in the in-
context list, the frequency of the pair in the context-
less list was taken alone, instead of multiplying it
by zero; i. e., the in-context term was max(freq,1)).
This achieves a sort of “intersective” effect, where
modifiers that are both commonly attributed to the
part and predicated of it in the context of the tar-
get concept are boosted up in the list, according to
the intuition that a good modifier should be both
plausible for the part in general, and typical for the
concept at hand.
</bodyText>
<subsectionHeader confidence="0.955818">
Cosine-Based Re-Ranking
</subsectionHeader>
<bodyText confidence="0.999983788461538">
An attempt to further improve performance is based
on the idea that parts are described by some spe-
cific types of attributes. For example, a leaf would
be characterised by its shape or consistency (e. g.,
long, stiff), whereas for fur rather colour should be
considered (e. g., white, brown). If we are able to
cluster modifiers for their attribute type and find
out which attribute types are in particular important
for a specific part, those could get a preference in
the rank list and be moved towards the top. To
approach this in a simple way, a re-ranking method
is used which is supposed to cluster and choose the
right cluster of modifiers implicitly: The modifiers
in the (productwise-) combined list were tested for
their similarity by looking if they co-occur with
the same relative frequency with the same set of
nouns. In case of high similarity (in this respect)
of a modifier to a single other modifier, or if the
modifier was similar to a lot of modifiers, it should
be re-ranked to a higher position. In more detail,
a vector was created for each modifier, denoting
its co-occurrence frequencies with each noun in
the corpus within a window of 4 tokens (on the
left side of the noun). Random indexing helped to
reduce the vector dimensionality from 27,345 to
3,000 elements (Sahlgren, 2005). These vectors
served for calculating the cosine distances between
modifiers. Then, for each of the top 200 modifiers
in the combined frequency rank list (covering 84%
of the triples from the German norms), the cosine
distance was calculated to each of the top 100 mod-
ifiers in the contextless rank list. A constant of 1
was added to each of the computed cosines, thus
obtaining a quantity between 1 and 2. The original
combined frequency value was multiplied by this
quantity (thus leaving it unchanged when the orig-
inal cosine was 0, increasing it otherwise). From
the re-ranked list resulting from this operation, we
selected, again, the top 5 modifiers of each concept–
part pair. For example, suppose that black is among
the modifiers of a crow’s beak in the combined list.
We compute the cosine similarity of black with the
top 100 modifiers of beak (in any context), and,
for each of these cosines, we multiply the original
combined value of black by cosine+1. Since the
colour is a common attribute of beaks, the presence
of modifiers like yellow and brown, high on the con-
textless beak list, helps re-ranking black high in the
crow-specific beak list. We hope that this method
helps out concept-specific values (e. g., black for
crow) of attributes that are in general typical of a
part (colour for beak).
</bodyText>
<subsectionHeader confidence="0.9850635">
4.2 Performance on Composite Parts From
the Production Norms
</subsectionHeader>
<bodyText confidence="0.99992752">
The feature norms data represented the gold stan-
dard for the evaluation of all sets of modifiers cho-
sen by each of the described methods for the given
concept–part pairs. Note that, even if a modifier–
part pair was produced only once in the fea-
ture production norms, the corresponding concept–
modifier–part triple was included in the gold stan-
dard – which contains 41 different concepts, 80
different parts, and 62 different modifiers, totalling
to 229 concept–modifier–part triples. As in the Ger-
man corpus there are 154,935 adjective–part-noun
pairs, the random baseline (random guessing) for
finding these 229 pairs is approaching 0 (similarly
for Italian and the judgement dataset).
Figure 1 displays the performance of the meth-
ods on German in the form of a recall–precision
graph. For each rank (1–5), overall recall and inter-
polated precision values are given for all modifier–
part pairs up to this rank – note that precision at
1% recall is overrated as it is based on an arbitrary
fraction of rank 1 pairs. As expected, extracting
modifiers of parts within a concept context (the in-
context list) achieves low recall. In contrast, modi-
fiers that were extracted by querying the corpus for
parts without considering the concept context have
</bodyText>
<page confidence="0.995758">
58
</page>
<figure confidence="0.9056875">
Method Performance
recall
</figure>
<figureCaption confidence="0.999837">
Figure 1: Evaluation on German norms
</figureCaption>
<figure confidence="0.9392815">
Method Performance
recall
</figure>
<figureCaption confidence="0.994367">
Figure 2: Evaluation on Italian norms
</figureCaption>
<figure confidence="0.972498823529412">
0.0 0.2 0.4 0.6 0.8 1.0
precision
0.0 0.2 0.4 0.6 0.8 1.0
parts in concept context (freq)
contextless (freq)
combination productwise (freqs)
cosine re−ranking (freq product)
parts in context (log−likelihood)
combination by sum (log rescaled freqs)
baseline: averaged random guessing
0.0 0.2 0.4 0.6 0.8 1.0
precision
0.0 0.2 0.4 0.6 0.8 1.0
parts in concept context (freq)
contextless (freq)
combination productwise (freqs)
cosine re−ranking (freq product)
</figure>
<bodyText confidence="0.999193296875">
a higher recall. But this method has a lower preci-
sion in general. The performance for the method
combining frequencies productwise and for the one
that re-ranks this combined list via cosine-based
smoothing are substantially better. Not only the pre-
cision is much higher at all recall levels, but also
their maximum recall values are higher than those
of the contextless lists, i. e., it was worth combin-
ing the complementing information in the two lists.
However, the performance of the cosine-based re-
ranked list compared to the productwise-combined
list is not considerably higher, as we might have
hoped. The remaining two alternative methods per-
formed much worse: the one using log-likelihood
values as ranking criterion had in general a low pre-
cision and a low recall, and the method combining
the in-context and the contextless rank list by sum-
ming up the rescaled logarithmic frequency values
performs as bad as the contextless rank list. Never-
theless, note that all methods perform distinctively
well above the baseline.
Qualitatively analysing the data collected with
the described methods did not give definite clues
about why some performed not as good as expected.
As a comprehensible example, the modifier short
for legs is at rank 5 in the contextless list, but be-
cause of the frequent co-occurrence with monkey it
rises to rank 2 in the productwise combination of
these lists, and even to rank 1 in the cosine-based
re-ranked list. An understandable bad performing
example is the modifier yellow for the eyes of an
owl: Although it appears in the in-context list at
rank 2, it is a quite infrequent modifier for eyes
in general (i. e., low in the contextless list), and
thus it is not contained in the top 5 modifiers in
the productwise combined rank list. On the other
hand, it is not perfectly clear to us why, e. g., flat for
the roof of a skyscraper, which is at rank 5 in the
contextless list and at rank 6 in the combined list,
is lowered to rank 9 in the cosine-based re-ranked
list (in the in-context list, it does not appear at all).
For all methods, collected modifiers include such
of undesired attributes not describing the part, but
other, rather situational aspects, e. g., own, left, new,
protecting, and famous. Furthermore, we observed
that some modifiers are reasonable for the respec-
tive concept–part pair, but they are counted as false
because they did not occur in the production experi-
ment (that we took as the evaluation basis), e. g., for
the blade of a sword, not only large is acceptable,
but also long and wide, essentially making the same
assertion about the size of the blade. This issue is
addressed further below by creating a new evalua-
tion standard based on plausibility judgements.
To evaluate the cross-lingual performance of
the extraction approach, the Italian norms were
explored similarly to the German norms for com-
posite parts. The gold standard here comprised
127 triples (from combinations of 30 different con-
cepts, 45 parts, and 50 different modifiers). The
same methods described above were used to ex-
tract modifiers from the Italian WaCky web corpus
(more than 1.5 billion tokens), with one difference
regarding the query for adjectives near nouns: As
</bodyText>
<page confidence="0.997371">
59
</page>
<bodyText confidence="0.999990352941177">
in the Italian language adjectives in a noun phrase
can be used both before and after the noun (with
differences in their meaning), and given that most
of them were produced after the noun, we collected
all adjectives occurring up to 2 words from the left
of the noun and up to 4 words to the right.
Figure 2 shows the performance curves of the
methods for the Italian data. In this evaluation, the
method using log-likelihood values and the method
combining lists via addition of logarithmic rescaled
frequencies are omitted as their performance was
not promising at all in the German data, and they
are conceptually similar to the contextless and
productwise-combination approaches, respectively.
Like in German, the in-context method yields a
low recall, in contrast to the method not consid-
ering the presence of concepts in context. Again,
cosine-based re-ranking performs very similarly to
the method using the productwise-combined list.
For the performance on the Italian data, their differ-
ence from the simple frequency rank lists is not as
large as it is for the German data, but it is clearly
visible, especially at higher recall values.
Summarising, our comparison of various corpus-
based ranking methods to the feature production
norms, both in German and Italian, suggests that
composite parts produced by subjects are best
mined in corpora by making use of both general in-
formation about typical modifiers of the parts (the
contextless rank) and more specific information
about modifiers that co-occur with the part near the
target concept. Moreover, it is better to combine
the two information sources productwise, which
suggests an intersecting effect (the most likely mod-
ifiers are both well-attested out of context and seen
near the target concept). For both languages, there
is no strong evidence that re-ranking by cosine sim-
ilarity (a method that should favour modifiers that
are values of common attributes of a part) is im-
proving on the plain combination method (although
re-ranking is not hurting, either).
By looking at the overall performance, the re-
sults are somewhat underwhelming, with precision
around 20% at around 30% recall for the best mod-
els in both languages. A natural question at this
point is whether the modifiers ranked at the top
by the best methods and treated as false positives
because they are not in the norms are nevertheless
sensible modifiers for the parts, or whether they are
truly noise. In order to explore this issue we turn
now to our next experiment.
</bodyText>
<subsectionHeader confidence="0.9973995">
4.3 Performance Evaluation Based on
Plausibility Judgements
</subsectionHeader>
<bodyText confidence="0.999966142857143">
The purpose of this judgement experiment was to
see which concept–modifier–part triples the ma-
jority of participants would rate as acceptable. It
allows us to investigate two topics: (i) the compari-
son of what people produce and what they perceive
as being a prominent modifier for a concept–part
pair (our algorithm might actually provide good
candidates which were just not produced, as we just
said) and (ii) a re-evaluation of the cosine-based
re-ranking method (it could be in fact better than
we thought because we only evaluated what was
produced, but did not have a definite plausibility
rating of the candidates missing in the norms).
The tested set contained the triples yielded by
our two best performing methods (productwise
combination and cosine-based re-ranking), which
were applied to the German feature norms (692
triples, comprising 41 concepts and 71 parts). From
this set, a set of triples was chosen randomly for
each of the 46 participants (recruited by e-mail
among acquaintances of the first author). The
triples were presented to participants embedded
into a natural-sounding sentence of the form “The
[part] of a [concept] is [modifier]”. Each partic-
ipant rated 333 sentences, presented on separate
lines of a text file (this set of sentences presented
comprised additional triples which were intended
for other purposes – for the current evaluation, we
used a subset of 110 of these from each partici-
pant, on the average). Participants were instructed
to read the sentences as general statements about
a concept’s part and mark them by typing a let-
ter (“w” for wonderful and “d” for dubious – to
facilitate one-handed typing and easy memorisa-
tion) at the beginning of the line, if they thought it
plausible/unlikely that someone used the sentence
to explain an aspect of the relevant part. In total,
5,525 judgements were collected; each sentence in
the set was judged on the average by 8 persons.
The performance evaluation is based on the ac-
ceptance rate of the participants: Modifiers ac-
cepted by at least 75% of the raters are consid-
ered plausible. Figure 3 shows the recall–precision
graph for the methods tested on the concept–part
pairs from the German norms. From the 692 triples
judged, around 13% were accepted by the majority
of speakers. The precision rate is comparable with
the evaluation on the basis of the modifiers pro-
duced by participants (highest recall is 1, of course,
</bodyText>
<page confidence="0.964254">
60
</page>
<figure confidence="0.865735">
Method Performance
recall
</figure>
<figureCaption confidence="0.999867">
Figure 3: Evaluation on judgements (German)
</figureCaption>
<bodyText confidence="0.999816695652174">
because all modifiers to be judged were exclusively
from the data set selected by our methods).
Again, the performance of the cosine-based re-
ranking method is similar to the performance of the
productwise-combination method. For a more ex-
act evaluation of the difference between these two,
a last test was conducted: Instead of measuring the
performance in the form of counts of modifiers that
were accepted by the majority of participants, we
used the acceptance rates of all modifiers: The ac-
ceptance rates of all judged triples were summed up
if they contained the same concept–part pair. This
means that each concept–part pair received a score
reflecting the overall acceptance of the set of modi-
fiers for that pair (e. g., for bear: fur, all acceptance
rates for bear: brown fur, bear: soft fur, ... were
summed up). Then, the score of each concept–part
pair in the productwise-combined list was com-
pared against the score of the same pair for the
cosine-based re-ranking method, using a pairwise
t-test (this procedure is sound because the modifiers
per pair are the same for the two methods). The
test showed a significant difference (p = 0.008), but
in favour of the productwise-combination method
(score means were slightly higher). That is, cosine-
based re-ranking in the current form brings no ad-
vantage over the simpler productwise combination
of the frequency lists.
Finally, turning to the qualitative comparison of
production and perception, there was a relatively
small overlap of triples (46) contrasting with modi-
fiers only produced but not accepted (53), and mod-
ifiers accepted but not produced (42). Intuitively,
we would have expected that what was produced
will be also accepted by the majority of people.
Possibly, some participants in the judgement ex-
periment found a few of the triples produced ques-
tionable (goose: long beak) – such triples were in
our gold standard because we deliberately did not
want to exclude composite parts even if produced
by only one speaker – whereas participants produc-
ing parts for given concepts probably just did not
think of specific parts or modifiers (e. g., aeroplane:
small windows and bear: dense fur). The important
fact regarding this difference is, however, that our
method captures both kinds of modifiers.
</bodyText>
<sectionHeader confidence="0.999774" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999995885714286">
We presented several corpus-based methods that
provide a set of adjective modifiers for each con-
crete concept–part pair, to be compared to those
modifiers that are salient to human subjects. The
general approach was to generate ranked lists, and
select the 5 candidates at the top of the ranks.
The best of our methods works on the simple
(productwise-) combination of frequency informa-
tion of co-occurring adjective–noun pairs with and
without considering a wide “concept context” in
which the part noun has to occur. This method per-
formed better than the one based on co-occurrence
frequency not in concept context (generic modi-
fiers, not appropriate for every concept) and the
one based on co-occurrence frequencies in concept
context, only (low recall because of sparse data).
We evaluated the methods on feature production
norms and on plausibility judgements of generated
concept–modifier–part triples to compare produc-
tion and perception of modifiers. The performance
was similar in precision – although the qualitative
analysis showed that modifiers produced and modi-
fiers perceived did not have a large overlap. This
means our algorithm is capable of collecting both
with the same performance.
After tuning the algorithm on German norms, we
evaluated its generalisation capability to a different
language (Italian). Performance was similar. Less
satisfying at first glance is the precision value of
just around 20% at the maximum recall level (how-
ever, when compared to the baseline of below 1%
precision, this is an essentially better value) – as
well as the fact that our implementation of the intu-
itive idea to re-rank modifiers that are similar (and
should instantiate the same attribute) did not have
</bodyText>
<figure confidence="0.9579448">
0.0 0.2 0.4 0.6 0.8 1.0
precision
0.0 0.2 0.4 0.6 0.8 1.0
combination productwise (freqs)
cosine re−ranking (freq product)
</figure>
<page confidence="0.99469">
61
</page>
<bodyText confidence="0.999829">
a performance advantage. This is subject to further
work. Moreover, using a machine-learning method
(building a binary classifier) could be tried. An-
other idea was to crawl the web and select concept-
specific text passages to build a specialised corpus.
Possibly, we could draw then from a richer infor-
mation source. A rough attempt to do this did not
seem to yield promising results.
So far, we included only adjectives as permis-
sible modifiers. A future extension could be also
aiming for numerals (e. g., four wheels). Then, for
the simulation of human-like behaviour we imag-
ine as part of the possible future work to enable
the algorithm to decide if a part noun should be
paired with a modifier, at all – or if the part itself is
sufficient to describe a concept (big ears vs. trunk).
Regarding the evaluation, a more exact perfor-
mance measure would probably be achieved by
either having more participants producing concept
descriptions and then only selecting those modi-
fiers for the gold standard that were produced by a
majority – or letting participants in a judgement ex-
periment also judge modifiers that were produced,
to filter out the unlikely ones.
A next step in the project will be extracting
salient parts for concepts (which we assumed to
have done already for the purpose of this paper),
possibly by integrating the information we already
collected by extracting modifiers. In the end, we
would like to come up with an adaptable method
that extracts not only parts but also other types
of relations (e. g., category, behaviour, function,
etc.), which have been already addressed in re-
lated works, though. The issue we presented in
this paper, however, is new and, we think, worth
exploring.
</bodyText>
<sectionHeader confidence="0.999279" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999449424242424">
Abdulrahman Almuhareb. 2006. Attributes in Lexical
Acquisition. Phd thesis, University of Essex.
Mark Andrews, Gabriella Vigliocco, and David Vin-
son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological Review, 116(3):463–498.
Marco Baroni, Eduard Barbu, Brian Murphy, and Mas-
simo Poesio. 2010. Strudel: A distributional seman-
tic model based on properties and types. Cognitive
Science, 34(2):222–254.
Kenneth Church and Peter Hanks. 1990. Word associ-
ation norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22–29.
Stefan Evert. 2008. Corpora and collocations. In
A. L¨udeling and M. Kyt¨o, editors, Corpus Linguis-
tics: An International Handbook, pages 1212–1248.
Mouton de Gruyter, Berlin, Germany.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Nikesh Garera and David Yarowsky. 2009. Structural,
transitive and latent models for biographic fact ex-
traction. In Proceedings of EACL, pages 300–308,
Athens, Greece.
Peter Garrard, Matthew Lambon Ralph, John Hodges,
and Karalyn Patterson. 2001. Prototypicality, dis-
tinctiveness, and intercorrelation: Analyses of the
semantic attributes of living and nonliving concepts.
Cognitive Neuropsychology, 18(2):25–174.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2006. Automatic discovery of part-whole relations.
Computational Linguistics, 32(1):83–135.
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
COLING, pages 539–545, Nantes, France.
Gerhard Kremer, Andrea Abel, and Marco Baroni.
2008. Cognitively salient relations for multilingual
lexicography. In Proceedings of the COGALEX
Workshop at COLING08, pages 94–101.
Hugo Liu and Push Singh. 2004. ConceptNet: A prac-
tical commonsense reasoning toolkit. BT Technol-
ogy Journal, pages 211–226.
Ken McRae, George Cree, Mark Seidenberg, and Chris
McNorgan. 2005. Semantic feature production
norms for a large set of living and nonliving things.
Behavior Research Methods, 37(4):547–559.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automat-
ically harvesting semantic relations. In Proceed-
ings of COLING-ACL, pages 113–120, Sydney, Aus-
tralia.
Magnus Sahlgren. 2005. An introduction to random
indexing. http://www.sics.se/˜mange/
papers/RI_intro.pdf.
Cyrus Shaoul and Chris Westbury. 2008. Performance
of HAL-like word space models on semantic cluster-
ing. In Proceedings of the ESSLLI Workshop on Dis-
tributional Lexical Semantics, pages 42–46, Ham-
burg, Germany.
Donald Spence and Kimberly Owens. 1990. Lexical
co-occurrence and association strength. Journal of
Psycholinguistic Research, 19(5):317–330.
David Vinson and Gabriella Vigliocco. 2008. Seman-
tic feature production norms for a large set of objects
and events. Behavior Research Methods, 40(1):183–
190.
</reference>
<page confidence="0.999175">
62
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.975706">
<title confidence="0.9974065">Predicting Cognitively Salient of the Constitutive Parts of Concepts</title>
<author confidence="0.984732">Kremer</author>
<affiliation confidence="0.999462">CIMeC, University of Trento,</affiliation>
<email confidence="0.998413">(gerhard.kremer|marco.baroni)@unitn.it</email>
<abstract confidence="0.9996161875">When subjects describe concepts in terms of their characteristic properties, they often e. g., rabbits said to have not just ears. We present a set of simple methods to extract the modifiers of composite properties (in particular: parts) from corpora. We achieve our best performance by combining evidence about the association between the modifier and the part both within the context of the target concept and independently of it. We show that this performance is relatively stable across languages (Italian and German) and for production vs. perception of properties.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Abdulrahman Almuhareb</author>
</authors>
<title>Attributes in Lexical Acquisition. Phd thesis,</title>
<date>2006</date>
<institution>University of Essex.</institution>
<contexts>
<context position="1864" citStr="Almuhareb, 2006" startWordPosition="272" endWordPosition="273">in the relevant literature) are used in simulations of cognitive tasks and experimental design. Moreover, vector spaces that have subject-generated properties as dimensions have been shown to be a good complement or alternative to traditional semantic models based on corpus collocates (Andrews et al., 2009; Baroni et al., 2010). Since the concept–property pairs in feature norms resemble the tuples that relation extraction algorithms extract from corpora (Hearst, 1992; Pantel and Pennacchiotti, 2006), recent research has attempted to extract feature-norm-like concept descriptions from corpora (Almuhareb, 2006; Baroni et al., 2010; Shaoul and Westbury, 2008). From a practical point of view, the success of this enterprise would mean being able to produce much larger norms without the need to resort to expensive and time-consuming elicitation experiments, leading to wider cognitive simulations and possibly better vector space models of semantics. From a theoretical point of view, a corpus-based system that produces human-like concept descriptions might provide cues of how humans themselves come up with such descriptions. However, the corpus-based models proposed for this task up to this point overloo</context>
<context position="6890" citStr="Almuhareb (2006)" startWordPosition="1081" endWordPosition="1082">d McRae et al. (2005). The latter was the main methodological inspiration for the bilingual norms we rely on (see section 3 below). The norms of McRae and colleagues include descriptions of 541 concrete concepts corresponding to English nouns. The 725 subjects that rated these concepts had to list their features on a paper questionnaire. The produced features were then normalised and classified into categories such as part and function by the experimenters. The published norms include, among other kinds of information, the frequency of production of each feature for a concept by the subjects. Almuhareb (2006) was the first to attempt to reproduce subject-generated features with text mining techniques. He computed precision and recall measures of various pattern-based feature extraction methods using Vinson and Vigliocco’s norms for 35 concepts as a gold standard. The best precision was around 16% at about 11% recall; maximum recall was around 60% with less than 2% precision, confirming how difficult the task is. Importantly for our purposes, Almuhareb removed the modifier from composite features before running the experiments (1 wheel converted to wheel), thus eschewing the main characteristic of </context>
</contexts>
<marker>Almuhareb, 2006</marker>
<rawString>Abdulrahman Almuhareb. 2006. Attributes in Lexical Acquisition. Phd thesis, University of Essex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Andrews</author>
<author>Gabriella Vigliocco</author>
<author>David Vinson</author>
</authors>
<title>Integrating experiential and distributional data to learn semantic representations.</title>
<date>2009</date>
<journal>Psychological Review,</journal>
<volume>116</volume>
<issue>3</issue>
<contexts>
<context position="1556" citStr="Andrews et al., 2009" startWordPosition="228" endWordPosition="231">, parts: they have long ears, behaviour: they jump, ... ) are widely used in cognitive science as proxies to feature-based representations of concepts in the mind (Garrard et al., 2001; McRae et al., 2005; Vinson and Vigliocco, 2008). These feature norms (as collections of subject-elicited properties are called in the relevant literature) are used in simulations of cognitive tasks and experimental design. Moreover, vector spaces that have subject-generated properties as dimensions have been shown to be a good complement or alternative to traditional semantic models based on corpus collocates (Andrews et al., 2009; Baroni et al., 2010). Since the concept–property pairs in feature norms resemble the tuples that relation extraction algorithms extract from corpora (Hearst, 1992; Pantel and Pennacchiotti, 2006), recent research has attempted to extract feature-norm-like concept descriptions from corpora (Almuhareb, 2006; Baroni et al., 2010; Shaoul and Westbury, 2008). From a practical point of view, the success of this enterprise would mean being able to produce much larger norms without the need to resort to expensive and time-consuming elicitation experiments, leading to wider cognitive simulations and </context>
</contexts>
<marker>Andrews, Vigliocco, Vinson, 2009</marker>
<rawString>Mark Andrews, Gabriella Vigliocco, and David Vinson. 2009. Integrating experiential and distributional data to learn semantic representations. Psychological Review, 116(3):463–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Eduard Barbu</author>
<author>Brian Murphy</author>
<author>Massimo Poesio</author>
</authors>
<title>Strudel: A distributional semantic model based on properties and types.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="1578" citStr="Baroni et al., 2010" startWordPosition="232" endWordPosition="235">g ears, behaviour: they jump, ... ) are widely used in cognitive science as proxies to feature-based representations of concepts in the mind (Garrard et al., 2001; McRae et al., 2005; Vinson and Vigliocco, 2008). These feature norms (as collections of subject-elicited properties are called in the relevant literature) are used in simulations of cognitive tasks and experimental design. Moreover, vector spaces that have subject-generated properties as dimensions have been shown to be a good complement or alternative to traditional semantic models based on corpus collocates (Andrews et al., 2009; Baroni et al., 2010). Since the concept–property pairs in feature norms resemble the tuples that relation extraction algorithms extract from corpora (Hearst, 1992; Pantel and Pennacchiotti, 2006), recent research has attempted to extract feature-norm-like concept descriptions from corpora (Almuhareb, 2006; Baroni et al., 2010; Shaoul and Westbury, 2008). From a practical point of view, the success of this enterprise would mean being able to produce much larger norms without the need to resort to expensive and time-consuming elicitation experiments, leading to wider cognitive simulations and possibly better vector</context>
<context position="7601" citStr="Baroni et al. (2010)" startWordPosition="1190" endWordPosition="1193">es. He computed precision and recall measures of various pattern-based feature extraction methods using Vinson and Vigliocco’s norms for 35 concepts as a gold standard. The best precision was around 16% at about 11% recall; maximum recall was around 60% with less than 2% precision, confirming how difficult the task is. Importantly for our purposes, Almuhareb removed the modifier from composite features before running the experiments (1 wheel converted to wheel), thus eschewing the main characteristic of subject-generated concept descriptions that we tackle here. Shaoul and Westbury (2008) and Baroni et al. (2010) used corpus-based semantic space models to predict the top 10 features of 44 concepts from the McRae norms. The best model (Baroni et al.’s Strudel) guesses on average 24% of the human-produced features, again confirming the difficulty of the task. And, again, the test set was pre-processed to remove modifiers of composite features, thus sidestepping the problem we want to deal with. It is worth remarking that, by removing modifiers, previous authors are making the task easier in terms of feature extraction procedure (because the algorithms only need to look for single words), but they also c</context>
</contexts>
<marker>Baroni, Barbu, Murphy, Poesio, 2010</marker>
<rawString>Marco Baroni, Eduard Barbu, Brian Murphy, and Massimo Poesio. 2010. Strudel: A distributional semantic model based on properties and types. Cognitive Science, 34(2):222–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Peter Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="8660" citStr="Church and Hanks, 1990" startWordPosition="1362" endWordPosition="1365">s, previous authors are making the task easier in terms of feature extraction procedure (because the algorithms only need to look for single words), but they also create artificial “salient” features that, once the modifier has been stripped of, are not that salient anymore (what distinguishes a monocycle from a tricycle is that one has 1 wheel, the other 3, not simply having wheels). It is conceivable that a method to assign sensible modifiers to features might actually improve the overall quality of feature extraction algorithms. Following a very long tradition in computational linguistics (Church and Hanks, 1990), we use cooccurrence statistics for words in certain contexts to hypothesise a meaningful connection between the words. In this respect, what we propose is not different from common methods to extract and rank 55 collocations, multi-word expressions or semantically related terms (Evert, 2008). From a technical point of view, the innovative aspect of our task is that we do not just look for co-occurrences between two items, but for co-occurrences in the context of a third element, i. e., we are interested in modifier– part pairs that are related when predicated of a certain concept. The method</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Church and Peter Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
</authors>
<title>Corpora and collocations.</title>
<date>2008</date>
<booktitle>Corpus Linguistics: An International Handbook,</booktitle>
<pages>1212--1248</pages>
<editor>In A. L¨udeling and M. Kyt¨o, editors,</editor>
<location>Berlin, Germany.</location>
<contexts>
<context position="8954" citStr="Evert, 2008" startWordPosition="1409" endWordPosition="1410">e from a tricycle is that one has 1 wheel, the other 3, not simply having wheels). It is conceivable that a method to assign sensible modifiers to features might actually improve the overall quality of feature extraction algorithms. Following a very long tradition in computational linguistics (Church and Hanks, 1990), we use cooccurrence statistics for words in certain contexts to hypothesise a meaningful connection between the words. In this respect, what we propose is not different from common methods to extract and rank 55 collocations, multi-word expressions or semantically related terms (Evert, 2008). From a technical point of view, the innovative aspect of our task is that we do not just look for co-occurrences between two items, but for co-occurrences in the context of a third element, i. e., we are interested in modifier– part pairs that are related when predicated of a certain concept. The method we apply to the extraction of modifier–part pairs when they co-occur with the target concept in a large window is similar to the idea of looking for partially untethered contextual patterns proposed by Garera and Yarowsky (2009), that extract name–pattern–property tuples where the pattern and</context>
</contexts>
<marker>Evert, 2008</marker>
<rawString>Stefan Evert. 2008. Corpora and collocations. In A. L¨udeling and M. Kyt¨o, editors, Corpus Linguistics: An International Handbook, pages 1212–1248. Mouton de Gruyter, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikesh Garera</author>
<author>David Yarowsky</author>
</authors>
<title>Structural, transitive and latent models for biographic fact extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>300--308</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="9489" citStr="Garera and Yarowsky (2009)" startWordPosition="1500" endWordPosition="1503">nd rank 55 collocations, multi-word expressions or semantically related terms (Evert, 2008). From a technical point of view, the innovative aspect of our task is that we do not just look for co-occurrences between two items, but for co-occurrences in the context of a third element, i. e., we are interested in modifier– part pairs that are related when predicated of a certain concept. The method we apply to the extraction of modifier–part pairs when they co-occur with the target concept in a large window is similar to the idea of looking for partially untethered contextual patterns proposed by Garera and Yarowsky (2009), that extract name–pattern–property tuples where the pattern and the property must be adjacent, but the target name is only required to occur in the same sentence. 3 Composite Parts in Feature Norms Our empirical starting point are the feature norms collected in parallel from 73 German and 69 Italian subjects by Kremer et al. (2008), following a methodology similar to that of McRae et al. (2005). The norms pertain to 50 concrete concepts from 10 classes such as mammals (e. g., dog), manipulable tools (e. g., comb), etc. The concept–part pairs in these norms served on the one hand as input to </context>
</contexts>
<marker>Garera, Yarowsky, 2009</marker>
<rawString>Nikesh Garera and David Yarowsky. 2009. Structural, transitive and latent models for biographic fact extraction. In Proceedings of EACL, pages 300–308, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Garrard</author>
<author>Matthew Lambon Ralph</author>
<author>John Hodges</author>
<author>Karalyn Patterson</author>
</authors>
<title>Prototypicality, distinctiveness, and intercorrelation: Analyses of the semantic attributes of living and nonliving concepts.</title>
<date>2001</date>
<journal>Cognitive Neuropsychology,</journal>
<volume>18</volume>
<issue>2</issue>
<contexts>
<context position="1120" citStr="Garrard et al., 2001" startWordPosition="163" endWordPosition="166">e our best performance by combining evidence about the association between the modifier and the part both within the context of the target concept and independently of it. We show that this performance is relatively stable across languages (Italian and German) and for production vs. perception of properties. 1 Introduction Subject-generated concept descriptions in terms of properties of different kinds (category: rabbits are mammals, parts: they have long ears, behaviour: they jump, ... ) are widely used in cognitive science as proxies to feature-based representations of concepts in the mind (Garrard et al., 2001; McRae et al., 2005; Vinson and Vigliocco, 2008). These feature norms (as collections of subject-elicited properties are called in the relevant literature) are used in simulations of cognitive tasks and experimental design. Moreover, vector spaces that have subject-generated properties as dimensions have been shown to be a good complement or alternative to traditional semantic models based on corpus collocates (Andrews et al., 2009; Baroni et al., 2010). Since the concept–property pairs in feature norms resemble the tuples that relation extraction algorithms extract from corpora (Hearst, 1992</context>
<context position="6242" citStr="Garrard et al. (2001)" startWordPosition="974" endWordPosition="977">s to harvest modifiers from a corpus and report the extraction experiments, whereas section 5 concludes by discussing directions for further work. 2 Related Work We are not aware of other attempts to extract concept-dependent modifiers of properties. We review instead related work in feature norm collection and prediction, and mention some relevant literature on the extraction of significant cooccurrences from corpora. Feature-based concept description norms have been collected in psychology for decades. Among the more recent publicly available norms of this sort, there are those collected by Garrard et al. (2001), Vinson and Vigliocco (2008) and McRae et al. (2005). The latter was the main methodological inspiration for the bilingual norms we rely on (see section 3 below). The norms of McRae and colleagues include descriptions of 541 concrete concepts corresponding to English nouns. The 725 subjects that rated these concepts had to list their features on a paper questionnaire. The produced features were then normalised and classified into categories such as part and function by the experimenters. The published norms include, among other kinds of information, the frequency of production of each feature</context>
</contexts>
<marker>Garrard, Ralph, Hodges, Patterson, 2001</marker>
<rawString>Peter Garrard, Matthew Lambon Ralph, John Hodges, and Karalyn Patterson. 2001. Prototypicality, distinctiveness, and intercorrelation: Analyses of the semantic attributes of living and nonliving concepts. Cognitive Neuropsychology, 18(2):25–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Adriana Badulescu</author>
<author>Dan Moldovan</author>
</authors>
<title>Automatic discovery of part-whole relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="3858" citStr="Girju et al., 2006" startWordPosition="595" endWordPosition="598"> and Singh, 2004), approximately 68,000 (36%) of the entries and 1,300 (32%) of the part entries being composites, respectively. In this paper, we tackle the problem of generating composite properties from corpus data by simplifying it in various ways. First, we focus on part properties only, because they are commonly encountered in feature norms, and because they are are commonly composite (cf. section 3). Second, we assume that an early step in the process of property extraction has already generated a list of simple parts, perhaps using an existing whole–part relation extraction algorithm (Girju et al., 2006). Finally, we focus on composite parts 54 Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 54–62, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics with an adjective–noun structure – together with numeral–noun cases, these constitute the near totality of composite parts in the norms described in section 3. Having thus delimited the scope of our exploration, we will adopt the following terminology: concept for the target nominal concept (rabbit), part for the (nominal) part property (ear) and modifier for the adj</context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2006</marker>
<rawString>Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2006. Automatic discovery of part-whole relations. Computational Linguistics, 32(1):83–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>539--545</pages>
<location>Nantes, France.</location>
<contexts>
<context position="1720" citStr="Hearst, 1992" startWordPosition="253" endWordPosition="254">et al., 2001; McRae et al., 2005; Vinson and Vigliocco, 2008). These feature norms (as collections of subject-elicited properties are called in the relevant literature) are used in simulations of cognitive tasks and experimental design. Moreover, vector spaces that have subject-generated properties as dimensions have been shown to be a good complement or alternative to traditional semantic models based on corpus collocates (Andrews et al., 2009; Baroni et al., 2010). Since the concept–property pairs in feature norms resemble the tuples that relation extraction algorithms extract from corpora (Hearst, 1992; Pantel and Pennacchiotti, 2006), recent research has attempted to extract feature-norm-like concept descriptions from corpora (Almuhareb, 2006; Baroni et al., 2010; Shaoul and Westbury, 2008). From a practical point of view, the success of this enterprise would mean being able to produce much larger norms without the need to resort to expensive and time-consuming elicitation experiments, leading to wider cognitive simulations and possibly better vector space models of semantics. From a theoretical point of view, a corpus-based system that produces human-like concept descriptions might provid</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of COLING, pages 539–545, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerhard Kremer</author>
<author>Andrea Abel</author>
<author>Marco Baroni</author>
</authors>
<title>Cognitively salient relations for multilingual lexicography.</title>
<date>2008</date>
<booktitle>In Proceedings of the COGALEX Workshop at COLING08,</booktitle>
<pages>94--101</pages>
<contexts>
<context position="9824" citStr="Kremer et al. (2008)" startWordPosition="1557" endWordPosition="1560">related when predicated of a certain concept. The method we apply to the extraction of modifier–part pairs when they co-occur with the target concept in a large window is similar to the idea of looking for partially untethered contextual patterns proposed by Garera and Yarowsky (2009), that extract name–pattern–property tuples where the pattern and the property must be adjacent, but the target name is only required to occur in the same sentence. 3 Composite Parts in Feature Norms Our empirical starting point are the feature norms collected in parallel from 73 German and 69 Italian subjects by Kremer et al. (2008), following a methodology similar to that of McRae et al. (2005). The norms pertain to 50 concrete concepts from 10 classes such as mammals (e. g., dog), manipulable tools (e. g., comb), etc. The concept–part pairs in these norms served on the one hand as input to our algorithm – on the other hand, its output (the set of selected modifiers from the corpus) could be evaluated against those modifiers that were produced by the subjects. Furthermore, the bilingual nature of the norms allows us to tune our algorithm on one language (German), and evaluate its performance on the other (Italian), to a</context>
</contexts>
<marker>Kremer, Abel, Baroni, 2008</marker>
<rawString>Gerhard Kremer, Andrea Abel, and Marco Baroni. 2008. Cognitively salient relations for multilingual lexicography. In Proceedings of the COGALEX Workshop at COLING08, pages 94–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Liu</author>
<author>Push Singh</author>
</authors>
<title>ConceptNet: A practical commonsense reasoning toolkit.</title>
<date>2004</date>
<journal>BT Technology Journal,</journal>
<pages>211--226</pages>
<contexts>
<context position="3256" citStr="Liu and Singh, 2004" startWordPosition="498" endWordPosition="501">c. Composite properties are not multi-word expressions in the usual sense. There is nothing special or idiomatic about long ears. It is just that we find it to be a remarkable fact about rabbits, worth stating in their description, that their ears are long. In the norms described in section 3, around one third of the part descriptions are composite. Note that while our focus is on feature norms, a similar point about the importance of composite properties could be made for other knowledge repositories of importance to computational linguistics, such as WordNet (Fellbaum, 1998) and ConceptNet (Liu and Singh, 2004), approximately 68,000 (36%) of the entries and 1,300 (32%) of the part entries being composites, respectively. In this paper, we tackle the problem of generating composite properties from corpus data by simplifying it in various ways. First, we focus on part properties only, because they are commonly encountered in feature norms, and because they are are commonly composite (cf. section 3). Second, we assume that an early step in the process of property extraction has already generated a list of simple parts, perhaps using an existing whole–part relation extraction algorithm (Girju et al., 200</context>
</contexts>
<marker>Liu, Singh, 2004</marker>
<rawString>Hugo Liu and Push Singh. 2004. ConceptNet: A practical commonsense reasoning toolkit. BT Technology Journal, pages 211–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken McRae</author>
<author>George Cree</author>
<author>Mark Seidenberg</author>
<author>Chris McNorgan</author>
</authors>
<title>Semantic feature production norms for a large set of living and nonliving things.</title>
<date>2005</date>
<journal>Behavior Research Methods,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="1140" citStr="McRae et al., 2005" startWordPosition="167" endWordPosition="170"> by combining evidence about the association between the modifier and the part both within the context of the target concept and independently of it. We show that this performance is relatively stable across languages (Italian and German) and for production vs. perception of properties. 1 Introduction Subject-generated concept descriptions in terms of properties of different kinds (category: rabbits are mammals, parts: they have long ears, behaviour: they jump, ... ) are widely used in cognitive science as proxies to feature-based representations of concepts in the mind (Garrard et al., 2001; McRae et al., 2005; Vinson and Vigliocco, 2008). These feature norms (as collections of subject-elicited properties are called in the relevant literature) are used in simulations of cognitive tasks and experimental design. Moreover, vector spaces that have subject-generated properties as dimensions have been shown to be a good complement or alternative to traditional semantic models based on corpus collocates (Andrews et al., 2009; Baroni et al., 2010). Since the concept–property pairs in feature norms resemble the tuples that relation extraction algorithms extract from corpora (Hearst, 1992; Pantel and Pennacc</context>
<context position="6295" citStr="McRae et al. (2005)" startWordPosition="983" endWordPosition="986">raction experiments, whereas section 5 concludes by discussing directions for further work. 2 Related Work We are not aware of other attempts to extract concept-dependent modifiers of properties. We review instead related work in feature norm collection and prediction, and mention some relevant literature on the extraction of significant cooccurrences from corpora. Feature-based concept description norms have been collected in psychology for decades. Among the more recent publicly available norms of this sort, there are those collected by Garrard et al. (2001), Vinson and Vigliocco (2008) and McRae et al. (2005). The latter was the main methodological inspiration for the bilingual norms we rely on (see section 3 below). The norms of McRae and colleagues include descriptions of 541 concrete concepts corresponding to English nouns. The 725 subjects that rated these concepts had to list their features on a paper questionnaire. The produced features were then normalised and classified into categories such as part and function by the experimenters. The published norms include, among other kinds of information, the frequency of production of each feature for a concept by the subjects. Almuhareb (2006) was </context>
<context position="9888" citStr="McRae et al. (2005)" startWordPosition="1568" endWordPosition="1571"> to the extraction of modifier–part pairs when they co-occur with the target concept in a large window is similar to the idea of looking for partially untethered contextual patterns proposed by Garera and Yarowsky (2009), that extract name–pattern–property tuples where the pattern and the property must be adjacent, but the target name is only required to occur in the same sentence. 3 Composite Parts in Feature Norms Our empirical starting point are the feature norms collected in parallel from 73 German and 69 Italian subjects by Kremer et al. (2008), following a methodology similar to that of McRae et al. (2005). The norms pertain to 50 concrete concepts from 10 classes such as mammals (e. g., dog), manipulable tools (e. g., comb), etc. The concept–part pairs in these norms served on the one hand as input to our algorithm – on the other hand, its output (the set of selected modifiers from the corpus) could be evaluated against those modifiers that were produced by the subjects. Furthermore, the bilingual nature of the norms allows us to tune our algorithm on one language (German), and evaluate its performance on the other (Italian), to assess its cross-lingual generalisation capability. To confirm th</context>
</contexts>
<marker>McRae, Cree, Seidenberg, McNorgan, 2005</marker>
<rawString>Ken McRae, George Cree, Mark Seidenberg, and Chris McNorgan. 2005. Semantic feature production norms for a large set of living and nonliving things. Behavior Research Methods, 37(4):547–559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>113--120</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="1753" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="255" endWordPosition="259">McRae et al., 2005; Vinson and Vigliocco, 2008). These feature norms (as collections of subject-elicited properties are called in the relevant literature) are used in simulations of cognitive tasks and experimental design. Moreover, vector spaces that have subject-generated properties as dimensions have been shown to be a good complement or alternative to traditional semantic models based on corpus collocates (Andrews et al., 2009; Baroni et al., 2010). Since the concept–property pairs in feature norms resemble the tuples that relation extraction algorithms extract from corpora (Hearst, 1992; Pantel and Pennacchiotti, 2006), recent research has attempted to extract feature-norm-like concept descriptions from corpora (Almuhareb, 2006; Baroni et al., 2010; Shaoul and Westbury, 2008). From a practical point of view, the success of this enterprise would mean being able to produce much larger norms without the need to resort to expensive and time-consuming elicitation experiments, leading to wider cognitive simulations and possibly better vector space models of semantics. From a theoretical point of view, a corpus-based system that produces human-like concept descriptions might provide cues of how humans themselves c</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In Proceedings of COLING-ACL, pages 113–120, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>An introduction to random indexing.</title>
<date>2005</date>
<note>http://www.sics.se/˜mange/ papers/RI_intro.pdf.</note>
<contexts>
<context position="20271" citStr="Sahlgren, 2005" startWordPosition="3299" endWordPosition="3300">e-) combined list were tested for their similarity by looking if they co-occur with the same relative frequency with the same set of nouns. In case of high similarity (in this respect) of a modifier to a single other modifier, or if the modifier was similar to a lot of modifiers, it should be re-ranked to a higher position. In more detail, a vector was created for each modifier, denoting its co-occurrence frequencies with each noun in the corpus within a window of 4 tokens (on the left side of the noun). Random indexing helped to reduce the vector dimensionality from 27,345 to 3,000 elements (Sahlgren, 2005). These vectors served for calculating the cosine distances between modifiers. Then, for each of the top 200 modifiers in the combined frequency rank list (covering 84% of the triples from the German norms), the cosine distance was calculated to each of the top 100 modifiers in the contextless rank list. A constant of 1 was added to each of the computed cosines, thus obtaining a quantity between 1 and 2. The original combined frequency value was multiplied by this quantity (thus leaving it unchanged when the original cosine was 0, increasing it otherwise). From the re-ranked list resulting fro</context>
</contexts>
<marker>Sahlgren, 2005</marker>
<rawString>Magnus Sahlgren. 2005. An introduction to random indexing. http://www.sics.se/˜mange/ papers/RI_intro.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyrus Shaoul</author>
<author>Chris Westbury</author>
</authors>
<title>Performance of HAL-like word space models on semantic clustering.</title>
<date>2008</date>
<booktitle>In Proceedings of the ESSLLI Workshop on Distributional Lexical Semantics,</booktitle>
<pages>42--46</pages>
<location>Hamburg, Germany.</location>
<contexts>
<context position="1913" citStr="Shaoul and Westbury, 2008" startWordPosition="278" endWordPosition="281">n simulations of cognitive tasks and experimental design. Moreover, vector spaces that have subject-generated properties as dimensions have been shown to be a good complement or alternative to traditional semantic models based on corpus collocates (Andrews et al., 2009; Baroni et al., 2010). Since the concept–property pairs in feature norms resemble the tuples that relation extraction algorithms extract from corpora (Hearst, 1992; Pantel and Pennacchiotti, 2006), recent research has attempted to extract feature-norm-like concept descriptions from corpora (Almuhareb, 2006; Baroni et al., 2010; Shaoul and Westbury, 2008). From a practical point of view, the success of this enterprise would mean being able to produce much larger norms without the need to resort to expensive and time-consuming elicitation experiments, leading to wider cognitive simulations and possibly better vector space models of semantics. From a theoretical point of view, a corpus-based system that produces human-like concept descriptions might provide cues of how humans themselves come up with such descriptions. However, the corpus-based models proposed for this task up to this point overlook the fact that subjects very often produce compo</context>
<context position="7576" citStr="Shaoul and Westbury (2008)" startWordPosition="1185" endWordPosition="1188">tures with text mining techniques. He computed precision and recall measures of various pattern-based feature extraction methods using Vinson and Vigliocco’s norms for 35 concepts as a gold standard. The best precision was around 16% at about 11% recall; maximum recall was around 60% with less than 2% precision, confirming how difficult the task is. Importantly for our purposes, Almuhareb removed the modifier from composite features before running the experiments (1 wheel converted to wheel), thus eschewing the main characteristic of subject-generated concept descriptions that we tackle here. Shaoul and Westbury (2008) and Baroni et al. (2010) used corpus-based semantic space models to predict the top 10 features of 44 concepts from the McRae norms. The best model (Baroni et al.’s Strudel) guesses on average 24% of the human-produced features, again confirming the difficulty of the task. And, again, the test set was pre-processed to remove modifiers of composite features, thus sidestepping the problem we want to deal with. It is worth remarking that, by removing modifiers, previous authors are making the task easier in terms of feature extraction procedure (because the algorithms only need to look for singl</context>
</contexts>
<marker>Shaoul, Westbury, 2008</marker>
<rawString>Cyrus Shaoul and Chris Westbury. 2008. Performance of HAL-like word space models on semantic clustering. In Proceedings of the ESSLLI Workshop on Distributional Lexical Semantics, pages 42–46, Hamburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Spence</author>
<author>Kimberly Owens</author>
</authors>
<title>Lexical co-occurrence and association strength.</title>
<date>1990</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>19</volume>
<issue>5</issue>
<contexts>
<context position="12435" citStr="Spence and Owens, 1990" startWordPosition="1988" endWordPosition="1991">erent parts in combination with one out of 62 (50) different modifiers, totalling to 229 (127) differently combined triples. 4 Experiments This section describes the approach we explored for ranking and extracting modifiers of composite parts and evaluates the performance of 6 different extraction methods in terms of the production norms. Acceptance rate data from a follow-up judgement experiment complete the evaluation. 4.1 Ranked Modifier Lists Based on the idea that the co-occurrence of words in a text corpus reflects to some extent how strong these words are associated in speakers’ minds (Spence and Owens, 1990), our extraction approach works on the lemmatised and POS-tagged German WaCky1 web corpus of about 1.2 billion tokens. Modifier–Part Frequencies Using the CQP2 tool, corpus frequencies were collected for all co-occurrences of adjectives with those part nouns that were produced in the experiment described above. A possible gap of up to 3 tokens between the pair of adjective and noun allowed to extract also adjectives that are not directly adjacent to the nouns in the corpus (but in a sequence of adjectives, for example). For each part noun, the 5 most frequent adjective modifiers from the ranke</context>
</contexts>
<marker>Spence, Owens, 1990</marker>
<rawString>Donald Spence and Kimberly Owens. 1990. Lexical co-occurrence and association strength. Journal of Psycholinguistic Research, 19(5):317–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vinson</author>
<author>Gabriella Vigliocco</author>
</authors>
<title>Semantic feature production norms for a large set of objects and events.</title>
<date>2008</date>
<journal>Behavior Research Methods,</journal>
<volume>40</volume>
<issue>1</issue>
<pages>190</pages>
<contexts>
<context position="1169" citStr="Vinson and Vigliocco, 2008" startWordPosition="171" endWordPosition="174">ce about the association between the modifier and the part both within the context of the target concept and independently of it. We show that this performance is relatively stable across languages (Italian and German) and for production vs. perception of properties. 1 Introduction Subject-generated concept descriptions in terms of properties of different kinds (category: rabbits are mammals, parts: they have long ears, behaviour: they jump, ... ) are widely used in cognitive science as proxies to feature-based representations of concepts in the mind (Garrard et al., 2001; McRae et al., 2005; Vinson and Vigliocco, 2008). These feature norms (as collections of subject-elicited properties are called in the relevant literature) are used in simulations of cognitive tasks and experimental design. Moreover, vector spaces that have subject-generated properties as dimensions have been shown to be a good complement or alternative to traditional semantic models based on corpus collocates (Andrews et al., 2009; Baroni et al., 2010). Since the concept–property pairs in feature norms resemble the tuples that relation extraction algorithms extract from corpora (Hearst, 1992; Pantel and Pennacchiotti, 2006), recent researc</context>
<context position="6271" citStr="Vinson and Vigliocco (2008)" startWordPosition="978" endWordPosition="981">from a corpus and report the extraction experiments, whereas section 5 concludes by discussing directions for further work. 2 Related Work We are not aware of other attempts to extract concept-dependent modifiers of properties. We review instead related work in feature norm collection and prediction, and mention some relevant literature on the extraction of significant cooccurrences from corpora. Feature-based concept description norms have been collected in psychology for decades. Among the more recent publicly available norms of this sort, there are those collected by Garrard et al. (2001), Vinson and Vigliocco (2008) and McRae et al. (2005). The latter was the main methodological inspiration for the bilingual norms we rely on (see section 3 below). The norms of McRae and colleagues include descriptions of 541 concrete concepts corresponding to English nouns. The 725 subjects that rated these concepts had to list their features on a paper questionnaire. The produced features were then normalised and classified into categories such as part and function by the experimenters. The published norms include, among other kinds of information, the frequency of production of each feature for a concept by the subject</context>
</contexts>
<marker>Vinson, Vigliocco, 2008</marker>
<rawString>David Vinson and Gabriella Vigliocco. 2008. Semantic feature production norms for a large set of objects and events. Behavior Research Methods, 40(1):183– 190.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>