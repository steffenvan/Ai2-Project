<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.980621">
Using SGML as a Basis for Data-Intensive NLP
</title>
<author confidence="0.999665">
David McKelvie, Chris Brew &amp; Henry Thompson
</author>
<affiliation confidence="0.999157">
Language Technology Group, Human Communication Research Centre,
University of Edinburgh, Edinburgh, Scotland
</affiliation>
<email confidence="0.950035">
David.McKelvie@ed.ac.uk &amp; Chris.Brew@ed.ac.uk &amp; H.Thompson@ed.ac.uk
</email>
<sectionHeader confidence="0.998218" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999993818181818">
This paper describes the LT NSL sys-
tem (McKelvie et al, 1996), an architec-
ture for writing corpus processing tools.
This system is then compared with two
other systems which address similar is-
sues, the GATE system (Cunningham et
al, 1995) and the IMS Corpus Workbench
(Christ, 1994). In particular we address
the advantages and disadvantages of an
SGML approach compared with a non-SGML
database approach.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975318181818">
The theme of this paper is the design of software
and data architectures for natural language process-
ing using corpora. Two major issues in corpus-based
NLP are: how best to deal with medium to large
scale corpora often with complex linguistic annota-
tions, and what system architecture best supports
the reuse of software components in a modular and
interchangeable fashion.
In this paper we describe the LT NSL system (McK-
elvie et al, 1996), an architecture for writing corpus
processing tools, which we have developed in an at-
tempt to address these issues. This system is then
compared with two other systems which address
some of the same issues, the GATE system (Cun-
ningham et al, 1995) and the IMS Corpus Work-
bench (Christ, 1994). In particular we address the
advantages and disadvantages of an SGML approach
compared with a non-SGML database approach. Fi-
nally, in order to back up our claims about the merits
of SGML-based corpus processing, we present a num-
ber of case studies of the use of the LT NSL system
for corpus preparation and linguistic analysis.
</bodyText>
<sectionHeader confidence="0.940504" genericHeader="method">
2 The LT NSL system
</sectionHeader>
<bodyText confidence="0.556379">
LT NSL is a tool architecture for SGML-based pro-
</bodyText>
<page confidence="0.984172">
229
</page>
<bodyText confidence="0.996607978021978">
cessing of (primarily) text corpora. It generalises
the UNIX pipe architecture, making it possible to
use pipelines of general-purpose tools to process an-
notated corpora. The original UNIX architecture al-
lows the rapid construction of efficient pipelines of
conceptually simple processes to carry out relatively
complex tasks, but is restricted to a simple model of
streams as sequences of bytes, lines or fields. LT NSL
lifts this restriction, allowing tools access to streams
which are sequences of tree-structured text (a repre-
sentation of SGML marked-up text).
The use of SGML as an I/O stream format between
programs has the advantage that SGML is a well de-
fined standard for representing structured text. Its
value is precisely that it closes off the option of a
proliferation of ad-hoc notations and the associated
software needed to read and write them. The most
important reason why we use SGML for all corpus lin-
guistic annotation is that it forces us to formally de-
scribe the markup we will be using and provides soft-
ware for checking that these markup invariants hold
in an annotated corpus. In practise this is extremely
useful. SGML is human readable, so that interme-
diate results can be inspected and understood. It
also means that it is easy for programs to access the
information which is relevant to them, while ignor-
ing additional markup. A further advantage is that
many text corpora are available in SGML, for exam-
ple, the British National Corpus (Burnage&amp;Dunlop,
1992).
The LT NSL system is released as C source code.
The software consists of a C-language Application
Program Interface (API) of function calls, and a num-
ber of stand-alone programs which use this API. The
current release is known to work on UNIX (SunOS
4.1.3, Solaris 2.4 and Linux), and a Windows-NT
version will be released during 1997. There is also
an API for the Python programming language.
One question which arises in respect to using
SGML as an I/O format is: what about the cost of
parsing SGML? Surely that makes pipelines too in-
efficient? Parsing SGML in its full generality, and
providing validation and adequate error detection
is indeed rather hard. For efficiency reasons, you
wouldn&apos;t want to use long pipelines of tools, if each
tool had to reparse the SGML and deal with the
full language. Fortunately, LT NSL doesn&apos;t require
this. The first stage of processing normalises the
input, producing a simplified, but informationally
equivalent form of the document. Subsequent tools
can and often will use the LT NSL API which parses
normalised SGML (henceforth NSGML) approximately
ten times more efficiently than the best parsers for
full SGML. The API then returns this parsed SGML
to the calling program as data-structures.
NSGML is a fully expanded text form of SGML in-
formationally equivalent to the ESIS output of SGML
parsers. This means that all markup minimisation
is expanded to its full form, SGML entities are ex-
panded into their value (except for SDATA entities),
and all SGML names (of elements, attributes, etc) are
normalised. The result is a format easily readable by
humans and programs.
The LT NSL programs consist of mknsg, a program
for converting arbitrary valid SGML into normalised
sGmL1, the first stage in a pipeline of LT NSL tools;
and a number of programs for manipulating nor-
malised SGML files, such as sggrep which finds SGML
elements which match some query. Other of our soft-
ware packages such as LT POS (a part of speech tag-
ger) and LT WB (Mikheev&amp;Finch, 1997) also use the
LT NSL library.
In addition to the normalised SGML, the mknsg
program writes a file containing a compiled form
of the Document Type Definition (DTD)2, which
LT NSL programs read in order to know what the
structure of their Nscrsa, input or output is.
How fast is it? Processes requiring sequential ac-
cess to large text corpora are well supported. It is
unlikely that LT NSL will prove the rate limiting step
in sequential corpus processing. The kinds of re-
peated search required by lexicographers are more
of a problem, since the system was not designed
for that purpose. The standard distribution is fast
enough for use as a search engine with files of up to
several million words. Searching 1% of the British
National Corpus (a total of 700,000 words (18 Mb))
is currently only 6 times slower using LT Nsi, sggrep
than using fgrep, and sggrep allows more complex
structure-sensitive queries. A prototype indexing
mechanism (Mikheev&amp;McKelvie, 1997), not yet in
</bodyText>
<footnote confidence="0.957947">
1Based on James Clark&apos;s SP parser (Clark, 1996).
2SGML&apos;s way of describing the structure (or grammar)
of the allowed markup in a document
</footnote>
<bodyText confidence="0.999801090909091">
the distribution, improves the performance of LT NSL
to acceptable levels for much larger datasets.
Why did we say &amp;quot;primarily for text corpora&amp;quot;? Be-
cause much of the technology is directly applicable
to multimedia corpora such as the Edinburgh Map
Task corpus (Anderson et al, 1991). There are tools
which interpret SGML elements in the corpus text as
offsets into files of audio-data, allowing very flexi-
ble retrieval and output of audio information using
queries defined over the corpus text and its annota-
tions. The same could be done for video clips, etc.
</bodyText>
<subsectionHeader confidence="0.999552">
2.1 Hyperlinking
</subsectionHeader>
<bodyText confidence="0.999857037037037">
We are inclined to steer a middle course between
a monolithic comprehensive view of corpus data, in
which all possible views, annotations, structurings
etc. of a corpus component are combined in a sin-
gle heavily structured document, and a massively
decentralised view in which a corpus component is
organised as a hyper-document, with all its informa-
tion stored in separate documents, utilising inter-
document pointers. Aspects of the LT NSL library
are aimed at supporting this approach. It is neces-
sary to distinguish between files, which are storage
units, (SGML) documents, which may be composed
of a number of files by means of external entity ref-
erences, and hyper-documents, which are linked en-
sembles of documents, using e.g. HyTime or TEl
(Sperberg-McQueen&amp;Burnard, 1994) link notation.
The implication of this is that corpus compo-
nents can be hyper-documents, with low-density (i.e.
above the token level) annotation being expressed in-
directly in terms of links. In the first instance, this
is constrained to situations where element content
at one level of one document is entirely composed
of elements from another document. Suppose, for
example, we already had segmented a file resulting
in a single document marked up with SGML headers
and paragraphs, and with the word segmentation
marked with &lt;w&gt; tags:
</bodyText>
<construct confidence="0.6426008">
&lt;p id=p4&gt;
&lt;w id=p4.1,1&gt;Timegw&gt;
&lt;1i id=p4.w2&gt;flies&lt;/w&gt;
&lt;II id=p4.113&gt;.&lt;/w&gt;
&lt;/p&gt;
</construct>
<bodyText confidence="0.968517">
The output of a phrase-level segmentation might
then be stored as follows:
</bodyText>
<subsectionHeader confidence="0.346548">
&lt;p id=p4&gt;
</subsectionHeader>
<bodyText confidence="0.385039666666667">
&lt;phr id=p4.phl type=n doc=filel from=&apos;id p4.wl&apos;&gt;
&lt;phr id=p4.ph2 type=v from=&apos;id p4.1721&gt;
&lt;/p&gt;
</bodyText>
<page confidence="0.993188">
230
</page>
<bodyText confidence="0.973640796296296">
Linking is specified using one of the available TEl
mechanisms. Details are not relevant here, suffice it
to say that doc=f ilel resolves to the word level file
and establishes a default for subsequent links. At
a minimum, links are able to target single elements
or sequences of contiguous elements. LT NSL imple-
ments a textual inclusion semantics for such links, in-
serting the referenced material as the content of the
element bearing the linking attributes. Although the
example above shows links to only one document, it
is possible to link to several documents, e.g. to a
word document and a lexicon document:
&lt;word&gt;
&lt;source doc=filel from=&apos;id p4.w1&apos;&gt;
&lt;lex doc=lexl from=did lex.40332&apos;&gt;
&lt;/word&gt;
Note that the architecture is recursive, in that
e.g. sentence-level segmentation could be expressed
in terms of links into the phrase-level segmentation
as presented above.
The data architecture needs to address not only
multiple levels of annotation but also alternative ver-
sions at a given level. Since our linking mechanism
uses the SGML entity mechanism to implement the
identification of target documents, we can use the
entity manager&apos;s catalogue as a means of managing
versions. For our example above, this means that the
connection between the phrase encoding document
and the segmented document would be in two steps:
the phrase document would use a PUBLIC identi-
fier, which the catalogue would map to the particular
file. Since catalogue entries are interpreted by tools
as local to the directory where the catalogue itself
is found, this means that binding together groups of
alternative versions can be easily achieved by storing
them under the same directory.
Subdirectories with catalogue fragments can thus
be used to represent both increasing detail of anno-
tation and alternatives at a given level of annotation.
Note also that with a modest extension of func-
tionality, it is possible to use the data architecture
described here to implement patches, e.g. to the to-
kenisation process. If alongside an inclusion seman-
tics, we have a special empty element &lt;repl&gt; which
is replaced by the range it points to, we can produce
a patch file, e.g. for a misspelled word, as follows
(irrelevant details omitted):
&lt;nsl&gt;
&lt;!-- to get the original header--&gt;
&lt;repl doc=original from=&apos;id hdrl&apos;&gt;
&lt;text&gt;
&lt;!-- the first swatch of unchanged text --&gt;
&lt;repl from=&apos;id p1&apos; to=&apos;id p324&apos;&gt;
&lt;!-- more unchanged text --&gt;
</bodyText>
<figure confidence="0.949878461538462">
&lt;p id=p325&gt;
&lt;repl from=&apos;id p325.t1&apos; to=&apos;id p325.t15&apos;&gt;
&lt;!-- the correction itself --&gt;
&lt;corr sic=&apos;procede&apos; resp=&apos;ispell&apos;&gt;
&lt;token id=p325.t16&gt;proceed&lt;/token&gt;
&lt;/corr&gt;
&lt;!-- more unchanged text--&gt;
&lt;repl from=&apos;id p325 t17&apos; to=&apos;id p325.t96&apos;&gt;
&lt;/p&gt;
&lt;!-- the rest of the unchanged text--&gt;
&lt;repl from=&apos;id p326&apos; to=&apos;id p402&apos;&gt;
&lt;/text&gt;
&lt;/usl&gt;
</figure>
<bodyText confidence="0.9990865">
Whether such a patch would have knock-on effects
on higher levels of annotation would depend, inter
alia, on whether a change in tokenisation crossed any
higher-level boundaries.
</bodyText>
<subsectionHeader confidence="0.807862">
2.2 sggrep and the LT NSL query language
</subsectionHeader>
<bodyText confidence="0.999995034482759">
The API provides the program(mer) with two alter-
native views of the NSGML stream: an object stream
view and a tree fragment view. The first, lower level
but more efficient, provides data structures and ac-
cess functions such as GetNextBit and PrintBit,
where there are different types of Bits for start (or
empty) tags with their attributes, text content, end
tags, and a few other bits and pieces.
The alternative, higher level, view, lets one
treat the NSGML input as a sequence of tree-
fragments. The API provides functions GetNextItem
and Pr int Item to read and write the next com-
plete SGML element. It also provides functionality
GetNextQueryElement ( inf ile , query , subquery,
regexp,outfile) where query is an LT NSL query
which allows one to specify particular elements on
the basis of their position in the document struc-
ture and their attribute values. The subquery and
regexp allow one to specify that the matching ele-
ment has a subelement matching the subquery with
text content matching the regular expression. El-
ements which do not match the query are passed
through unchanged to outf ile. Under both mod-
els, processing is essentially a loop over calls to the
API, in each case choosing to discard, modify or out-
put unchanged each Bit or Element.
Rather than define the query language here (de-
tails can be found in (McKelvie et al, 1996)), we will
just provide an example. The call
</bodyText>
<subsectionHeader confidence="0.409185">
GetNextQueryElement(iuf,&amp;quot;.*/TEXT/.*/P&amp;quot;,
</subsectionHeader>
<bodyText confidence="0.917262285714286">
&amp;quot;P/.*/S&amp;quot;,&amp;quot;th(eilie)r&amp;quot;, outf)
would return the next &lt;P&gt; element dominated
anywhere by &lt;TEXT&gt; at any depth, with the &lt;P&gt;
element satisfying the additional requirement that
it contain at least one &lt;S&gt; element at any depth
with text containing at least one instance of &apos;their&apos;
(possibly misspelt).
</bodyText>
<page confidence="0.994881">
231
</page>
<sectionHeader confidence="0.961832" genericHeader="method">
3 Comparisons with other systems
</sectionHeader>
<bodyText confidence="0.999959571428572">
The major alternative corpus architecture which has
been advocated is a database approach, where anno-
tations are kept separately from the base texts. The
annotations are linked to the base texts either by
means of character offsets or by a more sophisticated
indexing scheme. We will discuss two such systems
and compare them with the LT NSL approach.
</bodyText>
<subsectionHeader confidence="0.965315">
3.1 GATE
</subsectionHeader>
<bodyText confidence="0.999599">
The GATE system (Cunningham et al, 1995),
currently under development at the University of
Sheffield, is a system to support modular language
engineering.
</bodyText>
<subsectionHeader confidence="0.559206">
3.1.1 System components
</subsectionHeader>
<bodyText confidence="0.984395">
It consists of three main components:
</bodyText>
<listItem confidence="0.984303214285714">
• GDM - an object oriented database for stor-
ing information about the corpus texts. This
database is based on the TIPSTER document
architecture (Grishman, 1995), and stores text
annotations separate from the texts. Annota-
tions are linked to texts by means of character
offsets3.
• Creole - A library of program and data resource
wrappers, that allow one to interface externally
developed programs/resources into the GATE
architecture.
• GGI - a graphical tool shell for describing pro-
cessing algorithms and viewing and evaluating
the results.
</listItem>
<bodyText confidence="0.995153666666667">
A MUC-6 compatible information extraction sys-
tem, VIE, has been built using the GATE architec-
ture.
</bodyText>
<subsectionHeader confidence="0.556878">
3.1.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.994480222222222">
Separating corpus text from annotations is a gen-
eral and flexible method of describing arbitrary
structure on a text. It may be less useful as a means
of publishing corpora and may prove inefficient if the
underlying corpus is liable to change.
Although TIPSTER lets one define annotations
and their associated attributes, in the present ver-
sion (and presumably also in GATE) these defini-
tions are treated only as documentation and are
not validated by the system. In contrast, the SGML
parser validates its DTD, and hence provides some
check that annotations are being used in their in-
tended way. SGML has the concept of content mod-
els which restrict the allowed positions and nesting
3More precisely, by inter byte locations.
of annotations. GATE allows any annotation any-
where. Although this is more powerful, i.e. one is not
restricted to tree structures, it does make validation
of annotations more difficult.
The idea of having formalised interfaces for exter-
nal programs and data is a good one.
The GGI graphical tool shell lets one build,
store, and recover complex processing specifications.
There is merit in having a high level language to
specify tasks which can be translated automatically
into executable programs (e.g. shell scripts). This is
an area that LT NSL does not address.
</bodyText>
<subsubsectionHeader confidence="0.549367">
3.1.3 Comparison with LT NSL
</subsubsectionHeader>
<bodyText confidence="0.99628025">
In (Cunningham et al, 1996), the GATE archi-
tecture is compared with the earlier version of the
LT NSL architecture which was developed in the
MULTEXT project. We would like to answer these
points with reference to the latest version of our soft-
ware.
. It is claimed that using normalised SGML implies
a large storage overhead. Normally however, nor-
malised SGML will be created on the fly and passed
through pipes and only the final results will need to
be stored. This may however be a problem for very
large corpora such as the BNC.
It is stated that representing ambiguous or over-
lapping markup is complex in SGML. We do not
agree. One can represent overlapping markup in
SGML in a number of ways. As described above,
it is quite possible for SGML to represent &apos;stand-off&apos;
annotation in a similar way to TIPSTER. LT NSL
provides the hyperlinking semantics to interpret this
SGML.
The use of normalised SGML and a compiled DTD
file means that the overheads of parsing SGML in
each program are small, even for large DTDs, such
as the TEI.
LT NSL is not specific to particular applications
or DTDs. The MULTEXT architecture was tool-
specific, in that its API defined a predefined set of ab-
stract units of linguistic interest, words, sentences,
etc. and defined functions such as ReadSentence.
That was because MULTEXT was undecided about
the format of its I/O. LT NSL in contrast, since we
have decided on SGML as a common format, provides
functions such as GetNext Item which read the next
SGML element. Does this mean the LT NSL architec-
ture is application neutral? Yes and no.
Yes, because there is in principle no limit on what
can be encoded in an SGML document. In the TIP-
STER architecture there is an architectural require-
ment that all annotations be ultimately associated
with spans of a single base text, but LT NSL imposes
</bodyText>
<page confidence="0.986823">
232
</page>
<bodyText confidence="0.999961517857143">
no such requirement. This makes it easier to be clear
about what happens when a different view is needed
on fixed-format read-only information, or when it
turns out that the read-only information should be
systematically corrected. The details of this are a
matter of ongoing research, but an important moti-
vation for the architecture of LT NSL is to allow such
edits without requiring that the read-only informa-
tion be copied.
No, because in practice any corpus is encoded in a
way which reflects the assumptions of the corpus de-
velopers. Most corpora include a level of representa-
tion for words, and many include higher level group-
ings such as breath groups, sentences, paragraphs
and/or documents. The sample back-end tools dis-
tributed with LT NSL reflect this fact.
It is claimed that there is no easy way in SGML to
differentiate sets of results by who or what produced
them. But, to do this requires only a convention for
the encoding of meta-information about text cor-
pora. For example, SGML DTDs such as the TEl
include a &apos;resp&apos; attribute which identifies who was
responsible for changes. LT NSL does not require
tools to obey any particular conventions for meta-
information, but once a convention is fixed upon it
is straightforward to encode the necessary informa-
tion as SGML attributes.
Unlike TIPSTER, LT NSL is not built around a
database, so we cannot take advantage of built-in
mechanisms for version control. As far as corpus
annotation goes, UNIX rcs, has proved an adequate
solution to our version control needs. Alternatively,
version control can be provided by means of hyper-
linking.
The GATE idea of providing formal wrappers for
interfacing programs is a good one. In LT NSL
the corresponding interfaces are less formalised, but
can be defined by specifying the DTDs of a pro-
gram&apos;s input and output files. For example a part-
of-speech tagger would expect &lt;W&gt; elements inside
&lt;S&gt; elements, and a &apos;TAG&apos; attribute on the output
&lt;W&gt; elements. Any input file whose DTD satisfied
this constraint could be tagged. SGML architectural
forms (a method for DTD subsetting) could provide
a method of formalising these program interfaces.
As Cunningham et. al. say, there is no reason why
there could not be an implementation of LT NSL
which read SGML elements from a database rather
than from files. Similarly, a TIPSTER architecture
like GATE could read SGML and convert it into its
internal database. In that case, our point would
be that SGML is a suitable abstraction for programs
rather than a more abstract (and perhaps more lim-
ited) level of interface. We are currently in discus-
sion with the GATE team about how best to allow
the interoperability of the two systems.
</bodyText>
<subsectionHeader confidence="0.997041">
3.2 The IMS Corpus Workbench
</subsectionHeader>
<bodyText confidence="0.999977">
The IMS Corpus Workbench (Christ, 1994) includes
both a query engine (CQP) and a Motif-based user
visualisation tool (xkwic). CQP provides a query lan-
guage which is a conservative extension of famil-
iar UNIX regular expression facilities4. XKWIC is a
user interface tuned for corpus search. As well as
providing the standard keyword-in-context facilities
and giving access to the query language it gives the
user sophisticated tools for managing the query his-
tory, manipulating the display, and storing search
results. The most interesting points of comparison
with LT NSL are in the areas of query language and
underlying corpus representation.
</bodyText>
<sectionHeader confidence="0.4825" genericHeader="method">
3.2.1 The CQP model
</sectionHeader>
<bodyText confidence="0.9998754">
CQP treats corpora as sequences of attribute-value
bundles. Each attribute5 can be thought of as a total
function from corpus positions to attribute values.
Syntactic sugar apart, no special status is given to
the attribute word.
</bodyText>
<subsectionHeader confidence="0.69193">
3.2.2 The query language
</subsectionHeader>
<bodyText confidence="0.977196260869565">
The query language of IMS-CWB, which has the
usual regular expression operators, works uniformly
over both attribute values and corpus positions.
This regularity is a clear benefit to users, since only
one syntax must be learnt.
Expressions of considerable sophistication can be
generated and used successfully by beginners. Con-
sider:
[pos=&amp;quot;DT&amp;quot; &amp; word !=&amp;quot;the&apos;ll [pos=&amp;quot;JJ.*&amp;quot;]?
[pos=&amp;quot;N.+&amp;quot;]
This means, in the context of the Penn treebank
tagset, &amp;quot;Find me sequences beginning with deter-
miners other than the, followed by optional adjec-
tives, then things with nominal qualities&amp;quot;. The in-
tention is presumably to find a particular sub-class
of noun-phrases.
The workbench has plainly achieved an extremely
successful generalisation of regular expressions, and
one which has been validated by extensive use in
lexicography and corpus-building.
There is only limited access to structural infor-
mation. While it is possible, if sentence boundaries
are marked in the corpus, to restrict the search to
</bodyText>
<footnote confidence="0.9978755">
4Like LT NSL ims-cwa is built on top of Henry
Spencer&apos;s public domain regular expression package
51n CQP terminology these are the &amp;quot;positional
attributes&amp;quot;.
</footnote>
<page confidence="0.99835">
233
</page>
<bodyText confidence="0.9997145">
within-sentence matches, there are few facilities for
making more refined use of hierarchical structure.
The typical working style, if you are concerned with
syntax, is to search for sequences of attributes which
you believe to be highly correlated with particular
syntactic structures.
</bodyText>
<subsectionHeader confidence="0.78026">
3.2.3 Data representation
</subsectionHeader>
<bodyText confidence="0.999948666666667">
CQP requires users to transform the corpora which
will be searched into a fast internal format. This
format has the following properties:
</bodyText>
<listItem confidence="0.825562090909091">
• Because of the central role of corpus position it
is necessary to tokenise the input corpus, map-
ping each word in the raw input to a set of at-
tribute value pairs and a corpus position.
• There is a logically separate index for each at-
tribute name in the corpus.
• CQP uses an integerised representation, in which
corpus items having the same value for an at-
tribute are mapped into the same integer de-
scriptor in the index which represents that at-
tribute. This means that the character data cor-
responding to each distinct corpus token need
only be stored once.
• For each attribute there is an item list contain-
ing the sequence of integer descriptors corre-
sponding to the sequence of words in the corpus.
Because of the presence of this list the storage
cost of adding a new attribute is linear in the
size of the corpus. If the new attribute were
sparse, it would be possible to reduce the space
cost by switching (for that attribute) to a more
space efficient encoding6
</listItem>
<subsectionHeader confidence="0.861777">
3.2.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.9997316">
The IMS-CWB is a design dominated by the need
for frequent fast searches of a corpus with a fixed an-
notation scheme. Although disk space is now cheap,
the cost of preparing and storing the indices for IMS-
CWB is such that the architecture is mainly appro-
priate for linguistic and lexicographic exploration,
but less immediately useful in situations, such as
obtain in corpus development, where there is a re-
curring need to experiment with different or evolving
attributes and representational possibilities.
Some support is provided for user-written tools,
but as yet there is no published API to the poten-
tially very useful query language facilities. The in-
dexing tools which come with IMS-CWB are less flexi-
ble than those of LT NSL since the former must index
</bodyText>
<footnote confidence="0.677991333333333">
6IMS-CWB already supports compressed index files,
and special purpose encoding formats would presumably
save even more space.
</footnote>
<bodyText confidence="0.999656363636363">
on words, while the latter can index on any level of
the corpus annotation.
The query language of IMS-CWB is an elegant and
orthogonal design, which we believe it would be ap-
propriate to adopt or adapt as a standard for corpus
search. It stands in need of extension to provide
more flexible access to hierarchical structure7. The
query language of LT NSL is one possible template
for such extensions, as is the opaque but powerful
tgrep program (Pito, 1994) which is provided with
the Penn Treebank.
</bodyText>
<sectionHeader confidence="0.970296" genericHeader="method">
4 Case studies
</sectionHeader>
<subsectionHeader confidence="0.999988">
4.1 Creation of marked-up corpora
</subsectionHeader>
<bodyText confidence="0.999985741935484">
One application area where the paradigm of sequen-
tial adding of markup to an SGML stream fits very
closely, is that of the production of annotated cor-
pora. Marking of major sections, paragraphs and
headings, word tokenising, sentence boundary mark-
ing, part of speech tagging and parsing are all tasks
which can be performed sequentially using only a
small moving window of the texts. In addition, all
of them make use of the markup created by earlier
steps. If one is creating an annotated corpus for pub-
lic distribution, then SGML is (probably) the format
of choice and thus an SGML based NLP system such
as LT NSL will be appropriate.
Precursors to the LT NSL software were used to an-
notate the MLCC corpora used by the MULTEXT
project. Similarly LT NSL has been used to recode
the Edinburgh MapTask corpus into SGML markup,
a process which showed up a number of inconsis-
tencies in the original (non-SGML) markup. Because
LT NSL allows the use of multiple I/O files (with dif-
ferent DTDs), in (Brew&amp;McKelvie, 1996) it was pos-
sible to apply these tools to the task of finding trans-
lation equivalencies between English and French.
Using part of the MLCC corpus, part-of-speech
tagged and sentence aligned using LT NSL tools, they
explored various techniques for finding word align-
ments. The LT NSL programs were useful in eval-
uating these techniques. See also (Mikheev&amp;Finch,
1995), (Mikheev&amp;Finch, 1997) for other uses of the
LT NSL tools in annotating linguistic structures of
interest and extracting statistics from that markup.
</bodyText>
<subsectionHeader confidence="0.999215">
4.2 Transformation of corpus markup
</subsectionHeader>
<bodyText confidence="0.960526">
Although SGML is human readable, in practice once
the amount of markup is of the same order of magni-
</bodyText>
<footnote confidence="0.9646308">
7This may be a specialised need of academic linguists,
and for many applications it is undoubtedly more im-
portant to provide clean facilities for non-hierarchical
queries but it seems premature to close off the option
of such access.
</footnote>
<page confidence="0.996623">
234
</page>
<bodyText confidence="0.9999783">
tude as the textual content, reading SGML becomes
difficult. Similarly, editing such texts using a normal
text editor becomes tedious and error prone. Thus
if one is committed to the use of SGML for corpus-
based NLP, then one needs to have specialised soft-
ware to facilitate the viewing and editing of SGML.
A similar problem appears in the database approach
to corpora, where the difficulty is not in seeing the
original text, but in seeing the markup in relation-
ship to the text.
</bodyText>
<subsectionHeader confidence="0.76171">
4.2.1 Batch transformations
</subsectionHeader>
<bodyText confidence="0.999980909090909">
To address this issue LT NSL includes a num-
ber of text based tools for the conversion of SGML:
textonly, sgraltrans and sgrpg. With these tools
it is easy to select portions of text which are of inter-
est (using the query language) and to convert them
into either plain text or another text format, such as
IATEX or HTML. In addition, there are a large num-
ber of commercial and public domain software pack-
ages for transforming SGML. In the future, however,
the advent of the DSSSL transformation language will
undoubtably revolutionise this area.
</bodyText>
<subsubsectionHeader confidence="0.693368">
4.2.2 Hand correction
</subsubsectionHeader>
<bodyText confidence="0.999946">
Specialised editors for SGML are available, but
they are not always exactly what one wants, because
they are too powerful, in that they let all markup
and text be edited. What is required for markup
correction are specialised editors which only allow a
specific subset of the markup to be edited, and which
provide an optimised user interface for this limited
set of edit operations.
In order to support the writing of specialised ed-
itors, we have developed a Python (vanRossum,
1995) API for LT NSL, (Tobin&amp;McKelvie, 1996). This
allows us to rapidly prototype editors using the
Python/Tk graphics package. These editors can fit
into a pipeline of LT NSL tools allowing hand cor-
rection or disambiguation of markup automatically
added by previous tools. Using this API we are devel-
oping a generic SGML editor. It is an object-oriented
system where one can flexibly associate display and
interaction classes to particular SGML elements. Al-
ready, this generic editor has been used for a number
of tasks; the hand correction of part-of-speech tags
in the MapTask, the correction of turn boundaries
in the Innovation corpus (Carletta et al, 1996), and
the evaluation of translation equivalences between
aligned multilingual corpora.
We found that using this generic editor framework
made it possible to quickly write new editors for new
tasks on new corpora.
</bodyText>
<sectionHeader confidence="0.999411" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.998669333333333">
SGML is a good markup language for base level an-
notations of published corpora. Our experience with
LT NSL has shown that:
</bodyText>
<listItem confidence="0.967200238095238">
• It is a good system for sequential corpus pro-
cessing where there is locality of reference.
• It provides a modular architecture which does
not require a central database, thus allowing
distributed software development and reuse of
components.
• It works with existing corpora without extensive
pre-processing.
• It does support the Tipster approach of sep-
arating base texts from additional markup by
means of hyperlinks. In fact SGML (HyTime)
allows much more flexible addressing, not just
character offsets. This is of benefit when work-
ing with corpora which may change.
LT NSL is not so good for:
• Applications which require a database ap-
proach, i.e. those which need to access markup
at random from a text, for example lexico-
graphic browsing or the creation of book in-
dexes.
• Processing very large plain text or unnormalised
</listItem>
<bodyText confidence="0.967123846153846">
SGML corpora, where indexing is required, and
generation of normalised files is a large over-
head. We are working on extending LT NSL in
this direction, e.g. to allow processing of the
BNC corpus in its entirety.
In conclusion, the SGML and database approaches
are optimised for different NLP applications and
should be seen as complimentary rather than as con-
flicting. There is no reason why one should not at-
tempt to use the strengths of both the database and
the SGML stream approaches. It is recommended
that future work should include attention to allow-
ing interfacing between both approaches.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999872625">
This work was carried out at the Human Commu-
nication Research Centre, whose baseline funding
comes from the UK Economic and Social Research
Council. The LT NSL work began in the context of
the LRE project MULTEXT with support from the
European Union. It has benefited from discussions
with other MULTEXT partners, particularly ISSCO
Geneva, and drew on work at our own institution by
</bodyText>
<page confidence="0.991871">
235
</page>
<bodyText confidence="0.930254333333333">
Steve Finch and Andrei Mikheev. We also wish to
thank Hamish Cunningham and Oliver Christ for
useful discussions.
</bodyText>
<sectionHeader confidence="0.990993" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9991374">
A. H. Anderson, M. Bader, E. G. Bard, E. H.
Boyle, G. M. Doherty, S. C. Garrod, S. D. Isard,
J. C. Kowtko, J. M. McAllister, J. Miller, C. F.
Sotillo, H. S. Thompson, and R. Weinert. The
HCRC Map Task Corpus. Language and Speech,
34(4):351-366, 1991.
C. Brew and D. McKelvie. 1996. &amp;quot;Word-pair extrac-
tion for lexicography&amp;quot;. In Proceedings of NeM-
LaP&apos;96, pp 45-55, Ankara, Turkey.
G. Burnage and D. Dunlop. 1992. &amp;quot;Encoding
the British National Corpus&amp;quot;. In 13th Interna-
tional Conference on English Language research
on computerised corpora, Nijmegen. Available at
http://www.sil.org/sgra/bnc-encoding2 . html
See also http://info.ox.ac.uk/bnc/
J. Carletta, H. Fraser-Krauss and S. Garrod. 1996.
&amp;quot;An Empirical Study of Innovation in Manufac-
turing Teams: a preliminary report&amp;quot;. In Proceed-
ings of the International Workshop on Commu-
nication Modelling (LAP-96), ed. J. L. G. Dietz,
Springer-Verlag, Electronic Workshops in Com-
puting Series.
0. Christ. 1994. &amp;quot;A modular and flexible archi-
tecture for an integrated corpus query system&amp;quot;.
In Proceedings of COMPLEX &apos;94: 3rd Conference
on Computational Lexicography and Text Research
(Budapest, July 7-10, 1994), Budapest, Hungary.
CMP-LG archive id 9408005
J. Clark. 1996 &amp;quot;SP: An SGML System Conforming
to International Standard ISO 8879 - Standard
Generalized Markup Language&amp;quot;. Available from
http://www.jclark.com/sp/index.htm.
H. Cunningham, Y. Wilks and R. J. Gaizauskas.
1996. &amp;quot;New Methods, Current Trends and Soft-
ware Infrastructure for NLP&amp;quot;. In Proceedings of
the Second Conference on New Methods in Lan-
guage Processing, pages 283-298, Ankara, Turkey,
March.
H. Cunningham, R. Gaizauslcas and Y. Wilks. 1995.
&amp;quot;A General Architecture for Text Engineering
(GATE) - a new approach to Language Engineer-
ing R&amp;D&amp;quot;. Technical Report, Dept of Computer
Science, University of Sheffield. Available from
http: //www .des . shef .ac.uk/research/groups
/nip/gate/
</reference>
<bodyText confidence="0.763246970588235">
R. Grishman. 1995. &amp;quot;TIPSTER Phase II
Architecture Design Document Version 1.52&amp;quot;.
Technical Report, Dept. of Computer Sci-
ence, New York University. Available at
http://www.cs.nyu.edu/tipster
D. McKelvie, H. Thompson and S. Finch. 1996.
&amp;quot;The Normalised SGML Library LT NSL ver-
sion 1.4.6&amp;quot;. Technical Report, Language Technol-
ogy Group, University of Edinburgh. Available at
http://www.ltg.ed.ac.uk/software/nsl
A. Mikheev and S. Finch. 1995. &amp;quot;Towards a Work-
bench for Acquisition of Domain Knowledge from
Natural Language&amp;quot;. In Proceedings of the Seventh
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics (EACL &apos;95).
Dublin, Ireland.
A. Mikheev and S. Finch. 1997. &amp;quot;A Workbench for
Finding Structure in Texts&amp;quot;. in these proceedings.
A. Mikheev and D. McKelvie. 1997. &amp;quot;Indexing
SGML files using LT NSL&amp;quot;. Technical Report,
Language Technology Group, University of Edin-
burgh.
R. Pito. 1994. &amp;quot;Tgrep Manual Page&amp;quot;. Available
from
http://www.ldc .upenn . edu/ldc/online/treebank/man/
G. van Rossum. 1995. &amp;quot;Python Tutorial&amp;quot;. Available
from http://www.python.org/
C. M. Sperberg-McQueen &amp; L. Burnard, eds. 1994.
&amp;quot;Guidelines for Electronic Text Encoding and In-
terchange&amp;quot;. Text Encoding Initiative, Oxford.
R. Tobin and D. McKelvie. 1996. &amp;quot;The
Python Interface to the Normalised SGML Li-
brary (PythonNSL)&amp;quot;. Technical Report, Language
Technology Group, University of Edinburgh.
</bodyText>
<page confidence="0.997568">
236
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.476091">
<title confidence="0.999983">Using SGML as a Basis for Data-Intensive NLP</title>
<author confidence="0.99987">David McKelvie</author>
<author confidence="0.99987">Chris Brew</author>
<author confidence="0.99987">Henry Thompson</author>
<affiliation confidence="0.77147">Language Technology Group, Human Communication Research Centre, University of Edinburgh, Edinburgh, Scotland</affiliation>
<abstract confidence="0.971253833333333">paper describes the NSL system (McKelvie et al, 1996), an architecture for writing corpus processing tools. This system is then compared with two other systems which address similar issues, the GATE system (Cunningham et al, 1995) and the IMS Corpus Workbench (Christ, 1994). In particular we address the advantages and disadvantages of an compared with a non-SGML database approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A H Anderson</author>
<author>M Bader</author>
<author>E G Bard</author>
<author>E H Boyle</author>
<author>G M Doherty</author>
<author>S C Garrod</author>
<author>S D Isard</author>
<author>J C Kowtko</author>
<author>J M McAllister</author>
<author>J Miller</author>
<author>C F Sotillo</author>
<author>H S Thompson</author>
<author>R Weinert</author>
</authors>
<title>The HCRC Map Task Corpus. Language and Speech,</title>
<date>1991</date>
<pages>34--4</pages>
<contexts>
<context position="6724" citStr="Anderson et al, 1991" startWordPosition="1112" endWordPosition="1115">) is currently only 6 times slower using LT Nsi, sggrep than using fgrep, and sggrep allows more complex structure-sensitive queries. A prototype indexing mechanism (Mikheev&amp;McKelvie, 1997), not yet in 1Based on James Clark&apos;s SP parser (Clark, 1996). 2SGML&apos;s way of describing the structure (or grammar) of the allowed markup in a document the distribution, improves the performance of LT NSL to acceptable levels for much larger datasets. Why did we say &amp;quot;primarily for text corpora&amp;quot;? Because much of the technology is directly applicable to multimedia corpora such as the Edinburgh Map Task corpus (Anderson et al, 1991). There are tools which interpret SGML elements in the corpus text as offsets into files of audio-data, allowing very flexible retrieval and output of audio information using queries defined over the corpus text and its annotations. The same could be done for video clips, etc. 2.1 Hyperlinking We are inclined to steer a middle course between a monolithic comprehensive view of corpus data, in which all possible views, annotations, structurings etc. of a corpus component are combined in a single heavily structured document, and a massively decentralised view in which a corpus component is organi</context>
</contexts>
<marker>Anderson, Bader, Bard, Boyle, Doherty, Garrod, Isard, Kowtko, McAllister, Miller, Sotillo, Thompson, Weinert, 1991</marker>
<rawString>A. H. Anderson, M. Bader, E. G. Bard, E. H. Boyle, G. M. Doherty, S. C. Garrod, S. D. Isard, J. C. Kowtko, J. M. McAllister, J. Miller, C. F. Sotillo, H. S. Thompson, and R. Weinert. The HCRC Map Task Corpus. Language and Speech, 34(4):351-366, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Brew</author>
<author>D McKelvie</author>
</authors>
<title>Word-pair extraction for lexicography&amp;quot;.</title>
<date>1996</date>
<booktitle>In Proceedings of NeMLaP&apos;96,</booktitle>
<pages>45--55</pages>
<location>Ankara, Turkey.</location>
<marker>Brew, McKelvie, 1996</marker>
<rawString>C. Brew and D. McKelvie. 1996. &amp;quot;Word-pair extraction for lexicography&amp;quot;. In Proceedings of NeMLaP&apos;96, pp 45-55, Ankara, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Burnage</author>
<author>D Dunlop</author>
</authors>
<title>Encoding the British National Corpus&amp;quot;.</title>
<date>1992</date>
<booktitle>In 13th International Conference on English Language research on computerised corpora, Nijmegen. Available at http://www.sil.org/sgra/bnc-encoding2 .</booktitle>
<note>html See also http://info.ox.ac.uk/bnc/</note>
<marker>Burnage, Dunlop, 1992</marker>
<rawString>G. Burnage and D. Dunlop. 1992. &amp;quot;Encoding the British National Corpus&amp;quot;. In 13th International Conference on English Language research on computerised corpora, Nijmegen. Available at http://www.sil.org/sgra/bnc-encoding2 . html See also http://info.ox.ac.uk/bnc/</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
<author>H Fraser-Krauss</author>
<author>S Garrod</author>
</authors>
<title>An Empirical Study of Innovation in Manufacturing Teams: a preliminary report&amp;quot;.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Workshop on Communication Modelling (LAP-96),</booktitle>
<editor>ed. J. L. G. Dietz, Springer-Verlag,</editor>
<contexts>
<context position="29504" citStr="Carletta et al, 1996" startWordPosition="4860" endWordPosition="4863">96). This allows us to rapidly prototype editors using the Python/Tk graphics package. These editors can fit into a pipeline of LT NSL tools allowing hand correction or disambiguation of markup automatically added by previous tools. Using this API we are developing a generic SGML editor. It is an object-oriented system where one can flexibly associate display and interaction classes to particular SGML elements. Already, this generic editor has been used for a number of tasks; the hand correction of part-of-speech tags in the MapTask, the correction of turn boundaries in the Innovation corpus (Carletta et al, 1996), and the evaluation of translation equivalences between aligned multilingual corpora. We found that using this generic editor framework made it possible to quickly write new editors for new tasks on new corpora. 5 Conclusions SGML is a good markup language for base level annotations of published corpora. Our experience with LT NSL has shown that: • It is a good system for sequential corpus processing where there is locality of reference. • It provides a modular architecture which does not require a central database, thus allowing distributed software development and reuse of components. • It </context>
</contexts>
<marker>Carletta, Fraser-Krauss, Garrod, 1996</marker>
<rawString>J. Carletta, H. Fraser-Krauss and S. Garrod. 1996. &amp;quot;An Empirical Study of Innovation in Manufacturing Teams: a preliminary report&amp;quot;. In Proceedings of the International Workshop on Communication Modelling (LAP-96), ed. J. L. G. Dietz, Springer-Verlag, Electronic Workshops in Computing Series.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christ</author>
</authors>
<title>A modular and flexible architecture for an integrated corpus query system&amp;quot;.</title>
<date>1994</date>
<booktitle>In Proceedings of COMPLEX &apos;94: 3rd Conference on Computational Lexicography and Text Research</booktitle>
<pages>9408005</pages>
<location>Budapest,</location>
<contexts>
<context position="1430" citStr="Christ, 1994" startWordPosition="224" endWordPosition="225">wo major issues in corpus-based NLP are: how best to deal with medium to large scale corpora often with complex linguistic annotations, and what system architecture best supports the reuse of software components in a modular and interchangeable fashion. In this paper we describe the LT NSL system (McKelvie et al, 1996), an architecture for writing corpus processing tools, which we have developed in an attempt to address these issues. This system is then compared with two other systems which address some of the same issues, the GATE system (Cunningham et al, 1995) and the IMS Corpus Workbench (Christ, 1994). In particular we address the advantages and disadvantages of an SGML approach compared with a non-SGML database approach. Finally, in order to back up our claims about the merits of SGML-based corpus processing, we present a number of case studies of the use of the LT NSL system for corpus preparation and linguistic analysis. 2 The LT NSL system LT NSL is a tool architecture for SGML-based pro229 cessing of (primarily) text corpora. It generalises the UNIX pipe architecture, making it possible to use pipelines of general-purpose tools to process annotated corpora. The original UNIX architect</context>
<context position="20473" citStr="Christ, 1994" startWordPosition="3368" endWordPosition="3369">rfaces. As Cunningham et. al. say, there is no reason why there could not be an implementation of LT NSL which read SGML elements from a database rather than from files. Similarly, a TIPSTER architecture like GATE could read SGML and convert it into its internal database. In that case, our point would be that SGML is a suitable abstraction for programs rather than a more abstract (and perhaps more limited) level of interface. We are currently in discussion with the GATE team about how best to allow the interoperability of the two systems. 3.2 The IMS Corpus Workbench The IMS Corpus Workbench (Christ, 1994) includes both a query engine (CQP) and a Motif-based user visualisation tool (xkwic). CQP provides a query language which is a conservative extension of familiar UNIX regular expression facilities4. XKWIC is a user interface tuned for corpus search. As well as providing the standard keyword-in-context facilities and giving access to the query language it gives the user sophisticated tools for managing the query history, manipulating the display, and storing search results. The most interesting points of comparison with LT NSL are in the areas of query language and underlying corpus representa</context>
</contexts>
<marker>Christ, 1994</marker>
<rawString>0. Christ. 1994. &amp;quot;A modular and flexible architecture for an integrated corpus query system&amp;quot;. In Proceedings of COMPLEX &apos;94: 3rd Conference on Computational Lexicography and Text Research (Budapest, July 7-10, 1994), Budapest, Hungary. CMP-LG archive id 9408005</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clark</author>
</authors>
<title>SP: An SGML System Conforming to International Standard ISO 8879 - Standard Generalized Markup Language&amp;quot;. Available from http://www.jclark.com/sp/index.htm.</title>
<date>1996</date>
<contexts>
<context position="6352" citStr="Clark, 1996" startWordPosition="1053" endWordPosition="1054">iting step in sequential corpus processing. The kinds of repeated search required by lexicographers are more of a problem, since the system was not designed for that purpose. The standard distribution is fast enough for use as a search engine with files of up to several million words. Searching 1% of the British National Corpus (a total of 700,000 words (18 Mb)) is currently only 6 times slower using LT Nsi, sggrep than using fgrep, and sggrep allows more complex structure-sensitive queries. A prototype indexing mechanism (Mikheev&amp;McKelvie, 1997), not yet in 1Based on James Clark&apos;s SP parser (Clark, 1996). 2SGML&apos;s way of describing the structure (or grammar) of the allowed markup in a document the distribution, improves the performance of LT NSL to acceptable levels for much larger datasets. Why did we say &amp;quot;primarily for text corpora&amp;quot;? Because much of the technology is directly applicable to multimedia corpora such as the Edinburgh Map Task corpus (Anderson et al, 1991). There are tools which interpret SGML elements in the corpus text as offsets into files of audio-data, allowing very flexible retrieval and output of audio information using queries defined over the corpus text and its annotati</context>
</contexts>
<marker>Clark, 1996</marker>
<rawString>J. Clark. 1996 &amp;quot;SP: An SGML System Conforming to International Standard ISO 8879 - Standard Generalized Markup Language&amp;quot;. Available from http://www.jclark.com/sp/index.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>Y Wilks</author>
<author>R J Gaizauskas</author>
</authors>
<title>New Methods, Current Trends and Software Infrastructure for NLP&amp;quot;.</title>
<date>1996</date>
<booktitle>In Proceedings of the Second Conference on New Methods in Language Processing,</booktitle>
<pages>283--298</pages>
<location>Ankara, Turkey,</location>
<contexts>
<context position="15914" citStr="Cunningham et al, 1996" startWordPosition="2584" endWordPosition="2587">f annotations. GATE allows any annotation anywhere. Although this is more powerful, i.e. one is not restricted to tree structures, it does make validation of annotations more difficult. The idea of having formalised interfaces for external programs and data is a good one. The GGI graphical tool shell lets one build, store, and recover complex processing specifications. There is merit in having a high level language to specify tasks which can be translated automatically into executable programs (e.g. shell scripts). This is an area that LT NSL does not address. 3.1.3 Comparison with LT NSL In (Cunningham et al, 1996), the GATE architecture is compared with the earlier version of the LT NSL architecture which was developed in the MULTEXT project. We would like to answer these points with reference to the latest version of our software. . It is claimed that using normalised SGML implies a large storage overhead. Normally however, normalised SGML will be created on the fly and passed through pipes and only the final results will need to be stored. This may however be a problem for very large corpora such as the BNC. It is stated that representing ambiguous or overlapping markup is complex in SGML. We do not </context>
</contexts>
<marker>Cunningham, Wilks, Gaizauskas, 1996</marker>
<rawString>H. Cunningham, Y. Wilks and R. J. Gaizauskas. 1996. &amp;quot;New Methods, Current Trends and Software Infrastructure for NLP&amp;quot;. In Proceedings of the Second Conference on New Methods in Language Processing, pages 283-298, Ankara, Turkey, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>R Gaizauslcas</author>
<author>Y Wilks</author>
</authors>
<title>A General Architecture for Text Engineering (GATE) - a new approach to Language Engineering R&amp;D&amp;quot;.</title>
<date>1995</date>
<tech>Technical Report,</tech>
<institution>Dept of Computer Science, University of Sheffield.</institution>
<contexts>
<context position="1386" citStr="Cunningham et al, 1995" startWordPosition="213" endWordPosition="217">tures for natural language processing using corpora. Two major issues in corpus-based NLP are: how best to deal with medium to large scale corpora often with complex linguistic annotations, and what system architecture best supports the reuse of software components in a modular and interchangeable fashion. In this paper we describe the LT NSL system (McKelvie et al, 1996), an architecture for writing corpus processing tools, which we have developed in an attempt to address these issues. This system is then compared with two other systems which address some of the same issues, the GATE system (Cunningham et al, 1995) and the IMS Corpus Workbench (Christ, 1994). In particular we address the advantages and disadvantages of an SGML approach compared with a non-SGML database approach. Finally, in order to back up our claims about the merits of SGML-based corpus processing, we present a number of case studies of the use of the LT NSL system for corpus preparation and linguistic analysis. 2 The LT NSL system LT NSL is a tool architecture for SGML-based pro229 cessing of (primarily) text corpora. It generalises the UNIX pipe architecture, making it possible to use pipelines of general-purpose tools to process an</context>
<context position="13726" citStr="Cunningham et al, 1995" startWordPosition="2232" endWordPosition="2235">h the &lt;P&gt; element satisfying the additional requirement that it contain at least one &lt;S&gt; element at any depth with text containing at least one instance of &apos;their&apos; (possibly misspelt). 231 3 Comparisons with other systems The major alternative corpus architecture which has been advocated is a database approach, where annotations are kept separately from the base texts. The annotations are linked to the base texts either by means of character offsets or by a more sophisticated indexing scheme. We will discuss two such systems and compare them with the LT NSL approach. 3.1 GATE The GATE system (Cunningham et al, 1995), currently under development at the University of Sheffield, is a system to support modular language engineering. 3.1.1 System components It consists of three main components: • GDM - an object oriented database for storing information about the corpus texts. This database is based on the TIPSTER document architecture (Grishman, 1995), and stores text annotations separate from the texts. Annotations are linked to texts by means of character offsets3. • Creole - A library of program and data resource wrappers, that allow one to interface externally developed programs/resources into the GATE ar</context>
</contexts>
<marker>Cunningham, Gaizauslcas, Wilks, 1995</marker>
<rawString>H. Cunningham, R. Gaizauslcas and Y. Wilks. 1995. &amp;quot;A General Architecture for Text Engineering (GATE) - a new approach to Language Engineering R&amp;D&amp;quot;. Technical Report, Dept of Computer Science, University of Sheffield. Available from http: //www .des . shef .ac.uk/research/groups /nip/gate/</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>