<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002982">
<title confidence="0.9975095">
Weakly Supervised Part-of-Speech Tagging for Morphologically-Rich,
Resource-Scarce Languages
</title>
<author confidence="0.920129">
Kazi Saidul Hasan and Vincent Ng
</author>
<affiliation confidence="0.970622">
Human Language Technology Research Institute
University of Texas at Dallas
</affiliation>
<address confidence="0.917743">
Richardson, TX 75083-0688
</address>
<email confidence="0.999725">
{saidul,vince}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.997401" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999726363636364">
This paper examines unsupervised ap-
proaches to part-of-speech (POS) tagging
for morphologically-rich, resource-scarce
languages, with an emphasis on Goldwa-
ter and Griffiths’s (2007) fully-Bayesian
approach originally developed for En-
glish POS tagging. We argue that ex-
isting unsupervised POS taggers unreal-
istically assume as input a perfect POS
lexicon, and consequently, we propose
a weakly supervised fully-Bayesian ap-
proach to POS tagging, which relaxes the
unrealistic assumption by automatically
acquiring the lexicon from a small amount
of POS-tagged data. Since such relaxation
comes at the expense of a drop in tag-
ging accuracy, we propose two extensions
to the Bayesian framework and demon-
strate that they are effective in improv-
ing a fully-Bayesian POS tagger for Ben-
gali, our representative morphologically-
rich, resource-scarce language.
</bodyText>
<sectionHeader confidence="0.999473" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999708538461539">
Unsupervised POS tagging requires neither man-
ual encoding of tagging heuristics nor the avail-
ability of data labeled with POS information.
Rather, an unsupervised POS tagger operates by
only assuming as input a POS lexicon, which con-
sists of a list of possible POS tags for each word.
As we can see from the partial POS lexicon for
English in Figure 1, “the” is unambiguous with re-
spect to POS tagging, since it can only be a deter-
miner (DT), whereas “sting” is ambiguous, since
it can be a common noun (NN), a proper noun
(NNP) or a verb (VB). In other words, the lexi-
con imposes constraints on the possible POS tags
</bodyText>
<figure confidence="0.967305">
Word POS tag(s)
... ...
running NN, JJ
sting NN, NNP, VB
the DT
... ...
</figure>
<figureCaption confidence="0.999984">
Figure 1: A partial lexicon for English
</figureCaption>
<bodyText confidence="0.999916354838709">
of each word, and such constraints are then used
by an unsupervised tagger to label a new sentence.
Conceivably, tagging accuracy decreases with the
increase in ambiguity: unambiguous words such
as “the” will always be tagged correctly; on the
other hand, unseen words (or words not present
in the POS lexicon) are among the most ambigu-
ous words, since they are not constrained at all
and therefore can receive any of the POS tags.
Hence, unsupervised POS tagging can present sig-
nificant challenges to natural language processing
researchers, especially when a large fraction of
the words are ambiguous. Nevertheless, the de-
velopment of unsupervised taggers potentially al-
lows POS tagging technologies to be applied to a
substantially larger number of natural languages,
most of which are resource-scarce and, in particu-
lar, have little or no POS-tagged data.
The most common approach to unsupervised
POS tagging to date has been to train a hidden
Markov model (HMM) in an unsupervised man-
ner to maximize the likelihood of an unannotated
corpus, using a special instance of the expectation-
maximization (EM) algorithm (Dempster et al.,
1977) known as Baum-Welch (Baum, 1972).
More recently, a fully-Bayesian approach to un-
supervised POS tagging has been developed by
Goldwater and Griffiths (2007) [henceforth G&amp;G]
as a viable alternative to the traditional maximum-
likelihood-based HMM approach. While unsuper-
vised POS taggers adopting both approaches have
</bodyText>
<note confidence="0.922997">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 363–371,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998832">
363
</page>
<bodyText confidence="0.966619703703704">
demonstrated promising results, it is important to
note that they are typically evaluated by assuming
the availability of a perfect POS lexicon. This as-
sumption, however, is fairly unrealistic in practice,
as a perfect POS lexicon can only be constructed
by having a linguist manually label each word in
a language with its possible POS tags.1 In other
words, the labor-intensive POS lexicon construc-
tion process renders unsupervised POS taggers a
lot less unsupervised than they appear. To make
these unsupervised taggers practical, one could at-
tempt to automatically construct a POS lexicon, a
task commonly known as POS induction. How-
ever, POS induction is by no means an easy task,
and it is not clear how well unsupervised POS tag-
gers work when used in combination with an au-
tomatically constructed POS lexicon.
The goals of this paper are three-fold. First,
motivated by the successes of unsupervised ap-
proaches to English POS tagging, we aim to inves-
tigate whether such approaches, especially G&amp;G’s
fully-Bayesian approach, can deliver similar per-
formance for Bengali, our representative resource-
scarce language. Second, to relax the unrealis-
tic assumption of employing a perfect lexicon as
in existing unsupervised POS taggers, we propose
a weakly supervised fully-Bayesian approach to
POS tagging, where we automatically construct a
POS lexicon from a small amount of POS-tagged
data. Hence, unlike a perfect POS lexicon, our au-
tomatically constructed lexicon is necessarily in-
complete, yielding a large number of words that
are completely ambiguous. The high ambiguity
rate inherent in our weakly supervised approach
substantially complicates the POS tagging pro-
cess. Consequently, our third goal of this paper is
to propose two potentially performance-enhancing
extensions to G&amp;G’s Bayesian POS tagging ap-
proach, which exploit morphology and techniques
successfully used in supervised POS tagging.
The rest of the paper is organized as follows.
Section 2 presents related work on unsupervised
approaches to POS tagging. Section 3 gives an
introduction to G&amp;G’s fully-Bayesian approach
to unsupervised POS tagging. In Section 4, we
describe our two extensions to G&amp;G’s approach.
Section 5 presents experimental results on Bengali
POS tagging, focusing on evaluating the effective-
1When evaluating an unsupervised POS tagger, re-
searchers typically construct a pseudo-perfect POS lexicon
by collecting the possible POS tags of a word directly from
the corpus on which the tagger is to be evaluated.
ness of our two extensions in improving G&amp;G’s
approach. Finally, we conclude in Section 6.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999908765957447">
With the notable exception of Synder et
al.’s (2008; 2009) recent work on unsupervised
multilingual POS tagging, existing approaches to
unsupervised POS tagging have been developed
and tested primarily on English data. For instance,
Merialdo (1994) uses maximum likelihood esti-
mation to train a trigram HMM. Sch¨utze (1995)
and Clark (2000) apply syntactic clustering and
dimensionality reduction in a knowledge-free
setting to obtain meaningful clusters. Haghighi
and Klein (2006) develop a prototype-driven
approach, which requires just a few prototype
examples for each POS tag and exploits these
labeled words to constrain the labels of their
distributionally similar words. Smith and Eisner
(2005) train an unsupervised POS tagger using
contrastive estimation, which seeks to move
probability mass to a positive example e from
its neighbors (i.e., negative examples are created
by perturbing e). Wang and Schuurmans (2005)
improve an unsupervised HMM-based tagger by
constraining the learned structure to maintain
appropriate marginal tag probabilities and using
word similarities to smooth the lexical parameters.
As mentioned before, Goldwater and Griffiths
(2007) have recently proposed an unsupervised
fully-Bayesian POS tagging framework that op-
erates by integrating over the possible parameter
values instead of fixing a set of parameter values
for unsupervised sequence learning. Importantly,
this Bayesian approach facilitates the incorpora-
tion of sparse priors that result in a more practical
distribution of tokens to lexical categories (John-
son, 2007). Similar to Goldwater and Griffiths
(2007) and Johnson (2007), Toutanova and John-
son (2007) also use Bayesian inference for POS
tagging. However, their work departs from exist-
ing Bayesian approaches to POS tagging in that
they (1) introduce a new sparse prior on the dis-
tribution over tags for each word, (2) extend the
Latent Dirichlet Allocation model, and (3) explic-
itly model ambiguity class. While their tagging
model, like Goldwater and Griffiths’s, assumes as
input an incomplete POS lexicon and a large unla-
beled corpus, they consider their approach “semi-
supervised” simply because of the human knowl-
edge involved in constructing the POS lexicon.
</bodyText>
<page confidence="0.999232">
364
</page>
<sectionHeader confidence="0.978936" genericHeader="method">
3 A Fully Bayesian Approach
</sectionHeader>
<subsectionHeader confidence="0.997387">
3.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999973880952381">
As mentioned in the introduction, the most com-
mon approach to unsupervised POS tagging is to
train an HMM on an unannotated corpus using the
Baum-Welch algorithm so that the likelihood of
the corpus is maximized. To understand what the
HMM parameters are, let us revisit how an HMM
simultaneously generates an output sequence w
= (w0, w1, ..., wn) and the associated hidden state
sequence t = (t0, t1, ..., tn). In the context of POS
tagging, each state of the HMM corresponds to a
POS tag, the output sequence w is the given word
sequence, and the hidden state sequence t is the
associated POS tag sequence. To generate w and
t, the HMM begins by guessing a state t0 and then
emitting w0 from t0 according to a state-specific
output distribution over word tokens. After that,
we move to the next state t1, the choice of which
is based on t0’s transition distribution, and emit
w1 according to t1’s output distribution. This gen-
eration process repeats until the end of the word
sequence is reached. In other words, the parame-
ters of an HMM, θ, are composed of a set of state-
specific (1) output distributions (over word tokens)
and (2) transition distributions, both of which can
be learned using the EM algorithm. Once learning
is complete, we can use the resulting set of param-
eters to find the most likely hidden state sequence
given a word sequence using the Viterbi algorithm.
Nevertheless, EM sometimes fails to find good
parameter values.2 The reason is that EM tries to
assign roughly the same number of word tokens to
each of the hidden states (Johnson, 2007). In prac-
tice, however, the distribution of word tokens to
POS tags is highly skewed (i.e., some POS cate-
gories are more populated with tokens than oth-
ers). This motivates a fully-Bayesian approach,
which, rather than committing to a particular set
of parameter values as in an EM-based approach,
integrates over all possible values of θ and, most
importantly, allows the use of priors to favor the
learning of the skewed distributions, through the
use of the term P(θ|w) in the following equation:
</bodyText>
<equation confidence="0.999178">
�P(t|w) = P(t|w,θ)P(θ|w)dθ (1)
</equation>
<bodyText confidence="0.9820834">
The question, then, is: which priors on θ would
allow the acquisition of skewed distributions? To
2When given good parameter initializations, however, EM
can find good parameter values for an HMM-based POS tag-
ger. See Goldberg et al. (2008) for details.
answer this question, recall that in POS tagging, θ
is composed of a set of tag transition distributions
and output distributions. Each such distribution is
a multinomial (i.e., each trial produces exactly one
of some finite number of possible outcomes). For
a multinomial with K outcomes, a K-dimensional
Dirichlet distribution, which is conjugate to the
multinomial, is a natural choice of prior. For sim-
plicity, we assume that a distribution in θ is drawn
from a symmetric Dirichlet with a certain hyper-
parameter (see Teh et al. (2006) for details).
The value of a hyperparameter, α, affects the
skewness of the resulting distribution, as it as-
signs different probabilities to different distribu-
tions. For instance, when α &lt; 1, higher proba-
bilities are assigned to sparse multinomials (i.e.,
multinomials in which only a few entries are non-
zero). Intuitively, the tag transition distributions
and the output distributions in an HMM-based
POS tagger are sparse multinomials. As a re-
sult, it is logical to choose a Dirichlet prior with
α &lt; 1. By integrating over all possible param-
eter values, the probability that i-th outcome, yi,
takes the value k, given the previous i − 1 out-
comes y−i= (y1, y2, ..., yi−1), is
</bodyText>
<equation confidence="0.994806">
�P(k|y−i, α) = P(k|θ)P(θ|y−i,α)dθ(2)
nk + α (3)
i − 1 + Kα
</equation>
<bodyText confidence="0.9998535">
where nk is the frequency of k in y−i. See
MacKay and Peto (1995) for the derivation.
</bodyText>
<subsectionHeader confidence="0.998989">
3.2 Model
</subsectionHeader>
<bodyText confidence="0.9990216">
Our baseline POS tagging model is a standard tri-
gram HMM with tag transition distributions and
output distributions, each of which is a sparse
multinomial that is learned by applying a symmet-
ric Dirichlet prior:
</bodyText>
<equation confidence="0.99869875">
ti  |ti−1, ti−2, τ(ti−1,ti−2) ∼ Mult(τ(ti−1,ti−2))
wi  |ti, ω(ti) ∼ Mult(ω(ti))
τ(ti−1,ti−2)  |α ∼ Dirichlet(α)
ω(ti)  |β ∼ Dirichlet(β)
</equation>
<bodyText confidence="0.999968428571428">
where wi and ti denote the i-th word and tag. With
a tagset of size T (including a special tag used as
sentence delimiter), each of the tag transition dis-
tributions has T components. For the output sym-
bols, each of the ω(ti) has Wti components, where
Wti denotes the number of word types that can be
emitted from the state corresponding to ti.
</bodyText>
<page confidence="0.994992">
365
</page>
<bodyText confidence="0.999912">
From the closed form in Equation 3, given pre-
vious outcomes, we can compute the tag transition
and output probabilities of the model as follows:
</bodyText>
<equation confidence="0.996633833333333">
n(ti−2,ti−1,ti) + α
P(tiJt−i, α) = (4)
n(ti−2,ti−1) + Tα
n(ti,wi) + β
P(wiJti, t−i, w−i, β) = (5)
nti + Wti β
</equation>
<bodyText confidence="0.999929">
where n(ti−2,ti−1,ti) and n(ti,wi) are the frequen-
cies of observing the tag trigram (ti−2,ti−1,ti)
and the tag-word pair (ti, wi), respectively. These
counts are taken from the i − 1 tags and words
generated previously. The inference procedure de-
scribed next exploits the property that trigrams
(and outputs) are exchangeable; that is, the prob-
ability of a set of trigrams (and outputs) does not
depend on the order in which it was generated.
</bodyText>
<subsectionHeader confidence="0.904534">
3.3 Inference Procedure
</subsectionHeader>
<bodyText confidence="0.999969">
We perform inference using Gibbs sampling (Ge-
man and Geman, 1984), using the following pos-
terior distribution to generate samples:
</bodyText>
<equation confidence="0.982147">
P(tJw, α, β) a P(wJt, β)P(tJα)
</equation>
<bodyText confidence="0.9991975">
Starting with a random assignment of a POS tag
to each word (subject to the constraints in the POS
lexicon), we resample each POS tag, ti, accord-
ing to the conditional distribution shown in Figure
2. Note that the current counts of other trigrams
and outputs can be used as “previous” observa-
tions due to the property of exchangeability.
Following G&amp;G, we use simulated annealing to
find the MAP tag sequence. The temperature de-
creases by a factor of exp(log(θ2
N−1 ) after each iter-
θ1 )
ation, where θ1 is the initial temperature and θ2 is
the temperature after N sampling iterations.
</bodyText>
<sectionHeader confidence="0.996074" genericHeader="method">
4 Two Extensions
</sectionHeader>
<bodyText confidence="0.99990975">
In this section, we present two extensions to
G&amp;G’s fully-Bayesian framework to unsupervised
POS tagging, namely, induced suffix emission and
discriminative prediction.
</bodyText>
<subsectionHeader confidence="0.927054">
4.1 Induced Suffix Emission
</subsectionHeader>
<bodyText confidence="0.995298037037037">
For morphologically-rich languages like Bengali,
a lot of grammatical information (e.g., POS) is ex-
pressed via suffixes. In fact, several approaches to
unsupervised POS induction for morphologically-
rich languages have exploited the observation that
some suffixes can only be associated with a small
number of POS tags (e.g., Clark (2003), Dasgupta
and Ng (2007)). To exploit suffixes in HMM-
based POS tagging, one can (1) convert the word-
based POS lexicon to a suffix-based POS lexicon,
which lists the possible POS tags for each suffix;
and then (2) have the HMM emit suffixes rather
than words, subject to the constraints in the suffix-
based POS lexicon. Such a suffix-based HMM,
however, may suffer from over-generalization. To
prevent over-generalization and at the same time
exploit suffixes, we propose as our first exten-
sion to G&amp;G’s framework a hybrid approach to
word/suffix emission: a word is emitted if it is
present in the word-based POS lexicon; otherwise,
its suffix is emitted. In other words, our approach
imposes suffix-based constraints on the tagging of
words that are unseen w.r.t. the word-based POS
lexicon. Below we show how to induce the suffix
of a word and create the suffix-based POS lexicon.
Inducing suffixes To induce suffixes, we rely on
Keshava and Pitler’s (2006) method. Assume that
(1) V is a vocabulary (i.e., a set of distinct words)
extracted from a large, unannotated corpus, (2) C1
and C2 are two character sequences, and (3) C1C2
is the concatenation of C1 and C2. If C1C2 and
C1 are found in V , we extract C2 as a suffix.
However, this unsupervised suffix induction
method is arguably overly simplistic and hence
many of the induced affixes could be spurious. To
identify suffixes that are likely to be correct, we
employ a simple procedure: we (1) score each suf-
fix by multiplying its frequency (i.e., the number
of distinct words in V to which each suffix at-
taches) and its length3, and (2) select only those
whose score is above a certain threshold. In our
experiments, we set this threshold to 50, and gen-
erate our vocabulary from five years of articles
taken from the Bengali newspaper Prothom Alo.
This enables us to induce 975 suffixes.
Constructing a suffix-based POS lexicon
Next, we construct a suffix-based POS lexicon.
For each word w in the original word-based
POS lexicon, we (1) use the induced suffix list
obtained in the previous step to identify the
longest-matching suffix of w, and then (2) assign
all the POS tags associated with w to this suffix.
Incorporating suffix-based output distributions
Finally, we extend our trigram model by introduc-
</bodyText>
<footnote confidence="0.988290666666667">
3The dependence on frequency and length is motivated by
the observation that less frequent and shorter affixes are more
likely to be erroneous (see Goldsmith (2001)).
</footnote>
<page confidence="0.979021">
366
</page>
<equation confidence="0.98380825">
P(ti  |t−i, w, α, β) a n(ti,wz) + β n(ti−2,ti−1,ti) + α . n(ti−1,ti,ti+1) + I(ti−2 = ti−1 = ti = ti+1) + α
nti + Wtiβ . n(ti−2,ti−1) + Tα n(ti−1,ti) + I(ti−2 = ti−1 = ti) + Tα
n(ti,ti+1,ti+2) + I(ti−2 = ti = ti+2, ti−1 = ti+1) + I(ti−1 = ti = ti+1 = ti+2) + α
. n(ti,ti+1) + I(ti−2 = ti, ti−1 = ti+1) + I(ti−1 = ti = ti+1) + Tα
</equation>
<figureCaption confidence="0.997709">
Figure 2: The sampling distribution for ti (taken directly from Goldwater and Griffiths (2007)). All nx
values are computed from the current values of all tags except for ti. Here, I(arg) is a function that
returns 1 if arg is true and 0 otherwise, and t−i refers to the current values of all tags except for ti.
</figureCaption>
<bodyText confidence="0.9890095">
ing a state-specific probability distribution over in-
duced suffixes. Specifically, if the current word is
present in the word-based POS lexicon, or if we
cannot find any suffix for the word using the in-
duced suffix list, then we emit the word. Other-
wise, we emit its suffix according to a suffix-based
output distribution, which is drawn from a sym-
metric Dirichlet with hyperparameter γ:
</bodyText>
<equation confidence="0.997937">
si  |ti, σ(ti) — Mult(σ(ti))
σ(ti)  |γ — Dirichlet(γ)
</equation>
<bodyText confidence="0.999992666666667">
where si denotes the induced suffix of the i-th
word. The distribution, σ(ti), has Sti components,
where Sti denotes the number of induced suffixes
that can be emitted from the state corresponding to
ti. We compute the induced suffix emission prob-
abilities of the model as follows:
</bodyText>
<equation confidence="0.993372">
n(ti,si) + γ
P(si|ti, t−i, s−i,γ) = (6)
nti + Stiγ
</equation>
<bodyText confidence="0.999638583333333">
where n(ti,si) is the frequency of observing the
tag-suffix pair (ti, si).
This extension requires that we slightly modify
the inference procedure. Specifically, if the cur-
rent word is unseen (w.r.t. the word-based POS
lexicon) and has a suffix (according to the induced
suffix list), then we sample from a distribution that
is almost identical to the one shown in Figure 2,
except that we replace the first fraction (i.e., the
fraction involving the emission counts) with the
one shown in Equation (6). Otherwise, we simply
sample from the distribution in Figure 2.
</bodyText>
<subsectionHeader confidence="0.882631">
4.2 Discriminative Prediction
</subsectionHeader>
<bodyText confidence="0.999970276595745">
As mentioned in the introduction, the (word-
based) POS lexicons used in existing approaches
to unsupervised POS tagging were created some-
what unrealistically by collecting the possible
POS tags of a word directly from the corpus on
which the tagger is to be evaluated. To make the
lexicon formation process more realistic, we pro-
pose a weakly supervised approach to Bayesian
POS tagging, in which we automatically create the
word-based POS lexicon from a small set of POS-
tagged sentences that is disjoint from the test data.
Adopting a weakly supervised approach has an ad-
ditional advantage: the presence of POS-tagged
sentences makes it possible to exploit techniques
developed for supervised POS tagging, which is
the idea behind discriminative prediction, our sec-
ond extension to G&amp;G’s framework.
Given a small set of POS-tagged sentences L,
discriminative prediction uses the statistics col-
lected from L to predict the POS of a word in a
discriminative fashion whenever possible. More
specifically, discriminative prediction relies on
two simple ideas typically exploited by supervised
POS tagging algorithms: (1) if the target word
(i.e., the word whose POS tag is to be predicted)
appears in L, we can label the word with its POS
tag in L; and (2) if the target word does not appear
in L but its context does, we can use its context to
predict its POS tag. In bigram and trigram POS
taggers, the context of a word is represented us-
ing the preceding one or two words. Nevertheless,
since L is typically small in a weakly supervised
setting, it is common for a target word not to sat-
isfy any of the two conditions above. Hence, if it is
not possible to predict a target word in a discrim-
inative fashion (due to the limited size of L), we
resort to the sampling equation in Figure 2.
To incorporate the above discriminative deci-
sion steps into G&amp;G’s fully-Bayesian framework
for POS tagging, the algorithm estimates three
types of probability distributions from L. First,
to capture context, it computes (1) a distribu-
tion over the POS tags following a word bi-
gram, (wi−2, wi−1), that appears in L [henceforth
D1(wi−2,wi−1)] and (2) a distribution over the
POS tags following a word unigram, wi−1, that ap-
pears in L [henceforth D2(wi−1)]. Then, to cap-
</bodyText>
<page confidence="0.99538">
367
</page>
<table confidence="0.65705">
Algorithm 1 Algorithm for incorporating discrim-
inative prediction
Input: wi: current word
wi−1: previous word
wi−2: second previous word
L: a set of POS-tagged sentences
Output: Predicted tag, ti
</table>
<listItem confidence="0.892518416666667">
1: if wi E L then
2: ti +— Tag drawn from the distribution of wi’s candi-
date tags
3: else if (wi−2, wi−1) E L then
4: ti +— Tag drawn from the distribution of the POS tags
following the word bigram (wi−2, wi−1)
5: else if wi−1 E L then
6: ti +— Tag drawn from the distribution of the POS tags
following the word unigram wi−1
7: else
8: ti +— Tag obtained using the sampling equation
9: end if
</listItem>
<bodyText confidence="0.9999253">
ture the fact that a word can have more than one
POS tag, it also estimates a distribution over POS
tags for each word wi that appears in L [hence-
forth D3(wi)].
Implemented as a set of if-else clauses, the al-
gorithm uses these three types of distributions to
tag a target word, wi, in a discriminative manner.
First, it checks whether wi appears in L (line 1). If
so, it tags wi according to D3(wi). Otherwise, it
attempts to label wi based on its context. Specifi-
cally, if (wi−2, wi−1), the word bigram preceding
wi, appears in L (line 3), then wi is tagged accord-
ing to D1(wi−2, wi−1). Otherwise, it backs off to
a unigram distribution: if wi−1, the word preced-
ing wi, appears in L (line 5), then wi is tagged
according to D2(wi−1). Finally, if it is not possi-
ble to tag the word discriminatively (i.e., if all the
above cases fail), it resorts to the sampling equa-
tion (lines 7–8). We apply simulated annealing to
all four cases in this iterative tagging procedure.
</bodyText>
<sectionHeader confidence="0.999815" genericHeader="method">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.995263">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999625333333333">
Corpus Our evaluation corpus is the one used
in the shared task of the IJCNLP-08 Workshop on
NER for South and South East Asian Languages.4
Specifically, we use the portion of the Bengali
dataset that is manually POS-tagged. IIIT Hy-
derabad’s POS tagset5, which consists of 26 tags
specifically developed for Indian languages, has
been used to annotate the data. The corpus is com-
posed of a training set and a test set with approxi-
</bodyText>
<footnote confidence="0.998914333333333">
4The corpus is available from http://ltrc.iiit.ac.in/ner-ssea-
08/index.cgi?topic=5.
5http://shiva.iiit.ac.in/SPSAL2007/iiit tagset guidelines.pdf
</footnote>
<bodyText confidence="0.952298725490196">
mately 50K and 30K tokens, respectively. Impor-
tantly, all our POS tagging results will be reported
using only the test set; the training set will be used
for lexicon construction, as we will see shortly.
Tagset We collapse the set of 26 POS tags into
15 tags. Specifically, while we retain the tags cor-
responding to the major POS categories, we merge
some of the infrequent tags designed to capture
Indian language specific structure (e.g., reduplica-
tion, echo words) into a category called OTHERS.
Hyperparameter settings Recall that our tag-
ger consists of three types of distributions — tag
transition distributions, word-based output distri-
butions, and suffix-based output distributions —
drawn from a symmetric Dirichlet with α, Q,
and γ as the underlying hyperparameters, respec-
tively. We automatically determine the values of
these hyperparameters by (1) randomly initializ-
ing them and (2) resampling their values by using
a Metropolis-Hastings update (Gilks et al., 1996)
at the end of each sampling iteration. Details of
this update process can be found in G&amp;G.
Inference Inference is performed by running a
Gibbs sampler for 5000 iterations. The initial tem-
perature is set to 2.0, which is gradually lowered
to 0.08 over the iterations. Owing to the random-
ness involved in hyperparameter initialization, all
reported results are averaged over three runs.
Lexicon construction methods To better under-
stand the role of a POS lexicon in tagging perfor-
mance, we evaluate each POS tagging model by
employing lexicons constructed by three methods.
The first lexicon construction method, arguably
the most unrealistic among the three, follows that
of G&amp;G: for each word, w, in the test set, we (1)
collect from each occurrence of w in the training
set and the test set its POS tag, and then (2) insert
w and all the POS tags collected for w into the
POS lexicon. This method is unrealistic because
(1) in practice, a human needs to list all possible
POS tags for each word in order to construct this
lexicon, thus rendering the resulting tagger con-
siderably less unsupervised than it appears; and
(2) constructing the lexicon using the dataset on
which the tagger is to be evaluated implies that
there is no unseen word w.r.t. the lexicon, thus un-
realistically simplifies the POS tagging task. To
make the method more realistic, G&amp;G also create
a set of relaxed lexicons. Each of these lexicons
includes the tags for only the words that appear
at least d times in the test corpus, where d ranges
</bodyText>
<page confidence="0.997609">
368
</page>
<figureCaption confidence="0.999623">
Figure 3: Accuracies of POS tagging models using (a) Lexicon 1 and (b) Lexicon 2
</figureCaption>
<figure confidence="0.997579548387097">
(a) Lexicon 1
90
(b) Lexicon 2
75
80
MLHMM
BHMM
BHMM+IS
70
MLHMM
BHMM
BHMM+IS
65
70
55
60
50
50
Accuracy (%)
60
Accuracy (%)
45
40
40
35
30
1 2 3 4 5 6 7 8 9 10
d
30
1 2 3 4 5 6 7 8 9 10
d
</figure>
<bodyText confidence="0.997981486486486">
from 1 to 10 in our experiments. Any unseen (i.e.,
out-of-dictionary) word is ambiguous among the
15 possible tags. Not surprisingly, both ambigu-
ity and the unseen word rate increase with d. For
instance, the ambiguous token rate increases from
40.0% with 1.7 tags/token (d=1) to 77.7% with 8.1
tags/token (d=10). Similarly, the unseen word rate
increases from 16% (d=2) to 46% (d=10). We will
refer to this set of tag dictionaries as Lexicon 1.
The second method generates a set of relaxed
lexicons, Lexicon 2, in essentially the same way
as the first method, except that these lexicons in-
clude only the words that appear at least d times
in the training data. Importantly, the words that
appear solely in the test data are not included in
any of these relaxed POS lexicons. This makes
Lexicon 2 a bit more realistic than Lexicon 1 in
terms of the way they are constructed. As a result,
in comparison to Lexicon 1, Lexicon 2 has a con-
siderably higher ambiguous token rate and unseen
word rate: its ambiguous token rate ranges from
64.3% with 5.3 tags/token (d=1) to 80.5% with 8.6
tags/token (d=10), and its unseen word rate ranges
from 25% (d=1) to 50% (d=10).
The third method, arguably the most realistic
among the three, is motivated by our proposed
weakly supervised approach. In this method, we
(1) form ten different datasets from the (labeled)
training data of sizes 5K words, 10K words, ...,
50K words, and then (2) create one POS lexicon
from each dataset L by listing, for each word w in
L, all the tags associated with w in L. This set of
tag dictionaries, which we will refer to as Lexicon
3, has an ambiguous token rate that ranges from
57.7% with 5.1 tags/token (50K) to 61.5% with
8.1 tags/token (5K), and an unseen word rate that
ranges from 25% (50K) to 50% (5K).
</bodyText>
<subsectionHeader confidence="0.7392965">
5.2 Results and Discussion
5.2.1 Baseline Systems
</subsectionHeader>
<bodyText confidence="0.999988692307692">
We use as our first baseline system G&amp;G’s
Bayesian POS tagging model, as our goal is to
evaluate the effectiveness of our two extensions
in improving their model. To further gauge the
performance of G&amp;G’s model, we employ another
baseline commonly used in POS tagging exper-
iments, which is an unsupervised trigram HMM
trained by running EM to convergence.
As mentioned previously, we evaluate each tag-
ging model by employing the three POS lexicons
described in the previous subsection. Figure 3(a)
shows how the tagging accuracy varies with d
when Lexicon 1 is used. Perhaps not surpris-
ingly, the trigram HMM (MLHMM) and G&amp;G’s
Bayesian model (BHMM) achieve almost identi-
cal accuracies when d=1 (i.e., the complete lexi-
con with a zero unseen word rate). As d increases,
both ambiguity and the unseen word rate increase;
as a result, the tagging accuracy decreases. Also,
consistent with G&amp;G’s results, BHMM outper-
forms MLHMM by a large margin (4–7%).
Similar performance trends can be observed
when Lexicon 2 is used (see Figure 3(b)). How-
ever, both baselines achieve comparatively lower
tagging accuracies, as a result of the higher unseen
word rate associated with Lexicon 2.
</bodyText>
<page confidence="0.993058">
369
</page>
<figure confidence="0.982691">
Lexicon 3
Training data (K)
</figure>
<figureCaption confidence="0.9964385">
Figure 4: Accuracies of the POS tagging models
using Lexicon 3
</figureCaption>
<bodyText confidence="0.995456428571429">
Results using Lexicon 3 are shown in Figure
4. Owing to the availability of POS-tagged sen-
tences, we replace MLHMM with its supervised
counterpart that is trained on the available labeled
data, yielding the SHMM baseline. The accuracies
of SHMM range from 48% to 67%, outperforming
BHMM as the amount of labeled data increases.
</bodyText>
<subsectionHeader confidence="0.848268">
5.2.2 Adding Induced Suffix Emission
</subsectionHeader>
<bodyText confidence="0.9999453125">
Next, we augment BHMM with our first
extension, induced suffix emission, yielding
BHMM+IS. For Lexicon 1, BHMM+IS achieves
the same accuracy as the two baselines when d=1.
The reason is simple: as all the test words are
in the POS lexicon, the tagger never emits an in-
duced suffix. More importantly, BHMM+IS beats
BHMM and MLHMM by 4–9% and 10–14%, re-
spectively. Similar trends are observed for Lex-
icon 2, where BHMM+IS outperforms BHMM
and MLHMM by a larger margin of 5–10% and
12–16%, respectively. For Lexicon 3, BHMM+IS
outperforms SHMM, the stronger baseline, by 6–
11%. Overall, these results suggest that induced
suffix emission is a strong performance-enhancing
extension to G&amp;G’s approach.
</bodyText>
<subsectionHeader confidence="0.969943">
5.2.3 Adding Discriminative Prediction
</subsectionHeader>
<bodyText confidence="0.9987855">
Finally, we augment BHMM+IS with discrimi-
native prediction, yielding BHMM+IS+DP. Since
this extension requires labeled data, it can only be
applied in combination with Lexicon 3. As seen
in Figure 4, BHMM+IS+DP outperforms SHMM
by 10–14%. Its discriminative nature proves to be
</bodyText>
<table confidence="0.98195975">
Predicted Tag Correct Tag % of Error
NN NNP 8.4
NN JJ 6.9
VM VAUX 5.9
</table>
<tableCaption confidence="0.952129">
Table 1: Most frequent POS tagging errors for
BHMM+IS+DP on the 50K-word training set
</tableCaption>
<bodyText confidence="0.957199">
strong as it even beats BHMM+IS by 3–4%.
</bodyText>
<sectionHeader confidence="0.689202" genericHeader="method">
5.2.4 Error Analysis
</sectionHeader>
<bodyText confidence="0.999656230769231">
Table 1 lists the most common types of er-
rors made by the best-performing tagging model,
BHMM+IS+DP (50K-word labeled data). As we
can see, common nouns and proper nouns (row
1) are difficult to distinguish, due in part to the
case insensitivity of Bengali. Also, it is difficult
to distinguish Bengali common nouns and adjec-
tives (row 2), as they are distributionally similar
to each other. The confusion between main verbs
[VM] and auxiliary verbs [VAUX] (row 3) arises
from the fact that certain Bengali verbs can serve
as both a main verb and an auxiliary verb, depend-
ing on the role the verb plays in the verb sequence.
</bodyText>
<sectionHeader confidence="0.999603" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999864473684211">
While Goldwater and Griffiths’s fully-Bayesian
approach and the traditional maximum-likelihood
parameter-based approach to unsupervised POS
tagging have offered promising results for English,
we argued in this paper that such results were ob-
tained under the unrealistic assumption that a per-
fect POS lexicon is available, which renders these
taggers less unsupervised than they appear. As a
result, we investigated a weakly supervised fully-
Bayesian approach to POS tagging, which relaxes
the unrealistic assumption by automatically ac-
quiring the lexicon from a small amount of POS-
tagged data. Since such relaxation comes at the
expense of a drop in tagging accuracy, we pro-
posed two performance-enhancing extensions to
the Bayesian framework, namely, induced suffix
emission and discriminative prediction, which ef-
fectively exploit morphology and techniques from
supervised POS tagging, respectively.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9990055">
We thank the three anonymous reviewers and
Sajib Dasgupta for their comments. We also thank
CRBLP, BRAC University, Bangladesh, for pro-
viding us with Bengali resources and Taufiq Hasan
Al Banna for his MATLAB code. This work was
supported in part by NSF Grant IIS-0812261.
</bodyText>
<figure confidence="0.983999214285714">
80
75
70
Accuracy (%)
65
60
SHMM
BHMM
BHMM+IS
BHMM+IS+DP
45
5 10 15 20 25 30 35 40 45 50
55
50
</figure>
<page confidence="0.986931">
370
</page>
<sectionHeader confidence="0.997383" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996876000000001">
Leonard E. Baum. 1972. An equality and associ-
ated maximization technique in statistical estimation
for probabilistic functions of Markov processes. In-
equalities, 3:1–8.
Alexander Clark. 2000. Inducing syntactic categories
by context distribution clustering. In Proceedings of
CoNLL: Short Papers, pages 91–94.
Alexander Clark. 2003. Combining distributional and
morphological information for part-of-speech induc-
tion. In Proceedings of the EACL, pages 59–66.
Sajib Dasgupta and Vincent Ng. 2007. Unsupervised
part-of-speech acquisition for resource-scarce lan-
guages. In Proceedings of EMNLP-CoNLL, pages
218–227.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society. Series B (Methodological), 39:1–38.
Stuart Geman and Donald Geman. 1984. Stochas-
tic relaxation, Gibbs distributions, and the Bayesian
restoration of images. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 6:721–741.
Walter R. Gilks, Sylvia Richardson, and David
J. Spiegelhalter (editors). 1996. Markov Chain
Monte Carlo in Practice. Chapman &amp; Hall, Suffolk.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. EM can find pretty good HMM POS-taggers
(when given a good start). In Proceedings of ACL-
08:HLT, pages 746–754.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27(2):153–198.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the ACL, pages
744–751.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
HLT-NAACL, pages 320–327.
Mark Johnson. 2007. Why doesn’t EM find good
HMM POS-taggers? In Proceedings of EMNLP-
CoNLL, pages 296–305.
Samarth Keshava and Emily Pitler. 2006. A simpler,
intuitive approach to morpheme induction. In PAS-
CAL Challenge Workshop on Unsupervised Segmen-
tation of Words into Morphemes.
David J. C. MacKay and Linda C. Bauman Peto. 1995.
A hierarchical Dirichlet language model. Natural
Language Engineering, 1:289–307.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2):155–172.
Hinrich Sch¨utze. 1995. Distributional part-of-speech
tagging. In Proceedings of EACL, pages 141–148.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the ACL, pages 354–362.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein,
and Regina Barzilay. 2008. Unsupervised multi-
lingual learning for POS tagging. In Proceedings of
EMNLP, pages 1041–1050.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein,
and Regina Barzilay. 2009. Adding more lan-
guages improves unsupervised multilingual tagging.
In Proceedings ofNAACL-HLT.
Yee Whye Teh, Michael Jordan, Matthew Beal, and
David Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1527–1554.
Kristina Toutanova and Mark Johnson. 2007. A
Bayesian LDA-based model for semi-supervised
part-of-speech tagging. In Proceedings ofNIPS.
Qin Iris Wang and Dale Schuurmans. 2005. Im-
proved estimation for unsupervised part-of-speech
tagging. In Proceedings of the 2005 IEEE Interna-
tional Conference on Natural Language Processing
and Knowledge Engineering (IEEE NLP-KE), pages
219–224.
</reference>
<page confidence="0.998749">
371
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.929850">
<title confidence="0.999453">Weakly Supervised Part-of-Speech Tagging for Morphologically-Rich, Resource-Scarce Languages</title>
<author confidence="0.998757">Saidul Hasan Ng</author>
<affiliation confidence="0.999634">Human Language Technology Research Institute University of Texas at Dallas</affiliation>
<address confidence="0.987605">Richardson, TX 75083-0688</address>
<abstract confidence="0.997080043478261">This paper examines unsupervised approaches to part-of-speech (POS) tagging for morphologically-rich, resource-scarce languages, with an emphasis on Goldwater and Griffiths’s (2007) fully-Bayesian approach originally developed for English POS tagging. We argue that existing unsupervised POS taggers unrealistically assume as input a perfect POS lexicon, and consequently, we propose a weakly supervised fully-Bayesian approach to POS tagging, which relaxes the unrealistic assumption by automatically acquiring the lexicon from a small amount of POS-tagged data. Since such relaxation comes at the expense of a drop in tagging accuracy, we propose two extensions to the Bayesian framework and demonstrate that they are effective in improving a fully-Bayesian POS tagger for Bengali, our representative morphologicallyrich, resource-scarce language.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Leonard E Baum</author>
</authors>
<title>An equality and associated maximization technique in statistical estimation for probabilistic functions of Markov processes.</title>
<date>1972</date>
<journal>Inequalities,</journal>
<volume>3</volume>
<contexts>
<context position="3036" citStr="Baum, 1972" startWordPosition="477" endWordPosition="478">arge fraction of the words are ambiguous. Nevertheless, the development of unsupervised taggers potentially allows POS tagging technologies to be applied to a substantially larger number of natural languages, most of which are resource-scarce and, in particular, have little or no POS-tagged data. The most common approach to unsupervised POS tagging to date has been to train a hidden Markov model (HMM) in an unsupervised manner to maximize the likelihood of an unannotated corpus, using a special instance of the expectationmaximization (EM) algorithm (Dempster et al., 1977) known as Baum-Welch (Baum, 1972). More recently, a fully-Bayesian approach to unsupervised POS tagging has been developed by Goldwater and Griffiths (2007) [henceforth G&amp;G] as a viable alternative to the traditional maximumlikelihood-based HMM approach. While unsupervised POS taggers adopting both approaches have Proceedings of the 12th Conference of the European Chapter of the ACL, pages 363–371, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 363 demonstrated promising results, it is important to note that they are typically evaluated by assuming the availability of a perfect POS l</context>
</contexts>
<marker>Baum, 1972</marker>
<rawString>Leonard E. Baum. 1972. An equality and associated maximization technique in statistical estimation for probabilistic functions of Markov processes. Inequalities, 3:1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Inducing syntactic categories by context distribution clustering.</title>
<date>2000</date>
<booktitle>In Proceedings of CoNLL: Short Papers,</booktitle>
<pages>91--94</pages>
<contexts>
<context position="6451" citStr="Clark (2000)" startWordPosition="1001" endWordPosition="1002">chers typically construct a pseudo-perfect POS lexicon by collecting the possible POS tags of a word directly from the corpus on which the tagger is to be evaluated. ness of our two extensions in improving G&amp;G’s approach. Finally, we conclude in Section 6. 2 Related Work With the notable exception of Synder et al.’s (2008; 2009) recent work on unsupervised multilingual POS tagging, existing approaches to unsupervised POS tagging have been developed and tested primarily on English data. For instance, Merialdo (1994) uses maximum likelihood estimation to train a trigram HMM. Sch¨utze (1995) and Clark (2000) apply syntactic clustering and dimensionality reduction in a knowledge-free setting to obtain meaningful clusters. Haghighi and Klein (2006) develop a prototype-driven approach, which requires just a few prototype examples for each POS tag and exploits these labeled words to constrain the labels of their distributionally similar words. Smith and Eisner (2005) train an unsupervised POS tagger using contrastive estimation, which seeks to move probability mass to a positive example e from its neighbors (i.e., negative examples are created by perturbing e). Wang and Schuurmans (2005) improve an u</context>
</contexts>
<marker>Clark, 2000</marker>
<rawString>Alexander Clark. 2000. Inducing syntactic categories by context distribution clustering. In Proceedings of CoNLL: Short Papers, pages 91–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Combining distributional and morphological information for part-of-speech induction.</title>
<date>2003</date>
<booktitle>In Proceedings of the EACL,</booktitle>
<pages>59--66</pages>
<contexts>
<context position="14817" citStr="Clark (2003)" startWordPosition="2379" endWordPosition="2380">ature and θ2 is the temperature after N sampling iterations. 4 Two Extensions In this section, we present two extensions to G&amp;G’s fully-Bayesian framework to unsupervised POS tagging, namely, induced suffix emission and discriminative prediction. 4.1 Induced Suffix Emission For morphologically-rich languages like Bengali, a lot of grammatical information (e.g., POS) is expressed via suffixes. In fact, several approaches to unsupervised POS induction for morphologicallyrich languages have exploited the observation that some suffixes can only be associated with a small number of POS tags (e.g., Clark (2003), Dasgupta and Ng (2007)). To exploit suffixes in HMMbased POS tagging, one can (1) convert the wordbased POS lexicon to a suffix-based POS lexicon, which lists the possible POS tags for each suffix; and then (2) have the HMM emit suffixes rather than words, subject to the constraints in the suffixbased POS lexicon. Such a suffix-based HMM, however, may suffer from over-generalization. To prevent over-generalization and at the same time exploit suffixes, we propose as our first extension to G&amp;G’s framework a hybrid approach to word/suffix emission: a word is emitted if it is present in the wor</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>Alexander Clark. 2003. Combining distributional and morphological information for part-of-speech induction. In Proceedings of the EACL, pages 59–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sajib Dasgupta</author>
<author>Vincent Ng</author>
</authors>
<title>Unsupervised part-of-speech acquisition for resource-scarce languages.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>218--227</pages>
<contexts>
<context position="14841" citStr="Dasgupta and Ng (2007)" startWordPosition="2381" endWordPosition="2384">s the temperature after N sampling iterations. 4 Two Extensions In this section, we present two extensions to G&amp;G’s fully-Bayesian framework to unsupervised POS tagging, namely, induced suffix emission and discriminative prediction. 4.1 Induced Suffix Emission For morphologically-rich languages like Bengali, a lot of grammatical information (e.g., POS) is expressed via suffixes. In fact, several approaches to unsupervised POS induction for morphologicallyrich languages have exploited the observation that some suffixes can only be associated with a small number of POS tags (e.g., Clark (2003), Dasgupta and Ng (2007)). To exploit suffixes in HMMbased POS tagging, one can (1) convert the wordbased POS lexicon to a suffix-based POS lexicon, which lists the possible POS tags for each suffix; and then (2) have the HMM emit suffixes rather than words, subject to the constraints in the suffixbased POS lexicon. Such a suffix-based HMM, however, may suffer from over-generalization. To prevent over-generalization and at the same time exploit suffixes, we propose as our first extension to G&amp;G’s framework a hybrid approach to word/suffix emission: a word is emitted if it is present in the word-based POS lexicon; oth</context>
</contexts>
<marker>Dasgupta, Ng, 2007</marker>
<rawString>Sajib Dasgupta and Vincent Ng. 2007. Unsupervised part-of-speech acquisition for resource-scarce languages. In Proceedings of EMNLP-CoNLL, pages 218–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur P Dempster</author>
<author>Nan M Laird</author>
<author>Donald B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<pages>39--1</pages>
<contexts>
<context position="3003" citStr="Dempster et al., 1977" startWordPosition="470" endWordPosition="473"> processing researchers, especially when a large fraction of the words are ambiguous. Nevertheless, the development of unsupervised taggers potentially allows POS tagging technologies to be applied to a substantially larger number of natural languages, most of which are resource-scarce and, in particular, have little or no POS-tagged data. The most common approach to unsupervised POS tagging to date has been to train a hidden Markov model (HMM) in an unsupervised manner to maximize the likelihood of an unannotated corpus, using a special instance of the expectationmaximization (EM) algorithm (Dempster et al., 1977) known as Baum-Welch (Baum, 1972). More recently, a fully-Bayesian approach to unsupervised POS tagging has been developed by Goldwater and Griffiths (2007) [henceforth G&amp;G] as a viable alternative to the traditional maximumlikelihood-based HMM approach. While unsupervised POS taggers adopting both approaches have Proceedings of the 12th Conference of the European Chapter of the ACL, pages 363–371, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 363 demonstrated promising results, it is important to note that they are typically evaluated by assuming th</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Geman</author>
<author>Donald Geman</author>
</authors>
<title>Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images.</title>
<date>1984</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>6--721</pages>
<contexts>
<context position="13583" citStr="Geman and Geman, 1984" startWordPosition="2179" endWordPosition="2183">P(tiJt−i, α) = (4) n(ti−2,ti−1) + Tα n(ti,wi) + β P(wiJti, t−i, w−i, β) = (5) nti + Wti β where n(ti−2,ti−1,ti) and n(ti,wi) are the frequencies of observing the tag trigram (ti−2,ti−1,ti) and the tag-word pair (ti, wi), respectively. These counts are taken from the i − 1 tags and words generated previously. The inference procedure described next exploits the property that trigrams (and outputs) are exchangeable; that is, the probability of a set of trigrams (and outputs) does not depend on the order in which it was generated. 3.3 Inference Procedure We perform inference using Gibbs sampling (Geman and Geman, 1984), using the following posterior distribution to generate samples: P(tJw, α, β) a P(wJt, β)P(tJα) Starting with a random assignment of a POS tag to each word (subject to the constraints in the POS lexicon), we resample each POS tag, ti, according to the conditional distribution shown in Figure 2. Note that the current counts of other trigrams and outputs can be used as “previous” observations due to the property of exchangeability. Following G&amp;G, we use simulated annealing to find the MAP tag sequence. The temperature decreases by a factor of exp(log(θ2 N−1 ) after each iterθ1 ) ation, where θ1</context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>Stuart Geman and Donald Geman. 1984. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6:721–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter R Gilks</author>
<author>Sylvia Richardson</author>
<author>J David</author>
</authors>
<date>1996</date>
<booktitle>Markov Chain Monte Carlo in Practice.</booktitle>
<editor>Spiegelhalter (editors).</editor>
<publisher>Chapman &amp; Hall,</publisher>
<location>Suffolk.</location>
<contexts>
<context position="24672" citStr="Gilks et al., 1996" startWordPosition="4069" endWordPosition="4072">the infrequent tags designed to capture Indian language specific structure (e.g., reduplication, echo words) into a category called OTHERS. Hyperparameter settings Recall that our tagger consists of three types of distributions — tag transition distributions, word-based output distributions, and suffix-based output distributions — drawn from a symmetric Dirichlet with α, Q, and γ as the underlying hyperparameters, respectively. We automatically determine the values of these hyperparameters by (1) randomly initializing them and (2) resampling their values by using a Metropolis-Hastings update (Gilks et al., 1996) at the end of each sampling iteration. Details of this update process can be found in G&amp;G. Inference Inference is performed by running a Gibbs sampler for 5000 iterations. The initial temperature is set to 2.0, which is gradually lowered to 0.08 over the iterations. Owing to the randomness involved in hyperparameter initialization, all reported results are averaged over three runs. Lexicon construction methods To better understand the role of a POS lexicon in tagging performance, we evaluate each POS tagging model by employing lexicons constructed by three methods. The first lexicon construct</context>
</contexts>
<marker>Gilks, Richardson, David, 1996</marker>
<rawString>Walter R. Gilks, Sylvia Richardson, and David J. Spiegelhalter (editors). 1996. Markov Chain Monte Carlo in Practice. Chapman &amp; Hall, Suffolk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<title>EM can find pretty good HMM POS-taggers (when given a good start).</title>
<date>2008</date>
<booktitle>In Proceedings of ACL08:HLT,</booktitle>
<pages>746--754</pages>
<contexts>
<context position="10718" citStr="Goldberg et al. (2008)" startWordPosition="1689" endWordPosition="1692">others). This motivates a fully-Bayesian approach, which, rather than committing to a particular set of parameter values as in an EM-based approach, integrates over all possible values of θ and, most importantly, allows the use of priors to favor the learning of the skewed distributions, through the use of the term P(θ|w) in the following equation: �P(t|w) = P(t|w,θ)P(θ|w)dθ (1) The question, then, is: which priors on θ would allow the acquisition of skewed distributions? To 2When given good parameter initializations, however, EM can find good parameter values for an HMM-based POS tagger. See Goldberg et al. (2008) for details. answer this question, recall that in POS tagging, θ is composed of a set of tag transition distributions and output distributions. Each such distribution is a multinomial (i.e., each trial produces exactly one of some finite number of possible outcomes). For a multinomial with K outcomes, a K-dimensional Dirichlet distribution, which is conjugate to the multinomial, is a natural choice of prior. For simplicity, we assume that a distribution in θ is drawn from a symmetric Dirichlet with a certain hyperparameter (see Teh et al. (2006) for details). The value of a hyperparameter, α,</context>
</contexts>
<marker>Goldberg, Adler, Elhadad, 2008</marker>
<rawString>Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008. EM can find pretty good HMM POS-taggers (when given a good start). In Proceedings of ACL08:HLT, pages 746–754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="17251" citStr="Goldsmith (2001)" startWordPosition="2790" endWordPosition="2791">les us to induce 975 suffixes. Constructing a suffix-based POS lexicon Next, we construct a suffix-based POS lexicon. For each word w in the original word-based POS lexicon, we (1) use the induced suffix list obtained in the previous step to identify the longest-matching suffix of w, and then (2) assign all the POS tags associated with w to this suffix. Incorporating suffix-based output distributions Finally, we extend our trigram model by introduc3The dependence on frequency and length is motivated by the observation that less frequent and shorter affixes are more likely to be erroneous (see Goldsmith (2001)). 366 P(ti |t−i, w, α, β) a n(ti,wz) + β n(ti−2,ti−1,ti) + α . n(ti−1,ti,ti+1) + I(ti−2 = ti−1 = ti = ti+1) + α nti + Wtiβ . n(ti−2,ti−1) + Tα n(ti−1,ti) + I(ti−2 = ti−1 = ti) + Tα n(ti,ti+1,ti+2) + I(ti−2 = ti = ti+2, ti−1 = ti+1) + I(ti−1 = ti = ti+1 = ti+2) + α . n(ti,ti+1) + I(ti−2 = ti, ti−1 = ti+1) + I(ti−1 = ti = ti+1) + Tα Figure 2: The sampling distribution for ti (taken directly from Goldwater and Griffiths (2007)). All nx values are computed from the current values of all tags except for ti. Here, I(arg) is a function that returns 1 if arg is true and 0 otherwise, and t−i refers to</context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>John Goldsmith. 2001. Unsupervised learning of the morphology of a natural language. Computational Linguistics, 27(2):153–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-ofspeech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>744--751</pages>
<contexts>
<context position="3159" citStr="Goldwater and Griffiths (2007)" startWordPosition="493" endWordPosition="496">lly allows POS tagging technologies to be applied to a substantially larger number of natural languages, most of which are resource-scarce and, in particular, have little or no POS-tagged data. The most common approach to unsupervised POS tagging to date has been to train a hidden Markov model (HMM) in an unsupervised manner to maximize the likelihood of an unannotated corpus, using a special instance of the expectationmaximization (EM) algorithm (Dempster et al., 1977) known as Baum-Welch (Baum, 1972). More recently, a fully-Bayesian approach to unsupervised POS tagging has been developed by Goldwater and Griffiths (2007) [henceforth G&amp;G] as a viable alternative to the traditional maximumlikelihood-based HMM approach. While unsupervised POS taggers adopting both approaches have Proceedings of the 12th Conference of the European Chapter of the ACL, pages 363–371, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 363 demonstrated promising results, it is important to note that they are typically evaluated by assuming the availability of a perfect POS lexicon. This assumption, however, is fairly unrealistic in practice, as a perfect POS lexicon can only be constructed by ha</context>
<context position="7282" citStr="Goldwater and Griffiths (2007)" startWordPosition="1116" endWordPosition="1119">few prototype examples for each POS tag and exploits these labeled words to constrain the labels of their distributionally similar words. Smith and Eisner (2005) train an unsupervised POS tagger using contrastive estimation, which seeks to move probability mass to a positive example e from its neighbors (i.e., negative examples are created by perturbing e). Wang and Schuurmans (2005) improve an unsupervised HMM-based tagger by constraining the learned structure to maintain appropriate marginal tag probabilities and using word similarities to smooth the lexical parameters. As mentioned before, Goldwater and Griffiths (2007) have recently proposed an unsupervised fully-Bayesian POS tagging framework that operates by integrating over the possible parameter values instead of fixing a set of parameter values for unsupervised sequence learning. Importantly, this Bayesian approach facilitates the incorporation of sparse priors that result in a more practical distribution of tokens to lexical categories (Johnson, 2007). Similar to Goldwater and Griffiths (2007) and Johnson (2007), Toutanova and Johnson (2007) also use Bayesian inference for POS tagging. However, their work departs from existing Bayesian approaches to P</context>
<context position="17679" citStr="Goldwater and Griffiths (2007)" startWordPosition="2880" endWordPosition="2883">e extend our trigram model by introduc3The dependence on frequency and length is motivated by the observation that less frequent and shorter affixes are more likely to be erroneous (see Goldsmith (2001)). 366 P(ti |t−i, w, α, β) a n(ti,wz) + β n(ti−2,ti−1,ti) + α . n(ti−1,ti,ti+1) + I(ti−2 = ti−1 = ti = ti+1) + α nti + Wtiβ . n(ti−2,ti−1) + Tα n(ti−1,ti) + I(ti−2 = ti−1 = ti) + Tα n(ti,ti+1,ti+2) + I(ti−2 = ti = ti+2, ti−1 = ti+1) + I(ti−1 = ti = ti+1 = ti+2) + α . n(ti,ti+1) + I(ti−2 = ti, ti−1 = ti+1) + I(ti−1 = ti = ti+1) + Tα Figure 2: The sampling distribution for ti (taken directly from Goldwater and Griffiths (2007)). All nx values are computed from the current values of all tags except for ti. Here, I(arg) is a function that returns 1 if arg is true and 0 otherwise, and t−i refers to the current values of all tags except for ti. ing a state-specific probability distribution over induced suffixes. Specifically, if the current word is present in the word-based POS lexicon, or if we cannot find any suffix for the word using the induced suffix list, then we emit the word. Otherwise, we emit its suffix according to a suffix-based output distribution, which is drawn from a symmetric Dirichlet with hyperparame</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Thomas L. Griffiths. 2007. A fully Bayesian approach to unsupervised part-ofspeech tagging. In Proceedings of the ACL, pages 744–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>320--327</pages>
<contexts>
<context position="6592" citStr="Haghighi and Klein (2006)" startWordPosition="1017" endWordPosition="1020"> which the tagger is to be evaluated. ness of our two extensions in improving G&amp;G’s approach. Finally, we conclude in Section 6. 2 Related Work With the notable exception of Synder et al.’s (2008; 2009) recent work on unsupervised multilingual POS tagging, existing approaches to unsupervised POS tagging have been developed and tested primarily on English data. For instance, Merialdo (1994) uses maximum likelihood estimation to train a trigram HMM. Sch¨utze (1995) and Clark (2000) apply syntactic clustering and dimensionality reduction in a knowledge-free setting to obtain meaningful clusters. Haghighi and Klein (2006) develop a prototype-driven approach, which requires just a few prototype examples for each POS tag and exploits these labeled words to constrain the labels of their distributionally similar words. Smith and Eisner (2005) train an unsupervised POS tagger using contrastive estimation, which seeks to move probability mass to a positive example e from its neighbors (i.e., negative examples are created by perturbing e). Wang and Schuurmans (2005) improve an unsupervised HMM-based tagger by constraining the learned structure to maintain appropriate marginal tag probabilities and using word similari</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. 2006. Prototype-driven learning for sequence models. In Proceedings of HLT-NAACL, pages 320–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers?</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL,</booktitle>
<pages>296--305</pages>
<contexts>
<context position="7678" citStr="Johnson, 2007" startWordPosition="1174" endWordPosition="1176">MM-based tagger by constraining the learned structure to maintain appropriate marginal tag probabilities and using word similarities to smooth the lexical parameters. As mentioned before, Goldwater and Griffiths (2007) have recently proposed an unsupervised fully-Bayesian POS tagging framework that operates by integrating over the possible parameter values instead of fixing a set of parameter values for unsupervised sequence learning. Importantly, this Bayesian approach facilitates the incorporation of sparse priors that result in a more practical distribution of tokens to lexical categories (Johnson, 2007). Similar to Goldwater and Griffiths (2007) and Johnson (2007), Toutanova and Johnson (2007) also use Bayesian inference for POS tagging. However, their work departs from existing Bayesian approaches to POS tagging in that they (1) introduce a new sparse prior on the distribution over tags for each word, (2) extend the Latent Dirichlet Allocation model, and (3) explicitly model ambiguity class. While their tagging model, like Goldwater and Griffiths’s, assumes as input an incomplete POS lexicon and a large unlabeled corpus, they consider their approach “semisupervised” simply because of the hu</context>
<context position="9948" citStr="Johnson, 2007" startWordPosition="1565" endWordPosition="1566">ntil the end of the word sequence is reached. In other words, the parameters of an HMM, θ, are composed of a set of statespecific (1) output distributions (over word tokens) and (2) transition distributions, both of which can be learned using the EM algorithm. Once learning is complete, we can use the resulting set of parameters to find the most likely hidden state sequence given a word sequence using the Viterbi algorithm. Nevertheless, EM sometimes fails to find good parameter values.2 The reason is that EM tries to assign roughly the same number of word tokens to each of the hidden states (Johnson, 2007). In practice, however, the distribution of word tokens to POS tags is highly skewed (i.e., some POS categories are more populated with tokens than others). This motivates a fully-Bayesian approach, which, rather than committing to a particular set of parameter values as in an EM-based approach, integrates over all possible values of θ and, most importantly, allows the use of priors to favor the learning of the skewed distributions, through the use of the term P(θ|w) in the following equation: �P(t|w) = P(t|w,θ)P(θ|w)dθ (1) The question, then, is: which priors on θ would allow the acquisition </context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Why doesn’t EM find good HMM POS-taggers? In Proceedings of EMNLPCoNLL, pages 296–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samarth Keshava</author>
<author>Emily Pitler</author>
</authors>
<title>A simpler, intuitive approach to morpheme induction.</title>
<date>2006</date>
<booktitle>In PASCAL Challenge Workshop on Unsupervised Segmentation of Words into Morphemes.</booktitle>
<marker>Keshava, Pitler, 2006</marker>
<rawString>Samarth Keshava and Emily Pitler. 2006. A simpler, intuitive approach to morpheme induction. In PASCAL Challenge Workshop on Unsupervised Segmentation of Words into Morphemes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J C MacKay</author>
<author>Linda C Bauman Peto</author>
</authors>
<title>A hierarchical Dirichlet language model.</title>
<date>1995</date>
<journal>Natural Language Engineering,</journal>
<pages>1--289</pages>
<contexts>
<context position="12072" citStr="MacKay and Peto (1995)" startWordPosition="1921" endWordPosition="1924"> when α &lt; 1, higher probabilities are assigned to sparse multinomials (i.e., multinomials in which only a few entries are nonzero). Intuitively, the tag transition distributions and the output distributions in an HMM-based POS tagger are sparse multinomials. As a result, it is logical to choose a Dirichlet prior with α &lt; 1. By integrating over all possible parameter values, the probability that i-th outcome, yi, takes the value k, given the previous i − 1 outcomes y−i= (y1, y2, ..., yi−1), is �P(k|y−i, α) = P(k|θ)P(θ|y−i,α)dθ(2) nk + α (3) i − 1 + Kα where nk is the frequency of k in y−i. See MacKay and Peto (1995) for the derivation. 3.2 Model Our baseline POS tagging model is a standard trigram HMM with tag transition distributions and output distributions, each of which is a sparse multinomial that is learned by applying a symmetric Dirichlet prior: ti |ti−1, ti−2, τ(ti−1,ti−2) ∼ Mult(τ(ti−1,ti−2)) wi |ti, ω(ti) ∼ Mult(ω(ti)) τ(ti−1,ti−2) |α ∼ Dirichlet(α) ω(ti) |β ∼ Dirichlet(β) where wi and ti denote the i-th word and tag. With a tagset of size T (including a special tag used as sentence delimiter), each of the tag transition distributions has T components. For the output symbols, each of the ω(ti)</context>
</contexts>
<marker>MacKay, Peto, 1995</marker>
<rawString>David J. C. MacKay and Linda C. Bauman Peto. 1995. A hierarchical Dirichlet language model. Natural Language Engineering, 1:289–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="6359" citStr="Merialdo (1994)" startWordPosition="986" endWordPosition="987">agging, focusing on evaluating the effective1When evaluating an unsupervised POS tagger, researchers typically construct a pseudo-perfect POS lexicon by collecting the possible POS tags of a word directly from the corpus on which the tagger is to be evaluated. ness of our two extensions in improving G&amp;G’s approach. Finally, we conclude in Section 6. 2 Related Work With the notable exception of Synder et al.’s (2008; 2009) recent work on unsupervised multilingual POS tagging, existing approaches to unsupervised POS tagging have been developed and tested primarily on English data. For instance, Merialdo (1994) uses maximum likelihood estimation to train a trigram HMM. Sch¨utze (1995) and Clark (2000) apply syntactic clustering and dimensionality reduction in a knowledge-free setting to obtain meaningful clusters. Haghighi and Klein (2006) develop a prototype-driven approach, which requires just a few prototype examples for each POS tag and exploits these labeled words to constrain the labels of their distributionally similar words. Smith and Eisner (2005) train an unsupervised POS tagger using contrastive estimation, which seeks to move probability mass to a positive example e from its neighbors (i</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2):155–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Distributional part-of-speech tagging.</title>
<date>1995</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>141--148</pages>
<marker>Sch¨utze, 1995</marker>
<rawString>Hinrich Sch¨utze. 1995. Distributional part-of-speech tagging. In Proceedings of EACL, pages 141–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>354--362</pages>
<contexts>
<context position="6813" citStr="Smith and Eisner (2005)" startWordPosition="1050" endWordPosition="1053">ervised multilingual POS tagging, existing approaches to unsupervised POS tagging have been developed and tested primarily on English data. For instance, Merialdo (1994) uses maximum likelihood estimation to train a trigram HMM. Sch¨utze (1995) and Clark (2000) apply syntactic clustering and dimensionality reduction in a knowledge-free setting to obtain meaningful clusters. Haghighi and Klein (2006) develop a prototype-driven approach, which requires just a few prototype examples for each POS tag and exploits these labeled words to constrain the labels of their distributionally similar words. Smith and Eisner (2005) train an unsupervised POS tagger using contrastive estimation, which seeks to move probability mass to a positive example e from its neighbors (i.e., negative examples are created by perturbing e). Wang and Schuurmans (2005) improve an unsupervised HMM-based tagger by constraining the learned structure to maintain appropriate marginal tag probabilities and using word similarities to smooth the lexical parameters. As mentioned before, Goldwater and Griffiths (2007) have recently proposed an unsupervised fully-Bayesian POS tagging framework that operates by integrating over the possible paramet</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of the ACL, pages 354–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Tahira Naseem</author>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised multilingual learning for POS tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1041--1050</pages>
<marker>Snyder, Naseem, Eisenstein, Barzilay, 2008</marker>
<rawString>Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and Regina Barzilay. 2008. Unsupervised multilingual learning for POS tagging. In Proceedings of EMNLP, pages 1041–1050.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Tahira Naseem</author>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Adding more languages improves unsupervised multilingual tagging.</title>
<date>2009</date>
<booktitle>In Proceedings ofNAACL-HLT.</booktitle>
<marker>Snyder, Naseem, Eisenstein, Barzilay, 2009</marker>
<rawString>Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and Regina Barzilay. 2009. Adding more languages improves unsupervised multilingual tagging. In Proceedings ofNAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael Jordan</author>
<author>Matthew Beal</author>
<author>David Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="11270" citStr="Teh et al. (2006)" startWordPosition="1779" endWordPosition="1782">er values for an HMM-based POS tagger. See Goldberg et al. (2008) for details. answer this question, recall that in POS tagging, θ is composed of a set of tag transition distributions and output distributions. Each such distribution is a multinomial (i.e., each trial produces exactly one of some finite number of possible outcomes). For a multinomial with K outcomes, a K-dimensional Dirichlet distribution, which is conjugate to the multinomial, is a natural choice of prior. For simplicity, we assume that a distribution in θ is drawn from a symmetric Dirichlet with a certain hyperparameter (see Teh et al. (2006) for details). The value of a hyperparameter, α, affects the skewness of the resulting distribution, as it assigns different probabilities to different distributions. For instance, when α &lt; 1, higher probabilities are assigned to sparse multinomials (i.e., multinomials in which only a few entries are nonzero). Intuitively, the tag transition distributions and the output distributions in an HMM-based POS tagger are sparse multinomials. As a result, it is logical to choose a Dirichlet prior with α &lt; 1. By integrating over all possible parameter values, the probability that i-th outcome, yi, take</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee Whye Teh, Michael Jordan, Matthew Beal, and David Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1527–1554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Mark Johnson</author>
</authors>
<title>A Bayesian LDA-based model for semi-supervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings ofNIPS.</booktitle>
<contexts>
<context position="7770" citStr="Toutanova and Johnson (2007)" startWordPosition="1186" endWordPosition="1190">marginal tag probabilities and using word similarities to smooth the lexical parameters. As mentioned before, Goldwater and Griffiths (2007) have recently proposed an unsupervised fully-Bayesian POS tagging framework that operates by integrating over the possible parameter values instead of fixing a set of parameter values for unsupervised sequence learning. Importantly, this Bayesian approach facilitates the incorporation of sparse priors that result in a more practical distribution of tokens to lexical categories (Johnson, 2007). Similar to Goldwater and Griffiths (2007) and Johnson (2007), Toutanova and Johnson (2007) also use Bayesian inference for POS tagging. However, their work departs from existing Bayesian approaches to POS tagging in that they (1) introduce a new sparse prior on the distribution over tags for each word, (2) extend the Latent Dirichlet Allocation model, and (3) explicitly model ambiguity class. While their tagging model, like Goldwater and Griffiths’s, assumes as input an incomplete POS lexicon and a large unlabeled corpus, they consider their approach “semisupervised” simply because of the human knowledge involved in constructing the POS lexicon. 364 3 A Fully Bayesian Approach 3.1 </context>
</contexts>
<marker>Toutanova, Johnson, 2007</marker>
<rawString>Kristina Toutanova and Mark Johnson. 2007. A Bayesian LDA-based model for semi-supervised part-of-speech tagging. In Proceedings ofNIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Iris Wang</author>
<author>Dale Schuurmans</author>
</authors>
<title>Improved estimation for unsupervised part-of-speech tagging.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 IEEE International Conference on Natural Language Processing and Knowledge Engineering (IEEE NLP-KE),</booktitle>
<pages>219--224</pages>
<contexts>
<context position="7038" citStr="Wang and Schuurmans (2005)" startWordPosition="1084" endWordPosition="1087">m HMM. Sch¨utze (1995) and Clark (2000) apply syntactic clustering and dimensionality reduction in a knowledge-free setting to obtain meaningful clusters. Haghighi and Klein (2006) develop a prototype-driven approach, which requires just a few prototype examples for each POS tag and exploits these labeled words to constrain the labels of their distributionally similar words. Smith and Eisner (2005) train an unsupervised POS tagger using contrastive estimation, which seeks to move probability mass to a positive example e from its neighbors (i.e., negative examples are created by perturbing e). Wang and Schuurmans (2005) improve an unsupervised HMM-based tagger by constraining the learned structure to maintain appropriate marginal tag probabilities and using word similarities to smooth the lexical parameters. As mentioned before, Goldwater and Griffiths (2007) have recently proposed an unsupervised fully-Bayesian POS tagging framework that operates by integrating over the possible parameter values instead of fixing a set of parameter values for unsupervised sequence learning. Importantly, this Bayesian approach facilitates the incorporation of sparse priors that result in a more practical distribution of toke</context>
</contexts>
<marker>Wang, Schuurmans, 2005</marker>
<rawString>Qin Iris Wang and Dale Schuurmans. 2005. Improved estimation for unsupervised part-of-speech tagging. In Proceedings of the 2005 IEEE International Conference on Natural Language Processing and Knowledge Engineering (IEEE NLP-KE), pages 219–224.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>