<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.974894">
Translating HPSG-style Outputs of a Robust Parser
into Typed Dynamic Logic
</title>
<author confidence="0.91476">
Manabu Sato† Daisuke Bekki$ Yusuke Miyao† Jun’ichi Tsujii†*
</author>
<affiliation confidence="0.8331326">
† Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan
$ Center for Evolutionary Cognitive Sciences, University of Tokyo
Komaba 3-8-1, Meguro-ku, Tokyo 153-8902, Japan
*School of Informatics, University of Manchester
</affiliation>
<address confidence="0.76908625">
PO Box 88, Sackville St, Manchester M60 1QD, UK
*SORST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012, Japan
† {sa-ma, yusuke, tsujii}@is.s.u-tokyo.ac.jp
</address>
<email confidence="0.98741">
$ bekki@ecs.c.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.994297" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999872388888889">
The present paper proposes a method
by which to translate outputs of a ro-
bust HPSG parser into semantic rep-
resentations of Typed Dynamic Logic
(TDL), a dynamic plural semantics de-
fined in typed lambda calculus. With
its higher-order representations of con-
texts, TDL analyzes and describes
the inherently inter-sentential nature of
quantification and anaphora in a strictly
lexicalized and compositional manner.
The present study shows that the pro-
posed translation method successfully
combines robustness and descriptive ad-
equacy of contemporary semantics. The
present implementation achieves high
coverage, approximately 90%, for the
real text of the Penn Treebank corpus.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999925037037037">
Robust parsing technology is one result of the
recent fusion between symbolic and statistical
approaches in natural language processing and
has been applied to tasks such as information
extraction, information retrieval and machine
translation (Hockenmaier and Steedman, 2002;
Miyao et al., 2005). However, reflecting the
field boundary and unestablished interfaces be-
tween syntax and semantics in formal theory
of grammar, this fusion has achieved less in
semantics than in syntax.
For example, a system that translates the
output of a robust CCG parser into seman-
tic representations has been developed (Bos et
al., 2004). While its corpus-oriented parser at-
tained high coverage with respect to real text,
the expressive power of the resulting semantic
representations is confined to first-order predi-
cate logic.
The more elaborate tasks tied to discourse
information and plurality, such as resolution
of anaphora antecedent, scope ambiguity, pre-
supposition, topic and focus, are required to
refer to ‘deeper’ semantic structures, such as
dynamic semantics (Groenendijk and Stokhof,
1991).
However, most dynamic semantic theories
are not equipped with large-scale syntax that
covers more than a small fragment of target
languages. One of a few exceptions is Min-
imal Recursion Semantics (MRS) (Copestake
et al., 1999), which is compatible with large-
scale HPSG syntax (Pollard and Sag, 1994)
and has affinities with UDRS (Reyle, 1993).
For real text, however, its implementation, as
in the case of the ERG parser (Copestake
and Flickinger, 2000), restricts its target to the
static fragment of MRS and yet has a lower
coverage than corpus-oriented parsers (Baldwin,
to appear).
The lack of transparency between syntax and
discourse semantics appears to have created a
tension between the robustness of syntax and
the descriptive adequacy of semantics.
In the present paper, we will introduce
a robust method to obtain dynamic seman-
tic representations based on Typed Dynamic
Logic (TDL) (Bekki, 2000) from real text
by translating the outputs of a robust HPSG
parser (Miyao et al., 2005). Typed Dy-
namic Logic is a dynamic plural seman-
tics that formalizes the structure underlying
the semantic interactions between quantifica-
tion, plurality, bound variable/E-type anaphora
</bodyText>
<page confidence="0.955036">
707
</page>
<note confidence="0.9156485">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 707–714,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<figure confidence="0.983668142857143">
�rex···xe�→txi1 ···xin ≡ λ G(i�→e)�→t.λ gi�→e.g ∈ G∧r­gx1,... ,gxm
∼ φ prop ≡ λ G(i�→e)�→t.λ gi�→e.g ∈ G ∧ -∃hi�→e.h ∈ φ G
φ prop
...
ϕprop
⎡ ⎤
⎣ ⎦
</figure>
<table confidence="0.724294333333333">
≡ λ G(i&apos;e)&apos;t.(ϕ ···(φ G))
ref (xi)[φ prop][ϕprop] ≡λ G(i�→e)i+t. ⎧ if Glx=φ Gl ⎫
⎨ then λ gi�→e.g ∈ ϕG ∧ Gl ⎬
⎩ x x =ϕGlx ⎭
otherwise undefined
where prop ≡ ((i H e) H t) H (i H e) H t
⎛ gα ∈ Gα�→t ≡ Gg ⎞
⎜ ⎝ G(i&apos;e)�→t/xi ≡ ⎠ ⎟
λ de.∃gi&amp;quot;.g ∈ G∧gx = d
</table>
<figureCaption confidence="0.99971">
Figure 1: Propositions of TDL (Bekki, 2005)
</figureCaption>
<bodyText confidence="0.9864285">
and presuppositions. All of this complex
discourse/plurality-related information is encap-
sulated within higher-order structures in TDL,
and the analysis remains strictly lexical and
compositional, which makes its interface with
syntax transparent and straightforward. This is
a significant advantage for achieving robustness
in natural language processing.
</bodyText>
<sectionHeader confidence="0.995202" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.997973">
2.1 Typed Dynamic Logic
</subsectionHeader>
<bodyText confidence="0.997124235294117">
Figure 1 shows a number of propositions de-
fined in (Bekki, 2005), including atomic pred-
icate, negation, conjunction, and anaphoric ex-
pression. Typed Dynamic Logic is described in
typed lambda calculus (Gödel’s System T) with
four ground types: e(entity), i(index), n(natural
number), and t(truth). While assignment func-
tions in static logic are functions in meta-
language from type e variables (in the case of
first-order logic) to objects in the domain De,
assignment functions in TDL are functions in
object-language from indices to entities. Typed
Dynamic Logic defines the notion context as
a set of assignment functions (an object of
type (i H e) H t) and a proposition as a func-
tion from context to context (an object of type
((i H e) H t) H (i H e) H t). The conjunctions
of two propositions are then defined as com-
posite functions thereof. This setting conforms
to the view of “propositions as information
flow”, which is widely accepted in dynamic
semantics.
Since all of these higher-order notions are
described in lambda terms, the path for compo-
sitional type-theoretic semantics based on func-
tional application, functional composition and
type raising is clarified. The derivations of
TDL semantic representations for the sentences
“A boy ran. He tumbled.” are exemplified in
Figure 2 and Figure 3. With some instantia-
tion of variables, the semantic representations
of these two sentences are simply conjoined
and yield a single representation, as shown in
(1).
</bodyText>
<equation confidence="0.9907482">
⎡ ⎤
boy&apos;x1s1
⎢ run&apos;e1s1 ⎥
⎢ ⎥
⎢agent&apos;e1x1 ⎥
⎣ ⎦
r f tumble&apos;e s 1
L 22
ref (x2) [� J
agent e2x2
</equation>
<bodyText confidence="0.889358538461538">
The propositions boy&apos;x1s1, run&apos;e1s1 and
agent&apos;e1x1 roughly mean “the entity referred
to by x1 is a boy in the situation s1”, “the
event referred to by e1 is a running event in
the situation s1”, and “the agent of event e1
is x1”, respectively.
The former part of (1) that corresponds to
the first sentence, filtering and testing the input
context, returns the updated context schema-
tized in (2). The updated context is then
passed to the latter part, which corresponds to
the second sentence as its input.
··· x1 s1 e1 ···
john ... running1
john ... running2
. situation1 .
situation2
.
This mechanism makes anaphoric expressions,
such as “He” in “He tumbles”, accessible to its
preceding context; namely, the descriptions of
their presuppositions can refer to the preceding
context compositionally. Moreover, the refer-
ents of the anaphoric expressions are correctly
calculated as a result of previous filtering and
testing.
</bodyText>
<page confidence="0.914655">
708
</page>
<equation confidence="0.984220142857143">
“a”
λ ni7�i7�p7�p.λ wi7�i7�i7�p7�p.
λ ei.λ si.λ φ p.nx1s[wx1esφ ]
“boy”boy0xs
Jλ xi.λ si.λ φ p. φ
ei.λ si.λφ p. ∙ boy0x1s
wx1esφ J
“ran”
λ sbj(i7�i7�i7�p7�p)7�i7�i7�p7�p.
r run0es 11
sbj Cλ xi.λ ei.λ si.λ φ p. L agent0ex J J
φ
⎤
⎦⎥
⎡
⎢
λ ei.λ si.λφ p. ⎣
boy0x1s1
run0es
agent0ex1
φ
</equation>
<figureCaption confidence="0.975435">
Figure 2: Derivation of a TDL semantic representation of “A boy ran”.
</figureCaption>
<figure confidence="0.302726714285714">
“tumbled”
“he” λ sbj(i7�i7�i7�p7�p)7�i7�i7�p7�p.
λ wi7�i7�i7�p7�p. C r agentstumblex 1 \
λ ei.λ si.λφ p.ref (x2)[][wx2esφ ] sbj �,x&apos;.�,e&apos;.�,s&apos;.�,�p. L agent�ex Jll
φ
ei.λ si.λ φ p.ref (x2) [] [ agentltumble2x2 1
L agent�e2x2 J
</figure>
<figureCaption confidence="0.999366">
Figure 3: Derivation of TDL semantic representation of “He tumbled”.
</figureCaption>
<bodyText confidence="0.99978">
Although the antecedent for x2 is not de-
termined in this structure, the possible candi-
dates can be enumerated: x1, s1 and e1, which
precede x2. Since TDL seamlessly represents
linguistic notions such as “entity”, “event” and
“situation”, by indices, the anaphoric expres-
sions, such as “the event” and “that case”, can
be treated in the same manner.
</bodyText>
<subsectionHeader confidence="0.977146">
2.2 Head-driven Phrase Structure
Grammar
</subsectionHeader>
<bodyText confidence="0.999596428571429">
Head-driven Phrase Structure Grammar (Pollard
and Sag, 1994) is a kind of lexicalized gram-
mar that consists of lexical items and a small
number of composition rules called schema.
Schemata and lexical items are all described
in typed feature structures and the unification
operation defined thereon.
Figure 4 is an example of a parse tree,
where the feature structures marked with the
same boxed numbers have a shared struc-
ture. In the first stage of the derivation of
this tree, lexical items are assigned to each
of the strings, “John” and “runs.” Next, the
mother node, which dominates the two items,
</bodyText>
<figure confidence="0.983132777777778">
PHON “John runs”
HEAD 1
SUBJ h i
COMPS h i
PHON “John”
HEAD noun
SUBJ h i
COMPS h i
John runs
</figure>
<figureCaption confidence="0.999912">
Figure 4: An HPSG parse tree
</figureCaption>
<bodyText confidence="0.999973666666667">
is generated by the application of Subject-Head
Schema. The recursive application of these op-
erations derives the entire tree.
</bodyText>
<sectionHeader confidence="0.97115" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999838666666667">
In this section, we present a method to de-
rive TDL semantic representations from HPSG
parse trees, adopting, in part, a previous
method (Bos et al., 2004). Basically, we first
assign TDL representations to lexical items that
are terminal nodes of a parse tree, and then
compose the TDL representation for the en-
tire tree according to the tree structure (Figure
5). One problematic aspect of this approach is
that the composition process of TDL semantic
representations and that of HPSG parse trees
are not identical. For example, in the HPSG
</bodyText>
<figure confidence="0.996473159090909">
PHON “boy”
∙ J
noun
HEAD
MOD h i
� SUBJ h i
VAL COMPS h i J
SPR hdeti J
SLASH h i
⎤
⎤
⎥
⎥ ⎥
⎥ ⎥ ⎥
⎥ ⎥
⎥ ⎦
⎦
⎡
⎢ ⎢ ⎢ ⎢ ⎣
(3)
⎡
⎢ ⎢ ⎢ ⎢ ⎢ ⎣
SYN
SEM
⎡
⎢ ⎣
⎤
⎦⎥
⎡
⎢ ⎣
⎤
⎦⎥
: 2
⎡
⎢ ⎢ ⎣
PHON “runs”
HEAD verb : 1
SUBJ h 2 i
COMPS h i
⎤
⎦⎥⎥
709
PHON “John runs”
HEAD 1
SUBJ ( )
COMPS ( )
normal composition
word formation
nonlocal application
unary derivation
*
+
empty_
⎡
⎣
⎤
⎦
λ e.λ s.λ φ .
r run&apos;es 1
ref (x1)[John&apos;x1s1] L agent&apos;ex1 J
φ
∗run
Subject-Head Schema
PHON “John”
HEAD noun
SUBJ ( )
COMPS ( )
⎤ ⎡PHON “runs”
⎢ HEAD verb : 1
⎦ : 2 ⎣
SUBJ ( 2 )
COMPS ( )
Composition Rules
/ Aw.λ e.λ s.λ φ .
(/\ ref(x1)[John&apos;x1s1][wx1esφ ]
∗John
_empty_
λ sbj.sbj Ãr 11
λ x.λ e.λ s.λφ . L agent&apos;
ex J
φ
∗run
empty_
⎡
⎣
+
John runs John runs
Assignment Rules
</figure>
<figureCaption confidence="0.999986">
Figure 5: Example of the application of the rules
</figureCaption>
<bodyText confidence="0.998484607142857">
parser, a compound noun is regarded as two
distinct words, whereas in TDL, a compound
noun is regarded as one word. Long-distance
dependency is also treated differently in the
two systems. Furthermore, TDL has an opera-
tion called unary derivation to deal with empty
categories, whereas the HPSG parser does not
have such an operation.
In order to overcome these differences and
realize a straightforward composition of TDL
representations according to the HPSG parse
tree, we defined two extended composition
rules, word formation rule and non-local
application rule, and redefined TDL unary
derivation rules for the use in the HPSG
parser. At each step of the composition, one
composition rule is chosen from the set of
rules, based on the information of the schemata
applied to the HPSG tree and TDL represen-
tations of the constituents. In addition, we de-
fined extended TDL semantic representations,
referred to as TDL Extended Structures (TD-
LESs), to be paired with the extended compo-
sition rules.
In summary, the proposed method is com-
prised of TDLESs, assignment rules, composi-
tion rules, and unary derivation rules, as will
be elucidated in subsequent sections.
</bodyText>
<subsectionHeader confidence="0.999396">
3.1 Data Structure
</subsectionHeader>
<bodyText confidence="0.999883555555555">
A TDLES is a tuple (T, p,n), where T is an
extended TDL term, which can be either a
TDL term or a special value ω. Here, ω
is a value used by the word formation rule,
which indicates that the word is a word modi-
fier (See Section 3.3). In addition, p and n are
the necessary information for extended compo-
sition rules, where p is a matrix predicate in T
and is used by the word formation rule, and
n is a nonlocal argument, which takes either
a variable occurring in T or an empty value.
This element corresponds to the SLASH fea-
ture in HPSG and is used by the nonlocal
application rule.
The TDLES of the common noun “boy” is
given in (4). The contents of the structure
are T, p and n, beginning at the top. In
(4), T corresponds to the TDL term of “boy”
in Figure 2, p is the predicate boy, which is
identical to a predicate in the TDL term (the
identity relation between the two is indicated
by “∗”). If either T or p is changed, the other
will be changed accordingly. This mechanism
is a part of the word formation rule, which
offers advantages in creating a new predicate
from multiple words. Finally, n is an empty
value.
</bodyText>
<equation confidence="0.9509045">
* λ x.λ s.λ φ . L ∗boy&apos;xs 1
L0 J
∗boy
_empty_
</equation>
<subsectionHeader confidence="0.999391">
3.2 Assignment Rules
</subsectionHeader>
<bodyText confidence="0.9981325">
We define assignment rules to associate HPSG
lexical items with corresponding TDLESs. For
closed class words, such as “a”, “the” or
“not”, assignment rules are given in the form
of a template for each word as exemplified
below.
</bodyText>
<construct confidence="0.348135333333333">
&amp;quot; PHON “a” #
HEAD det
SPEC (noun)
</construct>
<equation confidence="0.4806816">
⇓ ¸
/λ x.As.λφ&apos; An.λ w.λ e.λ s.λ φ
nx1s£wx1esφ ¤ \
\ empty_ /
_empty_
</equation>
<page confidence="0.944176">
710
</page>
<bodyText confidence="0.915201310344828">
Shown in (5) is an assignment rule for the
indefinite determiner “a”. The upper half of
(5) shows a template of an HPSG lexical item
that specifies its phonetic form as “a”, where
POS is a determiner and specifies a noun. A
TDLES is shown in the lower half of the fig-
ure. The TDL term slot of this structure is
identical to that of “a” in Figure 2, while slots
for the matrix predicate and nonlocal argument
are empty.
For open class words, such as nouns, verbs,
adjectives, adverbs and others, assignment rules
are defined for each syntactic category.
711 function application, in the same manner as in
Hereinafter, let SL =
and SR =
be TDLESs of the left and the
right daughter nodes, respectively. In addition,
let SM be TDLESs of the mother node.
Function appli
hTL,pL,nLi
hTR, pR,nRi
cation rule: The composition
of TDL terms in the TDLESs is performed by
the original TDL, as explained in Section 2.1.
Definition 3.1 (function application rule). zf
=
and
=
</bodyText>
<equation confidence="0.9929478">
Type¡TL¢
α
Type¡TR¢
α7→ β then
=
Type¡TL¢
α7→ β and Type¡TR¢ = α then
nL,nR
empty if nL = nR = _empty_
n if nL = n, nR = _empty_
n if nL = _empty_, nR = n
undefined if
¡
¢=
nL =6 _empty_, nR =6 _empty_
T
Cat�L ,
R
nR
on
p
¢+
=
Type¡TR¢
ω then
</equation>
<bodyText confidence="0.916345238095238">
The assignment rule (6) is for common nouns.
The HPSG lexical item in the upper half of (6)
specifies that the phonetic form of this item is
a variable, P, that takes no arguments, does
not modify other words and takes a specifier.
Here, POS is a noun. In the TDLES assigned
to this item, an actual input word will be sub-
stituted for the variable P, from which the ma-
trix predicate
is produced. Note that we can
obtain the TDLES (4) by applying the rule of
(6) to the HPSG lexical item of (3).
As for verbs, a base TDL semantic represen-
tation is first assigned to a verb root, and the
representation is then modified by lexical rules
to reflect an
P0
inflected form of the verb. This
process corresponds to HPSG lexical rules for
verbs. Details are not presented herein due to
space limitations.
</bodyText>
<subsectionHeader confidence="0.993844">
3.3 Composition Rules
</subsectionHeader>
<bodyText confidence="0.999030333333333">
We define three composition rules: the func-
tion application rule, the word formation
rule, and the nonlocal application rule.
</bodyText>
<equation confidence="0.98425775">
* TRTL
SM = union¡nL,nR
pR ¢
Else if
</equation>
<bodyText confidence="0.983047">
In Definition 3.1, Type(T) is a function
that returns the type of TDL term T, an
</bodyText>
<equation confidence="0.959179">
* TLTR +
SM = union¡nL,nR
pL
d
union(nL,nR) is defined as:
{union
</equation>
<bodyText confidence="0.930109857142857">
This function corresponds to the behavior of
the union of SLASH in HPSG. The composi-
tion in the right-hand side of Figure 5 is an
example of the application of this rule.
Word formation rule: In natural language,
it is often the case that a new word is cre-
ated by combining multiple words, for exam-
ple, “orange juice”. This phenomenon is called
word formation. Typed Dynamic Logic and
the HPSG parser handle this phenomenon in
different ways. Typed Dynamic Logic does
not have any rule for word formation and re-
gards “orange juice” as a single word, whereas
most parsers treat “orange juice” as the sepa-
rate words “orange” and “juice”. This requires
a special composition rule for word formation
to be defined. Among the constituent words of
a compound word, we consider those that are
not HPSG heads as word modifiers and define
their value for T as ω. In addition, we apply
the word formation rule defined below.
</bodyText>
<equation confidence="0.903958571428571">
Definition 3.2 (word formation rule). zf
Type¡TL¢ = ω then
SM = (
\
Else if
T
SM = ( C
Cat (PL ,
R
on
p
¢ +\ nL
⇓
λ x.λ s.λφ . I *P0xs 1
</equation>
<figure confidence="0.9478545">
*J
φ
*P
+
_empty_
PHON P
HEAD noun
MOD hi
SUBJ hi
COMPS hi
SPR hdeti
�
� � � � �
1
</figure>
<bodyText confidence="0.9960688">
concat (pL, pR) in Definition 3.2 is a func-
tion that returns a concatenation of pL and pR.
For example, the composition of a word mod-
ifier “orange” (7) and and a common noun
“juice” (8) will generate the TDLES (9).
</bodyText>
<equation confidence="0.951403625">
¿ ω À
orange
_empty_
r *juice0xs 1
*jui φ J
ce
_empty_
(9)
</equation>
<tableCaption confidence="0.8260715">
Table 1: Number of implemented rules
assignment rules
</tableCaption>
<bodyText confidence="0.8543465">
HPSG-TDL template 51
for closed words 16
for open words 35
verb lexical rules 27
composition rules
binary composition rules 3
function application rule
word formation rule
nonlocal application rule
unary derivation rules 12
</bodyText>
<figure confidence="0.495371666666667">
*λ x.λ s.λφ
* λ x.λ s.λφ .
r *orange_juice0xs 1
φ J
*orange_ juice
_empty_
</figure>
<bodyText confidence="0.996614066666667">
Nonlocal application rule: Typed Dynamic
Logic and HPSG also handle the phenomenon
of wh-movement differently. In HPSG, a wh-
phrase is treated as a value of SLASH, and
the value is kept until the Filler-Head Schema
are applied. In TDL, however, wh-movement
is handled by the functional composition rule.
In order to resolve the difference between
these two approaches, we define the nonlocal
application rule, a special rule that introduces
a slot relating to HPSG SLASH to TDLESs.
This slot becomes the third element of TD-
LESs. This rule is applied when the Filler-
Head Schema are applied in HPSG parse trees.
Definition 3.3 (nonlocal application rule).
</bodyText>
<equation confidence="0.862586875">
If Type ¡TL¢ = (α 7� β ) 7� γ, Type¡TR¢ = β ,
Type¡nR¢ = α and the Filler-Head Schema are applied
in HPSG, then
¢
* TL +
¡λ nR.TR
SM = pL
_empty_
</equation>
<subsectionHeader confidence="0.989315">
3.4 Unary Derivation Rules
</subsectionHeader>
<bodyText confidence="0.9994641875">
In TDL, type-shifting of a word or a phrase is
performed by composition with an empty cat-
egory (a category that has no phonetic form,
but has syntactic/semantic functions). For ex-
ample, the phrase “this year” is a noun phrase
at the first stage and can be changed into a
verb modifier when combined with an empty
category. Since many of the type-shifting rules
are not available in HPSG, we defined unary
derivation rules in order to provide an equiva-
lent function to the type-shifting rules of TDL.
These unary rules are applied independently
with HPSG parse trees. (10) and (11) illus-
trate the unary derivation of “this year”. (11)
is derived from (10) using a unary derivation
rule.
</bodyText>
<equation confidence="0.926033571428571">
(10)
/ Aw.λ e.λ s.λ φ .ref ¡x1¢ £*year0x1s1¤ £wx1esφ ¤
(\ *year
_empty_
λ v.λ e.λ s.λ φ . resref ¡x1¢ £*year0x1s1¤ L mod0ex1
*year L
_empty_
</equation>
<sectionHeader confidence="0.983097" genericHeader="evaluation">
4 Experiment
</sectionHeader>
<bodyText confidence="0.99997675">
The number of rules we have implemented is
shown in Table 1. We used the Penn Treebank
(Marcus, 1994) Section 22 (1,527 sentences) to
develop and evaluate the proposed method and
Section 23 (2,144 sentences) as the final test
set.
We measured the coverage of the construc-
tion of TDL semantic representations, in the
manner described in a previous study (Bos
et al., 2004). Although the best method for
strictly evaluating the proposed method is to
measure the agreement between the obtained
semantic representations and the intuitions of
the speaker/writer of the texts, this type of
evaluation could not be performed because of
insufficient resources. Instead, we measured
the rate of successful derivations as an indica-
tor of the coverage of the proposed system.
The sentences in the test set were parsed by
a robust HPSG parser (Miyao et al., 2005),
and HPSG parse trees were successfully gen-
erated for 2,122 (98.9%) sentences. The pro-
posed method was then applied to these parse
trees. Table 2 shows that 88.3% of the un-
</bodyText>
<figure confidence="0.4748815">
*
+
</figure>
<page confidence="0.988661">
712
</page>
<tableCaption confidence="0.997583">
Table 2: Coverage with respect to the test set
</tableCaption>
<table confidence="0.7019172">
covered sentences 88.3 %
uncovered sentences 11.7 %
assignment failures 6.2 %
composition failures 5.5 %
word coverage 99.6 %
</table>
<tableCaption confidence="0.993288">
Table 3: Error analysis: the development set
</tableCaption>
<table confidence="0.998665125">
# assignment failures 103
# unimplemented words 61
# TDL unsupporting words 17
# nonlinguistic HPSG lexical items 25
# composition failures 72
# unsupported compositions 20
# invalid assignments 36
# nonlinguistic parse trees 16
</table>
<bodyText confidence="0.9998996">
seen sentences are assigned TDL semantic rep-
resentations. Although this number is slightly
less than 92.3%, as reported by Bos et al.,
(2004), it seems reasonable to say that the pro-
posed method attained a relatively high cover-
age, given the expressive power of TDL.
The construction of TDL semantic represen-
tations failed for 11.7% of the sentences. We
classified the causes of the failure into two
types. One of which is application failure of
the assignment rules (assignment failure); that
is, no assignment rules are applied to a num-
ber of HPSG lexical items, and so no TD-
LESs are assigned to these items. The other
is application failure of the composition rules
(composition failure). In this case, a type mis-
match occurred in the composition, and so a
TDLES was not derived.
Table 3 shows further classification of the
causes categorized into the two classes. We
manually investigated all of the failures in the
development set.
Assignment failures are caused by three fac-
tors. Most assignment failures occurred due to
the limitation in the number of the assignment
rules (as indicated by “unimplemented words”
in the table). In this experiment, we did not
implement rules for infrequent HPSG lexical
items. We believe that this type of failure
will be resolved by increasing the number of
</bodyText>
<figure confidence="0.800362526315789">
ref($1)[]
[lecture($2,$3) &amp;
past($3) &amp;
agent($2,$1) &amp;
content($2,$4) &amp;
ref($5)[]
[every($6)[ball($6,$4)]
[see($7,$4) &amp;
present($4) &amp;
agent($7,$5) &amp;
theme($7,$6) &amp;
tremendously($7,$4) &amp;
ref($8)[]
[ref($9)[groove($9,$10)]
[be($11,$4) &amp;
present($4) &amp;
agent($11,$8) &amp;
in($11,$9) &amp;
when($11,$7)]]]]]
</figure>
<figureCaption confidence="0.856976">
Figure 6: Output for the sentence: “When
you’re in the groove, you see every ball
tremendously,” he lectured.
</figureCaption>
<bodyText confidence="0.996813870967742">
assignment rules. The second factor in the
table, “TDL unsupported words”, refers to ex-
pressions that are not covered by the current
theory of TDL. In order to resolve this type of
failure, the development of TDL is required.
The third factor, “nonlinguistic HPSG lexical
items” includes a small number of cases in
which TDLESs are not assigned to the words
that are categorized as nonlinguistic syntactic
categories by the HPSG parser. This problem
is caused by ill-formed outputs of the parser.
The composition failures can be further clas-
sified into three classes according to their
causative factors. The first factor is the ex-
istence of HPSG schemata for which we have
not yet implemented composition rules. These
failures will be fixed by extending of the def-
inition of our composition rules. The sec-
ond factor is type mismatches due to the un-
intended assignments of TDLESs to lexical
items. We need to further elaborate the as-
signment rules in order to deal with this prob-
lem. The third factor is parse trees that are
linguistically invalid.
The error analysis given above indicates that
we can further increase the coverage through
the improvement of the assignment/composition
rules.
Figure 6 shows an example of the output
for a sentence in the development set. The
variables $1, ... ,$11 are indices that
</bodyText>
<page confidence="0.994486">
713
</page>
<bodyText confidence="0.999895">
represent entities, events and situations. For
example, $3 represents a situation and $2
represents the lecturing event that exists
in $3. past($3) requires that the sit-
uation is past. agent($2,$1) requires
that the entity $1 is the agent of $2.
content($2,$4) requires that $4 (as a
set of possible worlds) is the content of
$2. be($11,$4) refers to $4. Finally,
every($6)[ball($6,$4)][see($7,$4)
...] represents a generalized quantifier
“every ball”. The index $6 serves as an
antecedent both for bound-variable anaphora
within its scope and for E-type anaphora out-
side its scope. The entities that correspond to
the two occurrences of “you” are represented
by $8 and $5. Their unification is left as
an anaphora resolution task that can be easily
solved by existing statistical or rule-based
methods, given the structural information of
the TDL semantic representation.
</bodyText>
<sectionHeader confidence="0.999189" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999337517241379">
The present paper proposed a method by which
to translate HPSG-style outputs of a robust
parser (Miyao et al., 2005) into dynamic se-
mantic representations of TDL (Bekki, 2000).
We showed that our implementation achieved
high coverage, approximately 90%, for real
text of the Penn Treebank corpus and that the
resulting representations have sufficient expres-
sive power of contemporary semantic theory
involving quantification, plurality, inter/intra-
sentential anaphora and presupposition.
In the present study, we investigated the
possibility of achieving robustness and descrip-
tive adequacy of semantics. Although previ-
ously thought to have a trade-off relationship,
the present study proved that robustness and
descriptive adequacy of semantics are not in-
trinsically incompatible, given the transparency
between syntax and discourse semantics.
If the notion of robustness serves as a cri-
terion not only for the practical usefulness of
natural language processing but also for the
validity of linguistic theories, then the compo-
sitional transparency that penetrates all levels
of syntax, sentential semantics, and discourse
semantics, beyond the superficial difference be-
tween the laws that govern each of the levels,
might be reconsidered as an essential principle
of linguistic theories.
</bodyText>
<sectionHeader confidence="0.99138" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999903294117647">
Timothy Baldwin, John Beavers, Emily M. Bender,
Dan Flickinger, Ara Kim and Stephan Oepen (to
appear) Beauty and the Beast: What running a
broad-coverage precision grammar over the BNC
taught us about the grammar ? and the cor-
pus, In Linguistic Evidence: Empirical, Theoreti-
cal, and Computational Perspectives, Mouton de
Gruyter.
Daisuke Bekki. 2000. Typed Dynamic Logic for
Compositional Grammar, Doctoral Dissertation,
University of Tokyo.
Daisuke Bekki. 2005. Typed Dynamic Logic and
Grammar: the Introduction, manuscript, Univer-
sity of Tokyo,
Johan Bos, Stephen Clark, Mark Steedman, James
R. Curran and Julia Hockenmaier. 2004. Wide-
Coverage Semantic Representations from a CCG
Parser, In Proc. COLING ’04, Geneva.
Ann Copestake, Dan Flickinger, Ivan A. Sag and
Carl Pollard. 1999. Minimal Recursion Seman-
tics: An introduction, manuscript.
Ann Copestake and Dan Flickinger. 2000.
An open-source grammar development environ-
ment and broad-coverage English grammar using
HPSG In Proc. LREC-2000, Athens.
Jeroen Groenendijk and Martin Stokhof. 1991. Dy-
namic Predicate Logic, In Linguistics and Philos-
ophy 14, pp.39-100.
Julia Hockenmaier and Mark Steedman. 2002. Ac-
quiring Compact Lexicalized Grammars from a
Cleaner Treebank, In Proc. LREC-2002, Las Pal-
mas.
Mitch Marcus. 1994. The Penn Treebank: A
revised corpus design for extracting predicate-
argument structure. In Proceedings of the ARPA
Human Language Technolog Workshop, Prince-
ton, NJ.
Yusuke Miyao, Takashi Ninomiya and Jun’ichi Tsu-
jii. 2005. Corpus-oriented Grammar Develop-
ment for Acquiring a Head-driven Phrase Struc-
ture Grammar from the Penn Treebank, in IJC-
NLP 2004, LNAI3248, pp.684-693. Springer-
Verlag.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar, Studies in Contem-
porary Linguistics. University of Chicago Press,
Chicago, London.
Uwe Reyle. 1993. Dealing with Ambiguities by
Underspecification: Construction, Representation
and Deduction, In Journal of Semantics 10,
pp.123-179.
</reference>
<page confidence="0.998175">
714
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.172507">
<title confidence="0.997694">Translating HPSG-style Outputs of a Robust Parser</title>
<author confidence="0.426162">into Typed Dynamic Logic</author>
<affiliation confidence="0.762554">Department of Computer Science, University of Tokyo</affiliation>
<address confidence="0.670999">Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan</address>
<affiliation confidence="0.730036">Center for Evolutionary Cognitive Sciences, University of Tokyo</affiliation>
<address confidence="0.735199">Komaba 3-8-1, Meguro-ku, Tokyo 153-8902, Japan</address>
<affiliation confidence="0.762923">of Informatics, University of Manchester</affiliation>
<address confidence="0.670688">PO Box 88, Sackville St, Manchester M60 1QD, UK</address>
<affiliation confidence="0.99309">JST (Japan Science and Technology Corporation)</affiliation>
<address confidence="0.967872">Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012, Japan</address>
<email confidence="0.977811">yusuke,tsujii}@is.s.u-tokyo.ac.jp</email>
<abstract confidence="0.999115947368421">The present paper proposes a method by which to translate outputs of a robust HPSG parser into semantic representations of Typed Dynamic Logic (TDL), a dynamic plural semantics defined in typed lambda calculus. With its higher-order representations of contexts, TDL analyzes and describes the inherently inter-sentential nature of quantification and anaphora in a strictly lexicalized and compositional manner. The present study shows that the proposed translation method successfully combines robustness and descriptive adequacy of contemporary semantics. The present implementation achieves high coverage, approximately 90%, for the real text of the Penn Treebank corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Timothy Baldwin</author>
<author>John Beavers</author>
<author>Emily M Bender</author>
<author>Dan Flickinger</author>
</authors>
<title>Ara Kim and Stephan Oepen (to appear) Beauty and the Beast: What running a broad-coverage precision grammar over the BNC taught us about the grammar ? and the corpus,</title>
<booktitle>In Linguistic Evidence: Empirical, Theoretical, and Computational Perspectives, Mouton de Gruyter.</booktitle>
<marker>Baldwin, Beavers, Bender, Flickinger, </marker>
<rawString>Timothy Baldwin, John Beavers, Emily M. Bender, Dan Flickinger, Ara Kim and Stephan Oepen (to appear) Beauty and the Beast: What running a broad-coverage precision grammar over the BNC taught us about the grammar ? and the corpus, In Linguistic Evidence: Empirical, Theoretical, and Computational Perspectives, Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Bekki</author>
</authors>
<title>Typed Dynamic Logic for Compositional Grammar, Doctoral Dissertation,</title>
<date>2000</date>
<institution>University of Tokyo.</institution>
<contexts>
<context position="3322" citStr="Bekki, 2000" startWordPosition="487" endWordPosition="488">994) and has affinities with UDRS (Reyle, 1993). For real text, however, its implementation, as in the case of the ERG parser (Copestake and Flickinger, 2000), restricts its target to the static fragment of MRS and yet has a lower coverage than corpus-oriented parsers (Baldwin, to appear). The lack of transparency between syntax and discourse semantics appears to have created a tension between the robustness of syntax and the descriptive adequacy of semantics. In the present paper, we will introduce a robust method to obtain dynamic semantic representations based on Typed Dynamic Logic (TDL) (Bekki, 2000) from real text by translating the outputs of a robust HPSG parser (Miyao et al., 2005). Typed Dynamic Logic is a dynamic plural semantics that formalizes the structure underlying the semantic interactions between quantification, plurality, bound variable/E-type anaphora 707 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 707–714, Sydney, July 2006. c�2006 Association for Computational Linguistics �rex···xe�→txi1 ···xin ≡ λ G(i�→e)�→t.λ gi�→e.g ∈ G∧rgx1,... ,gxm ∼ φ prop ≡ λ G(i�→e)�→t.λ gi�→e.g ∈ G ∧ -∃hi�→e.h ∈ φ G φ prop ... ϕprop ⎡ ⎤ ⎣ ⎦ ≡ λ G(i&apos;e)&apos;t.(ϕ ···(φ G))</context>
<context position="24362" citStr="Bekki, 2000" startWordPosition="4203" endWordPosition="4204">l”. The index $6 serves as an antecedent both for bound-variable anaphora within its scope and for E-type anaphora outside its scope. The entities that correspond to the two occurrences of “you” are represented by $8 and $5. Their unification is left as an anaphora resolution task that can be easily solved by existing statistical or rule-based methods, given the structural information of the TDL semantic representation. 5 Conclusion The present paper proposed a method by which to translate HPSG-style outputs of a robust parser (Miyao et al., 2005) into dynamic semantic representations of TDL (Bekki, 2000). We showed that our implementation achieved high coverage, approximately 90%, for real text of the Penn Treebank corpus and that the resulting representations have sufficient expressive power of contemporary semantic theory involving quantification, plurality, inter/intrasentential anaphora and presupposition. In the present study, we investigated the possibility of achieving robustness and descriptive adequacy of semantics. Although previously thought to have a trade-off relationship, the present study proved that robustness and descriptive adequacy of semantics are not intrinsically incompa</context>
</contexts>
<marker>Bekki, 2000</marker>
<rawString>Daisuke Bekki. 2000. Typed Dynamic Logic for Compositional Grammar, Doctoral Dissertation, University of Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Bekki</author>
</authors>
<title>Typed Dynamic Logic and Grammar: the Introduction,</title>
<date>2005</date>
<institution>University of Tokyo,</institution>
<note>manuscript,</note>
<contexts>
<context position="4191" citStr="Bekki, 2005" startWordPosition="650" endWordPosition="651">-type anaphora 707 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 707–714, Sydney, July 2006. c�2006 Association for Computational Linguistics �rex···xe�→txi1 ···xin ≡ λ G(i�→e)�→t.λ gi�→e.g ∈ G∧rgx1,... ,gxm ∼ φ prop ≡ λ G(i�→e)�→t.λ gi�→e.g ∈ G ∧ -∃hi�→e.h ∈ φ G φ prop ... ϕprop ⎡ ⎤ ⎣ ⎦ ≡ λ G(i&apos;e)&apos;t.(ϕ ···(φ G)) ref (xi)[φ prop][ϕprop] ≡λ G(i�→e)i+t. ⎧ if Glx=φ Gl ⎫ ⎨ then λ gi�→e.g ∈ ϕG ∧ Gl ⎬ ⎩ x x =ϕGlx ⎭ otherwise undefined where prop ≡ ((i H e) H t) H (i H e) H t ⎛ gα ∈ Gα�→t ≡ Gg ⎞ ⎜ ⎝ G(i&apos;e)�→t/xi ≡ ⎠ ⎟ λ de.∃gi&amp;quot;.g ∈ G∧gx = d Figure 1: Propositions of TDL (Bekki, 2005) and presuppositions. All of this complex discourse/plurality-related information is encapsulated within higher-order structures in TDL, and the analysis remains strictly lexical and compositional, which makes its interface with syntax transparent and straightforward. This is a significant advantage for achieving robustness in natural language processing. 2 Background 2.1 Typed Dynamic Logic Figure 1 shows a number of propositions defined in (Bekki, 2005), including atomic predicate, negation, conjunction, and anaphoric expression. Typed Dynamic Logic is described in typed lambda calculus (Göd</context>
</contexts>
<marker>Bekki, 2005</marker>
<rawString>Daisuke Bekki. 2005. Typed Dynamic Logic and Grammar: the Introduction, manuscript, University of Tokyo,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
<author>James R Curran</author>
<author>Julia Hockenmaier</author>
</authors>
<title>WideCoverage Semantic Representations from a CCG Parser, In</title>
<date>2004</date>
<booktitle>Proc. COLING ’04,</booktitle>
<location>Geneva.</location>
<contexts>
<context position="1946" citStr="Bos et al., 2004" startWordPosition="274" endWordPosition="277">t parsing technology is one result of the recent fusion between symbolic and statistical approaches in natural language processing and has been applied to tasks such as information extraction, information retrieval and machine translation (Hockenmaier and Steedman, 2002; Miyao et al., 2005). However, reflecting the field boundary and unestablished interfaces between syntax and semantics in formal theory of grammar, this fusion has achieved less in semantics than in syntax. For example, a system that translates the output of a robust CCG parser into semantic representations has been developed (Bos et al., 2004). While its corpus-oriented parser attained high coverage with respect to real text, the expressive power of the resulting semantic representations is confined to first-order predicate logic. The more elaborate tasks tied to discourse information and plurality, such as resolution of anaphora antecedent, scope ambiguity, presupposition, topic and focus, are required to refer to ‘deeper’ semantic structures, such as dynamic semantics (Groenendijk and Stokhof, 1991). However, most dynamic semantic theories are not equipped with large-scale syntax that covers more than a small fragment of target l</context>
<context position="9138" citStr="Bos et al., 2004" startWordPosition="1470" endWordPosition="1473">ers have a shared structure. In the first stage of the derivation of this tree, lexical items are assigned to each of the strings, “John” and “runs.” Next, the mother node, which dominates the two items, PHON “John runs” HEAD 1 SUBJ h i COMPS h i PHON “John” HEAD noun SUBJ h i COMPS h i John runs Figure 4: An HPSG parse tree is generated by the application of Subject-Head Schema. The recursive application of these operations derives the entire tree. 3 Method In this section, we present a method to derive TDL semantic representations from HPSG parse trees, adopting, in part, a previous method (Bos et al., 2004). Basically, we first assign TDL representations to lexical items that are terminal nodes of a parse tree, and then compose the TDL representation for the entire tree according to the tree structure (Figure 5). One problematic aspect of this approach is that the composition process of TDL semantic representations and that of HPSG parse trees are not identical. For example, in the HPSG PHON “boy” ∙ J noun HEAD MOD h i � SUBJ h i VAL COMPS h i J SPR hdeti J SLASH h i ⎤ ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ ⎦ ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ (3) ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ SYN SEM ⎡ ⎢ ⎣ ⎤ ⎦⎥ ⎡ ⎢ ⎣ ⎤ ⎦⎥ : 2 ⎡ ⎢ ⎢ ⎣ PHON “runs” HEAD verb : 1 SUBJ h</context>
<context position="19170" citStr="Bos et al., 2004" startWordPosition="3364" endWordPosition="3367">e unary derivation of “this year”. (11) is derived from (10) using a unary derivation rule. (10) / Aw.λ e.λ s.λ φ .ref ¡x1¢ £*year0x1s1¤ £wx1esφ ¤ (\ *year _empty_ λ v.λ e.λ s.λ φ . resref ¡x1¢ £*year0x1s1¤ L mod0ex1 *year L _empty_ 4 Experiment The number of rules we have implemented is shown in Table 1. We used the Penn Treebank (Marcus, 1994) Section 22 (1,527 sentences) to develop and evaluate the proposed method and Section 23 (2,144 sentences) as the final test set. We measured the coverage of the construction of TDL semantic representations, in the manner described in a previous study (Bos et al., 2004). Although the best method for strictly evaluating the proposed method is to measure the agreement between the obtained semantic representations and the intuitions of the speaker/writer of the texts, this type of evaluation could not be performed because of insufficient resources. Instead, we measured the rate of successful derivations as an indicator of the coverage of the proposed system. The sentences in the test set were parsed by a robust HPSG parser (Miyao et al., 2005), and HPSG parse trees were successfully generated for 2,122 (98.9%) sentences. The proposed method was then applied to </context>
<context position="20419" citStr="Bos et al., (2004)" startWordPosition="3570" endWordPosition="3573">s that 88.3% of the un* + 712 Table 2: Coverage with respect to the test set covered sentences 88.3 % uncovered sentences 11.7 % assignment failures 6.2 % composition failures 5.5 % word coverage 99.6 % Table 3: Error analysis: the development set # assignment failures 103 # unimplemented words 61 # TDL unsupporting words 17 # nonlinguistic HPSG lexical items 25 # composition failures 72 # unsupported compositions 20 # invalid assignments 36 # nonlinguistic parse trees 16 seen sentences are assigned TDL semantic representations. Although this number is slightly less than 92.3%, as reported by Bos et al., (2004), it seems reasonable to say that the proposed method attained a relatively high coverage, given the expressive power of TDL. The construction of TDL semantic representations failed for 11.7% of the sentences. We classified the causes of the failure into two types. One of which is application failure of the assignment rules (assignment failure); that is, no assignment rules are applied to a number of HPSG lexical items, and so no TDLESs are assigned to these items. The other is application failure of the composition rules (composition failure). In this case, a type mismatch occurred in the com</context>
</contexts>
<marker>Bos, Clark, Steedman, Curran, Hockenmaier, 2004</marker>
<rawString>Johan Bos, Stephen Clark, Mark Steedman, James R. Curran and Julia Hockenmaier. 2004. WideCoverage Semantic Representations from a CCG Parser, In Proc. COLING ’04, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Ivan A Sag</author>
<author>Carl Pollard</author>
</authors>
<title>Minimal Recursion Semantics: An introduction, manuscript.</title>
<date>1999</date>
<journal>Ann Copestake</journal>
<contexts>
<context position="2641" citStr="Copestake et al., 1999" startWordPosition="377" endWordPosition="380">o real text, the expressive power of the resulting semantic representations is confined to first-order predicate logic. The more elaborate tasks tied to discourse information and plurality, such as resolution of anaphora antecedent, scope ambiguity, presupposition, topic and focus, are required to refer to ‘deeper’ semantic structures, such as dynamic semantics (Groenendijk and Stokhof, 1991). However, most dynamic semantic theories are not equipped with large-scale syntax that covers more than a small fragment of target languages. One of a few exceptions is Minimal Recursion Semantics (MRS) (Copestake et al., 1999), which is compatible with largescale HPSG syntax (Pollard and Sag, 1994) and has affinities with UDRS (Reyle, 1993). For real text, however, its implementation, as in the case of the ERG parser (Copestake and Flickinger, 2000), restricts its target to the static fragment of MRS and yet has a lower coverage than corpus-oriented parsers (Baldwin, to appear). The lack of transparency between syntax and discourse semantics appears to have created a tension between the robustness of syntax and the descriptive adequacy of semantics. In the present paper, we will introduce a robust method to obtain </context>
</contexts>
<marker>Copestake, Flickinger, Sag, Pollard, 1999</marker>
<rawString>Ann Copestake, Dan Flickinger, Ivan A. Sag and Carl Pollard. 1999. Minimal Recursion Semantics: An introduction, manuscript. Ann Copestake and Dan Flickinger. 2000.</rawString>
</citation>
<citation valid="false">
<title>An open-source grammar development environment and broad-coverage English grammar using HPSG In</title>
<booktitle>Proc. LREC-2000,</booktitle>
<location>Athens.</location>
<marker></marker>
<rawString>An open-source grammar development environment and broad-coverage English grammar using HPSG In Proc. LREC-2000, Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeroen Groenendijk</author>
<author>Martin Stokhof</author>
</authors>
<title>Dynamic Predicate Logic,</title>
<date>1991</date>
<journal>In Linguistics and Philosophy</journal>
<volume>14</volume>
<pages>39--100</pages>
<contexts>
<context position="2413" citStr="Groenendijk and Stokhof, 1991" startWordPosition="341" endWordPosition="344">mantics than in syntax. For example, a system that translates the output of a robust CCG parser into semantic representations has been developed (Bos et al., 2004). While its corpus-oriented parser attained high coverage with respect to real text, the expressive power of the resulting semantic representations is confined to first-order predicate logic. The more elaborate tasks tied to discourse information and plurality, such as resolution of anaphora antecedent, scope ambiguity, presupposition, topic and focus, are required to refer to ‘deeper’ semantic structures, such as dynamic semantics (Groenendijk and Stokhof, 1991). However, most dynamic semantic theories are not equipped with large-scale syntax that covers more than a small fragment of target languages. One of a few exceptions is Minimal Recursion Semantics (MRS) (Copestake et al., 1999), which is compatible with largescale HPSG syntax (Pollard and Sag, 1994) and has affinities with UDRS (Reyle, 1993). For real text, however, its implementation, as in the case of the ERG parser (Copestake and Flickinger, 2000), restricts its target to the static fragment of MRS and yet has a lower coverage than corpus-oriented parsers (Baldwin, to appear). The lack of </context>
</contexts>
<marker>Groenendijk, Stokhof, 1991</marker>
<rawString>Jeroen Groenendijk and Martin Stokhof. 1991. Dynamic Predicate Logic, In Linguistics and Philosophy 14, pp.39-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Acquiring Compact Lexicalized Grammars from a Cleaner Treebank, In</title>
<date>2002</date>
<booktitle>Proc. LREC-2002,</booktitle>
<location>Las Palmas.</location>
<contexts>
<context position="1599" citStr="Hockenmaier and Steedman, 2002" startWordPosition="218" endWordPosition="221">ntification and anaphora in a strictly lexicalized and compositional manner. The present study shows that the proposed translation method successfully combines robustness and descriptive adequacy of contemporary semantics. The present implementation achieves high coverage, approximately 90%, for the real text of the Penn Treebank corpus. 1 Introduction Robust parsing technology is one result of the recent fusion between symbolic and statistical approaches in natural language processing and has been applied to tasks such as information extraction, information retrieval and machine translation (Hockenmaier and Steedman, 2002; Miyao et al., 2005). However, reflecting the field boundary and unestablished interfaces between syntax and semantics in formal theory of grammar, this fusion has achieved less in semantics than in syntax. For example, a system that translates the output of a robust CCG parser into semantic representations has been developed (Bos et al., 2004). While its corpus-oriented parser attained high coverage with respect to real text, the expressive power of the resulting semantic representations is confined to first-order predicate logic. The more elaborate tasks tied to discourse information and pl</context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002. Acquiring Compact Lexicalized Grammars from a Cleaner Treebank, In Proc. LREC-2002, Las Palmas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitch Marcus</author>
</authors>
<title>The Penn Treebank: A revised corpus design for extracting predicateargument structure.</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA Human Language Technolog Workshop,</booktitle>
<location>Princeton, NJ.</location>
<contexts>
<context position="18900" citStr="Marcus, 1994" startWordPosition="3321" endWordPosition="3322"> many of the type-shifting rules are not available in HPSG, we defined unary derivation rules in order to provide an equivalent function to the type-shifting rules of TDL. These unary rules are applied independently with HPSG parse trees. (10) and (11) illustrate the unary derivation of “this year”. (11) is derived from (10) using a unary derivation rule. (10) / Aw.λ e.λ s.λ φ .ref ¡x1¢ £*year0x1s1¤ £wx1esφ ¤ (\ *year _empty_ λ v.λ e.λ s.λ φ . resref ¡x1¢ £*year0x1s1¤ L mod0ex1 *year L _empty_ 4 Experiment The number of rules we have implemented is shown in Table 1. We used the Penn Treebank (Marcus, 1994) Section 22 (1,527 sentences) to develop and evaluate the proposed method and Section 23 (2,144 sentences) as the final test set. We measured the coverage of the construction of TDL semantic representations, in the manner described in a previous study (Bos et al., 2004). Although the best method for strictly evaluating the proposed method is to measure the agreement between the obtained semantic representations and the intuitions of the speaker/writer of the texts, this type of evaluation could not be performed because of insufficient resources. Instead, we measured the rate of successful deri</context>
</contexts>
<marker>Marcus, 1994</marker>
<rawString>Mitch Marcus. 1994. The Penn Treebank: A revised corpus design for extracting predicateargument structure. In Proceedings of the ARPA Human Language Technolog Workshop, Princeton, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Takashi Ninomiya</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Corpus-oriented Grammar Development for Acquiring a Head-driven Phrase Structure Grammar from the Penn Treebank,</title>
<date>2005</date>
<booktitle>in IJCNLP 2004, LNAI3248,</booktitle>
<pages>684--693</pages>
<publisher>SpringerVerlag.</publisher>
<contexts>
<context position="1620" citStr="Miyao et al., 2005" startWordPosition="222" endWordPosition="225">rictly lexicalized and compositional manner. The present study shows that the proposed translation method successfully combines robustness and descriptive adequacy of contemporary semantics. The present implementation achieves high coverage, approximately 90%, for the real text of the Penn Treebank corpus. 1 Introduction Robust parsing technology is one result of the recent fusion between symbolic and statistical approaches in natural language processing and has been applied to tasks such as information extraction, information retrieval and machine translation (Hockenmaier and Steedman, 2002; Miyao et al., 2005). However, reflecting the field boundary and unestablished interfaces between syntax and semantics in formal theory of grammar, this fusion has achieved less in semantics than in syntax. For example, a system that translates the output of a robust CCG parser into semantic representations has been developed (Bos et al., 2004). While its corpus-oriented parser attained high coverage with respect to real text, the expressive power of the resulting semantic representations is confined to first-order predicate logic. The more elaborate tasks tied to discourse information and plurality, such as reso</context>
<context position="3409" citStr="Miyao et al., 2005" startWordPosition="501" endWordPosition="504">lementation, as in the case of the ERG parser (Copestake and Flickinger, 2000), restricts its target to the static fragment of MRS and yet has a lower coverage than corpus-oriented parsers (Baldwin, to appear). The lack of transparency between syntax and discourse semantics appears to have created a tension between the robustness of syntax and the descriptive adequacy of semantics. In the present paper, we will introduce a robust method to obtain dynamic semantic representations based on Typed Dynamic Logic (TDL) (Bekki, 2000) from real text by translating the outputs of a robust HPSG parser (Miyao et al., 2005). Typed Dynamic Logic is a dynamic plural semantics that formalizes the structure underlying the semantic interactions between quantification, plurality, bound variable/E-type anaphora 707 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 707–714, Sydney, July 2006. c�2006 Association for Computational Linguistics �rex···xe�→txi1 ···xin ≡ λ G(i�→e)�→t.λ gi�→e.g ∈ G∧rgx1,... ,gxm ∼ φ prop ≡ λ G(i�→e)�→t.λ gi�→e.g ∈ G ∧ -∃hi�→e.h ∈ φ G φ prop ... ϕprop ⎡ ⎤ ⎣ ⎦ ≡ λ G(i&apos;e)&apos;t.(ϕ ···(φ G)) ref (xi)[φ prop][ϕprop] ≡λ G(i�→e)i+t. ⎧ if Glx=φ Gl ⎫ ⎨ then λ gi�→e.g ∈ ϕG ∧ Gl ⎬ ⎩ </context>
<context position="19650" citStr="Miyao et al., 2005" startWordPosition="3441" endWordPosition="3444">e measured the coverage of the construction of TDL semantic representations, in the manner described in a previous study (Bos et al., 2004). Although the best method for strictly evaluating the proposed method is to measure the agreement between the obtained semantic representations and the intuitions of the speaker/writer of the texts, this type of evaluation could not be performed because of insufficient resources. Instead, we measured the rate of successful derivations as an indicator of the coverage of the proposed system. The sentences in the test set were parsed by a robust HPSG parser (Miyao et al., 2005), and HPSG parse trees were successfully generated for 2,122 (98.9%) sentences. The proposed method was then applied to these parse trees. Table 2 shows that 88.3% of the un* + 712 Table 2: Coverage with respect to the test set covered sentences 88.3 % uncovered sentences 11.7 % assignment failures 6.2 % composition failures 5.5 % word coverage 99.6 % Table 3: Error analysis: the development set # assignment failures 103 # unimplemented words 61 # TDL unsupporting words 17 # nonlinguistic HPSG lexical items 25 # composition failures 72 # unsupported compositions 20 # invalid assignments 36 # n</context>
<context position="24303" citStr="Miyao et al., 2005" startWordPosition="4192" endWordPosition="4195">4)][see($7,$4) ...] represents a generalized quantifier “every ball”. The index $6 serves as an antecedent both for bound-variable anaphora within its scope and for E-type anaphora outside its scope. The entities that correspond to the two occurrences of “you” are represented by $8 and $5. Their unification is left as an anaphora resolution task that can be easily solved by existing statistical or rule-based methods, given the structural information of the TDL semantic representation. 5 Conclusion The present paper proposed a method by which to translate HPSG-style outputs of a robust parser (Miyao et al., 2005) into dynamic semantic representations of TDL (Bekki, 2000). We showed that our implementation achieved high coverage, approximately 90%, for real text of the Penn Treebank corpus and that the resulting representations have sufficient expressive power of contemporary semantic theory involving quantification, plurality, inter/intrasentential anaphora and presupposition. In the present study, we investigated the possibility of achieving robustness and descriptive adequacy of semantics. Although previously thought to have a trade-off relationship, the present study proved that robustness and desc</context>
</contexts>
<marker>Miyao, Ninomiya, Tsujii, 2005</marker>
<rawString>Yusuke Miyao, Takashi Ninomiya and Jun’ichi Tsujii. 2005. Corpus-oriented Grammar Development for Acquiring a Head-driven Phrase Structure Grammar from the Penn Treebank, in IJCNLP 2004, LNAI3248, pp.684-693. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar, Studies in Contemporary Linguistics.</title>
<date>1994</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago, London.</location>
<contexts>
<context position="2714" citStr="Pollard and Sag, 1994" startWordPosition="389" endWordPosition="392">s is confined to first-order predicate logic. The more elaborate tasks tied to discourse information and plurality, such as resolution of anaphora antecedent, scope ambiguity, presupposition, topic and focus, are required to refer to ‘deeper’ semantic structures, such as dynamic semantics (Groenendijk and Stokhof, 1991). However, most dynamic semantic theories are not equipped with large-scale syntax that covers more than a small fragment of target languages. One of a few exceptions is Minimal Recursion Semantics (MRS) (Copestake et al., 1999), which is compatible with largescale HPSG syntax (Pollard and Sag, 1994) and has affinities with UDRS (Reyle, 1993). For real text, however, its implementation, as in the case of the ERG parser (Copestake and Flickinger, 2000), restricts its target to the static fragment of MRS and yet has a lower coverage than corpus-oriented parsers (Baldwin, to appear). The lack of transparency between syntax and discourse semantics appears to have created a tension between the robustness of syntax and the descriptive adequacy of semantics. In the present paper, we will introduce a robust method to obtain dynamic semantic representations based on Typed Dynamic Logic (TDL) (Bekk</context>
<context position="8181" citStr="Pollard and Sag, 1994" startWordPosition="1300" endWordPosition="1303">sbj �,x&apos;.�,e&apos;.�,s&apos;.�,�p. L agent�ex Jll φ ei.λ si.λ φ p.ref (x2) [] [ agentltumble2x2 1 L agent�e2x2 J Figure 3: Derivation of TDL semantic representation of “He tumbled”. Although the antecedent for x2 is not determined in this structure, the possible candidates can be enumerated: x1, s1 and e1, which precede x2. Since TDL seamlessly represents linguistic notions such as “entity”, “event” and “situation”, by indices, the anaphoric expressions, such as “the event” and “that case”, can be treated in the same manner. 2.2 Head-driven Phrase Structure Grammar Head-driven Phrase Structure Grammar (Pollard and Sag, 1994) is a kind of lexicalized grammar that consists of lexical items and a small number of composition rules called schema. Schemata and lexical items are all described in typed feature structures and the unification operation defined thereon. Figure 4 is an example of a parse tree, where the feature structures marked with the same boxed numbers have a shared structure. In the first stage of the derivation of this tree, lexical items are assigned to each of the strings, “John” and “runs.” Next, the mother node, which dominates the two items, PHON “John runs” HEAD 1 SUBJ h i COMPS h i PHON “John” H</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar, Studies in Contemporary Linguistics. University of Chicago Press, Chicago, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uwe Reyle</author>
</authors>
<title>Dealing with Ambiguities by Underspecification: Construction, Representation and Deduction,</title>
<date>1993</date>
<journal>In Journal of Semantics</journal>
<volume>10</volume>
<pages>123--179</pages>
<contexts>
<context position="2757" citStr="Reyle, 1993" startWordPosition="398" endWordPosition="399">e elaborate tasks tied to discourse information and plurality, such as resolution of anaphora antecedent, scope ambiguity, presupposition, topic and focus, are required to refer to ‘deeper’ semantic structures, such as dynamic semantics (Groenendijk and Stokhof, 1991). However, most dynamic semantic theories are not equipped with large-scale syntax that covers more than a small fragment of target languages. One of a few exceptions is Minimal Recursion Semantics (MRS) (Copestake et al., 1999), which is compatible with largescale HPSG syntax (Pollard and Sag, 1994) and has affinities with UDRS (Reyle, 1993). For real text, however, its implementation, as in the case of the ERG parser (Copestake and Flickinger, 2000), restricts its target to the static fragment of MRS and yet has a lower coverage than corpus-oriented parsers (Baldwin, to appear). The lack of transparency between syntax and discourse semantics appears to have created a tension between the robustness of syntax and the descriptive adequacy of semantics. In the present paper, we will introduce a robust method to obtain dynamic semantic representations based on Typed Dynamic Logic (TDL) (Bekki, 2000) from real text by translating the </context>
</contexts>
<marker>Reyle, 1993</marker>
<rawString>Uwe Reyle. 1993. Dealing with Ambiguities by Underspecification: Construction, Representation and Deduction, In Journal of Semantics 10, pp.123-179.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>