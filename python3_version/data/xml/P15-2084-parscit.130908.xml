<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008350">
<title confidence="0.990139">
Dependency Recurrent Neural Language Models for Sentence Completion
</title>
<author confidence="0.994422">
Piotr Mirowski Andreas Vlachos
</author>
<affiliation confidence="0.996945">
Google DeepMind University College London
</affiliation>
<email confidence="0.984144">
piotr.mirowski@computer.org a.vlachos@cs.ucl.ac.uk
</email>
<sectionHeader confidence="0.993543" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999814666666667">
Recent work on language modelling has
shifted focus from count-based models to
neural models. In these works, the words
in each sentence are always considered in
a left-to-right order. In this paper we show
how we can improve the performance of
the recurrent neural network (RNN) lan-
guage model by incorporating the syntac-
tic dependencies of a sentence, which have
the effect of bringing relevant contexts
closer to the word being predicted. We
evaluate our approach on the Microsoft
Research Sentence Completion Challenge
and show that the dependency RNN pro-
posed improves over the RNN by about
10 points in accuracy. Furthermore, we
achieve results comparable with the state-
of-the-art models on this task.
</bodyText>
<sectionHeader confidence="0.998976" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999823616666667">
Language Models (LM) are commonly used to
score a sequence of tokens according to its prob-
ability of occurring in natural language. They are
an essential building block in a variety of applica-
tions such as machine translation, speech recogni-
tion and grammatical error correction. The stan-
dard way of evaluating a language model has been
to calculate its perplexity on a large corpus. How-
ever, this evaluation assumes the output of the lan-
guage model to be probabilistic and it has been
observed that perplexity does not always correlate
with the downstream task performance.
For these reasons, Zweig and Burges (2012)
proposed the Sentence Completion Challenge, in
which the task is to pick the correct word to com-
plete a sentence out of five candidates. Perfor-
mance is evaluated by accuracy (how many sen-
tences were completed correctly), thus both prob-
abilistic and non-probabilistic models (e.g. Roark
et al. (2007)) can be compared. Recent approaches
for this task include both neural and count-based
language models (Zweig et al., 2012; Gubbins
and Vlachos, 2013; Mnih and Kavukcuoglu, 2013;
Mikolov et al., 2013).
Most neural language models consider the to-
kens in a sentence in the order they appear, and
the hidden state representation of the network
is typically reset at the beginning of each sen-
tence. In this work we propose a novel neu-
ral language model that learns a recurrent neu-
ral network (RNN) (Mikolov et al., 2010) on
top of the syntactic dependency parse of a sen-
tence. Syntactic dependencies bring relevant con-
texts closer to the word being predicted, thus en-
hancing performance as shown by Gubbins and
Vlachos (2013) for count-based language models.
Our Dependency RNN model is published simul-
taneously with another model, introduced in Tai et
al. (2015), who extend the Long-Short Term Mem-
ory (LSTM) architecture to tree-structured net-
work topologies and evaluate it at sentence-level
sentiment classification and semantic relatedness
tasks, but not as a language model.
Adapting the RNN to use the syntactic depen-
dency structure required to reset and run the net-
work on all the paths in the dependency parse tree
of a given sentence, while maintaining a count of
how often each token appears in those paths. Fur-
thermore, we explain how we can incorporate the
dependency labels as features.
Our results show that the dependency RNN lan-
guage model proposed outperforms the RNN pro-
posed by Mikolov et al. (2011) by about 10 points
in accuracy. Furthermore, it improves upon the
count-based dependency language model of Gub-
bins and Vlachos (2013), while achieving slightly
worse than the recent state-of-the-art results by
Mnih and Kavukcuoglu (2013). Finally, we make
the code and preprocessed data available to facili-
tate comparisons with future work.
</bodyText>
<page confidence="0.851124">
511
</page>
<bodyText confidence="0.302452666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 511–517,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.7443085" genericHeader="introduction">
2 Dependency Recurrent Neural
Network
</sectionHeader>
<bodyText confidence="0.999104043478261">
Count-based language models operate by assign-
ing probabilities to sentences by factorizing their
likelihood into n-grams. Neural language mod-
els further embed each word w(t) into a low-
dimensional vector representation (denoted by
s(t))1. These word representations are learned as
the language model is trained (Bengio et al., 2003)
and enable to define a word in relation to other
words in a metric space.
Recurrent Neural Network Mikolov et al.
(2010) suggested the use of Recurrent Neural Net-
works (RNN) to model long-range dependencies
between words as they are not restricted to a fixed
context length, like the feedforward neural net-
work (Bengio et al., 2003). The hidden representa-
tion s(t) for the word in position t of the sentence
in the RNN follows a first order auto-regressive
dynamic (Eq. 1), where W is the matrix connect-
ing the hidden representation of the previous word
s(t − 1) to the current one, w(t) is the one-hot in-
dex of the current word (in a vocabulary of size N
words) and U is the matrix containing the embed-
dings for all the words in the vocabulary:
</bodyText>
<equation confidence="0.998908">
s(t) = f (Ws(t − 1) + Uw(t)) (1)
</equation>
<bodyText confidence="0.903569">
The nonlinearity f is typically the logistic sigmoid
function f(x) = 1+exp(−x). At each time step, the
</bodyText>
<equation confidence="0.68496">
1
</equation>
<bodyText confidence="0.999150666666667">
RNN generates the word probability vector y(t)
for the next word w(t + 1), using the output word
embedding matrix V and the softmax nonlinearity
</bodyText>
<equation confidence="0.999890333333333">
g(xi) = exp(xi)
i exp(xi):
y(t) = g (Vs(t)) (2)
</equation>
<bodyText confidence="0.964030545454545">
RNN with Maximum Entropy Model Mikolov
et al. (2011) combined RNNs with a maximum en-
tropy model, essentially adding a matrix that di-
rectly connects the input words’ n-gram context
w(t − n + 1, ... , t) to the output word proba-
bilities. In practice, because of the large vocab-
ulary size N, designing such a matrix is computa-
tionally prohibitive. Instead, a hash-based imple-
mentation is used, where the word context is fed
through a hash function h that computes the in-
dex h(w(t − n + 1, ... , t)) of the context words
</bodyText>
<footnote confidence="0.93212425">
1In our notation, we make a distinction between the word
token w(t) at position t in the sentence and its one-hot vector
representation w(t). We note wi the i-th word token on a
breadth-first traversal of a dependency parse tree.
</footnote>
<bodyText confidence="0.99118275">
in a one-dimensional array d of size D (typically,
D = 109). Array d is trained in the same way as
the rest of the RNN model and contributes to the
output word probabilities:
</bodyText>
<equation confidence="0.912005">
( )
y(t) = g Vs(t) + dh(w(t−n+1,...,t)) (3)
</equation>
<bodyText confidence="0.99971588372093">
As we show in our experiments, this additional
matrix is crucial to a good performance on word
completion tasks.
Training RNNs RNNs are trained using maxi-
mum likelihood through gradient-based optimiza-
tion, such as Stochastic Gradient Descent (SGD)
with an annealed learning rate A. The Back-
Propagation Through Time (BPTT) variant of
SGD enables to sum-up gradients from consecu-
tive time steps before updating the parameters of
the RNN and to handle the long-range temporal
dependencies in the hidden s and output y se-
quences. The loss function is the cross-entropy
between the generated word distribution y(t) and
the target one-hot word distribution w(t + 1), and
involves the log-likelihood terms log yw(t+1)(t).
For speed-up, the estimation of the output word
probabilities is done using hierarchical softmax
outputs, i.e., class-based factorization (Mikolov
and Zweig, 2012). Each word wi is assigned to
a class ci and the corresponding log-likelihood is
effectively log ywi(t) = log yci(t) + log ywj(t),
where j is the index of word wi among words
belonging to class ci. In our experiments, we
binned the words found in our training corpus into
250 classes according to frequency, roughly corre-
sponding to the square root of the vocabulary size.
Dependency RNN RNNs are designed to pro-
cess sequential data by iteratively presenting them
with word w(t) and generating next word’s proba-
bility distribution y(t) at each time step. They can
be reset at the beginning of a sentence by setting
all the values of hidden vector s(t) to zero.
Dependency parsing (Nivre, 2005) generates,
for each sentence (which we note {w(t)JTt=0), a
parse tree with a single root, many leaves and an
unique path (also called unroll) from the root to
each leaf, as illustrated on Figure 1. We now note
{wiJi the set of word tokens appearing in the parse
tree of a sentence. The order in the notation de-
rives from the breadth-first traversal of that tree
(i.e., the root word is noted w0). Each of the un-
rolls can be seen as a different sequence of words
</bodyText>
<page confidence="0.973091">
512
</page>
<figure confidence="0.940603">
pobj
ROOT I saw the ship with very strong binoculars
</figure>
<figureCaption confidence="0.999937">
Figure 1: Example dependency tree
</figureCaption>
<bodyText confidence="0.998663352941176">
{wi}, starting from the single root w0, that are vis-
ited when one takes a specific path on the parse
tree. We propose a simple transformation to the
RNN algorithm so that it can process dependency
parse trees. The RNN is reset and independently
run on each such unroll. As detailed in the next
paragraph, when evaluating the log-probability of
the sentence, a word token wi can appear in mul-
tiple unrolls but its log-likelihood is counted only
once. During training, and to avoid over-training
the network on word tokens that appear in more
than one unroll (words near the root appear in
more unrolls than those nearer the leaves), each
word token wi is given a weight discount αi = nz1 ,
based on the number ni of unrolls the token ap-
pears in. Since the RNN is optimized using SGD
and updated at every time-step, the contribution of
word token wi can be discounted by multiplying
the learning rate by the discount factor: αiλ.
Sentence Probability in Dependency RNN
Given a word wi, let us define the ancestor se-
quence A(wi) to be the subsequence of words,
taken as a subset from {wk}i−1
k=0 and describing
the path from the root node w0 to the parent of wi.
For example, in Figure 1, the ancestors A(very)
of word token very are saw, binoculars and
strong. Assuming that each word wi is con-
ditionally independent of the words outside of
its ancestor sequence, given its ancestor sequence
A(wi), Gubbins and Vlachos (2013) showed that
the probability of a sentence (i.e., the probability
of a lexicalized tree 5T given an unlexicalized tree
T) could be written as:
</bodyText>
<equation confidence="0.9985345">
P[5T |T] = � |S |P[wi|A(wi)] (4)
i=1
</equation>
<bodyText confidence="0.999913363636364">
This means that the conditional likelihood of a
word given its ancestors needs to be counted only
once in the calculation of the sentence likelihood,
even though each word can appear in multiple un-
rolls. When modeling a sentence using an RNN,
the state sj that is used to generate the distribution
of words wi (where j is the parent of i in the tree),
represents the vector embedding of the history of
the ancestor words A(wi). Therefore, we count
the term P [wi|sj] only once when computing the
likelihood of the sentence.
</bodyText>
<sectionHeader confidence="0.972675" genericHeader="method">
3 Labelled Dependency RNN
</sectionHeader>
<bodyText confidence="0.998064">
The model presented so far does not use
dependency labels. For this purpose we
adapted the context-dependent RNN (Mikolov and
Zweig, 2012) to handle them as additional M-
dimensional label input features f(t). These fea-
tures require a matrix F that connects label fea-
tures to word vectors, thus yielding a new dynam-
ical model (Eq. 5) in the RNN, and a matrix G
that connects label features to output word proba-
bilities. The full model becomes as follows:
</bodyText>
<equation confidence="0.99999">
s(t) = f (Ws(t − 1) + Uw(t) + Ff(t))(5)
y(t) = g (Vs(t) + Gf(t) + dh(,t n+1))(6)
</equation>
<bodyText confidence="0.999812555555556">
On our training dataset, the dependency parsing
model found M = 44 distinct labels (e.g., nsubj,
det or prep). At each time step t, the context word
w(t) is associated a single dependency label f(t)
(a one-hot vector of dimension M).
Let G(w) be the sequence of grammatical rela-
tions (dependency tree labels) between successive
elements of (A(w), w). The factorization of the
sentence likelihood from Eq. 4 becomes:
</bodyText>
<equation confidence="0.9966955">
P[5T |T] = � |S |P[wi|A(wi), G(wi)] (7)
i=1
</equation>
<sectionHeader confidence="0.990253" genericHeader="method">
4 Implementation and Dataset
</sectionHeader>
<bodyText confidence="0.999925642857143">
We modified the Feature-Augmented RNN
toolkit2 and adapted it to handle tree-structured
data. Specifically, and instead of being run se-
quentially on the entire training corpus, the RNN
is run on all the word tokens in all unrolls of all
the sentences in all the books of the corpus. The
RNN is reset at the beginning of each unroll of a
sentence. When calculating the log-probability of
a sentence, the contribution of each word token
is counted only once (and stored in a hash-table
specific for that sentence). Once all the unrolls
of a sentence are processed, the log-probability
of the sentence is the sum of the per-token log-
probabilities in that hash-table. We also further
</bodyText>
<footnote confidence="0.632411">
2http://research.microsoft.com/en-us/projects/rnn/
</footnote>
<figure confidence="0.990151571428571">
prep
dobj
dep
ROOT
nsubj
amod
advmod
</figure>
<page confidence="0.994667">
513
</page>
<bodyText confidence="0.999925047619048">
enhanced the RNN library by replacing some
large matrix multiplication routines by calls to the
CBLAS library, thus yielding a two- to three-fold
speed-up in the test and training time.3
The training corpus consists of 522 19th cen-
tury novels from Project Gutenberg (Zweig and
Burges, 2012). All processing (sentence-splitting,
PoS tagging, syntactic parsing) was performed us-
ing the Stanford CoreNLP toolkit (Manning et al.,
2014). The test set contains 1040 sentences to be
completed. Each sentence consists of one ground
truth and 4 impostor sentences where a specific
word has been replaced with a syntactically cor-
rect but semantically incorrect impostor word. De-
pendency trees are generated for each sentence
candidate. We split that set into two, using the first
520 sentences in the validation (development) set
and the latter 520 sentences in the test set. Dur-
ing training, we start annealing the learning rate λ
with decay factor 0.66 as soon as the classification
error on the validation set starts to increase.
</bodyText>
<sectionHeader confidence="0.99992" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.999837166666667">
Table 1 shows the accuracy (validation and test
sets) obtained using a simple RNN with 50, 100,
200 and 300-dimensional hidden word represen-
tation and 250 frequency-based word classes (vo-
cabulary size N = 72846 words appearing at least
5 times in the training corpus). One notices that
adding the direct word context to target word con-
nections (using the additional matrix described in
section 2), enables to jump from a poor perfor-
mance of about 30% accuracy to about 40% test
accuracy, essentially matching the 39% accuracy
reported for Good-Turing n-gram language mod-
els in Zweig et al. (2012). Modelling 4-grams
yields even better results, closer to the 45% accu-
racy reported for RNNs in (Zweig et al., 2012).4
As Table 2 shows, dependency RNNs (de-
pRNN) enable about 10 point word accuracy im-
provement over sequential RNNs.
The best accuracy achieved by the depRNN on
the combined development and test sets used to re-
port results in previous work was 53.5%. The best
reported results in the MSR sentence completion
challenge have been achieved by Log-BiLinear
Models (LBLs) (Mnih and Hinton, 2007), a vari-
</bodyText>
<footnote confidence="0.994527">
3Our code and our preprocessed datasets are avail-
able from: https://github.com/piotrmirowski/
DependencyTreeRnn
4The paper did not provide details on the maximum en-
tropy features or on class-based hierarchical softmax).
</footnote>
<table confidence="0.999808777777778">
Architecture 50h 100h 200h 300h
RNN (dev) 29.6 30.0 30.0 30.6
RNN (test) 28.1 30.0 30.4 28.5
RNN+2g (dev) 29.6 28.7 29.4 29.8
RNN+2g (test) 29.6 28.7 28.1 30.2
RNN+3g (dev) 39.2 39.4 38.8 36.5
RNN+3g (test) 40.8 40.6 40.2 39.8
RNN+4g (dev) 40.2 40.6 40.0 40.2
RNN+4g (test) 42.3 41.2 40.4 39.2
</table>
<tableCaption confidence="0.9558715">
Table 1: Accuracy of sequential RNN on the MSR
Sentence Completion Challenge.
</tableCaption>
<table confidence="0.999935888888889">
Architecture 50h 100h 200h
depRNN+3g (dev) 53.3 54.2 54.2
depRNN+3g (test) 51.9 52.7 51.9
ldepRNN+3g (dev) 48.8 51.5 49.0
ldepRNN+3g (test) 44.8 45.4 47.7
depRNN+4g (dev) 52.7 54.0 52.7
depRNN+4g (test) 48.9 51.3 50.8
ldepRNN+4g (dev) 49.4 50.0 (48.5)
ldepRNN+4g (test) 47.7 51.4 (47.7)
</table>
<tableCaption confidence="0.898311">
Table 2: Accuracy of (un-)labeled dependency
RNN (depRNN and ldepRNN respectively).
</tableCaption>
<bodyText confidence="0.99993625">
ant of neural language models with 54.7% to
55.5% accuracy (Mnih and Teh, 2012; Mnih and
Kavukcuoglu, 2013). We conjecture that their su-
perior performance might stem from the fact that
LBLs, just like n-grams, take into account the or-
der of the words in the context and can thus model
higher-order Markovian dynamics than the simple
first-order autoregressive dynamics in RNNs. The
depRNN proposed ignores the left-to-right word
order, thus it is likely that a combination of these
approaches will result in even higher accuracies.
Gubbins and Vlachos (2013) developed a count-
based dependency language model achieving 50%
accuracy. Finally, Mikolov et al. (2013) report that
they achieved 55.4% accuracy with an ensemble of
RNNs, without giving any other details.
</bodyText>
<sectionHeader confidence="0.998974" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999730333333333">
Related work Mirowski et al. (2010) incorpo-
rated syntactic information into neural language
models using PoS tags as additional input to LBLs
but obtained only a small reduction of the word
error rate in a speech recognition task. Similarly,
Bian et al. (2014) enriched the Continuous Bag-of-
</bodyText>
<page confidence="0.9946">
514
</page>
<bodyText confidence="0.996030588235294">
Words (CBOW) model of Mikolov et al. (2013)
by incorporating morphology, PoS tags and en-
tity categories into 600-dimensional word embed-
dings trained on the Gutenberg dataset, increas-
ing sentence completion accuracy from 41% to
44%. Other work on incorporating syntax into lan-
guage modeling include Chelba et al. (1997) and
Pauls and Klein (2012), however none of these ap-
proaches considered neural language models, only
count-based ones. Levy and Goldberg (2014) and
Zhao et al. (2014) proposed to train neural word
embeddings using skip-grams and CBOWs on de-
pendency parse trees, but did not extend their ap-
proach to actual language models such as LBL and
RNN and did not evaluate the word embeddings
on word completion tasks.
Note that we assume that the dependency tree
is supplied prior to running the RNN which limits
the scope of the Dependency RNN to the scoring
of complete sentences, not to next word prediction
(unless a dependency tree parse for the sentence
to be generated is provided). Nevertheless, it is
common in speech recognition and machine trans-
lation to use a conventional decoder to produce an
N-best list of the most likely candidate sentences
and then re-score them with the language model.
(Chelba et al., 1997; Pauls and Klein, 2011)
Tai et al. (2015) propose a similar approach to
ours, learning Long Short-Term Memory (LSTM)
(Hochreiter and Schmidhuber, 1997; Graves,
2012) RNNs on dependency parse tree network
topologies. Their architectures is not designed to
predict next-word probability distributions, as in
a language model, but to classify the input words
(sentiment analysis task) or to measure the sim-
ilarity in hidden representations (semantic relat-
edness task). Their relative improvement in per-
formance (tree LSTMs vs standard LSTMs) on
these two tasks is smaller than ours, probably be-
cause the LSTMs are better than RNNs at storing
long-term dependencies and thus do not benefit
form the word ordering from dependency trees as
much as RNNs. In a similar vein to ours, Miceli-
Barone and Attardi (2015) simply propose to en-
hance RNN-based machine translation by permut-
ing the order of the words in the source sentence to
match the order of the words in the target sentence,
using a source-side dependency parsing.
Limitations of RNNs for word completion
Zweig et al. (2012) reported that RNNs achieve
lower perplexity than n-grams but do not always
</bodyText>
<figureCaption confidence="0.997575">
Figure 2: Perplexity vs. accuracy of RNNs
</figureCaption>
<bodyText confidence="0.999880214285714">
outperform them on word completion tasks. As
illustrated in Fig. 2, the validation set perplex-
ity (comprising all 5 choices for each sentence)
of the RNN keeps decreasing monotonically (once
we start annealing the learning rate), whereas the
validation accuracy rapidly reaches a plateau and
oscillates. Our observation confirms that, once an
RNN went through a few training epochs, change
in perplexity is no longer a good predictor of
change in word accuracy. We presume that the
log-likelihood of word distribution is not a train-
ing objective crafted for precision@1, and that
further perplexity reduction happens in the middle
and tail of the word distribution.
</bodyText>
<sectionHeader confidence="0.995638" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9993827">
In this paper we proposed a novel language model,
dependency RNN, which incorporates syntactic
dependencies into the RNN formulation. We eval-
uated its performance on the MSR sentence com-
pletion task and showed that it improves over
RNN by 10 points in accuracy, while achieving re-
sults comparable with the state-of-the-art. Further
work will include extending the dependency tree
language modeling to Long Short-Term Memory
RNNs to handle longer syntactic dependencies.
</bodyText>
<sectionHeader confidence="0.99495" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999835777777778">
We thank our anonymous reviewers for their
valuable feedback. PM also thanks Geoffrey
Zweig, Daniel Voinea, Francesco Nidito and Da-
vide di Gennaro for sharing the original Feature-
Augmented RNN toolkit on the Microsoft Re-
search website and for insights about that code, as
well as Bhaskar Mitra, Milad Shokouhi and An-
driy Mnih for enlighting discussions about word
embedding and sentence completion.
</bodyText>
<page confidence="0.997306">
515
</page>
<sectionHeader confidence="0.9901" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998536302752294">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137–1155.
Jiang Bian, Bin Gao, and Tie-Yan Liu. 2014.
Knowledge-powered deep learning for word embed-
ding. In Machine Learning and Knowledge Discov-
ery in Databases, Lecture Notes in Computer Sci-
ence, volume 8724, pages 132–148.
Ciprian Chelba, David Engle, Frederick Jelinek, Victor
Jimenez, Sanjeev Khudanpur, Lidia Mangu, Harry
Printz, Eric Ristad, Ronald Rosenfeld, Andreas Stol-
cke, et al. 1997. Structure and performance of a
dependency language model. In Proceedings of Eu-
rospeech, volume 5, pages 2775–2778.
Alex Graves. 2012. Supervised Sequence Labelling
with Recurrent Neural Networks. Studies in Com-
putational Intelligence. Springer.
Joseph Gubbins and Andreas Vlachos. 2013. Depen-
dency language models for sentence completion. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing.
Sepp Hochreiter and Jurgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9:17351780.
Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
302–308.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60.
Antonio Valerio Miceli-Barone and Giuseppe At-
tardi. 2015. Non-projective dependency-based pre-
reordering with recurrent neural network for ma-
chine translation. In The 53rd Annual Meeting of
the Association for Computational Linguistics and
The 7th International Joint Conference of the Asian
Federation of Natural Language Processing.
Tomas Mikolov and Geoff Zweig. 2012. Context de-
pendent recurrent neural network language model.
In Speech Language Technologies (SLT), 2012 IEEE
Workshop on. IEEE.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH 2010, 11th Annual Conference of the
International Speech Communication Association,
Makuhari, Chiba, Japan, September 26-30, 2010,
pages 1045–1048.
Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas
Burget, and Jan Cernocky. 2011. Strategies for
training large scale neural network language mod-
els. In Automatic Speech Recognition and Under-
standing (ASRU), 2011 IEEE Workshop on, pages
196–201. IEEE.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Piotr Mirowski, Sumit Chopra, Suhrid Balakrishnan,
and Srinivas Bangalore. 2010. Feature-rich continu-
ous language models for speech recognition. In Spo-
ken Language Technology Workshop (SLT), 2010
IEEE, pages 241–246. IEEE.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th International Conference
on Machine Learning, page 641648.
Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In C.J.C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K.Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 2265–2273. Curran Associates, Inc.
Andriy Mnih and Yee W Teh. 2012. A fast and simple
algorithm for training neural probabilistic language
models. In Proceedings of the 29th International
Conference on Machine Learning (ICML-12), pages
1751–1758.
Joakim Nivre. 2005. Dependency grammar and de-
pendency parsing. MSI report, 5133(1959):1–32.
Adam Pauls and Dan Klein. 2011. Faster and Smaller
N-Gram Language Models. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 258–267. Association for Computational
Linguistics.
Adam Pauls and Dan Klein. 2012. Large-scale syntac-
tic language modeling with treelets. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics: Long Papers-Volume 1,
pages 959–968. Association for Computational Lin-
guistics.
Brian Roark, Murat Saraclar, and Michael Collins.
2007. Discriminative n-gram language modeling.
Computer Speech &amp; Language, 21(2):373 – 392.
Kai Sheng Tai, Richard Socher, and Christopher Man-
ning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In The 53rd Annual Meeting of the Asso-
ciation for Computational Linguistics and The 7th
International Joint Conference of the Asian Federa-
tion of Natural Language Processing.
</reference>
<page confidence="0.982035">
516
</page>
<reference confidence="0.999066">
Yinggong Zhao, Shujian Huang, Xinyu Dai, Jianbing
Zhang, and Jiajun Chen. 2014. Learning word em-
beddings from dependency relations. In In Proceed-
ings of Asian Language Processing (IALP).
Geoffrey Zweig and Christopher J. C. Burges. 2012.
A challenge set for advancing language modeling.
In Proceedings of the NAACL-HLT 2012 Workshop:
Will We Ever Really Replace the N-gram Model? On
the Future of Language Modeling for HLT, pages
29–36. Association for Computational Linguistics.
Geoffrey Zweig, John C Platt, Christopher Meek,
Christopher J. C. Burges, Ainur Yessenalina, and
Qiang Liu. 2012. Computational approaches to sen-
tence completion. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics, pages 601–610.
</reference>
<page confidence="0.996853">
517
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.767033">
<title confidence="0.999697">Dependency Recurrent Neural Language Models for Sentence Completion</title>
<author confidence="0.989348">Piotr Mirowski Andreas Vlachos</author>
<affiliation confidence="0.840782">Google DeepMind University College London</affiliation>
<email confidence="0.930861">piotr.mirowski@computer.orga.vlachos@cs.ucl.ac.uk</email>
<abstract confidence="0.998761473684211">Recent work on language modelling has shifted focus from count-based models to neural models. In these works, the words in each sentence are always considered in a left-to-right order. In this paper we show how we can improve the performance of the recurrent neural network (RNN) language model by incorporating the syntactic dependencies of a sentence, which have the effect of bringing relevant contexts closer to the word being predicted. We evaluate our approach on the Microsoft Research Sentence Completion Challenge and show that the dependency RNN proposed improves over the RNN by about 10 points in accuracy. Furthermore, we achieve results comparable with the stateof-the-art models on this task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="4347" citStr="Bengio et al., 2003" startWordPosition="677" endWordPosition="680">nnual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 511–517, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Dependency Recurrent Neural Network Count-based language models operate by assigning probabilities to sentences by factorizing their likelihood into n-grams. Neural language models further embed each word w(t) into a lowdimensional vector representation (denoted by s(t))1. These word representations are learned as the language model is trained (Bengio et al., 2003) and enable to define a word in relation to other words in a metric space. Recurrent Neural Network Mikolov et al. (2010) suggested the use of Recurrent Neural Networks (RNN) to model long-range dependencies between words as they are not restricted to a fixed context length, like the feedforward neural network (Bengio et al., 2003). The hidden representation s(t) for the word in position t of the sentence in the RNN follows a first order auto-regressive dynamic (Eq. 1), where W is the matrix connecting the hidden representation of the previous word s(t − 1) to the current one, w(t) is the one-</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Bian</author>
<author>Bin Gao</author>
<author>Tie-Yan Liu</author>
</authors>
<title>Knowledge-powered deep learning for word embedding.</title>
<date>2014</date>
<booktitle>In Machine Learning and Knowledge Discovery in Databases, Lecture Notes in Computer Science,</booktitle>
<volume>8724</volume>
<pages>132--148</pages>
<contexts>
<context position="16606" citStr="Bian et al. (2014)" startWordPosition="2767" endWordPosition="2770">ight word order, thus it is likely that a combination of these approaches will result in even higher accuracies. Gubbins and Vlachos (2013) developed a countbased dependency language model achieving 50% accuracy. Finally, Mikolov et al. (2013) report that they achieved 55.4% accuracy with an ensemble of RNNs, without giving any other details. 6 Discussion Related work Mirowski et al. (2010) incorporated syntactic information into neural language models using PoS tags as additional input to LBLs but obtained only a small reduction of the word error rate in a speech recognition task. Similarly, Bian et al. (2014) enriched the Continuous Bag-of514 Words (CBOW) model of Mikolov et al. (2013) by incorporating morphology, PoS tags and entity categories into 600-dimensional word embeddings trained on the Gutenberg dataset, increasing sentence completion accuracy from 41% to 44%. Other work on incorporating syntax into language modeling include Chelba et al. (1997) and Pauls and Klein (2012), however none of these approaches considered neural language models, only count-based ones. Levy and Goldberg (2014) and Zhao et al. (2014) proposed to train neural word embeddings using skip-grams and CBOWs on dependen</context>
</contexts>
<marker>Bian, Gao, Liu, 2014</marker>
<rawString>Jiang Bian, Bin Gao, and Tie-Yan Liu. 2014. Knowledge-powered deep learning for word embedding. In Machine Learning and Knowledge Discovery in Databases, Lecture Notes in Computer Science, volume 8724, pages 132–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>David Engle</author>
<author>Frederick Jelinek</author>
<author>Victor Jimenez</author>
<author>Sanjeev Khudanpur</author>
<author>Lidia Mangu</author>
<author>Harry Printz</author>
</authors>
<title>Eric Ristad,</title>
<date>1997</date>
<booktitle>In Proceedings of Eurospeech,</booktitle>
<volume>5</volume>
<pages>2775--2778</pages>
<location>Ronald Rosenfeld, Andreas</location>
<contexts>
<context position="16959" citStr="Chelba et al. (1997)" startWordPosition="2822" endWordPosition="2825">ussion Related work Mirowski et al. (2010) incorporated syntactic information into neural language models using PoS tags as additional input to LBLs but obtained only a small reduction of the word error rate in a speech recognition task. Similarly, Bian et al. (2014) enriched the Continuous Bag-of514 Words (CBOW) model of Mikolov et al. (2013) by incorporating morphology, PoS tags and entity categories into 600-dimensional word embeddings trained on the Gutenberg dataset, increasing sentence completion accuracy from 41% to 44%. Other work on incorporating syntax into language modeling include Chelba et al. (1997) and Pauls and Klein (2012), however none of these approaches considered neural language models, only count-based ones. Levy and Goldberg (2014) and Zhao et al. (2014) proposed to train neural word embeddings using skip-grams and CBOWs on dependency parse trees, but did not extend their approach to actual language models such as LBL and RNN and did not evaluate the word embeddings on word completion tasks. Note that we assume that the dependency tree is supplied prior to running the RNN which limits the scope of the Dependency RNN to the scoring of complete sentences, not to next word predicti</context>
</contexts>
<marker>Chelba, Engle, Jelinek, Jimenez, Khudanpur, Mangu, Printz, 1997</marker>
<rawString>Ciprian Chelba, David Engle, Frederick Jelinek, Victor Jimenez, Sanjeev Khudanpur, Lidia Mangu, Harry Printz, Eric Ristad, Ronald Rosenfeld, Andreas Stolcke, et al. 1997. Structure and performance of a dependency language model. In Proceedings of Eurospeech, volume 5, pages 2775–2778.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
</authors>
<title>Supervised Sequence Labelling with Recurrent Neural Networks.</title>
<date>2012</date>
<booktitle>Studies in Computational Intelligence.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="18043" citStr="Graves, 2012" startWordPosition="3004" endWordPosition="3005">r to running the RNN which limits the scope of the Dependency RNN to the scoring of complete sentences, not to next word prediction (unless a dependency tree parse for the sentence to be generated is provided). Nevertheless, it is common in speech recognition and machine translation to use a conventional decoder to produce an N-best list of the most likely candidate sentences and then re-score them with the language model. (Chelba et al., 1997; Pauls and Klein, 2011) Tai et al. (2015) propose a similar approach to ours, learning Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997; Graves, 2012) RNNs on dependency parse tree network topologies. Their architectures is not designed to predict next-word probability distributions, as in a language model, but to classify the input words (sentiment analysis task) or to measure the similarity in hidden representations (semantic relatedness task). Their relative improvement in performance (tree LSTMs vs standard LSTMs) on these two tasks is smaller than ours, probably because the LSTMs are better than RNNs at storing long-term dependencies and thus do not benefit form the word ordering from dependency trees as much as RNNs. In a similar vein</context>
</contexts>
<marker>Graves, 2012</marker>
<rawString>Alex Graves. 2012. Supervised Sequence Labelling with Recurrent Neural Networks. Studies in Computational Intelligence. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Gubbins</author>
<author>Andreas Vlachos</author>
</authors>
<title>Dependency language models for sentence completion.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1990" citStr="Gubbins and Vlachos, 2013" startWordPosition="306" endWordPosition="309">ge model to be probabilistic and it has been observed that perplexity does not always correlate with the downstream task performance. For these reasons, Zweig and Burges (2012) proposed the Sentence Completion Challenge, in which the task is to pick the correct word to complete a sentence out of five candidates. Performance is evaluated by accuracy (how many sentences were completed correctly), thus both probabilistic and non-probabilistic models (e.g. Roark et al. (2007)) can be compared. Recent approaches for this task include both neural and count-based language models (Zweig et al., 2012; Gubbins and Vlachos, 2013; Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013). Most neural language models consider the tokens in a sentence in the order they appear, and the hidden state representation of the network is typically reset at the beginning of each sentence. In this work we propose a novel neural language model that learns a recurrent neural network (RNN) (Mikolov et al., 2010) on top of the syntactic dependency parse of a sentence. Syntactic dependencies bring relevant contexts closer to the word being predicted, thus enhancing performance as shown by Gubbins and Vlachos (2013) for count-based language mo</context>
<context position="3490" citStr="Gubbins and Vlachos (2013)" startWordPosition="555" endWordPosition="559">antic relatedness tasks, but not as a language model. Adapting the RNN to use the syntactic dependency structure required to reset and run the network on all the paths in the dependency parse tree of a given sentence, while maintaining a count of how often each token appears in those paths. Furthermore, we explain how we can incorporate the dependency labels as features. Our results show that the dependency RNN language model proposed outperforms the RNN proposed by Mikolov et al. (2011) by about 10 points in accuracy. Furthermore, it improves upon the count-based dependency language model of Gubbins and Vlachos (2013), while achieving slightly worse than the recent state-of-the-art results by Mnih and Kavukcuoglu (2013). Finally, we make the code and preprocessed data available to facilitate comparisons with future work. 511 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 511–517, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Dependency Recurrent Neural Network Count-based language models operate by assigning probabilities to sentence</context>
<context position="9939" citStr="Gubbins and Vlachos (2013)" startWordPosition="1666" endWordPosition="1669">every time-step, the contribution of word token wi can be discounted by multiplying the learning rate by the discount factor: αiλ. Sentence Probability in Dependency RNN Given a word wi, let us define the ancestor sequence A(wi) to be the subsequence of words, taken as a subset from {wk}i−1 k=0 and describing the path from the root node w0 to the parent of wi. For example, in Figure 1, the ancestors A(very) of word token very are saw, binoculars and strong. Assuming that each word wi is conditionally independent of the words outside of its ancestor sequence, given its ancestor sequence A(wi), Gubbins and Vlachos (2013) showed that the probability of a sentence (i.e., the probability of a lexicalized tree 5T given an unlexicalized tree T) could be written as: P[5T |T] = � |S |P[wi|A(wi)] (4) i=1 This means that the conditional likelihood of a word given its ancestors needs to be counted only once in the calculation of the sentence likelihood, even though each word can appear in multiple unrolls. When modeling a sentence using an RNN, the state sj that is used to generate the distribution of words wi (where j is the parent of i in the tree), represents the vector embedding of the history of the ancestor words</context>
<context position="16127" citStr="Gubbins and Vlachos (2013)" startWordPosition="2691" endWordPosition="2694">y of (un-)labeled dependency RNN (depRNN and ldepRNN respectively). ant of neural language models with 54.7% to 55.5% accuracy (Mnih and Teh, 2012; Mnih and Kavukcuoglu, 2013). We conjecture that their superior performance might stem from the fact that LBLs, just like n-grams, take into account the order of the words in the context and can thus model higher-order Markovian dynamics than the simple first-order autoregressive dynamics in RNNs. The depRNN proposed ignores the left-to-right word order, thus it is likely that a combination of these approaches will result in even higher accuracies. Gubbins and Vlachos (2013) developed a countbased dependency language model achieving 50% accuracy. Finally, Mikolov et al. (2013) report that they achieved 55.4% accuracy with an ensemble of RNNs, without giving any other details. 6 Discussion Related work Mirowski et al. (2010) incorporated syntactic information into neural language models using PoS tags as additional input to LBLs but obtained only a small reduction of the word error rate in a speech recognition task. Similarly, Bian et al. (2014) enriched the Continuous Bag-of514 Words (CBOW) model of Mikolov et al. (2013) by incorporating morphology, PoS tags and </context>
</contexts>
<marker>Gubbins, Vlachos, 2013</marker>
<rawString>Joseph Gubbins and Andreas Vlachos. 2013. Dependency language models for sentence completion. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>Jurgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<journal>Neural Computation,</journal>
<pages>9--17351780</pages>
<contexts>
<context position="18028" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="3000" endWordPosition="3003">e dependency tree is supplied prior to running the RNN which limits the scope of the Dependency RNN to the scoring of complete sentences, not to next word prediction (unless a dependency tree parse for the sentence to be generated is provided). Nevertheless, it is common in speech recognition and machine translation to use a conventional decoder to produce an N-best list of the most likely candidate sentences and then re-score them with the language model. (Chelba et al., 1997; Pauls and Klein, 2011) Tai et al. (2015) propose a similar approach to ours, learning Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997; Graves, 2012) RNNs on dependency parse tree network topologies. Their architectures is not designed to predict next-word probability distributions, as in a language model, but to classify the input words (sentiment analysis task) or to measure the similarity in hidden representations (semantic relatedness task). Their relative improvement in performance (tree LSTMs vs standard LSTMs) on these two tasks is smaller than ours, probably because the LSTMs are better than RNNs at storing long-term dependencies and thus do not benefit form the word ordering from dependency trees as much as RNNs. In</context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9:17351780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Dependencybased word embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>302--308</pages>
<contexts>
<context position="17103" citStr="Levy and Goldberg (2014)" startWordPosition="2844" endWordPosition="2847"> to LBLs but obtained only a small reduction of the word error rate in a speech recognition task. Similarly, Bian et al. (2014) enriched the Continuous Bag-of514 Words (CBOW) model of Mikolov et al. (2013) by incorporating morphology, PoS tags and entity categories into 600-dimensional word embeddings trained on the Gutenberg dataset, increasing sentence completion accuracy from 41% to 44%. Other work on incorporating syntax into language modeling include Chelba et al. (1997) and Pauls and Klein (2012), however none of these approaches considered neural language models, only count-based ones. Levy and Goldberg (2014) and Zhao et al. (2014) proposed to train neural word embeddings using skip-grams and CBOWs on dependency parse trees, but did not extend their approach to actual language models such as LBL and RNN and did not evaluate the word embeddings on word completion tasks. Note that we assume that the dependency tree is supplied prior to running the RNN which limits the scope of the Dependency RNN to the scoring of complete sentences, not to next word prediction (unless a dependency tree parse for the sentence to be generated is provided). Nevertheless, it is common in speech recognition and machine t</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Dependencybased word embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 302–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="12895" citStr="Manning et al., 2014" startWordPosition="2166" endWordPosition="2169"> of the sentence is the sum of the per-token logprobabilities in that hash-table. We also further 2http://research.microsoft.com/en-us/projects/rnn/ prep dobj dep ROOT nsubj amod advmod 513 enhanced the RNN library by replacing some large matrix multiplication routines by calls to the CBLAS library, thus yielding a two- to three-fold speed-up in the test and training time.3 The training corpus consists of 522 19th century novels from Project Gutenberg (Zweig and Burges, 2012). All processing (sentence-splitting, PoS tagging, syntactic parsing) was performed using the Stanford CoreNLP toolkit (Manning et al., 2014). The test set contains 1040 sentences to be completed. Each sentence consists of one ground truth and 4 impostor sentences where a specific word has been replaced with a syntactically correct but semantically incorrect impostor word. Dependency trees are generated for each sentence candidate. We split that set into two, using the first 520 sentences in the validation (development) set and the latter 520 sentences in the test set. During training, we start annealing the learning rate λ with decay factor 0.66 as soon as the classification error on the validation set starts to increase. 5 Result</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Valerio Miceli-Barone</author>
<author>Giuseppe Attardi</author>
</authors>
<title>Non-projective dependency-based prereordering with recurrent neural network for machine translation.</title>
<date>2015</date>
<booktitle>In The 53rd Annual Meeting of the Association for Computational Linguistics and The 7th International Joint Conference of the Asian Federation of Natural Language Processing.</booktitle>
<marker>Miceli-Barone, Attardi, 2015</marker>
<rawString>Antonio Valerio Miceli-Barone and Giuseppe Attardi. 2015. Non-projective dependency-based prereordering with recurrent neural network for machine translation. In The 53rd Annual Meeting of the Association for Computational Linguistics and The 7th International Joint Conference of the Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Geoff Zweig</author>
</authors>
<title>Context dependent recurrent neural network language model.</title>
<date>2012</date>
<booktitle>In Speech Language Technologies (SLT), 2012 IEEE Workshop on.</booktitle>
<publisher>IEEE.</publisher>
<contexts>
<context position="7269" citStr="Mikolov and Zweig, 2012" startWordPosition="1190" endWordPosition="1193">n annealed learning rate A. The BackPropagation Through Time (BPTT) variant of SGD enables to sum-up gradients from consecutive time steps before updating the parameters of the RNN and to handle the long-range temporal dependencies in the hidden s and output y sequences. The loss function is the cross-entropy between the generated word distribution y(t) and the target one-hot word distribution w(t + 1), and involves the log-likelihood terms log yw(t+1)(t). For speed-up, the estimation of the output word probabilities is done using hierarchical softmax outputs, i.e., class-based factorization (Mikolov and Zweig, 2012). Each word wi is assigned to a class ci and the corresponding log-likelihood is effectively log ywi(t) = log yci(t) + log ywj(t), where j is the index of word wi among words belonging to class ci. In our experiments, we binned the words found in our training corpus into 250 classes according to frequency, roughly corresponding to the square root of the vocabulary size. Dependency RNN RNNs are designed to process sequential data by iteratively presenting them with word w(t) and generating next word’s probability distribution y(t) at each time step. They can be reset at the beginning of a sente</context>
<context position="10807" citStr="Mikolov and Zweig, 2012" startWordPosition="1817" endWordPosition="1820">ncestors needs to be counted only once in the calculation of the sentence likelihood, even though each word can appear in multiple unrolls. When modeling a sentence using an RNN, the state sj that is used to generate the distribution of words wi (where j is the parent of i in the tree), represents the vector embedding of the history of the ancestor words A(wi). Therefore, we count the term P [wi|sj] only once when computing the likelihood of the sentence. 3 Labelled Dependency RNN The model presented so far does not use dependency labels. For this purpose we adapted the context-dependent RNN (Mikolov and Zweig, 2012) to handle them as additional Mdimensional label input features f(t). These features require a matrix F that connects label features to word vectors, thus yielding a new dynamical model (Eq. 5) in the RNN, and a matrix G that connects label features to output word probabilities. The full model becomes as follows: s(t) = f (Ws(t − 1) + Uw(t) + Ff(t))(5) y(t) = g (Vs(t) + Gf(t) + dh(,t n+1))(6) On our training dataset, the dependency parsing model found M = 44 distinct labels (e.g., nsubj, det or prep). At each time step t, the context word w(t) is associated a single dependency label f(t) (a on</context>
</contexts>
<marker>Mikolov, Zweig, 2012</marker>
<rawString>Tomas Mikolov and Geoff Zweig. 2012. Context dependent recurrent neural network language model. In Speech Language Technologies (SLT), 2012 IEEE Workshop on. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,</booktitle>
<pages>1045--1048</pages>
<location>Makuhari, Chiba, Japan,</location>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Anoop Deoras</author>
<author>Daniel Povey</author>
<author>Lukas Burget</author>
<author>Jan Cernocky</author>
</authors>
<title>Strategies for training large scale neural network language models.</title>
<date>2011</date>
<booktitle>In Automatic Speech Recognition and Understanding (ASRU), 2011 IEEE Workshop on,</booktitle>
<pages>196--201</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="3356" citStr="Mikolov et al. (2011)" startWordPosition="535" endWordPosition="538">mory (LSTM) architecture to tree-structured network topologies and evaluate it at sentence-level sentiment classification and semantic relatedness tasks, but not as a language model. Adapting the RNN to use the syntactic dependency structure required to reset and run the network on all the paths in the dependency parse tree of a given sentence, while maintaining a count of how often each token appears in those paths. Furthermore, we explain how we can incorporate the dependency labels as features. Our results show that the dependency RNN language model proposed outperforms the RNN proposed by Mikolov et al. (2011) by about 10 points in accuracy. Furthermore, it improves upon the count-based dependency language model of Gubbins and Vlachos (2013), while achieving slightly worse than the recent state-of-the-art results by Mnih and Kavukcuoglu (2013). Finally, we make the code and preprocessed data available to facilitate comparisons with future work. 511 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 511–517, Beijing, China, July 26-31, 2015. c�2015 Association for Comp</context>
<context position="5477" citStr="Mikolov et al. (2011)" startWordPosition="884" endWordPosition="887">he hidden representation of the previous word s(t − 1) to the current one, w(t) is the one-hot index of the current word (in a vocabulary of size N words) and U is the matrix containing the embeddings for all the words in the vocabulary: s(t) = f (Ws(t − 1) + Uw(t)) (1) The nonlinearity f is typically the logistic sigmoid function f(x) = 1+exp(−x). At each time step, the 1 RNN generates the word probability vector y(t) for the next word w(t + 1), using the output word embedding matrix V and the softmax nonlinearity g(xi) = exp(xi) i exp(xi): y(t) = g (Vs(t)) (2) RNN with Maximum Entropy Model Mikolov et al. (2011) combined RNNs with a maximum entropy model, essentially adding a matrix that directly connects the input words’ n-gram context w(t − n + 1, ... , t) to the output word probabilities. In practice, because of the large vocabulary size N, designing such a matrix is computationally prohibitive. Instead, a hash-based implementation is used, where the word context is fed through a hash function h that computes the index h(w(t − n + 1, ... , t)) of the context words 1In our notation, we make a distinction between the word token w(t) at position t in the sentence and its one-hot vector representation</context>
</contexts>
<marker>Mikolov, Deoras, Povey, Burget, Cernocky, 2011</marker>
<rawString>Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget, and Jan Cernocky. 2011. Strategies for training large scale neural network language models. In Automatic Speech Recognition and Understanding (ASRU), 2011 IEEE Workshop on, pages 196–201. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="2041" citStr="Mikolov et al., 2013" startWordPosition="314" endWordPosition="317">hat perplexity does not always correlate with the downstream task performance. For these reasons, Zweig and Burges (2012) proposed the Sentence Completion Challenge, in which the task is to pick the correct word to complete a sentence out of five candidates. Performance is evaluated by accuracy (how many sentences were completed correctly), thus both probabilistic and non-probabilistic models (e.g. Roark et al. (2007)) can be compared. Recent approaches for this task include both neural and count-based language models (Zweig et al., 2012; Gubbins and Vlachos, 2013; Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013). Most neural language models consider the tokens in a sentence in the order they appear, and the hidden state representation of the network is typically reset at the beginning of each sentence. In this work we propose a novel neural language model that learns a recurrent neural network (RNN) (Mikolov et al., 2010) on top of the syntactic dependency parse of a sentence. Syntactic dependencies bring relevant contexts closer to the word being predicted, thus enhancing performance as shown by Gubbins and Vlachos (2013) for count-based language models. Our Dependency RNN model is published simulta</context>
<context position="16231" citStr="Mikolov et al. (2013)" startWordPosition="2706" endWordPosition="2709">to 55.5% accuracy (Mnih and Teh, 2012; Mnih and Kavukcuoglu, 2013). We conjecture that their superior performance might stem from the fact that LBLs, just like n-grams, take into account the order of the words in the context and can thus model higher-order Markovian dynamics than the simple first-order autoregressive dynamics in RNNs. The depRNN proposed ignores the left-to-right word order, thus it is likely that a combination of these approaches will result in even higher accuracies. Gubbins and Vlachos (2013) developed a countbased dependency language model achieving 50% accuracy. Finally, Mikolov et al. (2013) report that they achieved 55.4% accuracy with an ensemble of RNNs, without giving any other details. 6 Discussion Related work Mirowski et al. (2010) incorporated syntactic information into neural language models using PoS tags as additional input to LBLs but obtained only a small reduction of the word error rate in a speech recognition task. Similarly, Bian et al. (2014) enriched the Continuous Bag-of514 Words (CBOW) model of Mikolov et al. (2013) by incorporating morphology, PoS tags and entity categories into 600-dimensional word embeddings trained on the Gutenberg dataset, increasing sent</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piotr Mirowski</author>
<author>Sumit Chopra</author>
<author>Suhrid Balakrishnan</author>
<author>Srinivas Bangalore</author>
</authors>
<title>Feature-rich continuous language models for speech recognition.</title>
<date>2010</date>
<booktitle>In Spoken Language Technology Workshop (SLT), 2010 IEEE,</booktitle>
<pages>241--246</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="16381" citStr="Mirowski et al. (2010)" startWordPosition="2730" endWordPosition="2733"> just like n-grams, take into account the order of the words in the context and can thus model higher-order Markovian dynamics than the simple first-order autoregressive dynamics in RNNs. The depRNN proposed ignores the left-to-right word order, thus it is likely that a combination of these approaches will result in even higher accuracies. Gubbins and Vlachos (2013) developed a countbased dependency language model achieving 50% accuracy. Finally, Mikolov et al. (2013) report that they achieved 55.4% accuracy with an ensemble of RNNs, without giving any other details. 6 Discussion Related work Mirowski et al. (2010) incorporated syntactic information into neural language models using PoS tags as additional input to LBLs but obtained only a small reduction of the word error rate in a speech recognition task. Similarly, Bian et al. (2014) enriched the Continuous Bag-of514 Words (CBOW) model of Mikolov et al. (2013) by incorporating morphology, PoS tags and entity categories into 600-dimensional word embeddings trained on the Gutenberg dataset, increasing sentence completion accuracy from 41% to 44%. Other work on incorporating syntax into language modeling include Chelba et al. (1997) and Pauls and Klein (</context>
</contexts>
<marker>Mirowski, Chopra, Balakrishnan, Bangalore, 2010</marker>
<rawString>Piotr Mirowski, Sumit Chopra, Suhrid Balakrishnan, and Srinivas Bangalore. 2010. Feature-rich continuous language models for speech recognition. In Spoken Language Technology Workshop (SLT), 2010 IEEE, pages 241–246. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th International Conference on Machine Learning,</booktitle>
<pages>641648</pages>
<contexts>
<context position="14598" citStr="Mnih and Hinton, 2007" startWordPosition="2450" endWordPosition="2453">ccuracy, essentially matching the 39% accuracy reported for Good-Turing n-gram language models in Zweig et al. (2012). Modelling 4-grams yields even better results, closer to the 45% accuracy reported for RNNs in (Zweig et al., 2012).4 As Table 2 shows, dependency RNNs (depRNN) enable about 10 point word accuracy improvement over sequential RNNs. The best accuracy achieved by the depRNN on the combined development and test sets used to report results in previous work was 53.5%. The best reported results in the MSR sentence completion challenge have been achieved by Log-BiLinear Models (LBLs) (Mnih and Hinton, 2007), a vari3Our code and our preprocessed datasets are available from: https://github.com/piotrmirowski/ DependencyTreeRnn 4The paper did not provide details on the maximum entropy features or on class-based hierarchical softmax). Architecture 50h 100h 200h 300h RNN (dev) 29.6 30.0 30.0 30.6 RNN (test) 28.1 30.0 30.4 28.5 RNN+2g (dev) 29.6 28.7 29.4 29.8 RNN+2g (test) 29.6 28.7 28.1 30.2 RNN+3g (dev) 39.2 39.4 38.8 36.5 RNN+3g (test) 40.8 40.6 40.2 39.8 RNN+4g (dev) 40.2 40.6 40.0 40.2 RNN+4g (test) 42.3 41.2 40.4 39.2 Table 1: Accuracy of sequential RNN on the MSR Sentence Completion Challenge. </context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In Proceedings of the 24th International Conference on Machine Learning, page 641648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Koray Kavukcuoglu</author>
</authors>
<title>Learning word embeddings efficiently with noise-contrastive estimation.</title>
<date>2013</date>
<booktitle>Advances in Neural Information Processing Systems 26,</booktitle>
<pages>2265--2273</pages>
<editor>In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors,</editor>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context position="2018" citStr="Mnih and Kavukcuoglu, 2013" startWordPosition="310" endWordPosition="313">c and it has been observed that perplexity does not always correlate with the downstream task performance. For these reasons, Zweig and Burges (2012) proposed the Sentence Completion Challenge, in which the task is to pick the correct word to complete a sentence out of five candidates. Performance is evaluated by accuracy (how many sentences were completed correctly), thus both probabilistic and non-probabilistic models (e.g. Roark et al. (2007)) can be compared. Recent approaches for this task include both neural and count-based language models (Zweig et al., 2012; Gubbins and Vlachos, 2013; Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013). Most neural language models consider the tokens in a sentence in the order they appear, and the hidden state representation of the network is typically reset at the beginning of each sentence. In this work we propose a novel neural language model that learns a recurrent neural network (RNN) (Mikolov et al., 2010) on top of the syntactic dependency parse of a sentence. Syntactic dependencies bring relevant contexts closer to the word being predicted, thus enhancing performance as shown by Gubbins and Vlachos (2013) for count-based language models. Our Dependency RNN mod</context>
<context position="3594" citStr="Mnih and Kavukcuoglu (2013)" startWordPosition="570" endWordPosition="573">structure required to reset and run the network on all the paths in the dependency parse tree of a given sentence, while maintaining a count of how often each token appears in those paths. Furthermore, we explain how we can incorporate the dependency labels as features. Our results show that the dependency RNN language model proposed outperforms the RNN proposed by Mikolov et al. (2011) by about 10 points in accuracy. Furthermore, it improves upon the count-based dependency language model of Gubbins and Vlachos (2013), while achieving slightly worse than the recent state-of-the-art results by Mnih and Kavukcuoglu (2013). Finally, we make the code and preprocessed data available to facilitate comparisons with future work. 511 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 511–517, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Dependency Recurrent Neural Network Count-based language models operate by assigning probabilities to sentences by factorizing their likelihood into n-grams. Neural language models further embed each word w(t) into</context>
<context position="15676" citStr="Mnih and Kavukcuoglu, 2013" startWordPosition="2619" endWordPosition="2622">9.8 RNN+4g (dev) 40.2 40.6 40.0 40.2 RNN+4g (test) 42.3 41.2 40.4 39.2 Table 1: Accuracy of sequential RNN on the MSR Sentence Completion Challenge. Architecture 50h 100h 200h depRNN+3g (dev) 53.3 54.2 54.2 depRNN+3g (test) 51.9 52.7 51.9 ldepRNN+3g (dev) 48.8 51.5 49.0 ldepRNN+3g (test) 44.8 45.4 47.7 depRNN+4g (dev) 52.7 54.0 52.7 depRNN+4g (test) 48.9 51.3 50.8 ldepRNN+4g (dev) 49.4 50.0 (48.5) ldepRNN+4g (test) 47.7 51.4 (47.7) Table 2: Accuracy of (un-)labeled dependency RNN (depRNN and ldepRNN respectively). ant of neural language models with 54.7% to 55.5% accuracy (Mnih and Teh, 2012; Mnih and Kavukcuoglu, 2013). We conjecture that their superior performance might stem from the fact that LBLs, just like n-grams, take into account the order of the words in the context and can thus model higher-order Markovian dynamics than the simple first-order autoregressive dynamics in RNNs. The depRNN proposed ignores the left-to-right word order, thus it is likely that a combination of these approaches will result in even higher accuracies. Gubbins and Vlachos (2013) developed a countbased dependency language model achieving 50% accuracy. Finally, Mikolov et al. (2013) report that they achieved 55.4% accuracy wit</context>
</contexts>
<marker>Mnih, Kavukcuoglu, 2013</marker>
<rawString>Andriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings efficiently with noise-contrastive estimation. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2265–2273. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Yee W Teh</author>
</authors>
<title>A fast and simple algorithm for training neural probabilistic language models.</title>
<date>2012</date>
<booktitle>In Proceedings of the 29th International Conference on Machine Learning (ICML-12),</booktitle>
<pages>1751--1758</pages>
<contexts>
<context position="15647" citStr="Mnih and Teh, 2012" startWordPosition="2615" endWordPosition="2618">st) 40.8 40.6 40.2 39.8 RNN+4g (dev) 40.2 40.6 40.0 40.2 RNN+4g (test) 42.3 41.2 40.4 39.2 Table 1: Accuracy of sequential RNN on the MSR Sentence Completion Challenge. Architecture 50h 100h 200h depRNN+3g (dev) 53.3 54.2 54.2 depRNN+3g (test) 51.9 52.7 51.9 ldepRNN+3g (dev) 48.8 51.5 49.0 ldepRNN+3g (test) 44.8 45.4 47.7 depRNN+4g (dev) 52.7 54.0 52.7 depRNN+4g (test) 48.9 51.3 50.8 ldepRNN+4g (dev) 49.4 50.0 (48.5) ldepRNN+4g (test) 47.7 51.4 (47.7) Table 2: Accuracy of (un-)labeled dependency RNN (depRNN and ldepRNN respectively). ant of neural language models with 54.7% to 55.5% accuracy (Mnih and Teh, 2012; Mnih and Kavukcuoglu, 2013). We conjecture that their superior performance might stem from the fact that LBLs, just like n-grams, take into account the order of the words in the context and can thus model higher-order Markovian dynamics than the simple first-order autoregressive dynamics in RNNs. The depRNN proposed ignores the left-to-right word order, thus it is likely that a combination of these approaches will result in even higher accuracies. Gubbins and Vlachos (2013) developed a countbased dependency language model achieving 50% accuracy. Finally, Mikolov et al. (2013) report that the</context>
</contexts>
<marker>Mnih, Teh, 2012</marker>
<rawString>Andriy Mnih and Yee W Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 1751–1758.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Dependency grammar and dependency parsing.</title>
<date>2005</date>
<tech>MSI report, 5133(1959):1–32.</tech>
<contexts>
<context position="7962" citStr="Nivre, 2005" startWordPosition="1312" endWordPosition="1313">effectively log ywi(t) = log yci(t) + log ywj(t), where j is the index of word wi among words belonging to class ci. In our experiments, we binned the words found in our training corpus into 250 classes according to frequency, roughly corresponding to the square root of the vocabulary size. Dependency RNN RNNs are designed to process sequential data by iteratively presenting them with word w(t) and generating next word’s probability distribution y(t) at each time step. They can be reset at the beginning of a sentence by setting all the values of hidden vector s(t) to zero. Dependency parsing (Nivre, 2005) generates, for each sentence (which we note {w(t)JTt=0), a parse tree with a single root, many leaves and an unique path (also called unroll) from the root to each leaf, as illustrated on Figure 1. We now note {wiJi the set of word tokens appearing in the parse tree of a sentence. The order in the notation derives from the breadth-first traversal of that tree (i.e., the root word is noted w0). Each of the unrolls can be seen as a different sequence of words 512 pobj ROOT I saw the ship with very strong binoculars Figure 1: Example dependency tree {wi}, starting from the single root w0, that a</context>
</contexts>
<marker>Nivre, 2005</marker>
<rawString>Joakim Nivre. 2005. Dependency grammar and dependency parsing. MSI report, 5133(1959):1–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Faster and Smaller N-Gram Language Models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>258--267</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17901" citStr="Pauls and Klein, 2011" startWordPosition="2981" endWordPosition="2984">ls such as LBL and RNN and did not evaluate the word embeddings on word completion tasks. Note that we assume that the dependency tree is supplied prior to running the RNN which limits the scope of the Dependency RNN to the scoring of complete sentences, not to next word prediction (unless a dependency tree parse for the sentence to be generated is provided). Nevertheless, it is common in speech recognition and machine translation to use a conventional decoder to produce an N-best list of the most likely candidate sentences and then re-score them with the language model. (Chelba et al., 1997; Pauls and Klein, 2011) Tai et al. (2015) propose a similar approach to ours, learning Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997; Graves, 2012) RNNs on dependency parse tree network topologies. Their architectures is not designed to predict next-word probability distributions, as in a language model, but to classify the input words (sentiment analysis task) or to measure the similarity in hidden representations (semantic relatedness task). Their relative improvement in performance (tree LSTMs vs standard LSTMs) on these two tasks is smaller than ours, probably because the LSTMs are better than </context>
</contexts>
<marker>Pauls, Klein, 2011</marker>
<rawString>Adam Pauls and Dan Klein. 2011. Faster and Smaller N-Gram Language Models. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 258–267. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Large-scale syntactic language modeling with treelets.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>959--968</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16986" citStr="Pauls and Klein (2012)" startWordPosition="2827" endWordPosition="2830">ski et al. (2010) incorporated syntactic information into neural language models using PoS tags as additional input to LBLs but obtained only a small reduction of the word error rate in a speech recognition task. Similarly, Bian et al. (2014) enriched the Continuous Bag-of514 Words (CBOW) model of Mikolov et al. (2013) by incorporating morphology, PoS tags and entity categories into 600-dimensional word embeddings trained on the Gutenberg dataset, increasing sentence completion accuracy from 41% to 44%. Other work on incorporating syntax into language modeling include Chelba et al. (1997) and Pauls and Klein (2012), however none of these approaches considered neural language models, only count-based ones. Levy and Goldberg (2014) and Zhao et al. (2014) proposed to train neural word embeddings using skip-grams and CBOWs on dependency parse trees, but did not extend their approach to actual language models such as LBL and RNN and did not evaluate the word embeddings on word completion tasks. Note that we assume that the dependency tree is supplied prior to running the RNN which limits the scope of the Dependency RNN to the scoring of complete sentences, not to next word prediction (unless a dependency tre</context>
</contexts>
<marker>Pauls, Klein, 2012</marker>
<rawString>Adam Pauls and Dan Klein. 2012. Large-scale syntactic language modeling with treelets. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 959–968. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
</authors>
<title>Discriminative n-gram language modeling.</title>
<date>2007</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>21</volume>
<issue>2</issue>
<pages>392</pages>
<contexts>
<context position="1841" citStr="Roark et al. (2007)" startWordPosition="283" endWordPosition="286">f evaluating a language model has been to calculate its perplexity on a large corpus. However, this evaluation assumes the output of the language model to be probabilistic and it has been observed that perplexity does not always correlate with the downstream task performance. For these reasons, Zweig and Burges (2012) proposed the Sentence Completion Challenge, in which the task is to pick the correct word to complete a sentence out of five candidates. Performance is evaluated by accuracy (how many sentences were completed correctly), thus both probabilistic and non-probabilistic models (e.g. Roark et al. (2007)) can be compared. Recent approaches for this task include both neural and count-based language models (Zweig et al., 2012; Gubbins and Vlachos, 2013; Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013). Most neural language models consider the tokens in a sentence in the order they appear, and the hidden state representation of the network is typically reset at the beginning of each sentence. In this work we propose a novel neural language model that learns a recurrent neural network (RNN) (Mikolov et al., 2010) on top of the syntactic dependency parse of a sentence. Syntactic dependencies brin</context>
</contexts>
<marker>Roark, Saraclar, Collins, 2007</marker>
<rawString>Brian Roark, Murat Saraclar, and Michael Collins. 2007. Discriminative n-gram language modeling. Computer Speech &amp; Language, 21(2):373 – 392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Sheng Tai</author>
<author>Richard Socher</author>
<author>Christopher Manning</author>
</authors>
<title>Improved semantic representations from tree-structured long short-term memory networks.</title>
<date>2015</date>
<booktitle>In The 53rd Annual Meeting of the Association for Computational Linguistics and The 7th International Joint Conference of the Asian Federation of Natural Language Processing.</booktitle>
<contexts>
<context position="2700" citStr="Tai et al. (2015)" startWordPosition="426" endWordPosition="429"> tokens in a sentence in the order they appear, and the hidden state representation of the network is typically reset at the beginning of each sentence. In this work we propose a novel neural language model that learns a recurrent neural network (RNN) (Mikolov et al., 2010) on top of the syntactic dependency parse of a sentence. Syntactic dependencies bring relevant contexts closer to the word being predicted, thus enhancing performance as shown by Gubbins and Vlachos (2013) for count-based language models. Our Dependency RNN model is published simultaneously with another model, introduced in Tai et al. (2015), who extend the Long-Short Term Memory (LSTM) architecture to tree-structured network topologies and evaluate it at sentence-level sentiment classification and semantic relatedness tasks, but not as a language model. Adapting the RNN to use the syntactic dependency structure required to reset and run the network on all the paths in the dependency parse tree of a given sentence, while maintaining a count of how often each token appears in those paths. Furthermore, we explain how we can incorporate the dependency labels as features. Our results show that the dependency RNN language model propos</context>
<context position="17919" citStr="Tai et al. (2015)" startWordPosition="2985" endWordPosition="2988">and did not evaluate the word embeddings on word completion tasks. Note that we assume that the dependency tree is supplied prior to running the RNN which limits the scope of the Dependency RNN to the scoring of complete sentences, not to next word prediction (unless a dependency tree parse for the sentence to be generated is provided). Nevertheless, it is common in speech recognition and machine translation to use a conventional decoder to produce an N-best list of the most likely candidate sentences and then re-score them with the language model. (Chelba et al., 1997; Pauls and Klein, 2011) Tai et al. (2015) propose a similar approach to ours, learning Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997; Graves, 2012) RNNs on dependency parse tree network topologies. Their architectures is not designed to predict next-word probability distributions, as in a language model, but to classify the input words (sentiment analysis task) or to measure the similarity in hidden representations (semantic relatedness task). Their relative improvement in performance (tree LSTMs vs standard LSTMs) on these two tasks is smaller than ours, probably because the LSTMs are better than RNNs at storing lo</context>
</contexts>
<marker>Tai, Socher, Manning, 2015</marker>
<rawString>Kai Sheng Tai, Richard Socher, and Christopher Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. In The 53rd Annual Meeting of the Association for Computational Linguistics and The 7th International Joint Conference of the Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yinggong Zhao</author>
<author>Shujian Huang</author>
<author>Xinyu Dai</author>
<author>Jianbing Zhang</author>
<author>Jiajun Chen</author>
</authors>
<title>Learning word embeddings from dependency relations. In</title>
<date>2014</date>
<booktitle>In Proceedings of Asian Language Processing (IALP).</booktitle>
<contexts>
<context position="17126" citStr="Zhao et al. (2014)" startWordPosition="2849" endWordPosition="2852">small reduction of the word error rate in a speech recognition task. Similarly, Bian et al. (2014) enriched the Continuous Bag-of514 Words (CBOW) model of Mikolov et al. (2013) by incorporating morphology, PoS tags and entity categories into 600-dimensional word embeddings trained on the Gutenberg dataset, increasing sentence completion accuracy from 41% to 44%. Other work on incorporating syntax into language modeling include Chelba et al. (1997) and Pauls and Klein (2012), however none of these approaches considered neural language models, only count-based ones. Levy and Goldberg (2014) and Zhao et al. (2014) proposed to train neural word embeddings using skip-grams and CBOWs on dependency parse trees, but did not extend their approach to actual language models such as LBL and RNN and did not evaluate the word embeddings on word completion tasks. Note that we assume that the dependency tree is supplied prior to running the RNN which limits the scope of the Dependency RNN to the scoring of complete sentences, not to next word prediction (unless a dependency tree parse for the sentence to be generated is provided). Nevertheless, it is common in speech recognition and machine translation to use a con</context>
</contexts>
<marker>Zhao, Huang, Dai, Zhang, Chen, 2014</marker>
<rawString>Yinggong Zhao, Shujian Huang, Xinyu Dai, Jianbing Zhang, and Jiajun Chen. 2014. Learning word embeddings from dependency relations. In In Proceedings of Asian Language Processing (IALP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Zweig</author>
<author>Christopher J C Burges</author>
</authors>
<title>A challenge set for advancing language modeling.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACL-HLT</booktitle>
<pages>29--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1541" citStr="Zweig and Burges (2012)" startWordPosition="234" endWordPosition="237">ction Language Models (LM) are commonly used to score a sequence of tokens according to its probability of occurring in natural language. They are an essential building block in a variety of applications such as machine translation, speech recognition and grammatical error correction. The standard way of evaluating a language model has been to calculate its perplexity on a large corpus. However, this evaluation assumes the output of the language model to be probabilistic and it has been observed that perplexity does not always correlate with the downstream task performance. For these reasons, Zweig and Burges (2012) proposed the Sentence Completion Challenge, in which the task is to pick the correct word to complete a sentence out of five candidates. Performance is evaluated by accuracy (how many sentences were completed correctly), thus both probabilistic and non-probabilistic models (e.g. Roark et al. (2007)) can be compared. Recent approaches for this task include both neural and count-based language models (Zweig et al., 2012; Gubbins and Vlachos, 2013; Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013). Most neural language models consider the tokens in a sentence in the order they appear, and the hi</context>
<context position="12754" citStr="Zweig and Burges, 2012" startWordPosition="2147" endWordPosition="2150">ounted only once (and stored in a hash-table specific for that sentence). Once all the unrolls of a sentence are processed, the log-probability of the sentence is the sum of the per-token logprobabilities in that hash-table. We also further 2http://research.microsoft.com/en-us/projects/rnn/ prep dobj dep ROOT nsubj amod advmod 513 enhanced the RNN library by replacing some large matrix multiplication routines by calls to the CBLAS library, thus yielding a two- to three-fold speed-up in the test and training time.3 The training corpus consists of 522 19th century novels from Project Gutenberg (Zweig and Burges, 2012). All processing (sentence-splitting, PoS tagging, syntactic parsing) was performed using the Stanford CoreNLP toolkit (Manning et al., 2014). The test set contains 1040 sentences to be completed. Each sentence consists of one ground truth and 4 impostor sentences where a specific word has been replaced with a syntactically correct but semantically incorrect impostor word. Dependency trees are generated for each sentence candidate. We split that set into two, using the first 520 sentences in the validation (development) set and the latter 520 sentences in the test set. During training, we star</context>
</contexts>
<marker>Zweig, Burges, 2012</marker>
<rawString>Geoffrey Zweig and Christopher J. C. Burges. 2012. A challenge set for advancing language modeling. In Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 29–36. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Zweig</author>
<author>John C Platt</author>
<author>Christopher Meek</author>
<author>Christopher J C Burges</author>
<author>Ainur Yessenalina</author>
<author>Qiang Liu</author>
</authors>
<title>Computational approaches to sentence completion.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>601--610</pages>
<contexts>
<context position="1963" citStr="Zweig et al., 2012" startWordPosition="302" endWordPosition="305">output of the language model to be probabilistic and it has been observed that perplexity does not always correlate with the downstream task performance. For these reasons, Zweig and Burges (2012) proposed the Sentence Completion Challenge, in which the task is to pick the correct word to complete a sentence out of five candidates. Performance is evaluated by accuracy (how many sentences were completed correctly), thus both probabilistic and non-probabilistic models (e.g. Roark et al. (2007)) can be compared. Recent approaches for this task include both neural and count-based language models (Zweig et al., 2012; Gubbins and Vlachos, 2013; Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013). Most neural language models consider the tokens in a sentence in the order they appear, and the hidden state representation of the network is typically reset at the beginning of each sentence. In this work we propose a novel neural language model that learns a recurrent neural network (RNN) (Mikolov et al., 2010) on top of the syntactic dependency parse of a sentence. Syntactic dependencies bring relevant contexts closer to the word being predicted, thus enhancing performance as shown by Gubbins and Vlachos (2013) </context>
<context position="14093" citStr="Zweig et al. (2012)" startWordPosition="2366" endWordPosition="2369">to increase. 5 Results Table 1 shows the accuracy (validation and test sets) obtained using a simple RNN with 50, 100, 200 and 300-dimensional hidden word representation and 250 frequency-based word classes (vocabulary size N = 72846 words appearing at least 5 times in the training corpus). One notices that adding the direct word context to target word connections (using the additional matrix described in section 2), enables to jump from a poor performance of about 30% accuracy to about 40% test accuracy, essentially matching the 39% accuracy reported for Good-Turing n-gram language models in Zweig et al. (2012). Modelling 4-grams yields even better results, closer to the 45% accuracy reported for RNNs in (Zweig et al., 2012).4 As Table 2 shows, dependency RNNs (depRNN) enable about 10 point word accuracy improvement over sequential RNNs. The best accuracy achieved by the depRNN on the combined development and test sets used to report results in previous work was 53.5%. The best reported results in the MSR sentence completion challenge have been achieved by Log-BiLinear Models (LBLs) (Mnih and Hinton, 2007), a vari3Our code and our preprocessed datasets are available from: https://github.com/piotrmir</context>
<context position="18955" citStr="Zweig et al. (2012)" startWordPosition="3151" endWordPosition="3154"> task). Their relative improvement in performance (tree LSTMs vs standard LSTMs) on these two tasks is smaller than ours, probably because the LSTMs are better than RNNs at storing long-term dependencies and thus do not benefit form the word ordering from dependency trees as much as RNNs. In a similar vein to ours, MiceliBarone and Attardi (2015) simply propose to enhance RNN-based machine translation by permuting the order of the words in the source sentence to match the order of the words in the target sentence, using a source-side dependency parsing. Limitations of RNNs for word completion Zweig et al. (2012) reported that RNNs achieve lower perplexity than n-grams but do not always Figure 2: Perplexity vs. accuracy of RNNs outperform them on word completion tasks. As illustrated in Fig. 2, the validation set perplexity (comprising all 5 choices for each sentence) of the RNN keeps decreasing monotonically (once we start annealing the learning rate), whereas the validation accuracy rapidly reaches a plateau and oscillates. Our observation confirms that, once an RNN went through a few training epochs, change in perplexity is no longer a good predictor of change in word accuracy. We presume that the </context>
</contexts>
<marker>Zweig, Platt, Meek, Burges, Yessenalina, Liu, 2012</marker>
<rawString>Geoffrey Zweig, John C Platt, Christopher Meek, Christopher J. C. Burges, Ainur Yessenalina, and Qiang Liu. 2012. Computational approaches to sentence completion. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 601–610.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>