<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000016">
<title confidence="0.961726">
Language Identification of Search Engine Queries
</title>
<author confidence="0.992198">
Hakan Ceylan
</author>
<affiliation confidence="0.9993245">
Department of Computer Science
University of North Texas
</affiliation>
<address confidence="0.907726">
Denton, TX, 76203
</address>
<email confidence="0.99788">
hakan@unt.edu
</email>
<note confidence="0.475030333333333">
Yookyung Kim
Yahoo! Inc.
2821 Mission College Blvd.
</note>
<address confidence="0.867856">
Santa Clara, CA, 95054
</address>
<email confidence="0.889085">
ykim@yahoo-inc.com
</email>
<sectionHeader confidence="0.991456" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999549363636364">
We consider the language identification
problem for search engine queries. First,
we propose a method to automatically
generate a data set, which uses click-
through logs of the Yahoo! Search En-
gine to derive the language of a query indi-
rectly from the language of the documents
clicked by the users. Next, we use this
data set to train two decision tree classi-
fiers; one that only uses linguistic features
and is aimed for textual language identi-
fication, and one that additionally uses a
non-linguistic feature, and is geared to-
wards the identification of the language
intended by the users of the search en-
gine. Our results show that our method
produces a highly reliable data set very ef-
ficiently, and our decision tree classifier
outperforms some of the best methods that
have been proposed for the task of written
language identification on the domain of
search engine queries.
</bodyText>
<sectionHeader confidence="0.99889" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999679375">
The language identification problem refers to the
task of deciding in which natural language a given
text is written. Although the problem is heav-
ily studied by the Natural Language Processing
community, most of the research carried out to
date has been concerned with relatively long texts
such as articles or web pages which usually con-
tain enough text for the systems built for this task
to reach almost perfect accuracy. Figure 1 shows
the performance of 6 different language identifi-
cation methods on written texts of 10 European
languages that use the Roman Alphabet. It can
be seen that the methods reach a very high ac-
curacy when the text has 100 or more characters.
However, search engine queries are very short in
length; they have about 2 to 3 words on average,
</bodyText>
<figureCaption confidence="0.906202">
Figure 1: Performance of six Language Identifica-
tion methods on varying text size. Adapted from
(Poutsma, 2001).
</figureCaption>
<bodyText confidence="0.999629642857143">
which requires a reconsideration of the existing
methods built for this problem.
Correct identification of the language of the
queries is of critical importance to search engines.
Major search engines such as Yahoo! Search
(www.yahoo.com), or Google (www.google.com)
crawl billions of web pages in more than 50 lan-
guages, and about a quarter of their queries are in
languages other than English. Therefore a correct
identification of the language of a query is needed
in order to aid the search engine towards more ac-
curate results. Moreover, it also helps further pro-
cessing of the queries, such as stemming or spell
checking of the query terms.
One of the challenges in this problem is the lack
of any standard or publicly available data set. Fur-
thermore, creating such a data set is expensive as
it requires an extensive amount of work by hu-
man annotators. In this paper, we introduce a new
method to overcome this bottleneck by automat-
ically generating a data set of queries with lan-
guage annotations. We show that the data gener-
ated this way is highly reliable and can be used to
train a machine learning algorithm.
We also distinguish the problem of identifying
the textual language vs. the language intended by
the users for the search engine queries. For search
engines, there are cases where a correct identifi-
</bodyText>
<page confidence="0.908722">
1066
</page>
<note confidence="0.9996025">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1066–1074,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.9999774">
cation of the language does not necessarily im-
ply that the user wants to see the results in the
same language. For example, although the textual
identification of the language for the query ”homo
sapiens” is Latin, a user entering this query from
Spain, would most probably want to see Spanish
web pages, rather than web pages in Latin. We ad-
dress this issue by adding a non-linguistic feature
to our system.
We organize the rest of the paper as follows:
First, we provide an overview of the previous re-
search in this area. Second, we present our method
to automatically generate a data set, and evaluate
the effectiveness of this technique. As a result of
this evaluation, we obtain a human-annotated data
set which we use to evaluate the systems imple-
mented in the following sections. In Section 4, we
implement some of the existing models and com-
pare their performance on our test set. We then
use the results from these models to build a deci-
sion tree system. Next, we consider identifying the
language intended by the user for the results of the
query, and describe a system geared towards this
task. Finally, we conclude our study and discuss
the future directions for the problem.
</bodyText>
<sectionHeader confidence="0.999791" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999965958333334">
Most of the work carried out to date on the writ-
ten language identification problem consists of su-
pervised approaches that are trained on a list of
words or n-gram models for each reference lan-
guage. The word based approaches use a list of
short words, common words, or a complete vocab-
ulary which are extracted from a corpus for each
language. The short words approach uses a list of
words with at most four or five characters; such as
determiners, prepositions, and conjunctions, and
is used in (Ingle, 1976; Grefenstette, 1995). The
common words method is a generalization over
the short words one which, in addition, includes
other frequently occuring words without limiting
them to a specific length, and is used in (Souter et
al., 1994; Cowie et al., 1999). For classification,
the word-based approaches sort the list of words in
descending order of their frequency in the corpus
from which they are extracted. Then the likelihood
of each word in a given text can be calculated by
using rank-order statistics or by transforming the
frequencies into probabilities.
The n-gram based approaches are based on the
counts of character or byte n-grams, which are se-
quences of n characters or bytes, extracted from
a corpus for each reference language. Different
classification models that use the n-gram features
have been proposed. (Cavnar and Trenkle, 1994)
used an out-of-place rank order statistic to mea-
sure the distance of a given text to the n-gram
profile of each language. (Dunning, 1994) pro-
posed a system that uses Markov Chains of byte n-
grams with Bayesian Decision Rules to minimize
the probability error. (Grefenstette, 1995) simply
used trigram counts that are transformed into prob-
abilities, and found this superior to the short words
technique. (Sibun and Reynar, 1996) used Rela-
tive Entropy by first generating n-gram probabil-
ity distributions for both training and test data, and
then measuring the distance between the two prob-
ability distributions by using the Kullback-Liebler
Distance. (Poutsma, 2001) developed a system
based on Monte Carlo Sampling.
Linguini, a system proposed by (Prager, 1999),
combines the word-based and n-gram models us-
ing a vector-space based model and examines the
effectiveness of the combined model and the in-
dividual features on varying text size. Similarly,
(Lena Grothe and Nrnberger, 2008) combines both
models using the ad-hoc method of (Cavnar and
Trenkle, 1994), and also presents a comparison
study. The work most closely related to ours is
presented very recently in (Hammarstr¨om, 2007),
which proposes a model that uses a frequency dic-
tionary together with affix information in order to
identify the language of texts as short as one word.
Other systems that use methods aside from
the ones discussed above have also been pro-
posed. (Takci and Sogukpinar, 2004) used letter
frequency features in a centroid based classifica-
tion model. (Kruengkrai et al., 2005) proposed a
feature based on alignment of string kernels us-
ing suffix trees, and used it in two different clas-
sifiers. Finally, (Biemann and Teresniak, 2005)
presented an unsupervised system that clusters the
words based on sentence co-occurence.
Recently, (Hughes et al., 2006) surveyed the
previous work in this area and suggested that the
problem of language identification for written re-
sources, although well studied, has too many open
challenges which requires a more systematic and
collaborative study.
</bodyText>
<sectionHeader confidence="0.975922" genericHeader="method">
3 Data Generation
</sectionHeader>
<bodyText confidence="0.999989">
We start the construction of our data set by re-
trieving the queries, together with the clicked urls,
from the Yahoo! Search Engine for a three months
time period. For each language desired in our data
set, we retrieve the queries from the corresponding
</bodyText>
<page confidence="0.985462">
1067
</page>
<bodyText confidence="0.9873263">
Yahoo! web site in which the default language is
the same as the one sought.1 Then we preprocess
the queries by getting rid of the ones that have any
numbers or special characters in them, removing
extra spaces between query terms, and lowercas-
ing all the letters of the queries2. Next, we ag-
gregate the queries that are exactly the same, by
calculating the frequencies of the urls clicked for
each query.
As we pointed out in Section 1, and illustrated
in Figure 1, the language identification methods
give almost perfect accuracy when the text has 100
or more characters. Furthermore, it is suggested in
(Levering and Cutler, 2006) that the average tex-
tual content in a web page is 474 words. Thus we
assume that it is a fairly trivial task to identify the
language for an average web page using one of the
existing methods.3 In our case, this task gets al-
ready accomplished by the crawler for all the web
pages crawled by the search engine.
Thus we can summarize our information in two
separate tables; T1 and T2. For Table T1, we have
a set of queries Q, and each q E Q maps to a
set of url-frequency pairs. Each mapping is of the
form (q, u, fu), where u is a url clicked for q, and
fu is the frequency of u. Table T2, on the other
hand, contains the urls of all the web pages known
to the search engine and has only two columns;
(u, l), where u is a unique url, and l is the language
identified for u. Since we do not consider multi-
lingual web pages, every url in T2 is unique and
has only one language associated with it.
Next, we combine the tables T1 and T2 using
an inner join operation on the url columns. After
the join, we group the results by the language and
query columns, during which we also count the
number of distinct urls per query, and sum their
frequencies. We illustrate this operation with a
SQL query in Algorithm 1. As a result of these
operations, we have, for each query q E Q, a set of
triplets (l, fl, cu,l) where l is a language, fl is the
count of clicks for l (which we obtained through
the urls in language l), and cu,l is the count of
unique urls in language l.
The resulting table T3 associates queries with
languages, but also contains a lot of noise. First,
1We do not make a distinction between the different di-
alects of the same languge. For English, Spanish and Por-
tuguese we gather queries from the web sites of United States,
Mexico, and Brazil respectively.
</bodyText>
<footnote confidence="0.9750446">
2In this study, we only considered languages that use the
Roman alphabet.
3Although not done in this study, the urls of web pages
that have less than a defined number of words, such as 100,
can be discarded to ensure a higher confidence.
</footnote>
<bodyText confidence="0.5087025">
Input: Tables T1:[q, u, fu], T2:[u, l]
Output: Table T3:[q, l, fl, cu,l]
</bodyText>
<equation confidence="0.775077285714286">
CREATE VIEW T3 AS
SELECT
T1.q, T2.l, COUNT(T1.u) AS cu,l, SUM(T1.fu) AS fl
FROM T1
INNER JOIN T2
ON T1.u = T2.u
GROUP BY q, l;
</equation>
<bodyText confidence="0.882151">
Algorithm 1: Join Tables T1 and T2, group by
query and language, aggregate distinct url and fre-
quency counts.
we have queries that map to more than one lan-
guage, which suggests that the users clicked on the
urls in different languages for the same query. To
quantify the strength of each of these mappings,
we calculate a weight wq,l for each mapping of a
query q to a language l as:
</bodyText>
<equation confidence="0.680396">
wq,l = fl/Fq
</equation>
<bodyText confidence="0.9978905">
where Fq, the total frequency of a query q, is de-
fined as:
</bodyText>
<equation confidence="0.9650405">
�Fq = fl
lELq
</equation>
<bodyText confidence="0.999941785714286">
where Lq is the set of languages for which q has a
mapping. Having computed a weight wq l for each
mapping, we introduce our first threshold param-
eter, W. We eliminate all the queries in our data
set, which have weights, wq,l, below the threshold
W.
Second, even though some of the queries map to
only one language, this mapping cannot be trusted
due to the high frequency of the queries together
with too few distinct urls. This case suggests that
the query is most likely navigational. The intent
of navigational queries, such as ”ACL 2009”, is to
find a particular web site. Therefore they usually
consist of proper names, or acronyms that would
not be of much use to our language identification
problem. Hence we would like to get rid of the
navigational queries in our data set by using some
of the features proposed for the task of automatic
taxonomy of search engine queries. For a more
detailed discussion of this task, we refer the reader
to (Broder, 2002; Rose and Levinson, 2004; Lee et
al., 2005; Liu et al., 2006; Jansen et al., 2008).
Two of the features used in (Liu et al., 2006)
in identification of the navigational queries from
click-through data, are the number of Clicks Satis-
fied (nCS) and number of Results Satisfied (nRS).
In our problem, we substitute nCS with Fq, the to-
tal click frequency of the query q, and nRS with
</bodyText>
<page confidence="0.945241">
1068
</page>
<bodyText confidence="0.930742363636364">
UQ, the number of distinct urls clicked for q. Thus
we eliminate the queries that have a total click fre-
quency above a given frequency threshold F, and,
that have less than a given distinct number of urls,
U. Thus, we have three parameters that help us in
eliminating the noise from the inital data; W, F,
and U. We show the usage of these parameters in
SQL queries, in Algorithm 2.
Input: Tables T1:[q, u, fu], T2:[u, l], T3:[q, l, fl, cu,l]
Parameters W, F, and U
Output: Table D:[q, l]
</bodyText>
<equation confidence="0.908011307692308">
CREATE VIEW T4 AS
SELECT T1.q, COUNT(T1.u) AS cu, SUM(T1.fu) AS Fq
FROM T1
INNER JOIN T2 ON T1.u = T2.u
GROUP BY q;
CREATE VIEW D AS
SELECT T3.q, T3.l, T3.fl / T4.Fq AS wq,l
FROM T1
INNER JOIN T4 ON T3.q = T4.q 10
WHERE
T4.Fq &lt; F AND
wq,l &gt;= W AND
T4.cu,l &gt;= U;
</equation>
<bodyText confidence="0.970409769230769">
Algorithm 2: Construction of the final data set
D, by eliminating queries from T3 based on the
parameters W, F, and U.
The parameters F, U, and W are actually de-
pendent on the size of the data set under consid-
eration, and the study in (Silverstein et al., 1999)
suggests that we can get enough click-through data
for our analysis by retrieving a large sample of
queries. Since we retrieve the queries that are sub-
mitted within a three months period, for each lan-
guage, we have millions of unique queries in our
data set. Investigating a held-out development set
of queries retrieved from the United States web
site (www.yahoo.com), we empirically decided
the following values for the parameters, W = 1,
F = 50, and U = 5. In other words, we only
accepted the queries for which the contents of the
urls agree on the same language, that are submit-
ted less than 50 times, and at least have 5 unique
urls clicked.
The filtering process leaves us with 5-10% of
the queries due to the conservative choice of the
parameters. From the resulting set, we randomly
picked 500 queries and asked a native speaker to
annotate them. For each query, the annotator was
to classify the query into one of three categories:
</bodyText>
<listItem confidence="0.99963">
• Category-1: If the query does not contain
any foreign terms.
</listItem>
<table confidence="0.999834416666667">
Language Category-1 Category-1+2 Category-3
English 90.6% 94.2% 5.8%
French 84.6% 93.4% 6.6%
Portuguese 85.2% 93.4% 6.6%
Spanish 86.6% 97.4% 2.6%
Italian 82.4% 96.6% 3.4%
German 76.8% 87.2% 12.8%
Dutch 81.0% 92.0% 8.0%
Danish 82.4% 93.2% 6.8%
Finnish 87.2% 94.0% 6.0%
Swedish 86.6% 95.4% 4.6%
Average 84.3% 93.7% 6.3%
</table>
<tableCaption confidence="0.9855865">
Table 1: Annotation of 500 sample queries drawn
from the automatically generated data.
</tableCaption>
<listItem confidence="0.955954">
• Category-2: If there exists some foreign
terms but the query would still be expected
to bring web pages in the same language.
• Category-3: If the query belongs to other
languages, or all the terms are foreign to the
annotator.4
</listItem>
<bodyText confidence="0.99295440625">
90.6% of the queries in our data set were anno-
tated as Category-1, and 94.2% as Category-1 and
Category-2 combined. Having successful results
for the United States data set, we applied the same
parameters to the data sets retrieved for other lan-
guages as well, and had the native speakers of each
language annotate the queries in the same way. We
list these results in Table 1.
The results for English have the highest accu-
racy for Category-1, mostly due to the fact that we
tuned our parameters using the United States data.
The scores for German on the other hand, are the
lowest. We attribute this fact to the highly multi-
linguality of the Yahoo! Germany website, which
receives a high number of non-German queries.
In order to see how much of this multi-linguality
our parameter selection successfully eliminate, we
randomly picked 500 queries from the aggregated
but unfiltered queries of the Yahoo! Germany
website, and had them annotated as before.
As suspected, the second annotation results
showed that, only 47.6% of the queries were an-
notated as Category-1 and 60.2% are annotated
as Category-1 and Category-2 combined. Our
method was indeed successful and achieved 29.2%
improvement over Category-1, and 27% improve-
ment over Category-1 and Category-2 queries
combined.
Another interesting fact to note is the absolute
differences between Category-1 and Category-1+2
scores. While this number is very low, 3.8%,
for English, it is much higher for the other lan-
</bodyText>
<footnote confidence="0.876997">
4We do not expect the annotators to know the etymology
of the words or have the knowledge of all the acronyms.
</footnote>
<page confidence="0.784434">
1069
</page>
<table confidence="0.999934833333333">
Language MinC MaxC µc MinW MaxW µW
English 7 46 21.8 1 6 3.35
French 6 74 22.6 1 10 3.38
Portug. 3 87 22.5 1 14 3.55
Spanish 5 57 23.5 1 9 3.51
Italian 4 51 21.9 1 8 3.09
German 3 53 18.1 1 6 2.05
Dutch 5 43 16.3 1 6 2.11
Danish 3 40 14.3 1 6 1.93
Finnish 3 34 13.3 1 5 1.49
Swedish 3 42 13.7 1 8 1.80
Average 4.2 52.7 18.8 1 7.8 2.63
</table>
<tableCaption confidence="0.9308985">
Table 2: Properties of the test set formed by taking
350 Category-1 queries from each language.
</tableCaption>
<bodyText confidence="0.9990555">
guages. Through an investigation of Category-2
non-English queries, we find out that this is mostly
due to the usage of some common internet or
computer terms such as ”download”, ”software”,
”flash player”, among other native language query
terms.
</bodyText>
<sectionHeader confidence="0.992722" genericHeader="method">
4 Language Identification
</sectionHeader>
<bodyText confidence="0.999809605263158">
We start this section with the implementation of
three models each of which use a different exist-
ing feature. We categorize these models as statis-
tical, knowledge based, and morphological. We
then combine all three models in a machine learn-
ing framework using a novel approach. Finally, we
extend this framework by adding a non-linguistic
feature in order to identify the language intended
by the search engine user.
To train each model implemented, we used the
EuroParl Corpora, (Koehn, 2005), and the same 10
languages in Section 3. EuroParl Corpora is well
balanced, so we would not have any bias towards
a particular language resulting from our choice of
the corpora.
We tested all the systems in this section on a
test set of 3500 human annotated queries, which
is formed by taking 350 Category-1 queries from
each language. All the queries in the test set are
obtained from the evaluation results in Section
3. In Table 2, we give the properties of this test
set. We list the minimum, maximum, and average
number of characters and words (MinC, MaxC,
µC, MinW, MaxW, and µW respectively).
As can be seen in Table 2, the queries in our test
set have 18.8 characters on average, which is much
lower than the threshold suggested by the existing
systems to achieve a good accuracy. Another in-
teresting fact about the test set is that, languages
which are in the bottom half of Table 2 (German,
Dutch, Danish, Finnish, and Swedish) have lower
number of characters and words on average com-
pared to the languages in the upper half. This
is due to the characteristics of those languages,
which allow the construction of composite words
from multiple words, or have a richer morphology.
Thus, the concepts can be expressed in less num-
ber of words or characters.
</bodyText>
<subsectionHeader confidence="0.936126">
4.1 Models for Language Identification
</subsectionHeader>
<bodyText confidence="0.999982608695652">
We implement a statistical model using a charac-
ter based n-gram feature. For each language, we
collect the n-gram counts (for n = 1 to n = 7
also using the word beginning and ending spaces)
from the vocabulary of the training corpus, and
then generate a probability distribution from these
counts. We implemented this model using the
SRILM Toolkit (Stolcke, 2002) with the mod-
ified Kneser-Ney Discounting and interpolation
options. For comparison purposes, we also imple-
mented the Rank-Order method using the parame-
ters described in (Cavnar and Trenkle, 1994).
For the knowledge based method, we used the
vocabulary of each language obtained from the
training corpora, together with the word counts.
From these counts, we obtained a probability dis-
tribution for all the words in our vocabulary. In
other words, this time we used a word-based n-
gram method, only with n = 1. It should be noted
that increasing the size of n, which might help in
language identification of other types of written
texts, will not be helpful in this task due to the
unique nature of the search engine queries.
For the morphological feature; we gathered the
affix information for each language from the cor-
pora in an unsupervised fashion as described in
(Hammarstr¨om, 2006). This method basically
considers each possible morphological segmenta-
tion of the words in the training corpora by as-
suming a high frequency of occurence of salient
affixes, and also assuming that words are made up
of random characters. Each possible affix is as-
signed a score based on its frequency, random ad-
justment, and curve-drop probabilities, which re-
spectively indicate the probability of the affix be-
ing a random sequence, and the probability of be-
ing a valid morphological segment based on the in-
formation of the preceding or the succeding char-
acter. In Table 3, we present the top 10 results of
the probability distributions obtained from the vo-
cabulary of English, Finnish, and German corpora.
We give the performance of each model on
our test set in Table 4. The character based n-
gram model outperforms all the other models with
the exception of French, Spanish, and Italian on
which the word-based unigram model is better.
</bodyText>
<page confidence="0.989102">
1070
</page>
<table confidence="0.994544090909091">
English Finnish German
-nts 0.133 erityis- 0.216 -ungen 0.172
-ity 0.119 ihmisoikeus- 0.050 -en 0.066
-ised 0.079 -inen 0.038 gesamt- 0.066
-ated 0.075 -iksi 0.037 gemeinschafts- 0.051
-ing 0.069 -iseksi 0.030 verhandlugs- 0.040
-tions 0.069 -ssaan 0.028 agrar- 0.024
-ted 0.048 maatalous- 0.028
-ed 0.047 -aisesta 0.024 menschenrechts- 0.018
-ically 0.041 -iseen 0.023 umwelt- 0.017
-ly 0.040 -amme 0.023 -ches 0.017
</table>
<tableCaption confidence="0.993369">
Table 3: Top 10 prefixes and suffixes together with
</tableCaption>
<bodyText confidence="0.990067028571429">
their probabilities, obtained for English, Finnish,
and German.
The word-based unigram model performs poorly
on languages that may have highly inflected or
composite words such as Finnish, Swedish, and
German. This result is expected as we cannot
make sure that the training corpus will include
all the possible inflections or compositions of the
words in the language. The Rank-Order method
performs poorly compared to the character based
n-gram model, which suggests that for shorter
texts, a well-defined probability distribution with a
proper discounting strategy is better than using an
ad-hoc ranking method. The success of the mor-
phological feature depends heavily on the prob-
ability distribution of affixes in each language,
which in turn depends on the corpus due to the un-
supervised affix extraction algorithm. As can be
seen in Table 3, English affixes have a more uni-
form distribution than both Finnish and German.
Each model implemented in the previous sec-
tion has both strengths and weaknesses. The sta-
tistical approach is more robust to noise, such as
misspellings, than the others, however it may fail
to identify short queries or single words because
of the lack of enough evidence, and it may confuse
two languages that are very similar. In such cases,
the knowledge-based model could be more useful,
as it can find those query terms in the vocabulary.
On the other hand, the knowledge-based model
would have a sparse vocabulary for languages that
can have heavily inflected words such as Turkish,
and Finnish. In such cases, the morphological fea-
ture could provide a strong clue for identification
from the affix information of the terms.
</bodyText>
<subsectionHeader confidence="0.987996">
4.2 Decision Tree Classification
</subsectionHeader>
<bodyText confidence="0.999867166666667">
Noting the fact that each model can complement
the other(s) in certain cases, we combined them by
using a decision tree (DT) classifier. We trained
the classifier using the automatically annotated
data set, which we created in Section 3. Since
this set comes with a certain amount of noise, we
</bodyText>
<table confidence="0.999811666666667">
Language Stat. Knowl. Morph. Rank-Order
English 90.3% 83.4% 60.6% 78.0%
French 77.4% 82.0% 4.86% 56.0%
Portuguese 79.7% 75.7% 11.7% 70.3%
Spanish 73.1% 78.3% 2.86% 46.3%
Italian 85.4% 87.1% 43.4% 77.7%
German 78.0% 60.0% 26.6% 58.3%
Dutch 85.7% 64.9% 23.1% 65.1%
Danish 87.7% 67.4% 46.9% 61.7%
Finnish 87.4% 49.4% 38.0% 82.3%
Swedish 81.7% 55.1% 2.0% 56.6%
Average 82.7% 70.3% 26.0% 65.2%
</table>
<tableCaption confidence="0.678801666666667">
Table 4: Evaluation of the models built from the
individual features, and the Rank-Order method
on the test set.
</tableCaption>
<bodyText confidence="0.999463894736842">
pruned the DT during the training phase to avoid
overfitting. This way, we built a robust machine
learning framework at a very low cost and without
any human labour.
As the features of our DT classifier, we use the
results of the models that are implemented in Sec-
tion 4.1, together with the confidence scores cal-
culated for each instance. To calculate a confi-
dence score for the models, we note that since
each model makes its selection based on the lan-
guage that gives the highest probability, a confi-
dence score should indicate the relative highness
of that probability compared to the probabilities
of other languages. To calculate this relative high-
ness, we use the Kurtosis measure, which indicates
how peaked or flat the probabilities in a distribu-
tion are compared to a normal distribution. To cal-
culate the Kurtosis value, κ, we use the equation
below.
</bodyText>
<equation confidence="0.9926445">
κ = El∈L(pl − µ)4
(N − 1)σ4
</equation>
<bodyText confidence="0.99204">
where L is the set of languages, N is the number
of languages in the set, pl is the probability for
language l E L, and µ and σ are respectively the
mean and the the standard deviation values of P =
{pl|l E L}.
We calculate a κ measure for the result of each
model, and then discretize it into one of three cat-
egories:
</bodyText>
<listItem confidence="0.999995">
• HIGH: If κ &gt; (µ0 + σ0)
• MEDIUM: If [κ &gt; (µ0−σ0)nκ &lt; (µ0+σ0)]
• LOW: If κ &lt; (µ0 − σ0)
</listItem>
<bodyText confidence="0.904323714285714">
where µ0 and σ0 are the mean and the standard
deviation values respectively, for a set of confi-
dence scores calculated for a model on a small de-
velopment set of 25 annotated queries from each
language. For the statistical model, we found
µ0 = 4.47, and σ0 = 1.96, for the knowledge
s¨ud- 0.018
</bodyText>
<page confidence="0.720594">
1071
</page>
<table confidence="0.999932416666667">
Language 500 1,000 5,000 10,000
English 78.6% 81.1% 84.3% 85.4%
French 83.4% 85.7% 85.4% 86.6%
Portuguese 81.1% 79.1% 81.7% 81.1%
Spanish 77.4% 79.4% 81.4% 82.3%
Italian 90.6% 89.7% 90.6% 90.0%
German 81.1% 82.3% 83.1% 83.1%
Dutch 86.3% 87.1% 88.3% 87.4%
Danish 86.3% 87.7% 88.0% 88.0%
Finnish 88.3% 88.3% 89.4% 90.3%
Swedish 81.4% 81.4% 81.1% 81.7%
Average 83.5% 84.2% 85.3% 85.6%
</table>
<tableCaption confidence="0.869615">
Table 5: Evaluation of the Decision Tree Classifier
with varying sizes of training data.
</tableCaption>
<bodyText confidence="0.902069826923077">
based µ&apos; = 4.69, and a&apos; = 3.31, and finally for
the morphological model we found µ&apos; = 4.65, and
Q&apos; = 2.25.
Hence, for a given query, we calculate the iden-
tification result of each model together with the
model’s confidence score, and then discretize the
confidence score into one of the three categories
described above. Finally, in order to form an as-
sociation between the output of the model and
its confidence, we create a composite attribute by
appending the discretized confidence to the iden-
tified language. As an example, our statistical
model identifies the query ”the sovereign individ-
ual” as English (en), and reports a n = 7.60,
which is greater than or equal to µ&apos; + a&apos; = 4.47 +
1.96 = 6.43. Therefore the resulting composite
attribute assigned to this query by the statistical
model is ”en-HIGH”.
We used the Weka Machine Learning Toolkit
(Witten and Frank, 2005) to implement our DT
classifier. We trained our system with 500, 1,000,
5,000, and 10,000 instances of the automatically
annotated data and evaluate it on the same test set
of 3500 human-annotated queries. We show the
results in Table 5.
The results in Table 5 show that our DT clas-
sifier, on average, outperforms all the models in
Table 4 for each size of the training data. Fur-
thermore, the performance of the system increases
with the increasing size of training data. In par-
ticular, the improvement that we get for Spanish,
French, and German queries are strikingly good.
This shows that our DT classifier can take ad-
vantage of the complementary features to make
a better classification. The classifier that uses
10,000 instances gets outperformed by the statis-
tical model (by 4.9%) only in the identification of
English queries.
In order to evaluate the significance of our im-
provement, we performed a paired t-test, with a
null hypothesis and a = 0.01 on the outputs of
da de en es fi fr it nl sv pt
da 308 4 9 0 2 3 1 7 14 2
de 7 291 6 2 4 4 5 19 9 3
en 6 8 299 3 3 9 4 5 8 5
es 3 2 4 288 2 2 10 1 1 37
fi 0 5 3 4 316 1 7 4 7 3
fr 2 7 6 3 2 303 10 7 2 8
it 0 1 2 7 4 4 315 2 1 14
nl 5 8 8 4 6 4 4 306 4 1
sv 24 8 6 5 6 2 2 6 286 5
pt 0 1 3 41 1 4 13 2 1 284
</bodyText>
<figureCaption confidence="0.795001">
Figure 2: Confusion Matrix for the Decision Tree
Classifier that uses 10,000 training instances.
</figureCaption>
<bodyText confidence="0.993999928571428">
the statistical model, and the DT classifier that
uses 10,000 training instances. The test resulted
in P = 1.12−10 « a, which strongly indicates
that the improvement of the DT classifier over the
statistical model is statistically significant.
In order to illustrate the errors made by our DT
classifier, we show the confusion matrix M in Fig-
ure 2. The matrix entry Mli�l; simply gives the
number of test instances that are in language li but
misclassified by the system as lj. From the figure,
we can infer that, Portuguese and Spanish are the
languages that are confused mostly by the system.
This is an expected result because of the high sim-
ilarity between the two languages.
</bodyText>
<subsectionHeader confidence="0.993117">
4.3 Towards Identifying the Language Intent
</subsectionHeader>
<bodyText confidence="0.9999856">
As a final step in our study, we build another DT
classifier by introducing a non-linguistic feature
to our system, which is the language information
of the country from which the user entered the
query.5 Our intuition behind introducing this extra
feature is to help the search engine in guessing the
language in which the user wants to see the result-
ing web pages. Since the real purpose of a search
engine is to bring the expected results to its users,
we believe that a correct identification of the lan-
guage that the user intended for the results when
typing the query is an important first part of this
process.
To illustrate this with an example, we con-
sider the query, ”how to tape for plantar fasci-
itis”, which we selected among the 500 human-
annotated queries retrieved from the United States
web site. This query is labelled as Category-2 by
the human annotator. Our DT classifier, together
with the statistical and knowledge-based models,
classifies this query falsely as a Porteguese query,
which is most likely caused due to the presence of
the Latin phrase ”plantar fasciitis”.
In order to test the effectiveness of our new fea-
ture, we introduce all the Category-2 queries to our
</bodyText>
<footnote confidence="0.996845">
5For countries, where the number of official languages is
more than one, we simply pick the first one listed in our table.
</footnote>
<page confidence="0.9422">
1072
</page>
<table confidence="0.999965083333333">
Language New Feat. Classifier-1 Classifier-2
English 74.9% 82.8% 89.5%
French 77.0% 85.6% 93.7%
Portuguese 79.1% 78.1% 93.3%
Spanish 84.1% 80.7% 94.2%
Italian 90.6% 86.7% 96.3%
German 80.2% 80.7% 94.2%
Dutch 91.6% 85.8% 95.3%
Danish 88.6% 87.0% 94.9%
Finnish 94.0% 87.7% 97.9%
Swedish 87.9% 80.9% 95.3%
Average 85.0% 83.6% 94.5%
</table>
<tableCaption confidence="0.922605">
Table 6: Evaluation of the new feature and the two
decision tree classifiers on the new test set.
</tableCaption>
<bodyText confidence="0.999898821428571">
test set and increase its size to 430 queries for each
language.6 Then we run both classifiers, with and
without the new feature, using a training data size
of 10,000 instances, and display the results in Ta-
ble 6. We also show the contribution of the new
feature as a standalone classifier in the first col-
umn of Table 6. We labeled the DT classifier that
we implemented in Section 4.2 as ”Classifier-1”
and the new one as ”Classifier-2”.
Interestingly, the results in Table 6 tell us that a
search engine can achieve a better accuracy than
Classifier-1 on average, should it decide to bring
the results based only on the geographical infor-
mation of its users. However one can argue that
this would be a bad idea for the web sites that re-
ceive a lot of visitors from all over the world, and
also are visited very often. For example, if the
search engine’s United States web site, which is
considered as one of the most important markets
in the world, was to employ such an approach, it’d
only receive 74.9% accuracy by misclassifying the
English queries entered from countries for which
the default language is not English. On the other
hand, when this geographical information is used
as a feature in our decision tree framework, we get
a very high boost on the accuracy of the results
for all the languages. As can be seen in Table 6,
Classifier-2 gives the best results.
</bodyText>
<sectionHeader confidence="0.99775" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.988959396226415">
In this paper, we considered the language identi-
fication problem for search engine queries. First,
we presented a completely automated method to
generate a reliable data set with language anno-
tations that can be used to train a decision tree
classifier. Second, we implemented three features
used in the existing language identification meth-
6We don’t have equal number of Category-2 queries in
each language. For example, English has only 18 of them
whereas Italian has 71. Hence the resulting data set won’t be
balanced in terms of this category.
ods, and compared their performance. Next, we
built a decision tree classifier that improves the re-
sults on average by combining the outputs of the
three models together with their confidence scores.
Finally, we considered the practical application of
this problem for search engines, and built a second
classifier that takes into account the geographical
information of the users.
Human annotations on 5000 automatically an-
notated queries showed that our data generation
method is highly accurate, achieving 84.3% accu-
racy on average for Category-1 queries, and 93.7%
accuracy for Category-1 and Category-2 queries
combined. Furthermore, the process is fast as we
can get a data set of size approximately 50,000
queries in a few hours by using only 15 computers
in a cluster.
The decision tree classifier that we built for the
textual language identification in Section 4.2 out-
performs all three models that we implemented in
Section 4.1, for all the languages except English,
for which the statistical model is better by 4.9%,
and Swedish, for which we get a tie. Introducing
the geographical information feature to our deci-
sion tree framework boosts the accuracy greatly
even in the case of a noisier test set. This sug-
gests that the search engines can do a better job in
presenting the results to their users by taking the
non-linguistic features into account in identifying
the intended language of the queries.
In future, we would like to improve the accu-
racy of our data generation system by considering
additional features proposed in the studies of au-
tomated query taxonomy, and doing a more care-
ful examination in the assignment of the parameter
values. We are also planning to extend the num-
ber of languages in our data set. Furthermore, we
would like to improve the accuracy of Classifier-
2 with additional non-linguistic features. Finally,
we will consider other alternatives to the decision
tree framework when combining the results of the
models with their confidence scores.
</bodyText>
<sectionHeader confidence="0.999348" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999959875">
We are grateful to Romain Vinot, and Rada Mi-
halcea, for their comments on an earlier draft of
this paper. We also would like to thank Sriram
Cherukiri for his contributions during the course
of this project. Finally, many thanks to Murat Bir-
inci, and Sec¸kin Kara, for their help on the data an-
notation process, and Cem S¨ozgen for his remarks
on the SQL formulations.
</bodyText>
<page confidence="0.992207">
1073
</page>
<sectionHeader confidence="0.993863" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999946245614036">
C. Biemann and S. Teresniak. 2005. Disentangling
from babylonian confusion - unsupervised language
identification. In Proceedings of CICLing-2005,
Computational Linguistics and Intelligent Text Pro-
cessing, pages 762–773. Springer.
Andrei Broder. 2002. A taxonomy of web search. SI-
GIR Forum, 36(2):3–10.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Proceedings of
SDAIR-94, 3rd Annual Symposium on Document
Analysis and Information Retrieval, pages 161–175,
Las Vegas, US.
J. Cowie, Y. Ludovic, and R. Zacharski. 1999. Lan-
guage recognition for mono- and multi-lingual docu-
ments. In Proceedings of Vextal Conference, Venice,
Italy.
Ted Dunning. 1994. Statistical identification of lan-
guage. Technical Report MCCS-94-273, Comput-
ing Research Lab (CRL), New Mexico State Uni-
versity.
Gregory Grefenstette. 1995. Comparing two language
identification schemes. In Proceedings of JADT-95,
3rd International Conference on the Statistical Anal-
ysis of Textual Data, Rome, Italy.
Harald Hammarstr¨om. 2006. A naive theory of affix-
ation and an algorithm for extraction. In Proceed-
ings of the Eighth Meeting of the ACL Special Inter-
est Group on Computational Phonology and Mor-
phology at HLT-NAACL 2006, pages 79–88, New
York City, USA, June. Association for Computa-
tional Linguistics.
Harald Hammarstr¨om. 2007. A fine-grained model for
language identification. In F. Lazarinis, J. Vilares,
J. Tait (eds) Improving Non-English Web Searching
(iNEWS07) SIGIR07 Workshop, pages 14–20.
B. Hughes, T. Baldwin, S. G. Bird, J. Nicholson, and
A. Mackinlay. 2006. Reconsidering language iden-
tification for written language resources. In 5th In-
ternational Conference on Language Resources and
Evaluation (LREC2006), Genoa, Italy.
Norman C Ingle. 1976. A language identification ta-
ble. The Incorporated Linguist, 15(4):98–101.
Bernard J. Jansen, Danielle L. Booth, and Amanda
Spink. 2008. Determining the informational, navi-
gational, and transactional intent of web queries. Inf.
Process. Manage., 44(3):1251–1266.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings of
the 10th Machine Translation Summit, Phuket, Thai-
land, pages 79–86.
Canasai Kruengkrai, Prapass Srichaivattana, Virach
Sornlertlamvanich, and Hitoshi Isahara. 2005. Lan-
guage identification based on string kernels. In
In Proceedings of the 5th International Symposium
on Communications and Information Technologies
(ISCIT-2005, pages 896–899.
Uichin Lee, Zhenyu Liu, and Junghoo Cho. 2005. Au-
tomatic identification of user goals in web search.
In WWW ’05: Proceedings of the 14th international
conference on World Wide Web, pages 391–400,
New York, NY, USA. ACM.
Ernesto William De Luca Lena Grothe and Andreas
Nrnberger. 2008. A comparative study on lan-
guage identification methods. In Proceedings of the
Sixth International Language Resources and Eval-
uation (LREC’08), Marrakech, Morocco, May. Eu-
ropean Language Resources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.
Ryan Levering and Michal Cutler. 2006. The portrait
of a common html web page. In DocEng ’06: Pro-
ceedings of the 2006 ACM symposium on Document
engineering, pages 198–204, New York, NY, USA.
ACM Press.
Yiqun Liu, Min Zhang, Liyun Ru, and Shaoping Ma.
2006. Automatic query type identification based on
click through information. In AIRS, pages 593–600.
Arjen Poutsma. 2001. Applying monte carlo tech-
niques to language identification. In In Proceed-
ings of Computational Linguistics in the Nether-
lands (CLIN).
John M. Prager. 1999. Linguini: Language identifi-
cation for multilingual documents. In HICSS ’99:
Proceedings of the Thirty-Second Annual Hawaii In-
ternational Conference on System Sciences-Volume
2, page 2035, Washington, DC, USA. IEEE Com-
puter Society.
Daniel E. Rose and Danny Levinson. 2004. Under-
standing user goals in web search. In WWW ’04:
Proceedings of the 13th international conference on
World Wide Web, pages 13–19, New York, NY, USA.
ACM.
Penelope Sibun and Jeffrey C. Reynar. 1996. Lan-
guage identification: Examining the issues. In
5th Symposium on Document Analysis and Informa-
tion Retrieval, pages 125–135, Las Vegas, Nevada,
U.S.A.
Craig Silverstein, Hannes Marais, Monika Henzinger,
and Michael Moricz. 1999. Analysis of a very
large web search engine query log. SIGIR Forum,
33(1):6–12.
C. Souter, G. Churcher, J. Hayes, and J. Hughes. 1994.
Natural language identification using corpus-based
models. Hermes Journal of Linguistics, 13:183–
203.
Andreas Stolcke. 2002. Srilm – an extensible language
modeling toolkit. In Proc. Intl. Conf. on Spoken
Language Processing, volume 2, pages 901–904,
Denver, CO.
Hidayet Takci and Ibrahim Sogukpinar. 2004.
Centroid-based language identification using letter
feature set. In CICLing, pages 640–648.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, 2 edition.
</reference>
<page confidence="0.99668">
1074
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.972641">
<title confidence="0.99998">Language Identification of Search Engine Queries</title>
<author confidence="0.996914">Hakan Ceylan</author>
<affiliation confidence="0.9995025">Department of Computer Science University of North Texas</affiliation>
<address confidence="0.999109">Denton, TX, 76203</address>
<email confidence="0.99952">hakan@unt.edu</email>
<author confidence="0.985699">Yookyung Kim</author>
<affiliation confidence="0.999514">Yahoo! Inc.</affiliation>
<address confidence="0.9992325">2821 Mission College Blvd. Santa Clara, CA, 95054</address>
<email confidence="0.999679">ykim@yahoo-inc.com</email>
<abstract confidence="0.999752739130435">We consider the language identification problem for search engine queries. First, we propose a method to automatically generate a data set, which uses clickthrough logs of the Yahoo! Search Engine to derive the language of a query indirectly from the language of the documents clicked by the users. Next, we use this data set to train two decision tree classifiers; one that only uses linguistic features and is aimed for textual language identification, and one that additionally uses a non-linguistic feature, and is geared towards the identification of the language intended by the users of the search engine. Our results show that our method produces a highly reliable data set very efficiently, and our decision tree classifier outperforms some of the best methods that have been proposed for the task of written language identification on the domain of search engine queries.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Biemann</author>
<author>S Teresniak</author>
</authors>
<title>Disentangling from babylonian confusion - unsupervised language identification.</title>
<date>2005</date>
<booktitle>In Proceedings of CICLing-2005, Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>762--773</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="7798" citStr="Biemann and Teresniak, 2005" startWordPosition="1295" endWordPosition="1298">n study. The work most closely related to ours is presented very recently in (Hammarstr¨om, 2007), which proposes a model that uses a frequency dictionary together with affix information in order to identify the language of texts as short as one word. Other systems that use methods aside from the ones discussed above have also been proposed. (Takci and Sogukpinar, 2004) used letter frequency features in a centroid based classification model. (Kruengkrai et al., 2005) proposed a feature based on alignment of string kernels using suffix trees, and used it in two different classifiers. Finally, (Biemann and Teresniak, 2005) presented an unsupervised system that clusters the words based on sentence co-occurence. Recently, (Hughes et al., 2006) surveyed the previous work in this area and suggested that the problem of language identification for written resources, although well studied, has too many open challenges which requires a more systematic and collaborative study. 3 Data Generation We start the construction of our data set by retrieving the queries, together with the clicked urls, from the Yahoo! Search Engine for a three months time period. For each language desired in our data set, we retrieve the queries</context>
</contexts>
<marker>Biemann, Teresniak, 2005</marker>
<rawString>C. Biemann and S. Teresniak. 2005. Disentangling from babylonian confusion - unsupervised language identification. In Proceedings of CICLing-2005, Computational Linguistics and Intelligent Text Processing, pages 762–773. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Broder</author>
</authors>
<title>A taxonomy of web search.</title>
<date>2002</date>
<journal>SIGIR Forum,</journal>
<volume>36</volume>
<issue>2</issue>
<contexts>
<context position="12703" citStr="Broder, 2002" startWordPosition="2196" endWordPosition="2197">o the high frequency of the queries together with too few distinct urls. This case suggests that the query is most likely navigational. The intent of navigational queries, such as ”ACL 2009”, is to find a particular web site. Therefore they usually consist of proper names, or acronyms that would not be of much use to our language identification problem. Hence we would like to get rid of the navigational queries in our data set by using some of the features proposed for the task of automatic taxonomy of search engine queries. For a more detailed discussion of this task, we refer the reader to (Broder, 2002; Rose and Levinson, 2004; Lee et al., 2005; Liu et al., 2006; Jansen et al., 2008). Two of the features used in (Liu et al., 2006) in identification of the navigational queries from click-through data, are the number of Clicks Satisfied (nCS) and number of Results Satisfied (nRS). In our problem, we substitute nCS with Fq, the total click frequency of the query q, and nRS with 1068 UQ, the number of distinct urls clicked for q. Thus we eliminate the queries that have a total click frequency above a given frequency threshold F, and, that have less than a given distinct number of urls, U. Thus,</context>
</contexts>
<marker>Broder, 2002</marker>
<rawString>Andrei Broder. 2002. A taxonomy of web search. SIGIR Forum, 36(2):3–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Cavnar</author>
<author>John M Trenkle</author>
</authors>
<title>Ngram-based text categorization.</title>
<date>1994</date>
<booktitle>In Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval,</booktitle>
<pages>161--175</pages>
<location>Las Vegas, US.</location>
<contexts>
<context position="6079" citStr="Cavnar and Trenkle, 1994" startWordPosition="1020" endWordPosition="1023">l., 1994; Cowie et al., 1999). For classification, the word-based approaches sort the list of words in descending order of their frequency in the corpus from which they are extracted. Then the likelihood of each word in a given text can be calculated by using rank-order statistics or by transforming the frequencies into probabilities. The n-gram based approaches are based on the counts of character or byte n-grams, which are sequences of n characters or bytes, extracted from a corpus for each reference language. Different classification models that use the n-gram features have been proposed. (Cavnar and Trenkle, 1994) used an out-of-place rank order statistic to measure the distance of a given text to the n-gram profile of each language. (Dunning, 1994) proposed a system that uses Markov Chains of byte ngrams with Bayesian Decision Rules to minimize the probability error. (Grefenstette, 1995) simply used trigram counts that are transformed into probabilities, and found this superior to the short words technique. (Sibun and Reynar, 1996) used Relative Entropy by first generating n-gram probability distributions for both training and test data, and then measuring the distance between the two probability dist</context>
<context position="20391" citStr="Cavnar and Trenkle, 1994" startWordPosition="3546" endWordPosition="3549">er of words or characters. 4.1 Models for Language Identification We implement a statistical model using a character based n-gram feature. For each language, we collect the n-gram counts (for n = 1 to n = 7 also using the word beginning and ending spaces) from the vocabulary of the training corpus, and then generate a probability distribution from these counts. We implemented this model using the SRILM Toolkit (Stolcke, 2002) with the modified Kneser-Ney Discounting and interpolation options. For comparison purposes, we also implemented the Rank-Order method using the parameters described in (Cavnar and Trenkle, 1994). For the knowledge based method, we used the vocabulary of each language obtained from the training corpora, together with the word counts. From these counts, we obtained a probability distribution for all the words in our vocabulary. In other words, this time we used a word-based ngram method, only with n = 1. It should be noted that increasing the size of n, which might help in language identification of other types of written texts, will not be helpful in this task due to the unique nature of the search engine queries. For the morphological feature; we gathered the affix information for ea</context>
</contexts>
<marker>Cavnar, Trenkle, 1994</marker>
<rawString>William B. Cavnar and John M. Trenkle. 1994. Ngram-based text categorization. In Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval, pages 161–175, Las Vegas, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cowie</author>
<author>Y Ludovic</author>
<author>R Zacharski</author>
</authors>
<title>Language recognition for mono- and multi-lingual documents.</title>
<date>1999</date>
<booktitle>In Proceedings of Vextal Conference,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="5483" citStr="Cowie et al., 1999" startWordPosition="926" endWordPosition="929">t of words or n-gram models for each reference language. The word based approaches use a list of short words, common words, or a complete vocabulary which are extracted from a corpus for each language. The short words approach uses a list of words with at most four or five characters; such as determiners, prepositions, and conjunctions, and is used in (Ingle, 1976; Grefenstette, 1995). The common words method is a generalization over the short words one which, in addition, includes other frequently occuring words without limiting them to a specific length, and is used in (Souter et al., 1994; Cowie et al., 1999). For classification, the word-based approaches sort the list of words in descending order of their frequency in the corpus from which they are extracted. Then the likelihood of each word in a given text can be calculated by using rank-order statistics or by transforming the frequencies into probabilities. The n-gram based approaches are based on the counts of character or byte n-grams, which are sequences of n characters or bytes, extracted from a corpus for each reference language. Different classification models that use the n-gram features have been proposed. (Cavnar and Trenkle, 1994) use</context>
</contexts>
<marker>Cowie, Ludovic, Zacharski, 1999</marker>
<rawString>J. Cowie, Y. Ludovic, and R. Zacharski. 1999. Language recognition for mono- and multi-lingual documents. In Proceedings of Vextal Conference, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Statistical identification of language.</title>
<date>1994</date>
<tech>Technical Report MCCS-94-273,</tech>
<institution>Computing Research Lab (CRL), New Mexico State University.</institution>
<contexts>
<context position="6217" citStr="Dunning, 1994" startWordPosition="1046" endWordPosition="1047">pus from which they are extracted. Then the likelihood of each word in a given text can be calculated by using rank-order statistics or by transforming the frequencies into probabilities. The n-gram based approaches are based on the counts of character or byte n-grams, which are sequences of n characters or bytes, extracted from a corpus for each reference language. Different classification models that use the n-gram features have been proposed. (Cavnar and Trenkle, 1994) used an out-of-place rank order statistic to measure the distance of a given text to the n-gram profile of each language. (Dunning, 1994) proposed a system that uses Markov Chains of byte ngrams with Bayesian Decision Rules to minimize the probability error. (Grefenstette, 1995) simply used trigram counts that are transformed into probabilities, and found this superior to the short words technique. (Sibun and Reynar, 1996) used Relative Entropy by first generating n-gram probability distributions for both training and test data, and then measuring the distance between the two probability distributions by using the Kullback-Liebler Distance. (Poutsma, 2001) developed a system based on Monte Carlo Sampling. Linguini, a system pro</context>
</contexts>
<marker>Dunning, 1994</marker>
<rawString>Ted Dunning. 1994. Statistical identification of language. Technical Report MCCS-94-273, Computing Research Lab (CRL), New Mexico State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Comparing two language identification schemes.</title>
<date>1995</date>
<booktitle>In Proceedings of JADT-95, 3rd International Conference on the Statistical Analysis of Textual Data,</booktitle>
<location>Rome, Italy.</location>
<contexts>
<context position="5251" citStr="Grefenstette, 1995" startWordPosition="889" endWordPosition="890"> we conclude our study and discuss the future directions for the problem. 2 Related Work Most of the work carried out to date on the written language identification problem consists of supervised approaches that are trained on a list of words or n-gram models for each reference language. The word based approaches use a list of short words, common words, or a complete vocabulary which are extracted from a corpus for each language. The short words approach uses a list of words with at most four or five characters; such as determiners, prepositions, and conjunctions, and is used in (Ingle, 1976; Grefenstette, 1995). The common words method is a generalization over the short words one which, in addition, includes other frequently occuring words without limiting them to a specific length, and is used in (Souter et al., 1994; Cowie et al., 1999). For classification, the word-based approaches sort the list of words in descending order of their frequency in the corpus from which they are extracted. Then the likelihood of each word in a given text can be calculated by using rank-order statistics or by transforming the frequencies into probabilities. The n-gram based approaches are based on the counts of chara</context>
</contexts>
<marker>Grefenstette, 1995</marker>
<rawString>Gregory Grefenstette. 1995. Comparing two language identification schemes. In Proceedings of JADT-95, 3rd International Conference on the Statistical Analysis of Textual Data, Rome, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Hammarstr¨om</author>
</authors>
<title>A naive theory of affixation and an algorithm for extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology and Morphology at HLT-NAACL</booktitle>
<pages>79--88</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<marker>Hammarstr¨om, 2006</marker>
<rawString>Harald Hammarstr¨om. 2006. A naive theory of affixation and an algorithm for extraction. In Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology and Morphology at HLT-NAACL 2006, pages 79–88, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Hammarstr¨om</author>
</authors>
<title>A fine-grained model for language identification. In</title>
<date>2007</date>
<pages>14--20</pages>
<marker>Hammarstr¨om, 2007</marker>
<rawString>Harald Hammarstr¨om. 2007. A fine-grained model for language identification. In F. Lazarinis, J. Vilares, J. Tait (eds) Improving Non-English Web Searching (iNEWS07) SIGIR07 Workshop, pages 14–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Hughes</author>
<author>T Baldwin</author>
<author>S G Bird</author>
<author>J Nicholson</author>
<author>A Mackinlay</author>
</authors>
<title>Reconsidering language identification for written language resources.</title>
<date>2006</date>
<booktitle>In 5th International Conference on Language Resources and Evaluation (LREC2006),</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="7919" citStr="Hughes et al., 2006" startWordPosition="1312" endWordPosition="1315">es a frequency dictionary together with affix information in order to identify the language of texts as short as one word. Other systems that use methods aside from the ones discussed above have also been proposed. (Takci and Sogukpinar, 2004) used letter frequency features in a centroid based classification model. (Kruengkrai et al., 2005) proposed a feature based on alignment of string kernels using suffix trees, and used it in two different classifiers. Finally, (Biemann and Teresniak, 2005) presented an unsupervised system that clusters the words based on sentence co-occurence. Recently, (Hughes et al., 2006) surveyed the previous work in this area and suggested that the problem of language identification for written resources, although well studied, has too many open challenges which requires a more systematic and collaborative study. 3 Data Generation We start the construction of our data set by retrieving the queries, together with the clicked urls, from the Yahoo! Search Engine for a three months time period. For each language desired in our data set, we retrieve the queries from the corresponding 1067 Yahoo! web site in which the default language is the same as the one sought.1 Then we prepro</context>
</contexts>
<marker>Hughes, Baldwin, Bird, Nicholson, Mackinlay, 2006</marker>
<rawString>B. Hughes, T. Baldwin, S. G. Bird, J. Nicholson, and A. Mackinlay. 2006. Reconsidering language identification for written language resources. In 5th International Conference on Language Resources and Evaluation (LREC2006), Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norman C Ingle</author>
</authors>
<title>A language identification table. The Incorporated Linguist,</title>
<date>1976</date>
<contexts>
<context position="5230" citStr="Ingle, 1976" startWordPosition="887" endWordPosition="888">ask. Finally, we conclude our study and discuss the future directions for the problem. 2 Related Work Most of the work carried out to date on the written language identification problem consists of supervised approaches that are trained on a list of words or n-gram models for each reference language. The word based approaches use a list of short words, common words, or a complete vocabulary which are extracted from a corpus for each language. The short words approach uses a list of words with at most four or five characters; such as determiners, prepositions, and conjunctions, and is used in (Ingle, 1976; Grefenstette, 1995). The common words method is a generalization over the short words one which, in addition, includes other frequently occuring words without limiting them to a specific length, and is used in (Souter et al., 1994; Cowie et al., 1999). For classification, the word-based approaches sort the list of words in descending order of their frequency in the corpus from which they are extracted. Then the likelihood of each word in a given text can be calculated by using rank-order statistics or by transforming the frequencies into probabilities. The n-gram based approaches are based o</context>
</contexts>
<marker>Ingle, 1976</marker>
<rawString>Norman C Ingle. 1976. A language identification table. The Incorporated Linguist, 15(4):98–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard J Jansen</author>
<author>Danielle L Booth</author>
<author>Amanda Spink</author>
</authors>
<title>Determining the informational, navigational, and transactional intent of web queries.</title>
<date>2008</date>
<journal>Inf. Process. Manage.,</journal>
<volume>44</volume>
<issue>3</issue>
<contexts>
<context position="12786" citStr="Jansen et al., 2008" startWordPosition="2210" endWordPosition="2213">is case suggests that the query is most likely navigational. The intent of navigational queries, such as ”ACL 2009”, is to find a particular web site. Therefore they usually consist of proper names, or acronyms that would not be of much use to our language identification problem. Hence we would like to get rid of the navigational queries in our data set by using some of the features proposed for the task of automatic taxonomy of search engine queries. For a more detailed discussion of this task, we refer the reader to (Broder, 2002; Rose and Levinson, 2004; Lee et al., 2005; Liu et al., 2006; Jansen et al., 2008). Two of the features used in (Liu et al., 2006) in identification of the navigational queries from click-through data, are the number of Clicks Satisfied (nCS) and number of Results Satisfied (nRS). In our problem, we substitute nCS with Fq, the total click frequency of the query q, and nRS with 1068 UQ, the number of distinct urls clicked for q. Thus we eliminate the queries that have a total click frequency above a given frequency threshold F, and, that have less than a given distinct number of urls, U. Thus, we have three parameters that help us in eliminating the noise from the inital dat</context>
</contexts>
<marker>Jansen, Booth, Spink, 2008</marker>
<rawString>Bernard J. Jansen, Danielle L. Booth, and Amanda Spink. 2008. Determining the informational, navigational, and transactional intent of web queries. Inf. Process. Manage., 44(3):1251–1266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Machine Translation</booktitle>
<pages>79--86</pages>
<location>Summit, Phuket, Thailand,</location>
<contexts>
<context position="18528" citStr="Koehn, 2005" startWordPosition="3229" endWordPosition="3230">rms such as ”download”, ”software”, ”flash player”, among other native language query terms. 4 Language Identification We start this section with the implementation of three models each of which use a different existing feature. We categorize these models as statistical, knowledge based, and morphological. We then combine all three models in a machine learning framework using a novel approach. Finally, we extend this framework by adding a non-linguistic feature in order to identify the language intended by the search engine user. To train each model implemented, we used the EuroParl Corpora, (Koehn, 2005), and the same 10 languages in Section 3. EuroParl Corpora is well balanced, so we would not have any bias towards a particular language resulting from our choice of the corpora. We tested all the systems in this section on a test set of 3500 human annotated queries, which is formed by taking 350 Category-1 queries from each language. All the queries in the test set are obtained from the evaluation results in Section 3. In Table 2, we give the properties of this test set. We list the minimum, maximum, and average number of characters and words (MinC, MaxC, µC, MinW, MaxW, and µW respectively).</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the 10th Machine Translation Summit, Phuket, Thailand, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Canasai Kruengkrai</author>
<author>Prapass Srichaivattana</author>
<author>Virach Sornlertlamvanich</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Language identification based on string kernels. In</title>
<date>2005</date>
<booktitle>In Proceedings of the 5th International Symposium on Communications and Information Technologies (ISCIT-2005,</booktitle>
<pages>896--899</pages>
<contexts>
<context position="7641" citStr="Kruengkrai et al., 2005" startWordPosition="1269" endWordPosition="1272">e. Similarly, (Lena Grothe and Nrnberger, 2008) combines both models using the ad-hoc method of (Cavnar and Trenkle, 1994), and also presents a comparison study. The work most closely related to ours is presented very recently in (Hammarstr¨om, 2007), which proposes a model that uses a frequency dictionary together with affix information in order to identify the language of texts as short as one word. Other systems that use methods aside from the ones discussed above have also been proposed. (Takci and Sogukpinar, 2004) used letter frequency features in a centroid based classification model. (Kruengkrai et al., 2005) proposed a feature based on alignment of string kernels using suffix trees, and used it in two different classifiers. Finally, (Biemann and Teresniak, 2005) presented an unsupervised system that clusters the words based on sentence co-occurence. Recently, (Hughes et al., 2006) surveyed the previous work in this area and suggested that the problem of language identification for written resources, although well studied, has too many open challenges which requires a more systematic and collaborative study. 3 Data Generation We start the construction of our data set by retrieving the queries, tog</context>
</contexts>
<marker>Kruengkrai, Srichaivattana, Sornlertlamvanich, Isahara, 2005</marker>
<rawString>Canasai Kruengkrai, Prapass Srichaivattana, Virach Sornlertlamvanich, and Hitoshi Isahara. 2005. Language identification based on string kernels. In In Proceedings of the 5th International Symposium on Communications and Information Technologies (ISCIT-2005, pages 896–899.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uichin Lee</author>
<author>Zhenyu Liu</author>
<author>Junghoo Cho</author>
</authors>
<title>Automatic identification of user goals in web search.</title>
<date>2005</date>
<booktitle>In WWW ’05: Proceedings of the 14th international conference on World Wide Web,</booktitle>
<pages>391--400</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="12746" citStr="Lee et al., 2005" startWordPosition="2202" endWordPosition="2205">ether with too few distinct urls. This case suggests that the query is most likely navigational. The intent of navigational queries, such as ”ACL 2009”, is to find a particular web site. Therefore they usually consist of proper names, or acronyms that would not be of much use to our language identification problem. Hence we would like to get rid of the navigational queries in our data set by using some of the features proposed for the task of automatic taxonomy of search engine queries. For a more detailed discussion of this task, we refer the reader to (Broder, 2002; Rose and Levinson, 2004; Lee et al., 2005; Liu et al., 2006; Jansen et al., 2008). Two of the features used in (Liu et al., 2006) in identification of the navigational queries from click-through data, are the number of Clicks Satisfied (nCS) and number of Results Satisfied (nRS). In our problem, we substitute nCS with Fq, the total click frequency of the query q, and nRS with 1068 UQ, the number of distinct urls clicked for q. Thus we eliminate the queries that have a total click frequency above a given frequency threshold F, and, that have less than a given distinct number of urls, U. Thus, we have three parameters that help us in e</context>
</contexts>
<marker>Lee, Liu, Cho, 2005</marker>
<rawString>Uichin Lee, Zhenyu Liu, and Junghoo Cho. 2005. Automatic identification of user goals in web search. In WWW ’05: Proceedings of the 14th international conference on World Wide Web, pages 391–400, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ernesto William De Luca Lena Grothe</author>
<author>Andreas Nrnberger</author>
</authors>
<title>A comparative study on language identification methods.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08),</booktitle>
<location>Marrakech, Morocco,</location>
<contexts>
<context position="7064" citStr="Grothe and Nrnberger, 2008" startWordPosition="1175" endWordPosition="1178"> this superior to the short words technique. (Sibun and Reynar, 1996) used Relative Entropy by first generating n-gram probability distributions for both training and test data, and then measuring the distance between the two probability distributions by using the Kullback-Liebler Distance. (Poutsma, 2001) developed a system based on Monte Carlo Sampling. Linguini, a system proposed by (Prager, 1999), combines the word-based and n-gram models using a vector-space based model and examines the effectiveness of the combined model and the individual features on varying text size. Similarly, (Lena Grothe and Nrnberger, 2008) combines both models using the ad-hoc method of (Cavnar and Trenkle, 1994), and also presents a comparison study. The work most closely related to ours is presented very recently in (Hammarstr¨om, 2007), which proposes a model that uses a frequency dictionary together with affix information in order to identify the language of texts as short as one word. Other systems that use methods aside from the ones discussed above have also been proposed. (Takci and Sogukpinar, 2004) used letter frequency features in a centroid based classification model. (Kruengkrai et al., 2005) proposed a feature bas</context>
</contexts>
<marker>Grothe, Nrnberger, 2008</marker>
<rawString>Ernesto William De Luca Lena Grothe and Andreas Nrnberger. 2008. A comparative study on language identification methods. In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), Marrakech, Morocco, May. European Language Resources Association (ELRA). http://www.lrec-conf.org/proceedings/lrec2008/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Levering</author>
<author>Michal Cutler</author>
</authors>
<title>The portrait of a common html web page.</title>
<date>2006</date>
<booktitle>In DocEng ’06: Proceedings of the 2006 ACM symposium on Document engineering,</booktitle>
<pages>198--204</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="9060" citStr="Levering and Cutler, 2006" startWordPosition="1504" endWordPosition="1507">web site in which the default language is the same as the one sought.1 Then we preprocess the queries by getting rid of the ones that have any numbers or special characters in them, removing extra spaces between query terms, and lowercasing all the letters of the queries2. Next, we aggregate the queries that are exactly the same, by calculating the frequencies of the urls clicked for each query. As we pointed out in Section 1, and illustrated in Figure 1, the language identification methods give almost perfect accuracy when the text has 100 or more characters. Furthermore, it is suggested in (Levering and Cutler, 2006) that the average textual content in a web page is 474 words. Thus we assume that it is a fairly trivial task to identify the language for an average web page using one of the existing methods.3 In our case, this task gets already accomplished by the crawler for all the web pages crawled by the search engine. Thus we can summarize our information in two separate tables; T1 and T2. For Table T1, we have a set of queries Q, and each q E Q maps to a set of url-frequency pairs. Each mapping is of the form (q, u, fu), where u is a url clicked for q, and fu is the frequency of u. Table T2, on the ot</context>
</contexts>
<marker>Levering, Cutler, 2006</marker>
<rawString>Ryan Levering and Michal Cutler. 2006. The portrait of a common html web page. In DocEng ’06: Proceedings of the 2006 ACM symposium on Document engineering, pages 198–204, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiqun Liu</author>
<author>Min Zhang</author>
<author>Liyun Ru</author>
<author>Shaoping Ma</author>
</authors>
<title>Automatic query type identification based on click through information.</title>
<date>2006</date>
<booktitle>In AIRS,</booktitle>
<pages>593--600</pages>
<contexts>
<context position="12764" citStr="Liu et al., 2006" startWordPosition="2206" endWordPosition="2209"> distinct urls. This case suggests that the query is most likely navigational. The intent of navigational queries, such as ”ACL 2009”, is to find a particular web site. Therefore they usually consist of proper names, or acronyms that would not be of much use to our language identification problem. Hence we would like to get rid of the navigational queries in our data set by using some of the features proposed for the task of automatic taxonomy of search engine queries. For a more detailed discussion of this task, we refer the reader to (Broder, 2002; Rose and Levinson, 2004; Lee et al., 2005; Liu et al., 2006; Jansen et al., 2008). Two of the features used in (Liu et al., 2006) in identification of the navigational queries from click-through data, are the number of Clicks Satisfied (nCS) and number of Results Satisfied (nRS). In our problem, we substitute nCS with Fq, the total click frequency of the query q, and nRS with 1068 UQ, the number of distinct urls clicked for q. Thus we eliminate the queries that have a total click frequency above a given frequency threshold F, and, that have less than a given distinct number of urls, U. Thus, we have three parameters that help us in eliminating the noi</context>
</contexts>
<marker>Liu, Zhang, Ru, Ma, 2006</marker>
<rawString>Yiqun Liu, Min Zhang, Liyun Ru, and Shaoping Ma. 2006. Automatic query type identification based on click through information. In AIRS, pages 593–600.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjen Poutsma</author>
</authors>
<title>Applying monte carlo techniques to language identification. In</title>
<date>2001</date>
<booktitle>In Proceedings of Computational Linguistics in the Netherlands (CLIN).</booktitle>
<contexts>
<context position="2035" citStr="Poutsma, 2001" startWordPosition="334" endWordPosition="335">elatively long texts such as articles or web pages which usually contain enough text for the systems built for this task to reach almost perfect accuracy. Figure 1 shows the performance of 6 different language identification methods on written texts of 10 European languages that use the Roman Alphabet. It can be seen that the methods reach a very high accuracy when the text has 100 or more characters. However, search engine queries are very short in length; they have about 2 to 3 words on average, Figure 1: Performance of six Language Identification methods on varying text size. Adapted from (Poutsma, 2001). which requires a reconsideration of the existing methods built for this problem. Correct identification of the language of the queries is of critical importance to search engines. Major search engines such as Yahoo! Search (www.yahoo.com), or Google (www.google.com) crawl billions of web pages in more than 50 languages, and about a quarter of their queries are in languages other than English. Therefore a correct identification of the language of a query is needed in order to aid the search engine towards more accurate results. Moreover, it also helps further processing of the queries, such a</context>
<context position="6744" citStr="Poutsma, 2001" startWordPosition="1127" endWordPosition="1128">re the distance of a given text to the n-gram profile of each language. (Dunning, 1994) proposed a system that uses Markov Chains of byte ngrams with Bayesian Decision Rules to minimize the probability error. (Grefenstette, 1995) simply used trigram counts that are transformed into probabilities, and found this superior to the short words technique. (Sibun and Reynar, 1996) used Relative Entropy by first generating n-gram probability distributions for both training and test data, and then measuring the distance between the two probability distributions by using the Kullback-Liebler Distance. (Poutsma, 2001) developed a system based on Monte Carlo Sampling. Linguini, a system proposed by (Prager, 1999), combines the word-based and n-gram models using a vector-space based model and examines the effectiveness of the combined model and the individual features on varying text size. Similarly, (Lena Grothe and Nrnberger, 2008) combines both models using the ad-hoc method of (Cavnar and Trenkle, 1994), and also presents a comparison study. The work most closely related to ours is presented very recently in (Hammarstr¨om, 2007), which proposes a model that uses a frequency dictionary together with affix</context>
</contexts>
<marker>Poutsma, 2001</marker>
<rawString>Arjen Poutsma. 2001. Applying monte carlo techniques to language identification. In In Proceedings of Computational Linguistics in the Netherlands (CLIN).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Prager</author>
</authors>
<title>Linguini: Language identification for multilingual documents.</title>
<date>1999</date>
<booktitle>In HICSS ’99: Proceedings of the Thirty-Second Annual Hawaii International Conference on System Sciences-Volume 2,</booktitle>
<pages>page</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="6840" citStr="Prager, 1999" startWordPosition="1142" endWordPosition="1143">a system that uses Markov Chains of byte ngrams with Bayesian Decision Rules to minimize the probability error. (Grefenstette, 1995) simply used trigram counts that are transformed into probabilities, and found this superior to the short words technique. (Sibun and Reynar, 1996) used Relative Entropy by first generating n-gram probability distributions for both training and test data, and then measuring the distance between the two probability distributions by using the Kullback-Liebler Distance. (Poutsma, 2001) developed a system based on Monte Carlo Sampling. Linguini, a system proposed by (Prager, 1999), combines the word-based and n-gram models using a vector-space based model and examines the effectiveness of the combined model and the individual features on varying text size. Similarly, (Lena Grothe and Nrnberger, 2008) combines both models using the ad-hoc method of (Cavnar and Trenkle, 1994), and also presents a comparison study. The work most closely related to ours is presented very recently in (Hammarstr¨om, 2007), which proposes a model that uses a frequency dictionary together with affix information in order to identify the language of texts as short as one word. Other systems that</context>
</contexts>
<marker>Prager, 1999</marker>
<rawString>John M. Prager. 1999. Linguini: Language identification for multilingual documents. In HICSS ’99: Proceedings of the Thirty-Second Annual Hawaii International Conference on System Sciences-Volume 2, page 2035, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel E Rose</author>
<author>Danny Levinson</author>
</authors>
<title>Understanding user goals in web search.</title>
<date>2004</date>
<booktitle>In WWW ’04: Proceedings of the 13th international conference on World Wide Web,</booktitle>
<pages>13--19</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="12728" citStr="Rose and Levinson, 2004" startWordPosition="2198" endWordPosition="2201">quency of the queries together with too few distinct urls. This case suggests that the query is most likely navigational. The intent of navigational queries, such as ”ACL 2009”, is to find a particular web site. Therefore they usually consist of proper names, or acronyms that would not be of much use to our language identification problem. Hence we would like to get rid of the navigational queries in our data set by using some of the features proposed for the task of automatic taxonomy of search engine queries. For a more detailed discussion of this task, we refer the reader to (Broder, 2002; Rose and Levinson, 2004; Lee et al., 2005; Liu et al., 2006; Jansen et al., 2008). Two of the features used in (Liu et al., 2006) in identification of the navigational queries from click-through data, are the number of Clicks Satisfied (nCS) and number of Results Satisfied (nRS). In our problem, we substitute nCS with Fq, the total click frequency of the query q, and nRS with 1068 UQ, the number of distinct urls clicked for q. Thus we eliminate the queries that have a total click frequency above a given frequency threshold F, and, that have less than a given distinct number of urls, U. Thus, we have three parameters</context>
</contexts>
<marker>Rose, Levinson, 2004</marker>
<rawString>Daniel E. Rose and Danny Levinson. 2004. Understanding user goals in web search. In WWW ’04: Proceedings of the 13th international conference on World Wide Web, pages 13–19, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Penelope Sibun</author>
<author>Jeffrey C Reynar</author>
</authors>
<title>Language identification: Examining the issues.</title>
<date>1996</date>
<booktitle>In 5th Symposium on Document Analysis and Information Retrieval,</booktitle>
<pages>125--135</pages>
<location>Las Vegas, Nevada, U.S.A.</location>
<contexts>
<context position="6506" citStr="Sibun and Reynar, 1996" startWordPosition="1090" endWordPosition="1093">sequences of n characters or bytes, extracted from a corpus for each reference language. Different classification models that use the n-gram features have been proposed. (Cavnar and Trenkle, 1994) used an out-of-place rank order statistic to measure the distance of a given text to the n-gram profile of each language. (Dunning, 1994) proposed a system that uses Markov Chains of byte ngrams with Bayesian Decision Rules to minimize the probability error. (Grefenstette, 1995) simply used trigram counts that are transformed into probabilities, and found this superior to the short words technique. (Sibun and Reynar, 1996) used Relative Entropy by first generating n-gram probability distributions for both training and test data, and then measuring the distance between the two probability distributions by using the Kullback-Liebler Distance. (Poutsma, 2001) developed a system based on Monte Carlo Sampling. Linguini, a system proposed by (Prager, 1999), combines the word-based and n-gram models using a vector-space based model and examines the effectiveness of the combined model and the individual features on varying text size. Similarly, (Lena Grothe and Nrnberger, 2008) combines both models using the ad-hoc met</context>
</contexts>
<marker>Sibun, Reynar, 1996</marker>
<rawString>Penelope Sibun and Jeffrey C. Reynar. 1996. Language identification: Examining the issues. In 5th Symposium on Document Analysis and Information Retrieval, pages 125–135, Las Vegas, Nevada, U.S.A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Craig Silverstein</author>
<author>Hannes Marais</author>
<author>Monika Henzinger</author>
<author>Michael Moricz</author>
</authors>
<title>Analysis of a very large web search engine query log.</title>
<date>1999</date>
<journal>SIGIR Forum,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="14101" citStr="Silverstein et al., 1999" startWordPosition="2464" endWordPosition="2467">t: Tables T1:[q, u, fu], T2:[u, l], T3:[q, l, fl, cu,l] Parameters W, F, and U Output: Table D:[q, l] CREATE VIEW T4 AS SELECT T1.q, COUNT(T1.u) AS cu, SUM(T1.fu) AS Fq FROM T1 INNER JOIN T2 ON T1.u = T2.u GROUP BY q; CREATE VIEW D AS SELECT T3.q, T3.l, T3.fl / T4.Fq AS wq,l FROM T1 INNER JOIN T4 ON T3.q = T4.q 10 WHERE T4.Fq &lt; F AND wq,l &gt;= W AND T4.cu,l &gt;= U; Algorithm 2: Construction of the final data set D, by eliminating queries from T3 based on the parameters W, F, and U. The parameters F, U, and W are actually dependent on the size of the data set under consideration, and the study in (Silverstein et al., 1999) suggests that we can get enough click-through data for our analysis by retrieving a large sample of queries. Since we retrieve the queries that are submitted within a three months period, for each language, we have millions of unique queries in our data set. Investigating a held-out development set of queries retrieved from the United States web site (www.yahoo.com), we empirically decided the following values for the parameters, W = 1, F = 50, and U = 5. In other words, we only accepted the queries for which the contents of the urls agree on the same language, that are submitted less than 50</context>
</contexts>
<marker>Silverstein, Marais, Henzinger, Moricz, 1999</marker>
<rawString>Craig Silverstein, Hannes Marais, Monika Henzinger, and Michael Moricz. 1999. Analysis of a very large web search engine query log. SIGIR Forum, 33(1):6–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Souter</author>
<author>G Churcher</author>
<author>J Hayes</author>
<author>J Hughes</author>
</authors>
<title>Natural language identification using corpus-based models.</title>
<date>1994</date>
<journal>Hermes Journal of Linguistics,</journal>
<volume>13</volume>
<pages>203</pages>
<contexts>
<context position="5462" citStr="Souter et al., 1994" startWordPosition="922" endWordPosition="925"> are trained on a list of words or n-gram models for each reference language. The word based approaches use a list of short words, common words, or a complete vocabulary which are extracted from a corpus for each language. The short words approach uses a list of words with at most four or five characters; such as determiners, prepositions, and conjunctions, and is used in (Ingle, 1976; Grefenstette, 1995). The common words method is a generalization over the short words one which, in addition, includes other frequently occuring words without limiting them to a specific length, and is used in (Souter et al., 1994; Cowie et al., 1999). For classification, the word-based approaches sort the list of words in descending order of their frequency in the corpus from which they are extracted. Then the likelihood of each word in a given text can be calculated by using rank-order statistics or by transforming the frequencies into probabilities. The n-gram based approaches are based on the counts of character or byte n-grams, which are sequences of n characters or bytes, extracted from a corpus for each reference language. Different classification models that use the n-gram features have been proposed. (Cavnar a</context>
</contexts>
<marker>Souter, Churcher, Hayes, Hughes, 1994</marker>
<rawString>C. Souter, G. Churcher, J. Hayes, and J. Hughes. 1994. Natural language identification using corpus-based models. Hermes Journal of Linguistics, 13:183– 203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. Intl. Conf. on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<location>Denver, CO.</location>
<contexts>
<context position="20195" citStr="Stolcke, 2002" startWordPosition="3519" endWordPosition="3520">he characteristics of those languages, which allow the construction of composite words from multiple words, or have a richer morphology. Thus, the concepts can be expressed in less number of words or characters. 4.1 Models for Language Identification We implement a statistical model using a character based n-gram feature. For each language, we collect the n-gram counts (for n = 1 to n = 7 also using the word beginning and ending spaces) from the vocabulary of the training corpus, and then generate a probability distribution from these counts. We implemented this model using the SRILM Toolkit (Stolcke, 2002) with the modified Kneser-Ney Discounting and interpolation options. For comparison purposes, we also implemented the Rank-Order method using the parameters described in (Cavnar and Trenkle, 1994). For the knowledge based method, we used the vocabulary of each language obtained from the training corpora, together with the word counts. From these counts, we obtained a probability distribution for all the words in our vocabulary. In other words, this time we used a word-based ngram method, only with n = 1. It should be noted that increasing the size of n, which might help in language identificat</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm – an extensible language modeling toolkit. In Proc. Intl. Conf. on Spoken Language Processing, volume 2, pages 901–904, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hidayet Takci</author>
<author>Ibrahim Sogukpinar</author>
</authors>
<title>Centroid-based language identification using letter feature set.</title>
<date>2004</date>
<booktitle>In CICLing,</booktitle>
<pages>640--648</pages>
<contexts>
<context position="7542" citStr="Takci and Sogukpinar, 2004" startWordPosition="1254" endWordPosition="1257">l and examines the effectiveness of the combined model and the individual features on varying text size. Similarly, (Lena Grothe and Nrnberger, 2008) combines both models using the ad-hoc method of (Cavnar and Trenkle, 1994), and also presents a comparison study. The work most closely related to ours is presented very recently in (Hammarstr¨om, 2007), which proposes a model that uses a frequency dictionary together with affix information in order to identify the language of texts as short as one word. Other systems that use methods aside from the ones discussed above have also been proposed. (Takci and Sogukpinar, 2004) used letter frequency features in a centroid based classification model. (Kruengkrai et al., 2005) proposed a feature based on alignment of string kernels using suffix trees, and used it in two different classifiers. Finally, (Biemann and Teresniak, 2005) presented an unsupervised system that clusters the words based on sentence co-occurence. Recently, (Hughes et al., 2006) surveyed the previous work in this area and suggested that the problem of language identification for written resources, although well studied, has too many open challenges which requires a more systematic and collaborativ</context>
</contexts>
<marker>Takci, Sogukpinar, 2004</marker>
<rawString>Hidayet Takci and Ibrahim Sogukpinar. 2004. Centroid-based language identification using letter feature set. In CICLing, pages 640–648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques.</booktitle>
<volume>2</volume>
<pages>edition.</pages>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="27925" citStr="Witten and Frank, 2005" startWordPosition="4824" endWordPosition="4827"> confidence score into one of the three categories described above. Finally, in order to form an association between the output of the model and its confidence, we create a composite attribute by appending the discretized confidence to the identified language. As an example, our statistical model identifies the query ”the sovereign individual” as English (en), and reports a n = 7.60, which is greater than or equal to µ&apos; + a&apos; = 4.47 + 1.96 = 6.43. Therefore the resulting composite attribute assigned to this query by the statistical model is ”en-HIGH”. We used the Weka Machine Learning Toolkit (Witten and Frank, 2005) to implement our DT classifier. We trained our system with 500, 1,000, 5,000, and 10,000 instances of the automatically annotated data and evaluate it on the same test set of 3500 human-annotated queries. We show the results in Table 5. The results in Table 5 show that our DT classifier, on average, outperforms all the models in Table 4 for each size of the training data. Furthermore, the performance of the system increases with the increasing size of training data. In particular, the improvement that we get for Spanish, French, and German queries are strikingly good. This shows that our DT c</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann, 2 edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>