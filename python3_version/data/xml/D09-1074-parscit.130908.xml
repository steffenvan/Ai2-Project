<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000025">
<title confidence="0.844242">
Discriminative Corpus Weight Estimation for Machine Translation
</title>
<note confidence="0.6663075">
Spyros Matsoukas and Antti-Veikko I. Rosti and Bing Zhang
BBN Technologies, 10 Moulton Street, Cambridge, MA 02138
</note>
<email confidence="0.972307">
{smatsouk,arosti,bzhang}@bbn.com
</email>
<sectionHeader confidence="0.994233" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99977964">
Current statistical machine translation
(SMT) systems are trained on sentence-
aligned and word-aligned parallel text col-
lected from various sources. Translation
model parameters are estimated from the
word alignments, and the quality of the
translations on a given test set depends
on the parameter estimates. There are
at least two factors affecting the parame-
ter estimation: domain match and training
data quality. This paper describes a novel
approach for automatically detecting and
down-weighing certain parts of the train-
ing corpus by assigning a weight to each
sentence in the training bitext so as to op-
timize a discriminative objective function
on a designated tuning set. This way, the
proposed method can limit the negative ef-
fects of low quality training data, and can
adapt the translation model to the domain
of interest. It is shown that such discrim-
inative corpus weights can provide sig-
nificant improvements in Arabic-English
translation on various conditions, using a
state-of-the-art SMT system.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999921169811321">
Statistical machine translation (SMT) systems rely
on a training corpus consisting of sentences in
the source language and their respective reference
translations to the target language. These paral-
lel sentences are used to perform automatic word
alignment, and extract translation rules with asso-
ciated probabilities. Typically, a parallel training
corpus is comprised of collections of varying qual-
ity and relevance to the translation problem of in-
terest. For example, an SMT system applied to
broadcast conversational data may be trained on
a corpus consisting mostly of United Nations and
newswire data, with only a very small amount of
in-domain broadcast news/conversational data. In
this case, it would be desirable to down-weigh the
out-of-domain data relative to the in-domain data
during the rule extraction and probability estima-
tion. Similarly, it would be good to assign a lower
weight to data of low quality (e.g., poorly aligned
or incorrectly translated sentences) relative to data
of high quality.
In this paper, we describe a novel discrimina-
tive training method that can be used to estimate a
weight for each sentence in the training bitext so as
to optimize an objective function – expected trans-
lation edit rate (TER) (Snover et al., 2006) – on a
held-out development set. The training bitext typ-
ically consists of millions of (parallel) sentences,
so in order to ensure robust estimation we express
each sentence weight as a function of sentence-
level features, and estimate the parameters of this
mapping function instead. Sentence-level fea-
tures may include the identifier of the collection or
genre that the sentence belongs to, the number of
tokens in the source or target side, alignment infor-
mation, etc. The mapping from features to weights
can be implemented via any differentiable func-
tion, but in our experiments we used a simple per-
ceptron. Sentence weights estimated in this fash-
ion are applied directly to the phrase and lexical
counts unlike any previously published method to
the author’s knowledge. The tuning framework is
developed for phrase-based SMT models, but the
tuned weights are also applicable to the training of
a hierarchical model. In cases where the tuning set
used for corpus weight estimation is a close match
to the test set, this method yields significant gains
in TER, BLEU (Papineni et al., 2002), and ME-
TEOR (Lavie and Agarwal, 2007) scores over a
state-of-the-art hierarchical baseline.
The paper is organized as follows. Related work
on data selection, data weighting, and model adap-
tation is presented in Section 2. The corpus weight
</bodyText>
<page confidence="0.960661">
708
</page>
<note confidence="0.9966115">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 708–717,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.9988758">
approach and estimation algorithm are described
in Section 3. Experimental evaluation of the ap-
proach is presented in Sections 4 and 5. Section 6
concludes the paper with a few directions for fu-
ture work.
</bodyText>
<sectionHeader confidence="0.999326" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999912568627451">
Previous work related to corpus weighting may
be split into three categories: data selection, data
weighting, and translation model adaptation. The
first two approaches may improve the quality
of the word alignment and prevent phrase-pairs
which are less useful for the domain to be learned.
The model adaptation, on the other hand, may
boost the weight of the more relevant phrase-
pairs or introduce translations for unseen source
phrases.
Resnik and Smith (2003) mined parallel text
from the web using various filters to identify likely
translations. The filtering may be viewed as a
data selection where poor quality translation are
discarded before word alignment. Yasuda et al.
(2008) selected subsets of an existing parallel cor-
pus to match the domain of the test set. The dis-
carded sentence pairs may be valid translations
but they do not necessarily improve the translation
quality on the test domain. Mandal et al. (2008)
used active learning to select suitable training data
for human translation. Hildebrand et al. (2005) se-
lected comparable sentences from parallel corpora
using information retrieval techniques.
Lu et al. (2007) proposed weighting compara-
ble portions of the parallel text before word align-
ment based on information retrieval. The relevant
portions of the parallel text were given a higher in-
teger weight in GIZA++ word alignment. Similar
effect may be achieved by replicating the relevant
subset in the training data.
Lu et al. (2007) also proposed training adapted
translation models which were interpolated with a
model trained on the entire parallel text. Snover
et al. (2008) used cross-lingual information re-
trieval to identify possible bias-rules to improve
the coverage on the source side. These rules may
cover source phrases for which no translations
were learned from the available parallel text.
Koehn and Schroeder (2007) described a pro-
cedure for domain adaptation that was using two
translation models in decoding, one trained on
in-domain data and the other on out-of-domain
data. Phrase translation scores from the two mod-
els where combined in a log-linear fashion, with
weights estimated based on minimum error rate
training (Och, 2003) on a designated tuning set.
The method described in this paper can also be
viewed as data filtering or (static) translation adap-
tation, but it has the following advantages over
previously published techniques:
</bodyText>
<listItem confidence="0.827515318181818">
1. The estimated corpus weights are discrim-
inative and are computed so as to directly
optimize an MT performance metric on a
pre-defined development set. Unlike the do-
main adaptation technique in (Koehn and
Schroeder, 2007), which also estimates the
adaptation parameters discriminatively, our
proposed method does not require a man-
ual specification of the in-domain and out-
of-domain training data collections. Instead,
it automatically determines which collections
are most relevant to the domain of interest,
and increases their weight while decreasing
the weight assigned to less relevant collec-
tions.
2. All sentences in the parallel corpus can in-
fluence the translation model, as opposed
to filtering/discarding data. However, the
proposed method can still assign very low
weights to parts of the corpus, if it determines
that it helps improve MT performance.
3. The framework used for estimating the cor-
</listItem>
<bodyText confidence="0.980738777777778">
pus weights can be easily extended to support
discriminative alignment link-level weights,
thus allowing the system to automatically
identify which portions of the training sen-
tences are most useful.
Naturally, as with any method, the proposed
technique has certain limitations. Specifically, it
is only concerned with influencing the translation
rule probabilities via the corpus weights; it does
not change the set of rules extracted. Thus, it is
unable to add new translation rules as in Snover
et al. (2008). Also, it can potentially lead to pa-
rameter over-fitting, especially if the function that
maps sentence features to weights is complex and
based on a large number of parameters, or if the
development set used for estimating the mapping
function does not match the characteristics of the
test set.
</bodyText>
<page confidence="0.998935">
709
</page>
<sectionHeader confidence="0.972463" genericHeader="method">
3 Corpus Weights Estimation
</sectionHeader>
<subsectionHeader confidence="0.998338">
3.1 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.999991567567568">
The purpose of feature extraction is to identify,
for each sentence in the parallel training data, a
set of features that can be useful in estimating a
weight that is correlated with quality or relevance
to the MT task at hand. Starting from sentence-
aligned, word-aligned parallel training data, one
could extract various types of sentence-level fea-
tures. For example, we could specify features that
describe the two sides of the parallel data or the
alignment between them, such as collection id,
genre id, number of source tokens, number of tar-
get tokens, ratio of number of source and target
tokens, number of word alignment links, fraction
of source tokens that are unaligned, and fraction
of target tokens that are unaligned. Additionally,
we could include information retrieval (IR) related
features that reflect the relevance of a training sen-
tence to the domain of interest, e.g., by measur-
ing vector space model (VSM) distance of the sen-
tence to the current tuning set, or its log likelihhod
with respect to an in-domain language model.
Note that the collection and genre identifiers
(ids) mentioned above are bit vectors. Each col-
lection in the training set is mapped to a number.
A collection may consist of sentences from multi-
ple genres (e.g., newswire, web, broadcast news,
broadcast conversations). Genres are also mapped
to a unique number across the whole training set.
Then, given a sentence in the training bitext, we
can extract a binary vector that contains two non-
zero bits, one indicating the collection id, and an-
other denoting the genre id.
It is worth mentioning that in the experiments
reported later in this paper we made use of only the
collection and genre ids as features, although the
framework supports general sentence-level fea-
tures.
</bodyText>
<subsectionHeader confidence="0.999782">
3.2 Mapping Features to Weights
</subsectionHeader>
<bodyText confidence="0.999989851851852">
As mentioned previously, one way to map a fea-
ture vector to a weight is to use a perceptron.
A multi-layer neural network may also be used,
but at the expense of slower training. In this
work, all of the experiments carried out made use
of a perceptron mapping function. However, it
is also possible to cluster the training sentences
into classes by training a Gaussian mixture model
(GMM) on their respective feature vectors1. Then,
given a feature vector we can compute the (poste-
rior) probability that it was generated by one of
the N Gaussians in the GMM, and use this N-
dimensional vector of posteriors as input to the
perceptron. This is similar to having a neural net-
work with a static hidden layer and Gaussian acti-
vation functions.
Given the many choices available in mapping
features to weights, we will describe the mapping
function in general terms. Let fi be the n x 1
feature vector corresponding to sentence i. Let
0(x; A) denote a function Rn _* (0, 1) that is pa-
rameterized in terms of the parameter vector A and
maps a feature vector x to a scalar weight in (0, 1).
The goal of the automatic corpus weight estima-
tion procedure is to estimate the parameter vector
A so as to optimize an objective function on a de-
velopment set.
</bodyText>
<subsectionHeader confidence="0.999708">
3.3 Training with Weighted Corpora
</subsectionHeader>
<bodyText confidence="0.99997075">
Once the sentence features have been mapped to
weights, the translation rule extraction and prob-
ability estimation can proceed as usual, but with
weighted counts. For example, let wi = 0(fi; A)
be the weight assigned to sentence i. Let (s, t) be
a source-target phrase pair that can be extracted
from the corpus, and A(s) and B(t) indicating the
sets of sentences that s and t occur in. Then,
</bodyText>
<equation confidence="0.9979475">
P(s |t) = Ej∈A(s)∩B(t) wjcj (s, t) (1)
Ej∈B(t) wjcj(t)
</equation>
<bodyText confidence="0.9999905">
where cj(�) denotes the number of occurrences of
the phrase (or phrase-pair) in sentence j.
</bodyText>
<subsectionHeader confidence="0.997696">
3.4 Optimizing the Mapping Function
</subsectionHeader>
<bodyText confidence="0.999969">
Estimation of the parameters A of the mapping
function 0 can be performed by directly optimiz-
ing a suitable objective function on a development
set. Ideally, we would like to estimate the param-
eters of the mapping function so as to directly op-
timize an automatic MT performance evaluation
metric, such as TER or BLEU on the full transla-
tion search space. However, this is extremely com-
putationally intensive for two reasons: (a) opti-
mizing in the full translation search space requires
a new decoding pass for each iteration of opti-
mization; and (b) a direct optimization of TER or
</bodyText>
<footnote confidence="0.749347666666667">
1Note that in order to train such a GMM it may be nec-
essary to first apply a decorrelating, dimensionality reducing,
transform (e.g., principal component analysis) to the features.
</footnote>
<page confidence="0.988427">
710
</page>
<bodyText confidence="0.999979130434783">
BLEU requires the use of a derivative free, slowly
converging optimization method such as MERT
(Och, 2003), because these objective functions are
not differentiable.
In our case, for every parameter vector update
we need to essentially retrain the translation model
(reestimate the phrase and lexical translation prob-
abilities based on the updated corpus weights), so
the cost of each iteration is significantly higher
than in a typical MERT application. For these rea-
sons, in this work we chose to minimize the ex-
pected TER over a translation N-best on a desig-
nated tuning set, which is a continuous and differ-
entiable function and can be optimized with stan-
dard gradient descent methods in a small number
of iterations. Note, that using expected TER is not
the only option here; any criterion that can be ex-
pressed as a continuous function of the phrase or
lexical translation probabilities can be used to op-
timize λ.
Given an N-best of translation hypotheses over
a development set of S sentences, we can define
the expected TER as follows
</bodyText>
<equation confidence="0.95992275">
S Ns
T _ Es=1 Ej=1 psjEsj (2)
ES
s=1 rs
</equation>
<bodyText confidence="0.97666656">
where Ns is the number of translation hypothe-
ses available for segment s; Esj is the minimum
raw edit distance between hypothesis j of seg-
ment s (or hsj, for short) and the reference transla-
tion(s) corresponding to segment s; rs is the aver-
age number of reference translation tokens in seg-
ment s, and psj is the posterior probability of hy-
pothesis hsj in the N-best. The latter is computed
as follows
psj _ ENs k=1 eγLsk (3)
where Lsj is the total log likelihood of hypothe-
sis hsj, and γ is a tunable scaling factor that can
be used to change the dynamic range of the likeli-
hood scores and hence the distribution of posteri-
ors over the N-best. The hypothesis likelihood Lsj
is typically computed as a dot product of a decod-
ing weight vector and a vector of various “feature”
scores, such as log phrase translation probability,
log lexical translation probability, log n-gram lan-
guage model probability, and number of tokens in
the hypothesis. However, in order to simplify this
presentation we will assume that it contains a sin-
gle translation model score, the log phrase transla-
tion probability of source given target. This score
is a sum of log conditional probabilities, similar
to the one defined in Equation 1. Therefore, Lsj
is indirectly a function of the training sentence
weights.
In order to minimize the expected TER T , we
need to compute the derivative of T with respect
to the mapping function parameters λ. Using the
chain rule, we get equations (4)-(8), where the
summation in Equation 6 is over all source-target
phrase pairs in the derivation of hypothesis hsm, ξ
is the decoding weight assigned to the log phrase
translation score, and the summation in Equation
7 is over all training sentences2.
Thus, in order to compute the derivative of
the objective function we first need to calculate
∂ ln P(sk|tk) for every phrase pair (sk,tk) in the
∂s
translation N-best based on Equations 7 and 8,
which requires time proportional to the number of
occurrences of these phrases in the parallel train-
ing data. After that, we can compute ∂Lsm
∂s for
each hypothesis hsm, based on Equation 6. Fi-
nally, we calculate ∂ ln psj
∂s and ∂T∂s based on Equa-
tions 5 and 4, respectively.
</bodyText>
<subsectionHeader confidence="0.778553">
3.5 Implementation Issues
</subsectionHeader>
<bodyText confidence="0.967215785714286">
In our system, the corpus weights were trained
based on N-best translation hypotheses generated
by a phrase-based MT system on a designated tun-
ing set. Each translation hypothesis in the N-best
has a score that is a (linear) function of the fol-
lowing log translation probabilities: target phrase
given source phrase, source phrase given target
phrase, and lexical smoothing term. Additionally,
each hypothesis specifies information about its
derivation, i.e., which source-target phrase pairs it
consists of. Therefore, given an N-best, we can
identify the set of unique phrase pairs and use this
information in order to perform a filtered accumu-
lation of the statistics needed for calculating the
derivative in Equation 8. This reduces the storage
needed for the sufficient statistics significantly.
Minimization of the expected TER of the N-
best hypotheses was performed using the limited-
memory BFGS algorithm (Liu and Nocedal,
1989). Typically, the parameter vector λ required
about 30 iterations of LBFGS to converge.
Since the N-best provides only a limited repre-
sentation of the MT hypothesis search space, we
regenerated the N-best after every 30 iterations
2In the general case where Lsj includes other translation
scores, e.g., lexical translation probabilities, the derivative
∂,Xmwill have to include additional terms.
as
</bodyText>
<figure confidence="0.960002119047619">
eγLsj
711
! XS XNs
1 a lnpsj
psj Esj (4)
aλ
=1 rs s=1 j=1
PS
s
aT
aT
alnpsj
aλ
aλ =
XS
s=1
alnpsj
XNs
j=1
alnpsj = XNs a ln psj aLsm aLsj − XNs psm aLsm!
aλ m=1 aλ = lr aλ m=1 (5)
aLsm aλ
aLsm
aλ
X=
(sk,tk)∈hsm
aLsm
a ln P(sk|tk)
aλ X
(sk,tk)∈hsm
ξ a lnP(sk|tk) (6)
aλ
a ln P(sk|tk) =
a lnP(sk|tk) X a ln P(sk|tk) awi (7)
= aλ
aλ
i
awi
a ln P(sk|tk) = Pj∈A(sk)∩B(tk) S (j − i) cj(sk, tk) Pj∈B(tk) S (j − i) cj(tk) −(8)
Pj∈B(tk) wjcj(tk)
awi P
j∈A(sk)∩B(tk) wjcj(sk, tk)
</figure>
<equation confidence="0.996859">
~ 1 x = 0
S(x) = (9)
0 x =60
</equation>
<bodyText confidence="0.98931025">
of LBFGS training, merging new hypotheses with
translations from previous iterations. The overall
training procedure is described in more detail be-
low:
</bodyText>
<listItem confidence="0.9825029">
1. Initialize parameter vector λ to small random
values, so that all training sentences receive
approximately equal weights.
2. Initialize phrase-based MT decoding weights
to previously tuned values.
3. Perform weighted phrase rule extraction as
described in Equation 1, to estimate the
phrase and lexical translation probabilities.
4. Decode the tuning set, generating N-best.
5. Merge N-best hypotheses from previous iter-
ations to current N-best.
6. Tune decoding weights so as to minimize
TER on merged N-best, using a derivative
free optimization method. In our case, we
used Powell’s algorithm (Powell, 1964) mod-
ified by Brent as described in (Brent, 1973) 3.
7. Identify set of unique source-target phrase
pairs in merged N-best.
8. Extract sufficient statistics from training data
for all phrases identified in step 7.
</listItem>
<footnote confidence="0.6129105">
3This method was first used for N-best based parameter
optimization in (Ostendorf et al., 1991).
</footnote>
<listItem confidence="0.943121166666667">
9. Run the LBFGS algorithm to minimize the
expected TER in the merged N-best, using
the derivative equations described previously.
10. Assign a weight to each training sentence
based on the λ values optimized in 9.
11. Go to step 3.
</listItem>
<bodyText confidence="0.999175">
Typically, the corpus weights converge in about
4-5 main iterations. The calculation of the deriva-
tive is parallelized to speed up computation, re-
quiring about 10 minutes per iteration of LBFGS.
</bodyText>
<sectionHeader confidence="0.998589" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9999754">
In this section we describe the setup that was used
for all experiments reported in this paper. Specif-
ically, we provide details about the training data,
development sets, and MT systems (phrase-based
and hierarchical).
</bodyText>
<subsectionHeader confidence="0.998356">
4.1 Training Data
</subsectionHeader>
<bodyText confidence="0.999908571428571">
All MT training experiments made use of an
Arabic-English corpus of approximately 200 mil-
lion tokens (English side). Most of the collections
in this corpus are available through the Linguis-
tic Data Consortium (LDC) and are regularly part
of the resources specified for the constrained data
track of the NIST MT evaluation4.
</bodyText>
<footnote confidence="0.99364775">
4For a list of the NIST MT09 constrained train-
ing condition resources, see http://www.itl.
nist.gov/iad/mig/tests/mt/2009/MT09_
ConstrainedResources.pdf
</footnote>
<page confidence="0.996206">
712
</page>
<bodyText confidence="0.999227">
The corpus includes data from multiple gen-
res, as shown in Table 1. The “Sakhr” newswire
collection is a set of Arabic-to-English and
English-to-Arabic data provided by Sakhr Soft-
ware, totaling about 30.8 million tokens, and
is only available to research teams participat-
ing in the Defense Advanced Research Projects
Agency (DARPA) Global Autonomous Language
Exploitation (GALE) program. The “LDC Giga-
word (ISI)” collection was produced by automati-
cally detecting and extracting portions of parallel
text from the monolingual LDC Arabic and En-
glish Gigaword collections, using a method devel-
oped at the Information Sciences Institute (ISI) of
the University of Southern California.
</bodyText>
<table confidence="0.9997894">
Data Origin Style Size
(K tokens)
U. Nations 118049
LDC pre-GALE Newswire 2700
Treebank 685
Newswire 14344
Treebank 292
LDC post-GALE Web 478
Broad. News 573
Broad. Conv. 1003
Lexicons 436
Web-found text Quran 406
Sakhr Newswire 30790
LDC Gigaword Newswire 29169
(ISI)
</table>
<tableCaption confidence="0.8183115">
Table 1: Composition of the Arabic-English par-
allel corpus used for MT training.
</tableCaption>
<bodyText confidence="0.99993235">
It is easy to see that most of the parallel train-
ing data are either newswire or from United Na-
tions. The amount of web text or broadcast
news/conversations is only a very small fraction
of the total corpus. In total, there are 31 collec-
tions in the training bitext. Some collections (es-
pecially those released recently by LDC for the
GALE project) consist of data from multiple gen-
res. The total number of unique genres (or data
types) in the training set is 10.
Besides the above bitext, we also used approxi-
mately 8 billion words of English text for language
model (LM) training (3.7B words from the LDC
Gigaword corpus, 3.3B words of web-downloaded
text, and 1.1B words of data from CNN archives).
This data was used to train two language mod-
els: an entropy-pruned trigram LM, used in decod-
ing, and an unpruned 5-gram LM used in N-best
rescoring. Kneser-Ney smoothing was applied to
the n-grams in both cases.
</bodyText>
<subsectionHeader confidence="0.99798">
4.2 Development Sets
</subsectionHeader>
<bodyText confidence="0.996316695652174">
The development sets used for tuning and testing
the corpus weights and other MT settings were
comprised of documents from previous Arabic-
English NIST MT evaluation sets and from GALE
development/evaluation sets.
Specifically, the newswire Tune and Test sets
consist of documents from the following col-
lections: the newswire portion of NIST MT04,
MT05, MT06, and MT08 evaluation sets, the
GALE Phase 1 (P1) and Phase 2 (P2) evaluation
sets, and the GALE P2 and P3 development sets.
The web Tune and Test sets are made of docu-
ments from NIST MT06 and MT08, the GALE P1
and P2 evaluation sets, the GALE P2 and P3 devel-
opment sets, and a held-out portion of the GALE
year 1 quarter 4 web training data release.
The audio Tune and Test sets consist of roughly
equal parts of news and conversations broadcast
from November 2005 through May 2007 by ma-
jor Arabic-speaking television and radio stations
(e.g., Al-Jazeera, Al-Arabiya, Syrian TV), totaling
approximately 14 hours of speech. The audio was
processed through automated speech recognition
(ASR) in order to produce (errorful) transcripts
that were used as input to all MT decoding experi-
ments reported in this paper. However, the corpus
weight estimation was carried out based on N-best
MT of the Arabic audio reference transcriptions
(i.e., the transcripts had no speech recognition er-
rors, and contained full punctuation).
It is important to note that some of the docu-
ments in the above devsets have multiple reference
translations (usually 4), while others have only
one. Most of the documents in the newswire sets
have 4 references, but unfortunately the web and
audio sets have, on average, less than 2 reference
translations per segment. More details are listed in
Table 2.
Another important note is that, although the au-
dio sets consist of both broadcast news (BN) and
broadcast conversations (BC), we did not perform
BN or BC-specific tuning. Corpus weights and
MT decoding parameters were optimized based on
a single Tune set, on a mix of BN and BC data.
However, when we report speech translation re-
sults in later sections, we break down the perfor-
</bodyText>
<page confidence="0.996521">
713
</page>
<table confidence="0.999246">
Genre #segs Tune #refs/seg #segs Test #refs/seg
#tokens #tokens
Newswire 1994 72359 3.94 3149 115700 3.67
Web 3278 99280 1.69 4425 125795 2.08
Audio BN 897 32990 1.00 1530 53067 1.00
Audio BC 765 24607 1.00 1416 44435 1.00
</table>
<tableCaption confidence="0.7896905">
Table 2: Characteristics of the tuning (Tune) and validation (Test) sets used for development on Arabic
newswire, web, and audio. The audio sets include material from both broadcast news and broadcast
conversations.
mance by genre.
</tableCaption>
<subsectionHeader confidence="0.997824">
4.3 MT Systems
</subsectionHeader>
<bodyText confidence="0.99985724">
Experiments were performed using two types of
statistical MT systems: a phrase-based system,
similar to Pharaoh (Koehn, 2004), and a state-
of-the-art, hierarchical string-to-dependency-tree
system, similar to (Shen et al., 2008).
The phrase-based MT system employs a pruned
3-gram LM in decoding, and can optionally gen-
erate N-best unique translation hypotheses which
are used to estimate the corpus weights, as de-
scribed in Section 3.
The hierarchical MT system performs decoding
with the same 3-gram LM, generates N-best of
unique translation hypotheses, and then rescores
them using a large, unpruned 5-gram LM in order
to select the best scoring translation. It is worth
mentioning that this hierarchical MT system pro-
vides a very strong baseline; it achieves a case-
sensitive BLEU score of 52.20 on the newswire
portion of the NIST MT08 evaluation set, which
is similar to the score of the second-best system
that participated in the unconstrained data track of
the NIST MT08 evaluation.
Both types of models were trained on the same
word alignments generated by GIZA++ (Och and
Ney, 2003).
</bodyText>
<sectionHeader confidence="0.999768" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999491857142857">
In this section we report results on the Arabic
newswire, web, and audio development sets, us-
ing both phrase-based and hierarchical MT sys-
tems, in terms of TER, BLEU5, and METEOR
(Lavie and Agarwal, 2007). Whenever corpus
weights are used, they were estimated on the des-
ignated Tune set using the phrase-based MT sys-
</bodyText>
<footnote confidence="0.962754">
5The brevity penalty was calculated using the formula in
the original IBM paper, rather than the more recent definition
implemented in the NIST mteval-v11b.pl script.
</footnote>
<bodyText confidence="0.999197222222222">
tem. Only the collection and genre ids were used
as sentence features in order to estimate the corpus
weights. As mentioned in Section 4.1, the train-
ing bitext consists of 31 collections and 10 gen-
res, so each training sentence was assigned a 41-
dimensional binary vector indicating its particu-
lar collection/genre combination. That vector was
then mapped into a single weight using a percep-
tron.
</bodyText>
<subsectionHeader confidence="0.511241">
5.1 Phrase-based MT
</subsectionHeader>
<bodyText confidence="0.999424545454545">
Results using the phrase-based MT system are
shown in Table 3. In all cases, the decoding
weights were optimized so as to minimize TER
on the designated Tune set. On newswire, the
discriminative corpus weights provide 0.8% abso-
lute gain in TER, in both Tune and Test sets. On
web, the TER gain is 0.9% absolute on Tune and
0.5% on Test. On the audio Test set, the TER gain
is 0.5% on BN and 1.4% on BC. Significant im-
provements were also obtained in the BLEU and
METEOR scores, on all sets and conditions.
</bodyText>
<subsectionHeader confidence="0.985903">
5.2 Hierarchical MT
</subsectionHeader>
<bodyText confidence="0.999572352941177">
Results using the hierarchical MT system are
shown in Table 4. The hierarchical system
used different tuning criteria in each genre. On
newswire, the decoding weights were optimized
so as to maximize BLEU, while on web and audio
the tuning was based on 0.5TER+0.5(1−BLEU)
(referred to as TERBLEU in what follows). Note
that these were the criteria for tuning the decoding
weights; whenever corpus weights were used, they
were taken from the phrase-based system.
It is interesting to see that gains from discrimi-
native corpus weights carry over to the more pow-
erful hierarchical MT system. On newswire Test,
the gain in BLEU is 0.8; on web Test, the gain in
TERBLEU is 0.3. On the audio Test set, the cor-
pus weights provide 0.7 and 0.75 TERBLEU re-
duction on BN and BC, respectively. As with the
</bodyText>
<page confidence="0.993038">
714
</page>
<table confidence="0.994499928571429">
Set Corpus Weights TER Newswire MTR TER Web MTR
BLEU BLEU
Tune No 42.3 48.2 67.5 60.0 21.9 51.3
Yes 41.5 49.6 68.7 59.1 22.8 52.3
Test No 43.2 46.2 66.5 58.6 24.2 52.2
Yes 42.4 47.5 67.8 58.1 25.4 52.9
(a) Results on Arabic text.
Set Corpus Weights TER BN MTR TER BC MTR
BLEU BLEU
Tune No 56.0 22.9 55.5 57.3 21.7 55.0
Yes 55.0 25.0 57.1 56.1 23.6 56.4
Test No 53.0 25.3 57.7 55.9 22.9 55.4
Yes 52.5 26.6 58.8 54.5 24.7 56.8
(b) Results on Arabic audio.
</table>
<tableCaption confidence="0.975059666666667">
Table 3: Phrase-based trigram decoding results on the Arabic text and audio development sets. Decoding
weights were optimized on the Tune set in order to directly minimize TER. Corpus weights were also
optimized on Tune set, but based on expected TER.
</tableCaption>
<bodyText confidence="0.857717">
phrase-based system, all metrics improve from the
use of corpus weights, in all sets/conditions.
</bodyText>
<sectionHeader confidence="0.999268" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999985875">
We have described a novel approach for estimat-
ing a weight for each sentence in a parallel train-
ing corpus so as to optimize MT performance of a
phrase-based statistical MT system. The sentence
weights influence MT performance by being ap-
plied to the phrase and lexical counts during trans-
lation rule extraction and probability estimation.
In order to ensure robust training of the weights,
we expressed them as a function of sentence-level
features. Then, we defined the process for opti-
mizing the parameters of that function based on
the expected TER of a translation hypothesis N-
best on a designated tuning set.
The proposed technique was evaluated in the
context of Arabic-English translation, on multiple
conditions. It was shown that encouraging results
were obtained by just using collection and genre
ids as features. Interestingly, the discriminative
corpus weights were found to be generally appli-
cable and provided gains in a state-of-the-art hi-
erarchical string-to-dependency-tree MT system,
even though they were trained using the phrase-
based MT system.
Next step is to include other sentence-level fea-
tures, as described in Section 3.1. Finally, the
technique described in this paper can be extended
to address the estimation of weights at the align-
ment link level, based on link-level features. We
believe that this will have a larger impact on the
lexical and phrase translation probabilities, since
there is a large number of parallel training sen-
tences that are partially correct, i.e., they contain
parts that are aligned and translated correctly, and
parts that are wrong. The current procedure tries
to assign a single weight to such sentences, so
there is no way to distinguish between the “good”
and “bad” portions of each sentence. Pushing the
weight estimation at the alignment link level will
alleviate this problem and will make the discrimi-
native training more targeted.
</bodyText>
<sectionHeader confidence="0.996981" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999626666666667">
This work was supported by DARPA/IPTO Con-
tract No. HR0011-06-C-0022 under the GALE
program.
</bodyText>
<sectionHeader confidence="0.998503" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9988185">
Richard P. Brent. 1973. Algorithms for Minimization
Without Derivatives. Prentice-Hall.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the transla-
</reference>
<page confidence="0.997498">
715
</page>
<table confidence="0.9770015">
Set Corpus Weights TER Newswire MTR TER Web MTR
BLEU BLEU
Tune No 39.5 54.4 70.3 58.2 25.2 53.8
Yes 38.8 55.6 71.2 58.0 25.5 54.0
Test No 40.7 52.1 69.3 57.0 28.3 54.7
Yes 40.1 52.9 69.8 56.6 28.5 55.0
(a) Results on Arabic text.
Set Corpus Weights TER BN MTR TER BC MTR
BLEU BLEU
Tune No 54.9 27.3 58.0 55.8 26.1 57.4
Yes 53.6 28.2 59.0 54.9 26.9 58.0
Test No 51.6 29.9 60.0 54.4 27.6 57.7
Yes 50.7 30.4 60.7 53.2 27.9 58.7
(b) Results on Arabic audio.
</table>
<tableCaption confidence="0.732577">
Table 4: Hierarchical 5-gram rescoring results on the Arabic text and audio development sets. Decod-
</tableCaption>
<bodyText confidence="0.588724">
ing/rescoring weights were optimized on the Tune set in order to directly maximize BLEU (for newswire)
or minimize TERBLEU (for web and audio). Corpus weights were the same as the ones used in the cor-
responding phrase-based decodings.
</bodyText>
<reference confidence="0.995077327586207">
tion model for statistical machine translation based
on information retrieval. In Proceedings of the 10th
Annual Conference ofEuropean Association for Ma-
chine Translation, pages 133–142.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine trans-
lation. In Proceedings of the Second Workshop on
Statistical Machine Translation, pages 224–227.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. In Proceedings of the 6th Conference
of the Association for Machine Translation in the
Americas.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An automatic metric for MT evaluation with high
levels of correlation with human judgments. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 228–231.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503–528.
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 343–350.
Arindam Mandal, Dimitra Vergyri, Wen Wang, Jing
Zheng, Andreas Stolcke, Gokhan Tur, Dilek
Hakkani-T¨ur, and Necip Fazil Ayan. 2008. Effi-
cient data selection for machine translation. In Pro-
ceedings of the Second IEEE/ACL Spoken Language
Technology Workshop, pages 261–264.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19–51.
F. J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160–167.
M. Ostendorf, A. Kannan, S. Austin, O. Kimball,
R. Schwartz, and J. R. Rohlicek. 1991. Integra-
tion of diverse recognition methodologies through
reevaluation of nbest sentence hypotheses. In Pro-
ceedings of the DARPA Workshop on Speech and
Natural Language, pages 83–87.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311–318.
M. J. D. Powell. 1964. An efficient method for finding
the minimum of a function of several variables with-
out calculating derivatives. The Computer Journal,
pages 155–162.
Philip Resnik and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349–380.
</reference>
<page confidence="0.982159">
716
</page>
<reference confidence="0.999047">
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 577–585.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, pages
223–231.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation
using comparable corpora. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 857–866.
Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto,
and Eiichiro Sumita. 2008. Method of selecting
training data to build a compact and efficient trans-
lation model. In Proceedings of the Third Interna-
tional Joint Conference on Natural Language Pro-
cessing, volume II, pages 655–660.
</reference>
<page confidence="0.99711">
717
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.608491">
<title confidence="0.999513">Discriminative Corpus Weight Estimation for Machine Translation</title>
<author confidence="0.998494">Matsoukas I Rosti</author>
<note confidence="0.644979">BBN Technologies, 10 Moulton Street, Cambridge, MA</note>
<abstract confidence="0.997905769230769">Current statistical machine translation (SMT) systems are trained on sentencealigned and word-aligned parallel text collected from various sources. Translation model parameters are estimated from the word alignments, and the quality of the translations on a given test set depends on the parameter estimates. There are at least two factors affecting the parameter estimation: domain match and training data quality. This paper describes a novel approach for automatically detecting and down-weighing certain parts of the training corpus by assigning a weight to each sentence in the training bitext so as to optimize a discriminative objective function on a designated tuning set. This way, the proposed method can limit the negative effects of low quality training data, and can adapt the translation model to the domain of interest. It is shown that such discriminative corpus weights can provide significant improvements in Arabic-English translation on various conditions, using a state-of-the-art SMT system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Richard P Brent</author>
</authors>
<title>Algorithms for Minimization Without Derivatives. Prentice-Hall. Almut Silja</title>
<date>1973</date>
<booktitle>In Proceedings of the 10th Annual Conference ofEuropean Association for Machine Translation,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="18750" citStr="Brent, 1973" startWordPosition="3092" endWordPosition="3093">values, so that all training sentences receive approximately equal weights. 2. Initialize phrase-based MT decoding weights to previously tuned values. 3. Perform weighted phrase rule extraction as described in Equation 1, to estimate the phrase and lexical translation probabilities. 4. Decode the tuning set, generating N-best. 5. Merge N-best hypotheses from previous iterations to current N-best. 6. Tune decoding weights so as to minimize TER on merged N-best, using a derivative free optimization method. In our case, we used Powell’s algorithm (Powell, 1964) modified by Brent as described in (Brent, 1973) 3. 7. Identify set of unique source-target phrase pairs in merged N-best. 8. Extract sufficient statistics from training data for all phrases identified in step 7. 3This method was first used for N-best based parameter optimization in (Ostendorf et al., 1991). 9. Run the LBFGS algorithm to minimize the expected TER in the merged N-best, using the derivative equations described previously. 10. Assign a weight to each training sentence based on the λ values optimized in 9. 11. Go to step 3. Typically, the corpus weights converge in about 4-5 main iterations. The calculation of the derivative is</context>
</contexts>
<marker>Brent, 1973</marker>
<rawString>Richard P. Brent. 1973. Algorithms for Minimization Without Derivatives. Prentice-Hall. Almut Silja Hildebrand, Matthias Eck, Stephan Vogel, and Alex Waibel. 2005. Adaptation of the translation model for statistical machine translation based on information retrieval. In Proceedings of the 10th Annual Conference ofEuropean Association for Machine Translation, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Josh Schroeder</author>
</authors>
<title>Experiments in domain adaptation for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>224--227</pages>
<contexts>
<context position="6083" citStr="Koehn and Schroeder (2007)" startWordPosition="951" endWordPosition="954">rmation retrieval. The relevant portions of the parallel text were given a higher integer weight in GIZA++ word alignment. Similar effect may be achieved by replicating the relevant subset in the training data. Lu et al. (2007) also proposed training adapted translation models which were interpolated with a model trained on the entire parallel text. Snover et al. (2008) used cross-lingual information retrieval to identify possible bias-rules to improve the coverage on the source side. These rules may cover source phrases for which no translations were learned from the available parallel text. Koehn and Schroeder (2007) described a procedure for domain adaptation that was using two translation models in decoding, one trained on in-domain data and the other on out-of-domain data. Phrase translation scores from the two models where combined in a log-linear fashion, with weights estimated based on minimum error rate training (Och, 2003) on a designated tuning set. The method described in this paper can also be viewed as data filtering or (static) translation adaptation, but it has the following advantages over previously published techniques: 1. The estimated corpus weights are discriminative and are computed s</context>
</contexts>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 224–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="24857" citStr="Koehn, 2004" startWordPosition="4087" endWordPosition="4088">wn the perfor713 Genre #segs Tune #refs/seg #segs Test #refs/seg #tokens #tokens Newswire 1994 72359 3.94 3149 115700 3.67 Web 3278 99280 1.69 4425 125795 2.08 Audio BN 897 32990 1.00 1530 53067 1.00 Audio BC 765 24607 1.00 1416 44435 1.00 Table 2: Characteristics of the tuning (Tune) and validation (Test) sets used for development on Arabic newswire, web, and audio. The audio sets include material from both broadcast news and broadcast conversations. mance by genre. 4.3 MT Systems Experiments were performed using two types of statistical MT systems: a phrase-based system, similar to Pharaoh (Koehn, 2004), and a stateof-the-art, hierarchical string-to-dependency-tree system, similar to (Shen et al., 2008). The phrase-based MT system employs a pruned 3-gram LM in decoding, and can optionally generate N-best unique translation hypotheses which are used to estimate the corpus weights, as described in Section 3. The hierarchical MT system performs decoding with the same 3-gram LM, generates N-best of unique translation hypotheses, and then rescores them using a large, unpruned 5-gram LM in order to select the best scoring translation. It is worth mentioning that this hierarchical MT system provide</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Abhaya Agarwal</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>228--231</pages>
<contexts>
<context position="3641" citStr="Lavie and Agarwal, 2007" startWordPosition="566" endWordPosition="569">s can be implemented via any differentiable function, but in our experiments we used a simple perceptron. Sentence weights estimated in this fashion are applied directly to the phrase and lexical counts unlike any previously published method to the author’s knowledge. The tuning framework is developed for phrase-based SMT models, but the tuned weights are also applicable to the training of a hierarchical model. In cases where the tuning set used for corpus weight estimation is a close match to the test set, this method yields significant gains in TER, BLEU (Papineni et al., 2002), and METEOR (Lavie and Agarwal, 2007) scores over a state-of-the-art hierarchical baseline. The paper is organized as follows. Related work on data selection, data weighting, and model adaptation is presented in Section 2. The corpus weight 708 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 708–717, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP approach and estimation algorithm are described in Section 3. Experimental evaluation of the approach is presented in Sections 4 and 5. Section 6 concludes the paper with a few directions for future work. 2 Related Work Previous work relate</context>
<context position="26040" citStr="Lavie and Agarwal, 2007" startWordPosition="4278" endWordPosition="4281"> that this hierarchical MT system provides a very strong baseline; it achieves a casesensitive BLEU score of 52.20 on the newswire portion of the NIST MT08 evaluation set, which is similar to the score of the second-best system that participated in the unconstrained data track of the NIST MT08 evaluation. Both types of models were trained on the same word alignments generated by GIZA++ (Och and Ney, 2003). 5 Results In this section we report results on the Arabic newswire, web, and audio development sets, using both phrase-based and hierarchical MT systems, in terms of TER, BLEU5, and METEOR (Lavie and Agarwal, 2007). Whenever corpus weights are used, they were estimated on the designated Tune set using the phrase-based MT sys5The brevity penalty was calculated using the formula in the original IBM paper, rather than the more recent definition implemented in the NIST mteval-v11b.pl script. tem. Only the collection and genre ids were used as sentence features in order to estimate the corpus weights. As mentioned in Section 4.1, the training bitext consists of 31 collections and 10 genres, so each training sentence was assigned a 41- dimensional binary vector indicating its particular collection/genre combi</context>
</contexts>
<marker>Lavie, Agarwal, 2007</marker>
<rawString>Alon Lavie and Abhaya Agarwal. 2007. METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 228–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming,</booktitle>
<pages>45--503</pages>
<contexts>
<context position="17040" citStr="Liu and Nocedal, 1989" startWordPosition="2788" endWordPosition="2791">ase, source phrase given target phrase, and lexical smoothing term. Additionally, each hypothesis specifies information about its derivation, i.e., which source-target phrase pairs it consists of. Therefore, given an N-best, we can identify the set of unique phrase pairs and use this information in order to perform a filtered accumulation of the statistics needed for calculating the derivative in Equation 8. This reduces the storage needed for the sufficient statistics significantly. Minimization of the expected TER of the Nbest hypotheses was performed using the limitedmemory BFGS algorithm (Liu and Nocedal, 1989). Typically, the parameter vector λ required about 30 iterations of LBFGS to converge. Since the N-best provides only a limited representation of the MT hypothesis search space, we regenerated the N-best after every 30 iterations 2In the general case where Lsj includes other translation scores, e.g., lexical translation probabilities, the derivative ∂,Xmwill have to include additional terms. as eγLsj 711 ! XS XNs 1 a lnpsj psj Esj (4) aλ =1 rs s=1 j=1 PS s aT aT alnpsj aλ aλ = XS s=1 alnpsj XNs j=1 alnpsj = XNs a ln psj aLsm aLsj − XNs psm aLsm! aλ m=1 aλ = lr aλ m=1 (5) aLsm aλ aLsm aλ X= (sk</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45:503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yajuan Lu</author>
<author>Jin Huang</author>
<author>Qun Liu</author>
</authors>
<title>Improving statistical machine translation performance by training data selection and optimization.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>343--350</pages>
<contexts>
<context position="5361" citStr="Lu et al. (2007)" startWordPosition="838" endWordPosition="841">entify likely translations. The filtering may be viewed as a data selection where poor quality translation are discarded before word alignment. Yasuda et al. (2008) selected subsets of an existing parallel corpus to match the domain of the test set. The discarded sentence pairs may be valid translations but they do not necessarily improve the translation quality on the test domain. Mandal et al. (2008) used active learning to select suitable training data for human translation. Hildebrand et al. (2005) selected comparable sentences from parallel corpora using information retrieval techniques. Lu et al. (2007) proposed weighting comparable portions of the parallel text before word alignment based on information retrieval. The relevant portions of the parallel text were given a higher integer weight in GIZA++ word alignment. Similar effect may be achieved by replicating the relevant subset in the training data. Lu et al. (2007) also proposed training adapted translation models which were interpolated with a model trained on the entire parallel text. Snover et al. (2008) used cross-lingual information retrieval to identify possible bias-rules to improve the coverage on the source side. These rules ma</context>
</contexts>
<marker>Lu, Huang, Liu, 2007</marker>
<rawString>Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving statistical machine translation performance by training data selection and optimization. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 343–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arindam Mandal</author>
<author>Dimitra Vergyri</author>
<author>Wen Wang</author>
<author>Jing Zheng</author>
</authors>
<title>Andreas Stolcke, Gokhan Tur, Dilek Hakkani-T¨ur, and Necip Fazil Ayan.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second IEEE/ACL Spoken Language Technology Workshop,</booktitle>
<pages>261--264</pages>
<contexts>
<context position="5150" citStr="Mandal et al. (2008)" startWordPosition="808" endWordPosition="811">ion, on the other hand, may boost the weight of the more relevant phrasepairs or introduce translations for unseen source phrases. Resnik and Smith (2003) mined parallel text from the web using various filters to identify likely translations. The filtering may be viewed as a data selection where poor quality translation are discarded before word alignment. Yasuda et al. (2008) selected subsets of an existing parallel corpus to match the domain of the test set. The discarded sentence pairs may be valid translations but they do not necessarily improve the translation quality on the test domain. Mandal et al. (2008) used active learning to select suitable training data for human translation. Hildebrand et al. (2005) selected comparable sentences from parallel corpora using information retrieval techniques. Lu et al. (2007) proposed weighting comparable portions of the parallel text before word alignment based on information retrieval. The relevant portions of the parallel text were given a higher integer weight in GIZA++ word alignment. Similar effect may be achieved by replicating the relevant subset in the training data. Lu et al. (2007) also proposed training adapted translation models which were inte</context>
</contexts>
<marker>Mandal, Vergyri, Wang, Zheng, 2008</marker>
<rawString>Arindam Mandal, Dimitra Vergyri, Wen Wang, Jing Zheng, Andreas Stolcke, Gokhan Tur, Dilek Hakkani-T¨ur, and Necip Fazil Ayan. 2008. Efficient data selection for machine translation. In Proceedings of the Second IEEE/ACL Spoken Language Technology Workshop, pages 261–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="25824" citStr="Och and Ney, 2003" startWordPosition="4241" endWordPosition="4244">oding with the same 3-gram LM, generates N-best of unique translation hypotheses, and then rescores them using a large, unpruned 5-gram LM in order to select the best scoring translation. It is worth mentioning that this hierarchical MT system provides a very strong baseline; it achieves a casesensitive BLEU score of 52.20 on the newswire portion of the NIST MT08 evaluation set, which is similar to the score of the second-best system that participated in the unconstrained data track of the NIST MT08 evaluation. Both types of models were trained on the same word alignments generated by GIZA++ (Och and Ney, 2003). 5 Results In this section we report results on the Arabic newswire, web, and audio development sets, using both phrase-based and hierarchical MT systems, in terms of TER, BLEU5, and METEOR (Lavie and Agarwal, 2007). Whenever corpus weights are used, they were estimated on the designated Tune set using the phrase-based MT sys5The brevity penalty was calculated using the formula in the original IBM paper, rather than the more recent definition implemented in the NIST mteval-v11b.pl script. tem. Only the collection and genre ids were used as sentence features in order to estimate the corpus wei</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="6403" citStr="Och, 2003" startWordPosition="1004" endWordPosition="1005"> parallel text. Snover et al. (2008) used cross-lingual information retrieval to identify possible bias-rules to improve the coverage on the source side. These rules may cover source phrases for which no translations were learned from the available parallel text. Koehn and Schroeder (2007) described a procedure for domain adaptation that was using two translation models in decoding, one trained on in-domain data and the other on out-of-domain data. Phrase translation scores from the two models where combined in a log-linear fashion, with weights estimated based on minimum error rate training (Och, 2003) on a designated tuning set. The method described in this paper can also be viewed as data filtering or (static) translation adaptation, but it has the following advantages over previously published techniques: 1. The estimated corpus weights are discriminative and are computed so as to directly optimize an MT performance metric on a pre-defined development set. Unlike the domain adaptation technique in (Koehn and Schroeder, 2007), which also estimates the adaptation parameters discriminatively, our proposed method does not require a manual specification of the in-domain and outof-domain train</context>
<context position="12916" citStr="Och, 2003" startWordPosition="2089" endWordPosition="2090">MT performance evaluation metric, such as TER or BLEU on the full translation search space. However, this is extremely computationally intensive for two reasons: (a) optimizing in the full translation search space requires a new decoding pass for each iteration of optimization; and (b) a direct optimization of TER or 1Note that in order to train such a GMM it may be necessary to first apply a decorrelating, dimensionality reducing, transform (e.g., principal component analysis) to the features. 710 BLEU requires the use of a derivative free, slowly converging optimization method such as MERT (Och, 2003), because these objective functions are not differentiable. In our case, for every parameter vector update we need to essentially retrain the translation model (reestimate the phrase and lexical translation probabilities based on the updated corpus weights), so the cost of each iteration is significantly higher than in a typical MERT application. For these reasons, in this work we chose to minimize the expected TER over a translation N-best on a designated tuning set, which is a continuous and differentiable function and can be optimized with standard gradient descent methods in a small number</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ostendorf</author>
<author>A Kannan</author>
<author>S Austin</author>
<author>O Kimball</author>
<author>R Schwartz</author>
<author>J R Rohlicek</author>
</authors>
<title>Integration of diverse recognition methodologies through reevaluation of nbest sentence hypotheses.</title>
<date>1991</date>
<booktitle>In Proceedings of the DARPA Workshop on Speech and Natural Language,</booktitle>
<pages>83--87</pages>
<contexts>
<context position="19010" citStr="Ostendorf et al., 1991" startWordPosition="3131" endWordPosition="3134">lexical translation probabilities. 4. Decode the tuning set, generating N-best. 5. Merge N-best hypotheses from previous iterations to current N-best. 6. Tune decoding weights so as to minimize TER on merged N-best, using a derivative free optimization method. In our case, we used Powell’s algorithm (Powell, 1964) modified by Brent as described in (Brent, 1973) 3. 7. Identify set of unique source-target phrase pairs in merged N-best. 8. Extract sufficient statistics from training data for all phrases identified in step 7. 3This method was first used for N-best based parameter optimization in (Ostendorf et al., 1991). 9. Run the LBFGS algorithm to minimize the expected TER in the merged N-best, using the derivative equations described previously. 10. Assign a weight to each training sentence based on the λ values optimized in 9. 11. Go to step 3. Typically, the corpus weights converge in about 4-5 main iterations. The calculation of the derivative is parallelized to speed up computation, requiring about 10 minutes per iteration of LBFGS. 4 Experimental Setup In this section we describe the setup that was used for all experiments reported in this paper. Specifically, we provide details about the training d</context>
</contexts>
<marker>Ostendorf, Kannan, Austin, Kimball, Schwartz, Rohlicek, 1991</marker>
<rawString>M. Ostendorf, A. Kannan, S. Austin, O. Kimball, R. Schwartz, and J. R. Rohlicek. 1991. Integration of diverse recognition methodologies through reevaluation of nbest sentence hypotheses. In Proceedings of the DARPA Workshop on Speech and Natural Language, pages 83–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="3603" citStr="Papineni et al., 2002" startWordPosition="559" endWordPosition="562"> The mapping from features to weights can be implemented via any differentiable function, but in our experiments we used a simple perceptron. Sentence weights estimated in this fashion are applied directly to the phrase and lexical counts unlike any previously published method to the author’s knowledge. The tuning framework is developed for phrase-based SMT models, but the tuned weights are also applicable to the training of a hierarchical model. In cases where the tuning set used for corpus weight estimation is a close match to the test set, this method yields significant gains in TER, BLEU (Papineni et al., 2002), and METEOR (Lavie and Agarwal, 2007) scores over a state-of-the-art hierarchical baseline. The paper is organized as follows. Related work on data selection, data weighting, and model adaptation is presented in Section 2. The corpus weight 708 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 708–717, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP approach and estimation algorithm are described in Section 3. Experimental evaluation of the approach is presented in Sections 4 and 5. Section 6 concludes the paper with a few directions for future wor</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J D Powell</author>
</authors>
<title>An efficient method for finding the minimum of a function of several variables without calculating derivatives. The Computer Journal,</title>
<date>1964</date>
<pages>155--162</pages>
<contexts>
<context position="18702" citStr="Powell, 1964" startWordPosition="3083" endWordPosition="3084">1. Initialize parameter vector λ to small random values, so that all training sentences receive approximately equal weights. 2. Initialize phrase-based MT decoding weights to previously tuned values. 3. Perform weighted phrase rule extraction as described in Equation 1, to estimate the phrase and lexical translation probabilities. 4. Decode the tuning set, generating N-best. 5. Merge N-best hypotheses from previous iterations to current N-best. 6. Tune decoding weights so as to minimize TER on merged N-best, using a derivative free optimization method. In our case, we used Powell’s algorithm (Powell, 1964) modified by Brent as described in (Brent, 1973) 3. 7. Identify set of unique source-target phrase pairs in merged N-best. 8. Extract sufficient statistics from training data for all phrases identified in step 7. 3This method was first used for N-best based parameter optimization in (Ostendorf et al., 1991). 9. Run the LBFGS algorithm to minimize the expected TER in the merged N-best, using the derivative equations described previously. 10. Assign a weight to each training sentence based on the λ values optimized in 9. 11. Go to step 3. Typically, the corpus weights converge in about 4-5 main </context>
</contexts>
<marker>Powell, 1964</marker>
<rawString>M. J. D. Powell. 1964. An efficient method for finding the minimum of a function of several variables without calculating derivatives. The Computer Journal, pages 155–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Noah A Smith</author>
</authors>
<title>The web as a parallel corpus.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="4684" citStr="Resnik and Smith (2003)" startWordPosition="731" endWordPosition="734">. Experimental evaluation of the approach is presented in Sections 4 and 5. Section 6 concludes the paper with a few directions for future work. 2 Related Work Previous work related to corpus weighting may be split into three categories: data selection, data weighting, and translation model adaptation. The first two approaches may improve the quality of the word alignment and prevent phrase-pairs which are less useful for the domain to be learned. The model adaptation, on the other hand, may boost the weight of the more relevant phrasepairs or introduce translations for unseen source phrases. Resnik and Smith (2003) mined parallel text from the web using various filters to identify likely translations. The filtering may be viewed as a data selection where poor quality translation are discarded before word alignment. Yasuda et al. (2008) selected subsets of an existing parallel corpus to match the domain of the test set. The discarded sentence pairs may be valid translations but they do not necessarily improve the translation quality on the test domain. Mandal et al. (2008) used active learning to select suitable training data for human translation. Hildebrand et al. (2005) selected comparable sentences f</context>
</contexts>
<marker>Resnik, Smith, 2003</marker>
<rawString>Philip Resnik and Noah A. Smith. 2003. The web as a parallel corpus. Computational Linguistics, 29(3):349–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>577--585</pages>
<contexts>
<context position="24959" citStr="Shen et al., 2008" startWordPosition="4098" endWordPosition="4101">2359 3.94 3149 115700 3.67 Web 3278 99280 1.69 4425 125795 2.08 Audio BN 897 32990 1.00 1530 53067 1.00 Audio BC 765 24607 1.00 1416 44435 1.00 Table 2: Characteristics of the tuning (Tune) and validation (Test) sets used for development on Arabic newswire, web, and audio. The audio sets include material from both broadcast news and broadcast conversations. mance by genre. 4.3 MT Systems Experiments were performed using two types of statistical MT systems: a phrase-based system, similar to Pharaoh (Koehn, 2004), and a stateof-the-art, hierarchical string-to-dependency-tree system, similar to (Shen et al., 2008). The phrase-based MT system employs a pruned 3-gram LM in decoding, and can optionally generate N-best unique translation hypotheses which are used to estimate the corpus weights, as described in Section 3. The hierarchical MT system performs decoding with the same 3-gram LM, generates N-best of unique translation hypotheses, and then rescores them using a large, unpruned 5-gram LM in order to select the best scoring translation. It is worth mentioning that this hierarchical MT system provides a very strong baseline; it achieves a casesensitive BLEU score of 52.20 on the newswire portion of t</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 577–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciula</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="2510" citStr="Snover et al., 2006" startWordPosition="380" endWordPosition="383"> in-domain broadcast news/conversational data. In this case, it would be desirable to down-weigh the out-of-domain data relative to the in-domain data during the rule extraction and probability estimation. Similarly, it would be good to assign a lower weight to data of low quality (e.g., poorly aligned or incorrectly translated sentences) relative to data of high quality. In this paper, we describe a novel discriminative training method that can be used to estimate a weight for each sentence in the training bitext so as to optimize an objective function – expected translation edit rate (TER) (Snover et al., 2006) – on a held-out development set. The training bitext typically consists of millions of (parallel) sentences, so in order to ensure robust estimation we express each sentence weight as a function of sentencelevel features, and estimate the parameters of this mapping function instead. Sentence-level features may include the identifier of the collection or genre that the sentence belongs to, the number of tokens in the source or target side, alignment information, etc. The mapping from features to weights can be implemented via any differentiable function, but in our experiments we used a simple</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciula, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciula, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Language and translation model adaptation using comparable corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>857--866</pages>
<contexts>
<context position="5829" citStr="Snover et al. (2008)" startWordPosition="913" endWordPosition="916">human translation. Hildebrand et al. (2005) selected comparable sentences from parallel corpora using information retrieval techniques. Lu et al. (2007) proposed weighting comparable portions of the parallel text before word alignment based on information retrieval. The relevant portions of the parallel text were given a higher integer weight in GIZA++ word alignment. Similar effect may be achieved by replicating the relevant subset in the training data. Lu et al. (2007) also proposed training adapted translation models which were interpolated with a model trained on the entire parallel text. Snover et al. (2008) used cross-lingual information retrieval to identify possible bias-rules to improve the coverage on the source side. These rules may cover source phrases for which no translations were learned from the available parallel text. Koehn and Schroeder (2007) described a procedure for domain adaptation that was using two translation models in decoding, one trained on in-domain data and the other on out-of-domain data. Phrase translation scores from the two models where combined in a log-linear fashion, with weights estimated based on minimum error rate training (Och, 2003) on a designated tuning se</context>
<context position="8034" citStr="Snover et al. (2008)" startWordPosition="1254" endWordPosition="1257">e corpus, if it determines that it helps improve MT performance. 3. The framework used for estimating the corpus weights can be easily extended to support discriminative alignment link-level weights, thus allowing the system to automatically identify which portions of the training sentences are most useful. Naturally, as with any method, the proposed technique has certain limitations. Specifically, it is only concerned with influencing the translation rule probabilities via the corpus weights; it does not change the set of rules extracted. Thus, it is unable to add new translation rules as in Snover et al. (2008). Also, it can potentially lead to parameter over-fitting, especially if the function that maps sentence features to weights is complex and based on a large number of parameters, or if the development set used for estimating the mapping function does not match the characteristics of the test set. 709 3 Corpus Weights Estimation 3.1 Feature Extraction The purpose of feature extraction is to identify, for each sentence in the parallel training data, a set of features that can be useful in estimating a weight that is correlated with quality or relevance to the MT task at hand. Starting from sente</context>
</contexts>
<marker>Snover, Dorr, Schwartz, 2008</marker>
<rawString>Matthew Snover, Bonnie Dorr, and Richard Schwartz. 2008. Language and translation model adaptation using comparable corpora. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 857–866.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiji Yasuda</author>
<author>Ruiqiang Zhang</author>
<author>Hirofumi Yamamoto</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Method of selecting training data to build a compact and efficient translation model.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third International Joint Conference on Natural Language Processing,</booktitle>
<volume>volume II,</volume>
<pages>655--660</pages>
<contexts>
<context position="4909" citStr="Yasuda et al. (2008)" startWordPosition="766" endWordPosition="769">tegories: data selection, data weighting, and translation model adaptation. The first two approaches may improve the quality of the word alignment and prevent phrase-pairs which are less useful for the domain to be learned. The model adaptation, on the other hand, may boost the weight of the more relevant phrasepairs or introduce translations for unseen source phrases. Resnik and Smith (2003) mined parallel text from the web using various filters to identify likely translations. The filtering may be viewed as a data selection where poor quality translation are discarded before word alignment. Yasuda et al. (2008) selected subsets of an existing parallel corpus to match the domain of the test set. The discarded sentence pairs may be valid translations but they do not necessarily improve the translation quality on the test domain. Mandal et al. (2008) used active learning to select suitable training data for human translation. Hildebrand et al. (2005) selected comparable sentences from parallel corpora using information retrieval techniques. Lu et al. (2007) proposed weighting comparable portions of the parallel text before word alignment based on information retrieval. The relevant portions of the para</context>
</contexts>
<marker>Yasuda, Zhang, Yamamoto, Sumita, 2008</marker>
<rawString>Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto, and Eiichiro Sumita. 2008. Method of selecting training data to build a compact and efficient translation model. In Proceedings of the Third International Joint Conference on Natural Language Processing, volume II, pages 655–660.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>