<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000121">
<title confidence="0.996206">
Bayesian Checking for Topic Models
</title>
<author confidence="0.998919">
David Mimno David Blei
</author>
<affiliation confidence="0.99215">
Department of Computer Science Department of Computer Science
Princeton University Princeton, NJ 08540 Princeton University Princeton, NJ 08540
</affiliation>
<email confidence="0.999543">
mimno@cs.princeton.edu blei@cs.princeton.edu
</email>
<sectionHeader confidence="0.99563" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997789363636364">
Real document collections do not fit the inde-
pendence assumptions asserted by most statis-
tical topic models, but how badly do they vi-
olate them? We present a Bayesian method
for measuring how well a topic model fits a
corpus. Our approach is based on posterior
predictive checking, a method for diagnosing
Bayesian models in user-defined ways. Our
method can identify where a topic model fits
the data, where it falls short, and in which di-
rections it might be improved.
</bodyText>
<sectionHeader confidence="0.998979" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996717472727273">
Probabilistic topic models are a suite of machine
learning algorithms that decompose a corpus into
a set of topics and represent each document with a
subset of those topics. The inferred topics often cor-
respond with the underlying themes of the analyzed
collection, and the topic modeling algorithm orga-
nizes the documents according to those themes.
Most topic models are evaluated by their predic-
tive performance on held out data. The idea is that
topic models are fit to maximize the likelihood (or
posterior probability) of a collection of documents,
and so a good model is one that assigns high likeli-
hood to a held out set (Blei et al., 2003; Wallach et
al., 2009).
But this evaluation is not in line with how
topic models are frequently used. Topic mod-
els seem to capture the underlying themes of a
collection—indeed the monicker “topic model” is
retrospective—and so we expect that these themes
are useful for exploring, summarizing, and learning
227
about its documents (Mimno and McCallum, 2007;
Chang et al., 2009). In such exploratory data anal-
ysis, however, we are not concerned with the fit to
held out data.
In this paper, we develop and study new methods
for evaluating topic models. Our methods are based
on posterior predictive checking, which is a model
diagnosis technique from Bayesian statistics (Rubin,
1984; Gelman et al., 1996). The goal of a posterior
predictive check (PPC) is to assess the validity of a
Bayesian model without requiring a specific alterna-
tive model. Given data, we first compute a posterior
distribution over the latent variables. Then, we esti-
mate the probability of the observed data under the
data-generating distribution that is induced by the
posterior (the “posterior predictive distribution”). A
data set that is unlikely calls the model into ques-
tion, and consequently the posterior. PPCs can show
where the model fits and doesn’t fit the observations.
They can help identify the parts of the posterior that
are worth exploring.
The key to a posterior predictive check is the dis-
crepancy function. This is a function of the data that
measures a property of the model which is impor-
tant to capture. While the model is often chosen
for computational reasons, the discrepancy function
might capture aspects of the data that are desirable
but difficult to model. In this work, we will design
a discrepancy function to measure an independence
assumption that is implicit in the modeling assump-
tions but is not enforced in the posterior. We will
embed this function in a posterior predictive check
and use it to evaluate and visualize topic models in
new ways.
</bodyText>
<note confidence="0.9483335">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 227–237,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999983818181818">
Specifically, we develop discrepancy functions
for latent Dirichlet allocation (the simplest topic
model) that measure how well its statistical assump-
tions about the topics are matched in the observed
corpus and inferred topics. LDA assumes that each
observed word in a corpus is assigned to a topic, and
that the words assigned to the same topic are drawn
independently from the same multinomial distribu-
tion (Blei et al., 2003). For each topic, we mea-
sure the whether this assumption holds by comput-
ing the mutual information between the words as-
signed to that topic and which document each word
appeared in. If the assumptions hold, these two vari-
ables should be independent: low mutual informa-
tion indicates that the assumptions hold; high mu-
tual information indicates a mismatch to the model-
ing assumptions.
We embed this discrepancy in a PPC and study
it in several ways. First, we focus on topics that
model their observations well; this helps separate
interpretable topics from noisy topics (and “boiler-
plate” topics, which exhibit too little noise). Sec-
ond, we focus on individual terms within topics; this
helps display a model applied to a corpus, and under-
stand which terms are modeled well. Third, we re-
place the document identity with an external variable
that might plausibly be incorporated into the model
(such as time stamp or author). This helps point the
modeler towards the most promising among more
complicated models, or save the effort in fitting one.
Finally, we validate this strategy by simulating data
from a topic model, and assessing whether the PPC
“accepts” the resulting data.
</bodyText>
<sectionHeader confidence="0.960062" genericHeader="method">
2 Probabilistic Topic Modeling
</sectionHeader>
<bodyText confidence="0.999090928571429">
Probabilistic topic models are statistical models of
text that assume that a small number of distributions
over words, called “topics,” are used to generate the
observed documents. One of the simplest topic mod-
els is latent Dirichlet allocation (LDA) (Blei et al.,
2003). In LDA, a set of K topics describes a cor-
pus; each document exhibits the topics with different
proportions. The words are assumed exchangeable
within each document; the documents are assumed
exchangeable within the corpus.
More formally, let O1, ... , OK be K topics, each
of which is a distribution over a fixed vocabulary.
For each document, LDA assumes the following
generative process
</bodyText>
<listItem confidence="0.98910525">
1. Choose topic proportions Bd — Dirichlet(α).
2. For each word
(a) Choose topic assignment zd,n — B.
(b) Choose word wd,n — Ozd,n.
</listItem>
<bodyText confidence="0.99849947826087">
This process articulates the statistical assumptions
behind LDA: Each document is endowed with its
own set of topic proportions Bd, but the same set of
topics O1:K governs the whole collection.
Notice that the probability of a word is indepen-
dent of its document Bd given its topic assignment
zd,n (i.e., wd,n 11 Bd  |zd,n). Two documents might
have different overall probabilities of containing a
word from the “vegetables” topic; however, all the
words in the collection (regardless of their docu-
ments) drawn from that topic will be drawn from the
same multinomial distribution.
The central computational problem for LDA is
posterior inference. Given a collection of docu-
ments, the problem is to compute the conditional
distribution of the hidden variables—the topics Ok,
topic proportions Bd, and topic assignments zd,n.
Researchers have developed many algorithms for
approximating this posterior, including sampling
methods (Griffiths and Steyvers, 2004) (used in this
paper), variational methods (Blei et al., 2003), dis-
tributed variants (Asuncion et al., 2008), and online
algorithms (Hoffman et al., 2010).
</bodyText>
<sectionHeader confidence="0.920476" genericHeader="method">
3 Checking Topic Models
</sectionHeader>
<bodyText confidence="0.999802285714286">
Once approximated, the posterior distribution is
used for the task at hand. Topic models have been
applied to many tasks, such as classification, predic-
tion, collaborative filtering, and others. We focus
on using them as an exploratory tool, where we as-
sume that the topic model posterior provides a good
decomposition of the corpus and that the topics pro-
vide good summaries of the corpus contents.
But what is meant by “good”? To answer this
question, we turn to Bayesian model checking (Ru-
bin, 1981; Gelman et al., 1996). The goal of
Bayesian model checking is to assess whether the
observed data matches the modeling assumptions in
the directions that are important to the analysis. The
</bodyText>
<page confidence="0.994157">
228
</page>
<figure confidence="0.999568494117647">
Topic850
Topic371
Topic760
10
12
14
10
12
14
10
12
14
2
4
6
8
2
4
6
8
2
4
6
8
Times
listing
selective
noteworthy
critics
weekend
Broadway
tickets
highly
recommended
denotes
booth
Iraq
Iraqi
Tickets
Street
TKTS
1 2 3 4
Hussein
Baghdad
Saddam
Shiite
governme nt
al
Topic87
Roberts
Topic628
Fort
Wo
Iraqis
Sunni
Kurdish
forces
country
military
troops
Grant
rth
Burke
Hunt
Kravis
Bass
Kohlberg
Grace
Rothschild
Baron
Borden
Texas
William
Tickets
Through
Street
Road
Saturdays
Sundays
New
Fridays
Jersey
Hours
Free
Tuesdays
MUSEUM
Thursdays
THEATER
Holy
da
1 2 3 4
Four
Freeman
Seasons
Da
Vinci
Code
Topic178
agency
safety
report
Federal
Administration
problems
investigation
Safety
violations
federal
failed
inspector
review
Topic750
Thomson
Wolff
Leonardo
Brown
Three
Dan
Cliff
general
department
Week
book
Warner
sales
List
Weeks
woman
bookstores
death
indicates
Advice
Putnam
report
New
OF
1 2 3 4
job
Leon
Levy
Topic632
Topic274
Hess
Bard
LEVY
Botstein
Atlas
jobs
working
office
business
career
wo rked
employees
hired
boss
manager
Panetta
David
Amerada
find
corporate
help
experienc
Shelby
Norma
HES
N
Rank
Score
</figure>
<figureCaption confidence="0.916271833333333">
Figure 1: Visualization of variability within topics. Nine randomly selected topics from the New York Times with
low (top row), medium (middle row) and high (bottom row) mutual information between words and documents. The
y-axis shows term rank within the topic, with size proportional to log probability. The x-axis represents divergence
from the multinomial assumption for each word: terms that are uniformly distributed across documents are towards
the left, while more specialized terms are to the right. Triangles represent real values, circles represent 20 replications
of this same plot from the posterior of the model.
</figureCaption>
<page confidence="0.988158">
229
</page>
<bodyText confidence="0.9999834">
intuition is that only when satisfied with the model
should the modeler use the posterior to learn about
her data. In complicated Bayesian models, such as
topic models, Bayesian model checking can point to
the parts of the posterior that better fit the observed
data set and are more likely to suggest something
meaningful about it.
In particular, we will develop posterior predictive
checks (PPC) for topic models. In a PPC, we spec-
ify a discrepancy function, which is a function of
the data that measures an important property that we
want the model to capture. We then assess whether
the observed value of the function is similar to val-
ues of the function drawn from the posterior, through
the distribution of the data that it induces. (This dis-
tribution of the data is called the “posterior predic-
tive distribution.”)
An innovation in PPCs is the realized discrepancy
function (Gelman et al., 1996), which is a function
of the data and any hidden variables that are in the
model. Realized discrepancies induce a traditional
discrepancy by marginalizing out the hidden vari-
ables. But they can also be used to evaluate assump-
tions about latent variables in the posterior, espe-
cially when combined with techniques like MCMC
sampling that provide realizations of them. In topic
models, as we will see below, we use a realized dis-
crepancy to factor the observations and to check spe-
cific components of the model that are discovered by
the posterior.
</bodyText>
<subsectionHeader confidence="0.999155">
3.1 A realized discrepancy for LDA
</subsectionHeader>
<bodyText confidence="0.942753352941177">
Returning to LDA, we design a discrepancy func-
tion that checks the independence assumption of
words given their topic assignments. As we men-
tioned above, given the topic assignment z the word
w should be independent of its document θ. Con-
sider a decomposition of a corpus from LDA, which
assigns every observed word wd,n to a topic zd,n.
Now restrict attention to all the words assigned to the
kth topic and form two random variables: W are the
words assigned to the topic and D are the document
indices of the words assigned to that topic. If the
LDA assumptions hold then knowing W gives no
information about D because the words are drawn
independently from the topic.
We measure this independence with the mutual
information between W and D:1
MI(W, D  |k)
</bodyText>
<equation confidence="0.98739725">
P(w  |d, k)P (d  |k)
P(w, d  |k) log P (w  |k)P (d  |k)
N(w, d, k)log N(w, d, k)N(k) (1)
N(k) N(d, k)N(w, k)
</equation>
<bodyText confidence="0.999641028571428">
Where N(w, d, k) is the number of tokens of type
w in topic k in document d, with N(w, k) =
Ed N(w, d, k), N(d, k) = Ew N(w, d, k), and
N(k) = Ew,d N(w, d, k). This function mea-
sures the divergence between the joint distribution
over word and document index and the product of
the marginal distributions. In the limit of infinite
samples, independent random variables have mutual
information of zero, but we expect finite samples
to have non-zero values even for truly independent
variables. Notice that this is a realized discrepancy;
it depends on the latent assignments of observed
words to topics.
Eq. 1 is defined as a sum over a set of documents
and a set of words. We can rearrange this summa-
tion as a weighted sum of the instantaneous mutual
information between words and documents:
IMI(w, D  |k) = H(D|k) − H(D  |W = w, k).
(2)
This quantity can be understood by considering the
per-topic distribution of document labels, p(d|k).
This distribution is formed by normalizing the
counts of how many words assigned to topic k ap-
peared in each document. The first term of Eq. 2
is the entropy—some topics are evenly distributed
across many documents (high entropy); others are
concentrated in fewer documents (low entropy).
The second term conditions this distribution on
a particular word type w by normalizing the per-
document number of times w appeared in each doc-
ument (in topic k). If this distribution is close
to p(d|k) then H(D|W = w, k) will be close to
H(D|k) and IMI(w, D|k) will be low. If, on the
other hand, word w occurs many times in only a few
documents, it will have lower entropy over docu-
</bodyText>
<footnote confidence="0.945879333333333">
1There are other choices of discrepancies, such as word-
word point-wise mutual information scores (Newman et al.,
2010).
</footnote>
<figure confidence="0.948911166666667">
�=
w
�= �
w d
�
d
</figure>
<page confidence="0.978107">
230
</page>
<bodyText confidence="0.999971533333333">
ments than the overall distribution over documents
for the topic and IMI(w, D|k) will be high.
We illustrate this discrepancy in Figure 1, which
shows nine topics trained from the New York Times.2
Each row contains randomly selected topics from
low, middle, and high ranges of MI, respectively.
Each triangle represents a word. Its place on the y-
axis is its rank in the topic. Its place on the x-axis
is its IMI(w|k), with more uniformly distributed
words (low IMI) to the left and more specific words
(high IMI) to the right. (For now, ignore the other
points in this figure.) IMI varies between topics, but
tends to increase with rank as less frequent words
appear in fewer documents.
The discrepancy captures different kinds of struc-
ture in the topics. The top left topic represents for-
mulaic language, language that occurs verbatim in
many documents. In particular, it models the boil-
erplate text “Here is a selective listing by critics of
The Times of new or noteworthy...” Identifying re-
peated phrases is a common phenomenon in topic
models. Most words show lower than expected IMI,
indicating that word use in this topic is less vari-
able than data drawn from a multinomial distribu-
tion. The middle-left topic is an example of a good
topic, according to this discrepancy, which is related
to Iraqi politics. The bottom-left topic is an example
of the opposite extreme from the top-left. It shows
a loosely connected series of proper names with no
overall theme.
</bodyText>
<subsectionHeader confidence="0.996904">
3.2 Posterior Predictive Checks for LDA
</subsectionHeader>
<bodyText confidence="0.999808230769231">
Intuitively, the middle row of topics in Figure 1 are
the sort of topics we look for in a model, while the
top and bottom rows contain topics that are less use-
ful. Using a PPC, we can formally measure the dif-
ference between these topics. For each of the real
topics in Figure 1 we regenerated the same figure
20 times. We sampled new words for every token
from the posterior distribution of the topic, and re-
calculated the rank and IMI for each word. These
“shadow” figures are shown as gray circles. The
density of those circles creates a reference distribu-
tion indicating the expected IMI values at each rank
under the multinomial assumption.
</bodyText>
<footnote confidence="0.9912125">
2Details about the corpus and model fitting are in Section
4.2. Similar figures for two other corpora are in the supplement.
</footnote>
<bodyText confidence="0.994580979591837">
By themselves, IMI scores give an indication of
the distribution of a word between documents within
a topic: small numbers are better, large numbers in-
dicate greater discrepancy. These scores, however,
are based on the specific allocation of words to top-
ics. For example, lower-ranked, less frequent words
within a topic tend to have higher IMI scores than
higher-ranked, more frequent words. This difference
may be due to greater violation of multinomial as-
sumptions, but may also simply be due to smaller
sample sizes, as the entropy H(D|W = w, k) is es-
timated from fewer tokens. The reference distribu-
tions help distinguish between these two cases.
In more detail, we generate replications of the
data by considering a Gibbs sampling state. This
state assigns each observed word to a topic. We
first record the number of instances of each term as-
signed to each topic, N(w|k). Then for each word
wd,n in the corpus, we sample a new observed word
wrep
d,n where P(w) ∝ N(w|zd,n). (We did not use
smoothing parameters.) Finally, we recalculate the
mutual information and instantenous mutual infor-
mation for each topic.
In the top-left topic, most of the words have much
lower IMI than the word at the same rank in repli-
cations, indicating lower than expected variability.
The exception is the word Broadway, which is more
variable than expected. In the middle-left topic,
IMI for the words Iraqi and Baghdad occur within
the expected range. These words fit the multino-
mial assumption: any word assigned to this topic
is equally likely to be Iraqi. Values for the words
Shiite, Sunni, and Kurdish are more specific to par-
ticular documents than we expect under the model.
In the bottom-left topic, almost all words occur with
greater variability than expected. This topic com-
bines many terms with only coincidental similarity,
such as Mets pitcher Grant Roberts and the firm
Kohlberg Kravis Roberts.
Turning to an analysis of the full mutual infor-
mation, Figure 2 shows the three left-hand topics
from Figure 1: Weekend, Iraq, and Roberts. The
histogram represents MI scores for 100 replications
of the topic, rescaled to have mean zero and unit
variance. The observed value, also rescaled, and
the mean replicated value (set to zero) are shown
with vertical lines. The formulaic Weekend topic
has significantly lower than expected MI. The Iraq
</bodyText>
<page confidence="0.975994">
231
</page>
<bodyText confidence="0.981278297297297">
expected.
variance
into the model using Dirich-
let compound multinomial distributions (Doyle and
Elkan, 2009). If there is no pattern to the deviations
from multinomial word use across documents, this
method is the best we can do.
In many corpora, however, there are systematic
deviations that can be explained by additional vari-
ables. LDA is the simplest generative topic model,
and researchers have developed many variants of
LDA that account for a variety of variables that can
be found or measured with a corpus. Examples in-
clude models that account for time (Blei and Laf-
ferty, 2006), books
and McCallum, 2007),
and aspect or perspective (Mei and Zhai, 2006; Lin
et al., 2008; Paul et al., 2010). In this section, we
show how we can use the mutual information dis-
crepancy function of Equation 1 and PPCs to guide
our choice in which topic model to fit.
Greater deviance implies that a particular group-
ing better explains the variation in word use within
a topic. The discrepancy functions are large when
words appear more than expected in some groups
and less than expected in others. We know that
the individual documents show significantly more
variation than we expect from replications from the
posterior distribution. If we combine docu-
ments randomly in a meaningless grouping, such de-
viance should decrease, as differences between doc-
uments are
If a grouping of docu-
ments shows equal or greater deviation, we can as-
sume that that grouping is maintaining the underly-
ing structure of the systematic deviation from the
multinomial assumption, an
</bodyText>
<equation confidence="0.48987575">
(“burstiness”)
(Mimno
model’s
“smoothedout.”
</equation>
<bodyText confidence="0.5710515">
d that further modeling
or visualization using that grouping might be useful.
</bodyText>
<figure confidence="0.999717576923077">
Topic850
Deviance
−2002040
count
30
25
20
15
10
30
25
20
15
10
5
0
30
25
20
15
10
5
0
5
0
Topic628
</figure>
<figureCaption confidence="0.899564166666667">
er. Topic87
Figure 2: News: Observed topic scores (vertical lines)
relative to replicated scores, rescaled so that replica-
tions have zero mean and unit variance. The Weekend
topic (top) has lower than expected MI. The Iraq (mid-
dle) and Roberts (bottom) topics both have MI greater
than
and Roberts topics have significantly greater than
expected MI.
For most topics the actual discrepancy is outside
the range of any replicated discrepancies. In their
original formulation, PPCs prescribe computing a
</figureCaption>
<bodyText confidence="0.948937361111111">
tail probability of a replicated discrepancy being
greater than (or less than) the observed discrepancy
under the posterior predictive distribution. For ex-
ample if an observed value is greater than 70 of 100
replicated values, we report aPPC p-value of 0.7.
When the observed value is far outside the range
of any replicated values, as in Figure 2, that tail
probability will be degenerate at 0 or 1. So, we re-
port instead a deviance value, an alternative way of
comparing an observed value to a reference distri-
bution. We compute the distribution of the repli-
cated discrepancies and compute its standard devi-
ation. We then compute how many standard devia-
tions the observed discrepancy is from the mean of
the replicated discrepancies.
This score allows us to compare topics. The ob-
served value for the Weekend topic is 31.8 standard
deviations below the mean replicated value, and thus
has deviance of -31.8, which is lower than expected.
The Iraq topic has deviance of 16.8 and the Roberts
topic has deviance of 47.7. This matches our intu-
ition that the former topic is more useful than the
latt
4 Searching for Systematic Deviations
We demonstrated that the mutual information dis-
crepancy function can detect violations of multi-
nomial assumptions, in which instances of a term
in a given topic are not independently distributed
among documents. One way to address this lack
of fit is to encode document-level extra-multinomial
4.1 PPCs for systematic discrepancy
The idea is that the words assigned to a topic should
be independent of both document and any other vari-
able that might be associated with the document. We
simply replace the document index d with an
other
</bodyText>
<page confidence="0.940585">
232
</page>
<figure confidence="0.871459083333333">
Documents Months Desks
Iraq Iraq Iraq Iraqi
Iraqi Iraqi
Hussein Hussein Baghdad Hussein Baghdad
Baghdad Saddam Saddam
Saddam
Shiite government _ Shiite government Shiite government
al Iraqis Sunni al Iraqis al Sunni
Sunni Iraqis
forces Kurdish forces Kurdish forces Kurdish
country country country military
military military
troops leaders leaders troops city troops leaders
city city
Kur Kurds security Sadr Kurds security
securi Sadr
5
Rank 10
15
20
0.0 0.5 1.0 1.5 2.0 2.5
0.0 0.5 1.0 1.5 2.0 2.5
0.0 0.5 1.0 1.5 2.0 2.5
Score
</figure>
<figureCaption confidence="0.990310333333333">
Figure 3: Groupings decrease MI, but values are still larger than expected. Three ways of grouping words in a
topic from the New York Times. The word leaders varies more between desks than by time, while Sadr varies more by
time than desk.
</figureCaption>
<bodyText confidence="0.999786791666667">
variable g in the discrepancy. For example, the New
York Times articles are each associated with a par-
ticular news desk and also associated with a time
stamp. If the topic modeling assumptions hold, the
words are independent of both these variables. If we
see a significant discrepancy relative to a grouping
defined by a metadata feature, this systematic vari-
ability suggests that we might want to take that fea-
ture into account in the model.
Let G be a set of groups and let γ ∈ GD be
a grouping of D documents. Let N(w, g, k) =
Ed N(w, d, k)Iγd=g, that is, the number of words of
type w in topic k in documents in group g, and define
the other count variables similarly. We can now sub-
stitute these group-specific counts for the document-
specific counts in the discrepancy function in Eq.
1. Note that the previous discrepancy functions are
equivalent to a trivial grouping, in which each docu-
ment is the only member of its own group. In the fol-
lowing experiments we explore groupings by pub-
lished volume, blog, preferred political candidate,
and newspaper desk, and evaluate the effect of those
groupings on the deviation between mean replicated
values and observed values of those functions.
</bodyText>
<subsectionHeader confidence="0.999745">
4.2 Case studies
</subsectionHeader>
<bodyText confidence="0.99982">
We analyze three corpora, each with its own meta-
data: the New York Times Annotated Corpus (1987–
2007)3, the CMU 2008 political blog corpus (Eisen-
stein and Xing, 2010), and speeches from the British
</bodyText>
<footnote confidence="0.689352">
3http://www.ldc.upenn.edu
</footnote>
<bodyText confidence="0.9650455">
House of Commons from 1830–1891.4 Descriptive
statistics are presented in Table 1. The realization
is represented by a single Gibbs sampling state after
1000 iterations of Gibbs sampling.
</bodyText>
<tableCaption confidence="0.990214">
Table 1: Statistics for models used as examples.
</tableCaption>
<table confidence="0.99876825">
Name Docs Tokens Vocab Topics
News 1.8M 76M 121k 1000
Blogs 13k 2.2M 90k 100
Parliament 540k 55M 52k 300
</table>
<bodyText confidence="0.976865833333334">
New York Times articles. Figure 3 shows three
groupings of words for the middle-left topic in Fig-
ure 1: by document, by month of publication (e.g.
May of 2005), and by desk (e.g. Editorial, Foreign,
Financial). Instantaneous mutual information values
are significantly smaller for the larger groupings, but
the actual values are still larger than expected under
the model. We are interested in measuring the de-
gree to which word usage varies within topics as a
function of both time and the perspective of the ar-
ticle. For example, we may expect that word choice
may differ between opinion articles, which overtly
reflect an author’s views, and news articles, which
take a more objective, factual approach.
We summarize each grouping by plotting the dis-
tribution of deviance scores for all topics. Results
for all 1000 topics grouped by documents, months,
and desks are shown in Figure 4.
</bodyText>
<footnote confidence="0.97991">
4http://www.hansard-archive.parliament.uk/
</footnote>
<page confidence="0.997642">
233
</page>
<figure confidence="0.87197">
Deviance
</figure>
<figureCaption confidence="0.953894">
Figure 4: News: Lack of fit correlates best with desks.
</figureCaption>
<bodyText confidence="0.994233870967742">
We calculate the number of standard deviations between
the mean replicated discrepancy and the actual discrep-
ancy for each topic under three groupings. Boxes repre-
sent typical ranges, points represent outliers.
Finally, we can analyze how individual words in-
teract with groupings like time or desk. Figure 5
breaks down the per-word discrepancy shown in Fig-
ure 3 by month, for the words with the largest overall
discrepancy. Kurdish is prominent during the Gulf
War and the 1996 cruise missile strikes, but is less
significant during the Iraq War. Individuals (Hus-
sein, Sadr, and Maliki) move on and off the stage.
Political blogs. The CMU 2008 political blog cor-
pus consists of six blogs, three of which supported
Barack Obama and three of which supported John
McCain. This corpus has previously been consid-
ered in the context of aspect-based topic models
(Ahmed and Xing, 2010) that assign distinct word
distributions to liberal and conservative bloggers. It
is reasonable to expect that blogs with different po-
litical leanings will use measurably different lan-
guage to describe the same themes, suggesting that
there will be systematic deviations from a multino-
mial hypothesis of exchangeability of words within
topics. Indeed, Ahmed and Xing obtained improved
results with such a model. Figure 6 shows the dis-
tribution of standard deviations from the mean repli-
cated value for a set of 150 topics grouped by doc-
ument, blog, and preferred candidate. Deviance is
greatest for blogs, followed by candidates and then
documents.
</bodyText>
<figure confidence="0.999136111111111">
Deviance
0 100 200 300 400
Grouping
Documents
Months
Desks
●●●
● ● ● ● ● ● ●
● ● ● ●
●
● ● ● ● ● ● ●●
● ● ● ● ● ●
● ●
● ● ● ● ● ●
●
●●●● ●
●● ● ● ● ●
● ● ●
● ● ● ● ●●●
●●●●
●
Kurdish
0.0015
0.0010
0.0005
0.0000
Hussein
Sunni
Maliki
Sadr
0.0015
0.0010
0.0005
0.0000
6e−04
4e−04
2e−04
Score
0e+00
0.0025
0.0020
0.0015
0.0010
0.0005
0.0000
68e−0
e044
4e−04
2e−04
0e+00
0 100 200 300 400
Grouping
Candidates
Documents
Blogs
●
● ●
●● ●●
●● ●
●
Shiite
1987 1992 1997 2002 2007
Month
</figure>
<figureCaption confidence="0.95683475">
Figure 5: News: Events change word distributions.
Words with the largest MI from a topic on Iraq’s gov-
ernment are shown, with individual scores grouped by
month.
</figureCaption>
<figure confidence="0.9943634">
6e−04
4e−04
2e−04
0e+00
−2e−04
</figure>
<figureCaption confidence="0.99812975">
Figure 6: Blogs: Lack of fit correlates more with blog
than preferred candidate. Grouping by preferred can-
didate has only slightly higher average deviance than by
documents, but the variance is greater.
</figureCaption>
<bodyText confidence="0.999856285714286">
Grouping by blogs appears to show greater de-
viance from mean replicated values than group-
ing by candidates, indicating that there is fur-
ther structure in word choice beyond a simple lib-
eral/conservative split. Are these results, however,
comparable? It may be that this difference is ex-
plained by the fact that there are six blogs and only
</bodyText>
<page confidence="0.991708">
234
</page>
<bodyText confidence="0.997123176470588">
two candidates. To determine whether this particular
assignment of documents to blogs is responsible for
the difference in discrepancy functions or whether
any such split would have greater deviance, we com-
pared random groupings to the real groupings and
recalculate the PPC. We generated 10 such group-
ings by permuting document blog labels and another
10 by permuting document candidate labels, each
time holding the topics fixed. The average number
of standard deviations across topics was 6.6 f 14.4
for permuted “candidates” compared to 37.9 f 39.2
for the real corpus, and 10.6 f 12.9 for permuted
“blogs” compared to 44.4 f 29.6 for real blogs.
British parliament proceedings. The parliament
corpus is divided into 305 volumes, each comprising
about three weeks of debates, with between 600 and
4000 speeches per session. In addition to volumes,
</bodyText>
<figure confidence="0.681408">
10 Prime Ministers were in office during this period.
Deviance
</figure>
<figureCaption confidence="0.976890333333333">
Figure 7: Parliament: Lack-of-fit correlates with time
(publication volume). Correlation with prime ministers
is not significantly better than with volume.
</figureCaption>
<bodyText confidence="0.999950647058824">
Grouping by prime minister shows greater av-
erage deviance than grouping by volumes, even
though there are substantially fewer divisions. Al-
though such results would need to be accompanied
by permutation experiments as in the blog corpus,
this methodology may be of interest to historians.
In order to provide insight into the nature of tem-
poral variation, we can group the terms in the sum-
mation in Equation 1 by word and rank the words by
their contribution to the discrepancy function. Fig-
ure 8 shows the most “mismatching” words for a
topic with the most probable words ships, vessels,
admiralty, iron, ship, navy, consistent with changes
in naval technology during the Victorian era (that
is, wooden ships to “iron clads”). Words that oc-
cur more prominently in the topic (ships, vessels)
are also variable, but more consistent across time.
</bodyText>
<figure confidence="0.674184">
Volume
</figure>
<figureCaption confidence="0.797793">
Figure 8: Parliament: iron-clads introduced in 1860s.
High probability words (ships, vessels) are variable, but
show less concentrated discrepancy than iron, wooden.
</figureCaption>
<sectionHeader confidence="0.943402" genericHeader="method">
5 Calibration on Synthetic Data
</sectionHeader>
<bodyText confidence="0.99997756">
A posterior predictive check asks “do observations
sampled from the learned model look like the origi-
nal data?” In the previous sections, we have consid-
ered PPCs that explore variability within a topic on
a per-word basis, measure discrepancy at the topic
level, and compare deviance over all topics between
groupings of documents. Those results show that
the PPC detects deviation from multinomial assump-
tions when it exists: as expected, variability in word
choice aligns with known divisions in corpora, for
example by time and author perspective. We now
consider the opposite direction. When documents
are generated from a multinomial topic model, PPCs
should not detect systematic deviation.
We must also distinguish between lack of fit due
to model misspecification and lack of fit due to ap-
proximate inference. In this section, we present syn-
thetic data experiments where the learned model is
precisely the model used to generate documents. We
show that there is significant lack of fit introduced
by approximate inference, which can be corrected
by considering only parts of the model that are well-
estimated.
We generated 10 synthetic corpora, each consist-
ing of 100,000 100-word documents, drawn from 20
</bodyText>
<figure confidence="0.994663166666667">
0 100 200 300 400
Grouping
Documents
Volumes
PMs
●●
iron
1830 1835 1840 1845 1850 1855 1860 1865 1870 1875 1880 1885 1890
Score
0.0015
0.0010
0.0005
0.0000
0.0015
0.0010
0.0005
0.0000
0.0015
0.0010
0.0005
0.0000
0.0015
0.0010
0.0005
0.0000
0.0015
0.0010
0.0005
0.0000
0.0015
0.0010
0.0005
0.0000
wooden
vessels
turret
clads
ships
235
All TopDocs TopWords TopWordsDocs
40
0
count
30
20
10
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
p
</figure>
<figureCaption confidence="0.891653666666667">
Figure 9: Replicating only documents with large allocation in the topic leads to more uniform p-values. p-values
for 200 topics estimated from synthetic data generated from an LDA model are either uniform or skewed towards 1.0.
Overly conservative p-values would be clustered around 0.5.
</figureCaption>
<bodyText confidence="0.999968846153846">
topics over a vocabulary of 100 terms. Hyperpa-
rameters for both the document-topic and topic-term
Dirichlet priors were 0.1 for each dimension. We
then trained a topic model with the same hyperpa-
rameters and number of topics on each corpus, sav-
ing a Gibbs sampling state.
We can measure the fit of a PPC by examining the
distribution of empirical p-values, that is, the propor-
tion of replications wTIP that result in discrepancies
less than the observed value. p-values should be uni-
formly distributed on (0, 1). Non-uniform p-values
indicate a lack of calibration. Unlike real collec-
tions, in synthetic corpora the range of discrepan-
cies from these replicated collections often includes
the real values, so p-values are meaningful. A his-
togram of p-values for 200 synthetic topics after 100
replications is shown in the left panel of Figure 9.
PPCs have been criticized for reusing training
data for model checking. For some models, the
posterior distribution is too close to the data, so all
replicated values are close to the real value, leading
to p-values clustered around 0.5 (Draper and Krn-
jajic, 2006; Bayarri and Castellanos, 2007). We
test divergence from a uniform distribution with a
Kolmogorov-Smirnov test. Our results indicate that
LDA is not overfitting, but that the distribution is not
uniform (KS p &lt; 0.00001).
The PPC framework allows us to choose discrep-
ancy functions that reflect the relative importance
of subsets of words and documents. The second
panel in Figure 9 sums only over the 20 documents
with the largest probability of the topic, the third
sums over all documents but only over the top 10
most probable words, and the fourth sums over only
the top words and documents. This test indicates
that the distribution of p-values for the subset Top-
Words is not uniform (KS p &lt; 0.00001), but that a
uniform distribution is a good fit for TopDocs (KS
p = 0.358) and TopWordsDocs (KS p = 0.069).
</bodyText>
<sectionHeader confidence="0.999849" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999987666666667">
We have developed a Bayesian model checking
method for probabilistic topic models. Conditioned
on their topic assignment, the words of the docu-
ments are independently and identically distributed
by a multinomial distribution. We developed a real-
ized discrepancy function—the mutual information
between words and document indices, conditioned
on a topic—that checks this assumption. We em-
bedded this function in a posterior predictive check.
We demonstrated that we can use this posterior
predictive check to identify particular topics that fit
the data, and particular topics that misfit the data in
different ways. Moreover, our method provides a
new way to visualize topic models.
We adapted the method to corpora with external
variables. In this setting, the PPC provides a way to
guide the modeler in searching through more com-
plicated models that involve more variables.
Finally, on simulated data, we demonstrated that
PPCs with the mutual information discrepancy func-
tion can identify model fit and model misfit.
</bodyText>
<sectionHeader confidence="0.99958" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.6829078">
David M. Blei is supported by ONR 175-6343, NSF
CAREER 0745520, AFOSR 09NL202, the Alfred P.
Sloan foundation, and a grant from Google. David
Mimno is supported by a Digital Humanities Re-
search grant from Google. Arthur Spirling and Andy
</reference>
<page confidence="0.996549">
236
</page>
<bodyText confidence="0.932827">
Eggers suggested the use of the Hansards corpus.
</bodyText>
<sectionHeader confidence="0.991792" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999821694915254">
Amr Ahmed and Eric Xing. 2010. Staying informed: Su-
pervised and semi-supervised multi-view topical anal-
ysis of ideological perspective. In EMNLP.
Arthur Asuncion, Padhraic Smyth, and Max Welling.
2008. Asynchronous distributed learning of topic
models. In NIPS.
M.J. Bayarri and M.E. Castellanos. 2007. Bayesian
checking of the second levels of hierarchical models.
Statistical Science, 22(3):322–343.
David M. Blei and John D. Lafferty. 2006. Dynamic
topic models. In ICML.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet allocation. Journal of Machine Learning
Research, 3:993–1022, January.
Jonathan Chang, Jordan Boyd-Graber, Chong Wang,
Sean Gerrish, and David M. Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Ad-
vances in Neural Information Processing Systems 22,
pages 288–296.
Gabriel Doyle and Charles Elkan. 2009. Accounting for
burstiness in topic models. In ICML.
David Draper and Milovan Krnjajic. 2006. Bayesian
model specification. Technical report, University of
California, Santa Cruz.
Jacob Eisenstein and Eric Xing. 2010. The CMU 2008
political blog corpus. Technical report, Carnegie Mel-
lon University.
A. Gelman, X.L. Meng, and H.S. Stern. 1996. poste-
rior predictive assessment of model fitness via realized
discrepancies. Statistica Sinica, 6:733–807.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS, 101(suppl. 1):5228–5235.
Matthew Hoffman, David Blei, and Francis Bach. 2010.
Online learning for latent dirichlet allocation. In NIPS.
Wei-Hao Lin, Eric Xing, and Alexander Hauptmann.
2008. A joint topic and perspective model for ideo-
logical discourse. In PKDD.
Qiaozhu Mei and ChengXiang Zhai. 2006. A mixture
model for contextual text mining. In KDD.
David Mimno and Andrew McCallum. 2007. Organizing
the OCA: learning faceted subjects from a library of
digital books. In JCDL.
David Newman, Jey Han Lau, Karl Grieser, and Timothy
Baldwin. 2010. Automatic evaluation of topic coher-
ence. In Human Language Technologies: The Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In EMNLP.
Donald B. Rubin. 1981. Estimation in parallel random-
ized experiments. Journal of Educational Statistics,
6:377–401.
D. Rubin. 1984. Bayesianly justifiable and relevant fre-
quency calculations for the applied statistician. The
Annals of Statistics, 12(4):1151–1172.
Hanna Wallach, Iain Murray, Ruslan Salakhutdinov, and
David Mimno. 2009. Evaluation methods for topic
models. In ICML.
</reference>
<page confidence="0.997413">
237
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.566591">
<title confidence="0.999965">Bayesian Checking for Topic Models</title>
<author confidence="0.999977">David Mimno David Blei</author>
<affiliation confidence="0.999929">Department of Computer Science Department of Computer</affiliation>
<address confidence="0.596554">Princeton University Princeton, NJ 08540 Princeton University Princeton, NJ 08540</address>
<email confidence="0.999362">mimno@cs.princeton.edublei@cs.princeton.edu</email>
<abstract confidence="0.995808083333333">Real document collections do not fit the independence assumptions asserted by most statistical topic models, but how badly do they violate them? We present a Bayesian method for measuring how well a topic model fits a corpus. Our approach is based on posterior predictive checking, a method for diagnosing Bayesian models in user-defined ways. Our method can identify where a topic model fits the data, where it falls short, and in which directions it might be improved.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>M David</author>
</authors>
<title>Blei is supported by ONR 175-6343, NSF CAREER 0745520, AFOSR 09NL202, the Alfred P. Sloan foundation, and a grant from Google. David Mimno is supported by a Digital Humanities Research grant from Google. Arthur Spirling and Andy</title>
<marker>David, </marker>
<rawString>David M. Blei is supported by ONR 175-6343, NSF CAREER 0745520, AFOSR 09NL202, the Alfred P. Sloan foundation, and a grant from Google. David Mimno is supported by a Digital Humanities Research grant from Google. Arthur Spirling and Andy</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amr Ahmed</author>
<author>Eric Xing</author>
</authors>
<title>Staying informed: Supervised and semi-supervised multi-view topical analysis of ideological perspective.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="26659" citStr="Ahmed and Xing, 2010" startWordPosition="4449" endWordPosition="4452">teract with groupings like time or desk. Figure 5 breaks down the per-word discrepancy shown in Figure 3 by month, for the words with the largest overall discrepancy. Kurdish is prominent during the Gulf War and the 1996 cruise missile strikes, but is less significant during the Iraq War. Individuals (Hussein, Sadr, and Maliki) move on and off the stage. Political blogs. The CMU 2008 political blog corpus consists of six blogs, three of which supported Barack Obama and three of which supported John McCain. This corpus has previously been considered in the context of aspect-based topic models (Ahmed and Xing, 2010) that assign distinct word distributions to liberal and conservative bloggers. It is reasonable to expect that blogs with different political leanings will use measurably different language to describe the same themes, suggesting that there will be systematic deviations from a multinomial hypothesis of exchangeability of words within topics. Indeed, Ahmed and Xing obtained improved results with such a model. Figure 6 shows the distribution of standard deviations from the mean replicated value for a set of 150 topics grouped by document, blog, and preferred candidate. Deviance is greatest for b</context>
</contexts>
<marker>Ahmed, Xing, 2010</marker>
<rawString>Amr Ahmed and Eric Xing. 2010. Staying informed: Supervised and semi-supervised multi-view topical analysis of ideological perspective. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur Asuncion</author>
<author>Padhraic Smyth</author>
<author>Max Welling</author>
</authors>
<title>Asynchronous distributed learning of topic models.</title>
<date>2008</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="7043" citStr="Asuncion et al., 2008" startWordPosition="1132" endWordPosition="1135"> the words in the collection (regardless of their documents) drawn from that topic will be drawn from the same multinomial distribution. The central computational problem for LDA is posterior inference. Given a collection of documents, the problem is to compute the conditional distribution of the hidden variables—the topics Ok, topic proportions Bd, and topic assignments zd,n. Researchers have developed many algorithms for approximating this posterior, including sampling methods (Griffiths and Steyvers, 2004) (used in this paper), variational methods (Blei et al., 2003), distributed variants (Asuncion et al., 2008), and online algorithms (Hoffman et al., 2010). 3 Checking Topic Models Once approximated, the posterior distribution is used for the task at hand. Topic models have been applied to many tasks, such as classification, prediction, collaborative filtering, and others. We focus on using them as an exploratory tool, where we assume that the topic model posterior provides a good decomposition of the corpus and that the topics provide good summaries of the corpus contents. But what is meant by “good”? To answer this question, we turn to Bayesian model checking (Rubin, 1981; Gelman et al., 1996). The</context>
</contexts>
<marker>Asuncion, Smyth, Welling, 2008</marker>
<rawString>Arthur Asuncion, Padhraic Smyth, and Max Welling. 2008. Asynchronous distributed learning of topic models. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Bayarri</author>
<author>M E Castellanos</author>
</authors>
<title>Bayesian checking of the second levels of hierarchical models.</title>
<date>2007</date>
<journal>Statistical Science,</journal>
<volume>22</volume>
<issue>3</issue>
<contexts>
<context position="33772" citStr="Bayarri and Castellanos, 2007" startWordPosition="5638" endWordPosition="5641">). Non-uniform p-values indicate a lack of calibration. Unlike real collections, in synthetic corpora the range of discrepancies from these replicated collections often includes the real values, so p-values are meaningful. A histogram of p-values for 200 synthetic topics after 100 replications is shown in the left panel of Figure 9. PPCs have been criticized for reusing training data for model checking. For some models, the posterior distribution is too close to the data, so all replicated values are close to the real value, leading to p-values clustered around 0.5 (Draper and Krnjajic, 2006; Bayarri and Castellanos, 2007). We test divergence from a uniform distribution with a Kolmogorov-Smirnov test. Our results indicate that LDA is not overfitting, but that the distribution is not uniform (KS p &lt; 0.00001). The PPC framework allows us to choose discrepancy functions that reflect the relative importance of subsets of words and documents. The second panel in Figure 9 sums only over the 20 documents with the largest probability of the topic, the third sums over all documents but only over the top 10 most probable words, and the fourth sums over only the top words and documents. This test indicates that the distri</context>
</contexts>
<marker>Bayarri, Castellanos, 2007</marker>
<rawString>M.J. Bayarri and M.E. Castellanos. 2007. Bayesian checking of the second levels of hierarchical models. Statistical Science, 22(3):322–343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>John D Lafferty</author>
</authors>
<title>Dynamic topic models.</title>
<date>2006</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="18864" citStr="Blei and Lafferty, 2006" startWordPosition="3145" endWordPosition="3149">lower than expected MI. The Iraq 231 expected. variance into the model using Dirichlet compound multinomial distributions (Doyle and Elkan, 2009). If there is no pattern to the deviations from multinomial word use across documents, this method is the best we can do. In many corpora, however, there are systematic deviations that can be explained by additional variables. LDA is the simplest generative topic model, and researchers have developed many variants of LDA that account for a variety of variables that can be found or measured with a corpus. Examples include models that account for time (Blei and Lafferty, 2006), books and McCallum, 2007), and aspect or perspective (Mei and Zhai, 2006; Lin et al., 2008; Paul et al., 2010). In this section, we show how we can use the mutual information discrepancy function of Equation 1 and PPCs to guide our choice in which topic model to fit. Greater deviance implies that a particular grouping better explains the variation in word use within a topic. The discrepancy functions are large when words appear more than expected in some groups and less than expected in others. We know that the individual documents show significantly more variation than we expect from replic</context>
</contexts>
<marker>Blei, Lafferty, 2006</marker>
<rawString>David M. Blei and John D. Lafferty. 2006. Dynamic topic models. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="1388" citStr="Blei et al., 2003" startWordPosition="220" endWordPosition="223">e a suite of machine learning algorithms that decompose a corpus into a set of topics and represent each document with a subset of those topics. The inferred topics often correspond with the underlying themes of the analyzed collection, and the topic modeling algorithm organizes the documents according to those themes. Most topic models are evaluated by their predictive performance on held out data. The idea is that topic models are fit to maximize the likelihood (or posterior probability) of a collection of documents, and so a good model is one that assigns high likelihood to a held out set (Blei et al., 2003; Wallach et al., 2009). But this evaluation is not in line with how topic models are frequently used. Topic models seem to capture the underlying themes of a collection—indeed the monicker “topic model” is retrospective—and so we expect that these themes are useful for exploring, summarizing, and learning 227 about its documents (Mimno and McCallum, 2007; Chang et al., 2009). In such exploratory data analysis, however, we are not concerned with the fit to held out data. In this paper, we develop and study new methods for evaluating topic models. Our methods are based on posterior predictive c</context>
<context position="3973" citStr="Blei et al., 2003" startWordPosition="638" endWordPosition="641">dings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 227–237, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics Specifically, we develop discrepancy functions for latent Dirichlet allocation (the simplest topic model) that measure how well its statistical assumptions about the topics are matched in the observed corpus and inferred topics. LDA assumes that each observed word in a corpus is assigned to a topic, and that the words assigned to the same topic are drawn independently from the same multinomial distribution (Blei et al., 2003). For each topic, we measure the whether this assumption holds by computing the mutual information between the words assigned to that topic and which document each word appeared in. If the assumptions hold, these two variables should be independent: low mutual information indicates that the assumptions hold; high mutual information indicates a mismatch to the modeling assumptions. We embed this discrepancy in a PPC and study it in several ways. First, we focus on topics that model their observations well; this helps separate interpretable topics from noisy topics (and “boilerplate” topics, whi</context>
<context position="5455" citStr="Blei et al., 2003" startWordPosition="881" endWordPosition="884"> incorporated into the model (such as time stamp or author). This helps point the modeler towards the most promising among more complicated models, or save the effort in fitting one. Finally, we validate this strategy by simulating data from a topic model, and assessing whether the PPC “accepts” the resulting data. 2 Probabilistic Topic Modeling Probabilistic topic models are statistical models of text that assume that a small number of distributions over words, called “topics,” are used to generate the observed documents. One of the simplest topic models is latent Dirichlet allocation (LDA) (Blei et al., 2003). In LDA, a set of K topics describes a corpus; each document exhibits the topics with different proportions. The words are assumed exchangeable within each document; the documents are assumed exchangeable within the corpus. More formally, let O1, ... , OK be K topics, each of which is a distribution over a fixed vocabulary. For each document, LDA assumes the following generative process 1. Choose topic proportions Bd — Dirichlet(α). 2. For each word (a) Choose topic assignment zd,n — B. (b) Choose word wd,n — Ozd,n. This process articulates the statistical assumptions behind LDA: Each documen</context>
<context position="6997" citStr="Blei et al., 2003" startWordPosition="1125" endWordPosition="1128"> from the “vegetables” topic; however, all the words in the collection (regardless of their documents) drawn from that topic will be drawn from the same multinomial distribution. The central computational problem for LDA is posterior inference. Given a collection of documents, the problem is to compute the conditional distribution of the hidden variables—the topics Ok, topic proportions Bd, and topic assignments zd,n. Researchers have developed many algorithms for approximating this posterior, including sampling methods (Griffiths and Steyvers, 2004) (used in this paper), variational methods (Blei et al., 2003), distributed variants (Asuncion et al., 2008), and online algorithms (Hoffman et al., 2010). 3 Checking Topic Models Once approximated, the posterior distribution is used for the task at hand. Topic models have been applied to many tasks, such as classification, prediction, collaborative filtering, and others. We focus on using them as an exploratory tool, where we assume that the topic model posterior provides a good decomposition of the corpus and that the topics provide good summaries of the corpus contents. But what is meant by “good”? To answer this question, we turn to Bayesian model ch</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David Blei, Andrew Ng, and Michael Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>Jordan Boyd-Graber</author>
<author>Chong Wang</author>
<author>Sean Gerrish</author>
<author>David M Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 22,</booktitle>
<pages>288--296</pages>
<contexts>
<context position="1766" citStr="Chang et al., 2009" startWordPosition="281" endWordPosition="284">formance on held out data. The idea is that topic models are fit to maximize the likelihood (or posterior probability) of a collection of documents, and so a good model is one that assigns high likelihood to a held out set (Blei et al., 2003; Wallach et al., 2009). But this evaluation is not in line with how topic models are frequently used. Topic models seem to capture the underlying themes of a collection—indeed the monicker “topic model” is retrospective—and so we expect that these themes are useful for exploring, summarizing, and learning 227 about its documents (Mimno and McCallum, 2007; Chang et al., 2009). In such exploratory data analysis, however, we are not concerned with the fit to held out data. In this paper, we develop and study new methods for evaluating topic models. Our methods are based on posterior predictive checking, which is a model diagnosis technique from Bayesian statistics (Rubin, 1984; Gelman et al., 1996). The goal of a posterior predictive check (PPC) is to assess the validity of a Bayesian model without requiring a specific alternative model. Given data, we first compute a posterior distribution over the latent variables. Then, we estimate the probability of the observed</context>
</contexts>
<marker>Chang, Boyd-Graber, Wang, Gerrish, Blei, 2009</marker>
<rawString>Jonathan Chang, Jordan Boyd-Graber, Chong Wang, Sean Gerrish, and David M. Blei. 2009. Reading tea leaves: How humans interpret topic models. In Advances in Neural Information Processing Systems 22, pages 288–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Doyle</author>
<author>Charles Elkan</author>
</authors>
<title>Accounting for burstiness in topic models.</title>
<date>2009</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="18385" citStr="Doyle and Elkan, 2009" startWordPosition="3064" endWordPosition="3067">Mets pitcher Grant Roberts and the firm Kohlberg Kravis Roberts. Turning to an analysis of the full mutual information, Figure 2 shows the three left-hand topics from Figure 1: Weekend, Iraq, and Roberts. The histogram represents MI scores for 100 replications of the topic, rescaled to have mean zero and unit variance. The observed value, also rescaled, and the mean replicated value (set to zero) are shown with vertical lines. The formulaic Weekend topic has significantly lower than expected MI. The Iraq 231 expected. variance into the model using Dirichlet compound multinomial distributions (Doyle and Elkan, 2009). If there is no pattern to the deviations from multinomial word use across documents, this method is the best we can do. In many corpora, however, there are systematic deviations that can be explained by additional variables. LDA is the simplest generative topic model, and researchers have developed many variants of LDA that account for a variety of variables that can be found or measured with a corpus. Examples include models that account for time (Blei and Lafferty, 2006), books and McCallum, 2007), and aspect or perspective (Mei and Zhai, 2006; Lin et al., 2008; Paul et al., 2010). In this</context>
</contexts>
<marker>Doyle, Elkan, 2009</marker>
<rawString>Gabriel Doyle and Charles Elkan. 2009. Accounting for burstiness in topic models. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Draper</author>
<author>Milovan Krnjajic</author>
</authors>
<title>Bayesian model specification.</title>
<date>2006</date>
<tech>Technical report,</tech>
<institution>University of California,</institution>
<location>Santa Cruz.</location>
<contexts>
<context position="33740" citStr="Draper and Krnjajic, 2006" startWordPosition="5633" endWordPosition="5637">formly distributed on (0, 1). Non-uniform p-values indicate a lack of calibration. Unlike real collections, in synthetic corpora the range of discrepancies from these replicated collections often includes the real values, so p-values are meaningful. A histogram of p-values for 200 synthetic topics after 100 replications is shown in the left panel of Figure 9. PPCs have been criticized for reusing training data for model checking. For some models, the posterior distribution is too close to the data, so all replicated values are close to the real value, leading to p-values clustered around 0.5 (Draper and Krnjajic, 2006; Bayarri and Castellanos, 2007). We test divergence from a uniform distribution with a Kolmogorov-Smirnov test. Our results indicate that LDA is not overfitting, but that the distribution is not uniform (KS p &lt; 0.00001). The PPC framework allows us to choose discrepancy functions that reflect the relative importance of subsets of words and documents. The second panel in Figure 9 sums only over the 20 documents with the largest probability of the topic, the third sums over all documents but only over the top 10 most probable words, and the fourth sums over only the top words and documents. Thi</context>
</contexts>
<marker>Draper, Krnjajic, 2006</marker>
<rawString>David Draper and Milovan Krnjajic. 2006. Bayesian model specification. Technical report, University of California, Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Eric Xing</author>
</authors>
<title>political blog corpus.</title>
<date>2010</date>
<booktitle>The CMU</booktitle>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="24379" citStr="Eisenstein and Xing, 2010" startWordPosition="4081" endWordPosition="4085">s in the discrepancy function in Eq. 1. Note that the previous discrepancy functions are equivalent to a trivial grouping, in which each document is the only member of its own group. In the following experiments we explore groupings by published volume, blog, preferred political candidate, and newspaper desk, and evaluate the effect of those groupings on the deviation between mean replicated values and observed values of those functions. 4.2 Case studies We analyze three corpora, each with its own metadata: the New York Times Annotated Corpus (1987– 2007)3, the CMU 2008 political blog corpus (Eisenstein and Xing, 2010), and speeches from the British 3http://www.ldc.upenn.edu House of Commons from 1830–1891.4 Descriptive statistics are presented in Table 1. The realization is represented by a single Gibbs sampling state after 1000 iterations of Gibbs sampling. Table 1: Statistics for models used as examples. Name Docs Tokens Vocab Topics News 1.8M 76M 121k 1000 Blogs 13k 2.2M 90k 100 Parliament 540k 55M 52k 300 New York Times articles. Figure 3 shows three groupings of words for the middle-left topic in Figure 1: by document, by month of publication (e.g. May of 2005), and by desk (e.g. Editorial, Foreign, F</context>
</contexts>
<marker>Eisenstein, Xing, 2010</marker>
<rawString>Jacob Eisenstein and Eric Xing. 2010. The CMU 2008 political blog corpus. Technical report, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gelman</author>
<author>X L Meng</author>
<author>H S Stern</author>
</authors>
<title>posterior predictive assessment of model fitness via realized discrepancies. Statistica Sinica,</title>
<date>1996</date>
<pages>6--733</pages>
<contexts>
<context position="2093" citStr="Gelman et al., 1996" startWordPosition="335" endWordPosition="338"> frequently used. Topic models seem to capture the underlying themes of a collection—indeed the monicker “topic model” is retrospective—and so we expect that these themes are useful for exploring, summarizing, and learning 227 about its documents (Mimno and McCallum, 2007; Chang et al., 2009). In such exploratory data analysis, however, we are not concerned with the fit to held out data. In this paper, we develop and study new methods for evaluating topic models. Our methods are based on posterior predictive checking, which is a model diagnosis technique from Bayesian statistics (Rubin, 1984; Gelman et al., 1996). The goal of a posterior predictive check (PPC) is to assess the validity of a Bayesian model without requiring a specific alternative model. Given data, we first compute a posterior distribution over the latent variables. Then, we estimate the probability of the observed data under the data-generating distribution that is induced by the posterior (the “posterior predictive distribution”). A data set that is unlikely calls the model into question, and consequently the posterior. PPCs can show where the model fits and doesn’t fit the observations. They can help identify the parts of the poster</context>
<context position="7638" citStr="Gelman et al., 1996" startWordPosition="1232" endWordPosition="1235">s (Asuncion et al., 2008), and online algorithms (Hoffman et al., 2010). 3 Checking Topic Models Once approximated, the posterior distribution is used for the task at hand. Topic models have been applied to many tasks, such as classification, prediction, collaborative filtering, and others. We focus on using them as an exploratory tool, where we assume that the topic model posterior provides a good decomposition of the corpus and that the topics provide good summaries of the corpus contents. But what is meant by “good”? To answer this question, we turn to Bayesian model checking (Rubin, 1981; Gelman et al., 1996). The goal of Bayesian model checking is to assess whether the observed data matches the modeling assumptions in the directions that are important to the analysis. The 228 Topic850 Topic371 Topic760 10 12 14 10 12 14 10 12 14 2 4 6 8 2 4 6 8 2 4 6 8 Times listing selective noteworthy critics weekend Broadway tickets highly recommended denotes booth Iraq Iraqi Tickets Street TKTS 1 2 3 4 Hussein Baghdad Saddam Shiite governme nt al Topic87 Roberts Topic628 Fort Wo Iraqis Sunni Kurdish forces country military troops Grant rth Burke Hunt Kravis Bass Kohlberg Grace Rothschild Baron Borden Texas Wi</context>
<context position="10457" citStr="Gelman et al., 1996" startWordPosition="1690" endWordPosition="1693">e likely to suggest something meaningful about it. In particular, we will develop posterior predictive checks (PPC) for topic models. In a PPC, we specify a discrepancy function, which is a function of the data that measures an important property that we want the model to capture. We then assess whether the observed value of the function is similar to values of the function drawn from the posterior, through the distribution of the data that it induces. (This distribution of the data is called the “posterior predictive distribution.”) An innovation in PPCs is the realized discrepancy function (Gelman et al., 1996), which is a function of the data and any hidden variables that are in the model. Realized discrepancies induce a traditional discrepancy by marginalizing out the hidden variables. But they can also be used to evaluate assumptions about latent variables in the posterior, especially when combined with techniques like MCMC sampling that provide realizations of them. In topic models, as we will see below, we use a realized discrepancy to factor the observations and to check specific components of the model that are discovered by the posterior. 3.1 A realized discrepancy for LDA Returning to LDA, </context>
</contexts>
<marker>Gelman, Meng, Stern, 1996</marker>
<rawString>A. Gelman, X.L. Meng, and H.S. Stern. 1996. posterior predictive assessment of model fitness via realized discrepancies. Statistica Sinica, 6:733–807.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<journal>PNAS,</journal>
<volume>101</volume>
<pages>1--5228</pages>
<contexts>
<context position="6935" citStr="Griffiths and Steyvers, 2004" startWordPosition="1115" endWordPosition="1118">documents might have different overall probabilities of containing a word from the “vegetables” topic; however, all the words in the collection (regardless of their documents) drawn from that topic will be drawn from the same multinomial distribution. The central computational problem for LDA is posterior inference. Given a collection of documents, the problem is to compute the conditional distribution of the hidden variables—the topics Ok, topic proportions Bd, and topic assignments zd,n. Researchers have developed many algorithms for approximating this posterior, including sampling methods (Griffiths and Steyvers, 2004) (used in this paper), variational methods (Blei et al., 2003), distributed variants (Asuncion et al., 2008), and online algorithms (Hoffman et al., 2010). 3 Checking Topic Models Once approximated, the posterior distribution is used for the task at hand. Topic models have been applied to many tasks, such as classification, prediction, collaborative filtering, and others. We focus on using them as an exploratory tool, where we assume that the topic model posterior provides a good decomposition of the corpus and that the topics provide good summaries of the corpus contents. But what is meant by</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. PNAS, 101(suppl. 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Hoffman</author>
<author>David Blei</author>
<author>Francis Bach</author>
</authors>
<title>Online learning for latent dirichlet allocation.</title>
<date>2010</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="7089" citStr="Hoffman et al., 2010" startWordPosition="1139" endWordPosition="1142">ir documents) drawn from that topic will be drawn from the same multinomial distribution. The central computational problem for LDA is posterior inference. Given a collection of documents, the problem is to compute the conditional distribution of the hidden variables—the topics Ok, topic proportions Bd, and topic assignments zd,n. Researchers have developed many algorithms for approximating this posterior, including sampling methods (Griffiths and Steyvers, 2004) (used in this paper), variational methods (Blei et al., 2003), distributed variants (Asuncion et al., 2008), and online algorithms (Hoffman et al., 2010). 3 Checking Topic Models Once approximated, the posterior distribution is used for the task at hand. Topic models have been applied to many tasks, such as classification, prediction, collaborative filtering, and others. We focus on using them as an exploratory tool, where we assume that the topic model posterior provides a good decomposition of the corpus and that the topics provide good summaries of the corpus contents. But what is meant by “good”? To answer this question, we turn to Bayesian model checking (Rubin, 1981; Gelman et al., 1996). The goal of Bayesian model checking is to assess </context>
</contexts>
<marker>Hoffman, Blei, Bach, 2010</marker>
<rawString>Matthew Hoffman, David Blei, and Francis Bach. 2010. Online learning for latent dirichlet allocation. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Hao Lin</author>
<author>Eric Xing</author>
<author>Alexander Hauptmann</author>
</authors>
<title>A joint topic and perspective model for ideological discourse.</title>
<date>2008</date>
<booktitle>In PKDD.</booktitle>
<contexts>
<context position="18956" citStr="Lin et al., 2008" startWordPosition="3162" endWordPosition="3165">inomial distributions (Doyle and Elkan, 2009). If there is no pattern to the deviations from multinomial word use across documents, this method is the best we can do. In many corpora, however, there are systematic deviations that can be explained by additional variables. LDA is the simplest generative topic model, and researchers have developed many variants of LDA that account for a variety of variables that can be found or measured with a corpus. Examples include models that account for time (Blei and Lafferty, 2006), books and McCallum, 2007), and aspect or perspective (Mei and Zhai, 2006; Lin et al., 2008; Paul et al., 2010). In this section, we show how we can use the mutual information discrepancy function of Equation 1 and PPCs to guide our choice in which topic model to fit. Greater deviance implies that a particular grouping better explains the variation in word use within a topic. The discrepancy functions are large when words appear more than expected in some groups and less than expected in others. We know that the individual documents show significantly more variation than we expect from replications from the posterior distribution. If we combine documents randomly in a meaningless gr</context>
</contexts>
<marker>Lin, Xing, Hauptmann, 2008</marker>
<rawString>Wei-Hao Lin, Eric Xing, and Alexander Hauptmann. 2008. A joint topic and perspective model for ideological discourse. In PKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>ChengXiang Zhai</author>
</authors>
<title>A mixture model for contextual text mining.</title>
<date>2006</date>
<booktitle>In KDD.</booktitle>
<contexts>
<context position="18938" citStr="Mei and Zhai, 2006" startWordPosition="3158" endWordPosition="3161">ichlet compound multinomial distributions (Doyle and Elkan, 2009). If there is no pattern to the deviations from multinomial word use across documents, this method is the best we can do. In many corpora, however, there are systematic deviations that can be explained by additional variables. LDA is the simplest generative topic model, and researchers have developed many variants of LDA that account for a variety of variables that can be found or measured with a corpus. Examples include models that account for time (Blei and Lafferty, 2006), books and McCallum, 2007), and aspect or perspective (Mei and Zhai, 2006; Lin et al., 2008; Paul et al., 2010). In this section, we show how we can use the mutual information discrepancy function of Equation 1 and PPCs to guide our choice in which topic model to fit. Greater deviance implies that a particular grouping better explains the variation in word use within a topic. The discrepancy functions are large when words appear more than expected in some groups and less than expected in others. We know that the individual documents show significantly more variation than we expect from replications from the posterior distribution. If we combine documents randomly i</context>
</contexts>
<marker>Mei, Zhai, 2006</marker>
<rawString>Qiaozhu Mei and ChengXiang Zhai. 2006. A mixture model for contextual text mining. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Andrew McCallum</author>
</authors>
<title>Organizing the OCA: learning faceted subjects from a library of digital books.</title>
<date>2007</date>
<booktitle>In JCDL.</booktitle>
<contexts>
<context position="1745" citStr="Mimno and McCallum, 2007" startWordPosition="277" endWordPosition="280">ed by their predictive performance on held out data. The idea is that topic models are fit to maximize the likelihood (or posterior probability) of a collection of documents, and so a good model is one that assigns high likelihood to a held out set (Blei et al., 2003; Wallach et al., 2009). But this evaluation is not in line with how topic models are frequently used. Topic models seem to capture the underlying themes of a collection—indeed the monicker “topic model” is retrospective—and so we expect that these themes are useful for exploring, summarizing, and learning 227 about its documents (Mimno and McCallum, 2007; Chang et al., 2009). In such exploratory data analysis, however, we are not concerned with the fit to held out data. In this paper, we develop and study new methods for evaluating topic models. Our methods are based on posterior predictive checking, which is a model diagnosis technique from Bayesian statistics (Rubin, 1984; Gelman et al., 1996). The goal of a posterior predictive check (PPC) is to assess the validity of a Bayesian model without requiring a specific alternative model. Given data, we first compute a posterior distribution over the latent variables. Then, we estimate the probab</context>
</contexts>
<marker>Mimno, McCallum, 2007</marker>
<rawString>David Mimno and Andrew McCallum. 2007. Organizing the OCA: learning faceted subjects from a library of digital books. In JCDL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Jey Han Lau</author>
<author>Karl Grieser</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic evaluation of topic coherence.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="13628" citStr="Newman et al., 2010" startWordPosition="2256" endWordPosition="2259">are evenly distributed across many documents (high entropy); others are concentrated in fewer documents (low entropy). The second term conditions this distribution on a particular word type w by normalizing the perdocument number of times w appeared in each document (in topic k). If this distribution is close to p(d|k) then H(D|W = w, k) will be close to H(D|k) and IMI(w, D|k) will be low. If, on the other hand, word w occurs many times in only a few documents, it will have lower entropy over docu1There are other choices of discrepancies, such as wordword point-wise mutual information scores (Newman et al., 2010). �= w �= � w d � d 230 ments than the overall distribution over documents for the topic and IMI(w, D|k) will be high. We illustrate this discrepancy in Figure 1, which shows nine topics trained from the New York Times.2 Each row contains randomly selected topics from low, middle, and high ranges of MI, respectively. Each triangle represents a word. Its place on the yaxis is its rank in the topic. Its place on the x-axis is its IMI(w|k), with more uniformly distributed words (low IMI) to the left and more specific words (high IMI) to the right. (For now, ignore the other points in this figure.</context>
</contexts>
<marker>Newman, Lau, Grieser, Baldwin, 2010</marker>
<rawString>David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Automatic evaluation of topic coherence. In Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Paul</author>
<author>ChengXiang Zhai</author>
<author>Roxana Girju</author>
</authors>
<title>Summarizing contrastive viewpoints in opinionated text.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="18976" citStr="Paul et al., 2010" startWordPosition="3166" endWordPosition="3169">ons (Doyle and Elkan, 2009). If there is no pattern to the deviations from multinomial word use across documents, this method is the best we can do. In many corpora, however, there are systematic deviations that can be explained by additional variables. LDA is the simplest generative topic model, and researchers have developed many variants of LDA that account for a variety of variables that can be found or measured with a corpus. Examples include models that account for time (Blei and Lafferty, 2006), books and McCallum, 2007), and aspect or perspective (Mei and Zhai, 2006; Lin et al., 2008; Paul et al., 2010). In this section, we show how we can use the mutual information discrepancy function of Equation 1 and PPCs to guide our choice in which topic model to fit. Greater deviance implies that a particular grouping better explains the variation in word use within a topic. The discrepancy functions are large when words appear more than expected in some groups and less than expected in others. We know that the individual documents show significantly more variation than we expect from replications from the posterior distribution. If we combine documents randomly in a meaningless grouping, such devianc</context>
</contexts>
<marker>Paul, Zhai, Girju, 2010</marker>
<rawString>Michael J. Paul, ChengXiang Zhai, and Roxana Girju. 2010. Summarizing contrastive viewpoints in opinionated text. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald B Rubin</author>
</authors>
<title>Estimation in parallel randomized experiments.</title>
<date>1981</date>
<journal>Journal of Educational Statistics,</journal>
<pages>6--377</pages>
<contexts>
<context position="7616" citStr="Rubin, 1981" startWordPosition="1229" endWordPosition="1231">buted variants (Asuncion et al., 2008), and online algorithms (Hoffman et al., 2010). 3 Checking Topic Models Once approximated, the posterior distribution is used for the task at hand. Topic models have been applied to many tasks, such as classification, prediction, collaborative filtering, and others. We focus on using them as an exploratory tool, where we assume that the topic model posterior provides a good decomposition of the corpus and that the topics provide good summaries of the corpus contents. But what is meant by “good”? To answer this question, we turn to Bayesian model checking (Rubin, 1981; Gelman et al., 1996). The goal of Bayesian model checking is to assess whether the observed data matches the modeling assumptions in the directions that are important to the analysis. The 228 Topic850 Topic371 Topic760 10 12 14 10 12 14 10 12 14 2 4 6 8 2 4 6 8 2 4 6 8 Times listing selective noteworthy critics weekend Broadway tickets highly recommended denotes booth Iraq Iraqi Tickets Street TKTS 1 2 3 4 Hussein Baghdad Saddam Shiite governme nt al Topic87 Roberts Topic628 Fort Wo Iraqis Sunni Kurdish forces country military troops Grant rth Burke Hunt Kravis Bass Kohlberg Grace Rothschild</context>
</contexts>
<marker>Rubin, 1981</marker>
<rawString>Donald B. Rubin. 1981. Estimation in parallel randomized experiments. Journal of Educational Statistics, 6:377–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Rubin</author>
</authors>
<title>Bayesianly justifiable and relevant frequency calculations for the applied statistician.</title>
<date>1984</date>
<journal>The Annals of Statistics,</journal>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="2071" citStr="Rubin, 1984" startWordPosition="333" endWordPosition="334">ic models are frequently used. Topic models seem to capture the underlying themes of a collection—indeed the monicker “topic model” is retrospective—and so we expect that these themes are useful for exploring, summarizing, and learning 227 about its documents (Mimno and McCallum, 2007; Chang et al., 2009). In such exploratory data analysis, however, we are not concerned with the fit to held out data. In this paper, we develop and study new methods for evaluating topic models. Our methods are based on posterior predictive checking, which is a model diagnosis technique from Bayesian statistics (Rubin, 1984; Gelman et al., 1996). The goal of a posterior predictive check (PPC) is to assess the validity of a Bayesian model without requiring a specific alternative model. Given data, we first compute a posterior distribution over the latent variables. Then, we estimate the probability of the observed data under the data-generating distribution that is induced by the posterior (the “posterior predictive distribution”). A data set that is unlikely calls the model into question, and consequently the posterior. PPCs can show where the model fits and doesn’t fit the observations. They can help identify t</context>
</contexts>
<marker>Rubin, 1984</marker>
<rawString>D. Rubin. 1984. Bayesianly justifiable and relevant frequency calculations for the applied statistician. The Annals of Statistics, 12(4):1151–1172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna Wallach</author>
<author>Iain Murray</author>
<author>Ruslan Salakhutdinov</author>
<author>David Mimno</author>
</authors>
<title>Evaluation methods for topic models.</title>
<date>2009</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="1411" citStr="Wallach et al., 2009" startWordPosition="224" endWordPosition="227">e learning algorithms that decompose a corpus into a set of topics and represent each document with a subset of those topics. The inferred topics often correspond with the underlying themes of the analyzed collection, and the topic modeling algorithm organizes the documents according to those themes. Most topic models are evaluated by their predictive performance on held out data. The idea is that topic models are fit to maximize the likelihood (or posterior probability) of a collection of documents, and so a good model is one that assigns high likelihood to a held out set (Blei et al., 2003; Wallach et al., 2009). But this evaluation is not in line with how topic models are frequently used. Topic models seem to capture the underlying themes of a collection—indeed the monicker “topic model” is retrospective—and so we expect that these themes are useful for exploring, summarizing, and learning 227 about its documents (Mimno and McCallum, 2007; Chang et al., 2009). In such exploratory data analysis, however, we are not concerned with the fit to held out data. In this paper, we develop and study new methods for evaluating topic models. Our methods are based on posterior predictive checking, which is a mod</context>
</contexts>
<marker>Wallach, Murray, Salakhutdinov, Mimno, 2009</marker>
<rawString>Hanna Wallach, Iain Murray, Ruslan Salakhutdinov, and David Mimno. 2009. Evaluation methods for topic models. In ICML.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>