<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000120">
<title confidence="0.9957005">
Speeding Up Full Syntactic Parsing by Leveraging Partial Parsing
Decisions
</title>
<author confidence="0.700497">
Elliot Glaysher and Dan Moldovan
</author>
<affiliation confidence="0.311869">
Language Computer Corporation
1701 N. Collins Blvd. Suite 2000
</affiliation>
<address confidence="0.516824">
Richardson, TX 75080
</address>
<email confidence="0.997263">
{eglaysher,moldovan}@languagecomputer.com
</email>
<sectionHeader confidence="0.99737" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999664">
Parsing is a computationally intensive task
due to the combinatorial explosion seen in
chart parsing algorithms that explore pos-
sible parse trees. In this paper, we propose
a method to limit the combinatorial explo-
sion by restricting the CYK chart parsing
algorithm based on the output of a chunk
parser. When tested on the three parsers
presented in (Collins, 1999), we observed
an approximate three–fold speedup with
only an average decrease of 0.17% in both
precision and recall.
</bodyText>
<sectionHeader confidence="0.999301" genericHeader="keywords">
1 Introduction
</sectionHeader>
<subsectionHeader confidence="0.972977">
1.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999964608695652">
Syntactic parsing is a computationally intensive
and slow task. The cost of parsing quickly be-
comes prohibitively expensive as the amount of
text to parse grows. Even worse, syntactic parsing
is a prerequisite for many natural language pro-
cessing tasks. These costs make it impossible to
work with large collections of documents in any
reasonable amount of time.
We started looking into methods and improve-
ments that would speed up syntactic parsing.
These are divided into simple software engineer-
ing solutions, which are only touched on briefly,
and an optimization to the CYK parsing algorithm,
which is the main topic of this paper.
While we made large speed gains through sim-
ple software engineering improvements, such as
internal symbolization, optimizing critical areas,
optimization of the training data format, et cetera,
the largest individual gain in speed was made by
modifying the CYK parsing algorithm to leverage
the decisions of a syntactic chunk parser so that it
avoided combinations that conflicted with the out-
put of the chunk parser.
</bodyText>
<subsectionHeader confidence="0.587026">
1.2 Previous Work
</subsectionHeader>
<bodyText confidence="0.999695617647059">
Chart parsing is a method of building a parse tree
that systematically explores combinations based
on a set of grammatical rules, while using a chart
to store partial results. The general CYK algo-
rithm is a bottom-up parsing algorithm that will
generate all possible parse trees that are accepted
by a formal grammar. Michael Collins, first
in (1996), and then in his PhD thesis (1999), de-
scribes a modification to the standard CYK chart
parse for natural languages which uses probabili-
ties instead of simple context free grammars.
The CYK algorithm considers all possible com-
binations. In Figure 1, we present a CYK chart
graph for the sentence “The red balloon flew
away.” The algorithm will search the pyramid,
from left to right, from the bottom to the top. Each
box contains a pair of numbers that we will re-
fer to as the span, which represent the sequence
of words currently being considered. Calculating
each “box” in the chart means trying all combina-
tions of the lower parts of the box’s sub-pyramid
to form possible sub-parse trees. For example, one
calculates the results for the span (1, 4) by trying
to combine the results in (1, 1) and (2, 4), (1, 2)
and (3, 4), and (1, 3) and (4, 4).
In (Collins, 1999), Collins describes three new
parsers. The Model 2 gives the best output, pars-
ing section 23 at 88.26% precision and 88.05%
recall in 40 minutes. The Model 1 is by far the
fastest of the three, parsing section 23 of Tree-
bank (Marcus et al., 1994) at 87.75% precision and
87.45% recall in 26 minutes.
Syntactic Chunking is the partial parsing
process of segmenting a sentence into non-
</bodyText>
<page confidence="0.984529">
295
</page>
<bodyText confidence="0.6102155">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 295–300,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</bodyText>
<equation confidence="0.8252748">
(1,5)
(1,4) (2,5)
(1,3) (2,4) (3,5)
(1,2) (2,3) (3,4) (4,5)
(1,1) (2,2) (3,3) (4,4) (5,5)
</equation>
<note confidence="0.304536">
The red balloon flew away
</note>
<figureCaption confidence="0.904146333333333">
Figure 1: The CYK parse visualized as a pyramid.
CYK will search from the left to right, bottom to
top.
</figureCaption>
<bodyText confidence="0.9993290625">
overlapping “chunks” of syntactically connected
words. (Tjong Kim Sang and Buchholz, 2000) Un-
like a parse tree, a set of syntactic chunks has
no hierarchical information on how sequences of
words relate to each other. The only information
given is an additional label describing the chunk.
We use the YamCha (Kudo and Matsumoto,
2003; Kudo and Matsumoto, 2001) chunker for
our text chunking. When trained on all of Penn
Treebank , except for section 23 and tested on sec-
tion 23, the model had a precision of 95.96% and
a recall of 96.08%. YamCha parses section 23 of
Treebank in 36 seconds.
Clause Identification is the partial parsing pro-
cess of annotating the hierarchical structure of
clauses—groupings of words that contain a sub-
ject and a predicate (Tjong Kim Sang and D´ejean,
2001). Our clause identifier is an implementa-
tion of (Carreras et al., 2002), except that we use
C5.0 as the machine learning method instead of
Carreras’ own TreeBoost algorithm (Carreras and
M´arquez, 2001). When trained and scored on the
CoNLL 2001 shared task datal with the results
of our chunker, our clause identifier performs at
90.73% precision, 73.72% recall on the develop-
ment set and 88.85% precision, 70.22% recall on
the test set.
In this paper, we describe modifications to
the version of the CYK algorithm described in
(Collins, 1999) and experiment with the modi-
fications to both our proprietary parser and the
(Collins, 1999) parser.
</bodyText>
<footnote confidence="0.91639">
1http://www.cnts.ua.ac.be/conll2001/clauses/clauses.tgz
</footnote>
<sectionHeader confidence="0.994336" genericHeader="introduction">
2 Methods
</sectionHeader>
<subsectionHeader confidence="0.999242">
2.1 Software Optimizations
</subsectionHeader>
<bodyText confidence="0.999450794871795">
While each of the following optimizations, in-
dividually, had a smaller effect on our parser’s
speed than the CYK restrictions, collectively, sim-
ple software engineering improvements resulted in
the largest speed increase to our syntactic parser.
In the experiments section, we will refer to this as
the “Optimized” version.
Optimization of the training data and inter-
nal symbolization: We discovered that our parser
was bound by the number of probability hash-table
lookups. We changed the format for our training
data/hash keys so that they were as short as possi-
ble, eliminating deliminators and using integers to
represent a closed set of POS tags that were seen
in the training data, reducing the two to four byte
POS tags such as “VP” or “ADJP” down to single
byte integers. In the most extreme cases, this re-
duces the length of non-word characters in a hash
from 28 characters to 6. The training data takes up
less space, hashes faster, and many string compar-
isons are reduced to simple integer comparisons.
Optimization of our hash-table implementa-
tion: The majority of look ups in the hash-table
at runtime were for non-existent keys. We put
a bloomfilter on each hash bucket so that such
lookups would often be trivially rejected, instead
of having to compare the lookup key with ev-
ery key in the bucket. We also switched to the
Fowler/Noll/Vo (Noll, 2005) hash function, which
is faster and has less collisions then our previous
hash function.
Optimization of critical areas: There were
several areas in our code that were optimized af-
ter profiling our parser.
Rules based pre/post-processing: We were
able to get very minor increases in precision, re-
call and speed by adding hard coded rules to our
parser that handle things that are handled poorly,
specifically parenthetical phrases and quotations.
</bodyText>
<subsectionHeader confidence="0.997489">
2.2 CYK restrictions
</subsectionHeader>
<bodyText confidence="0.999391625">
In this section, we describe modifications that re-
strict the chart search based on the output of a
partial parser (in this case, a chunker) that marks
groups of constituents.
First, we define a span to be a pair c = (s, t),
where s is the index of the first word in the span
and t is the index of the last word in the span. We
then define a set 5, where 5 is the set of spans
</bodyText>
<page confidence="0.988377">
296
</page>
<bodyText confidence="0.995469421052632">
c1, ... , cr,, that represent the restrictions placed on
the CYK parse. We say that c1 and c2 overlap iff
s1 &lt; s2 &lt; t1 &lt; t2 or s2 &lt; s1 &lt; t2 &lt; t1, and we
note it as c1 — c2.2
When using the output of a chunker, S is the set
of spans that describe the non-VP, non-PP chunks
where ti — si &gt; 0.
During the CYK parse, after a span’s start and
end points are selected, but before iterating across
all splits of that span and their generative rules,
we propose that the span in question be checked
to make sure that it does not overlap with any
span in set S. We give the pseudocode in Al-
gorithm 1, which is a modification of the parse()
function given in Appendix B of (Collins, 1999).
Algorithm 1 The modified parse() function
initialize()
for span = 2 to n do
for start = 1 to n — span + 1 do
</bodyText>
<equation confidence="0.476967222222222">
end +— start + span — 1
if bx E S(x 9L (start, end)) then
complete(start, end)
end if
end for
end for
X +— edge in chart[1,n,TOP] with highest
probability
return X
</equation>
<bodyText confidence="0.95418524137931">
For example, given the chunk parse:
[The red balloon]NP [flew]V P [away]ADV P,
S = {(1, 3)} because there is only one chunk
with a length greater than 1.
Suppose we are analyzing the span (3,4) on
the example sentence above. This span will be
rejected, as it overlaps with the chunk (1,3);
the leaf nodes “balloon” and “flew” are not go-
ing to be children of the same parsetree parent
node. Thus, this method does not compute the
generative rules for all the splits of the spans
{(2, 4), (2, 5), (3, 4), (3, 5)}. This will also reduce
the number of calculations done when calculating
higher spans. When computing (1, 4) in this ex-
ample, time will be saved since the spans (2,4)
and (3,4) were not considered. This example is
visualized in Figure 2.
A more complex, real–world example from
section 23 of Treebank is visualized in Fig-
2This notation was originally used in (Carreras et al.,
2002).
ure 3, using the sentence “Under an agreement
signed by the Big Board and the Chicago Mer-
cantile Exchange, trading was temporarily halted
in Chicago.” This sentence has three usable
chunks, [an agreement]NP, [the Big Board]NP,
and [the Chicago Mercantile Exchange]NP. This
example shows the effects of the above algorithm
on a longer sentence with multiple chunks.
</bodyText>
<figure confidence="0.9899575">
(4,5)
(1,1) (2,2) (3,3) (4,4) (5,5)
The red balloon flew away
Chunk Span
</figure>
<figureCaption confidence="0.9402445">
Figure 2: The same CYK example as in Fig-
ure 1. Blacked out box spans will not be calcu-
</figureCaption>
<bodyText confidence="0.989717666666667">
lated, while half toned box spans do not have to
calculate as many possibilities because they de-
pend on an uncalculated span.
</bodyText>
<sectionHeader confidence="0.997417" genericHeader="method">
3 Experiments &amp; Results
</sectionHeader>
<subsectionHeader confidence="0.995627">
3.1 Our parser with chunks
</subsectionHeader>
<bodyText confidence="0.999989333333333">
Our parser uses a simplified version of the model
presented in (Collins, 1996). For this experi-
ment,we tested four versions of our internal parser:
</bodyText>
<listItem confidence="0.98846525">
• Our original parser. No optimizations or
chunking information.
• Our original parser with chunking informa-
tion.
• Our optimized parser without chunking infor-
mation.
• Our optimized parser with chunking informa-
tion.
</listItem>
<bodyText confidence="0.99975575">
For parsers that use chunking information, the
runtime of the chunk parsing is included in the
parser’s runtime, to show that total gains in run-
time offset the cost of running the chunker.
We trained the chunk parser on all of Treebank
except for section 23, which will be used as the
test set. We trained our parser on all of Treebank
except for section 23. Scoring of the parse trees
</bodyText>
<figure confidence="0.999598222222222">
(1,5)
(1,4)
(2,4)
(3,5)
(1,3)
(2,5)
(1,2)
(2,3)
(3,4)
</figure>
<page confidence="0.9398465">
297
298
</page>
<figureCaption confidence="0.984693666666667">
Figure 3: The CYK chart for the chunk parsed sentence “[Under]PP [an agreement]NP
[signed]V P [by]PP [the Big Board]NP [and]NP [the Chicago Mercantile Exchange]NP, [trading]NP
[was temporarily halted]V P [in]PP [Chicago]NP.” The color coding scheme is the same as in Figure 2.
</figureCaption>
<figure confidence="0.996773945205479">
(2,19)
(1,18)
(2,2)
an
(11,11)
(13,13)
Chicago
Exchange
(6,6)
the
(8,8)
Board
(1,19)
(1,17)
(2,18)
(1,12)
(1,13)
(1,14)
(2,13)
(1,15)
(2,14)
(1,16)
(2,15)
(3,14)
(2,16)
(3,15)
(2,17)
(3,16)
(4,15)
(3,17)
(4,16)
(3,18)
(4,17)
(5,16)
(4,18)
(5,17)
(4,19)
(5,18)
(6,17)
(5,19)
(6,18)
(6,19)
(7,18)
(7,19)
(8,19)
(4,14)
(5,15)
(6,16)
(7,17)
(8,18)
(1,11) (2,12) (3,13)
(2,6)
(3,7)
(4,8)
(5,9)
(6,10)
(7,11) (8,12)
(9,13)
(10,14)
(1 1 , 1 5) ( 1 2, 1
(1,5)
(10,13)
(11,14) (12,15)
(7,10) (8,11) (9,12)
( 1 ,2)
(2,3)
(3,4)
(4,5)
(5,6)
(1,3)
(2,4)
(3,5) (4,6) (5,7)
(6,8)
(10, 1 2)
(11,13)
(12,14) ( 1 3, 1
(7,9) (8,10) (9,11)
(7,8)
(11,12)
(12,13)
(10,11)
(6,7)
(13,14)
(9,10)
(8,9)
(3,19)
(9, 1 8)
(2,11) (3,12)
(4,13)
(5,14)
(6,15)
(7,16) (8,17)
(1,10)
(5,13)
(6,14)
(7,15) (8,16)
(2,10) (3,11) (4,12)
(1,9)
(1,8)
(2,9)
(3,10) (4,11) (5,12)
(6,13)
(7,14)
(9,16)
( 10, 17)
(3,9) (4,10) (5,11) (6,12) (7,13) (8,14)
(1,6)
(2,7) (3,8)
(4,9)
(5,10) (6,11) (7,12) (8,13)
(3,6)
(1,4)
(2,5)
(4,7)
(5,8)
(6,9)
(10,1
(9,17)
(8,15)
(11,1
(10,16)
(9,15)
(2,8)
(9,14)
(10,15)
(14,1
(9,9)
(4,4)
tradin
and
signed
(11,16)
(1,7)
(3,3)
agreement
(10,10)
the
(12,12)
Mercantile
(7,7)
Big
(9,1
(1,1)
Under
(5,5)
by
</figure>
<table confidence="0.9991666">
Precision Recall Time
Original 82.79% 83.19% 25’45”
With chunks 84.40% 83.74% 7’37”
Optimized 83.86% 83.24% 4’28”
With chunks 84.42% 84.06% 1’35”
</table>
<tableCaption confidence="0.998855">
Table 1: Results from our parser on Section 23
</tableCaption>
<table confidence="0.999966285714286">
Precision Recall Time
Model 1 87.75% 87.45% 26’18”
With chunks 87.63% 87.27% 8’54”
Model 2 88.26% 88.05% 40’00”
With chunks 88.04% 87.87% 13’47”
Model 3 88.25% 88.04% 42’24”
With chunks 88.10% 87.89% 14’58”
</table>
<tableCaption confidence="0.829958">
Table 2: Results from the Collins parsers on Sec-
tion 23 with chunking information
</tableCaption>
<bodyText confidence="0.998922">
was done using the EVALB package that was used
to score the (Collins, 1999) parser. The numbers
represent the labeled bracketing of all sentences;
not just those with 40 words or less.
The experiment was run on a dual Pentium 4,
3.20Ghz machine with two gigabytes of memory.
The results are presented in Table 1.
The most notable result is the greatly reduced
time to parse when chunking information was
added. Both versions of our parser saw an average
three–fold increase in speed by leveraging chunk-
ing decisions. We also saw small increases in both
precision and recall.
</bodyText>
<subsectionHeader confidence="0.995334">
3.2 Collins Parsers with chunks
</subsectionHeader>
<bodyText confidence="0.991126176470588">
To show that this method is general and does
not exploit weaknesses in the lexical model of
our parser, we repeated the previous experiments
with the three models of parsers presented in the
(Collins, 1999). We made sure to use the exact
same chunk post-processing rules in the Collins
parser code to make sure that the same chunk in-
formation was being used. We used Collins’ train-
ing data. We did not retrain the parser in any way
to optimize for chunked input. We only modified
the parsing algorithm.
Once again, the chunk parser was trained on
all of Treebank except for section 23, the trees
are evaluated with EVALB, and these experiments
were run on the same dual Pentium 4 machine.
These results are presented in Table 2.
Like our parser, each Collins parser saw a
</bodyText>
<table confidence="0.9998654">
Precision Recall Time
Optimized 83.86% 83.24% 4’28”
With chunks 84.42% 84.06% 1’35”
With clauses 83.66% 83.06% 5’02”
With both 84.20% 83.84% 2’26”
</table>
<tableCaption confidence="0.8448915">
Table 3: Results from our parser on Section
23 with clause identification information. Data
copied from the first experiment has been itali-
cized for comparison.
</tableCaption>
<bodyText confidence="0.998192916666667">
slightly under three fold increase in speed. But
unlike our parser, all three models of the Collins
parser saw slight decreases in accuracy, averag-
ing at -0.17% for both precision and recall. We
theorize that this is because the errors in our lex-
ical model are more severe than the errors in the
chunks, but the Collins parser models make fewer
errors in word grouping at the leaf node level than
the chunker does. We theorize that a more accu-
rate chunker would result in an increase in the pre-
cision and recall of the Collins parsers, while pre-
serving the substantial speed gains.
</bodyText>
<subsectionHeader confidence="0.994777">
3.3 Clause Identification
</subsectionHeader>
<bodyText confidence="0.99900725">
Encouraged by the improvement brought by using
chunking as a source of restrictions, we used the
data from our clause identifier.
Again, our clause identifier was derived from
(Carreras et al., 2002), using boosted C5.0 deci-
sion trees instead of their boosted binary decision
tree method, which performs below their numbers:
88.85% precision, 70.22% recall on the CoNLL
2001 shared task test set.
These results are presented in Table 3.
Adding clause detection information hurt per-
formance in every category. The increases in run-
time are caused by the clause identifier’s runtime
complexity of over O(n3). The time to identify
clauses is greater then the speed increases gained
by using the output as restrictions.
In terms of the drop in precision and recall,
we believe that errors from the clause detector
are grouping words together that are not all con-
stituents of the same parent node. While errors in
a chunk parse are relatively localized, errors in the
hierarchical structure of clauses can affect the en-
tire parse tree, preventing the parser from explor-
ing the correct high-level structure of the sentence.
</bodyText>
<page confidence="0.997406">
299
</page>
<sectionHeader confidence="0.999786" genericHeader="method">
4 Future Work
</sectionHeader>
<bodyText confidence="0.999883545454545">
While the modification given in section 2.2 is
specific to CYK parsing, we believe that plac-
ing restrictions based on the output of a chunk
parser is general enough to be applied to any gen-
erative, statistical parser, such as the Charniak
parser (2000), or a Lexical Tree Adjoining Gram-
mar based parser (Sarkar, 2000). Restrictions can
be placed where the parser would explore possi-
ble trees that would violate the boundaries deter-
mined by the chunk parser, pruning paths that will
not yield the correct parse tree.
</bodyText>
<sectionHeader confidence="0.999568" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999907444444444">
Using decisions from partial parsing greatly re-
duces the time to perform full syntactic parses, and
we have presented a method to apply the informa-
tion from partial parsing to full syntactic parsers
that use a variant of the CYK algorithm. We have
shown that this method is not specific to the im-
plementation of our parser and causes a negligible
effect on precision and recall, while decreasing the
time to parse by an approximate factor of three.
</bodyText>
<sectionHeader confidence="0.999448" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999805516666667">
Xavier Carreras and Llu´ıs M´arquez. 2001. Boosting
trees for anti-spam email filtering. In Proceedings of
RANLP-01, 4th International Conference on Recent
Advances in Natural Language Processing, Tzigov
Chark, BG.
Xavier Carreras, Llu´ıs M`arquez, Vasin Punyakanok,
and Dan Roth. 2002. Learning and inference for
clause identification. In ECML ’02: Proceedings of
the 13th European Conference on Machine Learn-
ing, pages 35–47, London, UK. Springer-Verlag.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the first confer-
ence on North American chapter of the Association
for Computational Linguistics, pages 132–139, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Arivind
Joshi and Martha Palmer, editors, Proceedings of
the Thirty-Fourth Annual Meeting of the Association
for Computational Linguistics, pages 184–191, San
Francisco. Morgan Kaufmann Publishers.
Michael John Collins. 1999. Head-driven statistical
models for natural language parsing. Ph.D. thesis,
University of Pennsylvania. Supervisor-Mitchell P.
Marcus.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In NAACL ’01: Sec-
ond meeting of the North American Chapter of the
Association for Computational Linguistics on Lan-
guage technologies 2001, pages 1–8, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Taku Kudo and Yuji Matsumoto. 2003. Fast meth-
ods for kernel-based text analysis. In ACL ’03: Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 24–31, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313–330.
Landon C. Noll. 2005. Fnv hash.
http://www.isthe.com/chongo/tech/comp/fnv/.
Anoop Sarkar. 2000. Practical experiments in parsing
using tree adjoining grammars. In Proceedings of
the Fifth International Workshop on Tree Adjoining
Grammars, Paris, France.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the conll-2000 shared task: Chunk-
ing. In Claire Cardie, Walter Daelemans, Claire
Nedellec, and Erik Tjong Kim Sang, editors, Pro-
ceedings of CoNLL-2000 and LLL-2000, pages 127–
132. Lisbon, Portugal.
Erik F. Tjong Kim Sang and Herv´e D´ejean. 2001.
Introduction to the conll-2001 shared task: Clause
identification. In Walter Daelemans and R´emi Za-
jac, editors, Proceedings of CoNLL-2001, pages 53–
57. Toulouse, France.
</reference>
<page confidence="0.997987">
300
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.953059">
<title confidence="0.9991295">Speeding Up Full Syntactic Parsing by Leveraging Partial Parsing Decisions</title>
<author confidence="0.997613">Glaysher Moldovan</author>
<affiliation confidence="0.999924">Language Computer Corporation</affiliation>
<address confidence="0.9962485">1701 N. Collins Blvd. Suite 2000 Richardson, TX 75080</address>
<abstract confidence="0.997231846153846">Parsing is a computationally intensive task due to the combinatorial explosion seen in chart parsing algorithms that explore possible parse trees. In this paper, we propose a method to limit the combinatorial explosion by restricting the CYK chart parsing algorithm based on the output of a chunk parser. When tested on the three parsers presented in (Collins, 1999), we observed an approximate three–fold speedup with only an average decrease of 0.17% in both precision and recall.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M´arquez</author>
</authors>
<title>Boosting trees for anti-spam email filtering.</title>
<date>2001</date>
<booktitle>In Proceedings of RANLP-01, 4th International Conference on Recent Advances in Natural Language Processing,</booktitle>
<location>Tzigov Chark, BG.</location>
<marker>Carreras, M´arquez, 2001</marker>
<rawString>Xavier Carreras and Llu´ıs M´arquez. 2001. Boosting trees for anti-spam email filtering. In Proceedings of RANLP-01, 4th International Conference on Recent Advances in Natural Language Processing, Tzigov Chark, BG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
</authors>
<title>Learning and inference for clause identification.</title>
<date>2002</date>
<booktitle>In ECML ’02: Proceedings of the 13th European Conference on Machine Learning,</booktitle>
<pages>35--47</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<marker>Carreras, M`arquez, Punyakanok, Roth, 2002</marker>
<rawString>Xavier Carreras, Llu´ıs M`arquez, Vasin Punyakanok, and Dan Roth. 2002. Learning and inference for clause identification. In ECML ’02: Proceedings of the 13th European Conference on Machine Learning, pages 35–47, London, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the first conference on North American chapter of the Association for Computational Linguistics,</booktitle>
<pages>132--139</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of the first conference on North American chapter of the Association for Computational Linguistics, pages 132–139, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Arivind Joshi and Martha Palmer, editors, Proceedings of the Thirty-Fourth Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>184--191</pages>
<publisher>Morgan Kaufmann Publishers.</publisher>
<location>San Francisco.</location>
<contexts>
<context position="10171" citStr="Collins, 1996" startWordPosition="1743" endWordPosition="1744">has three usable chunks, [an agreement]NP, [the Big Board]NP, and [the Chicago Mercantile Exchange]NP. This example shows the effects of the above algorithm on a longer sentence with multiple chunks. (4,5) (1,1) (2,2) (3,3) (4,4) (5,5) The red balloon flew away Chunk Span Figure 2: The same CYK example as in Figure 1. Blacked out box spans will not be calculated, while half toned box spans do not have to calculate as many possibilities because they depend on an uncalculated span. 3 Experiments &amp; Results 3.1 Our parser with chunks Our parser uses a simplified version of the model presented in (Collins, 1996). For this experiment,we tested four versions of our internal parser: • Our original parser. No optimizations or chunking information. • Our original parser with chunking information. • Our optimized parser without chunking information. • Our optimized parser with chunking information. For parsers that use chunking information, the runtime of the chunk parsing is included in the parser’s runtime, to show that total gains in runtime offset the cost of running the chunker. We trained the chunk parser on all of Treebank except for section 23, which will be used as the test set. We trained our par</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael John Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Arivind Joshi and Martha Palmer, editors, Proceedings of the Thirty-Fourth Annual Meeting of the Association for Computational Linguistics, pages 184–191, San Francisco. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania. Supervisor-Mitchell</institution>
<contexts>
<context position="3047" citStr="Collins, 1999" startWordPosition="495" endWordPosition="496">resent a CYK chart graph for the sentence “The red balloon flew away.” The algorithm will search the pyramid, from left to right, from the bottom to the top. Each box contains a pair of numbers that we will refer to as the span, which represent the sequence of words currently being considered. Calculating each “box” in the chart means trying all combinations of the lower parts of the box’s sub-pyramid to form possible sub-parse trees. For example, one calculates the results for the span (1, 4) by trying to combine the results in (1, 1) and (2, 4), (1, 2) and (3, 4), and (1, 3) and (4, 4). In (Collins, 1999), Collins describes three new parsers. The Model 2 gives the best output, parsing section 23 at 88.26% precision and 88.05% recall in 40 minutes. The Model 1 is by far the fastest of the three, parsing section 23 of Treebank (Marcus et al., 1994) at 87.75% precision and 87.45% recall in 26 minutes. Syntactic Chunking is the partial parsing process of segmenting a sentence into non295 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 295–300, Sydney, July 2006. c�2006 Association for Computational Linguistics (1,5) (1,4) (2,5) (1,3) (2,4) (3,5) (1,2) (2,3) (3,4) (4,5) (1</context>
<context position="5137" citStr="Collins, 1999" startWordPosition="845" endWordPosition="846">t contain a subject and a predicate (Tjong Kim Sang and D´ejean, 2001). Our clause identifier is an implementation of (Carreras et al., 2002), except that we use C5.0 as the machine learning method instead of Carreras’ own TreeBoost algorithm (Carreras and M´arquez, 2001). When trained and scored on the CoNLL 2001 shared task datal with the results of our chunker, our clause identifier performs at 90.73% precision, 73.72% recall on the development set and 88.85% precision, 70.22% recall on the test set. In this paper, we describe modifications to the version of the CYK algorithm described in (Collins, 1999) and experiment with the modifications to both our proprietary parser and the (Collins, 1999) parser. 1http://www.cnts.ua.ac.be/conll2001/clauses/clauses.tgz 2 Methods 2.1 Software Optimizations While each of the following optimizations, individually, had a smaller effect on our parser’s speed than the CYK restrictions, collectively, simple software engineering improvements resulted in the largest speed increase to our syntactic parser. In the experiments section, we will refer to this as the “Optimized” version. Optimization of the training data and internal symbolization: We discovered that </context>
<context position="8223" citStr="Collins, 1999" startWordPosition="1391" endWordPosition="1392"> parse. We say that c1 and c2 overlap iff s1 &lt; s2 &lt; t1 &lt; t2 or s2 &lt; s1 &lt; t2 &lt; t1, and we note it as c1 — c2.2 When using the output of a chunker, S is the set of spans that describe the non-VP, non-PP chunks where ti — si &gt; 0. During the CYK parse, after a span’s start and end points are selected, but before iterating across all splits of that span and their generative rules, we propose that the span in question be checked to make sure that it does not overlap with any span in set S. We give the pseudocode in Algorithm 1, which is a modification of the parse() function given in Appendix B of (Collins, 1999). Algorithm 1 The modified parse() function initialize() for span = 2 to n do for start = 1 to n — span + 1 do end +— start + span — 1 if bx E S(x 9L (start, end)) then complete(start, end) end if end for end for X +— edge in chart[1,n,TOP] with highest probability return X For example, given the chunk parse: [The red balloon]NP [flew]V P [away]ADV P, S = {(1, 3)} because there is only one chunk with a length greater than 1. Suppose we are analyzing the span (3,4) on the example sentence above. This span will be rejected, as it overlaps with the chunk (1,3); the leaf nodes “balloon” and “flew”</context>
<context position="12932" citStr="Collins, 1999" startWordPosition="2194" endWordPosition="2195">e (12,12) Mercantile (7,7) Big (9,1 (1,1) Under (5,5) by Precision Recall Time Original 82.79% 83.19% 25’45” With chunks 84.40% 83.74% 7’37” Optimized 83.86% 83.24% 4’28” With chunks 84.42% 84.06% 1’35” Table 1: Results from our parser on Section 23 Precision Recall Time Model 1 87.75% 87.45% 26’18” With chunks 87.63% 87.27% 8’54” Model 2 88.26% 88.05% 40’00” With chunks 88.04% 87.87% 13’47” Model 3 88.25% 88.04% 42’24” With chunks 88.10% 87.89% 14’58” Table 2: Results from the Collins parsers on Section 23 with chunking information was done using the EVALB package that was used to score the (Collins, 1999) parser. The numbers represent the labeled bracketing of all sentences; not just those with 40 words or less. The experiment was run on a dual Pentium 4, 3.20Ghz machine with two gigabytes of memory. The results are presented in Table 1. The most notable result is the greatly reduced time to parse when chunking information was added. Both versions of our parser saw an average three–fold increase in speed by leveraging chunking decisions. We also saw small increases in both precision and recall. 3.2 Collins Parsers with chunks To show that this method is general and does not exploit weaknesses </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael John Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, University of Pennsylvania. Supervisor-Mitchell P. Marcus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In NAACL ’01: Second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies 2001,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4165" citStr="Kudo and Matsumoto, 2001" startWordPosition="679" endWordPosition="682">06 Association for Computational Linguistics (1,5) (1,4) (2,5) (1,3) (2,4) (3,5) (1,2) (2,3) (3,4) (4,5) (1,1) (2,2) (3,3) (4,4) (5,5) The red balloon flew away Figure 1: The CYK parse visualized as a pyramid. CYK will search from the left to right, bottom to top. overlapping “chunks” of syntactically connected words. (Tjong Kim Sang and Buchholz, 2000) Unlike a parse tree, a set of syntactic chunks has no hierarchical information on how sequences of words relate to each other. The only information given is an additional label describing the chunk. We use the YamCha (Kudo and Matsumoto, 2003; Kudo and Matsumoto, 2001) chunker for our text chunking. When trained on all of Penn Treebank , except for section 23 and tested on section 23, the model had a precision of 95.96% and a recall of 96.08%. YamCha parses section 23 of Treebank in 36 seconds. Clause Identification is the partial parsing process of annotating the hierarchical structure of clauses—groupings of words that contain a subject and a predicate (Tjong Kim Sang and D´ejean, 2001). Our clause identifier is an implementation of (Carreras et al., 2002), except that we use C5.0 as the machine learning method instead of Carreras’ own TreeBoost algorithm</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines. In NAACL ’01: Second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies 2001, pages 1–8, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Fast methods for kernel-based text analysis.</title>
<date>2003</date>
<booktitle>In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>24--31</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4138" citStr="Kudo and Matsumoto, 2003" startWordPosition="675" endWordPosition="678">0, Sydney, July 2006. c�2006 Association for Computational Linguistics (1,5) (1,4) (2,5) (1,3) (2,4) (3,5) (1,2) (2,3) (3,4) (4,5) (1,1) (2,2) (3,3) (4,4) (5,5) The red balloon flew away Figure 1: The CYK parse visualized as a pyramid. CYK will search from the left to right, bottom to top. overlapping “chunks” of syntactically connected words. (Tjong Kim Sang and Buchholz, 2000) Unlike a parse tree, a set of syntactic chunks has no hierarchical information on how sequences of words relate to each other. The only information given is an additional label describing the chunk. We use the YamCha (Kudo and Matsumoto, 2003; Kudo and Matsumoto, 2001) chunker for our text chunking. When trained on all of Penn Treebank , except for section 23 and tested on section 23, the model had a precision of 95.96% and a recall of 96.08%. YamCha parses section 23 of Treebank in 36 seconds. Clause Identification is the partial parsing process of annotating the hierarchical structure of clauses—groupings of words that contain a subject and a predicate (Tjong Kim Sang and D´ejean, 2001). Our clause identifier is an implementation of (Carreras et al., 2002), except that we use C5.0 as the machine learning method instead of Carrer</context>
</contexts>
<marker>Kudo, Matsumoto, 2003</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2003. Fast methods for kernel-based text analysis. In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 24–31, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="3293" citStr="Marcus et al., 1994" startWordPosition="539" endWordPosition="542">sent the sequence of words currently being considered. Calculating each “box” in the chart means trying all combinations of the lower parts of the box’s sub-pyramid to form possible sub-parse trees. For example, one calculates the results for the span (1, 4) by trying to combine the results in (1, 1) and (2, 4), (1, 2) and (3, 4), and (1, 3) and (4, 4). In (Collins, 1999), Collins describes three new parsers. The Model 2 gives the best output, parsing section 23 at 88.26% precision and 88.05% recall in 40 minutes. The Model 1 is by far the fastest of the three, parsing section 23 of Treebank (Marcus et al., 1994) at 87.75% precision and 87.45% recall in 26 minutes. Syntactic Chunking is the partial parsing process of segmenting a sentence into non295 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 295–300, Sydney, July 2006. c�2006 Association for Computational Linguistics (1,5) (1,4) (2,5) (1,3) (2,4) (3,5) (1,2) (2,3) (3,4) (4,5) (1,1) (2,2) (3,3) (4,4) (5,5) The red balloon flew away Figure 1: The CYK parse visualized as a pyramid. CYK will search from the left to right, bottom to top. overlapping “chunks” of syntactically connected words. (Tjong Kim Sang and Buchholz, 200</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1994. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Landon C Noll</author>
</authors>
<date>2005</date>
<note>Fnv hash. http://www.isthe.com/chongo/tech/comp/fnv/.</note>
<contexts>
<context position="6697" citStr="Noll, 2005" startWordPosition="1096" endWordPosition="1097"> to single byte integers. In the most extreme cases, this reduces the length of non-word characters in a hash from 28 characters to 6. The training data takes up less space, hashes faster, and many string comparisons are reduced to simple integer comparisons. Optimization of our hash-table implementation: The majority of look ups in the hash-table at runtime were for non-existent keys. We put a bloomfilter on each hash bucket so that such lookups would often be trivially rejected, instead of having to compare the lookup key with every key in the bucket. We also switched to the Fowler/Noll/Vo (Noll, 2005) hash function, which is faster and has less collisions then our previous hash function. Optimization of critical areas: There were several areas in our code that were optimized after profiling our parser. Rules based pre/post-processing: We were able to get very minor increases in precision, recall and speed by adding hard coded rules to our parser that handle things that are handled poorly, specifically parenthetical phrases and quotations. 2.2 CYK restrictions In this section, we describe modifications that restrict the chart search based on the output of a partial parser (in this case, a c</context>
</contexts>
<marker>Noll, 2005</marker>
<rawString>Landon C. Noll. 2005. Fnv hash. http://www.isthe.com/chongo/tech/comp/fnv/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Sarkar</author>
</authors>
<title>Practical experiments in parsing using tree adjoining grammars.</title>
<date>2000</date>
<booktitle>In Proceedings of the Fifth International Workshop on Tree Adjoining Grammars,</booktitle>
<location>Paris, France.</location>
<contexts>
<context position="16604" citStr="Sarkar, 2000" startWordPosition="2815" endWordPosition="2816">at are not all constituents of the same parent node. While errors in a chunk parse are relatively localized, errors in the hierarchical structure of clauses can affect the entire parse tree, preventing the parser from exploring the correct high-level structure of the sentence. 299 4 Future Work While the modification given in section 2.2 is specific to CYK parsing, we believe that placing restrictions based on the output of a chunk parser is general enough to be applied to any generative, statistical parser, such as the Charniak parser (2000), or a Lexical Tree Adjoining Grammar based parser (Sarkar, 2000). Restrictions can be placed where the parser would explore possible trees that would violate the boundaries determined by the chunk parser, pruning paths that will not yield the correct parse tree. 5 Conclusion Using decisions from partial parsing greatly reduces the time to perform full syntactic parses, and we have presented a method to apply the information from partial parsing to full syntactic parsers that use a variant of the CYK algorithm. We have shown that this method is not specific to the implementation of our parser and causes a negligible effect on precision and recall, while dec</context>
</contexts>
<marker>Sarkar, 2000</marker>
<rawString>Anoop Sarkar. 2000. Practical experiments in parsing using tree adjoining grammars. In Proceedings of the Fifth International Workshop on Tree Adjoining Grammars, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Sabine Buchholz</author>
</authors>
<title>Introduction to the conll-2000 shared task: Chunking.</title>
<date>2000</date>
<booktitle>Proceedings of CoNLL-2000 and LLL-2000,</booktitle>
<pages>127--132</pages>
<editor>In Claire Cardie, Walter Daelemans, Claire Nedellec, and Erik Tjong Kim Sang, editors,</editor>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="3895" citStr="Sang and Buchholz, 2000" startWordPosition="633" endWordPosition="636"> (Marcus et al., 1994) at 87.75% precision and 87.45% recall in 26 minutes. Syntactic Chunking is the partial parsing process of segmenting a sentence into non295 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 295–300, Sydney, July 2006. c�2006 Association for Computational Linguistics (1,5) (1,4) (2,5) (1,3) (2,4) (3,5) (1,2) (2,3) (3,4) (4,5) (1,1) (2,2) (3,3) (4,4) (5,5) The red balloon flew away Figure 1: The CYK parse visualized as a pyramid. CYK will search from the left to right, bottom to top. overlapping “chunks” of syntactically connected words. (Tjong Kim Sang and Buchholz, 2000) Unlike a parse tree, a set of syntactic chunks has no hierarchical information on how sequences of words relate to each other. The only information given is an additional label describing the chunk. We use the YamCha (Kudo and Matsumoto, 2003; Kudo and Matsumoto, 2001) chunker for our text chunking. When trained on all of Penn Treebank , except for section 23 and tested on section 23, the model had a precision of 95.96% and a recall of 96.08%. YamCha parses section 23 of Treebank in 36 seconds. Clause Identification is the partial parsing process of annotating the hierarchical structure of cl</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the conll-2000 shared task: Chunking. In Claire Cardie, Walter Daelemans, Claire Nedellec, and Erik Tjong Kim Sang, editors, Proceedings of CoNLL-2000 and LLL-2000, pages 127– 132. Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Herv´e D´ejean.</title>
<date>2001</date>
<booktitle>In Walter Daelemans and R´emi Zajac, editors, Proceedings of CoNLL-2001,</booktitle>
<pages>53--57</pages>
<location>Toulouse, France.</location>
<marker>Erik, 2001</marker>
<rawString>Erik F. Tjong Kim Sang and Herv´e D´ejean. 2001. Introduction to the conll-2001 shared task: Clause identification. In Walter Daelemans and R´emi Zajac, editors, Proceedings of CoNLL-2001, pages 53– 57. Toulouse, France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>