<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001830">
<note confidence="0.606360333333333">
COPING WITH DYNAMIC SYNTACTIC STRATEGIES: AN EXPERIMENTAL ENVIRONMENT FOR AN
EXPERIMENTAL PARSER
Oliviero Stock
L.P. - Consiglio Nazionale delle Ricerche
Via dei Monti Tiburtini 509
00157 Roma, Italy
</note>
<sectionHeader confidence="0.770577" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999616714285714">
An environment built around WEDNESDAY 2, a chart
based parser is introduced. The environment is in
particular oriented towards exploring dynamic aspects
of parsing. It includses a number of specialized tools
that consent an easy, graphics-based interaction with
the parser. It is shown in particular how a combination
of the characteristics of the parser (based on the lexicon
and on dynamic unification) and of the environment
allow a nonspecialized user to explore heuristics that
may alter the basica control of the system. In this way,
for instance, a psycholinguist may explore ideas on
human parsing strategies, or a &amp;quot;language engineer&amp;quot; may
find useful heuristics for parsing within a particular
application.
</bodyText>
<sectionHeader confidence="0.967812" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.9999575">
Computer-based environments for the linguist are
conceived as sophisticated workbenches, built on Al
workstations around a specific parser, where the
linguist can try out his/her ideas about a grammar for a
certain natural language. In doing so, he/she can take
advantage of rich and easy-to-use graphic interfaces
that &amp;quot;know&amp;quot; about linguistics. Of course behind all this
lies the idea that cooperation with linguists will provide
better results in NLP. To substantiate this assumption it
may be recalled that some of the most interesting recent
ideas on syntax have been developed by means of joint
contributions from linguists and computational
linguists. Lexical-Functional Grammar [Bresnan 8c
Kaplan 1982], GPSG [Gazdar 1981], Functional
Grammar [Kay 1979], DCG [Pereira &amp; Warren 1980],
TAG [Joshi &amp; Levy 19821 are some of these ideas.
Instances of the tools introduced above are the LFG
environment, which was probably the first of its kind, an
environment built by Ron Kaplan for Lexical-
Functional Grammars, DPATR, built by Lauri
Karttunen and conceived as an environment that would
suit linguists of a number of different schools all
committed to a view of parsing as a- process that makes
use of an unification algorithm.
We have developed an environment with a somewhat
different purpose. Besides a number of tools for entering
data in graphic mode and inspecting resulting
structures, it provides a means for experimenting with
strategies in the course of the parsing process. We think
that this can be a valuable tool for gaining insight in the
cognitive aspects of language processing as well as for
tailoring the behaviour of the processor when used with
a particular (sub )language.
In this way an attempt can be made to answer basic
questions when following a nondeterministic approach:
what heuristics to apply when facing a certain choice
point, what to do when facing a failure point, i.e. which
of the pending processes to activate, taking account of
information resulting from the failure?
Of course this kind of environment makes sense only
because the parser it works on has some characteristics
that make it a psychologically interesting realization.
</bodyText>
<sectionHeader confidence="0.789691" genericHeader="method">
2. Motivation of the parser
</sectionHeader>
<bodyText confidence="0.999654181818182">
We shall classify psychologically motivated parsers in
three main categories. First: those that embody a strong
claim on the specification of the general control structure
of the human parsing mechanism. The authors usually
consider the level of basic control of the system as the
level they are simulating and are not concerned with
more particular heuristics. An instance of this class of
parsers is Marcus&apos;s parser [Marcus 1979], based on the
claim that, basically, parsing is a deterministic process:
only sentences that we perceive as &amp;quot;surprising&amp;quot; (the so
called garden paths) actually imply backtracking.
</bodyText>
<page confidence="0.996038">
234
</page>
<bodyText confidence="0.999252763157895">
Connectionist parsers are also instances of this category.
The second category refers to general linguistic
performance notions such as the &amp;quot;Lexical Preference
Principle&amp;quot; and the &amp;quot;Final Argument Principle&amp;quot; (Fodor,
Bresnan and Kaplan 19821. It includes theories of
processing like the one expressed by Wanner and
Maratsos for ATNs in the mid Seventies. In this
category the arguments are at the level of general
structural preference analysis. A third category tends
to consider at every moment of the parsing process, the
full complexity of the data and the hypothesized partial
internal representation of the sentence, including, at
least in principle, interaction with knowledge of the
world, aspects of memory, and particular task-oriented
behaviour.
Worth mentioning here is Church and Patil&apos;s (1982)
work which attempts to put order in the chaos of
complexity and &amp;quot;computational load&amp;quot;.
Our parser lies between the second and the third of the
above categories. The parser is seen as a
nondeterministic apparatus that disambiguates and
gives a &amp;quot;shallow&amp;quot; interpretation and an incremental
functional representation of each processed fragment of
the sentence. The state of the parser is supposed to be
cognitively meaningful at every moment of the process.
Furthermore, in particular, we are concerned with
aspects of flexible word ordering. This phenomenon is
specially relevant in Italian, where, for declarative
sentences, Subject-Verb-Object is only the most likely
order - the other five permutations of Subject Verb and
Object may occur as well. We shall briefly describe the
parser and its environment and, by way of example,
illustrate its behaviour in analyzing &amp;quot;oscillating&amp;quot;
sentences, i.e. sentences in which one first perceives a
fragment in one way, then changes one&apos;s mind and takes
it in a different way, then, as further input comes in,
going back to the previous pattern (and posssibly
continuing like this till the end of the sentence).
</bodyText>
<sectionHeader confidence="0.853171" genericHeader="method">
3. The parser
</sectionHeader>
<bodyText confidence="0.99980375862069">
WEDNESDAY 2 [Stock 19861 is a parser based on
linguistic knowledge distributed fundamentally
through the lexicon. A word reading includes:
- a semantic representation of the word ,in the form of a
semantic net shred;
- static syntactic information, including the category,
features, indication of linguistic functions that are
bound to particular nodes in the net. One particular
specification is the Main node, head of the syntactic
constituent the word occurs in;
- dynamic syntactic information , including impulses to
connect pieces of semantic information, guided by
syntactic constraints. Impulses look for &amp;quot;Fillers&amp;quot; on a
given search space (usually a substring). They have
alternatives, (for instance the word TELL has an
impulse to merge its object node with the &amp;quot;main&amp;quot; of
either an NP or a subordinate clause). An alternative
includes: a contextual condition of applicability, a
category, features, marking, side effects (through which,
for example, coreference between subject of a
subordinate clause and a function of the main clause can
be indicated). Impulses may also be directed to a
different search space than the normal one (see below);
- measures of likelihood. These are measures that are
used for deriving an overall measure of likelihood of a
partial analysis. Measures are included for the
likelihood of that particular reading of the word and for
aspects attached to an impulse: a) for one particular
alternative b) for the relative position the filler c) for the
overall necessity of finding a filler.
- a characterization of idioms involving that word. (For a
description of the part of the parser that deals with the
interpretation of flexible idioms see [Stock 1987]).
The only other data are in the form of simple (non
augmented) transition networks that only provide
restrictions on search spaces where impulses can look
for fillers. In more traditional words it deals with the
distribution of constituents. A distinguishing symbol,
$EXP, indicates that only the occurrence of something
expected by preceding words (i.e. for which an impulse
was set up) will allow the transition.
The parser is based on of the idea of chart parsing [Kay
1980, Kaplan 19731 [see Stock 19861. What is relevant
here is the fact that &amp;quot;edges&amp;quot; correspond to search spaces.
Edges are complex data structures provided with a rich
amount of information including a semantic
interpretation of the fragment, syntactic data, pending
impulses, an overall measure of likelihood, etc. Data on
an edge are &amp;quot;unified&amp;quot; dynamically as indicated below
An agenda is provided which includes four kinds of
tasks: lexical tasks, traversal tasks, insertion tasks,
virtual tasks. A lexical task specifies a possible reading
of a word to be inserted in the chart. A traversal task
specifies an active edge and an inactive edge that can
extend it. An insertion task specifies a nondeterministic
unification act, and a virtual task involves extension of
an edge to include an inactive edge far away in the
string (used for long distance dependencies).
</bodyText>
<page confidence="0.989022">
235
</page>
<figure confidence="0.997925525423729">
•44.44
row.
1219r/42121
14416.1. aoatisen
14. f .0.104 .01140.4104 e sex
4&apos; •
111•••••
I 1410111081 UI UKUINIIt • 1111111
1411111111.
WAVLE I 11.01..0.1
I 41, e
I n le 6
I 4 to 2
1 2 V 10 2
0011161. 2
I 4- 4.24
4.1 1.,;144161 6
:6 1,11486114 to 8
I 41801066 10. 3
VtIgIti: 3 404.1.V1
11116 TO, 4
0111111: 4
1 4: 40 0
I 17 111 tO
I 14 1181110116 To C
VTII1EX: 11 61•610
I 14 44•4146 16 6
0844110: 11
I :a t.044411f 6
Is Ma TO ,
to 61114118116 TO 7
004118; 11,STSOL
.1116 I* 0
0111110: 8 to.
(MA 944I neatly.
64,41.4 ,
EEEEEE rot! 11.10 LYON IS
ono. NIL
hum*&amp;quot; /It
•4.14.464.103.•
(111.1 (C11741 II
MOM 114
oee0.41111.
011.1 (18764
mew 01
00•I9IA NIL
aeweea • 4464 ;.0
Out) 4.400.44
III
!SKI LIDA.: 215
•4.1.140.1111 IV 7
.4.414.1.... • *Maws
yoatto.11
•
a- III lta..our
Fig. 1
A 1.4.116,11 Nv L
0*1968; 114■1•4111E
\
</figure>
<bodyText confidence="0.928730636363636">
The parser works asymmetrically with respect to the
&amp;quot;arrival&amp;quot; of the Main node: before the Main node
arrives, an extension of an edge has almost no effect. On
the arrival of the Main, all the present impulses are
&amp;quot;unleashed&amp;quot; and must find satisfaction. If all this does
not happen then the new edge supposedly to be added to
the chart is not added: the situation is recognized as a
failure. After the arrival of the Main, each new head
must find an impulse to merge with , and each incoming
impulse must find satisfaction. Again, if all this does not
happen, the new edge will not be added to the chart..
</bodyText>
<listItem confidence="0.511533">
4. Overview of the environment
</listItem>
<bodyText confidence="0.95194275">
WEDNESDAY 2 and its environment are
implemented on a Xerox Lisp Machine. The
environment is composed of a series of specialized tools,
each one based on one or more windows (fig 1).
Using a mouse the user selects a desired behaviour from
menus attached to the windows. We have the following
windows:
- the main WEDNESDAY 2 window, in which the
sentence is entered. Menus attached to this window
specify different modalities (including &amp;quot;through&amp;quot; and
&amp;quot;stepping&amp;quot;, &amp;quot;all parsings&amp;quot; or &amp;quot;one parsing&amp;quot;) and a
number of facilities;
- a window where one can view, enter and modify
transition networks graphically (fig. 2).
- a window where one can view, enter and modify the
lexicon. As a word entry is a complex object for
WEDNESDAY 2, entering a new word can be greatly
facilitated by a set of subwindows, each specialized in
one aspect of the word, &amp;quot;knowing&amp;quot; how it may be and
facilitating editing. The lexicon is a lexicon of roots: a
morphological analyzer and a lexicon manager are
integrated in the system. Let us briefly describe this
point. A lexicalist theory such as ours requires that a
large quantity of information be included in the lexicon.
This information has different origins: some comes from
the root and some from the affixes. All the information
must be put into a coherent data structure, through a a
particularly constrained unification based process.
</bodyText>
<page confidence="0.987971">
236
</page>
<figure confidence="0.9846705">
TranwOon horn nosh $ tc. hOhe 5,CC.MI ar• atiOwoo undet the tomahtroi
CL.E.CL-CoaLo)
Fig. 2
EvleMoze InsposE (et EMtRee=&amp;quot;..
Fig.3
Furthermore we must emphasize the fact that, just as in the lexicon (the Subject and Object functions and the
LFG, phenomena such as passivization are treated in related impulses attached to the active form are
ONE•AFTERCOMTIOU.EE
WENIERiE (..4-)133)
(MUtT .0
or pe/mARK 1 NIL A
VEDNER6E (88J)
(NWT .S
((T NP .i NIL NIL N
‘WEONERGE (SUBJ)
(NOV .8)
(0&amp;quot; NP .3 NIL NIL NI
el PACKS Xi ICS)
NEL!.
Volohl
VERB3 -PP -DI -06J
c
Beforakketihoodreaturas
Mark Sidetfect DONE
Noll* RESET
ABORT
,S A
.s CON
.0 oA
.7 rmA
.0
.0
00 POM
rent
PER
reu
TR&lt;
4-138.1 X3 NIL
011.11 X2 NIL
SULU )(.1 NIL
Gap
Test
VER53-A-0&amp;3-PERPP/IMF
VERS3-IND -OW
0-01-14F-054 VER63-110 -ACC
Load lay*
</figure>
<page confidence="0.988868">
237
</page>
<bodyText confidence="0.989921632352941">
rearranged). This is something that the morphological
analyzer must deal with. The internal behaviour of the
morphological analyzer is beyond the scope of the
present paper. We shall instead briefly discuss the
lexicon manager, the role of which will be emphasized
here.
The lexicon manager deals with the complex process of
entering data, maintaining, and .preprocessing the
lexicon. One notable aspect is that we have arranged the
lexicon on a hierachical baseis according to inheritance,
so that properties of a particular word can be inherited
from a word class and a word class can inherit aspects
from another class. One consequence of this is that we
can introduce a graphic aspect (fig 3) and the user can
browse through the lattice (the lexicon appears as a tree
of classes where one has specialized editors at each
level). What is even more relevant is the fact that one
can factorize knowledge that is in the lexicon, so that if
one particular phenomenon needs to be treated
differently, the change of information is immediate for
the words concerned. Of course this means also that
there is a space gain: the same information needs not to
be duplicated: complete word data are reconstructed
when required.
There is also a modality by which one can enter the
syntactic aspects of a word through examples, a /a
TEAM (Grosz 19841. The results are less precise, but
may be useful in a more application-oriented use of the
environment.
- a window showing the present configuration of the
chart;
- a window that permits zooming into one edge, showing
several aspects of the edge, including: its structural
aspect, its likelihood, the functional aspect, the
specification of unrealized impulses etc.
- a window displaying in graphic form the semantic
interpretation of an edge as a semantic net, or , if one
prefers so (this is usually the case when the net is too
complex) in logic format;
- a window where one can manipulate the agenda (fig 4).
Attached to this window we have a menu including a set
of functionalities that the tasks included in the agenda
to be manipulated: MOVE BEFORE, MOVE AFTER,
DELETE, SWITCH,UNDO etc. One just points to the
two particular tasks one wishes to operate on with the
mouse and then to the menu entry. In this way the
desired effect is obtained. The effect corresponds to
applying a different scheduling function: the tasks will
be picked up in the order here prescribed by hand. This
tool, when the parser is in the &amp;quot;stepping&amp;quot; modality,
Fig. 4
provides a very easy way of altering the default
behaviour of the system and of trying out new
strategies. This modality of scheduling by hand is
complemented by a series of counters that provide
control over the penetrance of these strategies. (The
penetrance of a nondeterministic algorithm is the ratio
between the steps that lead to the solution and the steps
that are carried out as a whole in trying to obtain the
solution. Of course this measure is included between 0
and 1.)
Dynamically, one tries to find sensible strategies, by
interacting with the agenda. When, after
experimenting formalizable heuristics have been tried
out, they can be introduced permanently into the system
through a given specialized function. This is the only
place where some knowledge of LISP and of the internal
structure of WEDNESAY 2 is required.
</bodyText>
<sectionHeader confidence="0.438131" genericHeader="method">
5. An example of exploration: oscillating sentences
</sectionHeader>
<bodyText confidence="0.990748">
We shall now briefly discuss a processing example that
we have been able to understand using the environment
mentioned above. The following example is a good
instance of flexibility and parsing problems present in
Italian:
a Napoli preferisco Roma a Milano.
The complete sentence reads &amp;quot;while in Naples I prefer
Rome to Milan&amp;quot;. The problem arises during the parsing
process with the fact that the &amp;quot;to&amp;quot; argument of &amp;quot;prefer&amp;quot;
in Italian may occur before the verb, and the locative
preposition &amp;quot;in&amp;quot; is a, the same word as the marking
preposition corresponding to &amp;quot;to&amp;quot;.
</bodyText>
<table confidence="0.9750499">
LT vertex: 6 eat PREPNARK tat: i
LT vertex: 6 eat: PREP u4: 1
TT A..., 9 i:15 NEW LII: .56 NEW Tr, -,..
LT vertex: 5 eat; N us: .2
LT vertex: 6 cat AOJ lti: .6
LT vertex: 5 cat; V Lit .6
LT vertex: 4 cat: PREPART LH; 1
LT vertex: 2 cal: PREP Let 1
GO ON
%MN
</table>
<page confidence="0.996305">
238
</page>
<bodyText confidence="0.9999667">
The reader/hearer first takes a Napoli as an adverbial
location , then, as the verb preferisco is perceived, a
Napoli is clearly reinterpreted as an argument of the
verb, (with a sense of surprise). As the sentence proceeds
after the object Roma, the new word a causes things to
change again and we go back with a sense of surprise to
the first hypothesis.
The following things should be noted: - when this
second reconsideration takes place, we feel the surprise,
but this does not cause us to reconsider the sentence, we
only go back adding more to an hypothesis that we were
already working at; -the surprise seems to be caused not
by a heavy computational load, but by a sudden
readjustment of the weights of the hypotheses. In a sense
it is a matter of memory, rather than computation.
We have been in a position to get WEDNESDAY 2 to
perfOrm naturally in such situations, taking advantage
of the environment. The following simple heuristics
were found: a) try solutions that satisfy the impulses (if
there are alternatives consider likelihoods); b) maintain
viscosity (prefer the path you are already following); and
C) follow the alternative that yields the edge with the
greatest likelihood, among edges of comparable lengths.
The likelihood of an edge depends on: 1) the likelihood of
the &amp;quot;included&amp;quot; edges; 2) the level of obligatoriness of the
filled impulses; 3) the likelihood of a particular relative
position of an argument in the string; 4) the likelihood of
that transition in the network, given the previous
transition.
The critical points in the sentence are the following
(note that we distinguish between a PP and a &amp;quot;marked
NP&amp;quot; possible argument of a verb, where the preposition
has no semantics asociated:
i) At the beginning: only the PP edge is expanded, (not
the one including a &amp;quot;marked NP&amp;quot;, because of static
preference for the former expressed in the lexicon and in
the transition network.
ii) After the verb is detected: on the one hand there is an
edge that, if extended, would not satisfy an obligatory
impulse, on the other hand, one that would possibly
satisfy one . The &amp;quot;marked NP&amp;quot; alternative is chosen
because of a) of the above heuristics.
iii) After the object Roma: when the preposition a comes
in, the edge that may extend the sentence with a PP on
the one hand, and on the other hand a cycling active
edge that is a promising satisfaction for an impulse are
compared. Since this relative position of the argument is
so favourable for the particular verb preferisco (.9 to .1
for this position compared to the antecedent one), the
parser proceeds with the alternative view, taking a
Napoli. as a modifier So it goes on, after reentering that
working hypothesis. The object is already there,
analyzed for the other reading and does not need to be
reanalyzed. So a Milano is taken as the filler for the
impulse and the analysis is concluded properly.
It should be noted that the Final Argument Principle
[Fodor, Kaplan and Bresnan 1982] does not work with
the flexibility characteristic of Italian. (The principle
would cause the reading &amp;quot;I prefer [Rome [ in Milan]] to
Naples&amp;quot; to be chosen at point iii) above).
</bodyText>
<sectionHeader confidence="0.685352" genericHeader="conclusions">
Conclusions
</sectionHeader>
<bodyText confidence="0.9999248">
We have introduced an environment built around
WEDNESDAY 2, a nondeterministic parser, oriented
towards experimenting with dynamic strategies. The
combination of interesting theories and such tools
realizes both meanings of the word &amp;quot;experimental&amp;quot;: 1)
something that implements new ideas in a prototype; 2)
something built for the sake of making experiments. We
think that this approach, possibly integrated with
experiments in psycholinguistics, can help increase our
understanding of parsing.
</bodyText>
<sectionHeader confidence="0.995182" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.903121">
Federico Cecconi&apos;s help in the graphic aspects and
lexicon management has been precious.
</bodyText>
<sectionHeader confidence="0.986492" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999159357142857">
Church, K. &amp; Path, R. Coping with syntactic ambiguity
or how to put the block in the box on the table. American
Journal of Computational Linguistics, 8; 139449(1982)
Ferrari,G. &amp; Stock,O. Strategy selection for an ATN
syntactic parser. Proceedings of the 18th Meeting of the
Association for Computational Linguistics, Philadelphia
(1980)
Ford, M., Bresnan, J. &amp; Kaplan, R. A competence based
theory of syntactic closure. In Bresnan,J., Ed. The
Mental Representation of Grammatical Relations. The
MIT Press, Cambridge, (1982)
Gazdar, G. Phrase structure grammar. In Jacobson and
Pullman (Eds.), The Nature of Syntactic Representation.
Dordrecht: Reidel (1981)
</reference>
<page confidence="0.978204">
239
</page>
<reference confidence="0.999173688888889">
Grosz, B. TEAM, a transportable natural language
interface system. In Proceedings of the Conference on
Applied Natural Language Processing, Santa Monica
(1983)
Joshi, A., &amp; Levy, L. Phrase structure trees bear more
fruits then you would have thought. American Journal
of Computational Linguistics, 8; 1-11 (1982)
Kaplan, R. A general syntactic processor. In Rustin, R.
(Ed.), Natural Language Processing. Englewood Cliffs,
N.J.: Prentice-Hall (1973)
Kaplan,R. &amp; Bresnan,J. Lexical-Functional Grammar: a
formal system for grammatical representation. In
Bresnan,J., Ed. The Mental Representation of
Grammatical Relations. The MIT Press, Cambridge,
173-281(1982)
Kay, M. Algorithm Schemata and Data Structures in
Syntactic Processing. Xerox, Palo Alto Research Center
(October 1980)
Kay, M. Functional Grammar. In Proceedings of the 5th
Meeting of the Berkeley Linguistic Society, Berkeley,
142-158(1979)
Marcus, M. An overview of a theory of syntactic
recognition for natural language. (Al memo 531).
Cambridge, Mass: Artificial Intelligence Laboratory,
(1979)
Pereira, F. &amp; Warren, D., Definite Clause Grammars for
language analysis. A survey of the formalism and a
comparison with Augmented Transition Networks.
Artificial Intelligence, 13; 231-278(1980)
Small, S. Word expert parsing: a theory of distributed
word-based natural language understanding. (Technical
Report TR-954 NSG-7253). Maryland: University of
Maryland (1980)
Stock, 0. Dynamic Unification in Lexically Based
Parsing. In Proceedings of the Seuenth European
Conference on Artificial Intelligence. Brighton, 212-221
(1986)
Stock, 0. Putting Idioms into a Lexicon Based Parser&apos;s
Head. To appear in Proceedings of the 25th Meeting of
the Association for Computational Linguistics. Stanford,
Ca. [19871
Thompson, H.S. Chart parsing and rule schemata in
GPSG. In Proceedings of the 19th Annual Meeting of the
Association for Computational Linguistics. Alexandria,
Va. (1981)
</reference>
<page confidence="0.996966">
240
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.674887">
<title confidence="0.9962205">COPING WITH DYNAMIC SYNTACTIC STRATEGIES: AN EXPERIMENTAL ENVIRONMENT FOR AN EXPERIMENTAL PARSER</title>
<author confidence="0.992469">Oliviero Stock</author>
<affiliation confidence="0.764547">L.P. - Consiglio Nazionale delle Ricerche</affiliation>
<address confidence="0.9421315">Via dei Monti Tiburtini 509 00157 Roma, Italy</address>
<abstract confidence="0.995671133333333">An environment built around WEDNESDAY 2, a chart based parser is introduced. The environment is in particular oriented towards exploring dynamic aspects of parsing. It includses a number of specialized tools that consent an easy, graphics-based interaction with the parser. It is shown in particular how a combination of the characteristics of the parser (based on the lexicon and on dynamic unification) and of the environment allow a nonspecialized user to explore heuristics that may alter the basica control of the system. In this way, for instance, a psycholinguist may explore ideas on human parsing strategies, or a &amp;quot;language engineer&amp;quot; may find useful heuristics for parsing within a particular application.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Church</author>
<author>R Path</author>
</authors>
<title>Coping with syntactic ambiguity or how to put the block in the box on the table.</title>
<date>1980</date>
<journal>American Journal of Computational Linguistics,</journal>
<booktitle>Proceedings of the 18th Meeting of the Association for Computational Linguistics,</booktitle>
<volume>139449</volume>
<issue>1982</issue>
<location>Philadelphia</location>
<marker>Church, Path, 1980</marker>
<rawString>Church, K. &amp; Path, R. Coping with syntactic ambiguity or how to put the block in the box on the table. American Journal of Computational Linguistics, 8; 139449(1982) Ferrari,G. &amp; Stock,O. Strategy selection for an ATN syntactic parser. Proceedings of the 18th Meeting of the Association for Computational Linguistics, Philadelphia (1980)</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ford</author>
<author>J Bresnan</author>
<author>R Kaplan</author>
</authors>
<title>A competence based theory of syntactic closure. In Bresnan,J., Ed. The Mental Representation of Grammatical Relations.</title>
<date>1982</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge,</location>
<marker>Ford, Bresnan, Kaplan, 1982</marker>
<rawString>Ford, M., Bresnan, J. &amp; Kaplan, R. A competence based theory of syntactic closure. In Bresnan,J., Ed. The Mental Representation of Grammatical Relations. The MIT Press, Cambridge, (1982)</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
</authors>
<title>Phrase structure grammar. In Jacobson and Pullman (Eds.), The Nature of Syntactic Representation.</title>
<date>1981</date>
<location>Dordrecht: Reidel</location>
<contexts>
<context position="1668" citStr="Gazdar 1981" startWordPosition="248" endWordPosition="249">und a specific parser, where the linguist can try out his/her ideas about a grammar for a certain natural language. In doing so, he/she can take advantage of rich and easy-to-use graphic interfaces that &amp;quot;know&amp;quot; about linguistics. Of course behind all this lies the idea that cooperation with linguists will provide better results in NLP. To substantiate this assumption it may be recalled that some of the most interesting recent ideas on syntax have been developed by means of joint contributions from linguists and computational linguists. Lexical-Functional Grammar [Bresnan 8c Kaplan 1982], GPSG [Gazdar 1981], Functional Grammar [Kay 1979], DCG [Pereira &amp; Warren 1980], TAG [Joshi &amp; Levy 19821 are some of these ideas. Instances of the tools introduced above are the LFG environment, which was probably the first of its kind, an environment built by Ron Kaplan for LexicalFunctional Grammars, DPATR, built by Lauri Karttunen and conceived as an environment that would suit linguists of a number of different schools all committed to a view of parsing as a- process that makes use of an unification algorithm. We have developed an environment with a somewhat different purpose. Besides a number of tools for </context>
</contexts>
<marker>Gazdar, 1981</marker>
<rawString>Gazdar, G. Phrase structure grammar. In Jacobson and Pullman (Eds.), The Nature of Syntactic Representation. Dordrecht: Reidel (1981)</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
</authors>
<title>TEAM, a transportable natural language interface system.</title>
<date>1983</date>
<booktitle>In Proceedings of the Conference on Applied Natural Language Processing,</booktitle>
<location>Santa Monica</location>
<marker>Grosz, 1983</marker>
<rawString>Grosz, B. TEAM, a transportable natural language interface system. In Proceedings of the Conference on Applied Natural Language Processing, Santa Monica (1983)</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
<author>L Levy</author>
</authors>
<title>Phrase structure trees bear more fruits then you would have thought.</title>
<date>1982</date>
<journal>American Journal of Computational Linguistics,</journal>
<volume>8</volume>
<pages>1--11</pages>
<contexts>
<context position="1753" citStr="Joshi &amp; Levy 1982" startWordPosition="260" endWordPosition="263">mmar for a certain natural language. In doing so, he/she can take advantage of rich and easy-to-use graphic interfaces that &amp;quot;know&amp;quot; about linguistics. Of course behind all this lies the idea that cooperation with linguists will provide better results in NLP. To substantiate this assumption it may be recalled that some of the most interesting recent ideas on syntax have been developed by means of joint contributions from linguists and computational linguists. Lexical-Functional Grammar [Bresnan 8c Kaplan 1982], GPSG [Gazdar 1981], Functional Grammar [Kay 1979], DCG [Pereira &amp; Warren 1980], TAG [Joshi &amp; Levy 19821 are some of these ideas. Instances of the tools introduced above are the LFG environment, which was probably the first of its kind, an environment built by Ron Kaplan for LexicalFunctional Grammars, DPATR, built by Lauri Karttunen and conceived as an environment that would suit linguists of a number of different schools all committed to a view of parsing as a- process that makes use of an unification algorithm. We have developed an environment with a somewhat different purpose. Besides a number of tools for entering data in graphic mode and inspecting resulting structures, it provides a mean</context>
</contexts>
<marker>Joshi, Levy, 1982</marker>
<rawString>Joshi, A., &amp; Levy, L. Phrase structure trees bear more fruits then you would have thought. American Journal of Computational Linguistics, 8; 1-11 (1982)</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kaplan</author>
</authors>
<title>A general syntactic processor. In</title>
<date>1973</date>
<publisher>Prentice-Hall</publisher>
<contexts>
<context position="7912" citStr="Kaplan 1973" startWordPosition="1238" endWordPosition="1239">ord. (For a description of the part of the parser that deals with the interpretation of flexible idioms see [Stock 1987]). The only other data are in the form of simple (non augmented) transition networks that only provide restrictions on search spaces where impulses can look for fillers. In more traditional words it deals with the distribution of constituents. A distinguishing symbol, $EXP, indicates that only the occurrence of something expected by preceding words (i.e. for which an impulse was set up) will allow the transition. The parser is based on of the idea of chart parsing [Kay 1980, Kaplan 19731 [see Stock 19861. What is relevant here is the fact that &amp;quot;edges&amp;quot; correspond to search spaces. Edges are complex data structures provided with a rich amount of information including a semantic interpretation of the fragment, syntactic data, pending impulses, an overall measure of likelihood, etc. Data on an edge are &amp;quot;unified&amp;quot; dynamically as indicated below An agenda is provided which includes four kinds of tasks: lexical tasks, traversal tasks, insertion tasks, virtual tasks. A lexical task specifies a possible reading of a word to be inserted in the chart. A traversal task specifies an activ</context>
</contexts>
<marker>Kaplan, 1973</marker>
<rawString>Kaplan, R. A general syntactic processor. In Rustin, R. (Ed.), Natural Language Processing. Englewood Cliffs, N.J.: Prentice-Hall (1973)</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Kaplan</author>
<author>J Bresnan</author>
</authors>
<title>Lexical-Functional Grammar: a formal system for grammatical representation. In Bresnan,J., Ed. The Mental Representation of Grammatical Relations.</title>
<pages>173--281</pages>
<publisher>The MIT Press,</publisher>
<location>Cambridge,</location>
<marker>Kaplan, Bresnan, </marker>
<rawString>Kaplan,R. &amp; Bresnan,J. Lexical-Functional Grammar: a formal system for grammatical representation. In Bresnan,J., Ed. The Mental Representation of Grammatical Relations. The MIT Press, Cambridge, 173-281(1982)</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Algorithm Schemata and Data Structures</title>
<date>1980</date>
<booktitle>in Syntactic Processing. Xerox, Palo Alto Research</booktitle>
<location>Center</location>
<contexts>
<context position="7899" citStr="Kay 1980" startWordPosition="1236" endWordPosition="1237">ing that word. (For a description of the part of the parser that deals with the interpretation of flexible idioms see [Stock 1987]). The only other data are in the form of simple (non augmented) transition networks that only provide restrictions on search spaces where impulses can look for fillers. In more traditional words it deals with the distribution of constituents. A distinguishing symbol, $EXP, indicates that only the occurrence of something expected by preceding words (i.e. for which an impulse was set up) will allow the transition. The parser is based on of the idea of chart parsing [Kay 1980, Kaplan 19731 [see Stock 19861. What is relevant here is the fact that &amp;quot;edges&amp;quot; correspond to search spaces. Edges are complex data structures provided with a rich amount of information including a semantic interpretation of the fragment, syntactic data, pending impulses, an overall measure of likelihood, etc. Data on an edge are &amp;quot;unified&amp;quot; dynamically as indicated below An agenda is provided which includes four kinds of tasks: lexical tasks, traversal tasks, insertion tasks, virtual tasks. A lexical task specifies a possible reading of a word to be inserted in the chart. A traversal task speci</context>
</contexts>
<marker>Kay, 1980</marker>
<rawString>Kay, M. Algorithm Schemata and Data Structures in Syntactic Processing. Xerox, Palo Alto Research Center (October 1980)</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Kay</author>
</authors>
<title>Functional Grammar.</title>
<booktitle>In Proceedings of the 5th Meeting of the Berkeley Linguistic Society,</booktitle>
<pages>142--158</pages>
<location>Berkeley,</location>
<marker>Kay, </marker>
<rawString>Kay, M. Functional Grammar. In Proceedings of the 5th Meeting of the Berkeley Linguistic Society, Berkeley, 142-158(1979)</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
</authors>
<title>An overview of a theory of syntactic recognition for natural language. (Al memo 531).</title>
<date>1979</date>
<institution>Artificial Intelligence Laboratory,</institution>
<location>Cambridge, Mass:</location>
<contexts>
<context position="3572" citStr="Marcus 1979" startWordPosition="557" endWordPosition="558">his kind of environment makes sense only because the parser it works on has some characteristics that make it a psychologically interesting realization. 2. Motivation of the parser We shall classify psychologically motivated parsers in three main categories. First: those that embody a strong claim on the specification of the general control structure of the human parsing mechanism. The authors usually consider the level of basic control of the system as the level they are simulating and are not concerned with more particular heuristics. An instance of this class of parsers is Marcus&apos;s parser [Marcus 1979], based on the claim that, basically, parsing is a deterministic process: only sentences that we perceive as &amp;quot;surprising&amp;quot; (the so called garden paths) actually imply backtracking. 234 Connectionist parsers are also instances of this category. The second category refers to general linguistic performance notions such as the &amp;quot;Lexical Preference Principle&amp;quot; and the &amp;quot;Final Argument Principle&amp;quot; (Fodor, Bresnan and Kaplan 19821. It includes theories of processing like the one expressed by Wanner and Maratsos for ATNs in the mid Seventies. In this category the arguments are at the level of general stru</context>
</contexts>
<marker>Marcus, 1979</marker>
<rawString>Marcus, M. An overview of a theory of syntactic recognition for natural language. (Al memo 531). Cambridge, Mass: Artificial Intelligence Laboratory, (1979)</rawString>
</citation>
<citation valid="false">
<authors>
<author>F Pereira</author>
<author>D Warren</author>
</authors>
<title>Definite Clause Grammars for language analysis. A survey of the formalism and a comparison with Augmented Transition Networks.</title>
<journal>Artificial Intelligence,</journal>
<volume>13</volume>
<pages>231--278</pages>
<marker>Pereira, Warren, </marker>
<rawString>Pereira, F. &amp; Warren, D., Definite Clause Grammars for language analysis. A survey of the formalism and a comparison with Augmented Transition Networks. Artificial Intelligence, 13; 231-278(1980)</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Small</author>
</authors>
<title>Word expert parsing: a theory of distributed word-based natural language understanding.</title>
<date>1980</date>
<tech>(Technical Report TR-954 NSG-7253).</tech>
<institution>Maryland: University of Maryland</institution>
<marker>Small, 1980</marker>
<rawString>Small, S. Word expert parsing: a theory of distributed word-based natural language understanding. (Technical Report TR-954 NSG-7253). Maryland: University of Maryland (1980)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stock</author>
</authors>
<title>Dynamic Unification in Lexically Based Parsing.</title>
<date>1986</date>
<booktitle>In Proceedings of the Seuenth European Conference on Artificial Intelligence.</booktitle>
<location>Brighton,</location>
<contexts>
<context position="5750" citStr="Stock 1986" startWordPosition="893" endWordPosition="894"> Italian, where, for declarative sentences, Subject-Verb-Object is only the most likely order - the other five permutations of Subject Verb and Object may occur as well. We shall briefly describe the parser and its environment and, by way of example, illustrate its behaviour in analyzing &amp;quot;oscillating&amp;quot; sentences, i.e. sentences in which one first perceives a fragment in one way, then changes one&apos;s mind and takes it in a different way, then, as further input comes in, going back to the previous pattern (and posssibly continuing like this till the end of the sentence). 3. The parser WEDNESDAY 2 [Stock 19861 is a parser based on linguistic knowledge distributed fundamentally through the lexicon. A word reading includes: - a semantic representation of the word ,in the form of a semantic net shred; - static syntactic information, including the category, features, indication of linguistic functions that are bound to particular nodes in the net. One particular specification is the Main node, head of the syntactic constituent the word occurs in; - dynamic syntactic information , including impulses to connect pieces of semantic information, guided by syntactic constraints. Impulses look for &amp;quot;Fillers&amp;quot; </context>
<context position="7929" citStr="Stock 1986" startWordPosition="1241" endWordPosition="1242">ption of the part of the parser that deals with the interpretation of flexible idioms see [Stock 1987]). The only other data are in the form of simple (non augmented) transition networks that only provide restrictions on search spaces where impulses can look for fillers. In more traditional words it deals with the distribution of constituents. A distinguishing symbol, $EXP, indicates that only the occurrence of something expected by preceding words (i.e. for which an impulse was set up) will allow the transition. The parser is based on of the idea of chart parsing [Kay 1980, Kaplan 19731 [see Stock 19861. What is relevant here is the fact that &amp;quot;edges&amp;quot; correspond to search spaces. Edges are complex data structures provided with a rich amount of information including a semantic interpretation of the fragment, syntactic data, pending impulses, an overall measure of likelihood, etc. Data on an edge are &amp;quot;unified&amp;quot; dynamically as indicated below An agenda is provided which includes four kinds of tasks: lexical tasks, traversal tasks, insertion tasks, virtual tasks. A lexical task specifies a possible reading of a word to be inserted in the chart. A traversal task specifies an active edge and an ina</context>
</contexts>
<marker>Stock, 1986</marker>
<rawString>Stock, 0. Dynamic Unification in Lexically Based Parsing. In Proceedings of the Seuenth European Conference on Artificial Intelligence. Brighton, 212-221 (1986)</rawString>
</citation>
<citation valid="false">
<authors>
<author>Stock</author>
</authors>
<title>Putting Idioms into a Lexicon Based Parser&apos;s Head. To appear in</title>
<booktitle>Proceedings of the 25th Meeting of the Association for Computational Linguistics.</booktitle>
<pages>19871</pages>
<location>Stanford, Ca.</location>
<marker>Stock, </marker>
<rawString>Stock, 0. Putting Idioms into a Lexicon Based Parser&apos;s Head. To appear in Proceedings of the 25th Meeting of the Association for Computational Linguistics. Stanford, Ca. [19871</rawString>
</citation>
<citation valid="true">
<authors>
<author>H S Thompson</author>
</authors>
<title>Chart parsing and rule schemata in GPSG.</title>
<date>1981</date>
<booktitle>In Proceedings of the 19th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<location>Alexandria, Va.</location>
<marker>Thompson, 1981</marker>
<rawString>Thompson, H.S. Chart parsing and rule schemata in GPSG. In Proceedings of the 19th Annual Meeting of the Association for Computational Linguistics. Alexandria, Va. (1981)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>