<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000957">
<title confidence="0.55593675">
Dijkstra-WSA: A Graph-Based Approach to Word Sense Alignment
Michael Matuschek $ and Iryna Gurevych †$
† Ubiquitous Knowledge Processing Lab (UKP-DIPF),
German Institute for Educational Research and Educational Information
</title>
<author confidence="0.587571">
Schloßstr. 29, 60486 Frankfurt, Germany
$Ubiquitous Knowledge Processing Lab (UKP-TUDA),
</author>
<affiliation confidence="0.967406">
Department of Computer Science, Technische Universit¨at Darmstadt
</affiliation>
<address confidence="0.813009">
Hochschulstr. 10, 64289 Darmstadt, Germany
</address>
<email confidence="0.901627">
http://www.ukp.tu-darmstadt.de
</email>
<sectionHeader confidence="0.992984" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999668">
In this paper, we present Dijkstra-WSA, a
novel graph-based algorithm for word sense
alignment. We evaluate it on four different
pairs of lexical-semantic resources with dif-
ferent characteristics (WordNet-OmegaWiki,
WordNet-Wiktionary, GermaNet-Wiktionary
and WordNet-Wikipedia) and show that it
achieves competitive performance on 3 out
of 4 datasets. Dijkstra-WSA outperforms the
state of the art on every dataset if it is com-
bined with a back-off based on gloss similar-
ity. We also demonstrate that Dijkstra-WSA
is not only flexibly applicable to different re-
sources but also highly parameterizable to op-
timize for precision or recall.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99976894">
Lexical-semantic resources (LSRs) are a corner-
stone for many Natural Language Processing (NLP)
applications such as word sense disambiguation
(WSD) and information extraction. However, the
growing demand for large-scale resources in dif-
ferent languages is hard to meet. The Princeton
WordNet (WN) (Fellbaum, 1998) is widely used for
English, but for most languages corresponding re-
sources are considerably smaller or missing. Col-
laboratively constructed resources like Wiktionary
(WKT) and OmegaWiki (OW) provide a viable op-
tion for such cases and seem especially suitable
for smaller languages (Matuschek et al., 2013), but
there are still considerable gaps in coverage which
need to be filled. A related problem is that there
usually does not exist a single resource which works
best for all purposes, as different LSRs cover differ-
ent words, senses and information types.
These considerations have sparked increasing re-
search efforts in the area of word sense alignment
(WSA). It has been shown that aligned resources
can indeed lead to better performance than using the
resources individually. Examples include seman-
tic parsing using FrameNet (FN), WN, and VerbNet
(VN) (Shi and Mihalcea, 2005), word sense disam-
biguation using an alignment of WN and Wikipedia
(WP) (Navigli and Ponzetto, 2012) and semantic
role labeling using a combination of PropBank, VN
and FN in the SemLink project (Palmer, 2009).
Some of these approaches to WSA either rely heav-
ily on manual labor (e.g. Shi and Mihalcea (2005))
or on information which is only present in few
resources such as the most frequent sense (MFS)
(Suchanek et al., 2008). This makes it difficult to
apply them to a larger set of resources.
In earlier work, we presented the large-scale re-
source UBY (Gurevych et al., 2012). It contains
nine resources in two languages which are mapped
to a uniform representation using the LMF standard
(Eckle-Kohler et al., 2012). They are thus struc-
turally interoperable. UBY contains pairwise sense
alignments between a subset of these resources, and
this work also presented a framework for creat-
ing alignments based on the similarity of glosses
(Meyer and Gurevych, 2011). However, it is not
clear to what extent this approach can be applied to
resources which lack this kind of information (see
Section 3).
In summary, aligning senses is a key requirement
for semantic interoperability of LSRs to increase the
</bodyText>
<page confidence="0.988774">
151
</page>
<bodyText confidence="0.968278954545454">
Transactions of the Association for Computational Linguistics, 1 (2013) 151–164. Action Editor: Patrick Pantel.
Submitted 12/2012; Revised 2/2013; Published 5/2013. c�2013 Association for Computational Linguistics.
coverage and effectiveness in NLP tasks. Still, exist-
ing efforts are mostly focused on specific types of re-
sources (most often requiring glosses) or application
scenarios. In this paper, we propose an approach to
alleviate this and present Dijkstra-WSA, a novel, ro-
bust algorithm for word sense alignment which is
applicable to a wide variety of resource pairs and
languages. For the first time, we apply a graph-based
algorithm which works on full graph representations
of both resources to word sense alignment. This en-
ables us to take a more abstract perspective and re-
duce the problem of identifying equivalent senses to
the problem of matching nodes in these graphs. Also
for the first time, we comparatively evaluate a WSA
algorithm on a variety of different datasets with dif-
ferent characteristics.
The key properties of Dijkstra-WSA are:
Robustness The entities within the LSRs which
are to be aligned (usually senses or synsets) are mod-
eled as nodes in the graph. These nodes are con-
nected by an edge if they are semantically related.
While, for instance, semantic relations lend them-
selves very well to deriving edges, different possi-
bilities for graph construction are equally valid as
the algorithm is agnostic to the origin of the edges.
Language-independence No external resources
such as corpora or other dictionaries are needed; the
graph construction and alignment only rely on the
information from the considered LSRs.
Flexibility The graph construction as well as the
actual alignment are highly parameterizable to ac-
commodate different requirements regarding preci-
sion or recall.
The rest of this paper is structured as follows:
In Section 2 we give a precise problem description
and introduce the resources covered in our experi-
ments, in Section 3 we discuss some related work,
while our graph-based algorithm Dijkstra-WSA is
presented in Section 4. We describe an evaluation
on four datasets with different properties, including
an error analysis, in Section 5 and conclude in Sec-
tion 6, pointing out directions for future work.
</bodyText>
<sectionHeader confidence="0.942547" genericHeader="introduction">
2 Notation and Resources
</sectionHeader>
<subsectionHeader confidence="0.880431">
2.1 Problem Description
</subsectionHeader>
<bodyText confidence="0.999863125">
A word sense alignment, or alignment for short, is
formally defined as a list of pairs of senses from
two LSRs. A pair of aligned senses denote the same
meaning. E.g., the two senses of letter “The conven-
tional characters of the alphabet used to represent
speech” and “A symbol in an alphabet, bookstave”
(taken from WN and WKT, respectively) are clearly
equivalent and should be aligned.
</bodyText>
<subsectionHeader confidence="0.995004">
2.2 Evaluation Resources
</subsectionHeader>
<bodyText confidence="0.999913225">
For the evaluation of Dijkstra-WSA, we align four
pairs of LSRs used in previous work, namely WN-
OW (Gurevych et al., 2012), WN-WKT (Meyer and
Gurevych, 2011), GN-WKT (Henrich et al., 2011)
and WN-WP (Niemann and Gurevych, 2011). Our
goal is to cover resources with different character-
istics: Expert-built (WN, GN) and collaboratively
constructed LSRs (WP, WKT, OW), resources in
different languages (English and German) and also
resources with few sense descriptions (GN) or se-
mantic relations (WKT). We contrastively discuss
the results of the Dijkstra-WSA algorithm on these
different datasets and relate the results to the prop-
erties of the LSRs involved. Moreover, using exist-
ing datasets ensures comparability to previous work
which discusses only one dataset at a time.
WordNet (WN) (Fellbaum, 1998) is a lexical re-
source for the English language created at Princeton
University. The resource is organized in sets of syn-
onymous words (synsets) which are represented by
glosses (sometimes accompanied by example sen-
tences) and organized in a hierarchy. The latest ver-
sion 3.0 contains 117,659 synsets.
Wikipedia (WP) is a freely available, multilin-
gual online encyclopedia. WP can be edited by ev-
ery Web user, which causes rapid growth: By Febru-
ary 2013 the English WP contained over 4,000,000
article pages. Each article usually describes a dis-
tinct concept, and articles are connected by hyper-
links within the article texts.
Wiktionary (WKT) is the dictionary pendant to
WP. By February 2013 the English WKT contained
over 3,200,000 article pages, while the German edi-
tion contained over 200,000 ones. For each word,
multiple senses can be encoded. Similar to WN,
they are represented by a gloss and usage exam-
ples. There also exist hyperlinks to synonyms, hy-
pernyms, meronyms etc. The targets of these rela-
tions are not senses, however, but merely lexemes
(i.e. the relations are not disambiguated).
</bodyText>
<page confidence="0.996901">
152
</page>
<table confidence="0.999101444444444">
LSRs P/R/F /Acc. Approach
Meyer and Gurevych (2011) WN-WKT 0.67/0.65/0.66/0.91 Gloss similarity + Machine learning
Niemann and Gurevych (2011) WN-WP 0.78/0.78/0.78/0.95 Gloss similarity + Machine learning
Henrich et al. (2011) GN-WKT 0.84/0.85/0.84/0.94 Pseudo-gloss overlap
de Melo and Weikum (2010) WN-WP 0.86/NA/NA/NA Gloss/article overlap
Laparra et al. (2010) FN-WN 0.79/0.79/0.79/NA Dijkstra-SSI+ (WSD algorithm)
Navigli (2009) WN 0.64/0.64/0.64/NA Graph-based WSD of WN glosses
Ponzetto and Navigli (2009) WN-WP NA/NA/NA/0.81 Graph-based, only for WP categories
Navigli and Ponzetto (2012) WN-WP 0.81/0.75/0.78/0.83 Graph-based WSA using WN relations
</table>
<tableCaption confidence="0.999929">
Table 1: Summary of various approaches to WSA. “NA” stands for “Not Available”.
</tableCaption>
<bodyText confidence="0.997382928571429">
OmegaWiki (OW) is a freely editable online
dictionary like WKT. However, there do not ex-
ist distinct language editions as OW is organized
in language-independent concepts (“Defined Mean-
ings”) to which lexicalizations in various languages
are attached. These can be considered as multilin-
gual synsets, and they are interconnected by unam-
biguous relations just like WN. As of February 2013,
OW contains over 46,000 of these concepts and lex-
icalizations in over 400 languages.
GermaNet (GN) is the German counterpart to
WN (Hamp and Feldweg, 1997). It is also organized
in synsets (around 70,000 in the latest version 7.0)
which are connected via semantic relations.
</bodyText>
<sectionHeader confidence="0.999949" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999984833333333">
The are two strands of closely related work:
Similarity-based and graph-based approaches to
word sense alignment. To our knowledge, there ex-
ists no previous work which fully represents both
LSRs involved in an alignment as graphs. We give a
summary of different approaches in Table 1.
</bodyText>
<subsectionHeader confidence="0.99615">
3.1 Similarity-based Approaches
</subsectionHeader>
<bodyText confidence="0.999968605263158">
Niemann and Gurevych (2011) and Meyer and
Gurevych (2011) created WN-WP and WN-WKT
alignments using a framework which first calcu-
lates the similarity of glosses (or glosses and ar-
ticles in the case of WN-WP) using either cosine
or personalized page rank (PPR) similarity (Agirre
and Soroa, 2009) and then learns a threshold on the
gold standard to classify each pair of senses as a
(non-)valid alignment. This approach was later ex-
tended to cross-lingual alignment between the Ger-
man OW and WN (Gurevych et al., 2012) using a
machine translation component. However, its appli-
cability depends on the availability and quality of the
glosses, which are not present in every case (e.g. for
VN). Moreover, as it involves supervised machine
learning, it requires the initial effort of manually an-
notating a sufficient amount of training data. Hen-
rich et al. (2011) use a similar approach for align-
ing GN and WKT. However, they use word over-
lap as a similarity measure and do not require a ma-
chine learning component as they align to the can-
didate sense with the highest similarity regardless of
the absolute value. The alignment of WP articles
and WN synsets reported by de Melo and Weikum
(2010) also relies on word overlap.
Although these approaches give reasonable re-
sults (with precision in the range of 0.67-0.84), they
all depend on the lexical knowledge contained in the
glosses, yielding low recall if there is insufficient
lexical overlap (known as the “lexical gap”, see for
instance (Meyer and Gurevych, 2011)). Consider
these two senses of Thessalonian in WKT and WN:
“A native or inhabitant of Thessalonica” and “Some-
one or something from, or pertaining to, Thessa-
loniki”. These are (mostly) identical and should be
aligned, but there is no word overlap due to the in-
terchangeable usage of the synonyms Thessalonica
and Thessaloniki.
</bodyText>
<subsectionHeader confidence="0.996306">
3.2 Graph-based Approaches
</subsectionHeader>
<bodyText confidence="0.9998503">
Laparra et al. (2010) utilize the SSI-Dijkstra+ al-
gorithm to align FN lexical units (LUs) with WN
synsets. The basic idea is to align monosemous LUs
first and, based on this, find the closest synset in WN
for the other LUs in the same frame. However, as
SSI-Dijkstra+ is a word sense disambiguation (not
alignment) algorithm, the LUs are merely consid-
ered as texts which are to be disambiguated; there
is no attempt made to build a global graph structure
for FN. Moreover, the algorithm solely relies on the
</bodyText>
<page confidence="0.99864">
153
</page>
<bodyText confidence="0.999946756097561">
semantic relations found in WN and eXtended WN
(Mihalcea and Moldovan, 2001). Thus, it is not ap-
plicable to other resources which have no or only
few relations such as WKT.
Navigli (2009) aims at disambiguating WN
glosses, i.e. assigning the correct senses to all non-
stopwords in each WN gloss. His approach is to
find the shortest possible circles in the WN relation
graph to identify the correct disambiguation. In later
work, this idea was extended to the disambiguation
of translations in a bilingual dictionary (Flati and
Navigli, 2012). However, there is no discussion of
how this idea could be applied to word sense align-
ment of two or more resources. We build upon this
idea of finding shortest paths (circles are a special
kind of path) and extend it to multiple resources and
edges other than semantic relations, in particular WP
links and links to senses of monosemous lexemes
appearing in glosses.
Ponzetto and Navigli (2009) propose a graph-
based method to tackle the related, but slightly dif-
ferent problem of aligning WN synsets and WP cat-
egories (not articles). Using semantic relations, they
build WN subgraphs for each WP category and then
align those synsets which best match the category
structure. In later work, Navigli and Ponzetto (2012)
also align WN with the full WP. They build “disam-
biguation contexts” for the senses in both resources
by using, for instance, WP redirects or WN glosses
and then compute the similarity between these con-
texts. Again, a graph structure is built from WN se-
mantic relations covering all possible senses in these
contexts. The goal is to determine which WN sense
is closest to the WP sense to be aligned. While these
approaches are in some respects similar to Dijkstra-
WSA, they do not take the global structure of both
resources into account. Instead, they merely rely
on a (locally restricted) subset of WN relations for
creating the alignment. Thus, applying these ap-
proaches to resources in different languages might
be difficult if WN relations are not applicable.
</bodyText>
<sectionHeader confidence="0.992148" genericHeader="method">
4 Dijkstra-WSA
</sectionHeader>
<bodyText confidence="0.9926772">
In this section, we discuss our approach to aligning
lexical-semantic resources based on the graph struc-
ture. This includes two steps: (i) the initial construc-
tion of the graphs using appropriate parameters, and
(ii) the alignment itself.
</bodyText>
<subsectionHeader confidence="0.996474">
4.1 Graph Construction
</subsectionHeader>
<bodyText confidence="0.999995155555556">
We represent the set of senses (or synsets, if appli-
cable) of an LSR L as a set of nodes V where the set
of edges E, E ⊆ V × V between these nodes rep-
resents semantic relatedness between them. We call
this a resource graph. A WP article is considered a
sense as it represents a distinct concept.
There are multiple options for deriving the edges
from the resource. The most straightforward ap-
proach is to directly use the existing semantic rela-
tions (such as hyponymy), as it has been reported in
previous work (Laparra et al., 2010; Navigli, 2009).
For WP, we can directly use the given hyperlinks be-
tween articles as they also express a certain degree
of relatedness (Milne and Witten, 2008). However,
for many LSRs no or only few semantic relations
exist. Consider WKT: Its relations are not sense dis-
ambiguated (Meyer and Gurevych, 2012). We thus
cannot determine the correct target sense if a relation
is pointing to an ambiguous word.
Our solution to this is twofold: First, for each
sense s, we create an edge (s, t) for those seman-
tic relations which have a monosemous target t, as
in this case the target sense is unambiguous. This
approach, however, only recovers a subset of the re-
lations, and it is not applicable to resources where
no sense relations exist at all, e.g. FN. For this case,
we propose to use the glosses of senses in the LSR
to derive additional edges in the following way: For
each monosemous, non-stopword lexeme l (a com-
bination of lemma and part of speech) in the gloss
of a sense s1 with a sense sl, we introduce an edge
(s1, sl). Moreover, if there is another sense s2 with l
in its gloss, we also introduce an edge (s1, s2). This
technique will be called linking of monosemous lex-
emes or monosemous linking throughout the rest of
this paper. The intuition behind this is that monose-
mous lexemes usually have a rather specific mean-
ing, and thus it can be expected that the senses in
whose description they appear have at least a certain
degree of semantic relationship. This directly re-
lates to the notion of “information content” (Resnik,
1995), stating that senses in an LSR which are more
specific (and hence more likely to be monosemous)
are more useful for evaluating semantic similarity.
Note that this step requires part of speech tagging
</bodyText>
<page confidence="0.998807">
154
</page>
<bodyText confidence="0.99998976923077">
of the glosses, which we perform as a preprocess-
ing step. Thereby we filter out stopwords and words
tagged as “unknown” by the POS tagger.
As an example, consider the gloss of Java: “An
object-oriented programming language”. Even in
the absence of any semantic relations, we could
unambiguously derive an edge between this sense
of Java and the multiword noun programming lan-
guage if the latter is monosemous, i.e. if there exists
exactly one sense for this lexeme in the LSR. Also,
if programming language appears in the gloss of one
of the senses of Python, we can derive an edge be-
tween these senses of Java and Python, expressing
that they are semantically related.
An important factor to keep in mind, however, is
the density of the resulting graph. In preliminary ex-
periments, we discovered that linking every monose-
mous lexeme yielded very dense graphs with short
paths between most senses. In turn, we decided to
exclude “common” lexemes and focus on more spe-
cific ones in order to increase the graph’s meaning-
fulness. The indicator for this is the frequency of a
lexeme in the LSR, i.e. how often it occurs in the
glosses. Our experiments on small development sets
(100 random samples of each gold standard) indeed
show that a strict filter leads to discriminative edges
resulting in high precision, while at the same time
graph sparsity decreases recall. Independently of
the resource pair, we discovered that setting this fre-
quency limit value 0 to about 1/100 of the graph size
(e.g. 1,000 for a graph containing 100,000 senses)
gives the best balance between precision and recall;
larger values of 0 usually led to no significant im-
provement1 in recall while the precision was contin-
uously degrading. Note that WP was excluded from
these experiments as the identification and linking of
monosemous lexemes in all WP articles proved too
time-consuming; instead, we decided to use only the
already given links (see Section 5.3).
</bodyText>
<subsectionHeader confidence="0.999327">
4.2 Computing Sense Alignments
</subsectionHeader>
<bodyText confidence="0.999685">
Initialization After resource graphs for both LSRs
A and B are created, the trivial alignments are re-
trieved and introduced as edges between them. Triv-
ial alignments are those between senses which have
</bodyText>
<footnote confidence="0.8120515">
1All significance statements throughout the paper are based
on McNemar’s test and the confidence level of 1%.
</footnote>
<figure confidence="0.962106166666667">
Dijkstra-WSA(A,B)
1 ASenseSet = A.senses
2 BSenseSet = B.senses
3 UnalignableSenses = 0
4
5 foreach sense s E ASenseSet
6 if(s.isMonosemous)
7 t = findTrivialMatch(s, BSenseSet)
8 if(t != null)
9 ASenseSet.remove(s)
10 BSenseSet.remove(t)
11 createEdge(s,t)
12
13 foreach sense s’ E ASenseSet
14 ASenseSet.remove(s’)
15 T=findCandidatesWithSameLexeme(s’, B)
16 if(T!= 0)
17 t’=findShortestPathToCandidates(s’, T)
18 if(t’ != null)
19 createEdge(s’, t’)
20 else
21 UnalignableSenses.put(s’)
22 else
23 UnalignableSenses.put(s’)
</figure>
<tableCaption confidence="0.961885">
Table 2: Pseudocode of the Dijkstra-WSA algorithm.
</tableCaption>
<bodyText confidence="0.99972655">
the same attached lexeme in A and B and where this
lexeme is also monosemous within either resource.
E.g., if the noun phrase programming language is
contained in either resource and has exactly one
sense in each one, we can directly infer the align-
ment. For WP, a lexeme was considered monose-
mous if there was exactly one article with this title,
also counting titles with a bracketed disambiguation
(e.g., Java (programming language) and Java (is-
land) are two distinct senses of Java). While this
method does not work perfectly, we observed a pre-
cision &gt; 0.95 for monosemous gold standard senses,
which is in line with the observations by Henrich et
al. (2011).
Alignment We consider each sense s E A which
has not been aligned in the initialization step. For
this, we first retrieve the set of possible target senses
T C B (those with matching lemma and part of
speech) and compute the shortest path to each of
them with Dijkstra’s shortest path algorithm (Dijk-
</bodyText>
<page confidence="0.996071">
155
</page>
<bodyText confidence="0.9995965">
stra, 1959). The candidate t E T with the shortest
distance is then assigned as the alignment target, and
the algorithm continues with the next still unaligned
sense in A until either all senses are aligned or no
path can be found for the remaining senses. The in-
tuition behind this is that the trivial alignments from
the initialization serve as “bridges” between A and
B, such that a path starting from a sense s1 in A tra-
verses edges to find a nearby already aligned sense
s2, “jumps” to B using a cross-resource edge lead-
ing to t2 and then ideally finds an appropriate target
sense t1 in the vicinity of t2. Note that with each
successful alignment, edges are added to the graph
so that, in theory, a different ordering of the consid-
ered senses would lead to different results. While
we observed slight differences for repeated runs us-
ing the same configuration, these were in no case
statistically significant. The pseudo code of this al-
gorithm can be found in Table 2, while an example
can be found in Figure 1.
</bodyText>
<figureCaption confidence="0.935200333333333">
Figure 1: An example of how Dijkstra-WSA works.
While there exist 2 candidates for aligning a sense s1 E A
(dashed lines) (a), the correct one t1 E B can be deter-
mined by finding the shortest path using an already es-
tablished edge between two monosemous senses s2 E A
and t2 E B (solid line) (b).
</figureCaption>
<bodyText confidence="0.999877827586207">
Parameter Influence Apart from the already
mentioned parameter φ for limiting the number of
edges in the graph, another important variable is the
maximum allowed path length λ of Dijkstra’s algo-
rithm. In general, allowing an unbounded search for
the candidate senses is undesirable as long paths,
while increasing recall, usually also lead to a de-
crease in precision, as the nodes which can be
reached in many steps are usually also semantically
distant from the source sense. In this respect, we
found significant differences between the optimal
configuration for individual resource pairs. How-
ever, the general observation is that short paths (λ &lt;
3) lead to a very high precision, while paths longer
than 10 do not increase recall significantly any more.
A modification of the algorithm is to not only
align the closest target sense, but all senses which
can be reached with a certain number of steps. This
caters to the fact that, due to different sense granu-
larities, one coarser sense in A can be represented by
several senses in B and vice versa (see Table 3 for
the fraction of 1:n alignments in the datasets). Re-
garding this modification, we made the observation
that the recall improved (sometimes considerably),
but at the same time the precision decreased, some-
times to an extent where the overall F-Measure (the
harmonic mean of precision and recall) got worse.
In the evaluation section, we state which setting is
used for which datasets and configurations.
</bodyText>
<sectionHeader confidence="0.999946" genericHeader="method">
5 Experimental Work
</sectionHeader>
<subsectionHeader confidence="0.99079">
5.1 Datasets and their Characteristics
</subsectionHeader>
<bodyText confidence="0.9882512">
WN 3.0-English OW The previous alignment be-
tween these two resources reported in Gurevych et
al. (2012) is based on the German OW (database
dump from 2010/01/03) and WN 3.0 and utilizes
gloss similarities using machine translation as an in-
termediate component. This does not pose a prob-
lem since for each synset in the German part of OW,
there is a translation in the English part. This makes
the German-English gold standard directly usable
for our purposes.2 Table 3 presents the details about
this as well as the other evaluation datasets, includ-
ing the observed inter-rater agreement A0 (where
available) which can be considered as an upper
bound for automatic alignment accuracy and the de-
gree of polysemy (i.e. the number of possible align-
ment targets per sense) which is a hint towards the
difficulty of the alignment task.
WN 3.0-English WKT We use the gold stan-
dard dataset from Meyer and Gurevych (2011) with-
out any modification, thus for comparability to this
</bodyText>
<footnote confidence="0.903727">
2Cross-lingual alignment is left to future work.
</footnote>
<page confidence="0.996391">
156
</page>
<bodyText confidence="0.999769166666667">
work, we use the same WKT dump version (from
2010/02/01) which contains around 421,000 senses.
GN 7.0-German WKT Henrich et al. (2011)
aligned the German WKT (dump from 2011/04/02,
72,000 senses) and GN 7.0. This is the only existing
alignment between these two resources so far, and
we use their freely available dataset3 to test Dijkstra-
WSA on a language other than English. As this
alignment is fairly large (see Table 3), we created a
random sample as a gold standard to keep the com-
putation time at bay. However, the datasets are still
similar enough to allow direct comparison of the
results. Note that no inter-annotator agreement is
available for this study.
WN 3.0-English WP We use the gold standard
from Niemann and Gurevych (2011). For compa-
rability, we use the same Wikipeda dump version
(from 2009/08/22) with around 2,921,000 articles.
</bodyText>
<subsectionHeader confidence="0.991747">
5.2 Baselines
</subsectionHeader>
<bodyText confidence="0.999880375">
WN-OW We used the same configuration as in
Gurevych et al. (2012) to calculate a similarity-
based alignment for the monolingual case (i.e. with-
out the translation step) as a baseline and achieved
comparable results.
WN-WKT As stated above, the alignment4 pre-
sented in Meyer and Gurevych (2011) was created
by calculating the similarity of glosses and training
a machine-learning classifier on the gold standard to
classify each pair of senses.
GN-WKT The automatic alignment results (i.e.
the outcome of the algorithm without manual post-
correction) reported by Henrich et al. (2011) were
unavailable for us as a baseline. Thus, we utilize
the alignment approach by Meyer and Gurevych
(2011) to create a similarity-based baseline, with mi-
nor modifications. Unlike the original approach, we
directly align senses regardless of their similarity if
the decision is trivial (see Section 4.2). We also do
not train a machine learning component on a gold
standard. Instead, we adapt the idea of Henrich et al.
(2011) to align the most similar candidate regardless
of the absolute value.
WN-WP The alignment reported in Niemann and
Gurevych (2011) was created in the same way as
the WN-WKT alignment described in Meyer and
Gurevych (2011). Note that while the full alignment
results5 proved incomplete, the correct alignment re-
sults on the gold standard were available and thus
used in our experiments.
We will henceforth mark these similarity-based
results with SB.
</bodyText>
<subsectionHeader confidence="0.994164">
5.3 System Configurations
</subsectionHeader>
<bodyText confidence="0.999294194444444">
For the construction of the resource graphs we ex-
perimented with three options:
Semantic relations only (SR) OW, WN and GN
all feature disambiguated sense relations which can
be directly used as edges between senses. Note that
in the expert-built resources, the majority of nodes
are connected by sense relations, while this is not
the case for OW. For WKT, only the unambiguous
semantic relations can be used (see Section 4.1), re-
sulting in graphs less dense and with many isolated
nodes. However, as we reported in Matuschek et
al. (2013), the English WKT is almost 6 times as
large as the German one for the versions we used in
our experiments (421,000 senses vs. 72,000 senses),
while it contains not even twice as many relations
(720,000 vs. 430,000). This is directly reflected in
the fewer isolated nodes for the German WKT. WP
links are also unambiguous as they lead to a distinct
article. However, intuitively not all links in an arti-
cle are equally meaningful. Thus, for the SR config-
uration, we decided to retain only the category links
and the links within the first paragraph of the article.
We assume that the targets of these links are most
closely related to the sense an article represents as
the first paragraph usually includes a concise defini-
tion of a concept, and the category links allow deter-
mining the topic an article belongs to.
Linking of monosemous lexemes only (LM) For
this configuration, the limiting parameter φ was set
to 1/100 of the graph size for every resource ex-
cept WP as described in section 4.1. As our ex-
periments show, linking the monosemous lexemes in
the glosses while disregarding semantic relations re-
sults in well-connected graphs for all resources but
GN and WKT. Only about 10% of the GN senses
have a gloss, thus this option was completely disre-
</bodyText>
<footnote confidence="0.9911504">
3http://www.sfs.uni-tuebingen.de/GermaNet/wiktionary.shtml
4Available at http://www.ukp.tu-darmstadt.de/data/lexical-
resources/wordnet-wiktionary-alignment/
5Available at http://www.ukp.tu-darmstadt.de/data/lexical-
resources/wordnet-wikipedia-alignment/
</footnote>
<page confidence="0.990551">
157
</page>
<table confidence="0.999010833333333">
Pair Aligned Not Aligned Sum 1:n Alignments % Polysemy Sampling AO
WN-OW 210 473 683 10.7% 1.50 Random 0.85
WN-WKT 313 2,110 2,423 2.7% 4.76 Balanced 0.93
GN-WKT (full) 27,127 18,509 45,636 5.6% 1.78 All N/A
GN-WKT (sample) 1,000 751 1,751 4.8% 1.84 Random N/A
WN-WP 227 1,588 1,815 5.2% 5.7 Balanced 0.97
</table>
<tableCaption confidence="0.995495">
Table 3: Characteristics of the gold standards used in the evaluation. AO is the observed inter-rater agreement which
can be considered as an upper bound for alignment accuracy. The degree of polysemy (i.e. the number of possible
alignment targets per sense) hints towards the difficulty of the alignment task.
</tableCaption>
<bodyText confidence="0.994961894736842">
garded in this case. For both WKTs, an analysis of
the graphs revealed that the reason for the relatively
high number of isolated nodes are very short glosses,
containing many polysemous lexemes. For WP, we
refrained from monosemous linking due to the pro-
hibitive computation time. Instead, we decided to
use the fully linked WP (excluding the links used
for the SR configuration) in this case. The rationale
is that in the majority of articles many meaningful
terms link to the corresponding articles anyway, so
that the resulting graph is comparable with those for
the other LSRs.
Combining both (SR+LM) This configuration
yields the maximum number of available edges. We
report the results for GN only for this configuration
and omit the SR results for the sake of brevity as
the influence on the F-Measure for the GN-WKT
alignment (see Section 5.4) is not statistically sig-
nificant. For WKT, this configuration only increases
the number of connected nodes slightly (as insuffi-
cient glosses often coincide with missing semantic
relations), while for OW an almost connected graph
can be constructed.
Table 4 gives an overview of the fraction of iso-
lated nodes for each resource in every configuration.
Note again that for each alignment task (i.e. each
pair of resources), we tuned the parameters on 100
random samples from each gold standard for a result
balancing precision and recall as discussed above.
Individual tuning of parameters was necessary for
each pair due to the greatly varying properties of the
LSRs (e.g. the number of senses). While it would
have been ideal to train and test on disjoint sets, we
calculated the overall results on the full gold stan-
dards including the development sets to ensure com-
parability with the previous work.
Hybrid Approach Manual inspection of the re-
sults revealed that the alignments found by Dijkstra-
</bodyText>
<table confidence="0.999755">
Resource SR LM SR+LM
WN 0.25 0.07 0.02
GN 0.0 0.92 0.0
WKT-en 0.98 0.32 0.30
WKT-de 0.69 0.18 0.15
OW 0.41 0.33 0.04
WP 0.06 0.05 0.04
</table>
<tableCaption confidence="0.820417333333333">
Table 4: This table describes what percentage of nodes
remains isolated (i.e. with 0 attached edges) in differ-
ent graph configurations using semantic relations (SR),
</tableCaption>
<bodyText confidence="0.893512461538462">
monosemous linking (LM) (φ = 1/100) or both
(SR+LM). Note that this number is highest for the En-
glish WKT as the few semantic relations and short
glosses do not offer many possibilities for connecting
nodes, while the German WKT and OW do not suffer
from this problem as much. GN is fully linked via re-
lations, but has only few glosses which makes monose-
mous linking ineffective. WN and WP are relatively well-
linked in all configurations. Also note that for WP, SR
means that we used category links and links from the first
paragraph, while links from the rest of the article were
used for the LM configuration.
WSA are usually different from those based on the
gloss similarity. While the latter precisely recog-
nizes alignments with similar wording of glosses,
Dijkstra-WSA is advantageous if the glosses are dif-
ferent but the senses are still semantically close in
the graph. Section 5.5 will analyze this in greater
detail. Exploiting this fact, we experimented with
a hybrid approach: We perform an alignment us-
ing Dijkstra-WSA, tuned for high precision (i.e. us-
ing shorter path lengths) and fall back to using the
results of the similarity-based approaches for those
cases where no alignment target could be found in
the graph. These results are marked with +SB in the
result overview (Table 5).
</bodyText>
<page confidence="0.993561">
158
</page>
<table confidence="0.999854636363636">
WordNet-OmegaWiki WordNet-Wiktionary GermaNet-Wiktionary WordNet-Wikipedia
P R Fl Acc. P R Fl Acc. P R Fl Acc. P R Fl Acc.
Random 0.46 0.35 0.40 0.51 0.21 0.59 0.31 0.67 0.44 0.51 0.47 0.54 0.49 0.62 0.53 0.86
SB 0.55 0.53 0.54 0.73 0.67 0.65 0.66 0.91 0.93 0.74 0.83 0.83 0.78 0.78 0.78 0.95
SR 0.66 0.45 0.53 0.76 0.95 0.13 0.23 0.89 0.94 0.65 0.77 0.78 0.82 0.63 0.71 0.93
LM 0.62 0.54 0.58 0.77 0.72 0.24 0.36 0.89 0.89 0.75 0.81 0.80 0.65 0.66 0.65 0.91
SR+LM 0.56 0.69 0.62 0.74 0.68 0.27 0.39 0.89 0.90 0.78 0.83 0.82 0.75 0.67 0.71 0.93
SR+SB 0.60 0.65 0.63 0.76 0.68 0.67 0.68 0.92 0.90 0.82 0.86 0.84 0.75 0.87 0.81 0.95
LM+SB 0.60 0.70 0.64 0.76 0.68 0.70 0.69 0.92 0.86 0.87 0.87 0.85 0.70 0.87 0.78 0.94
SR+LM+SB 0.57 0.75 0.65 0.75 0.68 0.71 0.69 0.92 0.87 0.88 0.87 0.85 0.75 0.87 0.81 0.95
AO - - - 0.85 - - - 0.93 - - - N/A - - - 0.97
</table>
<tableCaption confidence="0.916289333333333">
Table 5: Alignment results for all datasets and configurations: Using semantic relations (SR), monosemous links
(LM) or both (SR+LM). The similarity-based (SB) baselines, also used as a back-off for the hybrid approaches (+SB),
were created using the approach reported in Gurevych et al. (2012). Note that for GN, the SR+LM configuration was
always used. The different configurations given for this alignment thus only apply to WKT. For WP, SR means that
only category links and links within the first paragraph were used, while LM uses links from the full article. A random
baseline and the inter-annotator agreement Ao of the gold standard annotation (if available) are given for reference.
</tableCaption>
<subsectionHeader confidence="0.987062">
5.4 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999964524590164">
WN-OW When using only semantic relations (SR),
we achieved an F-Measure of 0.53 which is com-
parable with the 0.54 from Gurevych et al. (2012).
Notably, our approach has a high precision, while
the recall is considerably worse due to the relative
sparsity of the resulting OW resource graph. When
adding more edges to the graph by linking monose-
mous lexemes (SR+LM), we can drastically improve
the recall, leading to an overall F-Measure of 0.62,
which is a significant improvement over our previ-
ous results (Gurevych et al., 2012). Using monose-
mous links only (LM), the result of 0.58 still out-
performs Gurevych et al. (2012) due to the higher
precision. Building a graph from glosses alone is
thus a viable approach if no or only few semantic
relations are available. Regarding the path lengths,
A = 10 works best when semantic relations are in-
cluded in the graph, while for the LM configura-
tion shorter paths (A &lt; 5) were more appropriate.
The intuition behind this is that for semantic rela-
tions, unlike monosemous links, even longer paths
still express a high degree of semantic relatedness.
Also, when semantic relations are involved allow-
ing multiple alignments increases the overall results
(which is in line with the relatively high number of
1:n alignments in the gold standard), while this is not
the case for the LM configuration; here, the edges
again do not sufficiently express relatedness.
Using the hybrid approach (+SB), we can increase
the F-Measure up to 0.65 if semantic relations and
monosemous linking are combined (SR+LM) and
the parameters are tuned for high precision (A &lt; 3,
1:1 alignments). This is significantly better than
Dijkstra-WSA alone in any configuration. In this
scenario, we also observe the best overall recall.
WN-WKT Experiments using only the semantic
relations (SR) yield a very low recall - the small
number of sense relations with monosemous targets
in WKT leaves the graph very sparse. Nevertheless,
the alignment targets which Dijkstra-WSA finds are
mostly correct, with a precision greater than 0.95
even when allowing 1:n alignments. Using only
monosemous links (LM) improves the recall consid-
erably, but unlike the WN-OW alignment, it stays
fairly low. Consequently, even when using seman-
tic relations and monosemous links in conjunction
(SR+LM), the recall can only be increased slightly,
leading to an overall F-Measure of 0.39. As men-
tioned above, this is due to the WKT glosses. In
many cases, they are very short, often consisting
of only 3-5 words, many of which are polysemous.
This leads to many isolated nodes in the graph with
no or only very few connecting edges. The ideal,
rather short path length A of 2-3 stems from the rela-
tively high polysemy of the gold standard (see Table
3). We experimented with A ≥ 4, achieving rea-
sonable recall, but in this case the precision was so
low that this configuration, in conclusion, does not
increase the F-Measure. However, 1:n alignments
work well with these short paths as the correct align-
ments are mostly in the close vicinity of a sense,
</bodyText>
<page confidence="0.997583">
159
</page>
<bodyText confidence="0.999984285714286">
hence we achieve an increase in recall in this case
without too much loss of precision.
For the hybrid approach, we achieve an
F-Measure of 0.69 when using all edges
(SR+LM+SB), setting the path length to 2, and
also allowing 1:n alignments. This is a statistically
significant improvement over Meyer and Gurevych
(2011) which again confirms the effectiveness of
the hybrid approach.
GN-WKT As stated above, we used the SR+LM
configuration for GN in every case. For the German
WKT, the much greater number of relations com-
pared to its English counterpart is directly reflected
in the results, as using the semantic relations only
(SR) not only yields the best precision of 0.94 but
also a good recall of 0.65. Using the semantic re-
lations together with monosemous links (SR+LM)
yields the F-Measure of 0.83, which is on par with
the similarity-based (SB) approach.
In the hybrid configuration, we can increase
the performance to an F-Measure of up to
0.87 (SR+LM+SB), significantly outperforming all
graph-based and similarity-based configurations.
In general, results for this pair of LSRs are higher
in comparison with the others. We attribute this to
the fact that the German WKT and GN both are
densely linked with semantic relations which is es-
pecially beneficial for the recall of Dijkstra-WSA.
This is also reflected in the ideal λ of 10-12: Many
high-confidence edges allow long paths which still
express a considerable degree of relatedness. How-
ever, while the results for 1:n alignments are al-
ready good, restricting oneself to 1:1 alignments
gives the best overall results as the precision can
then be pushed towards 0.90 without hurting recall
too much. An important factor in this respect is that
the GN-WKT dataset has a relatively low degree of
polysemy (compared to WN-WKT) and only few
1:n alignments (compared to WN-OW), two facts
which make the task significantly easier.
WN-WP The SR configuration (WN relations +
WP category/first paragraph links) yields the best
precision (0.82), even outperforming the SB ap-
proach, and an F-Measure of 0.71. This again shows
that using an appropriate parametrization (λ ≤ 4
in this case) Dijkstra-WSA can detect alignments
with high confidence. The relatively low recall of
0.63 could be increased by allowing longer paths,
however, as hyperlinks do not express relatedness as
reliably as semantic relations, this introduces many
false positives and thus hurts precision considerably.
This issue of “misleading” WP links becomes even
more prominent when the links from the full articles
are used as edges (LM); while the increase in recall
is relatively small the precision drops substantially.
However, using all possible links (SR+LM) allows
us to balance out precision and recall to some extent,
while yielding the same F-Measure as the SR config-
uration. Note that 1:1 alignments were enforced in
any case, as the high polysemy of the dataset in con-
junction with the dense WP link structure rendered
1:n alignments very imprecise.
Using the hybrid approach, we can increase the F-
Measure up to 0.81 (SR+SB), outperforming the re-
sults reported in Niemann and Gurevych (2011) by
a significant margin. The F-Measure for LM+SB is
slightly worse due to the lower precision. Combin-
ing all edges (SR+LM+SB) does not influence the
results any more, but in any case the hybrid configu-
ration achieves the best overall recall (0.87).
In conclusion, our experiments on all four
datasets consistently demonstrate that combining
Dijkstra-WSA with a similarity-based approach as
a back-off yields the strongest performance. The re-
sults of these best alignments will be made freely
available to the research community on our website
(http://www.ukp.tu-darmstadt.de).
</bodyText>
<sectionHeader confidence="0.554796" genericHeader="method">
5.5 Error Analyis
</sectionHeader>
<bodyText confidence="0.999961882352941">
The by far most significant error source, reflected in
the relatively low recall for different configurations,
is the high number of false negatives, i.e. sense pairs
which were not aligned although they should have
been. This is especially striking for the WN-WKT
alignment. As discussed earlier, WKT contains a
significant number of short glosses, which in many
cases also contain few or no monosemous terms. A
prototypical example is the first sense of seedling:
“A young plant grown from seed”. This gloss has
no monosemous words which could be linked, and
as there are also no semantic relations attached to
this sense which could be exploited, the node is iso-
lated in the graph. Our experiments show that for
the English WKT, even when optimizing the param-
eters for recall, around 30% of the senses remain
isolated, i.e. without edges. This is by far the high-
</bodyText>
<page confidence="0.988065">
160
</page>
<bodyText confidence="0.999991942857143">
est value across all resources (see Table 4). Solv-
ing this problem would require making the graph
more dense, and especially finding ways to include
isolated nodes as well. However, this example also
shows why the hybrid approach works so well: The
correct WN sense “young plant or tree grown from
a seed” was recognized by the similarity-based ap-
proach with high confidence.
With regard to false positives, Dijkstra-WSA and
the similarity-based approaches display very similar
performance. This is because senses with very sim-
ilar wording are likely to share the same monose-
mous words, leading to a close vicinity in the graph
and the false alignment. As an example, consider
two senses of bowdlerization in WN (“written mate-
rial that has been bowdlerized”) and WKT (“The ac-
tion or instance of bowdlerizing; the omission or re-
moval of material considered vulgar or indecent.”).
While these senses are clearly related, they are not
identical and should not be aligned, nevertheless the
similar wording (and especially the use of the highly
specific verb “bowdlerize”) results in an alignment.
Similarly to the similarity-based approaches, it is an
open question how this kind of error can be effec-
tively avoided (Meyer and Gurevych, 2011).
There is a considerable number of exam-
ples where Dijkstra-WSA recognizes an alignment
which similarity-based approaches do not. The two
senses of Thessalonian from the introductory exam-
ple (Section 3) contain the terms Thessalonica and
Thessaloniki in their glosses which are both monose-
mous in WN as well as in WKT, sharing the also
monosemous noun Greece in their glosses. This
yields the bridge between the resources to find a path
and correctly derive the alignment.
</bodyText>
<sectionHeader confidence="0.998619" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999989045454546">
In this work, we present Dijkstra-WSA, a graph-
based algorithm for word sense alignment. We
show that this algorithm performs competitively on
3 out of 4 evaluation datasets. A hybrid approach
leads to a statistically significant improvement over
similarity-based state of the art results on every
dataset. Dijkstra-WSA can operate on glosses or
semantic relations alone (although it is beneficial if
both are combined), and it does not require any ex-
ternal knowledge in the form of annotated training
data or corpora. Additionally, it is flexibly config-
urable for different pairs of LSRs in order to opti-
mize for precision or recall.
An important task for future work is to evalu-
ate Dijkstra-WSA on LSRs which structurally dif-
fer from the ones discussed here. It is important
to determine how resources like FN or VN can be
meaningfully transformed into a graph representa-
tion. Another idea is to extend the approach to
cross-lingual resource alignment, which would re-
quire a machine translation component to identify
sense alignment candidates with the correct lexeme.
Regarding the algorithm itself, the main direction
for future work is to increase recall while keeping
high precision. One possible way would be to not
only link monosemous lexemes, but also to create
edges for polysemous ones. Laparra et al. (2010)
discuss a possibility to do this with high precision.
The main idea is to focus on lexemes with a low de-
gree of polysemy and align if one of the possible
senses is clearly more similar to the source sense
than the other(s). If recall is still low, more poly-
semous lexemes can be examined.
A weighting of edges (e.g. based on gloss simi-
larities) has not been considered at all, but would be
easily applicable to the existing framework.
A more elaborate idea would be to investigate
entirely different graph-based algorithms, e.g. for
matching nodes in bipartite graphs. Also, we plan to
investigate if and how our approach can be extended
to align more than two resources at once using the
graph representations. This might improve align-
ment results as more information about the overall
alignment topology becomes available.
</bodyText>
<sectionHeader confidence="0.996549" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999783181818182">
This work has been supported by the Volkswagen
Foundation as part of the Lichtenberg Professorship
Program under grant No. I/82806 and by the
Hessian research excellence program “Landes-
Offensive zur Entwicklung Wissenschaftlich-
¨okonomischer Exzellenz (LOEWE)” as part of the
research center “Digital Humanities”. We would
like to thank Christian M. Meyer, Wolfgang Stille,
Karsten Weihe and Tristan Miller for insightful
discussions and comments. We also thank the
anonymous reviewers for their helpful remarks.
</bodyText>
<page confidence="0.997737">
161
</page>
<sectionHeader confidence="0.995846" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999670962616822">
Eneko Agirre and Aitor Soroa. 2009. Personaliz-
ing PageRank for Word Sense Disambiguation. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 33–41, Athens, Greece.
Gerard de Melo and Gerhard Weikum. 2010. Pro-
viding Multilingual, Multimodal Answers to Lexical
Database Queries. In Proceedings of the 7th Language
Resources and Evaluation Conference (LREC 2010),
pages 348–355, Valetta, Malta.
Edsger. W. Dijkstra. 1959. A note on two problems
in connexion with graphs. Numerische Mathematik,
1:269–271. 10.1007/BF01386390.
Judith Eckle-Kohler, Iryna Gurevych, Silvana Hartmann,
Michael Matuschek, and Christian M. Meyer. 2012.
UBY-LMF - A Uniform Model for Standardizing Het-
erogeneous Lexical-Semantic Resources in ISO-LMF.
In Proceedings of the 8th International Conference
on Language Resources and Evaluation (LREC’12),
pages 275–282, Istanbul, Turkey.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Tiziano Flati and Roberto Navigli. 2012. The CQC al-
gorithm: Cycling in graphs to semantically enrich and
enhance a bilingual dictionary. Journal of Artificial
Intelligence Research (JAIR), 43:135–171.
Iryna Gurevych, Judith Eckle-Kohler, Silvana Hartmann,
Michael Matuschek, Christian M. Meyer, and Chris-
tian Wirth. 2012. UBY - A Large-Scale Unified
Lexical-Semantic Resource Based on LMF. In Pro-
ceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL’12), pages 580–590, Avignon, France.
Birgit Hamp and Helmut Feldweg. 1997. Germanet - a
lexical-semantic net for german. In In Proceedings of
ACL workshop Automatic Information Extraction and
Building of Lexical Semantic Resources for NLP Ap-
plications, pages 9–15.
Verena Henrich, Erhard Hinrichs, and Tatiana Vodola-
zova. 2011. Semi-Automatic Extension of GermaNet
with Sense Definitions from Wiktionary. In Proceed-
ings of the 5th Language and Technology Conference
(LTC 2011), pages 126–130, Poznan, Poland.
Egoitz Laparra, German Rigau, and Montse Cuadros.
2010. Exploring the integration of WordNet and
FrameNet. In Proceedings of the 5th Global WordNet
Conference (GWC’10), Mumbai, India.
Michael Matuschek, Christian M. Meyer, and Iryna
Gurevych. 2013. Multilingual Knowledge in
Aligned Wiktionary and OmegaWiki for Computer-
Aided Translation. Translation: Computation, Cor-
pora, Cognition. Special Issue on “Language Technol-
ogy for a Multilingual Europe”, to appear.
Christian M. Meyer and Iryna Gurevych. 2011. What
Psycholinguists Know About Chemistry: Aligning
Wiktionary and WordNet for Increased Domain Cov-
erage. In Proceedings of the 5th International Joint
Conference on Natural Language Processing (IJC-
NLP), pages 883–892, Chiang Mai, Thailand.
Christian M. Meyer and Iryna Gurevych. 2012. Wik-
tionary: A new rival for expert-built lexicons? Ex-
ploring the possibilities of collaborative lexicography.
In Sylviane Granger and Magali Paquot, editors, Elec-
tronic Lexicography, chapter 13, pages 259–291. Ox-
ford University Press.
Rada Mihalcea and Dan I. Moldovan. 2001. eXtended
WordNet: progress report. In Proceedings of NAACL
Workshop on WordNet and Other Lexical Resources,
pages 95–100, Pittsburgh, PA, USA.
David Milne and Ian H. Witten. 2008. An effective,
low-cost measure of semantic relatedness obtained
from Wikipedia links. In Proceedings of the AAAI
Workshop on Wikipedia and Artificial Intelligence: an
Evolving Synergy, pages 25–30, Chicago, IL, USA.
Roberto Navigli and Simone Paolo Ponzetto. 2012. Ba-
belNet: The automatic construction, evaluation and
application of a wide-coverage multilingual semantic
network. Artificial Intelligence, 193:217–250.
Roberto Navigli. 2009. Using Cycles and Quasi-Cycles
to Disambiguate Dictionary Glosses. In Proceedings
of the 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL’09),
pages 594–602, Athens, Greece.
Elisabeth Niemann and Iryna Gurevych. 2011. The Peo-
ple’s Web meets Linguistic Knowledge: Automatic
Sense Alignment of Wikipedia and WordNet. In Pro-
ceedings of the 9th International Conference on Com-
putational Semantics (IWCS), pages 205–214, Oxford,
UK.
Martha Palmer. 2009. SemLink: Linking PropBank,
VerbNet and FrameNet. In Proceedings of the Genera-
tive Lexicon ConferenceGenLex-09, pages 9–15, Pisa,
Italy.
Simone Paolo Ponzetto and Roberto Navigli. 2009.
Large-scale taxonomy mapping for restructuring and
integrating Wikipedia. In Proceedings of the 21&amp;quot; In-
ternational Joint Conference on Artificial Intelligence,
pages 2083–2088, Pasadena, CA, USA.
Philip Resnik. 1995. Using Information Content to
Evaluate Semantic Similarity in a Taxonomy. In In-
ternational Joint Conference for Artificial Intelligence
(IJCAI-95), pages 448–453, Montreal, Canada.
Lei Shi and Rada Mihalcea. 2005. Putting Pieces To-
gether: Combining FrameNet, VerbNet and WordNet
for Robust Semantic Parsing. In Computational Lin-
guistics and Intelligent Text Processing: 6th Interna-
tional Conference, volume 3406 of Lecture Notes in
</reference>
<page confidence="0.974858">
162
</page>
<reference confidence="0.999397666666667">
Computer Science, pages 100–111. Berlin/Heidelberg:
Springer.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. YAGO: A Large Ontology from
Wikipedia and WordNet. Web Semantics, 6(3):203–
217.
</reference>
<page confidence="0.9995245">
163
164
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.159791">
<title confidence="0.999071">Dijkstra-WSA: A Graph-Based Approach to Word Sense Alignment</title>
<author confidence="0.869911">Matuschek Iryna Gurevych</author>
<affiliation confidence="0.9520645">Knowledge Processing Lab German Institute for Educational Research and Educational</affiliation>
<address confidence="0.724811">Schloßstr. 29, 60486 Frankfurt,</address>
<affiliation confidence="0.826283">Knowledge Processing Lab Department of Computer Science, Technische Universit¨at</affiliation>
<address confidence="0.790517">Hochschulstr. 10, 64289 Darmstadt,</address>
<web confidence="0.682197">http://www.ukp.tu-darmstadt.de</web>
<abstract confidence="0.992519125">In this paper, we present Dijkstra-WSA, a novel graph-based algorithm for word sense alignment. We evaluate it on four different pairs of lexical-semantic resources with different characteristics (WordNet-OmegaWiki, WordNet-Wiktionary, GermaNet-Wiktionary and WordNet-Wikipedia) and show that it achieves competitive performance on 3 out of 4 datasets. Dijkstra-WSA outperforms the state of the art on every dataset if it is combined with a back-off based on gloss similarity. We also demonstrate that Dijkstra-WSA is not only flexibly applicable to different resources but also highly parameterizable to optimize for precision or recall.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Personalizing PageRank for Word Sense Disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>33--41</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="10193" citStr="Agirre and Soroa, 2009" startWordPosition="1567" endWordPosition="1570">lated Work The are two strands of closely related work: Similarity-based and graph-based approaches to word sense alignment. To our knowledge, there exists no previous work which fully represents both LSRs involved in an alignment as graphs. We give a summary of different approaches in Table 1. 3.1 Similarity-based Approaches Niemann and Gurevych (2011) and Meyer and Gurevych (2011) created WN-WP and WN-WKT alignments using a framework which first calculates the similarity of glosses (or glosses and articles in the case of WN-WP) using either cosine or personalized page rank (PPR) similarity (Agirre and Soroa, 2009) and then learns a threshold on the gold standard to classify each pair of senses as a (non-)valid alignment. This approach was later extended to cross-lingual alignment between the German OW and WN (Gurevych et al., 2012) using a machine translation component. However, its applicability depends on the availability and quality of the glosses, which are not present in every case (e.g. for VN). Moreover, as it involves supervised machine learning, it requires the initial effort of manually annotating a sufficient amount of training data. Henrich et al. (2011) use a similar approach for aligning </context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2009. Personalizing PageRank for Word Sense Disambiguation. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 33–41, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard de Melo</author>
<author>Gerhard Weikum</author>
</authors>
<title>Providing Multilingual, Multimodal Answers to Lexical Database Queries.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th Language Resources and Evaluation Conference (LREC 2010),</booktitle>
<pages>348--355</pages>
<location>Valetta,</location>
<marker>de Melo, Weikum, 2010</marker>
<rawString>Gerard de Melo and Gerhard Weikum. 2010. Providing Multilingual, Multimodal Answers to Lexical Database Queries. In Proceedings of the 7th Language Resources and Evaluation Conference (LREC 2010), pages 348–355, Valetta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Dijkstra</author>
</authors>
<title>A note on two problems in connexion with graphs.</title>
<date>1959</date>
<journal>Numerische Mathematik,</journal>
<volume>1</volume>
<pages>10--1007</pages>
<marker>Dijkstra, 1959</marker>
<rawString>Edsger. W. Dijkstra. 1959. A note on two problems in connexion with graphs. Numerische Mathematik, 1:269–271. 10.1007/BF01386390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Eckle-Kohler</author>
<author>Iryna Gurevych</author>
<author>Silvana Hartmann</author>
<author>Michael Matuschek</author>
<author>Christian M Meyer</author>
</authors>
<title>UBY-LMF - A Uniform Model for Standardizing Heterogeneous Lexical-Semantic Resources in ISO-LMF.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<pages>275--282</pages>
<location>Istanbul, Turkey.</location>
<contexts>
<context position="3033" citStr="Eckle-Kohler et al., 2012" startWordPosition="450" endWordPosition="453"> 2012) and semantic role labeling using a combination of PropBank, VN and FN in the SemLink project (Palmer, 2009). Some of these approaches to WSA either rely heavily on manual labor (e.g. Shi and Mihalcea (2005)) or on information which is only present in few resources such as the most frequent sense (MFS) (Suchanek et al., 2008). This makes it difficult to apply them to a larger set of resources. In earlier work, we presented the large-scale resource UBY (Gurevych et al., 2012). It contains nine resources in two languages which are mapped to a uniform representation using the LMF standard (Eckle-Kohler et al., 2012). They are thus structurally interoperable. UBY contains pairwise sense alignments between a subset of these resources, and this work also presented a framework for creating alignments based on the similarity of glosses (Meyer and Gurevych, 2011). However, it is not clear to what extent this approach can be applied to resources which lack this kind of information (see Section 3). In summary, aligning senses is a key requirement for semantic interoperability of LSRs to increase the 151 Transactions of the Association for Computational Linguistics, 1 (2013) 151–164. Action Editor: Patrick Pantel</context>
</contexts>
<marker>Eckle-Kohler, Gurevych, Hartmann, Matuschek, Meyer, 2012</marker>
<rawString>Judith Eckle-Kohler, Iryna Gurevych, Silvana Hartmann, Michael Matuschek, and Christian M. Meyer. 2012. UBY-LMF - A Uniform Model for Standardizing Heterogeneous Lexical-Semantic Resources in ISO-LMF. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC’12), pages 275–282, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1428" citStr="Fellbaum, 1998" startWordPosition="191" endWordPosition="192"> Dijkstra-WSA outperforms the state of the art on every dataset if it is combined with a back-off based on gloss similarity. We also demonstrate that Dijkstra-WSA is not only flexibly applicable to different resources but also highly parameterizable to optimize for precision or recall. 1 Introduction Lexical-semantic resources (LSRs) are a cornerstone for many Natural Language Processing (NLP) applications such as word sense disambiguation (WSD) and information extraction. However, the growing demand for large-scale resources in different languages is hard to meet. The Princeton WordNet (WN) (Fellbaum, 1998) is widely used for English, but for most languages corresponding resources are considerably smaller or missing. Collaboratively constructed resources like Wiktionary (WKT) and OmegaWiki (OW) provide a viable option for such cases and seem especially suitable for smaller languages (Matuschek et al., 2013), but there are still considerable gaps in coverage which need to be filled. A related problem is that there usually does not exist a single resource which works best for all purposes, as different LSRs cover different words, senses and information types. These considerations have sparked incr</context>
<context position="7057" citStr="Fellbaum, 1998" startWordPosition="1087" endWordPosition="1088">nd WN-WP (Niemann and Gurevych, 2011). Our goal is to cover resources with different characteristics: Expert-built (WN, GN) and collaboratively constructed LSRs (WP, WKT, OW), resources in different languages (English and German) and also resources with few sense descriptions (GN) or semantic relations (WKT). We contrastively discuss the results of the Dijkstra-WSA algorithm on these different datasets and relate the results to the properties of the LSRs involved. Moreover, using existing datasets ensures comparability to previous work which discusses only one dataset at a time. WordNet (WN) (Fellbaum, 1998) is a lexical resource for the English language created at Princeton University. The resource is organized in sets of synonymous words (synsets) which are represented by glosses (sometimes accompanied by example sentences) and organized in a hierarchy. The latest version 3.0 contains 117,659 synsets. Wikipedia (WP) is a freely available, multilingual online encyclopedia. WP can be edited by every Web user, which causes rapid growth: By February 2013 the English WP contained over 4,000,000 article pages. Each article usually describes a distinct concept, and articles are connected by hyperlinks</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tiziano Flati</author>
<author>Roberto Navigli</author>
</authors>
<title>The CQC algorithm: Cycling in graphs to semantically enrich and enhance a bilingual dictionary.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>43--135</pages>
<contexts>
<context position="12819" citStr="Flati and Navigli, 2012" startWordPosition="2009" endWordPosition="2012"> a global graph structure for FN. Moreover, the algorithm solely relies on the 153 semantic relations found in WN and eXtended WN (Mihalcea and Moldovan, 2001). Thus, it is not applicable to other resources which have no or only few relations such as WKT. Navigli (2009) aims at disambiguating WN glosses, i.e. assigning the correct senses to all nonstopwords in each WN gloss. His approach is to find the shortest possible circles in the WN relation graph to identify the correct disambiguation. In later work, this idea was extended to the disambiguation of translations in a bilingual dictionary (Flati and Navigli, 2012). However, there is no discussion of how this idea could be applied to word sense alignment of two or more resources. We build upon this idea of finding shortest paths (circles are a special kind of path) and extend it to multiple resources and edges other than semantic relations, in particular WP links and links to senses of monosemous lexemes appearing in glosses. Ponzetto and Navigli (2009) propose a graphbased method to tackle the related, but slightly different problem of aligning WN synsets and WP categories (not articles). Using semantic relations, they build WN subgraphs for each WP ca</context>
</contexts>
<marker>Flati, Navigli, 2012</marker>
<rawString>Tiziano Flati and Roberto Navigli. 2012. The CQC algorithm: Cycling in graphs to semantically enrich and enhance a bilingual dictionary. Journal of Artificial Intelligence Research (JAIR), 43:135–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
<author>Judith Eckle-Kohler</author>
<author>Silvana Hartmann</author>
<author>Michael Matuschek</author>
<author>Christian M Meyer</author>
<author>Christian Wirth</author>
</authors>
<title>UBY - A Large-Scale Unified Lexical-Semantic Resource Based on LMF.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL’12),</booktitle>
<pages>580--590</pages>
<location>Avignon, France.</location>
<contexts>
<context position="2892" citStr="Gurevych et al., 2012" startWordPosition="428" endWordPosition="431">, and VerbNet (VN) (Shi and Mihalcea, 2005), word sense disambiguation using an alignment of WN and Wikipedia (WP) (Navigli and Ponzetto, 2012) and semantic role labeling using a combination of PropBank, VN and FN in the SemLink project (Palmer, 2009). Some of these approaches to WSA either rely heavily on manual labor (e.g. Shi and Mihalcea (2005)) or on information which is only present in few resources such as the most frequent sense (MFS) (Suchanek et al., 2008). This makes it difficult to apply them to a larger set of resources. In earlier work, we presented the large-scale resource UBY (Gurevych et al., 2012). It contains nine resources in two languages which are mapped to a uniform representation using the LMF standard (Eckle-Kohler et al., 2012). They are thus structurally interoperable. UBY contains pairwise sense alignments between a subset of these resources, and this work also presented a framework for creating alignments based on the similarity of glosses (Meyer and Gurevych, 2011). However, it is not clear to what extent this approach can be applied to resources which lack this kind of information (see Section 3). In summary, aligning senses is a key requirement for semantic interoperabili</context>
<context position="6374" citStr="Gurevych et al., 2012" startWordPosition="982" endWordPosition="985">ting out directions for future work. 2 Notation and Resources 2.1 Problem Description A word sense alignment, or alignment for short, is formally defined as a list of pairs of senses from two LSRs. A pair of aligned senses denote the same meaning. E.g., the two senses of letter “The conventional characters of the alphabet used to represent speech” and “A symbol in an alphabet, bookstave” (taken from WN and WKT, respectively) are clearly equivalent and should be aligned. 2.2 Evaluation Resources For the evaluation of Dijkstra-WSA, we align four pairs of LSRs used in previous work, namely WNOW (Gurevych et al., 2012), WN-WKT (Meyer and Gurevych, 2011), GN-WKT (Henrich et al., 2011) and WN-WP (Niemann and Gurevych, 2011). Our goal is to cover resources with different characteristics: Expert-built (WN, GN) and collaboratively constructed LSRs (WP, WKT, OW), resources in different languages (English and German) and also resources with few sense descriptions (GN) or semantic relations (WKT). We contrastively discuss the results of the Dijkstra-WSA algorithm on these different datasets and relate the results to the properties of the LSRs involved. Moreover, using existing datasets ensures comparability to prev</context>
<context position="10415" citStr="Gurevych et al., 2012" startWordPosition="1606" endWordPosition="1609">ignment as graphs. We give a summary of different approaches in Table 1. 3.1 Similarity-based Approaches Niemann and Gurevych (2011) and Meyer and Gurevych (2011) created WN-WP and WN-WKT alignments using a framework which first calculates the similarity of glosses (or glosses and articles in the case of WN-WP) using either cosine or personalized page rank (PPR) similarity (Agirre and Soroa, 2009) and then learns a threshold on the gold standard to classify each pair of senses as a (non-)valid alignment. This approach was later extended to cross-lingual alignment between the German OW and WN (Gurevych et al., 2012) using a machine translation component. However, its applicability depends on the availability and quality of the glosses, which are not present in every case (e.g. for VN). Moreover, as it involves supervised machine learning, it requires the initial effort of manually annotating a sufficient amount of training data. Henrich et al. (2011) use a similar approach for aligning GN and WKT. However, they use word overlap as a similarity measure and do not require a machine learning component as they align to the candidate sense with the highest similarity regardless of the absolute value. The alig</context>
<context position="23642" citStr="Gurevych et al. (2012)" startWordPosition="3848" endWordPosition="3851">ed by several senses in B and vice versa (see Table 3 for the fraction of 1:n alignments in the datasets). Regarding this modification, we made the observation that the recall improved (sometimes considerably), but at the same time the precision decreased, sometimes to an extent where the overall F-Measure (the harmonic mean of precision and recall) got worse. In the evaluation section, we state which setting is used for which datasets and configurations. 5 Experimental Work 5.1 Datasets and their Characteristics WN 3.0-English OW The previous alignment between these two resources reported in Gurevych et al. (2012) is based on the German OW (database dump from 2010/01/03) and WN 3.0 and utilizes gloss similarities using machine translation as an intermediate component. This does not pose a problem since for each synset in the German part of OW, there is a translation in the English part. This makes the German-English gold standard directly usable for our purposes.2 Table 3 presents the details about this as well as the other evaluation datasets, including the observed inter-rater agreement A0 (where available) which can be considered as an upper bound for automatic alignment accuracy and the degree of p</context>
<context position="25493" citStr="Gurevych et al. (2012)" startWordPosition="4159" endWordPosition="4162">lable dataset3 to test DijkstraWSA on a language other than English. As this alignment is fairly large (see Table 3), we created a random sample as a gold standard to keep the computation time at bay. However, the datasets are still similar enough to allow direct comparison of the results. Note that no inter-annotator agreement is available for this study. WN 3.0-English WP We use the gold standard from Niemann and Gurevych (2011). For comparability, we use the same Wikipeda dump version (from 2009/08/22) with around 2,921,000 articles. 5.2 Baselines WN-OW We used the same configuration as in Gurevych et al. (2012) to calculate a similaritybased alignment for the monolingual case (i.e. without the translation step) as a baseline and achieved comparable results. WN-WKT As stated above, the alignment4 presented in Meyer and Gurevych (2011) was created by calculating the similarity of glosses and training a machine-learning classifier on the gold standard to classify each pair of senses. GN-WKT The automatic alignment results (i.e. the outcome of the algorithm without manual postcorrection) reported by Henrich et al. (2011) were unavailable for us as a baseline. Thus, we utilize the alignment approach by M</context>
<context position="34184" citStr="Gurevych et al. (2012)" startWordPosition="5603" endWordPosition="5606"> 0.75 0.67 0.71 0.93 SR+SB 0.60 0.65 0.63 0.76 0.68 0.67 0.68 0.92 0.90 0.82 0.86 0.84 0.75 0.87 0.81 0.95 LM+SB 0.60 0.70 0.64 0.76 0.68 0.70 0.69 0.92 0.86 0.87 0.87 0.85 0.70 0.87 0.78 0.94 SR+LM+SB 0.57 0.75 0.65 0.75 0.68 0.71 0.69 0.92 0.87 0.88 0.87 0.85 0.75 0.87 0.81 0.95 AO - - - 0.85 - - - 0.93 - - - N/A - - - 0.97 Table 5: Alignment results for all datasets and configurations: Using semantic relations (SR), monosemous links (LM) or both (SR+LM). The similarity-based (SB) baselines, also used as a back-off for the hybrid approaches (+SB), were created using the approach reported in Gurevych et al. (2012). Note that for GN, the SR+LM configuration was always used. The different configurations given for this alignment thus only apply to WKT. For WP, SR means that only category links and links within the first paragraph were used, while LM uses links from the full article. A random baseline and the inter-annotator agreement Ao of the gold standard annotation (if available) are given for reference. 5.4 Experimental Results WN-OW When using only semantic relations (SR), we achieved an F-Measure of 0.53 which is comparable with the 0.54 from Gurevych et al. (2012). Notably, our approach has a high </context>
</contexts>
<marker>Gurevych, Eckle-Kohler, Hartmann, Matuschek, Meyer, Wirth, 2012</marker>
<rawString>Iryna Gurevych, Judith Eckle-Kohler, Silvana Hartmann, Michael Matuschek, Christian M. Meyer, and Christian Wirth. 2012. UBY - A Large-Scale Unified Lexical-Semantic Resource Based on LMF. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL’12), pages 580–590, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Birgit Hamp</author>
<author>Helmut Feldweg</author>
</authors>
<title>Germanet - a lexical-semantic net for german. In</title>
<date>1997</date>
<booktitle>In Proceedings of ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications,</booktitle>
<pages>9--15</pages>
<contexts>
<context position="9446" citStr="Hamp and Feldweg, 1997" startWordPosition="1448" endWordPosition="1451">able 1: Summary of various approaches to WSA. “NA” stands for “Not Available”. OmegaWiki (OW) is a freely editable online dictionary like WKT. However, there do not exist distinct language editions as OW is organized in language-independent concepts (“Defined Meanings”) to which lexicalizations in various languages are attached. These can be considered as multilingual synsets, and they are interconnected by unambiguous relations just like WN. As of February 2013, OW contains over 46,000 of these concepts and lexicalizations in over 400 languages. GermaNet (GN) is the German counterpart to WN (Hamp and Feldweg, 1997). It is also organized in synsets (around 70,000 in the latest version 7.0) which are connected via semantic relations. 3 Related Work The are two strands of closely related work: Similarity-based and graph-based approaches to word sense alignment. To our knowledge, there exists no previous work which fully represents both LSRs involved in an alignment as graphs. We give a summary of different approaches in Table 1. 3.1 Similarity-based Approaches Niemann and Gurevych (2011) and Meyer and Gurevych (2011) created WN-WP and WN-WKT alignments using a framework which first calculates the similarit</context>
</contexts>
<marker>Hamp, Feldweg, 1997</marker>
<rawString>Birgit Hamp and Helmut Feldweg. 1997. Germanet - a lexical-semantic net for german. In In Proceedings of ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications, pages 9–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Henrich</author>
<author>Erhard Hinrichs</author>
<author>Tatiana Vodolazova</author>
</authors>
<title>Semi-Automatic Extension of GermaNet with Sense Definitions from Wiktionary.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th Language and Technology Conference (LTC 2011),</booktitle>
<pages>126--130</pages>
<location>Poznan,</location>
<contexts>
<context position="6440" citStr="Henrich et al., 2011" startWordPosition="992" endWordPosition="995">roblem Description A word sense alignment, or alignment for short, is formally defined as a list of pairs of senses from two LSRs. A pair of aligned senses denote the same meaning. E.g., the two senses of letter “The conventional characters of the alphabet used to represent speech” and “A symbol in an alphabet, bookstave” (taken from WN and WKT, respectively) are clearly equivalent and should be aligned. 2.2 Evaluation Resources For the evaluation of Dijkstra-WSA, we align four pairs of LSRs used in previous work, namely WNOW (Gurevych et al., 2012), WN-WKT (Meyer and Gurevych, 2011), GN-WKT (Henrich et al., 2011) and WN-WP (Niemann and Gurevych, 2011). Our goal is to cover resources with different characteristics: Expert-built (WN, GN) and collaboratively constructed LSRs (WP, WKT, OW), resources in different languages (English and German) and also resources with few sense descriptions (GN) or semantic relations (WKT). We contrastively discuss the results of the Dijkstra-WSA algorithm on these different datasets and relate the results to the properties of the LSRs involved. Moreover, using existing datasets ensures comparability to previous work which discusses only one dataset at a time. WordNet (WN)</context>
<context position="8390" citStr="Henrich et al. (2011)" startWordPosition="1296" endWordPosition="1299">ntained over 3,200,000 article pages, while the German edition contained over 200,000 ones. For each word, multiple senses can be encoded. Similar to WN, they are represented by a gloss and usage examples. There also exist hyperlinks to synonyms, hypernyms, meronyms etc. The targets of these relations are not senses, however, but merely lexemes (i.e. the relations are not disambiguated). 152 LSRs P/R/F /Acc. Approach Meyer and Gurevych (2011) WN-WKT 0.67/0.65/0.66/0.91 Gloss similarity + Machine learning Niemann and Gurevych (2011) WN-WP 0.78/0.78/0.78/0.95 Gloss similarity + Machine learning Henrich et al. (2011) GN-WKT 0.84/0.85/0.84/0.94 Pseudo-gloss overlap de Melo and Weikum (2010) WN-WP 0.86/NA/NA/NA Gloss/article overlap Laparra et al. (2010) FN-WN 0.79/0.79/0.79/NA Dijkstra-SSI+ (WSD algorithm) Navigli (2009) WN 0.64/0.64/0.64/NA Graph-based WSD of WN glosses Ponzetto and Navigli (2009) WN-WP NA/NA/NA/0.81 Graph-based, only for WP categories Navigli and Ponzetto (2012) WN-WP 0.81/0.75/0.78/0.83 Graph-based WSA using WN relations Table 1: Summary of various approaches to WSA. “NA” stands for “Not Available”. OmegaWiki (OW) is a freely editable online dictionary like WKT. However, there do not ex</context>
<context position="10756" citStr="Henrich et al. (2011)" startWordPosition="1660" endWordPosition="1664">alized page rank (PPR) similarity (Agirre and Soroa, 2009) and then learns a threshold on the gold standard to classify each pair of senses as a (non-)valid alignment. This approach was later extended to cross-lingual alignment between the German OW and WN (Gurevych et al., 2012) using a machine translation component. However, its applicability depends on the availability and quality of the glosses, which are not present in every case (e.g. for VN). Moreover, as it involves supervised machine learning, it requires the initial effort of manually annotating a sufficient amount of training data. Henrich et al. (2011) use a similar approach for aligning GN and WKT. However, they use word overlap as a similarity measure and do not require a machine learning component as they align to the candidate sense with the highest similarity regardless of the absolute value. The alignment of WP articles and WN synsets reported by de Melo and Weikum (2010) also relies on word overlap. Although these approaches give reasonable results (with precision in the range of 0.67-0.84), they all depend on the lexical knowledge contained in the glosses, yielding low recall if there is insufficient lexical overlap (known as the “l</context>
<context position="20387" citStr="Henrich et al. (2011)" startWordPosition="3281" endWordPosition="3284">here this lexeme is also monosemous within either resource. E.g., if the noun phrase programming language is contained in either resource and has exactly one sense in each one, we can directly infer the alignment. For WP, a lexeme was considered monosemous if there was exactly one article with this title, also counting titles with a bracketed disambiguation (e.g., Java (programming language) and Java (island) are two distinct senses of Java). While this method does not work perfectly, we observed a precision &gt; 0.95 for monosemous gold standard senses, which is in line with the observations by Henrich et al. (2011). Alignment We consider each sense s E A which has not been aligned in the initialization step. For this, we first retrieve the set of possible target senses T C B (those with matching lemma and part of speech) and compute the shortest path to each of them with Dijkstra’s shortest path algorithm (Dijk155 stra, 1959). The candidate t E T with the shortest distance is then assigned as the alignment target, and the algorithm continues with the next still unaligned sense in A until either all senses are aligned or no path can be found for the remaining senses. The intuition behind this is that the</context>
<context position="24697" citStr="Henrich et al. (2011)" startWordPosition="4025" endWordPosition="4028">sets, including the observed inter-rater agreement A0 (where available) which can be considered as an upper bound for automatic alignment accuracy and the degree of polysemy (i.e. the number of possible alignment targets per sense) which is a hint towards the difficulty of the alignment task. WN 3.0-English WKT We use the gold standard dataset from Meyer and Gurevych (2011) without any modification, thus for comparability to this 2Cross-lingual alignment is left to future work. 156 work, we use the same WKT dump version (from 2010/02/01) which contains around 421,000 senses. GN 7.0-German WKT Henrich et al. (2011) aligned the German WKT (dump from 2011/04/02, 72,000 senses) and GN 7.0. This is the only existing alignment between these two resources so far, and we use their freely available dataset3 to test DijkstraWSA on a language other than English. As this alignment is fairly large (see Table 3), we created a random sample as a gold standard to keep the computation time at bay. However, the datasets are still similar enough to allow direct comparison of the results. Note that no inter-annotator agreement is available for this study. WN 3.0-English WP We use the gold standard from Niemann and Gurevyc</context>
<context position="26009" citStr="Henrich et al. (2011)" startWordPosition="4239" endWordPosition="4242">round 2,921,000 articles. 5.2 Baselines WN-OW We used the same configuration as in Gurevych et al. (2012) to calculate a similaritybased alignment for the monolingual case (i.e. without the translation step) as a baseline and achieved comparable results. WN-WKT As stated above, the alignment4 presented in Meyer and Gurevych (2011) was created by calculating the similarity of glosses and training a machine-learning classifier on the gold standard to classify each pair of senses. GN-WKT The automatic alignment results (i.e. the outcome of the algorithm without manual postcorrection) reported by Henrich et al. (2011) were unavailable for us as a baseline. Thus, we utilize the alignment approach by Meyer and Gurevych (2011) to create a similarity-based baseline, with minor modifications. Unlike the original approach, we directly align senses regardless of their similarity if the decision is trivial (see Section 4.2). We also do not train a machine learning component on a gold standard. Instead, we adapt the idea of Henrich et al. (2011) to align the most similar candidate regardless of the absolute value. WN-WP The alignment reported in Niemann and Gurevych (2011) was created in the same way as the WN-WKT </context>
</contexts>
<marker>Henrich, Hinrichs, Vodolazova, 2011</marker>
<rawString>Verena Henrich, Erhard Hinrichs, and Tatiana Vodolazova. 2011. Semi-Automatic Extension of GermaNet with Sense Definitions from Wiktionary. In Proceedings of the 5th Language and Technology Conference (LTC 2011), pages 126–130, Poznan, Poland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Egoitz Laparra</author>
<author>German Rigau</author>
<author>Montse Cuadros</author>
</authors>
<title>Exploring the integration of WordNet and FrameNet.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th Global WordNet Conference (GWC’10),</booktitle>
<location>Mumbai, India.</location>
<contexts>
<context position="8528" citStr="Laparra et al. (2010)" startWordPosition="1313" endWordPosition="1316"> Similar to WN, they are represented by a gloss and usage examples. There also exist hyperlinks to synonyms, hypernyms, meronyms etc. The targets of these relations are not senses, however, but merely lexemes (i.e. the relations are not disambiguated). 152 LSRs P/R/F /Acc. Approach Meyer and Gurevych (2011) WN-WKT 0.67/0.65/0.66/0.91 Gloss similarity + Machine learning Niemann and Gurevych (2011) WN-WP 0.78/0.78/0.78/0.95 Gloss similarity + Machine learning Henrich et al. (2011) GN-WKT 0.84/0.85/0.84/0.94 Pseudo-gloss overlap de Melo and Weikum (2010) WN-WP 0.86/NA/NA/NA Gloss/article overlap Laparra et al. (2010) FN-WN 0.79/0.79/0.79/NA Dijkstra-SSI+ (WSD algorithm) Navigli (2009) WN 0.64/0.64/0.64/NA Graph-based WSD of WN glosses Ponzetto and Navigli (2009) WN-WP NA/NA/NA/0.81 Graph-based, only for WP categories Navigli and Ponzetto (2012) WN-WP 0.81/0.75/0.78/0.83 Graph-based WSA using WN relations Table 1: Summary of various approaches to WSA. “NA” stands for “Not Available”. OmegaWiki (OW) is a freely editable online dictionary like WKT. However, there do not exist distinct language editions as OW is organized in language-independent concepts (“Defined Meanings”) to which lexicalizations in variou</context>
<context position="11787" citStr="Laparra et al. (2010)" startWordPosition="1833" endWordPosition="1836">s (with precision in the range of 0.67-0.84), they all depend on the lexical knowledge contained in the glosses, yielding low recall if there is insufficient lexical overlap (known as the “lexical gap”, see for instance (Meyer and Gurevych, 2011)). Consider these two senses of Thessalonian in WKT and WN: “A native or inhabitant of Thessalonica” and “Someone or something from, or pertaining to, Thessaloniki”. These are (mostly) identical and should be aligned, but there is no word overlap due to the interchangeable usage of the synonyms Thessalonica and Thessaloniki. 3.2 Graph-based Approaches Laparra et al. (2010) utilize the SSI-Dijkstra+ algorithm to align FN lexical units (LUs) with WN synsets. The basic idea is to align monosemous LUs first and, based on this, find the closest synset in WN for the other LUs in the same frame. However, as SSI-Dijkstra+ is a word sense disambiguation (not alignment) algorithm, the LUs are merely considered as texts which are to be disambiguated; there is no attempt made to build a global graph structure for FN. Moreover, the algorithm solely relies on the 153 semantic relations found in WN and eXtended WN (Mihalcea and Moldovan, 2001). Thus, it is not applicable to o</context>
<context position="15112" citStr="Laparra et al., 2010" startWordPosition="2401" endWordPosition="2404">phs using appropriate parameters, and (ii) the alignment itself. 4.1 Graph Construction We represent the set of senses (or synsets, if applicable) of an LSR L as a set of nodes V where the set of edges E, E ⊆ V × V between these nodes represents semantic relatedness between them. We call this a resource graph. A WP article is considered a sense as it represents a distinct concept. There are multiple options for deriving the edges from the resource. The most straightforward approach is to directly use the existing semantic relations (such as hyponymy), as it has been reported in previous work (Laparra et al., 2010; Navigli, 2009). For WP, we can directly use the given hyperlinks between articles as they also express a certain degree of relatedness (Milne and Witten, 2008). However, for many LSRs no or only few semantic relations exist. Consider WKT: Its relations are not sense disambiguated (Meyer and Gurevych, 2012). We thus cannot determine the correct target sense if a relation is pointing to an ambiguous word. Our solution to this is twofold: First, for each sense s, we create an edge (s, t) for those semantic relations which have a monosemous target t, as in this case the target sense is unambiguo</context>
<context position="45316" citStr="Laparra et al. (2010)" startWordPosition="7419" endWordPosition="7422"> LSRs which structurally differ from the ones discussed here. It is important to determine how resources like FN or VN can be meaningfully transformed into a graph representation. Another idea is to extend the approach to cross-lingual resource alignment, which would require a machine translation component to identify sense alignment candidates with the correct lexeme. Regarding the algorithm itself, the main direction for future work is to increase recall while keeping high precision. One possible way would be to not only link monosemous lexemes, but also to create edges for polysemous ones. Laparra et al. (2010) discuss a possibility to do this with high precision. The main idea is to focus on lexemes with a low degree of polysemy and align if one of the possible senses is clearly more similar to the source sense than the other(s). If recall is still low, more polysemous lexemes can be examined. A weighting of edges (e.g. based on gloss similarities) has not been considered at all, but would be easily applicable to the existing framework. A more elaborate idea would be to investigate entirely different graph-based algorithms, e.g. for matching nodes in bipartite graphs. Also, we plan to investigate i</context>
</contexts>
<marker>Laparra, Rigau, Cuadros, 2010</marker>
<rawString>Egoitz Laparra, German Rigau, and Montse Cuadros. 2010. Exploring the integration of WordNet and FrameNet. In Proceedings of the 5th Global WordNet Conference (GWC’10), Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Matuschek</author>
<author>Christian M Meyer</author>
<author>Iryna Gurevych</author>
</authors>
<title>Multilingual Knowledge in Aligned Wiktionary and OmegaWiki for ComputerAided Translation. Translation: Computation, Corpora, Cognition. Special Issue on “Language Technology for a Multilingual Europe”,</title>
<date>2013</date>
<note>to appear.</note>
<contexts>
<context position="1734" citStr="Matuschek et al., 2013" startWordPosition="235" endWordPosition="238">ion Lexical-semantic resources (LSRs) are a cornerstone for many Natural Language Processing (NLP) applications such as word sense disambiguation (WSD) and information extraction. However, the growing demand for large-scale resources in different languages is hard to meet. The Princeton WordNet (WN) (Fellbaum, 1998) is widely used for English, but for most languages corresponding resources are considerably smaller or missing. Collaboratively constructed resources like Wiktionary (WKT) and OmegaWiki (OW) provide a viable option for such cases and seem especially suitable for smaller languages (Matuschek et al., 2013), but there are still considerable gaps in coverage which need to be filled. A related problem is that there usually does not exist a single resource which works best for all purposes, as different LSRs cover different words, senses and information types. These considerations have sparked increasing research efforts in the area of word sense alignment (WSA). It has been shown that aligned resources can indeed lead to better performance than using the resources individually. Examples include semantic parsing using FrameNet (FN), WN, and VerbNet (VN) (Shi and Mihalcea, 2005), word sense disambig</context>
<context position="27452" citStr="Matuschek et al. (2013)" startWordPosition="4474" endWordPosition="4477"> henceforth mark these similarity-based results with SB. 5.3 System Configurations For the construction of the resource graphs we experimented with three options: Semantic relations only (SR) OW, WN and GN all feature disambiguated sense relations which can be directly used as edges between senses. Note that in the expert-built resources, the majority of nodes are connected by sense relations, while this is not the case for OW. For WKT, only the unambiguous semantic relations can be used (see Section 4.1), resulting in graphs less dense and with many isolated nodes. However, as we reported in Matuschek et al. (2013), the English WKT is almost 6 times as large as the German one for the versions we used in our experiments (421,000 senses vs. 72,000 senses), while it contains not even twice as many relations (720,000 vs. 430,000). This is directly reflected in the fewer isolated nodes for the German WKT. WP links are also unambiguous as they lead to a distinct article. However, intuitively not all links in an article are equally meaningful. Thus, for the SR configuration, we decided to retain only the category links and the links within the first paragraph of the article. We assume that the targets of these</context>
</contexts>
<marker>Matuschek, Meyer, Gurevych, 2013</marker>
<rawString>Michael Matuschek, Christian M. Meyer, and Iryna Gurevych. 2013. Multilingual Knowledge in Aligned Wiktionary and OmegaWiki for ComputerAided Translation. Translation: Computation, Corpora, Cognition. Special Issue on “Language Technology for a Multilingual Europe”, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian M Meyer</author>
<author>Iryna Gurevych</author>
</authors>
<title>What Psycholinguists Know About Chemistry: Aligning Wiktionary and WordNet for Increased Domain Coverage.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP),</booktitle>
<pages>883--892</pages>
<location>Chiang Mai, Thailand.</location>
<contexts>
<context position="3279" citStr="Meyer and Gurevych, 2011" startWordPosition="488" endWordPosition="491">present in few resources such as the most frequent sense (MFS) (Suchanek et al., 2008). This makes it difficult to apply them to a larger set of resources. In earlier work, we presented the large-scale resource UBY (Gurevych et al., 2012). It contains nine resources in two languages which are mapped to a uniform representation using the LMF standard (Eckle-Kohler et al., 2012). They are thus structurally interoperable. UBY contains pairwise sense alignments between a subset of these resources, and this work also presented a framework for creating alignments based on the similarity of glosses (Meyer and Gurevych, 2011). However, it is not clear to what extent this approach can be applied to resources which lack this kind of information (see Section 3). In summary, aligning senses is a key requirement for semantic interoperability of LSRs to increase the 151 Transactions of the Association for Computational Linguistics, 1 (2013) 151–164. Action Editor: Patrick Pantel. Submitted 12/2012; Revised 2/2013; Published 5/2013. c�2013 Association for Computational Linguistics. coverage and effectiveness in NLP tasks. Still, existing efforts are mostly focused on specific types of resources (most often requiring glos</context>
<context position="6409" citStr="Meyer and Gurevych, 2011" startWordPosition="987" endWordPosition="990">ork. 2 Notation and Resources 2.1 Problem Description A word sense alignment, or alignment for short, is formally defined as a list of pairs of senses from two LSRs. A pair of aligned senses denote the same meaning. E.g., the two senses of letter “The conventional characters of the alphabet used to represent speech” and “A symbol in an alphabet, bookstave” (taken from WN and WKT, respectively) are clearly equivalent and should be aligned. 2.2 Evaluation Resources For the evaluation of Dijkstra-WSA, we align four pairs of LSRs used in previous work, namely WNOW (Gurevych et al., 2012), WN-WKT (Meyer and Gurevych, 2011), GN-WKT (Henrich et al., 2011) and WN-WP (Niemann and Gurevych, 2011). Our goal is to cover resources with different characteristics: Expert-built (WN, GN) and collaboratively constructed LSRs (WP, WKT, OW), resources in different languages (English and German) and also resources with few sense descriptions (GN) or semantic relations (WKT). We contrastively discuss the results of the Dijkstra-WSA algorithm on these different datasets and relate the results to the properties of the LSRs involved. Moreover, using existing datasets ensures comparability to previous work which discusses only one </context>
<context position="8215" citStr="Meyer and Gurevych (2011)" startWordPosition="1274" endWordPosition="1277">cribes a distinct concept, and articles are connected by hyperlinks within the article texts. Wiktionary (WKT) is the dictionary pendant to WP. By February 2013 the English WKT contained over 3,200,000 article pages, while the German edition contained over 200,000 ones. For each word, multiple senses can be encoded. Similar to WN, they are represented by a gloss and usage examples. There also exist hyperlinks to synonyms, hypernyms, meronyms etc. The targets of these relations are not senses, however, but merely lexemes (i.e. the relations are not disambiguated). 152 LSRs P/R/F /Acc. Approach Meyer and Gurevych (2011) WN-WKT 0.67/0.65/0.66/0.91 Gloss similarity + Machine learning Niemann and Gurevych (2011) WN-WP 0.78/0.78/0.78/0.95 Gloss similarity + Machine learning Henrich et al. (2011) GN-WKT 0.84/0.85/0.84/0.94 Pseudo-gloss overlap de Melo and Weikum (2010) WN-WP 0.86/NA/NA/NA Gloss/article overlap Laparra et al. (2010) FN-WN 0.79/0.79/0.79/NA Dijkstra-SSI+ (WSD algorithm) Navigli (2009) WN 0.64/0.64/0.64/NA Graph-based WSD of WN glosses Ponzetto and Navigli (2009) WN-WP NA/NA/NA/0.81 Graph-based, only for WP categories Navigli and Ponzetto (2012) WN-WP 0.81/0.75/0.78/0.83 Graph-based WSA using WN rel</context>
<context position="9955" citStr="Meyer and Gurevych (2011)" startWordPosition="1528" endWordPosition="1531">ts and lexicalizations in over 400 languages. GermaNet (GN) is the German counterpart to WN (Hamp and Feldweg, 1997). It is also organized in synsets (around 70,000 in the latest version 7.0) which are connected via semantic relations. 3 Related Work The are two strands of closely related work: Similarity-based and graph-based approaches to word sense alignment. To our knowledge, there exists no previous work which fully represents both LSRs involved in an alignment as graphs. We give a summary of different approaches in Table 1. 3.1 Similarity-based Approaches Niemann and Gurevych (2011) and Meyer and Gurevych (2011) created WN-WP and WN-WKT alignments using a framework which first calculates the similarity of glosses (or glosses and articles in the case of WN-WP) using either cosine or personalized page rank (PPR) similarity (Agirre and Soroa, 2009) and then learns a threshold on the gold standard to classify each pair of senses as a (non-)valid alignment. This approach was later extended to cross-lingual alignment between the German OW and WN (Gurevych et al., 2012) using a machine translation component. However, its applicability depends on the availability and quality of the glosses, which are not pre</context>
<context position="11412" citStr="Meyer and Gurevych, 2011" startWordPosition="1773" endWordPosition="1776">gning GN and WKT. However, they use word overlap as a similarity measure and do not require a machine learning component as they align to the candidate sense with the highest similarity regardless of the absolute value. The alignment of WP articles and WN synsets reported by de Melo and Weikum (2010) also relies on word overlap. Although these approaches give reasonable results (with precision in the range of 0.67-0.84), they all depend on the lexical knowledge contained in the glosses, yielding low recall if there is insufficient lexical overlap (known as the “lexical gap”, see for instance (Meyer and Gurevych, 2011)). Consider these two senses of Thessalonian in WKT and WN: “A native or inhabitant of Thessalonica” and “Someone or something from, or pertaining to, Thessaloniki”. These are (mostly) identical and should be aligned, but there is no word overlap due to the interchangeable usage of the synonyms Thessalonica and Thessaloniki. 3.2 Graph-based Approaches Laparra et al. (2010) utilize the SSI-Dijkstra+ algorithm to align FN lexical units (LUs) with WN synsets. The basic idea is to align monosemous LUs first and, based on this, find the closest synset in WN for the other LUs in the same frame. Howe</context>
<context position="24452" citStr="Meyer and Gurevych (2011)" startWordPosition="3986" endWordPosition="3989">m since for each synset in the German part of OW, there is a translation in the English part. This makes the German-English gold standard directly usable for our purposes.2 Table 3 presents the details about this as well as the other evaluation datasets, including the observed inter-rater agreement A0 (where available) which can be considered as an upper bound for automatic alignment accuracy and the degree of polysemy (i.e. the number of possible alignment targets per sense) which is a hint towards the difficulty of the alignment task. WN 3.0-English WKT We use the gold standard dataset from Meyer and Gurevych (2011) without any modification, thus for comparability to this 2Cross-lingual alignment is left to future work. 156 work, we use the same WKT dump version (from 2010/02/01) which contains around 421,000 senses. GN 7.0-German WKT Henrich et al. (2011) aligned the German WKT (dump from 2011/04/02, 72,000 senses) and GN 7.0. This is the only existing alignment between these two resources so far, and we use their freely available dataset3 to test DijkstraWSA on a language other than English. As this alignment is fairly large (see Table 3), we created a random sample as a gold standard to keep the compu</context>
<context position="25720" citStr="Meyer and Gurevych (2011)" startWordPosition="4195" endWordPosition="4198">re still similar enough to allow direct comparison of the results. Note that no inter-annotator agreement is available for this study. WN 3.0-English WP We use the gold standard from Niemann and Gurevych (2011). For comparability, we use the same Wikipeda dump version (from 2009/08/22) with around 2,921,000 articles. 5.2 Baselines WN-OW We used the same configuration as in Gurevych et al. (2012) to calculate a similaritybased alignment for the monolingual case (i.e. without the translation step) as a baseline and achieved comparable results. WN-WKT As stated above, the alignment4 presented in Meyer and Gurevych (2011) was created by calculating the similarity of glosses and training a machine-learning classifier on the gold standard to classify each pair of senses. GN-WKT The automatic alignment results (i.e. the outcome of the algorithm without manual postcorrection) reported by Henrich et al. (2011) were unavailable for us as a baseline. Thus, we utilize the alignment approach by Meyer and Gurevych (2011) to create a similarity-based baseline, with minor modifications. Unlike the original approach, we directly align senses regardless of their similarity if the decision is trivial (see Section 4.2). We al</context>
<context position="37979" citStr="Meyer and Gurevych (2011)" startWordPosition="6234" endWordPosition="6237"> We experimented with A ≥ 4, achieving reasonable recall, but in this case the precision was so low that this configuration, in conclusion, does not increase the F-Measure. However, 1:n alignments work well with these short paths as the correct alignments are mostly in the close vicinity of a sense, 159 hence we achieve an increase in recall in this case without too much loss of precision. For the hybrid approach, we achieve an F-Measure of 0.69 when using all edges (SR+LM+SB), setting the path length to 2, and also allowing 1:n alignments. This is a statistically significant improvement over Meyer and Gurevych (2011) which again confirms the effectiveness of the hybrid approach. GN-WKT As stated above, we used the SR+LM configuration for GN in every case. For the German WKT, the much greater number of relations compared to its English counterpart is directly reflected in the results, as using the semantic relations only (SR) not only yields the best precision of 0.94 but also a good recall of 0.65. Using the semantic relations together with monosemous links (SR+LM) yields the F-Measure of 0.83, which is on par with the similarity-based (SB) approach. In the hybrid configuration, we can increase the perfor</context>
<context position="43494" citStr="Meyer and Gurevych, 2011" startWordPosition="7125" endWordPosition="7128">y in the graph and the false alignment. As an example, consider two senses of bowdlerization in WN (“written material that has been bowdlerized”) and WKT (“The action or instance of bowdlerizing; the omission or removal of material considered vulgar or indecent.”). While these senses are clearly related, they are not identical and should not be aligned, nevertheless the similar wording (and especially the use of the highly specific verb “bowdlerize”) results in an alignment. Similarly to the similarity-based approaches, it is an open question how this kind of error can be effectively avoided (Meyer and Gurevych, 2011). There is a considerable number of examples where Dijkstra-WSA recognizes an alignment which similarity-based approaches do not. The two senses of Thessalonian from the introductory example (Section 3) contain the terms Thessalonica and Thessaloniki in their glosses which are both monosemous in WN as well as in WKT, sharing the also monosemous noun Greece in their glosses. This yields the bridge between the resources to find a path and correctly derive the alignment. 6 Conclusions and Future Work In this work, we present Dijkstra-WSA, a graphbased algorithm for word sense alignment. We show t</context>
</contexts>
<marker>Meyer, Gurevych, 2011</marker>
<rawString>Christian M. Meyer and Iryna Gurevych. 2011. What Psycholinguists Know About Chemistry: Aligning Wiktionary and WordNet for Increased Domain Coverage. In Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP), pages 883–892, Chiang Mai, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian M Meyer</author>
<author>Iryna Gurevych</author>
</authors>
<title>Wiktionary: A new rival for expert-built lexicons? Exploring the possibilities of collaborative lexicography.</title>
<date>2012</date>
<booktitle>In Sylviane Granger and Magali Paquot, editors, Electronic Lexicography, chapter 13,</booktitle>
<pages>259--291</pages>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="15421" citStr="Meyer and Gurevych, 2012" startWordPosition="2452" endWordPosition="2455"> graph. A WP article is considered a sense as it represents a distinct concept. There are multiple options for deriving the edges from the resource. The most straightforward approach is to directly use the existing semantic relations (such as hyponymy), as it has been reported in previous work (Laparra et al., 2010; Navigli, 2009). For WP, we can directly use the given hyperlinks between articles as they also express a certain degree of relatedness (Milne and Witten, 2008). However, for many LSRs no or only few semantic relations exist. Consider WKT: Its relations are not sense disambiguated (Meyer and Gurevych, 2012). We thus cannot determine the correct target sense if a relation is pointing to an ambiguous word. Our solution to this is twofold: First, for each sense s, we create an edge (s, t) for those semantic relations which have a monosemous target t, as in this case the target sense is unambiguous. This approach, however, only recovers a subset of the relations, and it is not applicable to resources where no sense relations exist at all, e.g. FN. For this case, we propose to use the glosses of senses in the LSR to derive additional edges in the following way: For each monosemous, non-stopword lexem</context>
</contexts>
<marker>Meyer, Gurevych, 2012</marker>
<rawString>Christian M. Meyer and Iryna Gurevych. 2012. Wiktionary: A new rival for expert-built lexicons? Exploring the possibilities of collaborative lexicography. In Sylviane Granger and Magali Paquot, editors, Electronic Lexicography, chapter 13, pages 259–291. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan I Moldovan</author>
</authors>
<title>eXtended WordNet: progress report.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL Workshop on WordNet and Other Lexical Resources,</booktitle>
<pages>95--100</pages>
<location>Pittsburgh, PA, USA.</location>
<contexts>
<context position="12354" citStr="Mihalcea and Moldovan, 2001" startWordPosition="1931" endWordPosition="1934">hessaloniki. 3.2 Graph-based Approaches Laparra et al. (2010) utilize the SSI-Dijkstra+ algorithm to align FN lexical units (LUs) with WN synsets. The basic idea is to align monosemous LUs first and, based on this, find the closest synset in WN for the other LUs in the same frame. However, as SSI-Dijkstra+ is a word sense disambiguation (not alignment) algorithm, the LUs are merely considered as texts which are to be disambiguated; there is no attempt made to build a global graph structure for FN. Moreover, the algorithm solely relies on the 153 semantic relations found in WN and eXtended WN (Mihalcea and Moldovan, 2001). Thus, it is not applicable to other resources which have no or only few relations such as WKT. Navigli (2009) aims at disambiguating WN glosses, i.e. assigning the correct senses to all nonstopwords in each WN gloss. His approach is to find the shortest possible circles in the WN relation graph to identify the correct disambiguation. In later work, this idea was extended to the disambiguation of translations in a bilingual dictionary (Flati and Navigli, 2012). However, there is no discussion of how this idea could be applied to word sense alignment of two or more resources. We build upon thi</context>
</contexts>
<marker>Mihalcea, Moldovan, 2001</marker>
<rawString>Rada Mihalcea and Dan I. Moldovan. 2001. eXtended WordNet: progress report. In Proceedings of NAACL Workshop on WordNet and Other Lexical Resources, pages 95–100, Pittsburgh, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Milne</author>
<author>Ian H Witten</author>
</authors>
<title>An effective, low-cost measure of semantic relatedness obtained from Wikipedia links.</title>
<date>2008</date>
<booktitle>In Proceedings of the AAAI Workshop on Wikipedia and Artificial Intelligence: an Evolving Synergy,</booktitle>
<pages>25--30</pages>
<location>Chicago, IL, USA.</location>
<contexts>
<context position="15273" citStr="Milne and Witten, 2008" startWordPosition="2428" endWordPosition="2431"> as a set of nodes V where the set of edges E, E ⊆ V × V between these nodes represents semantic relatedness between them. We call this a resource graph. A WP article is considered a sense as it represents a distinct concept. There are multiple options for deriving the edges from the resource. The most straightforward approach is to directly use the existing semantic relations (such as hyponymy), as it has been reported in previous work (Laparra et al., 2010; Navigli, 2009). For WP, we can directly use the given hyperlinks between articles as they also express a certain degree of relatedness (Milne and Witten, 2008). However, for many LSRs no or only few semantic relations exist. Consider WKT: Its relations are not sense disambiguated (Meyer and Gurevych, 2012). We thus cannot determine the correct target sense if a relation is pointing to an ambiguous word. Our solution to this is twofold: First, for each sense s, we create an edge (s, t) for those semantic relations which have a monosemous target t, as in this case the target sense is unambiguous. This approach, however, only recovers a subset of the relations, and it is not applicable to resources where no sense relations exist at all, e.g. FN. For th</context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>David Milne and Ian H. Witten. 2008. An effective, low-cost measure of semantic relatedness obtained from Wikipedia links. In Proceedings of the AAAI Workshop on Wikipedia and Artificial Intelligence: an Evolving Synergy, pages 25–30, Chicago, IL, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<pages>193--217</pages>
<contexts>
<context position="2413" citStr="Navigli and Ponzetto, 2012" startWordPosition="344" endWordPosition="347">which need to be filled. A related problem is that there usually does not exist a single resource which works best for all purposes, as different LSRs cover different words, senses and information types. These considerations have sparked increasing research efforts in the area of word sense alignment (WSA). It has been shown that aligned resources can indeed lead to better performance than using the resources individually. Examples include semantic parsing using FrameNet (FN), WN, and VerbNet (VN) (Shi and Mihalcea, 2005), word sense disambiguation using an alignment of WN and Wikipedia (WP) (Navigli and Ponzetto, 2012) and semantic role labeling using a combination of PropBank, VN and FN in the SemLink project (Palmer, 2009). Some of these approaches to WSA either rely heavily on manual labor (e.g. Shi and Mihalcea (2005)) or on information which is only present in few resources such as the most frequent sense (MFS) (Suchanek et al., 2008). This makes it difficult to apply them to a larger set of resources. In earlier work, we presented the large-scale resource UBY (Gurevych et al., 2012). It contains nine resources in two languages which are mapped to a uniform representation using the LMF standard (Eckle-</context>
<context position="8760" citStr="Navigli and Ponzetto (2012)" startWordPosition="1342" endWordPosition="1345">ns are not disambiguated). 152 LSRs P/R/F /Acc. Approach Meyer and Gurevych (2011) WN-WKT 0.67/0.65/0.66/0.91 Gloss similarity + Machine learning Niemann and Gurevych (2011) WN-WP 0.78/0.78/0.78/0.95 Gloss similarity + Machine learning Henrich et al. (2011) GN-WKT 0.84/0.85/0.84/0.94 Pseudo-gloss overlap de Melo and Weikum (2010) WN-WP 0.86/NA/NA/NA Gloss/article overlap Laparra et al. (2010) FN-WN 0.79/0.79/0.79/NA Dijkstra-SSI+ (WSD algorithm) Navigli (2009) WN 0.64/0.64/0.64/NA Graph-based WSD of WN glosses Ponzetto and Navigli (2009) WN-WP NA/NA/NA/0.81 Graph-based, only for WP categories Navigli and Ponzetto (2012) WN-WP 0.81/0.75/0.78/0.83 Graph-based WSA using WN relations Table 1: Summary of various approaches to WSA. “NA” stands for “Not Available”. OmegaWiki (OW) is a freely editable online dictionary like WKT. However, there do not exist distinct language editions as OW is organized in language-independent concepts (“Defined Meanings”) to which lexicalizations in various languages are attached. These can be considered as multilingual synsets, and they are interconnected by unambiguous relations just like WN. As of February 2013, OW contains over 46,000 of these concepts and lexicalizations in over</context>
<context position="13538" citStr="Navigli and Ponzetto (2012)" startWordPosition="2130" endWordPosition="2133">of two or more resources. We build upon this idea of finding shortest paths (circles are a special kind of path) and extend it to multiple resources and edges other than semantic relations, in particular WP links and links to senses of monosemous lexemes appearing in glosses. Ponzetto and Navigli (2009) propose a graphbased method to tackle the related, but slightly different problem of aligning WN synsets and WP categories (not articles). Using semantic relations, they build WN subgraphs for each WP category and then align those synsets which best match the category structure. In later work, Navigli and Ponzetto (2012) also align WN with the full WP. They build “disambiguation contexts” for the senses in both resources by using, for instance, WP redirects or WN glosses and then compute the similarity between these contexts. Again, a graph structure is built from WN semantic relations covering all possible senses in these contexts. The goal is to determine which WN sense is closest to the WP sense to be aligned. While these approaches are in some respects similar to DijkstraWSA, they do not take the global structure of both resources into account. Instead, they merely rely on a (locally restricted) subset of</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network. Artificial Intelligence, 193:217–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Using Cycles and Quasi-Cycles to Disambiguate Dictionary Glosses.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL’09),</booktitle>
<pages>594--602</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="8597" citStr="Navigli (2009)" startWordPosition="1322" endWordPosition="1323">so exist hyperlinks to synonyms, hypernyms, meronyms etc. The targets of these relations are not senses, however, but merely lexemes (i.e. the relations are not disambiguated). 152 LSRs P/R/F /Acc. Approach Meyer and Gurevych (2011) WN-WKT 0.67/0.65/0.66/0.91 Gloss similarity + Machine learning Niemann and Gurevych (2011) WN-WP 0.78/0.78/0.78/0.95 Gloss similarity + Machine learning Henrich et al. (2011) GN-WKT 0.84/0.85/0.84/0.94 Pseudo-gloss overlap de Melo and Weikum (2010) WN-WP 0.86/NA/NA/NA Gloss/article overlap Laparra et al. (2010) FN-WN 0.79/0.79/0.79/NA Dijkstra-SSI+ (WSD algorithm) Navigli (2009) WN 0.64/0.64/0.64/NA Graph-based WSD of WN glosses Ponzetto and Navigli (2009) WN-WP NA/NA/NA/0.81 Graph-based, only for WP categories Navigli and Ponzetto (2012) WN-WP 0.81/0.75/0.78/0.83 Graph-based WSA using WN relations Table 1: Summary of various approaches to WSA. “NA” stands for “Not Available”. OmegaWiki (OW) is a freely editable online dictionary like WKT. However, there do not exist distinct language editions as OW is organized in language-independent concepts (“Defined Meanings”) to which lexicalizations in various languages are attached. These can be considered as multilingual syn</context>
<context position="12465" citStr="Navigli (2009)" startWordPosition="1954" endWordPosition="1955">LUs) with WN synsets. The basic idea is to align monosemous LUs first and, based on this, find the closest synset in WN for the other LUs in the same frame. However, as SSI-Dijkstra+ is a word sense disambiguation (not alignment) algorithm, the LUs are merely considered as texts which are to be disambiguated; there is no attempt made to build a global graph structure for FN. Moreover, the algorithm solely relies on the 153 semantic relations found in WN and eXtended WN (Mihalcea and Moldovan, 2001). Thus, it is not applicable to other resources which have no or only few relations such as WKT. Navigli (2009) aims at disambiguating WN glosses, i.e. assigning the correct senses to all nonstopwords in each WN gloss. His approach is to find the shortest possible circles in the WN relation graph to identify the correct disambiguation. In later work, this idea was extended to the disambiguation of translations in a bilingual dictionary (Flati and Navigli, 2012). However, there is no discussion of how this idea could be applied to word sense alignment of two or more resources. We build upon this idea of finding shortest paths (circles are a special kind of path) and extend it to multiple resources and e</context>
<context position="15128" citStr="Navigli, 2009" startWordPosition="2405" endWordPosition="2406">parameters, and (ii) the alignment itself. 4.1 Graph Construction We represent the set of senses (or synsets, if applicable) of an LSR L as a set of nodes V where the set of edges E, E ⊆ V × V between these nodes represents semantic relatedness between them. We call this a resource graph. A WP article is considered a sense as it represents a distinct concept. There are multiple options for deriving the edges from the resource. The most straightforward approach is to directly use the existing semantic relations (such as hyponymy), as it has been reported in previous work (Laparra et al., 2010; Navigli, 2009). For WP, we can directly use the given hyperlinks between articles as they also express a certain degree of relatedness (Milne and Witten, 2008). However, for many LSRs no or only few semantic relations exist. Consider WKT: Its relations are not sense disambiguated (Meyer and Gurevych, 2012). We thus cannot determine the correct target sense if a relation is pointing to an ambiguous word. Our solution to this is twofold: First, for each sense s, we create an edge (s, t) for those semantic relations which have a monosemous target t, as in this case the target sense is unambiguous. This approac</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Using Cycles and Quasi-Cycles to Disambiguate Dictionary Glosses. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL’09), pages 594–602, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elisabeth Niemann</author>
<author>Iryna Gurevych</author>
</authors>
<title>The People’s Web meets Linguistic Knowledge: Automatic Sense Alignment of Wikipedia and WordNet.</title>
<date>2011</date>
<booktitle>In Proceedings of the 9th International Conference on Computational Semantics (IWCS),</booktitle>
<pages>205--214</pages>
<location>Oxford, UK.</location>
<contexts>
<context position="6479" citStr="Niemann and Gurevych, 2011" startWordPosition="998" endWordPosition="1001">lignment, or alignment for short, is formally defined as a list of pairs of senses from two LSRs. A pair of aligned senses denote the same meaning. E.g., the two senses of letter “The conventional characters of the alphabet used to represent speech” and “A symbol in an alphabet, bookstave” (taken from WN and WKT, respectively) are clearly equivalent and should be aligned. 2.2 Evaluation Resources For the evaluation of Dijkstra-WSA, we align four pairs of LSRs used in previous work, namely WNOW (Gurevych et al., 2012), WN-WKT (Meyer and Gurevych, 2011), GN-WKT (Henrich et al., 2011) and WN-WP (Niemann and Gurevych, 2011). Our goal is to cover resources with different characteristics: Expert-built (WN, GN) and collaboratively constructed LSRs (WP, WKT, OW), resources in different languages (English and German) and also resources with few sense descriptions (GN) or semantic relations (WKT). We contrastively discuss the results of the Dijkstra-WSA algorithm on these different datasets and relate the results to the properties of the LSRs involved. Moreover, using existing datasets ensures comparability to previous work which discusses only one dataset at a time. WordNet (WN) (Fellbaum, 1998) is a lexical resource</context>
<context position="8306" citStr="Niemann and Gurevych (2011)" startWordPosition="1285" endWordPosition="1288">xts. Wiktionary (WKT) is the dictionary pendant to WP. By February 2013 the English WKT contained over 3,200,000 article pages, while the German edition contained over 200,000 ones. For each word, multiple senses can be encoded. Similar to WN, they are represented by a gloss and usage examples. There also exist hyperlinks to synonyms, hypernyms, meronyms etc. The targets of these relations are not senses, however, but merely lexemes (i.e. the relations are not disambiguated). 152 LSRs P/R/F /Acc. Approach Meyer and Gurevych (2011) WN-WKT 0.67/0.65/0.66/0.91 Gloss similarity + Machine learning Niemann and Gurevych (2011) WN-WP 0.78/0.78/0.78/0.95 Gloss similarity + Machine learning Henrich et al. (2011) GN-WKT 0.84/0.85/0.84/0.94 Pseudo-gloss overlap de Melo and Weikum (2010) WN-WP 0.86/NA/NA/NA Gloss/article overlap Laparra et al. (2010) FN-WN 0.79/0.79/0.79/NA Dijkstra-SSI+ (WSD algorithm) Navigli (2009) WN 0.64/0.64/0.64/NA Graph-based WSD of WN glosses Ponzetto and Navigli (2009) WN-WP NA/NA/NA/0.81 Graph-based, only for WP categories Navigli and Ponzetto (2012) WN-WP 0.81/0.75/0.78/0.83 Graph-based WSA using WN relations Table 1: Summary of various approaches to WSA. “NA” stands for “Not Available”. Omeg</context>
<context position="9925" citStr="Niemann and Gurevych (2011)" startWordPosition="1523" endWordPosition="1526">ains over 46,000 of these concepts and lexicalizations in over 400 languages. GermaNet (GN) is the German counterpart to WN (Hamp and Feldweg, 1997). It is also organized in synsets (around 70,000 in the latest version 7.0) which are connected via semantic relations. 3 Related Work The are two strands of closely related work: Similarity-based and graph-based approaches to word sense alignment. To our knowledge, there exists no previous work which fully represents both LSRs involved in an alignment as graphs. We give a summary of different approaches in Table 1. 3.1 Similarity-based Approaches Niemann and Gurevych (2011) and Meyer and Gurevych (2011) created WN-WP and WN-WKT alignments using a framework which first calculates the similarity of glosses (or glosses and articles in the case of WN-WP) using either cosine or personalized page rank (PPR) similarity (Agirre and Soroa, 2009) and then learns a threshold on the gold standard to classify each pair of senses as a (non-)valid alignment. This approach was later extended to cross-lingual alignment between the German OW and WN (Gurevych et al., 2012) using a machine translation component. However, its applicability depends on the availability and quality of </context>
<context position="25305" citStr="Niemann and Gurevych (2011)" startWordPosition="4129" endWordPosition="4132">nrich et al. (2011) aligned the German WKT (dump from 2011/04/02, 72,000 senses) and GN 7.0. This is the only existing alignment between these two resources so far, and we use their freely available dataset3 to test DijkstraWSA on a language other than English. As this alignment is fairly large (see Table 3), we created a random sample as a gold standard to keep the computation time at bay. However, the datasets are still similar enough to allow direct comparison of the results. Note that no inter-annotator agreement is available for this study. WN 3.0-English WP We use the gold standard from Niemann and Gurevych (2011). For comparability, we use the same Wikipeda dump version (from 2009/08/22) with around 2,921,000 articles. 5.2 Baselines WN-OW We used the same configuration as in Gurevych et al. (2012) to calculate a similaritybased alignment for the monolingual case (i.e. without the translation step) as a baseline and achieved comparable results. WN-WKT As stated above, the alignment4 presented in Meyer and Gurevych (2011) was created by calculating the similarity of glosses and training a machine-learning classifier on the gold standard to classify each pair of senses. GN-WKT The automatic alignment res</context>
<context position="26566" citStr="Niemann and Gurevych (2011)" startWordPosition="4329" endWordPosition="4332">rithm without manual postcorrection) reported by Henrich et al. (2011) were unavailable for us as a baseline. Thus, we utilize the alignment approach by Meyer and Gurevych (2011) to create a similarity-based baseline, with minor modifications. Unlike the original approach, we directly align senses regardless of their similarity if the decision is trivial (see Section 4.2). We also do not train a machine learning component on a gold standard. Instead, we adapt the idea of Henrich et al. (2011) to align the most similar candidate regardless of the absolute value. WN-WP The alignment reported in Niemann and Gurevych (2011) was created in the same way as the WN-WKT alignment described in Meyer and Gurevych (2011). Note that while the full alignment results5 proved incomplete, the correct alignment results on the gold standard were available and thus used in our experiments. We will henceforth mark these similarity-based results with SB. 5.3 System Configurations For the construction of the resource graphs we experimented with three options: Semantic relations only (SR) OW, WN and GN all feature disambiguated sense relations which can be directly used as edges between senses. Note that in the expert-built resourc</context>
<context position="40791" citStr="Niemann and Gurevych (2011)" startWordPosition="6688" endWordPosition="6691"> when the links from the full articles are used as edges (LM); while the increase in recall is relatively small the precision drops substantially. However, using all possible links (SR+LM) allows us to balance out precision and recall to some extent, while yielding the same F-Measure as the SR configuration. Note that 1:1 alignments were enforced in any case, as the high polysemy of the dataset in conjunction with the dense WP link structure rendered 1:n alignments very imprecise. Using the hybrid approach, we can increase the FMeasure up to 0.81 (SR+SB), outperforming the results reported in Niemann and Gurevych (2011) by a significant margin. The F-Measure for LM+SB is slightly worse due to the lower precision. Combining all edges (SR+LM+SB) does not influence the results any more, but in any case the hybrid configuration achieves the best overall recall (0.87). In conclusion, our experiments on all four datasets consistently demonstrate that combining Dijkstra-WSA with a similarity-based approach as a back-off yields the strongest performance. The results of these best alignments will be made freely available to the research community on our website (http://www.ukp.tu-darmstadt.de). 5.5 Error Analyis The </context>
</contexts>
<marker>Niemann, Gurevych, 2011</marker>
<rawString>Elisabeth Niemann and Iryna Gurevych. 2011. The People’s Web meets Linguistic Knowledge: Automatic Sense Alignment of Wikipedia and WordNet. In Proceedings of the 9th International Conference on Computational Semantics (IWCS), pages 205–214, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
</authors>
<title>SemLink: Linking PropBank, VerbNet and FrameNet.</title>
<date>2009</date>
<booktitle>In Proceedings of the Generative Lexicon ConferenceGenLex-09,</booktitle>
<pages>9--15</pages>
<location>Pisa, Italy.</location>
<contexts>
<context position="2521" citStr="Palmer, 2009" startWordPosition="364" endWordPosition="365"> purposes, as different LSRs cover different words, senses and information types. These considerations have sparked increasing research efforts in the area of word sense alignment (WSA). It has been shown that aligned resources can indeed lead to better performance than using the resources individually. Examples include semantic parsing using FrameNet (FN), WN, and VerbNet (VN) (Shi and Mihalcea, 2005), word sense disambiguation using an alignment of WN and Wikipedia (WP) (Navigli and Ponzetto, 2012) and semantic role labeling using a combination of PropBank, VN and FN in the SemLink project (Palmer, 2009). Some of these approaches to WSA either rely heavily on manual labor (e.g. Shi and Mihalcea (2005)) or on information which is only present in few resources such as the most frequent sense (MFS) (Suchanek et al., 2008). This makes it difficult to apply them to a larger set of resources. In earlier work, we presented the large-scale resource UBY (Gurevych et al., 2012). It contains nine resources in two languages which are mapped to a uniform representation using the LMF standard (Eckle-Kohler et al., 2012). They are thus structurally interoperable. UBY contains pairwise sense alignments betwe</context>
</contexts>
<marker>Palmer, 2009</marker>
<rawString>Martha Palmer. 2009. SemLink: Linking PropBank, VerbNet and FrameNet. In Proceedings of the Generative Lexicon ConferenceGenLex-09, pages 9–15, Pisa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Roberto Navigli</author>
</authors>
<title>Large-scale taxonomy mapping for restructuring and integrating Wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21&amp;quot; International Joint Conference on Artificial Intelligence,</booktitle>
<pages>2083--2088</pages>
<location>Pasadena, CA, USA.</location>
<contexts>
<context position="8676" citStr="Ponzetto and Navigli (2009)" startWordPosition="1331" endWordPosition="1334">ets of these relations are not senses, however, but merely lexemes (i.e. the relations are not disambiguated). 152 LSRs P/R/F /Acc. Approach Meyer and Gurevych (2011) WN-WKT 0.67/0.65/0.66/0.91 Gloss similarity + Machine learning Niemann and Gurevych (2011) WN-WP 0.78/0.78/0.78/0.95 Gloss similarity + Machine learning Henrich et al. (2011) GN-WKT 0.84/0.85/0.84/0.94 Pseudo-gloss overlap de Melo and Weikum (2010) WN-WP 0.86/NA/NA/NA Gloss/article overlap Laparra et al. (2010) FN-WN 0.79/0.79/0.79/NA Dijkstra-SSI+ (WSD algorithm) Navigli (2009) WN 0.64/0.64/0.64/NA Graph-based WSD of WN glosses Ponzetto and Navigli (2009) WN-WP NA/NA/NA/0.81 Graph-based, only for WP categories Navigli and Ponzetto (2012) WN-WP 0.81/0.75/0.78/0.83 Graph-based WSA using WN relations Table 1: Summary of various approaches to WSA. “NA” stands for “Not Available”. OmegaWiki (OW) is a freely editable online dictionary like WKT. However, there do not exist distinct language editions as OW is organized in language-independent concepts (“Defined Meanings”) to which lexicalizations in various languages are attached. These can be considered as multilingual synsets, and they are interconnected by unambiguous relations just like WN. As of </context>
<context position="13215" citStr="Ponzetto and Navigli (2009)" startWordPosition="2077" endWordPosition="2080"> to find the shortest possible circles in the WN relation graph to identify the correct disambiguation. In later work, this idea was extended to the disambiguation of translations in a bilingual dictionary (Flati and Navigli, 2012). However, there is no discussion of how this idea could be applied to word sense alignment of two or more resources. We build upon this idea of finding shortest paths (circles are a special kind of path) and extend it to multiple resources and edges other than semantic relations, in particular WP links and links to senses of monosemous lexemes appearing in glosses. Ponzetto and Navigli (2009) propose a graphbased method to tackle the related, but slightly different problem of aligning WN synsets and WP categories (not articles). Using semantic relations, they build WN subgraphs for each WP category and then align those synsets which best match the category structure. In later work, Navigli and Ponzetto (2012) also align WN with the full WP. They build “disambiguation contexts” for the senses in both resources by using, for instance, WP redirects or WN glosses and then compute the similarity between these contexts. Again, a graph structure is built from WN semantic relations coveri</context>
</contexts>
<marker>Ponzetto, Navigli, 2009</marker>
<rawString>Simone Paolo Ponzetto and Roberto Navigli. 2009. Large-scale taxonomy mapping for restructuring and integrating Wikipedia. In Proceedings of the 21&amp;quot; International Joint Conference on Artificial Intelligence, pages 2083–2088, Pasadena, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using Information Content to Evaluate Semantic Similarity in a Taxonomy.</title>
<date>1995</date>
<booktitle>In International Joint Conference for Artificial Intelligence (IJCAI-95),</booktitle>
<pages>448--453</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="16659" citStr="Resnik, 1995" startWordPosition="2678" endWordPosition="2679">ma and part of speech) in the gloss of a sense s1 with a sense sl, we introduce an edge (s1, sl). Moreover, if there is another sense s2 with l in its gloss, we also introduce an edge (s1, s2). This technique will be called linking of monosemous lexemes or monosemous linking throughout the rest of this paper. The intuition behind this is that monosemous lexemes usually have a rather specific meaning, and thus it can be expected that the senses in whose description they appear have at least a certain degree of semantic relationship. This directly relates to the notion of “information content” (Resnik, 1995), stating that senses in an LSR which are more specific (and hence more likely to be monosemous) are more useful for evaluating semantic similarity. Note that this step requires part of speech tagging 154 of the glosses, which we perform as a preprocessing step. Thereby we filter out stopwords and words tagged as “unknown” by the POS tagger. As an example, consider the gloss of Java: “An object-oriented programming language”. Even in the absence of any semantic relations, we could unambiguously derive an edge between this sense of Java and the multiword noun programming language if the latter </context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using Information Content to Evaluate Semantic Similarity in a Taxonomy. In International Joint Conference for Artificial Intelligence (IJCAI-95), pages 448–453, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Shi</author>
<author>Rada Mihalcea</author>
</authors>
<title>Putting Pieces Together: Combining FrameNet, VerbNet and WordNet for Robust Semantic Parsing.</title>
<date>2005</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing: 6th International Conference,</booktitle>
<volume>3406</volume>
<pages>100--111</pages>
<publisher>Berlin/Heidelberg: Springer.</publisher>
<contexts>
<context position="2313" citStr="Shi and Mihalcea, 2005" startWordPosition="328" endWordPosition="331">r smaller languages (Matuschek et al., 2013), but there are still considerable gaps in coverage which need to be filled. A related problem is that there usually does not exist a single resource which works best for all purposes, as different LSRs cover different words, senses and information types. These considerations have sparked increasing research efforts in the area of word sense alignment (WSA). It has been shown that aligned resources can indeed lead to better performance than using the resources individually. Examples include semantic parsing using FrameNet (FN), WN, and VerbNet (VN) (Shi and Mihalcea, 2005), word sense disambiguation using an alignment of WN and Wikipedia (WP) (Navigli and Ponzetto, 2012) and semantic role labeling using a combination of PropBank, VN and FN in the SemLink project (Palmer, 2009). Some of these approaches to WSA either rely heavily on manual labor (e.g. Shi and Mihalcea (2005)) or on information which is only present in few resources such as the most frequent sense (MFS) (Suchanek et al., 2008). This makes it difficult to apply them to a larger set of resources. In earlier work, we presented the large-scale resource UBY (Gurevych et al., 2012). It contains nine re</context>
</contexts>
<marker>Shi, Mihalcea, 2005</marker>
<rawString>Lei Shi and Rada Mihalcea. 2005. Putting Pieces Together: Combining FrameNet, VerbNet and WordNet for Robust Semantic Parsing. In Computational Linguistics and Intelligent Text Processing: 6th International Conference, volume 3406 of Lecture Notes in Computer Science, pages 100–111. Berlin/Heidelberg: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>YAGO: A Large Ontology from Wikipedia and WordNet. Web Semantics,</title>
<date>2008</date>
<volume>6</volume>
<issue>3</issue>
<pages>217</pages>
<contexts>
<context position="2740" citStr="Suchanek et al., 2008" startWordPosition="401" endWordPosition="404">aligned resources can indeed lead to better performance than using the resources individually. Examples include semantic parsing using FrameNet (FN), WN, and VerbNet (VN) (Shi and Mihalcea, 2005), word sense disambiguation using an alignment of WN and Wikipedia (WP) (Navigli and Ponzetto, 2012) and semantic role labeling using a combination of PropBank, VN and FN in the SemLink project (Palmer, 2009). Some of these approaches to WSA either rely heavily on manual labor (e.g. Shi and Mihalcea (2005)) or on information which is only present in few resources such as the most frequent sense (MFS) (Suchanek et al., 2008). This makes it difficult to apply them to a larger set of resources. In earlier work, we presented the large-scale resource UBY (Gurevych et al., 2012). It contains nine resources in two languages which are mapped to a uniform representation using the LMF standard (Eckle-Kohler et al., 2012). They are thus structurally interoperable. UBY contains pairwise sense alignments between a subset of these resources, and this work also presented a framework for creating alignments based on the similarity of glosses (Meyer and Gurevych, 2011). However, it is not clear to what extent this approach can b</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2008</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2008. YAGO: A Large Ontology from Wikipedia and WordNet. Web Semantics, 6(3):203– 217.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>