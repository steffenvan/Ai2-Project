<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000023">
<title confidence="0.982345">
Modeling Factuality Judgments in Social Media Text
</title>
<author confidence="0.994405">
Sandeep Soni Tanushree Mitra Eric Gilbert Jacob Eisenstein
</author>
<affiliation confidence="0.9989595">
School of Interactive Computing
Georgia Institute of Technology
</affiliation>
<email confidence="0.984528">
soni.sandeepb@gmail.com, {tmitra3,gilbert,jeisenst}@cc.gatech.edu
</email>
<sectionHeader confidence="0.997228" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999305">
How do journalists mark quoted content
as certain or uncertain, and how do read-
ers interpret these signals? Predicates such
as thinks, claims, and admits offer a range
of options for framing quoted content ac-
cording to the author’s own perceptions of
its credibility. We gather a new dataset
of direct and indirect quotes from Twit-
ter, and obtain annotations of the perceived
certainty of the quoted statements. We
then compare the ability of linguistic and
extra-linguistic features to predict readers’
assessment of the certainty of quoted con-
tent. We see that readers are indeed influ-
enced by such framing devices — and we
find no evidence that they consider other
factors, such as the source, journalist, or
the content itself. In addition, we examine
the impact of specific framing devices on
perceptions of credibility.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999532">
Contemporary journalism is increasingly con-
ducted through social media services like Twit-
ter (Lotan et al., 2011; Hermida et al., 2012). As
events unfold, journalists and political commen-
tators use quotes — often indirect — to convey
potentially uncertain information and claims from
their sources and informants, e.g.,
</bodyText>
<figureCaption confidence="0.996019">
Figure 1: Indirect quotations in Twitter
</figureCaption>
<bodyText confidence="0.999838953488372">
A key pragmatic goal of such messages is to
convey the provenance and uncertainty of the
quoted content. In some cases, the author may also
introduce their own perspective (Lin et al., 2006)
through the use of framing (Greene and Resnik,
2009). For instance, consider the use of the word
claims in Figure 1, which conveys the author’s
doubt about the indirectly quoted content.
Detecting and reasoning about the certainty of
propositional content has been identified as a key
task for information extraction, and is now sup-
ported by the FactBank corpus of annotations for
newstext (Saur´ı and Pustejovsky, 2009). However,
less is known about this phenomenon in social
media — a domain whose endemic uncertainty
makes proper treatment of factuality even more
crucial (Morris et al., 2012). Successful automa-
tion of factuality judgments could help to detect
online rumors (Qazvinian et al., 2011), and might
enable new applications, such as the computation
of reliability ratings for ongoing stories.
This paper investigates how linguistic resources
and extra-linguistic factors affect perceptions of
the certainty of quoted information in Twitter. We
present a new dataset of Twitter messages that use
FactBank predicates (e.g., claim, say, insist) to
scope the claims of named entity sources. This
dataset was annotated by Mechanical Turk work-
ers who gave ratings for the factuality of the
scoped claims in each Twitter message. This en-
ables us to build a predictive model of the fac-
tuality annotations, with the goal of determining
the full set of relevant factors, including the pred-
icate, the source, the journalist, and the content
of the claim itself. However, we find that these
extra-linguistic factors do not predict readers’ fac-
tuality judgments, suggesting that the journalist’s
own framing plays a decisive role in the cred-
ibility of the information being conveyed. We
explore the specific linguistic feature that affect
factuality judgments, and compare our findings
with previously-proposed groupings of factuality-
related predicates.
</bodyText>
<page confidence="0.980174">
415
</page>
<bodyText confidence="0.4467405">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 415–420,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</bodyText>
<subsectionHeader confidence="0.32121">
Cue Words
</subsectionHeader>
<figureCaption confidence="0.997934666666667">
Figure 2: Count of cue words in our dataset. Each
word is patterned according to its group, as shown
in Figure 3.
</figureCaption>
<figure confidence="0.99675675">
600
500
400
300
200
100
0
Cue Groups
</figure>
<figureCaption confidence="0.999883">
Figure 3: Count of cue groups in our dataset
</figureCaption>
<sectionHeader confidence="0.934728" genericHeader="method">
2 Text data
</sectionHeader>
<bodyText confidence="0.997619647058824">
We gathered a dataset of Twitter messages from
103 professional journalists and bloggers who
work in the field of American Politics.1 Tweets
were gathered using Twitter’s streaming API, ex-
tracting the complete permissible timeline up to
February 23, 2014. A total of 959,754 tweets were
gathered, and most were written in early 2014.
Our interest in this text is specifically in quoted
content — including “indirect” quotes, which may
include paraphrased quotations, as in the examples
in Figure 1. While labeled datasets for such quotes
have been created (O’Keefe et al., 2012; Pareti,
2012), these are not freely available at present. In
any case, the relevance of these datasets to Twitter
text is currently unproven. Therefore, rather than
train a supervised model to detect quotations, we
apply a simple dependency-based heuristic.
</bodyText>
<listItem confidence="0.730311166666667">
• We focus on tweets that contain any member of
a list of source-introducing predicates (we bor-
row the terminology of Pareti (2012) and call
this the CUE). Our complete list — shown in
Table 1 — was selected mainly from the exam-
ples presented by Saur´ı and Pustejovsky (2012),
</listItem>
<footnote confidence="0.8421945">
1We used the website http://muckrack.com.
Report say, report, tell, told, observe, state,
</footnote>
<table confidence="0.435139571428571">
accord, insist, assert, claim, main-
tain, explain, deny
Knowledge learn, admit, discover, forget, forgot
Belief think, thought, predict, suggest,
guess, believe
Doubt doubt, wonder, ask, hope
Perception sense,hear,feel
</table>
<tableCaption confidence="0.88954">
Table 1: Lemmas of source-introducing predicates
(cues) and groups (Saur´ı, 2008).
</tableCaption>
<bodyText confidence="0.867300137931034">
but with reference also to Saur´ı’s (2008) dis-
sertation for cues that are common in Twitter.
The Porter Stemmer is applied to match inflec-
tions, e.g. denies/denied; for irregular cases
not handled by the Porter Stemmer (e.g., for-
get/forgot), we include both forms. We use the
CMU Twitter Part-of-Speech Tagger (Owoputi
et al., 2013) to select only instances in the verb
sense. Figure 2 shows the distribution of the
cues and Figure 3 shows the distribution of the
cue groups. For cues that appear in multiple
groups, we chose the most common group.
• We run the Stanford Dependency parser to
obtain labeled dependencies (De Marneffe et
al., 2006), requiring that the cue has outgoing
edges of the type NSUBJ (noun subject) and
CCOMP (clausal complement). The subtree
headed by the modifier of the CCOMP relation
is considered the claim; the subtree headed by
the modifier of the NSUBJ relation is consid-
ered the source. See Figure 4 for an example.
• We use a combination of regular expressions
and dependency rules to capture expressions
of the type “CLAIM, according to SOURCE.”
Specifically, the PCOMP path from according
is searched for the pattern according to *.
The text that matches the * is the source and the
remaining text other than the source is taken as
the claim.
</bodyText>
<listItem confidence="0.6461355">
• Finally, we restrict consideration to tweets in
which the source contains a named entity or
twitter username. This eliminates expressions
of personal belief such as I doubt Obama will
win, as well as anonymous sources such as
Team sources report that Lebron has demanded
a trade to New York. Investigating the factual-
ity judgments formed in response to such tweets
is clearly an important problem for future re-
search, but is outside the scope of this paper.
</listItem>
<bodyText confidence="0.9981855">
This heuristic pipeline may miss many relevant
tweets, but since the overall volume is high, we
</bodyText>
<figure confidence="0.965677722222223">
150
Counts
100
50
0
say
tell
think
accord
suggest
claim
believ
admit
ask
predict
report
explain
deni
hope
learn
insist
hear
wonder
feel
state
discov
forget
assert
guess
observ
maintain
doubt
Counts
Report Belief Knowledge Doubt Perception
416
Source Cue Claim
</figure>
<figureCaption confidence="0.989998">
Figure 4: Dependency parse of an example message, with claim, source, and cue.
</figureCaption>
<figure confidence="0.915743142857143">
mark
nsubj
ccomp
nsubj
aux+neg dobj
I guess, since FBI claims
it couldn’t match Tsarnaev, we can assume ...
</figure>
<table confidence="0.979591375">
Total journalists 443
Total U.S. political journalists 103
Total tweets 959754
Tweets with cues 172706
Tweets with source and claims 40615
Total tweets annotated 1265
Unique sources in annotated dataset 766
Unigrams in annotated dataset 1345
</table>
<tableCaption confidence="0.9945315">
Table 2: Count Statistics of the entire data col-
lected and the annotated dataset
</tableCaption>
<figureCaption confidence="0.999081">
Figure 5: Turk annotation interface
</figureCaption>
<bodyText confidence="0.850706">
prioritize precision. The resulting dataset is sum-
marized in Table 2.
</bodyText>
<sectionHeader confidence="0.998109" genericHeader="method">
3 Annotation
</sectionHeader>
<bodyText confidence="0.999973375">
We used Amazon Mechanical Turk (AMT) to col-
lect ratings of claims. AMT has been widely used
by the NLP community to collect data (Snow et
al., 2008), with “best practices” defined to help
requesters best design Turk jobs (Callison-Burch
and Dredze, 2010). We followed these guidelines
to perform pilot experiments to test the instruction
set and the quality of responses. Based on the pi-
lot study we designed Human Intelligence Tasks
(HITs) to annotate 1265 claims.
Each HIT contained a batch of ten tweets and
rewarded $0.10 per hit. To ensure quality con-
trol we required the Turkers to have at least 85%
hit approval rating and to reside in the United
States, because the Twitter messages in our dataset
were related to American politics. For each tweet,
we obtained five independent ratings from Turk-
ers satisfying the above qualifications. The rat-
ings were based on a 5-point Likert scale rang-
ing from “[-2] Certainly False” to “[2] Certainly
True” and allowing for “[0] Uncertain”. We also
allowed for “Not Applicable” option to capture
ratings where the Turkers did not have sufficient
knowledge about the statement or if the statement
was not really a claim. Figure 6 shows the set of
instructions provided to the Turkers, and Figure 5
illustrates the annotation interface.2
We excluded tweets for which three or more
Turkers gave a rating of “Not Applicable,” leaving
us with a dataset of 1170 tweets. Within this set,
the average variance per tweet (excluding “Not
Applicable” ratings) was 0.585.
</bodyText>
<sectionHeader confidence="0.967534" genericHeader="method">
4 Modeling factuality judgments
</sectionHeader>
<bodyText confidence="0.9998055">
Having obtained a corpus of factuality ratings, we
now model the factors that drive these ratings.
</bodyText>
<subsectionHeader confidence="0.995978">
4.1 Predictive accuracy
</subsectionHeader>
<bodyText confidence="0.999663666666667">
First, we attempt to determine the impact of vari-
ous predictive features on rater judgments of fac-
tuality. We consider the following features:
</bodyText>
<listItem confidence="0.954754714285714">
• Cue word: after stemming
• Cue word group: as given in Table 1
• Source: represented by the named entity or
username in the source field (see Figure 4)
• Journalist: represented by their Twitter ID
• Claim: represented by a bag-of-words vector
from the claim field (Figure 4)
</listItem>
<bodyText confidence="0.999183636363636">
These features are used as predictors in a series
of linear ridge regressions, where the dependent
variable is the mean certainty rating. We throw
out tweets that were rated as “not applicable” by a
majority of raters, but otherwise ignore “not appli-
cable” ratings of the remaining tweets. The goal
of these regressions is to determine which fea-
tures are predictive of raters’ factuality judgments.
The ridge regression regularization parameter was
tuned via cross-validation in the training set. We
used the bootstrap to obtain multiple training/test
</bodyText>
<footnote confidence="0.992791">
2The data is available at https://www.github.
com/jacobeisenstein/twitter-certainty.
</footnote>
<page confidence="0.985184">
417
</page>
<figureCaption confidence="0.973777">
Figure 6: User instructions for the annotation task
</figureCaption>
<table confidence="0.999732444444444">
Features Error
Baseline .442
Cue word .404*
Cue word group .42
Source .447
Journalist .444
Claim .476
Cue word + cue word group .404*
All features .420
</table>
<tableCaption confidence="0.818324666666667">
Table 3: Linear regression error rates for each fea-
ture group. * indicates improvement over the base-
line at p &lt; .05.
</tableCaption>
<bodyText confidence="0.995390380952381">
splits (70% training), which were used for signifi-
cance testing.
Table 3 reports mean average error for each fea-
ture group, as well as a baseline that simply re-
ports the mean rating across the training set. Each
accuracy was compared with the baseline using a
paired z-test. Only the cue word features pass this
test at p &lt; .05. The other features do not help,
even in combination with the cue word.
While these findings must be interpreted with
caution, they suggest that readers — at least, Me-
chanical Turk workers — use relatively little inde-
pendent judgment to assess the validity of quoted
text that they encounter on Twitter. Of course,
richer linguistic models, more advanced machine
learning, or experiments with more carefully-
selected readers might offer a different view. But
the results at hand are most compatible with the
conclusion that readers base their assessments of
factuality only on the framing provided by the
journalist who reports the quote.
</bodyText>
<subsectionHeader confidence="0.997125">
4.2 Cue words and cue groups
</subsectionHeader>
<bodyText confidence="0.999779833333334">
Given the importance of cue words as a sig-
nal for factuality, we want to assess the factual-
ity judgments induced by each cue. A second
question is whether proposed groupings of cue
words into groups cohere with such perceptions.
Saur´ı (2008) describes several classes of source-
introducing predicates, which indicate how the
source relates to the quoted claim. These classes
are summarized in Table 1, along with frequently-
occuring cues from our corpus. We rely on Fact-
Bank to assign the cue words to classes; the only
word not covered by FactBank was sense, which
we placed in predicates of perception.
We performed another set of linear regressions,
again using the mean certainty rating as the de-
pendent variable. In this case, there was no train-
ing/test split, so confidence intervals on the result-
ing parameters are computed using the analytic
closed form. We performed two such regressions:
first using only the individual cues as predictors,
and then using only the cue groups. Results are
shown in Figures 7 and 8; Figure 7 includes only
cues which appear at least ten times, although all
cues were included in the regression.
The cues that give the highest factuality coef-
ficients are learn and admit, which are labeled as
predicates of knowledge. These cues carry a sub-
stantial amount of framing, as they purport to de-
scribe the private mental state of the source. The
word admit often applies to statements that are
perceived as damaging to the source, such as Bill
Gates admits Control-Alt-Delete was a mistake;
since there can be no self-interest behind such
statements, they may be perceived as more likely
to be true.
Several of the cues with the lowest factuality co-
efficients are predicates of belief: suggest, predict
and think. The words suggest, think, and believe
also purport to describe the private mental state of
the source, but their framing function is the op-
posite of the predicates of knowledge: they im-
ply that it is important to mark the claim as the
source’s belief, and not a widely-accepted fact.
For example, Mubarak clearly believes he has the
military leadership’s support.
A third group of interest are the predicates of
report, which have widely-varying certainty coef-
ficients. The cues according, report, say, and tell
</bodyText>
<page confidence="0.992764">
418
</page>
<figure confidence="0.999729653846154">
admit
accord
report
hear
say
tell
explain
ask
insist
believ
claim
suggest
predict
hope
think
deni
1.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0
learn
Cue Words
</figure>
<figureCaption confidence="0.980589">
Figure 7: Linear regression coefficients for
frequently-occurring cue words. Each word is pat-
terned according to its group, shown in Figure 8.
</figureCaption>
<figure confidence="0.9967649">
1.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0
Knowledge
Cue Groups
</figure>
<figureCaption confidence="0.997887">
Figure 8: Linear regression coefficients for cue
word group.
</figureCaption>
<bodyText confidence="0.999879583333333">
are strongly predictive of certainty, but the cues
claim and deny convey uncertainty. Both accord-
ing and report are often used in conjunction with
impersonal and institutional sources, e.g., Cuc-
cinelli trails McAuliffe by 24 points, according to
a new poll. In contrast, insist, claim, and deny im-
ply that there is uncertainty about the quoted state-
ment, e.g., Christie insists that Fort Lee Mayor
was never on my radar. In this case, the fact that
the predicate indicates a report is not enough to
determine the framing: different sorts of reports
carry radically different perceptions of factuality.
</bodyText>
<sectionHeader confidence="0.999951" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.999790102564103">
Factuality and Veridicality The creation of
FactBank (Sauriand Pustejovsky, 2009) has en-
abled recent work on the factuality (or “veridical-
ity”) of event mentions in text. Saur´ı and Puste-
jovsky (2012) propose a two-dimensional factual-
ity annotation scheme, including polarity and cer-
tainty; they then build a classifier to predict an-
notations of factuality from statements in Fact-
Bank. Their work on source-introducing predi-
cates provides part of the foundation for this re-
search, which focuses on quoted statements in so-
cial media text. de Marneffe et al. (2012) conduct
an empirical evaluation of FactBank ratings from
Mechanical Turk workers, finding a high degree of
disagreement between raters. They also construct
a statistical model to predict these ratings. We are
unaware of prior work comparing the contribution
of linguistic and extra-linguistic predictors (e.g.,
source and journalist features) for factuality rat-
ings. This prior work also does not measure the
impact of individual cues and cue classes on as-
sessment of factuality.
Credibility in social media Recent work in the
area of computational social science focuses on
understanding credibility cues on Twitter. Such
studies have found that users express concern over
the credibility of tweets belonging to certain topics
(politics, news, emergency). By manipulating sev-
eral features of a tweet, Morris et al. (2012) found
that in addition to content, users often use addi-
tional markers while assessing the tweet credibil-
ity, such as the user name of the source. The search
for reliable signals of information credibility in so-
cial media has led to the construction of automatic
classifiers to identify credible tweets (Castillo et
al., 2011). However, this prior work has not ex-
plored the linguistic basis of factuality judgments,
which we show to depend on framing devices such
as cue words.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99996552631579">
Perceptions of the factuality of quoted content are
influenced by the cue words used to introduce
them, while extra-linguistic factors, such as the
source and the author, did not appear to be rele-
vant in our experiments. This result is obtained
from real tweets written by journalists; a natural
counterpart study would be to experimentally ma-
nipulate this framing to see if the same perceptions
apply. Another future direction would be to test
whether the deployment of cue words as framing
devices reflects the ideology of the journalist. We
are also interested to group multiple instances of
the same quote (Leskovec et al., 2009), and exam-
ine how its framing varies across different news
outlets and over time.
Acknowledgments: This research was supported
by DARPA-W911NF-12-1-0043 and by a Compu-
tational Journalism research award from Google.
We thank the reviewers for their helpful feedback.
</bodyText>
<figure confidence="0.688698666666667">
Coefficients&apos; Estimates
Coefficients&apos; Estimates
Perception Report Doubt Belief
</figure>
<page confidence="0.9973">
419
</page>
<sectionHeader confidence="0.996179" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999823989130435">
Chris Callison-Burch and Mark Dredze. 2010. Cre-
ating speech and language data with amazon’s me-
chanical turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon’s Mechanical Turk, pages 1–12.
Association for Computational Linguistics.
Carlos Castillo, Marcelo Mendoza, and Barbara
Poblete. 2011. Information credibility on twitter.
In Proceedings of the 20th international conference
on World wide web, pages 675–684. ACM.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generat-
ing typed dependency parses from phrase structure
parses. In Proceedings of LREC, volume 6, pages
449–454.
Marie C. de Marneffe, Christopher D. Manning, and
Christopher Potts. 2012. Did it happen? the prag-
matic complexity of veridicality assessment. Com-
put. Linguist., 38(2):301–333, June.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 503–511, Boulder, Colorado, June.
Association for Computational Linguistics.
Alfred Hermida, Seth C Lewis, and Rodrigo Zamith.
2012. Sourcing the arab spring: A case study of
andy carvins sources during the tunisian and egyp-
tian revolutions. In international symposium on on-
line journalism, Austin, TX, April, pages 20–21.
Jure Leskovec, Lars Backstrom, and Jon Kleinberg.
2009. Meme-tracking and the dynamics of the news
cycle. In Proceedings of the 15th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, pages 497–506. ACM.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on?: Identifying perspectives at the document and
sentence levels. In Proceedings of the Tenth Con-
ference on Computational Natural Language Learn-
ing, CoNLL-X ’06, pages 109–116, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Gilad Lotan, Erhardt Graeff, Mike Ananny, Devin
Gaffney, Ian Pearce, et al. 2011. The arab spring—
the revolutions were tweeted: Information flows dur-
ing the 2011 tunisian and egyptian revolutions. In-
ternational Journal of Communication, 5:31.
Meredith Ringel Morris, Scott Counts, Asta Roseway,
Aaron Hoff, and Julia Schwarz. 2012. Tweeting
is believing?: understanding microblog credibility
perceptions. In Proceedings of the ACM 2012 con-
ference on Computer Supported Cooperative Work,
pages 441–450. ACM.
Tim O’Keefe, Silvia Pareti, James R Curran, Irena Ko-
prinska, and Matthew Honnibal. 2012. A sequence
labelling approach to quote attribution. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 790–
799. Association for Computational Linguistics.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT, pages 380–390.
Silvia Pareti. 2012. A database of attribution relations.
In LREC, pages 3213–3217.
Vahed Qazvinian, Emily Rosengren, Dragomir R
Radev, and Qiaozhu Mei. 2011. Rumor has it:
Identifying misinformation in microblogs. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 1589–1599.
Association for Computational Linguistics.
Roser Saur´ı and James Pustejovsky. 2009. Factbank:
A corpus annotated with event factuality. Language
Resources and Evaluation, 43(3):227–268.
Roser Sauriand James Pustejovsky. 2012. Are you
sure that this happened? assessing the factuality de-
gree of events in text. Comput. Linguist., 38(2):261–
299, June.
Roser Sauri. 2008. A Factuality Profiler for Eventual-
ities in Text. Ph.D. thesis, Brandeis University.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast—but is it
good?: Evaluating non-expert annotations for nat-
ural language tasks. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’08, pages 254–263, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.998369">
420
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.987192">
<title confidence="0.999598">Modeling Factuality Judgments in Social Media Text</title>
<author confidence="0.999886">Sandeep Soni Tanushree Mitra Eric Gilbert Jacob Eisenstein</author>
<affiliation confidence="0.9999145">School of Interactive Georgia Institute of Technology</affiliation>
<abstract confidence="0.999406952380952">How do journalists mark quoted content as certain or uncertain, and how do readers interpret these signals? Predicates such and a range of options for framing quoted content according to the author’s own perceptions of its credibility. We gather a new dataset of direct and indirect quotes from Twitter, and obtain annotations of the perceived certainty of the quoted statements. We then compare the ability of linguistic and extra-linguistic features to predict readers’ assessment of the certainty of quoted content. We see that readers are indeed influenced by such framing devices — and we find no evidence that they consider other factors, such as the source, journalist, or the content itself. In addition, we examine the impact of specific framing devices on perceptions of credibility.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Mark Dredze</author>
</authors>
<title>Creating speech and language data with amazon’s mechanical turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>1--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8414" citStr="Callison-Burch and Dredze, 2010" startWordPosition="1334" endWordPosition="1337"> tweets 959754 Tweets with cues 172706 Tweets with source and claims 40615 Total tweets annotated 1265 Unique sources in annotated dataset 766 Unigrams in annotated dataset 1345 Table 2: Count Statistics of the entire data collected and the annotated dataset Figure 5: Turk annotation interface prioritize precision. The resulting dataset is summarized in Table 2. 3 Annotation We used Amazon Mechanical Turk (AMT) to collect ratings of claims. AMT has been widely used by the NLP community to collect data (Snow et al., 2008), with “best practices” defined to help requesters best design Turk jobs (Callison-Burch and Dredze, 2010). We followed these guidelines to perform pilot experiments to test the instruction set and the quality of responses. Based on the pilot study we designed Human Intelligence Tasks (HITs) to annotate 1265 claims. Each HIT contained a batch of ten tweets and rewarded $0.10 per hit. To ensure quality control we required the Turkers to have at least 85% hit approval rating and to reside in the United States, because the Twitter messages in our dataset were related to American politics. For each tweet, we obtained five independent ratings from Turkers satisfying the above qualifications. The rating</context>
</contexts>
<marker>Callison-Burch, Dredze, 2010</marker>
<rawString>Chris Callison-Burch and Mark Dredze. 2010. Creating speech and language data with amazon’s mechanical turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 1–12. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Castillo</author>
<author>Marcelo Mendoza</author>
<author>Barbara Poblete</author>
</authors>
<title>Information credibility on twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th international conference on World wide web,</booktitle>
<pages>675--684</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="17194" citStr="Castillo et al., 2011" startWordPosition="2773" endWordPosition="2776"> area of computational social science focuses on understanding credibility cues on Twitter. Such studies have found that users express concern over the credibility of tweets belonging to certain topics (politics, news, emergency). By manipulating several features of a tweet, Morris et al. (2012) found that in addition to content, users often use additional markers while assessing the tweet credibility, such as the user name of the source. The search for reliable signals of information credibility in social media has led to the construction of automatic classifiers to identify credible tweets (Castillo et al., 2011). However, this prior work has not explored the linguistic basis of factuality judgments, which we show to depend on framing devices such as cue words. 6 Conclusion Perceptions of the factuality of quoted content are influenced by the cue words used to introduce them, while extra-linguistic factors, such as the source and the author, did not appear to be relevant in our experiments. This result is obtained from real tweets written by journalists; a natural counterpart study would be to experimentally manipulate this framing to see if the same perceptions apply. Another future direction would b</context>
</contexts>
<marker>Castillo, Mendoza, Poblete, 2011</marker>
<rawString>Carlos Castillo, Marcelo Mendoza, and Barbara Poblete. 2011. Information credibility on twitter. In Proceedings of the 20th international conference on World wide web, pages 675–684. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>6</volume>
<pages>449--454</pages>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill MacCartney, Christopher D Manning, et al. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC, volume 6, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie C de Marneffe</author>
<author>Christopher D Manning</author>
<author>Christopher Potts</author>
</authors>
<title>Did it happen? the pragmatic complexity of veridicality assessment.</title>
<date>2012</date>
<journal>Comput. Linguist.,</journal>
<volume>38</volume>
<issue>2</issue>
<marker>de Marneffe, Manning, Potts, 2012</marker>
<rawString>Marie C. de Marneffe, Christopher D. Manning, and Christopher Potts. 2012. Did it happen? the pragmatic complexity of veridicality assessment. Comput. Linguist., 38(2):301–333, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Greene</author>
<author>Philip Resnik</author>
</authors>
<title>More than words: Syntactic packaging and implicit sentiment.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>503--511</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="1694" citStr="Greene and Resnik, 2009" startWordPosition="255" endWordPosition="258">ity. 1 Introduction Contemporary journalism is increasingly conducted through social media services like Twitter (Lotan et al., 2011; Hermida et al., 2012). As events unfold, journalists and political commentators use quotes — often indirect — to convey potentially uncertain information and claims from their sources and informants, e.g., Figure 1: Indirect quotations in Twitter A key pragmatic goal of such messages is to convey the provenance and uncertainty of the quoted content. In some cases, the author may also introduce their own perspective (Lin et al., 2006) through the use of framing (Greene and Resnik, 2009). For instance, consider the use of the word claims in Figure 1, which conveys the author’s doubt about the indirectly quoted content. Detecting and reasoning about the certainty of propositional content has been identified as a key task for information extraction, and is now supported by the FactBank corpus of annotations for newstext (Saur´ı and Pustejovsky, 2009). However, less is known about this phenomenon in social media — a domain whose endemic uncertainty makes proper treatment of factuality even more crucial (Morris et al., 2012). Successful automation of factuality judgments could he</context>
</contexts>
<marker>Greene, Resnik, 2009</marker>
<rawString>Stephan Greene and Philip Resnik. 2009. More than words: Syntactic packaging and implicit sentiment. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 503–511, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred Hermida</author>
<author>Seth C Lewis</author>
<author>Rodrigo Zamith</author>
</authors>
<title>Sourcing the arab spring: A case study of andy carvins sources during the tunisian and egyptian revolutions.</title>
<date>2012</date>
<booktitle>In international symposium on online journalism,</booktitle>
<pages>20--21</pages>
<location>Austin, TX,</location>
<contexts>
<context position="1225" citStr="Hermida et al., 2012" startWordPosition="180" endWordPosition="183">ons of the perceived certainty of the quoted statements. We then compare the ability of linguistic and extra-linguistic features to predict readers’ assessment of the certainty of quoted content. We see that readers are indeed influenced by such framing devices — and we find no evidence that they consider other factors, such as the source, journalist, or the content itself. In addition, we examine the impact of specific framing devices on perceptions of credibility. 1 Introduction Contemporary journalism is increasingly conducted through social media services like Twitter (Lotan et al., 2011; Hermida et al., 2012). As events unfold, journalists and political commentators use quotes — often indirect — to convey potentially uncertain information and claims from their sources and informants, e.g., Figure 1: Indirect quotations in Twitter A key pragmatic goal of such messages is to convey the provenance and uncertainty of the quoted content. In some cases, the author may also introduce their own perspective (Lin et al., 2006) through the use of framing (Greene and Resnik, 2009). For instance, consider the use of the word claims in Figure 1, which conveys the author’s doubt about the indirectly quoted conte</context>
</contexts>
<marker>Hermida, Lewis, Zamith, 2012</marker>
<rawString>Alfred Hermida, Seth C Lewis, and Rodrigo Zamith. 2012. Sourcing the arab spring: A case study of andy carvins sources during the tunisian and egyptian revolutions. In international symposium on online journalism, Austin, TX, April, pages 20–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jure Leskovec</author>
<author>Lars Backstrom</author>
<author>Jon Kleinberg</author>
</authors>
<title>Meme-tracking and the dynamics of the news cycle.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>497--506</pages>
<publisher>ACM.</publisher>
<marker>Leskovec, Backstrom, Kleinberg, 2009</marker>
<rawString>Jure Leskovec, Lars Backstrom, and Jon Kleinberg. 2009. Meme-tracking and the dynamics of the news cycle. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 497–506. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Hao Lin</author>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Alexander Hauptmann</author>
</authors>
<title>Which side are you on?: Identifying perspectives at the document and sentence levels.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06,</booktitle>
<pages>109--116</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1641" citStr="Lin et al., 2006" startWordPosition="246" endWordPosition="249">fic framing devices on perceptions of credibility. 1 Introduction Contemporary journalism is increasingly conducted through social media services like Twitter (Lotan et al., 2011; Hermida et al., 2012). As events unfold, journalists and political commentators use quotes — often indirect — to convey potentially uncertain information and claims from their sources and informants, e.g., Figure 1: Indirect quotations in Twitter A key pragmatic goal of such messages is to convey the provenance and uncertainty of the quoted content. In some cases, the author may also introduce their own perspective (Lin et al., 2006) through the use of framing (Greene and Resnik, 2009). For instance, consider the use of the word claims in Figure 1, which conveys the author’s doubt about the indirectly quoted content. Detecting and reasoning about the certainty of propositional content has been identified as a key task for information extraction, and is now supported by the FactBank corpus of annotations for newstext (Saur´ı and Pustejovsky, 2009). However, less is known about this phenomenon in social media — a domain whose endemic uncertainty makes proper treatment of factuality even more crucial (Morris et al., 2012). S</context>
</contexts>
<marker>Lin, Wilson, Wiebe, Hauptmann, 2006</marker>
<rawString>Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and Alexander Hauptmann. 2006. Which side are you on?: Identifying perspectives at the document and sentence levels. In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06, pages 109–116, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gilad Lotan</author>
<author>Erhardt Graeff</author>
<author>Mike Ananny</author>
<author>Devin Gaffney</author>
<author>Ian Pearce</author>
</authors>
<title>The arab spring— the revolutions were tweeted: Information flows during the 2011 tunisian and egyptian revolutions.</title>
<date>2011</date>
<journal>International Journal of Communication,</journal>
<volume>5</volume>
<contexts>
<context position="1202" citStr="Lotan et al., 2011" startWordPosition="176" endWordPosition="179"> and obtain annotations of the perceived certainty of the quoted statements. We then compare the ability of linguistic and extra-linguistic features to predict readers’ assessment of the certainty of quoted content. We see that readers are indeed influenced by such framing devices — and we find no evidence that they consider other factors, such as the source, journalist, or the content itself. In addition, we examine the impact of specific framing devices on perceptions of credibility. 1 Introduction Contemporary journalism is increasingly conducted through social media services like Twitter (Lotan et al., 2011; Hermida et al., 2012). As events unfold, journalists and political commentators use quotes — often indirect — to convey potentially uncertain information and claims from their sources and informants, e.g., Figure 1: Indirect quotations in Twitter A key pragmatic goal of such messages is to convey the provenance and uncertainty of the quoted content. In some cases, the author may also introduce their own perspective (Lin et al., 2006) through the use of framing (Greene and Resnik, 2009). For instance, consider the use of the word claims in Figure 1, which conveys the author’s doubt about the </context>
</contexts>
<marker>Lotan, Graeff, Ananny, Gaffney, Pearce, 2011</marker>
<rawString>Gilad Lotan, Erhardt Graeff, Mike Ananny, Devin Gaffney, Ian Pearce, et al. 2011. The arab spring— the revolutions were tweeted: Information flows during the 2011 tunisian and egyptian revolutions. International Journal of Communication, 5:31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meredith Ringel Morris</author>
<author>Scott Counts</author>
<author>Asta Roseway</author>
<author>Aaron Hoff</author>
<author>Julia Schwarz</author>
</authors>
<title>Tweeting is believing?: understanding microblog credibility perceptions.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACM 2012 conference on Computer Supported Cooperative Work,</booktitle>
<pages>441--450</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2238" citStr="Morris et al., 2012" startWordPosition="341" endWordPosition="344">tive (Lin et al., 2006) through the use of framing (Greene and Resnik, 2009). For instance, consider the use of the word claims in Figure 1, which conveys the author’s doubt about the indirectly quoted content. Detecting and reasoning about the certainty of propositional content has been identified as a key task for information extraction, and is now supported by the FactBank corpus of annotations for newstext (Saur´ı and Pustejovsky, 2009). However, less is known about this phenomenon in social media — a domain whose endemic uncertainty makes proper treatment of factuality even more crucial (Morris et al., 2012). Successful automation of factuality judgments could help to detect online rumors (Qazvinian et al., 2011), and might enable new applications, such as the computation of reliability ratings for ongoing stories. This paper investigates how linguistic resources and extra-linguistic factors affect perceptions of the certainty of quoted information in Twitter. We present a new dataset of Twitter messages that use FactBank predicates (e.g., claim, say, insist) to scope the claims of named entity sources. This dataset was annotated by Mechanical Turk workers who gave ratings for the factuality of t</context>
<context position="16868" citStr="Morris et al. (2012)" startWordPosition="2719" endWordPosition="2722">We are unaware of prior work comparing the contribution of linguistic and extra-linguistic predictors (e.g., source and journalist features) for factuality ratings. This prior work also does not measure the impact of individual cues and cue classes on assessment of factuality. Credibility in social media Recent work in the area of computational social science focuses on understanding credibility cues on Twitter. Such studies have found that users express concern over the credibility of tweets belonging to certain topics (politics, news, emergency). By manipulating several features of a tweet, Morris et al. (2012) found that in addition to content, users often use additional markers while assessing the tweet credibility, such as the user name of the source. The search for reliable signals of information credibility in social media has led to the construction of automatic classifiers to identify credible tweets (Castillo et al., 2011). However, this prior work has not explored the linguistic basis of factuality judgments, which we show to depend on framing devices such as cue words. 6 Conclusion Perceptions of the factuality of quoted content are influenced by the cue words used to introduce them, while</context>
</contexts>
<marker>Morris, Counts, Roseway, Hoff, Schwarz, 2012</marker>
<rawString>Meredith Ringel Morris, Scott Counts, Asta Roseway, Aaron Hoff, and Julia Schwarz. 2012. Tweeting is believing?: understanding microblog credibility perceptions. In Proceedings of the ACM 2012 conference on Computer Supported Cooperative Work, pages 441–450. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim O’Keefe</author>
<author>Silvia Pareti</author>
<author>James R Curran</author>
<author>Irena Koprinska</author>
<author>Matthew Honnibal</author>
</authors>
<title>A sequence labelling approach to quote attribution.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>790--799</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>O’Keefe, Pareti, Curran, Koprinska, Honnibal, 2012</marker>
<rawString>Tim O’Keefe, Silvia Pareti, James R Curran, Irena Koprinska, and Matthew Honnibal. 2012. A sequence labelling approach to quote attribution. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 790– 799. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan OConnor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>380--390</pages>
<contexts>
<context position="5768" citStr="Owoputi et al., 2013" startWordPosition="891" endWordPosition="894">st, assert, claim, maintain, explain, deny Knowledge learn, admit, discover, forget, forgot Belief think, thought, predict, suggest, guess, believe Doubt doubt, wonder, ask, hope Perception sense,hear,feel Table 1: Lemmas of source-introducing predicates (cues) and groups (Saur´ı, 2008). but with reference also to Saur´ı’s (2008) dissertation for cues that are common in Twitter. The Porter Stemmer is applied to match inflections, e.g. denies/denied; for irregular cases not handled by the Porter Stemmer (e.g., forget/forgot), we include both forms. We use the CMU Twitter Part-of-Speech Tagger (Owoputi et al., 2013) to select only instances in the verb sense. Figure 2 shows the distribution of the cues and Figure 3 shows the distribution of the cue groups. For cues that appear in multiple groups, we chose the most common group. • We run the Stanford Dependency parser to obtain labeled dependencies (De Marneffe et al., 2006), requiring that the cue has outgoing edges of the type NSUBJ (noun subject) and CCOMP (clausal complement). The subtree headed by the modifier of the CCOMP relation is considered the claim; the subtree headed by the modifier of the NSUBJ relation is considered the source. See Figure 4</context>
</contexts>
<marker>Owoputi, OConnor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan OConnor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of NAACL-HLT, pages 380–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silvia Pareti</author>
</authors>
<title>A database of attribution relations.</title>
<date>2012</date>
<booktitle>In LREC,</booktitle>
<pages>3213--3217</pages>
<contexts>
<context position="4523" citStr="Pareti, 2012" startWordPosition="701" endWordPosition="702"> dataset 2 Text data We gathered a dataset of Twitter messages from 103 professional journalists and bloggers who work in the field of American Politics.1 Tweets were gathered using Twitter’s streaming API, extracting the complete permissible timeline up to February 23, 2014. A total of 959,754 tweets were gathered, and most were written in early 2014. Our interest in this text is specifically in quoted content — including “indirect” quotes, which may include paraphrased quotations, as in the examples in Figure 1. While labeled datasets for such quotes have been created (O’Keefe et al., 2012; Pareti, 2012), these are not freely available at present. In any case, the relevance of these datasets to Twitter text is currently unproven. Therefore, rather than train a supervised model to detect quotations, we apply a simple dependency-based heuristic. • We focus on tweets that contain any member of a list of source-introducing predicates (we borrow the terminology of Pareti (2012) and call this the CUE). Our complete list — shown in Table 1 — was selected mainly from the examples presented by Saur´ı and Pustejovsky (2012), 1We used the website http://muckrack.com. Report say, report, tell, told, obse</context>
</contexts>
<marker>Pareti, 2012</marker>
<rawString>Silvia Pareti. 2012. A database of attribution relations. In LREC, pages 3213–3217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vahed Qazvinian</author>
<author>Emily Rosengren</author>
<author>Dragomir R Radev</author>
<author>Qiaozhu Mei</author>
</authors>
<title>Rumor has it: Identifying misinformation in microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1589--1599</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2345" citStr="Qazvinian et al., 2011" startWordPosition="357" endWordPosition="360"> use of the word claims in Figure 1, which conveys the author’s doubt about the indirectly quoted content. Detecting and reasoning about the certainty of propositional content has been identified as a key task for information extraction, and is now supported by the FactBank corpus of annotations for newstext (Saur´ı and Pustejovsky, 2009). However, less is known about this phenomenon in social media — a domain whose endemic uncertainty makes proper treatment of factuality even more crucial (Morris et al., 2012). Successful automation of factuality judgments could help to detect online rumors (Qazvinian et al., 2011), and might enable new applications, such as the computation of reliability ratings for ongoing stories. This paper investigates how linguistic resources and extra-linguistic factors affect perceptions of the certainty of quoted information in Twitter. We present a new dataset of Twitter messages that use FactBank predicates (e.g., claim, say, insist) to scope the claims of named entity sources. This dataset was annotated by Mechanical Turk workers who gave ratings for the factuality of the scoped claims in each Twitter message. This enables us to build a predictive model of the factuality ann</context>
</contexts>
<marker>Qazvinian, Rosengren, Radev, Mei, 2011</marker>
<rawString>Vahed Qazvinian, Emily Rosengren, Dragomir R Radev, and Qiaozhu Mei. 2011. Rumor has it: Identifying misinformation in microblogs. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1589–1599. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Saur´ı</author>
<author>James Pustejovsky</author>
</authors>
<title>Factbank: A corpus annotated with event factuality.</title>
<date>2009</date>
<journal>Language Resources and Evaluation,</journal>
<volume>43</volume>
<issue>3</issue>
<marker>Saur´ı, Pustejovsky, 2009</marker>
<rawString>Roser Saur´ı and James Pustejovsky. 2009. Factbank: A corpus annotated with event factuality. Language Resources and Evaluation, 43(3):227–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Sauriand James Pustejovsky</author>
</authors>
<title>Are you sure that this happened? assessing the factuality degree of events in text.</title>
<date>2012</date>
<journal>Comput. Linguist.,</journal>
<volume>38</volume>
<issue>2</issue>
<pages>299</pages>
<contexts>
<context position="5043" citStr="Pustejovsky (2012)" startWordPosition="788" endWordPosition="789">e 1. While labeled datasets for such quotes have been created (O’Keefe et al., 2012; Pareti, 2012), these are not freely available at present. In any case, the relevance of these datasets to Twitter text is currently unproven. Therefore, rather than train a supervised model to detect quotations, we apply a simple dependency-based heuristic. • We focus on tweets that contain any member of a list of source-introducing predicates (we borrow the terminology of Pareti (2012) and call this the CUE). Our complete list — shown in Table 1 — was selected mainly from the examples presented by Saur´ı and Pustejovsky (2012), 1We used the website http://muckrack.com. Report say, report, tell, told, observe, state, accord, insist, assert, claim, maintain, explain, deny Knowledge learn, admit, discover, forget, forgot Belief think, thought, predict, suggest, guess, believe Doubt doubt, wonder, ask, hope Perception sense,hear,feel Table 1: Lemmas of source-introducing predicates (cues) and groups (Saur´ı, 2008). but with reference also to Saur´ı’s (2008) dissertation for cues that are common in Twitter. The Porter Stemmer is applied to match inflections, e.g. denies/denied; for irregular cases not handled by the Por</context>
<context position="15682" citStr="Pustejovsky (2012)" startWordPosition="2538" endWordPosition="2540">li trails McAuliffe by 24 points, according to a new poll. In contrast, insist, claim, and deny imply that there is uncertainty about the quoted statement, e.g., Christie insists that Fort Lee Mayor was never on my radar. In this case, the fact that the predicate indicates a report is not enough to determine the framing: different sorts of reports carry radically different perceptions of factuality. 5 Related work Factuality and Veridicality The creation of FactBank (Sauriand Pustejovsky, 2009) has enabled recent work on the factuality (or “veridicality”) of event mentions in text. Saur´ı and Pustejovsky (2012) propose a two-dimensional factuality annotation scheme, including polarity and certainty; they then build a classifier to predict annotations of factuality from statements in FactBank. Their work on source-introducing predicates provides part of the foundation for this research, which focuses on quoted statements in social media text. de Marneffe et al. (2012) conduct an empirical evaluation of FactBank ratings from Mechanical Turk workers, finding a high degree of disagreement between raters. They also construct a statistical model to predict these ratings. We are unaware of prior work compa</context>
</contexts>
<marker>Pustejovsky, 2012</marker>
<rawString>Roser Sauriand James Pustejovsky. 2012. Are you sure that this happened? assessing the factuality degree of events in text. Comput. Linguist., 38(2):261– 299, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Sauri</author>
</authors>
<title>A Factuality Profiler for Eventualities in Text.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Brandeis University.</institution>
<marker>Sauri, 2008</marker>
<rawString>Roser Sauri. 2008. A Factuality Profiler for Eventualities in Text. Ph.D. thesis, Brandeis University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>254--263</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast—but is it good?: Evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 254–263, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>