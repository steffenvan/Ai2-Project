<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000195">
<title confidence="0.995952">
Web-Scale Features for Full-Scale Parsing
</title>
<author confidence="0.992505">
Mohit Bansal and Dan Klein
</author>
<affiliation confidence="0.9979">
Computer Science Division
University of California, Berkeley
</affiliation>
<email confidence="0.999378">
{mbansal,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.994807" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999475833333333">
Counts from large corpora (like the web) can
be powerful syntactic cues. Past work has
used web counts to help resolve isolated am-
biguities, such as binary noun-verb PP attach-
ments and noun compound bracketings. In
this work, we first present a method for gener-
ating web count features that address the full
range of syntactic attachments. These fea-
tures encode both surface evidence of lexi-
cal affinities as well as paraphrase-based cues
to syntactic structure. We then integrate our
features into full-scale dependency and con-
stituent parsers. We show relative error re-
ductions of 7.0% over the second-order depen-
dency parser of McDonald and Pereira (2006),
9.2% over the constituent parser of Petrov et
al. (2006), and 3.4% over a non-local con-
stituent reranker.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956625">
Current state-of-the art syntactic parsers have
achieved accuracies in the range of 90% F1 on the
Penn Treebank, but a range of errors remain. From
a dependency viewpoint, structural errors can be
cast as incorrect attachments, even for constituent
(phrase-structure) parsers. For example, in the
Berkeley parser (Petrov et al., 2006), about 20%
of the errors are prepositional phrase attachment er-
rors as in Figure 1, where a preposition-headed (IN)
phrase was assigned an incorrect parent in the im-
plied dependency tree. Here, the Berkeley parser
(solid blue edges) incorrectly attaches from debt to
the noun phrase $ 30 billion whereas the correct at-
tachment (dashed gold edges) is to the verb rais-
ing. However, there are a range of error types, as
shown in Figure 2. Here, (a) is a non-canonical PP
</bodyText>
<sectionHeader confidence="0.632555" genericHeader="introduction">
VP
</sectionHeader>
<figureCaption confidence="0.99741775">
Figure 1: A PP attachment error in the parse output of the
Berkeley parser (on Penn Treebank). Guess edges are in solid
blue, gold edges are in dashed gold and edges common in guess
and gold parses are in black.
</figureCaption>
<bodyText confidence="0.999862565217391">
attachment ambiguity where by yesterday afternoon
should attach to had already, (b) is an NP-internal
ambiguity where half a should attach to dozen and
not to newspapers, and (c) is an adverb attachment
ambiguity, where just should modify fine and not the
verb ‚Äôs.
Resolving many of these errors requires informa-
tion that is simply not present in the approximately
1M words on which the parser was trained. One
way to access more information is to exploit sur-
face counts from large corpora like the web (Volk,
2001; Lapata and Keller, 2004). For example, the
phrase raising from is much more frequent on the
Web than $ x billion from. While this ‚Äòaffinity‚Äô is
only a surface correlation, Volk (2001) showed that
comparing such counts can often correctly resolve
tricky PP attachments. This basic idea has led to a
good deal of successful work on disambiguating iso-
lated, binary PP attachments. For example, Nakov
and Hearst (2005b) showed that looking for para-
phrase counts can further improve PP resolution.
In this case, the existence of reworded phrases like
raising it from on the Web also imply a verbal at-
</bodyText>
<figure confidence="0.994012714285714">
$ 30 billion
from debt ...
VBG
NP
... raising
NP
PP
</figure>
<page confidence="0.98687">
693
</page>
<note confidence="0.964283666666667">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 693‚Äì702,
Portland, Oregon, June 19-24, 2011. cÔøΩ2011 Association for Computational Linguistics
...Lehman Hutton Inc. by yesterday afternoon
</note>
<figureCaption confidence="0.995361">
Figure 2: Different kinds of attachment errors in the parse output of the Berkeley parser (on Penn Treebank). Guess edges are in
solid blue, gold edges are in dashed gold and edges common in guess and gold parses are in black.
</figureCaption>
<figure confidence="0.96489805">
PDT
... ·øΩs
a
dozen newspapers
(a) S (b) NP (c)
VP
NP VP QP VBZ
NP PP
ADJP
PDT
DT
PDT
had already ...
... half
ADVP
RB
just
ADJP
JJ
fine
</figure>
<bodyText confidence="0.998429125">
tachment. Still other work has exploited Web counts
for other isolated ambiguities, such as NP coordina-
tion (Nakov and Hearst, 2005b) and noun-sequence
bracketing (Nakov and Hearst, 2005a; Pitler et al.,
2010). For example, in (b), half dozen is more fre-
quent than half newspapers.
In this paper, we show how to apply these ideas
to all attachments in full-scale parsing. Doing so
requires three main issues to be addressed. First,
we show how features can be generated for arbitrary
head-argument configurations. Affinity features are
relatively straightforward, but paraphrase features,
which have been hand-developed in the past, are
more complex. Second, we integrate our features
into full-scale parsing systems. For dependency
parsing, we augment the features in the second-order
parser of McDonald and Pereira (2006). For con-
stituent parsing, we rerank the output of the Berke-
ley parser (Petrov et al., 2006). Third, past systems
have usually gotten their counts from web search
APIs, which does not scale to quadratically-many
attachments in each sentence. Instead, we consider
how to efficiently mine the Google n-grams corpus.
Given the success of Web counts for isolated am-
biguities, there is relatively little previous research
in this direction. The most similar work is Pitler
et al. (2010), which use Web-scale n-gram counts
for multi-way noun bracketing decisions, though
that work considers only sequences of nouns and
uses only affinity-based web features. Yates et al.
(2006) use Web counts to filter out certain ‚Äòseman-
tically bad‚Äô parses from extraction candidate sets
but are not concerned with distinguishing amongst
top parses. In an important contrast, Koo et al.
(2008) smooth the sparseness of lexical features in a
discriminative dependency parser by using cluster-
based word-senses as intermediate abstractions in
addition to POS tags (also see Finkel et al. (2008)).
Their work also gives a way to tap into corpora be-
yond the training data, through cluster membership
rather than explicit corpus counts and paraphrases.
This work uses a large web-scale corpus (Google
n-grams) to compute features for the full parsing
task. To show end-to-end effectiveness, we incor-
porate our features into state-of-the-art dependency
and constituent parsers. For the dependency case,
we can integrate them into the dynamic program-
ming of a base parser; we use the discriminatively-
trained MST dependency parser (McDonald et al.,
2005; McDonald and Pereira, 2006). Our first-order
web-features give 7.0% relative error reduction over
the second-order dependency baseline of McDon-
ald and Pereira (2006). For constituent parsing, we
use a reranking framework (Charniak and Johnson,
2005; Collins and Koo, 2005; Collins, 2000) and
show 9.2% relative error reduction over the Berke-
ley parser baseline. In the same framework, we
also achieve 3.4% error reduction over the non-local
syntactic features used in Huang (2008). Our web-
scale features reduce errors for a range of attachment
types. Finally, we present an analysis of influential
features. We not only reproduce features suggested
in previous work but also discover a range of new
ones.
</bodyText>
<sectionHeader confidence="0.991598" genericHeader="method">
2 Web-count Features
</sectionHeader>
<bodyText confidence="0.9980972">
Structural errors in the output of state-of-the-art
parsers, constituent or dependency, can be viewed
as attachment errors, examples of which are Figure 1
and Figure 2.1 One way to address attachment errors
is through features which factor over head-argument
</bodyText>
<footnote confidence="0.999302">
1For constituent parsers, there can be minor tree variations
which can result in the same set of induced dependencies, but
these are rare in comparison.
</footnote>
<page confidence="0.991897">
694
</page>
<figure confidence="0.737672">
ùúô(head arg)
</figure>
<figureCaption confidence="0.998926">
Figure 3: Features factored over head-argument pairs.
</figureCaption>
<bodyText confidence="0.999931333333333">
pairs, as is standard in the dependency parsing liter-
ature (see Figure 3). Here, we discuss which web-
count based features 0(h, a) should fire over a given
head-argument pair (we consider the words h and
a to be indexed, and so features can be sensitive to
their order and distance, as is also standard).
</bodyText>
<subsectionHeader confidence="0.99207">
2.1 Affinity Features
</subsectionHeader>
<bodyText confidence="0.999869745098039">
Affinity statistics, such as lexical co-occurrence
counts from large corpora, have been used previ-
ously for resolving individual attachments at least as
far back as Lauer (1995) for noun-compound brack-
eting, and later for PP attachment (Volk, 2001; La-
pata and Keller, 2004) and coordination ambigu-
ity (Nakov and Hearst, 2005b). The approach of
Lauer (1995), for example, would be to take an am-
biguous noun sequence like hydrogen ion exchange
and compare the various counts (or associated con-
ditional probabilities) of n-grams like hydrogen ion
and hydrogen exchange. The attachment with the
greater score is chosen. More recently, Pitler et al.
(2010) use web-scale n-grams to compute similar
association statistics for longer sequences of nouns.
Our affinity features closely follow this basic idea
of association statistics. However, because a real
parser will not have access to gold-standard knowl-
edge of the competing attachment sites (see Atterer
and Schutze (2007)‚Äôs criticism of previous work),
we must instead compute features for all possible
head-argument pairs from our web corpus. More-
over, when there are only two competing attachment
options, one can do things like directly compare two
count-based heuristics and choose the larger. Inte-
gration into a parser requires features to be functions
of single attachments, not pairwise comparisons be-
tween alternatives. A learning algorithm can then
weight features so that they compare appropriately
across parses.
We employ a collection of affinity features of
varying specificity. The basic feature is the core ad-
jacency count feature ADJ, which fires for all (h, a)
pairs. What is specific to a particular (h, a) is the
value of the feature, not its identity. For example, in
a naive approach, the value of the ADJ feature might
be the count of the query issued to the web-corpus ‚Äì
the 2-gram q = ha or q = ah depending on the or-
der of h and a in the sentence. However, it turns out
that there are several problems with this approach.
First, rather than a single all-purpose feature like
ADJ, the utility of such query counts will vary ac-
cording to aspects like the parts-of-speech of h and
a (because a high adjacency count is not equally in-
formative for all kinds of attachments). Hence, we
add more refined affinity features that are specific
to each pair of POS tags, i.e. ADJ n POS(h) n
POS(a). The values of these POS-specific features,
however, are still derived from the same queries as
before. Second, using real-valued features did not
work as well as binning the query-counts (we used
</bodyText>
<equation confidence="0.539512">
b = floor(log,(count)/5) * 5) and then firing in-
</equation>
<bodyText confidence="0.999406357142857">
dicator features ADJ n POS(h) n POS(a) n b for
values of b defined by the query count. Adding still
more complex features, we conjoin to the preceding
features the order of the words h and a as they occur
in the sentence, and the (binned) distance between
them. For features which mark distances, wildcards
(*) are used in the query q = h * a, where the num-
ber of wildcards allowed in the query is proportional
to the binned distance between h and a in the sen-
tence. Finally, we also include unigram variants of
the above features, which are sensitive to only one of
the head or argument. For all features used, we add
cumulative variants where indicators are fired for all
count bins bÔøΩ up to query count bin b.
</bodyText>
<subsectionHeader confidence="0.999538">
2.2 Paraphrase Features
</subsectionHeader>
<bodyText confidence="0.999953222222222">
In addition to measuring counts of the words present
in the sentence, there exist clever ways in which
paraphrases and other accidental indicators can help
resolve specific ambiguities, some of which are dis-
cussed in Nakov and Hearst (2005a), Nakov and
Hearst (2005b). For example, finding attestations of
eat: spaghetti with sauce suggests a nominal attach-
ment in Jean ate spaghetti with sauce. As another
example, one clue that the example in Figure 1 is
</bodyText>
<figure confidence="0.689244">
ùùì(raising from) ùùì($ from)
raising $
from debt
</figure>
<page confidence="0.98253">
695
</page>
<bodyText confidence="0.999865383333334">
a verbal attachment is that the proform paraphrase
raising it from is commonly attested. Similarly, the
attestation of be noun prep suggests nominal attach-
ment.
These paraphrase features hint at the correct at-
tachment decision by looking for web n-grams
with special contexts that reveal syntax superficially.
Again, while effective in their isolated disambigua-
tion tasks, past work has been limited by both the
range of attachments considered and the need to in-
tuit these special contexts. For instance, frequency
of the pattern The noun prep suggests noun attach-
ment and of the pattern verb adverb prep suggests
verb attachment for the preposition in the phrase
verb noun prep, but these features were not in the
manually brainstormed list.
In this work, we automatically generate a large
number of paraphrase-style features for arbitrary at-
tachment ambiguities. To induce our list of fea-
tures, we first mine useful context words. We take
each (correct) training dependency relation (h, a)
and consider web n-grams of the form cha, hca,
and hac. Aggregating over all h and a (of a given
POS pair), we determine which context words c are
most frequent in each position. For example, for h =
raising and a =from (see Figure 1), we look at web
n-grams of the form raising c from and see that one
of the most frequent values of c on the web turns out
to be the word it.
Once we have collected context words (for each
position p in {BEFORE, MIDDLE, AFTER}), we
turn each context word c into a collection of features
of the form PARA n POS(h) n POS(a) n c n p n
dir, where dir is the linear order of the attachment
in the sentence. Note that h and a are head and ar-
gument words and so actually occur in the sentence,
but c is a context word that generally does not. For
such features, the queries that determine their val-
ues are then of the form cha, hca, and so on. Con-
tinuing the previous example, if the test set has a
possible attachment of two words like h = lower-
ing and a = with, we will fire a feature PARA n
VBG n IN n it n MIDDLE n ‚Äî* with value (indi-
cator bins) set according to the results of the query
lowering it with. The idea is that if frequent oc-
currences of raising it from indicated a correct at-
tachment between raising and from, frequent occur-
rences of lowering it with will indicate the correct-
ness of an attachment between lowering and with.
Finally, to handle the cases where no induced con-
text word is helpful, we also construct abstracted
versions of these paraphrase features where the con-
text words c are collapsed to their parts-of-speech
POS(c), obtained using a unigram-tagger trained on
the parser training set. As discussed in Section 5, the
top features learned by our learning algorithm dupli-
cate the hand-crafted configurations used in previous
work (Nakov and Hearst, 2005b) but also add nu-
merous others, and, of course, apply to many more
attachment types.
</bodyText>
<sectionHeader confidence="0.833231" genericHeader="method">
3 Working with Web n-Grams
</sectionHeader>
<bodyText confidence="0.9994569375">
Previous approaches have generally used search en-
gines to collect count statistics (Lapata and Keller,
2004; Nakov and Hearst, 2005b; Nakov and Hearst,
2008). Lapata and Keller (2004) uses the number
of page hits as the web-count of the queried n-
gram (which is problematic according to Kilgarriff
(2007)). Nakov and Hearst (2008) post-processes
the first 1000 result snippets. One challenge with
this approach is that an external search API is now
embedded into the parser, raising issues of both
speed and daily query limits, especially if all pos-
sible attachments trigger queries. Such methods
also create a dependence on the quality and post-
processing of the search results, limitations of the
query process (for instance, search engines can ig-
nore punctuation (Nakov and Hearst, 2005b)).
Rather than working through a search API (or
scraper), we use an offline web corpus ‚Äì the Google
n-gram corpus (Brants and Franz, 2006) ‚Äì which
contains English n-grams (n = 1 to 5) and their ob-
served frequency counts, generated from nearly 1
trillion word tokens and 95 billion sentences. This
corpus allows us to efficiently access huge amounts
of web-derived information in a compressed way,
though in the process it limits us to local queries.
In particular, we only use counts of n-grams of the
form x * y where the gap length is &lt; 3.
Our system requires the counts from a large col-
lection of these n-gram queries (around 4.5 million).
The most basic queries are counts of head-argument
pairs in contiguous h a and gapped h * a configura-
tions.2 Here, we describe how we process queries
</bodyText>
<footnote confidence="0.919569">
2Paraphrase features give situations where we query * h a
</footnote>
<page confidence="0.971013">
696
</page>
<equation confidence="0.69492575">
ùííùüè= ùíòùüè
ùë§1 . . . ùë§ùëõ
ùííùüè ùííùüê
ùííùüê= ùíòùíè
SCAN
ùííùüè ‚àó ùííùüê
{ùëû1} hash
ùííùüè‚àó‚àó ùííùüê
</equation>
<bodyText confidence="0.999510804347826">
of the form (q1, q2) with some number of wildcards
in between. We first collect all such queries over
all trees in preprocessing (so a new test set requires
a new query-extraction phase). Next, we exploit a
simple but efficient trie-based hashing algorithm to
efficiently answer all of them in one pass over the
n-grams corpus.
Consider Figure 4, which illustrates the data
structure which holds our queries. We first create
a trie of the queries in the form of a nested hashmap.
The key of the outer hashmap is the first word q1
of the query. The entry for q1 points to an inner
hashmap whose key is the final word q2 of the query
bigram. The values of the inner map is an array of
4 counts, to accumulate each of (q1q2), (q1 * q2),
(q1 * *q2), and (q1 * * * q2), respectively. We use k-
grams to collect counts of (q1...q2) with gap length
= k ‚àí 2, i.e. 2-grams to get count(q1q2), 3-grams to
get count(q1 * q2) and so on.
With this representation of our collection of
queries, we go through the web n-grams (n = 2 to
5) one by one. For an n-gram w1...wn, if the first n-
gram word w1 doesn‚Äôt occur in the outer hashmap,
we move on. If it does match (say q1 = w1), then
we look into the inner map for q1 and check for the
final word wn. If we have a match, we increment the
appropriate query‚Äôs result value.
In similar ways, we also mine the most frequent
words that occur before, in between and after the
head and argument query pairs. For example, to col-
lect mid words, we go through the 3-grams w1w2w3;
if w1 matches q1 in the outer hashmap and w3 oc-
curs in the inner hashmap for q1, then we store w2
and the count of the 3-gram. After the sweep, we
sort the context words in decreasing order of count.
We also collect unigram counts of the head and ar-
gument words by sweeping over the unigrams once.
In this way, our work is linear in the size of the
n-gram corpus, but essentially constant in the num-
ber of queries. Of course, if the number of queries is
expected to be small, such as for a one-off parse of
a single sentence, other solutions might be more ap-
propriate; in our case, a large-batch setting, the num-
ber of queries was such that this formulation was
chosen. Our main experiments (with no paralleliza-
tion) took 115 minutes to sweep over the 3.8 billion
</bodyText>
<figure confidence="0.987318">
ùííùüè ‚àó‚àó‚àó ùííùüê
{ùëû2} hash
counts
Web N-grams Query Count-Trie
</figure>
<figureCaption confidence="0.995399">
Figure 4: Trie-based nested hashmap for collecting ngram web-
counts of queries.
</figureCaption>
<bodyText confidence="0.863354333333333">
n-grams (n = 1 to 5) to compute the answers to 4.5
million queries, much less than the time required to
train the baseline parsers.
</bodyText>
<sectionHeader confidence="0.945645" genericHeader="method">
4 Parsing Experiments
</sectionHeader>
<bodyText confidence="0.9969114">
Our features are designed to be used in full-sentence
parsing rather than for limited decisions about iso-
lated ambiguities. We first integrate our features into
a dependency parser, where the integration is more
natural and pushes all the way into the underlying
dynamic program. We then add them to a constituent
parser in a reranking approach. We also verify that
our features contribute on top of standard reranking
features.3
and h a *; these are handled similarly.
</bodyText>
<subsectionHeader confidence="0.842707">
4.1 Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.999816583333333">
For dependency parsing, we use the
discriminatively-trained MSTParser4, an im-
plementation of first and second order MST parsing
models of McDonald et al. (2005) and McDonald
and Pereira (2006). We use the standard splits of
Penn Treebank into training (sections 2-21), devel-
opment (section 22) and test (section 23). We used
the ‚Äòpennconverter‚Äô5 tool to convert Penn trees from
constituent format to dependency format. Following
Koo et al. (2008), we used the MXPOST tagger
(Ratnaparkhi, 1996) trained on the full training data
to provide part-of-speech tags for the development
</bodyText>
<footnote confidence="0.953329">
3All reported experiments are run on all sentences, i.e. with-
out any length limit.
4http://sourceforge.net/projects/mstparser
5This supersedes ‚ÄòPenn2Malt‚Äô and is available at
</footnote>
<bodyText confidence="0.800446333333333">
http://nlp.cs.lth.se/software/treebank converter. We follow
its recommendation to patch WSJ data with NP bracketing by
Vadas and Curran (2007).
</bodyText>
<page confidence="0.995175">
697
</page>
<table confidence="0.996541333333333">
Order 2 + Web features % Error Redn.
Dev (sec 22) 92.1 92.7 7.6%
Test (sec 23) 91.4 92.0 7.0%
</table>
<tableCaption confidence="0.919449">
Table 1: UAS results for English WSJ dependency parsing. Dev
is WSJ section 22 (all sentences) and Test is WSJ section 23
(all sentences). The order 2 baseline represents McDonald and
Pereira (2006).
</tableCaption>
<bodyText confidence="0.998932071428571">
and the test set, and we used 10-way jackknifing to
generate tags for the training set.
We added our first-order Web-scale features to
the MSTParser system to evaluate improvement over
the results of McDonald and Pereira (2006).6 Ta-
ble 1 shows unlabeled attachments scores (UAS)
for their second-order projective parser and the im-
proved numbers resulting from the addition of our
Web-scale features. Our first-order web-scale fea-
tures show significant improvement even over their
non-local second-order features.7 Additionally, our
web-scale features are at least an order of magnitude
fewer in number than even their first-order base fea-
tures.
</bodyText>
<subsectionHeader confidence="0.995499">
4.2 Constituent Parsing
</subsectionHeader>
<bodyText confidence="0.9999472">
We also evaluate the utility of web-scale features
on top of a state-of-the-art constituent parser ‚Äì the
Berkeley parser (Petrov et al., 2006), an unlexical-
ized phrase-structure parser. Because the underly-
ing parser does not factor along lexical attachments,
we instead adopt the discriminative reranking frame-
work, where we generate the top-k candidates from
the baseline system and then rerank this k-best list
using (generally non-local) features.
Our baseline system is the Berkeley parser, from
which we obtain k-best lists for the development set
(WSJ section 22) and test set (WSJ section 23) using
a grammar trained on all the training data (WSJ sec-
tions 2-21).8 To get k-best lists for the training set,
we use 3-fold jackknifing where we train a grammar
</bodyText>
<footnote confidence="0.998971333333333">
6Their README specifies ‚Äòtraining-k:5 iters:10 loss-
type:nopunc decode-type:proj‚Äô, which we used for all final ex-
periments; we used the faster ‚Äòtraining-k:1 iters:5‚Äô setting for
most development experiments.
7Work such as Smith and Eisner (2008), Martins et al.
(2009), Koo and Collins (2010) has been exploring more non-
local features for dependency parsing. It will be interesting to
see how these features interact with our web features.
8Settings: 6 iterations of split and merge with smoothing.
</footnote>
<table confidence="0.996788666666667">
k = 1 k = 2 k = 10 k = 25 k = 50 k = 100
Dev 90.6 92.3 95.1 95.8 96.2 96.5
Test 90.2 91.8 94.7 95.6 96.1 96.4
</table>
<tableCaption confidence="0.985752333333333">
Table 2: Oracle F1-scores for k-best lists output by Berkeley
parser for English WSJ parsing (Dev is section 22 and Test is
section 23, all lengths).
</tableCaption>
<bodyText confidence="0.999965884615384">
on 2 folds to get parses for the third fold.9 The ora-
cle scores of the k-best lists (for different values of
k) for the development and test sets are shown in Ta-
ble 2. Based on these results, we used 50-best lists
in our experiments. For discriminative learning, we
used the averaged perceptron (Collins, 2002; Huang,
2008).
Our core feature is the log conditional likelihood
of the underlying parser.10 All other features are in-
dicator features. First, we add all the Web-scale fea-
tures as defined above. These features alone achieve
a 9.2% relative error reduction. The affinity and
paraphrase features contribute about two-fifths and
three-fifths of this improvement, respectively. Next,
we rerank with only the features (both local and
non-local) from Huang (2008), a simplified merge
of Charniak and Johnson (2005) and Collins (2000)
(here configurational). These features alone achieve
around the same improvements over the baseline as
our web-scale features, even though they are highly
non-local and extensive. Finally, we rerank with
both our Web-scale features and the configurational
features. When combined, our web-scale features
give a further error reduction of 3.4% over the con-
figurational reranker (and a combined error reduc-
tion of 12.2%). All results are shown in Table 3.11
</bodyText>
<sectionHeader confidence="0.987789" genericHeader="method">
5 Analysis
</sectionHeader>
<bodyText confidence="0.999857">
Table 4 shows error counts and relative reductions
that our web features provide over the 2nd-order
dependency baseline. While we do see substantial
gains for classic PP (IN) attachment cases, we see
equal or greater error reductions for a range of at-
tachment types. Further, Table 5 shows how the to-
</bodyText>
<footnote confidence="0.870842857142857">
9Default: we ran the Berkeley parser in its default ‚Äòfast‚Äô
mode; the output k-best lists are ordered by max-rule-score.
10This is output by the flag -confidence. Note that baseline
results with just this feature are slightly worse than 1-best re-
sults because the k-best lists are generated by max-rule-score.
We report both numbers in Table 3.
11We follow Collins (1999) for head rules.
</footnote>
<page confidence="0.992067">
698
</page>
<table confidence="0.999548125">
Dev (sec 22) Test (sec 23)
F1 EX F1 EX
Parsing Model
Baseline (1-best) 90.6 39.4 90.2 37.3
log p(t|w) 90.4 38.9 89.9 37.3
+ Web features 91.6 42.5 91.1 40.6
+ Configurational features 91.8 43.8 91.1 40.6
+ Web + Configurational 92.1 44.0 91.4 41.4
</table>
<tableCaption confidence="0.937782666666667">
Table 3: Parsing results for reranking 50-best lists of Berkeley
parser (Dev is WSJ section 22 and Test is WSJ section 23, all
lengths).
</tableCaption>
<table confidence="0.9999603125">
Arg Tag # Attach Baseline This Work % ER
NN 5725 5387 5429 12.4
NNP 4043 3780 3804 9.1
IN 4026 3416 3490 12.1
DT 3511 3424 3429 5.8
NNS 2504 2319 2348 15.7
JJ 2472 2310 2329 11.7
CD 1845 1739 1738 -0.9
VBD 1705 1571 1580 6.7
RB 1308 1097 1100 1.4
CC 1000 855 854 -0.7
VB 983 940 945 11.6
TO 868 761 776 14.0
VBN 850 776 786 13.5
VBZ 705 633 629 -5.6
PRP 612 603 606 33.3
</table>
<tableCaption confidence="0.5621345">
Table 4: Error reduction for attachments of various child (argu-
ment) categories. The columns depict the tag, its total attach-
ments as argument, number of correct ones in baseline (Mc-
Donald and Pereira, 2006) and this work, and the relative error
reduction. Results are for dependency parsing on the dev set for
iters:5,training-k:1.
</tableCaption>
<bodyText confidence="0.9997848">
tal errors break down by gold head. For example,
the 12.1% total error reduction for attachments of an
IN argument (which includes PPs as well as comple-
mentized SBARs) includes many errors where the
gold attachments are to both noun and verb heads.
Similarly, for an NN-headed argument, the major
corrections are for attachments to noun and verb
heads, which includes both object-attachment am-
biguities and coordination ambiguities.
We next investigate the features that were given
high weight by our learning algorithm (in the con-
stituent parsing case). We first threshold features
by a minimum training count of 400 to focus on
frequently-firing ones (recall that our features are
not bilexical indicators and so are quite a bit more
</bodyText>
<note confidence="0.528879">
Arg Tag % Error Redn for Various Parent Tags
NN IN: 18, NN: 23, VB: 30, NNP:20, VBN: 33
IN NN: 11, VBD: 11, NNS: 20, VB:18, VBG: 23
NNS IN: 9, VBD: 29, VBP: 21, VB:15, CC: 33
</note>
<tableCaption confidence="0.999903">
Table 5: Error reduction for each type of parent attachment for
a given child in Table 4.
</tableCaption>
<table confidence="0.999888733333333">
POShead POSa,9 Example (head, arg)
RB IN back ‚Üí into
NN IN review ‚Üí of
NN DT The ‚Üê rate
NNP IN Regulation ‚Üí of
VB NN limit ‚Üí access
VBD NN government ‚Üê cleared
NNP NNP Dean ‚Üê Inc
NN TO ability ‚Üí to
JJ IN active ‚Üí for
NNS TO reasons ‚Üí to
IN NN under ‚Üí pressure
NNS IN reports ‚Üí on
NN NNP Warner ‚Üê studio
NNS JJ few ‚Üê plants
</table>
<tableCaption confidence="0.759635">
Table 6: The highest-weight features (thresholded at a count of
400) of the affinity schema. We list only the head and argu-
ment POS and the direction (arrow from head to arg). We omit
features involving punctuation.
</tableCaption>
<bodyText confidence="0.998844047619048">
frequent). We then sort them by descending (signed)
weight.
Table 6 shows which affinity features received the
highest weights, as well as examples of training set
attachments for which the feature fired (for concrete-
ness), suppressing both features involving punctua-
tion and the features‚Äô count and distance bins. With
the standard caveats that interpreting feature weights
in isolation is always to be taken for what it is,
the first feature (RB‚ÜíIN) indicates that high counts
for an adverb occurring adjacent to a preposition
(like back into the spotlight) is a useful indicator
that the adverb actually modifies that preposition.
The second row (NN‚ÜíIN) indicates that whether a
preposition is appropriate to attach to a noun is well
captured by how often that preposition follows that
noun. The fifth row (VB‚ÜíNN) indicates that when
considering an NP as the object of a verb, it is a good
sign if that NP‚Äôs head frequently occurs immediately
following that verb. All of these features essentially
state cases where local surface counts are good indi-
</bodyText>
<page confidence="0.997153">
699
</page>
<table confidence="0.999894411764706">
POShead mid-word POSa,g Example (head, arg)
VBN this IN leaned, from
VB this IN publish, in
VBG him IN using, as
VBG them IN joining, in
VBD directly IN converted, into
VBD held IN was, in
VBN jointly IN offered, by
VBZ it IN passes, in
VBG only IN consisting, of
VBN primarily IN developed, for
VB us IN exempt, from
VBG this IN using, as
VBD more IN looked, like
VB here IN stay, for
VBN themselves IN launched, into
VBG down IN lying, on
</table>
<tableCaption confidence="0.993000666666667">
Table 7: The highest-weight features (thresholded at a count of
400) of the mid-word schema for a verb head and preposition
argument (with head on left of argument).
</tableCaption>
<bodyText confidence="0.996295481481482">
cators of (possibly non-adjacent) attachments.
A subset of paraphrase features, which in the
automatically-extracted case don‚Äôt really correspond
to paraphrases at all, are shown in Table 7. Here
we show features for verbal heads and IN argu-
ments. The mid-words m which rank highly are
those where the occurrence of hma as an n-gram
is a good indicator that a attaches to h (m of course
does not have to actually occur in the sentence). In-
terestingly, the top such features capture exactly the
intuition from Nakov and Hearst (2005b), namely
that if the verb h and the preposition a occur with
a pronoun in between, we have evidence that a at-
taches to h (it certainly can‚Äôt attach to the pronoun).
However, we also see other indicators that the prepo-
sition is selected for by the verb, such as adverbs like
directly.
As another example of known useful features
being learned automatically, Table 8 shows the
previous-context-word paraphrase features for a
noun head and preposition argument (N ‚Äî* IN).
Nakov and Hearst (2005b) suggested that the attes-
tation of be N IN is a good indicator of attachment to
the noun (the IN cannot generally attach to forms of
auxiliaries). One such feature occurs on this top list
‚Äì for the context word have ‚Äì and others occur far-
ther down. We also find their surface marker / punc-
</bodyText>
<table confidence="0.999818571428571">
bfr-word POShead POSa,g Example (head, arg)
second NN IN season, in
The NN IN role, of
strong NN IN background, in
our NNS IN representatives, in
any NNS IN rights, against
A NN IN review, of
: NNS IN Results, in
three NNS IN years, in
In NN IN return, for
no NN IN argument, about
current NN IN head, of
no NNS IN plans, for
public NN IN appearance, at
from NNS IN sales, of
net NN IN revenue, of
, NNS IN names, of
you NN IN leave, in
have NN IN time, for
some NN IN money, for
annual NNS IN reports, on
</table>
<tableCaption confidence="0.981655666666667">
Table 8: The highest-weight features (thresholded at a count of
400) of the before-word schema for a noun head and preposition
argument (with head on left of argument).
</tableCaption>
<bodyText confidence="0.999886260869565">
tuation cues of : and, preceding the noun. However,
we additionally find other cues, most notably that if
the N IN sequence occurs following a capitalized de-
terminer, it tends to indicate a nominal attachment
(in the n-gram, the preposition cannot attach left-
ward to anything else because of the beginning of
the sentence).
In Table 9, we see the top-weight paraphrase fea-
tures that had a conjunction as a middle-word cue.
These features essentially say that if two heads w1
and w2 occur in the direct coordination n-gram w1
and w2, then they are good heads to coordinate (co-
ordination unfortunately looks the same as comple-
mentation or modification to a basic dependency
model). These features are relevant to a range of
coordination ambiguities.
Finally, Table 10 depicts the high-weight, high-
count general paraphrase-cue features for arbitrary
head and argument categories, with those shown
in previous tables suppressed. Again, many inter-
pretable features appear. For example, the top entry
(the JJ NNS) shows that when considering attaching
an adjective a to a noun h, it is a good sign if the
</bodyText>
<page confidence="0.98271">
700
</page>
<note confidence="0.748678181818182">
POShead mid-CC POSa,g Example (head, arg)
NNS and NNS purchases, sales
VB and VB buy, sell
NN and NN president, officer
NN and NNS public, media
VBD and VBD said, added
VBZ and VBZ makes, distributes
JJ and JJ deep, lasting
IN and IN before, during
VBD and RB named, now
VBP and VBP offer, need
</note>
<tableCaption confidence="0.941017">
Table 9: The highest-weight features (thresholded at a count
</tableCaption>
<bodyText confidence="0.965775894736842">
of 400) of the mid-word schema where the mid-word was a
conjunction. For variety, for a given head-argument POS pair,
we only list features corresponding to the and conjunction and
h --+ a direction.
trigram the a h is frequent ‚Äì in that trigram, the ad-
jective attaches to the noun. The second entry (NN
- NN) shows that one noun is a good modifier of
another if they frequently appear together hyphen-
ated (another punctuation-based cue mentioned in
previous work on noun bracketing, see Nakov and
Hearst (2005a)). While they were motivated on sep-
arate grounds, these features can also compensate
for inapplicability of the affinity features. For exam-
ple, the third entry (VBD this NN) is a case where
even if the head (a VBD like adopted) actually se-
lects strongly for the argument (a NN like plan), the
bigram adopted plan may not be as frequent as ex-
pected, because it requires a determiner in its mini-
mal analogous form adopted the plan.
</bodyText>
<sectionHeader confidence="0.999004" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999914444444445">
Web features are a way to bring evidence from a
large unlabeled corpus to bear on hard disambigua-
tion decisions that are not easily resolvable based on
limited parser training data. Our approach allows re-
vealing features to be mined for the entire range of
attachment types and then aggregated and balanced
in a full parsing setting. Our results show that these
web features resolve ambiguities not correctly han-
dled by current state-of-the-art systems.
</bodyText>
<sectionHeader confidence="0.998822" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999611">
We would like to thank the anonymous reviewers
for their helpful suggestions. This research is sup-
</bodyText>
<table confidence="0.955725333333333">
POSh POSa mid/bfr-word Example (h, a)
NNS JJ b = the other ‚Üê things
NN NN m = - auto ‚Üê maker
VBD NN m = this adopted ‚Üí plan
NNS NN b = of computer ‚Üê products
NN DT m = current the ‚Üê proposal
VBG IN b = of going ‚Üí into
NNS IN m = ‚Äù clusters ‚Üí of
IN NN m = your In ‚Üí review
TO VB b = used to ‚Üí ease
VBZ NN m = that issue ‚Üê has
IN NNS m = two than ‚Üí minutes
IN NN b = used as ‚Üí tool
IN VBD m = they since ‚Üí were
VB TO b = will fail ‚Üí to
</table>
<tableCaption confidence="0.628138">
Table 10: The high-weight high-count (thresholded at a count of
2000) general features of the mid and before paraphrase schema
(examples show head and arg in linear order with arrow from
head to arg).
</tableCaption>
<bodyText confidence="0.620938">
ported by BBN under DARPA contract HR0011-06-
C-0022.
</bodyText>
<sectionHeader confidence="0.974302" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999519423076923">
M. Atterer and H. Schutze. 2007. Prepositional phrase
attachment without oracles. Computational Linguis-
tics, 33(4):469476.
Thorsten Brants and Alex Franz. 2006. The Google Web
1T 5-gram corpus version 1.1. LDC2006T13.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings ofACL.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25‚Äì70.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of ICML.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL.
</reference>
<page confidence="0.972075">
701
</page>
<reference confidence="0.999922781818182">
Adam Kilgarriff. 2007. Googleology is bad science.
Computational Linguistics, 33(1).
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL.
Mirella Lapata and Frank Keller. 2004. The Web as a
baseline: Evaluating the performance of unsupervised
Web-based models for a range of NLP tasks. In Pro-
ceedings of HLT-NAACL.
M. Lauer. 1995. Corpus statistics meet the noun com-
pound: some empirical results. In Proceedings of
ACL.
Andr¬¥e F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formula-
tions for dependency parsing. In Proceedings of ACL-
IJCNLP.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL.
Preslav Nakov and Marti Hearst. 2005a. Search en-
gine statistics beyond the n-gram: Application to noun
compound bracketing. In Proceedings of CoNLL.
Preslav Nakov and Marti Hearst. 2005b. Using the web
as an implicit training set: Application to structural
ambiguity resolution. In Proceedings of EMNLP.
Preslav Nakov and Marti Hearst. 2008. Solving rela-
tional similarity problems using the web as a corpus.
In Proceedings of ACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and
Interpretable Tree Annotation. In Proceedings of
COLING-ACL.
Emily Pitler, Shane Bergsma, Dekang Lin, , and Kenneth
Church. 2010. Using web-scale n-grams to improve
base NP parsing performance. In Proceedings of COL-
ING.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of
EMNLP.
David Vadas and James R. Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proceedings
of ACL.
Martin Volk. 2001. Exploiting the WWW as a corpus to
resolve PP attachment ambiguities. In Proceedings of
Corpus Linguistics.
Alexander Yates, Stefan Schoenmackers, and Oren Et-
zioni. 2006. Detecting parser errors using web-based
semantic filters. In Proceedings of EMNLP.
</reference>
<page confidence="0.997789">
702
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.859377">
<title confidence="0.999621">Web-Scale Features for Full-Scale Parsing</title>
<author confidence="0.960824">Bansal</author>
<affiliation confidence="0.9998755">Computer Science University of California,</affiliation>
<abstract confidence="0.990783105263158">Counts from large corpora (like the web) can be powerful syntactic cues. Past work has used web counts to help resolve isolated ambiguities, such as binary noun-verb PP attachments and noun compound bracketings. In this work, we first present a method for generating web count features that address the full range of syntactic attachments. These features encode both surface evidence of lexical affinities as well as paraphrase-based cues to syntactic structure. We then integrate our features into full-scale dependency and constituent parsers. We show relative error reductions of 7.0% over the second-order dependency parser of McDonald and Pereira (2006), 9.2% over the constituent parser of Petrov et al. (2006), and 3.4% over a non-local constituent reranker.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Atterer</author>
<author>H Schutze</author>
</authors>
<title>Prepositional phrase attachment without oracles.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="8686" citStr="Atterer and Schutze (2007)" startWordPosition="1395" endWordPosition="1398">auer (1995), for example, would be to take an ambiguous noun sequence like hydrogen ion exchange and compare the various counts (or associated conditional probabilities) of n-grams like hydrogen ion and hydrogen exchange. The attachment with the greater score is chosen. More recently, Pitler et al. (2010) use web-scale n-grams to compute similar association statistics for longer sequences of nouns. Our affinity features closely follow this basic idea of association statistics. However, because a real parser will not have access to gold-standard knowledge of the competing attachment sites (see Atterer and Schutze (2007)‚Äôs criticism of previous work), we must instead compute features for all possible head-argument pairs from our web corpus. Moreover, when there are only two competing attachment options, one can do things like directly compare two count-based heuristics and choose the larger. Integration into a parser requires features to be functions of single attachments, not pairwise comparisons between alternatives. A learning algorithm can then weight features so that they compare appropriately across parses. We employ a collection of affinity features of varying specificity. The basic feature is the core</context>
</contexts>
<marker>Atterer, Schutze, 2007</marker>
<rawString>M. Atterer and H. Schutze. 2007. Prepositional phrase attachment without oracles. Computational Linguistics, 33(4):469476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>The Google Web 1T 5-gram corpus version 1.1.</title>
<date>2006</date>
<pages>2006--13</pages>
<contexts>
<context position="15410" citStr="Brants and Franz, 2006" startWordPosition="2570" endWordPosition="2573">. Nakov and Hearst (2008) post-processes the first 1000 result snippets. One challenge with this approach is that an external search API is now embedded into the parser, raising issues of both speed and daily query limits, especially if all possible attachments trigger queries. Such methods also create a dependence on the quality and postprocessing of the search results, limitations of the query process (for instance, search engines can ignore punctuation (Nakov and Hearst, 2005b)). Rather than working through a search API (or scraper), we use an offline web corpus ‚Äì the Google n-gram corpus (Brants and Franz, 2006) ‚Äì which contains English n-grams (n = 1 to 5) and their observed frequency counts, generated from nearly 1 trillion word tokens and 95 billion sentences. This corpus allows us to efficiently access huge amounts of web-derived information in a compressed way, though in the process it limits us to local queries. In particular, we only use counts of n-grams of the form x * y where the gap length is &lt; 3. Our system requires the counts from a large collection of these n-gram queries (around 4.5 million). The most basic queries are counts of head-argument pairs in contiguous h a and gapped h * a co</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. The Google Web 1T 5-gram corpus version 1.1. LDC2006T13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="6429" citStr="Charniak and Johnson, 2005" startWordPosition="1036" endWordPosition="1039">e web-scale corpus (Google n-grams) to compute features for the full parsing task. To show end-to-end effectiveness, we incorporate our features into state-of-the-art dependency and constituent parsers. For the dependency case, we can integrate them into the dynamic programming of a base parser; we use the discriminativelytrained MST dependency parser (McDonald et al., 2005; McDonald and Pereira, 2006). Our first-order web-features give 7.0% relative error reduction over the second-order dependency baseline of McDonald and Pereira (2006). For constituent parsing, we use a reranking framework (Charniak and Johnson, 2005; Collins and Koo, 2005; Collins, 2000) and show 9.2% relative error reduction over the Berkeley parser baseline. In the same framework, we also achieve 3.4% error reduction over the non-local syntactic features used in Huang (2008). Our webscale features reduce errors for a range of attachment types. Finally, we present an analysis of influential features. We not only reproduce features suggested in previous work but also discover a range of new ones. 2 Web-count Features Structural errors in the output of state-of-the-art parsers, constituent or dependency, can be viewed as attachment errors</context>
<context position="23447" citStr="Charniak and Johnson (2005)" startWordPosition="3957" endWordPosition="3960">we used 50-best lists in our experiments. For discriminative learning, we used the averaged perceptron (Collins, 2002; Huang, 2008). Our core feature is the log conditional likelihood of the underlying parser.10 All other features are indicator features. First, we add all the Web-scale features as defined above. These features alone achieve a 9.2% relative error reduction. The affinity and paraphrase features contribute about two-fifths and three-fifths of this improvement, respectively. Next, we rerank with only the features (both local and non-local) from Huang (2008), a simplified merge of Charniak and Johnson (2005) and Collins (2000) (here configurational). These features alone achieve around the same improvements over the baseline as our web-scale features, even though they are highly non-local and extensive. Finally, we rerank with both our Web-scale features and the configurational features. When combined, our web-scale features give a further error reduction of 3.4% over the configurational reranker (and a combined error reduction of 12.2%). All results are shown in Table 3.11 5 Analysis Table 4 shows error counts and relative reductions that our web features provide over the 2nd-order dependency ba</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and MaxEnt discriminative reranking. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="6452" citStr="Collins and Koo, 2005" startWordPosition="1040" endWordPosition="1043">-grams) to compute features for the full parsing task. To show end-to-end effectiveness, we incorporate our features into state-of-the-art dependency and constituent parsers. For the dependency case, we can integrate them into the dynamic programming of a base parser; we use the discriminativelytrained MST dependency parser (McDonald et al., 2005; McDonald and Pereira, 2006). Our first-order web-features give 7.0% relative error reduction over the second-order dependency baseline of McDonald and Pereira (2006). For constituent parsing, we use a reranking framework (Charniak and Johnson, 2005; Collins and Koo, 2005; Collins, 2000) and show 9.2% relative error reduction over the Berkeley parser baseline. In the same framework, we also achieve 3.4% error reduction over the non-local syntactic features used in Huang (2008). Our webscale features reduce errors for a range of attachment types. Finally, we present an analysis of influential features. We not only reproduce features suggested in previous work but also discover a range of new ones. 2 Web-count Features Structural errors in the output of state-of-the-art parsers, constituent or dependency, can be viewed as attachment errors, examples of which are</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>Michael Collins and Terry Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31(1):25‚Äì70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia.</location>
<contexts>
<context position="24604" citStr="Collins (1999)" startWordPosition="4144" endWordPosition="4145">our web features provide over the 2nd-order dependency baseline. While we do see substantial gains for classic PP (IN) attachment cases, we see equal or greater error reductions for a range of attachment types. Further, Table 5 shows how the to9Default: we ran the Berkeley parser in its default ‚Äòfast‚Äô mode; the output k-best lists are ordered by max-rule-score. 10This is output by the flag -confidence. Note that baseline results with just this feature are slightly worse than 1-best results because the k-best lists are generated by max-rule-score. We report both numbers in Table 3. 11We follow Collins (1999) for head rules. 698 Dev (sec 22) Test (sec 23) F1 EX F1 EX Parsing Model Baseline (1-best) 90.6 39.4 90.2 37.3 log p(t|w) 90.4 38.9 89.9 37.3 + Web features 91.6 42.5 91.1 40.6 + Configurational features 91.8 43.8 91.1 40.6 + Web + Configurational 92.1 44.0 91.4 41.4 Table 3: Parsing results for reranking 50-best lists of Berkeley parser (Dev is WSJ section 22 and Test is WSJ section 23, all lengths). Arg Tag # Attach Baseline This Work % ER NN 5725 5387 5429 12.4 NNP 4043 3780 3804 9.1 IN 4026 3416 3490 12.1 DT 3511 3424 3429 5.8 NNS 2504 2319 2348 15.7 JJ 2472 2310 2329 11.7 CD 1845 1739 17</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="6468" citStr="Collins, 2000" startWordPosition="1044" endWordPosition="1045">ures for the full parsing task. To show end-to-end effectiveness, we incorporate our features into state-of-the-art dependency and constituent parsers. For the dependency case, we can integrate them into the dynamic programming of a base parser; we use the discriminativelytrained MST dependency parser (McDonald et al., 2005; McDonald and Pereira, 2006). Our first-order web-features give 7.0% relative error reduction over the second-order dependency baseline of McDonald and Pereira (2006). For constituent parsing, we use a reranking framework (Charniak and Johnson, 2005; Collins and Koo, 2005; Collins, 2000) and show 9.2% relative error reduction over the Berkeley parser baseline. In the same framework, we also achieve 3.4% error reduction over the non-local syntactic features used in Huang (2008). Our webscale features reduce errors for a range of attachment types. Finally, we present an analysis of influential features. We not only reproduce features suggested in previous work but also discover a range of new ones. 2 Web-count Features Structural errors in the output of state-of-the-art parsers, constituent or dependency, can be viewed as attachment errors, examples of which are Figure 1 and Fi</context>
<context position="23466" citStr="Collins (2000)" startWordPosition="3962" endWordPosition="3963">eriments. For discriminative learning, we used the averaged perceptron (Collins, 2002; Huang, 2008). Our core feature is the log conditional likelihood of the underlying parser.10 All other features are indicator features. First, we add all the Web-scale features as defined above. These features alone achieve a 9.2% relative error reduction. The affinity and paraphrase features contribute about two-fifths and three-fifths of this improvement, respectively. Next, we rerank with only the features (both local and non-local) from Huang (2008), a simplified merge of Charniak and Johnson (2005) and Collins (2000) (here configurational). These features alone achieve around the same improvements over the baseline as our web-scale features, even though they are highly non-local and extensive. Finally, we rerank with both our Web-scale features and the configurational features. When combined, our web-scale features give a further error reduction of 3.4% over the configurational reranker (and a combined error reduction of 12.2%). All results are shown in Table 3.11 5 Analysis Table 4 shows error counts and relative reductions that our web features provide over the 2nd-order dependency baseline. While we do</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="22937" citStr="Collins, 2002" startWordPosition="3882" endWordPosition="3883">erations of split and merge with smoothing. k = 1 k = 2 k = 10 k = 25 k = 50 k = 100 Dev 90.6 92.3 95.1 95.8 96.2 96.5 Test 90.2 91.8 94.7 95.6 96.1 96.4 Table 2: Oracle F1-scores for k-best lists output by Berkeley parser for English WSJ parsing (Dev is section 22 and Test is section 23, all lengths). on 2 folds to get parses for the third fold.9 The oracle scores of the k-best lists (for different values of k) for the development and test sets are shown in Table 2. Based on these results, we used 50-best lists in our experiments. For discriminative learning, we used the averaged perceptron (Collins, 2002; Huang, 2008). Our core feature is the log conditional likelihood of the underlying parser.10 All other features are indicator features. First, we add all the Web-scale features as defined above. These features alone achieve a 9.2% relative error reduction. The affinity and paraphrase features contribute about two-fifths and three-fifths of this improvement, respectively. Next, we rerank with only the features (both local and non-local) from Huang (2008), a simplified merge of Charniak and Johnson (2005) and Collins (2000) (here configurational). These features alone achieve around the same i</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Alex Kleeman</author>
<author>Christopher D Manning</author>
</authors>
<title>Efficient, feature-based, conditional random field parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5626" citStr="Finkel et al. (2008)" startWordPosition="915" endWordPosition="918">lar work is Pitler et al. (2010), which use Web-scale n-gram counts for multi-way noun bracketing decisions, though that work considers only sequences of nouns and uses only affinity-based web features. Yates et al. (2006) use Web counts to filter out certain ‚Äòsemantically bad‚Äô parses from extraction candidate sets but are not concerned with distinguishing amongst top parses. In an important contrast, Koo et al. (2008) smooth the sparseness of lexical features in a discriminative dependency parser by using clusterbased word-senses as intermediate abstractions in addition to POS tags (also see Finkel et al. (2008)). Their work also gives a way to tap into corpora beyond the training data, through cluster membership rather than explicit corpus counts and paraphrases. This work uses a large web-scale corpus (Google n-grams) to compute features for the full parsing task. To show end-to-end effectiveness, we incorporate our features into state-of-the-art dependency and constituent parsers. For the dependency case, we can integrate them into the dynamic programming of a base parser; we use the discriminativelytrained MST dependency parser (McDonald et al., 2005; McDonald and Pereira, 2006). Our first-order </context>
</contexts>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning. 2008. Efficient, feature-based, conditional random field parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="6661" citStr="Huang (2008)" startWordPosition="1075" endWordPosition="1076">hem into the dynamic programming of a base parser; we use the discriminativelytrained MST dependency parser (McDonald et al., 2005; McDonald and Pereira, 2006). Our first-order web-features give 7.0% relative error reduction over the second-order dependency baseline of McDonald and Pereira (2006). For constituent parsing, we use a reranking framework (Charniak and Johnson, 2005; Collins and Koo, 2005; Collins, 2000) and show 9.2% relative error reduction over the Berkeley parser baseline. In the same framework, we also achieve 3.4% error reduction over the non-local syntactic features used in Huang (2008). Our webscale features reduce errors for a range of attachment types. Finally, we present an analysis of influential features. We not only reproduce features suggested in previous work but also discover a range of new ones. 2 Web-count Features Structural errors in the output of state-of-the-art parsers, constituent or dependency, can be viewed as attachment errors, examples of which are Figure 1 and Figure 2.1 One way to address attachment errors is through features which factor over head-argument 1For constituent parsers, there can be minor tree variations which can result in the same set o</context>
<context position="22951" citStr="Huang, 2008" startWordPosition="3884" endWordPosition="3885">it and merge with smoothing. k = 1 k = 2 k = 10 k = 25 k = 50 k = 100 Dev 90.6 92.3 95.1 95.8 96.2 96.5 Test 90.2 91.8 94.7 95.6 96.1 96.4 Table 2: Oracle F1-scores for k-best lists output by Berkeley parser for English WSJ parsing (Dev is section 22 and Test is section 23, all lengths). on 2 folds to get parses for the third fold.9 The oracle scores of the k-best lists (for different values of k) for the development and test sets are shown in Table 2. Based on these results, we used 50-best lists in our experiments. For discriminative learning, we used the averaged perceptron (Collins, 2002; Huang, 2008). Our core feature is the log conditional likelihood of the underlying parser.10 All other features are indicator features. First, we add all the Web-scale features as defined above. These features alone achieve a 9.2% relative error reduction. The affinity and paraphrase features contribute about two-fifths and three-fifths of this improvement, respectively. Next, we rerank with only the features (both local and non-local) from Huang (2008), a simplified merge of Charniak and Johnson (2005) and Collins (2000) (here configurational). These features alone achieve around the same improvements ov</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>Googleology is bad science.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="14786" citStr="Kilgarriff (2007)" startWordPosition="2471" endWordPosition="2472">trained on the parser training set. As discussed in Section 5, the top features learned by our learning algorithm duplicate the hand-crafted configurations used in previous work (Nakov and Hearst, 2005b) but also add numerous others, and, of course, apply to many more attachment types. 3 Working with Web n-Grams Previous approaches have generally used search engines to collect count statistics (Lapata and Keller, 2004; Nakov and Hearst, 2005b; Nakov and Hearst, 2008). Lapata and Keller (2004) uses the number of page hits as the web-count of the queried ngram (which is problematic according to Kilgarriff (2007)). Nakov and Hearst (2008) post-processes the first 1000 result snippets. One challenge with this approach is that an external search API is now embedded into the parser, raising issues of both speed and daily query limits, especially if all possible attachments trigger queries. Such methods also create a dependence on the quality and postprocessing of the search results, limitations of the query process (for instance, search engines can ignore punctuation (Nakov and Hearst, 2005b)). Rather than working through a search API (or scraper), we use an offline web corpus ‚Äì the Google n-gram corpus </context>
</contexts>
<marker>Kilgarriff, 2007</marker>
<rawString>Adam Kilgarriff. 2007. Googleology is bad science. Computational Linguistics, 33(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="22161" citStr="Koo and Collins (2010)" startWordPosition="3733" endWordPosition="3736">-local) features. Our baseline system is the Berkeley parser, from which we obtain k-best lists for the development set (WSJ section 22) and test set (WSJ section 23) using a grammar trained on all the training data (WSJ sections 2-21).8 To get k-best lists for the training set, we use 3-fold jackknifing where we train a grammar 6Their README specifies ‚Äòtraining-k:5 iters:10 losstype:nopunc decode-type:proj‚Äô, which we used for all final experiments; we used the faster ‚Äòtraining-k:1 iters:5‚Äô setting for most development experiments. 7Work such as Smith and Eisner (2008), Martins et al. (2009), Koo and Collins (2010) has been exploring more nonlocal features for dependency parsing. It will be interesting to see how these features interact with our web features. 8Settings: 6 iterations of split and merge with smoothing. k = 1 k = 2 k = 10 k = 25 k = 50 k = 100 Dev 90.6 92.3 95.1 95.8 96.2 96.5 Test 90.2 91.8 94.7 95.6 96.1 96.4 Table 2: Oracle F1-scores for k-best lists output by Berkeley parser for English WSJ parsing (Dev is section 22 and Test is section 23, all lengths). on 2 folds to get parses for the third fold.9 The oracle scores of the k-best lists (for different values of k) for the development a</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5428" citStr="Koo et al. (2008)" startWordPosition="885" endWordPosition="888">consider how to efficiently mine the Google n-grams corpus. Given the success of Web counts for isolated ambiguities, there is relatively little previous research in this direction. The most similar work is Pitler et al. (2010), which use Web-scale n-gram counts for multi-way noun bracketing decisions, though that work considers only sequences of nouns and uses only affinity-based web features. Yates et al. (2006) use Web counts to filter out certain ‚Äòsemantically bad‚Äô parses from extraction candidate sets but are not concerned with distinguishing amongst top parses. In an important contrast, Koo et al. (2008) smooth the sparseness of lexical features in a discriminative dependency parser by using clusterbased word-senses as intermediate abstractions in addition to POS tags (also see Finkel et al. (2008)). Their work also gives a way to tap into corpora beyond the training data, through cluster membership rather than explicit corpus counts and paraphrases. This work uses a large web-scale corpus (Google n-grams) to compute features for the full parsing task. To show end-to-end effectiveness, we incorporate our features into state-of-the-art dependency and constituent parsers. For the dependency cas</context>
<context position="19687" citStr="Koo et al. (2008)" startWordPosition="3353" endWordPosition="3356">n a reranking approach. We also verify that our features contribute on top of standard reranking features.3 and h a *; these are handled similarly. 4.1 Dependency Parsing For dependency parsing, we use the discriminatively-trained MSTParser4, an implementation of first and second order MST parsing models of McDonald et al. (2005) and McDonald and Pereira (2006). We use the standard splits of Penn Treebank into training (sections 2-21), development (section 22) and test (section 23). We used the ‚Äòpennconverter‚Äô5 tool to convert Penn trees from constituent format to dependency format. Following Koo et al. (2008), we used the MXPOST tagger (Ratnaparkhi, 1996) trained on the full training data to provide part-of-speech tags for the development 3All reported experiments are run on all sentences, i.e. without any length limit. 4http://sourceforge.net/projects/mstparser 5This supersedes ‚ÄòPenn2Malt‚Äô and is available at http://nlp.cs.lth.se/software/treebank converter. We follow its recommendation to patch WSJ data with NP bracketing by Vadas and Curran (2007). 697 Order 2 + Web features % Error Redn. Dev (sec 22) 92.1 92.7 7.6% Test (sec 23) 91.4 92.0 7.0% Table 1: UAS results for English WSJ dependency pa</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Frank Keller</author>
</authors>
<title>The Web as a baseline: Evaluating the performance of unsupervised Web-based models for a range of NLP tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="2510" citStr="Lapata and Keller, 2004" startWordPosition="408" endWordPosition="411">ld edges are in dashed gold and edges common in guess and gold parses are in black. attachment ambiguity where by yesterday afternoon should attach to had already, (b) is an NP-internal ambiguity where half a should attach to dozen and not to newspapers, and (c) is an adverb attachment ambiguity, where just should modify fine and not the verb ‚Äôs. Resolving many of these errors requires information that is simply not present in the approximately 1M words on which the parser was trained. One way to access more information is to exploit surface counts from large corpora like the web (Volk, 2001; Lapata and Keller, 2004). For example, the phrase raising from is much more frequent on the Web than $ x billion from. While this ‚Äòaffinity‚Äô is only a surface correlation, Volk (2001) showed that comparing such counts can often correctly resolve tricky PP attachments. This basic idea has led to a good deal of successful work on disambiguating isolated, binary PP attachments. For example, Nakov and Hearst (2005b) showed that looking for paraphrase counts can further improve PP resolution. In this case, the existence of reworded phrases like raising it from on the Web also imply a verbal at$ 30 billion from debt ... VB</context>
<context position="7988" citStr="Lapata and Keller, 2004" startWordPosition="1287" endWordPosition="1291">ver head-argument pairs. pairs, as is standard in the dependency parsing literature (see Figure 3). Here, we discuss which webcount based features 0(h, a) should fire over a given head-argument pair (we consider the words h and a to be indexed, and so features can be sensitive to their order and distance, as is also standard). 2.1 Affinity Features Affinity statistics, such as lexical co-occurrence counts from large corpora, have been used previously for resolving individual attachments at least as far back as Lauer (1995) for noun-compound bracketing, and later for PP attachment (Volk, 2001; Lapata and Keller, 2004) and coordination ambiguity (Nakov and Hearst, 2005b). The approach of Lauer (1995), for example, would be to take an ambiguous noun sequence like hydrogen ion exchange and compare the various counts (or associated conditional probabilities) of n-grams like hydrogen ion and hydrogen exchange. The attachment with the greater score is chosen. More recently, Pitler et al. (2010) use web-scale n-grams to compute similar association statistics for longer sequences of nouns. Our affinity features closely follow this basic idea of association statistics. However, because a real parser will not have a</context>
<context position="14590" citStr="Lapata and Keller, 2004" startWordPosition="2436" endWordPosition="2439">uced context word is helpful, we also construct abstracted versions of these paraphrase features where the context words c are collapsed to their parts-of-speech POS(c), obtained using a unigram-tagger trained on the parser training set. As discussed in Section 5, the top features learned by our learning algorithm duplicate the hand-crafted configurations used in previous work (Nakov and Hearst, 2005b) but also add numerous others, and, of course, apply to many more attachment types. 3 Working with Web n-Grams Previous approaches have generally used search engines to collect count statistics (Lapata and Keller, 2004; Nakov and Hearst, 2005b; Nakov and Hearst, 2008). Lapata and Keller (2004) uses the number of page hits as the web-count of the queried ngram (which is problematic according to Kilgarriff (2007)). Nakov and Hearst (2008) post-processes the first 1000 result snippets. One challenge with this approach is that an external search API is now embedded into the parser, raising issues of both speed and daily query limits, especially if all possible attachments trigger queries. Such methods also create a dependence on the quality and postprocessing of the search results, limitations of the query proc</context>
</contexts>
<marker>Lapata, Keller, 2004</marker>
<rawString>Mirella Lapata and Frank Keller. 2004. The Web as a baseline: Evaluating the performance of unsupervised Web-based models for a range of NLP tasks. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lauer</author>
</authors>
<title>Corpus statistics meet the noun compound: some empirical results.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="7892" citStr="Lauer (1995)" startWordPosition="1274" endWordPosition="1275">ies, but these are rare in comparison. 694 ùúô(head arg) Figure 3: Features factored over head-argument pairs. pairs, as is standard in the dependency parsing literature (see Figure 3). Here, we discuss which webcount based features 0(h, a) should fire over a given head-argument pair (we consider the words h and a to be indexed, and so features can be sensitive to their order and distance, as is also standard). 2.1 Affinity Features Affinity statistics, such as lexical co-occurrence counts from large corpora, have been used previously for resolving individual attachments at least as far back as Lauer (1995) for noun-compound bracketing, and later for PP attachment (Volk, 2001; Lapata and Keller, 2004) and coordination ambiguity (Nakov and Hearst, 2005b). The approach of Lauer (1995), for example, would be to take an ambiguous noun sequence like hydrogen ion exchange and compare the various counts (or associated conditional probabilities) of n-grams like hydrogen ion and hydrogen exchange. The attachment with the greater score is chosen. More recently, Pitler et al. (2010) use web-scale n-grams to compute similar association statistics for longer sequences of nouns. Our affinity features closely </context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>M. Lauer. 1995. Corpus statistics meet the noun compound: some empirical results. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr¬¥e F T Martins</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Concise integer linear programming formulations for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of ACLIJCNLP.</booktitle>
<contexts>
<context position="22137" citStr="Martins et al. (2009)" startWordPosition="3729" endWordPosition="3732">st using (generally non-local) features. Our baseline system is the Berkeley parser, from which we obtain k-best lists for the development set (WSJ section 22) and test set (WSJ section 23) using a grammar trained on all the training data (WSJ sections 2-21).8 To get k-best lists for the training set, we use 3-fold jackknifing where we train a grammar 6Their README specifies ‚Äòtraining-k:5 iters:10 losstype:nopunc decode-type:proj‚Äô, which we used for all final experiments; we used the faster ‚Äòtraining-k:1 iters:5‚Äô setting for most development experiments. 7Work such as Smith and Eisner (2008), Martins et al. (2009), Koo and Collins (2010) has been exploring more nonlocal features for dependency parsing. It will be interesting to see how these features interact with our web features. 8Settings: 6 iterations of split and merge with smoothing. k = 1 k = 2 k = 10 k = 25 k = 50 k = 100 Dev 90.6 92.3 95.1 95.8 96.2 96.5 Test 90.2 91.8 94.7 95.6 96.1 96.4 Table 2: Oracle F1-scores for k-best lists output by Berkeley parser for English WSJ parsing (Dev is section 22 and Test is section 23, all lengths). on 2 folds to get parses for the third fold.9 The oracle scores of the k-best lists (for different values of </context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>Andr¬¥e F. T. Martins, Noah A. Smith, and Eric P. Xing. 2009. Concise integer linear programming formulations for dependency parsing. In Proceedings of ACLIJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="829" citStr="McDonald and Pereira (2006)" startWordPosition="122" endWordPosition="125"> the web) can be powerful syntactic cues. Past work has used web counts to help resolve isolated ambiguities, such as binary noun-verb PP attachments and noun compound bracketings. In this work, we first present a method for generating web count features that address the full range of syntactic attachments. These features encode both surface evidence of lexical affinities as well as paraphrase-based cues to syntactic structure. We then integrate our features into full-scale dependency and constituent parsers. We show relative error reductions of 7.0% over the second-order dependency parser of McDonald and Pereira (2006), 9.2% over the constituent parser of Petrov et al. (2006), and 3.4% over a non-local constituent reranker. 1 Introduction Current state-of-the art syntactic parsers have achieved accuracies in the range of 90% F1 on the Penn Treebank, but a range of errors remain. From a dependency viewpoint, structural errors can be cast as incorrect attachments, even for constituent (phrase-structure) parsers. For example, in the Berkeley parser (Petrov et al., 2006), about 20% of the errors are prepositional phrase attachment errors as in Figure 1, where a preposition-headed (IN) phrase was assigned an inc</context>
<context position="4557" citStr="McDonald and Pereira (2006)" startWordPosition="747" endWordPosition="750">et al., 2010). For example, in (b), half dozen is more frequent than half newspapers. In this paper, we show how to apply these ideas to all attachments in full-scale parsing. Doing so requires three main issues to be addressed. First, we show how features can be generated for arbitrary head-argument configurations. Affinity features are relatively straightforward, but paraphrase features, which have been hand-developed in the past, are more complex. Second, we integrate our features into full-scale parsing systems. For dependency parsing, we augment the features in the second-order parser of McDonald and Pereira (2006). For constituent parsing, we rerank the output of the Berkeley parser (Petrov et al., 2006). Third, past systems have usually gotten their counts from web search APIs, which does not scale to quadratically-many attachments in each sentence. Instead, we consider how to efficiently mine the Google n-grams corpus. Given the success of Web counts for isolated ambiguities, there is relatively little previous research in this direction. The most similar work is Pitler et al. (2010), which use Web-scale n-gram counts for multi-way noun bracketing decisions, though that work considers only sequences </context>
<context position="6208" citStr="McDonald and Pereira, 2006" startWordPosition="1005" endWordPosition="1008">on to POS tags (also see Finkel et al. (2008)). Their work also gives a way to tap into corpora beyond the training data, through cluster membership rather than explicit corpus counts and paraphrases. This work uses a large web-scale corpus (Google n-grams) to compute features for the full parsing task. To show end-to-end effectiveness, we incorporate our features into state-of-the-art dependency and constituent parsers. For the dependency case, we can integrate them into the dynamic programming of a base parser; we use the discriminativelytrained MST dependency parser (McDonald et al., 2005; McDonald and Pereira, 2006). Our first-order web-features give 7.0% relative error reduction over the second-order dependency baseline of McDonald and Pereira (2006). For constituent parsing, we use a reranking framework (Charniak and Johnson, 2005; Collins and Koo, 2005; Collins, 2000) and show 9.2% relative error reduction over the Berkeley parser baseline. In the same framework, we also achieve 3.4% error reduction over the non-local syntactic features used in Huang (2008). Our webscale features reduce errors for a range of attachment types. Finally, we present an analysis of influential features. We not only reprodu</context>
<context position="19433" citStr="McDonald and Pereira (2006)" startWordPosition="3313" endWordPosition="3316">ing rather than for limited decisions about isolated ambiguities. We first integrate our features into a dependency parser, where the integration is more natural and pushes all the way into the underlying dynamic program. We then add them to a constituent parser in a reranking approach. We also verify that our features contribute on top of standard reranking features.3 and h a *; these are handled similarly. 4.1 Dependency Parsing For dependency parsing, we use the discriminatively-trained MSTParser4, an implementation of first and second order MST parsing models of McDonald et al. (2005) and McDonald and Pereira (2006). We use the standard splits of Penn Treebank into training (sections 2-21), development (section 22) and test (section 23). We used the ‚Äòpennconverter‚Äô5 tool to convert Penn trees from constituent format to dependency format. Following Koo et al. (2008), we used the MXPOST tagger (Ratnaparkhi, 1996) trained on the full training data to provide part-of-speech tags for the development 3All reported experiments are run on all sentences, i.e. without any length limit. 4http://sourceforge.net/projects/mstparser 5This supersedes ‚ÄòPenn2Malt‚Äô and is available at http://nlp.cs.lth.se/software/treebank</context>
<context position="20664" citStr="McDonald and Pereira (2006)" startWordPosition="3504" endWordPosition="3507">verter. We follow its recommendation to patch WSJ data with NP bracketing by Vadas and Curran (2007). 697 Order 2 + Web features % Error Redn. Dev (sec 22) 92.1 92.7 7.6% Test (sec 23) 91.4 92.0 7.0% Table 1: UAS results for English WSJ dependency parsing. Dev is WSJ section 22 (all sentences) and Test is WSJ section 23 (all sentences). The order 2 baseline represents McDonald and Pereira (2006). and the test set, and we used 10-way jackknifing to generate tags for the training set. We added our first-order Web-scale features to the MSTParser system to evaluate improvement over the results of McDonald and Pereira (2006).6 Table 1 shows unlabeled attachments scores (UAS) for their second-order projective parser and the improved numbers resulting from the addition of our Web-scale features. Our first-order web-scale features show significant improvement even over their non-local second-order features.7 Additionally, our web-scale features are at least an order of magnitude fewer in number than even their first-order base features. 4.2 Constituent Parsing We also evaluate the utility of web-scale features on top of a state-of-the-art constituent parser ‚Äì the Berkeley parser (Petrov et al., 2006), an unlexicaliz</context>
<context position="25588" citStr="McDonald and Pereira, 2006" startWordPosition="4333" endWordPosition="4337">ion 22 and Test is WSJ section 23, all lengths). Arg Tag # Attach Baseline This Work % ER NN 5725 5387 5429 12.4 NNP 4043 3780 3804 9.1 IN 4026 3416 3490 12.1 DT 3511 3424 3429 5.8 NNS 2504 2319 2348 15.7 JJ 2472 2310 2329 11.7 CD 1845 1739 1738 -0.9 VBD 1705 1571 1580 6.7 RB 1308 1097 1100 1.4 CC 1000 855 854 -0.7 VB 983 940 945 11.6 TO 868 761 776 14.0 VBN 850 776 786 13.5 VBZ 705 633 629 -5.6 PRP 612 603 606 33.3 Table 4: Error reduction for attachments of various child (argument) categories. The columns depict the tag, its total attachments as argument, number of correct ones in baseline (McDonald and Pereira, 2006) and this work, and the relative error reduction. Results are for dependency parsing on the dev set for iters:5,training-k:1. tal errors break down by gold head. For example, the 12.1% total error reduction for attachments of an IN argument (which includes PPs as well as complementized SBARs) includes many errors where the gold attachments are to both noun and verb heads. Similarly, for an NN-headed argument, the major corrections are for attachments to noun and verb heads, which includes both object-attachment ambiguities and coordination ambiguities. We next investigate the features that wer</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="6179" citStr="McDonald et al., 2005" startWordPosition="1001" endWordPosition="1004"> abstractions in addition to POS tags (also see Finkel et al. (2008)). Their work also gives a way to tap into corpora beyond the training data, through cluster membership rather than explicit corpus counts and paraphrases. This work uses a large web-scale corpus (Google n-grams) to compute features for the full parsing task. To show end-to-end effectiveness, we incorporate our features into state-of-the-art dependency and constituent parsers. For the dependency case, we can integrate them into the dynamic programming of a base parser; we use the discriminativelytrained MST dependency parser (McDonald et al., 2005; McDonald and Pereira, 2006). Our first-order web-features give 7.0% relative error reduction over the second-order dependency baseline of McDonald and Pereira (2006). For constituent parsing, we use a reranking framework (Charniak and Johnson, 2005; Collins and Koo, 2005; Collins, 2000) and show 9.2% relative error reduction over the Berkeley parser baseline. In the same framework, we also achieve 3.4% error reduction over the non-local syntactic features used in Huang (2008). Our webscale features reduce errors for a range of attachment types. Finally, we present an analysis of influential </context>
<context position="19401" citStr="McDonald et al. (2005)" startWordPosition="3308" endWordPosition="3311"> used in full-sentence parsing rather than for limited decisions about isolated ambiguities. We first integrate our features into a dependency parser, where the integration is more natural and pushes all the way into the underlying dynamic program. We then add them to a constituent parser in a reranking approach. We also verify that our features contribute on top of standard reranking features.3 and h a *; these are handled similarly. 4.1 Dependency Parsing For dependency parsing, we use the discriminatively-trained MSTParser4, an implementation of first and second order MST parsing models of McDonald et al. (2005) and McDonald and Pereira (2006). We use the standard splits of Penn Treebank into training (sections 2-21), development (section 22) and test (section 23). We used the ‚Äòpennconverter‚Äô5 tool to convert Penn trees from constituent format to dependency format. Following Koo et al. (2008), we used the MXPOST tagger (Ratnaparkhi, 1996) trained on the full training data to provide part-of-speech tags for the development 3All reported experiments are run on all sentences, i.e. without any length limit. 4http://sourceforge.net/projects/mstparser 5This supersedes ‚ÄòPenn2Malt‚Äô and is available at http:/</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Search engine statistics beyond the n-gram: Application to noun compound bracketing.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="2899" citStr="Nakov and Hearst (2005" startWordPosition="473" endWordPosition="476">formation that is simply not present in the approximately 1M words on which the parser was trained. One way to access more information is to exploit surface counts from large corpora like the web (Volk, 2001; Lapata and Keller, 2004). For example, the phrase raising from is much more frequent on the Web than $ x billion from. While this ‚Äòaffinity‚Äô is only a surface correlation, Volk (2001) showed that comparing such counts can often correctly resolve tricky PP attachments. This basic idea has led to a good deal of successful work on disambiguating isolated, binary PP attachments. For example, Nakov and Hearst (2005b) showed that looking for paraphrase counts can further improve PP resolution. In this case, the existence of reworded phrases like raising it from on the Web also imply a verbal at$ 30 billion from debt ... VBG NP ... raising NP PP 693 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 693‚Äì702, Portland, Oregon, June 19-24, 2011. cÔøΩ2011 Association for Computational Linguistics ...Lehman Hutton Inc. by yesterday afternoon Figure 2: Different kinds of attachment errors in the parse output of the Berkeley parser (on Penn Treebank). Guess edges are in</context>
<context position="8039" citStr="Nakov and Hearst, 2005" startWordPosition="1296" endWordPosition="1299"> dependency parsing literature (see Figure 3). Here, we discuss which webcount based features 0(h, a) should fire over a given head-argument pair (we consider the words h and a to be indexed, and so features can be sensitive to their order and distance, as is also standard). 2.1 Affinity Features Affinity statistics, such as lexical co-occurrence counts from large corpora, have been used previously for resolving individual attachments at least as far back as Lauer (1995) for noun-compound bracketing, and later for PP attachment (Volk, 2001; Lapata and Keller, 2004) and coordination ambiguity (Nakov and Hearst, 2005b). The approach of Lauer (1995), for example, would be to take an ambiguous noun sequence like hydrogen ion exchange and compare the various counts (or associated conditional probabilities) of n-grams like hydrogen ion and hydrogen exchange. The attachment with the greater score is chosen. More recently, Pitler et al. (2010) use web-scale n-grams to compute similar association statistics for longer sequences of nouns. Our affinity features closely follow this basic idea of association statistics. However, because a real parser will not have access to gold-standard knowledge of the competing a</context>
<context position="11297" citStr="Nakov and Hearst (2005" startWordPosition="1853" endWordPosition="1856">number of wildcards allowed in the query is proportional to the binned distance between h and a in the sentence. Finally, we also include unigram variants of the above features, which are sensitive to only one of the head or argument. For all features used, we add cumulative variants where indicators are fired for all count bins bÔøΩ up to query count bin b. 2.2 Paraphrase Features In addition to measuring counts of the words present in the sentence, there exist clever ways in which paraphrases and other accidental indicators can help resolve specific ambiguities, some of which are discussed in Nakov and Hearst (2005a), Nakov and Hearst (2005b). For example, finding attestations of eat: spaghetti with sauce suggests a nominal attachment in Jean ate spaghetti with sauce. As another example, one clue that the example in Figure 1 is ùùì(raising from) ùùì($ from) raising $ from debt 695 a verbal attachment is that the proform paraphrase raising it from is commonly attested. Similarly, the attestation of be noun prep suggests nominal attachment. These paraphrase features hint at the correct attachment decision by looking for web n-grams with special contexts that reveal syntax superficially. Again, while effective</context>
<context position="14370" citStr="Nakov and Hearst, 2005" startWordPosition="2400" endWordPosition="2403"> from indicated a correct attachment between raising and from, frequent occurrences of lowering it with will indicate the correctness of an attachment between lowering and with. Finally, to handle the cases where no induced context word is helpful, we also construct abstracted versions of these paraphrase features where the context words c are collapsed to their parts-of-speech POS(c), obtained using a unigram-tagger trained on the parser training set. As discussed in Section 5, the top features learned by our learning algorithm duplicate the hand-crafted configurations used in previous work (Nakov and Hearst, 2005b) but also add numerous others, and, of course, apply to many more attachment types. 3 Working with Web n-Grams Previous approaches have generally used search engines to collect count statistics (Lapata and Keller, 2004; Nakov and Hearst, 2005b; Nakov and Hearst, 2008). Lapata and Keller (2004) uses the number of page hits as the web-count of the queried ngram (which is problematic according to Kilgarriff (2007)). Nakov and Hearst (2008) post-processes the first 1000 result snippets. One challenge with this approach is that an external search API is now embedded into the parser, raising issue</context>
<context position="29449" citStr="Nakov and Hearst (2005" startWordPosition="5016" endWordPosition="5019"> mid-word schema for a verb head and preposition argument (with head on left of argument). cators of (possibly non-adjacent) attachments. A subset of paraphrase features, which in the automatically-extracted case don‚Äôt really correspond to paraphrases at all, are shown in Table 7. Here we show features for verbal heads and IN arguments. The mid-words m which rank highly are those where the occurrence of hma as an n-gram is a good indicator that a attaches to h (m of course does not have to actually occur in the sentence). Interestingly, the top such features capture exactly the intuition from Nakov and Hearst (2005b), namely that if the verb h and the preposition a occur with a pronoun in between, we have evidence that a attaches to h (it certainly can‚Äôt attach to the pronoun). However, we also see other indicators that the preposition is selected for by the verb, such as adverbs like directly. As another example of known useful features being learned automatically, Table 8 shows the previous-context-word paraphrase features for a noun head and preposition argument (N ‚Äî* IN). Nakov and Hearst (2005b) suggested that the attestation of be N IN is a good indicator of attachment to the noun (the IN cannot g</context>
<context position="32877" citStr="Nakov and Hearst (2005" startWordPosition="5627" endWordPosition="5630">ring VBD and RB named, now VBP and VBP offer, need Table 9: The highest-weight features (thresholded at a count of 400) of the mid-word schema where the mid-word was a conjunction. For variety, for a given head-argument POS pair, we only list features corresponding to the and conjunction and h --+ a direction. trigram the a h is frequent ‚Äì in that trigram, the adjective attaches to the noun. The second entry (NN - NN) shows that one noun is a good modifier of another if they frequently appear together hyphenated (another punctuation-based cue mentioned in previous work on noun bracketing, see Nakov and Hearst (2005a)). While they were motivated on separate grounds, these features can also compensate for inapplicability of the affinity features. For example, the third entry (VBD this NN) is a case where even if the head (a VBD like adopted) actually selects strongly for the argument (a NN like plan), the bigram adopted plan may not be as frequent as expected, because it requires a determiner in its minimal analogous form adopted the plan. 6 Conclusion Web features are a way to bring evidence from a large unlabeled corpus to bear on hard disambiguation decisions that are not easily resolvable based on lim</context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>Preslav Nakov and Marti Hearst. 2005a. Search engine statistics beyond the n-gram: Application to noun compound bracketing. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Using the web as an implicit training set: Application to structural ambiguity resolution.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2899" citStr="Nakov and Hearst (2005" startWordPosition="473" endWordPosition="476">formation that is simply not present in the approximately 1M words on which the parser was trained. One way to access more information is to exploit surface counts from large corpora like the web (Volk, 2001; Lapata and Keller, 2004). For example, the phrase raising from is much more frequent on the Web than $ x billion from. While this ‚Äòaffinity‚Äô is only a surface correlation, Volk (2001) showed that comparing such counts can often correctly resolve tricky PP attachments. This basic idea has led to a good deal of successful work on disambiguating isolated, binary PP attachments. For example, Nakov and Hearst (2005b) showed that looking for paraphrase counts can further improve PP resolution. In this case, the existence of reworded phrases like raising it from on the Web also imply a verbal at$ 30 billion from debt ... VBG NP ... raising NP PP 693 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 693‚Äì702, Portland, Oregon, June 19-24, 2011. cÔøΩ2011 Association for Computational Linguistics ...Lehman Hutton Inc. by yesterday afternoon Figure 2: Different kinds of attachment errors in the parse output of the Berkeley parser (on Penn Treebank). Guess edges are in</context>
<context position="8039" citStr="Nakov and Hearst, 2005" startWordPosition="1296" endWordPosition="1299"> dependency parsing literature (see Figure 3). Here, we discuss which webcount based features 0(h, a) should fire over a given head-argument pair (we consider the words h and a to be indexed, and so features can be sensitive to their order and distance, as is also standard). 2.1 Affinity Features Affinity statistics, such as lexical co-occurrence counts from large corpora, have been used previously for resolving individual attachments at least as far back as Lauer (1995) for noun-compound bracketing, and later for PP attachment (Volk, 2001; Lapata and Keller, 2004) and coordination ambiguity (Nakov and Hearst, 2005b). The approach of Lauer (1995), for example, would be to take an ambiguous noun sequence like hydrogen ion exchange and compare the various counts (or associated conditional probabilities) of n-grams like hydrogen ion and hydrogen exchange. The attachment with the greater score is chosen. More recently, Pitler et al. (2010) use web-scale n-grams to compute similar association statistics for longer sequences of nouns. Our affinity features closely follow this basic idea of association statistics. However, because a real parser will not have access to gold-standard knowledge of the competing a</context>
<context position="11297" citStr="Nakov and Hearst (2005" startWordPosition="1853" endWordPosition="1856">number of wildcards allowed in the query is proportional to the binned distance between h and a in the sentence. Finally, we also include unigram variants of the above features, which are sensitive to only one of the head or argument. For all features used, we add cumulative variants where indicators are fired for all count bins bÔøΩ up to query count bin b. 2.2 Paraphrase Features In addition to measuring counts of the words present in the sentence, there exist clever ways in which paraphrases and other accidental indicators can help resolve specific ambiguities, some of which are discussed in Nakov and Hearst (2005a), Nakov and Hearst (2005b). For example, finding attestations of eat: spaghetti with sauce suggests a nominal attachment in Jean ate spaghetti with sauce. As another example, one clue that the example in Figure 1 is ùùì(raising from) ùùì($ from) raising $ from debt 695 a verbal attachment is that the proform paraphrase raising it from is commonly attested. Similarly, the attestation of be noun prep suggests nominal attachment. These paraphrase features hint at the correct attachment decision by looking for web n-grams with special contexts that reveal syntax superficially. Again, while effective</context>
<context position="14370" citStr="Nakov and Hearst, 2005" startWordPosition="2400" endWordPosition="2403"> from indicated a correct attachment between raising and from, frequent occurrences of lowering it with will indicate the correctness of an attachment between lowering and with. Finally, to handle the cases where no induced context word is helpful, we also construct abstracted versions of these paraphrase features where the context words c are collapsed to their parts-of-speech POS(c), obtained using a unigram-tagger trained on the parser training set. As discussed in Section 5, the top features learned by our learning algorithm duplicate the hand-crafted configurations used in previous work (Nakov and Hearst, 2005b) but also add numerous others, and, of course, apply to many more attachment types. 3 Working with Web n-Grams Previous approaches have generally used search engines to collect count statistics (Lapata and Keller, 2004; Nakov and Hearst, 2005b; Nakov and Hearst, 2008). Lapata and Keller (2004) uses the number of page hits as the web-count of the queried ngram (which is problematic according to Kilgarriff (2007)). Nakov and Hearst (2008) post-processes the first 1000 result snippets. One challenge with this approach is that an external search API is now embedded into the parser, raising issue</context>
<context position="29449" citStr="Nakov and Hearst (2005" startWordPosition="5016" endWordPosition="5019"> mid-word schema for a verb head and preposition argument (with head on left of argument). cators of (possibly non-adjacent) attachments. A subset of paraphrase features, which in the automatically-extracted case don‚Äôt really correspond to paraphrases at all, are shown in Table 7. Here we show features for verbal heads and IN arguments. The mid-words m which rank highly are those where the occurrence of hma as an n-gram is a good indicator that a attaches to h (m of course does not have to actually occur in the sentence). Interestingly, the top such features capture exactly the intuition from Nakov and Hearst (2005b), namely that if the verb h and the preposition a occur with a pronoun in between, we have evidence that a attaches to h (it certainly can‚Äôt attach to the pronoun). However, we also see other indicators that the preposition is selected for by the verb, such as adverbs like directly. As another example of known useful features being learned automatically, Table 8 shows the previous-context-word paraphrase features for a noun head and preposition argument (N ‚Äî* IN). Nakov and Hearst (2005b) suggested that the attestation of be N IN is a good indicator of attachment to the noun (the IN cannot g</context>
<context position="32877" citStr="Nakov and Hearst (2005" startWordPosition="5627" endWordPosition="5630">ring VBD and RB named, now VBP and VBP offer, need Table 9: The highest-weight features (thresholded at a count of 400) of the mid-word schema where the mid-word was a conjunction. For variety, for a given head-argument POS pair, we only list features corresponding to the and conjunction and h --+ a direction. trigram the a h is frequent ‚Äì in that trigram, the adjective attaches to the noun. The second entry (NN - NN) shows that one noun is a good modifier of another if they frequently appear together hyphenated (another punctuation-based cue mentioned in previous work on noun bracketing, see Nakov and Hearst (2005a)). While they were motivated on separate grounds, these features can also compensate for inapplicability of the affinity features. For example, the third entry (VBD this NN) is a case where even if the head (a VBD like adopted) actually selects strongly for the argument (a NN like plan), the bigram adopted plan may not be as frequent as expected, because it requires a determiner in its minimal analogous form adopted the plan. 6 Conclusion Web features are a way to bring evidence from a large unlabeled corpus to bear on hard disambiguation decisions that are not easily resolvable based on lim</context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>Preslav Nakov and Marti Hearst. 2005b. Using the web as an implicit training set: Application to structural ambiguity resolution. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Solving relational similarity problems using the web as a corpus.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="14640" citStr="Nakov and Hearst, 2008" startWordPosition="2444" endWordPosition="2447">stracted versions of these paraphrase features where the context words c are collapsed to their parts-of-speech POS(c), obtained using a unigram-tagger trained on the parser training set. As discussed in Section 5, the top features learned by our learning algorithm duplicate the hand-crafted configurations used in previous work (Nakov and Hearst, 2005b) but also add numerous others, and, of course, apply to many more attachment types. 3 Working with Web n-Grams Previous approaches have generally used search engines to collect count statistics (Lapata and Keller, 2004; Nakov and Hearst, 2005b; Nakov and Hearst, 2008). Lapata and Keller (2004) uses the number of page hits as the web-count of the queried ngram (which is problematic according to Kilgarriff (2007)). Nakov and Hearst (2008) post-processes the first 1000 result snippets. One challenge with this approach is that an external search API is now embedded into the parser, raising issues of both speed and daily query limits, especially if all possible attachments trigger queries. Such methods also create a dependence on the quality and postprocessing of the search results, limitations of the query process (for instance, search engines can ignore punct</context>
</contexts>
<marker>Nakov, Hearst, 2008</marker>
<rawString>Preslav Nakov and Marti Hearst. 2008. Solving relational similarity problems using the web as a corpus. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning Accurate, Compact, and Interpretable Tree Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="887" citStr="Petrov et al. (2006)" startWordPosition="132" endWordPosition="135">counts to help resolve isolated ambiguities, such as binary noun-verb PP attachments and noun compound bracketings. In this work, we first present a method for generating web count features that address the full range of syntactic attachments. These features encode both surface evidence of lexical affinities as well as paraphrase-based cues to syntactic structure. We then integrate our features into full-scale dependency and constituent parsers. We show relative error reductions of 7.0% over the second-order dependency parser of McDonald and Pereira (2006), 9.2% over the constituent parser of Petrov et al. (2006), and 3.4% over a non-local constituent reranker. 1 Introduction Current state-of-the art syntactic parsers have achieved accuracies in the range of 90% F1 on the Penn Treebank, but a range of errors remain. From a dependency viewpoint, structural errors can be cast as incorrect attachments, even for constituent (phrase-structure) parsers. For example, in the Berkeley parser (Petrov et al., 2006), about 20% of the errors are prepositional phrase attachment errors as in Figure 1, where a preposition-headed (IN) phrase was assigned an incorrect parent in the implied dependency tree. Here, the Be</context>
<context position="4649" citStr="Petrov et al., 2006" startWordPosition="764" endWordPosition="767">, we show how to apply these ideas to all attachments in full-scale parsing. Doing so requires three main issues to be addressed. First, we show how features can be generated for arbitrary head-argument configurations. Affinity features are relatively straightforward, but paraphrase features, which have been hand-developed in the past, are more complex. Second, we integrate our features into full-scale parsing systems. For dependency parsing, we augment the features in the second-order parser of McDonald and Pereira (2006). For constituent parsing, we rerank the output of the Berkeley parser (Petrov et al., 2006). Third, past systems have usually gotten their counts from web search APIs, which does not scale to quadratically-many attachments in each sentence. Instead, we consider how to efficiently mine the Google n-grams corpus. Given the success of Web counts for isolated ambiguities, there is relatively little previous research in this direction. The most similar work is Pitler et al. (2010), which use Web-scale n-gram counts for multi-way noun bracketing decisions, though that work considers only sequences of nouns and uses only affinity-based web features. Yates et al. (2006) use Web counts to fi</context>
<context position="21248" citStr="Petrov et al., 2006" startWordPosition="3591" endWordPosition="3594">sults of McDonald and Pereira (2006).6 Table 1 shows unlabeled attachments scores (UAS) for their second-order projective parser and the improved numbers resulting from the addition of our Web-scale features. Our first-order web-scale features show significant improvement even over their non-local second-order features.7 Additionally, our web-scale features are at least an order of magnitude fewer in number than even their first-order base features. 4.2 Constituent Parsing We also evaluate the utility of web-scale features on top of a state-of-the-art constituent parser ‚Äì the Berkeley parser (Petrov et al., 2006), an unlexicalized phrase-structure parser. Because the underlying parser does not factor along lexical attachments, we instead adopt the discriminative reranking framework, where we generate the top-k candidates from the baseline system and then rerank this k-best list using (generally non-local) features. Our baseline system is the Berkeley parser, from which we obtain k-best lists for the development set (WSJ section 22) and test set (WSJ section 23) using a grammar trained on all the training data (WSJ sections 2-21).8 To get k-best lists for the training set, we use 3-fold jackknifing whe</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning Accurate, Compact, and Interpretable Tree Annotation. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
</authors>
<title>Using web-scale n-grams to improve base NP parsing performance.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="3943" citStr="Pitler et al., 2010" startWordPosition="654" endWordPosition="657">cs ...Lehman Hutton Inc. by yesterday afternoon Figure 2: Different kinds of attachment errors in the parse output of the Berkeley parser (on Penn Treebank). Guess edges are in solid blue, gold edges are in dashed gold and edges common in guess and gold parses are in black. PDT ... ·øΩs a dozen newspapers (a) S (b) NP (c) VP NP VP QP VBZ NP PP ADJP PDT DT PDT had already ... ... half ADVP RB just ADJP JJ fine tachment. Still other work has exploited Web counts for other isolated ambiguities, such as NP coordination (Nakov and Hearst, 2005b) and noun-sequence bracketing (Nakov and Hearst, 2005a; Pitler et al., 2010). For example, in (b), half dozen is more frequent than half newspapers. In this paper, we show how to apply these ideas to all attachments in full-scale parsing. Doing so requires three main issues to be addressed. First, we show how features can be generated for arbitrary head-argument configurations. Affinity features are relatively straightforward, but paraphrase features, which have been hand-developed in the past, are more complex. Second, we integrate our features into full-scale parsing systems. For dependency parsing, we augment the features in the second-order parser of McDonald and </context>
<context position="8366" citStr="Pitler et al. (2010)" startWordPosition="1348" endWordPosition="1351">xical co-occurrence counts from large corpora, have been used previously for resolving individual attachments at least as far back as Lauer (1995) for noun-compound bracketing, and later for PP attachment (Volk, 2001; Lapata and Keller, 2004) and coordination ambiguity (Nakov and Hearst, 2005b). The approach of Lauer (1995), for example, would be to take an ambiguous noun sequence like hydrogen ion exchange and compare the various counts (or associated conditional probabilities) of n-grams like hydrogen ion and hydrogen exchange. The attachment with the greater score is chosen. More recently, Pitler et al. (2010) use web-scale n-grams to compute similar association statistics for longer sequences of nouns. Our affinity features closely follow this basic idea of association statistics. However, because a real parser will not have access to gold-standard knowledge of the competing attachment sites (see Atterer and Schutze (2007)‚Äôs criticism of previous work), we must instead compute features for all possible head-argument pairs from our web corpus. Moreover, when there are only two competing attachment options, one can do things like directly compare two count-based heuristics and choose the larger. Int</context>
</contexts>
<marker>Pitler, Bergsma, Lin, 2010</marker>
<rawString>Emily Pitler, Shane Bergsma, Dekang Lin, , and Kenneth Church. 2010. Using web-scale n-grams to improve base NP parsing performance. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="19734" citStr="Ratnaparkhi, 1996" startWordPosition="3362" endWordPosition="3363">r features contribute on top of standard reranking features.3 and h a *; these are handled similarly. 4.1 Dependency Parsing For dependency parsing, we use the discriminatively-trained MSTParser4, an implementation of first and second order MST parsing models of McDonald et al. (2005) and McDonald and Pereira (2006). We use the standard splits of Penn Treebank into training (sections 2-21), development (section 22) and test (section 23). We used the ‚Äòpennconverter‚Äô5 tool to convert Penn trees from constituent format to dependency format. Following Koo et al. (2008), we used the MXPOST tagger (Ratnaparkhi, 1996) trained on the full training data to provide part-of-speech tags for the development 3All reported experiments are run on all sentences, i.e. without any length limit. 4http://sourceforge.net/projects/mstparser 5This supersedes ‚ÄòPenn2Malt‚Äô and is available at http://nlp.cs.lth.se/software/treebank converter. We follow its recommendation to patch WSJ data with NP bracketing by Vadas and Curran (2007). 697 Order 2 + Web features % Error Redn. Dev (sec 22) 92.1 92.7 7.6% Test (sec 23) 91.4 92.0 7.0% Table 1: UAS results for English WSJ dependency parsing. Dev is WSJ section 22 (all sentences) an</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Dependency parsing by belief propagation.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="22114" citStr="Smith and Eisner (2008)" startWordPosition="3725" endWordPosition="3728">hen rerank this k-best list using (generally non-local) features. Our baseline system is the Berkeley parser, from which we obtain k-best lists for the development set (WSJ section 22) and test set (WSJ section 23) using a grammar trained on all the training data (WSJ sections 2-21).8 To get k-best lists for the training set, we use 3-fold jackknifing where we train a grammar 6Their README specifies ‚Äòtraining-k:5 iters:10 losstype:nopunc decode-type:proj‚Äô, which we used for all final experiments; we used the faster ‚Äòtraining-k:1 iters:5‚Äô setting for most development experiments. 7Work such as Smith and Eisner (2008), Martins et al. (2009), Koo and Collins (2010) has been exploring more nonlocal features for dependency parsing. It will be interesting to see how these features interact with our web features. 8Settings: 6 iterations of split and merge with smoothing. k = 1 k = 2 k = 10 k = 25 k = 50 k = 100 Dev 90.6 92.3 95.1 95.8 96.2 96.5 Test 90.2 91.8 94.7 95.6 96.1 96.4 Table 2: Oracle F1-scores for k-best lists output by Berkeley parser for English WSJ parsing (Dev is section 22 and Test is section 23, all lengths). on 2 folds to get parses for the third fold.9 The oracle scores of the k-best lists (f</context>
</contexts>
<marker>Smith, Eisner, 2008</marker>
<rawString>David A. Smith and Jason Eisner. 2008. Dependency parsing by belief propagation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James R Curran</author>
</authors>
<title>Adding noun phrase structure to the Penn Treebank.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="20137" citStr="Vadas and Curran (2007)" startWordPosition="3413" endWordPosition="3416">lopment (section 22) and test (section 23). We used the ‚Äòpennconverter‚Äô5 tool to convert Penn trees from constituent format to dependency format. Following Koo et al. (2008), we used the MXPOST tagger (Ratnaparkhi, 1996) trained on the full training data to provide part-of-speech tags for the development 3All reported experiments are run on all sentences, i.e. without any length limit. 4http://sourceforge.net/projects/mstparser 5This supersedes ‚ÄòPenn2Malt‚Äô and is available at http://nlp.cs.lth.se/software/treebank converter. We follow its recommendation to patch WSJ data with NP bracketing by Vadas and Curran (2007). 697 Order 2 + Web features % Error Redn. Dev (sec 22) 92.1 92.7 7.6% Test (sec 23) 91.4 92.0 7.0% Table 1: UAS results for English WSJ dependency parsing. Dev is WSJ section 22 (all sentences) and Test is WSJ section 23 (all sentences). The order 2 baseline represents McDonald and Pereira (2006). and the test set, and we used 10-way jackknifing to generate tags for the training set. We added our first-order Web-scale features to the MSTParser system to evaluate improvement over the results of McDonald and Pereira (2006).6 Table 1 shows unlabeled attachments scores (UAS) for their second-orde</context>
</contexts>
<marker>Vadas, Curran, 2007</marker>
<rawString>David Vadas and James R. Curran. 2007. Adding noun phrase structure to the Penn Treebank. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Volk</author>
</authors>
<title>Exploiting the WWW as a corpus to resolve PP attachment ambiguities.</title>
<date>2001</date>
<booktitle>In Proceedings of Corpus Linguistics.</booktitle>
<contexts>
<context position="2484" citStr="Volk, 2001" startWordPosition="406" endWordPosition="407">lid blue, gold edges are in dashed gold and edges common in guess and gold parses are in black. attachment ambiguity where by yesterday afternoon should attach to had already, (b) is an NP-internal ambiguity where half a should attach to dozen and not to newspapers, and (c) is an adverb attachment ambiguity, where just should modify fine and not the verb ‚Äôs. Resolving many of these errors requires information that is simply not present in the approximately 1M words on which the parser was trained. One way to access more information is to exploit surface counts from large corpora like the web (Volk, 2001; Lapata and Keller, 2004). For example, the phrase raising from is much more frequent on the Web than $ x billion from. While this ‚Äòaffinity‚Äô is only a surface correlation, Volk (2001) showed that comparing such counts can often correctly resolve tricky PP attachments. This basic idea has led to a good deal of successful work on disambiguating isolated, binary PP attachments. For example, Nakov and Hearst (2005b) showed that looking for paraphrase counts can further improve PP resolution. In this case, the existence of reworded phrases like raising it from on the Web also imply a verbal at$ 3</context>
<context position="7962" citStr="Volk, 2001" startWordPosition="1285" endWordPosition="1286">s factored over head-argument pairs. pairs, as is standard in the dependency parsing literature (see Figure 3). Here, we discuss which webcount based features 0(h, a) should fire over a given head-argument pair (we consider the words h and a to be indexed, and so features can be sensitive to their order and distance, as is also standard). 2.1 Affinity Features Affinity statistics, such as lexical co-occurrence counts from large corpora, have been used previously for resolving individual attachments at least as far back as Lauer (1995) for noun-compound bracketing, and later for PP attachment (Volk, 2001; Lapata and Keller, 2004) and coordination ambiguity (Nakov and Hearst, 2005b). The approach of Lauer (1995), for example, would be to take an ambiguous noun sequence like hydrogen ion exchange and compare the various counts (or associated conditional probabilities) of n-grams like hydrogen ion and hydrogen exchange. The attachment with the greater score is chosen. More recently, Pitler et al. (2010) use web-scale n-grams to compute similar association statistics for longer sequences of nouns. Our affinity features closely follow this basic idea of association statistics. However, because a r</context>
</contexts>
<marker>Volk, 2001</marker>
<rawString>Martin Volk. 2001. Exploiting the WWW as a corpus to resolve PP attachment ambiguities. In Proceedings of Corpus Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
<author>Stefan Schoenmackers</author>
<author>Oren Etzioni</author>
</authors>
<title>Detecting parser errors using web-based semantic filters.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="5228" citStr="Yates et al. (2006)" startWordPosition="853" endWordPosition="856">he Berkeley parser (Petrov et al., 2006). Third, past systems have usually gotten their counts from web search APIs, which does not scale to quadratically-many attachments in each sentence. Instead, we consider how to efficiently mine the Google n-grams corpus. Given the success of Web counts for isolated ambiguities, there is relatively little previous research in this direction. The most similar work is Pitler et al. (2010), which use Web-scale n-gram counts for multi-way noun bracketing decisions, though that work considers only sequences of nouns and uses only affinity-based web features. Yates et al. (2006) use Web counts to filter out certain ‚Äòsemantically bad‚Äô parses from extraction candidate sets but are not concerned with distinguishing amongst top parses. In an important contrast, Koo et al. (2008) smooth the sparseness of lexical features in a discriminative dependency parser by using clusterbased word-senses as intermediate abstractions in addition to POS tags (also see Finkel et al. (2008)). Their work also gives a way to tap into corpora beyond the training data, through cluster membership rather than explicit corpus counts and paraphrases. This work uses a large web-scale corpus (Googl</context>
</contexts>
<marker>Yates, Schoenmackers, Etzioni, 2006</marker>
<rawString>Alexander Yates, Stefan Schoenmackers, and Oren Etzioni. 2006. Detecting parser errors using web-based semantic filters. In Proceedings of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>