<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000617">
<title confidence="0.998523">
Automatic identification of sentiment vocabulary: exploiting low associa-
tion with known sentiment terms
</title>
<author confidence="0.984466">
Michael Gamon Anthony Aue
</author>
<affiliation confidence="0.958673">
Natural Language Processing Group Natural Language Processing Group
Microsoft Research Microsoft Research
</affiliation>
<email confidence="0.997416">
mgamon@microsoft.com anthaue@microsoft.com
</email>
<sectionHeader confidence="0.998586" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999987565217391">
We describe an extension to the technique
for the automatic identification and label-
ing of sentiment terms described in Tur-
ney (2002) and Turney and Littman
(2002). Their basic assumption is that
sentiment terms of similar orientation
tend to co-occur at the document level.
We add a second assumption, namely that
sentiment terms of opposite orientation
tend not to co-occur at the sentence level.
This additional assumption allows us to
identify sentiment-bearing terms very re-
liably. We then use these newly identified
terms in various scenarios for the senti-
ment classification of sentences. We show
that our approach outperforms Turney’s
original approach. Combining our ap-
proach with a Naive Bayes bootstrapping
method yields a further small improve-
ment of classifier performance. We finally
compare our results to precision and recall
figures that can be obtained on the same
data set with labeled data.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999953225">
The field of sentiment classification has received
considerable attention from researchers in recent
years (Pang and Lee 2002, Pang et al. 2004, Tur-
ney 2002, Turney and Littman 2002, Wiebe et al.
2001, Bai et al. 2004, Yu and Hatzivassiloglou
2003 and many others). The identification and
classification of sentiment constitutes a problem
that is orthogonal to the usual task of text classifi-
cation. Whereas in traditional text classification the
focus is on topic identification, in sentiment classi-
fication the focus is on the assessment of the
writer’s sentiment toward the topic.
Movie and product reviews have been the main
focus of many of the recent studies in this area
(Pang and Lee 2002, Pang et al. 2004, Turney
2002, Turney and Littman 2002). Typically, these
reviews are classified at the document level, and
the class labels are “positive” and “negative”. In
this work, in contrast, we narrow the scope of in-
vestigation to the sentence level and expand the set
of labels, making a threefold distinction between
“positive”, “neutral”, and “negative”. The narrow-
ing of scope is motivated by the fact that for realis-
tic text mining on customer feedback, the
document level is too coarse, as described in Ga-
mon et al. (2005). The expansion of the label set is
also motivated by real-world concerns; while it is a
given that review text expresses positive or nega-
tive sentiment, in many cases it is necessary to also
identify the cases that don’t carry strong expres-
sions of sentiment at all.
Traditional approaches to text classification re-
quire large amounts of labeled training data. Ac-
quisition of such data can be costly and time-
consuming. Due to the highly domain-specific na-
ture of the sentiment classification task, moving
from one domain to another typically requires the
acquisition of a new set of training data. For this
reason, unsupervised or very weakly supervised
methods for sentiment classification are especially
</bodyText>
<page confidence="0.987127">
57
</page>
<note confidence="0.993119">
Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 57–64,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.98825">
desirable.1 Our focus, therefore, is on methods that
require very little data annotation.
We describe a method to automatically identify
the sentiment vocabulary in a domain. This method
rests on three special properties of the sentiment
domain:
</bodyText>
<listItem confidence="0.9968116">
1. the presence of certain words can serve as
a proxy for the class label
2. sentiment terms of similar orientation tend
to co-occur
3. sentiment terms of opposite orientation
</listItem>
<bodyText confidence="0.960955810810811">
tend to not co-occur at the sentence level.
Turney (2002) and Turney and Littman (2002)
exploit the first two generalizations for unsuper-
vised sentiment classification of movie reviews.
They use the two terms excellent and poor as seed
terms to determine the semantic orientation of
other terms. These seed terms can be viewed as
proxies for the class labels “positive” and “nega-
tive”, allowing for the exploitation of otherwise
unlabeled data: Terms that tend to co-occur with
excellent in documents tend to be of positive orien-
tation, and vice versa for poor. Turney (2002)
starts from a small (2 word) set of terms with
known orientation (excellent and poor). Given a set
of terms with unknown sentiment orientation, Tur-
ney (2002) then uses the PMI-IR algorithm (Tur-
ney 2001) to issue queries to the web and
determine, for each of these terms, its pointwise
mutual information (PMI) with the two seed words
across a large set of documents. Term candidates
are constrained to be adjectives, which tend to be
the strongest bearers of sentiment. The sentiment
orientation (SO) of a term is then determined by
the difference between its association (PMI) with
the positive seed term excellent and its association
with the negative seed term poor. The resulting list
of terms and associated sentiment orientations can
then be used to implement a classifier: semantic
orientation of the terms in a document of unknown
sentiment is added up, and if the overall score is
positive, the document is classified as being of
positive sentiment, otherwise it is classified as
negative.
Yu and Hatzivassiloglou (2003) extend this ap-
proach by (1) applying it at the sentence level (in-
stead of the document-level), (2) taking into
account non-adjectival parts-of-speech, and (3)
</bodyText>
<footnote confidence="0.50394">
1 For domain-specificity of sentiment classification see Eng-
ström (2004) and Aue and Gamon (2005).
</footnote>
<bodyText confidence="0.96457975">
using larger sets of seed words. Their classification
goal also differs from Turney’s: it is to distinguish
opinion sentences from factual statements.
Turney et al.’s approach is based on the assump-
tion that sentiment terms of similar orientation tend
to co-occur in documents. Our approach takes ad-
vantage of a second assumption: At the sentence
level, sentiment terms of opposite orientation tend
not to co-occur. This is, of course, an assumption
that will only hold in general, with exceptions. Ba-
sically, the assumption is that sentences of the fol-
lowing form:
I dislike X.
I really like X.
are more frequent than “mixed sentiment” sen-
tences such as
I dislike X but I really like Y.
It has been our experience that this generaliza-
tion does hold often enough to be useful.
We propose to utilize this assumption to identify
a set of sentiment terms in a domain. We select the
terms that have the lowest PMI scores on the sen-
tence level with respect to a set of manually se-
lected seed words. If our assumption about low
association at the sentence level is correct, this set
of low-scoring terms will be particularly rich in
sentiment terms. We can then use this newly iden-
tified set to:
</bodyText>
<listItem confidence="0.935968714285714">
(1) use Turney’s method to find the orienta-
tion for the terms and employ the terms
and their scores in a classifier, and
(2) use Turney’s method to find the orienta-
tion for the terms and add the new terms
as additional seed terms for a second it-
eration
</listItem>
<bodyText confidence="0.9998465">
As opposed to Turney (2002), we do not use the
web as a resource to find associations, rather we
apply the method directly to in-domain data. This
has the disadvantage of not being able to apply the
classification to any arbitrary domain. It is worth
noting, however, that even in Turney (2002) the
choice of seed words is explicitly motivated by
domain properties of movie reviews.
In the remainder of the paper we will describe
results from various experiments based on this as-
sumption. We also show how we can combine this
method with a Naive Bayes bootstrapping ap-
proach that takes further advantage of the unla-
beled data (Nigam et al. 2000).
</bodyText>
<page confidence="0.998946">
58
</page>
<sectionHeader confidence="0.994063" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.9999935">
For our experiments we used a set of car reviews
from the MSN Autos web site. The data consist of
406,818 customer car reviews written over a four-
year period. Aside from filtering out examples con-
taining profanity, the data was not edited. The re-
views range in length from a single sentence (56%
of all cases) to 50 sentences (a single review). Less
than 1% of reviews contain ten or more sentences.
There are almost 900,000 sentences in total. When
customers submitted reviews to the website, they
were asked for a recommendation on a scale of 1
(negative) to 10 (positive). The average score was
very high, at 8.3, yielding a strong skew in favor of
positive class labels. We annotated a randomly-
selected sample of 3,000 sentences for sentiment.
Each sentence was viewed in isolation and classi-
fied as positive, negative or neutral. The neutral
category was applied to sentences with no dis-
cernible sentiment, as well as to sentences that ex-
pressed both positive and negative sentiment.
Three annotators had pair-wise agreement scores
(Cohen’s Kappa score, Cohen 1960) of 70.10%,
71.78% and 79.93%, suggesting that the task of
sentiment classification on the sentence level is
feasible but difficult even for people. This set of
data was split into a development test set of 400
sentences and a blind test set of 2600 sentences.
Sentences are represented as vectors of binary
unigram features. The total number of observed
unigram features is 72988. In order to restrict the
number of features to a manageable size, we disre-
gard features that occur less than 10 times in the
corpus. With this restriction we obtain a reduced
feature set of 13317 features.
</bodyText>
<sectionHeader confidence="0.996997" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.99019880952381">
Our experiments were performed as follows: We
started with a small set of manually-selected and
annotated seed terms. We used 4 positive and 6
negative seed terms. We decided to use a few more
negative seed words because of the inherent posi-
tive skew in the data that makes the identification
of negative sentences particularly hard. The terms
we used are:
positive: negative:
good bad
excellent lousy
love terrible
happy hate
suck
unreliable
There was no tuning of the set of initial seed
terms; the 10 words were originally chosen intui-
tively, as words that we observed frequently when
manually inspecting the data.
We then used these seed terms in two basic
ways: (1) We used them as seeds for a Turney-
style determination of the semantic orientation of
words in the corpus (semantic orientation, or SO
method). As mentioned above, this process is
based on the assumption that terms of similar ori-
entation tend to co-occur. (2) We used them to
mine sentiment vocabulary from the unlabeled data
using the additional assumption that sentiment
terms of opposite orientation tend not to co-occur
at the sentence level (sentiment mining, or SM
method). This method yields a set of sentiment
terms, but no orientation for that set of terms. We
continue by using the SO method to find the se-
mantic orientation for this set of sentiment terms,
effectively using SM as a feature selection method
for sentiment terminology.
Pseudo-code for the SO and SM approaches is
provided in Figure 1 and Figure 2. As a first step
for both SO and SM methods (not shown in the
pseudocode), PMI needs to be calculated for each
pair (f, s) of feature f and seed word s over the col-
lection of feature vectors.
</bodyText>
<figureCaption confidence="0.954923">
Figure 1: SO method for determining semantic orienta-
tion
</figureCaption>
<page confidence="0.986533">
59
</page>
<figureCaption confidence="0.999577">
Figure 2: SM method for mining sentiment terms
</figureCaption>
<bodyText confidence="0.9917154">
In the first scenario (using straightforward SO),
features F range over all observed features in the
data (modulo the aforementioned count cutoff of
10). In the second scenario (SM + SO), features F
range over the n% of features with the lowest PMI
scores with respect to any of the seed words that
were identified using the sentiment mining tech-
nique in Figure 2.
The result of both SO and SM+SO is a list of
unigram features which have an associated seman-
tic orientation score, indicating their sentiment ori-
entation: the higher the score, the more “positive”
a term, and vice versa.
This list of features and associated scores can be
used to construct a simple classifier: for each sen-
</bodyText>
<figureCaption confidence="0.724095333333333">
tence with unknown sentiment, we take the sum of
the semantic orientation scores for all of the uni-
grams in that sentence. This overall score deter-
mines the classification of the sentence as
“positive”, “neutral” or “negative” as shown in
Figure 3.
</figureCaption>
<bodyText confidence="0.502323">
Scoring and classifying sentence vectors:
</bodyText>
<listItem confidence="0.97798725">
(1) assigning a sentence score:
FOREACH feature f in sentence vector v:
Score(v) = Score(v) + SO(f)
(2) assigning a class label based on the sentence score:
</listItem>
<equation confidence="0.986067166666667">
IF Score(v) &gt; threshold1:
Class(v) = “positive”
ELSE IF Score(v) &lt; threshold1 AND Score(v) &gt; threshold2:
Class(v) = “neutral”
ELSE
Class(v) = “negative”
</equation>
<figureCaption confidence="0.986961">
Figure 3: Using SO scores for sentence scoring and
classification
</figureCaption>
<bodyText confidence="0.999963285714286">
The two thresholds used in classification need to
be determined empirically by taking the distribu-
tion of class values in the corpus into account. For
our experiments we simply took the distribution of
class labels in the 400 sentence development test
set as an approximation of the overall class label
distribution: we determined that distribution to be
15.5% for negative sentences, 21.5% for neutral
sentences, and 63.0% for positive sentences.
Scores for all sentence vectors in the corpus are
then collected using the scoring part of the algo-
rithm in Figure 3. The scores are sorted and the
thresholds are determined as the cutoffs for the top
63% and bottom 15.5% of scores respectively.
</bodyText>
<sectionHeader confidence="0.999987" genericHeader="method">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.997817">
4.1. Comparing SO and SM+SO
</subsectionHeader>
<bodyText confidence="0.9990395">
In our first set of experiments we manipulated the
following parameters:
</bodyText>
<listItem confidence="0.83600525">
1. the choice of SO or SM+SO method
2. the choice of n when selecting the n% se-
mantic terms with lowest PMI score in the
SM method
</listItem>
<bodyText confidence="0.999704636363636">
The tables below show the results of classifying
sentence vectors using the unigram features and
associated scores produced by SO and SO+SM.
We used the 2,600-sentence manually-annotated
test set described previously to establish these
numbers. Since the data exhibit a strong skew in
favor of the positive class label, we measure per-
formance not in terms of accuracy but in terms of
average precision and recall across the three class
labels, as suggested in (Manning and Schütze
2002).
</bodyText>
<table confidence="0.990011">
Avg precision Avg recall
SO 0.4481 0.4511
</table>
<tableCaption confidence="0.999796">
Table 1: Using the SO approach.
</tableCaption>
<bodyText confidence="0.9950354">
Table 1 shows results of using the SO method
on the data. Table 2 presents the results of combin-
ing the SM and SO methods for different values of
n. The best results are shown in boldface.
As a comparison between Table 1 and Table 2
shows, the highest average precision and recall
scores were obtained by combining the SM and SO
methods. Using SM as a feature selection mecha-
nism also reduces the number of features signifi-
cantly. While the SO method employed on
sentence-level vectors uses 13,000 features, the
best-performing SM+SO combination uses only
20% of this feature set, indicating that SM is in-
deed effective in selecting the most important sen-
timent-bearing terms.
</bodyText>
<page confidence="0.992531">
60
</page>
<bodyText confidence="0.990466">
tures by taking the top features according to abso-
lute score, average precision is at 0.4445 and
average recall at 0.4464.
We also determined that the positive impact of
SM is not just a matter of reducing the number of
features. If SO - without the SM feature selection
step - is reduced to a comparable number of fea-
</bodyText>
<table confidence="0.99844625">
N=10 N=20 N=30 N=40 N=50
Avg Avg Avg Avg Avg Avg Avg Avg Avg Avg
prec rec prec rec prec rec prec rec prec rec
SM+SO 0.4351 0.4377 0.4568 0.4605 0.4528 0.4557 0.4457 0.4478 0.4451 0.4475
SO from
docu-
ment
level
</table>
<tableCaption confidence="0.999864">
Table 2: combining SM and SO.
</tableCaption>
<bodyText confidence="0.968181">
Sentiment terms in top 100 SM terms Sentiment terms in top 100 SO terms
excellent, terrible, broke, junk, alright, bargain, excellent, happy, stylish, sporty, smooth, love,
grin, highest, exceptional, exceeded, horrible, quiet, overall, pleased, plenty, dependable, solid,
loved, waste, ok, death, leaking, outstanding, roomy, safe, good, easy, smaller, luxury, comfort-
cracked, rebate, warped, hooked, sorry, refuses, able, style, loaded, space, classy, handling, joy,
excellant, satisfying, died, biggest, competitive, small, comfort, size, perfect, performance, room,
delight, avoid, awful, garbage, loud, okay, com- choice, recommended, package, compliments,
petent, upscale, dated, mistake, sucks, superior, awesome, unique, fun, holds, comfortably, ex-
high, kill, neither tremely, value, free, satisfied, little, recommend,
limited, great, pleasure
Non sentiment terms in top 100 SM terms Non sentiment terms in top 100 SO terms
alternative, wont, below, surprisingly, main- condition, very, handles, milage, definitely, defi-
tained, choosing, comparing, legal, vibration, nately, far, drives, shape, color, price, provides,
seemed, claim, demands, assistance, knew, engi- options, driving, rides, sports, heated, ride, sport,
neering, accelleration, ended, salesperson, per- forward, expected, fairly, anyone, test, fits, stor-
formed, started, midsize, site, gonna, lets, plugs, age, range, family, sedan, trunk, young, weve,
industry, alternator, month, told, vette, 180, black, college, suv, midsize, coupe, 30, shopping,
powertrain, write, mos, walk, causing, lift, es, kids, player, saturn, bose, truck, town, am, leather,
segment, $250, 300m, wanna, february, mod, stereo, car, husband
$50, nhtsa, suburbans, manufactured, tiburon,
$10, f150, 5000, posted, tt, him, saw, jan,
</bodyText>
<tableCaption confidence="0.986521">
Table 3: the top 100 terms identified by SM and SO
</tableCaption>
<bodyText confidence="0.99900625">
Table 3 shows the top 100 terms that were identi-
fied by each SM and SO methods. The terms are
categorized into sentiment-bearing and non-
sentiment bearing terms by human judgment. The
two sets seem to differ in both strength and orien-
tation of the identified terms. The SM-identified
words have a higher density of negative terms (22
out of 43 versus 2 out of 49 for the SO-identified
terms). The SM-identified terms also express sen-
timent more strongly, but this conclusion is more
tentative since it may be a consequence of the
higher density of negative terms.
</bodyText>
<subsectionHeader confidence="0.7045705">
4.2. Multiple iterations: increasing the
number of seed features by SM+SO
</subsectionHeader>
<bodyText confidence="0.9150525">
In a second set of experiments, we assessed the
question of whether it is possible to use multiple
iterations of the SM+SO method to gradually build
the list of seed words. We do this by adding the top
n% of features selected by SM, along with their
orientation as determined by SO, to the initial set
of seed words. The procedure for this round of ex-
periments is as follows:
• take the top n% of features identified by
SM (we used n=1 for the reported re-
</bodyText>
<page confidence="0.99846">
61
</page>
<bodyText confidence="0.993146666666667">
sults, since preliminary experiments
with other values for n did not improve
results)
</bodyText>
<listItem confidence="0.992939333333333">
• perform SO for these features to deter-
mine their orientation
• take the top 15.5% negative and top
</listItem>
<bodyText confidence="0.998300928571429">
63% positive (according to class label
distribution in the development test set)
of the features and add them as nega-
tive/positive seed features respectively
This iteration increases the number of seed fea-
tures from the original 10 manually-selected fea-
tures to a total of 111 seed features.
With this enhanced set of seed features we then
re-ran a subset of the experiments in Table 2. Re-
sults are shown in Table 4. Increasing the number
of seed features through the SM feature selection
method increases precision and recall by several
percentage points. In particular, precision and re-
call for negative sentences are boosted.
</bodyText>
<table confidence="0.9995585">
Avg Avg
precision recall
SM + SO, n=10, 0.4826 0.48.76
SO from document vectors
SM + SO, n=30, 0.4957 0.4995
SO from document vectors
SM + SO, n=50, 0.4914 0.4952
SO from document vectors
</table>
<tableCaption confidence="0.9751045">
Table 4: Using 2 iterations to increase the seed feature
set
</tableCaption>
<bodyText confidence="0.999906909090909">
We also confirmed that these results are truly at-
tributable to the use of the SM method for the first
iteration. If we take an equivalent number of fea-
tures with strongest semantic orientation according
to the SO method and add them to the list of seed
features, our results degrade significantly (the re-
sulting classifier performance is significantly dif-
ferent at the 99.9% level as established by the
McNemar test). This is further evidence that SM is
indeed an effective method for selecting sentiment
terms.
</bodyText>
<subsectionHeader confidence="0.999873">
4.3. Using the SO classifier to bootstrap a
Naive Bayes classifier
</subsectionHeader>
<bodyText confidence="0.889829222222222">
In a third set of experiments, we tried to improve
on the results of the SO classifier by combining it
with the bootstrapping approach described in (Ni-
gam et al. 2000). The basic idea here is to use the
SO classifier to label a subset of the data DL. This
labeled subset of the data is then used to bootstrap
a Naive Bayes (NB) classifier on the remaining
unlabeled data DU using the Expectation Maximi-
zation (EM) algorithm:
</bodyText>
<listItem confidence="0.9953594">
(1) An initial naive Bayes classifier with
parameters θ is trained on the docu-
ments in DL.
(2) This initial classifier is used to estimate
a probability distribution over all classes
for each of the documents in DU. (E-
Step)
(3) The labeled and unlabeled data are then
used to estimate parameters for a new
classifier. (M-Step)
</listItem>
<bodyText confidence="0.9998395">
Steps 2 and 3 are repeated until convergence is
achieved when the difference in the joint probabil-
ity of the data and the parameters falls below the
configurable threshold ε between iterations. An-
other free parameter, λ, can be used to control how
much weight is given to the unlabeled data.
For our experiments we used classifiers from the
best SM+SO combination (2 iterations at n=30)
from Table 4 above to label 30% of the total data.
Table 5 shows the average precision and recall
numbers for the converged NB classifier.2 In addi-
tion to improving average precision and recall, the
resulting classifier also has the advantage of pro-
ducing class probabilities instead of simple scores.3
</bodyText>
<table confidence="0.99845075">
Avg Avg
precision recall
Bootstrapped NB 0.5167 0.52
classifier
</table>
<tableCaption confidence="0.9718515">
Table 5: Results obtained by bootstrapping a NB classi-
fier
</tableCaption>
<subsectionHeader confidence="0.9551925">
4.4. Results from supervised learning:
using small sets of labeled data
</subsectionHeader>
<bodyText confidence="0.999984714285714">
Given infinite resources, we can always annotate
enough data to train a classifier using a supervised
algorithm that will outperform unsupervised or
weakly-supervised methods. Which approach to
take depends entirely on how much time and
money are available and on the accuracy require-
ments for the task at hand.
</bodyText>
<footnote confidence="0.9925568">
2 In this experiment, λ was set to 0.1 and ε was set to 0.05.
3 We also experimented with labeling the whole data set with the best of our SO
score classifiers, and then training a linear Support Vector Machine classifier on
the data. The results were considerably worse than any of the reported numbers,
so they are not included in this paper.
</footnote>
<page confidence="0.999037">
62
</page>
<bodyText confidence="0.998311727272727">
To help situate the precision and recall numbers
presented in the tables above, we trained Support
Vector Machines (SVMs) using small amounts of
labeled data. SVMs were trained with 500, 1000,
2000, and 2500 labeled sentences. Annotating
2500 sentences represents approximately eight per-
son-hours of work. The results can be found in Ta-
ble 5. We were pleasantly surprised at how well
the unsupervised classifiers described above per-
form in comparison to state-of-the-art supervised
methods (albeit trained on small amounts of data).
</bodyText>
<table confidence="0.9981605">
Labeled ex- Avg. Preci- Avg. Recall
amples sion
500 .4878 .4967
1000 .5161 .5105
2000 .5297 .5256
2500 .5017 .5083
</table>
<tableCaption confidence="0.9974815">
Table 6: Average precision and recall for SVMs for
small numbers of labeled examples
</tableCaption>
<sectionHeader confidence="0.608211" genericHeader="evaluation">
4.5. Results on the movie domain
</sectionHeader>
<subsectionHeader confidence="0.998622">
5.1 A note on statistical significance
</subsectionHeader>
<bodyText confidence="0.999516111111111">
combined with LLR-based feature reduction, to
achieve 91.4% accuracy. Using the Turney SO
method on in-domain data instead of web data
achieves 73.95% accuracy (using the same two
seed words that Turney does). Using one iteration
of SM+SO to increase the number of seed words,
followed by finding SO scores for all words with
respect to the enhanced seed word set, yields a
slightly higher accuracy of 74.85%. With addi-
tional parameter tuning, this number can be pushed
to 76.4%, at which point we achieve statistical sig-
nificance at the 0.95 level according to the McNe-
mar test, indicating that there is more room here
for improvement. Any reduction of the number of
overall features in this domain leads to decreased
accuracy, contrary to what we observed in the car
review domain. We attribute this observation to the
smaller data set.
</bodyText>
<sectionHeader confidence="0.996075" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999911857142857">
We also performed a small set of experiments on
the movie domain using Pang and Lee’s 2004 data
set. This set consists of 2000 reviews, 1000 each of
very positive and very negative reviews. Since this
data set is balanced and the task is only a two-way
classification between positive and negative re-
views, we only report accuracy numbers here.
</bodyText>
<table confidence="0.9971555">
accuracy Training data
Turney 66% unsupervised
(2002)
Pang &amp; Lee 87.15% supervised
(2004)
Aue &amp; Ga- 91.4% supervised
mon (2005)
SO 73.95% unsupervised
SM+SO to 74.85% weakly super-
increase seed vised
words, then
SO
</table>
<tableCaption confidence="0.969957">
Table 7: Classification accuracy on the movie review
domain
</tableCaption>
<bodyText confidence="0.999982045454546">
Turney (2002) achieves 66% accuracy on the
movie review domain using the PMI-IR algorithm
to gather association scores from the web. Pang
and Lee (2004) report 87.15% accuracy using a
unigram-based SVM classifier combined with sub-
jectivity detection. Aue and Gamon (2005) use a
simple linear SVM classifier based on unigrams,
We used the McNemar test to assess whether two
classifiers are performing significantly differently.
This test establishes whether the accuracy of two
classifiers differs significantly - it does not guaran-
tee significance for precision and recall differ-
ences. For the latter, other tests have been
proposed (e.g. Chinchor 1995), but time con-
straints prohibited us from implementing any of
those more computationally costly tests.
For the results presented in the previous sections
the McNemar test established statistical signifi-
cance at the 0.99 level over baseline (i.e. the SO
results in Table 1) for the multiple iterations results
(Table 4) and the bootstrapping approach (Table
5), but not for the SM+SO approach (Table 2).
</bodyText>
<subsectionHeader confidence="0.880644">
5.2 Future work
</subsectionHeader>
<bodyText confidence="0.999848222222222">
This exploratory set of experiments indicates a
number of interesting directions for future work. A
shortcoming of the present work is the manual tun-
ing of cutoff parameters. This problem could be
alleviated in at least two possible ways:
First, using a general combination of the ranking
of terms according to SM and SO. In other words,
calculate the semantic weight of a term as a com-
bination of SO and its rank in the SM scores.
</bodyText>
<page confidence="0.998156">
63
</page>
<bodyText confidence="0.9999654">
Secondly, following a suggestion by an anony-
mous reviewer, the Naive Bayes bootstrapping ap-
proach could be used in a feedback loop to inform
the SO score estimation in the absence of a manu-
ally annotated parameter tuning set.
</bodyText>
<subsectionHeader confidence="0.983978">
5.3 Summary
</subsectionHeader>
<bodyText confidence="0.9999553125">
Our results demonstrate that the SM method can
serve as a valid tool to mine sentiment-rich vo-
cabulary in a domain. SM will yield a list of terms
that are likely to have a strong sentiment orienta-
tion. SO can then be used to find the polarity for
the selected features by association with the senti-
ment terms of known polarity in the seed word list.
Performing this process iteratively by first enhanc-
ing the set of seed words through SM+SO yields
the best results. While this approach does not com-
pare to the results that can be achieved by super-
vised learning with large amounts of labeled data,
it does improve on results obtained by using SO
alone.
We believe that this result is relevant in two re-
spects. First, by improving average precision and
recall on the classification task, we move closer to
the goal of unsupervised sentiment classification.
This is a very important goal in itself given the
need for “out of the box” sentiment techniques in
business intelligence and the notorious difficulty of
rapidly adapting to a new domain (Engström 2004,
Aue and Gamon 2005). Second, the exploratory
results reported here may indicate a general source
of information for feature selection in natural lan-
guage tasks: features that have a tendency to be in
complementary distribution (especially in smaller
linguistic units such as sentences) may often form
a class that shares certain properties. In other
words, it is not only the strong association scores
that should be exploited but also the particularly
weak (negative) associations.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987601754386">
Anthony Aue and Michael Gamon (2005): “Customiz-
ing Sentiment Classifiers to a New Domain: A Case
Study. Under review.
Xue Bai, Rema Padman, and Edoardo Airoldi. (2004).
Sentiment Extraction from Unstructured Text Using
Tabu Search-Enhanced Markov Blanket. In: Proceed-
ings of the International Workshop on Mining for
and from the Semantic Web (MSW 2004), pp 24-35.
Nancy A. Chinchor (1995): Statistical significance of
MUC-6 results. Proceedings of the Sixth Message
Understanding Conference, pp. 39-44.
J. Cohen (1960): “A coefficient of agreement for nomi-
nal scales.” In: Educational and Psychological meas-
urements 20, pp. 37–46
Charlotta Engström. 2004. Topic dependence in Senti-
ment Classification. MPhil thesis, University of
Cambridge.
Michael Gamon, Anthony Aue, Simon Corston-Oliver,
and Eric Ringger. (2005): “Pulse: Mining Customer
Opinions from Free Text”. Under review.
Christopher D. Manning and Hinrich Schütze (2002):
Foundations of Statistical Natural Language Process-
ing. MIT Press, Cambridge, London.
Kamal Nigam, Andrew McCallum, Sebastian Thrun and
Tom Mitchell (2000): Text Classification from La-
beled and Unlabeled Documents using EM. In: Ma-
chine Learning 39 (2/3), pp. 103-134.
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan
(2002): “Thumbs up? Sentiment Classification using
Machine Learning Techniques”. Proceedings of
EMNLP 2002, pp. 79-86.
Bo Pang and Lillian Lee. (2004). A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Summa-
rization Based on Minimum Cuts. Proceedings of
ACL 2004, pp.217-278.
Peter D. Turney (2001): “Mining the Web for Syno-
nyms: PMI-IR versus LSA on TOEFL.” In Proceed-
ings of the Twelfth European Conference on
Machine Learning, pp. 491-502.
Peter D. Turney (2002): “Thumbs up or thumbs down?
Semantic orientation applied to unsupervised classi-
fication of reviews”. In: Proceedings of ACL 2002,
pp. 417-424.
Peter D. Turney and M. L. Littman (2002): “Unsuper-
vised Learning of Semantic Orientation from a Hun-
dred-Billion-Word Corpus.” Technical report ERC-
1094 (NRC 44929), National Research Council of
Canada.
Janyce Wiebe, Theresa Wilson and Matthew Bell
(2001): “Identifying Collocations for Recognizing
Opinions”. In: Proceedings of the ACL/EACL Work-
shop on Collocation.
Hong Yu and Vasileios Hatzivassiloglou (2003): “To-
wards Answering opinion Questions: Separating
Facts from Opinions and Identifying the Polarity of
Opinion Sentences”. In: Proceedings of EMNLP
2003.
</reference>
<page confidence="0.999418">
64
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.967980">
<title confidence="0.996565">Automatic identification of sentiment vocabulary: exploiting low association with known sentiment terms</title>
<author confidence="0.999906">Michael Gamon Anthony Aue</author>
<affiliation confidence="0.9989635">Natural Language Processing Group Natural Language Processing Group Microsoft Research Microsoft Research</affiliation>
<email confidence="0.987719">mgamon@microsoft.comanthaue@microsoft.com</email>
<abstract confidence="0.999502458333333">We describe an extension to the technique for the automatic identification and labeling of sentiment terms described in Turney (2002) and Turney and Littman (2002). Their basic assumption is that sentiment terms of similar orientation tend to co-occur at the document level. We add a second assumption, namely that sentiment terms of opposite orientation co-occur at the sentence level. This additional assumption allows us to identify sentiment-bearing terms very reliably. We then use these newly identified terms in various scenarios for the sentiment classification of sentences. We show that our approach outperforms Turney’s original approach. Combining our approach with a Naive Bayes bootstrapping method yields a further small improvement of classifier performance. We finally compare our results to precision and recall figures that can be obtained on the same data set with labeled data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anthony Aue</author>
<author>Michael Gamon</author>
</authors>
<title>Customizing Sentiment Classifiers to a New Domain: A Case Study. Under review.</title>
<date>2005</date>
<contexts>
<context position="5610" citStr="Aue and Gamon (2005)" startWordPosition="887" endWordPosition="890">or. The resulting list of terms and associated sentiment orientations can then be used to implement a classifier: semantic orientation of the terms in a document of unknown sentiment is added up, and if the overall score is positive, the document is classified as being of positive sentiment, otherwise it is classified as negative. Yu and Hatzivassiloglou (2003) extend this approach by (1) applying it at the sentence level (instead of the document-level), (2) taking into account non-adjectival parts-of-speech, and (3) 1 For domain-specificity of sentiment classification see Engström (2004) and Aue and Gamon (2005). using larger sets of seed words. Their classification goal also differs from Turney’s: it is to distinguish opinion sentences from factual statements. Turney et al.’s approach is based on the assumption that sentiment terms of similar orientation tend to co-occur in documents. Our approach takes advantage of a second assumption: At the sentence level, sentiment terms of opposite orientation tend not to co-occur. This is, of course, an assumption that will only hold in general, with exceptions. Basically, the assumption is that sentences of the following form: I dislike X. I really like X. ar</context>
<context position="24692" citStr="Aue and Gamon (2005)" startWordPosition="4088" endWordPosition="4091">tween positive and negative reviews, we only report accuracy numbers here. accuracy Training data Turney 66% unsupervised (2002) Pang &amp; Lee 87.15% supervised (2004) Aue &amp; Ga- 91.4% supervised mon (2005) SO 73.95% unsupervised SM+SO to 74.85% weakly superincrease seed vised words, then SO Table 7: Classification accuracy on the movie review domain Turney (2002) achieves 66% accuracy on the movie review domain using the PMI-IR algorithm to gather association scores from the web. Pang and Lee (2004) report 87.15% accuracy using a unigram-based SVM classifier combined with subjectivity detection. Aue and Gamon (2005) use a simple linear SVM classifier based on unigrams, We used the McNemar test to assess whether two classifiers are performing significantly differently. This test establishes whether the accuracy of two classifiers differs significantly - it does not guarantee significance for precision and recall differences. For the latter, other tests have been proposed (e.g. Chinchor 1995), but time constraints prohibited us from implementing any of those more computationally costly tests. For the results presented in the previous sections the McNemar test established statistical significance at the 0.9</context>
</contexts>
<marker>Aue, Gamon, 2005</marker>
<rawString>Anthony Aue and Michael Gamon (2005): “Customizing Sentiment Classifiers to a New Domain: A Case Study. Under review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xue Bai</author>
<author>Rema Padman</author>
<author>Edoardo Airoldi</author>
</authors>
<title>Sentiment Extraction from Unstructured Text Using Tabu Search-Enhanced Markov Blanket. In:</title>
<date>2004</date>
<booktitle>Proceedings of the International Workshop on Mining for and from the Semantic Web (MSW</booktitle>
<pages>24--35</pages>
<contexts>
<context position="1431" citStr="Bai et al. 2004" startWordPosition="210" endWordPosition="213">in various scenarios for the sentiment classification of sentences. We show that our approach outperforms Turney’s original approach. Combining our approach with a Naive Bayes bootstrapping method yields a further small improvement of classifier performance. We finally compare our results to precision and recall figures that can be obtained on the same data set with labeled data. 1 Introduction The field of sentiment classification has received considerable attention from researchers in recent years (Pang and Lee 2002, Pang et al. 2004, Turney 2002, Turney and Littman 2002, Wiebe et al. 2001, Bai et al. 2004, Yu and Hatzivassiloglou 2003 and many others). The identification and classification of sentiment constitutes a problem that is orthogonal to the usual task of text classification. Whereas in traditional text classification the focus is on topic identification, in sentiment classification the focus is on the assessment of the writer’s sentiment toward the topic. Movie and product reviews have been the main focus of many of the recent studies in this area (Pang and Lee 2002, Pang et al. 2004, Turney 2002, Turney and Littman 2002). Typically, these reviews are classified at the document level,</context>
</contexts>
<marker>Bai, Padman, Airoldi, 2004</marker>
<rawString>Xue Bai, Rema Padman, and Edoardo Airoldi. (2004). Sentiment Extraction from Unstructured Text Using Tabu Search-Enhanced Markov Blanket. In: Proceedings of the International Workshop on Mining for and from the Semantic Web (MSW 2004), pp 24-35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy A Chinchor</author>
</authors>
<title>Statistical significance of MUC-6 results.</title>
<date>1995</date>
<booktitle>Proceedings of the Sixth Message Understanding Conference,</booktitle>
<pages>39--44</pages>
<contexts>
<context position="25074" citStr="Chinchor 1995" startWordPosition="4148" endWordPosition="4149">the movie review domain using the PMI-IR algorithm to gather association scores from the web. Pang and Lee (2004) report 87.15% accuracy using a unigram-based SVM classifier combined with subjectivity detection. Aue and Gamon (2005) use a simple linear SVM classifier based on unigrams, We used the McNemar test to assess whether two classifiers are performing significantly differently. This test establishes whether the accuracy of two classifiers differs significantly - it does not guarantee significance for precision and recall differences. For the latter, other tests have been proposed (e.g. Chinchor 1995), but time constraints prohibited us from implementing any of those more computationally costly tests. For the results presented in the previous sections the McNemar test established statistical significance at the 0.99 level over baseline (i.e. the SO results in Table 1) for the multiple iterations results (Table 4) and the bootstrapping approach (Table 5), but not for the SM+SO approach (Table 2). 5.2 Future work This exploratory set of experiments indicates a number of interesting directions for future work. A shortcoming of the present work is the manual tuning of cutoff parameters. This p</context>
</contexts>
<marker>Chinchor, 1995</marker>
<rawString>Nancy A. Chinchor (1995): Statistical significance of MUC-6 results. Proceedings of the Sixth Message Understanding Conference, pp. 39-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.” In:</title>
<date>1960</date>
<booktitle>Educational and Psychological measurements 20,</booktitle>
<pages>37--46</pages>
<contexts>
<context position="8782" citStr="Cohen 1960" startWordPosition="1443" endWordPosition="1444"> submitted reviews to the website, they were asked for a recommendation on a scale of 1 (negative) to 10 (positive). The average score was very high, at 8.3, yielding a strong skew in favor of positive class labels. We annotated a randomlyselected sample of 3,000 sentences for sentiment. Each sentence was viewed in isolation and classified as positive, negative or neutral. The neutral category was applied to sentences with no discernible sentiment, as well as to sentences that expressed both positive and negative sentiment. Three annotators had pair-wise agreement scores (Cohen’s Kappa score, Cohen 1960) of 70.10%, 71.78% and 79.93%, suggesting that the task of sentiment classification on the sentence level is feasible but difficult even for people. This set of data was split into a development test set of 400 sentences and a blind test set of 2600 sentences. Sentences are represented as vectors of binary unigram features. The total number of observed unigram features is 72988. In order to restrict the number of features to a manageable size, we disregard features that occur less than 10 times in the corpus. With this restriction we obtain a reduced feature set of 13317 features. 3 Experiment</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>J. Cohen (1960): “A coefficient of agreement for nominal scales.” In: Educational and Psychological measurements 20, pp. 37–46</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charlotta Engström</author>
</authors>
<title>Topic dependence in Sentiment Classification. MPhil thesis,</title>
<date>2004</date>
<institution>University of Cambridge.</institution>
<contexts>
<context position="5585" citStr="Engström (2004)" startWordPosition="883" endWordPosition="885">egative seed term poor. The resulting list of terms and associated sentiment orientations can then be used to implement a classifier: semantic orientation of the terms in a document of unknown sentiment is added up, and if the overall score is positive, the document is classified as being of positive sentiment, otherwise it is classified as negative. Yu and Hatzivassiloglou (2003) extend this approach by (1) applying it at the sentence level (instead of the document-level), (2) taking into account non-adjectival parts-of-speech, and (3) 1 For domain-specificity of sentiment classification see Engström (2004) and Aue and Gamon (2005). using larger sets of seed words. Their classification goal also differs from Turney’s: it is to distinguish opinion sentences from factual statements. Turney et al.’s approach is based on the assumption that sentiment terms of similar orientation tend to co-occur in documents. Our approach takes advantage of a second assumption: At the sentence level, sentiment terms of opposite orientation tend not to co-occur. This is, of course, an assumption that will only hold in general, with exceptions. Basically, the assumption is that sentences of the following form: I disli</context>
</contexts>
<marker>Engström, 2004</marker>
<rawString>Charlotta Engström. 2004. Topic dependence in Sentiment Classification. MPhil thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Anthony Aue</author>
<author>Simon Corston-Oliver</author>
<author>Eric Ringger</author>
</authors>
<title>Pulse: Mining Customer Opinions from Free Text”. Under review.</title>
<date>2005</date>
<contexts>
<context position="2448" citStr="Gamon et al. (2005)" startWordPosition="377" endWordPosition="381">been the main focus of many of the recent studies in this area (Pang and Lee 2002, Pang et al. 2004, Turney 2002, Turney and Littman 2002). Typically, these reviews are classified at the document level, and the class labels are “positive” and “negative”. In this work, in contrast, we narrow the scope of investigation to the sentence level and expand the set of labels, making a threefold distinction between “positive”, “neutral”, and “negative”. The narrowing of scope is motivated by the fact that for realistic text mining on customer feedback, the document level is too coarse, as described in Gamon et al. (2005). The expansion of the label set is also motivated by real-world concerns; while it is a given that review text expresses positive or negative sentiment, in many cases it is necessary to also identify the cases that don’t carry strong expressions of sentiment at all. Traditional approaches to text classification require large amounts of labeled training data. Acquisition of such data can be costly and timeconsuming. Due to the highly domain-specific nature of the sentiment classification task, moving from one domain to another typically requires the acquisition of a new set of training data. F</context>
</contexts>
<marker>Gamon, Aue, Corston-Oliver, Ringger, 2005</marker>
<rawString>Michael Gamon, Anthony Aue, Simon Corston-Oliver, and Eric Ringger. (2005): “Pulse: Mining Customer Opinions from Free Text”. Under review.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D Christopher</author>
</authors>
<title>Manning and Hinrich Schütze (2002): Foundations of Statistical Natural Language Processing.</title>
<publisher>MIT Press,</publisher>
<location>Cambridge, London.</location>
<marker>Christopher, </marker>
<rawString>Christopher D. Manning and Hinrich Schütze (2002): Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Andrew McCallum</author>
<author>Sebastian Thrun</author>
<author>Tom Mitchell</author>
</authors>
<title>Text Classification from Labeled and Unlabeled Documents using EM.</title>
<date>2000</date>
<journal>In: Machine Learning</journal>
<volume>39</volume>
<issue>2</issue>
<pages>103--134</pages>
<contexts>
<context position="7700" citStr="Nigam et al. 2000" startWordPosition="1257" endWordPosition="1260">, we do not use the web as a resource to find associations, rather we apply the method directly to in-domain data. This has the disadvantage of not being able to apply the classification to any arbitrary domain. It is worth noting, however, that even in Turney (2002) the choice of seed words is explicitly motivated by domain properties of movie reviews. In the remainder of the paper we will describe results from various experiments based on this assumption. We also show how we can combine this method with a Naive Bayes bootstrapping approach that takes further advantage of the unlabeled data (Nigam et al. 2000). 58 2 Data For our experiments we used a set of car reviews from the MSN Autos web site. The data consist of 406,818 customer car reviews written over a fouryear period. Aside from filtering out examples containing profanity, the data was not edited. The reviews range in length from a single sentence (56% of all cases) to 50 sentences (a single review). Less than 1% of reviews contain ten or more sentences. There are almost 900,000 sentences in total. When customers submitted reviews to the website, they were asked for a recommendation on a scale of 1 (negative) to 10 (positive). The average </context>
<context position="20015" citStr="Nigam et al. 2000" startWordPosition="3307" endWordPosition="3311">quivalent number of features with strongest semantic orientation according to the SO method and add them to the list of seed features, our results degrade significantly (the resulting classifier performance is significantly different at the 99.9% level as established by the McNemar test). This is further evidence that SM is indeed an effective method for selecting sentiment terms. 4.3. Using the SO classifier to bootstrap a Naive Bayes classifier In a third set of experiments, we tried to improve on the results of the SO classifier by combining it with the bootstrapping approach described in (Nigam et al. 2000). The basic idea here is to use the SO classifier to label a subset of the data DL. This labeled subset of the data is then used to bootstrap a Naive Bayes (NB) classifier on the remaining unlabeled data DU using the Expectation Maximization (EM) algorithm: (1) An initial naive Bayes classifier with parameters θ is trained on the documents in DL. (2) This initial classifier is used to estimate a probability distribution over all classes for each of the documents in DU. (EStep) (3) The labeled and unlabeled data are then used to estimate parameters for a new classifier. (M-Step) Steps 2 and 3 a</context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>Kamal Nigam, Andrew McCallum, Sebastian Thrun and Tom Mitchell (2000): Text Classification from Labeled and Unlabeled Documents using EM. In: Machine Learning 39 (2/3), pp. 103-134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
</authors>
<title>Lillian Lee and Shivakumar Vaithyanathan (2002): “Thumbs up? Sentiment Classification using Machine Learning Techniques”.</title>
<date>2002</date>
<booktitle>Proceedings of EMNLP</booktitle>
<pages>79--86</pages>
<marker>Pang, 2002</marker>
<rawString>Bo Pang, Lillian Lee and Shivakumar Vaithyanathan (2002): “Thumbs up? Sentiment Classification using Machine Learning Techniques”. Proceedings of EMNLP 2002, pp. 79-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts.</title>
<date>2004</date>
<booktitle>Proceedings of ACL 2004,</booktitle>
<pages>217--278</pages>
<contexts>
<context position="24573" citStr="Pang and Lee (2004)" startWordPosition="4071" endWordPosition="4074">y positive and very negative reviews. Since this data set is balanced and the task is only a two-way classification between positive and negative reviews, we only report accuracy numbers here. accuracy Training data Turney 66% unsupervised (2002) Pang &amp; Lee 87.15% supervised (2004) Aue &amp; Ga- 91.4% supervised mon (2005) SO 73.95% unsupervised SM+SO to 74.85% weakly superincrease seed vised words, then SO Table 7: Classification accuracy on the movie review domain Turney (2002) achieves 66% accuracy on the movie review domain using the PMI-IR algorithm to gather association scores from the web. Pang and Lee (2004) report 87.15% accuracy using a unigram-based SVM classifier combined with subjectivity detection. Aue and Gamon (2005) use a simple linear SVM classifier based on unigrams, We used the McNemar test to assess whether two classifiers are performing significantly differently. This test establishes whether the accuracy of two classifiers differs significantly - it does not guarantee significance for precision and recall differences. For the latter, other tests have been proposed (e.g. Chinchor 1995), but time constraints prohibited us from implementing any of those more computationally costly tes</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. (2004). A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts. Proceedings of ACL 2004, pp.217-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL.”</title>
<date>2001</date>
<booktitle>In Proceedings of the Twelfth European Conference on Machine Learning,</booktitle>
<pages>491--502</pages>
<contexts>
<context position="4526" citStr="Turney 2001" startWordPosition="714" endWordPosition="716">n of movie reviews. They use the two terms excellent and poor as seed terms to determine the semantic orientation of other terms. These seed terms can be viewed as proxies for the class labels “positive” and “negative”, allowing for the exploitation of otherwise unlabeled data: Terms that tend to co-occur with excellent in documents tend to be of positive orientation, and vice versa for poor. Turney (2002) starts from a small (2 word) set of terms with known orientation (excellent and poor). Given a set of terms with unknown sentiment orientation, Turney (2002) then uses the PMI-IR algorithm (Turney 2001) to issue queries to the web and determine, for each of these terms, its pointwise mutual information (PMI) with the two seed words across a large set of documents. Term candidates are constrained to be adjectives, which tend to be the strongest bearers of sentiment. The sentiment orientation (SO) of a term is then determined by the difference between its association (PMI) with the positive seed term excellent and its association with the negative seed term poor. The resulting list of terms and associated sentiment orientations can then be used to implement a classifier: semantic orientation o</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter D. Turney (2001): “Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL.” In Proceedings of the Twelfth European Conference on Machine Learning, pp. 491-502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews”. In:</title>
<date>2002</date>
<booktitle>Proceedings of ACL</booktitle>
<pages>417--424</pages>
<contexts>
<context position="1370" citStr="Turney 2002" startWordPosition="199" endWordPosition="201"> very reliably. We then use these newly identified terms in various scenarios for the sentiment classification of sentences. We show that our approach outperforms Turney’s original approach. Combining our approach with a Naive Bayes bootstrapping method yields a further small improvement of classifier performance. We finally compare our results to precision and recall figures that can be obtained on the same data set with labeled data. 1 Introduction The field of sentiment classification has received considerable attention from researchers in recent years (Pang and Lee 2002, Pang et al. 2004, Turney 2002, Turney and Littman 2002, Wiebe et al. 2001, Bai et al. 2004, Yu and Hatzivassiloglou 2003 and many others). The identification and classification of sentiment constitutes a problem that is orthogonal to the usual task of text classification. Whereas in traditional text classification the focus is on topic identification, in sentiment classification the focus is on the assessment of the writer’s sentiment toward the topic. Movie and product reviews have been the main focus of many of the recent studies in this area (Pang and Lee 2002, Pang et al. 2004, Turney 2002, Turney and Littman 2002). T</context>
<context position="3805" citStr="Turney (2002)" startWordPosition="596" endWordPosition="597"> Feature Engineering for Machine Learning in NLP, pages 57–64, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics desirable.1 Our focus, therefore, is on methods that require very little data annotation. We describe a method to automatically identify the sentiment vocabulary in a domain. This method rests on three special properties of the sentiment domain: 1. the presence of certain words can serve as a proxy for the class label 2. sentiment terms of similar orientation tend to co-occur 3. sentiment terms of opposite orientation tend to not co-occur at the sentence level. Turney (2002) and Turney and Littman (2002) exploit the first two generalizations for unsupervised sentiment classification of movie reviews. They use the two terms excellent and poor as seed terms to determine the semantic orientation of other terms. These seed terms can be viewed as proxies for the class labels “positive” and “negative”, allowing for the exploitation of otherwise unlabeled data: Terms that tend to co-occur with excellent in documents tend to be of positive orientation, and vice versa for poor. Turney (2002) starts from a small (2 word) set of terms with known orientation (excellent and p</context>
<context position="7082" citStr="Turney (2002)" startWordPosition="1151" endWordPosition="1152">domain. We select the terms that have the lowest PMI scores on the sentence level with respect to a set of manually selected seed words. If our assumption about low association at the sentence level is correct, this set of low-scoring terms will be particularly rich in sentiment terms. We can then use this newly identified set to: (1) use Turney’s method to find the orientation for the terms and employ the terms and their scores in a classifier, and (2) use Turney’s method to find the orientation for the terms and add the new terms as additional seed terms for a second iteration As opposed to Turney (2002), we do not use the web as a resource to find associations, rather we apply the method directly to in-domain data. This has the disadvantage of not being able to apply the classification to any arbitrary domain. It is worth noting, however, that even in Turney (2002) the choice of seed words is explicitly motivated by domain properties of movie reviews. In the remainder of the paper we will describe results from various experiments based on this assumption. We also show how we can combine this method with a Naive Bayes bootstrapping approach that takes further advantage of the unlabeled data (</context>
<context position="24434" citStr="Turney (2002)" startWordPosition="4050" endWordPosition="4051"> small set of experiments on the movie domain using Pang and Lee’s 2004 data set. This set consists of 2000 reviews, 1000 each of very positive and very negative reviews. Since this data set is balanced and the task is only a two-way classification between positive and negative reviews, we only report accuracy numbers here. accuracy Training data Turney 66% unsupervised (2002) Pang &amp; Lee 87.15% supervised (2004) Aue &amp; Ga- 91.4% supervised mon (2005) SO 73.95% unsupervised SM+SO to 74.85% weakly superincrease seed vised words, then SO Table 7: Classification accuracy on the movie review domain Turney (2002) achieves 66% accuracy on the movie review domain using the PMI-IR algorithm to gather association scores from the web. Pang and Lee (2004) report 87.15% accuracy using a unigram-based SVM classifier combined with subjectivity detection. Aue and Gamon (2005) use a simple linear SVM classifier based on unigrams, We used the McNemar test to assess whether two classifiers are performing significantly differently. This test establishes whether the accuracy of two classifiers differs significantly - it does not guarantee significance for precision and recall differences. For the latter, other tests</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney (2002): “Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews”. In: Proceedings of ACL 2002, pp. 417-424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>M L Littman</author>
</authors>
<title>Unsupervised Learning of Semantic Orientation from a Hundred-Billion-Word Corpus.”</title>
<date>2002</date>
<tech>Technical report ERC1094 (NRC 44929),</tech>
<institution>National Research Council of Canada.</institution>
<contexts>
<context position="1395" citStr="Turney and Littman 2002" startWordPosition="202" endWordPosition="205">y. We then use these newly identified terms in various scenarios for the sentiment classification of sentences. We show that our approach outperforms Turney’s original approach. Combining our approach with a Naive Bayes bootstrapping method yields a further small improvement of classifier performance. We finally compare our results to precision and recall figures that can be obtained on the same data set with labeled data. 1 Introduction The field of sentiment classification has received considerable attention from researchers in recent years (Pang and Lee 2002, Pang et al. 2004, Turney 2002, Turney and Littman 2002, Wiebe et al. 2001, Bai et al. 2004, Yu and Hatzivassiloglou 2003 and many others). The identification and classification of sentiment constitutes a problem that is orthogonal to the usual task of text classification. Whereas in traditional text classification the focus is on topic identification, in sentiment classification the focus is on the assessment of the writer’s sentiment toward the topic. Movie and product reviews have been the main focus of many of the recent studies in this area (Pang and Lee 2002, Pang et al. 2004, Turney 2002, Turney and Littman 2002). Typically, these reviews a</context>
<context position="3835" citStr="Turney and Littman (2002)" startWordPosition="599" endWordPosition="602">ng for Machine Learning in NLP, pages 57–64, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics desirable.1 Our focus, therefore, is on methods that require very little data annotation. We describe a method to automatically identify the sentiment vocabulary in a domain. This method rests on three special properties of the sentiment domain: 1. the presence of certain words can serve as a proxy for the class label 2. sentiment terms of similar orientation tend to co-occur 3. sentiment terms of opposite orientation tend to not co-occur at the sentence level. Turney (2002) and Turney and Littman (2002) exploit the first two generalizations for unsupervised sentiment classification of movie reviews. They use the two terms excellent and poor as seed terms to determine the semantic orientation of other terms. These seed terms can be viewed as proxies for the class labels “positive” and “negative”, allowing for the exploitation of otherwise unlabeled data: Terms that tend to co-occur with excellent in documents tend to be of positive orientation, and vice versa for poor. Turney (2002) starts from a small (2 word) set of terms with known orientation (excellent and poor). Given a set of terms wit</context>
</contexts>
<marker>Turney, Littman, 2002</marker>
<rawString>Peter D. Turney and M. L. Littman (2002): “Unsupervised Learning of Semantic Orientation from a Hundred-Billion-Word Corpus.” Technical report ERC1094 (NRC 44929), National Research Council of Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Matthew Bell</author>
</authors>
<title>Identifying Collocations for Recognizing Opinions”. In:</title>
<date>2001</date>
<booktitle>Proceedings of the ACL/EACL Workshop on Collocation.</booktitle>
<contexts>
<context position="1414" citStr="Wiebe et al. 2001" startWordPosition="206" endWordPosition="209">y identified terms in various scenarios for the sentiment classification of sentences. We show that our approach outperforms Turney’s original approach. Combining our approach with a Naive Bayes bootstrapping method yields a further small improvement of classifier performance. We finally compare our results to precision and recall figures that can be obtained on the same data set with labeled data. 1 Introduction The field of sentiment classification has received considerable attention from researchers in recent years (Pang and Lee 2002, Pang et al. 2004, Turney 2002, Turney and Littman 2002, Wiebe et al. 2001, Bai et al. 2004, Yu and Hatzivassiloglou 2003 and many others). The identification and classification of sentiment constitutes a problem that is orthogonal to the usual task of text classification. Whereas in traditional text classification the focus is on topic identification, in sentiment classification the focus is on the assessment of the writer’s sentiment toward the topic. Movie and product reviews have been the main focus of many of the recent studies in this area (Pang and Lee 2002, Pang et al. 2004, Turney 2002, Turney and Littman 2002). Typically, these reviews are classified at th</context>
</contexts>
<marker>Wiebe, Wilson, Bell, 2001</marker>
<rawString>Janyce Wiebe, Theresa Wilson and Matthew Bell (2001): “Identifying Collocations for Recognizing Opinions”. In: Proceedings of the ACL/EACL Workshop on Collocation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards Answering opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences”. In:</title>
<date>2003</date>
<booktitle>Proceedings of EMNLP</booktitle>
<contexts>
<context position="1461" citStr="Yu and Hatzivassiloglou 2003" startWordPosition="214" endWordPosition="217">ios for the sentiment classification of sentences. We show that our approach outperforms Turney’s original approach. Combining our approach with a Naive Bayes bootstrapping method yields a further small improvement of classifier performance. We finally compare our results to precision and recall figures that can be obtained on the same data set with labeled data. 1 Introduction The field of sentiment classification has received considerable attention from researchers in recent years (Pang and Lee 2002, Pang et al. 2004, Turney 2002, Turney and Littman 2002, Wiebe et al. 2001, Bai et al. 2004, Yu and Hatzivassiloglou 2003 and many others). The identification and classification of sentiment constitutes a problem that is orthogonal to the usual task of text classification. Whereas in traditional text classification the focus is on topic identification, in sentiment classification the focus is on the assessment of the writer’s sentiment toward the topic. Movie and product reviews have been the main focus of many of the recent studies in this area (Pang and Lee 2002, Pang et al. 2004, Turney 2002, Turney and Littman 2002). Typically, these reviews are classified at the document level, and the class labels are “pos</context>
<context position="5353" citStr="Yu and Hatzivassiloglou (2003)" startWordPosition="847" endWordPosition="850">to be adjectives, which tend to be the strongest bearers of sentiment. The sentiment orientation (SO) of a term is then determined by the difference between its association (PMI) with the positive seed term excellent and its association with the negative seed term poor. The resulting list of terms and associated sentiment orientations can then be used to implement a classifier: semantic orientation of the terms in a document of unknown sentiment is added up, and if the overall score is positive, the document is classified as being of positive sentiment, otherwise it is classified as negative. Yu and Hatzivassiloglou (2003) extend this approach by (1) applying it at the sentence level (instead of the document-level), (2) taking into account non-adjectival parts-of-speech, and (3) 1 For domain-specificity of sentiment classification see Engström (2004) and Aue and Gamon (2005). using larger sets of seed words. Their classification goal also differs from Turney’s: it is to distinguish opinion sentences from factual statements. Turney et al.’s approach is based on the assumption that sentiment terms of similar orientation tend to co-occur in documents. Our approach takes advantage of a second assumption: At the sen</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Hong Yu and Vasileios Hatzivassiloglou (2003): “Towards Answering opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences”. In: Proceedings of EMNLP 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>