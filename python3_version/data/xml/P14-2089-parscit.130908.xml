<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000808">
<title confidence="0.909085">
Improving Lexical Embeddings with Semantic Knowledge
</title>
<author confidence="0.967705">
Mo Yu ∗ Mark Dredze
</author>
<affiliation confidence="0.9294495">
Machine Translation Lab Human Language Technology Center of Excellence
Harbin Institute of Technology Center for Language and Speech Processing
</affiliation>
<address confidence="0.5560335">
Harbin, China Johns Hopkins University
gflfof@gmail.com Baltimore, MD 21218
</address>
<email confidence="0.998592">
mdredze@cs.jhu.edu
</email>
<sectionHeader confidence="0.99738" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999465230769231">
Word embeddings learned on unlabeled
data are a popular tool in semantics, but
may not capture the desired semantics. We
propose a new learning objective that in-
corporates both a neural language model
objective (Mikolov et al., 2013) and prior
knowledge from semantic resources to
learn improved lexical semantic embed-
dings. We demonstrate that our embed-
dings improve over those learned solely on
raw text in three settings: language mod-
eling, measuring semantic similarity, and
predicting human judgements.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998603357142858">
Word embeddings are popular representations for
syntax (Turian et al., 2010; Collobert and We-
ston, 2008; Mnih and Hinton, 2007), semantics
(Huang et al., 2012; Socher et al., 2013), morphol-
ogy (Luong et al., 2013) and other areas. A long
line of embeddings work, such as LSA and ran-
domized embeddings (Ravichandran et al., 2005;
Van Durme and Lall, 2010), has recently turned
to neural language models (Bengio et al., 2006;
Collobert and Weston, 2008; Turian et al., 2010).
Unsupervised learning can take advantage of large
corpora, which can produce impressive results.
However, the main drawback of unsupervised
learning is that the learned embeddings may not
be suited for the task of interest. Consider se-
mantic embeddings, which may capture a notion
of semantics that improves one semantic task but
harms another. Controlling this behavior is chal-
lenging with an unsupervised objective. However,
rich prior knowledge exists for many tasks, and
there are numerous such semantic resources.
We propose a new training objective for learn-
ing word embeddings that incorporates prior
∗This work was done while the author was visiting JHU.
knowledge. Our model builds on word2vec
(Mikolov et al., 2013), a neural network based
language model that learns word embeddings by
maximizing the probability of raw text. We extend
the objective to include prior knowledge about
synonyms from semantic resources; we consider
both the Paraphrase Database (Ganitkevitch et al.,
2013) and WordNet (Fellbaum, 1999), which an-
notate semantic relatedness between words. The
latter was also used in (Bordes et al., 2012) for
training a network for predicting synset relation.
The combined objective maximizes both the prob-
ability of the raw corpus and encourages embed-
dings to capture semantic relations from the re-
sources. We demonstrate improvements in our
embeddings on three tasks: language modeling,
measuring word similarity, and predicting human
judgements on word pairs.
</bodyText>
<sectionHeader confidence="0.974973" genericHeader="method">
2 Learning Embeddings
</sectionHeader>
<bodyText confidence="0.999990125">
We present a general model for learning word em-
beddings that incorporates prior knowledge avail-
able for a domain. While in this work we con-
sider semantics, our model could incorporate prior
knowledge from many types of resources. We be-
gin by reviewing the word2vec objective and then
present augmentations of the objective for prior
knowledge, including different training strategies.
</bodyText>
<subsectionHeader confidence="0.997719">
2.1 Word2vec
</subsectionHeader>
<bodyText confidence="0.999530888888889">
Word2vec (Mikolov et al., 2013) is an algorithm
for learning embeddings using a neural language
model. Embeddings are represented by a set of
latent (hidden) variables, and each word is rep-
resented by a specific instantiation of these vari-
ables. Training learns these representations for
each word wt (the tth word in a corpus of size T)
so as to maximize the log likelihood of each token
given its context: words within a window sized c:
</bodyText>
<equation confidence="0.87169025">
log p (wt|wt+c ) , (1)
t−c
1
T
max
T
L
t=1
</equation>
<page confidence="0.96252">
545
</page>
<bodyText confidence="0.9437867">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 545–550,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
where wt+c
t−c is the set of words in the window of
size c centered at wt (wt excluded).
Word2vec offers two choices for modeling of
Eq. (1): a skip-gram model and a continuous bag-
of-words model (cbow). The latter worked better
in our experiments so we focus on it in our presen-
tation. cbow defines p(wt|wt+c
</bodyText>
<equation confidence="0.9708876">
t−c) as:
� �
&gt;
&apos; P
exp e, wt −c≤j≤c,j6=0 ewt+j
</equation>
<bodyText confidence="0.9995785">
the latent variables for word w. While some learn
a single representation for each word (e0w °= ew),
our results improved when we used a separate em-
bedding for input and output in cbow.
</bodyText>
<subsectionHeader confidence="0.985916">
2.2 Relation Constrained Model
</subsectionHeader>
<bodyText confidence="0.999941529411765">
Suppose we have a resource that indicates rela-
tions between words. In the case of semantics,
we could have a resource that encodes semantic
similarity between words. Based on this resource,
we learn embeddings that predict one word from
another related word. We define R as a set of rela-
tions between two words w and w0. R can contain
typed relations (e.g., w is related to w0 through
a specific type of semantic relation), and rela-
tions can have associated scores indicating their
strength. We assume a single relation type of uni-
form strength, though it is straightforward to in-
clude additional characteristics into the objective.
Define Rw to be the subset of relations in R
which involve word w. Our objective maximizes
the (log) probability of all relations by summing
over all words N in the vocabulary:
</bodyText>
<equation confidence="0.978762333333333">
log p (w|wi) , (3)
p(w |wi) = exp ( e 0
wT ewi) /Pw¯ exp (e0w¯T ewi)
</equation>
<bodyText confidence="0.970002">
takes a form similar to Eq. (2) but without the
context: e and e0 are again the input and output
embeddings. For our semantic relations e0w and
ew are symmetrical, so we use a single embedding.
Embeddings are learned such that they are predic-
tive of related words in the resource. We call this
the Relation Constrained Model (RCM).
</bodyText>
<subsectionHeader confidence="0.967456">
2.3 Joint Model
</subsectionHeader>
<bodyText confidence="0.999971857142857">
The cbow and RCM objectives use separate data
for learning. While RCM learns embeddings
suited to specific tasks based on knowledge re-
sources, cbow learns embeddings for words not in-
cluded in the resource but appear in a corpus. We
form a joint model through a linear combination
of the two (weighted by C):
</bodyText>
<equation confidence="0.994952333333333">
log p �wt|wt+c �+ C XN
t−c N
i=1
</equation>
<bodyText confidence="0.994238666666667">
Based on our initial experiments, RCM uses the
output embeddings of cbow.
We learn embeddings using stochastic gradient
</bodyText>
<equation confidence="0.7861186">
ascent. Updates for the first term for e0 and e are:
e0 w − αcbow �σ(f(w)) − I[w=wt] � &apos; Xt+c ewj
j=t−c
Xewj − αcbow (σ(f(w)) − I[w=wt]) &apos; e0w,
w
</equation>
<bodyText confidence="0.546888">
where σ(x) = exp{x}/(1 + exp{x}), I[x] is 1
</bodyText>
<equation confidence="0.996828">
when x is true, f(w) = e0 &gt;Pt+c
j=t−c ewj. Second
w
term updates are:
� �
e0w − αRCM σ(f0(w)) − I[w∈Rwi] &apos; e0wi
� �
σ(f0(w)) − I[w∈Rwi] &apos; e0w,
</equation>
<bodyText confidence="0.684839">
where f0(w) = e0 &gt;e0wi. We use two learning
</bodyText>
<equation confidence="0.632916">
w
</equation>
<bodyText confidence="0.756076">
rates: αcbow and αRCM.
</bodyText>
<subsectionHeader confidence="0.991681">
2.4 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.999933956521739">
All three models (cbow, RCM and joint) use the
same training scheme based on Mikolov et al.
(2013). There are several choices to make in pa-
rameter estimation; we present the best perform-
ing choices used in our results.
We use noise contrastive estimation (NCE)
(Mnih and Teh, 2012), which approximately max-
imizes the log probability of the softmax objec-
tive (Eq. 2). For each objective (cbow or RCM),
we sample 15 words as negative samples for each
training instance according to their frequencies in
raw texts (i.e. training data of cbow). Suppose w
has frequency u(w), then the probability of sam-
pling w is p(w) a u(w)3/4.
We use distributed training, where shared em-
beddings are updated by each thread based on
training data within the thread, i.e., asynchronous
stochastic gradient ascent. For the joint model,
we assign threads to the cbow or RCM objective
with a balance of 12:1(i.e. C is approximately 112).
We allow the cbow threads to control convergence;
training stops when these threads finish process-
ing the data. We found this an effective method
</bodyText>
<equation confidence="0.838178">
(2)
/ T
Pw exp I ew&apos; P−c≤j≤c,j6=0 ewt+j )
</equation>
<bodyText confidence="0.983648">
where ew and \ e,w represent the input and output
embeddings respectively, i.e., the assignments to
</bodyText>
<equation confidence="0.999193">
X
w∈Rwi
1
N
XN
i=1
1
T
XT
t=1
X log p (w|wi)
w∈Rwi
e0wi X− αRCM
w
</equation>
<page confidence="0.969954">
546
</page>
<bodyText confidence="0.999944708333333">
for balancing the two objectives. We trained each
cbow objective using a single pass over the data set
(except for those in Section 4.1), which we empir-
ically verified was sufficient to ensure stable per-
formances on semantic tasks.
Model pre-training is critical in deep learning
(Bengio et al., 2007; Erhan et al., 2010). We eval-
uate two strategies: random initialization, and pre-
training the embeddings. For pre-training, we first
learn using cbow with a random initialization. The
resulting trained model is then used to initialize
the RCM model. This enables the RCM model to
benefit from the unlabeled data, but refine the em-
beddings constrained by the given relations.
Finally, we consider a final model for training
embeddings that uses a specific training regime.
While the joint model balances between fitting the
text and learning relations, modeling the text at
the expense of the relations may negatively impact
the final embeddings for tasks that use the embed-
dings outside of the context of word2vec. There-
fore, we use the embeddings from a trained joint
model to pre-train an RCM model. We call this
setting Joint→RCM.
</bodyText>
<sectionHeader confidence="0.999408" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.9988145">
For training cbow we use the New York Times
(NYT) 1994-97 subset from Gigaword v5.0
(Parker et al., 2011). We select 1,000 paragraphs
each for dev and test data from the December 2010
portion of the NYT. Sentences are tokenized using
OpenNLP1, yielding 518,103,942 tokens for train-
ing, 42,953 tokens for dev and 41,344 for test.
We consider two resources for training the
RCM term: the Paraphrase Database (PPDB)
(Ganitkevitch et al., 2013) and WordNet (Fell-
baum, 1999). For each semantic pair extracted
from these resources, we add a relation to the
RCM objective. Since we use both resources for
evaluation, we divide each into train, dev and test.
PPDB is an automatically extracted dataset con-
taining tens of millions of paraphrase pairs, in-
cluding words and phrases. We used the “lexi-
cal” version of PPDB (no phrases) and filtered to
include pairs that contained words found in the
200,000 most frequent words in the NYT corpus,
which ensures each word in the relations had sup-
port in the text corpus. Next, we removed dupli-
cate pairs: if &lt;A,B&gt; occurred in PPDB, we re-
moved relations of &lt;B,A&gt;. PPDB is organized
</bodyText>
<footnote confidence="0.981454">
1https://opennlp.apache.org/
</footnote>
<table confidence="0.9998825">
PPDB Relations WordNet Relations
Train XL 115,041 Train 68,372
XXL 587,439 (not used in
XXXL 2,647,105 this work)
Dev 1,582 Dev 1,500
Test 1,583 Test 1,500
</table>
<tableCaption confidence="0.999809">
Table 1: Sizes of semantic resources datasets.
</tableCaption>
<bodyText confidence="0.99979452631579">
into 6 parts, ranging from S (small) to XXXL.
Division into these sets is based on an automat-
ically derived accuracy metric. Since S contains
the most accurate paraphrases, we used these for
evaluation. We divided S into a dev set (1582
pairs) and test set (1583 pairs). Training was based
on one of the other sets minus relations from S.
We created similar splits using WordNet, ex-
tracting synonyms using the 100,000 most fre-
quent NYT words. We divide the vocabulary into
three sets: the most frequent 10,000 words, words
with ranks between 10,001-30,000 and 30,001-
100,000. We sample 500 words from each set to
construct a dev and test set. For each word we
sample one synonym to form a pair. The remain-
ing words and their synonyms are used for train-
ing. However we did not use the training data be-
cause it is too small to affect the results. Table 1
summarizes the datasets.
</bodyText>
<sectionHeader confidence="0.999733" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999869608695652">
The goal of our experiments is to demonstrate the
value of learning semantic embeddings with infor-
mation from semantic resources. In each setting,
we will compare the word2vec baseline embed-
ding trained with cbow against RCM alone, the
joint model and Joint→RCM. We consider three
evaluation tasks: language modeling, measuring
semantic similarity, and predicting human judge-
ments on semantic relatedness. In all of our ex-
periments, we conducted model development and
tuned model parameters (C, αcbow, αRCM, PPDB
dataset, etc.) on development data, and evaluate
the best performing model on test data. The mod-
els are notated as follows: word2vec for the base-
line objective (cbow or skip-gram), RCM-r/p and
Joint-r/p for random and pre-trained initializations
of the RCM and Joint objectives, and Joint→RCM
for pre-training RCM with Joint embeddings. Un-
less otherwise notes, we train using PPDB XXL.
We initially created WordNet training data, but
found it too small to affect results. Therefore,
we include only RCM results trained on PPDB,
but show evaluations on both PPDB and WordNet.
</bodyText>
<page confidence="0.994688">
547
</page>
<table confidence="0.999621333333333">
Model NCE HS
word2vec (cbow) 8.75 6.90
RCM-p 8.55 7.07
Joint-r (αRCM = 1 × 10−2) 8.33 6.87
Joint-r (αRCM = 1 × 10−3) 8.20 6.75
Joint→RCM 8.40 6.92
</table>
<tableCaption confidence="0.998778">
Table 2: LM evaluation on held out NYT data.
</tableCaption>
<bodyText confidence="0.999448">
We trained 200-dimensional embeddings and used
output embeddings for measuring similarity. Dur-
ing the training of cbow objectives we remove all
words with frequencies less than 5, which is the
default setting of word2vec.
</bodyText>
<subsectionHeader confidence="0.995974">
4.1 Language Modeling
</subsectionHeader>
<bodyText confidence="0.999990581395349">
Word2vec is fundamentally a language model,
which allows us to compute standard evaluation
metrics on a held out dataset. After obtaining
trained embeddings from any of our objectives,
we use the embeddings in the word2vec model
to measure perplexity of the test set. Measuring
perplexity means computing the exact probability
of each word, which requires summation over all
words in the vocabulary in the denominator of the
softmax. Therefore, we also trained the language
models with hierarchical classification (Mikolov
et al., 2013) strategy (HS). The averaged perplexi-
ties are reported on the NYT test set.
While word2vec and joint are trained as lan-
guage models, RCM is not. In fact, RCM does not
even observe all the words that appear in the train-
ing set, so it makes little sense to use the RCM em-
beddings directly for language modeling. There-
fore, in order to make fair comparison, for every
set of trained embeddings, we fix them as input
embedding for word2vec, then learn the remain-
ing input embeddings (words not in the relations)
and all the output embeddings using cbow. Since
this involves running cbow on NYT data for 2 it-
erations (one iteration for word2vec-training/pre-
training/joint-modeling and the other for tuning
the language model), we use Joint-r (random ini-
tialization) for a fair comparison.
Table 2 shows the results for language mod-
eling on test data. All of our proposed models
improve over the baseline in terms of perplexity
when NCE is used for training LMs. When HS is
used, the perplexities are greatly improved. How-
ever in this situation only the joint models improve
the results; and Joint→RCM performs similar to
the baseline, although it is not designed for lan-
guage modeling. We include the optimal αRCM
in the table; while set αcbow = 0.025 (the default
setting of word2vec). Even when our goal is to
strictly model the raw text corpus, we obtain im-
provements by injecting semantic information into
the objective. RCM can effectively shift learning
to obtain more informative embeddings.
</bodyText>
<subsectionHeader confidence="0.999178">
4.2 Measuring Semantic Similarity
</subsectionHeader>
<bodyText confidence="0.999909173913044">
Our next task is to find semantically related words
using the embeddings, evaluating on relations
from PPDB and WordNet. For each of the word
pairs in the evaluation set &lt;A,B&gt;, we use the co-
sine distance between the embeddings to score A
with a candidate word B&apos;. We use a large sample
of candidate words (10k, 30k or 100k) and rank all
candidate words for pairs where B appears in the
candidates. We then measure the rank of the cor-
rect B to compute mean reciprocal rank (MRR).
Our goal is to use word A to select word B as
the closest matching word from the large set of
candidates. Using this strategy, we evaluate the
embeddings from all of our objectives and mea-
sure which embedding most accurately selected
the true correct word.
Table 3 shows MRR results for both PPDB
and WordNet dev and test datasets for all models.
All of our methods improve over the baselines in
nearly every test set result. In nearly every case,
Joint→RCM obtained the largest improvements.
Clearly, our embeddings are much more effective
at capturing semantic similarity.
</bodyText>
<subsectionHeader confidence="0.999603">
4.3 Human Judgements
</subsectionHeader>
<bodyText confidence="0.9999878">
Our final evaluation is to predict human judge-
ments of semantic relatedness. We have pairs of
words from PPDB scored by annotators on a scale
of 1 to 5 for quality of similarity. Our data are
the judgements used by Ganitkevitch et al. (2013),
which we filtered to include only those pairs for
which we learned embeddings, yielding 868 pairs.
We assign a score using the dot product between
the output embeddings of each word in the pair,
then order all 868 pairs according to this score.
Using the human judgements, we compute the
swapped pairs rate: the ratio between the number
of swapped pairs and the number of all pairs. For
pair p scored yP by the embeddings and judged ˆyP
by an annotator, the swapped pair rate is:
</bodyText>
<equation confidence="0.9749735">
EP1 ,P2 ∈D��I[(yP1 − yP2) (ˆyP2 − ˆyP1) &lt; 0] (4)
EP1,P2∈D I[yP1 =� yP2]
</equation>
<bodyText confidence="0.980313">
where I[x] is 1 when x is true.
</bodyText>
<page confidence="0.991776">
548
</page>
<table confidence="0.999673">
Model PPDB Test WordNet Test
Dev Dev
10k 30k 100k 10k 30k 100k 10k 30k 100k 10k 30k 100k
word2vec (cbow) 49.68 39.26 29.15 49.31 42.53 30.28 10.24 8.64 5.14 10.04 7.90 4.97
word2vec (skip-gram) 48.70 37.14 26.20 - - - 8.61 8.10 4.62 - - -
RCM-r 55.03 42.52 26.05 - - - 13.33 9.05 5.29 - - -
RCM-p 61.79 53.83 40.95 65.42 55.82 41.20 15.25 12.13 7.46 14.13 11.23 7.39
Joint-r 59.91 50.87 36.81 - - - 15.73 11.36 7.14 13.97 10.51 7.44
Joint-p 59.75 50.93 37.73 64.30 53.27 38.97 15.61 11.20 6.96 - - -
Joint--+RCM 64.22 54.99 41.34 68.20 57.87 42.64 16.81 11.67 7.55 16.16 11.21 7.56
</table>
<tableCaption confidence="0.980288333333333">
Table 3: MRR for semantic similarity on PPDB and WordNet dev and test data. Higher is better. All
RCM objectives are trained with PPDB XXL. To preserve test data integrity, only the best performing
setting of each model is evaluated on the test data.
</tableCaption>
<table confidence="0.999581666666667">
Model Swapped Pairs Rate
word2vec (cbow) 17.81
RCM-p 16.66
Joint-r 16.85
Joint-p 16.96
Joint--+RCM 16.62
</table>
<tableCaption confidence="0.979538">
Table 4: Results for ranking the quality of PPDB
pairs as compared to human judgements.
</tableCaption>
<table confidence="0.99984775">
Model Relations PPDB Dev
10k 30k 100k
RCM-r XL 24.02 15.26 9.55
RCM-p XL 54.97 45.35 32.95
RCM-r XXL 55.03 42.52 26.05
RCM-p XXL 61.79 53.83 40.95
RCM-r XXXL 51.00 44.61 28.42
RCM-p XXXL 53.01 46.35 34.19
</table>
<tableCaption confidence="0.890302">
Table 5: MRR on PPDB dev data for training on
an increasing number of relations.
Table 4 shows that all of our models obtain
reductions in error as compared to the baseline
(cbow), with Joint→RCM obtaining the largest re-
duction. This suggests that our embeddings are
better suited for semantic tasks, in this case judged
by human annotations.
</tableCaption>
<table confidence="0.999784333333333">
Model αRCM PPDB Dev
10k 30k 100k
Joint-p 1 x 10−1 47.17 36.74 24.50
5 x 10−2 54.31 44.52 33.07
1 x 10−2 59.75 50.93 37.73
1 x 10−3 57.00 46.84 34.45
</table>
<tableCaption confidence="0.971309">
Table 6: Effect of learning rate αRCM on MRR for
the RCM objective in Joint models.
</tableCaption>
<subsectionHeader confidence="0.991059">
4.4 Analysis
</subsectionHeader>
<bodyText confidence="0.999913619047619">
We conclude our experiments with an analysis of
modeling choices. First, pre-training RCM models
gives significant improvements in both measuring
semantic similarity and capturing human judge-
ments (compare “p” vs. “r” results.) Second, the
number of relations used for RCM training is an
important factor. Table 5 shows the effect on dev
data of using various numbers of relations. While
we see improvements from XL to XXL (5 times as
many relations), we get worse results on XXXL,
likely because this set contains the lowest quality
relations in PPDB. Finally, Table 6 shows different
learning rates αRCM for the RCM objective.
The baseline word2vec and the joint model have
nearly the same averaged running times (2,577s
and 2,644s respectively), since they have same
number of threads for the CBOW objective and the
joint model uses additional threads for the RCM
objective. The RCM models are trained with sin-
gle thread for 100 epochs. When trained on the
PPDB-XXL data, it spends 2,931s on average.
</bodyText>
<sectionHeader confidence="0.999348" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999987894736842">
We have presented a new learning objective for
neural language models that incorporates prior
knowledge contained in resources to improve
learned word embeddings. We demonstrated that
the Relation Constrained Model can lead to better
semantic embeddings by incorporating resources
like PPDB, leading to better language modeling,
semantic similarity metrics, and predicting hu-
man semantic judgements. Our implementation is
based on the word2vec package and we made it
available for general use 2.
We believe that our techniques have implica-
tions beyond those considered in this work. We
plan to explore the embeddings suitability for
other semantics tasks, including the use of re-
sources with both typed and scored relations. Ad-
ditionally, we see opportunities for jointly learn-
ing embeddings across many tasks with many re-
sources, and plan to extend our model accordingly.
</bodyText>
<footnote confidence="0.784464333333333">
Acknowledgements Yu is supported by China
Scholarship Council and by NSFC 61173073.
2https://github.com/Gorov/JointRCM
</footnote>
<page confidence="0.997387">
549
</page>
<sectionHeader confidence="0.996207" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999815152777778">
Yoshua Bengio, Holger Schwenk, Jean-S´ebastien
Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137–186.
Springer.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, Hugo
Larochelle, et al. 2007. Greedy layer-wise training
of deep networks. In Neural Information Processing
Systems (NIPS).
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2012. Joint learning of words
and meaning representations for open-text semantic
parsing. In International Conference on Artificial
Intelligence and Statistics, pages 127–135.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Interna-
tional Conference on Machine Learning (ICML).
Dumitru Erhan, Yoshua Bengio, Aaron Courville,
Pierre-Antoine Manzagol, Pascal Vincent, and Samy
Bengio. 2010. Why does unsupervised pre-training
help deep learning? Journal of Machine Learning
Research (JMLR), 11:625–660.
Christiane Fellbaum. 1999. WordNet. Wiley Online
Library.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL).
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Association for Computational Lin-
guistics (ACL), pages 873–882.
Minh-Thang Luong, Richard Socher, and Christo-
pher D Manning. 2013. Better word representa-
tions with recursive neural networks for morphol-
ogy. In Conference on Natural Language Learning
(CoNLL).
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. arXiv preprint arXiv:1310.4546.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In International Conference on Machine Learning
(ICML).
Andriy Mnih and Yee Whye Teh. 2012. A fast and
simple algorithm for training neural probabilistic
language models. arXiv preprint arXiv:1206.6426.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English gigaword fifth edi-
tion. Technical report, Linguistic Data Consortium.
Deepak Ravichandran, Patrick Pantel, and Eduard
Hovy. 2005. Randomized algorithms and nlp: us-
ing locality sensitive hash function for high speed
noun clustering. In Association for Computational
Linguistics (ACL).
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Empirical Methods in Natural Language
Processing (EMNLP), pages 1631–1642.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Association for
Computational Linguistics (ACL).
Benjamin Van Durme and Ashwin Lall. 2010. On-
line generation of locality sensitive hash signatures.
In Association for Computational Linguistics (ACL),
pages 231–235.
</reference>
<page confidence="0.997287">
550
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.838863">
<title confidence="0.999529">Improving Lexical Embeddings with Semantic Knowledge</title>
<author confidence="0.953155">Yu Dredze</author>
<affiliation confidence="0.951957666666667">Machine Translation Lab Human Language Technology Center of Excellence Harbin Institute of Technology Center for Language and Speech Processing Harbin, China Johns Hopkins University</affiliation>
<address confidence="0.990037">MD 21218</address>
<email confidence="0.999785">mdredze@cs.jhu.edu</email>
<abstract confidence="0.999313">Word embeddings learned on unlabeled data are a popular tool in semantics, but may not capture the desired semantics. We propose a new learning objective that incorporates both a neural language model objective (Mikolov et al., 2013) and prior knowledge from semantic resources to learn improved lexical semantic embeddings. We demonstrate that our embeddings improve over those learned solely on raw text in three settings: language modeling, measuring semantic similarity, and predicting human judgements.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Holger Schwenk</author>
<author>Jean-S´ebastien Sen´ecal</author>
<author>Fr´ederic Morin</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Neural probabilistic language models.</title>
<date>2006</date>
<booktitle>In Innovations in Machine Learning,</booktitle>
<pages>137--186</pages>
<publisher>Springer.</publisher>
<marker>Bengio, Schwenk, Sen´ecal, Morin, Gauvain, 2006</marker>
<rawString>Yoshua Bengio, Holger Schwenk, Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain. 2006. Neural probabilistic language models. In Innovations in Machine Learning, pages 137–186. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Pascal Lamblin</author>
<author>Dan Popovici</author>
<author>Hugo Larochelle</author>
</authors>
<title>Greedy layer-wise training of deep networks.</title>
<date>2007</date>
<booktitle>In Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="8181" citStr="Bengio et al., 2007" startWordPosition="1376" endWordPosition="1379">nvergence; training stops when these threads finish processing the data. We found this an effective method (2) / T Pw exp I ew&apos; P−c≤j≤c,j6=0 ewt+j ) where ew and \ e,w represent the input and output embeddings respectively, i.e., the assignments to X w∈Rwi 1 N XN i=1 1 T XT t=1 X log p (w|wi) w∈Rwi e0wi X− αRCM w 546 for balancing the two objectives. We trained each cbow objective using a single pass over the data set (except for those in Section 4.1), which we empirically verified was sufficient to ensure stable performances on semantic tasks. Model pre-training is critical in deep learning (Bengio et al., 2007; Erhan et al., 2010). We evaluate two strategies: random initialization, and pretraining the embeddings. For pre-training, we first learn using cbow with a random initialization. The resulting trained model is then used to initialize the RCM model. This enables the RCM model to benefit from the unlabeled data, but refine the embeddings constrained by the given relations. Finally, we consider a final model for training embeddings that uses a specific training regime. While the joint model balances between fitting the text and learning relations, modeling the text at the expense of the relation</context>
</contexts>
<marker>Bengio, Lamblin, Popovici, Larochelle, 2007</marker>
<rawString>Yoshua Bengio, Pascal Lamblin, Dan Popovici, Hugo Larochelle, et al. 2007. Greedy layer-wise training of deep networks. In Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Xavier Glorot</author>
<author>Jason Weston</author>
<author>Yoshua Bengio</author>
</authors>
<title>Joint learning of words and meaning representations for open-text semantic parsing.</title>
<date>2012</date>
<booktitle>In International Conference on Artificial Intelligence and Statistics,</booktitle>
<pages>127--135</pages>
<contexts>
<context position="2444" citStr="Bordes et al., 2012" startWordPosition="369" endWordPosition="372">c resources. We propose a new training objective for learning word embeddings that incorporates prior ∗This work was done while the author was visiting JHU. knowledge. Our model builds on word2vec (Mikolov et al., 2013), a neural network based language model that learns word embeddings by maximizing the probability of raw text. We extend the objective to include prior knowledge about synonyms from semantic resources; we consider both the Paraphrase Database (Ganitkevitch et al., 2013) and WordNet (Fellbaum, 1999), which annotate semantic relatedness between words. The latter was also used in (Bordes et al., 2012) for training a network for predicting synset relation. The combined objective maximizes both the probability of the raw corpus and encourages embeddings to capture semantic relations from the resources. We demonstrate improvements in our embeddings on three tasks: language modeling, measuring word similarity, and predicting human judgements on word pairs. 2 Learning Embeddings We present a general model for learning word embeddings that incorporates prior knowledge available for a domain. While in this work we consider semantics, our model could incorporate prior knowledge from many types of </context>
</contexts>
<marker>Bordes, Glorot, Weston, Bengio, 2012</marker>
<rawString>Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. 2012. Joint learning of words and meaning representations for open-text semantic parsing. In International Conference on Artificial Intelligence and Statistics, pages 127–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="947" citStr="Collobert and Weston, 2008" startWordPosition="132" endWordPosition="136">Word embeddings learned on unlabeled data are a popular tool in semantics, but may not capture the desired semantics. We propose a new learning objective that incorporates both a neural language model objective (Mikolov et al., 2013) and prior knowledge from semantic resources to learn improved lexical semantic embeddings. We demonstrate that our embeddings improve over those learned solely on raw text in three settings: language modeling, measuring semantic similarity, and predicting human judgements. 1 Introduction Word embeddings are popular representations for syntax (Turian et al., 2010; Collobert and Weston, 2008; Mnih and Hinton, 2007), semantics (Huang et al., 2012; Socher et al., 2013), morphology (Luong et al., 2013) and other areas. A long line of embeddings work, such as LSA and randomized embeddings (Ravichandran et al., 2005; Van Durme and Lall, 2010), has recently turned to neural language models (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010). Unsupervised learning can take advantage of large corpora, which can produce impressive results. However, the main drawback of unsupervised learning is that the learned embeddings may not be suited for the task of interest. Consi</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dumitru Erhan</author>
<author>Yoshua Bengio</author>
<author>Aaron Courville</author>
<author>Pierre-Antoine Manzagol</author>
<author>Pascal Vincent</author>
<author>Samy Bengio</author>
</authors>
<title>Why does unsupervised pre-training help deep learning?</title>
<date>2010</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<pages>11--625</pages>
<contexts>
<context position="8202" citStr="Erhan et al., 2010" startWordPosition="1380" endWordPosition="1383">tops when these threads finish processing the data. We found this an effective method (2) / T Pw exp I ew&apos; P−c≤j≤c,j6=0 ewt+j ) where ew and \ e,w represent the input and output embeddings respectively, i.e., the assignments to X w∈Rwi 1 N XN i=1 1 T XT t=1 X log p (w|wi) w∈Rwi e0wi X− αRCM w 546 for balancing the two objectives. We trained each cbow objective using a single pass over the data set (except for those in Section 4.1), which we empirically verified was sufficient to ensure stable performances on semantic tasks. Model pre-training is critical in deep learning (Bengio et al., 2007; Erhan et al., 2010). We evaluate two strategies: random initialization, and pretraining the embeddings. For pre-training, we first learn using cbow with a random initialization. The resulting trained model is then used to initialize the RCM model. This enables the RCM model to benefit from the unlabeled data, but refine the embeddings constrained by the given relations. Finally, we consider a final model for training embeddings that uses a specific training regime. While the joint model balances between fitting the text and learning relations, modeling the text at the expense of the relations may negatively impa</context>
</contexts>
<marker>Erhan, Bengio, Courville, Manzagol, Vincent, Bengio, 2010</marker>
<rawString>Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and Samy Bengio. 2010. Why does unsupervised pre-training help deep learning? Journal of Machine Learning Research (JMLR), 11:625–660.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<date>1999</date>
<publisher>WordNet. Wiley Online Library.</publisher>
<contexts>
<context position="2342" citStr="Fellbaum, 1999" startWordPosition="354" endWordPosition="355">jective. However, rich prior knowledge exists for many tasks, and there are numerous such semantic resources. We propose a new training objective for learning word embeddings that incorporates prior ∗This work was done while the author was visiting JHU. knowledge. Our model builds on word2vec (Mikolov et al., 2013), a neural network based language model that learns word embeddings by maximizing the probability of raw text. We extend the objective to include prior knowledge about synonyms from semantic resources; we consider both the Paraphrase Database (Ganitkevitch et al., 2013) and WordNet (Fellbaum, 1999), which annotate semantic relatedness between words. The latter was also used in (Bordes et al., 2012) for training a network for predicting synset relation. The combined objective maximizes both the probability of the raw corpus and encourages embeddings to capture semantic relations from the resources. We demonstrate improvements in our embeddings on three tasks: language modeling, measuring word similarity, and predicting human judgements on word pairs. 2 Learning Embeddings We present a general model for learning word embeddings that incorporates prior knowledge available for a domain. Whi</context>
<context position="9497" citStr="Fellbaum, 1999" startWordPosition="1593" endWordPosition="1595"> of word2vec. Therefore, we use the embeddings from a trained joint model to pre-train an RCM model. We call this setting Joint→RCM. 3 Evaluation For training cbow we use the New York Times (NYT) 1994-97 subset from Gigaword v5.0 (Parker et al., 2011). We select 1,000 paragraphs each for dev and test data from the December 2010 portion of the NYT. Sentences are tokenized using OpenNLP1, yielding 518,103,942 tokens for training, 42,953 tokens for dev and 41,344 for test. We consider two resources for training the RCM term: the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) and WordNet (Fellbaum, 1999). For each semantic pair extracted from these resources, we add a relation to the RCM objective. Since we use both resources for evaluation, we divide each into train, dev and test. PPDB is an automatically extracted dataset containing tens of millions of paraphrase pairs, including words and phrases. We used the “lexical” version of PPDB (no phrases) and filtered to include pairs that contained words found in the 200,000 most frequent words in the NYT corpus, which ensures each word in the relations had support in the text corpus. Next, we removed duplicate pairs: if &lt;A,B&gt; occurred in PPDB, w</context>
</contexts>
<marker>Fellbaum, 1999</marker>
<rawString>Christiane Fellbaum. 1999. WordNet. Wiley Online Library.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>PPDB: The paraphrase database.</title>
<date>2013</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<marker>Ganitkevitch, Van Durme, Callison-Burch, 2013</marker>
<rawString>Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. PPDB: The paraphrase database. In North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>873--882</pages>
<contexts>
<context position="1002" citStr="Huang et al., 2012" startWordPosition="142" endWordPosition="145"> semantics, but may not capture the desired semantics. We propose a new learning objective that incorporates both a neural language model objective (Mikolov et al., 2013) and prior knowledge from semantic resources to learn improved lexical semantic embeddings. We demonstrate that our embeddings improve over those learned solely on raw text in three settings: language modeling, measuring semantic similarity, and predicting human judgements. 1 Introduction Word embeddings are popular representations for syntax (Turian et al., 2010; Collobert and Weston, 2008; Mnih and Hinton, 2007), semantics (Huang et al., 2012; Socher et al., 2013), morphology (Luong et al., 2013) and other areas. A long line of embeddings work, such as LSA and randomized embeddings (Ravichandran et al., 2005; Van Durme and Lall, 2010), has recently turned to neural language models (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010). Unsupervised learning can take advantage of large corpora, which can produce impressive results. However, the main drawback of unsupervised learning is that the learned embeddings may not be suited for the task of interest. Consider semantic embeddings, which may capture a notion of </context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations via global context and multiple word prototypes. In Association for Computational Linguistics (ACL), pages 873–882.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh-Thang Luong</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Better word representations with recursive neural networks for morphology.</title>
<date>2013</date>
<booktitle>In Conference on Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="1057" citStr="Luong et al., 2013" startWordPosition="152" endWordPosition="155"> We propose a new learning objective that incorporates both a neural language model objective (Mikolov et al., 2013) and prior knowledge from semantic resources to learn improved lexical semantic embeddings. We demonstrate that our embeddings improve over those learned solely on raw text in three settings: language modeling, measuring semantic similarity, and predicting human judgements. 1 Introduction Word embeddings are popular representations for syntax (Turian et al., 2010; Collobert and Weston, 2008; Mnih and Hinton, 2007), semantics (Huang et al., 2012; Socher et al., 2013), morphology (Luong et al., 2013) and other areas. A long line of embeddings work, such as LSA and randomized embeddings (Ravichandran et al., 2005; Van Durme and Lall, 2010), has recently turned to neural language models (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010). Unsupervised learning can take advantage of large corpora, which can produce impressive results. However, the main drawback of unsupervised learning is that the learned embeddings may not be suited for the task of interest. Consider semantic embeddings, which may capture a notion of semantics that improves one semantic task but harms ano</context>
</contexts>
<marker>Luong, Socher, Manning, 2013</marker>
<rawString>Minh-Thang Luong, Richard Socher, and Christopher D Manning. 2013. Better word representations with recursive neural networks for morphology. In Conference on Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality. arXiv preprint arXiv:1310.4546.</title>
<date>2013</date>
<contexts>
<context position="2043" citStr="Mikolov et al., 2013" startWordPosition="308" endWordPosition="311">, the main drawback of unsupervised learning is that the learned embeddings may not be suited for the task of interest. Consider semantic embeddings, which may capture a notion of semantics that improves one semantic task but harms another. Controlling this behavior is challenging with an unsupervised objective. However, rich prior knowledge exists for many tasks, and there are numerous such semantic resources. We propose a new training objective for learning word embeddings that incorporates prior ∗This work was done while the author was visiting JHU. knowledge. Our model builds on word2vec (Mikolov et al., 2013), a neural network based language model that learns word embeddings by maximizing the probability of raw text. We extend the objective to include prior knowledge about synonyms from semantic resources; we consider both the Paraphrase Database (Ganitkevitch et al., 2013) and WordNet (Fellbaum, 1999), which annotate semantic relatedness between words. The latter was also used in (Bordes et al., 2012) for training a network for predicting synset relation. The combined objective maximizes both the probability of the raw corpus and encourages embeddings to capture semantic relations from the resour</context>
<context position="6706" citStr="Mikolov et al. (2013)" startWordPosition="1119" endWordPosition="1122">eriments, RCM uses the output embeddings of cbow. We learn embeddings using stochastic gradient ascent. Updates for the first term for e0 and e are: e0 w − αcbow �σ(f(w)) − I[w=wt] � &apos; Xt+c ewj j=t−c Xewj − αcbow (σ(f(w)) − I[w=wt]) &apos; e0w, w where σ(x) = exp{x}/(1 + exp{x}), I[x] is 1 when x is true, f(w) = e0 &gt;Pt+c j=t−c ewj. Second w term updates are: � � e0w − αRCM σ(f0(w)) − I[w∈Rwi] &apos; e0wi � � σ(f0(w)) − I[w∈Rwi] &apos; e0w, where f0(w) = e0 &gt;e0wi. We use two learning w rates: αcbow and αRCM. 2.4 Parameter Estimation All three models (cbow, RCM and joint) use the same training scheme based on Mikolov et al. (2013). There are several choices to make in parameter estimation; we present the best performing choices used in our results. We use noise contrastive estimation (NCE) (Mnih and Teh, 2012), which approximately maximizes the log probability of the softmax objective (Eq. 2). For each objective (cbow or RCM), we sample 15 words as negative samples for each training instance according to their frequencies in raw texts (i.e. training data of cbow). Suppose w has frequency u(w), then the probability of sampling w is p(w) a u(w)3/4. We use distributed training, where shared embeddings are updated by each </context>
<context position="13334" citStr="Mikolov et al., 2013" startWordPosition="2227" endWordPosition="2230">uencies less than 5, which is the default setting of word2vec. 4.1 Language Modeling Word2vec is fundamentally a language model, which allows us to compute standard evaluation metrics on a held out dataset. After obtaining trained embeddings from any of our objectives, we use the embeddings in the word2vec model to measure perplexity of the test set. Measuring perplexity means computing the exact probability of each word, which requires summation over all words in the vocabulary in the denominator of the softmax. Therefore, we also trained the language models with hierarchical classification (Mikolov et al., 2013) strategy (HS). The averaged perplexities are reported on the NYT test set. While word2vec and joint are trained as language models, RCM is not. In fact, RCM does not even observe all the words that appear in the training set, so it makes little sense to use the RCM embeddings directly for language modeling. Therefore, in order to make fair comparison, for every set of trained embeddings, we fix them as input embedding for word2vec, then learn the remaining input embeddings (words not in the relations) and all the output embeddings using cbow. Since this involves running cbow on NYT data for 2</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. arXiv preprint arXiv:1310.4546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="971" citStr="Mnih and Hinton, 2007" startWordPosition="137" endWordPosition="140">nlabeled data are a popular tool in semantics, but may not capture the desired semantics. We propose a new learning objective that incorporates both a neural language model objective (Mikolov et al., 2013) and prior knowledge from semantic resources to learn improved lexical semantic embeddings. We demonstrate that our embeddings improve over those learned solely on raw text in three settings: language modeling, measuring semantic similarity, and predicting human judgements. 1 Introduction Word embeddings are popular representations for syntax (Turian et al., 2010; Collobert and Weston, 2008; Mnih and Hinton, 2007), semantics (Huang et al., 2012; Socher et al., 2013), morphology (Luong et al., 2013) and other areas. A long line of embeddings work, such as LSA and randomized embeddings (Ravichandran et al., 2005; Van Durme and Lall, 2010), has recently turned to neural language models (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010). Unsupervised learning can take advantage of large corpora, which can produce impressive results. However, the main drawback of unsupervised learning is that the learned embeddings may not be suited for the task of interest. Consider semantic embeddings,</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Yee Whye Teh</author>
</authors>
<title>A fast and simple algorithm for training neural probabilistic language models. arXiv preprint arXiv:1206.6426.</title>
<date>2012</date>
<contexts>
<context position="6889" citStr="Mnih and Teh, 2012" startWordPosition="1150" endWordPosition="1153">Xt+c ewj j=t−c Xewj − αcbow (σ(f(w)) − I[w=wt]) &apos; e0w, w where σ(x) = exp{x}/(1 + exp{x}), I[x] is 1 when x is true, f(w) = e0 &gt;Pt+c j=t−c ewj. Second w term updates are: � � e0w − αRCM σ(f0(w)) − I[w∈Rwi] &apos; e0wi � � σ(f0(w)) − I[w∈Rwi] &apos; e0w, where f0(w) = e0 &gt;e0wi. We use two learning w rates: αcbow and αRCM. 2.4 Parameter Estimation All three models (cbow, RCM and joint) use the same training scheme based on Mikolov et al. (2013). There are several choices to make in parameter estimation; we present the best performing choices used in our results. We use noise contrastive estimation (NCE) (Mnih and Teh, 2012), which approximately maximizes the log probability of the softmax objective (Eq. 2). For each objective (cbow or RCM), we sample 15 words as negative samples for each training instance according to their frequencies in raw texts (i.e. training data of cbow). Suppose w has frequency u(w), then the probability of sampling w is p(w) a u(w)3/4. We use distributed training, where shared embeddings are updated by each thread based on training data within the thread, i.e., asynchronous stochastic gradient ascent. For the joint model, we assign threads to the cbow or RCM objective with a balance of 1</context>
</contexts>
<marker>Mnih, Teh, 2012</marker>
<rawString>Andriy Mnih and Yee Whye Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. arXiv preprint arXiv:1206.6426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<title>English gigaword fifth edition.</title>
<date>2011</date>
<tech>Technical report,</tech>
<institution>Linguistic Data Consortium.</institution>
<contexts>
<context position="9133" citStr="Parker et al., 2011" startWordPosition="1533" endWordPosition="1536">ddings constrained by the given relations. Finally, we consider a final model for training embeddings that uses a specific training regime. While the joint model balances between fitting the text and learning relations, modeling the text at the expense of the relations may negatively impact the final embeddings for tasks that use the embeddings outside of the context of word2vec. Therefore, we use the embeddings from a trained joint model to pre-train an RCM model. We call this setting Joint→RCM. 3 Evaluation For training cbow we use the New York Times (NYT) 1994-97 subset from Gigaword v5.0 (Parker et al., 2011). We select 1,000 paragraphs each for dev and test data from the December 2010 portion of the NYT. Sentences are tokenized using OpenNLP1, yielding 518,103,942 tokens for training, 42,953 tokens for dev and 41,344 for test. We consider two resources for training the RCM term: the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) and WordNet (Fellbaum, 1999). For each semantic pair extracted from these resources, we add a relation to the RCM objective. Since we use both resources for evaluation, we divide each into train, dev and test. PPDB is an automatically extracted dataset containing </context>
</contexts>
<marker>Parker, Graff, Kong, Chen, Maeda, 2011</marker>
<rawString>Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2011. English gigaword fifth edition. Technical report, Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Patrick Pantel</author>
<author>Eduard Hovy</author>
</authors>
<title>Randomized algorithms and nlp: using locality sensitive hash function for high speed noun clustering.</title>
<date>2005</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1171" citStr="Ravichandran et al., 2005" startWordPosition="172" endWordPosition="175">., 2013) and prior knowledge from semantic resources to learn improved lexical semantic embeddings. We demonstrate that our embeddings improve over those learned solely on raw text in three settings: language modeling, measuring semantic similarity, and predicting human judgements. 1 Introduction Word embeddings are popular representations for syntax (Turian et al., 2010; Collobert and Weston, 2008; Mnih and Hinton, 2007), semantics (Huang et al., 2012; Socher et al., 2013), morphology (Luong et al., 2013) and other areas. A long line of embeddings work, such as LSA and randomized embeddings (Ravichandran et al., 2005; Van Durme and Lall, 2010), has recently turned to neural language models (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010). Unsupervised learning can take advantage of large corpora, which can produce impressive results. However, the main drawback of unsupervised learning is that the learned embeddings may not be suited for the task of interest. Consider semantic embeddings, which may capture a notion of semantics that improves one semantic task but harms another. Controlling this behavior is challenging with an unsupervised objective. However, rich prior knowledge exist</context>
</contexts>
<marker>Ravichandran, Pantel, Hovy, 2005</marker>
<rawString>Deepak Ravichandran, Patrick Pantel, and Eduard Hovy. 2005. Randomized algorithms and nlp: using locality sensitive hash function for high speed noun clustering. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="1024" citStr="Socher et al., 2013" startWordPosition="146" endWordPosition="149">not capture the desired semantics. We propose a new learning objective that incorporates both a neural language model objective (Mikolov et al., 2013) and prior knowledge from semantic resources to learn improved lexical semantic embeddings. We demonstrate that our embeddings improve over those learned solely on raw text in three settings: language modeling, measuring semantic similarity, and predicting human judgements. 1 Introduction Word embeddings are popular representations for syntax (Turian et al., 2010; Collobert and Weston, 2008; Mnih and Hinton, 2007), semantics (Huang et al., 2012; Socher et al., 2013), morphology (Luong et al., 2013) and other areas. A long line of embeddings work, such as LSA and randomized embeddings (Ravichandran et al., 2005; Van Durme and Lall, 2010), has recently turned to neural language models (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010). Unsupervised learning can take advantage of large corpora, which can produce impressive results. However, the main drawback of unsupervised learning is that the learned embeddings may not be suited for the task of interest. Consider semantic embeddings, which may capture a notion of semantics that improve</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Empirical Methods in Natural Language Processing (EMNLP), pages 1631–1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="919" citStr="Turian et al., 2010" startWordPosition="128" endWordPosition="131">@cs.jhu.edu Abstract Word embeddings learned on unlabeled data are a popular tool in semantics, but may not capture the desired semantics. We propose a new learning objective that incorporates both a neural language model objective (Mikolov et al., 2013) and prior knowledge from semantic resources to learn improved lexical semantic embeddings. We demonstrate that our embeddings improve over those learned solely on raw text in three settings: language modeling, measuring semantic similarity, and predicting human judgements. 1 Introduction Word embeddings are popular representations for syntax (Turian et al., 2010; Collobert and Weston, 2008; Mnih and Hinton, 2007), semantics (Huang et al., 2012; Socher et al., 2013), morphology (Luong et al., 2013) and other areas. A long line of embeddings work, such as LSA and randomized embeddings (Ravichandran et al., 2005; Van Durme and Lall, 2010), has recently turned to neural language models (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010). Unsupervised learning can take advantage of large corpora, which can produce impressive results. However, the main drawback of unsupervised learning is that the learned embeddings may not be suited for</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Online generation of locality sensitive hash signatures.</title>
<date>2010</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>231--235</pages>
<marker>Van Durme, Lall, 2010</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2010. Online generation of locality sensitive hash signatures. In Association for Computational Linguistics (ACL), pages 231–235.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>