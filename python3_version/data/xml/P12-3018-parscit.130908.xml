<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.059031">
<title confidence="0.993847">
INPRO_iSS: A Component for Just-In-Time Incremental Speech Synthesis
</title>
<author confidence="0.996996">
Timo Baumann
</author>
<affiliation confidence="0.859675333333333">
University of Hamburg
Department for Informatics
Germany
</affiliation>
<email confidence="0.99105">
baumann@informatik.uni-hamburg.de
</email>
<author confidence="0.993664">
David Schlangen
</author>
<affiliation confidence="0.835849666666667">
University of Bielefeld
Faculty of Linguistics and Literary Studies
Germany
</affiliation>
<email confidence="0.642284">
david.schlangen@uni-bielefeld.de
</email>
<sectionHeader confidence="0.985743" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999389111111111">
We present a component for incremental
speech synthesis (iSS) and a set of applications
that demonstrate its capabilities. This compo-
nent can be used to increase the responsivity
and naturalness of spoken interactive systems.
While iSS can show its full strength in systems
that generate output incrementally, we also dis-
cuss how even otherwise unchanged systems
may profit from its capabilities.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999859534883721">
Current state of the art in speech synthesis for spoken
dialogue systems (SDSs) is for the synthesis com-
ponent to expect full utterances (in textual form) as
input and to deliver an audio stream verbalising this
full utterance. At best, timing information is returned
as well so that a control component can determine in
case of an interruption / barge-in by the user where
in the utterance this happened (Edlund, 2008; Mat-
suyama et al., 2010).
We want to argue here that providing capabilities
to speech synthesis components for dealing with units
smaller than full utterances can be beneficial for a
whole range of interactive speech-based systems. In
the easiest case, incremental synthesis simply reduces
the utterance-initial delay before speech output starts,
as output already starts when its beginning has been
produced. In an otherwise conventional dialogue sys-
tem, the synthesis module could make it possible
to interrupt the output speech stream (e. g., when a
noise event is detected that makes it likely that the
user will not be able to hear what is being said), and
continue production when the interruption is over. If
other SDS components are adapted more to take ad-
vantage of incremental speech synthesis, even more
flexible behaviours can be realised, such as providing
utterances in installments (Clark, 1996) that prompt
for backchannel signals, which in turn can prompt
different utterance continuations, or starting an utter-
ance before all information required in the utterance
is available (“so, uhm, there are flights to Seoul on uh
... ”), signaling that the turn is being held. Another,
less conventional type of speech-based system that
could profit from iSS is “babelfish-like” simultaneous
speech-to-speech translation.
Research on architectures, higher-level process-
ing modules and lower-level processing modules that
would enable such behaviour is currently underway
(Skantze and Schlangen, 2009; Skantze and Hjal-
marsson, 2010; Baumann and Schlangen, 2011), but
a synthesis component that would unlock the full
potential of such strategies is so far missing. In this
paper, we present such a component, which is capa-
ble of
</bodyText>
<listItem confidence="0.991077833333333">
(a) starting to speak before utterance processing has
finished;
(b) handling edits made to (as-yet unspoken) parts of
the utterance even while a prefix is already being
spoken;
(c) enabling adaptations of delivery parameters such
as speaking rate or pitch;
(d) autonomously making appropriate delivery-
related decisions;
(e) providing information about progress in delivery;
and, last but not least,
(f) running in real time.
</listItem>
<bodyText confidence="0.9993202">
Our iSS component is built on top of an exist-
ing non-incremental synthesis component, MaryTTS
(Schröder and Trouvain, 2003), and on an existing
architecture for incremental processing, INPROTK
(Baumann and Schlangen, 2012).
</bodyText>
<page confidence="0.991634">
103
</page>
<note confidence="0.6843135">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 103–108,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.99974325">
After a discussion of related work (Section 2), we
describe the basic elements of our iSS component
(Section 3) and some demonstrator applications that
we created which showcase certain abilities.1
</bodyText>
<sectionHeader confidence="0.999346" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.982229333333334">
Typically, in current SDSs utterances are gener-
ated (either by lookup/template-based generation, or,
less commonly, by concept-to-utterance natural lan-
guage generation (NLG)) and then synthesised in full
(McTear, 2002). There is very little work on incre-
mental synthesis (i.e., one that would work with units
smaller than full utterances). Edlund (2008) outlines
some requirements for incremental speech synthe-
sis: to give constant feedback to the dialogue system
about what has been delivered, to be interruptible
(and possibly continue from that position), and to run
in real time. Edlund (2008) also presents a prototype
that meets these requirements, but is limited to di-
phone synthesis that is performed non-incrementally
before utterance delivery starts. We go beyond this
in processing just-in-time, and also enabling changes
during delivery.
Skantze and Hjalmarsson (2010) describe a sys-
tem that generates utterances incrementally (albeit
in a WOz-enviroment), allowing earlier components
to incrementally produce and revise their hypothesis
about the user’s utterance. The system can automati-
cally play hesitations if by the time it has the turn it
does not know what to produce yet. They show that
users prefer such a system over a non-incremental
one, even though it produced longer dialogues. Our
approach is complementary to this work, as it tar-
gets a lower layer, the realisation or synthesis layer.
Where their system relies on ‘regular’ speech syn-
thesis which is called on relatively short utterance
fragments (and thus pays for the increase in respon-
siveness with a reduction in synthesis quality, esp.
regarding prosody), we aim to incrementalize the
speech synthesis component itself.
Dutoit et al. (2011) have presented an incremental
formulation for HMM-based speech synthesis. How-
ever, their system works offline and is fed by non-
incrementally produced phoneme target sequences.
1The code of the toolkit and its iSS component and the demo
applications discussed below have been released as open-source
athttp://inprotk.sourceforge.net.
We aim for a fully incremental speech synthesis com-
ponent that can be integrated into dialogue systems.
There is some work on incremental NLG (Kilger
and Finkler, 1995; Finkler, 1997; Guhe, 2007); how-
ever, that work does not concern itself with the actual
synthesis of speech and hence describes only what
would generate the input to our component.
</bodyText>
<sectionHeader confidence="0.986075" genericHeader="method">
3 Incremental Speech Synthesis
</sectionHeader>
<subsectionHeader confidence="0.999639">
3.1 Background on Speech Synthesis
</subsectionHeader>
<bodyText confidence="0.999972621621622">
Text-to-speech (TTS) synthesis normally proceeds in
a top-down fashion, starting on the utterance level
(for stress patterns and sentence-level intonation) and
descending to words and phonemes (for pronunci-
ation details), in order to make globally optimised
decisions (Taylor, 2009). In that way, target phoneme
sequences annotated with durations and pitch con-
tours are generated, in what is called the linguistic
pre-processing step.
The then following synthesis step proper can be
executed in one of several ways, with HMM-based
and unit-selection synthesis currently being seen as
producing the perceptually best results (Taylor, 2009).
The former works by first turning the target sequence
into a sequence of HMM states; a global optimiza-
tion then computes a stream of vocoding features
that optimize both HMM emission probabilities and
continuity constraints (Tokuda et al., 2000). Finally,
the parameter frames are fed to a vocoder which gen-
erates the speech audio signal. Unit-selection, in
contrast, searches for the best sequence of (variably
sized) units of speech in a large, annotated corpus
of recordings, aiming to find a sequence that closely
matches the target sequence.
As mentioned above, Dutoit et al. (2011) have pre-
sented an online formulation of the optimization step
in HMM-based synthesis. Beyond this, two other fac-
tors influenced our decision to follow the HMM-based
approach: (a) HMM-based synthesis nicely separates
the production of vocoding parameter frames from
the production of the speech audio signal, which
allows for more fine-grained concurrent processing
(see next subsection); (b) parameters are partially
independent in the vocoding frames, which makes
it possible to manipulate e. g. pitch independently
(and outside of the HMM framework) without altering
other parameters or deteriorating speech quality.
</bodyText>
<page confidence="0.997777">
104
</page>
<figureCaption confidence="0.992650333333333">
Figure 1: Hierarchic structure of incremental units describ-
ing an example utterance as it is being produced during
utterance delivery.
</figureCaption>
<subsectionHeader confidence="0.999478">
3.2 System Architecture
</subsectionHeader>
<bodyText confidence="0.99996690625">
Our component works by reducing the aforemen-
tioned top-down requirements. We found that it is
not necessary to work out all details at one level
of processing before starting to process at the next
lower level. For example, not all words of the utter-
ance need to be known to produce the sentence-level
intonation (which itself however is necessary to de-
termine pitch contours) as long as a structural outline
of the utterance is available. Likewise, post-lexical
phonological processes can be computed as long
as a local context of one word is available; vocod-
ing parameter computation (which must model co-
articulation effects) in turn can be satisfied with just
one phoneme of context; vocoding itself does not
need any lookahead at all (aside from audio buffering
considerations).
Thus, our component generates its data structures
incrementally in a top-down-and-left-to-right fashion
with different amounts of pre-planning, using sev-
eral processing modules that work concurrently. This
results in a ‘triangular’ structure (illustrated in Fig-
ure 1) where only the absolutely required minimum
has to be specified at each level, allowing for later
adaptations with few or no recomputations required.
As an aside, we observe that our component’s ar-
chitecture happens to correspond rather closely to
Levelt’s (1989) model of human speech production.
Levelt distinguishes several, partially independent
processing modules (conceptualization, formulation,
articulation, see Figure 1) that function incrementally
and “in a highly automatic, reflex-like way” (Levelt,
1989, p. 2).
</bodyText>
<subsectionHeader confidence="0.998155">
3.3 Technical Overview of Our System
</subsectionHeader>
<bodyText confidence="0.999924304347826">
As a basis, we use MaryTTS (Schröder and Trou-
vain, 2003), but we replace Mary’s internal data struc-
tures with structures that support incremental spec-
ifications; these we take from an extant incremen-
tal spoken dialogue system architecture and toolkit,
INPROTK (Schlangen et al., 2010; Baumann and
Schlangen, 2012). In this architecture, incremental
processing as the processing of incremental units
(IUs), which are the smallest ‘chunks’ of information
at a specific level (such as words, or phonemes, as
can be seen in Figure 1). IUs are interconnected to
form a network (e. g. words keep links to their asso-
ciated phonemes, and vice-versa) which stores the
system’s complete information state.
The iSS component takes an IU sequence of
chunks of words as input (from an NLG component).
Crucially, this sequence can then still be modified,
through: (a) continuations, which simply link further
words to the end of the sequence; or (b) replacements,
where elements in the sequence are “unlinked” and
other elements are spliced in. Additionally, a chunk
can be marked as open; this has the effect of linking
to a special hesitation word, which is produced only
if it is not replaced (by the NLG) in time with other
material.
Technically, the representation levels below the
chunk level are generated in our component by
MaryTTS’s linguistic preprocessing and converting
the output to IU structures. Our component provides
for two modes of operation: Either using MaryTTS’
HMM optimization routines which non-incrementally
solve a large matrix operation and subsequently iter-
atively optimize the global variance constraint (Toda
and Tokuda, 2007). Or, using the incremental algo-
rithm as proposed by Dutoit et al. (2011). In our
implementation of this algorithm, HMM emissions
are computed with one phoneme of context in both
directions; Dutoit et al. (2011) have found this set-
ting to only slightly degrade synthesis quality. While
the former mode incurs some utterance-initial delay,
switching between alternatives and prosodic alter-
ation can be performed at virtually no lookahead,
while requiring just little lookahead for the truly
incremental mode. The resulting vocoding frames
then are attached to their corresponding phoneme
units. Phoneme units then contain all the information
</bodyText>
<page confidence="0.998969">
105
</page>
<figureCaption confidence="0.970594333333333">
Figure 2: Example application that showcases just-in-time
manipulation of prosodic aspects (tempo and pitch) of the
ongoing utterance.
</figureCaption>
<bodyText confidence="0.999987642857143">
needed for the final vocoding step, in an accessible
form, which makes possible various manipulations
before the final synthesis step.
The lowest level module of our component is what
may be called a crawling vocoder, which actively
moves along the phoneme IU layer, querying each
phoneme for its parameter frames one-by-one and
producing the corresponding audio via vocoding. The
vocoding algorithm is entirely incremental, making
it possible to vocode “just-in-time”: only when audio
is needed to keep the sound card buffer full does the
vocoder query for a next parameter frame. This is
what gives the higher levels the maximal amount of
time for re-planning, i. e., to be incremental.
</bodyText>
<subsectionHeader confidence="0.998897">
3.4 Quality of Results
</subsectionHeader>
<bodyText confidence="0.999962590909091">
As these descriptions should have made clear, there
are some elements in the processing steps in our iSS
component that aren’t yet fully incremental, such as
assigning a sentence-level prosody. The best results
are thus achieved if a full utterance is presented to the
component initially, which is used for computation of
prosody, and of which then elements may be changed
(e. g., adjectives are replaced by different ones) on the
fly. It is unavoidable, though, that there can be some
“breaks” at the seams where elements are replaced.
Moreover, the way feature frames can be modified
(as described below) and the incremental HMM op-
timization method may lead to deviations from the
global optimum. Finally, our system still relies on
Mary’s non-incremental HMM state selection tech-
nique which uses decision trees with non-incremental
features.
However, preliminary evaluation of the compo-
nent’s prosody given varying amounts of lookahead
indicate that degradations are reasonably small. Also,
the benefits in naturalness of behaviour enabled by
iSS may outweigh the drawback in prosodic quality.
</bodyText>
<sectionHeader confidence="0.998621" genericHeader="method">
4 Interface Demonstrations
</sectionHeader>
<bodyText confidence="0.999984333333333">
We will describe the features of iSS, their implemen-
tation, their programming interface, and correspond-
ing demo applications in the following subsections.
</bodyText>
<subsectionHeader confidence="0.98459">
4.1 Low-Latency Changes to Prosody
</subsectionHeader>
<bodyText confidence="0.999975473684211">
Pitch and tempo can be adapted on the phoneme
IU layer (see Figure 1). Figure 2 shows a demo in-
terface to this functionality. Pitch is determined by
a single parameter in the vocoding frames and can
be adapted independently of other parameters in the
HMM approach. We have implemented capabilities of
adjusting all pitch values in a phoneme by an offset,
or to change the values gradually for all frames in
the phoneme. (The first feature is show-cased in the
application in Figure 2, the latter is used to cancel
utterance-final pitch changes when a continuation is
appended to an ongoing utterance.) Tempo can be
adapted by changing the phoneme units’ durations
which will then repeat (or skip) parameter frames
(for lengthened or shortened phonemes, respectively)
when passing them to the crawling vocoder. Adapta-
tions are conducted with virtually no lookahead, that
is, they can be executed even on a phoneme that is
currently being output.
</bodyText>
<subsectionHeader confidence="0.994242">
4.2 Feedback on Delivery
</subsectionHeader>
<bodyText confidence="0.999505181818182">
We implemented a fine-grained, hierarchical mech-
anism to give detailed feedback on delivery. A new
progress field on IUs marks whether the IU’s produc-
tion is upcoming, ongoing, or completed. Listeners
may subscribe to be notified about such progress
changes using an update interface on IUs. The appli-
cations in Figures 2 and 4 make use of this interface
to mark the words of the utterance in bold for com-
pleted, and in italic for ongoing words (incidentally,
the screenshot in Figure 4 was taken exactly at the
boundary between “delete” and “the”).
</bodyText>
<subsectionHeader confidence="0.999637">
4.3 Low-Latency Switching of Alternatives
</subsectionHeader>
<bodyText confidence="0.999863428571429">
A major goal of iSS is to change what is being said
while the utterance is ongoing. Forward-pointing
same-level links (SLLs, (Schlangen and Skantze,
2009; Baumann and Schlangen, 2012)) as shown
in Figure 3 allow to construct alternative utterance
paths beforehand. Deciding on the actual utterance
continuation is a simple re-ranking of the forward
</bodyText>
<page confidence="0.997382">
106
</page>
<figureCaption confidence="0.99642775">
Figure 3: Incremental units chained together via forward-
pointing same-level links to form an utterance tree.
Figure 4: Example application to showcase just-in-time
selection between different paths in a complex utterance.
</figureCaption>
<bodyText confidence="0.999885625">
SLLs which can be changed until immediately before
the word (or phoneme) in question is being uttered.
The demo application shown in Figure 4 allows the
user to select the path through a fairly complex utter-
ance tree. The user has already decided on the color,
but not on the type of piece to be deleted and hence
the currently selected plan is to play a hesitation (see
below).
</bodyText>
<subsectionHeader confidence="0.996931">
4.4 Extension of the Ongoing Utterance
</subsectionHeader>
<bodyText confidence="0.999841117647059">
In the previous subsection we have shown how alter-
natives in utterances can be selected with very low
latency. Adding continuations (or alternatives) to
an ongoing utterance incurs some delay (some hun-
dred milliseconds), as we ensure that an appropriate
sentence-level prosody for the alternative (or con-
tinuation) is produced by re-running the linguistic
pre-processing on the complete utterance; we then
integrate only the new, changed parts into the ICT
structure (or, if there still is time, parts just before the
change, to account for co-articulation).
Thus, practical applications which use incremen-
tal NLG must generate their next steps with some
lookahead to avoid stalling the output. However, ut-
terances can be marked as non-final, which results in
a special hesitation word being inserted, as explained
below.
</bodyText>
<subsectionHeader confidence="0.995066">
4.5 Autonomously Performing Disfluencies
</subsectionHeader>
<bodyText confidence="0.9998253">
In a multi-threaded, real-time system, the crawling
vocoder may reach the end of synthesis before the
NLG component (in its own thread) has been able
to add a continuation to the ongoing utterance. To
avoid this case, special hesitation words can be in-
serted at the end of a yet unfinished utterance. If the
crawling vocoder nears such a word, a hesitation will
be played, unless a continuation is available. In that
case, the hesitation is skipped (or aborted if currently
ongoing).2
</bodyText>
<subsectionHeader confidence="0.991803">
4.6 Type-to-Speech
</subsectionHeader>
<bodyText confidence="0.999971923076923">
A final demo application show-cases truly incremen-
tal HMM synthesis taken to its most extreme: A text
input window is presented, and each word that is
typed is treated as a single-word chunk which is im-
mediately sent to the incremental synthesizer. (For
this demonstration, synthesis is slowed to half the
regular speed, to account for slow typing speeds and
to highlight the prosodic improvements when more
right context becomes available to iSS.) A use case
with a similar (but probably lower) level of incre-
mentality could be simultaneous speech-to-speech
translation, or type-to-speech for people with speech
disabilities.
</bodyText>
<sectionHeader confidence="0.999567" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.970132058823529">
We have presented a component for incremental
speech synthesis (iSS) and demonstrated its capa-
bilities with a number of example applications. This
component can be used to increase the responsivity
and naturalness of spoken interactive systems. While
iSS can show its full strengths in systems that also
generate output incrementally (a strategy which is
currently seeing some renewed attention), we dis-
cussed how even otherwise unchanged systems may
profit from its capabilities, e. g., in the presence of
intermittent noise. We provide this component in the
hope that it will help spur research on incremental
natural language generation and more interactive spo-
ken dialogue systems, which so far had to made do
with inadequate ways of realising its output.
2Thus, in contrast to (Skantze and Hjalmarsson, 2010), hesi-
tations do not take up any additional time.
</bodyText>
<page confidence="0.99814">
107
</page>
<sectionHeader confidence="0.99025" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999934253521127">
Timo Baumann and David Schlangen. 2011. Predicting
the Micro-Timing of User Input for an Incremental Spo-
ken Dialogue System that Completes a User’s Ongoing
Turn. In Proceedings of SigDial 2011, pages 120–129,
Portland, USA, June.
Timo Baumann and David Schlangen. 2012. The
INPROTK 2012 release. In Proceedings of SDCTD.
to appear.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press.
Thierry Dutoit, Maria Astrinaki, Onur Babacan, Nico-
las d’Alessandro, and Benjamin Picart. 2011. pHTS
for Max/MSP: A Streaming Architecture for Statistical
Parametric Speech Synthesis. Technical Report 1, nu-
mediart Research Program on Digital Art Technologies,
March.
Jens Edlund. 2008. Incremental speech synthesis. In
Second Swedish Language Technology Conference,
pages 53–54, Stockholm, Sweden, November. System
Demonstration.
Wolfgang Finkler. 1997. Automatische Selbstkorrek-
tur bei der inkrementellen Generierung gesprochener
Sprache unter Realzeitbedingungen. Dissertationen zur
Künstlichen Intelligenz. infix Verlag.
Markus Guhe. 2007. Incremental Conceptualization for
Language Production. Lawrence Erlbaum Asso., Inc.,
Mahwah, USA.
Anne Kilger and Wolfgang Finkler. 1995. Incremen-
tal Generation for Real-time Applications. Technical
Report RR-95-11, DFKI, Saarbrücken, Germany.
William J.M. Levelt. 1989. Speaking: From Intention to
Articulation. MIT Press.
Kyoko Matsuyama, Kazunori Komatani, Ryu Takeda,
Toru Takahashi, Tetsuya Ogata, and Hiroshi G. Okuno.
2010. Analyzing User Utterances in Barge-in-able Spo-
ken Dialogue System for Improving Identification Ac-
curacy. In Proceedings of Interspeech, pages 3050–
3053, Makuhari, Japan, September.
Michael McTear. 2002. Spoken Dialogue Technology.
Toward the Conversational User-Interface. Springer,
London, UK.
David Schlangen and Gabriel Skantze. 2009. A General,
Abstract Model of Incremental Dialogue Processing.
In Proceedings of the EACL, Athens, Greece.
David Schlangen, Timo Baumann, Hendrik Buschmeier,
Okko Buß, Stefan Kopp, Gabriel Skantze, and Ramin
Yaghoubzadeh. 2010. Middleware for Incremental
Processing in Conversational Agents. In Proceedings of
SigDial 2010, pages 51–54, Tokyo, Japan, September.
Marc Schröder and Jürgen Trouvain. 2003. The German
Text-to-Speech Synthesis System MARY: A Tool for
Research, Development and Teaching. International
Journal of Speech Technology, 6(3):365–377, October.
Gabriel Skantze and Anna Hjalmarsson. 2010. Towards
incremental speech generation in dialogue systems. In
Proceedings of SigDial 2010, pages 1–8, Tokyo, Japan,
September.
Gabriel Skantze and David Schlangen. 2009. Incremental
dialogue processing in a micro-domain. In Proceedings
of EACL 2009, Athens, Greece, April.
Paul Taylor. 2009. Text-to-Speech Synthesis. Cambridge
Univ Press, Cambridge, UK.
Tomoki Toda and Keiichi Tokuda. 2007. A Speech Pa-
rameter Generation Algorithm Considering Global Vari-
ance for HMM-based Speech Synthesis. IEICE Trans-
actions on Information and Systems, 90(5):816–824.
Keiichi Tokuda, Takayoshi Yoshimura, Takashi Ma-
suko, Takao Kobayashi, and Tadashi Kitamura. 2000.
Speech Parameter Generation Algorithms for HMM-
based Speech Synthesis. In Proceedings of ICASSP
2000, pages 1315–1318, Istanbul, Turkey.
</reference>
<page confidence="0.998369">
108
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.893910">
<title confidence="0.999625">A Component for Just-In-Time Incremental Speech Synthesis</title>
<author confidence="0.951112">Timo</author>
<affiliation confidence="0.9971725">University of Department for</affiliation>
<address confidence="0.987586">Germany</address>
<email confidence="0.99806">baumann@informatik.uni-hamburg.de</email>
<author confidence="0.996442">David Schlangen</author>
<affiliation confidence="0.982337">University of Bielefeld Faculty of Linguistics and Literary Studies</affiliation>
<address confidence="0.987677">Germany</address>
<email confidence="0.999581">david.schlangen@uni-bielefeld.de</email>
<abstract confidence="0.9997594">We present a component for incremental speech synthesis (iSS) and a set of applications that demonstrate its capabilities. This component can be used to increase the responsivity and naturalness of spoken interactive systems. While iSS can show its full strength in systems that generate output incrementally, we also discuss how even otherwise unchanged systems may profit from its capabilities.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Timo Baumann</author>
<author>David Schlangen</author>
</authors>
<title>Predicting the Micro-Timing of User Input for an Incremental Spoken Dialogue System that Completes a User’s Ongoing Turn.</title>
<date>2011</date>
<booktitle>In Proceedings of SigDial</booktitle>
<pages>120--129</pages>
<location>Portland, USA,</location>
<contexts>
<context position="2707" citStr="Baumann and Schlangen, 2011" startWordPosition="400" endWordPosition="403">gnals, which in turn can prompt different utterance continuations, or starting an utterance before all information required in the utterance is available (“so, uhm, there are flights to Seoul on uh ... ”), signaling that the turn is being held. Another, less conventional type of speech-based system that could profit from iSS is “babelfish-like” simultaneous speech-to-speech translation. Research on architectures, higher-level processing modules and lower-level processing modules that would enable such behaviour is currently underway (Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 2010; Baumann and Schlangen, 2011), but a synthesis component that would unlock the full potential of such strategies is so far missing. In this paper, we present such a component, which is capable of (a) starting to speak before utterance processing has finished; (b) handling edits made to (as-yet unspoken) parts of the utterance even while a prefix is already being spoken; (c) enabling adaptations of delivery parameters such as speaking rate or pitch; (d) autonomously making appropriate deliveryrelated decisions; (e) providing information about progress in delivery; and, last but not least, (f) running in real time. Our iSS </context>
</contexts>
<marker>Baumann, Schlangen, 2011</marker>
<rawString>Timo Baumann and David Schlangen. 2011. Predicting the Micro-Timing of User Input for an Incremental Spoken Dialogue System that Completes a User’s Ongoing Turn. In Proceedings of SigDial 2011, pages 120–129, Portland, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timo Baumann</author>
<author>David Schlangen</author>
</authors>
<title>The INPROTK 2012 release.</title>
<date>2012</date>
<booktitle>In Proceedings of SDCTD.</booktitle>
<note>to appear.</note>
<contexts>
<context position="3521" citStr="Baumann and Schlangen, 2012" startWordPosition="526" endWordPosition="529">ak before utterance processing has finished; (b) handling edits made to (as-yet unspoken) parts of the utterance even while a prefix is already being spoken; (c) enabling adaptations of delivery parameters such as speaking rate or pitch; (d) autonomously making appropriate deliveryrelated decisions; (e) providing information about progress in delivery; and, last but not least, (f) running in real time. Our iSS component is built on top of an existing non-incremental synthesis component, MaryTTS (Schröder and Trouvain, 2003), and on an existing architecture for incremental processing, INPROTK (Baumann and Schlangen, 2012). 103 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 103–108, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics After a discussion of related work (Section 2), we describe the basic elements of our iSS component (Section 3) and some demonstrator applications that we created which showcase certain abilities.1 2 Related Work Typically, in current SDSs utterances are generated (either by lookup/template-based generation, or, less commonly, by concept-to-utterance natural language generation (NLG)) and then syn</context>
<context position="10342" citStr="Baumann and Schlangen, 2012" startWordPosition="1553" endWordPosition="1556"> Levelt’s (1989) model of human speech production. Levelt distinguishes several, partially independent processing modules (conceptualization, formulation, articulation, see Figure 1) that function incrementally and “in a highly automatic, reflex-like way” (Levelt, 1989, p. 2). 3.3 Technical Overview of Our System As a basis, we use MaryTTS (Schröder and Trouvain, 2003), but we replace Mary’s internal data structures with structures that support incremental specifications; these we take from an extant incremental spoken dialogue system architecture and toolkit, INPROTK (Schlangen et al., 2010; Baumann and Schlangen, 2012). In this architecture, incremental processing as the processing of incremental units (IUs), which are the smallest ‘chunks’ of information at a specific level (such as words, or phonemes, as can be seen in Figure 1). IUs are interconnected to form a network (e. g. words keep links to their associated phonemes, and vice-versa) which stores the system’s complete information state. The iSS component takes an IU sequence of chunks of words as input (from an NLG component). Crucially, this sequence can then still be modified, through: (a) continuations, which simply link further words to the end o</context>
<context position="16225" citStr="Baumann and Schlangen, 2012" startWordPosition="2482" endWordPosition="2485">duction is upcoming, ongoing, or completed. Listeners may subscribe to be notified about such progress changes using an update interface on IUs. The applications in Figures 2 and 4 make use of this interface to mark the words of the utterance in bold for completed, and in italic for ongoing words (incidentally, the screenshot in Figure 4 was taken exactly at the boundary between “delete” and “the”). 4.3 Low-Latency Switching of Alternatives A major goal of iSS is to change what is being said while the utterance is ongoing. Forward-pointing same-level links (SLLs, (Schlangen and Skantze, 2009; Baumann and Schlangen, 2012)) as shown in Figure 3 allow to construct alternative utterance paths beforehand. Deciding on the actual utterance continuation is a simple re-ranking of the forward 106 Figure 3: Incremental units chained together via forwardpointing same-level links to form an utterance tree. Figure 4: Example application to showcase just-in-time selection between different paths in a complex utterance. SLLs which can be changed until immediately before the word (or phoneme) in question is being uttered. The demo application shown in Figure 4 allows the user to select the path through a fairly complex uttera</context>
</contexts>
<marker>Baumann, Schlangen, 2012</marker>
<rawString>Timo Baumann and David Schlangen. 2012. The INPROTK 2012 release. In Proceedings of SDCTD. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
</authors>
<title>Using Language.</title>
<date>1996</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2048" citStr="Clark, 1996" startWordPosition="308" endWordPosition="309">rance-initial delay before speech output starts, as output already starts when its beginning has been produced. In an otherwise conventional dialogue system, the synthesis module could make it possible to interrupt the output speech stream (e. g., when a noise event is detected that makes it likely that the user will not be able to hear what is being said), and continue production when the interruption is over. If other SDS components are adapted more to take advantage of incremental speech synthesis, even more flexible behaviours can be realised, such as providing utterances in installments (Clark, 1996) that prompt for backchannel signals, which in turn can prompt different utterance continuations, or starting an utterance before all information required in the utterance is available (“so, uhm, there are flights to Seoul on uh ... ”), signaling that the turn is being held. Another, less conventional type of speech-based system that could profit from iSS is “babelfish-like” simultaneous speech-to-speech translation. Research on architectures, higher-level processing modules and lower-level processing modules that would enable such behaviour is currently underway (Skantze and Schlangen, 2009; </context>
</contexts>
<marker>Clark, 1996</marker>
<rawString>Herbert H. Clark. 1996. Using Language. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thierry Dutoit</author>
<author>Maria Astrinaki</author>
<author>Onur Babacan</author>
<author>Nicolas d’Alessandro</author>
<author>Benjamin Picart</author>
</authors>
<title>pHTS for Max/MSP: A Streaming Architecture for Statistical Parametric Speech Synthesis.</title>
<date>2011</date>
<booktitle>Research Program on Digital Art Technologies,</booktitle>
<tech>Technical Report 1, numediart</tech>
<marker>Dutoit, Astrinaki, Babacan, d’Alessandro, Picart, 2011</marker>
<rawString>Thierry Dutoit, Maria Astrinaki, Onur Babacan, Nicolas d’Alessandro, and Benjamin Picart. 2011. pHTS for Max/MSP: A Streaming Architecture for Statistical Parametric Speech Synthesis. Technical Report 1, numediart Research Program on Digital Art Technologies, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Edlund</author>
</authors>
<title>Incremental speech synthesis.</title>
<date>2008</date>
<booktitle>In Second Swedish Language Technology Conference,</booktitle>
<pages>53--54</pages>
<location>Stockholm, Sweden,</location>
<contexts>
<context position="1137" citStr="Edlund, 2008" startWordPosition="164" endWordPosition="165">ystems. While iSS can show its full strength in systems that generate output incrementally, we also discuss how even otherwise unchanged systems may profit from its capabilities. 1 Introduction Current state of the art in speech synthesis for spoken dialogue systems (SDSs) is for the synthesis component to expect full utterances (in textual form) as input and to deliver an audio stream verbalising this full utterance. At best, timing information is returned as well so that a control component can determine in case of an interruption / barge-in by the user where in the utterance this happened (Edlund, 2008; Matsuyama et al., 2010). We want to argue here that providing capabilities to speech synthesis components for dealing with units smaller than full utterances can be beneficial for a whole range of interactive speech-based systems. In the easiest case, incremental synthesis simply reduces the utterance-initial delay before speech output starts, as output already starts when its beginning has been produced. In an otherwise conventional dialogue system, the synthesis module could make it possible to interrupt the output speech stream (e. g., when a noise event is detected that makes it likely t</context>
<context position="4287" citStr="Edlund (2008)" startWordPosition="639" endWordPosition="640">2012 Association for Computational Linguistics After a discussion of related work (Section 2), we describe the basic elements of our iSS component (Section 3) and some demonstrator applications that we created which showcase certain abilities.1 2 Related Work Typically, in current SDSs utterances are generated (either by lookup/template-based generation, or, less commonly, by concept-to-utterance natural language generation (NLG)) and then synthesised in full (McTear, 2002). There is very little work on incremental synthesis (i.e., one that would work with units smaller than full utterances). Edlund (2008) outlines some requirements for incremental speech synthesis: to give constant feedback to the dialogue system about what has been delivered, to be interruptible (and possibly continue from that position), and to run in real time. Edlund (2008) also presents a prototype that meets these requirements, but is limited to diphone synthesis that is performed non-incrementally before utterance delivery starts. We go beyond this in processing just-in-time, and also enabling changes during delivery. Skantze and Hjalmarsson (2010) describe a system that generates utterances incrementally (albeit in a W</context>
</contexts>
<marker>Edlund, 2008</marker>
<rawString>Jens Edlund. 2008. Incremental speech synthesis. In Second Swedish Language Technology Conference, pages 53–54, Stockholm, Sweden, November. System Demonstration.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Finkler</author>
</authors>
<title>Automatische Selbstkorrektur bei der inkrementellen Generierung gesprochener Sprache unter Realzeitbedingungen. Dissertationen zur Künstlichen Intelligenz. infix Verlag.</title>
<date>1997</date>
<contexts>
<context position="6174" citStr="Finkler, 1997" startWordPosition="927" endWordPosition="928">ng prosody), we aim to incrementalize the speech synthesis component itself. Dutoit et al. (2011) have presented an incremental formulation for HMM-based speech synthesis. However, their system works offline and is fed by nonincrementally produced phoneme target sequences. 1The code of the toolkit and its iSS component and the demo applications discussed below have been released as open-source athttp://inprotk.sourceforge.net. We aim for a fully incremental speech synthesis component that can be integrated into dialogue systems. There is some work on incremental NLG (Kilger and Finkler, 1995; Finkler, 1997; Guhe, 2007); however, that work does not concern itself with the actual synthesis of speech and hence describes only what would generate the input to our component. 3 Incremental Speech Synthesis 3.1 Background on Speech Synthesis Text-to-speech (TTS) synthesis normally proceeds in a top-down fashion, starting on the utterance level (for stress patterns and sentence-level intonation) and descending to words and phonemes (for pronunciation details), in order to make globally optimised decisions (Taylor, 2009). In that way, target phoneme sequences annotated with durations and pitch contours a</context>
</contexts>
<marker>Finkler, 1997</marker>
<rawString>Wolfgang Finkler. 1997. Automatische Selbstkorrektur bei der inkrementellen Generierung gesprochener Sprache unter Realzeitbedingungen. Dissertationen zur Künstlichen Intelligenz. infix Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Guhe</author>
</authors>
<title>Incremental Conceptualization for Language Production. Lawrence Erlbaum Asso.,</title>
<date>2007</date>
<publisher>Inc.,</publisher>
<location>Mahwah, USA.</location>
<contexts>
<context position="6187" citStr="Guhe, 2007" startWordPosition="929" endWordPosition="930"> aim to incrementalize the speech synthesis component itself. Dutoit et al. (2011) have presented an incremental formulation for HMM-based speech synthesis. However, their system works offline and is fed by nonincrementally produced phoneme target sequences. 1The code of the toolkit and its iSS component and the demo applications discussed below have been released as open-source athttp://inprotk.sourceforge.net. We aim for a fully incremental speech synthesis component that can be integrated into dialogue systems. There is some work on incremental NLG (Kilger and Finkler, 1995; Finkler, 1997; Guhe, 2007); however, that work does not concern itself with the actual synthesis of speech and hence describes only what would generate the input to our component. 3 Incremental Speech Synthesis 3.1 Background on Speech Synthesis Text-to-speech (TTS) synthesis normally proceeds in a top-down fashion, starting on the utterance level (for stress patterns and sentence-level intonation) and descending to words and phonemes (for pronunciation details), in order to make globally optimised decisions (Taylor, 2009). In that way, target phoneme sequences annotated with durations and pitch contours are generated,</context>
</contexts>
<marker>Guhe, 2007</marker>
<rawString>Markus Guhe. 2007. Incremental Conceptualization for Language Production. Lawrence Erlbaum Asso., Inc., Mahwah, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Kilger</author>
<author>Wolfgang Finkler</author>
</authors>
<title>Incremental Generation for Real-time Applications.</title>
<date>1995</date>
<tech>Technical Report RR-95-11,</tech>
<location>DFKI, Saarbrücken, Germany.</location>
<contexts>
<context position="6159" citStr="Kilger and Finkler, 1995" startWordPosition="923" endWordPosition="926">esis quality, esp. regarding prosody), we aim to incrementalize the speech synthesis component itself. Dutoit et al. (2011) have presented an incremental formulation for HMM-based speech synthesis. However, their system works offline and is fed by nonincrementally produced phoneme target sequences. 1The code of the toolkit and its iSS component and the demo applications discussed below have been released as open-source athttp://inprotk.sourceforge.net. We aim for a fully incremental speech synthesis component that can be integrated into dialogue systems. There is some work on incremental NLG (Kilger and Finkler, 1995; Finkler, 1997; Guhe, 2007); however, that work does not concern itself with the actual synthesis of speech and hence describes only what would generate the input to our component. 3 Incremental Speech Synthesis 3.1 Background on Speech Synthesis Text-to-speech (TTS) synthesis normally proceeds in a top-down fashion, starting on the utterance level (for stress patterns and sentence-level intonation) and descending to words and phonemes (for pronunciation details), in order to make globally optimised decisions (Taylor, 2009). In that way, target phoneme sequences annotated with durations and p</context>
</contexts>
<marker>Kilger, Finkler, 1995</marker>
<rawString>Anne Kilger and Wolfgang Finkler. 1995. Incremental Generation for Real-time Applications. Technical Report RR-95-11, DFKI, Saarbrücken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William J M Levelt</author>
</authors>
<title>Speaking: From Intention to Articulation.</title>
<date>1989</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="9983" citStr="Levelt, 1989" startWordPosition="1498" endWordPosition="1499">dules that work concurrently. This results in a ‘triangular’ structure (illustrated in Figure 1) where only the absolutely required minimum has to be specified at each level, allowing for later adaptations with few or no recomputations required. As an aside, we observe that our component’s architecture happens to correspond rather closely to Levelt’s (1989) model of human speech production. Levelt distinguishes several, partially independent processing modules (conceptualization, formulation, articulation, see Figure 1) that function incrementally and “in a highly automatic, reflex-like way” (Levelt, 1989, p. 2). 3.3 Technical Overview of Our System As a basis, we use MaryTTS (Schröder and Trouvain, 2003), but we replace Mary’s internal data structures with structures that support incremental specifications; these we take from an extant incremental spoken dialogue system architecture and toolkit, INPROTK (Schlangen et al., 2010; Baumann and Schlangen, 2012). In this architecture, incremental processing as the processing of incremental units (IUs), which are the smallest ‘chunks’ of information at a specific level (such as words, or phonemes, as can be seen in Figure 1). IUs are interconnected </context>
</contexts>
<marker>Levelt, 1989</marker>
<rawString>William J.M. Levelt. 1989. Speaking: From Intention to Articulation. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyoko Matsuyama</author>
<author>Kazunori Komatani</author>
<author>Ryu Takeda</author>
<author>Toru Takahashi</author>
<author>Tetsuya Ogata</author>
<author>Hiroshi G Okuno</author>
</authors>
<title>Analyzing User Utterances in Barge-in-able Spoken Dialogue System for Improving Identification Accuracy.</title>
<date>2010</date>
<booktitle>In Proceedings of Interspeech,</booktitle>
<pages>3050--3053</pages>
<location>Makuhari, Japan,</location>
<contexts>
<context position="1162" citStr="Matsuyama et al., 2010" startWordPosition="166" endWordPosition="170">iSS can show its full strength in systems that generate output incrementally, we also discuss how even otherwise unchanged systems may profit from its capabilities. 1 Introduction Current state of the art in speech synthesis for spoken dialogue systems (SDSs) is for the synthesis component to expect full utterances (in textual form) as input and to deliver an audio stream verbalising this full utterance. At best, timing information is returned as well so that a control component can determine in case of an interruption / barge-in by the user where in the utterance this happened (Edlund, 2008; Matsuyama et al., 2010). We want to argue here that providing capabilities to speech synthesis components for dealing with units smaller than full utterances can be beneficial for a whole range of interactive speech-based systems. In the easiest case, incremental synthesis simply reduces the utterance-initial delay before speech output starts, as output already starts when its beginning has been produced. In an otherwise conventional dialogue system, the synthesis module could make it possible to interrupt the output speech stream (e. g., when a noise event is detected that makes it likely that the user will not be </context>
</contexts>
<marker>Matsuyama, Komatani, Takeda, Takahashi, Ogata, Okuno, 2010</marker>
<rawString>Kyoko Matsuyama, Kazunori Komatani, Ryu Takeda, Toru Takahashi, Tetsuya Ogata, and Hiroshi G. Okuno. 2010. Analyzing User Utterances in Barge-in-able Spoken Dialogue System for Improving Identification Accuracy. In Proceedings of Interspeech, pages 3050– 3053, Makuhari, Japan, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael McTear</author>
</authors>
<title>Spoken Dialogue Technology. Toward the Conversational User-Interface.</title>
<date>2002</date>
<publisher>Springer,</publisher>
<location>London, UK.</location>
<contexts>
<context position="4152" citStr="McTear, 2002" startWordPosition="617" endWordPosition="618">of the 50th Annual Meeting of the Association for Computational Linguistics, pages 103–108, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics After a discussion of related work (Section 2), we describe the basic elements of our iSS component (Section 3) and some demonstrator applications that we created which showcase certain abilities.1 2 Related Work Typically, in current SDSs utterances are generated (either by lookup/template-based generation, or, less commonly, by concept-to-utterance natural language generation (NLG)) and then synthesised in full (McTear, 2002). There is very little work on incremental synthesis (i.e., one that would work with units smaller than full utterances). Edlund (2008) outlines some requirements for incremental speech synthesis: to give constant feedback to the dialogue system about what has been delivered, to be interruptible (and possibly continue from that position), and to run in real time. Edlund (2008) also presents a prototype that meets these requirements, but is limited to diphone synthesis that is performed non-incrementally before utterance delivery starts. We go beyond this in processing just-in-time, and also en</context>
</contexts>
<marker>McTear, 2002</marker>
<rawString>Michael McTear. 2002. Spoken Dialogue Technology. Toward the Conversational User-Interface. Springer, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Schlangen</author>
<author>Gabriel Skantze</author>
</authors>
<title>A General, Abstract Model of Incremental Dialogue Processing.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="16195" citStr="Schlangen and Skantze, 2009" startWordPosition="2478" endWordPosition="2481">Us marks whether the IU’s production is upcoming, ongoing, or completed. Listeners may subscribe to be notified about such progress changes using an update interface on IUs. The applications in Figures 2 and 4 make use of this interface to mark the words of the utterance in bold for completed, and in italic for ongoing words (incidentally, the screenshot in Figure 4 was taken exactly at the boundary between “delete” and “the”). 4.3 Low-Latency Switching of Alternatives A major goal of iSS is to change what is being said while the utterance is ongoing. Forward-pointing same-level links (SLLs, (Schlangen and Skantze, 2009; Baumann and Schlangen, 2012)) as shown in Figure 3 allow to construct alternative utterance paths beforehand. Deciding on the actual utterance continuation is a simple re-ranking of the forward 106 Figure 3: Incremental units chained together via forwardpointing same-level links to form an utterance tree. Figure 4: Example application to showcase just-in-time selection between different paths in a complex utterance. SLLs which can be changed until immediately before the word (or phoneme) in question is being uttered. The demo application shown in Figure 4 allows the user to select the path t</context>
</contexts>
<marker>Schlangen, Skantze, 2009</marker>
<rawString>David Schlangen and Gabriel Skantze. 2009. A General, Abstract Model of Incremental Dialogue Processing. In Proceedings of the EACL, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Schlangen</author>
<author>Timo Baumann</author>
<author>Hendrik Buschmeier</author>
<author>Okko Buß</author>
<author>Stefan Kopp</author>
<author>Gabriel Skantze</author>
<author>Ramin Yaghoubzadeh</author>
</authors>
<title>Middleware for Incremental Processing in Conversational Agents.</title>
<date>2010</date>
<booktitle>In Proceedings of SigDial</booktitle>
<pages>51--54</pages>
<location>Tokyo, Japan,</location>
<contexts>
<context position="10312" citStr="Schlangen et al., 2010" startWordPosition="1549" endWordPosition="1552">espond rather closely to Levelt’s (1989) model of human speech production. Levelt distinguishes several, partially independent processing modules (conceptualization, formulation, articulation, see Figure 1) that function incrementally and “in a highly automatic, reflex-like way” (Levelt, 1989, p. 2). 3.3 Technical Overview of Our System As a basis, we use MaryTTS (Schröder and Trouvain, 2003), but we replace Mary’s internal data structures with structures that support incremental specifications; these we take from an extant incremental spoken dialogue system architecture and toolkit, INPROTK (Schlangen et al., 2010; Baumann and Schlangen, 2012). In this architecture, incremental processing as the processing of incremental units (IUs), which are the smallest ‘chunks’ of information at a specific level (such as words, or phonemes, as can be seen in Figure 1). IUs are interconnected to form a network (e. g. words keep links to their associated phonemes, and vice-versa) which stores the system’s complete information state. The iSS component takes an IU sequence of chunks of words as input (from an NLG component). Crucially, this sequence can then still be modified, through: (a) continuations, which simply l</context>
</contexts>
<marker>Schlangen, Baumann, Buschmeier, Buß, Kopp, Skantze, Yaghoubzadeh, 2010</marker>
<rawString>David Schlangen, Timo Baumann, Hendrik Buschmeier, Okko Buß, Stefan Kopp, Gabriel Skantze, and Ramin Yaghoubzadeh. 2010. Middleware for Incremental Processing in Conversational Agents. In Proceedings of SigDial 2010, pages 51–54, Tokyo, Japan, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Schröder</author>
<author>Jürgen Trouvain</author>
</authors>
<title>The German Text-to-Speech Synthesis System MARY: A Tool for Research, Development and Teaching.</title>
<date>2003</date>
<journal>International Journal of Speech Technology,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="3422" citStr="Schröder and Trouvain, 2003" startWordPosition="513" endWordPosition="516">so far missing. In this paper, we present such a component, which is capable of (a) starting to speak before utterance processing has finished; (b) handling edits made to (as-yet unspoken) parts of the utterance even while a prefix is already being spoken; (c) enabling adaptations of delivery parameters such as speaking rate or pitch; (d) autonomously making appropriate deliveryrelated decisions; (e) providing information about progress in delivery; and, last but not least, (f) running in real time. Our iSS component is built on top of an existing non-incremental synthesis component, MaryTTS (Schröder and Trouvain, 2003), and on an existing architecture for incremental processing, INPROTK (Baumann and Schlangen, 2012). 103 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 103–108, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics After a discussion of related work (Section 2), we describe the basic elements of our iSS component (Section 3) and some demonstrator applications that we created which showcase certain abilities.1 2 Related Work Typically, in current SDSs utterances are generated (either by lookup/template-based gen</context>
<context position="10085" citStr="Schröder and Trouvain, 2003" startWordPosition="1514" endWordPosition="1518"> Figure 1) where only the absolutely required minimum has to be specified at each level, allowing for later adaptations with few or no recomputations required. As an aside, we observe that our component’s architecture happens to correspond rather closely to Levelt’s (1989) model of human speech production. Levelt distinguishes several, partially independent processing modules (conceptualization, formulation, articulation, see Figure 1) that function incrementally and “in a highly automatic, reflex-like way” (Levelt, 1989, p. 2). 3.3 Technical Overview of Our System As a basis, we use MaryTTS (Schröder and Trouvain, 2003), but we replace Mary’s internal data structures with structures that support incremental specifications; these we take from an extant incremental spoken dialogue system architecture and toolkit, INPROTK (Schlangen et al., 2010; Baumann and Schlangen, 2012). In this architecture, incremental processing as the processing of incremental units (IUs), which are the smallest ‘chunks’ of information at a specific level (such as words, or phonemes, as can be seen in Figure 1). IUs are interconnected to form a network (e. g. words keep links to their associated phonemes, and vice-versa) which stores t</context>
</contexts>
<marker>Schröder, Trouvain, 2003</marker>
<rawString>Marc Schröder and Jürgen Trouvain. 2003. The German Text-to-Speech Synthesis System MARY: A Tool for Research, Development and Teaching. International Journal of Speech Technology, 6(3):365–377, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Skantze</author>
<author>Anna Hjalmarsson</author>
</authors>
<title>Towards incremental speech generation in dialogue systems.</title>
<date>2010</date>
<booktitle>In Proceedings of SigDial</booktitle>
<pages>1--8</pages>
<location>Tokyo, Japan,</location>
<contexts>
<context position="2677" citStr="Skantze and Hjalmarsson, 2010" startWordPosition="395" endWordPosition="399"> that prompt for backchannel signals, which in turn can prompt different utterance continuations, or starting an utterance before all information required in the utterance is available (“so, uhm, there are flights to Seoul on uh ... ”), signaling that the turn is being held. Another, less conventional type of speech-based system that could profit from iSS is “babelfish-like” simultaneous speech-to-speech translation. Research on architectures, higher-level processing modules and lower-level processing modules that would enable such behaviour is currently underway (Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 2010; Baumann and Schlangen, 2011), but a synthesis component that would unlock the full potential of such strategies is so far missing. In this paper, we present such a component, which is capable of (a) starting to speak before utterance processing has finished; (b) handling edits made to (as-yet unspoken) parts of the utterance even while a prefix is already being spoken; (c) enabling adaptations of delivery parameters such as speaking rate or pitch; (d) autonomously making appropriate deliveryrelated decisions; (e) providing information about progress in delivery; and, last but not least, (f) </context>
<context position="4814" citStr="Skantze and Hjalmarsson (2010)" startWordPosition="715" endWordPosition="718">ncremental synthesis (i.e., one that would work with units smaller than full utterances). Edlund (2008) outlines some requirements for incremental speech synthesis: to give constant feedback to the dialogue system about what has been delivered, to be interruptible (and possibly continue from that position), and to run in real time. Edlund (2008) also presents a prototype that meets these requirements, but is limited to diphone synthesis that is performed non-incrementally before utterance delivery starts. We go beyond this in processing just-in-time, and also enabling changes during delivery. Skantze and Hjalmarsson (2010) describe a system that generates utterances incrementally (albeit in a WOz-enviroment), allowing earlier components to incrementally produce and revise their hypothesis about the user’s utterance. The system can automatically play hesitations if by the time it has the turn it does not know what to produce yet. They show that users prefer such a system over a non-incremental one, even though it produced longer dialogues. Our approach is complementary to this work, as it targets a lower layer, the realisation or synthesis layer. Where their system relies on ‘regular’ speech synthesis which is c</context>
</contexts>
<marker>Skantze, Hjalmarsson, 2010</marker>
<rawString>Gabriel Skantze and Anna Hjalmarsson. 2010. Towards incremental speech generation in dialogue systems. In Proceedings of SigDial 2010, pages 1–8, Tokyo, Japan, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Skantze</author>
<author>David Schlangen</author>
</authors>
<title>Incremental dialogue processing in a micro-domain.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL 2009,</booktitle>
<location>Athens, Greece,</location>
<contexts>
<context position="2646" citStr="Skantze and Schlangen, 2009" startWordPosition="391" endWordPosition="394">in installments (Clark, 1996) that prompt for backchannel signals, which in turn can prompt different utterance continuations, or starting an utterance before all information required in the utterance is available (“so, uhm, there are flights to Seoul on uh ... ”), signaling that the turn is being held. Another, less conventional type of speech-based system that could profit from iSS is “babelfish-like” simultaneous speech-to-speech translation. Research on architectures, higher-level processing modules and lower-level processing modules that would enable such behaviour is currently underway (Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 2010; Baumann and Schlangen, 2011), but a synthesis component that would unlock the full potential of such strategies is so far missing. In this paper, we present such a component, which is capable of (a) starting to speak before utterance processing has finished; (b) handling edits made to (as-yet unspoken) parts of the utterance even while a prefix is already being spoken; (c) enabling adaptations of delivery parameters such as speaking rate or pitch; (d) autonomously making appropriate deliveryrelated decisions; (e) providing information about progress in delivery</context>
</contexts>
<marker>Skantze, Schlangen, 2009</marker>
<rawString>Gabriel Skantze and David Schlangen. 2009. Incremental dialogue processing in a micro-domain. In Proceedings of EACL 2009, Athens, Greece, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Taylor</author>
</authors>
<title>Text-to-Speech Synthesis.</title>
<date>2009</date>
<publisher>Cambridge Univ Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="6689" citStr="Taylor, 2009" startWordPosition="1003" endWordPosition="1004">o dialogue systems. There is some work on incremental NLG (Kilger and Finkler, 1995; Finkler, 1997; Guhe, 2007); however, that work does not concern itself with the actual synthesis of speech and hence describes only what would generate the input to our component. 3 Incremental Speech Synthesis 3.1 Background on Speech Synthesis Text-to-speech (TTS) synthesis normally proceeds in a top-down fashion, starting on the utterance level (for stress patterns and sentence-level intonation) and descending to words and phonemes (for pronunciation details), in order to make globally optimised decisions (Taylor, 2009). In that way, target phoneme sequences annotated with durations and pitch contours are generated, in what is called the linguistic pre-processing step. The then following synthesis step proper can be executed in one of several ways, with HMM-based and unit-selection synthesis currently being seen as producing the perceptually best results (Taylor, 2009). The former works by first turning the target sequence into a sequence of HMM states; a global optimization then computes a stream of vocoding features that optimize both HMM emission probabilities and continuity constraints (Tokuda et al., 20</context>
</contexts>
<marker>Taylor, 2009</marker>
<rawString>Paul Taylor. 2009. Text-to-Speech Synthesis. Cambridge Univ Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomoki Toda</author>
<author>Keiichi Tokuda</author>
</authors>
<title>A Speech Parameter Generation Algorithm Considering Global Variance for HMM-based Speech Synthesis.</title>
<date>2007</date>
<journal>IEICE Transactions on Information and Systems,</journal>
<volume>90</volume>
<issue>5</issue>
<contexts>
<context position="11673" citStr="Toda and Tokuda, 2007" startWordPosition="1763" endWordPosition="1766">iced in. Additionally, a chunk can be marked as open; this has the effect of linking to a special hesitation word, which is produced only if it is not replaced (by the NLG) in time with other material. Technically, the representation levels below the chunk level are generated in our component by MaryTTS’s linguistic preprocessing and converting the output to IU structures. Our component provides for two modes of operation: Either using MaryTTS’ HMM optimization routines which non-incrementally solve a large matrix operation and subsequently iteratively optimize the global variance constraint (Toda and Tokuda, 2007). Or, using the incremental algorithm as proposed by Dutoit et al. (2011). In our implementation of this algorithm, HMM emissions are computed with one phoneme of context in both directions; Dutoit et al. (2011) have found this setting to only slightly degrade synthesis quality. While the former mode incurs some utterance-initial delay, switching between alternatives and prosodic alteration can be performed at virtually no lookahead, while requiring just little lookahead for the truly incremental mode. The resulting vocoding frames then are attached to their corresponding phoneme units. Phonem</context>
</contexts>
<marker>Toda, Tokuda, 2007</marker>
<rawString>Tomoki Toda and Keiichi Tokuda. 2007. A Speech Parameter Generation Algorithm Considering Global Variance for HMM-based Speech Synthesis. IEICE Transactions on Information and Systems, 90(5):816–824.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiichi Tokuda</author>
<author>Takayoshi Yoshimura</author>
<author>Takashi Masuko</author>
<author>Takao Kobayashi</author>
<author>Tadashi Kitamura</author>
</authors>
<title>Speech Parameter Generation Algorithms for HMMbased Speech Synthesis.</title>
<date>2000</date>
<booktitle>In Proceedings of ICASSP</booktitle>
<pages>1315--1318</pages>
<location>Istanbul, Turkey.</location>
<contexts>
<context position="7292" citStr="Tokuda et al., 2000" startWordPosition="1093" endWordPosition="1096">ns (Taylor, 2009). In that way, target phoneme sequences annotated with durations and pitch contours are generated, in what is called the linguistic pre-processing step. The then following synthesis step proper can be executed in one of several ways, with HMM-based and unit-selection synthesis currently being seen as producing the perceptually best results (Taylor, 2009). The former works by first turning the target sequence into a sequence of HMM states; a global optimization then computes a stream of vocoding features that optimize both HMM emission probabilities and continuity constraints (Tokuda et al., 2000). Finally, the parameter frames are fed to a vocoder which generates the speech audio signal. Unit-selection, in contrast, searches for the best sequence of (variably sized) units of speech in a large, annotated corpus of recordings, aiming to find a sequence that closely matches the target sequence. As mentioned above, Dutoit et al. (2011) have presented an online formulation of the optimization step in HMM-based synthesis. Beyond this, two other factors influenced our decision to follow the HMM-based approach: (a) HMM-based synthesis nicely separates the production of vocoding parameter fram</context>
</contexts>
<marker>Tokuda, Yoshimura, Masuko, Kobayashi, Kitamura, 2000</marker>
<rawString>Keiichi Tokuda, Takayoshi Yoshimura, Takashi Masuko, Takao Kobayashi, and Tadashi Kitamura. 2000. Speech Parameter Generation Algorithms for HMMbased Speech Synthesis. In Proceedings of ICASSP 2000, pages 1315–1318, Istanbul, Turkey.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>