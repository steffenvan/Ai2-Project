<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003504">
<title confidence="0.690753">
Simple Type-Level Unsupervised POS Tagging
</title>
<author confidence="0.980037">
Yoong Keok Lee Aria Haghighi Regina Barzilay
</author>
<affiliation confidence="0.997487">
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
</affiliation>
<email confidence="0.995763">
{yklee, aria42, regina}@csail.mit.edu
</email>
<sectionHeader confidence="0.998576" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9991131">
Part-of-speech (POS) tag distributions are
known to exhibit sparsity — a word is likely
to take a single predominant tag in a corpus.
Recent research has demonstrated that incor-
porating this sparsity constraint improves tag-
ging accuracy. However, in existing systems,
this expansion come with a steep increase in
model complexity. This paper proposes a sim-
ple and effective tagging method that directly
models tag sparsity and other distributional
properties of valid POS tag assignments. In
addition, this formulation results in a dramatic
reduction in the number of model parame-
ters thereby, enabling unusually rapid training.
Our experiments consistently demonstrate that
this model architecture yields substantial per-
formance gains over more complex tagging
counterparts. On several languages, we report
performance exceeding that of more complex
state-of-the art systems.1
</bodyText>
<sectionHeader confidence="0.999473" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945">
Since the early days of statistical NLP, researchers
have observed that a part-of-speech tag distribution
exhibits “one tag per discourse” sparsity — words
are likely to select a single predominant tag in a cor-
pus, even when several tags are possible. Simply
assigning to each word its most frequent associated
tag in a corpus achieves 94.6% accuracy on the WSJ
portion of the Penn Treebank. This distributional
sparsity of syntactic tags is not unique to English
</bodyText>
<footnote confidence="0.999161">
1The source code for the work presented in this paper is
available at http://groups.csail.mit.edu/rbg/code/typetagging/.
</footnote>
<bodyText confidence="0.999898575757576">
— similar results have been observed across multi-
ple languages. Clearly, explicitly modeling such a
powerful constraint on tagging assignment has a po-
tential to significantly improve the accuracy of an
unsupervised part-of-speech tagger learned without
a tagging dictionary.
In practice, this sparsity constraint is difficult
to incorporate in a traditional POS induction sys-
tem (M´erialdo, 1994; Johnson, 2007; Gao and John-
son, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick
et al., 2010). These sequence models-based ap-
proaches commonly treat token-level tag assignment
as the primary latent variable. By design, they read-
ily capture regularities at the token-level. However,
these approaches are ill-equipped to directly repre-
sent type-based constraints such as sparsity. Pre-
vious work has attempted to incorporate such con-
straints into token-level models via heavy-handed
modifications to inference procedure and objective
function (e.g., posterior regularization and ILP de-
coding) (Grac¸a et al., 2009; Ravi and Knight, 2009).
In most cases, however, these expansions come with
a steep increase in model complexity, with respect
to training procedure and inference time.
In this work, we take a more direct approach and
treat a word type and its allowed POS tags as a pri-
mary element of the model. The model starts by gen-
erating a tag assignment for each word type in a vo-
cabulary, assuming one tag per word. Then, token-
level HMM emission parameters are drawn condi-
tioned on these assignments such that each word is
only allowed probability mass on a single assigned
tag. In this way we restrict the parameterization of a
</bodyText>
<page confidence="0.988512">
853
</page>
<note confidence="0.9563801">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 853–861,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
Language Original case
English 94.6
Danish 96.3
Dutch 96.6
German 95.5
Spanish 95.4
Swedish 93.3
Portuguese 95.6
</note>
<tableCaption confidence="0.99032275">
Table 1: Upper bound on tagging accuracy assuming each
word type is assigned to majority POS tag. Across all
languages, high performance can be attained by selecting
a single tag per word type.
</tableCaption>
<bodyText confidence="0.999966851851852">
token-level HMM to reflect lexicon sparsity. This
model admits a simple Gibbs sampling algorithm
where the number of latent variables is proportional
to the number of word types, rather than the size of
a corpus as for a standard HMM sampler (Johnson,
2007).
There are two key benefits of this model architec-
ture. First, it directly encodes linguistic intuitions
about POS tag assignments: the model structure
reflects the one-tag-per-word property, and a type-
level tag prior captures the skew on tag assignments
(e.g., there are fewer unique determiners than unique
nouns). Second, the reduced number of hidden vari-
ables and parameters dramatically speeds up learn-
ing and inference.
We evaluate our model on seven languages ex-
hibiting substantial syntactic variation. On several
languages, we report performance exceeding that of
state-of-the art systems. Our analysis identifies three
key factors driving our performance gain: 1) select-
ing a model structure which directly encodes tag
sparsity, 2) a type-level prior on tag assignments,
and 3) a straightforward naive-Bayes approach to
incorporate features. The observed performance
gains, coupled with the simplicity of model imple-
mentation, makes it a compelling alternative to ex-
isting more complex counterparts.
</bodyText>
<sectionHeader confidence="0.999914" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999931076923077">
Recent work has made significant progress on unsu-
pervised POS tagging (M´erialdo, 1994; Smith and
Eisner, 2005; Haghighi and Klein, 2006; Johnson,
2007; Goldwater and Griffiths, 2007; Gao and John-
son, 2008; Ravi and Knight, 2009). Our work is
closely related to recent approaches that incorporate
the sparsity constraint into the POS induction pro-
cess. This line of work has been motivated by em-
pirical findings that the standard EM-learned unsu-
pervised HMM does not exhibit sufficient word tag
sparsity.
The extent to which this constraint is enforced
varies greatly across existing methods. On one end
of the spectrum are clustering approaches that assign
a single POS tag to each word type (Schutze, 1995;
Lamar et al., 2010). These clusters are computed us-
ing an SVD variant without relying on transitional
structure. While our method also enforces a singe
tag per word constraint, it leverages the transition
distribution encoded in an HMM, thereby benefiting
from a richer representation of context.
Other approaches encode sparsity as a soft con-
straint. For instance, by altering the emission distri-
bution parameters, Johnson (2007) encourages the
model to put most of the probability mass on few
tags. This design does not guarantee “structural ze-
ros,” but biases towards sparsity. A more force-
ful approach for encoding sparsity is posterior reg-
ularization, which constrains the posterior to have
a small number of expected tag assignments (Grac¸a
et al., 2009). This approach makes the training ob-
jective more complex by adding linear constraints
proportional to the number of word types, which
is rather prohibitive. A more rigid mechanism for
modeling sparsity is proposed by Ravi and Knight
(2009), who minimize the size of tagging grammar
as measured by the number of transition types. The
use of ILP in learning the desired grammar signif-
icantly increases the computational complexity of
this method.
In contrast to these approaches, our method di-
rectly incorporates these constraints into the struc-
ture of the model. This design leads to a significant
reduction in the computational complexity of train-
ing and inference.
Another thread of relevant research has explored
the use of features in unsupervised POS induc-
tion (Smith and Eisner, 2005; Berg-Kirkpatrick et
al., 2010; Hasan and Ng, 2009). These methods
demonstrated the benefits of incorporating linguis-
tic features using a log-linear parameterization, but
requires elaborate machinery for training. In our
</bodyText>
<page confidence="0.998158">
854
</page>
<bodyText confidence="0.999973">
work, we demonstrate that using a simple naive-
Bayes approach also yields substantial performance
gains, without the associated training complexity.
</bodyText>
<sectionHeader confidence="0.996684" genericHeader="method">
3 Generative Story
</sectionHeader>
<bodyText confidence="0.994318958333334">
We consider the unsupervised POS induction prob-
lem without the use of a tagging dictionary. A graph-
ical depiction of our model as well as a summary
of random variables and parameters can be found in
Figure 1. As is standard, we use a fixed constant K
for the number of tagging states.
Model Overview The model starts by generating
a tag assignment T for each word type in a vocab-
ulary, assuming one tag per word. Conditioned on
T, features of word types W are drawn. We refer
to (T, W) as the lexicon of a language and 0 for
the parameters for their generation; 0 depends on a
single hyperparameter Q.
Once the lexicon has been drawn, the model pro-
ceeds similarly to the standard token-level HMM:
Emission parameters B are generated conditioned on
tag assignments T. We also draw transition param-
eters 0. Both parameters depend on a single hy-
perparameter α. Once HMM parameters (B, 0) are
drawn, a token-level tag and word sequence, (t, w),
is generated in the standard HMM fashion: a tag se-
quence t is generated from 0. The corresponding
token words w are drawn conditioned on t and B.2
Our full generative model is given by:
</bodyText>
<equation confidence="0.997129">
P(T,W,B,0,0,t,wJα,Q) =
P(T, W, 0JQ) [Lexicon]
P(0, BJT, α, Q) [Parameter]
P(w, tJ0, B) [Token]
</equation>
<bodyText confidence="0.993673545454546">
We refer to the components on the right hand side
as the lexicon, parameter, and token component re-
spectively. Since the parameter and token compo-
nents will remain fixed throughout experiments, we
briefly describe each.
Parameter Component As in the standard
Bayesian HMM (Goldwater and Griffiths, 2007),
all distributions are independently drawn from
symmetric Dirichlet distributions:
2Note that t and w denote tag and word sequences respec-
tively, rather than individual tokens or tags.
</bodyText>
<equation confidence="0.998520333333333">
K
P(0, BJT, α, Q) = Y (P(0tJα)P(BtJT, α))
t=1
</equation>
<bodyText confidence="0.999729318181818">
The transition distribution 0t for each tag t is drawn
according to DIRICHLET(α, K), where α is the
shared transition and emission distribution hyperpa-
rameter. In total there are O(K2) parameters asso-
ciated with the transition parameters.
In contrast to the Bayesian HMM, Bt is not
drawn from a distribution which has support for
each of the n word types. Instead, we condition
on the type-level tag assignments T. Specifically,
let St = {iJTz = t} denote the indices of the
word types which have been assigned tag t accord-
ing to the tag assignments T. Then Bt is drawn from
DIRICHLET(α, St), a symmetric Dirichlet which
only places mass on word types indicated by St.
This ensures that each word will only be assigned
a single tag at inference time (see Section 4).
Note that while the standard HMM, has O(Kn)
emission parameters, our model has O(n) effective
parameters.3
Token Component Once HMM parameters (0, B)
have been drawn, the HMM generates a token-level
corpus w in the standard way:
</bodyText>
<equation confidence="0.99583275">
P(w, tJ0, B) =
⎛ ⎞
⎝YP(tiJ0t;_1)P(wiJti,Bt;) ⎠
9
</equation>
<bodyText confidence="0.99778075">
Note that in our model, conditioned on T, there is
precisely one t which has non-zero probability for
the token component, since for each word, exactly
one Bt has support.
</bodyText>
<subsectionHeader confidence="0.99954">
3.1 Lexicon Component
</subsectionHeader>
<bodyText confidence="0.999983666666667">
We present several variations for the lexical com-
ponent P(T, WJ0), each adding more complex pa-
rameterizations.
Uniform Tag Prior (1TW) Our initial lexicon
component will be uniform over possible tag assign-
ments as well as word types. Its only purpose is
</bodyText>
<footnote confidence="0.771343">
3This follows since each Bt has St − 1 parameters and
</footnote>
<equation confidence="0.629801">
Et St = n.
Y
(w,t)E(w,t)
</equation>
<page confidence="0.821501">
855
</page>
<figure confidence="0.999689658536585">
VARIABLES
W: Word types (obs)
(W�, . . . , W�)
T: Tag assigns (Ti, . . . , Tn)
w: Token word seqs (obs)
t : Token tag assigns (det by )
T
PARAMETERS
ψ: Lexicon parameters
θ: Token word emission parameters
φ: Token tag transition parameters
β
α
θ
K
T W
ψ
0
0
wl
ti
0
0
W2
t2
0
0
w,MN
tom,,
T
O
K
E
N
α
φ
K
T
Y
P
E
</figure>
<figureCaption confidence="0.9944374">
Figure 1: Graphical depiction of our model and summary of latent variables and parameters. The type-level tag
assignments T generate features associated with word types W. The tag assignments constrain the HMM emission
parameters θ. The tokens w are generated by token-level tags t from an HMM parameterized by the lexicon structure.
The hyperparameters α and β represent the concentration parameters of the token- and type-level components of the
model respectively. They are set to fixed constants.
</figureCaption>
<bodyText confidence="0.997713">
to explore how well we can induce POS tags using
only the one-tag-per-word constraint. Specifically,
the lexicon is generated as:
</bodyText>
<equation confidence="0.99631975">
P(T, W|ψ) =P(T)P(W|T)
1 n
Kn
P(Ti)P(Wi|Ti) = ( I )
</equation>
<bodyText confidence="0.993208">
This model is equivalent to the standard HMM ex-
cept that it enforces the one-word-per-tag constraint.
Learned Tag Prior (PRIOR) We next assume
there exists a single prior distribution ψ over tag as-
signments drawn from DIRICHLET(β, K). This al-
ters generation of T as follows:
</bodyText>
<equation confidence="0.994294333333333">
n
P(T|ψ) = P(Ti|ψ)
i=1
</equation>
<bodyText confidence="0.999875428571429">
Note that this distribution captures the frequency of
a tag across word types, as opposed to tokens. The
P(T |ψ) distribution, in English for instance, should
have very low mass for the DT (determiner) tag,
since determiners are a very small portion of the vo-
cabulary. In contrast, NNP (proper nouns) form a
large portion of vocabulary. Note that these observa-
tions are not modeled by the standard HMM, which
instead can model token-level frequency.
Word Type Features (FEATS): Past unsuper-
vised POS work have derived benefits from features
on word types, such as suffix and capitalization fea-
tures (Hasan and Ng, 2009; Berg-Kirkpatrick et al.,
2010). Past work however, has typically associ-
ated these features with token occurrences, typically
in an HMM. In our model, we associate these fea-
tures at the type-level in the lexicon. Here, we con-
sider suffix features, capitalization features, punctu-
ation, and digit features. While possible to utilize
the feature-based log-linear approach described in
Berg-Kirkpatrick et al. (2010), we adopt a simpler
naive Bayes strategy, where all features are emitted
independently. Specifically, we assume each word
type W consists of feature-value pairs (f, v). For
each feature type f and tag t, a multinomial ψtf is
drawn from a symmetric Dirichlet distribution with
concentration parameter β. The P(W |T, ψ) term
in the lexicon component now decomposes as:
</bodyText>
<equation confidence="0.989788888888889">
n
P(W |T, ψ) = P(Wi|Ti, ψ)
i=1
(H P(v|ψTif) �
(f,v)EWi
n
i=1
n
i=1
</equation>
<page confidence="0.994519">
856
</page>
<sectionHeader confidence="0.968821" genericHeader="method">
4 Learning and Inference
</sectionHeader>
<bodyText confidence="0.948797157894737">
For inference, we are interested in the posterior
probability over the latent variables in our model.
During training, we treat as observed the language
word types W as well as the token-level corpus w.
We utilize Gibbs sampling to approximate our col-
lapsed model posterior:
P(T,t|W, w, α, β) a P(T, t, W, w|α, β)
J= P(T, t, W, w, ψ, θ, φ, w|α, β)dψdθdφ
Note that given tag assignments T, there is only one
setting of token-level tags t which has mass in the
above posterior. Specifically, for the ith word type,
the set of token-level tags associated with token oc-
currences of this word, denoted t(Z), must all take
the value TZ to have non-zero mass. Thus in the con-
text of Gibbs sampling, if we want to block sample
TZ with t(Z), we only need sample values for TZ and
consider this setting of t(Z).
The equation for sampling a single type-level as-
signment TZ is given by,
</bodyText>
<equation confidence="0.635156">
P(TZ, t(Z)|T−Z, W, t(−Z), w, α, β) =
P(TZ|W, T −Z, β)P(t(Z)|TZ, t(−Z), w, α)
</equation>
<bodyText confidence="0.999941166666667">
where T−Z denotes all type-level tag assignment ex-
cept TZ and t(−Z) denotes all token-level tags except
t(Z). The terms on the right-hand-side denote the
type-level and token-level probability terms respec-
tively. The type-level posterior term can be com-
puted according to,
</bodyText>
<equation confidence="0.990781333333333">
P(TZ|W,T−Z, β) a
P(TZ|T−Z, β) � P(v|TZ, f, W −Z, T−Z, β)
(f,v)∈Wi
</equation>
<bodyText confidence="0.999672333333333">
All of the probabilities on the right-hand-side are
Dirichlet, distributions which can be computed an-
alytically given counts.
The token-level term is similar to the standard
HMM sampling equations found in Johnson (2007).
The relevant variables are the set of token-level tags
that appear before and after each instance of the ith
word type; we denote these context pairs with the set
{(tb, ta)I and they are contained in t(−Z). We use w
</bodyText>
<figure confidence="0.998533444444444">
English
Danish
Dutch
Germany
Portuguese
Spanish
Swedish
5 10 15 20 25 30
Iteration
</figure>
<figureCaption confidence="0.9962812">
Figure 2: Graph of the one-to-one accuracy of our full
model (+FEATS) under the best hyperparameter setting
by iteration (see Section 5). Performance typically stabi-
lizes across languages after only a few number of itera-
tions.
</figureCaption>
<bodyText confidence="0.805356">
to represent the ith word type emitted by the HMM:
P(t(Z)|TZ, t(−Z), w, α) a
</bodyText>
<equation confidence="0.985810333333333">
ri P(w|TZ, t(−Z), w(−Z), α)
(tb,ta)
P(TZ|tb, t(−Z), α)P(ta|TZ, t(−Z), α)
</equation>
<bodyText confidence="0.999889923076923">
All terms are Dirichlet distributions and parameters
can be analytically computed from counts in t(−Z)
and w(−Z) (Johnson, 2007).
Note that each round of sampling TZ variables
takes time proportional to the size of the corpus, as
with the standard token-level HMM. A crucial dif-
ference is that the number of parameters is greatly
reduced as is the number of variables that are sam-
pled during each iteration. In contrast to results re-
ported in Johnson (2007), we found that the per-
formance of our Gibbs sampler on the basic 1TW
model stabilized very quickly after about 10 full it-
erations of sampling (see Figure 2 for a depiction).
</bodyText>
<sectionHeader confidence="0.999761" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999627833333333">
We evaluate our approach on seven languages: En-
glish, Danish, Dutch, German, Portuguese, Spanish,
and Swedish. On each language we investigate the
contribution of each component of our model. For
all languages we do not make use of a tagging dic-
tionary.
</bodyText>
<figure confidence="0.998682166666667">
1−1 Accuracy 0.7
0.6
0.5
0.4
0.3
0.20
</figure>
<page confidence="0.987109">
857
</page>
<table confidence="0.99851875">
Model Hyper- English Danish Dutch German Portuguese Spanish Swedish
param. 1-1 m-1 1-1 m-1 1-1 m-1 1-1 m-1 1-1 m-1 1-1 m-1 1-1 m-1
1TW best 45.2 62.6 37.2 56.2 47.4 53.7 44.2 62.2 49.0 68.4 34.3 54.4 36.0 55.3
median 45.1 61.7 32.1 53.8 43.9 61.0 39.3 68.4 48.5 68.1 33.6 54.3 34.9 50.2
+PRIOR best 47.9 65.5 42.3 58.3 51.4 65.9 50.7 62.2 56.2 70.7 42.8 54.8 38.9 58.0
median 46.5 64.7 40.0 57.3 48.3 60.7 41.7 68.3 52.0 70.9 37.1 55.8 36.8 57.3
+FEATS best 50.9 66.4 52.1 61.2 56.4 69.0 55.4 70.4 64.1 74.5 58.3 68.9 43.3 61.7
median 47.8 66.4 43.2 60.7 51.5 67.3 46.2 61.7 56.5 70.1 50.0 57.2 38.5 60.6
</table>
<tableCaption confidence="0.96805">
Table 3: Multi-lingual Results: We report token-level one-to-one and many-to-one accuracy on a variety of languages
under several experimental settings (Section 5). For each language and setting, we report one-to-one (1-1) and many-
to-one (m-1) accuracies. For each cell, the first row corresponds to the result using the best hyperparameter choice,
where best is defined by the 1-1 metric. The second row represents the performance of the median hyperparameter
setting. Model components cascade, so the row corresponding to +FEATS also includes the PRIOR component (see
Section 3).
</tableCaption>
<table confidence="0.999646125">
Language # Tokens # Word Types # Tags
English 1173766 49206 45
Danish 94386 18356 25
Dutch 203568 28393 12
German 699605 72325 54
Portuguese 206678 28931 22
Spanish 89334 16458 47
Swedish 191467 20057 41
</table>
<tableCaption confidence="0.9723708">
Table 2: Statistics for various corpora utilized in exper-
iments. See Section 5. The English data comes from
the WSJ portion of the Penn Treebank and the other lan-
guages from the training set of the CoNLL-X multilin-
gual dependency parsing shared task.
</tableCaption>
<subsectionHeader confidence="0.996433">
5.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.999777">
Following the set-up of Johnson (2007), we use
the whole of the Penn Treebank corpus for train-
ing and evaluation on English. For other languages,
we use the CoNLL-X multilingual dependency pars-
ing shared task corpora (Buchholz and Marsi, 2006)
which include gold POS tags (used for evaluation).
We train and test on the CoNLL-X training set.
Statistics for all data sets are shown in Table 2.
</bodyText>
<subsectionHeader confidence="0.998567">
5.2 Setup
</subsectionHeader>
<bodyText confidence="0.983666571428572">
Models To assess the marginal utility of each com-
ponent of the model (see Section 3), we incremen-
tally increase its sophistication. Specifically, we
evaluate three variants: The first model (1TW) only
encodes the one tag per word constraint and is uni-
form over type-level tag assignments. The second
model (+PRIOR) utilizes the independent prior over
type-level tag assignments P(T IO). The final model
(+FEATS) utilizes the tag prior as well as features
(e.g., suffixes and orthographic features), discussed
in Section 3, for the P(W IT, 0) component.
Hyperparameters Our model has two Dirichlet
concentration hyperparameters: α is the shared hy-
perparameter for the token-level HMM emission and
transition distributions. Q is the shared hyperparam-
eter for the tag assignment prior and word feature
multinomials. We experiment with four values for
each hyperparameter resulting in 16 (α, Q) combi-
nations:
α Q
0.001, 0.01, 0.1,1.0 0.01, 0.1,1.0,10
Iterations In each run, we performed 30 iterations
of Gibbs sampling for the type assignment variables
W.4 We use the final sample for evaluation.
Evaluation Metrics We report three metrics to
evaluate tagging performance. As is standard, we
report the greedy one-to-one (Haghighi and Klein,
2006) and the many-to-one token-level accuracy ob-
tained from mapping model states to gold POS tags.
We also report word type level accuracy, the fraction
of word types assigned their majority tag (where the
mapping between model state and tag is determined
by greedy one-to-one mapping discussed above).5
For each language, we aggregate results in the fol-
lowing way: First, for each hyperparameter setting,
</bodyText>
<footnote confidence="0.9599684">
4Typically, the performance stabilizes after only 10 itera-
tions.
5We choose these two metrics over the Variation Informa-
tion measure due to the deficiencies discussed in Gao and John-
son (2008).
</footnote>
<page confidence="0.995808">
858
</page>
<bodyText confidence="0.999964444444445">
we perform five runs with different random initial-
ization of sampling state. Hyperparameter settings
are sorted according to the median one-to-one met-
ric over runs. We report results for the best and me-
dian hyperparameter settings obtained in this way.
Specifically, for both settings we report results on
the median run for each setting.
Tag set As is standard, for all experiments, we set
the number of latent model tag states to the size of
the annotated tag set. The original tag set for the
CoNLL-X Dutch data set consists of compounded
tags that are used to tag multi-word units (MWUs)
resulting in a tag set of over 300 tags. We tokenize
MWUs and their POS tags; this reduces the tag set
size to 12. See Table 2 for the tag set size of other
languages. With the exception of the Dutch data set,
no other processing is performed on the annotated
tags.
</bodyText>
<sectionHeader confidence="0.999026" genericHeader="evaluation">
6 Results and Analysis
</sectionHeader>
<bodyText confidence="0.999061923076923">
We report token- and type-level accuracy in Table 3
and 6 for all languages and system settings. Our
analysis and comparison focuses primarily on the
one-to-one accuracy since it is a stricter metric than
many-to-one accuracy, but also report many-to-one
for completeness.
Comparison with state-of-the-art taggers For
comparison we consider two unsupervised tag-
gers: the HMM with log-linear features of Berg-
Kirkpatrick et al. (2010) and the posterior regular-
ization HMM of Grac¸a et al. (2009). The system
of Berg-Kirkpatrick et al. (2010) reports the best
unsupervised results for English. We consider two
variants of Berg-Kirkpatrick et al. (2010)’s richest
model: optimized via either EM or LBFGS, as their
relative performance depends on the language. Our
model outperforms theirs on four out of five lan-
guages on the best hyperparameter setting and three
out of five on the median setting, yielding an aver-
age absolute difference across languages of 12.9%
and 3.9% for best and median settings respectively
compared to their best EM or LBFGS performance.
While Berg-Kirkpatrick et al. (2010) consistently
outperforms ours on English, we obtain substantial
gains across other languages. For instance, on Span-
ish, the absolute gap on median performance is 10%.
</bodyText>
<table confidence="0.841311">
Top 5 Bottom 5
Gold NNP NN JJ CD NNS RBS PDT # ” ,
1TW CD WRB NNS VBN NN PRP$ WDT : MD .
+PRIOR CD JJ NNS WP$ NN -RRB- , $ ” .
+FEATS JJ NNS CD NNP UH , PRP$ # . “
</table>
<tableCaption confidence="0.704733">
Table 5: Type-level English POS Tag Ranking: We list
the top 5 and bottom 5 POS tags in the lexicon and the
predictions of our models under the best hyperparameter
setting.
</tableCaption>
<bodyText confidence="0.999979810810811">
Our second point of comparison is with Grac¸a
et al. (2009), who also incorporate a sparsity con-
straint, but does via altering the model objective us-
ing posterior regularization. We can only compare
with Grac¸a et al. (2009) on Portuguese (Grac¸a et al.
(2009) also report results on English, but on the re-
duced 17 tag set, which is not comparable to ours).
Their best model yields 44.5% one-to-one accuracy,
compared to our best median 56.5% result. How-
ever, our full model takes advantage of word features
not present in Grac¸a et al. (2009). Even without fea-
tures, but still using the tag prior, our median result
is 52.0%, still significantly outperforming Grac¸a et
al. (2009).
Ablation Analysis We evaluate the impact of
incorporating various linguistic features into our
model in Table 3. A novel element of our model is
the ability to capture type-level tag frequencies. For
this experiment, we compare our model with the uni-
form tag assignment prior (1TW) with the learned
prior (+PRIOR). Across all languages, +PRIOR
consistently outperforms 1TW, reducing error on av-
erage by 9.1% and 5.9% on best and median settings
respectively. Similar behavior is observed when
adding features. The difference between the feature-
less model (+PRIOR) and our full model (+FEATS)
is 13.6% and 7.7% average error reduction on best
and median settings respectively. Overall, the differ-
ence between our most basic model (1TW) and our
full model (+FEATS) is 21.2% and 13.1% for the
best and median settings respectively. One striking
example is the error reduction for Spanish, which
reduces error by 36.5% and 24.7% for the best and
median settings respectively. We observe similar
trends when using another measure – type-level ac-
curacy (defined as the fraction of words correctly
assigned their majority tag), according to which
</bodyText>
<page confidence="0.995855">
859
</page>
<table confidence="0.999301090909091">
Language Metric BK10 EM BK10 LBFGS G10 FEATS Best FEATS Median
English 1-1 48.3 56.0 – 50.9 47.8
m-1 68.1 75.5 – 66.4 66.4
Danish 1-1 42.3 42.6 – 52.1 43.2
m-1 66.7 58.0 – 61.2 60.7
Dutch 1-1 53.7 55.1 – 56.4 51.5
m-1 67.0 64.7 – 69.0 67.3
Portuguese 1-1 50.8 43.2 44.5 64.1 56.5
m-1 75.3 74.8 69.2 74.5 70.1
Spanish 1-1 – 40.6 – 58.3 50.0
m-1 – 73.2 – 68.9 57.2
</table>
<tableCaption confidence="0.97003525">
Table 4: Comparison of our method (FEATS) to state-of-the-art methods. Feature-based HMM Model (Berg-
Kirkpatrick et al., 2010): The KM model uses a variety of orthographic features and employs the EM or LBFGS
optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): The G10 model uses the posterior regular-
ization approach to ensure tag sparsity constraint.
</tableCaption>
<table confidence="0.9999375">
Language 1TW +PRIOR +FEATS
English 21.1 28.8 42.8
Danish 10.1 20.7 45.9
Dutch 23.8 32.3 44.3
German 12.8 35.2 60.6
Portuguese 18.4 29.6 61.5
Spanish 7.3 27.6 49.9
Swedish 8.9 14.2 33.9
</table>
<tableCaption confidence="0.988267">
Table 6: Type-level Results: Each cell report the type-
</tableCaption>
<bodyText confidence="0.996429083333333">
level accuracy computed against the most frequent tag of
each word type. The state-to-tag mapping is obtained
from the best hyperparameter setting for 1-1 mapping
shown in Table 3.
our full model yields 39.3% average error reduction
across languages when compared to the basic con-
figuration (1TW).
Table 5 provides insight into the behavior of dif-
ferent models in terms of the tagging lexicon they
generate. The table shows that the lexicon tag fre-
quency predicated by our full model are the closest
to the gold standard.
</bodyText>
<sectionHeader confidence="0.997928" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999981285714286">
We have presented a method for unsupervised part-
of-speech tagging that considers a word type and its
allowed POS tags as a primary element of the model.
This departure from the traditional token-based tag-
ging approach allows us to explicitly capture type-
level distributional properties of valid POS tag as-
signments as part of the model. The resulting model
is compact, efficiently learnable and linguistically
expressive. Our empirical results demonstrate that
the type-based tagger rivals state-of-the-art tag-level
taggers which employ more sophisticated learning
mechanisms to exploit similar constraints.
In this paper, we make a simplifying assump-
tion of one-tag-per-word. This assumption, how-
ever, is not inherent to type-based tagging models.
A promising direction for future work is to explicitly
model a distribution over tags for each word type.
We hypothesize that modeling morphological infor-
mation will greatly constrain the set of possible tags,
thereby further refining the representation of the tag
lexicon.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999990444444445">
The authors acknowledge the support of the NSF
(CAREER grant IIS-0448168, and grant IIS-
0904684). We are especially grateful to Taylor Berg-
Kirkpatrick for running additional experiments. We
thank members of the MIT NLP group for their sug-
gestions and comments. Any opinions, findings,
conclusions, or recommendations expressed in this
paper are those of the authors, and do not necessar-
ily reflect the views of the funding organizations.
</bodyText>
<sectionHeader confidence="0.999568" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9847905">
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e,
John DeNero, and Dan Klein. 2010. Painless un-
</reference>
<page confidence="0.972872">
860
</page>
<reference confidence="0.998753951219512">
supervised learning with features. In Proceedings of
NAACL-HLT, pages 582–590.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In In Proc.
of CoNLL, pages 149–164.
Jianfeng Gao and Mark Johnson. 2008. A comparison of
bayesian estimators for unsupervised hidden markov
model pos taggers. In Proceedings of the EMNLP,
pages 344–352.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the ACL, pages
744–751.
Jo˜ao Grac¸a, Kuzman Ganchev, Ben Taskar, and Fernando
Pereira. 2009. Posterior vs. parameter sparsity in la-
tent variable models. In Proceeding of NIPS, pages
664–672.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of the
HLT-NAACL, pages 320–327.
Kazi Saidul Hasan and Vincent Ng. 2009. Weakly super-
vised part-of-speech tagging for morphologically-rich,
resource-scarce languages. In Proceedings of EACL,
pages 363–371.
Mark Johnson. 2007. Why doesn’t em find good hmm
pos-taggers? In Proceedings of EMNLP-CoNLL,
pages 296–305.
Michael Lamar, Yariv Maron, Marko Johnson, and Elie
Bienstock. 2010. Svd Clustering for Unsupervised
POS Tagging. In Proceedings ofACL, pages 215–219.
Bernard M´erialdo. 1994. Tagging english text with
a probabilistic model. Computational Linguistics,
20(2):155–171.
Sujith Ravi and Kevin Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Proceed-
ings of ACL-IJCNLP, pages 504–512.
Hinrich Schutze. 1995. Distributional part of speech tag-
ging. In Proceedings of the EACL, pages 141–148.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of the ACL.
</reference>
<page confidence="0.998363">
861
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.252368">
<title confidence="0.99963">Simple Type-Level Unsupervised POS Tagging</title>
<author confidence="0.997121">Yoong Keok Lee Aria Haghighi Regina</author>
<affiliation confidence="0.9859035">Computer Science and Artificial Intelligence Massachusetts Institute of</affiliation>
<email confidence="0.59069">aria42,</email>
<abstract confidence="0.99983455">Part-of-speech (POS) tag distributions are to exhibit a word is likely to take a single predominant tag in a corpus. Recent research has demonstrated that incorporating this sparsity constraint improves tagging accuracy. However, in existing systems, this expansion come with a steep increase in model complexity. This paper proposes a simple and effective tagging method that directly models tag sparsity and other distributional properties of valid POS tag assignments. In addition, this formulation results in a dramatic reduction in the number of model parameters thereby, enabling unusually rapid training. Our experiments consistently demonstrate that this model architecture yields substantial performance gains over more complex tagging counterparts. On several languages, we report performance exceeding that of more complex</abstract>
<intro confidence="0.438587">art</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>582--590</pages>
<marker>Berg-Kirkpatrick, Bouchard-Cˆot´e, DeNero, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e, John DeNero, and Dan Klein. 2010. Painless unsupervised learning with features. In Proceedings of NAACL-HLT, pages 582–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>Conll-x shared task on multilingual dependency parsing. In</title>
<date>2006</date>
<booktitle>In Proc. of CoNLL,</booktitle>
<pages>149--164</pages>
<contexts>
<context position="18924" citStr="Buchholz and Marsi, 2006" startWordPosition="3110" endWordPosition="3113">tch 203568 28393 12 German 699605 72325 54 Portuguese 206678 28931 22 Spanish 89334 16458 47 Swedish 191467 20057 41 Table 2: Statistics for various corpora utilized in experiments. See Section 5. The English data comes from the WSJ portion of the Penn Treebank and the other languages from the training set of the CoNLL-X multilingual dependency parsing shared task. 5.1 Data Sets Following the set-up of Johnson (2007), we use the whole of the Penn Treebank corpus for training and evaluation on English. For other languages, we use the CoNLL-X multilingual dependency parsing shared task corpora (Buchholz and Marsi, 2006) which include gold POS tags (used for evaluation). We train and test on the CoNLL-X training set. Statistics for all data sets are shown in Table 2. 5.2 Setup Models To assess the marginal utility of each component of the model (see Section 3), we incrementally increase its sophistication. Specifically, we evaluate three variants: The first model (1TW) only encodes the one tag per word constraint and is uniform over type-level tag assignments. The second model (+PRIOR) utilizes the independent prior over type-level tag assignments P(T IO). The final model (+FEATS) utilizes the tag prior as we</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In In Proc. of CoNLL, pages 149–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mark Johnson</author>
</authors>
<title>A comparison of bayesian estimators for unsupervised hidden markov model pos taggers.</title>
<date>2008</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<pages>344--352</pages>
<contexts>
<context position="2140" citStr="Gao and Johnson, 2008" startWordPosition="305" endWordPosition="309">distributional sparsity of syntactic tags is not unique to English 1The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/typetagging/. — similar results have been observed across multiple languages. Clearly, explicitly modeling such a powerful constraint on tagging assignment has a potential to significantly improve the accuracy of an unsupervised part-of-speech tagger learned without a tagging dictionary. In practice, this sparsity constraint is difficult to incorporate in a traditional POS induction system (M´erialdo, 1994; Johnson, 2007; Gao and Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010). These sequence models-based approaches commonly treat token-level tag assignment as the primary latent variable. By design, they readily capture regularities at the token-level. However, these approaches are ill-equipped to directly represent type-based constraints such as sparsity. Previous work has attempted to incorporate such constraints into token-level models via heavy-handed modifications to inference procedure and objective function (e.g., posterior regularization and ILP decoding) (Grac¸a et al., 2009; Ravi and Knight, 2009). In m</context>
<context position="5326" citStr="Gao and Johnson, 2008" startWordPosition="799" endWordPosition="803">is identifies three key factors driving our performance gain: 1) selecting a model structure which directly encodes tag sparsity, 2) a type-level prior on tag assignments, and 3) a straightforward naive-Bayes approach to incorporate features. The observed performance gains, coupled with the simplicity of model implementation, makes it a compelling alternative to existing more complex counterparts. 2 Related Work Recent work has made significant progress on unsupervised POS tagging (M´erialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006; Johnson, 2007; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Ravi and Knight, 2009). Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process. This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity. The extent to which this constraint is enforced varies greatly across existing methods. On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010). These clusters are computed using an SVD variant without relying on transit</context>
<context position="20922" citStr="Gao and Johnson (2008)" startWordPosition="3425" endWordPosition="3429">e (Haghighi and Klein, 2006) and the many-to-one token-level accuracy obtained from mapping model states to gold POS tags. We also report word type level accuracy, the fraction of word types assigned their majority tag (where the mapping between model state and tag is determined by greedy one-to-one mapping discussed above).5 For each language, we aggregate results in the following way: First, for each hyperparameter setting, 4Typically, the performance stabilizes after only 10 iterations. 5We choose these two metrics over the Variation Information measure due to the deficiencies discussed in Gao and Johnson (2008). 858 we perform five runs with different random initialization of sampling state. Hyperparameter settings are sorted according to the median one-to-one metric over runs. We report results for the best and median hyperparameter settings obtained in this way. Specifically, for both settings we report results on the median run for each setting. Tag set As is standard, for all experiments, we set the number of latent model tag states to the size of the annotated tag set. The original tag set for the CoNLL-X Dutch data set consists of compounded tags that are used to tag multi-word units (MWUs) re</context>
</contexts>
<marker>Gao, Johnson, 2008</marker>
<rawString>Jianfeng Gao and Mark Johnson. 2008. A comparison of bayesian estimators for unsupervised hidden markov model pos taggers. In Proceedings of the EMNLP, pages 344–352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-ofspeech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>744--751</pages>
<contexts>
<context position="5303" citStr="Goldwater and Griffiths, 2007" startWordPosition="795" endWordPosition="798">-of-the art systems. Our analysis identifies three key factors driving our performance gain: 1) selecting a model structure which directly encodes tag sparsity, 2) a type-level prior on tag assignments, and 3) a straightforward naive-Bayes approach to incorporate features. The observed performance gains, coupled with the simplicity of model implementation, makes it a compelling alternative to existing more complex counterparts. 2 Related Work Recent work has made significant progress on unsupervised POS tagging (M´erialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006; Johnson, 2007; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Ravi and Knight, 2009). Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process. This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity. The extent to which this constraint is enforced varies greatly across existing methods. On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010). These clusters are computed using an SVD variant wit</context>
<context position="9297" citStr="Goldwater and Griffiths, 2007" startWordPosition="1449" endWordPosition="1452"> a token-level tag and word sequence, (t, w), is generated in the standard HMM fashion: a tag sequence t is generated from 0. The corresponding token words w are drawn conditioned on t and B.2 Our full generative model is given by: P(T,W,B,0,0,t,wJα,Q) = P(T, W, 0JQ) [Lexicon] P(0, BJT, α, Q) [Parameter] P(w, tJ0, B) [Token] We refer to the components on the right hand side as the lexicon, parameter, and token component respectively. Since the parameter and token components will remain fixed throughout experiments, we briefly describe each. Parameter Component As in the standard Bayesian HMM (Goldwater and Griffiths, 2007), all distributions are independently drawn from symmetric Dirichlet distributions: 2Note that t and w denote tag and word sequences respectively, rather than individual tokens or tags. K P(0, BJT, α, Q) = Y (P(0tJα)P(BtJT, α)) t=1 The transition distribution 0t for each tag t is drawn according to DIRICHLET(α, K), where α is the shared transition and emission distribution hyperparameter. In total there are O(K2) parameters associated with the transition parameters. In contrast to the Bayesian HMM, Bt is not drawn from a distribution which has support for each of the n word types. Instead, we </context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Thomas L. Griffiths. 2007. A fully Bayesian approach to unsupervised part-ofspeech tagging. In Proceedings of the ACL, pages 744–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jo˜ao Grac¸a</author>
<author>Kuzman Ganchev</author>
<author>Ben Taskar</author>
<author>Fernando Pereira</author>
</authors>
<title>Posterior vs. parameter sparsity in latent variable models.</title>
<date>2009</date>
<booktitle>In Proceeding of NIPS,</booktitle>
<pages>664--672</pages>
<marker>Grac¸a, Ganchev, Taskar, Pereira, 2009</marker>
<rawString>Jo˜ao Grac¸a, Kuzman Ganchev, Ben Taskar, and Fernando Pereira. 2009. Posterior vs. parameter sparsity in latent variable models. In Proceeding of NIPS, pages 664–672.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT-NAACL,</booktitle>
<pages>320--327</pages>
<contexts>
<context position="5257" citStr="Haghighi and Klein, 2006" startWordPosition="789" endWordPosition="792">eport performance exceeding that of state-of-the art systems. Our analysis identifies three key factors driving our performance gain: 1) selecting a model structure which directly encodes tag sparsity, 2) a type-level prior on tag assignments, and 3) a straightforward naive-Bayes approach to incorporate features. The observed performance gains, coupled with the simplicity of model implementation, makes it a compelling alternative to existing more complex counterparts. 2 Related Work Recent work has made significant progress on unsupervised POS tagging (M´erialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006; Johnson, 2007; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Ravi and Knight, 2009). Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process. This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity. The extent to which this constraint is enforced varies greatly across existing methods. On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010). These </context>
<context position="20328" citStr="Haghighi and Klein, 2006" startWordPosition="3332" endWordPosition="3335">ameters: α is the shared hyperparameter for the token-level HMM emission and transition distributions. Q is the shared hyperparameter for the tag assignment prior and word feature multinomials. We experiment with four values for each hyperparameter resulting in 16 (α, Q) combinations: α Q 0.001, 0.01, 0.1,1.0 0.01, 0.1,1.0,10 Iterations In each run, we performed 30 iterations of Gibbs sampling for the type assignment variables W.4 We use the final sample for evaluation. Evaluation Metrics We report three metrics to evaluate tagging performance. As is standard, we report the greedy one-to-one (Haghighi and Klein, 2006) and the many-to-one token-level accuracy obtained from mapping model states to gold POS tags. We also report word type level accuracy, the fraction of word types assigned their majority tag (where the mapping between model state and tag is determined by greedy one-to-one mapping discussed above).5 For each language, we aggregate results in the following way: First, for each hyperparameter setting, 4Typically, the performance stabilizes after only 10 iterations. 5We choose these two metrics over the Variation Information measure due to the deficiencies discussed in Gao and Johnson (2008). 858 </context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. 2006. Prototype-driven learning for sequence models. In Proceedings of the HLT-NAACL, pages 320–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazi Saidul Hasan</author>
<author>Vincent Ng</author>
</authors>
<title>Weakly supervised part-of-speech tagging for morphologically-rich, resource-scarce languages.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>363--371</pages>
<contexts>
<context position="7431" citStr="Hasan and Ng, 2009" startWordPosition="1136" endWordPosition="1139"> and Knight (2009), who minimize the size of tagging grammar as measured by the number of transition types. The use of ILP in learning the desired grammar significantly increases the computational complexity of this method. In contrast to these approaches, our method directly incorporates these constraints into the structure of the model. This design leads to a significant reduction in the computational complexity of training and inference. Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009). These methods demonstrated the benefits of incorporating linguistic features using a log-linear parameterization, but requires elaborate machinery for training. In our 854 work, we demonstrate that using a simple naiveBayes approach also yields substantial performance gains, without the associated training complexity. 3 Generative Story We consider the unsupervised POS induction problem without the use of a tagging dictionary. A graphical depiction of our model as well as a summary of random variables and parameters can be found in Figure 1. As is standard, we use a fixed constant K for the </context>
<context position="13016" citStr="Hasan and Ng, 2009" startWordPosition="2100" endWordPosition="2103">Note that this distribution captures the frequency of a tag across word types, as opposed to tokens. The P(T |ψ) distribution, in English for instance, should have very low mass for the DT (determiner) tag, since determiners are a very small portion of the vocabulary. In contrast, NNP (proper nouns) form a large portion of vocabulary. Note that these observations are not modeled by the standard HMM, which instead can model token-level frequency. Word Type Features (FEATS): Past unsupervised POS work have derived benefits from features on word types, such as suffix and capitalization features (Hasan and Ng, 2009; Berg-Kirkpatrick et al., 2010). Past work however, has typically associated these features with token occurrences, typically in an HMM. In our model, we associate these features at the type-level in the lexicon. Here, we consider suffix features, capitalization features, punctuation, and digit features. While possible to utilize the feature-based log-linear approach described in Berg-Kirkpatrick et al. (2010), we adopt a simpler naive Bayes strategy, where all features are emitted independently. Specifically, we assume each word type W consists of feature-value pairs (f, v). For each feature</context>
</contexts>
<marker>Hasan, Ng, 2009</marker>
<rawString>Kazi Saidul Hasan and Vincent Ng. 2009. Weakly supervised part-of-speech tagging for morphologically-rich, resource-scarce languages. In Proceedings of EACL, pages 363–371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why doesn’t em find good hmm pos-taggers?</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>296--305</pages>
<contexts>
<context position="2117" citStr="Johnson, 2007" startWordPosition="303" endWordPosition="304">Treebank. This distributional sparsity of syntactic tags is not unique to English 1The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/typetagging/. — similar results have been observed across multiple languages. Clearly, explicitly modeling such a powerful constraint on tagging assignment has a potential to significantly improve the accuracy of an unsupervised part-of-speech tagger learned without a tagging dictionary. In practice, this sparsity constraint is difficult to incorporate in a traditional POS induction system (M´erialdo, 1994; Johnson, 2007; Gao and Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010). These sequence models-based approaches commonly treat token-level tag assignment as the primary latent variable. By design, they readily capture regularities at the token-level. However, these approaches are ill-equipped to directly represent type-based constraints such as sparsity. Previous work has attempted to incorporate such constraints into token-level models via heavy-handed modifications to inference procedure and objective function (e.g., posterior regularization and ILP decoding) (Grac¸a et al., 2009; Ravi </context>
<context position="4095" citStr="Johnson, 2007" startWordPosition="618" endWordPosition="619"> c�2010 Association for Computational Linguistics Language Original case English 94.6 Danish 96.3 Dutch 96.6 German 95.5 Spanish 95.4 Swedish 93.3 Portuguese 95.6 Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag. Across all languages, high performance can be attained by selecting a single tag per word type. token-level HMM to reflect lexicon sparsity. This model admits a simple Gibbs sampling algorithm where the number of latent variables is proportional to the number of word types, rather than the size of a corpus as for a standard HMM sampler (Johnson, 2007). There are two key benefits of this model architecture. First, it directly encodes linguistic intuitions about POS tag assignments: the model structure reflects the one-tag-per-word property, and a typelevel tag prior captures the skew on tag assignments (e.g., there are fewer unique determiners than unique nouns). Second, the reduced number of hidden variables and parameters dramatically speeds up learning and inference. We evaluate our model on seven languages exhibiting substantial syntactic variation. On several languages, we report performance exceeding that of state-of-the art systems. </context>
<context position="6260" citStr="Johnson (2007)" startWordPosition="950" endWordPosition="951">this constraint is enforced varies greatly across existing methods. On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010). These clusters are computed using an SVD variant without relying on transitional structure. While our method also enforces a singe tag per word constraint, it leverages the transition distribution encoded in an HMM, thereby benefiting from a richer representation of context. Other approaches encode sparsity as a soft constraint. For instance, by altering the emission distribution parameters, Johnson (2007) encourages the model to put most of the probability mass on few tags. This design does not guarantee “structural zeros,” but biases towards sparsity. A more forceful approach for encoding sparsity is posterior regularization, which constrains the posterior to have a small number of expected tag assignments (Grac¸a et al., 2009). This approach makes the training objective more complex by adding linear constraints proportional to the number of word types, which is rather prohibitive. A more rigid mechanism for modeling sparsity is proposed by Ravi and Knight (2009), who minimize the size of tag</context>
<context position="15405" citStr="Johnson (2007)" startWordPosition="2508" endWordPosition="2509">Z), w, α, β) = P(TZ|W, T −Z, β)P(t(Z)|TZ, t(−Z), w, α) where T−Z denotes all type-level tag assignment except TZ and t(−Z) denotes all token-level tags except t(Z). The terms on the right-hand-side denote the type-level and token-level probability terms respectively. The type-level posterior term can be computed according to, P(TZ|W,T−Z, β) a P(TZ|T−Z, β) � P(v|TZ, f, W −Z, T−Z, β) (f,v)∈Wi All of the probabilities on the right-hand-side are Dirichlet, distributions which can be computed analytically given counts. The token-level term is similar to the standard HMM sampling equations found in Johnson (2007). The relevant variables are the set of token-level tags that appear before and after each instance of the ith word type; we denote these context pairs with the set {(tb, ta)I and they are contained in t(−Z). We use w English Danish Dutch Germany Portuguese Spanish Swedish 5 10 15 20 25 30 Iteration Figure 2: Graph of the one-to-one accuracy of our full model (+FEATS) under the best hyperparameter setting by iteration (see Section 5). Performance typically stabilizes across languages after only a few number of iterations. to represent the ith word type emitted by the HMM: P(t(Z)|TZ, t(−Z), w, </context>
<context position="18719" citStr="Johnson (2007)" startWordPosition="3078" endWordPosition="3079"> components cascade, so the row corresponding to +FEATS also includes the PRIOR component (see Section 3). Language # Tokens # Word Types # Tags English 1173766 49206 45 Danish 94386 18356 25 Dutch 203568 28393 12 German 699605 72325 54 Portuguese 206678 28931 22 Spanish 89334 16458 47 Swedish 191467 20057 41 Table 2: Statistics for various corpora utilized in experiments. See Section 5. The English data comes from the WSJ portion of the Penn Treebank and the other languages from the training set of the CoNLL-X multilingual dependency parsing shared task. 5.1 Data Sets Following the set-up of Johnson (2007), we use the whole of the Penn Treebank corpus for training and evaluation on English. For other languages, we use the CoNLL-X multilingual dependency parsing shared task corpora (Buchholz and Marsi, 2006) which include gold POS tags (used for evaluation). We train and test on the CoNLL-X training set. Statistics for all data sets are shown in Table 2. 5.2 Setup Models To assess the marginal utility of each component of the model (see Section 3), we incrementally increase its sophistication. Specifically, we evaluate three variants: The first model (1TW) only encodes the one tag per word const</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Why doesn’t em find good hmm pos-taggers? In Proceedings of EMNLP-CoNLL, pages 296–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lamar</author>
<author>Yariv Maron</author>
<author>Marko Johnson</author>
<author>Elie Bienstock</author>
</authors>
<title>Svd Clustering for Unsupervised POS Tagging.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>215--219</pages>
<contexts>
<context position="5849" citStr="Lamar et al., 2010" startWordPosition="886" endWordPosition="889">5; Haghighi and Klein, 2006; Johnson, 2007; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Ravi and Knight, 2009). Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process. This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity. The extent to which this constraint is enforced varies greatly across existing methods. On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010). These clusters are computed using an SVD variant without relying on transitional structure. While our method also enforces a singe tag per word constraint, it leverages the transition distribution encoded in an HMM, thereby benefiting from a richer representation of context. Other approaches encode sparsity as a soft constraint. For instance, by altering the emission distribution parameters, Johnson (2007) encourages the model to put most of the probability mass on few tags. This design does not guarantee “structural zeros,” but biases towards sparsity. A more forceful approach for encoding </context>
</contexts>
<marker>Lamar, Maron, Johnson, Bienstock, 2010</marker>
<rawString>Michael Lamar, Yariv Maron, Marko Johnson, and Elie Bienstock. 2010. Svd Clustering for Unsupervised POS Tagging. In Proceedings ofACL, pages 215–219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard M´erialdo</author>
</authors>
<title>Tagging english text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<marker>M´erialdo, 1994</marker>
<rawString>Bernard M´erialdo. 1994. Tagging english text with a probabilistic model. Computational Linguistics, 20(2):155–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Minimized models for unsupervised part-of-speech tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP,</booktitle>
<pages>504--512</pages>
<contexts>
<context position="2734" citStr="Ravi and Knight, 2009" startWordPosition="390" endWordPosition="393"> 2007; Gao and Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010). These sequence models-based approaches commonly treat token-level tag assignment as the primary latent variable. By design, they readily capture regularities at the token-level. However, these approaches are ill-equipped to directly represent type-based constraints such as sparsity. Previous work has attempted to incorporate such constraints into token-level models via heavy-handed modifications to inference procedure and objective function (e.g., posterior regularization and ILP decoding) (Grac¸a et al., 2009; Ravi and Knight, 2009). In most cases, however, these expansions come with a steep increase in model complexity, with respect to training procedure and inference time. In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model. The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word. Then, tokenlevel HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag. In this way we restrict the parameterization of a 853</context>
<context position="5350" citStr="Ravi and Knight, 2009" startWordPosition="804" endWordPosition="807"> factors driving our performance gain: 1) selecting a model structure which directly encodes tag sparsity, 2) a type-level prior on tag assignments, and 3) a straightforward naive-Bayes approach to incorporate features. The observed performance gains, coupled with the simplicity of model implementation, makes it a compelling alternative to existing more complex counterparts. 2 Related Work Recent work has made significant progress on unsupervised POS tagging (M´erialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006; Johnson, 2007; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Ravi and Knight, 2009). Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process. This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity. The extent to which this constraint is enforced varies greatly across existing methods. On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010). These clusters are computed using an SVD variant without relying on transitional structure. While o</context>
<context position="6830" citStr="Ravi and Knight (2009)" startWordPosition="1040" endWordPosition="1043">the emission distribution parameters, Johnson (2007) encourages the model to put most of the probability mass on few tags. This design does not guarantee “structural zeros,” but biases towards sparsity. A more forceful approach for encoding sparsity is posterior regularization, which constrains the posterior to have a small number of expected tag assignments (Grac¸a et al., 2009). This approach makes the training objective more complex by adding linear constraints proportional to the number of word types, which is rather prohibitive. A more rigid mechanism for modeling sparsity is proposed by Ravi and Knight (2009), who minimize the size of tagging grammar as measured by the number of transition types. The use of ILP in learning the desired grammar significantly increases the computational complexity of this method. In contrast to these approaches, our method directly incorporates these constraints into the structure of the model. This design leads to a significant reduction in the computational complexity of training and inference. Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009</context>
</contexts>
<marker>Ravi, Knight, 2009</marker>
<rawString>Sujith Ravi and Kevin Knight. 2009. Minimized models for unsupervised part-of-speech tagging. In Proceedings of ACL-IJCNLP, pages 504–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schutze</author>
</authors>
<title>Distributional part of speech tagging.</title>
<date>1995</date>
<booktitle>In Proceedings of the EACL,</booktitle>
<pages>141--148</pages>
<contexts>
<context position="5828" citStr="Schutze, 1995" startWordPosition="884" endWordPosition="885">and Eisner, 2005; Haghighi and Klein, 2006; Johnson, 2007; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Ravi and Knight, 2009). Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process. This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity. The extent to which this constraint is enforced varies greatly across existing methods. On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010). These clusters are computed using an SVD variant without relying on transitional structure. While our method also enforces a singe tag per word constraint, it leverages the transition distribution encoded in an HMM, thereby benefiting from a richer representation of context. Other approaches encode sparsity as a soft constraint. For instance, by altering the emission distribution parameters, Johnson (2007) encourages the model to put most of the probability mass on few tags. This design does not guarantee “structural zeros,” but biases towards sparsity. A more forceful a</context>
</contexts>
<marker>Schutze, 1995</marker>
<rawString>Hinrich Schutze. 1995. Distributional part of speech tagging. In Proceedings of the EACL, pages 141–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="5231" citStr="Smith and Eisner, 2005" startWordPosition="785" endWordPosition="788"> several languages, we report performance exceeding that of state-of-the art systems. Our analysis identifies three key factors driving our performance gain: 1) selecting a model structure which directly encodes tag sparsity, 2) a type-level prior on tag assignments, and 3) a straightforward naive-Bayes approach to incorporate features. The observed performance gains, coupled with the simplicity of model implementation, makes it a compelling alternative to existing more complex counterparts. 2 Related Work Recent work has made significant progress on unsupervised POS tagging (M´erialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006; Johnson, 2007; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Ravi and Knight, 2009). Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process. This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity. The extent to which this constraint is enforced varies greatly across existing methods. On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; L</context>
<context position="7379" citStr="Smith and Eisner, 2005" startWordPosition="1128" endWordPosition="1131">gid mechanism for modeling sparsity is proposed by Ravi and Knight (2009), who minimize the size of tagging grammar as measured by the number of transition types. The use of ILP in learning the desired grammar significantly increases the computational complexity of this method. In contrast to these approaches, our method directly incorporates these constraints into the structure of the model. This design leads to a significant reduction in the computational complexity of training and inference. Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009). These methods demonstrated the benefits of incorporating linguistic features using a log-linear parameterization, but requires elaborate machinery for training. In our 854 work, we demonstrate that using a simple naiveBayes approach also yields substantial performance gains, without the associated training complexity. 3 Generative Story We consider the unsupervised POS induction problem without the use of a tagging dictionary. A graphical depiction of our model as well as a summary of random variables and parameters can be found in Figure 1</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of the ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>