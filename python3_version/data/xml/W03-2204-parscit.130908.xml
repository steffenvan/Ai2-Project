<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000088">
<title confidence="0.985375">
Multi-language Machine Translation through Interactive
Document Normalization
</title>
<author confidence="0.890693">
Aurelien Max
</author>
<affiliation confidence="0.449550333333333">
Groupe d&apos;Etude pour la Traduction Automatique (GETA)
Xerox Research Centre Europe (XRCE)
Grenoble, France
</affiliation>
<email confidence="0.992817">
aurelien.max@imag.fr
</email>
<sectionHeader confidence="0.979603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998160590909091">
Document normalization is an interactive
process that transforms raw legacy docu-
ments into semantically well-formed and
linguistically controlled documents with
the same communicative intention con-
tent. A paradigm for content analysis has
been implemented to select candidate se-
mantic representations of the communica-
tive content of an input document. This
implementation reuses the formal content
specification of a multilingual controlled
authoring system. As a consequence,
a candidate semantic representation can
not only be associated with a text in the
language of the input document, but also
in all the languages supported by the sys-
tem. This paper presents how multilin-
gual versions of an input legacy document
can be obtained interactively with a pro-
posed implementation, and discusses the
advantages and limitations of this kind of
normalizing translation.
</bodyText>
<sectionHeader confidence="0.996314" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999829555555556">
Translating unrestricted text by machine is a prob-
lem that has been involving a lot of research for the
past decades, but is still far from solved (Cole et
al., 1996). This task arises so many problems in
computational linguistics, most of them only par-
tially solved, that a lot of research is still to be car-
ried out before one can ask a personal computer to
translate accurately an arbitrary piece of text from
one language to another. The performance bottle-
neck due to the lack of linguistic and knowledge
resources has led the builders of practical transla-
tion systems to constrain the input to controlled
languages, and/or to have recourse to human ex-
pertise on the source language and on the discourse
domain (e.g. (Boitet and Blanchon, 1996; Baker
et al., 1994)). Unsurprisingly, the most success-
ful systems to date operate on text in very lim-
ited domains, exemplified by the weather forecast
translation from English to French of the METED
system.
There exist many situations where documents
belonging to a constrained domain have to be
translated in several languages, as is the case of
official documents in multilingual communities or
product descriptions for international companies.
In these situations one have at least the following
expectations:
</bodyText>
<listItem confidence="0.999326">
• high-quality translation, which implies
that it be accurate and not necessarily literal
• outputs in possibly many target languages
• consistency across documents of the same
</listItem>
<bodyText confidence="0.911383333333334">
class (e.g. drug leaflets, experiment reports),
so that concepts are always expressed in the
same unambiguous manner and the texts pro-
duced can be regarded as gold standards for
the meaning they convey
Differents methods that do not impose con-
straints on the input text have been proposed to
achieve high-quality translation. Interaction with
a user can be used to disambiguate the input text,
and could be prefered to to post-editing as this has
to be done only once for all languages, thus reduc-
ing the time and efforts needed. Interlingual repre-
sentations (Hutchins and Somers, 1992) are well-
adapted to support the production of the target
text in several languages, and they can also be ef-
fectively used to check the semantic coherence and
well-formedness of a document. Reusing previous
translations, as proposed in the different flavours
of Example-based Machine Translation (Somers,
1999), is an interesting alternative to purely rule-
based approaches and allows the selection of non-
literal high-quality translation candidates.
This paper starts with a short presentation of
an authoring system that allows the creation of
</bodyText>
<page confidence="0.996503">
25
</page>
<bodyText confidence="0.999425">
multingual documents with all the above proper-
ties. Document &apos;normalization, which is described
next, stemmed from the question of whether such
an authoring system could be used in a reversed
mode to analyze existing documents from the class
of documents supported by it. After providing a
motivating example, we will briefly introduce fuzzy
inverted generation, a paradigm we proposed to
normalize documents reusing the formalism of the
abovementioned authoring system, and describe a
document normalization system. We will then at-
tempt to define how normalizing translation can
be achieved through document normalization, and
we will discuss the advantages and limitations of
such an approach.
</bodyText>
<sectionHeader confidence="0.749118" genericHeader="method">
2 Controlled Document Authoring
</sectionHeader>
<bodyText confidence="0.905864547169811">
Controlled Document Authoring is an active field
of research comprising approaches such as the
What You See Is What You Meant (WYSIWYM)
paradigm (Power and Scott, 1998) and Multilin-
gual Document Authoring (MDA) (Dymetman et
al., 2000). The systems allow authors to specify
document content representations interactively in
their own language, and then produce versions in
several languages using parallel resources.
In MDA, a system developed at XRCE, the au-
thor of a document has to select valid semantic
choices in active fields interspersed with the evolv-
ing text of the document in her language until the
document is complete (see figure 1). The system
can at any time produce current versions of the
documents from the content representation in all
the languages it supports. The documents thus
obtained are of high-quality, and are not necessar-
ily literal translations but rather adaptations to
a given language.&apos; In fact, the linguistic struc-
tures of two documents can be completely differ-
ent in two different languages, and communicative
intentions can be conveyed in quite different ways.
Moreover, since the generator of an MDA system is
deterministic, the texts produced will be consistent
across documents.
The specification of well-formed document con-
tent representations in MDA is recursively de-
scribed in a grammar formalism that is a variant
of Definite Clause Grammars (Pereira and Warren,
1980). Text strings can appear in right-hand sides
of rules, which allows text realizations to be as-
sociated to content representations, and thus pro-
vides a close coupling between semantic modelling
&apos;Different parts of the document can thus be easily
localized: for example, disclaimers and contact infor-
mation can be adapted to the targeted community.
and generation. Figure 2 shows an abstract typed
tree in the MDA formalism and its realizations as
English and French sentences.&apos; Non-terminals are
typed semantic elements whose type appears af-
ter the two colons. Dependencies can be enforced
through the use of shared variables between seman-
tic elements. The granularity of text fragments
in rules is not necessarily a fine-grained predicate-
argument structure of sentences commonly used in
NLG, so this is an intermediate level between full
NLG and templates (Reiter, 1995). This approach
proved to be adequate for classes of documents
where the productivity of certain choices could be
rendered as entire text spans, as is the case for ex-
ample of warning sections in drug leaflets (Brun et
al., 2000).
</bodyText>
<sectionHeader confidence="0.996513" genericHeader="method">
3 Document normalization
</sectionHeader>
<subsectionHeader confidence="0.998367">
3.1 A Motivating example
</subsectionHeader>
<bodyText confidence="0.999989">
The pharmaceutical domain produces yearly pub-
lications which are compendiums of documents
initially produced by pharmaceutical companies
which are presented in a consistent way (e.g.
(ABPI, 1996; OVP Editions du VIDAL, 1998)).
Several kinds of variations were observed in a cor-
pus study we conducted on a corpus of 50 patient
pharmaceutical leaflets for pain relievers from dif-
ferent drug vendors (Max, 2002). First, the struc-
tures of the leaflets could vary considerably, as well
as the locations where certain communicative goals
were expressed. (Paiva, 2000) showed the presence
of significant stylistic variation in a corpus of 342
patient leaflets. Our study also revealed that sim-
ilar communicative intentions could be expressed
in a variety of ways conveying more or less subtle
semantic distinctions. Seeing the content of such
documents as goal-driven communication, a given
utterance can be seen as an attempt to satisfy some
communicative goal on the part of the writer of the
document. We argue that for documents of the
importance of pharmaceutical leaflets consistency
of expression and of information presentation can
be beneficial to the reader by allowing a clear and
unambiguous understanding of the communicative
goals contained in different documents. It can in-
deed sometimes be confusing for a reader to find
various ways to express the same communicative
intentions, as in the following examples:
</bodyText>
<listItem confidence="0.982052">
• This product should not be taken for more than
</listItem>
<bodyText confidence="0.921808333333333">
2This example and its specification are inspired
from the Nespole! project, a speech-to-speech trans-
lation project.
</bodyText>
<page confidence="0.991448">
26
</page>
<table confidence="0.991331166666667">
rilInterlace tor Multilingual Document Authoring
File Edit Windows Traces _
111noticeVidal (3 Var) 0 English Editable
IltitreNotice (1 Var)
Q ll presentation (7 Var)
D Free -r&gt;Dibotron
D Variable 1
D Variable 2
D Variable 3
D gelule
D condGelule
iP comptage (1 Var)
D Variable 4
D uniteGelule
D Variable 5
D Variable 6
D Variable 7
IP CI dosagePactif
D Free ‹T.45&lt;fr
D uniteFormeGel
D Variable 8
0- li composition (1 Var)
Dibotron Laboratory name ---&apos;1- ----.-- -3recision
DESCRIPTION : Dibotron 45 mg person catego--
• box of Numbs
COMPOSITION : p gel Adult warning
Atenol 12 mg Child
excipient list infant
New born baby
all persons
INDICATIONS : descriptive class usage
CONTRAINDICATIONS : contraindications
WARNINGS : list of warnings driver warning sportsmen/sportswomen
DRUG INTERACTIONS chug interaction and breast-feeding: general pregnancy brea
PREGNANCY AND BREAST-FEEDING pregnancy
T17.77.:. I I •1
</table>
<figureCaption confidence="0.998761">
Figure 1: View of the MDA system during the authoring of a patient information leaflet
</figureCaption>
<bodyText confidence="0.984366833333333">
giyeInfo-disposition-stay:speech-act-e
wouldPrefer::disposition-e &apos;to stay at &apos; campsite::location-e
&apos;I would prefer&apos; &apos;a camp site &apos;
giveInfo-disposition-stay:speech-act-f
wouldPrefer::disposition-f &apos;sejourner dans &apos; campsite::location-f
&apos;Je prefererais&apos; &apos;un camping &apos;
</bodyText>
<figureCaption confidence="0.634242">
Figure 2: Abstract typed trees in English and French for the sentence I would prefer to stay at a camp
site
</figureCaption>
<page confidence="0.911047">
27
</page>
<bodyText confidence="0.384065">
14 days without first consulting a health pro-
fessional.
</bodyText>
<listItem confidence="0.98632675">
• If pain persists after 14 days, consult your doc-
tor before taking any more of this product.
• If symptoms persist for 2 weeks, stop using this
product and see a physician.
</listItem>
<bodyText confidence="0.995987117647059">
Document normalization can be achieved by an-
alyzing a legacy document into a semantically pos-
sible content representation, and producing a nor-
malized version from that content representation.
This normalized version expresses predefined con-
tent, which is conveyed in the input document, in a
structurally and linguistically controlled way. Pre-
defined content reveals communicative goals, which
should typically be described by an expert of the
discourse domain. Control on the production of
text from some content representation allows to
produce messages that can be seen as some sort of
&apos;gold standard&apos; for the communicative goal that are
conveyed and that can be augmented to be made
self-explaining (Boitet, 1996), and to obtain con-
sistent document structures as well as to impose
terminological and stylistic guidelines.
</bodyText>
<subsectionHeader confidence="0.99675">
3.2 Fuzzy inverted generation
</subsectionHeader>
<bodyText confidence="0.999944928571428">
For the purpose of document normalization we
would like to match texts that do not carry sig-
nificant communicative differences in a given class
of documents but may be of quite different surface
forms. Therefore, we proposed to concentrate more
on what counts as a well-formed document seman-
tic representation rather than on surface properties
of text, as the space of possible content represen-
tations is vastly more restricted than the space of
possible texts.
Bridging the gap between deep content and sur-
face text can be done by using the textual pre-
dictions made by the generator of an MDA sys-
tem from well-formed content representations to
match an input document. Indeed, an MDA sys-
tem can be used as a formal device for enumer-
ating well-formed document representations in a
constrained domain and associating textual repre-
sentations with them. If we can compute a rele-
vant measure of semantic similarity between the
text produced for any document content repre-
sentation and the text of a legacy document, we
could possibly consider the representations with
the best similarity score as those best correspond-
ing to the legacy document under analysis. Since
this kind of analysis uses predictions made by a
natural language generator, we named it inverted
generation (Max and Dymetman, 2002) (see fig.
</bodyText>
<figure confidence="0.806736285714286">
Nell-formed
content
representation
Prediction of
textual properties
Shallow analysis
Paw text
</figure>
<figureCaption confidence="0.9947175">
Figure 3: Deep content analysis through fuzzy in-
verted generation
</figureCaption>
<bodyText confidence="0.999037321428571">
3). We also qualified it fuzzy, because as a gen-
erator will seriously undergenerate with respect to
all the texts that could be normalized to the same
communicative intention, the matching procedure
has to be performed at a more abstract level than
on raw text to evaluate commonality of commu-
nicative content. Considering the types of docu-
ments that could be analyzed using this paradigm,
it seemed relevant to expand the generative power
of the system, so that different texts could be as-
sociated with the same content representations to
increase the robustness of the analysis. Although
this non-determinism proves beneficial for inverted
generation, we implemented it in such a way that
the generation process would still be done deter-
ministically.
To normalize an input document, we would like
to find the virtual document3 that is most simi-
lar to the input document in terms of the com-
municative content it conveys. The space of vir-
tual documents for a given class of documents be-
ing potentially huge, we proposed an admissible
heuristic search procedure (Nilsson, 1998), so that
the candidate structures are returned in an order
of decreasing similarity with the input text. The
evaluation function it uses is an optimistic mea-
sure of similarity that corresponds to a weighted
intersection between the lexical profile of the input
</bodyText>
<footnote confidence="0.869857">
3We call virtual document a document that can be
predicted by the authoring system but does not exist
a priori.
</footnote>
<page confidence="0.995444">
28
</page>
<figureCaption confidence="0.999908">
Figure 4: Architecture of the document normalization system
</figureCaption>
<bodyText confidence="0.999418">
document and that computed for a partial content
representation.4 The lexical profile for a text frag-
ment is defined as a vector of informative synsets5
associated with their number of occurrences, and
the lexical profile for an MDA semantic type gives
the maximum number of occurrences of any given
synset that could be attained by performing any
derivation from that type.
</bodyText>
<subsectionHeader confidence="0.991992">
3.3 Document normalization system
</subsectionHeader>
<bodyText confidence="0.999953384615385">
Figure 4 shows an overview of the document nor-
malization system that we have started to develop.
An MDA grammar is first compiled off-line to asso-
ciate profiles with all its semantic types by perco-
lating profiles in the grammar from the terminals
up to the root type. This compiled version of the
grammar is used in conjunction with the profile
computed for the input document in a first pass
analysis. The aim of this first pass analysis im-
plementing fuzzy inverted generation is to isolate
a limited set of candidate content representations.
A second pass analysis is then applied on those
candidates, which are now actual texts associated
</bodyText>
<footnote confidence="0.881070666666667">
4More details on how fuzzy inverted generation can
be implemented in MDA can be found in (Max, 2002).
5Synsets from WordNet, or ideally from a special-
ized thesaurus, have been prefered to lemma in order
to account for lexico-semantic variation (Gonzalo et al.,
1998).
</footnote>
<bodyText confidence="0.729940333333333">
with their content representation, using more fine-
grained linguistic analysis, in conjunction with in-
teractive disambiguation when needed.6.
</bodyText>
<sectionHeader confidence="0.973943" genericHeader="method">
4 Normalizing translation
</sectionHeader>
<bodyText confidence="0.999735625">
Using the resources of a multilingual authoring sys-
tem to analyze a legacy document offers a natu-
ral possibility: once the semantic content repre-
sentation is obtained through document normal-
ization, the generative capability of the author-
ing system can be reused to produce the docu-
ments corresponding to that representation in all
the supported languages (see figure 5). Normal-
izing translation uses the same resources for both
analysis and generation, and shares some proper-
ties with a pivot approach. A significant differ-
ence with previous approaches to translation us-
ing reversible grammars is that fuzzy matching is
used. As this approach to machine translation re-
lies on the matching with existing texts (those that
can be produced by the generator of the authoring
system), it shares some properties with Example-
based Machine Translation (Somers, 1999), with
the specificity that matched text fragments corre-
spond first to semantic types in the MDA formal-
ism and then eventually to their appropriate trans-
6Typically, interactive disambiguation will allow an
expert to prefer one of several ambiguous candidates
on the basis of the legacy document.
</bodyText>
<page confidence="0.998863">
29
</page>
<figureCaption confidence="0.96884">
Figure 5: Normalizing translation using MDA
grammars
</figureCaption>
<bodyText confidence="0.99507428">
lations in other languages. Under the assumption
that the authoring system produces high-quality
documents in all the languages it supports, then
evaluating normalizing translation can be limited
to evaluating the performance of document nor-
malization. Forthcoming publications will attempt
to address this issue.
A simple example of normalizing translation is
given on figure 6. The discourse domain is assumed
to be that of travel information, where some se-
mantic distinctions are considered uninformative.
In this case the English speaker wishes to men-
tion his family&apos;s first choice, but this is lost in
the translation. The utterance is best matched
with wouldPrefer-3, a possible English realization
for the semantic type disposition, in the context of
givelnfo-disposition-stay. It is then normalized to
the deterministic choice of the English generator,
wouldPrefer. The structure to which it belongs,
which is of type speech-act, can then be rendered
in English as I would prefer to stay at a camp
site. Using parallel MDA grammars for French
and Spanish allows to obtain the corresponding ab-
stract trees from which the French and Spanish ver-
sions of the normalized sentence can be obtained.
</bodyText>
<sectionHeader confidence="0.998896" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999852672413793">
The proposed approach to translation has impor-
tant limitations: first, only documents in con-
strained domains which can be modeled with the
MDA formalism can be dealt with. This excludes
arbitrary pieces of text, and requires an initial de-
velopment of the grammatical resources and its
transposition to as many languages as the system
should support. However, this would allow to reuse
the document modeling for authoring new docu-
ments from scratch, which could modify in a bene-
ficial way the documentation practices of technical
writers. As opposed to &apos;traditional&apos; machine trans-
lation, normalizing translation can only translate
those elements of a text that fit in well-formed
document content representations. Consequently,
elements that are not modeled in the grammar
used for analysis will not appear in the normal-
ized version of the document and its translations,
which makes normalizing translation performing a
kind of content selection. Another delicate aspect
of this approach is that if the normalization goes
wrong, even though an expert could control and
validate the whole process7, then the multilingual
versions of the resulting document will not be ac-
curate translations of the input document.
Despite the limitations given above, we think
that this approach proposes enough advantages to
be a viable solution for some well-defined contexts.
First and foremost, if normalization goes well, so
will translation, provided the parallel grammars of
the authoring system are correct. The fuzzy in-
verted generation we have proposed has the inher-
ent property of only producing candidates that are
semantically well-formed and coherent, thus pro-
viding a means to the expert to correct or to re-
ject an ill-formed legacy document. Validated doc-
uments are richer than usual textual documents
since they are associated with their semantic de-
scription, which can for example be used to index
the documents in a knowledge base for subsequent
retrieval.
On the architectural side, the same resource is
used for both analysis and generation, thus re-
ducing considerably development time. Moreover,
the fuzzy approach and the non-determinism of
the inverted generation makes it possible to match
a large range of inputs that could be more diffi-
cult to recognize using more traditional approaches
to content analysis, such as syntactic parsing fol-
lowed by semantic composition (Allen, 1995). Pro-
vided the necessary resources are available, notably
a lemmatizer, a lexico-semantic database such as
WordNet, and a human expert fluent in the ap-
propriate language, any grammar of the authoring
system could be used for analysis, therefore pos-
7The normalized document in the original language
can be used by the expert to validate the normalization
process, similarly to feedback texts in WYSIWYM.
</bodyText>
<figure confidence="0.998887941176471">
Legacy
document
(Language 11)
Document
Nomafixation
System
Document
typed abstract
tree
Grammar Grammar
L2 Ln
Parallel HDA
grammars
Normalized
translations of
the legacy
document
</figure>
<page confidence="0.896335">
30
</page>
<bodyText confidence="0.7893964">
Input text: Staying at a camping resort is always My family&apos;s first choice.
Best matching English abstract tree:
giveInfo-disposition-stay:speech-act-e
wouldPrefer-3::disposition-e &apos;to stay at&apos; campsite-2::location-e
&apos;My first choice would be&apos; &apos;a camping resort &apos;
</bodyText>
<subsectionHeader confidence="0.821511">
Corresponding normalized English abstract tree:
</subsectionHeader>
<bodyText confidence="0.993020666666667">
giveInfo-disposition-stay:speech-act-e
wouldPrefer::disposition-e &apos;to stay at &apos; campsite::location-e
&apos;I would prefer&apos; &apos;a camp site &apos;
</bodyText>
<subsectionHeader confidence="0.797635">
Corresponding French abstract tree:
</subsectionHeader>
<bodyText confidence="0.980377666666667">
giveInfo-disposition-stay:speech-act-f
wouldPrefer::disposition-f &apos;sejourner dans &apos; campsite::location-f
&apos;Je prefererais&apos; &apos;tin camping &apos;
</bodyText>
<subsectionHeader confidence="0.832895">
Corresponding Spanish abstract tree:
</subsectionHeader>
<bodyText confidence="0.95677625">
giveInfo-disposition-stay:speech-act-s
wouldPrefer::disposition-s &apos;quedarme en &apos; campsite::location-s
&apos;Preferirca&apos; &apos;un camping&apos;
Input text normalized translations:
</bodyText>
<listItem confidence="0.999923">
• English: I would prefer to stay at a camp site.
• French: Je prefererais se journer dans un camping.
• Spanish: Preferiria guedarine en un camping.
</listItem>
<figureCaption confidence="0.987366">
Figure 6: Example of normalizing translation for the English sentence Staying at a camping resort is
always my family&apos;s first choice in the context of travel information
</figureCaption>
<page confidence="0.999472">
31
</page>
<bodyText confidence="0.999873727272727">
sibly allowing an N-to-N normalizing translation
architecture. Finally, if the system has some su-
pervised learning ability, for example by augment-
ing its generative power with examples validated
by the expert, then it could be expected to per-
form better as more normalizations are done, as is
the case with translation memories.
Acknowledgements The author wishes to
thank Marc Dymetman and Christian Boitet for
their supervision of his PhD work. This work is
funded by a grant from ANRT.
</bodyText>
<sectionHeader confidence="0.999225" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999014500000001">
ABPI. 1996. ABPI Compendium of Patient Infor-
mation Leaflets. Datapharm Publications.
James Allen. 1995. Natural Language Understand-
ing. Benjamin/Cummings Publishing, Redwood
City, 2nd edition.
Kathryn L. Baker, Alexander M. Franz,
Pamela W. Jordan, Teruko Mitamura, and
Eric H. Nyberg. 1994. Coping with Ambiguity
in a Large-Scale Machine Translation System.
In Proceedings of COLING-94, Kyoto, Japan.
Christian Boitet and Herve Blanchon. 1996. Mul-
tilingual Dialogue-Based MT for monolingual
authors: the LIDIA project and a first mockup.
Machine Translation, 9:99-132.
Christian Boitet. 1996. Dialogue-based ma-
chine translation for monolinguals and future
self-explaining documents. In Proceedings of
MIDDIM-96, Le Col de Porte, France.
Caroline Brun, Marc Dymetman, and Veronika
Lux. 2000. Document Structure and Multilin-
gual Authoring. In Proceedings of INLG 2000,
Mitzpe Ramon, Israel.
Ronald A. Cole, Joseph Mariani, Hans Uszkoreit,
Annie Zaenen, and Victor Zile, editors. 1996.
Survey of the State of the Art in Human Lan-
guage Technology. Cambridge University Press.
Marc Dymetman, Veronika Lux, and Aarne Ranta.
2000. XML and Multilingual Document Author-
ing: Convergent Trends. In Proceedings of COL-
ING 2000, Saarbrucken, Germany.
Julio Gonzalo, Felisa Verdejo, Irina Chugur, and
Juan Cigarran. 1998. Indexing with WordNet
Synsets Can Improve Text Retrieval. In Pro-
ceedinys of the COLING/ACL Workshop on the
Usage of WordNet in Natural Lanyuage Process-
ing Systems.
W.J. Hutchins and Harold Somers. 1992. An In-
troduction to Machine Translation. Academic
Press, London.
Aurelien Max and Marc Dymetman. 2002. Docu-
ment Content Analysis through Inverted Gener-
ation. In Proceedings of the workshop on Using
(and Acquiring) Linguistic (and World) Knowl-
edge for Information Access of the AAAI Spring
Symposium Series, Stanford University, USA.
Aurelien Max. 2002. Normalisation de Documents
par Analyse du Contenu a l&apos;Aide d&apos;un Moclele
Semantique et d&apos;un Generateur. In Proceedings
of TALN-RECITAL 2002, Nancy, France.
Nils J. Nilsson. 1998. Artificial Intelligence: a New
Synthesis. Morgan Kaufmann, San Francisco.
OVP Editions du VIDAL, editor. 1998. Le VIDAL
de la famine. Hachette, Paris.
Daniel S. Paiva. 2000. Investing Style in a Cor-
pus of Pharmaceutical Leaflets: Result of a Fac-
tor Analysis. In Proceedings of the ACL Student
Research Workshop, Hong Kong.
Fernando Pereira and David Warren. 1980. Def-
inite Clauses for Language Analysis. Artificial
Intelligence, 13.
Richard Power and Donna Scott. 1998. Mul-
tilingual Authoring using Feedback Texts. In
Proceedings of COLING/ACL-98, Montreal,
Canada.
Ehud Reiter. 1995. NLG Vs Templates. In Pro-
ceedings of ENLGW-95, Leiden, The Nether-
lands.
Harold Somers. 1999. Review Article: Example-
based Machine Translation. Machine Transla-
tion, 14:113-157.
</reference>
<page confidence="0.999289">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.196184">
<title confidence="0.887768333333333">Multi-language Machine Translation through Interactive Document Normalization Aurelien</title>
<author confidence="0.321491">Groupe d&apos;Etude pour la Traduction Automatique</author>
<affiliation confidence="0.746944">Xerox Research Centre Europe</affiliation>
<address confidence="0.779237">Grenoble,</address>
<email confidence="0.996752">aurelien.max@imag.fr</email>
<abstract confidence="0.999512695652174">Document normalization is an interactive process that transforms raw legacy documents into semantically well-formed and linguistically controlled documents with the same communicative intention content. A paradigm for content analysis has been implemented to select candidate semantic representations of the communicative content of an input document. This implementation reuses the formal content specification of a multilingual controlled authoring system. As a consequence, a candidate semantic representation can not only be associated with a text in the language of the input document, but also in all the languages supported by the system. This paper presents how multilingual versions of an input legacy document can be obtained interactively with a proposed implementation, and discusses the advantages and limitations of this kind of normalizing translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ABPI</author>
</authors>
<title>ABPI Compendium of Patient Information Leaflets.</title>
<date>1996</date>
<publisher>Datapharm Publications.</publisher>
<contexts>
<context position="7166" citStr="ABPI, 1996" startWordPosition="1108" endWordPosition="1109">cateargument structure of sentences commonly used in NLG, so this is an intermediate level between full NLG and templates (Reiter, 1995). This approach proved to be adequate for classes of documents where the productivity of certain choices could be rendered as entire text spans, as is the case for example of warning sections in drug leaflets (Brun et al., 2000). 3 Document normalization 3.1 A Motivating example The pharmaceutical domain produces yearly publications which are compendiums of documents initially produced by pharmaceutical companies which are presented in a consistent way (e.g. (ABPI, 1996; OVP Editions du VIDAL, 1998)). Several kinds of variations were observed in a corpus study we conducted on a corpus of 50 patient pharmaceutical leaflets for pain relievers from different drug vendors (Max, 2002). First, the structures of the leaflets could vary considerably, as well as the locations where certain communicative goals were expressed. (Paiva, 2000) showed the presence of significant stylistic variation in a corpus of 342 patient leaflets. Our study also revealed that similar communicative intentions could be expressed in a variety of ways conveying more or less subtle semantic</context>
</contexts>
<marker>ABPI, 1996</marker>
<rawString>ABPI. 1996. ABPI Compendium of Patient Information Leaflets. Datapharm Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allen</author>
</authors>
<title>Natural Language Understanding. Benjamin/Cummings Publishing, Redwood City, 2nd edition.</title>
<date>1995</date>
<contexts>
<context position="20299" citStr="Allen, 1995" startWordPosition="3177" endWordPosition="3178">ual textual documents since they are associated with their semantic description, which can for example be used to index the documents in a knowledge base for subsequent retrieval. On the architectural side, the same resource is used for both analysis and generation, thus reducing considerably development time. Moreover, the fuzzy approach and the non-determinism of the inverted generation makes it possible to match a large range of inputs that could be more difficult to recognize using more traditional approaches to content analysis, such as syntactic parsing followed by semantic composition (Allen, 1995). Provided the necessary resources are available, notably a lemmatizer, a lexico-semantic database such as WordNet, and a human expert fluent in the appropriate language, any grammar of the authoring system could be used for analysis, therefore pos7The normalized document in the original language can be used by the expert to validate the normalization process, similarly to feedback texts in WYSIWYM. Legacy document (Language 11) Document Nomafixation System Document typed abstract tree Grammar Grammar L2 Ln Parallel HDA grammars Normalized translations of the legacy document 30 Input text: Sta</context>
</contexts>
<marker>Allen, 1995</marker>
<rawString>James Allen. 1995. Natural Language Understanding. Benjamin/Cummings Publishing, Redwood City, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathryn L Baker</author>
<author>Alexander M Franz</author>
<author>Pamela W Jordan</author>
<author>Teruko Mitamura</author>
<author>Eric H Nyberg</author>
</authors>
<title>Coping with Ambiguity in a Large-Scale Machine Translation System. In</title>
<date>1994</date>
<booktitle>Proceedings of COLING-94,</booktitle>
<location>Kyoto, Japan.</location>
<contexts>
<context position="1878" citStr="Baker et al., 1994" startWordPosition="285" endWordPosition="288">ole et al., 1996). This task arises so many problems in computational linguistics, most of them only partially solved, that a lot of research is still to be carried out before one can ask a personal computer to translate accurately an arbitrary piece of text from one language to another. The performance bottleneck due to the lack of linguistic and knowledge resources has led the builders of practical translation systems to constrain the input to controlled languages, and/or to have recourse to human expertise on the source language and on the discourse domain (e.g. (Boitet and Blanchon, 1996; Baker et al., 1994)). Unsurprisingly, the most successful systems to date operate on text in very limited domains, exemplified by the weather forecast translation from English to French of the METED system. There exist many situations where documents belonging to a constrained domain have to be translated in several languages, as is the case of official documents in multilingual communities or product descriptions for international companies. In these situations one have at least the following expectations: • high-quality translation, which implies that it be accurate and not necessarily literal • outputs in pos</context>
</contexts>
<marker>Baker, Franz, Jordan, Mitamura, Nyberg, 1994</marker>
<rawString>Kathryn L. Baker, Alexander M. Franz, Pamela W. Jordan, Teruko Mitamura, and Eric H. Nyberg. 1994. Coping with Ambiguity in a Large-Scale Machine Translation System. In Proceedings of COLING-94, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Boitet</author>
<author>Herve Blanchon</author>
</authors>
<title>Multilingual Dialogue-Based MT for monolingual authors: the LIDIA project and a first mockup.</title>
<date>1996</date>
<journal>Machine Translation,</journal>
<pages>9--99</pages>
<contexts>
<context position="1857" citStr="Boitet and Blanchon, 1996" startWordPosition="281" endWordPosition="284">is still far from solved (Cole et al., 1996). This task arises so many problems in computational linguistics, most of them only partially solved, that a lot of research is still to be carried out before one can ask a personal computer to translate accurately an arbitrary piece of text from one language to another. The performance bottleneck due to the lack of linguistic and knowledge resources has led the builders of practical translation systems to constrain the input to controlled languages, and/or to have recourse to human expertise on the source language and on the discourse domain (e.g. (Boitet and Blanchon, 1996; Baker et al., 1994)). Unsurprisingly, the most successful systems to date operate on text in very limited domains, exemplified by the weather forecast translation from English to French of the METED system. There exist many situations where documents belonging to a constrained domain have to be translated in several languages, as is the case of official documents in multilingual communities or product descriptions for international companies. In these situations one have at least the following expectations: • high-quality translation, which implies that it be accurate and not necessarily lit</context>
</contexts>
<marker>Boitet, Blanchon, 1996</marker>
<rawString>Christian Boitet and Herve Blanchon. 1996. Multilingual Dialogue-Based MT for monolingual authors: the LIDIA project and a first mockup. Machine Translation, 9:99-132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Boitet</author>
</authors>
<title>Dialogue-based machine translation for monolinguals and future self-explaining documents.</title>
<date>1996</date>
<booktitle>In Proceedings of MIDDIM-96, Le Col de Porte,</booktitle>
<location>France.</location>
<contexts>
<context position="10882" citStr="Boitet, 1996" startWordPosition="1683" endWordPosition="1684">nt representation, and producing a normalized version from that content representation. This normalized version expresses predefined content, which is conveyed in the input document, in a structurally and linguistically controlled way. Predefined content reveals communicative goals, which should typically be described by an expert of the discourse domain. Control on the production of text from some content representation allows to produce messages that can be seen as some sort of &apos;gold standard&apos; for the communicative goal that are conveyed and that can be augmented to be made self-explaining (Boitet, 1996), and to obtain consistent document structures as well as to impose terminological and stylistic guidelines. 3.2 Fuzzy inverted generation For the purpose of document normalization we would like to match texts that do not carry significant communicative differences in a given class of documents but may be of quite different surface forms. Therefore, we proposed to concentrate more on what counts as a well-formed document semantic representation rather than on surface properties of text, as the space of possible content representations is vastly more restricted than the space of possible texts.</context>
</contexts>
<marker>Boitet, 1996</marker>
<rawString>Christian Boitet. 1996. Dialogue-based machine translation for monolinguals and future self-explaining documents. In Proceedings of MIDDIM-96, Le Col de Porte, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Brun</author>
<author>Marc Dymetman</author>
<author>Veronika Lux</author>
</authors>
<title>Document Structure and Multilingual Authoring.</title>
<date>2000</date>
<booktitle>In Proceedings of INLG</booktitle>
<location>Mitzpe Ramon,</location>
<contexts>
<context position="6920" citStr="Brun et al., 2000" startWordPosition="1072" endWordPosition="1075">-terminals are typed semantic elements whose type appears after the two colons. Dependencies can be enforced through the use of shared variables between semantic elements. The granularity of text fragments in rules is not necessarily a fine-grained predicateargument structure of sentences commonly used in NLG, so this is an intermediate level between full NLG and templates (Reiter, 1995). This approach proved to be adequate for classes of documents where the productivity of certain choices could be rendered as entire text spans, as is the case for example of warning sections in drug leaflets (Brun et al., 2000). 3 Document normalization 3.1 A Motivating example The pharmaceutical domain produces yearly publications which are compendiums of documents initially produced by pharmaceutical companies which are presented in a consistent way (e.g. (ABPI, 1996; OVP Editions du VIDAL, 1998)). Several kinds of variations were observed in a corpus study we conducted on a corpus of 50 patient pharmaceutical leaflets for pain relievers from different drug vendors (Max, 2002). First, the structures of the leaflets could vary considerably, as well as the locations where certain communicative goals were expressed. </context>
</contexts>
<marker>Brun, Dymetman, Lux, 2000</marker>
<rawString>Caroline Brun, Marc Dymetman, and Veronika Lux. 2000. Document Structure and Multilingual Authoring. In Proceedings of INLG 2000, Mitzpe Ramon, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald A Cole</author>
<author>Joseph Mariani</author>
</authors>
<date>1996</date>
<booktitle>Survey of the State of the Art in Human Language Technology.</booktitle>
<editor>Hans Uszkoreit, Annie Zaenen, and Victor Zile, editors.</editor>
<publisher>Cambridge University Press.</publisher>
<marker>Cole, Mariani, 1996</marker>
<rawString>Ronald A. Cole, Joseph Mariani, Hans Uszkoreit, Annie Zaenen, and Victor Zile, editors. 1996. Survey of the State of the Art in Human Language Technology. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Dymetman</author>
<author>Veronika Lux</author>
<author>Aarne Ranta</author>
</authors>
<title>XML and Multilingual Document Authoring: Convergent Trends.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING</booktitle>
<location>Saarbrucken, Germany.</location>
<contexts>
<context position="4643" citStr="Dymetman et al., 2000" startWordPosition="711" endWordPosition="714">y inverted generation, a paradigm we proposed to normalize documents reusing the formalism of the abovementioned authoring system, and describe a document normalization system. We will then attempt to define how normalizing translation can be achieved through document normalization, and we will discuss the advantages and limitations of such an approach. 2 Controlled Document Authoring Controlled Document Authoring is an active field of research comprising approaches such as the What You See Is What You Meant (WYSIWYM) paradigm (Power and Scott, 1998) and Multilingual Document Authoring (MDA) (Dymetman et al., 2000). The systems allow authors to specify document content representations interactively in their own language, and then produce versions in several languages using parallel resources. In MDA, a system developed at XRCE, the author of a document has to select valid semantic choices in active fields interspersed with the evolving text of the document in her language until the document is complete (see figure 1). The system can at any time produce current versions of the documents from the content representation in all the languages it supports. The documents thus obtained are of high-quality, and </context>
</contexts>
<marker>Dymetman, Lux, Ranta, 2000</marker>
<rawString>Marc Dymetman, Veronika Lux, and Aarne Ranta. 2000. XML and Multilingual Document Authoring: Convergent Trends. In Proceedings of COLING 2000, Saarbrucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julio Gonzalo</author>
<author>Felisa Verdejo</author>
<author>Irina Chugur</author>
<author>Juan Cigarran</author>
</authors>
<title>Indexing with WordNet Synsets Can Improve Text Retrieval.</title>
<date>1998</date>
<booktitle>In Proceedinys of the COLING/ACL Workshop on the Usage of WordNet in Natural Lanyuage Processing Systems.</booktitle>
<contexts>
<context position="15318" citStr="Gonzalo et al., 1998" startWordPosition="2403" endWordPosition="2406">on of the grammar is used in conjunction with the profile computed for the input document in a first pass analysis. The aim of this first pass analysis implementing fuzzy inverted generation is to isolate a limited set of candidate content representations. A second pass analysis is then applied on those candidates, which are now actual texts associated 4More details on how fuzzy inverted generation can be implemented in MDA can be found in (Max, 2002). 5Synsets from WordNet, or ideally from a specialized thesaurus, have been prefered to lemma in order to account for lexico-semantic variation (Gonzalo et al., 1998). with their content representation, using more finegrained linguistic analysis, in conjunction with interactive disambiguation when needed.6. 4 Normalizing translation Using the resources of a multilingual authoring system to analyze a legacy document offers a natural possibility: once the semantic content representation is obtained through document normalization, the generative capability of the authoring system can be reused to produce the documents corresponding to that representation in all the supported languages (see figure 5). Normalizing translation uses the same resources for both an</context>
</contexts>
<marker>Gonzalo, Verdejo, Chugur, Cigarran, 1998</marker>
<rawString>Julio Gonzalo, Felisa Verdejo, Irina Chugur, and Juan Cigarran. 1998. Indexing with WordNet Synsets Can Improve Text Retrieval. In Proceedinys of the COLING/ACL Workshop on the Usage of WordNet in Natural Lanyuage Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J Hutchins</author>
<author>Harold Somers</author>
</authors>
<title>An Introduction to Machine Translation.</title>
<date>1992</date>
<publisher>Academic Press,</publisher>
<location>London.</location>
<contexts>
<context position="3135" citStr="Hutchins and Somers, 1992" startWordPosition="483" endWordPosition="486">onsistency across documents of the same class (e.g. drug leaflets, experiment reports), so that concepts are always expressed in the same unambiguous manner and the texts produced can be regarded as gold standards for the meaning they convey Differents methods that do not impose constraints on the input text have been proposed to achieve high-quality translation. Interaction with a user can be used to disambiguate the input text, and could be prefered to to post-editing as this has to be done only once for all languages, thus reducing the time and efforts needed. Interlingual representations (Hutchins and Somers, 1992) are welladapted to support the production of the target text in several languages, and they can also be effectively used to check the semantic coherence and well-formedness of a document. Reusing previous translations, as proposed in the different flavours of Example-based Machine Translation (Somers, 1999), is an interesting alternative to purely rulebased approaches and allows the selection of nonliteral high-quality translation candidates. This paper starts with a short presentation of an authoring system that allows the creation of 25 multingual documents with all the above properties. Do</context>
</contexts>
<marker>Hutchins, Somers, 1992</marker>
<rawString>W.J. Hutchins and Harold Somers. 1992. An Introduction to Machine Translation. Academic Press, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aurelien Max</author>
<author>Marc Dymetman</author>
</authors>
<title>Document Content Analysis through Inverted Generation.</title>
<date>2002</date>
<booktitle>In Proceedings of the workshop on Using (and Acquiring) Linguistic (and World) Knowledge for Information Access of the AAAI Spring Symposium Series,</booktitle>
<location>Stanford University, USA.</location>
<contexts>
<context position="12317" citStr="Max and Dymetman, 2002" startWordPosition="1914" endWordPosition="1917">eed, an MDA system can be used as a formal device for enumerating well-formed document representations in a constrained domain and associating textual representations with them. If we can compute a relevant measure of semantic similarity between the text produced for any document content representation and the text of a legacy document, we could possibly consider the representations with the best similarity score as those best corresponding to the legacy document under analysis. Since this kind of analysis uses predictions made by a natural language generator, we named it inverted generation (Max and Dymetman, 2002) (see fig. Nell-formed content representation Prediction of textual properties Shallow analysis Paw text Figure 3: Deep content analysis through fuzzy inverted generation 3). We also qualified it fuzzy, because as a generator will seriously undergenerate with respect to all the texts that could be normalized to the same communicative intention, the matching procedure has to be performed at a more abstract level than on raw text to evaluate commonality of communicative content. Considering the types of documents that could be analyzed using this paradigm, it seemed relevant to expand the genera</context>
</contexts>
<marker>Max, Dymetman, 2002</marker>
<rawString>Aurelien Max and Marc Dymetman. 2002. Document Content Analysis through Inverted Generation. In Proceedings of the workshop on Using (and Acquiring) Linguistic (and World) Knowledge for Information Access of the AAAI Spring Symposium Series, Stanford University, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aurelien Max</author>
</authors>
<title>Normalisation de Documents par Analyse du Contenu a l&apos;Aide d&apos;un Moclele Semantique et d&apos;un Generateur.</title>
<date>2002</date>
<booktitle>In Proceedings of TALN-RECITAL 2002,</booktitle>
<location>Nancy, France.</location>
<contexts>
<context position="7380" citStr="Max, 2002" startWordPosition="1144" endWordPosition="1145">ctivity of certain choices could be rendered as entire text spans, as is the case for example of warning sections in drug leaflets (Brun et al., 2000). 3 Document normalization 3.1 A Motivating example The pharmaceutical domain produces yearly publications which are compendiums of documents initially produced by pharmaceutical companies which are presented in a consistent way (e.g. (ABPI, 1996; OVP Editions du VIDAL, 1998)). Several kinds of variations were observed in a corpus study we conducted on a corpus of 50 patient pharmaceutical leaflets for pain relievers from different drug vendors (Max, 2002). First, the structures of the leaflets could vary considerably, as well as the locations where certain communicative goals were expressed. (Paiva, 2000) showed the presence of significant stylistic variation in a corpus of 342 patient leaflets. Our study also revealed that similar communicative intentions could be expressed in a variety of ways conveying more or less subtle semantic distinctions. Seeing the content of such documents as goal-driven communication, a given utterance can be seen as an attempt to satisfy some communicative goal on the part of the writer of the document. We argue t</context>
<context position="15152" citStr="Max, 2002" startWordPosition="2379" endWordPosition="2380">f-line to associate profiles with all its semantic types by percolating profiles in the grammar from the terminals up to the root type. This compiled version of the grammar is used in conjunction with the profile computed for the input document in a first pass analysis. The aim of this first pass analysis implementing fuzzy inverted generation is to isolate a limited set of candidate content representations. A second pass analysis is then applied on those candidates, which are now actual texts associated 4More details on how fuzzy inverted generation can be implemented in MDA can be found in (Max, 2002). 5Synsets from WordNet, or ideally from a specialized thesaurus, have been prefered to lemma in order to account for lexico-semantic variation (Gonzalo et al., 1998). with their content representation, using more finegrained linguistic analysis, in conjunction with interactive disambiguation when needed.6. 4 Normalizing translation Using the resources of a multilingual authoring system to analyze a legacy document offers a natural possibility: once the semantic content representation is obtained through document normalization, the generative capability of the authoring system can be reused to</context>
</contexts>
<marker>Max, 2002</marker>
<rawString>Aurelien Max. 2002. Normalisation de Documents par Analyse du Contenu a l&apos;Aide d&apos;un Moclele Semantique et d&apos;un Generateur. In Proceedings of TALN-RECITAL 2002, Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nils J Nilsson</author>
</authors>
<title>Artificial Intelligence: a New Synthesis.</title>
<date>1998</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="13564" citStr="Nilsson, 1998" startWordPosition="2116" endWordPosition="2117"> different texts could be associated with the same content representations to increase the robustness of the analysis. Although this non-determinism proves beneficial for inverted generation, we implemented it in such a way that the generation process would still be done deterministically. To normalize an input document, we would like to find the virtual document3 that is most similar to the input document in terms of the communicative content it conveys. The space of virtual documents for a given class of documents being potentially huge, we proposed an admissible heuristic search procedure (Nilsson, 1998), so that the candidate structures are returned in an order of decreasing similarity with the input text. The evaluation function it uses is an optimistic measure of similarity that corresponds to a weighted intersection between the lexical profile of the input 3We call virtual document a document that can be predicted by the authoring system but does not exist a priori. 28 Figure 4: Architecture of the document normalization system document and that computed for a partial content representation.4 The lexical profile for a text fragment is defined as a vector of informative synsets5 associated</context>
</contexts>
<marker>Nilsson, 1998</marker>
<rawString>Nils J. Nilsson. 1998. Artificial Intelligence: a New Synthesis. Morgan Kaufmann, San Francisco.</rawString>
</citation>
<citation valid="true">
<date>1998</date>
<booktitle>Le VIDAL de la famine. Hachette,</booktitle>
<editor>OVP Editions du VIDAL, editor.</editor>
<location>Paris.</location>
<marker>1998</marker>
<rawString>OVP Editions du VIDAL, editor. 1998. Le VIDAL de la famine. Hachette, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel S Paiva</author>
</authors>
<title>Investing Style in a Corpus of Pharmaceutical Leaflets: Result of a Factor Analysis.</title>
<date>2000</date>
<booktitle>In Proceedings of the ACL Student Research Workshop,</booktitle>
<location>Hong Kong.</location>
<contexts>
<context position="7533" citStr="Paiva, 2000" startWordPosition="1167" endWordPosition="1168"> 3 Document normalization 3.1 A Motivating example The pharmaceutical domain produces yearly publications which are compendiums of documents initially produced by pharmaceutical companies which are presented in a consistent way (e.g. (ABPI, 1996; OVP Editions du VIDAL, 1998)). Several kinds of variations were observed in a corpus study we conducted on a corpus of 50 patient pharmaceutical leaflets for pain relievers from different drug vendors (Max, 2002). First, the structures of the leaflets could vary considerably, as well as the locations where certain communicative goals were expressed. (Paiva, 2000) showed the presence of significant stylistic variation in a corpus of 342 patient leaflets. Our study also revealed that similar communicative intentions could be expressed in a variety of ways conveying more or less subtle semantic distinctions. Seeing the content of such documents as goal-driven communication, a given utterance can be seen as an attempt to satisfy some communicative goal on the part of the writer of the document. We argue that for documents of the importance of pharmaceutical leaflets consistency of expression and of information presentation can be beneficial to the reader </context>
</contexts>
<marker>Paiva, 2000</marker>
<rawString>Daniel S. Paiva. 2000. Investing Style in a Corpus of Pharmaceutical Leaflets: Result of a Factor Analysis. In Proceedings of the ACL Student Research Workshop, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>David Warren</author>
</authors>
<title>Definite Clauses for Language Analysis.</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<volume>13</volume>
<contexts>
<context position="5821" citStr="Pereira and Warren, 1980" startWordPosition="896" endWordPosition="899">cuments thus obtained are of high-quality, and are not necessarily literal translations but rather adaptations to a given language.&apos; In fact, the linguistic structures of two documents can be completely different in two different languages, and communicative intentions can be conveyed in quite different ways. Moreover, since the generator of an MDA system is deterministic, the texts produced will be consistent across documents. The specification of well-formed document content representations in MDA is recursively described in a grammar formalism that is a variant of Definite Clause Grammars (Pereira and Warren, 1980). Text strings can appear in right-hand sides of rules, which allows text realizations to be associated to content representations, and thus provides a close coupling between semantic modelling &apos;Different parts of the document can thus be easily localized: for example, disclaimers and contact information can be adapted to the targeted community. and generation. Figure 2 shows an abstract typed tree in the MDA formalism and its realizations as English and French sentences.&apos; Non-terminals are typed semantic elements whose type appears after the two colons. Dependencies can be enforced through th</context>
</contexts>
<marker>Pereira, Warren, 1980</marker>
<rawString>Fernando Pereira and David Warren. 1980. Definite Clauses for Language Analysis. Artificial Intelligence, 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Power</author>
<author>Donna Scott</author>
</authors>
<title>Multilingual Authoring using Feedback Texts.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL-98,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="4577" citStr="Power and Scott, 1998" startWordPosition="701" endWordPosition="704">ter providing a motivating example, we will briefly introduce fuzzy inverted generation, a paradigm we proposed to normalize documents reusing the formalism of the abovementioned authoring system, and describe a document normalization system. We will then attempt to define how normalizing translation can be achieved through document normalization, and we will discuss the advantages and limitations of such an approach. 2 Controlled Document Authoring Controlled Document Authoring is an active field of research comprising approaches such as the What You See Is What You Meant (WYSIWYM) paradigm (Power and Scott, 1998) and Multilingual Document Authoring (MDA) (Dymetman et al., 2000). The systems allow authors to specify document content representations interactively in their own language, and then produce versions in several languages using parallel resources. In MDA, a system developed at XRCE, the author of a document has to select valid semantic choices in active fields interspersed with the evolving text of the document in her language until the document is complete (see figure 1). The system can at any time produce current versions of the documents from the content representation in all the languages </context>
</contexts>
<marker>Power, Scott, 1998</marker>
<rawString>Richard Power and Donna Scott. 1998. Multilingual Authoring using Feedback Texts. In Proceedings of COLING/ACL-98, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
</authors>
<title>NLG Vs Templates.</title>
<date>1995</date>
<booktitle>In Proceedings of ENLGW-95,</booktitle>
<location>Leiden, The Netherlands.</location>
<contexts>
<context position="6692" citStr="Reiter, 1995" startWordPosition="1034" endWordPosition="1035">or example, disclaimers and contact information can be adapted to the targeted community. and generation. Figure 2 shows an abstract typed tree in the MDA formalism and its realizations as English and French sentences.&apos; Non-terminals are typed semantic elements whose type appears after the two colons. Dependencies can be enforced through the use of shared variables between semantic elements. The granularity of text fragments in rules is not necessarily a fine-grained predicateargument structure of sentences commonly used in NLG, so this is an intermediate level between full NLG and templates (Reiter, 1995). This approach proved to be adequate for classes of documents where the productivity of certain choices could be rendered as entire text spans, as is the case for example of warning sections in drug leaflets (Brun et al., 2000). 3 Document normalization 3.1 A Motivating example The pharmaceutical domain produces yearly publications which are compendiums of documents initially produced by pharmaceutical companies which are presented in a consistent way (e.g. (ABPI, 1996; OVP Editions du VIDAL, 1998)). Several kinds of variations were observed in a corpus study we conducted on a corpus of 50 pa</context>
</contexts>
<marker>Reiter, 1995</marker>
<rawString>Ehud Reiter. 1995. NLG Vs Templates. In Proceedings of ENLGW-95, Leiden, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Somers</author>
</authors>
<title>Review Article: Examplebased Machine Translation.</title>
<date>1999</date>
<journal>Machine Translation,</journal>
<pages>14--113</pages>
<contexts>
<context position="3444" citStr="Somers, 1999" startWordPosition="532" endWordPosition="533">oposed to achieve high-quality translation. Interaction with a user can be used to disambiguate the input text, and could be prefered to to post-editing as this has to be done only once for all languages, thus reducing the time and efforts needed. Interlingual representations (Hutchins and Somers, 1992) are welladapted to support the production of the target text in several languages, and they can also be effectively used to check the semantic coherence and well-formedness of a document. Reusing previous translations, as proposed in the different flavours of Example-based Machine Translation (Somers, 1999), is an interesting alternative to purely rulebased approaches and allows the selection of nonliteral high-quality translation candidates. This paper starts with a short presentation of an authoring system that allows the creation of 25 multingual documents with all the above properties. Document &apos;normalization, which is described next, stemmed from the question of whether such an authoring system could be used in a reversed mode to analyze existing documents from the class of documents supported by it. After providing a motivating example, we will briefly introduce fuzzy inverted generation, </context>
<context position="16346" citStr="Somers, 1999" startWordPosition="2562" endWordPosition="2563">em can be reused to produce the documents corresponding to that representation in all the supported languages (see figure 5). Normalizing translation uses the same resources for both analysis and generation, and shares some properties with a pivot approach. A significant difference with previous approaches to translation using reversible grammars is that fuzzy matching is used. As this approach to machine translation relies on the matching with existing texts (those that can be produced by the generator of the authoring system), it shares some properties with Examplebased Machine Translation (Somers, 1999), with the specificity that matched text fragments correspond first to semantic types in the MDA formalism and then eventually to their appropriate trans6Typically, interactive disambiguation will allow an expert to prefer one of several ambiguous candidates on the basis of the legacy document. 29 Figure 5: Normalizing translation using MDA grammars lations in other languages. Under the assumption that the authoring system produces high-quality documents in all the languages it supports, then evaluating normalizing translation can be limited to evaluating the performance of document normalizat</context>
</contexts>
<marker>Somers, 1999</marker>
<rawString>Harold Somers. 1999. Review Article: Examplebased Machine Translation. Machine Translation, 14:113-157.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>