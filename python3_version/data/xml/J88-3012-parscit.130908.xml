<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.028715">
<sectionHeader confidence="0.711042" genericHeader="abstract">
DISCOURSE MODELS, DIALOG MEMORIES, AND
USER MODELS
</sectionHeader>
<author confidence="0.489466">
Katharina Monk
</author>
<affiliation confidence="0.4242985">
Technical University
Berlin, West Germany
</affiliation>
<sectionHeader confidence="0.98552" genericHeader="introduction">
1 INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999965928571429">
In this paper, we discuss some terminological issues
related to the notions of discourse models, dialog mem-
ories, and user models. It is not our goal to show how
discourse modeling and user modeling should actually
interact in a cooperative system, but to show how the
notions of discourse model, dialog memory, and user
model can be defined and related in order to prevent
misunderstandings and confusion. We argue that dialog
memory may be subsumed under user model, as well as
under discourse model, but that the three concepts
should not be identified. Several separating criteria are
discussed. We conclude that discourse modeling and
user modeling are two lines of research that are orthog-
onal to each other.
</bodyText>
<sectionHeader confidence="0.718995" genericHeader="method">
2 DIALOG MEMORY AS PART OF USER MODEL
</sectionHeader>
<bodyText confidence="0.995194661764706">
A dialog memory can be viewed as part of a user model,
namely the part that represents the dialog-dependent
knowledge of the user (Monk 1984). Entries out of the
dialog memory may cause entries in the user model, and
entries of the user model may support the interpretation
of an utterance, the interpretation then being stored in
the dialog memory. However, in order to keep technical
terms precise, user modeling on the one hand, and
building and exploiting a dialog memory on the other
hand should not be identified. This would lead to a
reduction of what user modeling is about by disregard-
ing all aspects other than dialog-dependent knowledge
of the user as known to the system, while in fact there
is some information that is to be covered by a user
model and that may not be covered by a dialog memory.
Let us think, for example, of a visit to the dentist&apos;s.
The dentist will have some expectations concerning the
client before the client said a word—even before he
opened his mouth. This is due to the conversational
setting, the roles of dentist and client. The same two
persons meeting in another environment (e.g., at a
political event, a horse race, or the opera) would not
rely on the dentist-client expectations but on the infor-
mation that then belongs to their roles.
A user model contains explicit assumptions on the
role of the user and the way a particular user plays it.
The system exploits the user model systematically for
playing its role more cooperatively by adopting to
diverse users. To that end it uses rules which are
parametrized according to the facets of the user. A user
model is built up based on a &amp;quot;naive psychology&amp;quot;, which
forms a consistent image of the user.
Schuster also states that the user model covers
entities that do not belong into the dialog memory. In
addition to the argument mentioned above (the dialog
memory being the part of the user model that represents
the dialog-dependent knowledge of the user), she points
out that the dialog memory is used for building up parts
of the user model and that both, user model and dialog
memory, are used for generating an adequate answer.
But if this were a valid argument for establishing a
subsumption relation, we should also view the grammar
as part of the user model, because the grammar is
necessary for understanding and producing utterances.
All the knowledge sources of a natural language system
(hopefully) work together. We separate them conceptu-
ally not because of their independence, but because
they contain different kinds of knowledge that contrib-
ute to the overall task in different ways.
A dialog memory contains all beliefs that can be
inferred with certainty from utterances, so that they
belong to the mutual belief space. For example, the
objects and their properties introduced in a dialog are
typical entries in a dialog memory. Also, presupposi-
tions that can be inferred from articles or question
particles belong into the dialog memory. The linguistic
rules that determine the inferences are valid and binding
for all conversational settings. General rules establish
mutual beliefs on the basis of utterances. The dialog
memory is then used for, e.g., determining the appro-
Copyright 1988 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided
that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To
copy otherwise, or to republish, requires a fee and/or specific permission.
0362-613X/88 /0100•-•$03.00
Computational Linguistics, Volume 14, Number 3, September 1988 95
Katharine Monk Discourse Models, Dialog Memories, and User Models
priate description (definite/indefinite), anaphoric expres-
sion, or characterization.
</bodyText>
<sectionHeader confidence="0.963625" genericHeader="method">
3 DIALOG MEMORY AS PART OF DISCOURSE MODEL
</sectionHeader>
<bodyText confidence="0.999949608695652">
Another notion that is very close to user model as well
as to dialog memory is discourse model. Sometimes
dialog memory and discourse model are treated as
synonyms (e.g., Wahlster 1986). Given the above defi-
nition of dialog memories, however, there is a differ-
ence between the two notions. As opposed to Schuster,
who defines a discourse model as &amp;quot;containing represen-
tations of entities, along with their properties and rela-
tions they participate in&amp;quot;, which corresponds exactly to
our dialog memory, I use discourse model according to
the framework of Grosz and Sidner (1986), where a
discourse model is the syntactic structure of a dialog.
One part of it, though, could be identified with the
dialog memory, namely the focus space stack. The
overall discourse model additionally represents the
structure of the dialog with the segments and their
relations, which is not part of the user model. Decom-
posing a dialog into segments and establishing relations
between them does not depend on a particular conver-
sational setting. As is the case with dialog memories the
overall discourse model, too, is built up by general
linguistic rules that need not be parametrized according
to a certain user.
</bodyText>
<sectionHeader confidence="0.998179" genericHeader="conclusions">
4 SEPARATING CRITERIA
</sectionHeader>
<bodyText confidence="0.999963192771084">
Previous attempts to separate user models from dis-
course models have used the short-time/long-time crite-
rion, arguing that entries in the dialog memory can be
forgotten after the end of the dialog, whereas entries in
the user model are to be remembered. The same argu-
ment applies to dialog memories as part of the discourse
model. The rationale of this argument is that anaphors
are not applicable from one dialog to another and that
the structure of a dialog is unlikely to be recalled as the
syntactic structure of uttered sentences—just to men-
tion these two phenomena.
But does that mean that the entities with all their
properties and relations as communicated in the dialog
are forgotten? What would be the reason to talk to each
other then? How could we learn from each other?
Knowledge is to a great extent transferred via dialogs.
Second, how could speech acts have social obligations
as a consequence that may well hold for a long time?
(Think of promises, for example!) Although the speaker
may have forgotten the dialog, the hearer has—by very
general conventions of language use—the right to insist
on the speaker&apos;s commitments (Lewis 1975, Searle
1969, Wunderlich 1972).
The synthesis of those seemingly conflicting obser-
vations is that the content of the dialog memory is
integrated into the world knowledge. In other words,
the content of the focus space stack is partially incor-
porated into the world knowledge when it gets popped
off the stack. So, the structure is lost, but the content is
at least partly saved.
Turning things the other way around, why couldn&apos;t
properties or character traits of a user be forgotten?
What makes entries of a user model more stable? Think,
for instance, of a post office clerk. Although he may
adapt his behavior to the particular customer during the
dialog, he normally forgets the information about her or
him immediately after the dialog. As Rich (1979) pointed
out, user models may be short term or long term. Thus
short-time/long-time or forgettable/unforgettable is no
criterion for dividing user models from dialog memories
or discourse models.
Another criterion could be, whether the knowledge is
used for generating the linguistic form (how to say
something) or for establishing the content of a system&apos;s
utterance (what to say). Clearly, dialog memory and
overall discourse model deal with the linguistic struc-
ture of dialogs, e.g., the reference resolution and the
appropriate verbalization of concepts. The user model,
on the other hand, also covers information that directs
the selection of what to utter. The user&apos;s level of
expertise determines the particularity of a system utter-
ance, the realization of notes of caution, and the word
choice, for instance. The user&apos;s wants establish the
context of the user utterances and guide the system&apos;s
problem solving, thus keeping the system behavior
directed towards the user goals.
This distinction, however, is not clear cut either, for
two reasons. First, the line between what to say and
how to say it is rather fuzzy. Referencing a concept, for
example, also involves choosing the appropriate at-
tributes for characterization—and this is naturally a
matter of what to say. Second, this criterion would
exclude work as presented by Lehman and Carbonell
(1988) from the area of user modeling. There, linguistic
rules are specialized for a particular user in a particular
conversational setting. This is clearly not a matter of the
dialog memory, but of the user model, although it is
concerned with the linguistic form. Thus the form/
content distinction does not separate user models from
dialog memories and discourse models, either.
The difficulty to find criteria separating between
discourse models and user models indicates a case of
cross-classification. The criteria, namely what is spe-
cific to a user and what concerns dialog structure
naturally cross. Dialog memory falls into both catego-
ries.
On the one hand, from what the user utters his
beliefs, his level of expertise in a certain domain, his
wants, and his language style can be inferred. This
knowledge can be used by all the system components:
tuning syntactic analysis, resolving reference, determin-
ing the input speech act, disambiguating the input,
selecting relevant information, organizing the text to be
</bodyText>
<page confidence="0.828087">
96 Computational Linguistics, Volume 14, Number 3, September 1988
</page>
<note confidence="0.403414">
Katharine Monk Discourse Models, Dialog Memories, and User Models
</note>
<bodyText confidence="0.99985348">
outputted (Paris 1988), choosing the appropriate words,
referencing, topicalizing, etc. In order to do so, all the
components must include procedures that are parame-
trized according to the (particular) user model. Cur-
rently, the interactive systems do not put all the user
facets to good use in all their components. This is not
due to principled limits, however, but rather to a
shortcoming in the state of the art.
On the other hand, the user&apos;s utterances can also be
analyzed from another viewpoint, namely incorporating
them into a coherent discourse model as described by,
e.g., Grosz and Sidner (1986). Also, this model can be
used during all processing steps from understanding to
generating.
Both user models and discourse models are built up
(at least partially) from the user utterances. Both con-
tribute to a cooperative system behavior. But they do so
from different viewpoints with different aims. Adopting
to a particular user on the one hand and achieving a
coherent, well-formed dialog on the other hand are two
aims for a cooperative system which are orthogonal to
each other. The terms user model and discourse model
denote different aspects of a system. Thus although the
notions are intensionally different, the extension of their
respective definitions may overlap.
</bodyText>
<sectionHeader confidence="0.994497" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.989172827586207">
Grosz, B. and Sidner, C. 1986 Attention, Intentions, and the Structure
of Discourse. In Computational Linguistics 12: 175-204.
Lehman, J. F. and Carbonell, J. G. 1988 Learning the User&apos;s Lan-
guage: A Step Towards Automated Creation of User Models. In
Kobsa, A. and Wahlster, W. (eds.), User Models in Dialog
Systems. Springer-Verlag, Berlin—New York.
Lewis, D. 1975 Languages and Language. In Gunderson, K. (ed.),
Language, Mind and Knowledge, Minnesota Studies in the Phi-
losophy of Science 7. University of Minnesota Press, Minneapolis,
MN.
Monk, K. 1984 Partnermodellierung und Interessenprofile bei Dia-
logsystemen der Kiinstlichen Intelligenz. In Rollinger, C. R. (ed.),
Probleme des (Text-) Verstehens: Ansatze der Kanstlichen Intel-
ligenz. Niemeyer, Tubingen, W. Germany.
Paris, C. L. 1988 Tailoring Object Descriptions to a User&apos;s Level of
Expertise. In Kobsa, A. and Wahlster, W. (eds.), User Models in
Dialog Systems. Springer-Verlag, Berlin—New York.
Rich, E. 1979 Building and Exploiting User Models. Ph.D. thesis,
Department of Computer Science, Carnegie-Mellon University,
Pittsburgh, PA.
Searle, J. R. 1969 Speech Acts. Cambridge University Press, Cam-
bridge, England.
Wahlster, W. 1986 Some Terminological Remarks on User Modeling.
Paper presented at the International Workshop on User Modeling,
Maria Laach, W. Germany.
Wunderlich, D. 1972 Sprechakte. In Maas, U. and Wunderlich, D.
(eds.), Pragmatik und sprachliches Handeln, Athenaum Verlag,
Frankfurt, W. Germany.
Computational Linguistics, Volume 14, Number 3, September 1988 97
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.469690">
<affiliation confidence="0.564803333333333">DISCOURSE MODELS, DIALOG MEMORIES, USER MODELS Technical</affiliation>
<address confidence="0.91502">Berlin, West Germany</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Grosz</author>
<author>C Sidner</author>
</authors>
<title>Attention, Intentions, and the Structure of Discourse.</title>
<date>1986</date>
<journal>In Computational Linguistics</journal>
<volume>12</volume>
<pages>175--204</pages>
<contexts>
<context position="5291" citStr="Grosz and Sidner (1986)" startWordPosition="866" endWordPosition="869">ion. 3 DIALOG MEMORY AS PART OF DISCOURSE MODEL Another notion that is very close to user model as well as to dialog memory is discourse model. Sometimes dialog memory and discourse model are treated as synonyms (e.g., Wahlster 1986). Given the above definition of dialog memories, however, there is a difference between the two notions. As opposed to Schuster, who defines a discourse model as &amp;quot;containing representations of entities, along with their properties and relations they participate in&amp;quot;, which corresponds exactly to our dialog memory, I use discourse model according to the framework of Grosz and Sidner (1986), where a discourse model is the syntactic structure of a dialog. One part of it, though, could be identified with the dialog memory, namely the focus space stack. The overall discourse model additionally represents the structure of the dialog with the segments and their relations, which is not part of the user model. Decomposing a dialog into segments and establishing relations between them does not depend on a particular conversational setting. As is the case with dialog memories the overall discourse model, too, is built up by general linguistic rules that need not be parametrized according</context>
<context position="10961" citStr="Grosz and Sidner (1986)" startWordPosition="1784" endWordPosition="1787">and User Models outputted (Paris 1988), choosing the appropriate words, referencing, topicalizing, etc. In order to do so, all the components must include procedures that are parametrized according to the (particular) user model. Currently, the interactive systems do not put all the user facets to good use in all their components. This is not due to principled limits, however, but rather to a shortcoming in the state of the art. On the other hand, the user&apos;s utterances can also be analyzed from another viewpoint, namely incorporating them into a coherent discourse model as described by, e.g., Grosz and Sidner (1986). Also, this model can be used during all processing steps from understanding to generating. Both user models and discourse models are built up (at least partially) from the user utterances. Both contribute to a cooperative system behavior. But they do so from different viewpoints with different aims. Adopting to a particular user on the one hand and achieving a coherent, well-formed dialog on the other hand are two aims for a cooperative system which are orthogonal to each other. The terms user model and discourse model denote different aspects of a system. Thus although the notions are inten</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Grosz, B. and Sidner, C. 1986 Attention, Intentions, and the Structure of Discourse. In Computational Linguistics 12: 175-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Lehman</author>
<author>J G Carbonell</author>
</authors>
<title>Learning the User&apos;s Language: A Step Towards Automated Creation of User Models.</title>
<date>1988</date>
<editor>In Kobsa, A. and Wahlster, W. (eds.),</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin—New York.</location>
<contexts>
<context position="9204" citStr="Lehman and Carbonell (1988)" startWordPosition="1506" endWordPosition="1509">erance, the realization of notes of caution, and the word choice, for instance. The user&apos;s wants establish the context of the user utterances and guide the system&apos;s problem solving, thus keeping the system behavior directed towards the user goals. This distinction, however, is not clear cut either, for two reasons. First, the line between what to say and how to say it is rather fuzzy. Referencing a concept, for example, also involves choosing the appropriate attributes for characterization—and this is naturally a matter of what to say. Second, this criterion would exclude work as presented by Lehman and Carbonell (1988) from the area of user modeling. There, linguistic rules are specialized for a particular user in a particular conversational setting. This is clearly not a matter of the dialog memory, but of the user model, although it is concerned with the linguistic form. Thus the form/ content distinction does not separate user models from dialog memories and discourse models, either. The difficulty to find criteria separating between discourse models and user models indicates a case of cross-classification. The criteria, namely what is specific to a user and what concerns dialog structure naturally cross</context>
</contexts>
<marker>Lehman, Carbonell, 1988</marker>
<rawString>Lehman, J. F. and Carbonell, J. G. 1988 Learning the User&apos;s Language: A Step Towards Automated Creation of User Models. In Kobsa, A. and Wahlster, W. (eds.), User Models in Dialog Systems. Springer-Verlag, Berlin—New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
</authors>
<title>Languages and Language.</title>
<date>1975</date>
<booktitle>Language, Mind and Knowledge, Minnesota Studies in the Philosophy of Science 7.</booktitle>
<editor>In Gunderson, K. (ed.),</editor>
<publisher>University of Minnesota Press,</publisher>
<location>Minneapolis, MN.</location>
<contexts>
<context position="7084" citStr="Lewis 1975" startWordPosition="1167" endWordPosition="1168">o mention these two phenomena. But does that mean that the entities with all their properties and relations as communicated in the dialog are forgotten? What would be the reason to talk to each other then? How could we learn from each other? Knowledge is to a great extent transferred via dialogs. Second, how could speech acts have social obligations as a consequence that may well hold for a long time? (Think of promises, for example!) Although the speaker may have forgotten the dialog, the hearer has—by very general conventions of language use—the right to insist on the speaker&apos;s commitments (Lewis 1975, Searle 1969, Wunderlich 1972). The synthesis of those seemingly conflicting observations is that the content of the dialog memory is integrated into the world knowledge. In other words, the content of the focus space stack is partially incorporated into the world knowledge when it gets popped off the stack. So, the structure is lost, but the content is at least partly saved. Turning things the other way around, why couldn&apos;t properties or character traits of a user be forgotten? What makes entries of a user model more stable? Think, for instance, of a post office clerk. Although he may adapt </context>
</contexts>
<marker>Lewis, 1975</marker>
<rawString>Lewis, D. 1975 Languages and Language. In Gunderson, K. (ed.), Language, Mind and Knowledge, Minnesota Studies in the Philosophy of Science 7. University of Minnesota Press, Minneapolis, MN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Monk</author>
</authors>
<title>Partnermodellierung und Interessenprofile bei Dialogsystemen der Kiinstlichen Intelligenz.</title>
<date>1984</date>
<booktitle>Probleme des (Text-) Verstehens: Ansatze der Kanstlichen Intelligenz.</booktitle>
<editor>In Rollinger, C. R. (ed.),</editor>
<location>Niemeyer, Tubingen, W. Germany.</location>
<contexts>
<context position="1017" citStr="Monk 1984" startWordPosition="165" endWordPosition="166">ourse model, dialog memory, and user model can be defined and related in order to prevent misunderstandings and confusion. We argue that dialog memory may be subsumed under user model, as well as under discourse model, but that the three concepts should not be identified. Several separating criteria are discussed. We conclude that discourse modeling and user modeling are two lines of research that are orthogonal to each other. 2 DIALOG MEMORY AS PART OF USER MODEL A dialog memory can be viewed as part of a user model, namely the part that represents the dialog-dependent knowledge of the user (Monk 1984). Entries out of the dialog memory may cause entries in the user model, and entries of the user model may support the interpretation of an utterance, the interpretation then being stored in the dialog memory. However, in order to keep technical terms precise, user modeling on the one hand, and building and exploiting a dialog memory on the other hand should not be identified. This would lead to a reduction of what user modeling is about by disregarding all aspects other than dialog-dependent knowledge of the user as known to the system, while in fact there is some information that is to be cov</context>
</contexts>
<marker>Monk, 1984</marker>
<rawString>Monk, K. 1984 Partnermodellierung und Interessenprofile bei Dialogsystemen der Kiinstlichen Intelligenz. In Rollinger, C. R. (ed.), Probleme des (Text-) Verstehens: Ansatze der Kanstlichen Intelligenz. Niemeyer, Tubingen, W. Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Paris</author>
</authors>
<title>Tailoring Object Descriptions to a User&apos;s Level of Expertise.</title>
<date>1988</date>
<booktitle>User Models in Dialog Systems.</booktitle>
<editor>In Kobsa, A. and Wahlster, W. (eds.),</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin—New York.</location>
<contexts>
<context position="10376" citStr="Paris 1988" startWordPosition="1690" endWordPosition="1691">ncerns dialog structure naturally cross. Dialog memory falls into both categories. On the one hand, from what the user utters his beliefs, his level of expertise in a certain domain, his wants, and his language style can be inferred. This knowledge can be used by all the system components: tuning syntactic analysis, resolving reference, determining the input speech act, disambiguating the input, selecting relevant information, organizing the text to be 96 Computational Linguistics, Volume 14, Number 3, September 1988 Katharine Monk Discourse Models, Dialog Memories, and User Models outputted (Paris 1988), choosing the appropriate words, referencing, topicalizing, etc. In order to do so, all the components must include procedures that are parametrized according to the (particular) user model. Currently, the interactive systems do not put all the user facets to good use in all their components. This is not due to principled limits, however, but rather to a shortcoming in the state of the art. On the other hand, the user&apos;s utterances can also be analyzed from another viewpoint, namely incorporating them into a coherent discourse model as described by, e.g., Grosz and Sidner (1986). Also, this mo</context>
</contexts>
<marker>Paris, 1988</marker>
<rawString>Paris, C. L. 1988 Tailoring Object Descriptions to a User&apos;s Level of Expertise. In Kobsa, A. and Wahlster, W. (eds.), User Models in Dialog Systems. Springer-Verlag, Berlin—New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Rich</author>
</authors>
<title>Building and Exploiting User Models.</title>
<date>1979</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, Carnegie-Mellon University,</institution>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="7840" citStr="Rich (1979)" startWordPosition="1294" endWordPosition="1295"> into the world knowledge. In other words, the content of the focus space stack is partially incorporated into the world knowledge when it gets popped off the stack. So, the structure is lost, but the content is at least partly saved. Turning things the other way around, why couldn&apos;t properties or character traits of a user be forgotten? What makes entries of a user model more stable? Think, for instance, of a post office clerk. Although he may adapt his behavior to the particular customer during the dialog, he normally forgets the information about her or him immediately after the dialog. As Rich (1979) pointed out, user models may be short term or long term. Thus short-time/long-time or forgettable/unforgettable is no criterion for dividing user models from dialog memories or discourse models. Another criterion could be, whether the knowledge is used for generating the linguistic form (how to say something) or for establishing the content of a system&apos;s utterance (what to say). Clearly, dialog memory and overall discourse model deal with the linguistic structure of dialogs, e.g., the reference resolution and the appropriate verbalization of concepts. The user model, on the other hand, also c</context>
</contexts>
<marker>Rich, 1979</marker>
<rawString>Rich, E. 1979 Building and Exploiting User Models. Ph.D. thesis, Department of Computer Science, Carnegie-Mellon University, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Searle</author>
</authors>
<title>Speech Acts.</title>
<date>1969</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England.</location>
<contexts>
<context position="7097" citStr="Searle 1969" startWordPosition="1169" endWordPosition="1170">ese two phenomena. But does that mean that the entities with all their properties and relations as communicated in the dialog are forgotten? What would be the reason to talk to each other then? How could we learn from each other? Knowledge is to a great extent transferred via dialogs. Second, how could speech acts have social obligations as a consequence that may well hold for a long time? (Think of promises, for example!) Although the speaker may have forgotten the dialog, the hearer has—by very general conventions of language use—the right to insist on the speaker&apos;s commitments (Lewis 1975, Searle 1969, Wunderlich 1972). The synthesis of those seemingly conflicting observations is that the content of the dialog memory is integrated into the world knowledge. In other words, the content of the focus space stack is partially incorporated into the world knowledge when it gets popped off the stack. So, the structure is lost, but the content is at least partly saved. Turning things the other way around, why couldn&apos;t properties or character traits of a user be forgotten? What makes entries of a user model more stable? Think, for instance, of a post office clerk. Although he may adapt his behavior </context>
</contexts>
<marker>Searle, 1969</marker>
<rawString>Searle, J. R. 1969 Speech Acts. Cambridge University Press, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wahlster</author>
</authors>
<title>Some Terminological Remarks on User Modeling.</title>
<date>1986</date>
<booktitle>Paper presented at the International Workshop on User Modeling,</booktitle>
<location>Maria Laach, W.</location>
<contexts>
<context position="4901" citStr="Wahlster 1986" startWordPosition="805" endWordPosition="806">nce and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/88 /0100•-•$03.00 Computational Linguistics, Volume 14, Number 3, September 1988 95 Katharine Monk Discourse Models, Dialog Memories, and User Models priate description (definite/indefinite), anaphoric expression, or characterization. 3 DIALOG MEMORY AS PART OF DISCOURSE MODEL Another notion that is very close to user model as well as to dialog memory is discourse model. Sometimes dialog memory and discourse model are treated as synonyms (e.g., Wahlster 1986). Given the above definition of dialog memories, however, there is a difference between the two notions. As opposed to Schuster, who defines a discourse model as &amp;quot;containing representations of entities, along with their properties and relations they participate in&amp;quot;, which corresponds exactly to our dialog memory, I use discourse model according to the framework of Grosz and Sidner (1986), where a discourse model is the syntactic structure of a dialog. One part of it, though, could be identified with the dialog memory, namely the focus space stack. The overall discourse model additionally repre</context>
</contexts>
<marker>Wahlster, 1986</marker>
<rawString>Wahlster, W. 1986 Some Terminological Remarks on User Modeling. Paper presented at the International Workshop on User Modeling, Maria Laach, W. Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wunderlich</author>
</authors>
<date>1972</date>
<booktitle>Pragmatik und sprachliches Handeln, Athenaum Verlag,</booktitle>
<editor>Sprechakte. In Maas, U. and Wunderlich, D. (eds.),</editor>
<location>Frankfurt, W. Germany.</location>
<contexts>
<context position="7115" citStr="Wunderlich 1972" startWordPosition="1171" endWordPosition="1172">mena. But does that mean that the entities with all their properties and relations as communicated in the dialog are forgotten? What would be the reason to talk to each other then? How could we learn from each other? Knowledge is to a great extent transferred via dialogs. Second, how could speech acts have social obligations as a consequence that may well hold for a long time? (Think of promises, for example!) Although the speaker may have forgotten the dialog, the hearer has—by very general conventions of language use—the right to insist on the speaker&apos;s commitments (Lewis 1975, Searle 1969, Wunderlich 1972). The synthesis of those seemingly conflicting observations is that the content of the dialog memory is integrated into the world knowledge. In other words, the content of the focus space stack is partially incorporated into the world knowledge when it gets popped off the stack. So, the structure is lost, but the content is at least partly saved. Turning things the other way around, why couldn&apos;t properties or character traits of a user be forgotten? What makes entries of a user model more stable? Think, for instance, of a post office clerk. Although he may adapt his behavior to the particular </context>
</contexts>
<marker>Wunderlich, 1972</marker>
<rawString>Wunderlich, D. 1972 Sprechakte. In Maas, U. and Wunderlich, D. (eds.), Pragmatik und sprachliches Handeln, Athenaum Verlag, Frankfurt, W. Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Computational Linguistics</author>
</authors>
<date>1988</date>
<volume>14</volume>
<pages>97</pages>
<marker>Linguistics, 1988</marker>
<rawString>Computational Linguistics, Volume 14, Number 3, September 1988 97</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>