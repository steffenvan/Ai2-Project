<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001136">
<title confidence="0.92939">
Making Sense of Word Sense Variation
</title>
<author confidence="0.894828">
Rebecca J. Passonneau and Ansaf Salleb-Aouissi
</author>
<affiliation confidence="0.9859405">
Center for Computational Learning Systems
Columbia University
</affiliation>
<address confidence="0.808014">
New York, NY, USA
</address>
<email confidence="0.943885">
(becky@cs|ansaf@ccls).columbia.edu
</email>
<author confidence="0.995326">
Nancy Ide
</author>
<affiliation confidence="0.865169">
Department of Computer Science
Vassar College
</affiliation>
<address confidence="0.656563">
Poughkeepsie, NY, USA
</address>
<email confidence="0.99902">
ide@cs.vassar.edu
</email>
<sectionHeader confidence="0.995644" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999990333333333">
We present a pilot study of word-sense an-
notation using multiple annotators, relatively
polysemous words, and a heterogenous cor-
pus. Annotators selected senses for words in
context, using an annotation interface that pre-
sented WordNet senses. Interannotator agree-
ment (IA) results show that annotators agree
well or not, depending primarily on the indi-
vidual words and their general usage proper-
ties. Our focus is on identifying systematic
differences across words and annotators that
can account for IA variation. We identify three
lexical use factors: semantic specificity of the
context, sense concreteness, and similarity of
senses. We discuss systematic differences in
sense selection across annotators, and present
the use of association rules to mine the data
for systematic differences across annotators.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999904142857143">
Our goal is to grapple seriously with the natural
sense variation arising from individual differences in
word usage. It has been widely observed that usage
features such as vocabulary and syntax vary across
corpora of different genres and registers (Biber,
1995), and that serve different functions (Kittredge
et al., 1991). Still, we are far from able to pre-
dict specific morphosyntactic and lexical variations
across corpora (Kilgarriff, 2001), much less quan-
tify them in a way that makes it possible to apply
the same analysis tools (taggers, parsers) without re-
training. In comparison to morphosyntactic proper-
ties of language, word and phrasal meaning is fluid,
and to some degree, generative (Pustejovsky, 1991;
Nunberg, 1979). Based on our initial observations
from a word sense annotation task for relatively pol-
ysemous words, carried out by multiple annotators
on a heterogeneous corpus, we hypothesize that dif-
ferent words lead to greater or lesser interannota-
tor agreement (IA) for reasons that in the long run
should be explicitly modelled in order for Natural
Language Processing (NLP) applications to handle
usage differences more robustly. This pilot study is
a step in that direction.
We present related work in the next section, then
describe the annotation task in the following one. In
Section 4, we present examples of variation in agree-
ment on a matched subset of words. In Section 5
we discuss why we believe the observed variation
depends on the words and present three lexical use
factors we hypothesize to lead to greater or lesser
IA. In Section 6, we use association rules to mine
our data for systematic differences among annota-
tors, thus to explain the variations in IA. We con-
clude with a summary of our findings goals.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.970038909090909">
There has been a decade-long community-wide ef-
fort to evaluate word sense disambiguation (WSD)
systems across languages in the four Senseval ef-
forts (1998, 2001, 2004, and 2007, cf. (Kilgarriff,
1998; Pedersen, 2002a; Pedersen, 2002b; Palmer
et al., 2005)), with a corollary effort to investi-
gate the issues pertaining to preparation of man-
ually annotated gold standard corpora tagged for
word senses (Palmer et al., 2005). Differences in IA
and system performance across part-of-speech have
been examined, as in (Ng et al., 1999; Palmer et al.,
</bodyText>
<page confidence="0.968771">
2
</page>
<note confidence="0.994458">
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 2–9,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<table confidence="0.998344818181818">
Word POS No. senses No. occurrences
fair Adj 10 463
long Adj 9 2706
quiet Adj 6 244
land Noun 11 1288
time Noun 10 21790
work Noun 7 5780
know Verb 11 10334
say Verb 11 20372
show Verb 12 11877
tell Verb 8 4799
</table>
<tableCaption confidence="0.996736">
Table 1: Ten Words
</tableCaption>
<bodyText confidence="0.978294291666666">
2005). Pedersen (Pedersen, 2002a) examines varia-
tion across individual words in evaluating WSD sys-
tems, but does not attempt to explain it.
Factors that have been proposed as affecting
human or system sense disambiguation include
whether annotators are allowed to assign multilabels
(Veronis, 1998; Ide et al., 2002; Passonneau et al.,
2006), the number or granularity of senses (Ng et al.,
1999), merging of related senses (Snow et al., 2007),
sense similarity (Chugur et al., 2002), sense perplex-
ity (Diab, 2004), entropy (Diab, 2004; Palmer et
al., 2005), and in psycholinguistic experiments, re-
actions times required to distinguish senses (Klein
and Murphy, 2002; Ide and Wilks, 2006).
With respect to using multiple annotators, Snow
et al. included disambiguation of the word
president–a relatively non-polysemous word with
three senses–in a set of tasks given to Amazon Me-
chanical Turkers, aimed at determining how to com-
bine data from multiple non-experts for machine
learning tasks. The word sense task comprised 177
sentences taken from the SemEval Word Sense Dis-
ambiguation Lexical Sample task. Majority voting
among three annotators achieve 99% accuracy.
</bodyText>
<sectionHeader confidence="0.99069" genericHeader="method">
3 The Annotation Task
</sectionHeader>
<bodyText confidence="0.976914428571429">
The Manually Annotated Sub-Corpus (MASC)
project is creating a small, representative corpus
of American English written and spoken texts
drawn from the Open American National Cor-
pus (OANC).1 The MASC corpus includes hand-
validated or manually produced annotations for a va-
riety of linguistic phenomena. One of the goals of
</bodyText>
<footnote confidence="0.996238">
1http://www.anc.org
</footnote>
<figureCaption confidence="0.99918">
Figure 1: MASC word sense annotation tool
</figureCaption>
<bodyText confidence="0.999865318181818">
the project is to support efforts to harmonize Word-
Net (Miller et al., 1993) and FrameNet (Ruppen-
hofer et al., 2006), in order to bring the sense distinc-
tions each makes into better alignment. As a start-
ing sample, we chose ten fairly frequent, moderately
polysemous words for sense tagging, targeting in
particular words that do not yet exist in FrameNet, as
well as words with different numbers of senses in the
two resources. The ten words with part of speech,
number of senses, and occurrences in the OANC
are shown in Table 1. One thousand occurrences
of each word , including all occurrences appear-
ing in the MASC subset and others semi-randomly2
chosen from the remainder of the 15 million word
OANC, were annotated by at least one annotator of
six undergraduate annotators at Vassar College and
Columbia University.
Fifty occurrences of each word in context were
sense-tagged by all six annotators for the in-depth
study of inter-annotator agreement (IA) reported
here. We have just finished collecting annotations
of fifty new occurrences. All annotations are pro-
</bodyText>
<footnote confidence="0.994144">
2The occurrences were drawn equally from each of the
genre-specific portions of the OANC.
</footnote>
<page confidence="0.998315">
3
</page>
<bodyText confidence="0.999438">
duced using the custom-built interface to WordNet
shown in Figure 1: the sentence context is at the top
with the word in boldface (fair), a comment region
below that allows the annotator to keep notes, and
a scrollable area below that shows three of the ten
WordNet senses for “fair.”
</bodyText>
<sectionHeader confidence="0.726869" genericHeader="method">
4 Observation: Varying Agreement,
depending on Lexical Items
</sectionHeader>
<bodyText confidence="0.999158288888889">
We expected to find varying levels of interannotator
agreement (IA) among all six annotators, depend-
ing on obvious grouping factors such as the part of
speech, or the number of senses per word. We do
find widely varying levels of agreement, but as de-
scribed here, most of the variation does not depend
on these a priori factors. Inherent usage properties
of the words themselves, and systematic patterns of
variation across annotators, seem to be the primary
factors, with a secondary effect of part of speech.
In previous work (Passonneau, 2004), we have
discussed why we use Krippendorff’s α (Krippen-
dorff, 1980), and for purposes of comparison we
also report Cohen’s κ; note the similarity in values3.
As with the various agreement coefficients that fac-
tor out the agreement that would occur by chance,
values range from 1 for perfect agreement and -1
for perfect opposition, to 0 for chance agreement.
While there are no hard and fast criteria for what
constitutes good IA, Landis and Koch (Landis and
Koch, 1977) consider values between 0.40 and 0.60
to represent moderately good agreement, and values
above 0.60 as quite good; Krippendorff (Krippen-
dorff, 1980) considers values above 0.67 moderately
good, and values above 0.80 as quite good. (cf. (Art-
stein and Poesio, 2008) for discussion of agreement
measurement for computational linguistic tasks.)
Table 2 shows IA for a pair of adjectives, nouns
and verbs from our sample for which the IA scores
are at the extremes (high and low) in each pair: the
average delta is 0.24. Note that the agreement de-
creases as part-of-speech varies from adjectives to
nouns to verbs, but for all three parts-of-speech,
there is a wide spread of values. It is striking, given
that the same annotators did all words, that one in
each pair has relatively better agreement.
3α handles multiple annotators; Arstein and Poesio (Artstein
and Poesio, 2008) propose an extension of κ (κ3) we use here.
POS Word α κ No. senses Used
adj long 0.6664 0.6665 9 8
fair 0.3546 0.3593 10 5
noun work 0.5359 0.5358 7 7
land 0.2627 0.2671 11 8
verb tell 0.4152 0.4165 8 8
show 0.2636 0.2696 12 11
</bodyText>
<tableCaption confidence="0.98678">
Table 2: Varying interannotator agreement across words
</tableCaption>
<bodyText confidence="0.982553625">
The average of the agreement values shown in
Table 2 (α=0.4164; κ=0.4191) is somewhat higher
than the average 0.317 found for 191 words anno-
tated for WordNet senses in (Ng et al., 1999), but
lower than their recomputed κ of 0.85 for verbs, af-
ter they reanalyzed the data to merge senses for 42
of the verbs. It is widely recognized that achieving
high κ scores (or percent agreement between anno-
tators, cf. (Palmer et al., 2005)) is difficult for word
sense annotation.
Given that the same annotators have higher IA on
some words, and lower on others, we hypothesize
that it is the word usages themselves that lead to the
high deltas in IA for each part-of-speech pair. We
discuss the impact of three factors on the observed
variations in agreement:
</bodyText>
<listItem confidence="0.99749825">
1. Greater specificity in the contexts of use leads to
higher agreement
2. More concrete senses give rise to higher agreement
3. A sense inventory with closely related senses
</listItem>
<bodyText confidence="0.8053">
(e.g., relatively lower average inter-sense similarity
scores) gives rise to lower agreement
</bodyText>
<sectionHeader confidence="0.991376" genericHeader="method">
5 Explanatory Factors
</sectionHeader>
<bodyText confidence="0.999949363636364">
First we list factors that can not explain the variation
in Table 2. Then we turn to examples illustrating
factors that can, based on a manual search for exam-
ples of two types: examples where most annotators
agreed on a single sense, and examples where two
or three senses were agreed upon by multiple anno-
tators. Later we how how we use association rules
to detect these two types of cases automatically. For
these examples, the WordNet sense number is shown
(e.g., WN S1) with an abbreviated gloss, followed
by the number of annotators who chose it.
</bodyText>
<page confidence="0.976785">
4
</page>
<subsectionHeader confidence="0.976598">
5.1 Ruled Out Factors
</subsectionHeader>
<bodyText confidence="0.999524452380952">
It appears that neither annotator expertise, a word’s
part of speech, the number of senses in WordNet,
the number of senses annotators find in the corpus,
nor the nature of the distribution across senses, can
account for the variation in IA in Table 2. All six
annotators used the same annotation tool, the same
guidelines, and had already become experienced in
the word sense annotation task.
The six annotators all exhibit roughly the same
performance. We measure an individual annotator’s
performance by computing the average pairwise IA
(IA2). For every annotator AZ, we first compute the
pairwise agreement of AZ with every other annota-
tor, then average. This gives us a measure for com-
paring individual annotators with each other: an-
notators that have a higher IA2 have more agree-
ment, on average, with other annotators. Note that
we get the same ranking of individuals when for
each annotator, we calculate how much the agree-
ment among the five remaining annotators improves
over the agreement among all six annotators. If
agreement improves relatively more when annota-
tor AZ is dropped, then AZ agrees less well with the
other five annotators. While both approaches give
the same ranking among annotators, IA2 also pro-
vides a number that has an interpretable value.
On a word-by-word basis, some annotators do
better than others. For example, for long, the best
annotator (A) has IA2=0.79, and the worst (F) has
0.44. However, across ten words annotated by all
six, the average of their IA2 is 0.39 with a standard
deviation of 0.037. F at 0.32 is an outlier; apart from
F, annotators have similar IA across words.
Table 2 lists the distribution of available senses
in WordNet for the four words (column 4), and the
number of senses used (column 5). The words work
and tell have relatively fewer senses (seven and eig-
ith) compared with nine through twelve for the other
words. However, neither the number (or proportion)
of senses used by annotators, nor the distribution
across senses, has a significant correlation with IA,
as given by Pearson’s correlation test.
</bodyText>
<subsectionHeader confidence="0.996099">
5.2 Lexical Use Factors
</subsectionHeader>
<bodyText confidence="0.999572583333333">
Underspecified contexts lead to ambiguous word
meanings, a factor that has been recognized as be-
ing associated with polysemous contexts (Palmer et
al., 2005). We find that the converse is also true:
relatively specific contexts reduce ambiguity.
The word long seems to engender the greatest IA
primarily because the contexts are concrete and spe-
cific, with a secondary effect that adjectives have
higher IA overall than the other parts of speech. Sen-
tences such as (1.), where a specific unit of temporal
or spatial measurement is mentioned (months), re-
strict the sense to extent in space or time.
</bodyText>
<listItem confidence="0.972997">
1. For 18 long months Michael could not find a job.
WN S1. temporal extent [N=6 of 6]
</listItem>
<bodyText confidence="0.995761071428571">
In the few cases where annotators disagree on
long, the context is less specific or less concrete. In
example (2.), long is predicated of the word chap-
ter, which has non-concrete senses that exemplify
a certain type of productive polysemy (Pustejovsky,
1991). It can be taken to refer to a physical object
(a specific set of pages in an actual book), or a con-
ceptual object (the abstract literary work). The ad-
jective inherits this polysemy. The three annotators
who agree on sense two (spatial extent) might have
the physical object sense in mind; the two who select
sense one (temporal extent) possibly took the point
of view of the reader who requires a long time to
read the chapter.
</bodyText>
<listItem confidence="0.86668875">
2. After I had submitted the manuscript my editor at
Simon Schuster had suggested a number of cuts to
streamline what was already a long and involved
chapter on Brians ideas.
</listItem>
<bodyText confidence="0.937723733333333">
WN S2.spatial extent [N=3 of 6],
WN S1.temporal extent [N=2 of 6],
WN S9.more than normal or necessary [N=1 of 6]
Several of the senses of work are concrete, and
quite distinct: sense seven, “an artist’s or writer’s
output”; sense three, “the occupation you are paid
for”; sense five, “unit of force in physics”; sense
six, “the place where one works.” These are the
senses most often selected by a majority of annota-
tors. Senses one and two, which are closely related,
are the two senses most often selected by different
annotators for the same instance. They also repre-
sent examples of productive polysemy, here between
an activity sense (sense one) and a product-of-the-
activity sense (sense two). Example (3) shows a sen-
</bodyText>
<page confidence="0.965661">
5
</page>
<bodyText confidence="0.986004538461538">
tence where the verb perform restricts the meaning
to the activity sense, which all annotators selected.
pairs of senses are merged into three senses, the IA
goes up from 0.2627 to 0.3677.
3. The work performed by Rustom and colleagues
suggests that cell protrusions are a general mech-
anism for cell-to-cell communication and that in-
formation exchange is occurring through the direct
membrane continuity of connected cells indepen-
dently of exo- and endocytosis.
WN S1.activity of making something [N=6 of 6]
In sentence (4.), four annotators selected sense
one (activity) and two selected sense two (result):
</bodyText>
<listItem confidence="0.956984333333333">
4. A close friend is a plastic surgeon who did some
minor OK semi-major facial work on me in the past.
WN S1.activity directed toward making something
[N=4 of 6],
WN S2.product of the effort of a person or thing
[N=2 of 6]
</listItem>
<bodyText confidence="0.9993755">
For the word fair, if five or six annotators agree,
often they have selected sense one–”free of fa-
voritism or bias”–as in example (5). However, this
sense is often selected along with sense two–”not ex-
cessive or extreme”as in example (6). Both senses
are relatively abstract.
</bodyText>
<listItem confidence="0.992255222222222">
5. By insisting that everything Microsoft has done is
fair competition they risk the possibility that the
public if it accepts the judges finding to the con-
trary will conclude that Microsoft doesn’t know the
difference.
WN S1.free of favoritism/bias [N=6 of 6]
6. I I think that’s true I can remember times my parents
would say well what do you think would be a fair
punishment.
</listItem>
<bodyText confidence="0.976335166666667">
WN S1.free of favoritism/bias [N=3 of 6],
WN S2.not excessive or extreme [N=3 of 6]
Example (7) illustrates a case where all annota-
tors agreed on a sense for land. The named entity
India restricts the meaning to sense five, “territory
occupied by a nation.” Apart from a few such cases
of high consensus, land seems to have low agree-
ment due to senses being so closely related they can
be merged. Senses one and seven both have to do
with property (cf. example (8))., senses three and
five with geopolitical senses, and senses two and
four with the earth’s surface or soil. If these three
7. India is exhilarating exhausting and infuriating a
land where you’ll find the practicalities of daily life
overlay the mysteries that popular myth attaches to
India.
WN S5.territory occupied by a nation [N=6 of 6]
8. uh the Seattle area we lived outside outside of the
city in the country and uh we have five acres of land
up against a hillside where i grew up and so we did
have a garden about a one a half acre garden
WN S4.solid part of the earth’s surface [N=1 of 6],
WN S1.location of real estate [N=2 of 6],
WN S7.extensive landed property [N=3 of 6]
Examples for tell and show exhibit the same trend
in which agreement is greater when the sense is
more specific or concrete, which we illustrate briefly
with show. Example (9) describes a specific work of
art, an El Greco painting, and agreement is universal
among the six annotators on sense 5. In contrast, ex-
ample (10) shows a fifty-fifty split among annotators
for a sentence with a very specific context, an ex-
periment regarding delivery of a DNA solution, but
where the sense is abstract rather than concrete: the
argument of show is an abstract proposition, namely
a conclusion is drawn regarding what the experiment
demonstrates, rather than a concrete result such as a
specific measurement, or statistical outcome. Sense
two in fact contains the word “experiment” that oc-
curs in (9), which presumably biases the choice of
sense two. Impressionistically, senses two and three
appear to be quite similar.
</bodyText>
<listItem confidence="0.632776">
9. El Greco shows St. Augustine and St. Stephen,
in splendid ecclesiastical garb, lifting the count’s
body.
WN S5.show in, or as in, a picture, N=6 of 6
</listItem>
<bodyText confidence="0.925149142857143">
10. These experiments show that low-volume jet
injection specifically targeted delivery of a DNA
solution to the skin and that the injection paths did
not reach into the underlying tissue.
WN S2.establish the validity of something, as by
an example, explanation or experiment, N=3 of 6
WN S3.provide evidence for, N=3 of 6
</bodyText>
<page confidence="0.995278">
6
</page>
<subsectionHeader confidence="0.998225">
5.3 Quantifying Sense Similarity
</subsectionHeader>
<bodyText confidence="0.999855285714286">
Application of an inter-sense similarity measure
(ISM) proposed in (Ide, 2006) to the sense invento-
ries for each of the six words supports the observa-
tion that words with very similar senses have lower
IA scores. ISM is computed for each pair in a given
word’s sense inventory, using a variant of the lesk
measure (Banerjee and Pedersen, 2002). Agglom-
erative clustering may then be applied to the result-
ing similarity matrix to reveal the overall pattern of
inter-sense relations.
ISMs for senses pairs of long, fair, work, land,
tell, and show range from 0 to 1.44.4 We compute
a confusion threshhold CT based on the ISMs for all
250 sense pairs as
</bodyText>
<equation confidence="0.996564">
CT = µA + 2QA
</equation>
<bodyText confidence="0.999763066666667">
where A is the sum of the ISMs for the six words’ 250
sense pairs.
Table 3 shows the ISM statistics for the six words. The
values show that the ISMs for work and long are signifi-
cantly lower than for land and fair. The ISMs for the two
verbs in the study, show and tell, are distributed across
nearly the same range (0 - 1.38 and 0 - 1.22, respec-
tively), despite substantially lower IA scores for show.
However, the ISMs for three of show’s sense pairs are
well above CT, vs. one for tell, suggesting that in addi-
tion to the range of ISMs for a given word’s senses, the
number of sense pairs with high similarity contributes to
low IA. Overall, the correlation between the percentage
of ISMs above CT for the words in this study and their
IA scores is .8, which supports this claim.
</bodyText>
<table confidence="0.988051428571429">
POS Word Max Mean Std. Dev &gt; CT
adj long .71 .28 .18 0
fair 1.25 .28 .34 5
noun work .63 .22 .16 0
land 1.44 .17 .29 3
verb tell 1.22 .15 .25 1
show 1.38 .18 .27 3
</table>
<tableCaption confidence="0.999049">
Table 3: ISM statistics
</tableCaption>
<sectionHeader confidence="0.93568" genericHeader="method">
6 Association Rules
</sectionHeader>
<bodyText confidence="0.9984685">
Association rules express relations among instances
based on their attributes. Here the attributes of interest are
</bodyText>
<footnote confidence="0.8285842">
4Note that because the scores are based on overlaps among
WordNet relations, glosses, examples, etc., there is no pre-
defined ceiling value for the ISMs. For the words in this study,
we compute a ceiling value by taking the maximum of the ISMs
for each of the 57 senses with itself, 4.85 in this case.
</footnote>
<bodyText confidence="0.971294461538461">
the annotators who choose one sense versus those who
choose another. Mining association rules to find strong
relations has been studied in many domains (see for in-
stance (Agrawal et al., 1993; Zaki et al., 1997; Salleb-
Aouissi et al., 2007)). Here we illustrate how association
rules can be used to mine relations such as systematic dif-
ferences in word sense choices across annotators.
An association rule is an expression C1 C2, where
C1 and C2 express conditions on features describing the
instances in a dataset. The strength of the rules is usually
evaluated by means of measures such as Support (Supp)
and Confidence (Conf). Where C, C1 and C2 express con-
ditions on attributes:
</bodyText>
<listItem confidence="0.999655">
• Supp(C) is the fraction of instances satisfying C
• Supp(C1 C2) = Supp(C1 n C2)
• Conf(C1 C2) = Supp(C1 n C2)/Supp(C1)
</listItem>
<bodyText confidence="0.999840243243243">
Given two thresholds MinSupp (for minimum support)
and MinConf (for minimum confidence), a rule is strong
when its support is greater than MinSupp and its confi-
dence greater than MinConf. Discovering strong rules is
usually a two-step process of retrieving instances above
MinSupp, then from these retrieving instances above
MinConf.
The types of association rules to mine can include
any attributes in either the left hand side or the right
hand side of rules. In our data, the attributes consist
of the word sense assigned by annotators, the annota-
tors, and the instances (words). In order to find rules
that relate annotators to each other, the dataset must be
pre-processed to produce flat (two-dimensional) tables.
Here we focus on annotators to get a flat table in which
each line corresponds to an annotator/sense combination:
Annotator Sense. We denote the six annotators as A1
through A6, and word senses by WordNet sense number.
Here are 15 unique pairs of annotators, so one way
to look at where agreements occur is to determine how
many of these pairs choose the same sense with non-
negligible support and confidence. Tell has much bet-
ter IA than show, but less than long and work. We
would expect association rules among many pairs of
annotators for some but not all of its senses. We
find 11 pairs of rules of the form Ai Tell:Sense1 →
Aj Tell:Sense1, Aj Tell:Sense1 → Ai Tell:Sense1,
indicating a bi-directional relationship between pairs of
annotators choosing the same sense, with support rang-
ing from 14% to 44% and confidence ranging from 37%
to 96%. This indicates good support and confidence for
many possible pairs
Our interest here is primarily in mining for systematic
disagreements thus we now turn to pairs of rules where
in one rule, an attribute Annotator Sensei occurs in the
left hand side, and a distinct attribute Annotator Sensej
occurs in the right. Again, we are especially interested in
</bodyText>
<page confidence="0.999575">
7
</page>
<table confidence="0.99984555">
i j Supp(%) Confi(%) Confj(%)
Ai fair.S1 ↔ Aj fair.S2
A3 A6 20 100 32.3
A5 A6 20 100 31.2
A1 A2 16 80 40
Ai show.S2 ↔ Aj show.S3
A1 A3 32 84.2 69.6
A5 A3 24 63.2 80.0
A4 A3 22 91.7 57.9
A4 A6 14 58.3 46.7
A4 A2 12 60.0 50.0
A5 A2 12 60.0 40.0
Ai show.S5 ↔ Aj show.S10
A1 A6 12 85.7 40.0
A5 A2 10 83.3 50.0
A4 A2 10 83.3 30.5
A4 A6 10 71.4 38.5
A3 A2 8 66.7 40.0
A3 A6 8 57.1 40.0
A5 A6 8 57.1 40.0
</table>
<tableCaption confidence="0.999815">
Table 4: Association Rules for Systematic Disagreements
</tableCaption>
<bodyText confidence="0.999707588235294">
bi-directional cases where there is a corresponding rule
with the left and right hand clauses reversed. Table 4
shows some general classes of disagreement rules using a
compact representation with a bidirectional arrow, along
with a table of variables for the different pairs of annota-
tors associated with different levels of support and confi-
dence.
For fair, Table 4 summarizes three pairs of rules with
good support (16-20% of all instances) in which one an-
notator chooses sense 1 offair and another chooses sense
2: A3 and A5 choose sense 1 where A6 chooses sense 2,
and A1 chooses sense 1 where A2 chooses sense 2. The
confidence varies for each rule, thus in 100% of cases
where A6 selects sense 2 of fair, A3 selects sense 1, but
in only 32.3% of cases is the converse true. Example (6)
where half the annotators picked sense 1 of fair and half
picked sense 2 falls into the set of instances covered by
these rules. The rules indicate this is not isolated, but
rather part of a systematic pattern of usage.
The word land had the lowest interannotator agree-
ment among the six annotators, with eight of eleven
senses were used overall (cf. Table 2). Here we did not
find pairs of rules in which distinct Annotator Sense
attributes that occur in the left and right sides of one rule
occur in the right and left sides of another rule. For show,
Table 4 illustrates two systematic divisions among
groups of annotators. With rather good support rang-
ing from 12% to 32%, senses 2 and 3 exhibit a system-
atic difference: annotators A1, A4 and A5 select sense
2 where annotators A3, A3 and A6 select sense 3. Sim-
ilarly, senses 5 and 10 exhibit a systematic difference:
with a more modest support of 8% to 12%, annotators
A1, A3, A4 and A5 select sense 5 where annotators A2
and A6 select sense 10.
</bodyText>
<sectionHeader confidence="0.996742" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999960842105263">
We have performed a sense assignment experiment
among multiple annotators for word occurrences drawn
from a broad range of genres, rather than the domain-
specific data utilized in many studies. The selected words
were all moderately polysemous. Based on the results,
we identify several factors that distinguish words with
high vs. low interannotator agreement scores. We also
show the use of association rules to mine the data for
systematic annotator differences. Where relevant, the re-
sults can be used to merge senses, as done in much pre-
vious work, or to identify internal structure within a set
of senses, such as a word-based sense-hierarchy. In our
future work, we want to develop the use of association
rules in several ways. First, we hope to fully automated
the process of finding systematic patterns of difference
across annotators. Second, we hope to extend their use
to mining associations among the representations of in-
stances in order to further investigate the lexical use fac-
tors discussed here.
</bodyText>
<sectionHeader confidence="0.997507" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9823905">
This work was supported in part by National Science
Foundation grant CRI-0708952.
</bodyText>
<sectionHeader confidence="0.998547" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999222789473684">
Rakesh Agrawal, Tomasz Imielinski, and Arun N.
Swami. 1993. Mining association rules between sets
of items in large databases. In Peter Buneman and
Sushil Jajodia, editors, Proceedings of the 1993 ACM
SIGMOD International Conference on Management of
Data, Washington, D.C., May 26-28, 1993, pages 207–
216. ACM Press.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555–596.
Satanjeev Banerjee and Ted Pedersen. 2002. An adapted
Lesk algorithm for word sense disambiguation using
WordNet. In Proceedings of the third International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLing-2002), pages 136–45,
Mexico City, Mexico.
Douglas Biber. 1995. Dimensions of register variation :
a cross-linguistic comparison. Cambridge University
Press, Cambridge.
</reference>
<page confidence="0.991881">
8
</page>
<reference confidence="0.980733598039216">
Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002.
Polysemy and sense proximity in the senseval-2 test
suite. In Proceedings of the SIGLEX/SENSEVAL
Workshop on Word Sense Disambiguation: Re-
cent Successes and Future Directions, pages 32–39,
Philadelphia.
Mona Diab. 2004. Relieving the data acquisition bottle-
neck in word sense disambiguation. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, pages 303–311.
Nancy Ide and Yorick Wilks. 2006. Making sense about
sense. In E. Agirre and P. Edmonds, editors, Word
Sense Disambiguation: Algorithms and Applications,
pages 47–74, Dordrecht, The Netherlands. Springer.
Nancy Ide, Tomaz Erjavec, and Dan Tufis. 2002. Sense
discrimination with parallel corpora. In Proceedings
of ACL’02 Workshop on Word Sense Disambiguation:
Recent Successes and Future Directions, pages 54–60,
Philadelphia.
Nancy Ide. 2006. Making senses: Bootstrapping sense-
tagged lists of semantically-related words. In Alexan-
der Gelbukh, editor, Computational Linguistics and
Intelligent Text, pages 13–27, Dordrecht, The Nether-
lands. Springer.
Adam Kilgarriff. 1998. SENSEVAL: An exercise in
evaluating word sense disambiguation programs. In
Proceedings of the First International Conference on
Language Resources and Evaluation (LREC), pages
581–588, Granada.
Adam Kilgarriff. 2001. Comparing corpora. Interna-
tional Journal of Corpus Linguistics, 6:1–37.
Richard Kittredge, Tanya Korelsky, and Owen Rambow.
1991. On the need for domain communication knowl-
edge. Computational Intelligence, 7(4):305–314.
Devra Klein and Gregory Murphy. 2002. Paper has been
my ruin: Conceptual relations of polysemous words.
Journal of Memory and Language, 47:548–70.
Klaus Krippendorff. 1980. Content analysis: An intro-
duction to its methodology. Sage Publications, Bev-
erly Hills, CA.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159–174.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1993. In-
troduction to WordNet: An on-line lexical database
(revised). Technical Report Cognitive Science Labo-
ratory (CSL) Report 43, Princeton University, Prince-
ton. Revised March 1993.
Hwee Tou Ng, Chung Yong Lim, and Shou King Foo.
1999. A case study on inter-annotator agreement for
word sense disambiguation. In SIGLEX Workshop On
Standardizing Lexical Resources.
Geoffrey Nunberg. 1979. The non-uniqueness of seman-
tic solutions: Polysemy. Linguistics and Philosophy,
3:143–184.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. 2005. Making fin-egrained and coarse-grained
sense distinctions. Journal of Natural Language Engi-
neering, 13.2:137–163.
Rebecca J. Passonneau, Nizar Habash, and Owen Ram-
bow. 2006. Inter-annotator agreement on a multilin-
gual semantic annotation task. In Proceedings of the
International Conference on Language Resources and
Evaluation (LREC), pages 1951–1956, Genoa, Italy.
Rebecca J. Passonneau. 2004. Computing reliability for
coreference annotation. In Proceedings of the Interna-
tional Conference on Language Resources and Evalu-
ation (LREC), Portugal.
Ted Pedersen. 2002a. Assessing system agreement
and instance difficulty in the lexical sample tasks of
Senseval-2. In Proceedings of the ACL-02 Workshop
on Word Sense Disambiguation: Recent Successes and
Future Directions, pages 40–46.
Ted Pedersen. 2002b. Evaluating the effectiveness of
ensembles of decision trees in disambiguating SEN-
SEVAL lexical samples. In Proceedings of the ACL-
02 Workshop on Word Sense Disambiguation: Recent
Successes and Future Directions, pages 81–87.
James Pustejovsky. 1991. The generative lexicon. Com-
putational Linguitics, 17(4):409–441.
Josef Ruppenhofer, Michael Ellsworth, Miriam
R. L. Petruck, Christopher R. Johnson, and
Jan Scheffczyk. 2006. Framenet ii: Ex-
tended theory and practice. Available from
http://framenet.icsi.berkeley.edu/index.php.
Ansaf Salleb-Aouissi, Christel Vrain, and Cyril Nortet.
2007. Quantminer: A genetic algorithm for mining
quantitative association rules. In IJCAI, pages 1035–
1040.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2007.
Learning to merge word senses. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 1005–1014, Prague.
Jean Veronis. 1998. A study of polysemy judgements
and inter-annotator agreement. In SENSEVAL Work-
shop, pages Sussex, England.
Mohammed Javeed Zaki, Srinivasan Parthasarathy, Mit-
sunori Ogihara, and Wei Li. 1997. New algorithms
for fast discovery of association rules. In KDD, pages
283–286.
</reference>
<page confidence="0.997062">
9
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.232298">
<title confidence="0.99998">Making Sense of Word Sense Variation</title>
<author confidence="0.999919">J Passonneau</author>
<affiliation confidence="0.999116">Center for Computational Learning</affiliation>
<address confidence="0.936398">Columbia</address>
<author confidence="0.544835">New York</author>
<author confidence="0.544835">NY</author>
<email confidence="0.998765">(becky@cs|ansaf@ccls).columbia.edu</email>
<author confidence="0.944629">Nancy</author>
<affiliation confidence="0.781852">Department of Computer Vassar</affiliation>
<address confidence="0.824967">Poughkeepsie, NY,</address>
<email confidence="0.999621">ide@cs.vassar.edu</email>
<abstract confidence="0.99836">We present a pilot study of word-sense annotation using multiple annotators, relatively polysemous words, and a heterogenous corpus. Annotators selected senses for words in context, using an annotation interface that presented WordNet senses. Interannotator agreement (IA) results show that annotators agree well or not, depending primarily on the individual words and their general usage properties. Our focus is on identifying systematic differences across words and annotators that can account for IA variation. We identify three lexical use factors: semantic specificity of the context, sense concreteness, and similarity of senses. We discuss systematic differences in sense selection across annotators, and present the use of association rules to mine the data for systematic differences across annotators.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rakesh Agrawal</author>
<author>Tomasz Imielinski</author>
<author>Arun N Swami</author>
</authors>
<title>Mining association rules between sets of items in large databases.</title>
<date>1993</date>
<booktitle>In Peter Buneman and Sushil Jajodia, editors, Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data,</booktitle>
<pages>207--216</pages>
<publisher>ACM Press.</publisher>
<location>Washington, D.C.,</location>
<contexts>
<context position="21437" citStr="Agrawal et al., 1993" startWordPosition="3603" endWordPosition="3606">iation Rules Association rules express relations among instances based on their attributes. Here the attributes of interest are 4Note that because the scores are based on overlaps among WordNet relations, glosses, examples, etc., there is no predefined ceiling value for the ISMs. For the words in this study, we compute a ceiling value by taking the maximum of the ISMs for each of the 57 senses with itself, 4.85 in this case. the annotators who choose one sense versus those who choose another. Mining association rules to find strong relations has been studied in many domains (see for instance (Agrawal et al., 1993; Zaki et al., 1997; SallebAouissi et al., 2007)). Here we illustrate how association rules can be used to mine relations such as systematic differences in word sense choices across annotators. An association rule is an expression C1 C2, where C1 and C2 express conditions on features describing the instances in a dataset. The strength of the rules is usually evaluated by means of measures such as Support (Supp) and Confidence (Conf). Where C, C1 and C2 express conditions on attributes: • Supp(C) is the fraction of instances satisfying C • Supp(C1 C2) = Supp(C1 n C2) • Conf(C1 C2) = Supp(C1 n C</context>
</contexts>
<marker>Agrawal, Imielinski, Swami, 1993</marker>
<rawString>Rakesh Agrawal, Tomasz Imielinski, and Arun N. Swami. 1993. Mining association rules between sets of items in large databases. In Peter Buneman and Sushil Jajodia, editors, Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data, Washington, D.C., May 26-28, 1993, pages 207– 216. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="8234" citStr="Artstein and Poesio, 2008" startWordPosition="1308" endWordPosition="1312">report Cohen’s κ; note the similarity in values3. As with the various agreement coefficients that factor out the agreement that would occur by chance, values range from 1 for perfect agreement and -1 for perfect opposition, to 0 for chance agreement. While there are no hard and fast criteria for what constitutes good IA, Landis and Koch (Landis and Koch, 1977) consider values between 0.40 and 0.60 to represent moderately good agreement, and values above 0.60 as quite good; Krippendorff (Krippendorff, 1980) considers values above 0.67 moderately good, and values above 0.80 as quite good. (cf. (Artstein and Poesio, 2008) for discussion of agreement measurement for computational linguistic tasks.) Table 2 shows IA for a pair of adjectives, nouns and verbs from our sample for which the IA scores are at the extremes (high and low) in each pair: the average delta is 0.24. Note that the agreement decreases as part-of-speech varies from adjectives to nouns to verbs, but for all three parts-of-speech, there is a wide spread of values. It is striking, given that the same annotators did all words, that one in each pair has relatively better agreement. 3α handles multiple annotators; Arstein and Poesio (Artstein and Po</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4):555–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>An adapted Lesk algorithm for word sense disambiguation using WordNet.</title>
<date>2002</date>
<booktitle>In Proceedings of the third International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2002),</booktitle>
<pages>136--45</pages>
<location>Mexico City, Mexico.</location>
<contexts>
<context position="19518" citStr="Banerjee and Pedersen, 2002" startWordPosition="3244" endWordPosition="3247">geted delivery of a DNA solution to the skin and that the injection paths did not reach into the underlying tissue. WN S2.establish the validity of something, as by an example, explanation or experiment, N=3 of 6 WN S3.provide evidence for, N=3 of 6 6 5.3 Quantifying Sense Similarity Application of an inter-sense similarity measure (ISM) proposed in (Ide, 2006) to the sense inventories for each of the six words supports the observation that words with very similar senses have lower IA scores. ISM is computed for each pair in a given word’s sense inventory, using a variant of the lesk measure (Banerjee and Pedersen, 2002). Agglomerative clustering may then be applied to the resulting similarity matrix to reveal the overall pattern of inter-sense relations. ISMs for senses pairs of long, fair, work, land, tell, and show range from 0 to 1.44.4 We compute a confusion threshhold CT based on the ISMs for all 250 sense pairs as CT = µA + 2QA where A is the sum of the ISMs for the six words’ 250 sense pairs. Table 3 shows the ISM statistics for the six words. The values show that the ISMs for work and long are significantly lower than for land and fair. The ISMs for the two verbs in the study, show and tell, are dist</context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2002. An adapted Lesk algorithm for word sense disambiguation using WordNet. In Proceedings of the third International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2002), pages 136–45, Mexico City, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Biber</author>
</authors>
<title>Dimensions of register variation : a cross-linguistic comparison.</title>
<date>1995</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="1394" citStr="Biber, 1995" startWordPosition="198" endWordPosition="199">s that can account for IA variation. We identify three lexical use factors: semantic specificity of the context, sense concreteness, and similarity of senses. We discuss systematic differences in sense selection across annotators, and present the use of association rules to mine the data for systematic differences across annotators. 1 Introduction Our goal is to grapple seriously with the natural sense variation arising from individual differences in word usage. It has been widely observed that usage features such as vocabulary and syntax vary across corpora of different genres and registers (Biber, 1995), and that serve different functions (Kittredge et al., 1991). Still, we are far from able to predict specific morphosyntactic and lexical variations across corpora (Kilgarriff, 2001), much less quantify them in a way that makes it possible to apply the same analysis tools (taggers, parsers) without retraining. In comparison to morphosyntactic properties of language, word and phrasal meaning is fluid, and to some degree, generative (Pustejovsky, 1991; Nunberg, 1979). Based on our initial observations from a word sense annotation task for relatively polysemous words, carried out by multiple ann</context>
</contexts>
<marker>Biber, 1995</marker>
<rawString>Douglas Biber. 1995. Dimensions of register variation : a cross-linguistic comparison. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irina Chugur</author>
<author>Julio Gonzalo</author>
<author>Felisa Verdejo</author>
</authors>
<title>Polysemy and sense proximity in the senseval-2 test suite.</title>
<date>2002</date>
<booktitle>In Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions,</booktitle>
<pages>32--39</pages>
<location>Philadelphia.</location>
<contexts>
<context position="4354" citStr="Chugur et al., 2002" startWordPosition="682" endWordPosition="685"> 11 1288 time Noun 10 21790 work Noun 7 5780 know Verb 11 10334 say Verb 11 20372 show Verb 12 11877 tell Verb 8 4799 Table 1: Ten Words 2005). Pedersen (Pedersen, 2002a) examines variation across individual words in evaluating WSD systems, but does not attempt to explain it. Factors that have been proposed as affecting human or system sense disambiguation include whether annotators are allowed to assign multilabels (Veronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), sense perplexity (Diab, 2004), entropy (Diab, 2004; Palmer et al., 2005), and in psycholinguistic experiments, reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006). With respect to using multiple annotators, Snow et al. included disambiguation of the word president–a relatively non-polysemous word with three senses–in a set of tasks given to Amazon Mechanical Turkers, aimed at determining how to combine data from multiple non-experts for machine learning tasks. The word sense task comprised 177 sentences taken from the SemEval Word Sense Disambiguation</context>
</contexts>
<marker>Chugur, Gonzalo, Verdejo, 2002</marker>
<rawString>Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002. Polysemy and sense proximity in the senseval-2 test suite. In Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, pages 32–39, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
</authors>
<title>Relieving the data acquisition bottleneck in word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>303--311</pages>
<contexts>
<context position="4385" citStr="Diab, 2004" startWordPosition="689" endWordPosition="690">5780 know Verb 11 10334 say Verb 11 20372 show Verb 12 11877 tell Verb 8 4799 Table 1: Ten Words 2005). Pedersen (Pedersen, 2002a) examines variation across individual words in evaluating WSD systems, but does not attempt to explain it. Factors that have been proposed as affecting human or system sense disambiguation include whether annotators are allowed to assign multilabels (Veronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), sense perplexity (Diab, 2004), entropy (Diab, 2004; Palmer et al., 2005), and in psycholinguistic experiments, reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006). With respect to using multiple annotators, Snow et al. included disambiguation of the word president–a relatively non-polysemous word with three senses–in a set of tasks given to Amazon Mechanical Turkers, aimed at determining how to combine data from multiple non-experts for machine learning tasks. The word sense task comprised 177 sentences taken from the SemEval Word Sense Disambiguation Lexical Sample task. Majority </context>
</contexts>
<marker>Diab, 2004</marker>
<rawString>Mona Diab. 2004. Relieving the data acquisition bottleneck in word sense disambiguation. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages 303–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Yorick Wilks</author>
</authors>
<title>Making sense about sense.</title>
<date>2006</date>
<booktitle>Word Sense Disambiguation: Algorithms and Applications,</booktitle>
<pages>47--74</pages>
<editor>In E. Agirre and P. Edmonds, editors,</editor>
<publisher>Springer.</publisher>
<location>Dordrecht, The Netherlands.</location>
<contexts>
<context position="4559" citStr="Ide and Wilks, 2006" startWordPosition="713" endWordPosition="716">al words in evaluating WSD systems, but does not attempt to explain it. Factors that have been proposed as affecting human or system sense disambiguation include whether annotators are allowed to assign multilabels (Veronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), sense perplexity (Diab, 2004), entropy (Diab, 2004; Palmer et al., 2005), and in psycholinguistic experiments, reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006). With respect to using multiple annotators, Snow et al. included disambiguation of the word president–a relatively non-polysemous word with three senses–in a set of tasks given to Amazon Mechanical Turkers, aimed at determining how to combine data from multiple non-experts for machine learning tasks. The word sense task comprised 177 sentences taken from the SemEval Word Sense Disambiguation Lexical Sample task. Majority voting among three annotators achieve 99% accuracy. 3 The Annotation Task The Manually Annotated Sub-Corpus (MASC) project is creating a small, representative corpus of Ameri</context>
</contexts>
<marker>Ide, Wilks, 2006</marker>
<rawString>Nancy Ide and Yorick Wilks. 2006. Making sense about sense. In E. Agirre and P. Edmonds, editors, Word Sense Disambiguation: Algorithms and Applications, pages 47–74, Dordrecht, The Netherlands. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Tomaz Erjavec</author>
<author>Dan Tufis</author>
</authors>
<title>Sense discrimination with parallel corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL’02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions,</booktitle>
<pages>54--60</pages>
<location>Philadelphia.</location>
<contexts>
<context position="4186" citStr="Ide et al., 2002" startWordPosition="654" endWordPosition="657"> Colorado, June 2009. c�2009 Association for Computational Linguistics Word POS No. senses No. occurrences fair Adj 10 463 long Adj 9 2706 quiet Adj 6 244 land Noun 11 1288 time Noun 10 21790 work Noun 7 5780 know Verb 11 10334 say Verb 11 20372 show Verb 12 11877 tell Verb 8 4799 Table 1: Ten Words 2005). Pedersen (Pedersen, 2002a) examines variation across individual words in evaluating WSD systems, but does not attempt to explain it. Factors that have been proposed as affecting human or system sense disambiguation include whether annotators are allowed to assign multilabels (Veronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), sense perplexity (Diab, 2004), entropy (Diab, 2004; Palmer et al., 2005), and in psycholinguistic experiments, reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006). With respect to using multiple annotators, Snow et al. included disambiguation of the word president–a relatively non-polysemous word with three senses–in a set of tasks given to Amazon Mechanical Turkers, aimed at determinin</context>
</contexts>
<marker>Ide, Erjavec, Tufis, 2002</marker>
<rawString>Nancy Ide, Tomaz Erjavec, and Dan Tufis. 2002. Sense discrimination with parallel corpora. In Proceedings of ACL’02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, pages 54–60, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
</authors>
<title>Making senses: Bootstrapping sensetagged lists of semantically-related words.</title>
<date>2006</date>
<booktitle>Computational Linguistics and Intelligent Text,</booktitle>
<pages>13--27</pages>
<editor>In Alexander Gelbukh, editor,</editor>
<publisher>Springer.</publisher>
<location>Dordrecht, The Netherlands.</location>
<contexts>
<context position="19253" citStr="Ide, 2006" startWordPosition="3198" endWordPosition="3199">to be quite similar. 9. El Greco shows St. Augustine and St. Stephen, in splendid ecclesiastical garb, lifting the count’s body. WN S5.show in, or as in, a picture, N=6 of 6 10. These experiments show that low-volume jet injection specifically targeted delivery of a DNA solution to the skin and that the injection paths did not reach into the underlying tissue. WN S2.establish the validity of something, as by an example, explanation or experiment, N=3 of 6 WN S3.provide evidence for, N=3 of 6 6 5.3 Quantifying Sense Similarity Application of an inter-sense similarity measure (ISM) proposed in (Ide, 2006) to the sense inventories for each of the six words supports the observation that words with very similar senses have lower IA scores. ISM is computed for each pair in a given word’s sense inventory, using a variant of the lesk measure (Banerjee and Pedersen, 2002). Agglomerative clustering may then be applied to the resulting similarity matrix to reveal the overall pattern of inter-sense relations. ISMs for senses pairs of long, fair, work, land, tell, and show range from 0 to 1.44.4 We compute a confusion threshhold CT based on the ISMs for all 250 sense pairs as CT = µA + 2QA where A is the</context>
</contexts>
<marker>Ide, 2006</marker>
<rawString>Nancy Ide. 2006. Making senses: Bootstrapping sensetagged lists of semantically-related words. In Alexander Gelbukh, editor, Computational Linguistics and Intelligent Text, pages 13–27, Dordrecht, The Netherlands. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>SENSEVAL: An exercise in evaluating word sense disambiguation programs.</title>
<date>1998</date>
<booktitle>In Proceedings of the First International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>581--588</pages>
<location>Granada.</location>
<contexts>
<context position="3096" citStr="Kilgarriff, 1998" startWordPosition="477" endWordPosition="478"> in agreement on a matched subset of words. In Section 5 we discuss why we believe the observed variation depends on the words and present three lexical use factors we hypothesize to lead to greater or lesser IA. In Section 6, we use association rules to mine our data for systematic differences among annotators, thus to explain the variations in IA. We conclude with a summary of our findings goals. 2 Related Work There has been a decade-long community-wide effort to evaluate word sense disambiguation (WSD) systems across languages in the four Senseval efforts (1998, 2001, 2004, and 2007, cf. (Kilgarriff, 1998; Pedersen, 2002a; Pedersen, 2002b; Palmer et al., 2005)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 2–9, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Word POS No. senses No. occurrences fair Adj 10 463 lon</context>
</contexts>
<marker>Kilgarriff, 1998</marker>
<rawString>Adam Kilgarriff. 1998. SENSEVAL: An exercise in evaluating word sense disambiguation programs. In Proceedings of the First International Conference on Language Resources and Evaluation (LREC), pages 581–588, Granada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>Comparing corpora.</title>
<date>2001</date>
<journal>International Journal of Corpus Linguistics,</journal>
<pages>6--1</pages>
<contexts>
<context position="1577" citStr="Kilgarriff, 2001" startWordPosition="225" endWordPosition="226">ic differences in sense selection across annotators, and present the use of association rules to mine the data for systematic differences across annotators. 1 Introduction Our goal is to grapple seriously with the natural sense variation arising from individual differences in word usage. It has been widely observed that usage features such as vocabulary and syntax vary across corpora of different genres and registers (Biber, 1995), and that serve different functions (Kittredge et al., 1991). Still, we are far from able to predict specific morphosyntactic and lexical variations across corpora (Kilgarriff, 2001), much less quantify them in a way that makes it possible to apply the same analysis tools (taggers, parsers) without retraining. In comparison to morphosyntactic properties of language, word and phrasal meaning is fluid, and to some degree, generative (Pustejovsky, 1991; Nunberg, 1979). Based on our initial observations from a word sense annotation task for relatively polysemous words, carried out by multiple annotators on a heterogeneous corpus, we hypothesize that different words lead to greater or lesser interannotator agreement (IA) for reasons that in the long run should be explicitly mo</context>
</contexts>
<marker>Kilgarriff, 2001</marker>
<rawString>Adam Kilgarriff. 2001. Comparing corpora. International Journal of Corpus Linguistics, 6:1–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Kittredge</author>
<author>Tanya Korelsky</author>
<author>Owen Rambow</author>
</authors>
<title>On the need for domain communication knowledge.</title>
<date>1991</date>
<journal>Computational Intelligence,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="1455" citStr="Kittredge et al., 1991" startWordPosition="205" endWordPosition="208">hree lexical use factors: semantic specificity of the context, sense concreteness, and similarity of senses. We discuss systematic differences in sense selection across annotators, and present the use of association rules to mine the data for systematic differences across annotators. 1 Introduction Our goal is to grapple seriously with the natural sense variation arising from individual differences in word usage. It has been widely observed that usage features such as vocabulary and syntax vary across corpora of different genres and registers (Biber, 1995), and that serve different functions (Kittredge et al., 1991). Still, we are far from able to predict specific morphosyntactic and lexical variations across corpora (Kilgarriff, 2001), much less quantify them in a way that makes it possible to apply the same analysis tools (taggers, parsers) without retraining. In comparison to morphosyntactic properties of language, word and phrasal meaning is fluid, and to some degree, generative (Pustejovsky, 1991; Nunberg, 1979). Based on our initial observations from a word sense annotation task for relatively polysemous words, carried out by multiple annotators on a heterogeneous corpus, we hypothesize that differ</context>
</contexts>
<marker>Kittredge, Korelsky, Rambow, 1991</marker>
<rawString>Richard Kittredge, Tanya Korelsky, and Owen Rambow. 1991. On the need for domain communication knowledge. Computational Intelligence, 7(4):305–314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Devra Klein</author>
<author>Gregory Murphy</author>
</authors>
<title>Paper has been my ruin: Conceptual relations of polysemous words.</title>
<date>2002</date>
<journal>Journal of Memory and Language,</journal>
<pages>47--548</pages>
<contexts>
<context position="4537" citStr="Klein and Murphy, 2002" startWordPosition="709" endWordPosition="712">ariation across individual words in evaluating WSD systems, but does not attempt to explain it. Factors that have been proposed as affecting human or system sense disambiguation include whether annotators are allowed to assign multilabels (Veronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), sense perplexity (Diab, 2004), entropy (Diab, 2004; Palmer et al., 2005), and in psycholinguistic experiments, reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006). With respect to using multiple annotators, Snow et al. included disambiguation of the word president–a relatively non-polysemous word with three senses–in a set of tasks given to Amazon Mechanical Turkers, aimed at determining how to combine data from multiple non-experts for machine learning tasks. The word sense task comprised 177 sentences taken from the SemEval Word Sense Disambiguation Lexical Sample task. Majority voting among three annotators achieve 99% accuracy. 3 The Annotation Task The Manually Annotated Sub-Corpus (MASC) project is creating a small, represen</context>
</contexts>
<marker>Klein, Murphy, 2002</marker>
<rawString>Devra Klein and Gregory Murphy. 2002. Paper has been my ruin: Conceptual relations of polysemous words. Journal of Memory and Language, 47:548–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content analysis: An introduction to its methodology. Sage Publications,</title>
<date>1980</date>
<location>Beverly Hills, CA.</location>
<contexts>
<context position="7567" citStr="Krippendorff, 1980" startWordPosition="1200" endWordPosition="1202">tems We expected to find varying levels of interannotator agreement (IA) among all six annotators, depending on obvious grouping factors such as the part of speech, or the number of senses per word. We do find widely varying levels of agreement, but as described here, most of the variation does not depend on these a priori factors. Inherent usage properties of the words themselves, and systematic patterns of variation across annotators, seem to be the primary factors, with a secondary effect of part of speech. In previous work (Passonneau, 2004), we have discussed why we use Krippendorff’s α (Krippendorff, 1980), and for purposes of comparison we also report Cohen’s κ; note the similarity in values3. As with the various agreement coefficients that factor out the agreement that would occur by chance, values range from 1 for perfect agreement and -1 for perfect opposition, to 0 for chance agreement. While there are no hard and fast criteria for what constitutes good IA, Landis and Koch (Landis and Koch, 1977) consider values between 0.40 and 0.60 to represent moderately good agreement, and values above 0.60 as quite good; Krippendorff (Krippendorff, 1980) considers values above 0.67 moderately good, an</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff. 1980. Content analysis: An introduction to its methodology. Sage Publications, Beverly Hills, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Richard Landis</author>
<author>Gary G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="7970" citStr="Landis and Koch, 1977" startWordPosition="1268" endWordPosition="1271">patterns of variation across annotators, seem to be the primary factors, with a secondary effect of part of speech. In previous work (Passonneau, 2004), we have discussed why we use Krippendorff’s α (Krippendorff, 1980), and for purposes of comparison we also report Cohen’s κ; note the similarity in values3. As with the various agreement coefficients that factor out the agreement that would occur by chance, values range from 1 for perfect agreement and -1 for perfect opposition, to 0 for chance agreement. While there are no hard and fast criteria for what constitutes good IA, Landis and Koch (Landis and Koch, 1977) consider values between 0.40 and 0.60 to represent moderately good agreement, and values above 0.60 as quite good; Krippendorff (Krippendorff, 1980) considers values above 0.67 moderately good, and values above 0.80 as quite good. (cf. (Artstein and Poesio, 2008) for discussion of agreement measurement for computational linguistic tasks.) Table 2 shows IA for a pair of adjectives, nouns and verbs from our sample for which the IA scores are at the extremes (high and low) in each pair: the average delta is 0.24. Note that the agreement decreases as part-of-speech varies from adjectives to nouns</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. Richard Landis and Gary G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33(1):159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine Miller</author>
</authors>
<title>Introduction to WordNet: An on-line lexical database (revised).</title>
<date>1993</date>
<tech>Technical Report Cognitive Science Laboratory (CSL) Report 43,</tech>
<institution>Princeton University, Princeton. Revised</institution>
<contexts>
<context position="5519" citStr="Miller et al., 1993" startWordPosition="861" endWordPosition="864">tences taken from the SemEval Word Sense Disambiguation Lexical Sample task. Majority voting among three annotators achieve 99% accuracy. 3 The Annotation Task The Manually Annotated Sub-Corpus (MASC) project is creating a small, representative corpus of American English written and spoken texts drawn from the Open American National Corpus (OANC).1 The MASC corpus includes handvalidated or manually produced annotations for a variety of linguistic phenomena. One of the goals of 1http://www.anc.org Figure 1: MASC word sense annotation tool the project is to support efforts to harmonize WordNet (Miller et al., 1993) and FrameNet (Ruppenhofer et al., 2006), in order to bring the sense distinctions each makes into better alignment. As a starting sample, we chose ten fairly frequent, moderately polysemous words for sense tagging, targeting in particular words that do not yet exist in FrameNet, as well as words with different numbers of senses in the two resources. The ten words with part of speech, number of senses, and occurrences in the OANC are shown in Table 1. One thousand occurrences of each word , including all occurrences appearing in the MASC subset and others semi-randomly2 chosen from the remaind</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1993</marker>
<rawString>George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller. 1993. Introduction to WordNet: An on-line lexical database (revised). Technical Report Cognitive Science Laboratory (CSL) Report 43, Princeton University, Princeton. Revised March 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Chung Yong Lim</author>
<author>Shou King Foo</author>
</authors>
<title>A case study on inter-annotator agreement for word sense disambiguation.</title>
<date>1999</date>
<booktitle>In SIGLEX Workshop On Standardizing Lexical Resources.</booktitle>
<contexts>
<context position="3426" citStr="Ng et al., 1999" startWordPosition="527" endWordPosition="530">he variations in IA. We conclude with a summary of our findings goals. 2 Related Work There has been a decade-long community-wide effort to evaluate word sense disambiguation (WSD) systems across languages in the four Senseval efforts (1998, 2001, 2004, and 2007, cf. (Kilgarriff, 1998; Pedersen, 2002a; Pedersen, 2002b; Palmer et al., 2005)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 2–9, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Word POS No. senses No. occurrences fair Adj 10 463 long Adj 9 2706 quiet Adj 6 244 land Noun 11 1288 time Noun 10 21790 work Noun 7 5780 know Verb 11 10334 say Verb 11 20372 show Verb 12 11877 tell Verb 8 4799 Table 1: Ten Words 2005). Pedersen (Pedersen, 2002a) examines variation across individual words in evaluating WSD systems, but does not attempt to explain it. Factors that ha</context>
<context position="9315" citStr="Ng et al., 1999" startWordPosition="1500" endWordPosition="1503">d all words, that one in each pair has relatively better agreement. 3α handles multiple annotators; Arstein and Poesio (Artstein and Poesio, 2008) propose an extension of κ (κ3) we use here. POS Word α κ No. senses Used adj long 0.6664 0.6665 9 8 fair 0.3546 0.3593 10 5 noun work 0.5359 0.5358 7 7 land 0.2627 0.2671 11 8 verb tell 0.4152 0.4165 8 8 show 0.2636 0.2696 12 11 Table 2: Varying interannotator agreement across words The average of the agreement values shown in Table 2 (α=0.4164; κ=0.4191) is somewhat higher than the average 0.317 found for 191 words annotated for WordNet senses in (Ng et al., 1999), but lower than their recomputed κ of 0.85 for verbs, after they reanalyzed the data to merge senses for 42 of the verbs. It is widely recognized that achieving high κ scores (or percent agreement between annotators, cf. (Palmer et al., 2005)) is difficult for word sense annotation. Given that the same annotators have higher IA on some words, and lower on others, we hypothesize that it is the word usages themselves that lead to the high deltas in IA for each part-of-speech pair. We discuss the impact of three factors on the observed variations in agreement: 1. Greater specificity in the conte</context>
</contexts>
<marker>Ng, Lim, Foo, 1999</marker>
<rawString>Hwee Tou Ng, Chung Yong Lim, and Shou King Foo. 1999. A case study on inter-annotator agreement for word sense disambiguation. In SIGLEX Workshop On Standardizing Lexical Resources.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Nunberg</author>
</authors>
<title>The non-uniqueness of semantic solutions: Polysemy. Linguistics and Philosophy,</title>
<date>1979</date>
<pages>3--143</pages>
<contexts>
<context position="1864" citStr="Nunberg, 1979" startWordPosition="271" endWordPosition="272">t has been widely observed that usage features such as vocabulary and syntax vary across corpora of different genres and registers (Biber, 1995), and that serve different functions (Kittredge et al., 1991). Still, we are far from able to predict specific morphosyntactic and lexical variations across corpora (Kilgarriff, 2001), much less quantify them in a way that makes it possible to apply the same analysis tools (taggers, parsers) without retraining. In comparison to morphosyntactic properties of language, word and phrasal meaning is fluid, and to some degree, generative (Pustejovsky, 1991; Nunberg, 1979). Based on our initial observations from a word sense annotation task for relatively polysemous words, carried out by multiple annotators on a heterogeneous corpus, we hypothesize that different words lead to greater or lesser interannotator agreement (IA) for reasons that in the long run should be explicitly modelled in order for Natural Language Processing (NLP) applications to handle usage differences more robustly. This pilot study is a step in that direction. We present related work in the next section, then describe the annotation task in the following one. In Section 4, we present examp</context>
</contexts>
<marker>Nunberg, 1979</marker>
<rawString>Geoffrey Nunberg. 1979. The non-uniqueness of semantic solutions: Polysemy. Linguistics and Philosophy, 3:143–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Hoa Trang Dang</author>
<author>Christiane Fellbaum</author>
</authors>
<title>Making fin-egrained and coarse-grained sense distinctions.</title>
<date>2005</date>
<journal>Journal of Natural Language Engineering,</journal>
<pages>13--2</pages>
<contexts>
<context position="3152" citStr="Palmer et al., 2005" startWordPosition="483" endWordPosition="486">on 5 we discuss why we believe the observed variation depends on the words and present three lexical use factors we hypothesize to lead to greater or lesser IA. In Section 6, we use association rules to mine our data for systematic differences among annotators, thus to explain the variations in IA. We conclude with a summary of our findings goals. 2 Related Work There has been a decade-long community-wide effort to evaluate word sense disambiguation (WSD) systems across languages in the four Senseval efforts (1998, 2001, 2004, and 2007, cf. (Kilgarriff, 1998; Pedersen, 2002a; Pedersen, 2002b; Palmer et al., 2005)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 2–9, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Word POS No. senses No. occurrences fair Adj 10 463 long Adj 9 2706 quiet Adj 6 244 land Noun 11 1288 time Noun</context>
<context position="4428" citStr="Palmer et al., 2005" startWordPosition="694" endWordPosition="697">1 20372 show Verb 12 11877 tell Verb 8 4799 Table 1: Ten Words 2005). Pedersen (Pedersen, 2002a) examines variation across individual words in evaluating WSD systems, but does not attempt to explain it. Factors that have been proposed as affecting human or system sense disambiguation include whether annotators are allowed to assign multilabels (Veronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), sense perplexity (Diab, 2004), entropy (Diab, 2004; Palmer et al., 2005), and in psycholinguistic experiments, reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006). With respect to using multiple annotators, Snow et al. included disambiguation of the word president–a relatively non-polysemous word with three senses–in a set of tasks given to Amazon Mechanical Turkers, aimed at determining how to combine data from multiple non-experts for machine learning tasks. The word sense task comprised 177 sentences taken from the SemEval Word Sense Disambiguation Lexical Sample task. Majority voting among three annotators achieve 99% a</context>
<context position="9558" citStr="Palmer et al., 2005" startWordPosition="1544" endWordPosition="1547">665 9 8 fair 0.3546 0.3593 10 5 noun work 0.5359 0.5358 7 7 land 0.2627 0.2671 11 8 verb tell 0.4152 0.4165 8 8 show 0.2636 0.2696 12 11 Table 2: Varying interannotator agreement across words The average of the agreement values shown in Table 2 (α=0.4164; κ=0.4191) is somewhat higher than the average 0.317 found for 191 words annotated for WordNet senses in (Ng et al., 1999), but lower than their recomputed κ of 0.85 for verbs, after they reanalyzed the data to merge senses for 42 of the verbs. It is widely recognized that achieving high κ scores (or percent agreement between annotators, cf. (Palmer et al., 2005)) is difficult for word sense annotation. Given that the same annotators have higher IA on some words, and lower on others, we hypothesize that it is the word usages themselves that lead to the high deltas in IA for each part-of-speech pair. We discuss the impact of three factors on the observed variations in agreement: 1. Greater specificity in the contexts of use leads to higher agreement 2. More concrete senses give rise to higher agreement 3. A sense inventory with closely related senses (e.g., relatively lower average inter-sense similarity scores) gives rise to lower agreement 5 Explanat</context>
<context position="13001" citStr="Palmer et al., 2005" startWordPosition="2119" endWordPosition="2122">s the distribution of available senses in WordNet for the four words (column 4), and the number of senses used (column 5). The words work and tell have relatively fewer senses (seven and eigith) compared with nine through twelve for the other words. However, neither the number (or proportion) of senses used by annotators, nor the distribution across senses, has a significant correlation with IA, as given by Pearson’s correlation test. 5.2 Lexical Use Factors Underspecified contexts lead to ambiguous word meanings, a factor that has been recognized as being associated with polysemous contexts (Palmer et al., 2005). We find that the converse is also true: relatively specific contexts reduce ambiguity. The word long seems to engender the greatest IA primarily because the contexts are concrete and specific, with a secondary effect that adjectives have higher IA overall than the other parts of speech. Sentences such as (1.), where a specific unit of temporal or spatial measurement is mentioned (months), restrict the sense to extent in space or time. 1. For 18 long months Michael could not find a job. WN S1. temporal extent [N=6 of 6] In the few cases where annotators disagree on long, the context is less s</context>
</contexts>
<marker>Palmer, Dang, Fellbaum, 2005</marker>
<rawString>Martha Palmer, Hoa Trang Dang, and Christiane Fellbaum. 2005. Making fin-egrained and coarse-grained sense distinctions. Journal of Natural Language Engineering, 13.2:137–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Inter-annotator agreement on a multilingual semantic annotation task.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>1951--1956</pages>
<location>Genoa, Italy.</location>
<contexts>
<context position="4212" citStr="Passonneau et al., 2006" startWordPosition="658" endWordPosition="661">09. c�2009 Association for Computational Linguistics Word POS No. senses No. occurrences fair Adj 10 463 long Adj 9 2706 quiet Adj 6 244 land Noun 11 1288 time Noun 10 21790 work Noun 7 5780 know Verb 11 10334 say Verb 11 20372 show Verb 12 11877 tell Verb 8 4799 Table 1: Ten Words 2005). Pedersen (Pedersen, 2002a) examines variation across individual words in evaluating WSD systems, but does not attempt to explain it. Factors that have been proposed as affecting human or system sense disambiguation include whether annotators are allowed to assign multilabels (Veronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), sense perplexity (Diab, 2004), entropy (Diab, 2004; Palmer et al., 2005), and in psycholinguistic experiments, reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006). With respect to using multiple annotators, Snow et al. included disambiguation of the word president–a relatively non-polysemous word with three senses–in a set of tasks given to Amazon Mechanical Turkers, aimed at determining how to combine data from</context>
</contexts>
<marker>Passonneau, Habash, Rambow, 2006</marker>
<rawString>Rebecca J. Passonneau, Nizar Habash, and Owen Rambow. 2006. Inter-annotator agreement on a multilingual semantic annotation task. In Proceedings of the International Conference on Language Resources and Evaluation (LREC), pages 1951–1956, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
</authors>
<title>Computing reliability for coreference annotation.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation (LREC),</booktitle>
<contexts>
<context position="7499" citStr="Passonneau, 2004" startWordPosition="1190" endWordPosition="1191">r “fair.” 4 Observation: Varying Agreement, depending on Lexical Items We expected to find varying levels of interannotator agreement (IA) among all six annotators, depending on obvious grouping factors such as the part of speech, or the number of senses per word. We do find widely varying levels of agreement, but as described here, most of the variation does not depend on these a priori factors. Inherent usage properties of the words themselves, and systematic patterns of variation across annotators, seem to be the primary factors, with a secondary effect of part of speech. In previous work (Passonneau, 2004), we have discussed why we use Krippendorff’s α (Krippendorff, 1980), and for purposes of comparison we also report Cohen’s κ; note the similarity in values3. As with the various agreement coefficients that factor out the agreement that would occur by chance, values range from 1 for perfect agreement and -1 for perfect opposition, to 0 for chance agreement. While there are no hard and fast criteria for what constitutes good IA, Landis and Koch (Landis and Koch, 1977) consider values between 0.40 and 0.60 to represent moderately good agreement, and values above 0.60 as quite good; Krippendorff </context>
</contexts>
<marker>Passonneau, 2004</marker>
<rawString>Rebecca J. Passonneau. 2004. Computing reliability for coreference annotation. In Proceedings of the International Conference on Language Resources and Evaluation (LREC), Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>Assessing system agreement and instance difficulty in the lexical sample tasks of Senseval-2.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions,</booktitle>
<pages>40--46</pages>
<contexts>
<context position="3112" citStr="Pedersen, 2002" startWordPosition="479" endWordPosition="480"> matched subset of words. In Section 5 we discuss why we believe the observed variation depends on the words and present three lexical use factors we hypothesize to lead to greater or lesser IA. In Section 6, we use association rules to mine our data for systematic differences among annotators, thus to explain the variations in IA. We conclude with a summary of our findings goals. 2 Related Work There has been a decade-long community-wide effort to evaluate word sense disambiguation (WSD) systems across languages in the four Senseval efforts (1998, 2001, 2004, and 2007, cf. (Kilgarriff, 1998; Pedersen, 2002a; Pedersen, 2002b; Palmer et al., 2005)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 2–9, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Word POS No. senses No. occurrences fair Adj 10 463 long Adj 9 2706 qui</context>
<context position="19518" citStr="Pedersen, 2002" startWordPosition="3246" endWordPosition="3247">y of a DNA solution to the skin and that the injection paths did not reach into the underlying tissue. WN S2.establish the validity of something, as by an example, explanation or experiment, N=3 of 6 WN S3.provide evidence for, N=3 of 6 6 5.3 Quantifying Sense Similarity Application of an inter-sense similarity measure (ISM) proposed in (Ide, 2006) to the sense inventories for each of the six words supports the observation that words with very similar senses have lower IA scores. ISM is computed for each pair in a given word’s sense inventory, using a variant of the lesk measure (Banerjee and Pedersen, 2002). Agglomerative clustering may then be applied to the resulting similarity matrix to reveal the overall pattern of inter-sense relations. ISMs for senses pairs of long, fair, work, land, tell, and show range from 0 to 1.44.4 We compute a confusion threshhold CT based on the ISMs for all 250 sense pairs as CT = µA + 2QA where A is the sum of the ISMs for the six words’ 250 sense pairs. Table 3 shows the ISM statistics for the six words. The values show that the ISMs for work and long are significantly lower than for land and fair. The ISMs for the two verbs in the study, show and tell, are dist</context>
</contexts>
<marker>Pedersen, 2002</marker>
<rawString>Ted Pedersen. 2002a. Assessing system agreement and instance difficulty in the lexical sample tasks of Senseval-2. In Proceedings of the ACL-02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, pages 40–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>Evaluating the effectiveness of ensembles of decision trees in disambiguating SENSEVAL lexical samples.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions,</booktitle>
<pages>81--87</pages>
<contexts>
<context position="3112" citStr="Pedersen, 2002" startWordPosition="479" endWordPosition="480"> matched subset of words. In Section 5 we discuss why we believe the observed variation depends on the words and present three lexical use factors we hypothesize to lead to greater or lesser IA. In Section 6, we use association rules to mine our data for systematic differences among annotators, thus to explain the variations in IA. We conclude with a summary of our findings goals. 2 Related Work There has been a decade-long community-wide effort to evaluate word sense disambiguation (WSD) systems across languages in the four Senseval efforts (1998, 2001, 2004, and 2007, cf. (Kilgarriff, 1998; Pedersen, 2002a; Pedersen, 2002b; Palmer et al., 2005)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 2–9, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Word POS No. senses No. occurrences fair Adj 10 463 long Adj 9 2706 qui</context>
<context position="19518" citStr="Pedersen, 2002" startWordPosition="3246" endWordPosition="3247">y of a DNA solution to the skin and that the injection paths did not reach into the underlying tissue. WN S2.establish the validity of something, as by an example, explanation or experiment, N=3 of 6 WN S3.provide evidence for, N=3 of 6 6 5.3 Quantifying Sense Similarity Application of an inter-sense similarity measure (ISM) proposed in (Ide, 2006) to the sense inventories for each of the six words supports the observation that words with very similar senses have lower IA scores. ISM is computed for each pair in a given word’s sense inventory, using a variant of the lesk measure (Banerjee and Pedersen, 2002). Agglomerative clustering may then be applied to the resulting similarity matrix to reveal the overall pattern of inter-sense relations. ISMs for senses pairs of long, fair, work, land, tell, and show range from 0 to 1.44.4 We compute a confusion threshhold CT based on the ISMs for all 250 sense pairs as CT = µA + 2QA where A is the sum of the ISMs for the six words’ 250 sense pairs. Table 3 shows the ISM statistics for the six words. The values show that the ISMs for work and long are significantly lower than for land and fair. The ISMs for the two verbs in the study, show and tell, are dist</context>
</contexts>
<marker>Pedersen, 2002</marker>
<rawString>Ted Pedersen. 2002b. Evaluating the effectiveness of ensembles of decision trees in disambiguating SENSEVAL lexical samples. In Proceedings of the ACL02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, pages 81–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>The generative lexicon.</title>
<date>1991</date>
<journal>Computational Linguitics,</journal>
<volume>17</volume>
<issue>4</issue>
<contexts>
<context position="1848" citStr="Pustejovsky, 1991" startWordPosition="269" endWordPosition="270">es in word usage. It has been widely observed that usage features such as vocabulary and syntax vary across corpora of different genres and registers (Biber, 1995), and that serve different functions (Kittredge et al., 1991). Still, we are far from able to predict specific morphosyntactic and lexical variations across corpora (Kilgarriff, 2001), much less quantify them in a way that makes it possible to apply the same analysis tools (taggers, parsers) without retraining. In comparison to morphosyntactic properties of language, word and phrasal meaning is fluid, and to some degree, generative (Pustejovsky, 1991; Nunberg, 1979). Based on our initial observations from a word sense annotation task for relatively polysemous words, carried out by multiple annotators on a heterogeneous corpus, we hypothesize that different words lead to greater or lesser interannotator agreement (IA) for reasons that in the long run should be explicitly modelled in order for Natural Language Processing (NLP) applications to handle usage differences more robustly. This pilot study is a step in that direction. We present related work in the next section, then describe the annotation task in the following one. In Section 4, </context>
<context position="13786" citStr="Pustejovsky, 1991" startWordPosition="2255" endWordPosition="2256"> are concrete and specific, with a secondary effect that adjectives have higher IA overall than the other parts of speech. Sentences such as (1.), where a specific unit of temporal or spatial measurement is mentioned (months), restrict the sense to extent in space or time. 1. For 18 long months Michael could not find a job. WN S1. temporal extent [N=6 of 6] In the few cases where annotators disagree on long, the context is less specific or less concrete. In example (2.), long is predicated of the word chapter, which has non-concrete senses that exemplify a certain type of productive polysemy (Pustejovsky, 1991). It can be taken to refer to a physical object (a specific set of pages in an actual book), or a conceptual object (the abstract literary work). The adjective inherits this polysemy. The three annotators who agree on sense two (spatial extent) might have the physical object sense in mind; the two who select sense one (temporal extent) possibly took the point of view of the reader who requires a long time to read the chapter. 2. After I had submitted the manuscript my editor at Simon Schuster had suggested a number of cuts to streamline what was already a long and involved chapter on Brians id</context>
</contexts>
<marker>Pustejovsky, 1991</marker>
<rawString>James Pustejovsky. 1991. The generative lexicon. Computational Linguitics, 17(4):409–441.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Josef Ruppenhofer</author>
<author>Michael Ellsworth</author>
<author>Miriam R L Petruck</author>
<author>Christopher R Johnson</author>
</authors>
<location>and</location>
<marker>Ruppenhofer, Ellsworth, Petruck, Johnson, </marker>
<rawString>Josef Ruppenhofer, Michael Ellsworth, Miriam R. L. Petruck, Christopher R. Johnson, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Scheffczyk</author>
</authors>
<title>Framenet ii: Extended theory and practice. Available from http://framenet.icsi.berkeley.edu/index.php.</title>
<date>2006</date>
<marker>Scheffczyk, 2006</marker>
<rawString>Jan Scheffczyk. 2006. Framenet ii: Extended theory and practice. Available from http://framenet.icsi.berkeley.edu/index.php.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ansaf Salleb-Aouissi</author>
<author>Christel Vrain</author>
<author>Cyril Nortet</author>
</authors>
<title>Quantminer: A genetic algorithm for mining quantitative association rules.</title>
<date>2007</date>
<booktitle>In IJCAI,</booktitle>
<pages>1035--1040</pages>
<marker>Salleb-Aouissi, Vrain, Nortet, 2007</marker>
<rawString>Ansaf Salleb-Aouissi, Christel Vrain, and Cyril Nortet. 2007. Quantminer: A genetic algorithm for mining quantitative association rules. In IJCAI, pages 1035– 1040.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning to merge word senses.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1005--1014</pages>
<location>Prague.</location>
<contexts>
<context position="4314" citStr="Snow et al., 2007" startWordPosition="676" endWordPosition="679">g Adj 9 2706 quiet Adj 6 244 land Noun 11 1288 time Noun 10 21790 work Noun 7 5780 know Verb 11 10334 say Verb 11 20372 show Verb 12 11877 tell Verb 8 4799 Table 1: Ten Words 2005). Pedersen (Pedersen, 2002a) examines variation across individual words in evaluating WSD systems, but does not attempt to explain it. Factors that have been proposed as affecting human or system sense disambiguation include whether annotators are allowed to assign multilabels (Veronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), sense perplexity (Diab, 2004), entropy (Diab, 2004; Palmer et al., 2005), and in psycholinguistic experiments, reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006). With respect to using multiple annotators, Snow et al. included disambiguation of the word president–a relatively non-polysemous word with three senses–in a set of tasks given to Amazon Mechanical Turkers, aimed at determining how to combine data from multiple non-experts for machine learning tasks. The word sense task comprised 177 sentences taken fr</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2007</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2007. Learning to merge word senses. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1005–1014, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Veronis</author>
</authors>
<title>A study of polysemy judgements and inter-annotator agreement.</title>
<date>1998</date>
<booktitle>In SENSEVAL Workshop,</booktitle>
<pages>pages</pages>
<location>Sussex, England.</location>
<contexts>
<context position="4168" citStr="Veronis, 1998" startWordPosition="652" endWordPosition="653">s 2–9, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Word POS No. senses No. occurrences fair Adj 10 463 long Adj 9 2706 quiet Adj 6 244 land Noun 11 1288 time Noun 10 21790 work Noun 7 5780 know Verb 11 10334 say Verb 11 20372 show Verb 12 11877 tell Verb 8 4799 Table 1: Ten Words 2005). Pedersen (Pedersen, 2002a) examines variation across individual words in evaluating WSD systems, but does not attempt to explain it. Factors that have been proposed as affecting human or system sense disambiguation include whether annotators are allowed to assign multilabels (Veronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), sense perplexity (Diab, 2004), entropy (Diab, 2004; Palmer et al., 2005), and in psycholinguistic experiments, reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006). With respect to using multiple annotators, Snow et al. included disambiguation of the word president–a relatively non-polysemous word with three senses–in a set of tasks given to Amazon Mechanical Turkers, a</context>
</contexts>
<marker>Veronis, 1998</marker>
<rawString>Jean Veronis. 1998. A study of polysemy judgements and inter-annotator agreement. In SENSEVAL Workshop, pages Sussex, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammed Javeed Zaki</author>
<author>Srinivasan Parthasarathy</author>
<author>Mitsunori Ogihara</author>
<author>Wei Li</author>
</authors>
<title>New algorithms for fast discovery of association rules.</title>
<date>1997</date>
<booktitle>In KDD,</booktitle>
<pages>283--286</pages>
<contexts>
<context position="21456" citStr="Zaki et al., 1997" startWordPosition="3607" endWordPosition="3610">on rules express relations among instances based on their attributes. Here the attributes of interest are 4Note that because the scores are based on overlaps among WordNet relations, glosses, examples, etc., there is no predefined ceiling value for the ISMs. For the words in this study, we compute a ceiling value by taking the maximum of the ISMs for each of the 57 senses with itself, 4.85 in this case. the annotators who choose one sense versus those who choose another. Mining association rules to find strong relations has been studied in many domains (see for instance (Agrawal et al., 1993; Zaki et al., 1997; SallebAouissi et al., 2007)). Here we illustrate how association rules can be used to mine relations such as systematic differences in word sense choices across annotators. An association rule is an expression C1 C2, where C1 and C2 express conditions on features describing the instances in a dataset. The strength of the rules is usually evaluated by means of measures such as Support (Supp) and Confidence (Conf). Where C, C1 and C2 express conditions on attributes: • Supp(C) is the fraction of instances satisfying C • Supp(C1 C2) = Supp(C1 n C2) • Conf(C1 C2) = Supp(C1 n C2)/Supp(C1) Given t</context>
</contexts>
<marker>Zaki, Parthasarathy, Ogihara, Li, 1997</marker>
<rawString>Mohammed Javeed Zaki, Srinivasan Parthasarathy, Mitsunori Ogihara, and Wei Li. 1997. New algorithms for fast discovery of association rules. In KDD, pages 283–286.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>