<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.103702">
<title confidence="0.947566">
Domain-Independent Captioning of Domain-Specific Images
</title>
<author confidence="0.894577">
Rebecca Mason
</author>
<affiliation confidence="0.6713215">
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University, Providence, RI 02912
</affiliation>
<email confidence="0.996769">
rebecca@cs.brown.edu
</email>
<sectionHeader confidence="0.998589" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999615238095238">
Automatically describing visual content is an
extremely difficult task, with hard AI prob-
lems in Computer Vision (CV) and Natural
Language Processing (NLP) at its core. Pre-
vious work relies on supervised visual recog-
nition systems to determine the content of im-
ages. These systems require massive amounts
of hand-labeled data for training, so the num-
ber of visual classes that can be recognized is
typically very small. We argue that these ap-
proaches place unrealistic limits on the kinds
of images that can be captioned, and are un-
likely to produce captions which reflect hu-
man interpretations.
We present a framework for image caption
generation that does not rely on visual recog-
nition systems, which we have implemented
on a dataset of online shopping images and
product descriptions. We propose future work
to improve this method, and extensions for
other domains of images and natural text.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955">
As the number of images on the web continues to in-
crease, the task of automatically describing images
becomes especially important. Image captions can
provide background information about what is seen
in the image, can improve accessibility of websites
for visually-impaired users, and can improve im-
age retrieval by providing text to search user queries
against. Typically, online search engines rely on col-
located textual information to resolve queries, rather
than analyzing visual content directly. Likewise,
earlier image captioning research from the Natural
</bodyText>
<page confidence="0.989668">
69
</page>
<bodyText confidence="0.999735848484848">
Language Processing (NLP) community use collo-
cated information such as news articles or GPS co-
ordinates, to decide what information to include in
the generated caption (Deschacht and Moens, 2007;
Aker and Gaizauskas, 2010; Fan et al., 2010; Feng
and Lapata, 2010a).
However, in some instances visual recognition is
necessary because collocated information is miss-
ing, irrelevant, or unreliable. Recognition is a clas-
sic Computer Vision (CV) problem including tasks
such as recognizing instances of object classes in
images (such as car, cat, or sofa); classifying
images by scene (such as beach or forest); or
detecting attributes in an image (such as wooden
or feathered). Recent works in image caption
generation represent visual content via the output
of trained recognition systems for a pre-defined set
of visual classes. They then use linguistic models
to correct noisy initial detections (Kulkarni et al.,
2011; Yang et al., 2011), and generate more natural-
sounding text (Li et al., 2011; Mitchell et al., 2012;
Kuznetsova et al., 2012).
A key problem with this approach is that it as-
sumes that image captioning is a grounding prob-
lem, with language acting only as labels for visual
meaning. One good reason to challenge this assump-
tion is that it imposes unrealistic constraints on the
kinds of images that can be automatically described.
Previous work only recognizes a limited number of
visual classes – typically no more than a few dozen
in total – because training CV systems requires a
huge amount of hand-annotated data. For example,
the PASCAL VOC dataset1 has 11,530 training im-
</bodyText>
<footnote confidence="0.989583">
1http://pascallin.ecs.soton.ac.uk/
</footnote>
<note confidence="0.938074">
Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 69–76,
Atlanta, Georgia, 13 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.991948059701493">
ages with 27,450 labeled objects, in order to learn
only 20 object classes. Since training visual recog-
nition systems is such a burden, “general-domain”
image captioning datasets are limited by the current
technology. For example, the SBU-Flickr dataset
(Ordonez et al., 2011), which contains 1 million im-
ages and captions, is built by first querying Flickr
using a pre-defined set of queries, then further filter-
ing to remove instances where the caption does not
contain at least two words belonging to their term
list. Furthermore, detections are too noisy to gener-
ate a good caption for the majority of images. For
example, Kuznetsova et al. (2012) select their test
set according to which images receive the most con-
fident visual object detection scores.
We instead direct our attention to the domain-
specific image captioning task, assuming that we
know a general object or scene category for the
query image, and that we have access to a dataset of
images and captions from the same domain. While
some techniques may be unrealistic in assuming that
high-quality collocated text is always available, as-
suming that there is no collocated information at
all is equally unrealistic. Data sources such as
file names, website text, Facebook likes, and web
searches all provide clues to the content of an im-
age. Even an image file by itself carries metadata
on where and when it was taken, and the camera
settings used to take it. Since visual recognition is
much easier for domain-specific tasks, there is more
potential for natural language researchers to do re-
search that will impact the greater community.
Finally, labeling visual content is often not
enough to provide an adequate caption. The mean-
ing of an image to a user is more than just listing the
objects in the image, and can even change for dif-
ferent users. This problem is commonly known as
“bridging the semantic gap”:
“The semantic gap is the lack of coinci-
dence between the information that one
can extract from the visual data and the
interpretation that the same data have for
a user in a given situation. A linguis-
tic description is almost always contex-
tual, whereas an image may live by itself.”
(Smeulders et al., 2000)
challenges/VOC/
General-domain models of caption generation fail to
capture context because they assume that all the rel-
evant information has been provided in the image.
However, training models on data from the same do-
main gives implicit context about what information
should be provided in the generated text.
This thesis proposes a framework for image cap-
tioning that does not require supervision in the form
of hand-labeled examples. We train a topic model on
a corpus of images and captions in the same domain,
in order to jointly learn image features and natural
language descriptions. The trained topic model is
used to estimate the likelihood of words appearing
in a caption, given an unseen query image. We then
use these likelihoods to rewrite an extracted human-
written caption to accurately describe the query im-
age. We have implemented our framework using a
dataset of online shopping images and captions, and
propose to extend this model to other domains, in-
cluding natural images.
</bodyText>
<sectionHeader confidence="0.994388" genericHeader="introduction">
2 Framework
</sectionHeader>
<bodyText confidence="0.999916173913044">
In this section, we provide an overview of our im-
age captioning framework, as it is currently imple-
mented. As shown in Figure 1, the data that we use
are a set of images and captions in a specific do-
main, and a query image that is from the same do-
main, but is not included in the training data. The
training data is used in two ways: for sentence ex-
traction from the captions of training images that
are visually similar to the query image overall; and
for training a topic model of individual words and
local image features, in order to capture fine-grained
details. Finally, a sentence compression algorithm
is used to remove details from the extracted captions
that do not fit the query image.
The work that we have done so far has been imple-
mented using the Attribute Discovery Dataset (Berg
et al., 2010), a publicly available dataset of shop-
ping images and product descriptions.2 Here, we
run our framework on the women’s shoes section,
which has over 14000 images and captions, rep-
resenting a wide variety of attributes for texture,
shapes, materials, colors, and other visual quali-
ties. The women’s shoes section is formally split
</bodyText>
<footnote confidence="0.954311">
2http://tamaraberg.com/
attributesDataset/index.html
</footnote>
<page confidence="0.995494">
70
</page>
<figureCaption confidence="0.999894">
Figure 1: Overview of our framework for image caption generation.
</figureCaption>
<bodyText confidence="0.999974933333333">
into ten subcategories, such as wedding shoes,
sneakers, and rainboots. However, many of
the subcategories contain multiple visually distinct
kinds of shoes. We do not make use of the sub-
categories, instead we group all of the categories of
shoe images together. The shoes in the images are
mostly posed against solid color backgrounds, while
the captions have much more variability in length
and linguistic quality.
For our thesis work, we intend to extend our cur-
rent framework to different domains of data, includ-
ing natural images. However, it is important to point
out that no part of the framework as it is currently
implemented is specific to describing shoes or shop-
ping images. This will be described in Section 4.
</bodyText>
<subsectionHeader confidence="0.99848">
2.1 Sentence Extraction
</subsectionHeader>
<bodyText confidence="0.9999695625">
GIST (Oliva and Torralba, 2001) is a global image
descriptor which describes how gradients are ori-
ented in different regions of an image. It is com-
monly used for classifying background scenes in
images, however images in the Attribute Discovery
Dataset do not have “backgrounds” per se. Instead,
we treat the overall shape of the object as the “scene”
and extract a caption sentence using GIST nearest
neighbors between the query image and the images
in the training set. Because similar objects and at-
tributes tend to appear in similar scenes, we expect
that at least some of the extracted caption will de-
scribe local attributes that are also in the query im-
age. The rest of our framework finds and removes
the parts of the extracted caption that are not accu-
rate to the query image.
</bodyText>
<subsectionHeader confidence="0.989079">
2.2 Topic Model
</subsectionHeader>
<bodyText confidence="0.999991176470588">
Image captions often act as more than labels of vi-
sual content. Some visual ideas can be described
using several different words, while others are typ-
ically not described at all. Likewise, some words
describe background information that is not shown
visually, or contextual information that is interpreted
by the user. Rather than modeling images and text
such that one generates the other, we a topic model
based on LDA (Blei et al., 2003) where both an im-
age and its caption are generated by a shared latent
distribution of topics.
Previous work by (Feng and Lapata, 2010b)
shows that topic models where image features or re-
gions generate text features (such as Blei and Jor-
dan (2003)) are not appropriate for modeling images
with captions or other collocated text. We use a topic
model designed for multi-lingual data, specifically
the Polylingual Topic Model (Mimno et al., 2009).
This model was developed for correlated documents
in different languages that are topically similar, but
are not direct translations, such as Wikipedia or
news articles in different languages. We train the
topic model with images and text as two languages.
For query images, we estimate the topic distribu-
tion that generated just the image, and then In the
model, images and their captions are represented us-
ing bag-of-words, a commonly-used technique for
document representation in both CV and NLP re-
search. The textual features are non-function words
in the model, including words that describe specific
objects or attributes (such as boot, snake-skin,
buckle, and metallic) in addition to words that
describe more abstract attributes and affordances
(such as professional, flirty, support,
</bodyText>
<page confidence="0.988059">
71
</page>
<table confidence="0.999106785714286">
Original: Go all-out glam in the shimmer- Original: Find the softness of shearling combined with sup- Original: Perforated leather with cap toe and
ing Dyeables Roxie sandals. Metallic faux port in this clog slipper. The cork footbed mimics the foot’s bow detail.
natural shape, offering arch support, while a flexible outsole
flexes with your steps and resists slips....
leather upper in a dress thong
PP on g sandal style
with a round open toe. ...
Extracted: Shimmering snake- Extracted: This sporty sneaker clog Extracted: Italian patent leather peep-
embossed leather upper in a slingback keeps foot cool and comfortable and toe ballet flat with a signature tailored
evening dress sandal style with a round fully supported. grosgrain bow.
open toe .
System: Shimmering upper in a sling- System: This clog keeps foot comfort- System: leather ballet flat with a signa-
back evening dress sandal style with a able and supported. ture tailored grosgrain bow .
round open toe .
</table>
<tableCaption confidence="0.908613666666667">
Table 1: Some examples of shoes images from the Attribute Discovery Dataset and performance with our image
captioning model. Left: Correctly removes explicitly visual feature “snake-embossed leather” from extraction; leaves
in correct visual attributes “shimmering”, “slingback”, and “round open toe”. Center: Extracted sentence with some
contextually visual attributes; the model correctly infers that “sporty” and “cool” are not likely given an image of a
wool bedroom slipper, but “comfortable” and “supported” are likely because of the visible cork soles. Right: Extracted
sentence with some non-visual attributes; model removes “Italian” but keeps “signature tailored”.
</tableCaption>
<bodyText confidence="0.999319727272727">
and waterproof). For “image words”, we com-
pute features at several points in the image such as
the color values of pixels, the angles of edges or
corners, and response to various filters, and cluster
them into discrete image words. However, the in-
formation that an image word conveys is very dif-
ferent than the information conveyed in a text word,
so models which require direct correspondence be-
tween features in the two modalities would not be
appropriate here.
We train the topic model with images and text as
two languages. We estimate the probabilities of tex-
tual words given a query image by first estimating
the topic distribution that generated the image, and
then using the same distribution to find the probabil-
ities of textual words given the query image. How-
ever, we also perform an annotation task similarly
to Feng and Lapata (2010b), in order to evaluate
the topic model on its own. Our method has a 30-
35% improvement in finding words from the held-
out image caption, compared to previous methods
and baselines.
</bodyText>
<subsectionHeader confidence="0.9116405">
2.3 Sentence Compression via Caption
Generation
</subsectionHeader>
<bodyText confidence="0.999979777777778">
We describe an ILP for caption generation, draw-
ing inspiration from sentence compression work by
Clarke and Lapata (2008). The ILP has three in-
puts: the extracted caption; the prior probabilities
words appearing in captions, p(w); and their pos-
terior probabilities of words appearing in captions
given the query image, p(w|query). The latter is
estimated using the topic model we have just de-
scribed. The output of the ILP is a compressed im-
age caption where the inaccurate words have been
deleted.
Objective: The formal ILP objective3 is to max-
imize a weighted linear combination of two mea-
sures. The first we define as Eni=1 Ji · I(wi), where
wi, ..., wn are words in the extracted caption, Ji is a
binary decision variable which is true if we include
wi in the compressed output, and I(wi) is a score for
the accuracy of each word. For non-function words,
</bodyText>
<footnote confidence="0.994505">
3To formulate this problem as a linear program, the proba-
bilities are actually log probabilities, but we omit the logs in this
paper to save space.
</footnote>
<page confidence="0.996319">
72
</page>
<bodyText confidence="0.999685263157895">
I(wi) = p(w|query) − p(w), which can have a pos-
itive or negative value. We do not use p(wi|query)
directly in order to distinguish between cases where
p(wi|query) is low because wi is inaccurate, and
cases where p(wi|query) is low because p(wi) is
low generally. Function words do not affect the ac-
curacy of the generated caption, so I(wi) = 0.
The second measure in the objective is a tri-
gram language model, described in detail in Clarke
(2008). In the original sentence compression task,
the language model is a component as it naturally
prefers shorter output sentences. However, our ob-
jective is not to generate a shorter caption, but to
generate a more accurate caption. However, we still
include the language model in the objective, with a
weighting factor E, as it helps remove unnecessary
function words and help reduce the search space of
possible sentence compressions.
Constraints: The ILP constraints include sequen-
tial constraints to ensure the mathematical validity
of the model, and syntactic constraints that ensure
the grammatical correctness of the compressed sen-
tence. We do not have space here to describe all
of the constraints, but basically, using the “semantic
head” version of the headfinder from Collins (1999),
we constrain that the head word of the sentence and
the head word of the sentence’s object cannot be
deleted, and for any word that we include in the out-
put sentence, we must include its head word as well.
We also have constraints that define valid use of co-
ordinating conjunctions and punctuation.
We evaluate generated captions using automatic
metrics such as BLEU (Papineni et al., 2002) and
ROUGE (Lin, 2004). These metrics are commonly
used in summarization and translation research and
have been previously used in image captioning re-
search to compare automatically generated captions
to human-written captions for each image (Ordonez
et al., 2011; Yang et al., 2011; Kuznetsova et al.,
2012). Although human-written captions may use
synonyms to describe a visual object or attribute, or
even describe entirely different attributes than what
is described in the generated captions, computing
the automatic metrics over a large test set finds sta-
tistically significant improvements in the accuracy
of the extracted and compressed captions over ex-
traction alone.
For our proposed work (Section 4), we also plan
to perform manual evaluations of our captions based
on their content and language quality. However,
cross-system comparisons would be more difficult
because our method uses an entirely different kind
of data. In order to compare our work to related
methods (Section 3), we would have to train for vi-
sual recognition systems for hundreds of visual at-
tributes, which would mean having to hand-label the
entire dataset.
</bodyText>
<sectionHeader confidence="0.999739" genericHeader="related work">
3 Related Work in Image Captioning
</sectionHeader>
<bodyText confidence="0.997564166666667">
In addition to visual recognition, caption genera-
tion is a very challenging problem. In some ap-
proaches, sentences are constructed using templates
or grammar rules, where content words are selected
according to the output of visual recognition systems
(Kulkarni et al., 2011; Yang et al., 2011; Mitchell et
al., 2012). Function words, as well as words like
verbs and prepositions which are difficult to rec-
ognize visually, may be selected using a language
model trained on non-visual text. There is also simi-
lar work that uses large-scale ngram models to make
the generated output sound more natural (Li et al.,
2011).
In other approaches, captions are extracted in
whole or in part from similar images in a database.
For example, Farhadi et al. (2010) and Ordonez et
al. (2011) build semantic representations for visual
content of query images, and extract captions from
database images with similar content. Kuznetsova et
al. (2012) extract phrases corresponding to classes of
objects and scenes detected in the query image, and
combine extracted phrases into a single sentence.
Our work is different than these approaches, because
we directly measure how visually relevant individual
words are, rather than only using visual similarity to
extract sentences or phrases.
Our method is most similar to that of Feng and
Lapata (2010a), who generate captions for news im-
ages. Like them, we train an LDA-like model on
both images and text to find latent topics that gener-
ate both. However, their model requires both an im-
age and collocated text (a news article) to estimate
the topic distribution for an unseen image, while our
topic model only needs related text for the training
data. They also use the news article to help gen-
erate captions, which means that optimizing their
</bodyText>
<page confidence="0.997412">
73
</page>
<bodyText confidence="0.999923380952381">
generated output for content and grammaticality is
a much easier problem. Although their model com-
bines phrases and n-grams from different sentences
to form an image caption, they only consider the text
from a single news article for extraction, and they
can assume that the text is mostly accurate and rele-
vant to the content of the image.
In this sense, our method is more like Kuznetsova
et al. (2012), which also uses an Integer Linear Pro-
gram (ILP) to rapidly optimize how well their gen-
erated caption fits the content of the image model.
However, it is easier to get coherent image captions
from our model since we are not combining parts
of sentences from multiple sources. Since we build
our output from extracted sentences, not phrases, our
ILP requires fewer grammaticality and coherence
constraints than it would for building new sentences
from scratch. We also model how relevant each in-
dividual word is to the query image, while they ex-
tract phrases based on visual similarity of detected
objects in the images.
</bodyText>
<sectionHeader confidence="0.997725" genericHeader="conclusions">
4 Proposed Work
</sectionHeader>
<bodyText confidence="0.999968189655173">
One clear direction for future work is to extend our
image captioning framework to natural images. By
“natural images” we refer to images of everyday
scenes seen by people, unlike the shopping images,
where objects tend to be posed in similar positions
against plain backgrounds. Instead of domains such
as handbags and shoes, we propose to cluster the
training data based on visual scene domains such as
mountains, beaches, and living rooms. We are par-
ticularly interested in the scene attributes and clas-
sifiers by Patterson and Hays (2012) which builds
an attribute-based taxonomy of scene types using
crowd-sourcing, rather than categorical scene types
which are typically used.
Visual recognition is generally much more diffi-
cult in natural scenes than in posed images, since
lighting and viewpoints are not consistent, and ob-
jects may be occluded by other objects or truncated
by the edge of the image. However, we are opti-
mistic because we do not need to solve the general
visual recognition task, since our model only learns
how visual objects and attributes appear in specific
domains of scenes, a much easier problem. Addi-
tionally, the space of likely objects and attributes to
detect is limited by what typically appears in that
type of scene. Finally, we can use the fact that our
image captioning method is not grounded in our fa-
vor, and assume that if an object is partially occluded
or truncated in an image, than it is less likely that
the photographer considered that object to be inter-
esting, so it is not as important whether that object
is described in the caption or not.
Finally, there is also much that could be done to
improve the text generation component on its own.
Our framework currently extracts only a single cap-
tion sentence to compress, while recent work in
summarization has focused on the problem of learn-
ing how to jointly extract and compress (Martins and
Smith, 2009; Berg-Kirkpatrick et al., 2011). Since
a poor extraction choice can make finding an accu-
rate compression impossible, we should also study
different methods of extraction to learn about what
kinds of features are most likely to help us find good
sentences. As mentioned in Section 2.1, we have
already found that global feature descriptors are bet-
ter than bag of image word descriptors for extract-
ing sentences to use in image caption compressions
in the shopping dataset. As we extend our frame-
work to other domains of images, we are interested
in finding whether scene-based descriptors and clas-
sifiers in general are better at finding good sentences
than local descriptors, and whether there is a con-
nection between region and phrase-based detectors
correlating better with sentence and phrase-length
text, while local image descriptors are more related
to single words. Finding patterns like this in visual
text in general would be helpful for many other tasks
besides image captioning.
</bodyText>
<sectionHeader confidence="0.997752" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.987059583333333">
Ahmet Aker and Robert Gaizauskas. 2010. Generating
image descriptions using dependency relational pat-
terns. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, ACL
’10, pages 1250–1258, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Tamara L. Berg, Alexander C. Berg, and Jonathan Shih.
2010. Automatic attribute discovery and character-
ization from noisy web data. In Proceedings of
the 11th European conference on Computer vision:
Part I, ECCV’10, pages 663–676, Berlin, Heidelberg.
Springer-Verlag.
</reference>
<page confidence="0.992047">
74
</page>
<reference confidence="0.999209419047619">
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ’11, pages 481–
490, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
David M. Blei and Michael I. Jordan. 2003. Modeling
annotated data. In Proceedings of the 26th annual in-
ternational ACM SIGIR conference on Research and
development in informaion retrieval, SIGIR ’03, pages
127–134, New York, NY, USA. ACM.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022, March.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression an integer linear pro-
gramming approach. J. Artif. Int. Res., 31(1):399–429,
March.
James Clarke. 2008. Global Inference for Sentence Com-
pression: An Integer Linear Programming Approach.
Dissertation, University of Edinburgh.
Michael John Collins. 1999. Head-driven statistical
models for natural language parsing. Ph.D. thesis,
Philadelphia, PA, USA. AAI9926110.
Koen Deschacht and Marie-Francine Moens. 2007. Text
analysis for automatic image annotation. In ACL, vol-
ume 45, page 1000.
Xin Fan, Ahmet Aker, Martin Tomko, Philip Smart, Mark
Sanderson, and Robert Gaizauskas. 2010. Automatic
image captioning from the web for gps photographs.
In Proceedings of the international conference on Mul-
timedia information retrieval, MIR ’10, pages 445–
448, New York, NY, USA. ACM.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi,
Peter Young, Cyrus Rashtchian, Julia Hockenmaier,
and David Forsyth. 2010. Every picture tells a story:
generating sentences from images. In Proceedings of
the 11th European conference on Computer vision:
Part IV, ECCV’10, pages 15–29, Berlin, Heidelberg.
Springer-Verlag.
Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? automatic caption gener-
ation for news images. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ’10, pages 1239–1249, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Yansong Feng and Mirella Lapata. 2010b. Topic mod-
els for image annotation and text illustration. In HLT-
NAACL, pages 831–839.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and generat-
ing simple image descriptions. In CVPR, pages 1601–
1608.
Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg,
Tamara L. Berg, and Yejin Choi. 2012. Collective
generation of natural image descriptions. In ACL.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing
simple image descriptions using web-scale n-grams.
In Proceedings of the Fifteenth Conference on Com-
putational Natural Language Learning, CoNLL ’11,
pages 220–228, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Stan Szpakowicz Marie-
Francine Moens, editor, Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pages 74–
81, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Andr´e F. T. Martins and Noah A. Smith. 2009. Summa-
rization with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on In-
teger Linear Programming for Natural Langauge Pro-
cessing, ILP ’09, pages 1–9, Stroudsburg, PA, USA.
Association for Computational Linguistics.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 2 - Volume 2, EMNLP ’09,
pages 880–889, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Xufeng Han, Alyssa Mensch,
Alexander C. Berg, Tamara L. Berg, and Hal Daum´e
III. 2012. Midge: Generating image descriptions
from computer vision detections. In European Chap-
ter of the Association for Computational Linguistics
(EACL).
Aude Oliva and Antonio Torralba. 2001. Modeling the
shape of the scene: A holistic representation of the
spatial envelope. International Journal of Computer
Vision, 42:145–175.
V. Ordonez, G. Kulkarni, and T.L. Berg. 2011. Im2text:
Describing images using 1 million captioned pho-
tographs. In NIPS.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ’02, pages 311–318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.979357">
75
</page>
<reference confidence="0.999653285714286">
G. Patterson and J. Hays. 2012. Sun attribute database:
Discovering, annotating, and recognizing scene at-
tributes. 2012 IEEE Conference on Computer Vision
and Pattern Recognition, 0:2751–2758.
Arnold W. M. Smeulders, Marcel Worring, Simone San-
tini, Amarnath Gupta, and Ramesh Jain. 2000.
Content-based image retrieval at the end of the early
years. IEEE Trans. Pattern Anal. Mach. Intell.,
22(12):1349–1380, December.
Yezhou Yang, Ching Lik Teo, Hal Daum´e III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods in
Natural Language Processing (EMNLP), Edinburgh,
Scotland.
</reference>
<page confidence="0.991751">
76
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.773999">
<title confidence="0.999174">Domain-Independent Captioning of Domain-Specific Images</title>
<author confidence="0.885723">Rebecca</author>
<affiliation confidence="0.893461">Brown Laboratory for Linguistic Information Processing Brown University, Providence, RI</affiliation>
<email confidence="0.99906">rebecca@cs.brown.edu</email>
<abstract confidence="0.999145409090909">Automatically describing visual content is an extremely difficult task, with hard AI problems in Computer Vision (CV) and Natural Language Processing (NLP) at its core. Previous work relies on supervised visual recognition systems to determine the content of images. These systems require massive amounts of hand-labeled data for training, so the number of visual classes that can be recognized is typically very small. We argue that these approaches place unrealistic limits on the kinds of images that can be captioned, and are unlikely to produce captions which reflect human interpretations. We present a framework for image caption generation that does not rely on visual recognition systems, which we have implemented on a dataset of online shopping images and product descriptions. We propose future work to improve this method, and extensions for other domains of images and natural text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ahmet Aker</author>
<author>Robert Gaizauskas</author>
</authors>
<title>Generating image descriptions using dependency relational patterns.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>1250--1258</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1902" citStr="Aker and Gaizauskas, 2010" startWordPosition="284" endWordPosition="287">ground information about what is seen in the image, can improve accessibility of websites for visually-impaired users, and can improve image retrieval by providing text to search user queries against. Typically, online search engines rely on collocated textual information to resolve queries, rather than analyzing visual content directly. Likewise, earlier image captioning research from the Natural 69 Language Processing (NLP) community use collocated information such as news articles or GPS coordinates, to decide what information to include in the generated caption (Deschacht and Moens, 2007; Aker and Gaizauskas, 2010; Fan et al., 2010; Feng and Lapata, 2010a). However, in some instances visual recognition is necessary because collocated information is missing, irrelevant, or unreliable. Recognition is a classic Computer Vision (CV) problem including tasks such as recognizing instances of object classes in images (such as car, cat, or sofa); classifying images by scene (such as beach or forest); or detecting attributes in an image (such as wooden or feathered). Recent works in image caption generation represent visual content via the output of trained recognition systems for a pre-defined set of visual cla</context>
</contexts>
<marker>Aker, Gaizauskas, 2010</marker>
<rawString>Ahmet Aker and Robert Gaizauskas. 2010. Generating image descriptions using dependency relational patterns. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 1250–1258, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara L Berg</author>
<author>Alexander C Berg</author>
<author>Jonathan Shih</author>
</authors>
<title>Automatic attribute discovery and characterization from noisy web data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th European conference on Computer vision: Part I, ECCV’10,</booktitle>
<pages>663--676</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="7472" citStr="Berg et al., 2010" startWordPosition="1213" endWordPosition="1216">cific domain, and a query image that is from the same domain, but is not included in the training data. The training data is used in two ways: for sentence extraction from the captions of training images that are visually similar to the query image overall; and for training a topic model of individual words and local image features, in order to capture fine-grained details. Finally, a sentence compression algorithm is used to remove details from the extracted captions that do not fit the query image. The work that we have done so far has been implemented using the Attribute Discovery Dataset (Berg et al., 2010), a publicly available dataset of shopping images and product descriptions.2 Here, we run our framework on the women’s shoes section, which has over 14000 images and captions, representing a wide variety of attributes for texture, shapes, materials, colors, and other visual qualities. The women’s shoes section is formally split 2http://tamaraberg.com/ attributesDataset/index.html 70 Figure 1: Overview of our framework for image caption generation. into ten subcategories, such as wedding shoes, sneakers, and rainboots. However, many of the subcategories contain multiple visually distinct kinds </context>
</contexts>
<marker>Berg, Berg, Shih, 2010</marker>
<rawString>Tamara L. Berg, Alexander C. Berg, and Jonathan Shih. 2010. Automatic attribute discovery and characterization from noisy web data. In Proceedings of the 11th European conference on Computer vision: Part I, ECCV’10, pages 663–676, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Gillick</author>
<author>Dan Klein</author>
</authors>
<title>Jointly learning to extract and compress.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>481--490</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="22451" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="3671" endWordPosition="3674">ing method is not grounded in our favor, and assume that if an object is partially occluded or truncated in an image, than it is less likely that the photographer considered that object to be interesting, so it is not as important whether that object is described in the caption or not. Finally, there is also much that could be done to improve the text generation component on its own. Our framework currently extracts only a single caption sentence to compress, while recent work in summarization has focused on the problem of learning how to jointly extract and compress (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011). Since a poor extraction choice can make finding an accurate compression impossible, we should also study different methods of extraction to learn about what kinds of features are most likely to help us find good sentences. As mentioned in Section 2.1, we have already found that global feature descriptors are better than bag of image word descriptors for extracting sentences to use in image caption compressions in the shopping dataset. As we extend our framework to other domains of images, we are interested in finding whether scene-based descriptors and classifiers in general are better at fi</context>
</contexts>
<marker>Berg-Kirkpatrick, Gillick, Klein, 2011</marker>
<rawString>Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 481– 490, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Michael I Jordan</author>
</authors>
<title>Modeling annotated data.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, SIGIR ’03,</booktitle>
<pages>127--134</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="10159" citStr="Blei and Jordan (2003)" startWordPosition="1659" endWordPosition="1663">Some visual ideas can be described using several different words, while others are typically not described at all. Likewise, some words describe background information that is not shown visually, or contextual information that is interpreted by the user. Rather than modeling images and text such that one generates the other, we a topic model based on LDA (Blei et al., 2003) where both an image and its caption are generated by a shared latent distribution of topics. Previous work by (Feng and Lapata, 2010b) shows that topic models where image features or regions generate text features (such as Blei and Jordan (2003)) are not appropriate for modeling images with captions or other collocated text. We use a topic model designed for multi-lingual data, specifically the Polylingual Topic Model (Mimno et al., 2009). This model was developed for correlated documents in different languages that are topically similar, but are not direct translations, such as Wikipedia or news articles in different languages. We train the topic model with images and text as two languages. For query images, we estimate the topic distribution that generated just the image, and then In the model, images and their captions are represe</context>
</contexts>
<marker>Blei, Jordan, 2003</marker>
<rawString>David M. Blei and Michael I. Jordan. 2003. Modeling annotated data. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, SIGIR ’03, pages 127–134, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--993</pages>
<contexts>
<context position="9913" citStr="Blei et al., 2003" startWordPosition="1616" endWordPosition="1619">ibutes that are also in the query image. The rest of our framework finds and removes the parts of the extracted caption that are not accurate to the query image. 2.2 Topic Model Image captions often act as more than labels of visual content. Some visual ideas can be described using several different words, while others are typically not described at all. Likewise, some words describe background information that is not shown visually, or contextual information that is interpreted by the user. Rather than modeling images and text such that one generates the other, we a topic model based on LDA (Blei et al., 2003) where both an image and its caption are generated by a shared latent distribution of topics. Previous work by (Feng and Lapata, 2010b) shows that topic models where image features or regions generate text features (such as Blei and Jordan (2003)) are not appropriate for modeling images with captions or other collocated text. We use a topic model designed for multi-lingual data, specifically the Polylingual Topic Model (Mimno et al., 2009). This model was developed for correlated documents in different languages that are topically similar, but are not direct translations, such as Wikipedia or </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Global inference for sentence compression an integer linear programming approach.</title>
<date>2008</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="14000" citStr="Clarke and Lapata (2008)" startWordPosition="2266" endWordPosition="2269">s given a query image by first estimating the topic distribution that generated the image, and then using the same distribution to find the probabilities of textual words given the query image. However, we also perform an annotation task similarly to Feng and Lapata (2010b), in order to evaluate the topic model on its own. Our method has a 30- 35% improvement in finding words from the heldout image caption, compared to previous methods and baselines. 2.3 Sentence Compression via Caption Generation We describe an ILP for caption generation, drawing inspiration from sentence compression work by Clarke and Lapata (2008). The ILP has three inputs: the extracted caption; the prior probabilities words appearing in captions, p(w); and their posterior probabilities of words appearing in captions given the query image, p(w|query). The latter is estimated using the topic model we have just described. The output of the ILP is a compressed image caption where the inaccurate words have been deleted. Objective: The formal ILP objective3 is to maximize a weighted linear combination of two measures. The first we define as Eni=1 Ji · I(wi), where wi, ..., wn are words in the extracted caption, Ji is a binary decision vari</context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>James Clarke and Mirella Lapata. 2008. Global inference for sentence compression an integer linear programming approach. J. Artif. Int. Res., 31(1):399–429, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
</authors>
<title>Global Inference for Sentence Compression: An Integer Linear Programming Approach. Dissertation,</title>
<date>2008</date>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="15335" citStr="Clarke (2008)" startWordPosition="2504" endWordPosition="2505">function words, 3To formulate this problem as a linear program, the probabilities are actually log probabilities, but we omit the logs in this paper to save space. 72 I(wi) = p(w|query) − p(w), which can have a positive or negative value. We do not use p(wi|query) directly in order to distinguish between cases where p(wi|query) is low because wi is inaccurate, and cases where p(wi|query) is low because p(wi) is low generally. Function words do not affect the accuracy of the generated caption, so I(wi) = 0. The second measure in the objective is a trigram language model, described in detail in Clarke (2008). In the original sentence compression task, the language model is a component as it naturally prefers shorter output sentences. However, our objective is not to generate a shorter caption, but to generate a more accurate caption. However, we still include the language model in the objective, with a weighting factor E, as it helps remove unnecessary function words and help reduce the search space of possible sentence compressions. Constraints: The ILP constraints include sequential constraints to ensure the mathematical validity of the model, and syntactic constraints that ensure the grammatic</context>
</contexts>
<marker>Clarke, 2008</marker>
<rawString>James Clarke. 2008. Global Inference for Sentence Compression: An Integer Linear Programming Approach. Dissertation, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<booktitle>Ph.D. thesis,</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="16126" citStr="Collins (1999)" startWordPosition="2627" endWordPosition="2628">ter caption, but to generate a more accurate caption. However, we still include the language model in the objective, with a weighting factor E, as it helps remove unnecessary function words and help reduce the search space of possible sentence compressions. Constraints: The ILP constraints include sequential constraints to ensure the mathematical validity of the model, and syntactic constraints that ensure the grammatical correctness of the compressed sentence. We do not have space here to describe all of the constraints, but basically, using the “semantic head” version of the headfinder from Collins (1999), we constrain that the head word of the sentence and the head word of the sentence’s object cannot be deleted, and for any word that we include in the output sentence, we must include its head word as well. We also have constraints that define valid use of coordinating conjunctions and punctuation. We evaluate generated captions using automatic metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). These metrics are commonly used in summarization and translation research and have been previously used in image captioning research to compare automatically generated captions to huma</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael John Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, Philadelphia, PA, USA. AAI9926110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koen Deschacht</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Text analysis for automatic image annotation.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<volume>45</volume>
<pages>1000</pages>
<contexts>
<context position="1875" citStr="Deschacht and Moens, 2007" startWordPosition="280" endWordPosition="283">e captions can provide background information about what is seen in the image, can improve accessibility of websites for visually-impaired users, and can improve image retrieval by providing text to search user queries against. Typically, online search engines rely on collocated textual information to resolve queries, rather than analyzing visual content directly. Likewise, earlier image captioning research from the Natural 69 Language Processing (NLP) community use collocated information such as news articles or GPS coordinates, to decide what information to include in the generated caption (Deschacht and Moens, 2007; Aker and Gaizauskas, 2010; Fan et al., 2010; Feng and Lapata, 2010a). However, in some instances visual recognition is necessary because collocated information is missing, irrelevant, or unreliable. Recognition is a classic Computer Vision (CV) problem including tasks such as recognizing instances of object classes in images (such as car, cat, or sofa); classifying images by scene (such as beach or forest); or detecting attributes in an image (such as wooden or feathered). Recent works in image caption generation represent visual content via the output of trained recognition systems for a pr</context>
</contexts>
<marker>Deschacht, Moens, 2007</marker>
<rawString>Koen Deschacht and Marie-Francine Moens. 2007. Text analysis for automatic image annotation. In ACL, volume 45, page 1000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Fan</author>
<author>Ahmet Aker</author>
<author>Martin Tomko</author>
<author>Philip Smart</author>
<author>Mark Sanderson</author>
<author>Robert Gaizauskas</author>
</authors>
<title>Automatic image captioning from the web for gps photographs.</title>
<date>2010</date>
<booktitle>In Proceedings of the international conference on Multimedia information retrieval, MIR ’10,</booktitle>
<pages>445--448</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1920" citStr="Fan et al., 2010" startWordPosition="288" endWordPosition="291">at is seen in the image, can improve accessibility of websites for visually-impaired users, and can improve image retrieval by providing text to search user queries against. Typically, online search engines rely on collocated textual information to resolve queries, rather than analyzing visual content directly. Likewise, earlier image captioning research from the Natural 69 Language Processing (NLP) community use collocated information such as news articles or GPS coordinates, to decide what information to include in the generated caption (Deschacht and Moens, 2007; Aker and Gaizauskas, 2010; Fan et al., 2010; Feng and Lapata, 2010a). However, in some instances visual recognition is necessary because collocated information is missing, irrelevant, or unreliable. Recognition is a classic Computer Vision (CV) problem including tasks such as recognizing instances of object classes in images (such as car, cat, or sofa); classifying images by scene (such as beach or forest); or detecting attributes in an image (such as wooden or feathered). Recent works in image caption generation represent visual content via the output of trained recognition systems for a pre-defined set of visual classes. They then us</context>
</contexts>
<marker>Fan, Aker, Tomko, Smart, Sanderson, Gaizauskas, 2010</marker>
<rawString>Xin Fan, Ahmet Aker, Martin Tomko, Philip Smart, Mark Sanderson, and Robert Gaizauskas. 2010. Automatic image captioning from the web for gps photographs. In Proceedings of the international conference on Multimedia information retrieval, MIR ’10, pages 445– 448, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>Mohsen Hejrati</author>
<author>Mohammad Amin Sadeghi</author>
<author>Peter Young</author>
<author>Cyrus Rashtchian</author>
<author>Julia Hockenmaier</author>
<author>David Forsyth</author>
</authors>
<title>Every picture tells a story: generating sentences from images.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th European conference on Computer vision: Part IV, ECCV’10,</booktitle>
<pages>15--29</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="18447" citStr="Farhadi et al. (2010)" startWordPosition="3000" endWordPosition="3003">using templates or grammar rules, where content words are selected according to the output of visual recognition systems (Kulkarni et al., 2011; Yang et al., 2011; Mitchell et al., 2012). Function words, as well as words like verbs and prepositions which are difficult to recognize visually, may be selected using a language model trained on non-visual text. There is also similar work that uses large-scale ngram models to make the generated output sound more natural (Li et al., 2011). In other approaches, captions are extracted in whole or in part from similar images in a database. For example, Farhadi et al. (2010) and Ordonez et al. (2011) build semantic representations for visual content of query images, and extract captions from database images with similar content. Kuznetsova et al. (2012) extract phrases corresponding to classes of objects and scenes detected in the query image, and combine extracted phrases into a single sentence. Our work is different than these approaches, because we directly measure how visually relevant individual words are, rather than only using visual similarity to extract sentences or phrases. Our method is most similar to that of Feng and Lapata (2010a), who generate capt</context>
</contexts>
<marker>Farhadi, Hejrati, Sadeghi, Young, Rashtchian, Hockenmaier, Forsyth, 2010</marker>
<rawString>Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David Forsyth. 2010. Every picture tells a story: generating sentences from images. In Proceedings of the 11th European conference on Computer vision: Part IV, ECCV’10, pages 15–29, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>How many words is a picture worth? automatic caption generation for news images.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>1239--1249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1943" citStr="Feng and Lapata, 2010" startWordPosition="292" endWordPosition="295">image, can improve accessibility of websites for visually-impaired users, and can improve image retrieval by providing text to search user queries against. Typically, online search engines rely on collocated textual information to resolve queries, rather than analyzing visual content directly. Likewise, earlier image captioning research from the Natural 69 Language Processing (NLP) community use collocated information such as news articles or GPS coordinates, to decide what information to include in the generated caption (Deschacht and Moens, 2007; Aker and Gaizauskas, 2010; Fan et al., 2010; Feng and Lapata, 2010a). However, in some instances visual recognition is necessary because collocated information is missing, irrelevant, or unreliable. Recognition is a classic Computer Vision (CV) problem including tasks such as recognizing instances of object classes in images (such as car, cat, or sofa); classifying images by scene (such as beach or forest); or detecting attributes in an image (such as wooden or feathered). Recent works in image caption generation represent visual content via the output of trained recognition systems for a pre-defined set of visual classes. They then use linguistic models to </context>
<context position="10046" citStr="Feng and Lapata, 2010" startWordPosition="1640" endWordPosition="1643">ot accurate to the query image. 2.2 Topic Model Image captions often act as more than labels of visual content. Some visual ideas can be described using several different words, while others are typically not described at all. Likewise, some words describe background information that is not shown visually, or contextual information that is interpreted by the user. Rather than modeling images and text such that one generates the other, we a topic model based on LDA (Blei et al., 2003) where both an image and its caption are generated by a shared latent distribution of topics. Previous work by (Feng and Lapata, 2010b) shows that topic models where image features or regions generate text features (such as Blei and Jordan (2003)) are not appropriate for modeling images with captions or other collocated text. We use a topic model designed for multi-lingual data, specifically the Polylingual Topic Model (Mimno et al., 2009). This model was developed for correlated documents in different languages that are topically similar, but are not direct translations, such as Wikipedia or news articles in different languages. We train the topic model with images and text as two languages. For query images, we estimate t</context>
<context position="13648" citStr="Feng and Lapata (2010" startWordPosition="2209" endWordPosition="2212"> words. However, the information that an image word conveys is very different than the information conveyed in a text word, so models which require direct correspondence between features in the two modalities would not be appropriate here. We train the topic model with images and text as two languages. We estimate the probabilities of textual words given a query image by first estimating the topic distribution that generated the image, and then using the same distribution to find the probabilities of textual words given the query image. However, we also perform an annotation task similarly to Feng and Lapata (2010b), in order to evaluate the topic model on its own. Our method has a 30- 35% improvement in finding words from the heldout image caption, compared to previous methods and baselines. 2.3 Sentence Compression via Caption Generation We describe an ILP for caption generation, drawing inspiration from sentence compression work by Clarke and Lapata (2008). The ILP has three inputs: the extracted caption; the prior probabilities words appearing in captions, p(w); and their posterior probabilities of words appearing in captions given the query image, p(w|query). The latter is estimated using the topi</context>
<context position="19026" citStr="Feng and Lapata (2010" startWordPosition="3089" endWordPosition="3092">tabase. For example, Farhadi et al. (2010) and Ordonez et al. (2011) build semantic representations for visual content of query images, and extract captions from database images with similar content. Kuznetsova et al. (2012) extract phrases corresponding to classes of objects and scenes detected in the query image, and combine extracted phrases into a single sentence. Our work is different than these approaches, because we directly measure how visually relevant individual words are, rather than only using visual similarity to extract sentences or phrases. Our method is most similar to that of Feng and Lapata (2010a), who generate captions for news images. Like them, we train an LDA-like model on both images and text to find latent topics that generate both. However, their model requires both an image and collocated text (a news article) to estimate the topic distribution for an unseen image, while our topic model only needs related text for the training data. They also use the news article to help generate captions, which means that optimizing their 73 generated output for content and grammaticality is a much easier problem. Although their model combines phrases and n-grams from different sentences to </context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010a. How many words is a picture worth? automatic caption generation for news images. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 1239–1249, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Topic models for image annotation and text illustration.</title>
<date>2010</date>
<booktitle>In HLTNAACL,</booktitle>
<pages>831--839</pages>
<contexts>
<context position="1943" citStr="Feng and Lapata, 2010" startWordPosition="292" endWordPosition="295">image, can improve accessibility of websites for visually-impaired users, and can improve image retrieval by providing text to search user queries against. Typically, online search engines rely on collocated textual information to resolve queries, rather than analyzing visual content directly. Likewise, earlier image captioning research from the Natural 69 Language Processing (NLP) community use collocated information such as news articles or GPS coordinates, to decide what information to include in the generated caption (Deschacht and Moens, 2007; Aker and Gaizauskas, 2010; Fan et al., 2010; Feng and Lapata, 2010a). However, in some instances visual recognition is necessary because collocated information is missing, irrelevant, or unreliable. Recognition is a classic Computer Vision (CV) problem including tasks such as recognizing instances of object classes in images (such as car, cat, or sofa); classifying images by scene (such as beach or forest); or detecting attributes in an image (such as wooden or feathered). Recent works in image caption generation represent visual content via the output of trained recognition systems for a pre-defined set of visual classes. They then use linguistic models to </context>
<context position="10046" citStr="Feng and Lapata, 2010" startWordPosition="1640" endWordPosition="1643">ot accurate to the query image. 2.2 Topic Model Image captions often act as more than labels of visual content. Some visual ideas can be described using several different words, while others are typically not described at all. Likewise, some words describe background information that is not shown visually, or contextual information that is interpreted by the user. Rather than modeling images and text such that one generates the other, we a topic model based on LDA (Blei et al., 2003) where both an image and its caption are generated by a shared latent distribution of topics. Previous work by (Feng and Lapata, 2010b) shows that topic models where image features or regions generate text features (such as Blei and Jordan (2003)) are not appropriate for modeling images with captions or other collocated text. We use a topic model designed for multi-lingual data, specifically the Polylingual Topic Model (Mimno et al., 2009). This model was developed for correlated documents in different languages that are topically similar, but are not direct translations, such as Wikipedia or news articles in different languages. We train the topic model with images and text as two languages. For query images, we estimate t</context>
<context position="13648" citStr="Feng and Lapata (2010" startWordPosition="2209" endWordPosition="2212"> words. However, the information that an image word conveys is very different than the information conveyed in a text word, so models which require direct correspondence between features in the two modalities would not be appropriate here. We train the topic model with images and text as two languages. We estimate the probabilities of textual words given a query image by first estimating the topic distribution that generated the image, and then using the same distribution to find the probabilities of textual words given the query image. However, we also perform an annotation task similarly to Feng and Lapata (2010b), in order to evaluate the topic model on its own. Our method has a 30- 35% improvement in finding words from the heldout image caption, compared to previous methods and baselines. 2.3 Sentence Compression via Caption Generation We describe an ILP for caption generation, drawing inspiration from sentence compression work by Clarke and Lapata (2008). The ILP has three inputs: the extracted caption; the prior probabilities words appearing in captions, p(w); and their posterior probabilities of words appearing in captions given the query image, p(w|query). The latter is estimated using the topi</context>
<context position="19026" citStr="Feng and Lapata (2010" startWordPosition="3089" endWordPosition="3092">tabase. For example, Farhadi et al. (2010) and Ordonez et al. (2011) build semantic representations for visual content of query images, and extract captions from database images with similar content. Kuznetsova et al. (2012) extract phrases corresponding to classes of objects and scenes detected in the query image, and combine extracted phrases into a single sentence. Our work is different than these approaches, because we directly measure how visually relevant individual words are, rather than only using visual similarity to extract sentences or phrases. Our method is most similar to that of Feng and Lapata (2010a), who generate captions for news images. Like them, we train an LDA-like model on both images and text to find latent topics that generate both. However, their model requires both an image and collocated text (a news article) to estimate the topic distribution for an unseen image, while our topic model only needs related text for the training data. They also use the news article to help generate captions, which means that optimizing their 73 generated output for content and grammaticality is a much easier problem. Although their model combines phrases and n-grams from different sentences to </context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010b. Topic models for image annotation and text illustration. In HLTNAACL, pages 831–839.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Girish Kulkarni</author>
<author>Visruth Premraj</author>
<author>Sagnik Dhar</author>
<author>Siming Li</author>
<author>Yejin Choi</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
</authors>
<title>Baby talk: Understanding and generating simple image descriptions.</title>
<date>2011</date>
<booktitle>In CVPR,</booktitle>
<pages>1601--1608</pages>
<contexts>
<context position="2598" citStr="Kulkarni et al., 2011" startWordPosition="392" endWordPosition="395">ual recognition is necessary because collocated information is missing, irrelevant, or unreliable. Recognition is a classic Computer Vision (CV) problem including tasks such as recognizing instances of object classes in images (such as car, cat, or sofa); classifying images by scene (such as beach or forest); or detecting attributes in an image (such as wooden or feathered). Recent works in image caption generation represent visual content via the output of trained recognition systems for a pre-defined set of visual classes. They then use linguistic models to correct noisy initial detections (Kulkarni et al., 2011; Yang et al., 2011), and generate more naturalsounding text (Li et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012). A key problem with this approach is that it assumes that image captioning is a grounding problem, with language acting only as labels for visual meaning. One good reason to challenge this assumption is that it imposes unrealistic constraints on the kinds of images that can be automatically described. Previous work only recognizes a limited number of visual classes – typically no more than a few dozen in total – because training CV systems requires a huge amount of han</context>
<context position="17969" citStr="Kulkarni et al., 2011" startWordPosition="2918" endWordPosition="2921">-system comparisons would be more difficult because our method uses an entirely different kind of data. In order to compare our work to related methods (Section 3), we would have to train for visual recognition systems for hundreds of visual attributes, which would mean having to hand-label the entire dataset. 3 Related Work in Image Captioning In addition to visual recognition, caption generation is a very challenging problem. In some approaches, sentences are constructed using templates or grammar rules, where content words are selected according to the output of visual recognition systems (Kulkarni et al., 2011; Yang et al., 2011; Mitchell et al., 2012). Function words, as well as words like verbs and prepositions which are difficult to recognize visually, may be selected using a language model trained on non-visual text. There is also similar work that uses large-scale ngram models to make the generated output sound more natural (Li et al., 2011). In other approaches, captions are extracted in whole or in part from similar images in a database. For example, Farhadi et al. (2010) and Ordonez et al. (2011) build semantic representations for visual content of query images, and extract captions from da</context>
</contexts>
<marker>Kulkarni, Premraj, Dhar, Li, Choi, Berg, Berg, 2011</marker>
<rawString>Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg. 2011. Baby talk: Understanding and generating simple image descriptions. In CVPR, pages 1601– 1608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Polina Kuznetsova</author>
<author>Vicente Ordonez</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Collective generation of natural image descriptions.</title>
<date>2012</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2724" citStr="Kuznetsova et al., 2012" startWordPosition="414" endWordPosition="417"> Computer Vision (CV) problem including tasks such as recognizing instances of object classes in images (such as car, cat, or sofa); classifying images by scene (such as beach or forest); or detecting attributes in an image (such as wooden or feathered). Recent works in image caption generation represent visual content via the output of trained recognition systems for a pre-defined set of visual classes. They then use linguistic models to correct noisy initial detections (Kulkarni et al., 2011; Yang et al., 2011), and generate more naturalsounding text (Li et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012). A key problem with this approach is that it assumes that image captioning is a grounding problem, with language acting only as labels for visual meaning. One good reason to challenge this assumption is that it imposes unrealistic constraints on the kinds of images that can be automatically described. Previous work only recognizes a limited number of visual classes – typically no more than a few dozen in total – because training CV systems requires a huge amount of hand-annotated data. For example, the PASCAL VOC dataset1 has 11,530 training im1http://pascallin.ecs.soton.ac.uk/ Proceedings of</context>
<context position="4116" citStr="Kuznetsova et al. (2012)" startWordPosition="636" endWordPosition="639">jects, in order to learn only 20 object classes. Since training visual recognition systems is such a burden, “general-domain” image captioning datasets are limited by the current technology. For example, the SBU-Flickr dataset (Ordonez et al., 2011), which contains 1 million images and captions, is built by first querying Flickr using a pre-defined set of queries, then further filtering to remove instances where the caption does not contain at least two words belonging to their term list. Furthermore, detections are too noisy to generate a good caption for the majority of images. For example, Kuznetsova et al. (2012) select their test set according to which images receive the most confident visual object detection scores. We instead direct our attention to the domainspecific image captioning task, assuming that we know a general object or scene category for the query image, and that we have access to a dataset of images and captions from the same domain. While some techniques may be unrealistic in assuming that high-quality collocated text is always available, assuming that there is no collocated information at all is equally unrealistic. Data sources such as file names, website text, Facebook likes, and </context>
<context position="16826" citStr="Kuznetsova et al., 2012" startWordPosition="2740" endWordPosition="2743">entence’s object cannot be deleted, and for any word that we include in the output sentence, we must include its head word as well. We also have constraints that define valid use of coordinating conjunctions and punctuation. We evaluate generated captions using automatic metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). These metrics are commonly used in summarization and translation research and have been previously used in image captioning research to compare automatically generated captions to human-written captions for each image (Ordonez et al., 2011; Yang et al., 2011; Kuznetsova et al., 2012). Although human-written captions may use synonyms to describe a visual object or attribute, or even describe entirely different attributes than what is described in the generated captions, computing the automatic metrics over a large test set finds statistically significant improvements in the accuracy of the extracted and compressed captions over extraction alone. For our proposed work (Section 4), we also plan to perform manual evaluations of our captions based on their content and language quality. However, cross-system comparisons would be more difficult because our method uses an entirel</context>
<context position="18629" citStr="Kuznetsova et al. (2012)" startWordPosition="3027" endWordPosition="3030">, 2012). Function words, as well as words like verbs and prepositions which are difficult to recognize visually, may be selected using a language model trained on non-visual text. There is also similar work that uses large-scale ngram models to make the generated output sound more natural (Li et al., 2011). In other approaches, captions are extracted in whole or in part from similar images in a database. For example, Farhadi et al. (2010) and Ordonez et al. (2011) build semantic representations for visual content of query images, and extract captions from database images with similar content. Kuznetsova et al. (2012) extract phrases corresponding to classes of objects and scenes detected in the query image, and combine extracted phrases into a single sentence. Our work is different than these approaches, because we directly measure how visually relevant individual words are, rather than only using visual similarity to extract sentences or phrases. Our method is most similar to that of Feng and Lapata (2010a), who generate captions for news images. Like them, we train an LDA-like model on both images and text to find latent topics that generate both. However, their model requires both an image and collocat</context>
<context position="19878" citStr="Kuznetsova et al. (2012)" startWordPosition="3238" endWordPosition="3241">to estimate the topic distribution for an unseen image, while our topic model only needs related text for the training data. They also use the news article to help generate captions, which means that optimizing their 73 generated output for content and grammaticality is a much easier problem. Although their model combines phrases and n-grams from different sentences to form an image caption, they only consider the text from a single news article for extraction, and they can assume that the text is mostly accurate and relevant to the content of the image. In this sense, our method is more like Kuznetsova et al. (2012), which also uses an Integer Linear Program (ILP) to rapidly optimize how well their generated caption fits the content of the image model. However, it is easier to get coherent image captions from our model since we are not combining parts of sentences from multiple sources. Since we build our output from extracted sentences, not phrases, our ILP requires fewer grammaticality and coherence constraints than it would for building new sentences from scratch. We also model how relevant each individual word is to the query image, while they extract phrases based on visual similarity of detected ob</context>
</contexts>
<marker>Kuznetsova, Ordonez, Berg, Berg, Choi, 2012</marker>
<rawString>Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg, Tamara L. Berg, and Yejin Choi. 2012. Collective generation of natural image descriptions. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siming Li</author>
<author>Girish Kulkarni</author>
<author>Tamara L Berg</author>
<author>Alexander C Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Composing simple image descriptions using web-scale n-grams.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, CoNLL ’11,</booktitle>
<pages>220--228</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2675" citStr="Li et al., 2011" startWordPosition="406" endWordPosition="409"> or unreliable. Recognition is a classic Computer Vision (CV) problem including tasks such as recognizing instances of object classes in images (such as car, cat, or sofa); classifying images by scene (such as beach or forest); or detecting attributes in an image (such as wooden or feathered). Recent works in image caption generation represent visual content via the output of trained recognition systems for a pre-defined set of visual classes. They then use linguistic models to correct noisy initial detections (Kulkarni et al., 2011; Yang et al., 2011), and generate more naturalsounding text (Li et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012). A key problem with this approach is that it assumes that image captioning is a grounding problem, with language acting only as labels for visual meaning. One good reason to challenge this assumption is that it imposes unrealistic constraints on the kinds of images that can be automatically described. Previous work only recognizes a limited number of visual classes – typically no more than a few dozen in total – because training CV systems requires a huge amount of hand-annotated data. For example, the PASCAL VOC dataset1 has 11,530 training im</context>
<context position="18312" citStr="Li et al., 2011" startWordPosition="2977" endWordPosition="2980">n addition to visual recognition, caption generation is a very challenging problem. In some approaches, sentences are constructed using templates or grammar rules, where content words are selected according to the output of visual recognition systems (Kulkarni et al., 2011; Yang et al., 2011; Mitchell et al., 2012). Function words, as well as words like verbs and prepositions which are difficult to recognize visually, may be selected using a language model trained on non-visual text. There is also similar work that uses large-scale ngram models to make the generated output sound more natural (Li et al., 2011). In other approaches, captions are extracted in whole or in part from similar images in a database. For example, Farhadi et al. (2010) and Ordonez et al. (2011) build semantic representations for visual content of query images, and extract captions from database images with similar content. Kuznetsova et al. (2012) extract phrases corresponding to classes of objects and scenes detected in the query image, and combine extracted phrases into a single sentence. Our work is different than these approaches, because we directly measure how visually relevant individual words are, rather than only us</context>
</contexts>
<marker>Li, Kulkarni, Berg, Berg, Choi, 2011</marker>
<rawString>Siming Li, Girish Kulkarni, Tamara L. Berg, Alexander C. Berg, and Yejin Choi. 2011. Composing simple image descriptions using web-scale n-grams. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, CoNLL ’11, pages 220–228, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Stan Szpakowicz MarieFrancine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74– 81,</booktitle>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="16540" citStr="Lin, 2004" startWordPosition="2699" endWordPosition="2700">matical correctness of the compressed sentence. We do not have space here to describe all of the constraints, but basically, using the “semantic head” version of the headfinder from Collins (1999), we constrain that the head word of the sentence and the head word of the sentence’s object cannot be deleted, and for any word that we include in the output sentence, we must include its head word as well. We also have constraints that define valid use of coordinating conjunctions and punctuation. We evaluate generated captions using automatic metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). These metrics are commonly used in summarization and translation research and have been previously used in image captioning research to compare automatically generated captions to human-written captions for each image (Ordonez et al., 2011; Yang et al., 2011; Kuznetsova et al., 2012). Although human-written captions may use synonyms to describe a visual object or attribute, or even describe entirely different attributes than what is described in the generated captions, computing the automatic metrics over a large test set finds statistically significant improvements in the accuracy of the ex</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Stan Szpakowicz MarieFrancine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74– 81, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
</authors>
<title>Summarization with a joint model for sentence extraction and compression.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, ILP ’09,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="22419" citStr="Martins and Smith, 2009" startWordPosition="3667" endWordPosition="3670">ct that our image captioning method is not grounded in our favor, and assume that if an object is partially occluded or truncated in an image, than it is less likely that the photographer considered that object to be interesting, so it is not as important whether that object is described in the caption or not. Finally, there is also much that could be done to improve the text generation component on its own. Our framework currently extracts only a single caption sentence to compress, while recent work in summarization has focused on the problem of learning how to jointly extract and compress (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011). Since a poor extraction choice can make finding an accurate compression impossible, we should also study different methods of extraction to learn about what kinds of features are most likely to help us find good sentences. As mentioned in Section 2.1, we have already found that global feature descriptors are better than bag of image word descriptors for extracting sentences to use in image caption compressions in the shopping dataset. As we extend our framework to other domains of images, we are interested in finding whether scene-based descriptors and classif</context>
</contexts>
<marker>Martins, Smith, 2009</marker>
<rawString>Andr´e F. T. Martins and Noah A. Smith. 2009. Summarization with a joint model for sentence extraction and compression. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, ILP ’09, pages 1–9, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna M Wallach</author>
<author>Jason Naradowsky</author>
<author>David A Smith</author>
<author>Andrew McCallum</author>
</authors>
<title>Polylingual topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09,</booktitle>
<pages>880--889</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="10356" citStr="Mimno et al., 2009" startWordPosition="1690" endWordPosition="1693">ontextual information that is interpreted by the user. Rather than modeling images and text such that one generates the other, we a topic model based on LDA (Blei et al., 2003) where both an image and its caption are generated by a shared latent distribution of topics. Previous work by (Feng and Lapata, 2010b) shows that topic models where image features or regions generate text features (such as Blei and Jordan (2003)) are not appropriate for modeling images with captions or other collocated text. We use a topic model designed for multi-lingual data, specifically the Polylingual Topic Model (Mimno et al., 2009). This model was developed for correlated documents in different languages that are topically similar, but are not direct translations, such as Wikipedia or news articles in different languages. We train the topic model with images and text as two languages. For query images, we estimate the topic distribution that generated just the image, and then In the model, images and their captions are represented using bag-of-words, a commonly-used technique for document representation in both CV and NLP research. The textual features are non-function words in the model, including words that describe s</context>
</contexts>
<marker>Mimno, Wallach, Naradowsky, Smith, McCallum, 2009</marker>
<rawString>David Mimno, Hanna M. Wallach, Jason Naradowsky, David A. Smith, and Andrew McCallum. 2009. Polylingual topic models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09, pages 880–889, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Jesse Dodge</author>
<author>Amit Goyal</author>
<author>Kota Yamaguchi</author>
<author>Karl Stratos</author>
<author>Xufeng Han</author>
<author>Alyssa Mensch</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
<author>Hal Daum´e</author>
</authors>
<title>Midge: Generating image descriptions from computer vision detections.</title>
<date>2012</date>
<booktitle>In European Chapter of the Association for Computational Linguistics (EACL).</booktitle>
<marker>Mitchell, Dodge, Goyal, Yamaguchi, Stratos, Han, Mensch, Berg, Berg, Daum´e, 2012</marker>
<rawString>Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Yamaguchi, Karl Stratos, Xufeng Han, Alyssa Mensch, Alexander C. Berg, Tamara L. Berg, and Hal Daum´e III. 2012. Midge: Generating image descriptions from computer vision detections. In European Chapter of the Association for Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aude Oliva</author>
<author>Antonio Torralba</author>
</authors>
<title>Modeling the shape of the scene: A holistic representation of the spatial envelope.</title>
<date>2001</date>
<journal>International Journal of Computer Vision,</journal>
<pages>42--145</pages>
<contexts>
<context position="8703" citStr="Oliva and Torralba, 2001" startWordPosition="1407" endWordPosition="1410">es. We do not make use of the subcategories, instead we group all of the categories of shoe images together. The shoes in the images are mostly posed against solid color backgrounds, while the captions have much more variability in length and linguistic quality. For our thesis work, we intend to extend our current framework to different domains of data, including natural images. However, it is important to point out that no part of the framework as it is currently implemented is specific to describing shoes or shopping images. This will be described in Section 4. 2.1 Sentence Extraction GIST (Oliva and Torralba, 2001) is a global image descriptor which describes how gradients are oriented in different regions of an image. It is commonly used for classifying background scenes in images, however images in the Attribute Discovery Dataset do not have “backgrounds” per se. Instead, we treat the overall shape of the object as the “scene” and extract a caption sentence using GIST nearest neighbors between the query image and the images in the training set. Because similar objects and attributes tend to appear in similar scenes, we expect that at least some of the extracted caption will describe local attributes t</context>
</contexts>
<marker>Oliva, Torralba, 2001</marker>
<rawString>Aude Oliva and Antonio Torralba. 2001. Modeling the shape of the scene: A holistic representation of the spatial envelope. International Journal of Computer Vision, 42:145–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ordonez</author>
<author>G Kulkarni</author>
<author>T L Berg</author>
</authors>
<title>Im2text: Describing images using 1 million captioned photographs.</title>
<date>2011</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="3741" citStr="Ordonez et al., 2011" startWordPosition="572" endWordPosition="575">few dozen in total – because training CV systems requires a huge amount of hand-annotated data. For example, the PASCAL VOC dataset1 has 11,530 training im1http://pascallin.ecs.soton.ac.uk/ Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 69–76, Atlanta, Georgia, 13 June 2013. c�2013 Association for Computational Linguistics ages with 27,450 labeled objects, in order to learn only 20 object classes. Since training visual recognition systems is such a burden, “general-domain” image captioning datasets are limited by the current technology. For example, the SBU-Flickr dataset (Ordonez et al., 2011), which contains 1 million images and captions, is built by first querying Flickr using a pre-defined set of queries, then further filtering to remove instances where the caption does not contain at least two words belonging to their term list. Furthermore, detections are too noisy to generate a good caption for the majority of images. For example, Kuznetsova et al. (2012) select their test set according to which images receive the most confident visual object detection scores. We instead direct our attention to the domainspecific image captioning task, assuming that we know a general object o</context>
<context position="16781" citStr="Ordonez et al., 2011" startWordPosition="2732" endWordPosition="2735">f the sentence and the head word of the sentence’s object cannot be deleted, and for any word that we include in the output sentence, we must include its head word as well. We also have constraints that define valid use of coordinating conjunctions and punctuation. We evaluate generated captions using automatic metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). These metrics are commonly used in summarization and translation research and have been previously used in image captioning research to compare automatically generated captions to human-written captions for each image (Ordonez et al., 2011; Yang et al., 2011; Kuznetsova et al., 2012). Although human-written captions may use synonyms to describe a visual object or attribute, or even describe entirely different attributes than what is described in the generated captions, computing the automatic metrics over a large test set finds statistically significant improvements in the accuracy of the extracted and compressed captions over extraction alone. For our proposed work (Section 4), we also plan to perform manual evaluations of our captions based on their content and language quality. However, cross-system comparisons would be more</context>
<context position="18473" citStr="Ordonez et al. (2011)" startWordPosition="3005" endWordPosition="3008"> rules, where content words are selected according to the output of visual recognition systems (Kulkarni et al., 2011; Yang et al., 2011; Mitchell et al., 2012). Function words, as well as words like verbs and prepositions which are difficult to recognize visually, may be selected using a language model trained on non-visual text. There is also similar work that uses large-scale ngram models to make the generated output sound more natural (Li et al., 2011). In other approaches, captions are extracted in whole or in part from similar images in a database. For example, Farhadi et al. (2010) and Ordonez et al. (2011) build semantic representations for visual content of query images, and extract captions from database images with similar content. Kuznetsova et al. (2012) extract phrases corresponding to classes of objects and scenes detected in the query image, and combine extracted phrases into a single sentence. Our work is different than these approaches, because we directly measure how visually relevant individual words are, rather than only using visual similarity to extract sentences or phrases. Our method is most similar to that of Feng and Lapata (2010a), who generate captions for news images. Like</context>
</contexts>
<marker>Ordonez, Kulkarni, Berg, 2011</marker>
<rawString>V. Ordonez, G. Kulkarni, and T.L. Berg. 2011. Im2text: Describing images using 1 million captioned photographs. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="16518" citStr="Papineni et al., 2002" startWordPosition="2693" endWordPosition="2696">c constraints that ensure the grammatical correctness of the compressed sentence. We do not have space here to describe all of the constraints, but basically, using the “semantic head” version of the headfinder from Collins (1999), we constrain that the head word of the sentence and the head word of the sentence’s object cannot be deleted, and for any word that we include in the output sentence, we must include its head word as well. We also have constraints that define valid use of coordinating conjunctions and punctuation. We evaluate generated captions using automatic metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). These metrics are commonly used in summarization and translation research and have been previously used in image captioning research to compare automatically generated captions to human-written captions for each image (Ordonez et al., 2011; Yang et al., 2011; Kuznetsova et al., 2012). Although human-written captions may use synonyms to describe a visual object or attribute, or even describe entirely different attributes than what is described in the generated captions, computing the automatic metrics over a large test set finds statistically significant improvements in </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Patterson</author>
<author>J Hays</author>
</authors>
<title>Sun attribute database: Discovering, annotating, and recognizing scene attributes.</title>
<date>2012</date>
<booktitle>IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>0--2751</pages>
<contexts>
<context position="21057" citStr="Patterson and Hays (2012)" startWordPosition="3434" endWordPosition="3437">ases based on visual similarity of detected objects in the images. 4 Proposed Work One clear direction for future work is to extend our image captioning framework to natural images. By “natural images” we refer to images of everyday scenes seen by people, unlike the shopping images, where objects tend to be posed in similar positions against plain backgrounds. Instead of domains such as handbags and shoes, we propose to cluster the training data based on visual scene domains such as mountains, beaches, and living rooms. We are particularly interested in the scene attributes and classifiers by Patterson and Hays (2012) which builds an attribute-based taxonomy of scene types using crowd-sourcing, rather than categorical scene types which are typically used. Visual recognition is generally much more difficult in natural scenes than in posed images, since lighting and viewpoints are not consistent, and objects may be occluded by other objects or truncated by the edge of the image. However, we are optimistic because we do not need to solve the general visual recognition task, since our model only learns how visual objects and attributes appear in specific domains of scenes, a much easier problem. Additionally, </context>
</contexts>
<marker>Patterson, Hays, 2012</marker>
<rawString>G. Patterson and J. Hays. 2012. Sun attribute database: Discovering, annotating, and recognizing scene attributes. 2012 IEEE Conference on Computer Vision and Pattern Recognition, 0:2751–2758.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arnold W M Smeulders</author>
<author>Marcel Worring</author>
<author>Simone Santini</author>
<author>Amarnath Gupta</author>
<author>Ramesh Jain</author>
</authors>
<title>Content-based image retrieval at the end of the early years.</title>
<date>2000</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell.,</journal>
<volume>22</volume>
<issue>12</issue>
<contexts>
<context position="5655" citStr="Smeulders et al., 2000" startWordPosition="901" endWordPosition="904"> that will impact the greater community. Finally, labeling visual content is often not enough to provide an adequate caption. The meaning of an image to a user is more than just listing the objects in the image, and can even change for different users. This problem is commonly known as “bridging the semantic gap”: “The semantic gap is the lack of coincidence between the information that one can extract from the visual data and the interpretation that the same data have for a user in a given situation. A linguistic description is almost always contextual, whereas an image may live by itself.” (Smeulders et al., 2000) challenges/VOC/ General-domain models of caption generation fail to capture context because they assume that all the relevant information has been provided in the image. However, training models on data from the same domain gives implicit context about what information should be provided in the generated text. This thesis proposes a framework for image captioning that does not require supervision in the form of hand-labeled examples. We train a topic model on a corpus of images and captions in the same domain, in order to jointly learn image features and natural language descriptions. The tra</context>
</contexts>
<marker>Smeulders, Worring, Santini, Gupta, Jain, 2000</marker>
<rawString>Arnold W. M. Smeulders, Marcel Worring, Simone Santini, Amarnath Gupta, and Ramesh Jain. 2000. Content-based image retrieval at the end of the early years. IEEE Trans. Pattern Anal. Mach. Intell., 22(12):1349–1380, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yezhou Yang</author>
</authors>
<title>Ching Lik Teo, Hal Daum´e III, and Yiannis Aloimonos.</title>
<date>2011</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Edinburgh, Scotland.</location>
<marker>Yang, 2011</marker>
<rawString>Yezhou Yang, Ching Lik Teo, Hal Daum´e III, and Yiannis Aloimonos. 2011. Corpus-guided sentence generation of natural images. In Empirical Methods in Natural Language Processing (EMNLP), Edinburgh, Scotland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>