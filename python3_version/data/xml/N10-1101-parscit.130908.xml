<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.035858">
<title confidence="0.993014">
Automatic Generation of Personalized Annotation Tags for Twitter Users
</title>
<author confidence="0.995059">
Wei Wu, Bin Zhang, Mari Ostendorf
</author>
<affiliation confidence="0.9486">
Electrical Engineering
University of Washington, Seattle, WA
</affiliation>
<email confidence="0.967123">
{weiwu, binz, ostendor}@uw.edu
</email>
<sectionHeader confidence="0.993573" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998987111111111">
This paper introduces a system designed for
automatically generating personalized annota-
tion tags to label Twitter user’s interests and
concerns. We applied TFIDF ranking and
TextRank to extract keywords from Twitter
messages to tag the user. The user tagging pre-
cision we obtained is comparable to the preci-
sion of keyword extraction from web pages for
content-targeted advertising.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999920575757576">
Twitter is a communication platform which com-
bines SMS, instant messages and social networks. It
enables users to share information with their friends
or the public by updating their Twitter messages.
A large majority of the Twitter users are individ-
ual subscribers, who use Twitter to share informa-
tion on “what am I doing” or “what’s happening
right now”. Most of them update their Twitter mes-
sages very frequently, in which case the Twitter mes-
sages compose a detailed log of the user’s everyday
life. These Twitter messages contain rich informa-
tion about an individual user, including what s/he is
interested in and concerned about. Identifying an
individual user’s interests and concerns can help po-
tential commercial applications. For instance, this
information can be employed to produce “follow-
ing” suggestions, either a person who shares simi-
lar interests (for expanding their social network) or
a company providing products or services the user is
interested in (for personalized advertisement).
In this work, we focus on automatically gener-
ating personalized annotation tags to label Twitter
user’s interests and concerns. We formulate this
problem as a keyword extraction task, by selecting
words from each individual user’s Twitter messages
as his/her tags. Due to the lack of human generated
annotations, we employ an unsupervised strategy.
Specifically, we apply TFIDF ranking and TextRank
(Mihalcea and Tarau, 2004) keyword extraction on
Twitter messages after a series of text preprocess-
ing steps. Experiments on randomly selected users
showed good results with TextRank, but high vari-
ability among users.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999932290322581">
Research work related to Twitter message analysis
includes a user sentiment study (Jansen et al., 2009)
and information retrieval indexing. To our knowl-
edge, no previously published research has yet ad-
dressed problems on tagging user’s personal inter-
ests from Twitter messages via keyword extraction,
though several studies have looked at keyword ex-
traction using other genres.
For supervised keyword extraction, (Turney,
2000; Turney, 2003; Hulth, 2003; Yih et al., 2006;
Liu et al., 2008) employed TFIDF or its variants
with Part-of-Speech (POS), capitalization, phrase
and sentence length, etc., as features to train key-
word extraction models, and discriminative training
is usually adopted. Yih et al. (2006) use logis-
tic regression to extract keywords from web pages
for content-targeted advertising, which has the most
similar application to our work. However, due to the
lack of human annotation on Twitter messages, we
have to adopt an unsupervised strategy.
For unsupervised keyword extraction, TFIDF
ranking is a popular method, and its effective-
ness has been shown in (Hulth, 2003; Yih et al.,
2006). TextRank and its variants (Mihalcea and Ta-
rau, 2004; Wan et al., 2007; Liu et al., 2009) are
graph-based text ranking models, which are derived
from Google’s PageRank algorithm (Brin and Page,
1998). It outperforms TFIDF ranking on traditional
keyword extraction tasks. However, previous work
on both TFIDF ranking and TextRank has been done
mainly on academic papers, spoken documents or
</bodyText>
<page confidence="0.980137">
689
</page>
<subsubsectionHeader confidence="0.577094">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 689–692,
</subsubsectionHeader>
<subsectionHeader confidence="0.276578">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.999719818181818">
web pages, whose language style is more formal (or,
less “conversational”) than that of Twitter messages.
Twitter messages contain large amounts of “noise”
like emoticons, internet slang words, abbreviations,
and misspelled words. In addition, Twitter messages
are a casual log of a user’s everyday life, which often
lacks of a coherent topic sequence compared to aca-
demic papers and most spoken documents. Hence,
it remains to see whether TFIDF ranking and Tex-
tRank are effective for identifying user’s interests
from Twitter messages.
</bodyText>
<sectionHeader confidence="0.96366" genericHeader="method">
3 System Architecture
</sectionHeader>
<bodyText confidence="0.996885857142857">
Figure 1 shows the framework of our system for
tagging Twitter user’s interests. A preprocessing
pipeline is designed to deal with various types of
“noise” in Twitter messages and produce candidate
words for user tags. Then the TFIDF ranking or Tex-
tRank algorithm is applied to select user tags from
the candidate words.
</bodyText>
<figureCaption confidence="0.947285">
Figure 1: Framework of the personalized annotation tag
generation system for Twitter users
</figureCaption>
<subsectionHeader confidence="0.999015">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999975214285714">
In addition to messages describing “What am I do-
ing” or “what’s happening right now”, Twitter users
also write replying messages to comment on other
users’ messages. This kind of message generally
contains more information about the users they re-
ply to than about themselves, and therefore they are
removed in the preprocessing pipeline.
Emoticons frequently appear in Twitter messages.
Although some of them help express user’s senti-
ment on certain topics, they are not directly helpful
for keyword analysis and may interfere with POS
tagging in the preprocessing pipeline. Therefore, we
designed a set of regular expressions to detect and
remove them.
Internet slang words and abbreviations are widely
used in Twitter messages. Most of them are out-of-
vocabulary words in the POS tagging model used
in the next step, and thus will deteriorate the POS
tagging accuracy. Hence, we build a lookup table
based on the list of abbreviations in the NoSlang on-
line dictionary,1 which we divide by hand into three
sets for different processing. The first set includes
422 content words and phrases, such as “bff” (best
friend forever) and “fone” (phone), with valid can-
didate words for user tags. The second set includes
67 abbreviations of function words that usually form
grammatical parts in a sentence, such as “im” (i’m),
“abt” (about). Simply removing them will affect the
POS tagging. Thus, the abbreviations in both these
sets are replaced with the corresponding complete
words or phrases. The third set includes 4576 phrase
abbreviations that are usually separable parts of a
sentence that do not directly indicate discussion top-
ics, such as “lol” (laugh out loud), “clm” (cool like
me), which are removed in this step.
We apply the Stanford POS tagger (Toutanova
and Manning, 2000) on Twitter messages, and only
select nouns and adjectives as valid candidates for
user tags. At the end of the preprocessing pipeline,
the candidate words are processed with the rule-
based Porter stemmer2 and stopwords are filtered us-
ing a publicly available list.3
</bodyText>
<footnote confidence="0.98185975">
1www.noslang.com/dictionary
2tartarus.org/ martin/PorterStemmer/
3armandbrahaj.blog.al/2009/04/14/
list-of-english-stop-words/
</footnote>
<table confidence="0.995049384615385">
TFIDF ranking / TextRank
Personalized annotation tags for
the Twitter user
Preprocessing
Stemming and stopword removing
Substituting/removing internet
slang words and abbreviations
Removing replying messages
Part-of-Speech tagging and
filtering
Removing emoticons
Messages from
one Twitter user
</table>
<page confidence="0.968612">
690
</page>
<subsectionHeader confidence="0.844112">
3.2 User Tag Extraction
3.2.1 TFIDF ranking
</subsectionHeader>
<bodyText confidence="0.999744">
In the TFIDF ranking algorithm, messages from
user u are put together as one document. The TFIDF
value of word i from this user’s messages is com-
puted as
</bodyText>
<equation confidence="0.982615">
t f idfi,u = ni,u U
,u log( )
r-j nj Ui
</equation>
<bodyText confidence="0.9996026">
where ni,u is the count of word i in user u’s mes-
sages, Ui is the number of users whose messages
contain word i, and U is the total number of users in
the Twitter corpus. For each user, words with top N
TFIDF values are selected as his/her tags.
</bodyText>
<subsectionHeader confidence="0.4841">
3.2.2 TextRank
</subsectionHeader>
<bodyText confidence="0.999599866666667">
According to the TextRank algorithm (Mihalcea
and Tarau, 2004), each candidate word is repre-
sented by a vertex in the graph; edges are added
between two candidate words according to their co-
occurrence. In the context of user tag extraction, we
build a TextRank graph with undirected edges for
each Twitter user. One edge is added between two
candidate words if they co-exist within at least one
message; the edge weight is set to be the total count
of within-message co-occurrence of the two words
throughout all messages of this user.
Starting with an arbitrarily assigned value (e.g.
1.0), the rank value R(Vi) of the candidate word at
vertex Vi is updated iteratively according to the fol-
lowing equation,
</bodyText>
<equation confidence="0.99493">
R(Vi) = (1−d)+d
VjEE(Vi)
� r-VkEE(Vj) wjk
wji R(Vj )
</equation>
<bodyText confidence="0.999968625">
where wji is the weight of the edge that links Vj
and Vi, E(Vi) is the set of vertices which Vi is con-
nected to, and d is a damping factor that is usually
set to 0.85 (Brin and Page, 1998). The rank update
iteration continues until convergence. The candidate
words are then sorted according to their rank values.
Words with top-N rank values are selected as tags
for this user.
</bodyText>
<sectionHeader confidence="0.999847" genericHeader="evaluation">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.991115">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.996278">
We employed the Twitter API to download Twitter
messages. A unigram English language model was
</bodyText>
<table confidence="0.9998176">
Precision (%) TFIDF TextRank
top-1 59.6 67.3
top-3 61.5 66.0
top-5 61.2 63.0
top-10 59.0 58.3
</table>
<tableCaption confidence="0.999923">
Table 1: Tagging precision on all users in the test set
</tableCaption>
<bodyText confidence="0.999768909090909">
used to filter out non-English users. We obtained
messages from 11,376 Twitter users, each of them
had 180 to 200 messages. The word IDF for TFIDF
ranking was computed over these users.
We adopted an evaluation measure similar to the
one proposed in (Yih et al., 2006) for identifying
advertising keywords on web pages, which empha-
sizes precision. We randomly selected 156 Twit-
ter users to evaluate the top-N precision of TFIDF
ranking and TextRank. After we obtained the top-
N outputs from the system, three human evaluators
were asked to judge whether the output tags from the
two systems (unidentified) reflected the correspond-
ing Twitter user’s interests or concerns according to
the full set of his/her messages.4 We adopted a con-
servative standard in the evaluation: when a person’s
name is extracted as a user tag, which is frequent
among Twitter users, we judge it as a correct tag
only when it is a name of a famous person (pop star,
football player, etc). The percentage of the correct
tags among the top-N selected tags corresponds to
the top-N precision of the system.
</bodyText>
<subsectionHeader confidence="0.993006">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999919538461539">
Table 1 gives the top-N precision for TFIDF and
TextRank for different values of N, showing that
TextRank leads to higher precision for small N. Al-
though Twitter messages are much “noisier” than
regular web pages, the top-N precision we obtained
for Twitter user tagging is comparable to the web
page advertising keyword extraction result reported
in (Yih et al., 2006).
Figure 2 shows an example of the candidate word
ranking result of a Twitter user by TextRank (the
font size is set to be proportional to each word’s
TextRank value). By examining the Twitter mes-
sages, we found that this user is an information tech-
</bodyText>
<footnote confidence="0.9929735">
4The pairwise Kappa value for inter-evaluator agreement
ranged from 0.77-0.83, showing good agreement.
</footnote>
<page confidence="0.993796">
691
</page>
<figureCaption confidence="0.9810415">
Figure 2: Example of a Twitter user’s word ranks (the
font size is proportional to each word’s TextRank value)
</figureCaption>
<table confidence="0.999935">
Precision (%)
top-N Q &gt;0.6 Q &lt;0.6 H&gt;5.4 H&lt;5.4
top-1 71.6 60.7 78.5 50.8
top-3 71.9 56.8 74.2 54.0
top-5 69.3 53.1 69.2 53.7
top-10 65.1 47.7 63.8 50.2
</table>
<tableCaption confidence="0.992664333333333">
Table 2: TextRank tagging precision on users with dif-
ferent Top-10 TextRank value standard deviation (Q) and
user message text entropy (H).
</tableCaption>
<bodyText confidence="0.999966590909091">
nology “geek”, who is very interested in writing Ap-
ple’s iPhone applications, and also a user of Google
Wave. In this work, we use only isolated words as
user tags, however, “google”, “wave”, and “palo”,
“alto” extracted in this example indicate that phrase
level tagging can bring us more information about
the user, which is typical of many users.
Although most Twitter users express their inter-
ests to some extent in their messages, there are some
users whose message content is not rich enough to
extract reliable information. We investigated two
measures for identifying such users: standard devi-
ation of the top-10 TextRank values and the user’s
message text entropy. Table 2 shows a compari-
son of tagging precision where the users are divided
into two groups with a threshold on each of the two
measures. It is shown that users with larger Tex-
tRank value standard deviation or message text en-
tropy tend to have higher tagging precision, and the
message text entropy has better correlation with the
top-10 tagging precision than TextRank value stan-
dard deviation (0.33 v.s. 0.20 absolute).
</bodyText>
<sectionHeader confidence="0.998959" genericHeader="conclusions">
5 Summary
</sectionHeader>
<bodyText confidence="0.999954066666667">
In this paper, we designed a system to automat-
ically extract keywords from Twitter messages to
tag user interests and concerns. We evaluated two
tagging algorithms, finding that TextRank outper-
formed TFIDF ranking, but both gave a tagging pre-
cision that was comparable to that reported for web
page advertizing keyword extraction. We noticed
substantial variation in performance across users,
with low entropy indicative of users with fewer key-
words, and a need for extracting key phrases (in ad-
dition to words). Other follow-on work might con-
sider temporal characteristics of messages in terms
of the amount of data needed for reliable tags vs.
their time-varying nature, as well as sentiment asso-
ciated with the identified tags.
</bodyText>
<sectionHeader confidence="0.99944" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999717971428571">
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual web search engine. Computer Networks
and ISDN Systems, 30(1-7):107–117.
A. Hulth. 2003. Improved automatic keyword extraction
given more linguistic knowledge. In Proc. EMNLP,
pages 216–223.
B. J. Jansen, M. Zhang, K. Sobel, and A. Chowdury.
2009. Twitter power: Tweets as electronic word of
mouth. Journal of the American Society for Informa-
tion Science and Technology, 60(11):2169–2188.
F. Liu, F. Liu, and Y. Liu. 2008. Automatic keyword
extraction for the meeting corpus using supervised ap-
proach and bigram expansion. In Proc. IEEE SLT,
pages 181–184.
F. Liu, D. Pennell, F. Liu, and Y. Liu. 2009. Unsuper-
vised approaches for automatic keyword extraction us-
ing meeting transcripts. In Proc. HLT/NAACL, pages
620–628.
R. Mihalcea and P. Tarau. 2004. Textrank: Bringing
order into texts. In Proc. EMNLP.
K. Toutanova and C. D. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger. In Proc. EMNLP, pages 63–77.
P. D. Turney. 2000. Learning algorithms for keyphrase.
Information Retrieval, 2(4):303–336.
P. D. Turney. 2003. Coherent keyphrase extraction via
web mining. In Proc. IJCAI, pages 434–439.
X. Wan, J. Yang, and J. Xiao. 2007. Towards an iter-
ative reinforcement approach for simultaneous docu-
ment summarization and keyword extraction. In Proc.
ACL, pages 552–559.
W.-T. Yih, J. Goodman, and V. R. Carvalho. 2006. Find-
ing advertising keywords on web pages. In Proc. 15th
International Conference on World Wide Web, pages
213–222.
</reference>
<page confidence="0.997906">
692
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.539956">
<title confidence="0.999756">Automatic Generation of Personalized Annotation Tags for Twitter Users</title>
<author confidence="0.999771">Wei Wu</author>
<author confidence="0.999771">Bin Zhang</author>
<author confidence="0.999771">Mari</author>
<affiliation confidence="0.8949555">Electrical University of Washington, Seattle,</affiliation>
<address confidence="0.775221">binz,</address>
<abstract confidence="0.986837">This paper introduces a system designed for automatically generating personalized annotation tags to label Twitter user’s interests and concerns. We applied TFIDF ranking and TextRank to extract keywords from Twitter messages to tag the user. The user tagging precision we obtained is comparable to the precision of keyword extraction from web pages for content-targeted advertising.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Brin</author>
<author>L Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual web search engine.</title>
<date>1998</date>
<journal>Computer Networks and ISDN Systems,</journal>
<pages>30--1</pages>
<contexts>
<context position="3553" citStr="Brin and Page, 1998" startWordPosition="540" endWordPosition="543">ted. Yih et al. (2006) use logistic regression to extract keywords from web pages for content-targeted advertising, which has the most similar application to our work. However, due to the lack of human annotation on Twitter messages, we have to adopt an unsupervised strategy. For unsupervised keyword extraction, TFIDF ranking is a popular method, and its effectiveness has been shown in (Hulth, 2003; Yih et al., 2006). TextRank and its variants (Mihalcea and Tarau, 2004; Wan et al., 2007; Liu et al., 2009) are graph-based text ranking models, which are derived from Google’s PageRank algorithm (Brin and Page, 1998). It outperforms TFIDF ranking on traditional keyword extraction tasks. However, previous work on both TFIDF ranking and TextRank has been done mainly on academic papers, spoken documents or 689 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 689–692, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics web pages, whose language style is more formal (or, less “conversational”) than that of Twitter messages. Twitter messages contain large amounts of “noise” like emoticons, internet slang words, abbreviations</context>
<context position="8858" citStr="Brin and Page, 1998" startWordPosition="1394" endWordPosition="1397"> between two candidate words if they co-exist within at least one message; the edge weight is set to be the total count of within-message co-occurrence of the two words throughout all messages of this user. Starting with an arbitrarily assigned value (e.g. 1.0), the rank value R(Vi) of the candidate word at vertex Vi is updated iteratively according to the following equation, R(Vi) = (1−d)+d VjEE(Vi) � r-VkEE(Vj) wjk wji R(Vj ) where wji is the weight of the edge that links Vj and Vi, E(Vi) is the set of vertices which Vi is connected to, and d is a damping factor that is usually set to 0.85 (Brin and Page, 1998). The rank update iteration continues until convergence. The candidate words are then sorted according to their rank values. Words with top-N rank values are selected as tags for this user. 4 Experiment 4.1 Experimental Setup We employed the Twitter API to download Twitter messages. A unigram English language model was Precision (%) TFIDF TextRank top-1 59.6 67.3 top-3 61.5 66.0 top-5 61.2 63.0 top-10 59.0 58.3 Table 1: Tagging precision on all users in the test set used to filter out non-English users. We obtained messages from 11,376 Twitter users, each of them had 180 to 200 messages. The w</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>S. Brin and L. Page. 1998. The anatomy of a large-scale hypertextual web search engine. Computer Networks and ISDN Systems, 30(1-7):107–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hulth</author>
</authors>
<title>Improved automatic keyword extraction given more linguistic knowledge.</title>
<date>2003</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>216--223</pages>
<contexts>
<context position="2696" citStr="Hulth, 2003" startWordPosition="405" endWordPosition="406">eries of text preprocessing steps. Experiments on randomly selected users showed good results with TextRank, but high variability among users. 2 Related Work Research work related to Twitter message analysis includes a user sentiment study (Jansen et al., 2009) and information retrieval indexing. To our knowledge, no previously published research has yet addressed problems on tagging user’s personal interests from Twitter messages via keyword extraction, though several studies have looked at keyword extraction using other genres. For supervised keyword extraction, (Turney, 2000; Turney, 2003; Hulth, 2003; Yih et al., 2006; Liu et al., 2008) employed TFIDF or its variants with Part-of-Speech (POS), capitalization, phrase and sentence length, etc., as features to train keyword extraction models, and discriminative training is usually adopted. Yih et al. (2006) use logistic regression to extract keywords from web pages for content-targeted advertising, which has the most similar application to our work. However, due to the lack of human annotation on Twitter messages, we have to adopt an unsupervised strategy. For unsupervised keyword extraction, TFIDF ranking is a popular method, and its effect</context>
</contexts>
<marker>Hulth, 2003</marker>
<rawString>A. Hulth. 2003. Improved automatic keyword extraction given more linguistic knowledge. In Proc. EMNLP, pages 216–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Jansen</author>
<author>M Zhang</author>
<author>K Sobel</author>
<author>A Chowdury</author>
</authors>
<title>Twitter power: Tweets as electronic word of mouth.</title>
<date>2009</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>60</volume>
<issue>11</issue>
<contexts>
<context position="2346" citStr="Jansen et al., 2009" startWordPosition="352" endWordPosition="355">nd concerns. We formulate this problem as a keyword extraction task, by selecting words from each individual user’s Twitter messages as his/her tags. Due to the lack of human generated annotations, we employ an unsupervised strategy. Specifically, we apply TFIDF ranking and TextRank (Mihalcea and Tarau, 2004) keyword extraction on Twitter messages after a series of text preprocessing steps. Experiments on randomly selected users showed good results with TextRank, but high variability among users. 2 Related Work Research work related to Twitter message analysis includes a user sentiment study (Jansen et al., 2009) and information retrieval indexing. To our knowledge, no previously published research has yet addressed problems on tagging user’s personal interests from Twitter messages via keyword extraction, though several studies have looked at keyword extraction using other genres. For supervised keyword extraction, (Turney, 2000; Turney, 2003; Hulth, 2003; Yih et al., 2006; Liu et al., 2008) employed TFIDF or its variants with Part-of-Speech (POS), capitalization, phrase and sentence length, etc., as features to train keyword extraction models, and discriminative training is usually adopted. Yih et a</context>
</contexts>
<marker>Jansen, Zhang, Sobel, Chowdury, 2009</marker>
<rawString>B. J. Jansen, M. Zhang, K. Sobel, and A. Chowdury. 2009. Twitter power: Tweets as electronic word of mouth. Journal of the American Society for Information Science and Technology, 60(11):2169–2188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Liu</author>
<author>F Liu</author>
<author>Y Liu</author>
</authors>
<title>Automatic keyword extraction for the meeting corpus using supervised approach and bigram expansion.</title>
<date>2008</date>
<booktitle>In Proc. IEEE SLT,</booktitle>
<pages>181--184</pages>
<contexts>
<context position="2733" citStr="Liu et al., 2008" startWordPosition="411" endWordPosition="414">ps. Experiments on randomly selected users showed good results with TextRank, but high variability among users. 2 Related Work Research work related to Twitter message analysis includes a user sentiment study (Jansen et al., 2009) and information retrieval indexing. To our knowledge, no previously published research has yet addressed problems on tagging user’s personal interests from Twitter messages via keyword extraction, though several studies have looked at keyword extraction using other genres. For supervised keyword extraction, (Turney, 2000; Turney, 2003; Hulth, 2003; Yih et al., 2006; Liu et al., 2008) employed TFIDF or its variants with Part-of-Speech (POS), capitalization, phrase and sentence length, etc., as features to train keyword extraction models, and discriminative training is usually adopted. Yih et al. (2006) use logistic regression to extract keywords from web pages for content-targeted advertising, which has the most similar application to our work. However, due to the lack of human annotation on Twitter messages, we have to adopt an unsupervised strategy. For unsupervised keyword extraction, TFIDF ranking is a popular method, and its effectiveness has been shown in (Hulth, 200</context>
</contexts>
<marker>Liu, Liu, Liu, 2008</marker>
<rawString>F. Liu, F. Liu, and Y. Liu. 2008. Automatic keyword extraction for the meeting corpus using supervised approach and bigram expansion. In Proc. IEEE SLT, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Liu</author>
<author>D Pennell</author>
<author>F Liu</author>
<author>Y Liu</author>
</authors>
<title>Unsupervised approaches for automatic keyword extraction using meeting transcripts.</title>
<date>2009</date>
<booktitle>In Proc. HLT/NAACL,</booktitle>
<pages>620--628</pages>
<contexts>
<context position="3443" citStr="Liu et al., 2009" startWordPosition="524" endWordPosition="527">e length, etc., as features to train keyword extraction models, and discriminative training is usually adopted. Yih et al. (2006) use logistic regression to extract keywords from web pages for content-targeted advertising, which has the most similar application to our work. However, due to the lack of human annotation on Twitter messages, we have to adopt an unsupervised strategy. For unsupervised keyword extraction, TFIDF ranking is a popular method, and its effectiveness has been shown in (Hulth, 2003; Yih et al., 2006). TextRank and its variants (Mihalcea and Tarau, 2004; Wan et al., 2007; Liu et al., 2009) are graph-based text ranking models, which are derived from Google’s PageRank algorithm (Brin and Page, 1998). It outperforms TFIDF ranking on traditional keyword extraction tasks. However, previous work on both TFIDF ranking and TextRank has been done mainly on academic papers, spoken documents or 689 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 689–692, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics web pages, whose language style is more formal (or, less “conversational”) than that of Twitter m</context>
</contexts>
<marker>Liu, Pennell, Liu, Liu, 2009</marker>
<rawString>F. Liu, D. Pennell, F. Liu, and Y. Liu. 2009. Unsupervised approaches for automatic keyword extraction using meeting transcripts. In Proc. HLT/NAACL, pages 620–628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Tarau</author>
</authors>
<title>Textrank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="2036" citStr="Mihalcea and Tarau, 2004" startWordPosition="304" endWordPosition="307"> suggestions, either a person who shares similar interests (for expanding their social network) or a company providing products or services the user is interested in (for personalized advertisement). In this work, we focus on automatically generating personalized annotation tags to label Twitter user’s interests and concerns. We formulate this problem as a keyword extraction task, by selecting words from each individual user’s Twitter messages as his/her tags. Due to the lack of human generated annotations, we employ an unsupervised strategy. Specifically, we apply TFIDF ranking and TextRank (Mihalcea and Tarau, 2004) keyword extraction on Twitter messages after a series of text preprocessing steps. Experiments on randomly selected users showed good results with TextRank, but high variability among users. 2 Related Work Research work related to Twitter message analysis includes a user sentiment study (Jansen et al., 2009) and information retrieval indexing. To our knowledge, no previously published research has yet addressed problems on tagging user’s personal interests from Twitter messages via keyword extraction, though several studies have looked at keyword extraction using other genres. For supervised </context>
<context position="3406" citStr="Mihalcea and Tarau, 2004" startWordPosition="515" endWordPosition="519">ch (POS), capitalization, phrase and sentence length, etc., as features to train keyword extraction models, and discriminative training is usually adopted. Yih et al. (2006) use logistic regression to extract keywords from web pages for content-targeted advertising, which has the most similar application to our work. However, due to the lack of human annotation on Twitter messages, we have to adopt an unsupervised strategy. For unsupervised keyword extraction, TFIDF ranking is a popular method, and its effectiveness has been shown in (Hulth, 2003; Yih et al., 2006). TextRank and its variants (Mihalcea and Tarau, 2004; Wan et al., 2007; Liu et al., 2009) are graph-based text ranking models, which are derived from Google’s PageRank algorithm (Brin and Page, 1998). It outperforms TFIDF ranking on traditional keyword extraction tasks. However, previous work on both TFIDF ranking and TextRank has been done mainly on academic papers, spoken documents or 689 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 689–692, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics web pages, whose language style is more formal (or, less “co</context>
<context position="7971" citStr="Mihalcea and Tarau, 2004" startWordPosition="1233" endWordPosition="1236">filtering Removing emoticons Messages from one Twitter user 690 3.2 User Tag Extraction 3.2.1 TFIDF ranking In the TFIDF ranking algorithm, messages from user u are put together as one document. The TFIDF value of word i from this user’s messages is computed as t f idfi,u = ni,u U ,u log( ) r-j nj Ui where ni,u is the count of word i in user u’s messages, Ui is the number of users whose messages contain word i, and U is the total number of users in the Twitter corpus. For each user, words with top N TFIDF values are selected as his/her tags. 3.2.2 TextRank According to the TextRank algorithm (Mihalcea and Tarau, 2004), each candidate word is represented by a vertex in the graph; edges are added between two candidate words according to their cooccurrence. In the context of user tag extraction, we build a TextRank graph with undirected edges for each Twitter user. One edge is added between two candidate words if they co-exist within at least one message; the edge weight is set to be the total count of within-message co-occurrence of the two words throughout all messages of this user. Starting with an arbitrarily assigned value (e.g. 1.0), the rank value R(Vi) of the candidate word at vertex Vi is updated ite</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>R. Mihalcea and P. Tarau. 2004. Textrank: Bringing order into texts. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>C D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy partof-speech tagger.</title>
<date>2000</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>63--77</pages>
<contexts>
<context position="6718" citStr="Toutanova and Manning, 2000" startWordPosition="1039" endWordPosition="1042">id candidate words for user tags. The second set includes 67 abbreviations of function words that usually form grammatical parts in a sentence, such as “im” (i’m), “abt” (about). Simply removing them will affect the POS tagging. Thus, the abbreviations in both these sets are replaced with the corresponding complete words or phrases. The third set includes 4576 phrase abbreviations that are usually separable parts of a sentence that do not directly indicate discussion topics, such as “lol” (laugh out loud), “clm” (cool like me), which are removed in this step. We apply the Stanford POS tagger (Toutanova and Manning, 2000) on Twitter messages, and only select nouns and adjectives as valid candidates for user tags. At the end of the preprocessing pipeline, the candidate words are processed with the rulebased Porter stemmer2 and stopwords are filtered using a publicly available list.3 1www.noslang.com/dictionary 2tartarus.org/ martin/PorterStemmer/ 3armandbrahaj.blog.al/2009/04/14/ list-of-english-stop-words/ TFIDF ranking / TextRank Personalized annotation tags for the Twitter user Preprocessing Stemming and stopword removing Substituting/removing internet slang words and abbreviations Removing replying messages</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>K. Toutanova and C. D. Manning. 2000. Enriching the knowledge sources used in a maximum entropy partof-speech tagger. In Proc. EMNLP, pages 63–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Learning algorithms for keyphrase.</title>
<date>2000</date>
<journal>Information Retrieval,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="2669" citStr="Turney, 2000" startWordPosition="401" endWordPosition="402">n Twitter messages after a series of text preprocessing steps. Experiments on randomly selected users showed good results with TextRank, but high variability among users. 2 Related Work Research work related to Twitter message analysis includes a user sentiment study (Jansen et al., 2009) and information retrieval indexing. To our knowledge, no previously published research has yet addressed problems on tagging user’s personal interests from Twitter messages via keyword extraction, though several studies have looked at keyword extraction using other genres. For supervised keyword extraction, (Turney, 2000; Turney, 2003; Hulth, 2003; Yih et al., 2006; Liu et al., 2008) employed TFIDF or its variants with Part-of-Speech (POS), capitalization, phrase and sentence length, etc., as features to train keyword extraction models, and discriminative training is usually adopted. Yih et al. (2006) use logistic regression to extract keywords from web pages for content-targeted advertising, which has the most similar application to our work. However, due to the lack of human annotation on Twitter messages, we have to adopt an unsupervised strategy. For unsupervised keyword extraction, TFIDF ranking is a pop</context>
</contexts>
<marker>Turney, 2000</marker>
<rawString>P. D. Turney. 2000. Learning algorithms for keyphrase. Information Retrieval, 2(4):303–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Coherent keyphrase extraction via web mining.</title>
<date>2003</date>
<booktitle>In Proc. IJCAI,</booktitle>
<pages>434--439</pages>
<contexts>
<context position="2683" citStr="Turney, 2003" startWordPosition="403" endWordPosition="404">ages after a series of text preprocessing steps. Experiments on randomly selected users showed good results with TextRank, but high variability among users. 2 Related Work Research work related to Twitter message analysis includes a user sentiment study (Jansen et al., 2009) and information retrieval indexing. To our knowledge, no previously published research has yet addressed problems on tagging user’s personal interests from Twitter messages via keyword extraction, though several studies have looked at keyword extraction using other genres. For supervised keyword extraction, (Turney, 2000; Turney, 2003; Hulth, 2003; Yih et al., 2006; Liu et al., 2008) employed TFIDF or its variants with Part-of-Speech (POS), capitalization, phrase and sentence length, etc., as features to train keyword extraction models, and discriminative training is usually adopted. Yih et al. (2006) use logistic regression to extract keywords from web pages for content-targeted advertising, which has the most similar application to our work. However, due to the lack of human annotation on Twitter messages, we have to adopt an unsupervised strategy. For unsupervised keyword extraction, TFIDF ranking is a popular method, a</context>
</contexts>
<marker>Turney, 2003</marker>
<rawString>P. D. Turney. 2003. Coherent keyphrase extraction via web mining. In Proc. IJCAI, pages 434–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
<author>J Yang</author>
<author>J Xiao</author>
</authors>
<title>Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction.</title>
<date>2007</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>552--559</pages>
<contexts>
<context position="3424" citStr="Wan et al., 2007" startWordPosition="520" endWordPosition="523">phrase and sentence length, etc., as features to train keyword extraction models, and discriminative training is usually adopted. Yih et al. (2006) use logistic regression to extract keywords from web pages for content-targeted advertising, which has the most similar application to our work. However, due to the lack of human annotation on Twitter messages, we have to adopt an unsupervised strategy. For unsupervised keyword extraction, TFIDF ranking is a popular method, and its effectiveness has been shown in (Hulth, 2003; Yih et al., 2006). TextRank and its variants (Mihalcea and Tarau, 2004; Wan et al., 2007; Liu et al., 2009) are graph-based text ranking models, which are derived from Google’s PageRank algorithm (Brin and Page, 1998). It outperforms TFIDF ranking on traditional keyword extraction tasks. However, previous work on both TFIDF ranking and TextRank has been done mainly on academic papers, spoken documents or 689 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 689–692, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics web pages, whose language style is more formal (or, less “conversational”) tha</context>
</contexts>
<marker>Wan, Yang, Xiao, 2007</marker>
<rawString>X. Wan, J. Yang, and J. Xiao. 2007. Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction. In Proc. ACL, pages 552–559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W-T Yih</author>
<author>J Goodman</author>
<author>V R Carvalho</author>
</authors>
<title>Finding advertising keywords on web pages.</title>
<date>2006</date>
<booktitle>In Proc. 15th International Conference on World Wide Web,</booktitle>
<pages>213--222</pages>
<contexts>
<context position="2714" citStr="Yih et al., 2006" startWordPosition="407" endWordPosition="410"> preprocessing steps. Experiments on randomly selected users showed good results with TextRank, but high variability among users. 2 Related Work Research work related to Twitter message analysis includes a user sentiment study (Jansen et al., 2009) and information retrieval indexing. To our knowledge, no previously published research has yet addressed problems on tagging user’s personal interests from Twitter messages via keyword extraction, though several studies have looked at keyword extraction using other genres. For supervised keyword extraction, (Turney, 2000; Turney, 2003; Hulth, 2003; Yih et al., 2006; Liu et al., 2008) employed TFIDF or its variants with Part-of-Speech (POS), capitalization, phrase and sentence length, etc., as features to train keyword extraction models, and discriminative training is usually adopted. Yih et al. (2006) use logistic regression to extract keywords from web pages for content-targeted advertising, which has the most similar application to our work. However, due to the lack of human annotation on Twitter messages, we have to adopt an unsupervised strategy. For unsupervised keyword extraction, TFIDF ranking is a popular method, and its effectiveness has been s</context>
<context position="9597" citStr="Yih et al., 2006" startWordPosition="1518" endWordPosition="1521">s. Words with top-N rank values are selected as tags for this user. 4 Experiment 4.1 Experimental Setup We employed the Twitter API to download Twitter messages. A unigram English language model was Precision (%) TFIDF TextRank top-1 59.6 67.3 top-3 61.5 66.0 top-5 61.2 63.0 top-10 59.0 58.3 Table 1: Tagging precision on all users in the test set used to filter out non-English users. We obtained messages from 11,376 Twitter users, each of them had 180 to 200 messages. The word IDF for TFIDF ranking was computed over these users. We adopted an evaluation measure similar to the one proposed in (Yih et al., 2006) for identifying advertising keywords on web pages, which emphasizes precision. We randomly selected 156 Twitter users to evaluate the top-N precision of TFIDF ranking and TextRank. After we obtained the topN outputs from the system, three human evaluators were asked to judge whether the output tags from the two systems (unidentified) reflected the corresponding Twitter user’s interests or concerns according to the full set of his/her messages.4 We adopted a conservative standard in the evaluation: when a person’s name is extracted as a user tag, which is frequent among Twitter users, we judge</context>
</contexts>
<marker>Yih, Goodman, Carvalho, 2006</marker>
<rawString>W.-T. Yih, J. Goodman, and V. R. Carvalho. 2006. Finding advertising keywords on web pages. In Proc. 15th International Conference on World Wide Web, pages 213–222.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>