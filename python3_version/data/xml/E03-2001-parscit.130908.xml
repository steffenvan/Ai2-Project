<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000252">
<title confidence="0.997222">
Talking through procedures:
An intelligent Space Station procedure assistant
</title>
<author confidence="0.981723666666667">
G. Aistl, J. Dowding&apos;, B. A. Hockey&apos;, M. Rayner&apos;, J. Hieronymus&apos;,
D. Bohus2, B. Boven3, N. Blaylock4, E. Campana4,
S. Early5, G. Gorre116, and S. Phan7
</author>
<affiliation confidence="0.737774333333333">
IRIACS/NASA Ames Research Center
{aist , jdowding, bahockey,
2Carnegie Mellon University
</affiliation>
<email confidence="0.855818">
dbohus@cs.cmu.edu
</email>
<affiliation confidence="0.940478">
4University of Rochester
</affiliation>
<email confidence="0.929761">
blaylock@cs.rochester.edu
ecampana@bcs.rochester.edu
</email>
<author confidence="0.451592">
Linkopings Universitet
</author>
<email confidence="0.665798">
gengo@ida.liu.se
</email>
<author confidence="0.23209">
mrayner, j imh}@riacs . edu
</author>
<affiliation confidence="0.549952">
3Kalamazoo College
</affiliation>
<email confidence="0.739107">
bboven@acm.org
</email>
<affiliation confidence="0.7551095">
5DeAnza College/NASA Ames
Research Center
searly@mail.arc.nasa.gov
7Santa Clara University
</affiliation>
<email confidence="0.994455">
nphan@scudc.scu.edu
</email>
<sectionHeader confidence="0.997506" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999699">
We present a prototype system aimed at
providing spoken dialogue support for
complex procedures aboard the Interna-
tional Space Station. The system allows
navigation one line at a time or in larger
steps. Other user functions include issu-
ing spoken corrections, requesting images
and diagrams, recording voice notes and
spoken alarms, and controlling audio vol-
ume.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999780965517241">
The International Space Station recently entered
its second year as the first permanent human
presence in space. Astronauts on board Station
engage in a wide variety of tasks on orbit, includ-
ing medical procedures, extravehicular activity
(EVA), scientific payloads, and station repair and
maintenance. These tasks are documented in the
form of hierarchically organized procedures. In
some cases, a procedure will be performed by
one astronaut with another astronaut reading the
procedure out loud; in other cases the astronaut
will use the procedure and reference a paper (or
onscreen) copy of the procedure. The RIALIST
group has been developing a spoken dialogue
system for providing assistance with Space Sta-
tion procedures. This system has been developed
in a cooperative, iterative endeavor with substan-
tial input from astronauts, trainers, engineers, and
other NASA personnel. The first version of the
system operated on a simplified (and invented)
procedure for unpacking and operating a digital
camera (Aist et al. 2002), and included speech
input and speech output only. In this paper we
report on the current version of the checklist as-
sistant as of December 2002, which is set up to
run on XML-formatted actual Space Station pro-
cedures and includes speech input and multimo-
dal output (speech, images, and display of
HTML-formatted text.)
</bodyText>
<sectionHeader confidence="0.981587" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.999980857142857">
The current crew on the ISS is limited to 3 astro-
nauts. During pre-flight training, astronauts re-
ceive training on basic systems operation, and
practice carrying out carefully designed proce-
dures to handle both nominal and off-nominal
operations. The number and variety of the pro-
cedures, as well as the duration of ISS missions,
</bodyText>
<page confidence="0.996868">
187
</page>
<bodyText confidence="0.999787944444445">
precludes the kind of detailed training common
to shorter Apollo and Shuttle missions. Astro-
nauts on Station need to carry out procedures that
they may not have trained on specifically in ad-
vance, or may not have practiced for a consider-
able time. Current practice may require the
astronaut to follow through the procedure using a
text or computer monitor, or to have a second
astronaut read the procedure out loud to the one
executing it.
Our approach is to develop a spoken dia-
logue system provide assistance in reading the
procedure, tracking the progress through the pro-
cedure, and providing other assistance to support
correct and complete execution. The dialogue
system would thus free up the second astronaut
for other tasks, increasing Space Station utiliza-
tion.
</bodyText>
<sectionHeader confidence="0.951369" genericHeader="method">
3 System description
</sectionHeader>
<bodyText confidence="0.9999622">
The fundamental architecture of the system con-
sists of several components: audio processing,
speech recognition, language understanding, dia-
logue management, HTML and language genera-
tion, and visual display and speech synthesis.
</bodyText>
<subsectionHeader confidence="0.997636">
3.1 Audio processing, speech recognition
</subsectionHeader>
<bodyText confidence="0.999175142857143">
We use noise-canceling headset microphones for
audio input, transmitted via Sennheiser wireless
units to a laptop. Speech recognition is done with
Nuance 8 using a context-free language model
constructed from a unification grammar and then
compiled into a recognition model (Dowding et
al. 1993; Rayner, Dowding, and Hockey 2001).
</bodyText>
<subsectionHeader confidence="0.999772">
3.2 Parsing and interpretation
</subsectionHeader>
<bodyText confidence="0.999953333333333">
The output of the speech recognizer is parsed
using SRI&apos; s Gemini parser. The text of the rec-
ognized speech and the resulting parse are then
fed to Alterf (Rayner and Hockey 2003), a robust
interpretation module which combines statistical
and rule-based interpretation to produce a se-
quence of tokens, such as &amp;quot;[load, water]&apos;. These
tokens are then assembled into predicate-
argument structure such as &amp;quot;load(water)&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.994744">
3.3 Dialogue management
</subsectionHeader>
<bodyText confidence="0.969309833333333">
We adopt a TRIPS-style division (Allen, Fergu-
son, and Stent 2001) of dialogue management
into three sections: input management, behavior
management, and output management (Figure 1).
In the December 2002 Checklist architecture,
however, there are multiple behavior agents, each
specialized by dialogue task: handling annota-
tions (e.g. pictures and voice notes), manipulat-
ing system settings (e.g. volume), and handling
procedure-based tasks (e.g. navigation). The dia-
logue input manager coordinates the interactions
between the multiple behavior agents.&apos;
</bodyText>
<figureCaption confidence="0.990169">
Figure 1. December 2002 Checklist architecture.
</figureCaption>
<subsectionHeader confidence="0.388012">
3.4 Dialogue Input Management
</subsectionHeader>
<bodyText confidence="0.999934266666667">
Each behavior agent has an agenda (Rudnicky
and Xu 1999) of the types of input it is expecting.
The behavior agents are maintained in a priority
queue according to recency of use. Incoming in-
terpretations such as &amp;quot;load(water)&amp;quot; or &amp;quot;in-
crease(volume)&amp;quot; are matched against each
behavior agent in turn. When a match is found,
that behavior agent is promoted to the top of the
queue and the message is dispatched to the agent.
This scheme allows us to coordinate multiple
behavior agents. Although in the December 2002
implementation the agenda is fixed for each dia-
logue agent, a better extension would make the
agendas dynamic in response to changes in dia-
logue state.
</bodyText>
<footnote confidence="0.88116975">
1 At one point we were labeling each behavior agent a &amp;quot;dia-
logue manager&amp;quot;. This resulted in calling the input manager
the &amp;quot;dialogue manager manager&amp;quot;; such reduplicative termi-
nology seemed baroque, so we fixed it.
</footnote>
<figure confidence="0.998928">
Input
Manager
/117
\I Annotations
\ \ 1
41
Speech Output
Synthesizer Manager
Visual
Display
Procedures
Speech Parser
Recognizer
Audio
</figure>
<page confidence="0.914701">
188
</page>
<subsectionHeader confidence="0.867581">
3.5 Dialogue Behavior Agents
</subsectionHeader>
<bodyText confidence="0.999697628571428">
The Checklist system is capable of a number of
functions, as provided by the following dialogue
behavior agents.
Procedure agent (RavenClaw â€” Bohus and
Rudnicky 2002). Available functions include the
following.
Loading a procedure by saying, for example,
&amp;quot;Load water procedure&amp;quot;. The procedure is loaded
from disk as a XML document and converted
into HTML via XSL, and then rendered using
Cascading Style Sheets (CSS). At the same time,
the procedure is processed using XSL into a task
description for use by the task-oriented dialogue
management component (RavenClaw).
Asking yes/no questions of the user, for example
&amp;quot;Are you ready to begin the procedure?&amp;quot; when
indicated by task constraints or by the structure
of the procedure itself.
Navigating through the procedure one line at a
time (&amp;quot;next line&amp;quot;) or one numbered step at a time
(&amp;quot;next step&amp;quot;), and returning to previous lines
(&amp;quot;previous line&amp;quot;) or previous numbered steps
(&amp;quot;previous step&amp;quot;).
Annotation agent handles a variety of tasks.
Requesting a list of available images by saying
&amp;quot;What pictures do you have?&amp;quot;.
Requesting a specific image by saying, for exam-
ple, &amp;quot;Show me the small waste water bag.&amp;quot;
Taking a voice note by saying, for example,
&amp;quot;Take a voice note.&amp;quot;
Audio agent handles request to change settings.
Setting an audio alarm by saying, for example,
&amp;quot;Set an alarm for three minutes from now.&amp;quot;
Controlling audio output volume by saying, for
example, &amp;quot;Speak up&amp;quot;, or &amp;quot;Quieter.&amp;quot;
</bodyText>
<subsectionHeader confidence="0.931945">
3.6 Dialogue Output Management
</subsectionHeader>
<bodyText confidence="0.9998618">
Output requests from dialogue agents enter a
common output queue, where they are trans-
formed into a series of one or more display ac-
tions and/or strings of text to speak. The resulting
actions are played one at a time.
</bodyText>
<subsectionHeader confidence="0.994263">
3.7 Visual display
</subsectionHeader>
<bodyText confidence="0.999973666666667">
The procedure is displayed as an HTML docu-
ment (Figure 2), with the current step highlighted
in light blue, and the current substep (if any) dis-
played in black text on a white background (to
contrast with the light blue of the current (larger)
step).
</bodyText>
<subsectionHeader confidence="0.998665">
3.8 Speech synthesis
</subsectionHeader>
<bodyText confidence="0.999826">
Speech synthesis is handled by AT&amp;T&apos;s speech
synthesizer, equipped with a customized pronun-
ciation dictionary.
</bodyText>
<sectionHeader confidence="0.984772" genericHeader="method">
4 Particular challenges
</sectionHeader>
<bodyText confidence="0.999374666666667">
Particular challenges to be addressed for the
Checklist system include telling whether the user
is speaking to the computer or to another person,
also known as &amp;quot;open mic&amp;quot; (work completed and
currently being evaluated), and robustness to the
noisy environment on board the Space Station.
</bodyText>
<figure confidence="0.950282">
5 Example dialogue
[System] Welcome to the Intelligent Procedure Assis-
tant.
[User] Load water sampling procedure.
</figure>
<figureCaption confidence="0.957710666666667">
[System] I am loading the procedure. Okay, I&apos;ve
loaded it. The full title of this procedure is ... (the
title). Are you ready to begin the procedure?
</figureCaption>
<figure confidence="0.720016833333333">
[User] Yes.
[System] I have a note at this point. Shall I read it?
[User] No.
[System] Okay, now moving to step 1. (reads step 1
out loud).
6 Demonstration scenario
</figure>
<bodyText confidence="0.999623285714286">
Conference attendees will be able to use the sys-
tem as an assistant while performing a (simu-
lated) Space Station task, such as collecting and
analyzing a water sample from the drinking water
supply. Participants may also have an opportu-
nity to annotate the procedure using voice notes,
and use other features as time permits.
</bodyText>
<page confidence="0.993013">
189
</page>
<figure confidence="0.996972583333333">
speak up
pommand Accepted
Fie 215Ano
- , aleron.i,oq
1 took CAil Rc1aie Left I Rotate Right I
d6&amp;quot;
Cosiker1501&amp;_
aler7Gai San,.
Pial1,0151
Aria-pai Rag
Callia1 125 vale
lahshi Sample In Fipry
n
Sc11152 25 Ad. in
Small Wide OM. Bag
Analyses Bag
Coded 1000 511. n
Vacro-Sampheincal-
,IfiPtAnelyem Bag
Rpm I - &amp;quot;Wm Cads [WM [kr.= agâ–  1111â€¢0-1,1e E. Fri *lignâ– 
Unstow nom Wan &amp;Awl&amp; adv! Archival (Wtf &amp; A) 1:35
Potable Water Collection Subpack (one), Sharpie Pm, Water
1,licrobial,gyKst (MY)
Note
</figure>
<figureCaption confidence="0.735097333333333">
SRV-K Water Tam SRV-K heat&amp; on before
collecting water samples Start sampling only after
heating cycle is completed Each heating cycle
requres 15nvnutes for pasteirnation of 525 inL of
water. One debvery = 25 mi.
SVO-Z,V Tbse hand pump may be used to p. ovate
suffment pressure to peen* water sample
collectson. Th,re Is no device for accurate SVO-PV
water amount measurement Crewmember wit be
req&amp;red to perform visual estimation of 25 a&amp; of
flrssbsvateraod1EKnoLoiodt25rnLsvmp1eobo
comparison to SRV-K samples
</figureCaption>
<figure confidence="0.9790662">
Caution
To avoid contananation, use new poLable Water sampler
for each tap
2 Wipe appropnate tap SRV-K (SrO-ZV) with Dinsfectant Wspo.
Dsscard Wipe
3 Remove one patable water sampler from Water Sampler &amp;
Aral-aver (WI &amp; A) Subpack and remove frosts protective
package
Place potable water sampler package in WS &amp; A
[111A21 Araaligair.6111.1L&apos; eaca02 I Alba:ester I @jeed5K3s..1 Vooree/spe...1 Mckrespett I 111110cytela,h1.0h,....
</figure>
<figureCaption confidence="0.9909225">
Figure 2. Visual display of December 2002
Checklist system.
</figureCaption>
<sectionHeader confidence="0.995073" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998306029411765">
Aist, G., Dowding, J., Hockey, B.A., Hierony-
mus, J. 2002. A Demonstration of a Spoken
Dialogue Interface to an Intelligent Procedure
Assistant for Astronaut Training and Support,
ACL 2002, Demo Session, Philadelphia, July
7-12.
Allen, J., Ferguson, G., and Stent, A. 2001. An
architecture for more realistic conversational
systems. In Proceedings of Intelligent User In-
terfaces 2001 (IUI-01), Santa Fe, NM, January
14-17, 2001.
Bohus, D., and Rudnicky, A. 2002. LARRI: A
Language-Based Maintenance and Repair As-
sistant. IDS-2002, Kloster Irsee, Germany.
John Dowding, Jean Mark Gawron, Doug Ap-
pelt, John Bear, Lynn Cherny, Robert Moore,
Douglas Moran. 1993. Gemini: A Natural
Language System For Spoken-Language Un-
derstanding. Meeting of the Association for
Computational Linguistics.
Rayner, M., Dowding, J., and Hockey, B. A.
2001. A baseline method for compiling typed
unification grammars into context-free lan-
guage models. Proceedings of Eurospeech
2001, Aalborg, Denmark, pp. 729-732.
Rayner, M., and Hockey, B. A. 2003. Transpar-
ent combination of rule-based and data-driven
approaches in a speech understanding architec-
ture. EACL 2003, Budapest, Hungary.
Rudnicky, A. and Xu W. 1999. An agenda-based
dialog management architecture for spoken
language systems. IEEE Automatic Speech
Recognition and Understanding Workshop,
1999, p 1-337.
</reference>
<page confidence="0.997858">
190
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.016312">
<title confidence="0.997323">Talking through procedures: An intelligent Space Station procedure assistant</title>
<author confidence="0.999961">J Dowding&apos;</author>
<author confidence="0.999961">B A Hockey&apos;</author>
<author confidence="0.999961">M Rayner&apos;</author>
<author confidence="0.999961">J Hieronymus&apos;</author>
<affiliation confidence="0.8338922">B. N. E. G. and S. Ames Research Center {aist , jdowding, bahockey, Mellon University</affiliation>
<email confidence="0.992205">dbohus@cs.cmu.edu</email>
<affiliation confidence="0.550956">of Rochester</affiliation>
<email confidence="0.9938625">blaylock@cs.rochester.eduecampana@bcs.rochester.edu</email>
<author confidence="0.376326">Linkopings Universitet</author>
<email confidence="0.451186">gengo@ida.liu.se</email>
<author confidence="0.4041">j imhriacs mrayner</author>
<affiliation confidence="0.495179">College</affiliation>
<email confidence="0.925964">bboven@acm.org</email>
<affiliation confidence="0.8133045">Ames Research Center</affiliation>
<email confidence="0.958116">searly@mail.arc.nasa.gov</email>
<affiliation confidence="0.99983">Clara University</affiliation>
<email confidence="0.998618">nphan@scudc.scu.edu</email>
<abstract confidence="0.982502090909091">We present a prototype system aimed at providing spoken dialogue support for complex procedures aboard the International Space Station. The system allows navigation one line at a time or in larger steps. Other user functions include issuing spoken corrections, requesting images and diagrams, recording voice notes and spoken alarms, and controlling audio volume.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Aist</author>
<author>J Dowding</author>
<author>B A Hockey</author>
<author>J Hieronymus</author>
</authors>
<title>A Demonstration of a Spoken Dialogue Interface to an Intelligent Procedure Assistant for Astronaut Training and Support,</title>
<date>2002</date>
<pages>7--12</pages>
<location>ACL</location>
<contexts>
<context position="2044" citStr="Aist et al. 2002" startWordPosition="285" endWordPosition="288">l be performed by one astronaut with another astronaut reading the procedure out loud; in other cases the astronaut will use the procedure and reference a paper (or onscreen) copy of the procedure. The RIALIST group has been developing a spoken dialogue system for providing assistance with Space Station procedures. This system has been developed in a cooperative, iterative endeavor with substantial input from astronauts, trainers, engineers, and other NASA personnel. The first version of the system operated on a simplified (and invented) procedure for unpacking and operating a digital camera (Aist et al. 2002), and included speech input and speech output only. In this paper we report on the current version of the checklist assistant as of December 2002, which is set up to run on XML-formatted actual Space Station procedures and includes speech input and multimodal output (speech, images, and display of HTML-formatted text.) 2 Motivation The current crew on the ISS is limited to 3 astronauts. During pre-flight training, astronauts receive training on basic systems operation, and practice carrying out carefully designed procedures to handle both nominal and off-nominal operations. The number and vari</context>
</contexts>
<marker>Aist, Dowding, Hockey, Hieronymus, 2002</marker>
<rawString>Aist, G., Dowding, J., Hockey, B.A., Hieronymus, J. 2002. A Demonstration of a Spoken Dialogue Interface to an Intelligent Procedure Assistant for Astronaut Training and Support, ACL 2002, Demo Session, Philadelphia, July 7-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allen</author>
<author>G Ferguson</author>
<author>A Stent</author>
</authors>
<title>An architecture for more realistic conversational systems.</title>
<date>2001</date>
<booktitle>In Proceedings of Intelligent User Interfaces 2001 (IUI-01),</booktitle>
<location>Santa Fe, NM,</location>
<marker>Allen, Ferguson, Stent, 2001</marker>
<rawString>Allen, J., Ferguson, G., and Stent, A. 2001. An architecture for more realistic conversational systems. In Proceedings of Intelligent User Interfaces 2001 (IUI-01), Santa Fe, NM, January 14-17, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>A Rudnicky</author>
</authors>
<title>LARRI: A Language-Based Maintenance and Repair Assistant. IDS-2002,</title>
<date>2002</date>
<location>Kloster Irsee, Germany.</location>
<contexts>
<context position="6418" citStr="Bohus and Rudnicky 2002" startWordPosition="969" endWordPosition="972">nsion would make the agendas dynamic in response to changes in dialogue state. 1 At one point we were labeling each behavior agent a &amp;quot;dialogue manager&amp;quot;. This resulted in calling the input manager the &amp;quot;dialogue manager manager&amp;quot;; such reduplicative terminology seemed baroque, so we fixed it. Input Manager /117 \I Annotations \ \ 1 41 Speech Output Synthesizer Manager Visual Display Procedures Speech Parser Recognizer Audio 188 3.5 Dialogue Behavior Agents The Checklist system is capable of a number of functions, as provided by the following dialogue behavior agents. Procedure agent (RavenClaw â€” Bohus and Rudnicky 2002). Available functions include the following. Loading a procedure by saying, for example, &amp;quot;Load water procedure&amp;quot;. The procedure is loaded from disk as a XML document and converted into HTML via XSL, and then rendered using Cascading Style Sheets (CSS). At the same time, the procedure is processed using XSL into a task description for use by the task-oriented dialogue management component (RavenClaw). Asking yes/no questions of the user, for example &amp;quot;Are you ready to begin the procedure?&amp;quot; when indicated by task constraints or by the structure of the procedure itself. Navigating through the proce</context>
</contexts>
<marker>Bohus, Rudnicky, 2002</marker>
<rawString>Bohus, D., and Rudnicky, A. 2002. LARRI: A Language-Based Maintenance and Repair Assistant. IDS-2002, Kloster Irsee, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Dowding</author>
<author>Jean Mark Gawron</author>
<author>Doug Appelt</author>
<author>John Bear</author>
</authors>
<title>Gemini: A Natural Language System For Spoken-Language Understanding. Meeting of the Association for Computational Linguistics.</title>
<date>1993</date>
<location>Lynn Cherny, Robert Moore, Douglas Moran.</location>
<contexts>
<context position="4068" citStr="Dowding et al. 1993" startWordPosition="606" endWordPosition="609">easing Space Station utilization. 3 System description The fundamental architecture of the system consists of several components: audio processing, speech recognition, language understanding, dialogue management, HTML and language generation, and visual display and speech synthesis. 3.1 Audio processing, speech recognition We use noise-canceling headset microphones for audio input, transmitted via Sennheiser wireless units to a laptop. Speech recognition is done with Nuance 8 using a context-free language model constructed from a unification grammar and then compiled into a recognition model (Dowding et al. 1993; Rayner, Dowding, and Hockey 2001). 3.2 Parsing and interpretation The output of the speech recognizer is parsed using SRI&apos; s Gemini parser. The text of the recognized speech and the resulting parse are then fed to Alterf (Rayner and Hockey 2003), a robust interpretation module which combines statistical and rule-based interpretation to produce a sequence of tokens, such as &amp;quot;[load, water]&apos;. These tokens are then assembled into predicateargument structure such as &amp;quot;load(water)&amp;quot;. 3.3 Dialogue management We adopt a TRIPS-style division (Allen, Ferguson, and Stent 2001) of dialogue management into</context>
</contexts>
<marker>Dowding, Gawron, Appelt, Bear, 1993</marker>
<rawString>John Dowding, Jean Mark Gawron, Doug Appelt, John Bear, Lynn Cherny, Robert Moore, Douglas Moran. 1993. Gemini: A Natural Language System For Spoken-Language Understanding. Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>J Dowding</author>
<author>B A Hockey</author>
</authors>
<title>A baseline method for compiling typed unification grammars into context-free language models.</title>
<date>2001</date>
<booktitle>Proceedings of Eurospeech 2001,</booktitle>
<pages>729--732</pages>
<location>Aalborg, Denmark,</location>
<marker>Rayner, Dowding, Hockey, 2001</marker>
<rawString>Rayner, M., Dowding, J., and Hockey, B. A. 2001. A baseline method for compiling typed unification grammars into context-free language models. Proceedings of Eurospeech 2001, Aalborg, Denmark, pp. 729-732.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>B A Hockey</author>
</authors>
<title>Transparent combination of rule-based and data-driven approaches in a speech understanding architecture. EACL</title>
<date>2003</date>
<location>Budapest, Hungary.</location>
<contexts>
<context position="4315" citStr="Rayner and Hockey 2003" startWordPosition="648" endWordPosition="651">and visual display and speech synthesis. 3.1 Audio processing, speech recognition We use noise-canceling headset microphones for audio input, transmitted via Sennheiser wireless units to a laptop. Speech recognition is done with Nuance 8 using a context-free language model constructed from a unification grammar and then compiled into a recognition model (Dowding et al. 1993; Rayner, Dowding, and Hockey 2001). 3.2 Parsing and interpretation The output of the speech recognizer is parsed using SRI&apos; s Gemini parser. The text of the recognized speech and the resulting parse are then fed to Alterf (Rayner and Hockey 2003), a robust interpretation module which combines statistical and rule-based interpretation to produce a sequence of tokens, such as &amp;quot;[load, water]&apos;. These tokens are then assembled into predicateargument structure such as &amp;quot;load(water)&amp;quot;. 3.3 Dialogue management We adopt a TRIPS-style division (Allen, Ferguson, and Stent 2001) of dialogue management into three sections: input management, behavior management, and output management (Figure 1). In the December 2002 Checklist architecture, however, there are multiple behavior agents, each specialized by dialogue task: handling annotations (e.g. pictu</context>
</contexts>
<marker>Rayner, Hockey, 2003</marker>
<rawString>Rayner, M., and Hockey, B. A. 2003. Transparent combination of rule-based and data-driven approaches in a speech understanding architecture. EACL 2003, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rudnicky</author>
<author>W Xu</author>
</authors>
<title>An agenda-based dialog management architecture for spoken language systems.</title>
<date>1999</date>
<journal>IEEE Automatic Speech</journal>
<pages>1--337</pages>
<contexts>
<context position="5264" citStr="Rudnicky and Xu 1999" startWordPosition="780" endWordPosition="783"> dialogue management into three sections: input management, behavior management, and output management (Figure 1). In the December 2002 Checklist architecture, however, there are multiple behavior agents, each specialized by dialogue task: handling annotations (e.g. pictures and voice notes), manipulating system settings (e.g. volume), and handling procedure-based tasks (e.g. navigation). The dialogue input manager coordinates the interactions between the multiple behavior agents.&apos; Figure 1. December 2002 Checklist architecture. 3.4 Dialogue Input Management Each behavior agent has an agenda (Rudnicky and Xu 1999) of the types of input it is expecting. The behavior agents are maintained in a priority queue according to recency of use. Incoming interpretations such as &amp;quot;load(water)&amp;quot; or &amp;quot;increase(volume)&amp;quot; are matched against each behavior agent in turn. When a match is found, that behavior agent is promoted to the top of the queue and the message is dispatched to the agent. This scheme allows us to coordinate multiple behavior agents. Although in the December 2002 implementation the agenda is fixed for each dialogue agent, a better extension would make the agendas dynamic in response to changes in dialogu</context>
</contexts>
<marker>Rudnicky, Xu, 1999</marker>
<rawString>Rudnicky, A. and Xu W. 1999. An agenda-based dialog management architecture for spoken language systems. IEEE Automatic Speech Recognition and Understanding Workshop, 1999, p 1-337.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>