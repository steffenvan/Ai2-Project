<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.064386">
<title confidence="0.978714">
Adaptation Using Out-of-Domain Corpus within EBMT
</title>
<author confidence="0.915798">
Takao Doi, Eiichiro Sumita, Hirofumi Yamamoto
</author>
<affiliation confidence="0.627201">
ATR Spoken Language Translation Research Laboratories
</affiliation>
<address confidence="0.533398">
2-2-2 Hikaridai, Kansai Science City, Kyoto, 619-0288 Japan
</address>
<email confidence="0.98386">
{takao.doi, eiichiro.sumita, hirofumi.yamamoto}@atr.co.jp
</email>
<sectionHeader confidence="0.998564" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999657">
In order to boost the translation quality of
EBMT based on a small-sized bilingual cor-
pus, we use an out-of-domain bilingual corpus
and, in addition, the language model of an in-
domain monolingual corpus. We conducted
experiments with an EBMT system. The two
evaluation measures of the BLEU score and
the NIST score demonstrated the effect of us-
ing an out-of-domain bilingual corpus and the
possibility of using the language model.
</bodyText>
<sectionHeader confidence="0.999512" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99878376">
Example-Based Machine Translation (EBMT) is adapt-
able to new domains. If you simply prepare a bilingual
corpus of a new domain, you’ll get a translation system
for the domain. However, if only a small-sized corpus is
available, low translation quality is obtained. We ex-
plored methods to boost translation quality based on a
small-sized bilingual corpus in the domain. Among
these methods, we use an out-of-domain bilingual cor-
pus and, in addition, the language model (LM) of an in-
domain monolingual corpus. For accuracy of the LM, a
larger training set is better. The training set is a target
language corpus, which can be more easily prepared
than a bilingual corpus.
In prior works, statistical machine translation
(Brown, 1993) used not only LM but also translation
models. However, making a translation model requires a
bilingual corpus. On the other hand, in some studies on
multiple-translation selection, the LM of the target lan-
guage is used to calculate translation scores (Kaki,
1999; Callison-Burch, 2001). For adaptation, we use the
LM of an in-domain target language.
In the following sections, we describe the methods
using an out-of-domain bilingual corpus and an in-
domain monolingual corpus. Moreover, we report on
our experiments.
</bodyText>
<sectionHeader confidence="0.999338" genericHeader="method">
2 Adaptation Methods
</sectionHeader>
<bodyText confidence="0.9995915">
EBMT (Nagao, 1984) retrieves the translation ex-
amples that are most similar to an input expression and
adjusts the examples to obtain the translation. The
EBMT system in our approach retrieves not only in-
domain examples, but also out-of-domain examples.
When using out-of-domain examples, suitability to the
target domain is considered. We tried the following
three types of adaptation methods.
</bodyText>
<listItem confidence="0.730637">
(1) Merging equally
</listItem>
<bodyText confidence="0.894289428571429">
An in-domain corpus and an out-of-domain corpus are
simply merged and used without distinction.
(2) Merging with preference for in-domain corpus
An in-domain corpus and an out-of-domain corpus are
merged. However, when multiple examples with the
same similarity are retrieved, the in-domain examples
are used.
</bodyText>
<listItem confidence="0.947519">
(3) Using LM
</listItem>
<bodyText confidence="0.9955975625">
Beforehand, we make an LM of an in-domain target
language corpus and, according to the LM, assign a
probability to the target sentence of each out-of-domain
example.
In the example retrieval phase of the EBMT system,
two types of examples are handled differently.
(3-1) From in-domain examples, the most similar exam-
ples are retrieved.
(3-2) From out-of-domain examples, not only the most
similar examples but also other examples that are
nearly as similar are retrieved. In the retrieved ex-
amples, examples with the highest probabilities of
their target sentences by the LM are selected.
(3-3) From the results of both (3-1) and (3-2), the most
similar examples are selected. Examples of (3-1) are
used when the similarities are equal to each other.
</bodyText>
<sectionHeader confidence="0.948453" genericHeader="method">
3 Translation Experiments
</sectionHeader>
<subsectionHeader confidence="0.952417">
3.1 Conditions
</subsectionHeader>
<bodyText confidence="0.99977625">
In order to evaluate the adaptability of an EBMT with
out-of-domain examples, we applied the methods de-
scribed in Section 2 to the EBMT and evaluated the
translation quality in Japanese-to-English translation.
We used an EBMT, DP-match Driven transDucer (D3,
Sumita, 2001) as a test bed.
We used two Japanese-and-English bilingual cor-
pora. In this experiment on adaptation, as an out-of-
domain corpus, we used Basic Travel Expression Cor-
pus (BTEC, described as BE-corpus in Takezawa,
2002); as an in-domain corpus, we used a telephone
conversation corpus (TEL). The statistics of the corpora
are shown in Table 1. TEL is split into two parts: a test
set of 1,653 sentence pairs and a training set of 9,918.
Perplexities reveal the large difference between the in-
domain and out-of-domain corpora.
</bodyText>
<tableCaption confidence="0.994644">
Table 1. Corpus Statistics
</tableCaption>
<table confidence="0.999373636363636">
BTEC TEL
Japanese English Japanese English
# of sentences 152,172 11,571
# of words 1,045,694 909,270 103,860 92,749
Vocabulary size 19,999 12,268 5,242 4,086
Average sen- 6.87 5.98 8.98 8.02
tence length
Perplexity 24.19 28.85 37.22 40.04
(word trigram)
TEL language model BTEC language model
190.77 142.04 57.27 81.26
</table>
<bodyText confidence="0.99792784">
The translation qualities were evaluated by the
BLEU score (Papineni, 2001) and the NIST score
(Doddington, 2002). The evaluation methods compare
the system output translation with a set of reference
translations of the same source text by finding se-
quences of words in the reference translations that
match those in the system output translation. We used
the English sentence corresponding to each input Japa-
nese sentence in the test set as the reference translation.
Therefore, achieving a better score by the evaluation
means that the translation results can be regarded as
more adequate translations for the domain.
In order to simulate incremental expansion of an in-
domain bilingual corpus and to observe the relationship
between corpus size and translation quality, translations
were performed with some subsets of the training cor-
pus. The numbers of the sentence pairs are 0, 1000, .. ,
5000 and 9918, adding randomly selected examples
from the training set.
The LM of the domain’s target language was the
word trigram model of the English sentences of the
training set of TEL. We tried two patterns of training set
quantities in making the LM: 1) all of the training set,
and 2) the part of the set used for translation examples
according to the numbers mentioned above.
</bodyText>
<subsectionHeader confidence="0.907244">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999748136363636">
Table 2 shows the BLEU scores from the translation
experiment, which show certain tendencies. Generally,
by using more in-domain examples, the translation re-
sults steadily achieve better scores. The score when us-
ing 4,000 in-domain examples exceeded that when
using 152,172 out-of-domain examples. Equal merging
outperformed using only out-of-domain examples.
Merging with in-domain preference outperformed equal
merging, and using LM outperformed merging with in-
domain preference. Comparing the two cases using LM,
using LM made from all of the training set got a slightly
better scores than the other, which implies that better
LM is made from a larger corpus. All of the adaptation
methods are more effective when a smaller-sized in-
domain corpus is available. When using no in-domain
examples, the effect of using LM made from the entire
training set was relatively large.
Table 3 shows the NIST scores for the same experi-
ment. We can observe the same tendencies as in the
table of BLEU scores, except that the advantage of us-
ing LM made from all of the training set over that from
a partial set was not observed.
</bodyText>
<sectionHeader confidence="0.991988" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999670470588235">
A corpus-based approach is able to quickly build a ma-
chine translation system for a new domain if a bilingual
corpus of that domain is available. However, if only a
small-sized corpus is available, a low translation quality
is obtained. In order to boost the performance, several
methods using out-of-domain data were explored in this
paper. The experimental results showed the effect of
using an out-of-domain corpus by two evaluation meas-
ures, i.e., the BLEU score and the NIST score.
We also showed the possibility of increasing the
translation quality by using the LM of the domain’s
target language. However, the gains from using the LM
in the evaluation scores were not significant. We must
continue experiments with other corpora and under
various conditions. In addition, though we’ve implicitly
assumed a high-quality in-domain corpus, next we’d
like to investigate using a low-quality corpus.
</bodyText>
<tableCaption confidence="0.997431">
Table 2. Experimental results of translation by BLEU scores
</tableCaption>
<table confidence="0.986344571428571">
# of in-domain examples 0 1,000 2,000 3,000 4,000 5,000 9,918
Using in-domain examples --- 0.0190 0.0602 0.0942 0.1200 0.1436 0.2100
Using out-of-domain examples 0.1099
Merging equally 0.1271 0.1430 0.1590 0.1727 0.1868 0.2303
Merging with preference for in-domain 0.1099 0.1296 0.1469 0.1632 0.1776 0.1922 0.2333
Using LM of partial training set 0.1361 0.1538 0.1686 0.1829 0.1976 0.2387
Using LM of all training set 0.1225 0.1393 0.1557 0.1716 0.1852 0.1987
</table>
<tableCaption confidence="0.980118">
Table 3. Experimental results of translation by NIST scores
</tableCaption>
<table confidence="0.983678571428571">
# of in-domain examples 0 1,000 2,000 3,000 4,000 5,000 9,918
Using in-domain examples --- 0.0037 0.1130 0.4168 0.7567 1.1619 2.7400
Using out-of-domain examples 1.1126
Merging equally 1.4283 1.7367 2.0690 2.3405 2.6142 3.5772
Merging with preference for in-domain 1.1126 1.4580 1.7975 2.1343 2.4045 2.7088 3.6255
Using LM of partial training set 1.7454 2.0449 2.3639 2.5825 2.9304 3.7544
Using LM of all training set 1.4404 1.7007 2.0125 2.3484 2.5992 2.8973
</table>
<sectionHeader confidence="0.997262" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99985475">
The research reported here was supported in part by a
contract with the Telecommunications Advancement
Organization of Japan entitled, “A study of speech dia-
logue translation technology based on a large corpus”.
</bodyText>
<sectionHeader confidence="0.998964" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992588076923077">
Takezawa, T. et al. 2002. Toward a Broad-coverage
Bilingual Corpus for Speech Translation of Travel
Conversations in the Real World, Proc. of LREC-
2002
Papineni, K. et al. 2001. Bleu: a Method for Automatic
Evaluation of Machine Translation, RC22176, Sep-
tember 17, 2001, Computer Science
Doddington, G. 2002. Automatic Evaluation of Machine
Translation Quality Using N-gram Co-Occurrence
Statistics. Proc. of the HLT 2002 Conference
Nagao, M. 1984. A Framework of a Mechanical Trans-
lation between Japanese and English by Analogy
Principle, in Artificial and Human Intelligence,
Elithorn, A. and Banerji, R. (eds.). North-Holland
Sumita, E. 2001 Example-based machine translation
using DP-matching between word sequences, Proc.
of DDMT Workshop of 39th ACL
Brown, P. F. et al. 1993. The mathematics of statistical
machine translation: Parameter estimation, Computa-
tional Linguistics, 19(2)
Kaki, S. et al. 1999. Scoring multiple translations using
character N-gram, Proc. of NLPRS-99
Callison-Burch, C. et al. 2001. A Program for Auto-
matically Selecting the Best Output from Multiple
Machine Translation Engines, Proc. of MT Summit
VIII
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.905243">
<title confidence="0.999413">Adaptation Using Out-of-Domain Corpus within EBMT</title>
<author confidence="0.997716">Takao Doi</author>
<author confidence="0.997716">Eiichiro Sumita</author>
<author confidence="0.997716">Hirofumi</author>
<affiliation confidence="0.993831">ATR Spoken Language Translation Research</affiliation>
<address confidence="0.926744">2-2-2 Hikaridai, Kansai Science City, Kyoto, 619-0288 Japan</address>
<email confidence="0.984792">takao.doi@atr.co.jp</email>
<email confidence="0.984792">eiichiro.sumita@atr.co.jp</email>
<email confidence="0.984792">hirofumi.yamamoto@atr.co.jp</email>
<abstract confidence="0.999418181818182">In order to boost the translation quality of EBMT based on a small-sized bilingual corpus, we use an out-of-domain bilingual corpus and, in addition, the language model of an indomain monolingual corpus. We conducted experiments with an EBMT system. The two evaluation measures of the BLEU score and the NIST score demonstrated the effect of using an out-of-domain bilingual corpus and the possibility of using the language model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Takezawa</author>
</authors>
<title>Toward a Broad-coverage Bilingual Corpus for Speech Translation of Travel Conversations in the Real World,</title>
<date>2002</date>
<booktitle>Proc. of LREC2002</booktitle>
<contexts>
<context position="4011" citStr="Takezawa, 2002" startWordPosition="618" endWordPosition="619">e selected. Examples of (3-1) are used when the similarities are equal to each other. 3 Translation Experiments 3.1 Conditions In order to evaluate the adaptability of an EBMT with out-of-domain examples, we applied the methods described in Section 2 to the EBMT and evaluated the translation quality in Japanese-to-English translation. We used an EBMT, DP-match Driven transDucer (D3, Sumita, 2001) as a test bed. We used two Japanese-and-English bilingual corpora. In this experiment on adaptation, as an out-ofdomain corpus, we used Basic Travel Expression Corpus (BTEC, described as BE-corpus in Takezawa, 2002); as an in-domain corpus, we used a telephone conversation corpus (TEL). The statistics of the corpora are shown in Table 1. TEL is split into two parts: a test set of 1,653 sentence pairs and a training set of 9,918. Perplexities reveal the large difference between the indomain and out-of-domain corpora. Table 1. Corpus Statistics BTEC TEL Japanese English Japanese English # of sentences 152,172 11,571 # of words 1,045,694 909,270 103,860 92,749 Vocabulary size 19,999 12,268 5,242 4,086 Average sen- 6.87 5.98 8.98 8.02 tence length Perplexity 24.19 28.85 37.22 40.04 (word trigram) TEL languag</context>
</contexts>
<marker>Takezawa, 2002</marker>
<rawString>Takezawa, T. et al. 2002. Toward a Broad-coverage Bilingual Corpus for Speech Translation of Travel Conversations in the Real World, Proc. of LREC2002</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation,</title>
<date>2001</date>
<institution>Computer Science</institution>
<location>RC22176,</location>
<contexts>
<context position="4740" citStr="Papineni, 2001" startWordPosition="734" endWordPosition="735">n Table 1. TEL is split into two parts: a test set of 1,653 sentence pairs and a training set of 9,918. Perplexities reveal the large difference between the indomain and out-of-domain corpora. Table 1. Corpus Statistics BTEC TEL Japanese English Japanese English # of sentences 152,172 11,571 # of words 1,045,694 909,270 103,860 92,749 Vocabulary size 19,999 12,268 5,242 4,086 Average sen- 6.87 5.98 8.98 8.02 tence length Perplexity 24.19 28.85 37.22 40.04 (word trigram) TEL language model BTEC language model 190.77 142.04 57.27 81.26 The translation qualities were evaluated by the BLEU score (Papineni, 2001) and the NIST score (Doddington, 2002). The evaluation methods compare the system output translation with a set of reference translations of the same source text by finding sequences of words in the reference translations that match those in the system output translation. We used the English sentence corresponding to each input Japanese sentence in the test set as the reference translation. Therefore, achieving a better score by the evaluation means that the translation results can be regarded as more adequate translations for the domain. In order to simulate incremental expansion of an indoma</context>
</contexts>
<marker>Papineni, 2001</marker>
<rawString>Papineni, K. et al. 2001. Bleu: a Method for Automatic Evaluation of Machine Translation, RC22176, September 17, 2001, Computer Science</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using N-gram Co-Occurrence Statistics.</title>
<date>2002</date>
<booktitle>Proc. of the HLT 2002 Conference</booktitle>
<contexts>
<context position="4778" citStr="Doddington, 2002" startWordPosition="740" endWordPosition="741">ts: a test set of 1,653 sentence pairs and a training set of 9,918. Perplexities reveal the large difference between the indomain and out-of-domain corpora. Table 1. Corpus Statistics BTEC TEL Japanese English Japanese English # of sentences 152,172 11,571 # of words 1,045,694 909,270 103,860 92,749 Vocabulary size 19,999 12,268 5,242 4,086 Average sen- 6.87 5.98 8.98 8.02 tence length Perplexity 24.19 28.85 37.22 40.04 (word trigram) TEL language model BTEC language model 190.77 142.04 57.27 81.26 The translation qualities were evaluated by the BLEU score (Papineni, 2001) and the NIST score (Doddington, 2002). The evaluation methods compare the system output translation with a set of reference translations of the same source text by finding sequences of words in the reference translations that match those in the system output translation. We used the English sentence corresponding to each input Japanese sentence in the test set as the reference translation. Therefore, achieving a better score by the evaluation means that the translation results can be regarded as more adequate translations for the domain. In order to simulate incremental expansion of an indomain bilingual corpus and to observe the</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>Doddington, G. 2002. Automatic Evaluation of Machine Translation Quality Using N-gram Co-Occurrence Statistics. Proc. of the HLT 2002 Conference</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nagao</author>
</authors>
<title>A Framework of a Mechanical Translation between Japanese and English by Analogy Principle,</title>
<date>1984</date>
<booktitle>in Artificial and Human Intelligence, Elithorn,</booktitle>
<editor>A. and Banerji, R. (eds.).</editor>
<publisher>North-Holland</publisher>
<contexts>
<context position="2011" citStr="Nagao, 1984" startWordPosition="306" endWordPosition="307">or works, statistical machine translation (Brown, 1993) used not only LM but also translation models. However, making a translation model requires a bilingual corpus. On the other hand, in some studies on multiple-translation selection, the LM of the target language is used to calculate translation scores (Kaki, 1999; Callison-Burch, 2001). For adaptation, we use the LM of an in-domain target language. In the following sections, we describe the methods using an out-of-domain bilingual corpus and an indomain monolingual corpus. Moreover, we report on our experiments. 2 Adaptation Methods EBMT (Nagao, 1984) retrieves the translation examples that are most similar to an input expression and adjusts the examples to obtain the translation. The EBMT system in our approach retrieves not only indomain examples, but also out-of-domain examples. When using out-of-domain examples, suitability to the target domain is considered. We tried the following three types of adaptation methods. (1) Merging equally An in-domain corpus and an out-of-domain corpus are simply merged and used without distinction. (2) Merging with preference for in-domain corpus An in-domain corpus and an out-of-domain corpus are merged</context>
</contexts>
<marker>Nagao, 1984</marker>
<rawString>Nagao, M. 1984. A Framework of a Mechanical Translation between Japanese and English by Analogy Principle, in Artificial and Human Intelligence, Elithorn, A. and Banerji, R. (eds.). North-Holland</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sumita</author>
</authors>
<title>Example-based machine translation using DP-matching between word sequences,</title>
<date>2001</date>
<booktitle>Proc. of DDMT Workshop of 39th ACL</booktitle>
<contexts>
<context position="3795" citStr="Sumita, 2001" startWordPosition="583" endWordPosition="584">milar are retrieved. In the retrieved examples, examples with the highest probabilities of their target sentences by the LM are selected. (3-3) From the results of both (3-1) and (3-2), the most similar examples are selected. Examples of (3-1) are used when the similarities are equal to each other. 3 Translation Experiments 3.1 Conditions In order to evaluate the adaptability of an EBMT with out-of-domain examples, we applied the methods described in Section 2 to the EBMT and evaluated the translation quality in Japanese-to-English translation. We used an EBMT, DP-match Driven transDucer (D3, Sumita, 2001) as a test bed. We used two Japanese-and-English bilingual corpora. In this experiment on adaptation, as an out-ofdomain corpus, we used Basic Travel Expression Corpus (BTEC, described as BE-corpus in Takezawa, 2002); as an in-domain corpus, we used a telephone conversation corpus (TEL). The statistics of the corpora are shown in Table 1. TEL is split into two parts: a test set of 1,653 sentence pairs and a training set of 9,918. Perplexities reveal the large difference between the indomain and out-of-domain corpora. Table 1. Corpus Statistics BTEC TEL Japanese English Japanese English # of se</context>
</contexts>
<marker>Sumita, 2001</marker>
<rawString>Sumita, E. 2001 Example-based machine translation using DP-matching between word sequences, Proc. of DDMT Workshop of 39th ACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation,</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1454" citStr="Brown, 1993" startWordPosition="220" endWordPosition="221"> domain, you’ll get a translation system for the domain. However, if only a small-sized corpus is available, low translation quality is obtained. We explored methods to boost translation quality based on a small-sized bilingual corpus in the domain. Among these methods, we use an out-of-domain bilingual corpus and, in addition, the language model (LM) of an indomain monolingual corpus. For accuracy of the LM, a larger training set is better. The training set is a target language corpus, which can be more easily prepared than a bilingual corpus. In prior works, statistical machine translation (Brown, 1993) used not only LM but also translation models. However, making a translation model requires a bilingual corpus. On the other hand, in some studies on multiple-translation selection, the LM of the target language is used to calculate translation scores (Kaki, 1999; Callison-Burch, 2001). For adaptation, we use the LM of an in-domain target language. In the following sections, we describe the methods using an out-of-domain bilingual corpus and an indomain monolingual corpus. Moreover, we report on our experiments. 2 Adaptation Methods EBMT (Nagao, 1984) retrieves the translation examples that ar</context>
</contexts>
<marker>Brown, 1993</marker>
<rawString>Brown, P. F. et al. 1993. The mathematics of statistical machine translation: Parameter estimation, Computational Linguistics, 19(2)</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kaki</author>
</authors>
<title>Scoring multiple translations using character N-gram,</title>
<date>1999</date>
<booktitle>Proc. of NLPRS-99</booktitle>
<contexts>
<context position="1717" citStr="Kaki, 1999" startWordPosition="262" endWordPosition="263">hods, we use an out-of-domain bilingual corpus and, in addition, the language model (LM) of an indomain monolingual corpus. For accuracy of the LM, a larger training set is better. The training set is a target language corpus, which can be more easily prepared than a bilingual corpus. In prior works, statistical machine translation (Brown, 1993) used not only LM but also translation models. However, making a translation model requires a bilingual corpus. On the other hand, in some studies on multiple-translation selection, the LM of the target language is used to calculate translation scores (Kaki, 1999; Callison-Burch, 2001). For adaptation, we use the LM of an in-domain target language. In the following sections, we describe the methods using an out-of-domain bilingual corpus and an indomain monolingual corpus. Moreover, we report on our experiments. 2 Adaptation Methods EBMT (Nagao, 1984) retrieves the translation examples that are most similar to an input expression and adjusts the examples to obtain the translation. The EBMT system in our approach retrieves not only indomain examples, but also out-of-domain examples. When using out-of-domain examples, suitability to the target domain is</context>
</contexts>
<marker>Kaki, 1999</marker>
<rawString>Kaki, S. et al. 1999. Scoring multiple translations using character N-gram, Proc. of NLPRS-99</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
</authors>
<title>A Program for Automatically Selecting the Best Output from Multiple Machine Translation Engines,</title>
<date>2001</date>
<booktitle>Proc. of MT Summit VIII</booktitle>
<contexts>
<context position="1740" citStr="Callison-Burch, 2001" startWordPosition="264" endWordPosition="265"> an out-of-domain bilingual corpus and, in addition, the language model (LM) of an indomain monolingual corpus. For accuracy of the LM, a larger training set is better. The training set is a target language corpus, which can be more easily prepared than a bilingual corpus. In prior works, statistical machine translation (Brown, 1993) used not only LM but also translation models. However, making a translation model requires a bilingual corpus. On the other hand, in some studies on multiple-translation selection, the LM of the target language is used to calculate translation scores (Kaki, 1999; Callison-Burch, 2001). For adaptation, we use the LM of an in-domain target language. In the following sections, we describe the methods using an out-of-domain bilingual corpus and an indomain monolingual corpus. Moreover, we report on our experiments. 2 Adaptation Methods EBMT (Nagao, 1984) retrieves the translation examples that are most similar to an input expression and adjusts the examples to obtain the translation. The EBMT system in our approach retrieves not only indomain examples, but also out-of-domain examples. When using out-of-domain examples, suitability to the target domain is considered. We tried t</context>
</contexts>
<marker>Callison-Burch, 2001</marker>
<rawString>Callison-Burch, C. et al. 2001. A Program for Automatically Selecting the Best Output from Multiple Machine Translation Engines, Proc. of MT Summit VIII</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>