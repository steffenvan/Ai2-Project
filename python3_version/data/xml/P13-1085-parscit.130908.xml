<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000051">
<title confidence="0.991479">
Improved Lexical Acquisition through DPP-based Verb Clustering
</title>
<author confidence="0.997943">
Roi Reichart Anna Korhonen
</author>
<affiliation confidence="0.997748">
University of Cambridge, UK University of Cambridge, UK
</affiliation>
<email confidence="0.992672">
rr439@cam.ac.uk alk23@cam.ac.uk
</email>
<sectionHeader confidence="0.99728" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996595">
Subcategorization frames (SCFs), selec-
tional preferences (SPs) and verb classes
capture related aspects of the predicate-
argument structure. We present the first
unified framework for unsupervised learn-
ing of these three types of information.
We show how to utilize Determinantal
Point Processes (DPPs), elegant proba-
bilistic models that are defined over the
possible subsets of a given dataset and
give higher probability mass to high qual-
ity and diverse subsets, for clustering. Our
novel clustering algorithm constructs a
joint SCF-DPP DPP kernel matrix and uti-
lizes the efficient sampling algorithms of
DPPs to cluster together verbs with sim-
ilar SCFs and SPs. We evaluate the in-
duced clusters in the context of the three
tasks and show results that are superior to
strong baselines for each 1.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999669166666667">
Verb classes (VCs), subcategorization frames
(SCFs) and selectional preferences (SPs) capture
different aspects of predicate-argument structure.
SCFs describe the syntactic realization of verbal
predicate-argument structure, SPs capture the se-
mantic preferences verbs have for their arguments
and VCs in the Levin (1993) tradition provide a
shared level of abstraction for verbs that share
many aspects of their syntactic and semantic be-
havior.
These three of types of information have proved
useful for Natural Language Processing (NLP)
</bodyText>
<footnote confidence="0.858264666666667">
1The source code of the clustering algorithms and evalu-
ation is submitted with this paper and will be made publicly
available upon acceptance of the paper.
</footnote>
<bodyText confidence="0.98730312">
tasks which require information about predicate-
argument structure, including parsing (Shi and Mi-
halcea, 2005; Cholakov and van Noord, 2010;
Zhou et al., 2011), semantic role labeling (Swier
and Stevenson, 2004; Dang, 2004; Bharati et al.,
2005; Moschitti and Basili, 2005; zap, 2008; Zapi-
rain et al., 2009), and word sense disambiguation
(Dang, 2004; Thater et al., 2010; O´ S´eaghdha and
Korhonen, 2011), among many others.
Because lexical information is highly sensitive
to domain variation, approaches that can identify
VCs, SCFs and SPs in corpora have become in-
creasingly popular, e.g. (O’Donovan et al., 2005;
Schulte im Walde, 2006; Erk, 2007; Preiss et al.,
2007; Van de Cruys, 2009; Reisinger and Mooney,
2011; Sun and Korhonen, 2011; Lippincott et al.,
2012).
The task of SCF induction involves identifying
the arguments of a verb lemma and generalizing
about the frames (i.e. SCFs) taken by the verb,
where each frame includes a number of arguments
and their syntactic types. For example, in (1),
the verb ”show” takes the frame SUBJ-DOBJ-
CCOMP (subject, direct object, and clausal
complement).
</bodyText>
<listItem confidence="0.80293075">
(1) [A number of SCF acquisition papers]SUBJ
[show]VERB [their readers]DOBJ [which fea-
tures are most valuable for the acquisition
process]CCOMP.
</listItem>
<bodyText confidence="0.9985008">
SP induction involves identifying and classify-
ing the lexical items in a given argument slot. In
sentence (2), for example, the verb ”show” takes
the frame SUBJ-DOBJ. The direct object in this
frame is likely to be inanimate.
</bodyText>
<listItem confidence="0.929731">
(2) [Most SCF and SP acquisition papers]SUBJ,
</listItem>
<page confidence="0.963312">
862
</page>
<note confidence="0.9142715">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 862–872,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.99982725">
[show]VERB [no evidence to the usefulness of
joint learning leaning for these tasks]DOBJ.
Finally, VC induction involves clustering to-
gether verbs with similar meaning, reflected in
similar SCFs and SPs. For example, ”show” in the
above examples could get clustered together with
”demonstrate” and ”indicate”.
Because these challenging tasks capture com-
plementary information about predicate argument
structure, they should be able to inform and sup-
port each other. Recently, researchers have be-
gun to investigate the benefits of their joint learn-
ing. Schulte im Walde et al. (2008) integrated SCF
and VC acquisition and used it for WordNet-based
SP classification. O´ S´eaghdha (2010) presented a
“dual-topic” model for SPs that induces also verb
clusters. Both works reported SP evaluation with
promising results. Lippincott et al. (2012) pre-
sented a joint model for inducing simple syntac-
tic frames and VCs. They reported high accuracy
results on VCs. de Cruys et al. (2012) introduced
a joint model for SCF and SP acquisition. They
evaluated both the SCFs and SPs, obtaining rea-
sonable result on both tasks.
In this paper, we present the first unified frame-
work for unsupervised learning of the three types
of information - SCFs, SPs and VCs. Our frame-
work is based on Determinantal Point Processes
(DPPs, (Kulesza, 2012; Kulesza and Taskar,
2012c)), elegant probabilistic models that are de-
fined over the possible subsets of a given dataset
and give higher probability mass to high quality
and diverse subsets.
We first show how individual-task DPP kernel
matrices can be naturally combined to construct a
joint kernel. We use this to construct a joint SCF-
SP kernel. We then introduce a novel clustering
algorithm based on iterative DPP sampling which
can (contrary to other probabilistic frameworks
such as Markov random fields) be performed both
accurately and efficiently. When defined over the
joint SCF and SP kernel, this new algorithm can
be used to induce VCs that are valuable for both
tasks.
We also contribute by evaluating the value of
the clusters induced by our model for the acquisi-
tion of the three information types. Our evaluation
against a well-known VC gold standard shows that
our clustering model outperforms the state-of-the-
art verb clustering algorithm of Sun and Korhonen
(2009), in our setup where no manually created
SCF or SP data is available. Our evaluation against
a well-known SCF gold standard and in the con-
text of SP disambiguation tasks shows results that
are superior to strong baselines, demonstrating the
benefit our approach.
</bodyText>
<sectionHeader confidence="0.995552" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999749837209302">
SCF acquisition Most current works induce SCFs
from the output of an unlexicalized parser (i.e.
a parser trained without SCF annotations) using
hand-written rules (Briscoe and Carroll, 1997; Ko-
rhonen, 2002; Preiss et al., 2007) or grammatical
relation (GR) co-occurrence statistics (O’Donovan
et al., 2005; Chesley and Salmon-Alt, 2006; Ienco
et al., 2008; Messiant et al., 2008; Lenci et al.,
2008; Altamirano and Alonso i Alemany, 2010;
Kawahara and Kurohashi, 2010).
Only a handful of SCF induction works are
unsupervised. Carroll and Rooth (1996) applied
an EM-based approach to a context-free grammar
based model, Dkebowski (2009) used point-wise
co-occurrence of arguments in parsed Polish data
and Lippincott et al. (2012) presented a Bayesian
network model for syntactic frame induction that
identifies SPs on argument types. However, the
frames induced by Lippincott et al. (2012) do not
capture sets of arguments for verbs so are far sim-
pler than traditional SCFs.
Current approaches to SCF acquisition suffer
from lack of semantic information which is needed
to guide the purely syntax-driven acquisition pro-
cess. Previous works have showed the benefit of
hand-coded semantic information in SCF acquisi-
tion (Korhonen, 2002). We will address this prob-
lem in an unsupervised way: our approach is to
consider SCFs together with semantic SPs through
VCs which generalize over syntactically and se-
mantically similar verbs.
SP acquisition Considerable research has been
conducted on SP acquisition, with a variety of
unsupervised models proposed for this task that
use no hand-crafted information during training.
The latter approaches include latent variable mod-
els ( O´ S´eaghdha, 2010; Ritter and Etzioni, 2010;
Reisinger and Mooney, 2011), distributional sim-
ilarity methods (Bhagat et al., 2007; Basili et
al., 2007; Erk, 2007) and methods based on
non-negative tensor factorization (Van de Cruys,
2009). These works use a variety of linguistic fea-
tures in the acquisition process but none of them
</bodyText>
<page confidence="0.99787">
863
</page>
<bodyText confidence="0.999221094339623">
integrates the three information types covered in
our work.
ther motivate the development of a framework that
acquires the three types of information together.
Verb clustering A variety of VC approaches
have been proposed in the literature. These in-
clude syntactic, semantic and mixed syntactic-
semantic classifications (Grishman et al., 1994;
Miller, 1995; Baker et al., 1998; Palmer et al.,
2005; Schuler, 2006; Hovy et al., 2006). We fo-
cus on Levin style classes (Levin, 1993) which
are defined in terms of diathesis alternations and
capture generalizations over a range of syntactic
and semantic properties. Previous unsupervised
VC acquisition approaches clustered a variety of
linguistic features using different (e.g. K-means
and spectral) algorithms (Schulte im Walde, 2006;
Joanis et al., 2008; Sun et al., 2008; Li and Brew,
2008; Korhonen et al., 2008; Sun and Korhonen,
2009; Vlachos et al., 2009; Sun and Korhonen,
2011). The linguistic features included SCFs and
SPs, but these were induced separately and then
feeded as features to the clustering algorithm. Our
framework combines together SCF-motivated and
SP-motivated kernel matrices , and uses the joint
kernel to induce verb clusters which are likely to
be highly relevant for both tasks. Importantly, no
manual or automatic system for SCF or SP acqui-
sition has been utilized when constructing the ker-
nel matrices, we only consider features extracted
from the output of an unlexicalized parser. Our ap-
proach hence provides a framework for acquiring
valuable information for the three tasks together.
Joint Modeling A small number of works have
recently investigated joint approaches to SCFs,
SPs and VCs. Each of them has addressed only
a subset of the tasks and all but one have eval-
uated the performance in the context of one task
only. O´ S´eaghdha (2010) presented a “dual-topic”
model for SPs that induces VCs, reporting evalua-
tion of SPs only. Lippincott et al. (2012) presented
a Bayesian network model for syntactic frame
(rather than full SCF) induction that induces VCs.
Only VCs are evaluated. de Cruys et al. (2012)
presented a joint unsupervised model of SCF and
SP acquisition based on non-negative tensor fac-
torization. Both SCFs and SPs were evaluated. Fi-
nally, the model of Schulte im Walde et al. (2008)
addresses the three types of information but SP
parameters are estimated with a WordNet based
method and only the SPs are evaluated. Although
evaluation of these recent joint models has been
partial, the results have been encouraging and fur-
</bodyText>
<sectionHeader confidence="0.983952" genericHeader="method">
3 The Unified Framework
</sectionHeader>
<bodyText confidence="0.9999642">
In this section we present our unified framework.
Our idea is to utilize DPPs for verb clustering that
informs both SCF and SP acquisition. DPPs define
a probability distribution over the possible subsets
of a given set. These models assign higher prob-
ability mass to subsets that are both high quality
and diverse.
Our novel clustering algorithm makes use of
three DPP properties that are appealing for our
purpose: (1) The existence of efficient sam-
pling algorithms for these models, which enable
tractable sampling of high quality and diverse verb
subsets; (2) Such verb subsets form natural high
quality seeds for hierarchical clustering; and (3)
Given individual-task DPP kernel matrices there
are various simple and natural ways to combine
them into a new DPP kernel matrix.
Individual task DPP kernels represent (i) the
quality of a data point (verb) as its average feature-
based similarity with the other points in the data
set and (ii) the divergence between a pair of points
as the inverse similarity between them. For dif-
ferent tasks, different feature sets are used for the
kernel construction. The high quality and diverse
subsets sampled from the DPP model are consid-
ered good cluster seeds as they are likely to be rel-
atively uniformly spread and to provide good cov-
erage of the data set. The algorithm induces an
hierarchical clustering, which is particularly suit-
able for semantic tasks, where a set of clusters that
share a parent consists of pure members (i.e. most
of the points in each cluster member belong to the
same gold cluster) and together provide good cov-
erage of the verb space.
After a brief description of the Determinantal
Point Processes (DPP) framework (Section 3.1),
we discuss the construction of the joint DPP ker-
nel, given a kernel for each individual task, In sec-
tion 3.3 we present the DPP-Cluster clustering al-
gorithm.
</bodyText>
<subsectionHeader confidence="0.998193">
3.1 Determinantal Point Processes
</subsectionHeader>
<bodyText confidence="0.9981842">
Determinantal point processes (DPPs) are elegant
probabilistic models of repulsion that offer effi-
cient and exact algorithms for sampling, marginal-
ization, conditioning, and other inference tasks.
Recently (Kulesza, 2012; Kulesza and Taskar,
</bodyText>
<page confidence="0.990057">
864
</page>
<bodyText confidence="0.999971833333333">
2012c) introduced them to the machine learning
community and demonstrated their usefulness for
a variety of tasks including document summariza-
tion, image search, modeling non-overlapping hu-
man poses in images and video and automati-
cally building timelines of important news stories
(Kulesza and Taskar, 2010; Kulesza and Taskar,
2012a; Gillenwater et al., 2012; Kulesza and
Taskar, 2012b). Below we provide a brief descrip-
tion of the framework, a comprehensive survey
can be found in (Kulesza and Taskar, 2012c).
Given a set of items Y = {y1,... , yN}, a DPP
P defines a probability measure on the set of all
subsets of Y, 2Y. Kulesza and Taskar (2012c) re-
stricted their discussion of DDPs to L-ensembles,
where the probability of a subset Y E Y is defined
through a positive semi-definite matrix L indexed
by the elements of Y:
</bodyText>
<equation confidence="0.99949125">
PL(Y = Y ) = E Y ⊆Y det(LY) det(L + I)
det(LY ) det(LY )
=
(1)
</equation>
<bodyText confidence="0.980455363636364">
Where I is the N x N identity matrix and
det(LO) = 1. Since L is positive semi-definite, it
can be decomposed to L = BT B. This allows the
construction of an intuitively interpretable model
where each column Bi is the product of a quality
term qi E R+ and a vector of (normalized) diver-
sity features 0i E RD, ||0i ||= 1. In this model,
qi measures an inherent quality of the i − th item
in Y while 0Ti 0j E [−1, 1] is a similarity measure
between items i and j. With this representation we
can write:
</bodyText>
<equation confidence="0.999259833333333">
Lij = qi0Ti 0jqj (2)
Sij = 0Ti 0j = j (3)
V/Li
LiiLjj
PL(Y = Y ) a ( � q2i )det(SY ) (4)
i∈Y
</equation>
<bodyText confidence="0.999894785714286">
It can be shown that the first term in equation 4 in-
creases with the quality of the selected items, and
the second term increases with their diversity. As
a consequence, this distribution places most of its
weight on sets that are both high quality and di-
verse.
Although the number of possible realizations of
Y is exponential in N, many inference procedures
can be performed accurately and efficiently (i.e.
in polynomial time which is very short in prac-
tice). In particular, sampling, which NP-hard for
alternative models such as Markov Random Fields
(MRFs), is efficient, theoretically and practically,
for DPPs.
</bodyText>
<subsectionHeader confidence="0.999939">
3.2 Constructing a Joint Kernel Matrix
</subsectionHeader>
<bodyText confidence="0.948653913043478">
DPPs are particularly suitable for joint modeling
as they come with various simple and intuitive
ways to combine individual model kernel matrices
into a joint kernel. This stems from the fact that
every positive-semidefinite matrix forms a legal
DPP kernel (equation 1). Given individual model
DPP kernels, we would therefore like to combine
them into a positive-semidefinite matrix.
While there are various ways to construct a
positive-semidefinite matrix from two positive-
semidefinite matrices – for example, by taking
their sum – in this work we are motivated by the
product of experts approach (Hinton, 2002), rea-
soning that high quality assignments according to
a product of models have to be of high quality ac-
cording to each individual model, and sick for a
product combination. 2
In practice we construct the joint kernel in the
following way. We build on the aforementioned
property that a matrix L is positive semi-definite
iff L = BT B. Given two DPPs, PL1 defined by
L1 = AT1 A1 and PL2 defined by L2 = AT2 A2, we
construct the joint kernel L12:
</bodyText>
<equation confidence="0.837884">
L12 = L1L2L2L1 = CTC (5)
Where C = AT2 A2AT1A1 and CT =
AT1 A1AT2 A2.
</equation>
<subsectionHeader confidence="0.999313">
3.3 Clustering Algorithm
</subsectionHeader>
<bodyText confidence="0.998701315789474">
Algorithm (1) and Figure (1) provide a pseudo-
code of the algorithm and an example output. Be-
low is a detailed description.
Features Our algorithm builds two DPP ker-
nel matrices (the GenKernelMatrix function),
in which the rows and columns correspond to the
verbs in the data set, such that the (i, j)-th entry
corresponds to verbs number i and j. Following
equations 2 and 3 one matrix is built for SCF and
one for SP, and they are then combined into the
2Note that we do not take a product of the individual mod-
els but only of their kernel matrices. Yet, if we construct the
joint matrix by a multiplication then it follows from a simple
generalization of the Cauchy-Binet formula that its principle
minors, which define the subset probabilities (equation 1), are
a sum of multiplications of the principle minors of the indi-
vidual model kernels. Still, we do not have guarantees that
our choice of kernel combination is the right one. We leave
this for future research.
</bodyText>
<page confidence="0.993187">
865
</page>
<bodyText confidence="0.985394673076923">
joint kernel matrix (the GenJointMat function)
following equation 5. Each kernel matrix requires
a proper feature representation φ and quality score
q.
In both kernels we represent a verb by the
counts of the grammatical relations (GRs) it par-
ticipates in. In the SCF kernel a GR is represented
by the GR type and the POS tags of the verb and
its arguments. In the SP kernels the GRs are rep-
resented by the POS tags of the verb and its ar-
guments as well as by the argument head word.
Based on this feature representation, the similarity
(opposite divergence) is encoded to the model by
equation 3 as the dot product between the normal-
ized feature vectors. The quality score qi of the
i-th verb is the average similarity of this verb with
the other verbs in the dataset.
Cluster set construction In its while loop, the
algorithm iteratively generates fixed-size cluster
sets such that each data point belongs to exactly
one cluster in one set. These cluster sets form
the leaf level of the tree in Figure (1). It does
so by extracting the T highest probability K-point
samples from a set of M subsets, each of which
sampled from the joint DPP model, and cluster-
ing them by the cluster procedure. The sampling
is done by the K-DPP sampling process ((Kulesza
and Taskar, 2012c), page 62) 3.
The cluster procedure first seeds a K-cluster
set with the highest probability sample. Then, it
gradually extends the clusters by iteratively map-
ping the samples, in decreasing order of probabil-
ity, to the existing clusters (the mlMapping func-
tion). Mapping is done by attaching every point
in the mapped subset to its closet cluster, where
the distance between a point and the cluster is the
maximum over the distances between the point
and each of the points in the cluster. The map-
ping is many-to-one, that is, multiple points in the
subset can be assigned to the same cluster.
Based on the DPP properties, the higher the
probability of a sampled subset, the more likely it
is to consist of distinct points that provide a good
coverage of the verb set. By iteratively extending
the clusters with high probability subsets, we thus
expect each cluster set to consist of clusters that
demonstrate these properties.
3K-DPP is a DPP conditioned on the sample size. As
shown in ((Kulesza and Taskar, 2012c), Section 2.4.3) this
conditional distribution is also a DPP. We could have obtained
samples of size K by sampling the DPP and rejecting sam-
ples of other sizes but this would have been slower.
</bodyText>
<note confidence="0.652829">
SET 1-2-3-4 (45,K)
</note>
<figureCaption confidence="0.939733">
Figure 1: An example output hierarchy of DPP-
</figureCaption>
<bodyText confidence="0.99704524137931">
Cluster for a set of 45 data points. Each set is
augmented with the number of points (left num-
ber) and clusters (right number) it includes. The
iterative DPP-samples clustering (the While loop)
generates the lowest level of the tree, by dividing
the data set into cluster sets, each of which con-
sists of K clusters. Each point in the data set be-
longs to exactly one cluster in exactly one set. The
agglomerative clustering then iteratively combines
cluster sets such that in each iteration two sets are
combined to one set with K clusters.
Agglomerative Clustering Finally, the
AgglomerativeClustering function builds a
hierarchy of cluster sets, by iteratively combining
cluster set pairs. In each iteration it computes the
similarity between any such pair, defined to be the
lowest similarity between their cluster members,
which is in turn defined to be the lowest cosine
similarity between their point members. The most
similar cluster sets are combined such that each
of the clusters in one set is mapped to its most
similar cluster in the other set. In this step the
algorithm generates data partitions at different
granularity levels from finest (from the iterative
sampling step) to the coarsest set (generated by
the last agglomerative clustering iteration and
consisting of exactly K clusters). This property is
useful as the optimal level of generalization may
be task dependent.
</bodyText>
<sectionHeader confidence="0.99944" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999814384615385">
Data sets and gold standards We evaluated the
SCFs and verb clusters on gold standard datasets.
We based our set of the largest available joint set
for SCFs and VCs - that of (de Cruys et al., 2012).
It provides SCF annotations for 183 verbs (an av-
erage of 12.3 SCF types per verb) obtained by
annotating 250 corpus occurrences per verb with
the SCF types of (de Cruys et al., 2012). The
verbs represent a range of Levin classes at the top
level of the hierarchy in VerbNet (Kipper-Schuler,
2005). Where a verb has more than one Verb-
Net class, we assign it to the one supported by the
highest number of member verbs. To ensure suf-
</bodyText>
<equation confidence="0.77260075">
SET 1-2 (23,K)
SET3-4(22,K)
SET1 (12,K) SET2 (11,K)
SET 3 (12,K) SET4 (10,K)
</equation>
<page confidence="0.852709">
866
</page>
<table confidence="0.9994842">
|C |= 20, 21.6 |C |= 40, 41 |C |= 60, 58.6 |C |= 69, 77.6 |C |= 89, 97.4
Model R P F R P F R P F R P F R P F
DPP-cluster 93.1 17.3 29.3 77.9 25.4 38.3 63 31.9 42.3 43.8 33.6 38.1 34.4 40.6 37.2
AC 67 17.8 28.2 46.6 24 31.7 40.5 29.4 34 33 34.9 33.9 24.7 41.1 30.9
SC 32.1 27.5 29.6 26.6 35.9 30.6 23.7 41.5 30.2 22.8 43.6 29.9 21.6 48.7 29.9
</table>
<tableCaption confidence="0.564589">
Table 1: Verb clustering evaluation for the last five iterations of our DPP-cluster model and the baseline
agglomerative clustering algorithm (AC, see text for its description), and for the spectral clustering (SC)
algorithm of (Sun and Korhonen, 2009) with the same number of clusters induced by DPP-cluster. ICI is
the number of clusters for DPP-cluster and SC (first number) and for AC (second number). The F-score
performance of DPP-cluster is superior in 4 out of 5 cases.
</tableCaption>
<table confidence="0.99162075">
Arg. per verb P (DPP) P(AC) P (B) P (NF) R (DPP) R (AC) R (B) R(NF) ERR DPP ERR AC ERR B
≤ 200 (133 verbs) 27.3 23.7 27.3 23.1 9.9 7.6 8 11.3 3.4 0.16 1.55
≤ 600 (205 verbs) 26.5 25 27.3 22.6 14.8 11.5 11.9 16.6 2.3 0.50 1.1
≤ 1000 (238 verbs) 24.6 23.6 25.6 21.1 17.5 13.8 14.7 19.8 1.6 0.42 0.95
</table>
<tableCaption confidence="0.9377085">
Table 2: Performance of the Corpus Statistics SP baseline (non-filtered, NF) as well as for three filtering
methods: frequency based (filter-baseline, B), DPP-cluster based (DPP) and AC cluster based (AC). P
</tableCaption>
<bodyText confidence="0.957375390625">
(method) and R (method) present the precision and recall of the method respectively. The error reduc-
tion ratio (ERR) is the ratio between the reduction in precision error achieved by each method and the
increase in recall error (each method is compared to the NF baseline). Ratio greater than 1 means that
the reduction in precision error is larger than the increase in recall error (see text for exact definition).
DPP based filtering provides substantially better ratio.
ficient representation of each class, we collected
from VerbNet the verbs for which at least one of
the possible classes is represented in the 183 verbs
set by at least one and at most seven verbs. This
yielded 101 additional verbs which we added to
the gold standard with the initial 183 verbs.
We parsed the BNC corpus with the RASP
parser (Briscoe et al., 2006) and used it for feature
extraction. Since 176 out of the 183 initial verbs
are represented in this corpus, our final gold stan-
dard consists of 34 classes containing 277 verbs,
of which 176 have SCF gold standard and has been
evaluated for this task. We set the parameters of
our algorithm on an held-out data, consisting of
different verbs than those used in our experiments,
to be M = 10000, K = 20 and T = 10.
Clustering Evaluation We first evaluate the
quality of the clusters induced by our algorithm
(DPP-cluster) compared to the gold standard VCs
(table 1). To evaluate the importance of the DPP
component, we compare to the performance of a
version of our algorithm where everything is kept
fixed except from the sampling which is done from
a uniform distribution rather than from the DPP
joint kernel (this model is denoted in the table
with AC for agglomerative clustering) 4. We also
compare to the state-of-the-art spectral clustering
method of Sun and Korhonen (2009) where our
4Importantly, the kernel matrix L used in the agglomera-
tive clustering process is also used by AC.
kernel matrix is used for the distance between data
points (SC) 5.
We evaluated the unified cluster set induced in
each iteration of our algorithm and of the AC base-
line and induced the same number of clusters as in
each iteration of our algorithm using the SC base-
line. Since the number of clusters in each iteration
is not an argument for our algorithm or for the AC
baseline, the number of clusters slightly differ be-
tween the two. The AC and SC baseline results
were averaged over 5 and 100 runs respectively.
DPP-cluster has produced identical output across
runs.
The table demonstrates the superiority of the
DPP-cluster model. For four out of five conditions
its F-score performance outperforms the baselines
by 4.2-8.3%. Moreover, in all conditions its recall
performances are substantially higher than those
of the baselines (by 9.7-26.1%). Note that DPP-
cluster runs for 17 iterations while the AC baseline
performs only 6. We therefore evaluated only the
last 5 iterations of each model 6.
SCF evaluation For this evaluation, we first
built a baseline SCF lexicon based on the parsed
5Sun and Korhonen (2009) report better results than those
we report for their algorithm (on a different data set). Note,
however, that they used the output of a rule-based SCF sys-
tem as a source of features, as opposed to our unsupervised
approach.
</bodyText>
<footnote confidence="0.996080333333333">
6For the additional comparable iteration the result pattern
is very similar to the (C = 89, 97.4) case in the table, and is
not presented due to space limitations.
</footnote>
<page confidence="0.9974">
867
</page>
<bodyText confidence="0.9911484">
Algorithm 1 The DPP-cluster clustering algo-
rithm. K is the size of the sampled subsets, M is
the number of subsets sampled at each iteration, Y
is the verb set, T is the number of most probable
samples to be used in each iteration
</bodyText>
<equation confidence="0.909546172413793">
Algorithm DPP-cluster :
Arguments: K,M,Y,T
Return: cluster sets S = Isl.... Sn}
i +- 1
S +- 0
while Y =� 0 do
(L1, S1) +- GenKernelMatrix(Y, SCF)
(L2, S2) +- GenKernelMatrix(Y, SP)
(L12, S12) +- GenJointMat(L1, L2)
samples +- sampleDpp(L, K, M)
topSamples +- exTop(samples,T)
Si +- cluster(topSamples, L)
Y +- Y − elements(Si)
S +- S U Si
i +- i + 1
end while
AgglomerativeClustering(S)
Function cluster :
Arguments: topSamples,L
Return: S
S +- 0, topSample +- 0
i +- 1
while (topSample n elements(S) = 0) do
topSample +- topSamples(i)
S +- m1Mapping(topSample, S)
i +- i + 1
if (i &gt; size(topSamples)) then
return S
end if
</equation>
<bodyText confidence="0.96152625">
end while
BNC corpus. We do this by gathering the GR com-
binations for each of the verbs in our gold stan-
dard, assuming they are frames and gathering their
frequencies. Note that this corpus statistics base-
line is a very strong baseline that performs very
similarly to (de Cruys et al., 2012), the best unsu-
pervised SCF model we are aware of, when run on
their dataset 7.
As shown in table 3 the corpus statistics base-
line achieves high recall (84%) at the cost of
low precision (52.5%) (similar pattern has been
</bodyText>
<subsectionHeader confidence="0.482296">
7personal communication with the authors.
</subsectionHeader>
<bodyText confidence="0.999937021276596">
demonstrated for the system of de Cruys et al.
(2012)). On the other extreme, two other com-
monly used baselines strongly prefer precision.
These are the Most Frequent SCF (O’Donovan et
al., 2005) which uniformly assigns to all verbs the
two most frequent SCFs in general language, tran-
sitive (SUBJ-DOBJ) and intransitive (SUBJ) (and
results in poor F-score), and a filtering that re-
moves frames with low corpus frequencies (which
results in low recall even when trying to provide
the maximum recall for a given precision level).
The task we address is therefore to improve the
precision of the corpus statistics baseline in a way
that does not substantially harm the F-score.
To remedy this imbalance, we apply a cluster
based filtering method on top of the maximum-
recall frequency filter. This filter excludes a candi-
date frame from a verb’s lexicon only if it meets
the frequency filter criterion and appears in no
more than N other members of the cluster of the
verb in question. The filter utilizes the clustering
produced by the seventh to last iteration of DPP-
cluster that contains seven clusters with approxi-
mately 30 members each. Such clustering should
provide a good generalization level for the task.
We report results for moderate as well as ag-
gressive filtering (N = 3 and N = 7 respectively).
Table 3 clearly demonstrates that cluster based fil-
tering (DPP-cluster and AC) is the only method
that provides a good balance between the recall
and the precision of the SCF lexicon. Moreover,
the lexicon induced by this method includes a sub-
stantially higher number of frames per verb com-
pared to the other filtering methods. While both
AC and DPP-cluster still prefer recall to precision,
DPP-cluster does so to a smaller extent 8. This
clearly demonstrates that the clustering serves to
provide SCF acquisition with semantic informa-
tion needed for improved performance.
SP evaluation We explore a variant of the
pseudo-disambiguation task of Rooth et al. (1999)
which has been applied to SP acquisition by a
number of recent papers (e.g. (de Cruys et al.,
2012)). Rooth et al. (1999) proposed to judge
which of two verbs v and v˜ is more likely to take a
given noun n as its argument. In their experiments
the model has to choose between a pair (v, n) that
</bodyText>
<footnote confidence="0.9964632">
8We show results for the maximum recall frequency fil-
tering with precision equals to 80 or 90. When the frequency
threshold is further reduced from 0.03, the same result pat-
tern hold. We do not give a detailed description due to space
limitations.
</footnote>
<page confidence="0.986115">
868
</page>
<table confidence="0.9990951">
Corpus Statistics: [P = 52.5, R = 84, F = 64.6, AF = 12.3]
Most Frequent SCF: [P = 86.7, R = 22.5, F = 35.8, AF = 2]
Clustering Moderate Clustering Aggressive
Maximum Recall Frequency Threshold Model P R F AF P R F AF
threshold = 0.03, Prec. &gt; 80 DPP-cluster 60.8 68.3 64.3 8.7 64.1 64.2 64.2 7.7
[P=88.7,R=52.4,F=65.9,AF=4.5]
AC 58 73.2 64.6 9.7 61.3 68.9 64.7 8.6
threshold = 0.05, Prec. &gt; 90 DPP-cluster 60.1 64.6 62.3 8.7 63.3 59.3 61.3 7.2
[P=92.3,R=44.4,F=59.9,AF=3.7]
AC 57.5 70.6 63.2 9.4 60.7 65.4 62.7 8.3
</table>
<tableCaption confidence="0.99719">
Table 3: SCF Results for the DPP-cluster model compared to the Corpus Statistics baseline, Most Fre-
</tableCaption>
<bodyText confidence="0.981884741379311">
quent SCF baseline, maximum-recall frequency thresholding with the maximum threshold values that
keep precision above 80 (threshold = 0.03) and above 90 (threshold = 0.05), and the AC clustering base-
line. AF is the average number of frames per verb. All methods except from cluster based filtering
(DPP-cluster and AC) induce lexicons with strong recall/precision imbalance. Cluster based fil-
tering keeps a larger number of frames in the lexicon compared to the frequency thresholding
baseline, while keeping similar F-score levels. DPP-cluster provides better recall/precision balance
than AC.
appears only in the test corpus and a pair (˜v, n)
that appears neither in the test nor in the training
corpus. Note, however, that this test only evaluates
the capability of a model to distinguish a correct
unseen verb-argument pair from an incorrect one,
but not its capability to identify erroneous pairs
when no alternative pair is presented. This last
property can strongly affect the precision of the
model.
We therefore propose to measure both aspects
of the SP task by computing both the recall and the
precision between the list of possible arguments a
verb can take according to the model and the cor-
responding test corpus list 9.
We evaluate the value of our clustering for SP
acquisition in the particularly challenging scenario
of domain adaptation. For each of the verbs in
our set we induce a list of possible noun direct ob-
jects from the BNC corpus and an equivalent list
from the North American News Text (NANT) cor-
pus. Following previous work (e.g. (de Cruys et
al., 2012)) arguments are identified using a parser
(RASP in our case). Using the verb clusters we
create a filtered version of the BNC argument lex-
icon which includes in the noun argument list of
a verb only those nouns that appear in the BNC
as arguments of that verb and of one of its cluster
members. For each verb we then compare the fil-
tered as well as the non-filtered BNC induced lex-
icon to the NANT lexicon by computing the aver-
age recall and precision between the argument lists
9In principle these measures can take into account the
probability assigned by the model to each argument and the
corresponding test corpus frequency. In this work we com-
pute probability-ignorant scores and keep more sophisticated
evaluations for future research.
and then report the average scores across all verbs.
We compare to a baseline which maintains only
noun arguments that appear at least twice in BNC
10. As a final measure of performance we compute
the ratio between the reduction in precision error
(i.e. pmodel−pbaseline
100−pbaseline ) and the increase in recall er-
ror (rbaseline−rmodel 100−rmodel ).
Table 2 presents the results for verbs with up to
200, 600 and 1000 noun arguments in the training
data. In all cases, the relative error reduction of the
DPP cluster filter is substantially higher than that
of the frequency baseline. Note that for this task
the baseline AC clusters are of low quality which
is reflects by an error reduction ratio of up to 0.5.
</bodyText>
<sectionHeader confidence="0.999015" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.95691">
In this paper we have presented the first unified
framework for the induction of verb clusters, sub-
categorization frames and selectional preferences
from corpus data. Our key idea is to cluster to-
gether verbs with similar SCFs and SPs and to use
the resulting clusters for SCF and SP induction. To
implement our idea we presented a novel method
which involves constructing a product DPP model
for SCFs and SPs and introduced a new algorithm
that utilizes the efficient DPP sampling algorithms
to cluster together verbs with similar SCFs and
SPs. The induced clusters performed well in eval-
uation against a VerbNet -based gold standard and
proved useful in improving the quality of SCFs
and SPs over strong baselines.
Our results demonstrate the benefits of a uni-
fied framework for acquiring lexical informa-
10we experimented with other threshold values for this
baseline but the recall in those case becomes very low.
</bodyText>
<page confidence="0.996165">
869
</page>
<bodyText confidence="0.9999894">
tion about different aspects of verbal predicate-
argument structure. Not only the acquisition of
different types information (syntactic and seman-
tic) can support and inform each other, but also a
unified framework can be useful for NLP tasks and
applications which require rich information about
predicate-argument structure. In future work we
plan to apply our approach on larger scale data
sets and gold standards and to evaluate it in differ-
ent domains, languages and in the context of NLP
tasks such as syntactic parsing and SRL.
In addition, in our current framework SCF and
SP information is used for clustering which is in
turn used to improve SCF and SP quality. At this
stage no further information flows from the SCF
and SP models to the clustering model. A natural
extension of our unified framework is to construct
a joint model in which the predictions for all three
tasks inform each other at all stages of the predic-
tion process.
</bodyText>
<sectionHeader confidence="0.996961" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.969326">
The work in this paper was funded by the Royal
Society University Research Fellowship (UK).
</bodyText>
<sectionHeader confidence="0.993481" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.98120264">
Ivana Romina Altamirano and Laura Alonso i Ale-
many. 2010. IRASubcat, a highly customizable,
language independent tool for the acquisition of ver-
bal subcategorization information from corpus. In
Proceedings of the NAACL HLT 2010 Young Inves-
tigators Workshop on Computational Approaches to
Languages of the Americas.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In COLING-
ACL-98.
Roberto Basili, Diego De Cao, Paolo Marocco, and
Marco Pennacchiotti. 2007. Learning selectional
preferences for entailment or paraphrasing rules. In
RANLP 2007, Borovets, Bulgaria.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
Ledir: An unsupervised algorithm for learning di-
rectionality of inference rules. In EMNLP-07, page
161170, Prague, Czech Republic.
Akshar Bharati, Sriram Venkatapathy, and Prashanth
Reddy. 2005. Inferring semantic roles using sub-
categorization frames and maximum entropy model.
In CoNLL-05.
Ted Briscoe and John Carroll. 1997. Automatic extrac-
tion of subcategorization from corpora. In ANLP-
97.
E.J. Briscoe, J. Carroll, and R. Watson. 2006. The
second relsease of the rasp system. In COLING/ACL
interactive presentation session.
Glenn Carroll and Mats Rooth. 1996. Valence induc-
tion with a head-lexicalized pcfg. In EMNLP-96.
Paula Chesley and Susanne Salmon-Alt. 2006. Au-
tomatic extraction of subcategorization frames for
french. In LREC-06.
Kostadin Cholakov and Gertjan van Noord. 2010. Us-
ing unknown word techniques to learn known words.
In EMNLP-10.
Hoa Trang Dang. 2004. Investigations into the Role of
Lexical Semantics in Word Sense Disambiguation.
Ph.D. thesis, CIS, University of Pennsylvania.
Tim Van de Cruys, Laura Rimell, Thierry Poibeau,
and Anna Korhonen. 2012. Multi-way tensor fac-
torization for unsupervised lexical acquisition. In
COLING-12.
Lukasz Dkebowski. 2009. Valence extraction us-
ing EM selection and co-occurrence matrices. Lan-
guage resources and evaluation, 43(4):301–327.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In ACL 2007, Prague,
Czech Republic.
J. Gillenwater, A. Kulesza, and B. Taskar. 2012. Dis-
covering diverse and salient threads in document
collections. In EMNLP-12.
Ralph Grishman, Catherine Macleod, and Adam Mey-
ers. 1994. Comlex syntax: Building a computa-
tional lexicon. In COLNIG-94.
G.E. Hinton. 2002. Training products of experts by
minimizing contrastive divergence. Neural Compu-
tation, 14:1771–1800.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Porceedings 0f NAACL-HLT-06
short papers.
Dino Ienco, Serena Villata, and Cristina Bosco. 2008.
Automatic extraction of subcategorization frames
for italian. In LREC-08.
Eric Joanis, Suzanne Stevenson, and David James.
2008. A general feature space for automatic verb
classification. Natural Language Engineering.
Daisuke Kawahara and Sadao Kurohashi. 2010. Ac-
quiring reliable predicate-argument structures from
raw corpora for case frame compilation. In LREC-
10.
Karin Kipper-Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA, June.
</reference>
<page confidence="0.976596">
870
</page>
<reference confidence="0.999540495145631">
Anna Korhonen, Yuval Krymolowski, and Nigel Col-
lier. 2008. The choice of features for classifica-
tion of verbs in biomedical texts. In Proceddings
of COLING-08.
Anna Korhonen. 2002. Semantically motivated
subcategorization acquisition. In Proceedings of
the ACL-02 workshop on Unsupervised lexical
acquisition-Volume 9.
A. Kulesza and B. Taskar. 2010. Structured determi-
nantal point processes. In NIPS-10.
A. Kulesza and B. Taskar. 2012a. k-dpps: fixed-size
determinantal point processes. In ICML-11.
A. Kulesza and B. Taskar. 2012b. Learning determi-
nantal point processes. In UAI-12.
Alex Kulesza and Ben Taskar. 2012c. Determi-
nantal point processes for machine learning. In
arXiv:1207.6083.
A. Kulesza. 2012. Learning with determinantal point
processes. Ph.D. thesis, CIS, University of Pennsyl-
vania.
Alessandro Lenci, Barbara McGillivray, Simonetta
Montemagni, and Vito Pirrelli. 2008. Unsuper-
vised acquisition of verb subcategorization frames
from shallow-parsed corpora. In LREC-08.
Beth Levin. 1993. English verb classes and alterna-
tions: A preliminary investigation. Chicago, IL.
Jianguo Li and Chris Brew. 2008. Which are the best
features for automatic verb classification. In ACL-
08.
Tom Lippincott, Anna Korhonen, and Diarmuid O´
S´eaghdha. 2012. Learning syntactic verb frames
using graphical models. In ACL-12, Jeju, Korea.
C´edric Messiant, Anna Korhonen, and Thierry
Poibeau. 2008. LexSchem: A large subcategoriza-
tion lexicon for French verbs. In LREC-08.
George A. Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39–41.
Alessandro Moschitti and Roberto Basili. 2005. Verb
subcategorization kernels for automatic semantic la-
beling. In Proceedings of the ACL-SIGLEX Work-
shop on Deep Lexical Acquisition.
Ruth O’Donovan, Michael Burke, Aoife Cahill, Josef
van Genabith, and Andy Way. 2005. Large-scale
induction and evaluation of lexical resources from
the penn-ii and penn-iii treebanks. Computational
Linguistics, 31:328–365.
Diarmuid O´ S´eaghdha and Anna Korhonen. 2011.
Probabilistic models of similarity in syntactic con-
text. In EMNLP-11, Edinburgh, UK.
Diarmuid O´ S´eaghdha. 2010. Latent variable models
of selectional preference. In ACL-10, Uppsala, Swe-
den.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.
Judita Preiss, Ted Briscoe, and Anna Korhonen. 2007.
A system for large-scale acquisition of verbal, nom-
inal and adjectival subcategorization frames from
corpora. In ACL-07.
Joseph Reisinger and Raymond Mooney. 2011. Cross-
cutting models of lexical semantics. In EMNLP-11,
Edinburgh, UK.
Alan Ritter and Oren Etzioni. 2010. A latent dirich-
let allocation method for selectional preferences. In
ACL-10.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via em-based clustering. In
ACL-99.
Karin Kipper Schuler. 2006. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. the-
sis, University of Pennsylvania.
S. Schulte im Walde, C. Hying, C. Scheible, and
H. Schmid. 2008. Combining EM training and the
MDL principle for an automatic verb classification
incorporating selectional preferences. In ACL-08,
pages 496–504.
Sabine Schulte im Walde. 2006. Experiments on
the automatic induction of german semantic verb
classes. Computational Linguistics, 32(2):159–194.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining framenet, verbnet and wordnet
for robust semantic parsing. In CICLING-05.
Lin Sun and Anna Korhonen. 2009. Improving verb
clustering with automatically acquired selectional
preferences. In EMNLP-09, Singapore.
Lin Sun and Anna Korhonen. 2011. Hierarchical verb
clustering using graph factorization. In EMNLP-11.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
2008. Verb class discovery from rich syntactic data.
Lecture Notes in Computer Science, 4919(16).
Robert Swier and Suzanne Stevenson. 2004. Unsuper-
vised semantic role labelling. In EMNLP-04.
Stefan Thater, Hagen Furstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In ACL-
10, Uppsala, Sweden.
Tim Van de Cruys. 2009. A non-negative tensor factor-
ization model for selectional preference induction.
In Proceedings of the workshop on Geometric Mod-
els for Natural Language Semantics (GEMS).
</reference>
<page confidence="0.980294">
871
</page>
<reference confidence="0.998596066666667">
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. 2009. Unsupervised and constrained
dirichlet process mixture models for verb cluster-
ing. In Proceedings of the Workshop on Geometrical
Models of Natural Language Semantics.
2008. Robustness and generalization of role sets:
PropBank vs. VerbNet.
Benat Zapirain, Eneko Agirre, and Lluis Marquex.
2009. Generalizing over lexical features: Selec-
tional preferences for semantic role classification. In
ACL-IJCNLP-09, Singapore.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai.
2011. Exploiting web-derived selectional prefer-
ence to improve statistical dependency parsing. In
ACL-11, Portland, OR.
</reference>
<page confidence="0.998095">
872
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.404204">
<title confidence="0.999572">Improved Lexical Acquisition through DPP-based Verb Clustering</title>
<author confidence="0.999971">Roi Reichart Anna Korhonen</author>
<affiliation confidence="0.9968">University of Cambridge, UK University of Cambridge, UK</affiliation>
<email confidence="0.780391">rr439@cam.ac.ukalk23@cam.ac.uk</email>
<abstract confidence="0.99984785">Subcategorization frames (SCFs), selectional preferences (SPs) and verb classes capture related aspects of the predicateargument structure. We present the first unified framework for unsupervised learning of these three types of information. We show how to utilize Determinantal Point Processes (DPPs), elegant probabilistic models that are defined over the possible subsets of a given dataset and give higher probability mass to high quality and diverse subsets, for clustering. Our novel clustering algorithm constructs a joint SCF-DPP DPP kernel matrix and utilizes the efficient sampling algorithms of DPPs to cluster together verbs with similar SCFs and SPs. We evaluate the induced clusters in the context of the three tasks and show results that are superior to</abstract>
<intro confidence="0.520719">baselines for each</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Ivana Romina Altamirano and Laura Alonso i Alemany.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas.</booktitle>
<contexts>
<context position="4092" citStr="(2010)" startWordPosition="620" endWordPosition="620">asks]DOBJ. Finally, VC induction involves clustering together verbs with similar meaning, reflected in similar SCFs and SPs. For example, ”show” in the above examples could get clustered together with ”demonstrate” and ”indicate”. Because these challenging tasks capture complementary information about predicate argument structure, they should be able to inform and support each other. Recently, researchers have begun to investigate the benefits of their joint learning. Schulte im Walde et al. (2008) integrated SCF and VC acquisition and used it for WordNet-based SP classification. O´ S´eaghdha (2010) presented a “dual-topic” model for SPs that induces also verb clusters. Both works reported SP evaluation with promising results. Lippincott et al. (2012) presented a joint model for inducing simple syntactic frames and VCs. They reported high accuracy results on VCs. de Cruys et al. (2012) introduced a joint model for SCF and SP acquisition. They evaluated both the SCFs and SPs, obtaining reasonable result on both tasks. In this paper, we present the first unified framework for unsupervised learning of the three types of information - SCFs, SPs and VCs. Our framework is based on Determinanta</context>
<context position="9832" citStr="(2010)" startWordPosition="1532" endWordPosition="1532">o be highly relevant for both tasks. Importantly, no manual or automatic system for SCF or SP acquisition has been utilized when constructing the kernel matrices, we only consider features extracted from the output of an unlexicalized parser. Our approach hence provides a framework for acquiring valuable information for the three tasks together. Joint Modeling A small number of works have recently investigated joint approaches to SCFs, SPs and VCs. Each of them has addressed only a subset of the tasks and all but one have evaluated the performance in the context of one task only. O´ S´eaghdha (2010) presented a “dual-topic” model for SPs that induces VCs, reporting evaluation of SPs only. Lippincott et al. (2012) presented a Bayesian network model for syntactic frame (rather than full SCF) induction that induces VCs. Only VCs are evaluated. de Cruys et al. (2012) presented a joint unsupervised model of SCF and SP acquisition based on non-negative tensor factorization. Both SCFs and SPs were evaluated. Finally, the model of Schulte im Walde et al. (2008) addresses the three types of information but SP parameters are estimated with a WordNet based method and only the SPs are evaluated. Alt</context>
</contexts>
<marker>2010</marker>
<rawString>Ivana Romina Altamirano and Laura Alonso i Alemany. 2010. IRASubcat, a highly customizable, language independent tool for the acquisition of verbal subcategorization information from corpus. In Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In COLINGACL-98.</booktitle>
<contexts>
<context position="8380" citStr="Baker et al., 1998" startWordPosition="1295" endWordPosition="1298">larity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007) and methods based on non-negative tensor factorization (Van de Cruys, 2009). These works use a variety of linguistic features in the acquisition process but none of them 863 integrates the three information types covered in our work. ther motivate the development of a framework that acquires the three types of information together. Verb clustering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project. In COLINGACL-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Basili</author>
<author>Diego De Cao</author>
<author>Paolo Marocco</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Learning selectional preferences for entailment or paraphrasing rules.</title>
<date>2007</date>
<booktitle>In RANLP 2007, Borovets,</booktitle>
<marker>Basili, De Cao, Marocco, Pennacchiotti, 2007</marker>
<rawString>Roberto Basili, Diego De Cao, Paolo Marocco, and Marco Pennacchiotti. 2007. Learning selectional preferences for entailment or paraphrasing rules. In RANLP 2007, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Bhagat</author>
<author>Patrick Pantel</author>
<author>Eduard Hovy</author>
</authors>
<title>Ledir: An unsupervised algorithm for learning directionality of inference rules.</title>
<date>2007</date>
<booktitle>In EMNLP-07,</booktitle>
<pages>161170</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="7797" citStr="Bhagat et al., 2007" startWordPosition="1203" endWordPosition="1206">ic information in SCF acquisition (Korhonen, 2002). We will address this problem in an unsupervised way: our approach is to consider SCFs together with semantic SPs through VCs which generalize over syntactically and semantically similar verbs. SP acquisition Considerable research has been conducted on SP acquisition, with a variety of unsupervised models proposed for this task that use no hand-crafted information during training. The latter approaches include latent variable models ( O´ S´eaghdha, 2010; Ritter and Etzioni, 2010; Reisinger and Mooney, 2011), distributional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007) and methods based on non-negative tensor factorization (Van de Cruys, 2009). These works use a variety of linguistic features in the acquisition process but none of them 863 integrates the three information types covered in our work. ther motivate the development of a framework that acquires the three types of information together. Verb clustering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., </context>
</contexts>
<marker>Bhagat, Pantel, Hovy, 2007</marker>
<rawString>Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007. Ledir: An unsupervised algorithm for learning directionality of inference rules. In EMNLP-07, page 161170, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akshar Bharati</author>
<author>Sriram Venkatapathy</author>
<author>Prashanth Reddy</author>
</authors>
<title>Inferring semantic roles using subcategorization frames and maximum entropy model.</title>
<date>2005</date>
<booktitle>In CoNLL-05.</booktitle>
<contexts>
<context position="1937" citStr="Bharati et al., 2005" startWordPosition="286" endWordPosition="289">93) tradition provide a shared level of abstraction for verbs that share many aspects of their syntactic and semantic behavior. These three of types of information have proved useful for Natural Language Processing (NLP) 1The source code of the clustering algorithms and evaluation is submitted with this paper and will be made publicly available upon acceptance of the paper. tasks which require information about predicateargument structure, including parsing (Shi and Mihalcea, 2005; Cholakov and van Noord, 2010; Zhou et al., 2011), semantic role labeling (Swier and Stevenson, 2004; Dang, 2004; Bharati et al., 2005; Moschitti and Basili, 2005; zap, 2008; Zapirain et al., 2009), and word sense disambiguation (Dang, 2004; Thater et al., 2010; O´ S´eaghdha and Korhonen, 2011), among many others. Because lexical information is highly sensitive to domain variation, approaches that can identify VCs, SCFs and SPs in corpora have become increasingly popular, e.g. (O’Donovan et al., 2005; Schulte im Walde, 2006; Erk, 2007; Preiss et al., 2007; Van de Cruys, 2009; Reisinger and Mooney, 2011; Sun and Korhonen, 2011; Lippincott et al., 2012). The task of SCF induction involves identifying the arguments of a verb le</context>
</contexts>
<marker>Bharati, Venkatapathy, Reddy, 2005</marker>
<rawString>Akshar Bharati, Sriram Venkatapathy, and Prashanth Reddy. 2005. Inferring semantic roles using subcategorization frames and maximum entropy model. In CoNLL-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Automatic extraction of subcategorization from corpora.</title>
<date>1997</date>
<booktitle>In ANLP97.</booktitle>
<contexts>
<context position="6183" citStr="Briscoe and Carroll, 1997" startWordPosition="957" endWordPosition="960">on against a well-known VC gold standard shows that our clustering model outperforms the state-of-theart verb clustering algorithm of Sun and Korhonen (2009), in our setup where no manually created SCF or SP data is available. Our evaluation against a well-known SCF gold standard and in the context of SP disambiguation tasks shows results that are superior to strong baselines, demonstrating the benefit our approach. 2 Previous Work SCF acquisition Most current works induce SCFs from the output of an unlexicalized parser (i.e. a parser trained without SCF annotations) using hand-written rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or grammatical relation (GR) co-occurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010). Only a handful of SCF induction works are unsupervised. Carroll and Rooth (1996) applied an EM-based approach to a context-free grammar based model, Dkebowski (2009) used point-wise co-occurrence of arguments in parsed Polish data and Lippincott et al. (2012) presented a Bayesian network model for syntactic frame indu</context>
</contexts>
<marker>Briscoe, Carroll, 1997</marker>
<rawString>Ted Briscoe and John Carroll. 1997. Automatic extraction of subcategorization from corpora. In ANLP97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
<author>J Carroll</author>
<author>R Watson</author>
</authors>
<title>The second relsease of the rasp system.</title>
<date>2006</date>
<booktitle>In COLING/ACL interactive presentation session.</booktitle>
<contexts>
<context position="23795" citStr="Briscoe et al., 2006" startWordPosition="3967" endWordPosition="3970">ecall error (each method is compared to the NF baseline). Ratio greater than 1 means that the reduction in precision error is larger than the increase in recall error (see text for exact definition). DPP based filtering provides substantially better ratio. ficient representation of each class, we collected from VerbNet the verbs for which at least one of the possible classes is represented in the 183 verbs set by at least one and at most seven verbs. This yielded 101 additional verbs which we added to the gold standard with the initial 183 verbs. We parsed the BNC corpus with the RASP parser (Briscoe et al., 2006) and used it for feature extraction. Since 176 out of the 183 initial verbs are represented in this corpus, our final gold standard consists of 34 classes containing 277 verbs, of which 176 have SCF gold standard and has been evaluated for this task. We set the parameters of our algorithm on an held-out data, consisting of different verbs than those used in our experiments, to be M = 10000, K = 20 and T = 10. Clustering Evaluation We first evaluate the quality of the clusters induced by our algorithm (DPP-cluster) compared to the gold standard VCs (table 1). To evaluate the importance of the D</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>E.J. Briscoe, J. Carroll, and R. Watson. 2006. The second relsease of the rasp system. In COLING/ACL interactive presentation session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Carroll</author>
<author>Mats Rooth</author>
</authors>
<title>Valence induction with a head-lexicalized pcfg.</title>
<date>1996</date>
<booktitle>In EMNLP-96.</booktitle>
<contexts>
<context position="6544" citStr="Carroll and Rooth (1996)" startWordPosition="1013" endWordPosition="1016">strong baselines, demonstrating the benefit our approach. 2 Previous Work SCF acquisition Most current works induce SCFs from the output of an unlexicalized parser (i.e. a parser trained without SCF annotations) using hand-written rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or grammatical relation (GR) co-occurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010). Only a handful of SCF induction works are unsupervised. Carroll and Rooth (1996) applied an EM-based approach to a context-free grammar based model, Dkebowski (2009) used point-wise co-occurrence of arguments in parsed Polish data and Lippincott et al. (2012) presented a Bayesian network model for syntactic frame induction that identifies SPs on argument types. However, the frames induced by Lippincott et al. (2012) do not capture sets of arguments for verbs so are far simpler than traditional SCFs. Current approaches to SCF acquisition suffer from lack of semantic information which is needed to guide the purely syntax-driven acquisition process. Previous works have showe</context>
</contexts>
<marker>Carroll, Rooth, 1996</marker>
<rawString>Glenn Carroll and Mats Rooth. 1996. Valence induction with a head-lexicalized pcfg. In EMNLP-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paula Chesley</author>
<author>Susanne Salmon-Alt</author>
</authors>
<title>Automatic extraction of subcategorization frames for french.</title>
<date>2006</date>
<booktitle>In LREC-06.</booktitle>
<contexts>
<context position="6329" citStr="Chesley and Salmon-Alt, 2006" startWordPosition="978" endWordPosition="981">orhonen (2009), in our setup where no manually created SCF or SP data is available. Our evaluation against a well-known SCF gold standard and in the context of SP disambiguation tasks shows results that are superior to strong baselines, demonstrating the benefit our approach. 2 Previous Work SCF acquisition Most current works induce SCFs from the output of an unlexicalized parser (i.e. a parser trained without SCF annotations) using hand-written rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or grammatical relation (GR) co-occurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010). Only a handful of SCF induction works are unsupervised. Carroll and Rooth (1996) applied an EM-based approach to a context-free grammar based model, Dkebowski (2009) used point-wise co-occurrence of arguments in parsed Polish data and Lippincott et al. (2012) presented a Bayesian network model for syntactic frame induction that identifies SPs on argument types. However, the frames induced by Lippincott et al. (2012) do not capture sets of arguments for verbs so</context>
</contexts>
<marker>Chesley, Salmon-Alt, 2006</marker>
<rawString>Paula Chesley and Susanne Salmon-Alt. 2006. Automatic extraction of subcategorization frames for french. In LREC-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kostadin Cholakov</author>
<author>Gertjan van Noord</author>
</authors>
<title>Using unknown word techniques to learn known words.</title>
<date>2010</date>
<booktitle>In EMNLP-10.</booktitle>
<marker>Cholakov, van Noord, 2010</marker>
<rawString>Kostadin Cholakov and Gertjan van Noord. 2010. Using unknown word techniques to learn known words. In EMNLP-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
</authors>
<title>Investigations into the Role of Lexical Semantics in Word Sense Disambiguation.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>CIS, University of Pennsylvania.</institution>
<contexts>
<context position="1915" citStr="Dang, 2004" startWordPosition="284" endWordPosition="285">he Levin (1993) tradition provide a shared level of abstraction for verbs that share many aspects of their syntactic and semantic behavior. These three of types of information have proved useful for Natural Language Processing (NLP) 1The source code of the clustering algorithms and evaluation is submitted with this paper and will be made publicly available upon acceptance of the paper. tasks which require information about predicateargument structure, including parsing (Shi and Mihalcea, 2005; Cholakov and van Noord, 2010; Zhou et al., 2011), semantic role labeling (Swier and Stevenson, 2004; Dang, 2004; Bharati et al., 2005; Moschitti and Basili, 2005; zap, 2008; Zapirain et al., 2009), and word sense disambiguation (Dang, 2004; Thater et al., 2010; O´ S´eaghdha and Korhonen, 2011), among many others. Because lexical information is highly sensitive to domain variation, approaches that can identify VCs, SCFs and SPs in corpora have become increasingly popular, e.g. (O’Donovan et al., 2005; Schulte im Walde, 2006; Erk, 2007; Preiss et al., 2007; Van de Cruys, 2009; Reisinger and Mooney, 2011; Sun and Korhonen, 2011; Lippincott et al., 2012). The task of SCF induction involves identifying the </context>
</contexts>
<marker>Dang, 2004</marker>
<rawString>Hoa Trang Dang. 2004. Investigations into the Role of Lexical Semantics in Word Sense Disambiguation. Ph.D. thesis, CIS, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
<author>Laura Rimell</author>
<author>Thierry Poibeau</author>
<author>Anna Korhonen</author>
</authors>
<title>Multi-way tensor factorization for unsupervised lexical acquisition.</title>
<date>2012</date>
<booktitle>In COLING-12.</booktitle>
<marker>Van de Cruys, Rimell, Poibeau, Korhonen, 2012</marker>
<rawString>Tim Van de Cruys, Laura Rimell, Thierry Poibeau, and Anna Korhonen. 2012. Multi-way tensor factorization for unsupervised lexical acquisition. In COLING-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lukasz Dkebowski</author>
</authors>
<title>Valence extraction using EM selection and co-occurrence matrices. Language resources and evaluation,</title>
<date>2009</date>
<pages>43--4</pages>
<contexts>
<context position="6629" citStr="Dkebowski (2009)" startWordPosition="1027" endWordPosition="1028">t current works induce SCFs from the output of an unlexicalized parser (i.e. a parser trained without SCF annotations) using hand-written rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or grammatical relation (GR) co-occurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010). Only a handful of SCF induction works are unsupervised. Carroll and Rooth (1996) applied an EM-based approach to a context-free grammar based model, Dkebowski (2009) used point-wise co-occurrence of arguments in parsed Polish data and Lippincott et al. (2012) presented a Bayesian network model for syntactic frame induction that identifies SPs on argument types. However, the frames induced by Lippincott et al. (2012) do not capture sets of arguments for verbs so are far simpler than traditional SCFs. Current approaches to SCF acquisition suffer from lack of semantic information which is needed to guide the purely syntax-driven acquisition process. Previous works have showed the benefit of hand-coded semantic information in SCF acquisition (Korhonen, 2002).</context>
</contexts>
<marker>Dkebowski, 2009</marker>
<rawString>Lukasz Dkebowski. 2009. Valence extraction using EM selection and co-occurrence matrices. Language resources and evaluation, 43(4):301–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>A simple, similarity-based model for selectional preferences.</title>
<date>2007</date>
<booktitle>In ACL 2007,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2343" citStr="Erk, 2007" startWordPosition="352" endWordPosition="353">edicateargument structure, including parsing (Shi and Mihalcea, 2005; Cholakov and van Noord, 2010; Zhou et al., 2011), semantic role labeling (Swier and Stevenson, 2004; Dang, 2004; Bharati et al., 2005; Moschitti and Basili, 2005; zap, 2008; Zapirain et al., 2009), and word sense disambiguation (Dang, 2004; Thater et al., 2010; O´ S´eaghdha and Korhonen, 2011), among many others. Because lexical information is highly sensitive to domain variation, approaches that can identify VCs, SCFs and SPs in corpora have become increasingly popular, e.g. (O’Donovan et al., 2005; Schulte im Walde, 2006; Erk, 2007; Preiss et al., 2007; Van de Cruys, 2009; Reisinger and Mooney, 2011; Sun and Korhonen, 2011; Lippincott et al., 2012). The task of SCF induction involves identifying the arguments of a verb lemma and generalizing about the frames (i.e. SCFs) taken by the verb, where each frame includes a number of arguments and their syntactic types. For example, in (1), the verb ”show” takes the frame SUBJ-DOBJCCOMP (subject, direct object, and clausal complement). (1) [A number of SCF acquisition papers]SUBJ [show]VERB [their readers]DOBJ [which features are most valuable for the acquisition process]CCOMP.</context>
<context position="7830" citStr="Erk, 2007" startWordPosition="1211" endWordPosition="1212">n, 2002). We will address this problem in an unsupervised way: our approach is to consider SCFs together with semantic SPs through VCs which generalize over syntactically and semantically similar verbs. SP acquisition Considerable research has been conducted on SP acquisition, with a variety of unsupervised models proposed for this task that use no hand-crafted information during training. The latter approaches include latent variable models ( O´ S´eaghdha, 2010; Ritter and Etzioni, 2010; Reisinger and Mooney, 2011), distributional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007) and methods based on non-negative tensor factorization (Van de Cruys, 2009). These works use a variety of linguistic features in the acquisition process but none of them 863 integrates the three information types covered in our work. ther motivate the development of a framework that acquires the three types of information together. Verb clustering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al.,</context>
</contexts>
<marker>Erk, 2007</marker>
<rawString>Katrin Erk. 2007. A simple, similarity-based model for selectional preferences. In ACL 2007, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gillenwater</author>
<author>A Kulesza</author>
<author>B Taskar</author>
</authors>
<title>Discovering diverse and salient threads in document collections.</title>
<date>2012</date>
<booktitle>In EMNLP-12.</booktitle>
<contexts>
<context position="13061" citStr="Gillenwater et al., 2012" startWordPosition="2050" endWordPosition="2053">l Point Processes Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. Recently (Kulesza, 2012; Kulesza and Taskar, 864 2012c) introduced them to the machine learning community and demonstrated their usefulness for a variety of tasks including document summarization, image search, modeling non-overlapping human poses in images and video and automatically building timelines of important news stories (Kulesza and Taskar, 2010; Kulesza and Taskar, 2012a; Gillenwater et al., 2012; Kulesza and Taskar, 2012b). Below we provide a brief description of the framework, a comprehensive survey can be found in (Kulesza and Taskar, 2012c). Given a set of items Y = {y1,... , yN}, a DPP P defines a probability measure on the set of all subsets of Y, 2Y. Kulesza and Taskar (2012c) restricted their discussion of DDPs to L-ensembles, where the probability of a subset Y E Y is defined through a positive semi-definite matrix L indexed by the elements of Y: PL(Y = Y ) = E Y ⊆Y det(LY) det(L + I) det(LY ) det(LY ) = (1) Where I is the N x N identity matrix and det(LO) = 1. Since L is pos</context>
</contexts>
<marker>Gillenwater, Kulesza, Taskar, 2012</marker>
<rawString>J. Gillenwater, A. Kulesza, and B. Taskar. 2012. Discovering diverse and salient threads in document collections. In EMNLP-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>Catherine Macleod</author>
<author>Adam Meyers</author>
</authors>
<title>Comlex syntax: Building a computational lexicon.</title>
<date>1994</date>
<booktitle>In COLNIG-94.</booktitle>
<contexts>
<context position="8346" citStr="Grishman et al., 1994" startWordPosition="1289" endWordPosition="1292">nd Mooney, 2011), distributional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007) and methods based on non-negative tensor factorization (Van de Cruys, 2009). These works use a variety of linguistic features in the acquisition process but none of them 863 integrates the three information types covered in our work. ther motivate the development of a framework that acquires the three types of information together. Verb clustering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The lin</context>
</contexts>
<marker>Grishman, Macleod, Meyers, 1994</marker>
<rawString>Ralph Grishman, Catherine Macleod, and Adam Meyers. 1994. Comlex syntax: Building a computational lexicon. In COLNIG-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
</authors>
<title>Training products of experts by minimizing contrastive divergence.</title>
<date>2002</date>
<journal>Neural Computation,</journal>
<pages>14--1771</pages>
<contexts>
<context position="15454" citStr="Hinton, 2002" startWordPosition="2485" endWordPosition="2486"> DPPs are particularly suitable for joint modeling as they come with various simple and intuitive ways to combine individual model kernel matrices into a joint kernel. This stems from the fact that every positive-semidefinite matrix forms a legal DPP kernel (equation 1). Given individual model DPP kernels, we would therefore like to combine them into a positive-semidefinite matrix. While there are various ways to construct a positive-semidefinite matrix from two positivesemidefinite matrices – for example, by taking their sum – in this work we are motivated by the product of experts approach (Hinton, 2002), reasoning that high quality assignments according to a product of models have to be of high quality according to each individual model, and sick for a product combination. 2 In practice we construct the joint kernel in the following way. We build on the aforementioned property that a matrix L is positive semi-definite iff L = BT B. Given two DPPs, PL1 defined by L1 = AT1 A1 and PL2 defined by L2 = AT2 A2, we construct the joint kernel L12: L12 = L1L2L2L1 = CTC (5) Where C = AT2 A2AT1A1 and CT = AT1 A1AT2 A2. 3.3 Clustering Algorithm Algorithm (1) and Figure (1) provide a pseudocode of the al</context>
</contexts>
<marker>Hinton, 2002</marker>
<rawString>G.E. Hinton. 2002. Training products of experts by minimizing contrastive divergence. Neural Computation, 14:1771–1800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>Ontonotes: the 90% solution.</title>
<date>2006</date>
<booktitle>In Porceedings 0f NAACL-HLT-06</booktitle>
<pages>short papers.</pages>
<contexts>
<context position="8436" citStr="Hovy et al., 2006" startWordPosition="1305" endWordPosition="1308">; Erk, 2007) and methods based on non-negative tensor factorization (Van de Cruys, 2009). These works use a variety of linguistic features in the acquisition process but none of them 863 integrates the three information types covered in our work. ther motivate the development of a framework that acquires the three types of information together. Verb clustering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were induced separately and then feeded </context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: the 90% solution. In Porceedings 0f NAACL-HLT-06 short papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dino Ienco</author>
<author>Serena Villata</author>
<author>Cristina Bosco</author>
</authors>
<title>Automatic extraction of subcategorization frames for italian.</title>
<date>2008</date>
<booktitle>In LREC-08.</booktitle>
<contexts>
<context position="6349" citStr="Ienco et al., 2008" startWordPosition="982" endWordPosition="985">here no manually created SCF or SP data is available. Our evaluation against a well-known SCF gold standard and in the context of SP disambiguation tasks shows results that are superior to strong baselines, demonstrating the benefit our approach. 2 Previous Work SCF acquisition Most current works induce SCFs from the output of an unlexicalized parser (i.e. a parser trained without SCF annotations) using hand-written rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or grammatical relation (GR) co-occurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010). Only a handful of SCF induction works are unsupervised. Carroll and Rooth (1996) applied an EM-based approach to a context-free grammar based model, Dkebowski (2009) used point-wise co-occurrence of arguments in parsed Polish data and Lippincott et al. (2012) presented a Bayesian network model for syntactic frame induction that identifies SPs on argument types. However, the frames induced by Lippincott et al. (2012) do not capture sets of arguments for verbs so are far simpler tha</context>
</contexts>
<marker>Ienco, Villata, Bosco, 2008</marker>
<rawString>Dino Ienco, Serena Villata, and Cristina Bosco. 2008. Automatic extraction of subcategorization frames for italian. In LREC-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Joanis</author>
<author>Suzanne Stevenson</author>
<author>David James</author>
</authors>
<title>A general feature space for automatic verb classification. Natural Language Engineering.</title>
<date>2008</date>
<contexts>
<context position="8806" citStr="Joanis et al., 2008" startWordPosition="1359" endWordPosition="1362">ety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were induced separately and then feeded as features to the clustering algorithm. Our framework combines together SCF-motivated and SP-motivated kernel matrices , and uses the joint kernel to induce verb clusters which are likely to be highly relevant for both tasks. Importantly, no manual or automatic system for SCF or SP acquisition has been utilized when constructing the kernel matrices, we only consider </context>
</contexts>
<marker>Joanis, Stevenson, James, 2008</marker>
<rawString>Eric Joanis, Suzanne Stevenson, and David James. 2008. A general feature space for automatic verb classification. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Acquiring reliable predicate-argument structures from raw corpora for case frame compilation.</title>
<date>2010</date>
<booktitle>In LREC10.</booktitle>
<contexts>
<context position="6462" citStr="Kawahara and Kurohashi, 2010" startWordPosition="1000" endWordPosition="1003">ndard and in the context of SP disambiguation tasks shows results that are superior to strong baselines, demonstrating the benefit our approach. 2 Previous Work SCF acquisition Most current works induce SCFs from the output of an unlexicalized parser (i.e. a parser trained without SCF annotations) using hand-written rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or grammatical relation (GR) co-occurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010). Only a handful of SCF induction works are unsupervised. Carroll and Rooth (1996) applied an EM-based approach to a context-free grammar based model, Dkebowski (2009) used point-wise co-occurrence of arguments in parsed Polish data and Lippincott et al. (2012) presented a Bayesian network model for syntactic frame induction that identifies SPs on argument types. However, the frames induced by Lippincott et al. (2012) do not capture sets of arguments for verbs so are far simpler than traditional SCFs. Current approaches to SCF acquisition suffer from lack of semantic information which is neede</context>
</contexts>
<marker>Kawahara, Kurohashi, 2010</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2010. Acquiring reliable predicate-argument structures from raw corpora for case frame compilation. In LREC10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper-Schuler</author>
</authors>
<title>VerbNet: A broadcoverage, comprehensive verb lexicon.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA,</location>
<contexts>
<context position="21417" citStr="Kipper-Schuler, 2005" startWordPosition="3525" endWordPosition="3526">g of exactly K clusters). This property is useful as the optimal level of generalization may be task dependent. 4 Evaluation Data sets and gold standards We evaluated the SCFs and verb clusters on gold standard datasets. We based our set of the largest available joint set for SCFs and VCs - that of (de Cruys et al., 2012). It provides SCF annotations for 183 verbs (an average of 12.3 SCF types per verb) obtained by annotating 250 corpus occurrences per verb with the SCF types of (de Cruys et al., 2012). The verbs represent a range of Levin classes at the top level of the hierarchy in VerbNet (Kipper-Schuler, 2005). Where a verb has more than one VerbNet class, we assign it to the one supported by the highest number of member verbs. To ensure sufSET 1-2 (23,K) SET3-4(22,K) SET1 (12,K) SET2 (11,K) SET 3 (12,K) SET4 (10,K) 866 |C |= 20, 21.6 |C |= 40, 41 |C |= 60, 58.6 |C |= 69, 77.6 |C |= 89, 97.4 Model R P F R P F R P F R P F R P F DPP-cluster 93.1 17.3 29.3 77.9 25.4 38.3 63 31.9 42.3 43.8 33.6 38.1 34.4 40.6 37.2 AC 67 17.8 28.2 46.6 24 31.7 40.5 29.4 34 33 34.9 33.9 24.7 41.1 30.9 SC 32.1 27.5 29.6 26.6 35.9 30.6 23.7 41.5 30.2 22.8 43.6 29.9 21.6 48.7 29.9 Table 1: Verb clustering evaluation for the</context>
</contexts>
<marker>Kipper-Schuler, 2005</marker>
<rawString>Karin Kipper-Schuler. 2005. VerbNet: A broadcoverage, comprehensive verb lexicon. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
<author>Yuval Krymolowski</author>
<author>Nigel Collier</author>
</authors>
<title>The choice of features for classification of verbs in biomedical texts.</title>
<date>2008</date>
<booktitle>In Proceddings of COLING-08.</booktitle>
<contexts>
<context position="8866" citStr="Korhonen et al., 2008" startWordPosition="1371" endWordPosition="1374"> These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were induced separately and then feeded as features to the clustering algorithm. Our framework combines together SCF-motivated and SP-motivated kernel matrices , and uses the joint kernel to induce verb clusters which are likely to be highly relevant for both tasks. Importantly, no manual or automatic system for SCF or SP acquisition has been utilized when constructing the kernel matrices, we only consider features extracted from the output of an unlexicalized parse</context>
</contexts>
<marker>Korhonen, Krymolowski, Collier, 2008</marker>
<rawString>Anna Korhonen, Yuval Krymolowski, and Nigel Collier. 2008. The choice of features for classification of verbs in biomedical texts. In Proceddings of COLING-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
</authors>
<title>Semantically motivated subcategorization acquisition.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 workshop on Unsupervised lexical acquisition-Volume 9.</booktitle>
<contexts>
<context position="6199" citStr="Korhonen, 2002" startWordPosition="961" endWordPosition="963">gold standard shows that our clustering model outperforms the state-of-theart verb clustering algorithm of Sun and Korhonen (2009), in our setup where no manually created SCF or SP data is available. Our evaluation against a well-known SCF gold standard and in the context of SP disambiguation tasks shows results that are superior to strong baselines, demonstrating the benefit our approach. 2 Previous Work SCF acquisition Most current works induce SCFs from the output of an unlexicalized parser (i.e. a parser trained without SCF annotations) using hand-written rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or grammatical relation (GR) co-occurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010). Only a handful of SCF induction works are unsupervised. Carroll and Rooth (1996) applied an EM-based approach to a context-free grammar based model, Dkebowski (2009) used point-wise co-occurrence of arguments in parsed Polish data and Lippincott et al. (2012) presented a Bayesian network model for syntactic frame induction that ident</context>
</contexts>
<marker>Korhonen, 2002</marker>
<rawString>Anna Korhonen. 2002. Semantically motivated subcategorization acquisition. In Proceedings of the ACL-02 workshop on Unsupervised lexical acquisition-Volume 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kulesza</author>
<author>B Taskar</author>
</authors>
<title>Structured determinantal point processes.</title>
<date>2010</date>
<booktitle>In NIPS-10.</booktitle>
<contexts>
<context position="13008" citStr="Kulesza and Taskar, 2010" startWordPosition="2042" endWordPosition="2045">he DPP-Cluster clustering algorithm. 3.1 Determinantal Point Processes Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. Recently (Kulesza, 2012; Kulesza and Taskar, 864 2012c) introduced them to the machine learning community and demonstrated their usefulness for a variety of tasks including document summarization, image search, modeling non-overlapping human poses in images and video and automatically building timelines of important news stories (Kulesza and Taskar, 2010; Kulesza and Taskar, 2012a; Gillenwater et al., 2012; Kulesza and Taskar, 2012b). Below we provide a brief description of the framework, a comprehensive survey can be found in (Kulesza and Taskar, 2012c). Given a set of items Y = {y1,... , yN}, a DPP P defines a probability measure on the set of all subsets of Y, 2Y. Kulesza and Taskar (2012c) restricted their discussion of DDPs to L-ensembles, where the probability of a subset Y E Y is defined through a positive semi-definite matrix L indexed by the elements of Y: PL(Y = Y ) = E Y ⊆Y det(LY) det(L + I) det(LY ) det(LY ) = (1) Where I is the </context>
</contexts>
<marker>Kulesza, Taskar, 2010</marker>
<rawString>A. Kulesza and B. Taskar. 2010. Structured determinantal point processes. In NIPS-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kulesza</author>
<author>B Taskar</author>
</authors>
<title>k-dpps: fixed-size determinantal point processes.</title>
<date>2012</date>
<booktitle>In ICML-11.</booktitle>
<contexts>
<context position="4757" citStr="Kulesza and Taskar, 2012" startWordPosition="728" endWordPosition="731">hat induces also verb clusters. Both works reported SP evaluation with promising results. Lippincott et al. (2012) presented a joint model for inducing simple syntactic frames and VCs. They reported high accuracy results on VCs. de Cruys et al. (2012) introduced a joint model for SCF and SP acquisition. They evaluated both the SCFs and SPs, obtaining reasonable result on both tasks. In this paper, we present the first unified framework for unsupervised learning of the three types of information - SCFs, SPs and VCs. Our framework is based on Determinantal Point Processes (DPPs, (Kulesza, 2012; Kulesza and Taskar, 2012c)), elegant probabilistic models that are defined over the possible subsets of a given dataset and give higher probability mass to high quality and diverse subsets. We first show how individual-task DPP kernel matrices can be naturally combined to construct a joint kernel. We use this to construct a joint SCFSP kernel. We then introduce a novel clustering algorithm based on iterative DPP sampling which can (contrary to other probabilistic frameworks such as Markov random fields) be performed both accurately and efficiently. When defined over the joint SCF and SP kernel, this new algorithm can</context>
<context position="13034" citStr="Kulesza and Taskar, 2012" startWordPosition="2046" endWordPosition="2049">algorithm. 3.1 Determinantal Point Processes Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. Recently (Kulesza, 2012; Kulesza and Taskar, 864 2012c) introduced them to the machine learning community and demonstrated their usefulness for a variety of tasks including document summarization, image search, modeling non-overlapping human poses in images and video and automatically building timelines of important news stories (Kulesza and Taskar, 2010; Kulesza and Taskar, 2012a; Gillenwater et al., 2012; Kulesza and Taskar, 2012b). Below we provide a brief description of the framework, a comprehensive survey can be found in (Kulesza and Taskar, 2012c). Given a set of items Y = {y1,... , yN}, a DPP P defines a probability measure on the set of all subsets of Y, 2Y. Kulesza and Taskar (2012c) restricted their discussion of DDPs to L-ensembles, where the probability of a subset Y E Y is defined through a positive semi-definite matrix L indexed by the elements of Y: PL(Y = Y ) = E Y ⊆Y det(LY) det(L + I) det(LY ) det(LY ) = (1) Where I is the N x N identity matrix and </context>
<context position="18242" citStr="Kulesza and Taskar, 2012" startWordPosition="2986" endWordPosition="2989">ectors. The quality score qi of the i-th verb is the average similarity of this verb with the other verbs in the dataset. Cluster set construction In its while loop, the algorithm iteratively generates fixed-size cluster sets such that each data point belongs to exactly one cluster in one set. These cluster sets form the leaf level of the tree in Figure (1). It does so by extracting the T highest probability K-point samples from a set of M subsets, each of which sampled from the joint DPP model, and clustering them by the cluster procedure. The sampling is done by the K-DPP sampling process ((Kulesza and Taskar, 2012c), page 62) 3. The cluster procedure first seeds a K-cluster set with the highest probability sample. Then, it gradually extends the clusters by iteratively mapping the samples, in decreasing order of probability, to the existing clusters (the mlMapping function). Mapping is done by attaching every point in the mapped subset to its closet cluster, where the distance between a point and the cluster is the maximum over the distances between the point and each of the points in the cluster. The mapping is many-to-one, that is, multiple points in the subset can be assigned to the same cluster. Bas</context>
</contexts>
<marker>Kulesza, Taskar, 2012</marker>
<rawString>A. Kulesza and B. Taskar. 2012a. k-dpps: fixed-size determinantal point processes. In ICML-11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kulesza</author>
<author>B Taskar</author>
</authors>
<title>Learning determinantal point processes.</title>
<date>2012</date>
<booktitle>In UAI-12.</booktitle>
<contexts>
<context position="4757" citStr="Kulesza and Taskar, 2012" startWordPosition="728" endWordPosition="731">hat induces also verb clusters. Both works reported SP evaluation with promising results. Lippincott et al. (2012) presented a joint model for inducing simple syntactic frames and VCs. They reported high accuracy results on VCs. de Cruys et al. (2012) introduced a joint model for SCF and SP acquisition. They evaluated both the SCFs and SPs, obtaining reasonable result on both tasks. In this paper, we present the first unified framework for unsupervised learning of the three types of information - SCFs, SPs and VCs. Our framework is based on Determinantal Point Processes (DPPs, (Kulesza, 2012; Kulesza and Taskar, 2012c)), elegant probabilistic models that are defined over the possible subsets of a given dataset and give higher probability mass to high quality and diverse subsets. We first show how individual-task DPP kernel matrices can be naturally combined to construct a joint kernel. We use this to construct a joint SCFSP kernel. We then introduce a novel clustering algorithm based on iterative DPP sampling which can (contrary to other probabilistic frameworks such as Markov random fields) be performed both accurately and efficiently. When defined over the joint SCF and SP kernel, this new algorithm can</context>
<context position="13034" citStr="Kulesza and Taskar, 2012" startWordPosition="2046" endWordPosition="2049">algorithm. 3.1 Determinantal Point Processes Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. Recently (Kulesza, 2012; Kulesza and Taskar, 864 2012c) introduced them to the machine learning community and demonstrated their usefulness for a variety of tasks including document summarization, image search, modeling non-overlapping human poses in images and video and automatically building timelines of important news stories (Kulesza and Taskar, 2010; Kulesza and Taskar, 2012a; Gillenwater et al., 2012; Kulesza and Taskar, 2012b). Below we provide a brief description of the framework, a comprehensive survey can be found in (Kulesza and Taskar, 2012c). Given a set of items Y = {y1,... , yN}, a DPP P defines a probability measure on the set of all subsets of Y, 2Y. Kulesza and Taskar (2012c) restricted their discussion of DDPs to L-ensembles, where the probability of a subset Y E Y is defined through a positive semi-definite matrix L indexed by the elements of Y: PL(Y = Y ) = E Y ⊆Y det(LY) det(L + I) det(LY ) det(LY ) = (1) Where I is the N x N identity matrix and </context>
<context position="18242" citStr="Kulesza and Taskar, 2012" startWordPosition="2986" endWordPosition="2989">ectors. The quality score qi of the i-th verb is the average similarity of this verb with the other verbs in the dataset. Cluster set construction In its while loop, the algorithm iteratively generates fixed-size cluster sets such that each data point belongs to exactly one cluster in one set. These cluster sets form the leaf level of the tree in Figure (1). It does so by extracting the T highest probability K-point samples from a set of M subsets, each of which sampled from the joint DPP model, and clustering them by the cluster procedure. The sampling is done by the K-DPP sampling process ((Kulesza and Taskar, 2012c), page 62) 3. The cluster procedure first seeds a K-cluster set with the highest probability sample. Then, it gradually extends the clusters by iteratively mapping the samples, in decreasing order of probability, to the existing clusters (the mlMapping function). Mapping is done by attaching every point in the mapped subset to its closet cluster, where the distance between a point and the cluster is the maximum over the distances between the point and each of the points in the cluster. The mapping is many-to-one, that is, multiple points in the subset can be assigned to the same cluster. Bas</context>
</contexts>
<marker>Kulesza, Taskar, 2012</marker>
<rawString>A. Kulesza and B. Taskar. 2012b. Learning determinantal point processes. In UAI-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Kulesza</author>
<author>Ben Taskar</author>
</authors>
<title>Determinantal point processes for machine learning.</title>
<date>2012</date>
<booktitle>In arXiv:1207.6083.</booktitle>
<contexts>
<context position="4757" citStr="Kulesza and Taskar, 2012" startWordPosition="728" endWordPosition="731">hat induces also verb clusters. Both works reported SP evaluation with promising results. Lippincott et al. (2012) presented a joint model for inducing simple syntactic frames and VCs. They reported high accuracy results on VCs. de Cruys et al. (2012) introduced a joint model for SCF and SP acquisition. They evaluated both the SCFs and SPs, obtaining reasonable result on both tasks. In this paper, we present the first unified framework for unsupervised learning of the three types of information - SCFs, SPs and VCs. Our framework is based on Determinantal Point Processes (DPPs, (Kulesza, 2012; Kulesza and Taskar, 2012c)), elegant probabilistic models that are defined over the possible subsets of a given dataset and give higher probability mass to high quality and diverse subsets. We first show how individual-task DPP kernel matrices can be naturally combined to construct a joint kernel. We use this to construct a joint SCFSP kernel. We then introduce a novel clustering algorithm based on iterative DPP sampling which can (contrary to other probabilistic frameworks such as Markov random fields) be performed both accurately and efficiently. When defined over the joint SCF and SP kernel, this new algorithm can</context>
<context position="13034" citStr="Kulesza and Taskar, 2012" startWordPosition="2046" endWordPosition="2049">algorithm. 3.1 Determinantal Point Processes Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. Recently (Kulesza, 2012; Kulesza and Taskar, 864 2012c) introduced them to the machine learning community and demonstrated their usefulness for a variety of tasks including document summarization, image search, modeling non-overlapping human poses in images and video and automatically building timelines of important news stories (Kulesza and Taskar, 2010; Kulesza and Taskar, 2012a; Gillenwater et al., 2012; Kulesza and Taskar, 2012b). Below we provide a brief description of the framework, a comprehensive survey can be found in (Kulesza and Taskar, 2012c). Given a set of items Y = {y1,... , yN}, a DPP P defines a probability measure on the set of all subsets of Y, 2Y. Kulesza and Taskar (2012c) restricted their discussion of DDPs to L-ensembles, where the probability of a subset Y E Y is defined through a positive semi-definite matrix L indexed by the elements of Y: PL(Y = Y ) = E Y ⊆Y det(LY) det(L + I) det(LY ) det(LY ) = (1) Where I is the N x N identity matrix and </context>
<context position="18242" citStr="Kulesza and Taskar, 2012" startWordPosition="2986" endWordPosition="2989">ectors. The quality score qi of the i-th verb is the average similarity of this verb with the other verbs in the dataset. Cluster set construction In its while loop, the algorithm iteratively generates fixed-size cluster sets such that each data point belongs to exactly one cluster in one set. These cluster sets form the leaf level of the tree in Figure (1). It does so by extracting the T highest probability K-point samples from a set of M subsets, each of which sampled from the joint DPP model, and clustering them by the cluster procedure. The sampling is done by the K-DPP sampling process ((Kulesza and Taskar, 2012c), page 62) 3. The cluster procedure first seeds a K-cluster set with the highest probability sample. Then, it gradually extends the clusters by iteratively mapping the samples, in decreasing order of probability, to the existing clusters (the mlMapping function). Mapping is done by attaching every point in the mapped subset to its closet cluster, where the distance between a point and the cluster is the maximum over the distances between the point and each of the points in the cluster. The mapping is many-to-one, that is, multiple points in the subset can be assigned to the same cluster. Bas</context>
</contexts>
<marker>Kulesza, Taskar, 2012</marker>
<rawString>Alex Kulesza and Ben Taskar. 2012c. Determinantal point processes for machine learning. In arXiv:1207.6083.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kulesza</author>
</authors>
<title>Learning with determinantal point processes.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>CIS, University of Pennsylvania.</institution>
<contexts>
<context position="4731" citStr="Kulesza, 2012" startWordPosition="726" endWordPosition="727">model for SPs that induces also verb clusters. Both works reported SP evaluation with promising results. Lippincott et al. (2012) presented a joint model for inducing simple syntactic frames and VCs. They reported high accuracy results on VCs. de Cruys et al. (2012) introduced a joint model for SCF and SP acquisition. They evaluated both the SCFs and SPs, obtaining reasonable result on both tasks. In this paper, we present the first unified framework for unsupervised learning of the three types of information - SCFs, SPs and VCs. Our framework is based on Determinantal Point Processes (DPPs, (Kulesza, 2012; Kulesza and Taskar, 2012c)), elegant probabilistic models that are defined over the possible subsets of a given dataset and give higher probability mass to high quality and diverse subsets. We first show how individual-task DPP kernel matrices can be naturally combined to construct a joint kernel. We use this to construct a joint SCFSP kernel. We then introduce a novel clustering algorithm based on iterative DPP sampling which can (contrary to other probabilistic frameworks such as Markov random fields) be performed both accurately and efficiently. When defined over the joint SCF and SP kern</context>
<context position="12675" citStr="Kulesza, 2012" startWordPosition="1995" endWordPosition="1996">each cluster member belong to the same gold cluster) and together provide good coverage of the verb space. After a brief description of the Determinantal Point Processes (DPP) framework (Section 3.1), we discuss the construction of the joint DPP kernel, given a kernel for each individual task, In section 3.3 we present the DPP-Cluster clustering algorithm. 3.1 Determinantal Point Processes Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. Recently (Kulesza, 2012; Kulesza and Taskar, 864 2012c) introduced them to the machine learning community and demonstrated their usefulness for a variety of tasks including document summarization, image search, modeling non-overlapping human poses in images and video and automatically building timelines of important news stories (Kulesza and Taskar, 2010; Kulesza and Taskar, 2012a; Gillenwater et al., 2012; Kulesza and Taskar, 2012b). Below we provide a brief description of the framework, a comprehensive survey can be found in (Kulesza and Taskar, 2012c). Given a set of items Y = {y1,... , yN}, a DPP P defines a pro</context>
</contexts>
<marker>Kulesza, 2012</marker>
<rawString>A. Kulesza. 2012. Learning with determinantal point processes. Ph.D. thesis, CIS, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Lenci</author>
<author>Barbara McGillivray</author>
<author>Simonetta Montemagni</author>
<author>Vito Pirrelli</author>
</authors>
<title>Unsupervised acquisition of verb subcategorization frames from shallow-parsed corpora.</title>
<date>2008</date>
<booktitle>In LREC-08.</booktitle>
<contexts>
<context position="6392" citStr="Lenci et al., 2008" startWordPosition="990" endWordPosition="993">available. Our evaluation against a well-known SCF gold standard and in the context of SP disambiguation tasks shows results that are superior to strong baselines, demonstrating the benefit our approach. 2 Previous Work SCF acquisition Most current works induce SCFs from the output of an unlexicalized parser (i.e. a parser trained without SCF annotations) using hand-written rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or grammatical relation (GR) co-occurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010). Only a handful of SCF induction works are unsupervised. Carroll and Rooth (1996) applied an EM-based approach to a context-free grammar based model, Dkebowski (2009) used point-wise co-occurrence of arguments in parsed Polish data and Lippincott et al. (2012) presented a Bayesian network model for syntactic frame induction that identifies SPs on argument types. However, the frames induced by Lippincott et al. (2012) do not capture sets of arguments for verbs so are far simpler than traditional SCFs. Current approaches to S</context>
</contexts>
<marker>Lenci, McGillivray, Montemagni, Pirrelli, 2008</marker>
<rawString>Alessandro Lenci, Barbara McGillivray, Simonetta Montemagni, and Vito Pirrelli. 2008. Unsupervised acquisition of verb subcategorization frames from shallow-parsed corpora. In LREC-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English verb classes and alternations: A preliminary investigation.</title>
<date>1993</date>
<location>Chicago, IL.</location>
<contexts>
<context position="1320" citStr="Levin (1993)" startWordPosition="191" endWordPosition="192"> a joint SCF-DPP DPP kernel matrix and utilizes the efficient sampling algorithms of DPPs to cluster together verbs with similar SCFs and SPs. We evaluate the induced clusters in the context of the three tasks and show results that are superior to strong baselines for each 1. 1 Introduction Verb classes (VCs), subcategorization frames (SCFs) and selectional preferences (SPs) capture different aspects of predicate-argument structure. SCFs describe the syntactic realization of verbal predicate-argument structure, SPs capture the semantic preferences verbs have for their arguments and VCs in the Levin (1993) tradition provide a shared level of abstraction for verbs that share many aspects of their syntactic and semantic behavior. These three of types of information have proved useful for Natural Language Processing (NLP) 1The source code of the clustering algorithms and evaluation is submitted with this paper and will be made publicly available upon acceptance of the paper. tasks which require information about predicateargument structure, including parsing (Shi and Mihalcea, 2005; Cholakov and van Noord, 2010; Zhou et al., 2011), semantic role labeling (Swier and Stevenson, 2004; Dang, 2004; Bha</context>
<context position="8483" citStr="Levin, 1993" startWordPosition="1316" endWordPosition="1317"> factorization (Van de Cruys, 2009). These works use a variety of linguistic features in the acquisition process but none of them 863 integrates the three information types covered in our work. ther motivate the development of a framework that acquires the three types of information together. Verb clustering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were induced separately and then feeded as features to the clustering algorithm. Our fr</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English verb classes and alternations: A preliminary investigation. Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianguo Li</author>
<author>Chris Brew</author>
</authors>
<title>Which are the best features for automatic verb classification.</title>
<date>2008</date>
<booktitle>In ACL08.</booktitle>
<contexts>
<context position="8843" citStr="Li and Brew, 2008" startWordPosition="1367" endWordPosition="1370"> in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were induced separately and then feeded as features to the clustering algorithm. Our framework combines together SCF-motivated and SP-motivated kernel matrices , and uses the joint kernel to induce verb clusters which are likely to be highly relevant for both tasks. Importantly, no manual or automatic system for SCF or SP acquisition has been utilized when constructing the kernel matrices, we only consider features extracted from the output of</context>
</contexts>
<marker>Li, Brew, 2008</marker>
<rawString>Jianguo Li and Chris Brew. 2008. Which are the best features for automatic verb classification. In ACL08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Lippincott</author>
<author>Anna Korhonen</author>
<author>Diarmuid O´ S´eaghdha</author>
</authors>
<title>Learning syntactic verb frames using graphical models.</title>
<date>2012</date>
<booktitle>In ACL-12, Jeju,</booktitle>
<marker>Lippincott, Korhonen, S´eaghdha, 2012</marker>
<rawString>Tom Lippincott, Anna Korhonen, and Diarmuid O´ S´eaghdha. 2012. Learning syntactic verb frames using graphical models. In ACL-12, Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C´edric Messiant</author>
<author>Anna Korhonen</author>
<author>Thierry Poibeau</author>
</authors>
<title>LexSchem: A large subcategorization lexicon for French verbs.</title>
<date>2008</date>
<booktitle>In LREC-08.</booktitle>
<contexts>
<context position="6372" citStr="Messiant et al., 2008" startWordPosition="986" endWordPosition="989">ated SCF or SP data is available. Our evaluation against a well-known SCF gold standard and in the context of SP disambiguation tasks shows results that are superior to strong baselines, demonstrating the benefit our approach. 2 Previous Work SCF acquisition Most current works induce SCFs from the output of an unlexicalized parser (i.e. a parser trained without SCF annotations) using hand-written rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or grammatical relation (GR) co-occurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010). Only a handful of SCF induction works are unsupervised. Carroll and Rooth (1996) applied an EM-based approach to a context-free grammar based model, Dkebowski (2009) used point-wise co-occurrence of arguments in parsed Polish data and Lippincott et al. (2012) presented a Bayesian network model for syntactic frame induction that identifies SPs on argument types. However, the frames induced by Lippincott et al. (2012) do not capture sets of arguments for verbs so are far simpler than traditional SCFs. Cur</context>
</contexts>
<marker>Messiant, Korhonen, Poibeau, 2008</marker>
<rawString>C´edric Messiant, Anna Korhonen, and Thierry Poibeau. 2008. LexSchem: A large subcategorization lexicon for French verbs. In LREC-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="8360" citStr="Miller, 1995" startWordPosition="1293" endWordPosition="1294">ibutional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007) and methods based on non-negative tensor factorization (Van de Cruys, 2009). These works use a variety of linguistic features in the acquisition process but none of them 863 integrates the three information types covered in our work. ther motivate the development of a framework that acquires the three types of information together. Verb clustering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic featur</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Roberto Basili</author>
</authors>
<title>Verb subcategorization kernels for automatic semantic labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition.</booktitle>
<contexts>
<context position="1965" citStr="Moschitti and Basili, 2005" startWordPosition="290" endWordPosition="293">a shared level of abstraction for verbs that share many aspects of their syntactic and semantic behavior. These three of types of information have proved useful for Natural Language Processing (NLP) 1The source code of the clustering algorithms and evaluation is submitted with this paper and will be made publicly available upon acceptance of the paper. tasks which require information about predicateargument structure, including parsing (Shi and Mihalcea, 2005; Cholakov and van Noord, 2010; Zhou et al., 2011), semantic role labeling (Swier and Stevenson, 2004; Dang, 2004; Bharati et al., 2005; Moschitti and Basili, 2005; zap, 2008; Zapirain et al., 2009), and word sense disambiguation (Dang, 2004; Thater et al., 2010; O´ S´eaghdha and Korhonen, 2011), among many others. Because lexical information is highly sensitive to domain variation, approaches that can identify VCs, SCFs and SPs in corpora have become increasingly popular, e.g. (O’Donovan et al., 2005; Schulte im Walde, 2006; Erk, 2007; Preiss et al., 2007; Van de Cruys, 2009; Reisinger and Mooney, 2011; Sun and Korhonen, 2011; Lippincott et al., 2012). The task of SCF induction involves identifying the arguments of a verb lemma and generalizing about t</context>
</contexts>
<marker>Moschitti, Basili, 2005</marker>
<rawString>Alessandro Moschitti and Roberto Basili. 2005. Verb subcategorization kernels for automatic semantic labeling. In Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruth O’Donovan</author>
<author>Michael Burke</author>
<author>Aoife Cahill</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Large-scale induction and evaluation of lexical resources from the penn-ii and penn-iii treebanks.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<pages>31--328</pages>
<marker>O’Donovan, Burke, Cahill, van Genabith, Way, 2005</marker>
<rawString>Ruth O’Donovan, Michael Burke, Aoife Cahill, Josef van Genabith, and Andy Way. 2005. Large-scale induction and evaluation of lexical resources from the penn-ii and penn-iii treebanks. Computational Linguistics, 31:328–365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
<author>Anna Korhonen</author>
</authors>
<title>Probabilistic models of similarity in syntactic context.</title>
<date>2011</date>
<booktitle>In EMNLP-11,</booktitle>
<location>Edinburgh, UK.</location>
<marker>S´eaghdha, Korhonen, 2011</marker>
<rawString>Diarmuid O´ S´eaghdha and Anna Korhonen. 2011. Probabilistic models of similarity in syntactic context. In EMNLP-11, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
</authors>
<title>Latent variable models of selectional preference.</title>
<date>2010</date>
<booktitle>In ACL-10,</booktitle>
<location>Uppsala,</location>
<marker>S´eaghdha, 2010</marker>
<rawString>Diarmuid O´ S´eaghdha. 2010. Latent variable models of selectional preference. In ACL-10, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="8401" citStr="Palmer et al., 2005" startWordPosition="1299" endWordPosition="1302">at et al., 2007; Basili et al., 2007; Erk, 2007) and methods based on non-negative tensor factorization (Van de Cruys, 2009). These works use a variety of linguistic features in the acquisition process but none of them 863 integrates the three information types covered in our work. ther motivate the development of a framework that acquires the three types of information together. Verb clustering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were </context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judita Preiss</author>
<author>Ted Briscoe</author>
<author>Anna Korhonen</author>
</authors>
<title>A system for large-scale acquisition of verbal, nominal and adjectival subcategorization frames from corpora.</title>
<date>2007</date>
<booktitle>In ACL-07.</booktitle>
<contexts>
<context position="2364" citStr="Preiss et al., 2007" startWordPosition="354" endWordPosition="357">ment structure, including parsing (Shi and Mihalcea, 2005; Cholakov and van Noord, 2010; Zhou et al., 2011), semantic role labeling (Swier and Stevenson, 2004; Dang, 2004; Bharati et al., 2005; Moschitti and Basili, 2005; zap, 2008; Zapirain et al., 2009), and word sense disambiguation (Dang, 2004; Thater et al., 2010; O´ S´eaghdha and Korhonen, 2011), among many others. Because lexical information is highly sensitive to domain variation, approaches that can identify VCs, SCFs and SPs in corpora have become increasingly popular, e.g. (O’Donovan et al., 2005; Schulte im Walde, 2006; Erk, 2007; Preiss et al., 2007; Van de Cruys, 2009; Reisinger and Mooney, 2011; Sun and Korhonen, 2011; Lippincott et al., 2012). The task of SCF induction involves identifying the arguments of a verb lemma and generalizing about the frames (i.e. SCFs) taken by the verb, where each frame includes a number of arguments and their syntactic types. For example, in (1), the verb ”show” takes the frame SUBJ-DOBJCCOMP (subject, direct object, and clausal complement). (1) [A number of SCF acquisition papers]SUBJ [show]VERB [their readers]DOBJ [which features are most valuable for the acquisition process]CCOMP. SP induction involve</context>
<context position="6221" citStr="Preiss et al., 2007" startWordPosition="964" endWordPosition="967">ows that our clustering model outperforms the state-of-theart verb clustering algorithm of Sun and Korhonen (2009), in our setup where no manually created SCF or SP data is available. Our evaluation against a well-known SCF gold standard and in the context of SP disambiguation tasks shows results that are superior to strong baselines, demonstrating the benefit our approach. 2 Previous Work SCF acquisition Most current works induce SCFs from the output of an unlexicalized parser (i.e. a parser trained without SCF annotations) using hand-written rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or grammatical relation (GR) co-occurrence statistics (O’Donovan et al., 2005; Chesley and Salmon-Alt, 2006; Ienco et al., 2008; Messiant et al., 2008; Lenci et al., 2008; Altamirano and Alonso i Alemany, 2010; Kawahara and Kurohashi, 2010). Only a handful of SCF induction works are unsupervised. Carroll and Rooth (1996) applied an EM-based approach to a context-free grammar based model, Dkebowski (2009) used point-wise co-occurrence of arguments in parsed Polish data and Lippincott et al. (2012) presented a Bayesian network model for syntactic frame induction that identifies SPs on argument </context>
</contexts>
<marker>Preiss, Briscoe, Korhonen, 2007</marker>
<rawString>Judita Preiss, Ted Briscoe, and Anna Korhonen. 2007. A system for large-scale acquisition of verbal, nominal and adjectival subcategorization frames from corpora. In ACL-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond Mooney</author>
</authors>
<title>Crosscutting models of lexical semantics.</title>
<date>2011</date>
<booktitle>In EMNLP-11,</booktitle>
<location>Edinburgh, UK.</location>
<contexts>
<context position="2412" citStr="Reisinger and Mooney, 2011" startWordPosition="362" endWordPosition="365">d Mihalcea, 2005; Cholakov and van Noord, 2010; Zhou et al., 2011), semantic role labeling (Swier and Stevenson, 2004; Dang, 2004; Bharati et al., 2005; Moschitti and Basili, 2005; zap, 2008; Zapirain et al., 2009), and word sense disambiguation (Dang, 2004; Thater et al., 2010; O´ S´eaghdha and Korhonen, 2011), among many others. Because lexical information is highly sensitive to domain variation, approaches that can identify VCs, SCFs and SPs in corpora have become increasingly popular, e.g. (O’Donovan et al., 2005; Schulte im Walde, 2006; Erk, 2007; Preiss et al., 2007; Van de Cruys, 2009; Reisinger and Mooney, 2011; Sun and Korhonen, 2011; Lippincott et al., 2012). The task of SCF induction involves identifying the arguments of a verb lemma and generalizing about the frames (i.e. SCFs) taken by the verb, where each frame includes a number of arguments and their syntactic types. For example, in (1), the verb ”show” takes the frame SUBJ-DOBJCCOMP (subject, direct object, and clausal complement). (1) [A number of SCF acquisition papers]SUBJ [show]VERB [their readers]DOBJ [which features are most valuable for the acquisition process]CCOMP. SP induction involves identifying and classifying the lexical items </context>
<context position="7741" citStr="Reisinger and Mooney, 2011" startWordPosition="1195" endWordPosition="1198">ess. Previous works have showed the benefit of hand-coded semantic information in SCF acquisition (Korhonen, 2002). We will address this problem in an unsupervised way: our approach is to consider SCFs together with semantic SPs through VCs which generalize over syntactically and semantically similar verbs. SP acquisition Considerable research has been conducted on SP acquisition, with a variety of unsupervised models proposed for this task that use no hand-crafted information during training. The latter approaches include latent variable models ( O´ S´eaghdha, 2010; Ritter and Etzioni, 2010; Reisinger and Mooney, 2011), distributional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007) and methods based on non-negative tensor factorization (Van de Cruys, 2009). These works use a variety of linguistic features in the acquisition process but none of them 863 integrates the three information types covered in our work. ther motivate the development of a framework that acquires the three types of information together. Verb clustering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al.,</context>
</contexts>
<marker>Reisinger, Mooney, 2011</marker>
<rawString>Joseph Reisinger and Raymond Mooney. 2011. Crosscutting models of lexical semantics. In EMNLP-11, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Oren Etzioni</author>
</authors>
<title>A latent dirichlet allocation method for selectional preferences.</title>
<date>2010</date>
<booktitle>In ACL-10.</booktitle>
<contexts>
<context position="7712" citStr="Ritter and Etzioni, 2010" startWordPosition="1191" endWordPosition="1194">ax-driven acquisition process. Previous works have showed the benefit of hand-coded semantic information in SCF acquisition (Korhonen, 2002). We will address this problem in an unsupervised way: our approach is to consider SCFs together with semantic SPs through VCs which generalize over syntactically and semantically similar verbs. SP acquisition Considerable research has been conducted on SP acquisition, with a variety of unsupervised models proposed for this task that use no hand-crafted information during training. The latter approaches include latent variable models ( O´ S´eaghdha, 2010; Ritter and Etzioni, 2010; Reisinger and Mooney, 2011), distributional similarity methods (Bhagat et al., 2007; Basili et al., 2007; Erk, 2007) and methods based on non-negative tensor factorization (Van de Cruys, 2009). These works use a variety of linguistic features in the acquisition process but none of them 863 integrates the three information types covered in our work. ther motivate the development of a framework that acquires the three types of information together. Verb clustering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic clas</context>
</contexts>
<marker>Ritter, Etzioni, 2010</marker>
<rawString>Alan Ritter and Oren Etzioni. 2010. A latent dirichlet allocation method for selectional preferences. In ACL-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mats Rooth</author>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
<author>Glenn Carroll</author>
<author>Franz Beil</author>
</authors>
<title>Inducing a semantically annotated lexicon via em-based clustering.</title>
<date>1999</date>
<booktitle>In ACL-99.</booktitle>
<contexts>
<context position="29776" citStr="Rooth et al. (1999)" startWordPosition="4997" endWordPosition="5000">d filtering (DPP-cluster and AC) is the only method that provides a good balance between the recall and the precision of the SCF lexicon. Moreover, the lexicon induced by this method includes a substantially higher number of frames per verb compared to the other filtering methods. While both AC and DPP-cluster still prefer recall to precision, DPP-cluster does so to a smaller extent 8. This clearly demonstrates that the clustering serves to provide SCF acquisition with semantic information needed for improved performance. SP evaluation We explore a variant of the pseudo-disambiguation task of Rooth et al. (1999) which has been applied to SP acquisition by a number of recent papers (e.g. (de Cruys et al., 2012)). Rooth et al. (1999) proposed to judge which of two verbs v and v˜ is more likely to take a given noun n as its argument. In their experiments the model has to choose between a pair (v, n) that 8We show results for the maximum recall frequency filtering with precision equals to 80 or 90. When the frequency threshold is further reduced from 0.03, the same result pattern hold. We do not give a detailed description due to space limitations. 868 Corpus Statistics: [P = 52.5, R = 84, F = 64.6, AF =</context>
</contexts>
<marker>Rooth, Riezler, Prescher, Carroll, Beil, 1999</marker>
<rawString>Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Carroll, and Franz Beil. 1999. Inducing a semantically annotated lexicon via em-based clustering. In ACL-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper Schuler</author>
</authors>
<title>VerbNet: A BroadCoverage, Comprehensive Verb Lexicon.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="8416" citStr="Schuler, 2006" startWordPosition="1303" endWordPosition="1304">li et al., 2007; Erk, 2007) and methods based on non-negative tensor factorization (Van de Cruys, 2009). These works use a variety of linguistic features in the acquisition process but none of them 863 integrates the three information types covered in our work. ther motivate the development of a framework that acquires the three types of information together. Verb clustering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were induced separat</context>
</contexts>
<marker>Schuler, 2006</marker>
<rawString>Karin Kipper Schuler. 2006. VerbNet: A BroadCoverage, Comprehensive Verb Lexicon. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Schulte im Walde</author>
<author>C Hying</author>
<author>C Scheible</author>
<author>H Schmid</author>
</authors>
<title>Combining EM training and the MDL principle for an automatic verb classification incorporating selectional preferences.</title>
<date>2008</date>
<booktitle>In ACL-08,</booktitle>
<pages>496--504</pages>
<contexts>
<context position="3989" citStr="Walde et al. (2008)" startWordPosition="602" endWordPosition="605">iation for Computational Linguistics [show]VERB [no evidence to the usefulness of joint learning leaning for these tasks]DOBJ. Finally, VC induction involves clustering together verbs with similar meaning, reflected in similar SCFs and SPs. For example, ”show” in the above examples could get clustered together with ”demonstrate” and ”indicate”. Because these challenging tasks capture complementary information about predicate argument structure, they should be able to inform and support each other. Recently, researchers have begun to investigate the benefits of their joint learning. Schulte im Walde et al. (2008) integrated SCF and VC acquisition and used it for WordNet-based SP classification. O´ S´eaghdha (2010) presented a “dual-topic” model for SPs that induces also verb clusters. Both works reported SP evaluation with promising results. Lippincott et al. (2012) presented a joint model for inducing simple syntactic frames and VCs. They reported high accuracy results on VCs. de Cruys et al. (2012) introduced a joint model for SCF and SP acquisition. They evaluated both the SCFs and SPs, obtaining reasonable result on both tasks. In this paper, we present the first unified framework for unsupervised</context>
<context position="10295" citStr="Walde et al. (2008)" startWordPosition="1606" endWordPosition="1609">s. Each of them has addressed only a subset of the tasks and all but one have evaluated the performance in the context of one task only. O´ S´eaghdha (2010) presented a “dual-topic” model for SPs that induces VCs, reporting evaluation of SPs only. Lippincott et al. (2012) presented a Bayesian network model for syntactic frame (rather than full SCF) induction that induces VCs. Only VCs are evaluated. de Cruys et al. (2012) presented a joint unsupervised model of SCF and SP acquisition based on non-negative tensor factorization. Both SCFs and SPs were evaluated. Finally, the model of Schulte im Walde et al. (2008) addresses the three types of information but SP parameters are estimated with a WordNet based method and only the SPs are evaluated. Although evaluation of these recent joint models has been partial, the results have been encouraging and fur3 The Unified Framework In this section we present our unified framework. Our idea is to utilize DPPs for verb clustering that informs both SCF and SP acquisition. DPPs define a probability distribution over the possible subsets of a given set. These models assign higher probability mass to subsets that are both high quality and diverse. Our novel clusteri</context>
</contexts>
<marker>Walde, Hying, Scheible, Schmid, 2008</marker>
<rawString>S. Schulte im Walde, C. Hying, C. Scheible, and H. Schmid. 2008. Combining EM training and the MDL principle for an automatic verb classification incorporating selectional preferences. In ACL-08, pages 496–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Experiments on the automatic induction of german semantic verb classes.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>2</issue>
<contexts>
<context position="2332" citStr="Walde, 2006" startWordPosition="350" endWordPosition="351">tion about predicateargument structure, including parsing (Shi and Mihalcea, 2005; Cholakov and van Noord, 2010; Zhou et al., 2011), semantic role labeling (Swier and Stevenson, 2004; Dang, 2004; Bharati et al., 2005; Moschitti and Basili, 2005; zap, 2008; Zapirain et al., 2009), and word sense disambiguation (Dang, 2004; Thater et al., 2010; O´ S´eaghdha and Korhonen, 2011), among many others. Because lexical information is highly sensitive to domain variation, approaches that can identify VCs, SCFs and SPs in corpora have become increasingly popular, e.g. (O’Donovan et al., 2005; Schulte im Walde, 2006; Erk, 2007; Preiss et al., 2007; Van de Cruys, 2009; Reisinger and Mooney, 2011; Sun and Korhonen, 2011; Lippincott et al., 2012). The task of SCF induction involves identifying the arguments of a verb lemma and generalizing about the frames (i.e. SCFs) taken by the verb, where each frame includes a number of arguments and their syntactic types. For example, in (1), the verb ”show” takes the frame SUBJ-DOBJCCOMP (subject, direct object, and clausal complement). (1) [A number of SCF acquisition papers]SUBJ [show]VERB [their readers]DOBJ [which features are most valuable for the acquisition pro</context>
<context position="8785" citStr="Walde, 2006" startWordPosition="1357" endWordPosition="1358">tering A variety of VC approaches have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were induced separately and then feeded as features to the clustering algorithm. Our framework combines together SCF-motivated and SP-motivated kernel matrices , and uses the joint kernel to induce verb clusters which are likely to be highly relevant for both tasks. Importantly, no manual or automatic system for SCF or SP acquisition has been utilized when constructing the kernel matric</context>
</contexts>
<marker>Walde, 2006</marker>
<rawString>Sabine Schulte im Walde. 2006. Experiments on the automatic induction of german semantic verb classes. Computational Linguistics, 32(2):159–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Shi</author>
<author>Rada Mihalcea</author>
</authors>
<title>Putting pieces together: Combining framenet, verbnet and wordnet for robust semantic parsing.</title>
<date>2005</date>
<booktitle>In CICLING-05.</booktitle>
<contexts>
<context position="1802" citStr="Shi and Mihalcea, 2005" startWordPosition="263" endWordPosition="267">tion of verbal predicate-argument structure, SPs capture the semantic preferences verbs have for their arguments and VCs in the Levin (1993) tradition provide a shared level of abstraction for verbs that share many aspects of their syntactic and semantic behavior. These three of types of information have proved useful for Natural Language Processing (NLP) 1The source code of the clustering algorithms and evaluation is submitted with this paper and will be made publicly available upon acceptance of the paper. tasks which require information about predicateargument structure, including parsing (Shi and Mihalcea, 2005; Cholakov and van Noord, 2010; Zhou et al., 2011), semantic role labeling (Swier and Stevenson, 2004; Dang, 2004; Bharati et al., 2005; Moschitti and Basili, 2005; zap, 2008; Zapirain et al., 2009), and word sense disambiguation (Dang, 2004; Thater et al., 2010; O´ S´eaghdha and Korhonen, 2011), among many others. Because lexical information is highly sensitive to domain variation, approaches that can identify VCs, SCFs and SPs in corpora have become increasingly popular, e.g. (O’Donovan et al., 2005; Schulte im Walde, 2006; Erk, 2007; Preiss et al., 2007; Van de Cruys, 2009; Reisinger and Mo</context>
</contexts>
<marker>Shi, Mihalcea, 2005</marker>
<rawString>Lei Shi and Rada Mihalcea. 2005. Putting pieces together: Combining framenet, verbnet and wordnet for robust semantic parsing. In CICLING-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Sun</author>
<author>Anna Korhonen</author>
</authors>
<title>Improving verb clustering with automatically acquired selectional preferences.</title>
<date>2009</date>
<booktitle>In EMNLP-09,</booktitle>
<contexts>
<context position="5715" citStr="Sun and Korhonen (2009)" startWordPosition="883" endWordPosition="886">duce a novel clustering algorithm based on iterative DPP sampling which can (contrary to other probabilistic frameworks such as Markov random fields) be performed both accurately and efficiently. When defined over the joint SCF and SP kernel, this new algorithm can be used to induce VCs that are valuable for both tasks. We also contribute by evaluating the value of the clusters induced by our model for the acquisition of the three information types. Our evaluation against a well-known VC gold standard shows that our clustering model outperforms the state-of-theart verb clustering algorithm of Sun and Korhonen (2009), in our setup where no manually created SCF or SP data is available. Our evaluation against a well-known SCF gold standard and in the context of SP disambiguation tasks shows results that are superior to strong baselines, demonstrating the benefit our approach. 2 Previous Work SCF acquisition Most current works induce SCFs from the output of an unlexicalized parser (i.e. a parser trained without SCF annotations) using hand-written rules (Briscoe and Carroll, 1997; Korhonen, 2002; Preiss et al., 2007) or grammatical relation (GR) co-occurrence statistics (O’Donovan et al., 2005; Chesley and Sa</context>
<context position="8890" citStr="Sun and Korhonen, 2009" startWordPosition="1375" endWordPosition="1378">c, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were induced separately and then feeded as features to the clustering algorithm. Our framework combines together SCF-motivated and SP-motivated kernel matrices , and uses the joint kernel to induce verb clusters which are likely to be highly relevant for both tasks. Importantly, no manual or automatic system for SCF or SP acquisition has been utilized when constructing the kernel matrices, we only consider features extracted from the output of an unlexicalized parser. Our approach hence pr</context>
<context position="22226" citStr="Sun and Korhonen, 2009" startWordPosition="3686" endWordPosition="3689">SET 3 (12,K) SET4 (10,K) 866 |C |= 20, 21.6 |C |= 40, 41 |C |= 60, 58.6 |C |= 69, 77.6 |C |= 89, 97.4 Model R P F R P F R P F R P F R P F DPP-cluster 93.1 17.3 29.3 77.9 25.4 38.3 63 31.9 42.3 43.8 33.6 38.1 34.4 40.6 37.2 AC 67 17.8 28.2 46.6 24 31.7 40.5 29.4 34 33 34.9 33.9 24.7 41.1 30.9 SC 32.1 27.5 29.6 26.6 35.9 30.6 23.7 41.5 30.2 22.8 43.6 29.9 21.6 48.7 29.9 Table 1: Verb clustering evaluation for the last five iterations of our DPP-cluster model and the baseline agglomerative clustering algorithm (AC, see text for its description), and for the spectral clustering (SC) algorithm of (Sun and Korhonen, 2009) with the same number of clusters induced by DPP-cluster. ICI is the number of clusters for DPP-cluster and SC (first number) and for AC (second number). The F-score performance of DPP-cluster is superior in 4 out of 5 cases. Arg. per verb P (DPP) P(AC) P (B) P (NF) R (DPP) R (AC) R (B) R(NF) ERR DPP ERR AC ERR B ≤ 200 (133 verbs) 27.3 23.7 27.3 23.1 9.9 7.6 8 11.3 3.4 0.16 1.55 ≤ 600 (205 verbs) 26.5 25 27.3 22.6 14.8 11.5 11.9 16.6 2.3 0.50 1.1 ≤ 1000 (238 verbs) 24.6 23.6 25.6 21.1 17.5 13.8 14.7 19.8 1.6 0.42 0.95 Table 2: Performance of the Corpus Statistics SP baseline (non-filtered, NF)</context>
<context position="24775" citStr="Sun and Korhonen (2009)" startWordPosition="4137" endWordPosition="4140">in our experiments, to be M = 10000, K = 20 and T = 10. Clustering Evaluation We first evaluate the quality of the clusters induced by our algorithm (DPP-cluster) compared to the gold standard VCs (table 1). To evaluate the importance of the DPP component, we compare to the performance of a version of our algorithm where everything is kept fixed except from the sampling which is done from a uniform distribution rather than from the DPP joint kernel (this model is denoted in the table with AC for agglomerative clustering) 4. We also compare to the state-of-the-art spectral clustering method of Sun and Korhonen (2009) where our 4Importantly, the kernel matrix L used in the agglomerative clustering process is also used by AC. kernel matrix is used for the distance between data points (SC) 5. We evaluated the unified cluster set induced in each iteration of our algorithm and of the AC baseline and induced the same number of clusters as in each iteration of our algorithm using the SC baseline. Since the number of clusters in each iteration is not an argument for our algorithm or for the AC baseline, the number of clusters slightly differ between the two. The AC and SC baseline results were averaged over 5 and</context>
<context position="26003" citStr="Sun and Korhonen (2009)" startWordPosition="4344" endWordPosition="4347">runs respectively. DPP-cluster has produced identical output across runs. The table demonstrates the superiority of the DPP-cluster model. For four out of five conditions its F-score performance outperforms the baselines by 4.2-8.3%. Moreover, in all conditions its recall performances are substantially higher than those of the baselines (by 9.7-26.1%). Note that DPPcluster runs for 17 iterations while the AC baseline performs only 6. We therefore evaluated only the last 5 iterations of each model 6. SCF evaluation For this evaluation, we first built a baseline SCF lexicon based on the parsed 5Sun and Korhonen (2009) report better results than those we report for their algorithm (on a different data set). Note, however, that they used the output of a rule-based SCF system as a source of features, as opposed to our unsupervised approach. 6For the additional comparable iteration the result pattern is very similar to the (C = 89, 97.4) case in the table, and is not presented due to space limitations. 867 Algorithm 1 The DPP-cluster clustering algorithm. K is the size of the sampled subsets, M is the number of subsets sampled at each iteration, Y is the verb set, T is the number of most probable samples to be</context>
</contexts>
<marker>Sun, Korhonen, 2009</marker>
<rawString>Lin Sun and Anna Korhonen. 2009. Improving verb clustering with automatically acquired selectional preferences. In EMNLP-09, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Sun</author>
<author>Anna Korhonen</author>
</authors>
<title>Hierarchical verb clustering using graph factorization.</title>
<date>2011</date>
<booktitle>In EMNLP-11.</booktitle>
<contexts>
<context position="2436" citStr="Sun and Korhonen, 2011" startWordPosition="366" endWordPosition="369">nd van Noord, 2010; Zhou et al., 2011), semantic role labeling (Swier and Stevenson, 2004; Dang, 2004; Bharati et al., 2005; Moschitti and Basili, 2005; zap, 2008; Zapirain et al., 2009), and word sense disambiguation (Dang, 2004; Thater et al., 2010; O´ S´eaghdha and Korhonen, 2011), among many others. Because lexical information is highly sensitive to domain variation, approaches that can identify VCs, SCFs and SPs in corpora have become increasingly popular, e.g. (O’Donovan et al., 2005; Schulte im Walde, 2006; Erk, 2007; Preiss et al., 2007; Van de Cruys, 2009; Reisinger and Mooney, 2011; Sun and Korhonen, 2011; Lippincott et al., 2012). The task of SCF induction involves identifying the arguments of a verb lemma and generalizing about the frames (i.e. SCFs) taken by the verb, where each frame includes a number of arguments and their syntactic types. For example, in (1), the verb ”show” takes the frame SUBJ-DOBJCCOMP (subject, direct object, and clausal complement). (1) [A number of SCF acquisition papers]SUBJ [show]VERB [their readers]DOBJ [which features are most valuable for the acquisition process]CCOMP. SP induction involves identifying and classifying the lexical items in a given argument slot</context>
<context position="8937" citStr="Sun and Korhonen, 2011" startWordPosition="1383" endWordPosition="1386">fications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were induced separately and then feeded as features to the clustering algorithm. Our framework combines together SCF-motivated and SP-motivated kernel matrices , and uses the joint kernel to induce verb clusters which are likely to be highly relevant for both tasks. Importantly, no manual or automatic system for SCF or SP acquisition has been utilized when constructing the kernel matrices, we only consider features extracted from the output of an unlexicalized parser. Our approach hence provides a framework for acquiring valuable infor</context>
</contexts>
<marker>Sun, Korhonen, 2011</marker>
<rawString>Lin Sun and Anna Korhonen. 2011. Hierarchical verb clustering using graph factorization. In EMNLP-11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Sun</author>
<author>Anna Korhonen</author>
<author>Yuval Krymolowski</author>
</authors>
<title>Verb class discovery from rich syntactic data.</title>
<date>2008</date>
<journal>Lecture Notes in Computer Science,</journal>
<volume>4919</volume>
<issue>16</issue>
<contexts>
<context position="8824" citStr="Sun et al., 2008" startWordPosition="1363" endWordPosition="1366">have been proposed in the literature. These include syntactic, semantic and mixed syntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were induced separately and then feeded as features to the clustering algorithm. Our framework combines together SCF-motivated and SP-motivated kernel matrices , and uses the joint kernel to induce verb clusters which are likely to be highly relevant for both tasks. Importantly, no manual or automatic system for SCF or SP acquisition has been utilized when constructing the kernel matrices, we only consider features extracted</context>
</contexts>
<marker>Sun, Korhonen, Krymolowski, 2008</marker>
<rawString>Lin Sun, Anna Korhonen, and Yuval Krymolowski. 2008. Verb class discovery from rich syntactic data. Lecture Notes in Computer Science, 4919(16).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Swier</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Unsupervised semantic role labelling.</title>
<date>2004</date>
<booktitle>In EMNLP-04.</booktitle>
<contexts>
<context position="1903" citStr="Swier and Stevenson, 2004" startWordPosition="280" endWordPosition="283">heir arguments and VCs in the Levin (1993) tradition provide a shared level of abstraction for verbs that share many aspects of their syntactic and semantic behavior. These three of types of information have proved useful for Natural Language Processing (NLP) 1The source code of the clustering algorithms and evaluation is submitted with this paper and will be made publicly available upon acceptance of the paper. tasks which require information about predicateargument structure, including parsing (Shi and Mihalcea, 2005; Cholakov and van Noord, 2010; Zhou et al., 2011), semantic role labeling (Swier and Stevenson, 2004; Dang, 2004; Bharati et al., 2005; Moschitti and Basili, 2005; zap, 2008; Zapirain et al., 2009), and word sense disambiguation (Dang, 2004; Thater et al., 2010; O´ S´eaghdha and Korhonen, 2011), among many others. Because lexical information is highly sensitive to domain variation, approaches that can identify VCs, SCFs and SPs in corpora have become increasingly popular, e.g. (O’Donovan et al., 2005; Schulte im Walde, 2006; Erk, 2007; Preiss et al., 2007; Van de Cruys, 2009; Reisinger and Mooney, 2011; Sun and Korhonen, 2011; Lippincott et al., 2012). The task of SCF induction involves iden</context>
</contexts>
<marker>Swier, Stevenson, 2004</marker>
<rawString>Robert Swier and Suzanne Stevenson. 2004. Unsupervised semantic role labelling. In EMNLP-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen Furstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Contextualizing semantic representations using syntactically enriched vector models.</title>
<date>2010</date>
<booktitle>In ACL10,</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="2064" citStr="Thater et al., 2010" startWordPosition="307" endWordPosition="310">These three of types of information have proved useful for Natural Language Processing (NLP) 1The source code of the clustering algorithms and evaluation is submitted with this paper and will be made publicly available upon acceptance of the paper. tasks which require information about predicateargument structure, including parsing (Shi and Mihalcea, 2005; Cholakov and van Noord, 2010; Zhou et al., 2011), semantic role labeling (Swier and Stevenson, 2004; Dang, 2004; Bharati et al., 2005; Moschitti and Basili, 2005; zap, 2008; Zapirain et al., 2009), and word sense disambiguation (Dang, 2004; Thater et al., 2010; O´ S´eaghdha and Korhonen, 2011), among many others. Because lexical information is highly sensitive to domain variation, approaches that can identify VCs, SCFs and SPs in corpora have become increasingly popular, e.g. (O’Donovan et al., 2005; Schulte im Walde, 2006; Erk, 2007; Preiss et al., 2007; Van de Cruys, 2009; Reisinger and Mooney, 2011; Sun and Korhonen, 2011; Lippincott et al., 2012). The task of SCF induction involves identifying the arguments of a verb lemma and generalizing about the frames (i.e. SCFs) taken by the verb, where each frame includes a number of arguments and their </context>
</contexts>
<marker>Thater, Furstenau, Pinkal, 2010</marker>
<rawString>Stefan Thater, Hagen Furstenau, and Manfred Pinkal. 2010. Contextualizing semantic representations using syntactically enriched vector models. In ACL10, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
</authors>
<title>A non-negative tensor factorization model for selectional preference induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the workshop on Geometric Models for Natural Language Semantics (GEMS).</booktitle>
<marker>Van de Cruys, 2009</marker>
<rawString>Tim Van de Cruys. 2009. A non-negative tensor factorization model for selectional preference induction. In Proceedings of the workshop on Geometric Models for Natural Language Semantics (GEMS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Vlachos</author>
<author>Anna Korhonen</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Unsupervised and constrained dirichlet process mixture models for verb clustering.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics.</booktitle>
<contexts>
<context position="8912" citStr="Vlachos et al., 2009" startWordPosition="1379" endWordPosition="1382">ntacticsemantic classifications (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Schuler, 2006; Hovy et al., 2006). We focus on Levin style classes (Levin, 1993) which are defined in terms of diathesis alternations and capture generalizations over a range of syntactic and semantic properties. Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e.g. K-means and spectral) algorithms (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Sun and Korhonen, 2011). The linguistic features included SCFs and SPs, but these were induced separately and then feeded as features to the clustering algorithm. Our framework combines together SCF-motivated and SP-motivated kernel matrices , and uses the joint kernel to induce verb clusters which are likely to be highly relevant for both tasks. Importantly, no manual or automatic system for SCF or SP acquisition has been utilized when constructing the kernel matrices, we only consider features extracted from the output of an unlexicalized parser. Our approach hence provides a framework for</context>
</contexts>
<marker>Vlachos, Korhonen, Ghahramani, 2009</marker>
<rawString>Andreas Vlachos, Anna Korhonen, and Zoubin Ghahramani. 2009. Unsupervised and constrained dirichlet process mixture models for verb clustering. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics.</rawString>
</citation>
<citation valid="true">
<title>Robustness and generalization of role sets: PropBank vs.</title>
<date>2008</date>
<publisher>VerbNet.</publisher>
<contexts>
<context position="3989" citStr="(2008)" startWordPosition="605" endWordPosition="605">mputational Linguistics [show]VERB [no evidence to the usefulness of joint learning leaning for these tasks]DOBJ. Finally, VC induction involves clustering together verbs with similar meaning, reflected in similar SCFs and SPs. For example, ”show” in the above examples could get clustered together with ”demonstrate” and ”indicate”. Because these challenging tasks capture complementary information about predicate argument structure, they should be able to inform and support each other. Recently, researchers have begun to investigate the benefits of their joint learning. Schulte im Walde et al. (2008) integrated SCF and VC acquisition and used it for WordNet-based SP classification. O´ S´eaghdha (2010) presented a “dual-topic” model for SPs that induces also verb clusters. Both works reported SP evaluation with promising results. Lippincott et al. (2012) presented a joint model for inducing simple syntactic frames and VCs. They reported high accuracy results on VCs. de Cruys et al. (2012) introduced a joint model for SCF and SP acquisition. They evaluated both the SCFs and SPs, obtaining reasonable result on both tasks. In this paper, we present the first unified framework for unsupervised</context>
<context position="10295" citStr="(2008)" startWordPosition="1609" endWordPosition="1609">em has addressed only a subset of the tasks and all but one have evaluated the performance in the context of one task only. O´ S´eaghdha (2010) presented a “dual-topic” model for SPs that induces VCs, reporting evaluation of SPs only. Lippincott et al. (2012) presented a Bayesian network model for syntactic frame (rather than full SCF) induction that induces VCs. Only VCs are evaluated. de Cruys et al. (2012) presented a joint unsupervised model of SCF and SP acquisition based on non-negative tensor factorization. Both SCFs and SPs were evaluated. Finally, the model of Schulte im Walde et al. (2008) addresses the three types of information but SP parameters are estimated with a WordNet based method and only the SPs are evaluated. Although evaluation of these recent joint models has been partial, the results have been encouraging and fur3 The Unified Framework In this section we present our unified framework. Our idea is to utilize DPPs for verb clustering that informs both SCF and SP acquisition. DPPs define a probability distribution over the possible subsets of a given set. These models assign higher probability mass to subsets that are both high quality and diverse. Our novel clusteri</context>
</contexts>
<marker>2008</marker>
<rawString>2008. Robustness and generalization of role sets: PropBank vs. VerbNet.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benat Zapirain</author>
</authors>
<title>Eneko Agirre, and Lluis Marquex.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP-09,</booktitle>
<marker>Zapirain, 2009</marker>
<rawString>Benat Zapirain, Eneko Agirre, and Lluis Marquex. 2009. Generalizing over lexical features: Selectional preferences for semantic role classification. In ACL-IJCNLP-09, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guangyou Zhou</author>
<author>Jun Zhao</author>
<author>Kang Liu</author>
<author>Li Cai</author>
</authors>
<title>Exploiting web-derived selectional preference to improve statistical dependency parsing.</title>
<date>2011</date>
<booktitle>In ACL-11,</booktitle>
<location>Portland, OR.</location>
<contexts>
<context position="1852" citStr="Zhou et al., 2011" startWordPosition="273" endWordPosition="276">re the semantic preferences verbs have for their arguments and VCs in the Levin (1993) tradition provide a shared level of abstraction for verbs that share many aspects of their syntactic and semantic behavior. These three of types of information have proved useful for Natural Language Processing (NLP) 1The source code of the clustering algorithms and evaluation is submitted with this paper and will be made publicly available upon acceptance of the paper. tasks which require information about predicateargument structure, including parsing (Shi and Mihalcea, 2005; Cholakov and van Noord, 2010; Zhou et al., 2011), semantic role labeling (Swier and Stevenson, 2004; Dang, 2004; Bharati et al., 2005; Moschitti and Basili, 2005; zap, 2008; Zapirain et al., 2009), and word sense disambiguation (Dang, 2004; Thater et al., 2010; O´ S´eaghdha and Korhonen, 2011), among many others. Because lexical information is highly sensitive to domain variation, approaches that can identify VCs, SCFs and SPs in corpora have become increasingly popular, e.g. (O’Donovan et al., 2005; Schulte im Walde, 2006; Erk, 2007; Preiss et al., 2007; Van de Cruys, 2009; Reisinger and Mooney, 2011; Sun and Korhonen, 2011; Lippincott et </context>
</contexts>
<marker>Zhou, Zhao, Liu, Cai, 2011</marker>
<rawString>Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011. Exploiting web-derived selectional preference to improve statistical dependency parsing. In ACL-11, Portland, OR.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>