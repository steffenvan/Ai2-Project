<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017026">
<title confidence="0.9425115">
Book Reviews
Towards a Theory of Cognition and Computing
</title>
<author confidence="0.990902">
J. Gerard Wolff
</author>
<affiliation confidence="0.708613">
(University of Wales, Bangor)
Chichester, U.K.: Ellis Horwood
</affiliation>
<figure confidence="0.5325905">
Limited (Prentice-Hall), 1991, 194 pp.
Hardbound, ISBN 0-13-925025-5, $52.00
Reviewed by
Philip Swann
</figure>
<affiliation confidence="0.63197">
University of Geneva
</affiliation>
<bodyText confidence="0.99988534375">
This expensive volume comprises a selection of the author&apos;s published papers (Wolff
1982, 1988, 1989, 1990; Wolff and Chipperfield 1990). There is some new material,
in the form of a general introduction, a paragraph or so of preface to each paper,
and a brief (11-page) additional chapter. None of this supplementary material adds
anything of significance to the original papers. Four earlier papers (Wolff 1975, 1976,
1977, 1980) are not included, although the work reported in them is repeatedly referred
to. The addition of one or more of these earlier papers would have helped the reader
considerably. The papers in the book have been reset, but otherwise reprinted verbatim.
As the reader also has to struggle with the author&apos;s frequent changes of direction—
and with his casual, at times slapdash, expository style—the resulting collage does not
make for easy or enjoyable reading. Among computational linguists, probably only
those with a special interest in grammar induction will want to know about Wolff&apos;s
work, and they can limit their attention to Wolff 1982 and 1988.
Wolff started his career as a psychologist with an interest in the computational
modeling of first-language acquisition. He then spent four years with a commercial
software house before becoming a lecturer in computer systems engineering at the
University of Wales. As the title of this book suggests, his goal is to integrate this
varied work experience into a general theory that could be considered a contribution
to cognitive science. The actual content of the book, however, does not live up to
the ambitious title. The first half (Wolff 1982, 1988) reports on the development of a
grammar inference program (SNPR) based on statistics, which went through numerous
versions over a period of some ten years. The program is informally analyzed in terms
of data compression and offered, speculatively, as a model of first-language acquisition
and concept formation. In the second half of the book, Wolff generalizes these ideas
and methods in the form of a computational notation (SP) analogous to a simplified
Prolog. It is this notation that is claimed to be a new language for a unified &amp;quot;theory
of cognition and computing&amp;quot; (as well as a potential solution to all the problems of
software engineering): but neither real evidence nor a convincing argument is offered
in support. The rest of this review will therefore focus on the first half of the book,
which contains all that is likely to be of interest to computational linguists.
The SNPR program was built on top of an earlier algorithm (Wolff 1975). Taking an
input text without spacing or punctuation, that program (MK10) builds up a grammar
</bodyText>
<subsectionHeader confidence="0.718949">
Computational Linguistics Volume 18, Number 3
</subsectionHeader>
<bodyText confidence="0.872871">
as a list of hierarchically bracketed sub-strings. It uses the following algorithm:
</bodyText>
<listItem confidence="0.998678125">
• Initialize the grammar to the alphabet used by the text.
• Repeat:
1. Parse the whole text from left to right, at each point matching
against the largest possible grammar element;
2. calculate the frequency of co-occurrence for all pairs of elements
in the parsed text;
3. add the most frequent pair to the grammar as a new element
(choose at random between ties).
</listItem>
<bodyText confidence="0.999804166666667">
For example, starting with the text abcabc and the grammar initialized to {a, b, c} , the
first iteration would parse the text as abcabc and then add the new element (ab) to the
grammar. The second iteration would parse the text as (ab)c(ab)c and add the element
((ab)(c)) to the grammar. The third iteration would create a full parse and add it to the
grammar as ((ab)(c))((ab)(c)). MK10 was remarkably successful at its assigned task of
segmenting text into words, both on artificial texts and natural language (Wolff 1977).
If the program is allowed to continue iterating over the segmented text, it will build
higher-level constituent structures; but, as might be expected, these do not correspond
to a realistic grammatical parse. Interestingly, the program did work quite well on a
text transcribed as a sequence of word classes.
The SNPR program (Wolff 1982) adds three new operations to MK10 to be per-
formed on each iteration:
</bodyText>
<listItem confidence="0.998058">
1. &amp;quot;Folding&amp;quot; builds disjunctive rules to capture alternation of elements in a
fixed context. For example, if the grammar contains apb, aqb, and arb,
then a new element is built, (X 191(41 r), and the three original
elements are merged to aXb.
2. &amp;quot;Generalization&amp;quot; finds other occurrences of the elements of the newly
created disjunction and replaces them with the nonterminal symbol. So,
if xyp is in the grammar, it will be replaced by xyX.
3. &amp;quot;Rebuilding&amp;quot; is used to remove unwanted generalizations created in the
previous step. To do this, all instances of a new disjunction in each of its
contexts are checked. If the disjunction fails to use all of the constituents
in a context, then it is rebuilt for that context without the unused
constituent(s). Continuing the example: If cdX always rewrote only as cdp
or cdq, then X would be rebuilt as (Y —+ p I q), and all occurrences in the
grammar of cdX would be replaced by cdY.
</listItem>
<bodyText confidence="0.9991785">
Wolff does not give a formal statement or analysis of the full SNPR algorithm,
and his description of its implementation is impressionistic and very hard to follow.
This is probably not a serious omission, since any implementation of such a complex
set of operations is going to be very costly indeed. The program was tested on several
</bodyText>
<page confidence="0.992202">
354
</page>
<figure confidence="0.853844555555555">
Book Reviews
miniature finite state grammars. It was reasonably successful with only this one:
S (1)(2)(3) I (4)(5)(6)
1 —&gt; david I john
2 loves I hated
3 —&gt; mary I susan
4 —&gt; we I you
5 -4 walk I run
6 fast I slowly
</figure>
<bodyText confidence="0.999377888888889">
The sample text for the test was a 2,500-character string, built presumably (no details
are given) by stratified random selection of the rules. As might be expected, SNPR
does not work on natural language texts.
Since the point of constructing SNPR was to model child language acquisition,
Wolff devotes a great deal of space to claiming that this goal was at least partially
achieved. To this end, he develops an account of grammar induction as data compres-
sion. The &amp;quot;efficiency&amp;quot; of a grammar is defined as its compression capacity divided by
its size, where &amp;quot;compression capacity&amp;quot; is the ratio of encoded text to its raw form. He
can then claim that:
</bodyText>
<listItem confidence="0.998105833333333">
• efficient data compression is a natural property of biological systems,
including children learning their mother tongue;
• children should at all times try to maximize the efficiency of the
grammar they are developing;
• SNPR does just that and, therefore, is a valid model of first-language
acquisition.
</listItem>
<bodyText confidence="0.997766173913043">
While the last claim would have been easy to test empirically, this was never actually
done. It would have been more difficult, but presumably possible, to prove it formally,
but this was not done either. As a result, the 35 (very redundant) pages devoted to
the topic are largely a waste of the reader&apos;s time.
Wolff 1988 also makes a valiant but largely unconvincing attempt to relate SNPR
to the literature on child language, with the aim of justifying his belief that the al-
gorithm captures some of the essentials of real language learning. As he notes, his
approach is rooted in the pre-Chomsky era of distributional analysis and association-
ist psychology. The learner (SNPR) is a formal device abstracting patterns according
to the frequency distribution of symbols, without the support of semantics, negative
evidence, or correction. In the case of word segmentation, the approach works because
of the well-known statistical properties of text: words get repeated far more often than
do strings of words. The induction of syntactic rules is a much tougher problem, and
it is not at all clear that SNPR actually tackles it, since the target text and grammar
seem to be closely matched to the learning mechanism.
In conclusion, then, Wolff&apos;s early technical work represented a useful contribution
to research on grammar induction. It is a pity that he was unable to provide a proper
formulation and analysis of SNPR; a good example of what can be achieved in this
area is offered by the work of Angluin (1982) and Berwick and Pilato (1987) on the
induction of automata. A similar lack of rigor occurs in Wolff&apos;s more general theoriz-
ing. He brings together interesting ideas from psychology, statistics, and computing
in a process of synthesis by loose analogy. In the later part of the book the result is
almost comic: &amp;quot;In SP, the boundary between &apos;knowledge engineering&apos; and other kinds
</bodyText>
<page confidence="0.995576">
355
</page>
<note confidence="0.636243">
Computational Linguistics Volume 18, Number 3
</note>
<bodyText confidence="0.999764833333333">
of information engineering breaks down. In SP there is the potential for full integration
of artificial intelligence, software engineering, and other aspects of computing—with
Shannon&apos;s information theory as the unifying framework. SP also offers a bridge be-
tween &apos;connectionist&apos; and &apos;symbolic&apos; views of computing&amp;quot; (page 101). One is left with
the feeling that the drift of the book is away from, rather than toward, a general theory
of cognition.
</bodyText>
<sectionHeader confidence="0.829705" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.9889895">
Angluin, Dana (1982). &amp;quot;Inference of
reversible languages.&amp;quot; Journal of the
Association for Computing Machinery, 29(3),
741-765.
Berwick, Robert C., and Pilato, Sam (1987).
&amp;quot;Learning syntax by automata induction.&amp;quot;
Machine Learning, 2,9-38.
Wolff, J. Gerard (1975). &amp;quot;An algorithm for
the segmentation of an artificial language
analogue.&amp;quot; British Journal of Psychology, 66,
79-90.
Wolff, J. Gerard (1976). &amp;quot;Frequency, con-
ceptual structure and pattern recogni-
tion.&amp;quot; British Journal of Psychology, 67,
377-390.
Wolff, J. Gerard (1977). &amp;quot;The discovery of
segments in natural language.&amp;quot; British
Journal of Psychology, 68, 97-106.
Wolff, J. Gerard (1980). &amp;quot;Language
acquisition and the discovery of phrase
structure.&amp;quot; Language and Speech, 23,
255-269.
Wolff, J. Gerard (1982). &amp;quot;Language
acquisition, data compression and
generalization.&amp;quot; Language and
Communication, 2(1), 57-89.
Wolff, J. Gerard (1988). &amp;quot;Learning syntax
and meanings through optimization and
distributional analysis.&amp;quot; In Categories and
processes in language acquisition, edited by
Y. Levy, I. M. Schlesinger, and
M. D. S. Braine, 179-215. Lawrence
Erlbaum.
Wolff, J. Gerard (1989). &amp;quot;The management of
risk in system development: &apos;Project SP&apos;
and the &apos;new spiral model.&apos; &amp;quot; Software
Engineering Journal, 4(3), 134-142.
Wolff, J. Gerard (1990). &amp;quot;Simplicity and
power: Some unifying ideas in compu-
ting.&amp;quot; Computer Journal, 33(6), 518-534.
Wolff, J. Gerard, and Chipperfield, Andrew
J. (1990). &amp;quot;Unifying computing: Inductive
learning and logic.&amp;quot; In Research and
development in expert systems VII, edited by
T. R. Addis and R. M. Muir, 263-276.
Cambridge University Press.
Philip Swann is a Research Fellow in the Department of Psychology and Education at the Uni-
versity of Geneva. He holds a Ph.D. in Computer-Assisted Learning from the Open University.
His main research interest is in the psycholinguistics of language learning. Swann&apos;s address is:
FPSE, University of Geneva, 1211 Geneva 4, Switzerland. e-mail: swann@divsun.unige.ch
</reference>
<page confidence="0.999018">
356
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.681495">
<title confidence="0.999439">Book Reviews Towards a Theory of Cognition and Computing</title>
<author confidence="0.999706">J Gerard Wolff</author>
<affiliation confidence="0.974068">(University of Wales, Bangor)</affiliation>
<address confidence="0.864964">Chichester, U.K.: Ellis Horwood</address>
<note confidence="0.986868666666667">Limited (Prentice-Hall), 1991, 194 pp. Hardbound, ISBN 0-13-925025-5, $52.00 Reviewed by</note>
<author confidence="0.999869">Philip Swann</author>
<affiliation confidence="0.997944">University of Geneva</affiliation>
<note confidence="0.868486">This expensive volume comprises a selection of the author&apos;s published papers (Wolff 1982, 1988, 1989, 1990; Wolff and Chipperfield 1990). There is some new material,</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dana Angluin</author>
</authors>
<title>Inference of reversible languages.&amp;quot;</title>
<date>1982</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<volume>29</volume>
<issue>3</issue>
<pages>741--765</pages>
<contexts>
<context position="8334" citStr="Angluin (1982)" startWordPosition="1392" endWordPosition="1393">e of the well-known statistical properties of text: words get repeated far more often than do strings of words. The induction of syntactic rules is a much tougher problem, and it is not at all clear that SNPR actually tackles it, since the target text and grammar seem to be closely matched to the learning mechanism. In conclusion, then, Wolff&apos;s early technical work represented a useful contribution to research on grammar induction. It is a pity that he was unable to provide a proper formulation and analysis of SNPR; a good example of what can be achieved in this area is offered by the work of Angluin (1982) and Berwick and Pilato (1987) on the induction of automata. A similar lack of rigor occurs in Wolff&apos;s more general theorizing. He brings together interesting ideas from psychology, statistics, and computing in a process of synthesis by loose analogy. In the later part of the book the result is almost comic: &amp;quot;In SP, the boundary between &apos;knowledge engineering&apos; and other kinds 355 Computational Linguistics Volume 18, Number 3 of information engineering breaks down. In SP there is the potential for full integration of artificial intelligence, software engineering, and other aspects of computing—</context>
</contexts>
<marker>Angluin, 1982</marker>
<rawString>Angluin, Dana (1982). &amp;quot;Inference of reversible languages.&amp;quot; Journal of the Association for Computing Machinery, 29(3), 741-765.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Berwick</author>
<author>Sam Pilato</author>
</authors>
<title>Learning syntax by automata induction.&amp;quot;</title>
<date>1987</date>
<booktitle>Machine Learning,</booktitle>
<pages>2--9</pages>
<contexts>
<context position="8364" citStr="Berwick and Pilato (1987)" startWordPosition="1395" endWordPosition="1398"> statistical properties of text: words get repeated far more often than do strings of words. The induction of syntactic rules is a much tougher problem, and it is not at all clear that SNPR actually tackles it, since the target text and grammar seem to be closely matched to the learning mechanism. In conclusion, then, Wolff&apos;s early technical work represented a useful contribution to research on grammar induction. It is a pity that he was unable to provide a proper formulation and analysis of SNPR; a good example of what can be achieved in this area is offered by the work of Angluin (1982) and Berwick and Pilato (1987) on the induction of automata. A similar lack of rigor occurs in Wolff&apos;s more general theorizing. He brings together interesting ideas from psychology, statistics, and computing in a process of synthesis by loose analogy. In the later part of the book the result is almost comic: &amp;quot;In SP, the boundary between &apos;knowledge engineering&apos; and other kinds 355 Computational Linguistics Volume 18, Number 3 of information engineering breaks down. In SP there is the potential for full integration of artificial intelligence, software engineering, and other aspects of computing—with Shannon&apos;s information the</context>
</contexts>
<marker>Berwick, Pilato, 1987</marker>
<rawString>Berwick, Robert C., and Pilato, Sam (1987). &amp;quot;Learning syntax by automata induction.&amp;quot; Machine Learning, 2,9-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gerard Wolff</author>
</authors>
<title>An algorithm for the segmentation of an artificial language analogue.&amp;quot;</title>
<date>1975</date>
<journal>British Journal of Psychology,</journal>
<volume>66</volume>
<pages>79--90</pages>
<contexts>
<context position="670" citStr="Wolff 1975" startWordPosition="99" endWordPosition="100">Gerard Wolff (University of Wales, Bangor) Chichester, U.K.: Ellis Horwood Limited (Prentice-Hall), 1991, 194 pp. Hardbound, ISBN 0-13-925025-5, $52.00 Reviewed by Philip Swann University of Geneva This expensive volume comprises a selection of the author&apos;s published papers (Wolff 1982, 1988, 1989, 1990; Wolff and Chipperfield 1990). There is some new material, in the form of a general introduction, a paragraph or so of preface to each paper, and a brief (11-page) additional chapter. None of this supplementary material adds anything of significance to the original papers. Four earlier papers (Wolff 1975, 1976, 1977, 1980) are not included, although the work reported in them is repeatedly referred to. The addition of one or more of these earlier papers would have helped the reader considerably. The papers in the book have been reset, but otherwise reprinted verbatim. As the reader also has to struggle with the author&apos;s frequent changes of direction— and with his casual, at times slapdash, expository style—the resulting collage does not make for easy or enjoyable reading. Among computational linguists, probably only those with a special interest in grammar induction will want to know about Wol</context>
<context position="2847" citStr="Wolff 1975" startWordPosition="456" endWordPosition="457">k, Wolff generalizes these ideas and methods in the form of a computational notation (SP) analogous to a simplified Prolog. It is this notation that is claimed to be a new language for a unified &amp;quot;theory of cognition and computing&amp;quot; (as well as a potential solution to all the problems of software engineering): but neither real evidence nor a convincing argument is offered in support. The rest of this review will therefore focus on the first half of the book, which contains all that is likely to be of interest to computational linguists. The SNPR program was built on top of an earlier algorithm (Wolff 1975). Taking an input text without spacing or punctuation, that program (MK10) builds up a grammar Computational Linguistics Volume 18, Number 3 as a list of hierarchically bracketed sub-strings. It uses the following algorithm: • Initialize the grammar to the alphabet used by the text. • Repeat: 1. Parse the whole text from left to right, at each point matching against the largest possible grammar element; 2. calculate the frequency of co-occurrence for all pairs of elements in the parsed text; 3. add the most frequent pair to the grammar as a new element (choose at random between ties). For exam</context>
</contexts>
<marker>Wolff, 1975</marker>
<rawString>Wolff, J. Gerard (1975). &amp;quot;An algorithm for the segmentation of an artificial language analogue.&amp;quot; British Journal of Psychology, 66, 79-90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gerard Wolff</author>
</authors>
<title>Frequency, conceptual structure and pattern recognition.&amp;quot;</title>
<date>1976</date>
<journal>British Journal of Psychology,</journal>
<volume>67</volume>
<pages>377--390</pages>
<marker>Wolff, 1976</marker>
<rawString>Wolff, J. Gerard (1976). &amp;quot;Frequency, conceptual structure and pattern recognition.&amp;quot; British Journal of Psychology, 67, 377-390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gerard Wolff</author>
</authors>
<title>The discovery of segments in natural language.&amp;quot;</title>
<date>1977</date>
<journal>British Journal of Psychology,</journal>
<volume>68</volume>
<pages>97--106</pages>
<contexts>
<context position="3964" citStr="Wolff 1977" startWordPosition="644" endWordPosition="645">e most frequent pair to the grammar as a new element (choose at random between ties). For example, starting with the text abcabc and the grammar initialized to {a, b, c} , the first iteration would parse the text as abcabc and then add the new element (ab) to the grammar. The second iteration would parse the text as (ab)c(ab)c and add the element ((ab)(c)) to the grammar. The third iteration would create a full parse and add it to the grammar as ((ab)(c))((ab)(c)). MK10 was remarkably successful at its assigned task of segmenting text into words, both on artificial texts and natural language (Wolff 1977). If the program is allowed to continue iterating over the segmented text, it will build higher-level constituent structures; but, as might be expected, these do not correspond to a realistic grammatical parse. Interestingly, the program did work quite well on a text transcribed as a sequence of word classes. The SNPR program (Wolff 1982) adds three new operations to MK10 to be performed on each iteration: 1. &amp;quot;Folding&amp;quot; builds disjunctive rules to capture alternation of elements in a fixed context. For example, if the grammar contains apb, aqb, and arb, then a new element is built, (X 191(41 r)</context>
</contexts>
<marker>Wolff, 1977</marker>
<rawString>Wolff, J. Gerard (1977). &amp;quot;The discovery of segments in natural language.&amp;quot; British Journal of Psychology, 68, 97-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gerard Wolff</author>
</authors>
<title>Language acquisition and the discovery of phrase structure.&amp;quot;</title>
<date>1980</date>
<journal>Language and Speech,</journal>
<volume>23</volume>
<pages>255--269</pages>
<marker>Wolff, 1980</marker>
<rawString>Wolff, J. Gerard (1980). &amp;quot;Language acquisition and the discovery of phrase structure.&amp;quot; Language and Speech, 23, 255-269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gerard Wolff</author>
</authors>
<title>Language acquisition, data compression and generalization.&amp;quot;</title>
<date>1982</date>
<journal>Language and Communication,</journal>
<volume>2</volume>
<issue>1</issue>
<pages>57--89</pages>
<contexts>
<context position="1329" citStr="Wolff 1982" startWordPosition="205" endWordPosition="206">e work reported in them is repeatedly referred to. The addition of one or more of these earlier papers would have helped the reader considerably. The papers in the book have been reset, but otherwise reprinted verbatim. As the reader also has to struggle with the author&apos;s frequent changes of direction— and with his casual, at times slapdash, expository style—the resulting collage does not make for easy or enjoyable reading. Among computational linguists, probably only those with a special interest in grammar induction will want to know about Wolff&apos;s work, and they can limit their attention to Wolff 1982 and 1988. Wolff started his career as a psychologist with an interest in the computational modeling of first-language acquisition. He then spent four years with a commercial software house before becoming a lecturer in computer systems engineering at the University of Wales. As the title of this book suggests, his goal is to integrate this varied work experience into a general theory that could be considered a contribution to cognitive science. The actual content of the book, however, does not live up to the ambitious title. The first half (Wolff 1982, 1988) reports on the development of a gr</context>
<context position="4304" citStr="Wolff 1982" startWordPosition="698" endWordPosition="699"> element ((ab)(c)) to the grammar. The third iteration would create a full parse and add it to the grammar as ((ab)(c))((ab)(c)). MK10 was remarkably successful at its assigned task of segmenting text into words, both on artificial texts and natural language (Wolff 1977). If the program is allowed to continue iterating over the segmented text, it will build higher-level constituent structures; but, as might be expected, these do not correspond to a realistic grammatical parse. Interestingly, the program did work quite well on a text transcribed as a sequence of word classes. The SNPR program (Wolff 1982) adds three new operations to MK10 to be performed on each iteration: 1. &amp;quot;Folding&amp;quot; builds disjunctive rules to capture alternation of elements in a fixed context. For example, if the grammar contains apb, aqb, and arb, then a new element is built, (X 191(41 r), and the three original elements are merged to aXb. 2. &amp;quot;Generalization&amp;quot; finds other occurrences of the elements of the newly created disjunction and replaces them with the nonterminal symbol. So, if xyp is in the grammar, it will be replaced by xyX. 3. &amp;quot;Rebuilding&amp;quot; is used to remove unwanted generalizations created in the previous step. </context>
</contexts>
<marker>Wolff, 1982</marker>
<rawString>Wolff, J. Gerard (1982). &amp;quot;Language acquisition, data compression and generalization.&amp;quot; Language and Communication, 2(1), 57-89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gerard Wolff</author>
</authors>
<title>Learning syntax and meanings through optimization and distributional analysis.&amp;quot; In Categories and processes in language acquisition, edited by</title>
<date>1988</date>
<pages>179--215</pages>
<institution>Lawrence Erlbaum.</institution>
<contexts>
<context position="7142" citStr="Wolff 1988" startWordPosition="1195" endWordPosition="1196">ompression is a natural property of biological systems, including children learning their mother tongue; • children should at all times try to maximize the efficiency of the grammar they are developing; • SNPR does just that and, therefore, is a valid model of first-language acquisition. While the last claim would have been easy to test empirically, this was never actually done. It would have been more difficult, but presumably possible, to prove it formally, but this was not done either. As a result, the 35 (very redundant) pages devoted to the topic are largely a waste of the reader&apos;s time. Wolff 1988 also makes a valiant but largely unconvincing attempt to relate SNPR to the literature on child language, with the aim of justifying his belief that the algorithm captures some of the essentials of real language learning. As he notes, his approach is rooted in the pre-Chomsky era of distributional analysis and associationist psychology. The learner (SNPR) is a formal device abstracting patterns according to the frequency distribution of symbols, without the support of semantics, negative evidence, or correction. In the case of word segmentation, the approach works because of the well-known st</context>
</contexts>
<marker>Wolff, 1988</marker>
<rawString>Wolff, J. Gerard (1988). &amp;quot;Learning syntax and meanings through optimization and distributional analysis.&amp;quot; In Categories and processes in language acquisition, edited by Y. Levy, I. M. Schlesinger, and M. D. S. Braine, 179-215. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gerard Wolff</author>
</authors>
<title>The management of risk in system development: &apos;Project SP&apos; and the &apos;new spiral model.&apos; &amp;quot;</title>
<date>1989</date>
<journal>Software Engineering Journal,</journal>
<volume>4</volume>
<issue>3</issue>
<pages>134--142</pages>
<marker>Wolff, 1989</marker>
<rawString>Wolff, J. Gerard (1989). &amp;quot;The management of risk in system development: &apos;Project SP&apos; and the &apos;new spiral model.&apos; &amp;quot; Software Engineering Journal, 4(3), 134-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gerard Wolff</author>
</authors>
<title>Simplicity and power: Some unifying ideas in computing.&amp;quot;</title>
<date>1990</date>
<journal>Computer Journal,</journal>
<volume>33</volume>
<issue>6</issue>
<pages>518--534</pages>
<marker>Wolff, 1990</marker>
<rawString>Wolff, J. Gerard (1990). &amp;quot;Simplicity and power: Some unifying ideas in computing.&amp;quot; Computer Journal, 33(6), 518-534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gerard Wolff</author>
<author>Andrew J Chipperfield</author>
</authors>
<title>Unifying computing: Inductive learning and logic.&amp;quot;</title>
<date>1990</date>
<booktitle>In Research and development in expert systems VII,</booktitle>
<pages>263--276</pages>
<publisher>Cambridge University Press.</publisher>
<note>edited by</note>
<marker>Wolff, Chipperfield, 1990</marker>
<rawString>Wolff, J. Gerard, and Chipperfield, Andrew J. (1990). &amp;quot;Unifying computing: Inductive learning and logic.&amp;quot; In Research and development in expert systems VII, edited by T. R. Addis and R. M. Muir, 263-276. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip</author>
</authors>
<title>Swann is a Research Fellow in the Department of Psychology and Education at the University of Geneva. He holds a Ph.D. in Computer-Assisted Learning from the Open University. His main research interest is in the psycholinguistics of language learning. Swann&apos;s address is: FPSE,</title>
<date></date>
<journal>University of Geneva, 1211 Geneva</journal>
<volume>4</volume>
<note>e-mail: swann@divsun.unige.ch</note>
<marker>Philip, </marker>
<rawString>Philip Swann is a Research Fellow in the Department of Psychology and Education at the University of Geneva. He holds a Ph.D. in Computer-Assisted Learning from the Open University. His main research interest is in the psycholinguistics of language learning. Swann&apos;s address is: FPSE, University of Geneva, 1211 Geneva 4, Switzerland. e-mail: swann@divsun.unige.ch</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>