<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001281">
<title confidence="0.9948325">
PUTOP: Turning Predominant Senses into a Topic Model for Word Sense
Disambiguation
</title>
<author confidence="0.995104">
Jordan Boyd-Graber
</author>
<affiliation confidence="0.9675215">
Computer Science
Princeton University
</affiliation>
<address confidence="0.933617">
Princeton, NJ 08540
</address>
<email confidence="0.998892">
jbg@princeton.edu
</email>
<sectionHeader confidence="0.993898" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998970785714286">
We extend on McCarthy et al.’s predom-
inant sense method to create an unsuper-
vised method of word sense disambiguation
that uses automatically derived topics us-
ing Latent Dirichlet allocation. Using topic-
specific synset similarity measures, we cre-
ate predictions for each word in each doc-
ument using only word frequency informa-
tion. It is hoped that this procedure can im-
prove upon the method for larger numbers
of topics by providing more relevant train-
ing corpora for the individual topics. This
method is evaluated on SemEval-2007 Task
1 and Task 17.
</bodyText>
<sectionHeader confidence="0.997698" genericHeader="method">
1 Generative Model of WSD
</sectionHeader>
<bodyText confidence="0.99750675">
Word Sense Disambiguation (WSD) is the problem
of labeling text with the appropriate semantic labels
automatically. Although WSD is claimed to be an
essential step in information retrieval and machine
translation, it has not seen effective practical appli-
cation because the dearth of labeled data has pre-
vented the use of established supervised statistical
methods that have been successfully applied to other
natural language problems.
Unsupervised methods have been developed for
WSD, but despite modest success have not al-
ways been well understood statistically (Abney,
2004). Unsupervised methods are particularly ap-
pealing because they do not require expensive sense-
annotated data and can use the ever-increasing
amount of raw text freely available. This paper ex-
pands on an effective unsupervised method for WSD
and embeds it into a topic model, thus allowing an
algorithm trained on a single, monolithic corpora to
instead hand-pick relevant documents in choosing
</bodyText>
<author confidence="0.833205">
David Blei
</author>
<affiliation confidence="0.9235945">
Computer Science
Princeton University
</affiliation>
<address confidence="0.804937">
Princeton, NJ 08540
</address>
<email confidence="0.975443">
blei@cs.princeton.edu
</email>
<bodyText confidence="0.993913">
a disambiguation. After developing this generative
statistical model, we present its performance on a
number of tasks.
</bodyText>
<subsectionHeader confidence="0.531067">
1.1 The Intersection of Syntactic and Semantic
Similarity
</subsectionHeader>
<bodyText confidence="0.999881875">
McCarthy et al. (2004) outlined a method for learn-
ing a word’s most-used sense given an untagged cor-
pus that ranks each sense wsi using a distributional
syntactic similarity γ and a WORDNET-derived se-
mantic similarity α. This process for a word w uses
its distributional neighbors Nw, the possible senses
of not only the word in question, Sw, and also those
of the distributionally similar words, Snj. Thus,
</bodyText>
<equation confidence="0.996894">
P(wsi) =
α(a, s). (2)
</equation>
<bodyText confidence="0.9863156">
One can view finding the appropriate sense as a
search in two types of space. In determining how
good a particular synset wsi is, α guides the search
in the semantic space and γ drives the search in the
syntactic space. We consider all of the words used
in syntactically similar contexts, which we call “cor-
roborators,” and for each of them we find the closest
meaning to wsi using a measure of semantic sim-
ilarity α, for instance a WORDNET-based similar-
ity measure such as Jiang-Conrath (1997). Each of
the neighboring words’ contributions is weighted by
the syntactic probability, as provided by Lin’s distri-
butional similarity measure (1998), which rates two
words to be similar if they enter into similar syntac-
tic constructions.
� wnss(wsi, nj) ( )
njENw γ(w, nj) rwsjESw wnss(wsj, nj), 1
where wnss(s, c) =
max
aESc
</bodyText>
<page confidence="0.953798">
277
</page>
<note confidence="0.470718">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 277–281,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<figureCaption confidence="0.587274">
Figure 1: A reinterpretation of McCarthy et al.’s pre-
dominant sense method as a generative model. Note
that this model has no notion of context; a synset is
assigned in an identical manner for all of the words
in a vocabulary.
</figureCaption>
<bodyText confidence="0.999816928571429">
One can think of this process as a generative
model, even though it was not originally posed in
such a manner. For each word w in the vocabulary,
we generate one of the neighbor corroborators ac-
cording to the Lin similarity, -y(c, w), between the
two words. We then generate a synset s for that
word proportional to the maximum semantic sim-
ilarity between s and any synset that contains the
corroborator c (see Figure 1).
Our aim in this paper is to extend the method of
McCarthy et al. using topic models. It is hoped that
allowing the method to in effect “choose” the con-
texts that it uses will improve its ability to disam-
biguate sentences.
</bodyText>
<subsectionHeader confidence="0.9937045">
1.2 Using Topic Models to Partition a
Document’s Words
</subsectionHeader>
<bodyText confidence="0.999982296296296">
Topic models like Latent Dirichlet allocation
(LDA) (Blei et al., 2003) assume a model of text
generation where each document has a multinomial
distribution over topics and each word comes from
one of these topics. In LDA, each topic is a multino-
mial distribution, and each document has a multino-
mial distribution over topics drawn from a Dirichlet
prior that selects the topic for each word in a docu-
ment. Previous work has shown that such a model
improves WSD over using a single corpus (Boyd-
Graber et al., 2007), and we use this insight to de-
velop an extension of McCarthy’s method for multi-
ple topics.
Although describing the statistical background
and motivations behind topic models are beyond the
scope of this paper, it suffices to note that the topics
induced from a corpus provide a statistical group-
ing of words that often occur together and a proba-
bilistic assignment of each word in a corpus to top-
ics. Thus, one topic might have terms like “gov-
ernment,” “president,” “govern,” and “regal,” while
another topic might have terms like “finance,” “high-
yield,” “investor,” and “market.” This paper assumes
that the machinery for learning these distributions
can, given a corpus and a specified number of top-
ics, return the topic distributions most likely to have
generated the corpus.
</bodyText>
<subsectionHeader confidence="0.989896">
1.3 Defining the Model
</subsectionHeader>
<bodyText confidence="0.997461692307692">
While the original predominant senses method used
Lin’s thesaurus similarity method alone in generat-
ing the corroborator, we will also use the probability
of that word being part of the same topic as the word
to be disambiguated. Thus the process of choosing
the “corroborator” is no longer identical for each
word; it is affected by its topic, which changes for
every document. This new generative process can
be thought of as a modified LDA system that, after
selecting the word generated by the topic, continues
on by generating a corroborator and a sense for the
original word:
For each document d E {1 ... D}:
</bodyText>
<listItem confidence="0.97029975">
1. Select a topic distribution Bd — Dir(τ)
2. For each word in the document n E {1 ... N}:
(a) Select a topic zn — Mult(1, Bd)
(b) Select a word from that topic wn — Mult(1, Qz)
(c) Select a ”corroborator” cn also proportional to how
important it is to the topic and its similarity to w
(d) Now, select a synset sn for that word based on a
distribution p(sn|wn, cn, zn)
</listItem>
<bodyText confidence="0.999740428571429">
The conditional dependencies for generating a
synset are shown in Figure 2. Our goal, like Mc-
Carthy et al.’s, is to determine the most likely sense
for each word. This amounts to posterior inference,
which we address by marginalizing over the unob-
served variables (the topics and the corroborators),
where p(wsi) =
</bodyText>
<equation confidence="0.985183333333333">
p(s|w) = I
E E p(s|w, c, z)p(c|z, w)p(z|w, 0).
z c
</equation>
<bodyText confidence="0.924296">
(3)
In order to fully specify this, we must determine the
distribution from which the corroborator is drawn
and the distribution from which the synset is drawn.
Ideally, we would want a distribution that for a
single topic would be identical to McCarthy et al.’s
</bodyText>
<page confidence="0.991182">
278
</page>
<figureCaption confidence="0.589675">
Figure 2: Our generative model assumes that doc-
</figureCaption>
<bodyText confidence="0.982983416666667">
uments are divided into topics and that these topics
generate both the observed word and a “corrobora-
tor,” a term similar in usage to the word. Next, a
sense that minimizes the semantic distance between
the corroborator and the word is generated.
method but would, as more topics are added, favor
corroborators in the same topic as the number of top-
ics increases. In McCarthy et al.’s method, the prob-
ability of the corroborator given a word w is pro-
portional to the Lin similarity γ(w, c) between the
word and the corroborator. Here, the probability of
a corroborator c is
</bodyText>
<equation confidence="0.9835685">
p(c|z, w) ∝ βz,c γ(w, c), (4)
β� c
</equation>
<bodyText confidence="0.999897357142857">
where βz,c is the multinomial probability of word c
in the zth topic, and β° is the multinomial probabil-
ity of the word with a single topic (i.e. background
word probability).
Before, the corroborator was weighted simply
based on its syntactic similarity to the word w, now
we also weight that contribution by how important
(or unimportant) that word is to the topic that w has
been assigned to. This has the effect of increasing
the probability of words pertinent to the topic that
also have high syntactic similarity. Thus, whenever
the syntactic similarity captures polysemous usage,
we hope to be able to separate the different usages.
Note, however, that since for a single topic the β
term cancels out and the procedure is equivalent to
McCarthy et al.
We adapt the semantic similarity in much the
same way to make it topic specific. Because the
Jiang-Conrath similarity measure uses an underly-
ing term frequency to generate a similarity score, we
use the topic term frequency instead of the undivided
term frequency. Thus, the probability of a sense is
proportional to semantic similarity between it and
the closest sense among the senses of a corroborator
with respect to this topic-specific similarity (c.f. the
global similarity in Equation 2). The probability of
selecting a synset s given the corroborator c and a
topic z then becomes
</bodyText>
<equation confidence="0.9925595">
p(s|w, c, z) ∝ max
s&apos;∈S(c)
</equation>
<bodyText confidence="0.999942142857143">
This new dependence on the topic happens be-
cause we recompute the information content used by
Jiang-Conrath with the distribution over words im-
plied by each topic. We then use the similarity im-
plied by that similarity for αz. Following the lead of
McCarthy, for notational ease, this becomes defined
as wnss in Equation 8.
</bodyText>
<subsectionHeader confidence="0.999441">
1.4 Choosing a Synset
</subsectionHeader>
<bodyText confidence="0.9995025">
The problem of choosing a synset then is reduced to
finding the synset with the highest probability under
this model. The model is also designed so that the
task of learning the assignment of topics to words
and documents is not affected by this new machin-
ery for corroborators and senses that we’ve added
onto the model. Thus, we can use the variational in-
ference method described in (Blei et al., 2003) as a
foundation for the problem of synset inference.
Taking p(z|w) as a given (i.e. determined by run-
ning LDA on the corpus), the probability for a synset
s given a word w then becomes
</bodyText>
<equation confidence="0.9811685">
p(s|w, z) = X X p(s|w, c, z)p(c|z)p(z|w), (6)
z c
</equation>
<bodyText confidence="0.981263333333333">
whose terms have been described in the previous
section. With all of the normalization terms, we now
see that p(s|w, z) becomes
</bodyText>
<equation confidence="0.91538">
βz,c
β0 c γ(w, c)
Ps&apos;∈Sw wnss(s0, c, z).
(7)
and wnss(s, c, z) now becomes, for the zth topic,
αz(a, s). (8)
</equation>
<bodyText confidence="0.9995615">
Thus, we’ve now assigned a probability to each of
the possible senses a word can take in a document.
</bodyText>
<equation confidence="0.609964588235294">
θ
Z W C
β
S
K
N
D
αz(s, s0). (5)
X
c
X
z
Pc&apos; βz,c γ(w, c0)
β0
wnss(s, c, z)
max
a∈S(c)
</equation>
<page confidence="0.99294">
279
</page>
<subsectionHeader confidence="0.918586">
1.5 Intuition
</subsectionHeader>
<bodyText confidence="0.999918571428571">
For example, consider the word “fly,” which has two
other words that have high syntactic similarity (in
our formulation, -y) with the terms “fly ball” and “in-
sect.” Both of these words would, given the seman-
tic similarity provided by WORDNET, point to a sin-
gle sense of “fly;” one of them would give a higher
value, however, and thus all senses of the word “fly”
would be assigned that sense. By separately weight-
ing these words by the topic frequencies, we would
hope to choose the sports sense in topics that have
a higher probability of the terms like “foul ball,”
“pop fly,” and “grounder” and the other sense in the
contexts where insect has a higher probability in the
topic.
</bodyText>
<sectionHeader confidence="0.988769" genericHeader="method">
2 Evaluations
</sectionHeader>
<bodyText confidence="0.9999148">
This section describes three experiments to deter-
mine the effectiveness of this unsupervised system.
The first was used to help understand the system,
and the second two were part of the SemEval 2007
competition.
</bodyText>
<subsectionHeader confidence="0.981175">
2.1 SemCor
</subsectionHeader>
<bodyText confidence="0.999979333333333">
As an initial evaluation, we learned LDA topics on
the British National corpus with paragraphs as the
underlying “document” (this allowed for a more uni-
form document length). These documents were then
used to infer topic probabilities for each of the words
in SemCor (Miller et al., 1993), and the model de-
scribed in the previous section was run to determine
the most likely synset. The results of this procedure
are shown in Table 1. Accuracy is determined as the
percentage of words for which the most likely sense
was the one tagged in the corpus.
While the method does roughly recreate Mc-
Carthy et al.’s result for a single topic, it only of-
fers a one percent improvement over McCarthy et
al. on five topics and then falls below McCarthy for
all greater numbers of topics tried. Thus, for all
subsequent experiments we used a five topic model
trained on the BNC.
</bodyText>
<subsectionHeader confidence="0.998431">
2.2 SemEval-2007 Task 1: CLIR
</subsectionHeader>
<bodyText confidence="0.984774666666667">
Using IR metrics, this disambiguation scheme was
evaluated against another competing platform and
an algorithm provided by the Task 1 (Agirre et al.,
</bodyText>
<table confidence="0.9990146">
Topics All Nouns
1 .393 .467
5 .397 .478
25 .387 .456
200 .359 .420
</table>
<tableCaption confidence="0.888998">
Table 1: Accuracy on disambiguating words in Sem-
Cor
</tableCaption>
<table confidence="0.9994025">
Task PUTOP
Topic Expansion 0.30
Document Expansion 0.15
English Translation 0.17
SensEval2 0.39
SensEval3 0.33
</table>
<tableCaption confidence="0.999006">
Table 2: Performance results on Task 1
</tableCaption>
<bodyText confidence="0.9996933">
2007) organizers. Our system had the best results of
any expansion scheme considered (0.30) , although
none of the expansion schemes did better than us-
ing no expansion (0.36). Although our technique
also yielded a better score than the other competing
platform for cross-language queries (0.17), it did not
surpass the first sense-heuristic (0.26), but this is not
surprising given that our algorithm does not assume
the existence of such information. For an overview
of Task 1 results, see Table 2.
</bodyText>
<subsectionHeader confidence="0.992545">
2.3 SemEval-2007 Task 17: All-Words
</subsectionHeader>
<bodyText confidence="0.999885090909091">
Task 17 (Pradhan et al., 2007) asked participants
to submit results as probability distributions over
senses. Because this is also the output of this algo-
rithm, we submitted the probabilities to the contest
before realizing that the distributions are very close
to uniform over all senses and thus yielded a pre-
cision of 0.12, very close to the random baseline.
Placing a point distribution on the argmax with our
original submission to the task, however, (consistent
with our methodology for evaluation on SemCor),
gives a precision of 0.39.
</bodyText>
<sectionHeader confidence="0.999519" genericHeader="conclusions">
3 Conclusion
</sectionHeader>
<bodyText confidence="0.99979075">
While the small improvement over the single topic
suggests that topic techniques might have traction
in determining the best sense, the addition is not ap-
preciable. In a way the failure of the technique is en-
</bodyText>
<page confidence="0.977191">
280
</page>
<bodyText confidence="0.999975">
couraging in that it affirms the original methodology
of McCarthy et al. in finding a single predominant
sense for each word. While the syntactic similarity
measure indeed usually offers high values of similar-
ity for words related to a single sense of a word, the
similarity for words related to other senses, which
we had hoped to strengthen by using topic features,
are on par with words observed because of noise.
Thus, for a word like “bank,” words like
“firm,” “commercial bank,” “company,” and “finan-
cial institution” are the closest in terms of the syn-
tactic similarity, and this allows the financial senses
to be selected without any difficulty. Even if we had
corroborating words for another sense in some topic,
these words are absent from the syntactically simi-
lar words. If we want the meaning similar to that of
“riverbank,” the word with the most similar mean-
ing, “side,” had a syntactic similarity on par with the
unrelated words “individual” and “group.” Thus, in-
terpretations other than the dominant sense as deter-
mined by the baseline method of McCarthy et al. are
hard to find.
Because one topic is equivalent to McCarthy et
al.’s method, this means that we do no worse on
disambiguation. However, contrary to our hope, in-
creasing the number of topics does not lead to sig-
nificantly better sense predictions. This work has not
investigated using a topic-based procedure for deter-
mining the syntactic similarity, but we feel that this
extension could provide real improvement to the un-
supervised techniques that can make use of the co-
pious amounts of available unlabeled data.
</bodyText>
<sectionHeader confidence="0.999281" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999932848484848">
Steven Abney. 2004. Understanding the yarowsky algo-
rithm. Comput. Linguist., 30(3):365–395.
Eneko Agirre, Oier Lopez de Lacalle, Arantxa Otegi,
German Rigau, and Piek Vossen. 2007. The Senseval-
2007 Task 1: Evaluating WSD on cross-language in-
formation retrieval. In Proceedings of SemEval-2007.
Association for Computational Linguistics.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet allocation. Journal of Machine Learning
Research, 3:993–1022, January.
Jordan L. Boyd-Graber, David M. Blei, and Jerry Zhu.
2007. Probabalistic walks in semantic hierarchies as a
topic model for WSD. In Proc. EMNLP 2007.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical taxon-
omy. In Proceedings on International Conference on
Research in Computational Linguistics, Taiwan.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proc. 15th International Conf. on Ma-
chine Learning, pages 296–304. Morgan Kaufmann,
San Francisco, CA.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In In 42nd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 280–287.
George Miller, Claudia Leacock, Randee Tengi, and Ross
Bunker. 1993. A semantic concordance. In 3rd
DARPA Workshop on Human Language Technology,
pages 303–308.
Sameer Pradhan, Martha Palmer, and Edward Loper.
2007. The Senseval-2007 Task 17: English fine-
grained all-words. In Proceedings of SemEval-2007.
Association for Computational Linguistics.
</reference>
<page confidence="0.997916">
281
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000050">
<title confidence="0.998705">PUTOP: Turning Predominant Senses into a Topic Model for Word Sense Disambiguation</title>
<author confidence="0.999979">Jordan Boyd-Graber</author>
<affiliation confidence="0.999696">Computer Science Princeton University</affiliation>
<address confidence="0.999882">Princeton, NJ 08540</address>
<email confidence="0.999733">jbg@princeton.edu</email>
<abstract confidence="0.998926857142857">We extend on McCarthy et al.’s predominant sense method to create an unsupervised method of word sense disambiguation that uses automatically derived topics using Latent Dirichlet allocation. Using topicspecific synset similarity measures, we create predictions for each word in each document using only word frequency information. It is hoped that this procedure can improve upon the method for larger numbers of topics by providing more relevant training corpora for the individual topics. This method is evaluated on SemEval-2007 Task 1 and Task 17. 1 Generative Model of WSD Word Sense Disambiguation (WSD) is the problem of labeling text with the appropriate semantic labels automatically. Although WSD is claimed to be an essential step in information retrieval and machine translation, it has not seen effective practical application because the dearth of labeled data has prevented the use of established supervised statistical methods that have been successfully applied to other natural language problems. Unsupervised methods have been developed for WSD, but despite modest success have not always been well understood statistically (Abney, 2004). Unsupervised methods are particularly appealing because they do not require expensive senseannotated data and can use the ever-increasing amount of raw text freely available. This paper expands on an effective unsupervised method for WSD and embeds it into a topic model, thus allowing an algorithm trained on a single, monolithic corpora to</abstract>
<title confidence="0.504365">instead hand-pick relevant documents in choosing</title>
<author confidence="0.999604">David Blei</author>
<affiliation confidence="0.9998415">Computer Science Princeton University</affiliation>
<address confidence="0.99983">Princeton, NJ 08540</address>
<email confidence="0.988909">blei@cs.princeton.edu</email>
<abstract confidence="0.983841944223108">a disambiguation. After developing this generative statistical model, we present its performance on a number of tasks. 1.1 The Intersection of Syntactic and Semantic Similarity McCarthy et al. (2004) outlined a method for learning a word’s most-used sense given an untagged corthat ranks each sense a distributional similarity a sesimilarity This process for a word distributional neighbors the possible senses not only the word in question, and also those the distributionally similar words, = One can view finding the appropriate sense as a search in two types of space. In determining how a particular synset the search the semantic space and the search in the syntactic space. We consider all of the words used in syntactically similar contexts, which we call “corroborators,” and for each of them we find the closest to a measure of semantic simfor instance a similarity measure such as Jiang-Conrath (1997). Each of the neighboring words’ contributions is weighted by the syntactic probability, as provided by Lin’s distributional similarity measure (1998), which rates two words to be similar if they enter into similar syntactic constructions. � ) = max 277 of the 4th International Workshop on Semantic Evaluations pages 277–281, June 2007. Association for Computational Linguistics Figure 1: A reinterpretation of McCarthy et al.’s predominant sense method as a generative model. Note that this model has no notion of context; a synset is assigned in an identical manner for all of the words in a vocabulary. One can think of this process as a generative model, even though it was not originally posed in a manner. For each word the vocabulary, we generate one of the neighbor corroborators acto the Lin similarity, between the words. We then generate a synset that word proportional to the maximum semantic simbetween any synset that contains the Figure 1). Our aim in this paper is to extend the method of McCarthy et al. using topic models. It is hoped that allowing the method to in effect “choose” the contexts that it uses will improve its ability to disambiguate sentences. 1.2 Using Topic Models to Partition a Document’s Words Topic models like Latent Dirichlet allocation (LDA) (Blei et al., 2003) assume a model of text generation where each document has a multinomial distribution over topics and each word comes from one of these topics. In LDA, each topic is a multinomial distribution, and each document has a multinomial distribution over topics drawn from a Dirichlet prior that selects the topic for each word in a document. Previous work has shown that such a model improves WSD over using a single corpus (Boyd- Graber et al., 2007), and we use this insight to develop an extension of McCarthy’s method for multiple topics. Although describing the statistical background and motivations behind topic models are beyond the scope of this paper, it suffices to note that the topics induced from a corpus provide a statistical grouping of words that often occur together and a probabilistic assignment of each word in a corpus to topics. Thus, one topic might have terms like “government,” “president,” “govern,” and “regal,” while another topic might have terms like “finance,” “highyield,” “investor,” and “market.” This paper assumes that the machinery for learning these distributions can, given a corpus and a specified number of topics, return the topic distributions most likely to have generated the corpus. 1.3 Defining the Model While the original predominant senses method used Lin’s thesaurus similarity method alone in generating the corroborator, we will also use the probability of that word being part of the same topic as the word to be disambiguated. Thus the process of choosing the “corroborator” is no longer identical for each word; it is affected by its topic, which changes for every document. This new generative process can be thought of as a modified LDA system that, after selecting the word generated by the topic, continues on by generating a corroborator and a sense for the original word: each document Select a topic distribution For each word in the document Select a topic Select a word from that topic Select a ”corroborator” proportional to how it is to the topic and its similarity to Now, select a synset that word based on a The conditional dependencies for generating a synset are shown in Figure 2. Our goal, like Mc- Carthy et al.’s, is to determine the most likely sense for each word. This amounts to posterior inference, which we address by marginalizing over the unobserved variables (the topics and the corroborators), = = E c, z c (3) In order to fully specify this, we must determine the distribution from which the corroborator is drawn and the distribution from which the synset is drawn. Ideally, we would want a distribution that for a single topic would be identical to McCarthy et al.’s 278 Figure 2: Our generative model assumes that documents are divided into topics and that these topics generate both the observed word and a “corroborator,” a term similar in usage to the word. Next, a sense that minimizes the semantic distance between the corroborator and the word is generated. method but would, as more topics are added, favor corroborators in the same topic as the number of topics increases. In McCarthy et al.’s method, the probof the corroborator given a word proto the Lin similarity the word and the corroborator. Here, the probability of corroborator is the multinomial probability of word the topic, and the multinomial probability of the word with a single topic (i.e. background word probability). Before, the corroborator was weighted simply on its syntactic similarity to the word now we also weight that contribution by how important unimportant) that word is to the topic that been assigned to. This has the effect of increasing the probability of words pertinent to the topic that also have high syntactic similarity. Thus, whenever the syntactic similarity captures polysemous usage, we hope to be able to separate the different usages. however, that since for a single topic the term cancels out and the procedure is equivalent to McCarthy et al. We adapt the semantic similarity in much the same way to make it topic specific. Because the Jiang-Conrath similarity measure uses an underlying term frequency to generate a similarity score, we use the topic term frequency instead of the undivided term frequency. Thus, the probability of a sense is proportional to semantic similarity between it and the closest sense among the senses of a corroborator with respect to this topic-specific similarity (c.f. the global similarity in Equation 2). The probability of a synset the corroborator a becomes c, This new dependence on the topic happens because we recompute the information content used by Jiang-Conrath with the distribution over words implied by each topic. We then use the similarity imby that similarity for Following the lead of McCarthy, for notational ease, this becomes defined Equation 8. 1.4 Choosing a Synset The problem of choosing a synset then is reduced to finding the synset with the highest probability under this model. The model is also designed so that the task of learning the assignment of topics to words and documents is not affected by this new machinery for corroborators and senses that we’ve added onto the model. Thus, we can use the variational inference method described in (Blei et al., 2003) as a foundation for the problem of synset inference. a given (i.e. determined by running LDA on the corpus), the probability for a synset a word becomes = XXc, z c whose terms have been described in the previous section. With all of the normalization terms, we now that c, (7) c, becomes, for the topic, Thus, we’ve now assigned a probability to each of the possible senses a word can take in a document. θ Z W C β S K N D X c X z c, max 279 1.5 Intuition For example, consider the word “fly,” which has two other words that have high syntactic similarity (in formulation, with the terms “fly ball” and “insect.” Both of these words would, given the semansimilarity provided by point to a single sense of “fly;” one of them would give a higher value, however, and thus all senses of the word “fly” would be assigned that sense. By separately weighting these words by the topic frequencies, we would hope to choose the sports sense in topics that have a higher probability of the terms like “foul ball,” “pop fly,” and “grounder” and the other sense in the contexts where insect has a higher probability in the topic. 2 Evaluations This section describes three experiments to determine the effectiveness of this unsupervised system. The first was used to help understand the system, and the second two were part of the SemEval 2007 competition. 2.1 SemCor As an initial evaluation, we learned LDA topics on the British National corpus with paragraphs as the underlying “document” (this allowed for a more uniform document length). These documents were then used to infer topic probabilities for each of the words in SemCor (Miller et al., 1993), and the model described in the previous section was run to determine the most likely synset. The results of this procedure are shown in Table 1. Accuracy is determined as the percentage of words for which the most likely sense was the one tagged in the corpus. While the method does roughly recreate Mc- Carthy et al.’s result for a single topic, it only offers a one percent improvement over McCarthy et al. on five topics and then falls below McCarthy for all greater numbers of topics tried. Thus, for all subsequent experiments we used a five topic model trained on the BNC. 2.2 SemEval-2007 Task 1: CLIR Using IR metrics, this disambiguation scheme was evaluated against another competing platform and an algorithm provided by the Task 1 (Agirre et al.,</abstract>
<author confidence="0.301289">Topics All Nouns</author>
<phone confidence="0.757283">1 .393 .467 5 .397 .478 25 .387 .456 200 .359 .420</phone>
<note confidence="0.8929845">Table 1: Accuracy on disambiguating words in Sem- Cor Task PUTOP Topic Expansion 0.30 Document Expansion 0.15 English Translation 0.17 SensEval2 0.39 SensEval3 0.33</note>
<abstract confidence="0.994479049180328">Table 2: Performance results on Task 1 2007) organizers. Our system had the best results of any expansion scheme considered (0.30) , although none of the expansion schemes did better than using no expansion (0.36). Although our technique also yielded a better score than the other competing platform for cross-language queries (0.17), it did not surpass the first sense-heuristic (0.26), but this is not surprising given that our algorithm does not assume the existence of such information. For an overview of Task 1 results, see Table 2. 2.3 SemEval-2007 Task 17: All-Words Task 17 (Pradhan et al., 2007) asked participants to submit results as probability distributions over senses. Because this is also the output of this algorithm, we submitted the probabilities to the contest before realizing that the distributions are very close to uniform over all senses and thus yielded a precision of 0.12, very close to the random baseline. Placing a point distribution on the argmax with our original submission to the task, however, (consistent with our methodology for evaluation on SemCor), gives a precision of 0.39. 3 Conclusion While the small improvement over the single topic suggests that topic techniques might have traction in determining the best sense, the addition is not ap- In a way the failure of the technique is en- 280 couraging in that it affirms the original methodology of McCarthy et al. in finding a single predominant sense for each word. While the syntactic similarity measure indeed usually offers high values of similarity for words related to a single sense of a word, the similarity for words related to other senses, which we had hoped to strengthen by using topic features, are on par with words observed because of noise. Thus, for a word like “bank,” words like “firm,” “commercial bank,” “company,” and “financial institution” are the closest in terms of the syntactic similarity, and this allows the financial senses to be selected without any difficulty. Even if we had corroborating words for another sense in some topic, these words are absent from the syntactically similar words. If we want the meaning similar to that of “riverbank,” the word with the most similar meaning, “side,” had a syntactic similarity on par with the unrelated words “individual” and “group.” Thus, interpretations other than the dominant sense as determined by the baseline method of McCarthy et al. are hard to find. Because one topic is equivalent to McCarthy et al.’s method, this means that we do no worse on disambiguation. However, contrary to our hope, increasing the number of topics does not lead to significantly better sense predictions. This work has not investigated using a topic-based procedure for determining the syntactic similarity, but we feel that this extension could provide real improvement to the unsupervised techniques that can make use of the copious amounts of available unlabeled data.</abstract>
<note confidence="0.783979">References Steven Abney. 2004. Understanding the yarowsky algo- 30(3):365–395. Eneko Agirre, Oier Lopez de Lacalle, Arantxa Otegi, German Rigau, and Piek Vossen. 2007. The Senseval- 2007 Task 1: Evaluating WSD on cross-language in-</note>
<title confidence="0.7011715">retrieval. In of Association for Computational Linguistics.</title>
<author confidence="0.81007">La-</author>
<affiliation confidence="0.836728">Dirichlet allocation. of Machine Learning</affiliation>
<address confidence="0.938315">3:993–1022, January.</address>
<author confidence="0.923916">Jordan L Boyd-Graber</author>
<author confidence="0.923916">David M Blei</author>
<author confidence="0.923916">Jerry Zhu</author>
<note confidence="0.302526">2007. Probabalistic walks in semantic hierarchies as a</note>
<title confidence="0.789085">model for WSD. In EMNLP</title>
<author confidence="0.915018">Semantic</author>
<abstract confidence="0.695235333333333">similarity based on corpus statistics and lexical taxon- In on International Conference on in Computational Taiwan.</abstract>
<keyword confidence="0.184107">Dekang Lin. 1998. An information-theoretic definition</keyword>
<note confidence="0.8890711875">similarity. In 15th International Conf. on Mapages 296–304. Morgan Kaufmann, San Francisco, CA. Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant word senses in text. In 42nd Annual Meeting of the Assofor Computational pages 280–287. George Miller, Claudia Leacock, Randee Tengi, and Ross 1993. A semantic concordance. In Workshop on Human Language pages 303–308. Sameer Pradhan, Martha Palmer, and Edward Loper. 2007. The Senseval-2007 Task 17: English fineall-words. In of Association for Computational Linguistics. 281</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Understanding the yarowsky algorithm.</title>
<date>2004</date>
<journal>Comput. Linguist.,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="1344" citStr="Abney, 2004" startWordPosition="204" endWordPosition="205">ask 17. 1 Generative Model of WSD Word Sense Disambiguation (WSD) is the problem of labeling text with the appropriate semantic labels automatically. Although WSD is claimed to be an essential step in information retrieval and machine translation, it has not seen effective practical application because the dearth of labeled data has prevented the use of established supervised statistical methods that have been successfully applied to other natural language problems. Unsupervised methods have been developed for WSD, but despite modest success have not always been well understood statistically (Abney, 2004). Unsupervised methods are particularly appealing because they do not require expensive senseannotated data and can use the ever-increasing amount of raw text freely available. This paper expands on an effective unsupervised method for WSD and embeds it into a topic model, thus allowing an algorithm trained on a single, monolithic corpora to instead hand-pick relevant documents in choosing David Blei Computer Science Princeton University Princeton, NJ 08540 blei@cs.princeton.edu a disambiguation. After developing this generative statistical model, we present its performance on a number of task</context>
</contexts>
<marker>Abney, 2004</marker>
<rawString>Steven Abney. 2004. Understanding the yarowsky algorithm. Comput. Linguist., 30(3):365–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
</authors>
<title>Oier Lopez de Lacalle, Arantxa Otegi, German Rigau, and Piek Vossen.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval-2007. Association for Computational Linguistics.</booktitle>
<marker>Agirre, 2007</marker>
<rawString>Eneko Agirre, Oier Lopez de Lacalle, Arantxa Otegi, German Rigau, and Piek Vossen. 2007. The Senseval2007 Task 1: Evaluating WSD on cross-language information retrieval. In Proceedings of SemEval-2007. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="4427" citStr="Blei et al., 2003" startWordPosition="714" endWordPosition="717">erate one of the neighbor corroborators according to the Lin similarity, -y(c, w), between the two words. We then generate a synset s for that word proportional to the maximum semantic similarity between s and any synset that contains the corroborator c (see Figure 1). Our aim in this paper is to extend the method of McCarthy et al. using topic models. It is hoped that allowing the method to in effect “choose” the contexts that it uses will improve its ability to disambiguate sentences. 1.2 Using Topic Models to Partition a Document’s Words Topic models like Latent Dirichlet allocation (LDA) (Blei et al., 2003) assume a model of text generation where each document has a multinomial distribution over topics and each word comes from one of these topics. In LDA, each topic is a multinomial distribution, and each document has a multinomial distribution over topics drawn from a Dirichlet prior that selects the topic for each word in a document. Previous work has shown that such a model improves WSD over using a single corpus (BoydGraber et al., 2007), and we use this insight to develop an extension of McCarthy’s method for multiple topics. Although describing the statistical background and motivations be</context>
<context position="10070" citStr="Blei et al., 2003" startWordPosition="1696" endWordPosition="1699">ver words implied by each topic. We then use the similarity implied by that similarity for αz. Following the lead of McCarthy, for notational ease, this becomes defined as wnss in Equation 8. 1.4 Choosing a Synset The problem of choosing a synset then is reduced to finding the synset with the highest probability under this model. The model is also designed so that the task of learning the assignment of topics to words and documents is not affected by this new machinery for corroborators and senses that we’ve added onto the model. Thus, we can use the variational inference method described in (Blei et al., 2003) as a foundation for the problem of synset inference. Taking p(z|w) as a given (i.e. determined by running LDA on the corpus), the probability for a synset s given a word w then becomes p(s|w, z) = X X p(s|w, c, z)p(c|z)p(z|w), (6) z c whose terms have been described in the previous section. With all of the normalization terms, we now see that p(s|w, z) becomes βz,c β0 c γ(w, c) Ps&apos;∈Sw wnss(s0, c, z). (7) and wnss(s, c, z) now becomes, for the zth topic, αz(a, s). (8) Thus, we’ve now assigned a probability to each of the possible senses a word can take in a document. θ Z W C β S K N D αz(s, s0</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David Blei, Andrew Ng, and Michael Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan L Boyd-Graber</author>
<author>David M Blei</author>
<author>Jerry Zhu</author>
</authors>
<title>Probabalistic walks in semantic hierarchies as a topic model for WSD.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP</booktitle>
<marker>Boyd-Graber, Blei, Zhu, 2007</marker>
<rawString>Jordan L. Boyd-Graber, David M. Blei, and Jerry Zhu. 2007. Probabalistic walks in semantic hierarchies as a topic model for WSD. In Proc. EMNLP 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings on International Conference on Research in Computational Linguistics,</booktitle>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings on International Conference on Research in Computational Linguistics, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proc. 15th International Conf. on Machine Learning,</booktitle>
<pages>296--304</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proc. 15th International Conf. on Machine Learning, pages 296–304. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Finding predominant word senses in untagged text.</title>
<date>2004</date>
<booktitle>In In 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>280--287</pages>
<contexts>
<context position="2027" citStr="McCarthy et al. (2004)" startWordPosition="302" endWordPosition="305">y do not require expensive senseannotated data and can use the ever-increasing amount of raw text freely available. This paper expands on an effective unsupervised method for WSD and embeds it into a topic model, thus allowing an algorithm trained on a single, monolithic corpora to instead hand-pick relevant documents in choosing David Blei Computer Science Princeton University Princeton, NJ 08540 blei@cs.princeton.edu a disambiguation. After developing this generative statistical model, we present its performance on a number of tasks. 1.1 The Intersection of Syntactic and Semantic Similarity McCarthy et al. (2004) outlined a method for learning a word’s most-used sense given an untagged corpus that ranks each sense wsi using a distributional syntactic similarity γ and a WORDNET-derived semantic similarity α. This process for a word w uses its distributional neighbors Nw, the possible senses of not only the word in question, Sw, and also those of the distributionally similar words, Snj. Thus, P(wsi) = α(a, s). (2) One can view finding the appropriate sense as a search in two types of space. In determining how good a particular synset wsi is, α guides the search in the semantic space and γ drives the sea</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant word senses in untagged text. In In 42nd Annual Meeting of the Association for Computational Linguistics, pages 280–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Claudia Leacock</author>
<author>Randee Tengi</author>
<author>Ross Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In 3rd DARPA Workshop on Human Language Technology,</booktitle>
<pages>303--308</pages>
<contexts>
<context position="11957" citStr="Miller et al., 1993" startWordPosition="2043" endWordPosition="2046">nder” and the other sense in the contexts where insect has a higher probability in the topic. 2 Evaluations This section describes three experiments to determine the effectiveness of this unsupervised system. The first was used to help understand the system, and the second two were part of the SemEval 2007 competition. 2.1 SemCor As an initial evaluation, we learned LDA topics on the British National corpus with paragraphs as the underlying “document” (this allowed for a more uniform document length). These documents were then used to infer topic probabilities for each of the words in SemCor (Miller et al., 1993), and the model described in the previous section was run to determine the most likely synset. The results of this procedure are shown in Table 1. Accuracy is determined as the percentage of words for which the most likely sense was the one tagged in the corpus. While the method does roughly recreate McCarthy et al.’s result for a single topic, it only offers a one percent improvement over McCarthy et al. on five topics and then falls below McCarthy for all greater numbers of topics tried. Thus, for all subsequent experiments we used a five topic model trained on the BNC. 2.2 SemEval-2007 Task</context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>George Miller, Claudia Leacock, Randee Tengi, and Ross Bunker. 1993. A semantic concordance. In 3rd DARPA Workshop on Human Language Technology, pages 303–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Martha Palmer</author>
<author>Edward Loper</author>
</authors>
<title>The Senseval-2007 Task 17: English finegrained all-words.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval-2007. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="13552" citStr="Pradhan et al., 2007" startWordPosition="2314" endWordPosition="2317"> SensEval3 0.33 Table 2: Performance results on Task 1 2007) organizers. Our system had the best results of any expansion scheme considered (0.30) , although none of the expansion schemes did better than using no expansion (0.36). Although our technique also yielded a better score than the other competing platform for cross-language queries (0.17), it did not surpass the first sense-heuristic (0.26), but this is not surprising given that our algorithm does not assume the existence of such information. For an overview of Task 1 results, see Table 2. 2.3 SemEval-2007 Task 17: All-Words Task 17 (Pradhan et al., 2007) asked participants to submit results as probability distributions over senses. Because this is also the output of this algorithm, we submitted the probabilities to the contest before realizing that the distributions are very close to uniform over all senses and thus yielded a precision of 0.12, very close to the random baseline. Placing a point distribution on the argmax with our original submission to the task, however, (consistent with our methodology for evaluation on SemCor), gives a precision of 0.39. 3 Conclusion While the small improvement over the single topic suggests that topic tech</context>
</contexts>
<marker>Pradhan, Palmer, Loper, 2007</marker>
<rawString>Sameer Pradhan, Martha Palmer, and Edward Loper. 2007. The Senseval-2007 Task 17: English finegrained all-words. In Proceedings of SemEval-2007. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>