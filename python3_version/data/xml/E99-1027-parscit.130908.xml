<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000013">
<note confidence="0.712868">
Proceedings of EACL &apos;99
</note>
<title confidence="0.998648">
An experiment on the upper bound of interjudge agreement:
the case of tagging
</title>
<author confidence="0.711284">
Atro Voutilainen
</author>
<affiliation confidence="0.63616625">
Research Unit for Multilingual Language Technology
P.O. Box 4
FIN-00014 University of Helsinki
Finland
</affiliation>
<email confidence="0.631658">
Atro.Voutilainen©ling.Helsinki.FI
</email>
<sectionHeader confidence="0.995171" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999948583333333">
We investigate the controversial issue
about the upper bound of interjudge
agreement in the use of a low-level
grammatical representation. Pessimistic
views suggest that several percent of
words in running text are undecidable in
terms of part-of-speech categories. Our
experiments with 55kW data give rea-
son for optimism: linguists with only 30
hours&apos; training apply the EngCG-2 mor-
phological tags with almost 100% inter-
judge agreement.
</bodyText>
<sectionHeader confidence="0.996429" genericHeader="keywords">
1 Orientation
</sectionHeader>
<bodyText confidence="0.999823432835821">
Linguistic analysers are developed for assign-
ing linguistic descriptions to linguistic utterances.
Linguistic descriptions are based on a fixed inven-
tory of descriptors plus their usage principles: in
short, a grammatical representation specified by
linguists for the specific kind of analysis — e.g.
morphological analysis, tagging, syntax, discourse
structure — that the program should perform.
Because automatic linguistic analysis generally
is a very difficult problem, various methods for
evaluating their success have been used. One such
is based on the degree of correctness of the analysis
provided, e.g. the percentage of linguistic tokens
in the text analysed that receives the appropriate
description relative to analyses provided indepen-
dently of the program by competent linguists ide-
ally not involved in the development of the anal-
yser itself.
Now use of benchmark corpora like this turns
out to be problematic because arguments have
been made to the effect that linguists themselves
make erroneous and inconsistent analyses. Unin-
tentional mistakes due e.g. to slips of attention
are obviously unavoidable, but these errors can
largely be identified by the double-blind method:
first by having two (or more) linguists analyse the
same text independently by using the same gram-
matical representation, and then identifying dif-
ferences of analysis by automatically comparing
the analysed text versions with each other and fi-
nally having the linguists discuss the differences
and modify the resulting benchmark corpus ac-
cordingly. Clerical errors should be easily (i.e.
consensually) identified as such, but, perhaps sur-
prisingly, many attested differences do not belong
to this category. Opinions may genuinely differ
about which of the competing analyses is the cor-
rect one, i.e. sometimes the grammatical repre-
sentation is used inconsistently. In short, linguis-
tic &apos;truth&apos; seems to be uncertain in many cases.
Evaluating — or even developing — linguistic anal-
ysers seems to be on uncertain ground if the goal
of these analysers cannot be satisfactorily speci-
fied.
Arguments concerning the magnitude of this
problem have been made especially in relation to
tagging, the attempt to automatically assign lex-
ically and contextually correct morphological de-
scriptors (tags) to words. A pessimistic view is
taken by Church (1992) who argues that even af-
ter negotiations of the kind described above, no
consensus can be reached about the correct anal-
ysis of several percent of all word tokens in the
text. A more mixed view on the matter is taken
by Marcus et al. (1993) who on the one hand note
that in one experiment moderately trained human
text annotators made different analyses even after
negotiations in over 3% of all words, and on the
other hand argue that an expert can do much bet-
ter.
An optimistic view on the matter has been pre-
sented by Eyes and Leech (1993). Empirical ev-
idence for a high agreement rate is reported by
Voutilainen and Jarvinen (1995). Their results
suggest that at least with one grammatical repre-
sentation, namely the ENGCG tag set (cf. Karls-
son et al., eds., 1995), a 100% consistency can be
</bodyText>
<page confidence="0.996924">
204
</page>
<bodyText confidence="0.962042625">
Proceedings of EACL &apos;99
reached after negotiations at the level of parts of
speech (or morphology in this case). In short, rea-
sonable evidence has been given for the position
that at least some tag sets can be applied consis-
tently, i.e. earlier observations about potentially
more problematic tag sets should not be taken as
predictions about all tag sets.
</bodyText>
<subsectionHeader confidence="0.988806">
1.1 Open questions
</subsectionHeader>
<bodyText confidence="0.998214015151515">
Admittedly Voutilainen and Jarvinen&apos;s experi-
ment provides evidence for the possibility that
two highly experienced linguists, one of them a
developer of the ENGCG tag set, can apply the
tag set consistently, at least when compared with
each others&apos; performance. However, the practical
significance of their result seems questionable, for
two reasons.
Firstly, large-scale corpus annotation by hand
is generally a work that is carried out by less ex-
perienced linguists, quite typically advanced stu-
dents hired as project workers. Voutilainen and
Jarvinen&apos;s experiment leaves open the question,
how consistently the ENGCG tag set can be ap-
plied by a less experienced annotator.
Secondly, consider the question of tagger evalu-
ation. Because tagger developers presumably tend
to learn, perhaps partly subconsciously, much
about the behaviour, desired or otherwise, of the
tagger, it may well be that if the developers also
annotate the benchmark corpus used for evaluat-
ing the tagger, some of the tagger&apos;s misanalyses
remain undetected because the tagger developers,
due to their subconscious mimicking of their tag-
ger, make the same misanalyses when annotating
the benchmark corpus. So 100% tagging consis-
tency in the benchmark corpus alone does not nec-
essarily suffice for getting an objective view of the
tagger&apos;s performance. Subconscious &apos;bad&apos; habits
of this type need to be factored out. One way to do
this is having the benchmark corpus consistently
(i.e. with approximately 100% consensus about
the correct analysis) analysed by people with no
familiarity with the tagger&apos;s behaviour in differ-
ent situations — provided this is possible in the
first place.
Another two minor questions left open by Vou-
tilainen and Jarvinen concern the (i) typology of
the differences and (ii) the reliability of their ex-
periment.
Concerning the typology of the differences: in
Voutilainen and Jarvinen&apos;s experiment the lin-
guists negotiated about an initial difference, al-
most one per cent of all words in the texts.
Though they finally agreed about the correct anal-
ysis in almost all these differences, with a slight
improvement in the experimental setting a clear
categorisation of the initial differences into un-
intentional mistakes and other, more interesting
types, could have been made.
Secondly, the texts used in Voutilainen and
Jarvinen&apos;s experiment comprised only about 6,000
words. This is probably enough to give a general
indication of the nature of the analysis task with
the ENGCG tag set, but a larger data would in-
crease the reliability of the experiment.
In this paper, we address all these three ques-
tions. Two young linguistsl with no background
in ENGCG tagging were hired for making an elab-
orated version of the Voutilainen and Jarvinen ex-
periment with a considerably larger corpus.
The rest of this paper is structured as follows.
Next, the ENGCG tag set is described in outline.
Then the training of the new linguists is described,
as well as the test data and experimental setting.
Finally, the results are presented.
</bodyText>
<sectionHeader confidence="0.965939" genericHeader="introduction">
2 ENGCG tag set
</sectionHeader>
<bodyText confidence="0.970280133333333">
Descriptions of the morphological tags used by
the English Constraint Grammar tagger are avail-
able in several publications. Brief descriptions can
be found in several recent ACL conference pro-
ceedings by Voutilainen and his colleagues (e.g.
EACL93, ANLP94, EACL95, ANLP97, ACL-
EACL97). An in-depth description is given in
Karlsson et al., eds., 1995 (chapters 3-6). Here,
only a brief sample is given.
ENGCG tagging is a two-phase process. First,
a lexical analyser assigns one or more alternative
analyses to each word. The following is a mor-
phological analysis of the sentence The raids were
coordinated under a recently expanded federal pro-
gram:
</bodyText>
<table confidence="0.883385352941176">
&amp;quot;&lt;The&gt;&amp;quot; &amp;quot;the&amp;quot; &lt;Def&gt; DET CENTRAL ART SG/PL
&amp;quot;&lt;raids&gt;&amp;quot;
&amp;quot;raid&amp;quot; &lt;Count&gt; N NOM PL
&amp;quot;raid&amp;quot; &lt;SVO&gt; V PRES SG3
&amp;quot;&lt;were&gt;&amp;quot;
&amp;quot;be&amp;quot; &lt;SVC/A&gt; &lt;SVC/N&gt; V PAST
&amp;quot;&lt;coordinated&gt;&amp;quot;
&amp;quot;coordinate&amp;quot; &lt;SVO&gt; EN
&amp;quot;coordinate&amp;quot; &lt;SVO&gt; V PAST
&amp;quot;&lt;under&gt;&amp;quot;
&amp;quot;under&amp;quot; ADV ADVL
&amp;quot;under&amp;quot; PREP
&amp;quot;under&amp;quot; &lt;Attr&gt; A ABS
&amp;quot;&lt;a&gt;&amp;quot;
&amp;quot;a&amp;quot; ABBR NOM SG
&amp;quot;a&amp;quot; &lt;Indef&gt; DET CENTRAL ART SG
1Ms. Pirldco Paljakka and Mr. Markku Lappalainen
</table>
<page confidence="0.951221">
205
</page>
<listItem confidence="0.693751">
Proceedings of EACL &apos;99
1. The text was morphologically analysed us-
ing the ENGCG morphological analyser. For
the analysis of unrecognised words, we used
EN a rule-based heuristic component that assigns
</listItem>
<bodyText confidence="0.92997725">
V PAST morphological analyses, one or more, to each
word not represented in the lexicon of the sys-
tem. Of the analysed text, two identical ver-
sions were made, one for each linguist.
</bodyText>
<listItem confidence="0.739991555555556">
2. Two linguists trained to disambiguate the
ENGCG morphological representation (see
the subsection on training below) indepen-
dently marked the correct alternative anal-
yses in the ambiguous input, using mainly
structural, but in some structurally unresolv-
able cases also higher-level, information. The
corpora consisted of continuous text rather
than isolated sentences; this made the use
</listItem>
<bodyText confidence="0.913157107142857">
of textual knowledge possible in the selection
of the correct alternative. In the rare cases
where two analyses were regarded as equally
legitimate, both could be masked. The judges
were encouraged to consult the documenta-
tion of the grammatical representation. In
addition, both linguists were provided with a
checking program to be used after the text
was analysed. The program identifies words
left without an analysis, in which case the
linguist was to provide the missing analysis.
3. These analysed versions of the same text were
compared to each other using the Unix sdiff
program. For each corpus version, words with
a different analysis were marked with a &amp;quot;RE-
CONSIDER&amp;quot; symbol. The &amp;quot;RECONSIDER&amp;quot;
symbol was also added to a number of other
ambiguous words in the corpus. These addi-
tional words were marked in order to &apos;force&apos;
each linguist to think independently about
the correct analysis, i.e. to prevent the emer-
gence of the situation where one linguist con-
siders the other to be always right (or wrong)
and so &apos;reconsiders&apos; only in terms of the ex-
isting analysis. The linguists were told that
some of the words marked with the &amp;quot;RECON-
SIDER&amp;quot; symbol were analysed differently by
them.
</bodyText>
<listItem confidence="0.89319675">
4. Statistics were generated about the num-
ber of differing analyses (number of &amp;quot;RE-
CONSIDER&amp;quot; symbols) in the corpus versions
(&amp;quot;difFl&amp;quot; in the following table).
5. The reanalysed versions were automatically
compared to each other. To words with a
different analysis, a &amp;quot;NEGOTIATE&amp;quot; symbol
was added.
</listItem>
<figure confidence="0.997660571428571">
&amp;quot;&lt;recently&gt;&amp;quot;
&amp;quot;recent&amp;quot; &lt;DER:ly&gt; ADV
&amp;quot;&lt;expanded&gt;&amp;quot;
&amp;quot;expand&amp;quot; &lt;SVO&gt; &lt;Pion&gt;
&amp;quot;expand&amp;quot; &lt;SVO&gt; &lt;Pion&gt;
&amp;quot;&lt;federal&gt;&amp;quot;
&amp;quot;federal&amp;quot; A ABS
&amp;quot;&lt;program&gt;&amp;quot;
&amp;quot;program&amp;quot; N NOM SG
&amp;quot;program&amp;quot; &lt;SVO&gt; V PRES -SG3
&amp;quot;program&amp;quot; &lt;SVO&gt; V INF
&amp;quot;program&amp;quot; &lt;SVO&gt; V IMP
&amp;quot;program&amp;quot; &lt;SVO&gt; V SUBJUNCTIVE
111‹.&gt;11
</figure>
<figureCaption confidence="0.973473666666667">
Each indented line constitutes one morphologi-
cal analysis. Thus program is five-ways ambiguous
after ENGCG morphology. The disambiguation
part of the ENGCG tagger2 then removes those
alternative analyses that are contextually illegit-
imate according to the tagger&apos;s hand-coded con-
straint rules (Voutilainen 1995). The remaining
analyses constitute the output of the tagger, in
this case:
</figureCaption>
<figure confidence="0.944581769230769">
&amp;quot;&lt;The&gt;&amp;quot;
&amp;quot;the&amp;quot; &lt;Def&gt; DET CENTRAL ART SG/PL
&amp;quot;&lt;raids&gt;&amp;quot;
&amp;quot;raid&amp;quot; &lt;Count&gt; N NOM PL
&amp;quot;&lt;were&gt;&amp;quot;
&amp;quot;be&amp;quot; &lt;SVC/A&gt; &lt;SVC/N&gt; V PAST
&amp;quot;&lt;coordinated&gt;&amp;quot;
&amp;quot;coordinate&amp;quot; &lt;SVO&gt; EN
&amp;quot;&lt;under&gt;&amp;quot;
&amp;quot;under&amp;quot; PREP
&amp;quot;&lt;a&gt;&amp;quot;
&amp;quot;a&amp;quot; &lt;Indef&gt; DET CENTRAL ART SG
&amp;quot;&lt;recently&gt;&amp;quot;
&amp;quot;recent&amp;quot; &lt;DER:ly&gt; ADV
&amp;quot;&lt;expanded&gt;&amp;quot;
&amp;quot;expand&amp;quot; &lt;SVO&gt; &lt;P/on&gt; EN
&amp;quot;&lt;federal&gt;&amp;quot;
&amp;quot;federal&amp;quot; A ABS
&amp;quot;&lt;program&gt;&amp;quot;
&amp;quot;program&amp;quot; N NOM SG •
Overall, this tag set represents about 180 differ-
ent analyses when certain optional auxiliary tags
(e.g. verb subcategorisation tags) are ignored.
3 Preparations for the experiment
3.1 Experimental setting
The experiment was conducted as follows.
</figure>
<footnote confidence="0.9728795">
2A new version of the tagger, known as EngCG-2,
can be studied and tested at http://www.conexor.fi.
</footnote>
<page confidence="0.993391">
206
</page>
<bodyText confidence="0.836690761904762">
Proceedings of EACL &apos;99
6. Statistics were generated about the num-
ber of differing analyses (number of &amp;quot;NE-
GOTIATE&amp;quot; symbols) in the corpus versions
(&amp;quot;diff2&amp;quot; in the following table).
7. The remaining differences in the analyses
were jointly examined by the linguists in or-
der to see whether they were due to (i) inat-
tention on the part of one linguist (as a result
of which a correct unique analysis was jointly
agreed upon), (ii) joint uncertainty about the
correct analysis (both linguists feel unsure
about the correct analysis), or (iii) conflict-
ing opinions about the correct analysis (both
linguists have a strong but different opinion
about the correct analysis).
8. Statistics were generated about the number
of conflicting opinions (&amp;quot;diff3&amp;quot; below) and
joint uncertainty (&amp;quot;unsure&amp;quot; below).
This routine was successively applied to each
text.
</bodyText>
<subsectionHeader confidence="0.999674">
3.2 Training
</subsectionHeader>
<bodyText confidence="0.999975789473684">
Two people were hired for the experiment. One
had recently completed a Master&apos;s degree from
English Philology. The other was an advanced un-
dergraduate student majoring in English Philol-
ogy. Neither of them were familiar with the
ENGCG tagger.
All available documentation about the linguistic
representation used by ENGCG was made avail-
able to them. The chief source was chapters 3-6
in Karlsson et al. (eds., 1995). Because the lin-
guistic solutions in ENGCG are largely based on
the comprehensive descriptive grammar by Quirk
et al. (1985), also that work was made available
to them, as well as a number of modern English
dictionaries.
The training was based on the disambiguation
of ten smallish text extracts. Each of the extracts
was first analysed by the ENGCG morphological
analyser, and then each trainee was to indepen-
dently perform Step 3 (see the previous subsec-
tion) on it. The disambiguated text was then au-
tomatically compared to another version of the
same extract that was disambiguated by an expert
on ENGCG. The ENGCG expert then discussed
the analytic differences with the trainee who had
also disambiguated the text and explained why
the expert&apos;s analysis was correct (almost always
by identifying a relevant section in the available
ENGCG documentation; in very rare cases where
the documentation was underspecific, new docu-
mentation was created for future use in the exper-
iments).
After analysis and subsequent consultation with
the ENGCG expert, the trainee processed the fol-
lowing sample.
The training lasted about 30 hours. It was con-
cluded by familiarising the linguists with the rou-
tine used in the experiment.
</bodyText>
<subsectionHeader confidence="0.999848">
3.3 Test corpus
</subsectionHeader>
<bodyText confidence="0.999989692307692">
Four texts were used in the experiment, to-
talling 55724 words and 102527 morphologi-
cal analyses (an average of 1.84 analyses per
word). One was an article about Japanese
culture (`Pop&apos;); one concerned patents (&apos;Pat&apos;);
one contained excerpts from the law of Cali-
fornia; one was a medical text (`Med&apos;). None
of them had been used in the development of
the ENGCG grammatical representation or other
parts of the system. By mid-June 1999, a sam-
ple of this data will be available for inspection
at http://www.ling.helsinki.fi/ voutilai/eac199-
data.html.
</bodyText>
<sectionHeader confidence="0.997983" genericHeader="background">
4 Results and discussion
</sectionHeader>
<figureCaption confidence="0.8837785">
The following table presents the main findings.
Figure 1: .Results from a human annotation task.
</figureCaption>
<bodyText confidence="0.999961714285715">
It is interesting to note how high the agree-
ment between the linguists is even before the first
negotiations (99.80% of all words are analysed
identically). Of the remaining differences, most,
somewhat disappointingly, turned out to be clas-
sified as &apos;slips of attention&apos;; upon inspection they
seemed to contain little linguistic interest. Espe-
cially one of the linguists admitted that most of
the job seemed too much of a routine to keep one
mentally alert enough. The number of genuine
conflicts of opinion were much in line with obser-
vations by Voutilainen and Jarvinen. However,
the negotiations were not altogether easy, consid-
ering that in all they took almost nine hours. Pre-
sumably uncertain analyses and conflicts of opin-
ion were not easily passed by.
The main finding of this experiment is that
basically Voutilainen and Jarvinen&apos;s observations
about the high specifiability and consistent usabil-
ity of the ENGCG morphological tag set seem to
be extendable to new users of the tag set. In
</bodyText>
<table confidence="0.991012333333333">
words diffl diff2 diff3 unsure
Pop 14861 188/1.3% 44/.3% 0 4/.0%
Pat 13183 92/.7% 11/.1% 2/.0% 1/.0%
Law 15495 107/.7% 18/.1% 10/.1% 0
Med 12185 126/1.0% 39/.3% 1/.0% 9/.1%
ALL 55724 513/.9% 112/.2% 13/.0% 14/.0%
</table>
<page confidence="0.990838">
207
</page>
<bodyText confidence="0.97779625">
Proceedings of EACL &apos;99
other words, the reputedly surface-syntactic tag
set seems to be learnable as well. Overall, the ex-
periment reported here provides evidence for the
optimistic position about the specifiability of at
least certain kinds of linguistic representations.
It remains for future research, perhaps as a col-
laboration between teams working with different
tag sets, to find out, what exactly are the prop-
erties that make some linguistic representations
consistently learnable and usable, and others less
SO.
</bodyText>
<sectionHeader confidence="0.998875" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.984299">
I am grateful to anonymous EACL99 referees for
useful comments.
</bodyText>
<sectionHeader confidence="0.99933" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999765290322581">
Kenneth W. Church 1992. Current Practice in
Part of Speech Tagging and Suggestions for the
Future. In Simmons (ed.), Sbornik praci: In
Honor of Henry Kucera, Michigan Slavic Studies.
Michigan. 13-48.
Elizabeth Eyes and Geoffrey Leech 1993. Syn-
tactic Annotation: Linguistic Aspects of Gram-
matical Tagging and Skeleton Parsing. In Ezra
Black, Roger Garside and Geoffrey Leech (eds.)
1993. Statistically-Driven Computer Grammars
of English: The IBM/Lancaster Approach. Am-
sterdam and Atlanta: Rodopi. 36-61.
Fred Karlsson, Atro Voutilainen, Julia Heikkila
and Arto Anttila (eds.) 1995. Constraint Gram-
mar. A Language-Independent System for Pars-
ing Unrestricted Text. Berlin and New York:
Mouton de Gruyter.
Mitchell Marcus, Beatrice Santorini and Mary
Ann Marcinkiewicz 1993. Building a Large An-
notated Corpus of English: The Penn Treebank.
Computational Linguistics 19:2. 313-330.
Randolph Quirk, Sidney Greenbaum, Jan
Svartvik and Geoffrey Leech 1985. A Comprehen-
sive Grammar of the English Language. Longman.
Atro Voutilainen 1995. Morphological disam-
biguation. In Karlsson et al., eds.
Atro Voutilainen and Timo Jarvinen 1995.
Specifying a shallow grammatical representation
for parsing purposes. In Proceedings of the Sev-
enth Conference of the European Chapter of the
Association for Computational Linguistics. ACL.
</reference>
<page confidence="0.997777">
208
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000021">
<note confidence="0.702395">Proceedings of EACL &apos;99 An experiment on the upper bound of interjudge agreement:</note>
<title confidence="0.895309">the case of tagging</title>
<author confidence="0.982422">Atro Voutilainen</author>
<affiliation confidence="0.893344">Research Unit for Multilingual Language Technology</affiliation>
<address confidence="0.959665">P.O. Box 4</address>
<affiliation confidence="0.855815">FIN-00014 University of Helsinki Finland</affiliation>
<email confidence="0.457504">Atro.Voutilainen©ling.Helsinki.FI</email>
<abstract confidence="0.985653675949367">We investigate the controversial issue about the upper bound of interjudge agreement in the use of a low-level grammatical representation. Pessimistic views suggest that several percent of words in running text are undecidable in terms of part-of-speech categories. Our experiments with 55kW data give reason for optimism: linguists with only 30 hours&apos; training apply the EngCG-2 morphological tags with almost 100% interjudge agreement. 1 Orientation Linguistic analysers are developed for assigning linguistic descriptions to linguistic utterances. Linguistic descriptions are based on a fixed inventory of descriptors plus their usage principles: in a representation by linguists for the specific kind of analysis — e.g. morphological analysis, tagging, syntax, discourse structure — that the program should perform. Because automatic linguistic analysis generally is a very difficult problem, various methods for evaluating their success have been used. One such is based on the degree of correctness of the analysis provided, e.g. the percentage of linguistic tokens in the text analysed that receives the appropriate description relative to analyses provided independently of the program by competent linguists ideally not involved in the development of the analyser itself. Now use of benchmark corpora like this turns out to be problematic because arguments have been made to the effect that linguists themselves make erroneous and inconsistent analyses. Unintentional mistakes due e.g. to slips of attention are obviously unavoidable, but these errors can largely be identified by the double-blind method: first by having two (or more) linguists analyse the same text independently by using the same grammatical representation, and then identifying differences of analysis by automatically comparing the analysed text versions with each other and finally having the linguists discuss the differences and modify the resulting benchmark corpus accordingly. Clerical errors should be easily (i.e. consensually) identified as such, but, perhaps surprisingly, many attested differences do not belong to this category. Opinions may genuinely differ about which of the competing analyses is the correct one, i.e. sometimes the grammatical representation is used inconsistently. In short, linguistic &apos;truth&apos; seems to be uncertain in many cases. Evaluating — or even developing — linguistic analysers seems to be on uncertain ground if the goal of these analysers cannot be satisfactorily specified. Arguments concerning the magnitude of this problem have been made especially in relation to attempt to automatically assign lexically and contextually correct morphological descriptors (tags) to words. A pessimistic view is taken by Church (1992) who argues that even after negotiations of the kind described above, no consensus can be reached about the correct analysis of several percent of all word tokens in the text. A more mixed view on the matter is taken by Marcus et al. (1993) who on the one hand note that in one experiment moderately trained human text annotators made different analyses even after in over of words, and on the other hand argue that an expert can do much better. An optimistic view on the matter has been presented by Eyes and Leech (1993). Empirical evidence for a high agreement rate is reported by Voutilainen and Jarvinen (1995). Their results suggest that at least with one grammatical representation, namely the ENGCG tag set (cf. Karlsson et al., eds., 1995), a 100% consistency can be 204 Proceedings of EACL &apos;99 reached after negotiations at the level of parts of speech (or morphology in this case). In short, reasonable evidence has been given for the position that at least some tag sets can be applied consistently, i.e. earlier observations about potentially more problematic tag sets should not be taken as predictions about all tag sets. Open Admittedly Voutilainen and Jarvinen&apos;s experiment provides evidence for the possibility that two highly experienced linguists, one of them a developer of the ENGCG tag set, can apply the tag set consistently, at least when compared with each others&apos; performance. However, the practical significance of their result seems questionable, for two reasons. Firstly, large-scale corpus annotation by hand is generally a work that is carried out by less experienced linguists, quite typically advanced students hired as project workers. Voutilainen and Jarvinen&apos;s experiment leaves open the question, how consistently the ENGCG tag set can be applied by a less experienced annotator. Secondly, consider the question of tagger evaluation. Because tagger developers presumably tend to learn, perhaps partly subconsciously, much about the behaviour, desired or otherwise, of the tagger, it may well be that if the developers also annotate the benchmark corpus used for evaluating the tagger, some of the tagger&apos;s misanalyses remain undetected because the tagger developers, due to their subconscious mimicking of their tagger, make the same misanalyses when annotating the benchmark corpus. So 100% tagging consistency in the benchmark corpus alone does not necessarily suffice for getting an objective view of the tagger&apos;s performance. Subconscious &apos;bad&apos; habits of this type need to be factored out. One way to do this is having the benchmark corpus consistently (i.e. with approximately 100% consensus about the correct analysis) analysed by people with no familiarity with the tagger&apos;s behaviour in different situations — provided this is possible in the first place. Another two minor questions left open by Vouand Jarvinen concern the of differences and reliability of their experiment. Concerning the typology of the differences: in Voutilainen and Jarvinen&apos;s experiment the linguists negotiated about an initial difference, alone per cent of all words in texts. they finally agreed about the correct analysis in almost all these differences, with a slight improvement in the experimental setting a clear categorisation of the initial differences into unintentional mistakes and other, more interesting types, could have been made. Secondly, the texts used in Voutilainen and Jarvinen&apos;s experiment comprised only about 6,000 words. This is probably enough to give a general indication of the nature of the analysis task with the ENGCG tag set, but a larger data would increase the reliability of the experiment. In this paper, we address all these three ques- Two young with no background in ENGCG tagging were hired for making an elaborated version of the Voutilainen and Jarvinen experiment with a considerably larger corpus. The rest of this paper is structured as follows. Next, the ENGCG tag set is described in outline. Then the training of the new linguists is described, as well as the test data and experimental setting. Finally, the results are presented. 2 ENGCG tag set Descriptions of the morphological tags used by the English Constraint Grammar tagger are available in several publications. Brief descriptions can found in several recent ACL conference pro- Voutilainen and his colleagues (e.g. EACL93, ANLP94, EACL95, ANLP97, ACL- EACL97). An in-depth description is given in Karlsson et al., eds., 1995 (chapters 3-6). Here, only a brief sample is given. ENGCG tagging is a two-phase process. First, a lexical analyser assigns one or more alternative analyses to each word. The following is a moranalysis of the sentence raids were coordinated under a recently expanded federal program: &amp;quot;&lt;The&gt;&amp;quot; &amp;quot;the&amp;quot; &lt;Def&gt; DET CENTRAL ART SG/PL &amp;quot;&lt;raids&gt;&amp;quot; &amp;quot;raid&amp;quot; &lt;Count&gt; N NOM PL &amp;quot;raid&amp;quot; &lt;SVO&gt; V PRES SG3 &amp;quot;&lt;were&gt;&amp;quot; &amp;quot;be&amp;quot; &lt;SVC/A&gt; &lt;SVC/N&gt; V PAST &amp;quot;&lt;coordinated&gt;&amp;quot; &amp;quot;coordinate&amp;quot; &lt;SVO&gt; EN &amp;quot;coordinate&amp;quot; &lt;SVO&gt; V PAST &amp;quot;&lt;under&gt;&amp;quot; &amp;quot;under&amp;quot; ADV ADVL &amp;quot;under&amp;quot; PREP &amp;quot;under&amp;quot; &lt;Attr&gt; A ABS &amp;quot;&lt;a&gt;&amp;quot; &amp;quot;a&amp;quot; ABBR NOM SG &amp;quot;a&amp;quot; &lt;Indef&gt; DET CENTRAL ART SG Pirldco Paljakka and Mr. Markku Lappalainen 205 Proceedings of EACL &apos;99 1. The text was morphologically analysed using the ENGCG morphological analyser. For the analysis of unrecognised words, we used rule-based heuristic component that assigns morphological analyses, one or more, to each represented in the lexicon of the system. Of the analysed text, two identical versions were made, one for each linguist. 2. Two linguists trained to disambiguate the ENGCG morphological representation (see the subsection on training below) independently marked the correct alternative analyses in the ambiguous input, using mainly structural, but in some structurally unresolvable cases also higher-level, information. The corpora consisted of continuous text rather than isolated sentences; this made the use of textual knowledge possible in the selection of the correct alternative. In the rare cases where two analyses were regarded as equally legitimate, both could be masked. The judges were encouraged to consult the documentation of the grammatical representation. In addition, both linguists were provided with a checking program to be used after the text was analysed. The program identifies words left without an analysis, in which case the linguist was to provide the missing analysis. 3. These analysed versions of the same text were compared to each other using the Unix sdiff program. For each corpus version, words with a different analysis were marked with a &amp;quot;RE- CONSIDER&amp;quot; symbol. The &amp;quot;RECONSIDER&amp;quot; symbol was also added to a number of other ambiguous words in the corpus. These additional words were marked in order to &apos;force&apos; linguist to about the correct analysis, i.e. to prevent the emergence of the situation where one linguist considers the other to be always right (or wrong) and so &apos;reconsiders&apos; only in terms of the existing analysis. The linguists were told that some of the words marked with the &amp;quot;RECON- SIDER&amp;quot; symbol were analysed differently by them. 4. Statistics were generated about the number of differing analyses (number of &amp;quot;RE- CONSIDER&amp;quot; symbols) in the corpus versions (&amp;quot;difFl&amp;quot; in the following table). 5. The reanalysed versions were automatically compared to each other. To words with a different analysis, a &amp;quot;NEGOTIATE&amp;quot; symbol was added. &amp;quot;&lt;recently&gt;&amp;quot; &amp;quot;recent&amp;quot; &lt;DER:ly&gt; ADV &amp;quot;&lt;expanded&gt;&amp;quot; &amp;quot;expand&amp;quot; &lt;SVO&gt; &lt;Pion&gt; &amp;quot;expand&amp;quot; &lt;SVO&gt; &lt;Pion&gt; &amp;quot;&lt;federal&gt;&amp;quot; &amp;quot;federal&amp;quot; A ABS &amp;quot;&lt;program&gt;&amp;quot; &amp;quot;program&amp;quot; N NOM SG &amp;quot;program&amp;quot; &lt;SVO&gt; V PRES -SG3 &amp;quot;program&amp;quot; &lt;SVO&gt; V INF &amp;quot;program&amp;quot; &lt;SVO&gt; V IMP &amp;quot;program&amp;quot; &lt;SVO&gt; V SUBJUNCTIVE Each indented line constitutes one morphologianalysis. Thus five-ways ambiguous after ENGCG morphology. The disambiguation of the ENGCG then removes those alternative analyses that are contextually illegitimate according to the tagger&apos;s hand-coded constraint rules (Voutilainen 1995). The remaining analyses constitute the output of the tagger, in this case: &amp;quot;&lt;The&gt;&amp;quot; &amp;quot;the&amp;quot; &lt;Def&gt; DET CENTRAL ART SG/PL &amp;quot;&lt;raids&gt;&amp;quot; &amp;quot;raid&amp;quot; &lt;Count&gt; N NOM PL &amp;quot;&lt;were&gt;&amp;quot; &amp;quot;be&amp;quot; &lt;SVC/A&gt; &lt;SVC/N&gt; V PAST &amp;quot;&lt;coordinated&gt;&amp;quot; &amp;quot;coordinate&amp;quot; &lt;SVO&gt; EN &amp;quot;&lt;under&gt;&amp;quot; &amp;quot;under&amp;quot; PREP &amp;quot;&lt;a&gt;&amp;quot; &amp;quot;a&amp;quot; &lt;Indef&gt; DET CENTRAL ART SG &amp;quot;&lt;recently&gt;&amp;quot; &amp;quot;recent&amp;quot; &lt;DER:ly&gt; ADV &amp;quot;&lt;expanded&gt;&amp;quot; &amp;quot;expand&amp;quot; &lt;SVO&gt; &lt;P/on&gt; EN &amp;quot;&lt;federal&gt;&amp;quot; &amp;quot;federal&amp;quot; A ABS &amp;quot;&lt;program&gt;&amp;quot; &amp;quot;program&amp;quot; N NOM SG • Overall, this tag set represents about 180 different analyses when certain optional auxiliary tags (e.g. verb subcategorisation tags) are ignored. 3 Preparations for the experiment 3.1 Experimental setting The experiment was conducted as follows. new tagger, known as EngCG-2, can be studied and tested at http://www.conexor.fi. 206 Proceedings of EACL &apos;99 6. Statistics were generated about the number of differing analyses (number of &amp;quot;NE- GOTIATE&amp;quot; symbols) in the corpus versions (&amp;quot;diff2&amp;quot; in the following table). 7. The remaining differences in the analyses were jointly examined by the linguists in order to see whether they were due to (i) inattention on the part of one linguist (as a result of which a correct unique analysis was jointly agreed upon), (ii) joint uncertainty about the correct analysis (both linguists feel unsure about the correct analysis), or (iii) conflicting opinions about the correct analysis (both linguists have a strong but different opinion about the correct analysis). 8. Statistics were generated about the number of conflicting opinions (&amp;quot;diff3&amp;quot; below) and joint uncertainty (&amp;quot;unsure&amp;quot; below). This routine was successively applied to each text. 3.2 Training Two people were hired for the experiment. One had recently completed a Master&apos;s degree from English Philology. The other was an advanced undergraduate student majoring in English Philology. Neither of them were familiar with the ENGCG tagger. All available documentation about the linguistic representation used by ENGCG was made available to them. The chief source was chapters 3-6 in Karlsson et al. (eds., 1995). Because the linguistic solutions in ENGCG are largely based on the comprehensive descriptive grammar by Quirk et al. (1985), also that work was made available to them, as well as a number of modern English dictionaries. The training was based on the disambiguation of ten smallish text extracts. Each of the extracts was first analysed by the ENGCG morphological analyser, and then each trainee was to independently perform Step 3 (see the previous subsection) on it. The disambiguated text was then automatically compared to another version of the same extract that was disambiguated by an expert on ENGCG. The ENGCG expert then discussed the analytic differences with the trainee who had also disambiguated the text and explained why the expert&apos;s analysis was correct (almost always by identifying a relevant section in the available ENGCG documentation; in very rare cases where the documentation was underspecific, new documentation was created for future use in the experiments). After analysis and subsequent consultation with the ENGCG expert, the trainee processed the following sample. The training lasted about 30 hours. It was concluded by familiarising the linguists with the routine used in the experiment. 3.3 Test corpus Four texts were used in the experiment, totalling 55724 words and 102527 morphological analyses (an average of 1.84 analyses per word). One was an article about Japanese culture (`Pop&apos;); one concerned patents (&apos;Pat&apos;); one contained excerpts from the law of California; one was a medical text (`Med&apos;). None of them had been used in the development of the ENGCG grammatical representation or other parts of the system. By mid-June 1999, a sample of this data will be available for inspection at http://www.ling.helsinki.fi/ voutilai/eac199data.html. 4 Results and discussion The following table presents the main findings. Figure 1: .Results from a human annotation task. It is interesting to note how high the agreement between the linguists is even before the first negotiations (99.80% of all words are analysed identically). Of the remaining differences, most, somewhat disappointingly, turned out to be classified as &apos;slips of attention&apos;; upon inspection they seemed to contain little linguistic interest. Especially one of the linguists admitted that most of the job seemed too much of a routine to keep one mentally alert enough. The number of genuine conflicts of opinion were much in line with observations by Voutilainen and Jarvinen. However, the negotiations were not altogether easy, considering that in all they took almost nine hours. Presumably uncertain analyses and conflicts of opinion were not easily passed by. The main finding of this experiment is that basically Voutilainen and Jarvinen&apos;s observations about the high specifiability and consistent usability of the ENGCG morphological tag set seem to be extendable to new users of the tag set. In words diffl diff2 diff3 unsure</abstract>
<note confidence="0.827882428571429">Pop 14861 188/1.3% 44/.3% 0 4/.0% Pat 13183 92/.7% 11/.1% 2/.0% 1/.0% Law 15495 107/.7% 18/.1% 10/.1% 0 Med 12185 126/1.0% 39/.3% 1/.0% 9/.1% ALL 55724 513/.9% 112/.2% 13/.0% 14/.0% 207 Proceedings of EACL &apos;99</note>
<abstract confidence="0.952444214285714">other words, the reputedly surface-syntactic tag set seems to be learnable as well. Overall, the experiment reported here provides evidence for the optimistic position about the specifiability of at least certain kinds of linguistic representations. It remains for future research, perhaps as a collaboration between teams working with different tag sets, to find out, what exactly are the properties that make some linguistic representations consistently learnable and usable, and others less SO. Acknowledgments I am grateful to anonymous EACL99 referees for useful comments.</abstract>
<note confidence="0.630620181818182">References Kenneth W. Church 1992. Current Practice in Part of Speech Tagging and Suggestions for the In Simmons (ed.), praci: In of Slavic Studies. Michigan. 13-48. Elizabeth Eyes and Geoffrey Leech 1993. Syntactic Annotation: Linguistic Aspects of Grammatical Tagging and Skeleton Parsing. In Ezra Black, Roger Garside and Geoffrey Leech (eds.) Computer Grammars English: The IBM/Lancaster Approach. Amsterdam and Atlanta: Rodopi. 36-61. Fred Karlsson, Atro Voutilainen, Julia Heikkila Arto Anttila (eds.) 1995. Grammar. A Language-Independent System for Pars- Unrestricted Text. and New York: Mouton de Gruyter. Mitchell Marcus, Beatrice Santorini and Mary Ann Marcinkiewicz 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Linguistics 313-330. Randolph Quirk, Sidney Greenbaum, Jan and Geoffrey Leech 1985. A Comprehenthe English Language. Atro Voutilainen 1995. Morphological disambiguation. In Karlsson et al., eds. Atro Voutilainen and Timo Jarvinen 1995. Specifying a shallow grammatical representation parsing purposes. In of the Seventh Conference of the European Chapter of the for Computational Linguistics. 208</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
</authors>
<title>Current Practice in Part of Speech Tagging and Suggestions for the Future.</title>
<date>1992</date>
<booktitle>Sbornik praci: In Honor of Henry Kucera, Michigan Slavic Studies. Michigan.</booktitle>
<pages>13--48</pages>
<editor>In Simmons (ed.),</editor>
<contexts>
<context position="3053" citStr="Church (1992)" startWordPosition="455" endWordPosition="456">iffer about which of the competing analyses is the correct one, i.e. sometimes the grammatical representation is used inconsistently. In short, linguistic &apos;truth&apos; seems to be uncertain in many cases. Evaluating — or even developing — linguistic analysers seems to be on uncertain ground if the goal of these analysers cannot be satisfactorily specified. Arguments concerning the magnitude of this problem have been made especially in relation to tagging, the attempt to automatically assign lexically and contextually correct morphological descriptors (tags) to words. A pessimistic view is taken by Church (1992) who argues that even after negotiations of the kind described above, no consensus can be reached about the correct analysis of several percent of all word tokens in the text. A more mixed view on the matter is taken by Marcus et al. (1993) who on the one hand note that in one experiment moderately trained human text annotators made different analyses even after negotiations in over 3% of all words, and on the other hand argue that an expert can do much better. An optimistic view on the matter has been presented by Eyes and Leech (1993). Empirical evidence for a high agreement rate is reported</context>
</contexts>
<marker>Church, 1992</marker>
<rawString>Kenneth W. Church 1992. Current Practice in Part of Speech Tagging and Suggestions for the Future. In Simmons (ed.), Sbornik praci: In Honor of Henry Kucera, Michigan Slavic Studies. Michigan. 13-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Eyes</author>
<author>Geoffrey Leech</author>
</authors>
<title>Syntactic Annotation: Linguistic Aspects of Grammatical Tagging and Skeleton Parsing.</title>
<date>1993</date>
<booktitle>Statistically-Driven Computer Grammars of English: The IBM/Lancaster Approach. Amsterdam and Atlanta: Rodopi.</booktitle>
<pages>36--61</pages>
<editor>In Ezra Black, Roger Garside and Geoffrey Leech (eds.)</editor>
<contexts>
<context position="3595" citStr="Eyes and Leech (1993)" startWordPosition="555" endWordPosition="558">gical descriptors (tags) to words. A pessimistic view is taken by Church (1992) who argues that even after negotiations of the kind described above, no consensus can be reached about the correct analysis of several percent of all word tokens in the text. A more mixed view on the matter is taken by Marcus et al. (1993) who on the one hand note that in one experiment moderately trained human text annotators made different analyses even after negotiations in over 3% of all words, and on the other hand argue that an expert can do much better. An optimistic view on the matter has been presented by Eyes and Leech (1993). Empirical evidence for a high agreement rate is reported by Voutilainen and Jarvinen (1995). Their results suggest that at least with one grammatical representation, namely the ENGCG tag set (cf. Karlsson et al., eds., 1995), a 100% consistency can be 204 Proceedings of EACL &apos;99 reached after negotiations at the level of parts of speech (or morphology in this case). In short, reasonable evidence has been given for the position that at least some tag sets can be applied consistently, i.e. earlier observations about potentially more problematic tag sets should not be taken as predictions about</context>
</contexts>
<marker>Eyes, Leech, 1993</marker>
<rawString>Elizabeth Eyes and Geoffrey Leech 1993. Syntactic Annotation: Linguistic Aspects of Grammatical Tagging and Skeleton Parsing. In Ezra Black, Roger Garside and Geoffrey Leech (eds.) 1993. Statistically-Driven Computer Grammars of English: The IBM/Lancaster Approach. Amsterdam and Atlanta: Rodopi. 36-61.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Fred Karlsson</author>
</authors>
<title>Atro Voutilainen, Julia Heikkila and Arto Anttila (eds.) 1995. Constraint Grammar. A Language-Independent System for Parsing Unrestricted Text.</title>
<location>Berlin and New York: Mouton</location>
<note>de Gruyter.</note>
<marker>Karlsson, </marker>
<rawString>Fred Karlsson, Atro Voutilainen, Julia Heikkila and Arto Anttila (eds.) 1995. Constraint Grammar. A Language-Independent System for Parsing Unrestricted Text. Berlin and New York: Mouton de Gruyter.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mitchell Marcus</author>
</authors>
<institution>Beatrice Santorini and Mary</institution>
<marker>Marcus, </marker>
<rawString>Mitchell Marcus, Beatrice Santorini and Mary</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<pages>313--330</pages>
<marker>Marcinkiewicz, 1993</marker>
<rawString>Ann Marcinkiewicz 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics 19:2. 313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randolph Quirk</author>
<author>Sidney Greenbaum</author>
<author>Jan Svartvik</author>
<author>Geoffrey Leech</author>
</authors>
<date>1985</date>
<journal>A Comprehensive Grammar of the English Language. Longman.</journal>
<contexts>
<context position="13294" citStr="Quirk et al. (1985)" startWordPosition="2096" endWordPosition="2099">nsure&amp;quot; below). This routine was successively applied to each text. 3.2 Training Two people were hired for the experiment. One had recently completed a Master&apos;s degree from English Philology. The other was an advanced undergraduate student majoring in English Philology. Neither of them were familiar with the ENGCG tagger. All available documentation about the linguistic representation used by ENGCG was made available to them. The chief source was chapters 3-6 in Karlsson et al. (eds., 1995). Because the linguistic solutions in ENGCG are largely based on the comprehensive descriptive grammar by Quirk et al. (1985), also that work was made available to them, as well as a number of modern English dictionaries. The training was based on the disambiguation of ten smallish text extracts. Each of the extracts was first analysed by the ENGCG morphological analyser, and then each trainee was to independently perform Step 3 (see the previous subsection) on it. The disambiguated text was then automatically compared to another version of the same extract that was disambiguated by an expert on ENGCG. The ENGCG expert then discussed the analytic differences with the trainee who had also disambiguated the text and e</context>
</contexts>
<marker>Quirk, Greenbaum, Svartvik, Leech, 1985</marker>
<rawString>Randolph Quirk, Sidney Greenbaum, Jan Svartvik and Geoffrey Leech 1985. A Comprehensive Grammar of the English Language. Longman.</rawString>
</citation>
<citation valid="false">
<title>Atro Voutilainen 1995. Morphological disambiguation.</title>
<editor>In Karlsson et al., eds.</editor>
<marker></marker>
<rawString>Atro Voutilainen 1995. Morphological disambiguation. In Karlsson et al., eds.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atro Voutilainen</author>
<author>Timo Jarvinen</author>
</authors>
<title>Specifying a shallow grammatical representation for parsing purposes.</title>
<date>1995</date>
<booktitle>In Proceedings of the Seventh Conference of the European Chapter of the Association for Computational Linguistics. ACL.</booktitle>
<contexts>
<context position="3688" citStr="Voutilainen and Jarvinen (1995)" startWordPosition="570" endWordPosition="573"> argues that even after negotiations of the kind described above, no consensus can be reached about the correct analysis of several percent of all word tokens in the text. A more mixed view on the matter is taken by Marcus et al. (1993) who on the one hand note that in one experiment moderately trained human text annotators made different analyses even after negotiations in over 3% of all words, and on the other hand argue that an expert can do much better. An optimistic view on the matter has been presented by Eyes and Leech (1993). Empirical evidence for a high agreement rate is reported by Voutilainen and Jarvinen (1995). Their results suggest that at least with one grammatical representation, namely the ENGCG tag set (cf. Karlsson et al., eds., 1995), a 100% consistency can be 204 Proceedings of EACL &apos;99 reached after negotiations at the level of parts of speech (or morphology in this case). In short, reasonable evidence has been given for the position that at least some tag sets can be applied consistently, i.e. earlier observations about potentially more problematic tag sets should not be taken as predictions about all tag sets. 1.1 Open questions Admittedly Voutilainen and Jarvinen&apos;s experiment provides e</context>
</contexts>
<marker>Voutilainen, Jarvinen, 1995</marker>
<rawString>Atro Voutilainen and Timo Jarvinen 1995. Specifying a shallow grammatical representation for parsing purposes. In Proceedings of the Seventh Conference of the European Chapter of the Association for Computational Linguistics. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>