<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.9734345">
Using Information about Multi-word Expressions
for the Word-Alignment Task
</title>
<author confidence="0.966879">
Sriram Venkatapathyl
</author>
<affiliation confidence="0.90977875">
Language Technologies Research Center,
Indian Institute of
Information Technology,
Hyderabad, India.
</affiliation>
<email confidence="0.994462">
sriramv@linc.cis.upenn.edu
</email>
<author confidence="0.945594">
Aravind K. Joshi
</author>
<affiliation confidence="0.99389675">
Department of Computer and
Information Science and Institute for
Research in Cognitive Science,
University of Pennsylvania, PA, USA.
</affiliation>
<email confidence="0.996846">
joshi@linc.cis.upenn.edu
</email>
<sectionHeader confidence="0.997373" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997798">
It is well known that multi-word expres-
sions are problematic in natural language
processing. In previous literature, it has
been suggested that information about
their degree of compositionality can be
helpful in various applications but it has
not been proven empirically. In this pa-
per, we propose a framework in which
information about the multi-word expres-
sions can be used in the word-alignment
task. We have shown that even simple
features like point-wise mutual informa-
tion are useful for word-alignment task in
English-Hindi parallel corpora. The align-
ment error rate which we achieve (AER =
0.5040) is significantly better (about 10%
decrease in AER) than the alignment error
rates of the state-of-art models (Och and
Ney, 2003) (Best AER = 0.5518) on the
English-Hindi dataset.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999829272727273">
In this paper, we show that measures representing
compositionality of multi-word expressions can
be useful for tasks such as Machine Translation,
word-alignment to be specific here. We use an on-
line learning framework called MIRA (McDon-
ald et al., 2005; Crammer and Singer, 2003) for
training a discriminative model for the word align-
ment task (Taskar et al., 2005; Moore, 2005). The
discriminative model makes use of features which
represent the compositionality of multi-word ex-
pressions.
</bodyText>
<footnote confidence="0.5915875">
1At present visiting Institute for Research in Cognitive
Science, University of Pennsylvania, PA, USA.
</footnote>
<bodyText confidence="0.998538394736842">
Multi-word expressions (MWEs) are those
whose structure and meaning cannot be derived
from their component words, as they occur inde-
pendently. Examples include conjunctions such
as ‘as well as’ (meaning ‘including’), idioms like
‘kick the bucket’ (meaning ‘die’) phrasal verbs
such as ‘find out’ (meaning ‘search’) and com-
pounds like ‘village community’. They can be de-
fined roughly as idiosyncratic interpretations that
cross word boundaries (Sag et al., 2002).
A large number of MWEs have standard
syntactic structure but are semantically non-
compositional. Here, we consider the class of verb
based expressions (verb is the head of the phrase),
which occur very frequently. This class of verb
based multi-word expressions include verbal id-
ioms, support-verb constructions, among others.
The example ‘take place’ is a MWE but ‘take a
gift’ is not.
In the past, various measures have been sug-
gested for measuring the compositionality of
multi-word expressions. Some of these are mu-
tual information (Church and Hanks, 1989), dis-
tributed frequency (Tapanainen et al., 1998) and
Latent Semantic Analysis (LSA) model (Baldwin
et al., 2003). Even though, these measures have
been shown to represent compositionality quite
well, compositionality itself has not been shown to
be useful in any application yet. In this paper, we
explore this possibility of using the information
about compositionality of MWEs (verb based) for
the word alignment task. In this preliminary work,
we use simple measures (such as point-wise mu-
tual information) to measure compositionality.
The paper is organized as follows. In section 2,
we discuss the word-alignment task with respect
to the class of multi-word expressions of interest
in this paper. In section 3, we show empirically,
</bodyText>
<page confidence="0.938334">
20
</page>
<note confidence="0.691335">
Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 20–27,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999808222222222">
the behavior of verb based expressions in a paral-
lel corpus (English-Hindi in our case). We then
discuss our alignment algorithm in section 4. In
section 5, we describe the features which we have
used in our training model. Section 6 discusses the
training algorithm and in section 7, the results of
our discriminative model for the word alignment
task. Related work and conclusion follow in sec-
tion 8 and 9 respectively.
</bodyText>
<sectionHeader confidence="0.949192" genericHeader="introduction">
2 Task: Word alignment of verbs and
their dependents
</sectionHeader>
<bodyText confidence="0.999530875">
The task is to align the verbs and their dependents
(arguments and adjuncts) in the source language
sentence (English) with words in the target lan-
guage sentence (Hindi). The dependents of the
verbs in the source sentence are represented by
their head words. Figure 1. shows an example
of the type of multi-word expressions which we
consider for alignment.
</bodyText>
<figure confidence="0.94945225">
took
event place Philadelphia
The cycling
(The cycling event took place in Philadelphia)
</figure>
<figureCaption confidence="0.999985">
Figure 1: Example of MWEs we consider
</figureCaption>
<bodyText confidence="0.999890882352941">
In the above example, the goal will the to align
the words ‘took’, ‘event’, ‘place’ and ‘Philadel-
phia’ with corresponding word(s) in the target lan-
guage sentence (which is not parsed) using a dis-
criminative approach. The advantage in using the
discriminative approach for alignment is that it lets
you use various compositionality based features
which are crucial towards aligning these expres-
sions. Figure 2. shows the appropriate alignment
of the expression in Figure 1. with the words in the
target language. The pair (take place), in English,
a verb and one of its dependents is aligned with a
single verbal unit in Hindi.
It is essential to obtain the syntactic roles for de-
pendents in the source language sentence as they
are required for computing the compositionality
value between the dependents and their verbs. The
</bodyText>
<figure confidence="0.529857">
took
Philadelphia mein saikling kii pratiyogitaa hui
</figure>
<figureCaption confidence="0.999203">
Figure 2: Alignment of Verb based expression
</figureCaption>
<bodyText confidence="0.99978975">
syntactic roles on the source side are obtained by
applying simple rules to the output of a depen-
dency parser. The dependency parser which we
used in our experiments is a stochastic TAG based
dependency parser (Shen, 2006). A sentence
could have one or more verbs. We would like
to align all the expressions represented by those
verbs with words in the target language.
</bodyText>
<sectionHeader confidence="0.811701" genericHeader="method">
3 Behavior of MWEs in parallel corpora
</sectionHeader>
<bodyText confidence="0.999936166666667">
In this section, we will briefly discuss the com-
plexity of the alignment problem based on the
verb based MWE’s. From the word aligned sen-
tence pairs, we compute the fraction of times a
source sentence verb and its dependent are aligned
together with the same word in the target lan-
guage sentence. We count the number of times a
source sentence verb and its dependent are aligned
together with the same word in the target lan-
guage sentence, and divide it by the total num-
ber of dependents. The total size of our word
aligned corpus is 400 sentence pairs which in-
cludes both training and test sentences. The total
number of dependents present in these sentences
are 2209. Total number of verb dependent pairs
which aligned with same word in target language
are 193. Hence, the percentage of such occur-
rences is 9%, which is a significant number.
</bodyText>
<sectionHeader confidence="0.972805" genericHeader="method">
4 Alignment algorithm
</sectionHeader>
<bodyText confidence="0.9988672">
In this section, we describe the algorithm for align-
ing verbs and their dependents in the source lan-
guage sentence with the words in the target lan-
guage. Let V be the number of verbs and A be the
number of dependents. Let the number of words in
</bodyText>
<figure confidence="0.9939413">
obj
subj
prep_in
subj
prep_in
obj
Philadelphia
event
place
The cycling
</figure>
<page confidence="0.999068">
21
</page>
<bodyText confidence="0.9958645">
the target language be N. If we explore all the ways
in which theV+Awords in the source sentence
are aligned with words in the target language be-
fore choosing the best alignment, the total number
ofpossibilites areNV+A. This is computationally
very expensive. Hence, we use a Beam-search al-
gorithm to obtain the K-best alignments.
Our algorithm has three main steps.
</bodyText>
<listItem confidence="0.9092455">
1. Populate the Beam : Use the local features
(which largely capture the co-occurence in-
formation between the source word and the
target word) to determine the K-best align-
ments of verbs and their dependents with
words in the target language.
2. Re-order the Beam: Re-order the above
alignments using more complex features
(which include the global features and the
compositionality based feature(s)).
3. Post-processing : Extend the alignment(s) of
the verb(s) (on the source side) to include
words which can be part of the verbal unit
on the target side.
</listItem>
<bodyText confidence="0.997806">
For a source sentence, let the verbs and depen-
dents be denoted by sij. Here i is the index of
the verb (1 &lt;= i &lt;= V). The variable j is
the index of the dependents (0 &lt;= j &lt;= A)
except when j = 0 which is used to represent
the verb itself. Let the source sentences be de-
noted as S = {sij } and the target sentences by
T = {tn }. The alignment from a source sen-
tence S to target sentence T is defined as the map-
ping a = {aijn I aijn - (sij —� tn), di, j}. A
beam is used to store a set of K-best alignments
between a source sentence and the target sentence.
It is represented using the symbolBwhereBk
(0&lt;=h&lt;=K) is used to refer to a particular
alignment configuration.
</bodyText>
<subsectionHeader confidence="0.999254">
4.1 Populate the Beam
</subsectionHeader>
<bodyText confidence="0.999876363636364">
The task in this step is to obtain the K-best can-
didate alignments using local features. The local
features mainly contain the coccurence informa-
tion between a source and a target word and are in-
dependent of other alignment links or words in the
sentences. Let the local feature vector be denoted
as A(sij, tk). The score of a particular alignment
link is computed by taking the dot product of the
weight vector W with the local feature vector (of
words connected by the alignment link). Hence,
the local score will be
</bodyText>
<equation confidence="0.615129">
seoreL(sij,tk)=W�A(sij,tk)
</equation>
<bodyText confidence="0.99990825">
The total score of an alignment configuration is
computed by adding the scores of individual links
in the alignment configuration. Hence, the align-
ment score will be
</bodyText>
<equation confidence="0.69101">
seoreLa(a,S,T)=1:seoreL(sij,tk)
dsijES&amp;sij—�tkEa
</equation>
<bodyText confidence="0.999761166666667">
We propose an algorithm of order O ((V +
A)Nlog(N) + K) to compute the K-best align-
ment configurations. First, the local scores of each
verb and its dependents are computed for each
word in the target sentence and stored in a lo-
cal beam denoted by bij. The local beams cor-
responding to all the verbs and dependents are
then sorted. This operation has the complexity
(V+A)Nlog(N).
The goal now is to pick the K-best configura-
tions of alignment links. A single slot in the local
beam corresponds to one alignment link. We de-
fine a boundary which partitions each local beam
into two sets of slots. The slots above the bound-
ary represent the slots which have been explored
by the algorithm while slots below the boundary
have still to be explored. The figure 3. shows the
boundary which cuts across the local beams.
</bodyText>
<figure confidence="0.429928">
Boundary
</figure>
<figureCaption confidence="0.995449">
Figure 3: Boundary
</figureCaption>
<bodyText confidence="0.9999535">
We keep on modifying the boundary untill all
the K slots in the Alignment Beam are filled with
the K-best configurations. At the beginning of the
algorithm, the boundary is a straight line passing
through the top of all the local beams. The top slot
of the alignment beam at the beginning represents
</bodyText>
<figure confidence="0.9989916">
b (i,j)
B
Alignment
Beam
Local Beams
</figure>
<page confidence="0.992314">
22
</page>
<bodyText confidence="0.999420315789474">
the combination of alignment links with the best
local scores.
The next slotbij[p](from the set of unexplored
slots) to be included in the boundary is the slot
which has the least difference in score from the
score of the slot at the top of its local beam. That
is, we pick the slot bij [p] such that score (bij [p]) —
score (bij [1]) is the least among all the unexplored
slots (or alignment links). Trivially, bij [p — 1] was
already a part of the boundary.
When the slot bij [p] is included in the boundary,
various configurations, which now contain bij [p],
are added to the alignment beam. The new con-
figurations are the same as the ones which previ-
ously contained bij [p — 1] but with the replace-
ment of bij [p — 1] by bij [p]. The above procedure
ensures that the the alignment configurations are
K-best and are sorted according to the scores ob-
tained using local features.
</bodyText>
<subsectionHeader confidence="0.970128">
4.2 Re-order the beam
</subsectionHeader>
<bodyText confidence="0.973501266666667">
We now use global features to re-order the beam.
The global features look at the properties of the en-
tire alignment configuration instead of alignment
links locally.
The global score is defined as the dot product of
the weight vector and the global feature vector.
scoreG(�d)=W:fG(�d)
The overall score is calculated by adding the local
score and the global score.
score(�d)=scoreLa(�d)+scoreG(�d)
The beam is now sorted based on the overall
scores of each alignment. The alignment config-
uration at the top of the beam is the best possible
alignment between source sentence and the target
sentence.
</bodyText>
<subsectionHeader confidence="0.999699">
4.3 Post-processing
</subsectionHeader>
<bodyText confidence="0.999964818181818">
The first two steps in our alignment algorithm
compute alignments such that one verb or depen-
dent in the source language side is aligned with
only one word in the target side. But, in the case
of compound verbs in Hindi, the verb in English is
aligned to all the words which represent the com-
pound verb in Hindi. For example, in Figure 3, the
verb “lost” is aligned to both ’khoo’ and ’dii’.
Our alignment algorithm would have aligned
“lost” only to ’khoo’. Hence, we look at the win-
dow of words after the word which is aligned to
</bodyText>
<figureCaption confidence="0.99786">
Figure 4: Case of compound verb in Hindi
</figureCaption>
<bodyText confidence="0.99955775">
the source verb and check if any of them is a verb
which has not been aligned with any word in the
source sentence. If this condition is satisfied, we
align the source verb to these words too.
</bodyText>
<sectionHeader confidence="0.998931" genericHeader="method">
5 Parameters
</sectionHeader>
<bodyText confidence="0.9999825">
As the number of training examples (294 sen-
tences) is small, we choose to use very representa-
tive features. Some of the features which we used
in this experiment are as follows,
</bodyText>
<subsectionHeader confidence="0.971877">
5.1 Local features (FL)
</subsectionHeader>
<bodyText confidence="0.998818">
The local features which we consider are mainly
co-occurence features. These features estimate the
likelihood of a source word aligning to a target
word based on the co-occurence information ob-
tained from a large sentence aligned corpora1.
</bodyText>
<listItem confidence="0.9992125">
1. DiceWords: Dice Coefficient of the source
word and the target word
</listItem>
<equation confidence="0.73954">
DCoeff (sij; tk) = Count(sij) + Count(tk)
</equation>
<bodyText confidence="0.975273">
where Count(sij; tk) is the number of times
the word tk was present in the translation of
sentences containing the word sij in the par-
allel corpus.
</bodyText>
<listItem confidence="0.986357222222222">
2. DiceRoots: Dice Coefficient of the lemma-
tized forms of the source and target words.
It is important to consider this feature be-
cause the English-Hindi parallel corpus is not
large and co-occurence information can be
learnt effectively only after we lemmatize the
words.
3. Dict: Whether there exists a dictionary entry
from the source wordsijto the target word
</listItem>
<footnote confidence="0.5326515">
150K sentence pairs originally collected as part of TIDES
MT project and later refined at IIIT-Hyderabad, India.
</footnote>
<figure confidence="0.9925865">
lost
I
book
Shyam’s
mainee Shyam ki kitaaba khoo dii
2*Count(sij;tk)
</figure>
<page confidence="0.968392">
23
</page>
<listItem confidence="0.97705475">
tk. For English-Hindi, we used a dictionary
available at IIIT - Hyderabad, India.
4. Null: Whether the source words2jis aligned
to nothing in the target language.
</listItem>
<subsectionHeader confidence="0.992769">
5.2 Global features
</subsectionHeader>
<bodyText confidence="0.990744146341463">
The following are the four global features which
we have considered,
•AvgDist: The average distance between the
words in the target language sentence which
are aligned to the verbs in the source lan-
guage sentence . AvgDist is then normalized
by dividing itself by the number of words in
the target language sentence. If the average
distance is small, it means that the verbs in
the source language sentence are aligned with
words in the target language sentence which
are located at relatively close distances, rela-
tive to the length of the target language sen-
tence.
This feature expresses the distribution of
predicates in the target language.
•Overlap: This feature stores the count of
pairs of verbs in the source language sentence
which align with the same word in the target
language sentence. Overlap is normalized by
dividing itself by the total pairs of verbs.
This feature is used to discourage overlaps
among the words which are alignments of
verbs in the source language sentence.
•MergePos: This feature can be considered as
a compositionality based feature. The part
of speech tag of a dependent is essential to
determine the likelihood of the dependent to
align with the same word in the target lan-
guage sentence as the word to which its verb
is aligned.
This binary feature is active when the align-
ment links of a dependent and its verb
merge. For example, in Figure 5., the feature
‘merge RP’ will be active (that is, merge RP
= 1).
•MergeMI: This is a compositionality based
feature which associates point-wise mutual
information (apart from the POS informa-
tion) with the cases where the dependents
which have the same alignment in the target
</bodyText>
<figure confidence="0.89586225">
ran/V
away/RP
He/N
vaha bhaaga gayaa
</figure>
<figureCaption confidence="0.999912">
Figure 5: Example of MergePos feature
</figureCaption>
<bodyText confidence="0.999942214285714">
language as their verbs. This features which
notes the the compositionality value (repre-
sented by point-wise mutual information in
our experiments) is active if the alignment
links of dependent and its verb merge.
The mutual information (MI) is classified
into three groups depending on its absolute
value. If the absolute value of mutual infor-
mation rounded to nearest integer is in the
range 0-2, it is considered LOW. If the value
is in the range 3-5, it is considered MEDIUM
and if it is above 5, it is considered HIGH.
The feature “merge RP HIGH” is active in
the example shown in figure 6.
</bodyText>
<equation confidence="0.6743622">
ran/V
MI = HIGH
away/RP
He/N
vaha bhaaga gayaa
</equation>
<figureCaption confidence="0.998073">
Figure 6: Example of MergeMI feature
</figureCaption>
<sectionHeader confidence="0.995532" genericHeader="method">
6 Online large margin training
</sectionHeader>
<bodyText confidence="0.999326333333333">
For parameter optimization, we have used an on-
line large margin algorithm called MIRA (Mc-
Donald et al., 2005) (Crammer and Singer, 2003).
We describe the training algorithm that we used
very briefly. Our training set is a set of English-
Hindi word aligned parallel corpus. We get the
verb based expressions in English by running a de-
pendency parser (Shen, 2006). Let the number of
sentence pairs in the training data bem. We have
</bodyText>
<page confidence="0.997196">
24
</page>
<bodyText confidence="0.998412222222222">
fSq, Tq, ^aqg for training where q &lt;= m is the in-
dex number of the sentence pair f Sq, Tq g in the
training set and ^aq is the gold alignment for the
pair f Sq, Tq g. Let W be the weight vector which
has to be learnt, Wi be the weight vector after the
end of ith update. To avoid over-fitting, W is ob-
tained by averaging over all the weight vectors Wi.
A generic large margin algorithm is defined fol-
lows for the training instancesfSq,Tq,^aqg,
</bodyText>
<listItem confidence="0.877041363636364">
1. InitializeW0,W,i
2. for p:1 to NIterations
3. for q:1 to m
4. Get K-Best predictionsaq=fa1,a2...akg
for the training example(Sq,Tq,^aq)using
the current modelWiand applying step
1 and 2 of section 4. Compute Wi+1 by
updating Wi based on (Sq, Tq, a^q, aq).
5. i = i + 1
6. W=W+Wi+1
7.W= WNIterations*m
</listItem>
<bodyText confidence="0.999736785714286">
The goal of MIRA is to minimize the change in
Wi such that the score of the gold alignment ^a ex-
ceeds the score of each of the predictions in a by a
margin which is equal to the number of mistakes in
the predictions when compared to gold alignment.
While computing the number of mistakes, the mis-
takes due to the mis-alignment of head verb could
be given greater weight, thus prompting the opti-
mization algorithm to give greater importance to
verb related mistakes and thereby improving over-
all performance.
Step 4 in the algorithm mentioned above can
be substituted by the following optimization
problem,
</bodyText>
<equation confidence="0.927110666666667">
minimize k (Wi+1 — Wi)
s.t. 8k, seore(^aq, Sq, Tq) — seore(aq,k, Sq, Tq)
&gt;=Mistakes(ak,^aq,Sq,Tq)
</equation>
<bodyText confidence="0.999176272727273">
The above optimization problem is converted to
the Dual form using one Lagrangian multiplier for
each constraint. In the Dual form, the Lagrangian
multipliers are solved using Hildreth’s algorithm.
Here, prediction ofais similar to the prediction
ofK—bestclasses in a multi-class classification
problem. Ideally, we need to consider all the possi-
ble classes and assign margin constraints based on
every class. But, here the number of such classes
is exponential and thus we restrict ourselves to the
K—bestclasses.
</bodyText>
<sectionHeader confidence="0.997297" genericHeader="method">
7 Results on word-alignment task
</sectionHeader>
<subsectionHeader confidence="0.969386">
7.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999987388888889">
We have divided the 400 word aligned sentence
pairs into a training set consisting of 294 sen-
tence pairs and a test set consisting of 106 sentence
pairs. The source sentences are all dependency
parsed (Shen, 2006) and only the verb and its de-
pendents are considered for both training and test-
ing our algorithm. Our training algorithm requires
that the each of the source words is aligned to only
one or zero target words. For this, we use simple
heuristics to convert the training data to the appro-
priate format. For the words aligned to a source
verb, the first verb is chosen as the gold alignment.
For the words aligned to any dependent which is
not a verb, the last content word is chosen as the
alignment link. For test data, we do not make any
modifications and the final output from our align-
ment algorithm is compared with the original test
data.
</bodyText>
<subsectionHeader confidence="0.999248">
7.2 Experiments with Giza
</subsectionHeader>
<bodyText confidence="0.979901846153846">
We evaluated our discriminative approach by com-
paring it with the state-of-art Giza++ alignments
(Och and Ney, 2003). The metric that we have
used to do the comparison is the Alignment Error
Rate (AER). The results shown below also contain
Precision, Recall and F-measure.
Giza was trained using an English-Hindi
aligned corpus of 50000 sentence pairs. In Table
1., we report the results of the GIZA++ alignments
run from both the directions (English to Hindi and
Hindi to English). We also show the results of the
intersected model. See Table 1. for the results of
the GIZA++ alignments.
</bodyText>
<table confidence="0.999594">
Prec. Recall F-meas. AER
Eng Hin 0.45 0.38 0.41 0.5874
Hin Eng 0.46 0.27 0.34 0.6584
Intersected 0.82 0.19 0.31 0.6892
</table>
<tableCaption confidence="0.999945">
Table 1: Results of GIZA++ - Original dataset
</tableCaption>
<bodyText confidence="0.995853333333333">
We then lemmatize the words in both the source
and target sides of the parallel corpora and then
run Giza++ again. As the English-Hindi dataset
</bodyText>
<page confidence="0.995307">
25
</page>
<bodyText confidence="0.998741">
of 50000 sentence pairs is relatively small, we ex-
pect lemmatizing to improve the results. Table 2.
shows the results. As we hoped, the results after
lemmatizing the word forms are better than those
without.
</bodyText>
<table confidence="0.999765">
Prec. Recall F-meas. AER
Eng Hin 0.52 0.40 0.45 0.5518
Hin Eng 0.53 0.30 0.38 0.6185
Intersected 0.82 0.23 0.36 0.6446
</table>
<tableCaption confidence="0.999387">
Table 2: Results of GIZA++ - lemmatized set
</tableCaption>
<subsectionHeader confidence="0.993518">
7.3 Experiments with our model
</subsectionHeader>
<bodyText confidence="0.997132142857143">
We trained our model using the training set of 294
word aligned sentence pairs. For training the pa-
rameters, we used a beam size of 3 and number of
iterations equal to 3. Table 3. shows the results
when we used only the basic local features (Dice-
Words, DiceRoots, Dict and Null) to train and test
our model.
</bodyText>
<table confidence="0.997758">
Prec. Recall F-meas. AER
Local Feats. 0.47 0.38 0.42 0.5798
</table>
<tableCaption confidence="0.9999">
Table 3: Results using the basic features
</tableCaption>
<bodyText confidence="0.9981255">
When we add the the global features (AvgDist,
Overlap), we obtain the AER shown in Table 4.
</bodyText>
<table confidence="0.998903">
Prec. Recall F-meas. AER
+ AvgD., Ove. 0.49 0.39 0.43 0.5689
</table>
<tableCaption confidence="0.878529">
Table 4: Results using the features - AvgDist,
Overlap
</tableCaption>
<bodyText confidence="0.999906583333333">
Now, we add the transition probabilities ob-
tained from the experiments with Giza++ as fea-
tures in our model. Table 5. contains the results.
The compositionality related features are now
added to our discriminative model to see if there is
any improvement in performance. Table 6. shows
the results by adding one feature at a time.
We observe that there is an improvement in the
AER by using the compositionality based features,
thus showing that compositionality based features
aid in the word-alignment task in a significant way
(AER = 0.5045).
</bodyText>
<sectionHeader confidence="0.999948" genericHeader="method">
8 Related work
</sectionHeader>
<bodyText confidence="0.9700695">
Various measures have been proposed in the past
to measure the compositionality of multi-word ex-
</bodyText>
<table confidence="0.9932315">
Prec. Recall F-meas. AER
+ Giza++ prob. 0.54 0.44 0.49 0.5155
</table>
<tableCaption confidence="0.989754">
Table 5: Results using the Giza++ probabilities
</tableCaption>
<table confidence="0.999895">
Prec. Recall F-meas. AER
+ MergePos 0.54 0.45 0.49 0.5101
+ MergeMI 0.55 0.45 0.50 0.5045
</table>
<tableCaption confidence="0.976058">
Table 6: Results using the compositionality based
features
</tableCaption>
<bodyText confidence="0.99978625">
pressions of various types. Some of them are Fre-
quency, Point-wise mutual information (Church
and Hanks, 1989), Distributed frequency of object
(Tapanainen et al., 1998), Distributed frequency
of object using verb information (Venkatapathy
and Joshi, 2005), Similarity of object in verb-
object pair using the LSA model (Baldwin et al.,
2003), (Venkatapathy and Joshi, 2005) and Lex-
ical and Syntactic fixedness (Fazly and Steven-
son, 2006). These features have largely been eval-
uated by the correlation of the compositionality
value predicted by these measures with the gold
standard value suggested by human judges. It has
been shown that the correlation of these measures
is higher than simple baseline measures suggest-
ing that these measures represent compositionality
quite well. But, the compositionality as such has
not been used in any specific application yet.
In this paper, we have suggested a framework
for using the compositionality of multi-word ex-
pressions for the word alignment task. State-of-art
systems for doing word alignment use generative
models like GIZA++ (Och and Ney, 2003; Brown
et al., 1993). Discriminative models have been
tried recently for word-alignment (Taskar et al.,
2005; Moore, 2005) as these models give the abil-
ity to harness variety of complex features which
cannot be provided in the generative models. In
our work, we have used the compositionality of
multi-word expressions to predict how they align
with the words in the target language sentence.
For parameter optimization for the word-
alignment task, Taskar, Simon and Klein (Taskar
et al., 2005) used a large margin approach by fac-
toring the structure level constraints to constraints
at the level of an alignment link. We cannot do
such a factorization because the scores of align-
ment links in our case are not computed in a com-
pletely isolated manner. We use an online large
margin approach called MIRA (McDonald et al.,
</bodyText>
<page confidence="0.98376">
26
</page>
<bodyText confidence="0.999533833333333">
2005; Crammer and Singer, 2003) which fits well
with our framework. MIRA has previously been
used by McDonald, Pereira, Ribarov and Hajic
(McDonald et al., 2005) for learning the param-
eter values in the task of dependency parsing.
It should be noted that previous word-alignment
experiments such as Taskar, Simon and Klein
(Taskar et al., 2005) have been done with very
large datasets and there is little word-order vari-
ation in the languages involved. Our dataset is
small at present and there is substantial word order
variation between the source and target languages.
</bodyText>
<sectionHeader confidence="0.973531" genericHeader="conclusions">
9 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.99997132">
In this paper, we have proposed a discriminative
approach for using the compositionality informa-
tion about verb-based multi-word expressions for
the word-alignment task. For training our model,
use used an online large margin algorithm (Mc-
Donald et al., 2005). For predicting the alignment
given a model, we proposed a K-Best beam search
algorithm to make our prediction algorithm com-
putationally feasible.
We have investigated the usefulness of simple
features such as point-wise mutual information for
the word-alignment task in English-Hindi bilin-
gual corpus. We have show that by adding the
compositionality based features to our model, we
obtain an decrease in AER from 0.5155 to 0.5045.
Our overall results are better than those obtained
using the GIZA++ models (Och and Ney, 2003).
In future, we will experiment with more ad-
vanced compositionality based features. But, this
would require a larger dataset for training and we
are working towards buidling such a large dataset.
Also, we would like to conduct similar exper-
iments on other language pairs (e.g. English-
French) and compare the results with the state-of-
art results reported for those languages.
</bodyText>
<sectionHeader confidence="0.999646" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999886552238806">
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Di-
ana McCarthy Francis Bond, Anna Korhonen and
Aline Villavicencio, editors, Proceedings of the ACL
2003 Workshop on Multiword Expressions: Analy-
sis, Acquisition and Treatment, pages 89–96.
P. Brown, S. A. Pietra, V. J. Della, Pietra, and R. L.
Mercer. 1993. The mathmatics of stastistical ma-
chine translation. In Computational Linguistics.
Kenneth Church and Patrick Hanks. 1989. Word as-
sociation norms, mutual information, and lexicog-
raphy. In Proceedings of the 27th. Annual Meet-
ing of the Association for Computational Linguis-
tics, 1990.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. In
Journal ofMachine Learning Research.
Afsaneh Fazly and Suzanne Stevenson. 2006. Auto-
matically constructing a lexicon of verb phrase id-
iomatic combinations. In Proceedings of European
Chapter of Association of Computational Linguis-
tics. Trento, Italy, April.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 523–530, Vancouver,
British Columbia, Canada, October. Association of
Computational Linguistics.
Robert C. Moore. 2005. A discriminative frame-
work for bilingual word alignment. In Proceedings
of Human Language Technology Conference and
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 81–88, Vancouver, British
Columbia, Canada, October. Association of Compu-
tational Linguistics.
F. Och and H. Ney. 2003. A systematic comparisoin
of various statistical alignment models. In Compu-
tational Linguistics.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multi-word
expressions: a pain in the neck for nlp. In Proceed-
ings of CICLing , 2002.
Libin Shen. 2006. Statistical LTAG Parsing. Ph.D.
thesis.
Pasi Tapanainen, Jussi Piitulaine, and Timo Jarvinen.
1998. Idiomatic object usage and support verbs. In
36th Annual Meeting of the Association for Compu-
tational Linguistics.
Ben Taskar, Locoste-Julien Simon, and Klein Dan.
2005. A discriminative machine learning approach
to word alignment. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Process-
ing, pages 73–80, Vancouver, British Columbia,
Canada, October. Association of Computational
Linguistics.
Sriram Venkatapathy and Aravind Joshi. 2005. Mea-
suring the relative compositionality of verb-noun (v-
n) collocations by integrating features. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 899–906. Association
of Computational Linguistics, Vancouver, British
Columbia, Canada, October.
</reference>
<page confidence="0.998811">
27
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.325561">
<title confidence="0.915980333333333">Using Information about Multi-word for the Word-Alignment Task Language Technologies Research</title>
<affiliation confidence="0.885397">Indian Institute Information Hyderabad,</affiliation>
<email confidence="0.999736">sriramv@linc.cis.upenn.edu</email>
<author confidence="0.964426">K Aravind</author>
<affiliation confidence="0.961061">Department of Computer Information Science and Institute Research in Cognitive University of Pennsylvania, PA,</affiliation>
<email confidence="0.999634">joshi@linc.cis.upenn.edu</email>
<abstract confidence="0.981508952380952">It is well known that multi-word expressions are problematic in natural language processing. In previous literature, it has been suggested that information about their degree of compositionality can be helpful in various applications but it has not been proven empirically. In this paper, we propose a framework in which information about the multi-word expressions can be used in the word-alignment task. We have shown that even simple features like point-wise mutual information are useful for word-alignment task in English-Hindi parallel corpora. The alignment error rate which we achieve (AER = 0.5040) is significantly better (about 10% decrease in AER) than the alignment error rates of the state-of-art models (Och and Ney, 2003) (Best AER = 0.5518) on the English-Hindi dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Colin Bannard</author>
<author>Takaaki Tanaka</author>
<author>Dominic Widdows</author>
</authors>
<title>An empirical model of multiword expression decomposability.</title>
<date>2003</date>
<booktitle>Proceedings of the ACL 2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment,</booktitle>
<pages>89--96</pages>
<editor>In Diana McCarthy Francis Bond, Anna Korhonen and Aline Villavicencio, editors,</editor>
<contexts>
<context position="2940" citStr="Baldwin et al., 2003" startWordPosition="434" endWordPosition="437">cture but are semantically noncompositional. Here, we consider the class of verb based expressions (verb is the head of the phrase), which occur very frequently. This class of verb based multi-word expressions include verbal idioms, support-verb constructions, among others. The example ‘take place’ is a MWE but ‘take a gift’ is not. In the past, various measures have been suggested for measuring the compositionality of multi-word expressions. Some of these are mutual information (Church and Hanks, 1989), distributed frequency (Tapanainen et al., 1998) and Latent Semantic Analysis (LSA) model (Baldwin et al., 2003). Even though, these measures have been shown to represent compositionality quite well, compositionality itself has not been shown to be useful in any application yet. In this paper, we explore this possibility of using the information about compositionality of MWEs (verb based) for the word alignment task. In this preliminary work, we use simple measures (such as point-wise mutual information) to measure compositionality. The paper is organized as follows. In section 2, we discuss the word-alignment task with respect to the class of multi-word expressions of interest in this paper. In section</context>
<context position="23467" citStr="Baldwin et al., 2003" startWordPosition="3958" endWordPosition="3961">ality of multi-word exPrec. Recall F-meas. AER + Giza++ prob. 0.54 0.44 0.49 0.5155 Table 5: Results using the Giza++ probabilities Prec. Recall F-meas. AER + MergePos 0.54 0.45 0.49 0.5101 + MergeMI 0.55 0.45 0.50 0.5045 Table 6: Results using the compositionality based features pressions of various types. Some of them are Frequency, Point-wise mutual information (Church and Hanks, 1989), Distributed frequency of object (Tapanainen et al., 1998), Distributed frequency of object using verb information (Venkatapathy and Joshi, 2005), Similarity of object in verbobject pair using the LSA model (Baldwin et al., 2003), (Venkatapathy and Joshi, 2005) and Lexical and Syntactic fixedness (Fazly and Stevenson, 2006). These features have largely been evaluated by the correlation of the compositionality value predicted by these measures with the gold standard value suggested by human judges. It has been shown that the correlation of these measures is higher than simple baseline measures suggesting that these measures represent compositionality quite well. But, the compositionality as such has not been used in any specific application yet. In this paper, we have suggested a framework for using the compositionalit</context>
</contexts>
<marker>Baldwin, Bannard, Tanaka, Widdows, 2003</marker>
<rawString>Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and Dominic Widdows. 2003. An empirical model of multiword expression decomposability. In Diana McCarthy Francis Bond, Anna Korhonen and Aline Villavicencio, editors, Proceedings of the ACL 2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, pages 89–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S A Pietra</author>
<author>V J Della</author>
<author>Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathmatics of stastistical machine translation. In Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="24243" citStr="Brown et al., 1993" startWordPosition="4079" endWordPosition="4082">on of the compositionality value predicted by these measures with the gold standard value suggested by human judges. It has been shown that the correlation of these measures is higher than simple baseline measures suggesting that these measures represent compositionality quite well. But, the compositionality as such has not been used in any specific application yet. In this paper, we have suggested a framework for using the compositionality of multi-word expressions for the word alignment task. State-of-art systems for doing word alignment use generative models like GIZA++ (Och and Ney, 2003; Brown et al., 1993). Discriminative models have been tried recently for word-alignment (Taskar et al., 2005; Moore, 2005) as these models give the ability to harness variety of complex features which cannot be provided in the generative models. In our work, we have used the compositionality of multi-word expressions to predict how they align with the words in the target language sentence. For parameter optimization for the wordalignment task, Taskar, Simon and Klein (Taskar et al., 2005) used a large margin approach by factoring the structure level constraints to constraints at the level of an alignment link. We</context>
</contexts>
<marker>Brown, Pietra, Della, Pietra, Mercer, 1993</marker>
<rawString>P. Brown, S. A. Pietra, V. J. Della, Pietra, and R. L. Mercer. 1993. The mathmatics of stastistical machine translation. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th. Annual Meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="2827" citStr="Church and Hanks, 1989" startWordPosition="417" endWordPosition="420"> interpretations that cross word boundaries (Sag et al., 2002). A large number of MWEs have standard syntactic structure but are semantically noncompositional. Here, we consider the class of verb based expressions (verb is the head of the phrase), which occur very frequently. This class of verb based multi-word expressions include verbal idioms, support-verb constructions, among others. The example ‘take place’ is a MWE but ‘take a gift’ is not. In the past, various measures have been suggested for measuring the compositionality of multi-word expressions. Some of these are mutual information (Church and Hanks, 1989), distributed frequency (Tapanainen et al., 1998) and Latent Semantic Analysis (LSA) model (Baldwin et al., 2003). Even though, these measures have been shown to represent compositionality quite well, compositionality itself has not been shown to be useful in any application yet. In this paper, we explore this possibility of using the information about compositionality of MWEs (verb based) for the word alignment task. In this preliminary work, we use simple measures (such as point-wise mutual information) to measure compositionality. The paper is organized as follows. In section 2, we discuss </context>
<context position="23237" citStr="Church and Hanks, 1989" startWordPosition="3924" endWordPosition="3927">ionality based features, thus showing that compositionality based features aid in the word-alignment task in a significant way (AER = 0.5045). 8 Related work Various measures have been proposed in the past to measure the compositionality of multi-word exPrec. Recall F-meas. AER + Giza++ prob. 0.54 0.44 0.49 0.5155 Table 5: Results using the Giza++ probabilities Prec. Recall F-meas. AER + MergePos 0.54 0.45 0.49 0.5101 + MergeMI 0.55 0.45 0.50 0.5045 Table 6: Results using the compositionality based features pressions of various types. Some of them are Frequency, Point-wise mutual information (Church and Hanks, 1989), Distributed frequency of object (Tapanainen et al., 1998), Distributed frequency of object using verb information (Venkatapathy and Joshi, 2005), Similarity of object in verbobject pair using the LSA model (Baldwin et al., 2003), (Venkatapathy and Joshi, 2005) and Lexical and Syntactic fixedness (Fazly and Stevenson, 2006). These features have largely been evaluated by the correlation of the compositionality value predicted by these measures with the gold standard value suggested by human judges. It has been shown that the correlation of these measures is higher than simple baseline measures</context>
</contexts>
<marker>Church, Hanks, 1989</marker>
<rawString>Kenneth Church and Patrick Hanks. 1989. Word association norms, mutual information, and lexicography. In Proceedings of the 27th. Annual Meeting of the Association for Computational Linguistics, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>In Journal ofMachine Learning Research.</journal>
<contexts>
<context position="1490" citStr="Crammer and Singer, 2003" startWordPosition="213" endWordPosition="216">wise mutual information are useful for word-alignment task in English-Hindi parallel corpora. The alignment error rate which we achieve (AER = 0.5040) is significantly better (about 10% decrease in AER) than the alignment error rates of the state-of-art models (Och and Ney, 2003) (Best AER = 0.5518) on the English-Hindi dataset. 1 Introduction In this paper, we show that measures representing compositionality of multi-word expressions can be useful for tasks such as Machine Translation, word-alignment to be specific here. We use an online learning framework called MIRA (McDonald et al., 2005; Crammer and Singer, 2003) for training a discriminative model for the word alignment task (Taskar et al., 2005; Moore, 2005). The discriminative model makes use of features which represent the compositionality of multi-word expressions. 1At present visiting Institute for Research in Cognitive Science, University of Pennsylvania, PA, USA. Multi-word expressions (MWEs) are those whose structure and meaning cannot be derived from their component words, as they occur independently. Examples include conjunctions such as ‘as well as’ (meaning ‘including’), idioms like ‘kick the bucket’ (meaning ‘die’) phrasal verbs such as </context>
<context position="17101" citStr="Crammer and Singer, 2003" startWordPosition="2865" endWordPosition="2868">al information (MI) is classified into three groups depending on its absolute value. If the absolute value of mutual information rounded to nearest integer is in the range 0-2, it is considered LOW. If the value is in the range 3-5, it is considered MEDIUM and if it is above 5, it is considered HIGH. The feature “merge RP HIGH” is active in the example shown in figure 6. ran/V MI = HIGH away/RP He/N vaha bhaaga gayaa Figure 6: Example of MergeMI feature 6 Online large margin training For parameter optimization, we have used an online large margin algorithm called MIRA (McDonald et al., 2005) (Crammer and Singer, 2003). We describe the training algorithm that we used very briefly. Our training set is a set of EnglishHindi word aligned parallel corpus. We get the verb based expressions in English by running a dependency parser (Shen, 2006). Let the number of sentence pairs in the training data bem. We have 24 fSq, Tq, ^aqg for training where q &lt;= m is the index number of the sentence pair f Sq, Tq g in the training set and ^aq is the gold alignment for the pair f Sq, Tq g. Let W be the weight vector which has to be learnt, Wi be the weight vector after the end of ith update. To avoid over-fitting, W is obtai</context>
<context position="25078" citStr="Crammer and Singer, 2003" startWordPosition="4218" endWordPosition="4221">erative models. In our work, we have used the compositionality of multi-word expressions to predict how they align with the words in the target language sentence. For parameter optimization for the wordalignment task, Taskar, Simon and Klein (Taskar et al., 2005) used a large margin approach by factoring the structure level constraints to constraints at the level of an alignment link. We cannot do such a factorization because the scores of alignment links in our case are not computed in a completely isolated manner. We use an online large margin approach called MIRA (McDonald et al., 26 2005; Crammer and Singer, 2003) which fits well with our framework. MIRA has previously been used by McDonald, Pereira, Ribarov and Hajic (McDonald et al., 2005) for learning the parameter values in the task of dependency parsing. It should be noted that previous word-alignment experiments such as Taskar, Simon and Klein (Taskar et al., 2005) have been done with very large datasets and there is little word-order variation in the languages involved. Our dataset is small at present and there is substantial word order variation between the source and target languages. 9 Conclusion and future work In this paper, we have propose</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. In Journal ofMachine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Afsaneh Fazly</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Automatically constructing a lexicon of verb phrase idiomatic combinations.</title>
<date>2006</date>
<booktitle>In Proceedings of European Chapter of Association of Computational Linguistics.</booktitle>
<location>Trento, Italy,</location>
<contexts>
<context position="23563" citStr="Fazly and Stevenson, 2006" startWordPosition="3972" endWordPosition="3976">: Results using the Giza++ probabilities Prec. Recall F-meas. AER + MergePos 0.54 0.45 0.49 0.5101 + MergeMI 0.55 0.45 0.50 0.5045 Table 6: Results using the compositionality based features pressions of various types. Some of them are Frequency, Point-wise mutual information (Church and Hanks, 1989), Distributed frequency of object (Tapanainen et al., 1998), Distributed frequency of object using verb information (Venkatapathy and Joshi, 2005), Similarity of object in verbobject pair using the LSA model (Baldwin et al., 2003), (Venkatapathy and Joshi, 2005) and Lexical and Syntactic fixedness (Fazly and Stevenson, 2006). These features have largely been evaluated by the correlation of the compositionality value predicted by these measures with the gold standard value suggested by human judges. It has been shown that the correlation of these measures is higher than simple baseline measures suggesting that these measures represent compositionality quite well. But, the compositionality as such has not been used in any specific application yet. In this paper, we have suggested a framework for using the compositionality of multi-word expressions for the word alignment task. State-of-art systems for doing word ali</context>
</contexts>
<marker>Fazly, Stevenson, 2006</marker>
<rawString>Afsaneh Fazly and Suzanne Stevenson. 2006. Automatically constructing a lexicon of verb phrase idiomatic combinations. In Proceedings of European Chapter of Association of Computational Linguistics. Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<journal>Association of Computational Linguistics.</journal>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>523--530</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="1463" citStr="McDonald et al., 2005" startWordPosition="208" endWordPosition="212">le features like point-wise mutual information are useful for word-alignment task in English-Hindi parallel corpora. The alignment error rate which we achieve (AER = 0.5040) is significantly better (about 10% decrease in AER) than the alignment error rates of the state-of-art models (Och and Ney, 2003) (Best AER = 0.5518) on the English-Hindi dataset. 1 Introduction In this paper, we show that measures representing compositionality of multi-word expressions can be useful for tasks such as Machine Translation, word-alignment to be specific here. We use an online learning framework called MIRA (McDonald et al., 2005; Crammer and Singer, 2003) for training a discriminative model for the word alignment task (Taskar et al., 2005; Moore, 2005). The discriminative model makes use of features which represent the compositionality of multi-word expressions. 1At present visiting Institute for Research in Cognitive Science, University of Pennsylvania, PA, USA. Multi-word expressions (MWEs) are those whose structure and meaning cannot be derived from their component words, as they occur independently. Examples include conjunctions such as ‘as well as’ (meaning ‘including’), idioms like ‘kick the bucket’ (meaning ‘d</context>
<context position="17074" citStr="McDonald et al., 2005" startWordPosition="2860" endWordPosition="2864">its verb merge. The mutual information (MI) is classified into three groups depending on its absolute value. If the absolute value of mutual information rounded to nearest integer is in the range 0-2, it is considered LOW. If the value is in the range 3-5, it is considered MEDIUM and if it is above 5, it is considered HIGH. The feature “merge RP HIGH” is active in the example shown in figure 6. ran/V MI = HIGH away/RP He/N vaha bhaaga gayaa Figure 6: Example of MergeMI feature 6 Online large margin training For parameter optimization, we have used an online large margin algorithm called MIRA (McDonald et al., 2005) (Crammer and Singer, 2003). We describe the training algorithm that we used very briefly. Our training set is a set of EnglishHindi word aligned parallel corpus. We get the verb based expressions in English by running a dependency parser (Shen, 2006). Let the number of sentence pairs in the training data bem. We have 24 fSq, Tq, ^aqg for training where q &lt;= m is the index number of the sentence pair f Sq, Tq g in the training set and ^aq is the gold alignment for the pair f Sq, Tq g. Let W be the weight vector which has to be learnt, Wi be the weight vector after the end of ith update. To avo</context>
<context position="25208" citStr="McDonald et al., 2005" startWordPosition="4239" endWordPosition="4242">the target language sentence. For parameter optimization for the wordalignment task, Taskar, Simon and Klein (Taskar et al., 2005) used a large margin approach by factoring the structure level constraints to constraints at the level of an alignment link. We cannot do such a factorization because the scores of alignment links in our case are not computed in a completely isolated manner. We use an online large margin approach called MIRA (McDonald et al., 26 2005; Crammer and Singer, 2003) which fits well with our framework. MIRA has previously been used by McDonald, Pereira, Ribarov and Hajic (McDonald et al., 2005) for learning the parameter values in the task of dependency parsing. It should be noted that previous word-alignment experiments such as Taskar, Simon and Klein (Taskar et al., 2005) have been done with very large datasets and there is little word-order variation in the languages involved. Our dataset is small at present and there is substantial word order variation between the source and target languages. 9 Conclusion and future work In this paper, we have proposed a discriminative approach for using the compositionality information about verb-based multi-word expressions for the word-alignm</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 523–530, Vancouver, British Columbia, Canada, October. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>A discriminative framework for bilingual word alignment.</title>
<date>2005</date>
<journal>Association of Computational Linguistics.</journal>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>81--88</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="1589" citStr="Moore, 2005" startWordPosition="232" endWordPosition="233">r rate which we achieve (AER = 0.5040) is significantly better (about 10% decrease in AER) than the alignment error rates of the state-of-art models (Och and Ney, 2003) (Best AER = 0.5518) on the English-Hindi dataset. 1 Introduction In this paper, we show that measures representing compositionality of multi-word expressions can be useful for tasks such as Machine Translation, word-alignment to be specific here. We use an online learning framework called MIRA (McDonald et al., 2005; Crammer and Singer, 2003) for training a discriminative model for the word alignment task (Taskar et al., 2005; Moore, 2005). The discriminative model makes use of features which represent the compositionality of multi-word expressions. 1At present visiting Institute for Research in Cognitive Science, University of Pennsylvania, PA, USA. Multi-word expressions (MWEs) are those whose structure and meaning cannot be derived from their component words, as they occur independently. Examples include conjunctions such as ‘as well as’ (meaning ‘including’), idioms like ‘kick the bucket’ (meaning ‘die’) phrasal verbs such as ‘find out’ (meaning ‘search’) and compounds like ‘village community’. They can be defined roughly a</context>
<context position="24345" citStr="Moore, 2005" startWordPosition="4095" endWordPosition="4096">judges. It has been shown that the correlation of these measures is higher than simple baseline measures suggesting that these measures represent compositionality quite well. But, the compositionality as such has not been used in any specific application yet. In this paper, we have suggested a framework for using the compositionality of multi-word expressions for the word alignment task. State-of-art systems for doing word alignment use generative models like GIZA++ (Och and Ney, 2003; Brown et al., 1993). Discriminative models have been tried recently for word-alignment (Taskar et al., 2005; Moore, 2005) as these models give the ability to harness variety of complex features which cannot be provided in the generative models. In our work, we have used the compositionality of multi-word expressions to predict how they align with the words in the target language sentence. For parameter optimization for the wordalignment task, Taskar, Simon and Klein (Taskar et al., 2005) used a large margin approach by factoring the structure level constraints to constraints at the level of an alignment link. We cannot do such a factorization because the scores of alignment links in our case are not computed in </context>
</contexts>
<marker>Moore, 2005</marker>
<rawString>Robert C. Moore. 2005. A discriminative framework for bilingual word alignment. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 81–88, Vancouver, British Columbia, Canada, October. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparisoin of various statistical alignment models.</title>
<date>2003</date>
<booktitle>In Computational Linguistics.</booktitle>
<contexts>
<context position="1145" citStr="Och and Ney, 2003" startWordPosition="159" endWordPosition="162"> has been suggested that information about their degree of compositionality can be helpful in various applications but it has not been proven empirically. In this paper, we propose a framework in which information about the multi-word expressions can be used in the word-alignment task. We have shown that even simple features like point-wise mutual information are useful for word-alignment task in English-Hindi parallel corpora. The alignment error rate which we achieve (AER = 0.5040) is significantly better (about 10% decrease in AER) than the alignment error rates of the state-of-art models (Och and Ney, 2003) (Best AER = 0.5518) on the English-Hindi dataset. 1 Introduction In this paper, we show that measures representing compositionality of multi-word expressions can be useful for tasks such as Machine Translation, word-alignment to be specific here. We use an online learning framework called MIRA (McDonald et al., 2005; Crammer and Singer, 2003) for training a discriminative model for the word alignment task (Taskar et al., 2005; Moore, 2005). The discriminative model makes use of features which represent the compositionality of multi-word expressions. 1At present visiting Institute for Research</context>
<context position="20406" citStr="Och and Ney, 2003" startWordPosition="3441" endWordPosition="3444"> aligned to only one or zero target words. For this, we use simple heuristics to convert the training data to the appropriate format. For the words aligned to a source verb, the first verb is chosen as the gold alignment. For the words aligned to any dependent which is not a verb, the last content word is chosen as the alignment link. For test data, we do not make any modifications and the final output from our alignment algorithm is compared with the original test data. 7.2 Experiments with Giza We evaluated our discriminative approach by comparing it with the state-of-art Giza++ alignments (Och and Ney, 2003). The metric that we have used to do the comparison is the Alignment Error Rate (AER). The results shown below also contain Precision, Recall and F-measure. Giza was trained using an English-Hindi aligned corpus of 50000 sentence pairs. In Table 1., we report the results of the GIZA++ alignments run from both the directions (English to Hindi and Hindi to English). We also show the results of the intersected model. See Table 1. for the results of the GIZA++ alignments. Prec. Recall F-meas. AER Eng Hin 0.45 0.38 0.41 0.5874 Hin Eng 0.46 0.27 0.34 0.6584 Intersected 0.82 0.19 0.31 0.6892 Table 1:</context>
<context position="24222" citStr="Och and Ney, 2003" startWordPosition="4075" endWordPosition="4078">ed by the correlation of the compositionality value predicted by these measures with the gold standard value suggested by human judges. It has been shown that the correlation of these measures is higher than simple baseline measures suggesting that these measures represent compositionality quite well. But, the compositionality as such has not been used in any specific application yet. In this paper, we have suggested a framework for using the compositionality of multi-word expressions for the word alignment task. State-of-art systems for doing word alignment use generative models like GIZA++ (Och and Ney, 2003; Brown et al., 1993). Discriminative models have been tried recently for word-alignment (Taskar et al., 2005; Moore, 2005) as these models give the ability to harness variety of complex features which cannot be provided in the generative models. In our work, we have used the compositionality of multi-word expressions to predict how they align with the words in the target language sentence. For parameter optimization for the wordalignment task, Taskar, Simon and Klein (Taskar et al., 2005) used a large margin approach by factoring the structure level constraints to constraints at the level of </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. Och and H. Ney. 2003. A systematic comparisoin of various statistical alignment models. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Timothy Baldwin</author>
<author>Francis Bond</author>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>Multi-word expressions: a pain in the neck for nlp.</title>
<date>2002</date>
<booktitle>In Proceedings of CICLing ,</booktitle>
<contexts>
<context position="2266" citStr="Sag et al., 2002" startWordPosition="328" endWordPosition="331">sent the compositionality of multi-word expressions. 1At present visiting Institute for Research in Cognitive Science, University of Pennsylvania, PA, USA. Multi-word expressions (MWEs) are those whose structure and meaning cannot be derived from their component words, as they occur independently. Examples include conjunctions such as ‘as well as’ (meaning ‘including’), idioms like ‘kick the bucket’ (meaning ‘die’) phrasal verbs such as ‘find out’ (meaning ‘search’) and compounds like ‘village community’. They can be defined roughly as idiosyncratic interpretations that cross word boundaries (Sag et al., 2002). A large number of MWEs have standard syntactic structure but are semantically noncompositional. Here, we consider the class of verb based expressions (verb is the head of the phrase), which occur very frequently. This class of verb based multi-word expressions include verbal idioms, support-verb constructions, among others. The example ‘take place’ is a MWE but ‘take a gift’ is not. In the past, various measures have been suggested for measuring the compositionality of multi-word expressions. Some of these are mutual information (Church and Hanks, 1989), distributed frequency (Tapanainen et </context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and Dan Flickinger. 2002. Multi-word expressions: a pain in the neck for nlp. In Proceedings of CICLing , 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
</authors>
<date>2006</date>
<tech>Statistical LTAG Parsing. Ph.D. thesis.</tech>
<contexts>
<context position="5858" citStr="Shen, 2006" startWordPosition="904" endWordPosition="905">in English, a verb and one of its dependents is aligned with a single verbal unit in Hindi. It is essential to obtain the syntactic roles for dependents in the source language sentence as they are required for computing the compositionality value between the dependents and their verbs. The took Philadelphia mein saikling kii pratiyogitaa hui Figure 2: Alignment of Verb based expression syntactic roles on the source side are obtained by applying simple rules to the output of a dependency parser. The dependency parser which we used in our experiments is a stochastic TAG based dependency parser (Shen, 2006). A sentence could have one or more verbs. We would like to align all the expressions represented by those verbs with words in the target language. 3 Behavior of MWEs in parallel corpora In this section, we will briefly discuss the complexity of the alignment problem based on the verb based MWE’s. From the word aligned sentence pairs, we compute the fraction of times a source sentence verb and its dependent are aligned together with the same word in the target language sentence. We count the number of times a source sentence verb and its dependent are aligned together with the same word in the</context>
<context position="17325" citStr="Shen, 2006" startWordPosition="2906" endWordPosition="2907"> considered MEDIUM and if it is above 5, it is considered HIGH. The feature “merge RP HIGH” is active in the example shown in figure 6. ran/V MI = HIGH away/RP He/N vaha bhaaga gayaa Figure 6: Example of MergeMI feature 6 Online large margin training For parameter optimization, we have used an online large margin algorithm called MIRA (McDonald et al., 2005) (Crammer and Singer, 2003). We describe the training algorithm that we used very briefly. Our training set is a set of EnglishHindi word aligned parallel corpus. We get the verb based expressions in English by running a dependency parser (Shen, 2006). Let the number of sentence pairs in the training data bem. We have 24 fSq, Tq, ^aqg for training where q &lt;= m is the index number of the sentence pair f Sq, Tq g in the training set and ^aq is the gold alignment for the pair f Sq, Tq g. Let W be the weight vector which has to be learnt, Wi be the weight vector after the end of ith update. To avoid over-fitting, W is obtained by averaging over all the weight vectors Wi. A generic large margin algorithm is defined follows for the training instancesfSq,Tq,^aqg, 1. InitializeW0,W,i 2. for p:1 to NIterations 3. for q:1 to m 4. Get K-Best predicti</context>
<context position="19622" citStr="Shen, 2006" startWordPosition="3303" endWordPosition="3304"> solved using Hildreth’s algorithm. Here, prediction ofais similar to the prediction ofK—bestclasses in a multi-class classification problem. Ideally, we need to consider all the possible classes and assign margin constraints based on every class. But, here the number of such classes is exponential and thus we restrict ourselves to the K—bestclasses. 7 Results on word-alignment task 7.1 Dataset We have divided the 400 word aligned sentence pairs into a training set consisting of 294 sentence pairs and a test set consisting of 106 sentence pairs. The source sentences are all dependency parsed (Shen, 2006) and only the verb and its dependents are considered for both training and testing our algorithm. Our training algorithm requires that the each of the source words is aligned to only one or zero target words. For this, we use simple heuristics to convert the training data to the appropriate format. For the words aligned to a source verb, the first verb is chosen as the gold alignment. For the words aligned to any dependent which is not a verb, the last content word is chosen as the alignment link. For test data, we do not make any modifications and the final output from our alignment algorithm</context>
</contexts>
<marker>Shen, 2006</marker>
<rawString>Libin Shen. 2006. Statistical LTAG Parsing. Ph.D. thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
<author>Jussi Piitulaine</author>
<author>Timo Jarvinen</author>
</authors>
<title>Idiomatic object usage and support verbs.</title>
<date>1998</date>
<booktitle>In 36th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2876" citStr="Tapanainen et al., 1998" startWordPosition="424" endWordPosition="427"> et al., 2002). A large number of MWEs have standard syntactic structure but are semantically noncompositional. Here, we consider the class of verb based expressions (verb is the head of the phrase), which occur very frequently. This class of verb based multi-word expressions include verbal idioms, support-verb constructions, among others. The example ‘take place’ is a MWE but ‘take a gift’ is not. In the past, various measures have been suggested for measuring the compositionality of multi-word expressions. Some of these are mutual information (Church and Hanks, 1989), distributed frequency (Tapanainen et al., 1998) and Latent Semantic Analysis (LSA) model (Baldwin et al., 2003). Even though, these measures have been shown to represent compositionality quite well, compositionality itself has not been shown to be useful in any application yet. In this paper, we explore this possibility of using the information about compositionality of MWEs (verb based) for the word alignment task. In this preliminary work, we use simple measures (such as point-wise mutual information) to measure compositionality. The paper is organized as follows. In section 2, we discuss the word-alignment task with respect to the class</context>
<context position="23296" citStr="Tapanainen et al., 1998" startWordPosition="3932" endWordPosition="3935">y based features aid in the word-alignment task in a significant way (AER = 0.5045). 8 Related work Various measures have been proposed in the past to measure the compositionality of multi-word exPrec. Recall F-meas. AER + Giza++ prob. 0.54 0.44 0.49 0.5155 Table 5: Results using the Giza++ probabilities Prec. Recall F-meas. AER + MergePos 0.54 0.45 0.49 0.5101 + MergeMI 0.55 0.45 0.50 0.5045 Table 6: Results using the compositionality based features pressions of various types. Some of them are Frequency, Point-wise mutual information (Church and Hanks, 1989), Distributed frequency of object (Tapanainen et al., 1998), Distributed frequency of object using verb information (Venkatapathy and Joshi, 2005), Similarity of object in verbobject pair using the LSA model (Baldwin et al., 2003), (Venkatapathy and Joshi, 2005) and Lexical and Syntactic fixedness (Fazly and Stevenson, 2006). These features have largely been evaluated by the correlation of the compositionality value predicted by these measures with the gold standard value suggested by human judges. It has been shown that the correlation of these measures is higher than simple baseline measures suggesting that these measures represent compositionality </context>
</contexts>
<marker>Tapanainen, Piitulaine, Jarvinen, 1998</marker>
<rawString>Pasi Tapanainen, Jussi Piitulaine, and Timo Jarvinen. 1998. Idiomatic object usage and support verbs. In 36th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Locoste-Julien Simon</author>
<author>Klein Dan</author>
</authors>
<title>A discriminative machine learning approach to word alignment.</title>
<date>2005</date>
<journal>Association of Computational Linguistics.</journal>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>73--80</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="1575" citStr="Taskar et al., 2005" startWordPosition="228" endWordPosition="231">a. The alignment error rate which we achieve (AER = 0.5040) is significantly better (about 10% decrease in AER) than the alignment error rates of the state-of-art models (Och and Ney, 2003) (Best AER = 0.5518) on the English-Hindi dataset. 1 Introduction In this paper, we show that measures representing compositionality of multi-word expressions can be useful for tasks such as Machine Translation, word-alignment to be specific here. We use an online learning framework called MIRA (McDonald et al., 2005; Crammer and Singer, 2003) for training a discriminative model for the word alignment task (Taskar et al., 2005; Moore, 2005). The discriminative model makes use of features which represent the compositionality of multi-word expressions. 1At present visiting Institute for Research in Cognitive Science, University of Pennsylvania, PA, USA. Multi-word expressions (MWEs) are those whose structure and meaning cannot be derived from their component words, as they occur independently. Examples include conjunctions such as ‘as well as’ (meaning ‘including’), idioms like ‘kick the bucket’ (meaning ‘die’) phrasal verbs such as ‘find out’ (meaning ‘search’) and compounds like ‘village community’. They can be def</context>
<context position="24331" citStr="Taskar et al., 2005" startWordPosition="4091" endWordPosition="4094">e suggested by human judges. It has been shown that the correlation of these measures is higher than simple baseline measures suggesting that these measures represent compositionality quite well. But, the compositionality as such has not been used in any specific application yet. In this paper, we have suggested a framework for using the compositionality of multi-word expressions for the word alignment task. State-of-art systems for doing word alignment use generative models like GIZA++ (Och and Ney, 2003; Brown et al., 1993). Discriminative models have been tried recently for word-alignment (Taskar et al., 2005; Moore, 2005) as these models give the ability to harness variety of complex features which cannot be provided in the generative models. In our work, we have used the compositionality of multi-word expressions to predict how they align with the words in the target language sentence. For parameter optimization for the wordalignment task, Taskar, Simon and Klein (Taskar et al., 2005) used a large margin approach by factoring the structure level constraints to constraints at the level of an alignment link. We cannot do such a factorization because the scores of alignment links in our case are no</context>
</contexts>
<marker>Taskar, Simon, Dan, 2005</marker>
<rawString>Ben Taskar, Locoste-Julien Simon, and Klein Dan. 2005. A discriminative machine learning approach to word alignment. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 73–80, Vancouver, British Columbia, Canada, October. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sriram Venkatapathy</author>
<author>Aravind Joshi</author>
</authors>
<title>Measuring the relative compositionality of verb-noun (vn) collocations by integrating features.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>899--906</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="23383" citStr="Venkatapathy and Joshi, 2005" startWordPosition="3943" endWordPosition="3946">). 8 Related work Various measures have been proposed in the past to measure the compositionality of multi-word exPrec. Recall F-meas. AER + Giza++ prob. 0.54 0.44 0.49 0.5155 Table 5: Results using the Giza++ probabilities Prec. Recall F-meas. AER + MergePos 0.54 0.45 0.49 0.5101 + MergeMI 0.55 0.45 0.50 0.5045 Table 6: Results using the compositionality based features pressions of various types. Some of them are Frequency, Point-wise mutual information (Church and Hanks, 1989), Distributed frequency of object (Tapanainen et al., 1998), Distributed frequency of object using verb information (Venkatapathy and Joshi, 2005), Similarity of object in verbobject pair using the LSA model (Baldwin et al., 2003), (Venkatapathy and Joshi, 2005) and Lexical and Syntactic fixedness (Fazly and Stevenson, 2006). These features have largely been evaluated by the correlation of the compositionality value predicted by these measures with the gold standard value suggested by human judges. It has been shown that the correlation of these measures is higher than simple baseline measures suggesting that these measures represent compositionality quite well. But, the compositionality as such has not been used in any specific applica</context>
</contexts>
<marker>Venkatapathy, Joshi, 2005</marker>
<rawString>Sriram Venkatapathy and Aravind Joshi. 2005. Measuring the relative compositionality of verb-noun (vn) collocations by integrating features. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 899–906. Association of Computational Linguistics, Vancouver, British Columbia, Canada, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>