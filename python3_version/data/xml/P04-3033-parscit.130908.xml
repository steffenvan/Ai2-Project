<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.070727">
<title confidence="0.944258">
MATCHKiosk: A Multimodal Interactive City Guide
</title>
<author confidence="0.794078">
Michael Johnston
</author>
<affiliation confidence="0.475942">
AT&amp;T Research
</affiliation>
<address confidence="0.9178545">
180 Park Avenue
Florham Park, NJ 07932
</address>
<email confidence="0.988545">
johnston@research.att.com
</email>
<subsectionHeader confidence="0.318558">
Srinivas Bangalore
</subsectionHeader>
<sectionHeader confidence="0.641158" genericHeader="abstract">
AT&amp;T Research
</sectionHeader>
<address confidence="0.8997675">
180 Park Avenue
Florham Park, NJ 07932
</address>
<email confidence="0.996616">
srini@research.att.com
</email>
<sectionHeader confidence="0.997361" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.9998272">
Multimodal interfaces provide more flexible and
compelling interaction and can enable public infor-
mation kiosks to support more complex tasks for
a broader community of users. MATCHKiosk is
a multimodal interactive city guide which provides
users with the freedom to interact using speech,
pen, touch or multimodal inputs. The system re-
sponds by generating multimodal presentations that
synchronize synthetic speech with a life-like virtual
agent and dynamically generated graphics.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999987866666667">
Since the introduction of automated teller machines
in the late 1970s, public kiosks have been intro-
duced to provide users with automated access to
a broad range of information, assistance, and ser-
vices. These include self check-in at airports, ticket
machines in railway and bus stations, directions and
maps in car rental offices, interactive tourist and vis-
itor guides in tourist offices and museums, and more
recently, automated check-out in retail stores. The
majority of these systems provide a rigid structured
graphical interface and user input by only touch or
keypad, and as a result can only support a small
number of simple tasks. As automated kiosks be-
come more commonplace and have to support more
complex tasks for a broader community of users,
they will need to provide a more flexible and com-
pelling user interface.
One major motivation for developing multimodal
interfaces for mobile devices is the lack of a key-
board or mouse (Oviatt and Cohen, 2000; Johnston
and Bangalore, 2000). This limitation is also true of
many different kinds of public information kiosks
where security, hygiene, or space concerns make a
physical keyboard or mouse impractical. Also, mo-
bile users interacting with kiosks are often encum-
bered with briefcases, phones, or other equipment,
leaving only one hand free for interaction. Kiosks
often provide a touchscreen for input, opening up
the possibility of an onscreen keyboard, but these
can be awkward to use and occupy a considerable
amount of screen real estate, generally leading to a
more moded and cumbersome graphical interface.
A number of experimental systems have inves-
tigated adding speech input to interactive graphi-
cal kiosks (Raisamo, 1998; Gustafson et al., 1999;
Narayanan et al., 2000; Lamel et al., 2002). Other
work has investigated adding both speech and ges-
ture input (using computer vision) in an interactive
kiosk (Wahlster, 2003; Cassell et al., 2002).
We describe MATCHKiosk, (Multimodal Access
To City Help Kiosk) an interactive public infor-
mation kiosk with a multimodal interface which
provides users with the flexibility to provide in-
put using speech, handwriting, touch, or composite
multimodal commands combining multiple differ-
ent modes. The system responds to the user by gen-
erating multimodal presentations which combine
spoken output, a life-like graphical talking head,
and dynamic graphical displays. MATCHKiosk
provides an interactive city guide for New York
and Washington D.C., including information about
restaurants and directions on the subway or metro.
It develops on our previous work on a multimodal
city guide on a mobile tablet (MATCH) (Johnston
et al., 2001; Johnston et al., 2002b; Johnston et al.,
2002a). The system has been deployed for testing
and data collection in an AT&amp;T facility in Wash-
ington, D.C. where it provides visitors with infor-
mation about places to eat, points of interest, and
getting around on the DC Metro.
</bodyText>
<sectionHeader confidence="0.970335" genericHeader="method">
2 The MATCHKiosk
</sectionHeader>
<bodyText confidence="0.999935">
The MATCHKiosk runs on a Windows PC mounted
in a rugged cabinet (Figure 1). It has a touch screen
which supports both touch and pen input, and also
contains a printer, whose output emerges from a slot
below the screen. The cabinet also contains speak-
ers and an array microphone is mounted above the
screen. There are three main components to the
graphical user interface (Figure 2). On the right,
there is a panel with a dynamic map display, a
click-to-speak button, and a window for feedback
on speech recognition. As the user interacts with
the system the map display dynamically pans and
zooms and the locations of restaurants and other
points of interest, graphical callouts with informa-
tion, and subway route segments are displayed. In
</bodyText>
<figureCaption confidence="0.999146">
Figure 1: Kiosk Hardware
</figureCaption>
<bodyText confidence="0.999861285714286">
the top left there is a photo-realistic virtual agent
(Cosatto and Graf, 2000), synthesized by concate-
nating and blending image samples. Below the
agent, there is a panel with large buttons which en-
able easy access to help and common functions. The
buttons presented are context sensitive and change
over the course of interaction.
</bodyText>
<figureCaption confidence="0.995667">
Figure 2: Kiosk Interface
</figureCaption>
<bodyText confidence="0.999988552238806">
The basic functions of the system are to enable
users to locate restaurants and other points of inter-
est based on attributes such as price, location, and
food type, to request information about them such
as phone numbers, addresses, and reviews, and to
provide directions on the subway or metro between
locations. There are also commands for panning and
zooming the map. The system provides users with
a high degree of flexibility in the inputs they use
in accessing these functions. For example, when
looking for restaurants the user can employ speech
e.g. find me moderately priced italian restaurants
in Alexandria, a multimodal combination of speech
and pen, e.g. moderate italian restaurants in this
area and circling Alexandria on the map, or solely
pen, e.g. user writes moderate italian and alexan-
dria. Similarly, when requesting directions they can
use speech, e.g. How do I get to the Smithsonian?,
multimodal, e.g. How do I get from here to here?
and circling or touching two locations on the map,
or pen, e.g. in Figure 2 the user has circled a loca-
tion on the map and handwritten the word route.
System output consists of coordinated presenta-
tions combining synthetic speech with graphical ac-
tions on the map. For example, when showing a
subway route, as the virtual agent speaks each in-
struction in turn, the map display zooms and shows
the corresponding route segment graphically. The
kiosk system also has a print capability. When a
route has been presented, one of the context sensi-
tive buttons changes to Print Directions. When this
is pressed the system generates an XHTML doc-
ument containing a map with step by step textual
directions and this is sent to the printer using an
XHTML-print capability.
If the system has low confidence in a user in-
put, based on the ASR or pen recognition score,
it requests confirmation from the user. The user
can confirm using speech, pen, or by touching on
a checkmark or cross mark which appear in the bot-
tom right of the screen. Context-sensitive graphi-
cal widgets are also used for resolving ambiguity
and vagueness in the user inputs. For example, if
the user asks for the Smithsonian Museum a small
menu appears in the bottom right of the map en-
abling them to select between the different museum
sites. If the user asks to see restaurants near a partic-
ular location, e.g. show restaurants near the white
house, a graphical slider appears enabling the user
to fine tune just how near.
The system also features a context-sensitive mul-
timodal help mechanism (Hastie et al., 2002) which
provides assistance to users in the context of their
current task, without redirecting them to separate
help system. The help system is triggered by spoken
or written requests for help, by touching the help
buttons on the left, or when the user has made sev-
eral unsuccessful inputs. The type of help is chosen
based on the current dialog state and the state of the
visual interface. If more than one type of help is ap-
plicable a graphical menu appears. Help messages
consist of multimodal presentations combining spo-
ken output with ink drawn on the display by the sys-
tem. For example, if the user has just requested to
see restaurants and they are now clearly visible on
the display, the system will provide help on getting
information about them.
</bodyText>
<sectionHeader confidence="0.998034" genericHeader="method">
3 Multimodal Kiosk Architecture
</sectionHeader>
<bodyText confidence="0.999468538461538">
The underlying architecture of MATCHKiosk con-
sists of a series of re-usable components which
communicate using XML messages sent over sock-
ets through a facilitator (MCUBE) (Figure 3). Users
interact with the system through the Multimodal UI
displayed on the touchscreen. Their speech and
ink are processed by speech recognition (ASR) and
handwriting/gesture recognition (GESTURE, HW
RECO) components respectively. These recogni-
tion processes result in lattices of potential words
and gestures/handwriting. These are then com-
bined and assigned a meaning representation using a
multimodal language processing architecture based
on finite-state techniques (MMFST) (Johnston and
Bangalore, 2000; Johnston et al., 2002b). This pro-
vides as output a lattice encoding all of the potential
meaning representations assigned to the user inputs.
This lattice is flattened to an N-best list and passed
to a multimodal dialog manager (MDM) (Johnston
et al., 2002b) which re-ranks them in accordance
with the current dialogue state. If additional infor-
mation or confirmation is required, the MDM uses
the virtual agent to enter into a short information
gathering dialogue with the user. Once a command
or query is complete, it is passed to the multimodal
generation component (MMGEN), which builds a
multimodal score indicating a coordinated sequence
of graphical actions and TTS prompts. This score
is passed back to the Multimodal UI. The Multi-
modal UI passes prompts to a visual text-to-speech
component (Cosatto and Graf, 2000) which com-
municates with the AT&amp;T Natural Voices TTS en-
gine (Beutnagel et al., 1999) in order to coordinate
the lip movements of the virtual agent with synthetic
speech output. As prompts are realized the Multi-
modal UI receives notifications and presents coordi-
nated graphical actions. The subway route server is
an application server which identifies the best route
between any two locations.
</bodyText>
<figureCaption confidence="0.994614">
Figure 3: Multimodal Kiosk Architecture
</figureCaption>
<sectionHeader confidence="0.981927" genericHeader="evaluation">
4 Discussion and Related Work
</sectionHeader>
<bodyText confidence="0.999945956989247">
A number of design issues arose in the development
of the kiosk, many of which highlight differences
between multimodal interfaces for kiosks and those
for mobile systems.
Array Microphone While on a mobile device a
close-talking headset or on-device microphone can
be used, we found that a single microphone had very
poor performance on the kiosk. Users stand in dif-
ferent positions with respect to the display and there
may be more than one person standing in front. To
overcome this problem we mounted an array micro-
phone above the touchscreen which tracks the loca-
tion of the talker.
Robust Recognition and Understanding is par-
ticularly important for kiosks since they have so
many first-time users. We utilize the techniques
for robust language modelling and multimodal
understanding described in Bangalore and John-
ston (2004).
Social Interaction For mobile multimodal inter-
faces, even those with graphical embodiment, we
found there to be little or no need to support so-
cial greetings and small talk. However, for a public
kiosk which different unknown users will approach
those capabilities are important. We added basic
support for social interaction to the language under-
standing and dialog components. The system is able
to respond to inputs such as Hello, How are you?,
Would you like to join us for lunch? and so on.
Context-sensitive GUI Compared to mobile sys-
tems, on palmtops, phones, and tablets, kiosks can
offer more screen real estate for graphical interac-
tion. This allowed for large easy to read buttons
for accessing help and other functions. The sys-
tem alters these as the dialog progresses. These but-
tons enable the system to support a kind of mixed-
initiative in multimodal interaction where the user
can take initiative in the spoken and handwritten
modes while the system is also able to provide
a more system-oriented initiative in the graphical
mode.
Printing Kiosks can make use of printed output
as a modality. One of the issues that arises is that
it is frequently the case that printed outputs such as
directions should take a very different style and for-
mat from onscreen presentations.
In previous work, a number of different multi-
modal kiosk systems supporting different sets of
input and output modalities have been developed.
The Touch-N-Speak kiosk (Raisamo, 1998) com-
bines spoken language input with a touchscreen.
The August system (Gustafson et al., 1999) is a mul-
timodal dialog system mounted in a public kiosk.
It supported spoken input from users and multi-
modal output with a talking head, text to speech,
and two graphical displays. The system was de-
ployed in a cultural center in Stockholm, enabling
collection of realistic data from the general public.
SmartKom-Public (Wahlster, 2003) is an interactive
public information kiosk that supports multimodal
input through speech, hand gestures, and facial ex-
pressions. The system uses a number of cameras
and a video projector for the display. The MASK
kiosk (Lamel et al., 2002) , developed by LIMSI and
the French national railway (SNCF), provides rail
tickets and information using a speech and touch in-
terface. The mVPQ kiosk system (Narayanan et al.,
2000) provides access to corporate directory infor-
mation and call completion. Users can provide in-
put by either speech or touching options presented
on a graphical display. MACK, the Media Lab
Autonomous Conversational Kiosk, (Cassell et al.,
2002) provides information about groups and indi-
viduals at the MIT Media Lab. Users interact us-
ing speech and gestures on a paper map that sits be-
tween the user and an embodied agent.
In contrast to August and mVPQ, MATCHKiosk
supports composite multimodal input combining
speech with pen drawings and touch. The
SmartKom-Public kiosk supports composite input,
but differs in that it uses free hand gesture for point-
ing while MATCH utilizes pen input and touch.
August, SmartKom-Public, and MATCHKiosk all
employ graphical embodiments. SmartKom uses
an animated character, August a model-based talk-
ing head, and MATCHKiosk a sample-based video-
realistic talking head. MACK uses articulated
graphical embodiment with ability to gesture. In
Touch-N-Speak a number of different techniques
using time and pressure are examined for enabling
selection of areas on a map using touch input. In
MATCHKiosk, this issue does not arise since areas
can be selected precisely by drawing with the pen.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.998828529411765">
We have presented a multimodal public informa-
tion kiosk, MATCHKiosk, which supports complex
unstructured tasks such as browsing for restaurants
and subway directions. Users have the flexibility to
interact using speech, pen/touch, or multimodal in-
puts. The system responds with multimodal presen-
tations which coordinate synthetic speech, a virtual
agent, graphical displays, and system use of elec-
tronic ink.
Acknowledgements Thanks to Eric Cosatto,
Hans Peter Graf, and Joern Ostermann for their help
with integrating the talking head. Thanks also to
Patrick Ehlen, Amanda Stent, Helen Hastie, Guna
Vasireddy, Mazin Rahim, Candy Kamm, Marilyn
Walker, Steve Whittaker, and Preetam Maloor for
their contributions to the MATCH project. Thanks
to Paul Burke for his assistance with XHTML-print.
</bodyText>
<sectionHeader confidence="0.99923" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999047904761905">
S. Bangalore and M. Johnston. 2004. Balancing
Data-driven and Rule-based Approaches in the
Context of a Multimodal Conversational System.
In Proceedings ofHLT-NAACL, Boston, MA.
M. Beutnagel, A. Conkie, J. Schroeter, Y. Stylianou,
and A. Syrdal. 1999. The AT&amp;T Next-
Generation TTS. In In Joint Meeting of ASA;
EAA and DAGA.
J. Cassell, T. Stocky, T. Bickmore, Y. Gao,
Y. Nakano, K. Ryokai, D. Tversky, C. Vaucelle,
and H. Vilhjalmsson. 2002. MACK: Media lab
autonomous conversational kiosk. In Proceed-
ings ofIMAGINA02, Monte Carlo.
E. Cosatto and H. P. Graf. 2000. Photo-realistic
Talking-heads from Image Samples. IEEE Trans-
actions on Multimedia, 2(3):152–163.
J. Gustafson, N. Lindberg, and M. Lundeberg.
1999. The August spoken dialogue system. In
Proceedings of Eurospeech 99, pages 1151–
1154.
H. Hastie, M. Johnston, and P. Ehlen. 2002.
Context-sensitive Help for Multimodal Dialogue.
In Proceedings of the 4th IEEE International
Conference on Multimodal Interfaces, pages 93–
98, Pittsburgh, PA.
M. Johnston and S. Bangalore. 2000. Finite-
state Multimodal Parsing and Understanding. In
Proceedings of COLING 2000, pages 369–375,
Saarbr¨ucken, Germany.
M. Johnston, S. Bangalore, and G. Vasireddy. 2001.
MATCH: Multimodal Access To City Help. In
Workshop on Automatic Speech Recognition and
Understanding, Madonna di Campiglio, Italy.
M. Johnston, S. Bangalore, A. Stent, G. Vasireddy,
and P. Ehlen. 2002a. Multimodal Language Pro-
cessing for Mobile Information Access. In Pro-
ceedings ofICSLP 2002, pages 2237–2240.
M. Johnston, S. Bangalore, G. Vasireddy, A. Stent,
P. Ehlen, M. Walker, S. Whittaker, and P. Mal-
oor. 2002b. MATCH: An Architecture for Mul-
timodal Dialog Systems. In Proceedings ofACL-
02, pages 376–383.
L. Lamel, S. Bennacef, J. L. Gauvain, H. Dartigues,
and J. N. Temem. 2002. User Evaluation of
the MASK Kiosk. Speech Communication, 38(1-
2):131–139.
S. Narayanan, G. DiFabbrizio, C. Kamm,
J. Hubbell, B. Buntschuh, P. Ruscitti, and
J. Wright. 2000. Effects of Dialog Initiative and
Multi-modal Presentation Strategies on Large
Directory Information Access. In Proceedings of
ICSLP 2000, pages 636–639.
S. Oviatt and P. Cohen. 2000. Multimodal Inter-
faces That Process What Comes Naturally. Com-
munications of the ACM, 43(3):45–53.
R. Raisamo. 1998. A Multimodal User Interface
for Public Information Kiosks. In Proceedings of
PUI Workshop, San Francisco.
W. Wahlster. 2003. SmartKom: Symmetric Multi-
modality in an Adaptive and Reusable Dialogue
Shell. In R. Krahl and D. Gunther, editors, Pro-
ceedings of the Human Computer Interaction Sta-
tus Conference 2003, pages 47–62.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.919175">
<title confidence="0.999798">MATCHKiosk: A Multimodal Interactive City Guide</title>
<author confidence="0.999984">Michael Johnston</author>
<affiliation confidence="0.999931">AT&amp;T Research</affiliation>
<address confidence="0.999046">180 Park Avenue Florham Park, NJ 07932</address>
<email confidence="0.999784">johnston@research.att.com</email>
<author confidence="0.939076">Srinivas Bangalore</author>
<affiliation confidence="0.999811">AT&amp;T Research</affiliation>
<address confidence="0.998932">180 Park Avenue Florham Park, NJ 07932</address>
<email confidence="0.999848">srini@research.att.com</email>
<abstract confidence="0.998492727272727">Multimodal interfaces provide more flexible and compelling interaction and can enable public information kiosks to support more complex tasks for a broader community of users. MATCHKiosk is a multimodal interactive city guide which provides users with the freedom to interact using speech, pen, touch or multimodal inputs. The system responds by generating multimodal presentations that synchronize synthetic speech with a life-like virtual agent and dynamically generated graphics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>M Johnston</author>
</authors>
<title>Balancing Data-driven and Rule-based Approaches in the Context of a Multimodal Conversational System.</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT-NAACL,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="10945" citStr="Bangalore and Johnston (2004)" startWordPosition="1758" endWordPosition="1762">bile device a close-talking headset or on-device microphone can be used, we found that a single microphone had very poor performance on the kiosk. Users stand in different positions with respect to the display and there may be more than one person standing in front. To overcome this problem we mounted an array microphone above the touchscreen which tracks the location of the talker. Robust Recognition and Understanding is particularly important for kiosks since they have so many first-time users. We utilize the techniques for robust language modelling and multimodal understanding described in Bangalore and Johnston (2004). Social Interaction For mobile multimodal interfaces, even those with graphical embodiment, we found there to be little or no need to support social greetings and small talk. However, for a public kiosk which different unknown users will approach those capabilities are important. We added basic support for social interaction to the language understanding and dialog components. The system is able to respond to inputs such as Hello, How are you?, Would you like to join us for lunch? and so on. Context-sensitive GUI Compared to mobile systems, on palmtops, phones, and tablets, kiosks can offer m</context>
</contexts>
<marker>Bangalore, Johnston, 2004</marker>
<rawString>S. Bangalore and M. Johnston. 2004. Balancing Data-driven and Rule-based Approaches in the Context of a Multimodal Conversational System. In Proceedings ofHLT-NAACL, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Beutnagel</author>
<author>A Conkie</author>
<author>J Schroeter</author>
<author>Y Stylianou</author>
<author>A Syrdal</author>
</authors>
<date>1999</date>
<booktitle>The AT&amp;T NextGeneration TTS. In In Joint Meeting of ASA; EAA and DAGA.</booktitle>
<contexts>
<context position="9734" citStr="Beutnagel et al., 1999" startWordPosition="1567" endWordPosition="1570">ordance with the current dialogue state. If additional information or confirmation is required, the MDM uses the virtual agent to enter into a short information gathering dialogue with the user. Once a command or query is complete, it is passed to the multimodal generation component (MMGEN), which builds a multimodal score indicating a coordinated sequence of graphical actions and TTS prompts. This score is passed back to the Multimodal UI. The Multimodal UI passes prompts to a visual text-to-speech component (Cosatto and Graf, 2000) which communicates with the AT&amp;T Natural Voices TTS engine (Beutnagel et al., 1999) in order to coordinate the lip movements of the virtual agent with synthetic speech output. As prompts are realized the Multimodal UI receives notifications and presents coordinated graphical actions. The subway route server is an application server which identifies the best route between any two locations. Figure 3: Multimodal Kiosk Architecture 4 Discussion and Related Work A number of design issues arose in the development of the kiosk, many of which highlight differences between multimodal interfaces for kiosks and those for mobile systems. Array Microphone While on a mobile device a clos</context>
</contexts>
<marker>Beutnagel, Conkie, Schroeter, Stylianou, Syrdal, 1999</marker>
<rawString>M. Beutnagel, A. Conkie, J. Schroeter, Y. Stylianou, and A. Syrdal. 1999. The AT&amp;T NextGeneration TTS. In In Joint Meeting of ASA; EAA and DAGA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cassell</author>
<author>T Stocky</author>
<author>T Bickmore</author>
<author>Y Gao</author>
<author>Y Nakano</author>
<author>K Ryokai</author>
<author>D Tversky</author>
<author>C Vaucelle</author>
<author>H Vilhjalmsson</author>
</authors>
<title>MACK: Media lab autonomous conversational kiosk.</title>
<date>2002</date>
<booktitle>In Proceedings ofIMAGINA02,</booktitle>
<location>Monte Carlo.</location>
<contexts>
<context position="2667" citStr="Cassell et al., 2002" startWordPosition="407" endWordPosition="410">and free for interaction. Kiosks often provide a touchscreen for input, opening up the possibility of an onscreen keyboard, but these can be awkward to use and occupy a considerable amount of screen real estate, generally leading to a more moded and cumbersome graphical interface. A number of experimental systems have investigated adding speech input to interactive graphical kiosks (Raisamo, 1998; Gustafson et al., 1999; Narayanan et al., 2000; Lamel et al., 2002). Other work has investigated adding both speech and gesture input (using computer vision) in an interactive kiosk (Wahlster, 2003; Cassell et al., 2002). We describe MATCHKiosk, (Multimodal Access To City Help Kiosk) an interactive public information kiosk with a multimodal interface which provides users with the flexibility to provide input using speech, handwriting, touch, or composite multimodal commands combining multiple different modes. The system responds to the user by generating multimodal presentations which combine spoken output, a life-like graphical talking head, and dynamic graphical displays. MATCHKiosk provides an interactive city guide for New York and Washington D.C., including information about restaurants and directions on</context>
<context position="13511" citStr="Cassell et al., 2002" startWordPosition="2178" endWordPosition="2181">that supports multimodal input through speech, hand gestures, and facial expressions. The system uses a number of cameras and a video projector for the display. The MASK kiosk (Lamel et al., 2002) , developed by LIMSI and the French national railway (SNCF), provides rail tickets and information using a speech and touch interface. The mVPQ kiosk system (Narayanan et al., 2000) provides access to corporate directory information and call completion. Users can provide input by either speech or touching options presented on a graphical display. MACK, the Media Lab Autonomous Conversational Kiosk, (Cassell et al., 2002) provides information about groups and individuals at the MIT Media Lab. Users interact using speech and gestures on a paper map that sits between the user and an embodied agent. In contrast to August and mVPQ, MATCHKiosk supports composite multimodal input combining speech with pen drawings and touch. The SmartKom-Public kiosk supports composite input, but differs in that it uses free hand gesture for pointing while MATCH utilizes pen input and touch. August, SmartKom-Public, and MATCHKiosk all employ graphical embodiments. SmartKom uses an animated character, August a model-based talking hea</context>
</contexts>
<marker>Cassell, Stocky, Bickmore, Gao, Nakano, Ryokai, Tversky, Vaucelle, Vilhjalmsson, 2002</marker>
<rawString>J. Cassell, T. Stocky, T. Bickmore, Y. Gao, Y. Nakano, K. Ryokai, D. Tversky, C. Vaucelle, and H. Vilhjalmsson. 2002. MACK: Media lab autonomous conversational kiosk. In Proceedings ofIMAGINA02, Monte Carlo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Cosatto</author>
<author>H P Graf</author>
</authors>
<title>Photo-realistic Talking-heads from Image Samples.</title>
<date>2000</date>
<journal>IEEE Transactions on Multimedia,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="4532" citStr="Cosatto and Graf, 2000" startWordPosition="709" endWordPosition="712">e cabinet also contains speakers and an array microphone is mounted above the screen. There are three main components to the graphical user interface (Figure 2). On the right, there is a panel with a dynamic map display, a click-to-speak button, and a window for feedback on speech recognition. As the user interacts with the system the map display dynamically pans and zooms and the locations of restaurants and other points of interest, graphical callouts with information, and subway route segments are displayed. In Figure 1: Kiosk Hardware the top left there is a photo-realistic virtual agent (Cosatto and Graf, 2000), synthesized by concatenating and blending image samples. Below the agent, there is a panel with large buttons which enable easy access to help and common functions. The buttons presented are context sensitive and change over the course of interaction. Figure 2: Kiosk Interface The basic functions of the system are to enable users to locate restaurants and other points of interest based on attributes such as price, location, and food type, to request information about them such as phone numbers, addresses, and reviews, and to provide directions on the subway or metro between locations. There </context>
<context position="9650" citStr="Cosatto and Graf, 2000" startWordPosition="1552" endWordPosition="1555"> multimodal dialog manager (MDM) (Johnston et al., 2002b) which re-ranks them in accordance with the current dialogue state. If additional information or confirmation is required, the MDM uses the virtual agent to enter into a short information gathering dialogue with the user. Once a command or query is complete, it is passed to the multimodal generation component (MMGEN), which builds a multimodal score indicating a coordinated sequence of graphical actions and TTS prompts. This score is passed back to the Multimodal UI. The Multimodal UI passes prompts to a visual text-to-speech component (Cosatto and Graf, 2000) which communicates with the AT&amp;T Natural Voices TTS engine (Beutnagel et al., 1999) in order to coordinate the lip movements of the virtual agent with synthetic speech output. As prompts are realized the Multimodal UI receives notifications and presents coordinated graphical actions. The subway route server is an application server which identifies the best route between any two locations. Figure 3: Multimodal Kiosk Architecture 4 Discussion and Related Work A number of design issues arose in the development of the kiosk, many of which highlight differences between multimodal interfaces for k</context>
</contexts>
<marker>Cosatto, Graf, 2000</marker>
<rawString>E. Cosatto and H. P. Graf. 2000. Photo-realistic Talking-heads from Image Samples. IEEE Transactions on Multimedia, 2(3):152–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gustafson</author>
<author>N Lindberg</author>
<author>M Lundeberg</author>
</authors>
<title>The August spoken dialogue system.</title>
<date>1999</date>
<booktitle>In Proceedings of Eurospeech 99,</booktitle>
<pages>1151--1154</pages>
<contexts>
<context position="2469" citStr="Gustafson et al., 1999" startWordPosition="375" endWordPosition="378">giene, or space concerns make a physical keyboard or mouse impractical. Also, mobile users interacting with kiosks are often encumbered with briefcases, phones, or other equipment, leaving only one hand free for interaction. Kiosks often provide a touchscreen for input, opening up the possibility of an onscreen keyboard, but these can be awkward to use and occupy a considerable amount of screen real estate, generally leading to a more moded and cumbersome graphical interface. A number of experimental systems have investigated adding speech input to interactive graphical kiosks (Raisamo, 1998; Gustafson et al., 1999; Narayanan et al., 2000; Lamel et al., 2002). Other work has investigated adding both speech and gesture input (using computer vision) in an interactive kiosk (Wahlster, 2003; Cassell et al., 2002). We describe MATCHKiosk, (Multimodal Access To City Help Kiosk) an interactive public information kiosk with a multimodal interface which provides users with the flexibility to provide input using speech, handwriting, touch, or composite multimodal commands combining multiple different modes. The system responds to the user by generating multimodal presentations which combine spoken output, a life-</context>
<context position="12510" citStr="Gustafson et al., 1999" startWordPosition="2017" endWordPosition="2020">n modes while the system is also able to provide a more system-oriented initiative in the graphical mode. Printing Kiosks can make use of printed output as a modality. One of the issues that arises is that it is frequently the case that printed outputs such as directions should take a very different style and format from onscreen presentations. In previous work, a number of different multimodal kiosk systems supporting different sets of input and output modalities have been developed. The Touch-N-Speak kiosk (Raisamo, 1998) combines spoken language input with a touchscreen. The August system (Gustafson et al., 1999) is a multimodal dialog system mounted in a public kiosk. It supported spoken input from users and multimodal output with a talking head, text to speech, and two graphical displays. The system was deployed in a cultural center in Stockholm, enabling collection of realistic data from the general public. SmartKom-Public (Wahlster, 2003) is an interactive public information kiosk that supports multimodal input through speech, hand gestures, and facial expressions. The system uses a number of cameras and a video projector for the display. The MASK kiosk (Lamel et al., 2002) , developed by LIMSI an</context>
</contexts>
<marker>Gustafson, Lindberg, Lundeberg, 1999</marker>
<rawString>J. Gustafson, N. Lindberg, and M. Lundeberg. 1999. The August spoken dialogue system. In Proceedings of Eurospeech 99, pages 1151– 1154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hastie</author>
<author>M Johnston</author>
<author>P Ehlen</author>
</authors>
<title>Context-sensitive Help for Multimodal Dialogue.</title>
<date>2002</date>
<booktitle>In Proceedings of the 4th IEEE International Conference on Multimodal Interfaces,</booktitle>
<pages>93--98</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="7347" citStr="Hastie et al., 2002" startWordPosition="1187" endWordPosition="1190">eckmark or cross mark which appear in the bottom right of the screen. Context-sensitive graphical widgets are also used for resolving ambiguity and vagueness in the user inputs. For example, if the user asks for the Smithsonian Museum a small menu appears in the bottom right of the map enabling them to select between the different museum sites. If the user asks to see restaurants near a particular location, e.g. show restaurants near the white house, a graphical slider appears enabling the user to fine tune just how near. The system also features a context-sensitive multimodal help mechanism (Hastie et al., 2002) which provides assistance to users in the context of their current task, without redirecting them to separate help system. The help system is triggered by spoken or written requests for help, by touching the help buttons on the left, or when the user has made several unsuccessful inputs. The type of help is chosen based on the current dialog state and the state of the visual interface. If more than one type of help is applicable a graphical menu appears. Help messages consist of multimodal presentations combining spoken output with ink drawn on the display by the system. For example, if the u</context>
</contexts>
<marker>Hastie, Johnston, Ehlen, 2002</marker>
<rawString>H. Hastie, M. Johnston, and P. Ehlen. 2002. Context-sensitive Help for Multimodal Dialogue. In Proceedings of the 4th IEEE International Conference on Multimodal Interfaces, pages 93– 98, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
<author>S Bangalore</author>
</authors>
<title>Finitestate Multimodal Parsing and Understanding.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING 2000,</booktitle>
<pages>369--375</pages>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="1745" citStr="Johnston and Bangalore, 2000" startWordPosition="262" endWordPosition="265">s in tourist offices and museums, and more recently, automated check-out in retail stores. The majority of these systems provide a rigid structured graphical interface and user input by only touch or keypad, and as a result can only support a small number of simple tasks. As automated kiosks become more commonplace and have to support more complex tasks for a broader community of users, they will need to provide a more flexible and compelling user interface. One major motivation for developing multimodal interfaces for mobile devices is the lack of a keyboard or mouse (Oviatt and Cohen, 2000; Johnston and Bangalore, 2000). This limitation is also true of many different kinds of public information kiosks where security, hygiene, or space concerns make a physical keyboard or mouse impractical. Also, mobile users interacting with kiosks are often encumbered with briefcases, phones, or other equipment, leaving only one hand free for interaction. Kiosks often provide a touchscreen for input, opening up the possibility of an onscreen keyboard, but these can be awkward to use and occupy a considerable amount of screen real estate, generally leading to a more moded and cumbersome graphical interface. A number of exper</context>
<context position="8824" citStr="Johnston and Bangalore, 2000" startWordPosition="1419" endWordPosition="1422"> of re-usable components which communicate using XML messages sent over sockets through a facilitator (MCUBE) (Figure 3). Users interact with the system through the Multimodal UI displayed on the touchscreen. Their speech and ink are processed by speech recognition (ASR) and handwriting/gesture recognition (GESTURE, HW RECO) components respectively. These recognition processes result in lattices of potential words and gestures/handwriting. These are then combined and assigned a meaning representation using a multimodal language processing architecture based on finite-state techniques (MMFST) (Johnston and Bangalore, 2000; Johnston et al., 2002b). This provides as output a lattice encoding all of the potential meaning representations assigned to the user inputs. This lattice is flattened to an N-best list and passed to a multimodal dialog manager (MDM) (Johnston et al., 2002b) which re-ranks them in accordance with the current dialogue state. If additional information or confirmation is required, the MDM uses the virtual agent to enter into a short information gathering dialogue with the user. Once a command or query is complete, it is passed to the multimodal generation component (MMGEN), which builds a multi</context>
</contexts>
<marker>Johnston, Bangalore, 2000</marker>
<rawString>M. Johnston and S. Bangalore. 2000. Finitestate Multimodal Parsing and Understanding. In Proceedings of COLING 2000, pages 369–375, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
<author>S Bangalore</author>
<author>G Vasireddy</author>
</authors>
<title>MATCH: Multimodal Access To City Help.</title>
<date>2001</date>
<booktitle>In Workshop on Automatic Speech Recognition and Understanding, Madonna di Campiglio,</booktitle>
<location>Italy.</location>
<contexts>
<context position="3398" citStr="Johnston et al., 2001" startWordPosition="516" endWordPosition="519">a multimodal interface which provides users with the flexibility to provide input using speech, handwriting, touch, or composite multimodal commands combining multiple different modes. The system responds to the user by generating multimodal presentations which combine spoken output, a life-like graphical talking head, and dynamic graphical displays. MATCHKiosk provides an interactive city guide for New York and Washington D.C., including information about restaurants and directions on the subway or metro. It develops on our previous work on a multimodal city guide on a mobile tablet (MATCH) (Johnston et al., 2001; Johnston et al., 2002b; Johnston et al., 2002a). The system has been deployed for testing and data collection in an AT&amp;T facility in Washington, D.C. where it provides visitors with information about places to eat, points of interest, and getting around on the DC Metro. 2 The MATCHKiosk The MATCHKiosk runs on a Windows PC mounted in a rugged cabinet (Figure 1). It has a touch screen which supports both touch and pen input, and also contains a printer, whose output emerges from a slot below the screen. The cabinet also contains speakers and an array microphone is mounted above the screen. The</context>
</contexts>
<marker>Johnston, Bangalore, Vasireddy, 2001</marker>
<rawString>M. Johnston, S. Bangalore, and G. Vasireddy. 2001. MATCH: Multimodal Access To City Help. In Workshop on Automatic Speech Recognition and Understanding, Madonna di Campiglio, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
<author>S Bangalore</author>
<author>A Stent</author>
<author>G Vasireddy</author>
<author>P Ehlen</author>
</authors>
<title>Multimodal Language Processing for Mobile Information Access.</title>
<date>2002</date>
<booktitle>In Proceedings ofICSLP 2002,</booktitle>
<pages>2237--2240</pages>
<contexts>
<context position="3421" citStr="Johnston et al., 2002" startWordPosition="520" endWordPosition="523">which provides users with the flexibility to provide input using speech, handwriting, touch, or composite multimodal commands combining multiple different modes. The system responds to the user by generating multimodal presentations which combine spoken output, a life-like graphical talking head, and dynamic graphical displays. MATCHKiosk provides an interactive city guide for New York and Washington D.C., including information about restaurants and directions on the subway or metro. It develops on our previous work on a multimodal city guide on a mobile tablet (MATCH) (Johnston et al., 2001; Johnston et al., 2002b; Johnston et al., 2002a). The system has been deployed for testing and data collection in an AT&amp;T facility in Washington, D.C. where it provides visitors with information about places to eat, points of interest, and getting around on the DC Metro. 2 The MATCHKiosk The MATCHKiosk runs on a Windows PC mounted in a rugged cabinet (Figure 1). It has a touch screen which supports both touch and pen input, and also contains a printer, whose output emerges from a slot below the screen. The cabinet also contains speakers and an array microphone is mounted above the screen. There are three main compo</context>
<context position="8847" citStr="Johnston et al., 2002" startWordPosition="1423" endWordPosition="1426"> communicate using XML messages sent over sockets through a facilitator (MCUBE) (Figure 3). Users interact with the system through the Multimodal UI displayed on the touchscreen. Their speech and ink are processed by speech recognition (ASR) and handwriting/gesture recognition (GESTURE, HW RECO) components respectively. These recognition processes result in lattices of potential words and gestures/handwriting. These are then combined and assigned a meaning representation using a multimodal language processing architecture based on finite-state techniques (MMFST) (Johnston and Bangalore, 2000; Johnston et al., 2002b). This provides as output a lattice encoding all of the potential meaning representations assigned to the user inputs. This lattice is flattened to an N-best list and passed to a multimodal dialog manager (MDM) (Johnston et al., 2002b) which re-ranks them in accordance with the current dialogue state. If additional information or confirmation is required, the MDM uses the virtual agent to enter into a short information gathering dialogue with the user. Once a command or query is complete, it is passed to the multimodal generation component (MMGEN), which builds a multimodal score indicating </context>
</contexts>
<marker>Johnston, Bangalore, Stent, Vasireddy, Ehlen, 2002</marker>
<rawString>M. Johnston, S. Bangalore, A. Stent, G. Vasireddy, and P. Ehlen. 2002a. Multimodal Language Processing for Mobile Information Access. In Proceedings ofICSLP 2002, pages 2237–2240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
<author>S Bangalore</author>
<author>G Vasireddy</author>
<author>A Stent</author>
<author>P Ehlen</author>
<author>M Walker</author>
<author>S Whittaker</author>
<author>P Maloor</author>
</authors>
<title>MATCH: An Architecture for Multimodal Dialog Systems.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL02,</booktitle>
<pages>376--383</pages>
<contexts>
<context position="3421" citStr="Johnston et al., 2002" startWordPosition="520" endWordPosition="523">which provides users with the flexibility to provide input using speech, handwriting, touch, or composite multimodal commands combining multiple different modes. The system responds to the user by generating multimodal presentations which combine spoken output, a life-like graphical talking head, and dynamic graphical displays. MATCHKiosk provides an interactive city guide for New York and Washington D.C., including information about restaurants and directions on the subway or metro. It develops on our previous work on a multimodal city guide on a mobile tablet (MATCH) (Johnston et al., 2001; Johnston et al., 2002b; Johnston et al., 2002a). The system has been deployed for testing and data collection in an AT&amp;T facility in Washington, D.C. where it provides visitors with information about places to eat, points of interest, and getting around on the DC Metro. 2 The MATCHKiosk The MATCHKiosk runs on a Windows PC mounted in a rugged cabinet (Figure 1). It has a touch screen which supports both touch and pen input, and also contains a printer, whose output emerges from a slot below the screen. The cabinet also contains speakers and an array microphone is mounted above the screen. There are three main compo</context>
<context position="8847" citStr="Johnston et al., 2002" startWordPosition="1423" endWordPosition="1426"> communicate using XML messages sent over sockets through a facilitator (MCUBE) (Figure 3). Users interact with the system through the Multimodal UI displayed on the touchscreen. Their speech and ink are processed by speech recognition (ASR) and handwriting/gesture recognition (GESTURE, HW RECO) components respectively. These recognition processes result in lattices of potential words and gestures/handwriting. These are then combined and assigned a meaning representation using a multimodal language processing architecture based on finite-state techniques (MMFST) (Johnston and Bangalore, 2000; Johnston et al., 2002b). This provides as output a lattice encoding all of the potential meaning representations assigned to the user inputs. This lattice is flattened to an N-best list and passed to a multimodal dialog manager (MDM) (Johnston et al., 2002b) which re-ranks them in accordance with the current dialogue state. If additional information or confirmation is required, the MDM uses the virtual agent to enter into a short information gathering dialogue with the user. Once a command or query is complete, it is passed to the multimodal generation component (MMGEN), which builds a multimodal score indicating </context>
</contexts>
<marker>Johnston, Bangalore, Vasireddy, Stent, Ehlen, Walker, Whittaker, Maloor, 2002</marker>
<rawString>M. Johnston, S. Bangalore, G. Vasireddy, A. Stent, P. Ehlen, M. Walker, S. Whittaker, and P. Maloor. 2002b. MATCH: An Architecture for Multimodal Dialog Systems. In Proceedings ofACL02, pages 376–383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lamel</author>
<author>S Bennacef</author>
<author>J L Gauvain</author>
<author>H Dartigues</author>
<author>J N Temem</author>
</authors>
<date>2002</date>
<journal>User Evaluation of the MASK Kiosk. Speech Communication,</journal>
<pages>38--1</pages>
<contexts>
<context position="2514" citStr="Lamel et al., 2002" startWordPosition="383" endWordPosition="386">d or mouse impractical. Also, mobile users interacting with kiosks are often encumbered with briefcases, phones, or other equipment, leaving only one hand free for interaction. Kiosks often provide a touchscreen for input, opening up the possibility of an onscreen keyboard, but these can be awkward to use and occupy a considerable amount of screen real estate, generally leading to a more moded and cumbersome graphical interface. A number of experimental systems have investigated adding speech input to interactive graphical kiosks (Raisamo, 1998; Gustafson et al., 1999; Narayanan et al., 2000; Lamel et al., 2002). Other work has investigated adding both speech and gesture input (using computer vision) in an interactive kiosk (Wahlster, 2003; Cassell et al., 2002). We describe MATCHKiosk, (Multimodal Access To City Help Kiosk) an interactive public information kiosk with a multimodal interface which provides users with the flexibility to provide input using speech, handwriting, touch, or composite multimodal commands combining multiple different modes. The system responds to the user by generating multimodal presentations which combine spoken output, a life-like graphical talking head, and dynamic grap</context>
<context position="13086" citStr="Lamel et al., 2002" startWordPosition="2111" endWordPosition="2114"> The August system (Gustafson et al., 1999) is a multimodal dialog system mounted in a public kiosk. It supported spoken input from users and multimodal output with a talking head, text to speech, and two graphical displays. The system was deployed in a cultural center in Stockholm, enabling collection of realistic data from the general public. SmartKom-Public (Wahlster, 2003) is an interactive public information kiosk that supports multimodal input through speech, hand gestures, and facial expressions. The system uses a number of cameras and a video projector for the display. The MASK kiosk (Lamel et al., 2002) , developed by LIMSI and the French national railway (SNCF), provides rail tickets and information using a speech and touch interface. The mVPQ kiosk system (Narayanan et al., 2000) provides access to corporate directory information and call completion. Users can provide input by either speech or touching options presented on a graphical display. MACK, the Media Lab Autonomous Conversational Kiosk, (Cassell et al., 2002) provides information about groups and individuals at the MIT Media Lab. Users interact using speech and gestures on a paper map that sits between the user and an embodied age</context>
</contexts>
<marker>Lamel, Bennacef, Gauvain, Dartigues, Temem, 2002</marker>
<rawString>L. Lamel, S. Bennacef, J. L. Gauvain, H. Dartigues, and J. N. Temem. 2002. User Evaluation of the MASK Kiosk. Speech Communication, 38(1-2):131–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Narayanan</author>
<author>G DiFabbrizio</author>
<author>C Kamm</author>
<author>J Hubbell</author>
<author>B Buntschuh</author>
<author>P Ruscitti</author>
<author>J Wright</author>
</authors>
<title>Effects of Dialog Initiative and Multi-modal Presentation Strategies on Large Directory Information Access.</title>
<date>2000</date>
<booktitle>In Proceedings of ICSLP</booktitle>
<pages>636--639</pages>
<contexts>
<context position="2493" citStr="Narayanan et al., 2000" startWordPosition="379" endWordPosition="382"> make a physical keyboard or mouse impractical. Also, mobile users interacting with kiosks are often encumbered with briefcases, phones, or other equipment, leaving only one hand free for interaction. Kiosks often provide a touchscreen for input, opening up the possibility of an onscreen keyboard, but these can be awkward to use and occupy a considerable amount of screen real estate, generally leading to a more moded and cumbersome graphical interface. A number of experimental systems have investigated adding speech input to interactive graphical kiosks (Raisamo, 1998; Gustafson et al., 1999; Narayanan et al., 2000; Lamel et al., 2002). Other work has investigated adding both speech and gesture input (using computer vision) in an interactive kiosk (Wahlster, 2003; Cassell et al., 2002). We describe MATCHKiosk, (Multimodal Access To City Help Kiosk) an interactive public information kiosk with a multimodal interface which provides users with the flexibility to provide input using speech, handwriting, touch, or composite multimodal commands combining multiple different modes. The system responds to the user by generating multimodal presentations which combine spoken output, a life-like graphical talking h</context>
<context position="13268" citStr="Narayanan et al., 2000" startWordPosition="2141" endWordPosition="2144">d, text to speech, and two graphical displays. The system was deployed in a cultural center in Stockholm, enabling collection of realistic data from the general public. SmartKom-Public (Wahlster, 2003) is an interactive public information kiosk that supports multimodal input through speech, hand gestures, and facial expressions. The system uses a number of cameras and a video projector for the display. The MASK kiosk (Lamel et al., 2002) , developed by LIMSI and the French national railway (SNCF), provides rail tickets and information using a speech and touch interface. The mVPQ kiosk system (Narayanan et al., 2000) provides access to corporate directory information and call completion. Users can provide input by either speech or touching options presented on a graphical display. MACK, the Media Lab Autonomous Conversational Kiosk, (Cassell et al., 2002) provides information about groups and individuals at the MIT Media Lab. Users interact using speech and gestures on a paper map that sits between the user and an embodied agent. In contrast to August and mVPQ, MATCHKiosk supports composite multimodal input combining speech with pen drawings and touch. The SmartKom-Public kiosk supports composite input, b</context>
</contexts>
<marker>Narayanan, DiFabbrizio, Kamm, Hubbell, Buntschuh, Ruscitti, Wright, 2000</marker>
<rawString>S. Narayanan, G. DiFabbrizio, C. Kamm, J. Hubbell, B. Buntschuh, P. Ruscitti, and J. Wright. 2000. Effects of Dialog Initiative and Multi-modal Presentation Strategies on Large Directory Information Access. In Proceedings of ICSLP 2000, pages 636–639.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oviatt</author>
<author>P Cohen</author>
</authors>
<title>Multimodal Interfaces That Process What Comes Naturally.</title>
<date>2000</date>
<journal>Communications of the ACM,</journal>
<volume>43</volume>
<issue>3</issue>
<contexts>
<context position="1714" citStr="Oviatt and Cohen, 2000" startWordPosition="258" endWordPosition="261">ourist and visitor guides in tourist offices and museums, and more recently, automated check-out in retail stores. The majority of these systems provide a rigid structured graphical interface and user input by only touch or keypad, and as a result can only support a small number of simple tasks. As automated kiosks become more commonplace and have to support more complex tasks for a broader community of users, they will need to provide a more flexible and compelling user interface. One major motivation for developing multimodal interfaces for mobile devices is the lack of a keyboard or mouse (Oviatt and Cohen, 2000; Johnston and Bangalore, 2000). This limitation is also true of many different kinds of public information kiosks where security, hygiene, or space concerns make a physical keyboard or mouse impractical. Also, mobile users interacting with kiosks are often encumbered with briefcases, phones, or other equipment, leaving only one hand free for interaction. Kiosks often provide a touchscreen for input, opening up the possibility of an onscreen keyboard, but these can be awkward to use and occupy a considerable amount of screen real estate, generally leading to a more moded and cumbersome graphic</context>
</contexts>
<marker>Oviatt, Cohen, 2000</marker>
<rawString>S. Oviatt and P. Cohen. 2000. Multimodal Interfaces That Process What Comes Naturally. Communications of the ACM, 43(3):45–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Raisamo</author>
</authors>
<title>A Multimodal User Interface for Public Information Kiosks.</title>
<date>1998</date>
<booktitle>In Proceedings of PUI Workshop,</booktitle>
<location>San Francisco.</location>
<contexts>
<context position="2445" citStr="Raisamo, 1998" startWordPosition="373" endWordPosition="374">re security, hygiene, or space concerns make a physical keyboard or mouse impractical. Also, mobile users interacting with kiosks are often encumbered with briefcases, phones, or other equipment, leaving only one hand free for interaction. Kiosks often provide a touchscreen for input, opening up the possibility of an onscreen keyboard, but these can be awkward to use and occupy a considerable amount of screen real estate, generally leading to a more moded and cumbersome graphical interface. A number of experimental systems have investigated adding speech input to interactive graphical kiosks (Raisamo, 1998; Gustafson et al., 1999; Narayanan et al., 2000; Lamel et al., 2002). Other work has investigated adding both speech and gesture input (using computer vision) in an interactive kiosk (Wahlster, 2003; Cassell et al., 2002). We describe MATCHKiosk, (Multimodal Access To City Help Kiosk) an interactive public information kiosk with a multimodal interface which provides users with the flexibility to provide input using speech, handwriting, touch, or composite multimodal commands combining multiple different modes. The system responds to the user by generating multimodal presentations which combin</context>
<context position="12416" citStr="Raisamo, 1998" startWordPosition="2004" endWordPosition="2005">ultimodal interaction where the user can take initiative in the spoken and handwritten modes while the system is also able to provide a more system-oriented initiative in the graphical mode. Printing Kiosks can make use of printed output as a modality. One of the issues that arises is that it is frequently the case that printed outputs such as directions should take a very different style and format from onscreen presentations. In previous work, a number of different multimodal kiosk systems supporting different sets of input and output modalities have been developed. The Touch-N-Speak kiosk (Raisamo, 1998) combines spoken language input with a touchscreen. The August system (Gustafson et al., 1999) is a multimodal dialog system mounted in a public kiosk. It supported spoken input from users and multimodal output with a talking head, text to speech, and two graphical displays. The system was deployed in a cultural center in Stockholm, enabling collection of realistic data from the general public. SmartKom-Public (Wahlster, 2003) is an interactive public information kiosk that supports multimodal input through speech, hand gestures, and facial expressions. The system uses a number of cameras and </context>
</contexts>
<marker>Raisamo, 1998</marker>
<rawString>R. Raisamo. 1998. A Multimodal User Interface for Public Information Kiosks. In Proceedings of PUI Workshop, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wahlster</author>
</authors>
<title>SmartKom: Symmetric Multimodality in an Adaptive and Reusable Dialogue Shell. In</title>
<date>2003</date>
<booktitle>Proceedings of the Human Computer Interaction Status Conference</booktitle>
<pages>47--62</pages>
<editor>R. Krahl and D. Gunther, editors,</editor>
<contexts>
<context position="2644" citStr="Wahlster, 2003" startWordPosition="405" endWordPosition="406">aving only one hand free for interaction. Kiosks often provide a touchscreen for input, opening up the possibility of an onscreen keyboard, but these can be awkward to use and occupy a considerable amount of screen real estate, generally leading to a more moded and cumbersome graphical interface. A number of experimental systems have investigated adding speech input to interactive graphical kiosks (Raisamo, 1998; Gustafson et al., 1999; Narayanan et al., 2000; Lamel et al., 2002). Other work has investigated adding both speech and gesture input (using computer vision) in an interactive kiosk (Wahlster, 2003; Cassell et al., 2002). We describe MATCHKiosk, (Multimodal Access To City Help Kiosk) an interactive public information kiosk with a multimodal interface which provides users with the flexibility to provide input using speech, handwriting, touch, or composite multimodal commands combining multiple different modes. The system responds to the user by generating multimodal presentations which combine spoken output, a life-like graphical talking head, and dynamic graphical displays. MATCHKiosk provides an interactive city guide for New York and Washington D.C., including information about restau</context>
<context position="12846" citStr="Wahlster, 2003" startWordPosition="2074" endWordPosition="2075">s. In previous work, a number of different multimodal kiosk systems supporting different sets of input and output modalities have been developed. The Touch-N-Speak kiosk (Raisamo, 1998) combines spoken language input with a touchscreen. The August system (Gustafson et al., 1999) is a multimodal dialog system mounted in a public kiosk. It supported spoken input from users and multimodal output with a talking head, text to speech, and two graphical displays. The system was deployed in a cultural center in Stockholm, enabling collection of realistic data from the general public. SmartKom-Public (Wahlster, 2003) is an interactive public information kiosk that supports multimodal input through speech, hand gestures, and facial expressions. The system uses a number of cameras and a video projector for the display. The MASK kiosk (Lamel et al., 2002) , developed by LIMSI and the French national railway (SNCF), provides rail tickets and information using a speech and touch interface. The mVPQ kiosk system (Narayanan et al., 2000) provides access to corporate directory information and call completion. Users can provide input by either speech or touching options presented on a graphical display. MACK, the </context>
</contexts>
<marker>Wahlster, 2003</marker>
<rawString>W. Wahlster. 2003. SmartKom: Symmetric Multimodality in an Adaptive and Reusable Dialogue Shell. In R. Krahl and D. Gunther, editors, Proceedings of the Human Computer Interaction Status Conference 2003, pages 47–62.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>