<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000424">
<title confidence="0.96399">
HITS-based Seed Selection and Stop List Construction for Bootstrapping
</title>
<author confidence="0.991292">
Tetsuo Kiso Masashi Shimbo Mamoru Komachi Yuji Matsumoto
</author>
<affiliation confidence="0.938667666666667">
Graduate School of Information Science
Nara Institute of Science and Technology
Ikoma, Nara 630-0192, Japan
</affiliation>
<email confidence="0.999097">
{tetsuo-s,shimbo,komachi,matsu}@is.naist.jp
</email>
<sectionHeader confidence="0.997392" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999938428571429">
In bootstrapping (seed set expansion), select-
ing good seeds and creating stop lists are two
effective ways to reduce semantic drift, but
these methods generally need human super-
vision. In this paper, we propose a graph-
based approach to helping editors choose ef-
fective seeds and stop list instances, appli-
cable to Pantel and Pennacchiotti’s Espresso
bootstrapping algorithm. The idea is to select
seeds and create a stop list using the rankings
of instances and patterns computed by Klein-
berg’s HITS algorithm. Experimental results
on a variation of the lexical sample task show
the effectiveness of our method.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997437">
Bootstrapping (Yarowsky, 1995; Abney, 2004) is a
technique frequently used in natural language pro-
cessing to expand limited resources with minimal
supervision. Given a small amount of sample data
(seeds) representing a particular semantic class of
interest, bootstrapping first trains a classifier (which
often is a weighted list of surface patterns character-
izing the seeds) using the seeds, and then apply it on
the remaining data to select instances most likely to
be of the same class as the seeds. These selected in-
stances are added to the seed set, and the process is
iterated until sufficient labeled data are acquired.
Many bootstrapping algorithms have been pro-
posed for a variety of tasks: word sense disambigua-
tion (Yarowsky, 1995; Abney, 2004), information
extraction (Hearst, 1992; Riloff and Jones, 1999;
Thelen and Riloff, 2002; Pantel and Pennacchiotti,
2006), named entity recognition (Collins and Singer,
1999), part-of-speech tagging (Clark et al., 2003),
</bodyText>
<page confidence="0.733667">
30
</page>
<figureCaption confidence="0.3366525">
and statistical parsing (Steedman et al., 2003; Mc-
Closky et al., 2006).
</figureCaption>
<bodyText confidence="0.984510827160494">
Bootstrapping algorithms, however, are known to
suffer from the problem called semantic drift: as the
iteration proceeds, the algorithms tend to select in-
stances increasingly irrelevant to the seed instances
(Curran et al., 2007). For example, suppose we want
to collect the names of common tourist sites from a
web corpus. Given seed instances {New York City,
Maldives Islands}, bootstrapping might learn, at one
point of the iteration, patterns like “pictures of X”
and “photos of X,” which also co-occur with many
irrelevant instances. In this case, a later iteration
would likely acquire frequent words co-occurring
with these generic patterns, such as Michael Jack-
son.
Previous work has tried to reduce the effect of se-
mantic drift by making the stop list of instances that
must not be extracted (Curran et al., 2007; McIntosh
and Curran, 2009). Drift can also be reduced with
carefully selected seeds. However, both of these ap-
proaches require expert knowledge.
In this paper, we propose a graph-based approach
to seed selection and stop list creation for the state-
of-the-art bootstrapping algorithm Espresso (Pantel
and Pennacchiotti, 2006). An advantage of this ap-
proach is that it requires zero or minimal super-
vision. The idea is to use the hubness score of
instances and patterns computed from the point-
wise mutual information matrix with the HITS al-
gorithm (Kleinberg, 1999). Komachi et al. (2008)
pointed out that semantic drift in Espresso has the
same root as topic drift (Bharat and Henzinger,
1998) observed with HITS, noting the algorithmic
similarity between them. While Komachi et al. pro-
posed to use algorithms different from Espresso to
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 30–36,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
avoid semantic drift, in this paper we take advantage Algorithm 1 Espresso algorithm
of this similarity to make better use of Espresso.
We demonstrate the effectiveness of our approach
on a word sense disambiguation task.
2 Background
In this section, we review related work on seed se-
lection and stop list construction. We also briefly in-
troduce the Espresso bootstrapping algorithm (Pan-
tel and Pennacchiotti, 2006) for which we build our
seed selection and stop list construction methods.
2.1 Seed Selection
The performance of bootstrapping can be greatly in-
fluenced by a number of factors such as the size of
the seed set, the composition of the seed set and the
coherence of the concept being expanded (Vyas et
al., 2009). Vyas et al. (2009) studied the impact of
the composition of the seed sets on the expansion
performance, confirming that seed set composition
has a significant impact on the quality of expansions.
They also found that the seeds chosen by non-expert
editors are often worse than randomly chosen ones.
A similar observation was made by McIntosh and
Curran (2009), who reported that randomly chosen
seeds from the gold-standard set often outperformed
seeds chosen by domain experts. These results sug-
gest that even for humans, selecting good seeds is a
non-trivial task.
2.2 Stop Lists
Yangarber et al. (2002) proposed to run multiple
bootstrapping sessions in parallel, with each session
trying to extract one of several mutually exclusive
semantic classes. Thus, the instances harvested in
one bootstrapping session can be used as the stop
list of the other sessions. Curran et al. (2007) pur-
sued a similar idea in their Mutual Exclusion Boot-
strapping, which uses multiple semantic classes in
addition to hand-crafted stop lists. While multi-class
bootstrapping is a clever way to reduce human su-
pervision in stop list construction, it is not generally
applicable to bootstrapping for a single class. To ap-
ply the idea of multi-class bootstrapping to single-
class bootstrapping, one has to first find appropri-
ate competing semantic classes and good seeds for
them, which is in itself a difficult problem. Along
this line of research, McIntosh (2010) recently used
</bodyText>
<page confidence="0.432421">
31
</page>
<listItem confidence="0.990915368421053">
1: Input: Seed vector i0
2: Instance-pattern co-occurrence matrix A
3: Instance cutoff parameter k
4: Pattern cutoff parameter m
5: Number of iterations ti
6: Output: Instance score vector i
7: Pattern score vector p
8: function ESPRESSO(i0,A,k,m,ti)
9: i ← i0
10: fort= 1,2,...,ti do
11: p ← ATi
12: Scale p so that the components sum to one.
13: p ← SELECTKBEST(p,k)
14: i ← Ap
15: Scale i so that the components sum to one.
16: i ← SELECTKBEST(i,m)
17: return i and p
18: function SELECTKBEST(v,k)
19: Retain only the k largest components of v, resetting the
</listItem>
<bodyText confidence="0.95146724137931">
remaining components to 0.
20: return v
clustering to find competing semantic classes (nega-
tive categories).
2.3 Espresso
Espresso (Pantel and Pennacchiotti, 2006) is one of
the state-of-the-art bootstrapping algorithms used in
many natural language tasks (Komachi and Suzuki,
2008; Abe et al., 2008; Ittoo and Bouma, 2010;
Yoshida et al., 2010). Espresso takes advantage of
pointwise mutual information (pmi) (Manning and
Sch¨utze, 1999) between instances and patterns to
evaluate their reliability. Let n be the number of all
instances in the corpus, and p the number of all pos-
sible patterns. We denote all pmi values as an n × p
instance-pattern matrix A, with the (i, j) element of
A holding the value of pmi between the ith instance
and the jth pattern. Let AT denote the matrix trans-
pose of A.
Algorithm 1 shows the pseudocode of Espresso.
The input vector i0 (called seed vector) is an n-
dimensional binary vector with 1 at the ith com-
ponent for every seed instance i, and 0 elsewhere.
The algorithm outputs an n-dimensional vector i and
an p-dimensional vector p, respectively representing
the final scores of instances and patterns. Note that
for brevity, the pseudocode assumes fixed numbers
(k and m) of components in i and p are carried over
to the subsequent iteration, but the original Espresso
</bodyText>
<figureCaption confidence="0.81178388">
allows them to gradually increase with the number (b) If not, these instances are likely to make a
of iterations. vector for which semantic drift is directed;
3 HITS-based Approach to Seed Selection hence, use them as the stop list. In this
and Stop List Construction case, the seed set must be prepared manu-
3.1 Espresso and HITS ally, just like the usual bootstrapping pro-
Komachi et al. (2008) pointed out the similarity cedure.
between Espresso and Kleinberg’s HITS web page 4. Run Espresso with the seeds or stop list found
ranking algorithm (Kleinberg, 1999). Indeed, if we in the last step.
remove the pattern/instance selection steps of Algo- 4 Experimental Setup
rithm 1 (lines 13 and 16), the algorithm essentially We evaluate our methods on a variant of the lexi-
reduces to HITS. In this case, the outputs i and p cal sample word sense disambiguation task. In the
match respectively the hubness and authority score lexical sample task, a small pre-selected set of a tar-
vectors of HITS, computed on the bipartite graph of get word is given, along with an inventory of senses
instances and patterns induced by matrix A. for each word (Jurafsky and Martin, 2008). Each
An implication of this algorithmic similarity is word comes with a number of instances (context
that the outputs of Espresso are inherently biased sentences) in which the target word occur, and some
towards the HITS vectors, which is likely to be of these sentences are manually labeled with the cor-
the cause of semantic drift. Even though the pat- rect sense of the target word in each context. The
tern/instance selection steps in Espresso reduce such goal of the task is to classify unlabeled context sen-
a bias to some extent, the bias still persists, as em- tences by the sense of the target word in each con-
pirically verified by Komachi et al. (2008). In other text, using the set of labeled sentences.
words, the expansion process does not drift in ran- To apply Espresso for this task, we reformulate
dom directions, but tend towards the set of instances the task to be that of seed set expansion, and not
and patterns with the highest HITS scores, regard- classification. That is, the hand-labeled sentences
less of the target semantic class. We exploit this ob- having the same sense label are used as the seed set,
</figureCaption>
<table confidence="0.88804735">
servation in seed selection and stop list construction and it is expanded over all the remaining (unlabeled)
for Espresso, in order to reduce semantic drift. sentences.
3.2 The Procedure The reason we use the lexical sample task is that
Our strategy is extremely simple, and can be sum- every sentence (instance) belongs to one of the pre-
marized as follows. defined senses (classes), and we can expect the most
1. First, compute the HITS ranking of instances frequent sense in the corpus to form the highest
in the graph induced by the pmi matrix A. This HITS ranking instances. This allows us to com-
can be done by calling Algorithm 1 with k = pletely automate our experiments, without the need
m = — and a sufficiently large r. to manually check the HITS ranking in Step 2 of
2. Next, check the top instances in the HITS rank- Section 3.2. That is, for the most frequent sense
ing list manually, and see if these belong to the (majority sense), we take Step 3a and use the highest
target class. ranked instances as seeds; for the rest of the senses
3. The third step depends on the outcome of the (minority senses), we take Step 3b and use them as
second step. the stop list.
(a) If the top instances are of the target class, 4.1 Datasets
use them as the seeds. We do not use a We used the seven most frequent polysemous nouns
stop list in this case. (arm, bank, degree, difference, paper, party and
32 shelter) in the SENSEVAL-3 dataset, and line (Lea-
cock et al., 1993) and interest (Bruce and Wiebe,
Task Method MAP AUC R-Precision P@30 P@50 P@100
arm Random 84.3 ±4.1 59.6 ±8.1 80.9 ±2.2 89.5 ±10.8 87.7 ±9.6 85.4 ±7.2
HITS 85.9 59.7 79.3 100 98.0 89.0
bank Random 74.8 ±6.5 61.6 ±9.6 72.6 ±4.5 82.9 ±14.8 80.1 ±13.5 76.6 ±10.9
HITS 84.8 77.6 78.0 100 100 94.0
degree Random 69.4 ±3.0 54.3 ±4.2 66.7 ±2.3 76.8 ±9.5 73.8 ±7.5 70.5 ±5.3
HITS 62.4 49.3 63.2 56.7 64.0 66.0
difference Random 48.3 ±3.8 54.5 ±5.0 47.0 ±4.4 53.9 ±10.7 50.7 ±8.8 47.9 ±6.1
HITS 50.2 60.1 51.1 60.0 60.0 48.0
paper Random 75.2 ±4.1 56.4 ±7.1 71.6 ±3.3 82.3 ±9.8 79.6 ±8.8 76.9 ±6.1
HITS 75.2 61.0 75.2 73.3 80.0 78.0
party Random 79.1 ±5.0 57.0 ±9.7 76.6 ±3.1 84.5 ±10.7 82.7 ±9.2 80.2 ±7.5
HITS 85.2 68.2 78.5 100 96.0 87.0
shelter Random 74.9 ±2.3 51.5 ±3.3 73.2 ±1.3 77.3 ±7.8 76.0 ±5.6 74.5 ±3.5
HITS 77.0 54.6 72.0 76.7 84.0 79.0
line Random 44.5 ±15.1 36.3 ±16.9 40.1 ±14.6 75.0 ±21.0 69.8 ±24.1 62.3 ±27.9
HITS 72.2 68.6 68.5 100 100 100
interest Random 64.9 ±8.3 64.9 ±12.0 63.7 ±10.2 87.6 ±13.2 85.3 ±13.7 81.2 ±13.9
HITS 75.3 83.0 80.1 100 94.0 77.0
Avg. Random 68.4 55.1 65.8 78.9 76.2 72.8
HITS 74.2 64.7 71.8 85.2 86.2 79.8
</table>
<tableCaption confidence="0.690227666666667">
Table 1: Comparison of seed selection for Espresso (ti = 5, nseed = 7). For Random, results are reported as (mean ±
standard deviation). All figures are expressed in percentage terms. The row labeled “Avg.” lists the values macro-
averaged over the nine tasks.
</tableCaption>
<bodyText confidence="0.996991777777778">
1994) datasets1 for our experiments. We lowercased
words in the sentence and pre-processed them with
the Porter stemmer (Porter, 1980) to get the stems of
words.
Following (Komachi et al., 2008), we used two
types of features extracted from neighboring con-
texts: collocational features and bag-of-words fea-
tures. For collocational features, we set a window of
three words to the right and left of the target word.
</bodyText>
<subsectionHeader confidence="0.988418">
4.2 Evaluation methodology
</subsectionHeader>
<bodyText confidence="0.999840272727273">
We run Espresso on the above datasets using differ-
ent seed selection methods (for majority sense of tar-
get words), and with or without stop lists created by
our method (for minority senses of target words).
We evaluate the performance of the systems ac-
cording to the following evaluation metrics: mean
average precision (MAP), area under the ROC curve
(AUC), R-precision, and precision@n (P@n) (Man-
ning et al., 2008). The output of Espresso may con-
tain seed instances input to the system, but seeds are
excluded from the evaluation.
</bodyText>
<footnote confidence="0.914132">
1http://www.d.umn.edu/-tpederse/data.html
</footnote>
<sectionHeader confidence="0.997475" genericHeader="introduction">
5 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.999296">
5.1 Effect of Seed Selection
</subsectionHeader>
<bodyText confidence="0.999962045454546">
We first evaluate the performance of our seed se-
lection method for the majority sense of the nine
polysemous nouns. Table 1 shows the performance
of Espresso with the seeds chosen by the proposed
HITS-based seed selection method (HITS), and with
the seed sets randomly chosen from the gold stan-
dard sets (Random; baseline). The results for Ran-
dom were averaged over 1000 runs. We set the num-
ber of seeds nseed = 7 and number of iterations r = 5
in this experiment.
As shown in the table, HITS outperforms the
baseline systems except degree. Especially, the
MAP reported in Table 1 shows that our approach
achieved improvements of 10 percentage points on
bank, 6.1 points on party, 27.7 points on line, and
10.4 points on interest over the baseline, respec-
tively. AUC and R-precision mostly exhibit a trend
similar to MAP, except R-precision in arm and shel-
ter, for which the baseline is better. It can be seen
from the P@n (P@30, P@50 and P@100) reported
in Table 1 that our approach performed considerably
better than baseline, e.g., around 17–20 points above
</bodyText>
<page confidence="0.998626">
33
</page>
<table confidence="0.999741238095238">
Task Method MAP AUC R-Precision P@10 P@20 P@30
arm NoStop 12.7 ±4.3 51.8 ±10.8 13.9 ±9.8 21.4 ±19.1 15.1 ±12.0 14.1 ±10.4
HITS 13.4 ±4.1 53.7 ±10.5 15.0 ±9.5 23.8 ±17.7 17.5 ±12.0 15.5 ±10.2
bank NoStop 32.5 ±5.1 73.0 ±8.5 45.1 ±10.3 80.4 ±21.8 70.3 ±21.2 62.6 ±18.1
HITS 33.7 ±3.7 75.4 ±5.7 47.6 ±8.1 82.6 ±18.1 72.7 ±18.5 65.3 ±15.5
degree NoStop 34.7 ±4.2 69.7 ±5.6 43.0 ±7.1 70.0 ±18.7 62.8 ±15.7 55.8 ±14.3
HITS 35.7 ±4.3 71.7 ±5.6 44.3 ±7.6 72.4 ±16.4 64.4 ±15.9 58.3 ±16.2
difference NoStop 20.2 ±3.9 57.1 ±6.7 22.3 ±8.3 35.8 ±18.7 27.7 ±14.0 25.5 ±11.9
HITS 21.2 ±3.8 59.1 ±6.3 24.2 ±8.4 38.2 ±20.5 30.2 ±14.0 28.0 ±11.9
paper NoStop 25.9 ±6.6 53.1 ±10.0 27.7 ±9.8 55.2 ±34.7 42.4 ±25.4 36.0 ±17.8
HITS 27.2 ±6.3 56.3 ±9.1 29.4 ±9.5 57.4 ±35.3 45.6 ±25.3 38.7 ±17.5
party NoStop 23.0 ±5.3 59.4 ±10.8 30.5 ±9.1 59.6 ±25.8 46.8 ±17.4 38.7 ±12.7
HITS 24.1 ±5.0 62.5 ±9.8 32.1 ±9.4 61.6 ±26.4 47.9 ±16.6 40.8 ±12.7
shelter NoStop 24.3 ±2.4 50.6 ±3.2 25.1 ±4.6 25.4 ±11.7 26.9 ±10.3 25.9 ±8.7
HITS 25.6 ±2.3 53.4 ±3.0 26.5 ±4.8 28.8 ±12.9 29.0 ±10.4 28.1 ±8.2
line NoStop 6.5 ±1.8 38.3 ±5.3 2.1 ±4.1 0.8 ±4.4 1.8 ±8.9 2.3 ±11.0
HITS 6.7 ±1.9 38.8 ±5.8 2.4 ±4.4 1.0 ±4.6 2.0 ±8.9 2.5 ±11.1
interest NoStop 29.4 ±7.6 61.0 ±12.1 33.7 ±13.2 69.6 ±40.3 67.0 ±39.1 65.7 ±37.8
HITS 31.2 ±5.6 63.6 ±9.1 36.1 ±10.5 81.0 ±29.4 78.1 ±27.0 77.4 ±24.3
Avg. NoStop 23.2 57.1 27.0 46.5 40.1 36.3
HITS 24.3 59.4 28.6 49.6 43.0 39.4
</table>
<tableCaption confidence="0.940194">
Table 2: Effect of stop lists for Espresso (nstop = 10, nseed = 10, ti = 20). Results are reported as (mean ± standard
deviation). All figures are expressed in percentage. The row labeled “Avg.” shows the values macro-averaged over all
nine tasks.
</tableCaption>
<bodyText confidence="0.672864">
the baseline on bank and 25–37 points on line. 6 Conclusions
</bodyText>
<subsectionHeader confidence="0.999754">
5.2 Effect of Stop List
</subsectionHeader>
<bodyText confidence="0.99997671875">
Table 2 shows the performance of Espresso using
the stop list built with our proposed method (HITS),
compared with the vanilla Espresso not using any
stop list (NoStop).
In this case, the size of the stop list is set to nstop =
10, and the number of seeds nseed = 10 and iterations
i = 20. For both HITS and NoStop, the seeds are
selected at random from the gold standard data, and
the reported results were averaged over 50 runs of
each system. Due to lack of space, only the results
for the second most frequent sense for each word are
reported; i.e., the results for more minor senses are
not in the table. However, they also showed a similar
trend.
As shown in the table, our method (HITS) outper-
forms the baseline not using a stop list (NoStop), in
all evaluation metrics. In particular, the P@n listed
in Table 2 shows that our method provides about
11 percentage points absolute improvement over the
baseline on interest, for all n = 10, 20, and 30.
We have proposed a HITS-based method for allevi-
ating semantic drift in the bootstrapping algorithm
Espresso. Our idea is built around the concept of
hubs in the sense of Kleinberg’s HITS algorithm, as
well as the algorithmic similarity between Espresso
and HITS. Hub instances are influential and hence
make good seeds if they are of the target seman-
tic class, but otherwise, they may trigger semantic
drift. We have demonstrated that our method works
effectively on lexical sample tasks. We are currently
evaluating our method on other bootstrapping tasks,
including named entity extraction.
</bodyText>
<sectionHeader confidence="0.975742" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9963536">
We thank Masayuki Asahara and Kazuo Hara for
helpful discussions and the anonymous reviewers
for valuable comments. MS was partially supported
by Kakenhi Grant-in-Aid for Scientific Research C
21500141.
</bodyText>
<page confidence="0.998434">
34
</page>
<sectionHeader confidence="0.930111" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.646453102564103">
Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008.
Acquiring event relation knowledge by learning cooc-
currence patterns and fertilizing cooccurrence samples
with verbal nouns. In Proceedings of the 3rd Interna-
tional Joint Conference on Natural Language Process-
ing (IJCNLP ’08), pages 497–504.
Steven Abney. 2004. Understanding the Yarowsky algo-
rithm. Computational Linguistics, 30:365–395.
Krishna Bharat and Monika R. Henzinger. 1998. Im-
proved algorithms for topic distillation environment in
a hyperlinked. In Proceedings of the 21st Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR ’98),
pages 104–111.
Rebecca Bruce and Janyce Wiebe. 1994. Word-sense
disambiguation using decomposable models. In Pro-
ceedings of the 32nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL ’94), pages
139–146.
Stephen Clark, James R. Curran, and Miles Osborne.
2003. Bootstrapping POS taggers using unlabelled
data. In Proceedings of the 7th Conference on Natural
Language Learning (CoNLL ’03), pages 49–55.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora (EMNLP-VLC ’99), pages 189–196.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with mutual exclu-
sion bootstrapping. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics (PACLING ’07), pages 172–180.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th Conference on Computational Linguistics (COL-
ING ’92), pages 539–545.
Ashwin Ittoo and Gosse Bouma. 2010. On learning
subtypes of the part-whole relation: do not mix your
</bodyText>
<reference confidence="0.990705738461539">
seeds. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL
’10), pages 1328–1336.
Daniel Jurafsky and James H. Martin. 2008. Speech and
Language Processing. Prentice Hall, 2nd edition.
Jon M. Kleinberg. 1999. Authoritative sources in
a hyperlinked environment. Journal of the ACM,
46(5):604–632.
Mamoru Komachi and Hisami Suzuki. 2008. Minimally
supervised learning of semantic knowledge from query
logs. In Proceedings of the 3rd International Joint
Conference on Natural Language Processing (IJCNLP
’08), pages 358–365.
Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of se-
mantic drift in Espresso-like bootstrapping algorithms.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP ’08),
pages 1011–1020.
Claudia Leacock, Geoffrey Towell, and Ellen Voorhees.
1993. Corpus-based statistical sense resolution. In
Proceedings of the ARPA Workshop on Human Lan-
guage Technology (HLT ’93), pages 260–265.
Christopher D. Manning and Hinrich Sch¨utze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Sch¨utze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the North American Chapter of the Association of
Computational Linguistics (HLT-NAACL ’06), pages
152–159.
Tara McIntosh and James R. Curran. 2009. Reducing
semantic drift with bagging and distributional similar-
ity. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP (ACL-IJCNLP ’09), volume 1, pages 396–
404.
Tara McIntosh. 2010. Unsupervised discovery of nega-
tive categories in lexicon bootstrapping. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing (EMNLP ’10), pages
356–365.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL ’06), pages
113–120.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130–137.
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level boot-
strapping. In Proceedings of the 16th National Confer-
ence on Artificial Intelligence and the 11th Innovative
Applications ofArtificial Intelligence (AAAI/IAAI ’99),
pages 474–479.
Mark Steedman, Rebecca Hwa, Stephen Clark, Miles Os-
borne, Anoop Sarkar, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Example
</reference>
<page confidence="0.983828">
35
</page>
<reference confidence="0.993057689655172">
selection for bootstrapping statistical parsers. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology (HLT-
NAACL ’03), volume 1, pages 157–164.
Michael Thelen and Ellen Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extraction
pattern contexts. In Proceedings of the ACL-02 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP ’02), pages 214–221.
Vishnu Vyas, Patrick Pantel, and Eric Crestan. 2009.
Helping editors choose better seed sets for entity set
expansion. In Proceeding of the 18th ACM Conference
on Information and Knowledge Management (CIKM
’09), pages 225–234.
Roman Yangarber, Winston Lin, and Ralph Grishman.
2002. Unsupervised learning of generalized names.
In Proceedings of the 19th International Conference
on Computational Linguistics (COLING ’02).
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting on Association for
Computational Linguistics (ACL ’95), pages 189–196.
Minoru Yoshida, Masaki Ikeda, Shingo Ono, Issei Sato,
and Hiroshi Nakagawa. 2010. Person name dis-
ambiguation by bootstrapping. In Proceeding of the
33rd International ACM SIGIR Conference on Re-
search and Development in Information Retrieval (SI-
GIR ’10), pages 10–17.
</reference>
<page confidence="0.998911">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.801836">
<title confidence="0.999904">HITS-based Seed Selection and Stop List Construction for Bootstrapping</title>
<author confidence="0.997126">Tetsuo Kiso Masashi Shimbo Mamoru Komachi Yuji</author>
<affiliation confidence="0.9996935">Graduate School of Information Nara Institute of Science and</affiliation>
<address confidence="0.812649">Ikoma, Nara 630-0192,</address>
<abstract confidence="0.999315466666667">In bootstrapping (seed set expansion), selecting good seeds and creating stop lists are two effective ways to reduce semantic drift, but these methods generally need human supervision. In this paper, we propose a graphbased approach to helping editors choose effective seeds and stop list instances, applito Pantel and Pennacchiotti’s bootstrapping algorithm. The idea is to select seeds and create a stop list using the rankings of instances and patterns computed by Kleinberg’s HITS algorithm. Experimental results on a variation of the lexical sample task show the effectiveness of our method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>seeds</author>
</authors>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL ’10),</booktitle>
<pages>1328--1336</pages>
<marker>seeds, </marker>
<rawString>seeds. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL ’10), pages 1328–1336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Speech and Language Processing.</title>
<date>2008</date>
<publisher>Prentice Hall,</publisher>
<note>2nd edition.</note>
<contexts>
<context position="9053" citStr="Jurafsky and Martin, 2008" startWordPosition="1461" endWordPosition="1464">thm (Kleinberg, 1999). Indeed, if we in the last step. remove the pattern/instance selection steps of Algo- 4 Experimental Setup rithm 1 (lines 13 and 16), the algorithm essentially We evaluate our methods on a variant of the lexireduces to HITS. In this case, the outputs i and p cal sample word sense disambiguation task. In the match respectively the hubness and authority score lexical sample task, a small pre-selected set of a tarvectors of HITS, computed on the bipartite graph of get word is given, along with an inventory of senses instances and patterns induced by matrix A. for each word (Jurafsky and Martin, 2008). Each An implication of this algorithmic similarity is word comes with a number of instances (context that the outputs of Espresso are inherently biased sentences) in which the target word occur, and some towards the HITS vectors, which is likely to be of these sentences are manually labeled with the corthe cause of semantic drift. Even though the pat- rect sense of the target word in each context. The tern/instance selection steps in Espresso reduce such goal of the task is to classify unlabeled context sena bias to some extent, the bias still persists, as em- tences by the sense of the targ</context>
</contexts>
<marker>Jurafsky, Martin, 2008</marker>
<rawString>Daniel Jurafsky and James H. Martin. 2008. Speech and Language Processing. Prentice Hall, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon M Kleinberg</author>
</authors>
<title>Authoritative sources in a hyperlinked environment.</title>
<date>1999</date>
<journal>Journal of the ACM,</journal>
<volume>46</volume>
<issue>5</issue>
<contexts>
<context position="3352" citStr="Kleinberg, 1999" startWordPosition="518" endWordPosition="519">ces that must not be extracted (Curran et al., 2007; McIntosh and Curran, 2009). Drift can also be reduced with carefully selected seeds. However, both of these approaches require expert knowledge. In this paper, we propose a graph-based approach to seed selection and stop list creation for the stateof-the-art bootstrapping algorithm Espresso (Pantel and Pennacchiotti, 2006). An advantage of this approach is that it requires zero or minimal supervision. The idea is to use the hubness score of instances and patterns computed from the pointwise mutual information matrix with the HITS algorithm (Kleinberg, 1999). Komachi et al. (2008) pointed out that semantic drift in Espresso has the same root as topic drift (Bharat and Henzinger, 1998) observed with HITS, noting the algorithmic similarity between them. While Komachi et al. proposed to use algorithms different from Espresso to Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 30–36, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics avoid semantic drift, in this paper we take advantage Algorithm 1 Espresso algorithm of this similarity to make better use of Es</context>
<context position="8448" citStr="Kleinberg, 1999" startWordPosition="1358" endWordPosition="1359">o the subsequent iteration, but the original Espresso allows them to gradually increase with the number (b) If not, these instances are likely to make a of iterations. vector for which semantic drift is directed; 3 HITS-based Approach to Seed Selection hence, use them as the stop list. In this and Stop List Construction case, the seed set must be prepared manu3.1 Espresso and HITS ally, just like the usual bootstrapping proKomachi et al. (2008) pointed out the similarity cedure. between Espresso and Kleinberg’s HITS web page 4. Run Espresso with the seeds or stop list found ranking algorithm (Kleinberg, 1999). Indeed, if we in the last step. remove the pattern/instance selection steps of Algo- 4 Experimental Setup rithm 1 (lines 13 and 16), the algorithm essentially We evaluate our methods on a variant of the lexireduces to HITS. In this case, the outputs i and p cal sample word sense disambiguation task. In the match respectively the hubness and authority score lexical sample task, a small pre-selected set of a tarvectors of HITS, computed on the bipartite graph of get word is given, along with an inventory of senses instances and patterns induced by matrix A. for each word (Jurafsky and Martin, </context>
</contexts>
<marker>Kleinberg, 1999</marker>
<rawString>Jon M. Kleinberg. 1999. Authoritative sources in a hyperlinked environment. Journal of the ACM, 46(5):604–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mamoru Komachi</author>
<author>Hisami Suzuki</author>
</authors>
<title>Minimally supervised learning of semantic knowledge from query logs.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd International Joint Conference on Natural Language Processing (IJCNLP ’08),</booktitle>
<pages>358--365</pages>
<contexts>
<context position="6858" citStr="Komachi and Suzuki, 2008" startWordPosition="1082" endWordPosition="1085"> function ESPRESSO(i0,A,k,m,ti) 9: i ← i0 10: fort= 1,2,...,ti do 11: p ← ATi 12: Scale p so that the components sum to one. 13: p ← SELECTKBEST(p,k) 14: i ← Ap 15: Scale i so that the components sum to one. 16: i ← SELECTKBEST(i,m) 17: return i and p 18: function SELECTKBEST(v,k) 19: Retain only the k largest components of v, resetting the remaining components to 0. 20: return v clustering to find competing semantic classes (negative categories). 2.3 Espresso Espresso (Pantel and Pennacchiotti, 2006) is one of the state-of-the-art bootstrapping algorithms used in many natural language tasks (Komachi and Suzuki, 2008; Abe et al., 2008; Ittoo and Bouma, 2010; Yoshida et al., 2010). Espresso takes advantage of pointwise mutual information (pmi) (Manning and Sch¨utze, 1999) between instances and patterns to evaluate their reliability. Let n be the number of all instances in the corpus, and p the number of all possible patterns. We denote all pmi values as an n × p instance-pattern matrix A, with the (i, j) element of A holding the value of pmi between the ith instance and the jth pattern. Let AT denote the matrix transpose of A. Algorithm 1 shows the pseudocode of Espresso. The input vector i0 (called seed v</context>
</contexts>
<marker>Komachi, Suzuki, 2008</marker>
<rawString>Mamoru Komachi and Hisami Suzuki. 2008. Minimally supervised learning of semantic knowledge from query logs. In Proceedings of the 3rd International Joint Conference on Natural Language Processing (IJCNLP ’08), pages 358–365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mamoru Komachi</author>
<author>Taku Kudo</author>
<author>Masashi Shimbo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Graph-based analysis of semantic drift in Espresso-like bootstrapping algorithms.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP ’08),</booktitle>
<pages>1011--1020</pages>
<contexts>
<context position="3375" citStr="Komachi et al. (2008)" startWordPosition="520" endWordPosition="523">be extracted (Curran et al., 2007; McIntosh and Curran, 2009). Drift can also be reduced with carefully selected seeds. However, both of these approaches require expert knowledge. In this paper, we propose a graph-based approach to seed selection and stop list creation for the stateof-the-art bootstrapping algorithm Espresso (Pantel and Pennacchiotti, 2006). An advantage of this approach is that it requires zero or minimal supervision. The idea is to use the hubness score of instances and patterns computed from the pointwise mutual information matrix with the HITS algorithm (Kleinberg, 1999). Komachi et al. (2008) pointed out that semantic drift in Espresso has the same root as topic drift (Bharat and Henzinger, 1998) observed with HITS, noting the algorithmic similarity between them. While Komachi et al. proposed to use algorithms different from Espresso to Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 30–36, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics avoid semantic drift, in this paper we take advantage Algorithm 1 Espresso algorithm of this similarity to make better use of Espresso. We demonstrate </context>
<context position="8280" citStr="Komachi et al. (2008)" startWordPosition="1329" endWordPosition="1333">ly representing the final scores of instances and patterns. Note that for brevity, the pseudocode assumes fixed numbers (k and m) of components in i and p are carried over to the subsequent iteration, but the original Espresso allows them to gradually increase with the number (b) If not, these instances are likely to make a of iterations. vector for which semantic drift is directed; 3 HITS-based Approach to Seed Selection hence, use them as the stop list. In this and Stop List Construction case, the seed set must be prepared manu3.1 Espresso and HITS ally, just like the usual bootstrapping proKomachi et al. (2008) pointed out the similarity cedure. between Espresso and Kleinberg’s HITS web page 4. Run Espresso with the seeds or stop list found ranking algorithm (Kleinberg, 1999). Indeed, if we in the last step. remove the pattern/instance selection steps of Algo- 4 Experimental Setup rithm 1 (lines 13 and 16), the algorithm essentially We evaluate our methods on a variant of the lexireduces to HITS. In this case, the outputs i and p cal sample word sense disambiguation task. In the match respectively the hubness and authority score lexical sample task, a small pre-selected set of a tarvectors of HITS, </context>
<context position="9715" citStr="Komachi et al. (2008)" startWordPosition="1577" endWordPosition="1580"> similarity is word comes with a number of instances (context that the outputs of Espresso are inherently biased sentences) in which the target word occur, and some towards the HITS vectors, which is likely to be of these sentences are manually labeled with the corthe cause of semantic drift. Even though the pat- rect sense of the target word in each context. The tern/instance selection steps in Espresso reduce such goal of the task is to classify unlabeled context sena bias to some extent, the bias still persists, as em- tences by the sense of the target word in each conpirically verified by Komachi et al. (2008). In other text, using the set of labeled sentences. words, the expansion process does not drift in ran- To apply Espresso for this task, we reformulate dom directions, but tend towards the set of instances the task to be that of seed set expansion, and not and patterns with the highest HITS scores, regard- classification. That is, the hand-labeled sentences less of the target semantic class. We exploit this ob- having the same sense label are used as the seed set, servation in seed selection and stop list construction and it is expanded over all the remaining (unlabeled) for Espresso, in orde</context>
<context position="13254" citStr="Komachi et al., 2008" startWordPosition="2220" endWordPosition="2223">3 64.9 ±12.0 63.7 ±10.2 87.6 ±13.2 85.3 ±13.7 81.2 ±13.9 HITS 75.3 83.0 80.1 100 94.0 77.0 Avg. Random 68.4 55.1 65.8 78.9 76.2 72.8 HITS 74.2 64.7 71.8 85.2 86.2 79.8 Table 1: Comparison of seed selection for Espresso (ti = 5, nseed = 7). For Random, results are reported as (mean ± standard deviation). All figures are expressed in percentage terms. The row labeled “Avg.” lists the values macroaveraged over the nine tasks. 1994) datasets1 for our experiments. We lowercased words in the sentence and pre-processed them with the Porter stemmer (Porter, 1980) to get the stems of words. Following (Komachi et al., 2008), we used two types of features extracted from neighboring contexts: collocational features and bag-of-words features. For collocational features, we set a window of three words to the right and left of the target word. 4.2 Evaluation methodology We run Espresso on the above datasets using different seed selection methods (for majority sense of target words), and with or without stop lists created by our method (for minority senses of target words). We evaluate the performance of the systems according to the following evaluation metrics: mean average precision (MAP), area under the ROC curve (</context>
</contexts>
<marker>Komachi, Kudo, Shimbo, Matsumoto, 2008</marker>
<rawString>Mamoru Komachi, Taku Kudo, Masashi Shimbo, and Yuji Matsumoto. 2008. Graph-based analysis of semantic drift in Espresso-like bootstrapping algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP ’08), pages 1011–1020.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Geoffrey Towell</author>
<author>Ellen Voorhees</author>
</authors>
<title>Corpus-based statistical sense resolution.</title>
<date>1993</date>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology (HLT ’93),</booktitle>
<pages>260--265</pages>
<contexts>
<context position="11656" citStr="Leacock et al., 1993" startWordPosition="1927" endWordPosition="1931">for the most frequent sense ing list manually, and see if these belong to the (majority sense), we take Step 3a and use the highest target class. ranked instances as seeds; for the rest of the senses 3. The third step depends on the outcome of the (minority senses), we take Step 3b and use them as second step. the stop list. (a) If the top instances are of the target class, 4.1 Datasets use them as the seeds. We do not use a We used the seven most frequent polysemous nouns stop list in this case. (arm, bank, degree, difference, paper, party and 32 shelter) in the SENSEVAL-3 dataset, and line (Leacock et al., 1993) and interest (Bruce and Wiebe, Task Method MAP AUC R-Precision P@30 P@50 P@100 arm Random 84.3 ±4.1 59.6 ±8.1 80.9 ±2.2 89.5 ±10.8 87.7 ±9.6 85.4 ±7.2 HITS 85.9 59.7 79.3 100 98.0 89.0 bank Random 74.8 ±6.5 61.6 ±9.6 72.6 ±4.5 82.9 ±14.8 80.1 ±13.5 76.6 ±10.9 HITS 84.8 77.6 78.0 100 100 94.0 degree Random 69.4 ±3.0 54.3 ±4.2 66.7 ±2.3 76.8 ±9.5 73.8 ±7.5 70.5 ±5.3 HITS 62.4 49.3 63.2 56.7 64.0 66.0 difference Random 48.3 ±3.8 54.5 ±5.0 47.0 ±4.4 53.9 ±10.7 50.7 ±8.8 47.9 ±6.1 HITS 50.2 60.1 51.1 60.0 60.0 48.0 paper Random 75.2 ±4.1 56.4 ±7.1 71.6 ±3.3 82.3 ±9.8 79.6 ±8.8 76.9 ±6.1 HITS 75.2 </context>
</contexts>
<marker>Leacock, Towell, Voorhees, 1993</marker>
<rawString>Claudia Leacock, Geoffrey Towell, and Ellen Voorhees. 1993. Corpus-based statistical sense resolution. In Proceedings of the ARPA Workshop on Human Language Technology (HLT ’93), pages 260–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press.</publisher>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL ’06),</booktitle>
<pages>152--159</pages>
<contexts>
<context position="1964" citStr="McClosky et al., 2006" startWordPosition="292" endWordPosition="296">g data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Many bootstrapping algorithms have been proposed for a variety of tasks: word sense disambiguation (Yarowsky, 1995; Abney, 2004), information extraction (Hearst, 1992; Riloff and Jones, 1999; Thelen and Riloff, 2002; Pantel and Pennacchiotti, 2006), named entity recognition (Collins and Singer, 1999), part-of-speech tagging (Clark et al., 2003), 30 and statistical parsing (Steedman et al., 2003; McClosky et al., 2006). Bootstrapping algorithms, however, are known to suffer from the problem called semantic drift: as the iteration proceeds, the algorithms tend to select instances increasingly irrelevant to the seed instances (Curran et al., 2007). For example, suppose we want to collect the names of common tourist sites from a web corpus. Given seed instances {New York City, Maldives Islands}, bootstrapping might learn, at one point of the iteration, patterns like “pictures of X” and “photos of X,” which also co-occur with many irrelevant instances. In this case, a later iteration would likely acquire freque</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL ’06), pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tara McIntosh</author>
<author>James R Curran</author>
</authors>
<title>Reducing semantic drift with bagging and distributional similarity.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP ’09),</booktitle>
<volume>1</volume>
<pages>396--404</pages>
<contexts>
<context position="2815" citStr="McIntosh and Curran, 2009" startWordPosition="430" endWordPosition="433"> 2007). For example, suppose we want to collect the names of common tourist sites from a web corpus. Given seed instances {New York City, Maldives Islands}, bootstrapping might learn, at one point of the iteration, patterns like “pictures of X” and “photos of X,” which also co-occur with many irrelevant instances. In this case, a later iteration would likely acquire frequent words co-occurring with these generic patterns, such as Michael Jackson. Previous work has tried to reduce the effect of semantic drift by making the stop list of instances that must not be extracted (Curran et al., 2007; McIntosh and Curran, 2009). Drift can also be reduced with carefully selected seeds. However, both of these approaches require expert knowledge. In this paper, we propose a graph-based approach to seed selection and stop list creation for the stateof-the-art bootstrapping algorithm Espresso (Pantel and Pennacchiotti, 2006). An advantage of this approach is that it requires zero or minimal supervision. The idea is to use the hubness score of instances and patterns computed from the pointwise mutual information matrix with the HITS algorithm (Kleinberg, 1999). Komachi et al. (2008) pointed out that semantic drift in Espr</context>
<context position="4910" citStr="McIntosh and Curran (2009)" startWordPosition="764" endWordPosition="767">top list construction methods. 2.1 Seed Selection The performance of bootstrapping can be greatly influenced by a number of factors such as the size of the seed set, the composition of the seed set and the coherence of the concept being expanded (Vyas et al., 2009). Vyas et al. (2009) studied the impact of the composition of the seed sets on the expansion performance, confirming that seed set composition has a significant impact on the quality of expansions. They also found that the seeds chosen by non-expert editors are often worse than randomly chosen ones. A similar observation was made by McIntosh and Curran (2009), who reported that randomly chosen seeds from the gold-standard set often outperformed seeds chosen by domain experts. These results suggest that even for humans, selecting good seeds is a non-trivial task. 2.2 Stop Lists Yangarber et al. (2002) proposed to run multiple bootstrapping sessions in parallel, with each session trying to extract one of several mutually exclusive semantic classes. Thus, the instances harvested in one bootstrapping session can be used as the stop list of the other sessions. Curran et al. (2007) pursued a similar idea in their Mutual Exclusion Bootstrapping, which us</context>
</contexts>
<marker>McIntosh, Curran, 2009</marker>
<rawString>Tara McIntosh and James R. Curran. 2009. Reducing semantic drift with bagging and distributional similarity. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP ’09), volume 1, pages 396– 404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tara McIntosh</author>
</authors>
<title>Unsupervised discovery of negative categories in lexicon bootstrapping.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP ’10),</booktitle>
<pages>356--365</pages>
<contexts>
<context position="5997" citStr="McIntosh (2010)" startWordPosition="939" endWordPosition="940">e stop list of the other sessions. Curran et al. (2007) pursued a similar idea in their Mutual Exclusion Bootstrapping, which uses multiple semantic classes in addition to hand-crafted stop lists. While multi-class bootstrapping is a clever way to reduce human supervision in stop list construction, it is not generally applicable to bootstrapping for a single class. To apply the idea of multi-class bootstrapping to singleclass bootstrapping, one has to first find appropriate competing semantic classes and good seeds for them, which is in itself a difficult problem. Along this line of research, McIntosh (2010) recently used 31 1: Input: Seed vector i0 2: Instance-pattern co-occurrence matrix A 3: Instance cutoff parameter k 4: Pattern cutoff parameter m 5: Number of iterations ti 6: Output: Instance score vector i 7: Pattern score vector p 8: function ESPRESSO(i0,A,k,m,ti) 9: i ← i0 10: fort= 1,2,...,ti do 11: p ← ATi 12: Scale p so that the components sum to one. 13: p ← SELECTKBEST(p,k) 14: i ← Ap 15: Scale i so that the components sum to one. 16: i ← SELECTKBEST(i,m) 17: return i and p 18: function SELECTKBEST(v,k) 19: Retain only the k largest components of v, resetting the remaining components</context>
</contexts>
<marker>McIntosh, 2010</marker>
<rawString>Tara McIntosh. 2010. Unsupervised discovery of negative categories in lexicon bootstrapping. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP ’10), pages 356–365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL ’06),</booktitle>
<pages>113--120</pages>
<contexts>
<context position="1791" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="267" endWordPosition="270"> of interest, bootstrapping first trains a classifier (which often is a weighted list of surface patterns characterizing the seeds) using the seeds, and then apply it on the remaining data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Many bootstrapping algorithms have been proposed for a variety of tasks: word sense disambiguation (Yarowsky, 1995; Abney, 2004), information extraction (Hearst, 1992; Riloff and Jones, 1999; Thelen and Riloff, 2002; Pantel and Pennacchiotti, 2006), named entity recognition (Collins and Singer, 1999), part-of-speech tagging (Clark et al., 2003), 30 and statistical parsing (Steedman et al., 2003; McClosky et al., 2006). Bootstrapping algorithms, however, are known to suffer from the problem called semantic drift: as the iteration proceeds, the algorithms tend to select instances increasingly irrelevant to the seed instances (Curran et al., 2007). For example, suppose we want to collect the names of common tourist sites from a web corpus. Given seed instances {New York City, Maldives Islands}, bootstrapping might learn, at one point of th</context>
<context position="3113" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="474" endWordPosition="477"> irrelevant instances. In this case, a later iteration would likely acquire frequent words co-occurring with these generic patterns, such as Michael Jackson. Previous work has tried to reduce the effect of semantic drift by making the stop list of instances that must not be extracted (Curran et al., 2007; McIntosh and Curran, 2009). Drift can also be reduced with carefully selected seeds. However, both of these approaches require expert knowledge. In this paper, we propose a graph-based approach to seed selection and stop list creation for the stateof-the-art bootstrapping algorithm Espresso (Pantel and Pennacchiotti, 2006). An advantage of this approach is that it requires zero or minimal supervision. The idea is to use the hubness score of instances and patterns computed from the pointwise mutual information matrix with the HITS algorithm (Kleinberg, 1999). Komachi et al. (2008) pointed out that semantic drift in Espresso has the same root as topic drift (Bharat and Henzinger, 1998) observed with HITS, noting the algorithmic similarity between them. While Komachi et al. proposed to use algorithms different from Espresso to Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:</context>
<context position="6740" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="1065" endWordPosition="1068">er k 4: Pattern cutoff parameter m 5: Number of iterations ti 6: Output: Instance score vector i 7: Pattern score vector p 8: function ESPRESSO(i0,A,k,m,ti) 9: i ← i0 10: fort= 1,2,...,ti do 11: p ← ATi 12: Scale p so that the components sum to one. 13: p ← SELECTKBEST(p,k) 14: i ← Ap 15: Scale i so that the components sum to one. 16: i ← SELECTKBEST(i,m) 17: return i and p 18: function SELECTKBEST(v,k) 19: Retain only the k largest components of v, resetting the remaining components to 0. 20: return v clustering to find competing semantic classes (negative categories). 2.3 Espresso Espresso (Pantel and Pennacchiotti, 2006) is one of the state-of-the-art bootstrapping algorithms used in many natural language tasks (Komachi and Suzuki, 2008; Abe et al., 2008; Ittoo and Bouma, 2010; Yoshida et al., 2010). Espresso takes advantage of pointwise mutual information (pmi) (Manning and Sch¨utze, 1999) between instances and patterns to evaluate their reliability. Let n be the number of all instances in the corpus, and p the number of all possible patterns. We denote all pmi values as an n × p instance-pattern matrix A, with the (i, j) element of A holding the value of pmi between the ith instance and the jth pattern. Let</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL ’06), pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="13194" citStr="Porter, 1980" startWordPosition="2211" endWordPosition="2212"> 72.2 68.6 68.5 100 100 100 interest Random 64.9 ±8.3 64.9 ±12.0 63.7 ±10.2 87.6 ±13.2 85.3 ±13.7 81.2 ±13.9 HITS 75.3 83.0 80.1 100 94.0 77.0 Avg. Random 68.4 55.1 65.8 78.9 76.2 72.8 HITS 74.2 64.7 71.8 85.2 86.2 79.8 Table 1: Comparison of seed selection for Espresso (ti = 5, nseed = 7). For Random, results are reported as (mean ± standard deviation). All figures are expressed in percentage terms. The row labeled “Avg.” lists the values macroaveraged over the nine tasks. 1994) datasets1 for our experiments. We lowercased words in the sentence and pre-processed them with the Porter stemmer (Porter, 1980) to get the stems of words. Following (Komachi et al., 2008), we used two types of features extracted from neighboring contexts: collocational features and bag-of-words features. For collocational features, we set a window of three words to the right and left of the target word. 4.2 Evaluation methodology We run Espresso on the above datasets using different seed selection methods (for majority sense of target words), and with or without stop lists created by our method (for minority senses of target words). We evaluate the performance of the systems according to the following evaluation metri</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M. F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Rosie Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th National Conference on Artificial Intelligence and the 11th Innovative Applications ofArtificial Intelligence (AAAI/IAAI ’99),</booktitle>
<pages>474--479</pages>
<contexts>
<context position="1733" citStr="Riloff and Jones, 1999" startWordPosition="259" endWordPosition="262"> (seeds) representing a particular semantic class of interest, bootstrapping first trains a classifier (which often is a weighted list of surface patterns characterizing the seeds) using the seeds, and then apply it on the remaining data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Many bootstrapping algorithms have been proposed for a variety of tasks: word sense disambiguation (Yarowsky, 1995; Abney, 2004), information extraction (Hearst, 1992; Riloff and Jones, 1999; Thelen and Riloff, 2002; Pantel and Pennacchiotti, 2006), named entity recognition (Collins and Singer, 1999), part-of-speech tagging (Clark et al., 2003), 30 and statistical parsing (Steedman et al., 2003; McClosky et al., 2006). Bootstrapping algorithms, however, are known to suffer from the problem called semantic drift: as the iteration proceeds, the algorithms tend to select instances increasingly irrelevant to the seed instances (Curran et al., 2007). For example, suppose we want to collect the names of common tourist sites from a web corpus. Given seed instances {New York City, Maldiv</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In Proceedings of the 16th National Conference on Artificial Intelligence and the 11th Innovative Applications ofArtificial Intelligence (AAAI/IAAI ’99), pages 474–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
<author>Rebecca Hwa</author>
<author>Stephen Clark</author>
<author>Miles Osborne</author>
<author>Anoop Sarkar</author>
<author>Julia Hockenmaier</author>
<author>Paul Ruhlen</author>
<author>Steven Baker</author>
<author>Jeremiah Crim</author>
</authors>
<title>Example selection for bootstrapping statistical parsers.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (HLTNAACL ’03),</booktitle>
<volume>1</volume>
<pages>157--164</pages>
<contexts>
<context position="1940" citStr="Steedman et al., 2003" startWordPosition="288" endWordPosition="291">pply it on the remaining data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Many bootstrapping algorithms have been proposed for a variety of tasks: word sense disambiguation (Yarowsky, 1995; Abney, 2004), information extraction (Hearst, 1992; Riloff and Jones, 1999; Thelen and Riloff, 2002; Pantel and Pennacchiotti, 2006), named entity recognition (Collins and Singer, 1999), part-of-speech tagging (Clark et al., 2003), 30 and statistical parsing (Steedman et al., 2003; McClosky et al., 2006). Bootstrapping algorithms, however, are known to suffer from the problem called semantic drift: as the iteration proceeds, the algorithms tend to select instances increasingly irrelevant to the seed instances (Curran et al., 2007). For example, suppose we want to collect the names of common tourist sites from a web corpus. Given seed instances {New York City, Maldives Islands}, bootstrapping might learn, at one point of the iteration, patterns like “pictures of X” and “photos of X,” which also co-occur with many irrelevant instances. In this case, a later iteration wou</context>
</contexts>
<marker>Steedman, Hwa, Clark, Osborne, Sarkar, Hockenmaier, Ruhlen, Baker, Crim, 2003</marker>
<rawString>Mark Steedman, Rebecca Hwa, Stephen Clark, Miles Osborne, Anoop Sarkar, Julia Hockenmaier, Paul Ruhlen, Steven Baker, and Jeremiah Crim. 2003. Example selection for bootstrapping statistical parsers. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (HLTNAACL ’03), volume 1, pages 157–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Thelen</author>
<author>Ellen Riloff</author>
</authors>
<title>A bootstrapping method for learning semantic lexicons using extraction pattern contexts.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing (EMNLP ’02),</booktitle>
<pages>214--221</pages>
<contexts>
<context position="1758" citStr="Thelen and Riloff, 2002" startWordPosition="263" endWordPosition="266">particular semantic class of interest, bootstrapping first trains a classifier (which often is a weighted list of surface patterns characterizing the seeds) using the seeds, and then apply it on the remaining data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Many bootstrapping algorithms have been proposed for a variety of tasks: word sense disambiguation (Yarowsky, 1995; Abney, 2004), information extraction (Hearst, 1992; Riloff and Jones, 1999; Thelen and Riloff, 2002; Pantel and Pennacchiotti, 2006), named entity recognition (Collins and Singer, 1999), part-of-speech tagging (Clark et al., 2003), 30 and statistical parsing (Steedman et al., 2003; McClosky et al., 2006). Bootstrapping algorithms, however, are known to suffer from the problem called semantic drift: as the iteration proceeds, the algorithms tend to select instances increasingly irrelevant to the seed instances (Curran et al., 2007). For example, suppose we want to collect the names of common tourist sites from a web corpus. Given seed instances {New York City, Maldives Islands}, bootstrappin</context>
</contexts>
<marker>Thelen, Riloff, 2002</marker>
<rawString>Michael Thelen and Ellen Riloff. 2002. A bootstrapping method for learning semantic lexicons using extraction pattern contexts. In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing (EMNLP ’02), pages 214–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vishnu Vyas</author>
</authors>
<title>Patrick Pantel, and Eric Crestan.</title>
<date>2009</date>
<booktitle>In Proceeding of the 18th ACM Conference on Information and Knowledge Management (CIKM ’09),</booktitle>
<pages>225--234</pages>
<marker>Vyas, 2009</marker>
<rawString>Vishnu Vyas, Patrick Pantel, and Eric Crestan. 2009. Helping editors choose better seed sets for entity set expansion. In Proceeding of the 18th ACM Conference on Information and Knowledge Management (CIKM ’09), pages 225–234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
<author>Winston Lin</author>
<author>Ralph Grishman</author>
</authors>
<title>Unsupervised learning of generalized names.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics (COLING ’02).</booktitle>
<contexts>
<context position="5156" citStr="Yangarber et al. (2002)" startWordPosition="803" endWordPosition="806">as et al., 2009). Vyas et al. (2009) studied the impact of the composition of the seed sets on the expansion performance, confirming that seed set composition has a significant impact on the quality of expansions. They also found that the seeds chosen by non-expert editors are often worse than randomly chosen ones. A similar observation was made by McIntosh and Curran (2009), who reported that randomly chosen seeds from the gold-standard set often outperformed seeds chosen by domain experts. These results suggest that even for humans, selecting good seeds is a non-trivial task. 2.2 Stop Lists Yangarber et al. (2002) proposed to run multiple bootstrapping sessions in parallel, with each session trying to extract one of several mutually exclusive semantic classes. Thus, the instances harvested in one bootstrapping session can be used as the stop list of the other sessions. Curran et al. (2007) pursued a similar idea in their Mutual Exclusion Bootstrapping, which uses multiple semantic classes in addition to hand-crafted stop lists. While multi-class bootstrapping is a clever way to reduce human supervision in stop list construction, it is not generally applicable to bootstrapping for a single class. To app</context>
</contexts>
<marker>Yangarber, Lin, Grishman, 2002</marker>
<rawString>Roman Yangarber, Winston Lin, and Ralph Grishman. 2002. Unsupervised learning of generalized names. In Proceedings of the 19th International Conference on Computational Linguistics (COLING ’02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting on Association for Computational Linguistics (ACL ’95),</booktitle>
<pages>189--196</pages>
<contexts>
<context position="945" citStr="Yarowsky, 1995" startWordPosition="135" endWordPosition="136">ecting good seeds and creating stop lists are two effective ways to reduce semantic drift, but these methods generally need human supervision. In this paper, we propose a graphbased approach to helping editors choose effective seeds and stop list instances, applicable to Pantel and Pennacchiotti’s Espresso bootstrapping algorithm. The idea is to select seeds and create a stop list using the rankings of instances and patterns computed by Kleinberg’s HITS algorithm. Experimental results on a variation of the lexical sample task show the effectiveness of our method. 1 Introduction Bootstrapping (Yarowsky, 1995; Abney, 2004) is a technique frequently used in natural language processing to expand limited resources with minimal supervision. Given a small amount of sample data (seeds) representing a particular semantic class of interest, bootstrapping first trains a classifier (which often is a weighted list of surface patterns characterizing the seeds) using the seeds, and then apply it on the remaining data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Ma</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting on Association for Computational Linguistics (ACL ’95), pages 189–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minoru Yoshida</author>
<author>Masaki Ikeda</author>
<author>Shingo Ono</author>
<author>Issei Sato</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>Person name disambiguation by bootstrapping.</title>
<date>2010</date>
<booktitle>In Proceeding of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’10),</booktitle>
<pages>10--17</pages>
<contexts>
<context position="6922" citStr="Yoshida et al., 2010" startWordPosition="1094" endWordPosition="1097">1: p ← ATi 12: Scale p so that the components sum to one. 13: p ← SELECTKBEST(p,k) 14: i ← Ap 15: Scale i so that the components sum to one. 16: i ← SELECTKBEST(i,m) 17: return i and p 18: function SELECTKBEST(v,k) 19: Retain only the k largest components of v, resetting the remaining components to 0. 20: return v clustering to find competing semantic classes (negative categories). 2.3 Espresso Espresso (Pantel and Pennacchiotti, 2006) is one of the state-of-the-art bootstrapping algorithms used in many natural language tasks (Komachi and Suzuki, 2008; Abe et al., 2008; Ittoo and Bouma, 2010; Yoshida et al., 2010). Espresso takes advantage of pointwise mutual information (pmi) (Manning and Sch¨utze, 1999) between instances and patterns to evaluate their reliability. Let n be the number of all instances in the corpus, and p the number of all possible patterns. We denote all pmi values as an n × p instance-pattern matrix A, with the (i, j) element of A holding the value of pmi between the ith instance and the jth pattern. Let AT denote the matrix transpose of A. Algorithm 1 shows the pseudocode of Espresso. The input vector i0 (called seed vector) is an ndimensional binary vector with 1 at the ith compon</context>
</contexts>
<marker>Yoshida, Ikeda, Ono, Sato, Nakagawa, 2010</marker>
<rawString>Minoru Yoshida, Masaki Ikeda, Shingo Ono, Issei Sato, and Hiroshi Nakagawa. 2010. Person name disambiguation by bootstrapping. In Proceeding of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’10), pages 10–17.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>