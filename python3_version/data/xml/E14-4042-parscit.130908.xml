<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.137002">
<title confidence="0.984481">
One Sense per Tweeter ... and Other Lexical Semantic Tales of Twitter
</title>
<author confidence="0.998145">
Spandana Gella, Paul Cook and Timothy Baldwin
</author>
<affiliation confidence="0.997026">
Department of Computing and Information Systems
The University of Melbourne
</affiliation>
<email confidence="0.988791">
sgella@student.unimelb.edu.au, paulcook@unimelb.edu.au,tb@ldwin.net
</email>
<sectionHeader confidence="0.993691" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999983933333333">
In recent years, microblogs such as Twit-
ter have emerged as a new communication
channel. Twitter in particular has become
the target of a myriad of content-based
applications including trend analysis and
event detection, but there has been little
fundamental work on the analysis of word
usage patterns in this text type. In this
paper — inspired by the one-sense-per-
discourse heuristic of Gale et al. (1992)
— we investigate user-level sense distri-
butions, and detect strong support for “one
sense per tweeter”. As part of this, we con-
struct a novel sense-tagged lexical sample
dataset based on Twitter and a web corpus.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999888928571429">
Social media applications such as Twitter enable
users from all over the world to create and share
web content spontaneously. The resulting user-
generated content has been identified as having
potential in a myriad of applications including
real-time event detection (Petrovi´c et al., 2010),
trend analysis (Lau et al., 2012) and natural dis-
aster response co-ordination (Earle et al., 2010).
However, the dynamism and conversational na-
ture of the text contained in social media can
cause problems for traditional NLP approaches
such as parsing (Baldwin et al., 2013), mean-
ing that most content-based approaches use sim-
ple keyword search or a bag-of-words representa-
tion of the text. This paper is a first step towards
full lexical semantic analysis of social media text,
in investigating the sense distribution of a range
of polysemous words in Twitter and a general-
purpose web corpus.
The primary finding of this paper is that there
are strong user-level lexical semantic priors in
Twitter, equivalent in strength to document-level
lexical semantic priors, popularly termed the “one
sense per discourse” heuristic (Gale et al., 1992).
This has potential implications for future applica-
tions over Twitter which attempt to move beyond a
simple string-based meaning representation to ex-
plicit lexical semantic analysis.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999867272727273">
The traditional approach to the analysis of word-
level lexical semantics is via word sense dis-
ambiguation (WSD), where usages of a given
word are mapped onto discrete “senses” in a pre-
existing sense inventory (Navigli, 2009). The most
popular sense inventory used in WSD research has
been WordNet (Fellbaum, 1998), although its fine-
grained sense distinctions have proven to be diffi-
cult to make for human annotators and WSD sys-
tems alike. This has resulted in a move towards
more coarse-grained sense inventories (Palmer et
al., 2004; Hovy et al., 2006; Navigli et al., 2007),
or alternatively away from pre-existing sense in-
ventories altogether, towards joint word sense in-
duction (WSI) and disambiguation (Navigli and
Vannella, 2013; Jurgens and Klapaftis, 2013).
Two heuristics that have proven highly powerful
in WSD and WSI research are: (1) first sense tag-
ging, and (2) one sense per discourse. First sense
tagging is based on the observation that sense dis-
tributions tend to be Zipfian, such that if the pre-
dominant or “first” sense can be identified, simply
tagging all occurrences of a given word with this
sense can achieve high WSD accuracy (McCarthy
et al., 2007). Unsurprisingly, there are significant
differences in sense distributions across domains
(cf. cloud in the COMPUTING and METEOROLOG-
ICAL domains), motivating the need for unsuper-
vised first sense learning over domain-specific cor-
pora (Koeling et al., 2005).
One sense per discourse is the observation that
a given word will often occur with a single sense
across multiple usages in a single document (Gale
</bodyText>
<page confidence="0.986955">
215
</page>
<note confidence="0.6884725">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 215–220,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99997625">
et al., 1992). Gale et al. established the heuristic
on the basis of 9 ambiguous words using a coarse-
grained sense inventory, finding that the probabil-
ity of a given pair of usages of a word taken from a
given document having the same sense was 94%.
However, Krovetz (1998) found that for a fine-
grained sense inventory, only 67% of words exhib-
ited the single-sense-per-discourse property for all
documents in a corpus.
A radically different view on WSD is word us-
age similarity, whereby two usages of a given
word are rated on a continuous scale for similar-
ity, in isolation of any sense inventory (Erk et al.,
2009). Gella et al. (2013) constructed a word us-
age similarity dataset for Twitter messages, and
developed a topic modelling approach to the task,
building on the work of Lui et al. (2012). To the
best of our knowledge, this has been the only at-
tempt to carry out explicit word-level lexical se-
mantic analysis of Twitter text.
</bodyText>
<sectionHeader confidence="0.99615" genericHeader="method">
3 Dataset Construction
</sectionHeader>
<bodyText confidence="0.999988333333333">
In order to study sense distributions of words in
Twitter, we need a sense inventory to annotate
against, and also a set of Twitter messages to an-
notate. Further, as a point of comparison for the
sense distributions in Twitter, we require a second
corpus; here we use the ukWaC (Ferraresi et al.,
2008), a corpus built from web documents.
For the sense inventory, we chose the Macmil-
lan English Dictionary Online1 (MACMILLAN,
hereafter), on the basis of: (1) its coarse-grained
general-purpose sense distinctions, and (2) its reg-
ular update cycle (i.e. it contains many recently-
emerged senses). These criteria are important
in terms of inter-annotator agreement (especially
as we crowdsourced the sense annotation, as de-
scribed below) and also sense coverage. The
other obvious candidate sense inventory which po-
tentially satisfied these criteria was ONTONOTES
(Hovy et al., 2006), but a preliminary sense-
tagging exercise indicated that MACMILLAN bet-
ter captured Twitter-specific usages.
Rather than annotating all words, we opted for
a lexical sample of 20 polysemous nouns, as listed
in Table 1. Our target nouns were selected to span
the high- to mid-frequency range in both Twitter
and the web corpus, and have at least 3 MACMIL-
LAN senses. The average sense ambiguity is 5.5.
</bodyText>
<footnote confidence="0.963948">
1http://www.macmillandictionary.com
</footnote>
<bodyText confidence="0.890842">
band bar case charge deal
degree field form function issue
job light match panel paper
position post rule sign track
</bodyText>
<tableCaption confidence="0.997397">
Table 1: The 20 target nouns used in this research
</tableCaption>
<subsectionHeader confidence="0.999059">
3.1 Data Sampling
</subsectionHeader>
<bodyText confidence="0.999980279069767">
We sampled tweets from a crawl made using the
Twitter Streaming API from January 3, 2012 to
February 29, 2012. The web corpus was built from
ukWaC (Ferraresi et al., 2008), which was based
on a crawl of the .uk domain from 2007. In con-
trast to ukWaC, the tweets are not restricted to doc-
uments from any particular country.
For both corpora, we first selected only the
English documents using langid.py, an off-the-
shelf language identification tool (Lui and Bald-
win, 2012). We next identified documents which
contained nominal usages of the target words,
based on the POS tags supplied with the corpus
in the case of ukWaC, and the output of the CMU
ARK Twitter POS tagger v2.0 (Owoputi et al.,
2012) in the case of Twitter.
For Twitter, we are interested in not just the
overall lexical distribution of each target noun,
but also per-user lexical distributions. As such,
we construct two Twitter-based datasets: (1)
TWITTERRAND, a random sample of 100 usages of
each target noun; and (2) TWITTERUSER, 5 usages
of each target noun from each member of a ran-
dom sample of 20 Twitter users. Naively select-
ing users for TWITTERUSER without filtering re-
sulted in a preponderance of messages from ac-
counts that were clearly bots, e.g. from commer-
cial sites with a single post per item advertised for
sale, with artificially-skewed sense distributions.
In order to obtain a more natural set of messages
from “real” people, we introduced a number of
user-level filters, including removing users who
posted the same message with different user men-
tions or hashtags, and users who used the target
nouns more than 50 times over a 2-week period.
From the remaining users, we randomly selected
20 users per target noun, resulting in 20 nouns ×
20 users × 5 messages = 2000 messages.
For ukWaC, we similarly constructed two
datasets: (1) UKWACRAND, a random sample
of 100 usages of each target noun; and (2)
UKWACDOC, 5 usages of each target noun from 20
documents which contained that noun in at least
</bodyText>
<page confidence="0.9986">
216
</page>
<figureCaption confidence="0.999913">
Figure 1: Screenshot of a sense annotation HIT for position
</figureCaption>
<bodyText confidence="0.875119">
5 sentences. 5 such sentences were selected for
annotation, resulting in a total of 20 nouns x 20
documents x 5 sentences = 2000 sentences.
</bodyText>
<subsectionHeader confidence="0.999482">
3.2 Annotation Settings
</subsectionHeader>
<bodyText confidence="0.9998839">
We sense-tagged each of the four datasets using
Amazon Mechanical Turk (AMT). Each Human
Intelligence Task (HIT) comprised 5 occurrences
of a given target noun, with the target noun high-
lighted in each. Sense definitions and an exam-
ple sentence (where available) were provided from
MACMILLAN. Turkers were free to select multi-
ple sense labels where applicable, in line with best
practice in sense labelling (Mihalcea et al., 2004).
We also provided an “Other” sense option, in cases
where none of the MACMILLAN senses were ap-
plicable to the current usage of the target noun. A
screenshot of the annotation interface for a single
usage is provided in Figure 1.
Of the five sentences in each HIT, one was a
heldout example sentence for one of the senses of
the target noun, taken from MACMILLAN. This
gold-standard example was used exclusively for
quality assurance purposes, and used to filter the
annotations as follows:
</bodyText>
<listItem confidence="0.985764777777778">
1. Accept all HITs from Turkers whose gold-
standard tagging accuracy was ≥ 80%;
2. Reject all HITs from Turkers whose gold-
standard tagging accuracy was ≤ 20%;
3. Otherwise, accept single HITs with correct
gold-standard sense tags, or at least 2/4 (non-
gold-standard) annotations in common with
Turkers who correctly annotated the gold-
standard usage; reject any other HITs.
</listItem>
<bodyText confidence="0.99874488">
This style of quality assurance has been shown
to be successful for sense tagging tasks on AMT
(Bentivogli et al., 2011; Vuurens et al., 2011), and
resulted in us accepting around 95% of HITs.
In total, the annotation was made up of 500
HITs (= 2000/4 usages per HIT) for each of the
four datasets, each of which was annotated by
5 Turkers. Our analysis of sense distribution is
based on only those HITs which were accepted in
accordance with the above methodology, exclud-
ing the gold-standard items. We arrive at a single
sense label per usage by unweighted voting across
the annotations, allowing multiple votes from a
single Turker in the case of multiple sense annota-
tions. In this, the “Other” sense label is considered
as a discrete sense label.
Relative to the majority sense, inter-annotator
agreement post-filtering was respectably high in
terms of Fleiss’ kappa at n = 0.64 for both
UKWACRAND and UKWACDOC. For TWITTERUSER,
the agreement was actually higher at n = 0.71, but
for TWITTERRAND it was much weaker, n = 0.47.
All four datasets have been released for pub-
lic use: http://www.csse.unimelb.edu.au/
~tim/etc/twitter_sense.tgz.
</bodyText>
<sectionHeader confidence="0.996827" genericHeader="method">
4 Analysis
</sectionHeader>
<bodyText confidence="0.999957">
In TWITTERUSER, the proportion of users who used
a target noun with one sense across all 5 usages
ranged from 7/20 for form to 20/20 for degree, at
an average of 65%. That is, for 65% of users, a
given noun (with average polysemy = 5.5 senses)
is used with the same sense across 5 separate mes-
sages. For UKWACDOC the proportion of docu-
ments with a single sense of a given target noun
</bodyText>
<page confidence="0.996801">
217
</page>
<table confidence="0.645986">
Partition Agreement (%)
</table>
<tableCaption confidence="0.838660666666667">
Table 2: Pairwise agreement for each dataset,
based on different partitions of the data (“—” indi-
cates no partitioning, and exhaustive comparison)
</tableCaption>
<bodyText confidence="0.998884909090909">
across all usages ranged from 1/20 for case to
20/20 for band, at an average of 63%. As such,
the one sense per tweeter heuristic is at least as
strong as the one sense per discourse heuristic in
UKWACDOC.
Looking back to the original work of Gale et
al. (1992), it is important to realise that their re-
ported agreement of 94% was calculated pairwise
between usages in a given document. When we
recalculate the agreement in TWITTERUSER and
UKWACDOC using this methodology, as detailed
in Table 2 (calculating pairwise agreement within
partitions of the data based on “user” and “docu-
ment”, respectively), we see that the numbers for
our datasets are very close to those of Gale et al.
on the basis of more than twice as many nouns,
and many more instances per noun. Moreover, the
one sense per tweeter trend again appears to be
slightly stronger than the one sense per discourse
heuristic in UKWACDOC.
One possible interpretation of these results is
that they are due to a single predominant sense,
common to all users/documents rather than user-
specific predominant senses. To test this hy-
pothesis, we calculate the pairwise agreement for
TWITTERUSER and UKWACDOC across all anno-
tations (without partitioning on user/document),
and also for TWITTERRAND and UKWACRAND.
The results are, once again, presented in Ta-
ble 2 (with partition indicated as “—” for the
respective datasets), and are substantially lower
in all cases (&lt; 66%). This indicates that the
first sense preference varies considerably between
users/documents. Note that the agreement is
slightly lower for TWITTERRAND and UKWACRAND
simply because of the absence of the biasing effect
for users/documents.
Comparing TWITTERRAND and UKWACRAND,
there were marked differences in first sense pref-
erences, with 8/20 of the target nouns having a
different first sense across the two corpora. One
surprising observation was that the sense distri-
butions in UKWACRAND were in general more
skewed than in TWITTERRAND, with the entropy of
the sense distribution being lower (= more biased)
in UKWACRAND for 15/20 of the target nouns.
All datasets included instances of “Other”
senses (i.e. usages which didn’t conform to any
of the MACMILLAN senses), with the highest rel-
ative such occurrence being in TWITTERRAND at
12.3%, as compared to 6.6% for UKWACRAND.
Interestingly, the number of such usages in
the user/document-biased datasets was around
half these numbers, at 7.4% and 3.6% for
TWITTERUSER and UKWACDOC, respectively.
</bodyText>
<sectionHeader confidence="0.998896" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999943117647059">
It is worthwhile speculating why Twitter users
would have such a strong tendency to use a given
word with only one sense. This could arise in
part due to patterns of user behaviour, in a given
Twitter account being used predominantly to com-
ment on a favourite sports team or political events,
and as such is domain-driven. Alternatively, it can
perhaps be explained by the “reactive” nature of
Twitter, in that posts are often emotive responses
to happenings in a user’s life, and while different
things excite different individuals, a given individ-
ual will tend to be excited by events of similar
kinds. Clearly more research is required to test
these hypotheses.
One highly promising direction for this research
would be to overlay analysis of sense distributions
with analysis of user profiles (e.g. Bergsma et al.
(2013)), and test the impact of geospatial and soci-
olinguistic factors on sense preferences. We would
also like to consider the impact of time on the one
sense per tweeter heuristic, and consider whether
“one sense per Twitter conversation” also holds.
To summarise, we have investigated sense dis-
tributions in Twitter and a general web corpus,
over both a random sample of usages and a sample
of usages from a single user/document. We found
strong evidence for Twitter users to use a given
word with a single sense, and also that individual
first sense preferences differ between users, sug-
gesting that methods for determining first senses
on a per user basis could be valuable for lexical se-
mantic analysis of tweets. Furthermore, we found
that sense distributions in Twitter are overall less
skewed than in a web corpus.
</bodyText>
<table confidence="0.815667714285714">
Gale et al. (1992) document 94.4
TWITTERUSER user 95.4
TWITTERUSER — 62.9
TWITTERRAND — 55.1
UKWACDOC document 94.2
UKWACDOC — 65.9
UKWACRAND — 60.2
</table>
<page confidence="0.99567">
218
</page>
<sectionHeader confidence="0.958817" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999085718181819">
Timothy Baldwin, Paul Cook, Marco Lui, Andrew
MacKinlay, and Li Wang. 2013. How noisy so-
cial media text, how diffrnt social media sources?
In Proceedings of the 6th International Joint Con-
ference on Natural Language Processing (IJCNLP
2013), pages 356–364, Nagoya, Japan.
Luisa Bentivogli, Marcello Federico, Giovanni
Moretti, and Michael Paul. 2011. Getting expert
quality from the crowd for machine translation
evaluation. Proceedings of the MT Summmit,
13:521–528.
Shane Bergsma, Mark Dredze, Benjamin Van Durme,
Theresa Wilson, and David Yarowsky. 2013.
Broadly improving user classification via
communication-based name and location clus-
tering on Twitter. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL HLT 2013), pages
1010–1019, Atlanta, USA.
Paul Earle, Michelle Guy, Richard Buckmaster, Chris
Ostrum, Scott Horvath, and Amy Vaughan. 2010.
OMG earthquake! can Twitter improve earth-
quake response? Seismological Research Letters,
81(2):246–251.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2009. Investigations on word senses and word us-
ages. In Proceedings of the Joint conference of the
47th Annual Meeting of the Association for Compu-
tational Linguistics and the 4th International Joint
Conference on Natural Language Processing of the
Asian Federation of Natural Language Processing
(ACL-IJCNLP 2009), pages 10–18, Singapore.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
USA.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
In Proceedings of the 4th Web as Corpus Workshop:
Can we beat Google, pages 47–54, Marrakech, Mo-
rocco.
William A Gale, Kenneth W Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural
Language, pages 233–237.
Spandana Gella, Paul Cook, and Bo Han. 2013. Unsu-
pervised word usage similarity in social media texts.
In Proceedings of the Second Joint Conference on
Lexical and Computational Semantics (*SEM 2013),
pages 248–253, Atlanta, USA.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
the Main Conference on Human Language Technol-
ogy Conference of the North American Chapter of
the Association of Computational Linguistics, pages
57–60, New York City, USA.
David Jurgens and Ioannis Klapaftis. 2013. Semeval-
2013 task 13: Word sense induction for graded and
non-graded senses. In Proceedings of the 7th In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2013), pages 290–299, Atlanta, USA.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of the
2005 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2005), pages 419–
426, Vancouver, Canada.
Robert Krovetz. 1998. More than one sense per dis-
course. NEC Princeton NJ Labs., Research Memo-
randum.
Jey Han Lau, Nigel Collier, and Timothy Baldwin.
2012. On-line trend analysis with topic models:
#twitter trends detection topic model online. In Pro-
ceedings of the 24th International Conference on
Computational Linguistics (COLING 2012), pages
1519–1534, Mumbai, India.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2012)
Demo Session, pages 25–30, Jeju, Republic of Ko-
rea.
Marco Lui, Timothy Baldwin, and Diana McCarthy.
2012. Unsupervised estimation of word usage simi-
larity. In Proceedings of the Australasian Language
Technology Workshop 2012 (ALTW 2012), pages
33–41, Dunedin, New Zealand.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
4(33):553–590.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample
task. In Proceedings of Senseval-3: Third Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text, pages 25–28, Barcelona,
Spain.
Roberto Navigli and Daniele Vannella. 2013.
SemEval-2013 task 11: Word sense induction and
disambiguation within an end-user application. In
Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013), pages 193–
201, Atlanta, USA.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. SemEval-2007 task 07: Coarse-
grained English all-words task. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations, pages 30–35, Prague, Czech Republic.
</reference>
<page confidence="0.986812">
219
</page>
<reference confidence="0.998146153846154">
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys (CSUR), 41(2):10.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, and Nathan Schneider. 2012. Part-
of-speech tagging for Twitter: Word clusters and
other advances. Technical Report CMU-ML-12-
107, Machine Learning Department, Carnegie Mel-
lon University.
Martha Palmer, Olga Babko-Malaya, and Hoa Trang
Dang. 2004. Different sense granularities for differ-
ent applications. In Proceedings of the HLT-NAACL
2004 Workshop: 2nd Workshop on Scalable Natu-
ral Language Understanding, pages 49–56, Boston,
USA.
Sasa Petrovi´c, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with appli-
cation to twitter. In Proceedings of Human Lan-
guage Technologies: The 11th Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL HLT 2010),
pages 181–189, Los Angeles, USA.
Jeroen Vuurens, Arjen P de Vries, and Carsten Eick-
hoff. 2011. How much spam can you take? an anal-
ysis of crowdsourcing results to increase accuracy.
In Proc. ACM SIGIR Workshop on Crowdsourcing
for Information Retrieval (CIR 2011), pages 21–26.
</reference>
<page confidence="0.997428">
220
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.970641">
<title confidence="0.998755">One Sense per Tweeter ... and Other Lexical Semantic Tales of Twitter</title>
<author confidence="0.994651">Paul Cook Gella</author>
<affiliation confidence="0.9932885">Department of Computing and Information The University of</affiliation>
<abstract confidence="0.9993401875">In recent years, microblogs such as Twitter have emerged as a new communication channel. Twitter in particular has become the target of a myriad of content-based applications including trend analysis and event detection, but there has been little fundamental work on the analysis of word usage patterns in this text type. In this paper — inspired by the one-sense-perdiscourse heuristic of Gale et al. (1992) — we investigate user-level sense distributions, and detect strong support for “one sense per tweeter”. As part of this, we construct a novel sense-tagged lexical sample dataset based on Twitter and a web corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Paul Cook</author>
<author>Marco Lui</author>
<author>Andrew MacKinlay</author>
<author>Li Wang</author>
</authors>
<title>How noisy social media text, how diffrnt social media sources?</title>
<date>2013</date>
<booktitle>In Proceedings of the 6th International Joint Conference on Natural Language Processing (IJCNLP 2013),</booktitle>
<pages>356--364</pages>
<location>Nagoya, Japan.</location>
<contexts>
<context position="1472" citStr="Baldwin et al., 2013" startWordPosition="221" endWordPosition="224">aset based on Twitter and a web corpus. 1 Introduction Social media applications such as Twitter enable users from all over the world to create and share web content spontaneously. The resulting usergenerated content has been identified as having potential in a myriad of applications including real-time event detection (Petrovi´c et al., 2010), trend analysis (Lau et al., 2012) and natural disaster response co-ordination (Earle et al., 2010). However, the dynamism and conversational nature of the text contained in social media can cause problems for traditional NLP approaches such as parsing (Baldwin et al., 2013), meaning that most content-based approaches use simple keyword search or a bag-of-words representation of the text. This paper is a first step towards full lexical semantic analysis of social media text, in investigating the sense distribution of a range of polysemous words in Twitter and a generalpurpose web corpus. The primary finding of this paper is that there are strong user-level lexical semantic priors in Twitter, equivalent in strength to document-level lexical semantic priors, popularly termed the “one sense per discourse” heuristic (Gale et al., 1992). This has potential implication</context>
</contexts>
<marker>Baldwin, Cook, Lui, MacKinlay, Wang, 2013</marker>
<rawString>Timothy Baldwin, Paul Cook, Marco Lui, Andrew MacKinlay, and Li Wang. 2013. How noisy social media text, how diffrnt social media sources? In Proceedings of the 6th International Joint Conference on Natural Language Processing (IJCNLP 2013), pages 356–364, Nagoya, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Marcello Federico</author>
<author>Giovanni Moretti</author>
<author>Michael Paul</author>
</authors>
<title>Getting expert quality from the crowd for machine translation evaluation.</title>
<date>2011</date>
<booktitle>Proceedings of the MT Summmit,</booktitle>
<pages>13--521</pages>
<contexts>
<context position="10124" citStr="Bentivogli et al., 2011" startWordPosition="1651" endWordPosition="1654">LLAN. This gold-standard example was used exclusively for quality assurance purposes, and used to filter the annotations as follows: 1. Accept all HITs from Turkers whose goldstandard tagging accuracy was ≥ 80%; 2. Reject all HITs from Turkers whose goldstandard tagging accuracy was ≤ 20%; 3. Otherwise, accept single HITs with correct gold-standard sense tags, or at least 2/4 (nongold-standard) annotations in common with Turkers who correctly annotated the goldstandard usage; reject any other HITs. This style of quality assurance has been shown to be successful for sense tagging tasks on AMT (Bentivogli et al., 2011; Vuurens et al., 2011), and resulted in us accepting around 95% of HITs. In total, the annotation was made up of 500 HITs (= 2000/4 usages per HIT) for each of the four datasets, each of which was annotated by 5 Turkers. Our analysis of sense distribution is based on only those HITs which were accepted in accordance with the above methodology, excluding the gold-standard items. We arrive at a single sense label per usage by unweighted voting across the annotations, allowing multiple votes from a single Turker in the case of multiple sense annotations. In this, the “Other” sense label is consi</context>
</contexts>
<marker>Bentivogli, Federico, Moretti, Paul, 2011</marker>
<rawString>Luisa Bentivogli, Marcello Federico, Giovanni Moretti, and Michael Paul. 2011. Getting expert quality from the crowd for machine translation evaluation. Proceedings of the MT Summmit, 13:521–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Mark Dredze</author>
<author>Benjamin Van Durme</author>
<author>Theresa Wilson</author>
<author>David Yarowsky</author>
</authors>
<title>Broadly improving user classification via communication-based name and location clustering on Twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2013),</booktitle>
<pages>1010--1019</pages>
<location>Atlanta, USA.</location>
<marker>Bergsma, Dredze, Van Durme, Wilson, Yarowsky, 2013</marker>
<rawString>Shane Bergsma, Mark Dredze, Benjamin Van Durme, Theresa Wilson, and David Yarowsky. 2013. Broadly improving user classification via communication-based name and location clustering on Twitter. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2013), pages 1010–1019, Atlanta, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Earle</author>
<author>Michelle Guy</author>
<author>Richard Buckmaster</author>
<author>Chris Ostrum</author>
<author>Scott Horvath</author>
<author>Amy Vaughan</author>
</authors>
<title>OMG earthquake! can Twitter improve earthquake response?</title>
<date>2010</date>
<journal>Seismological Research Letters,</journal>
<volume>81</volume>
<issue>2</issue>
<contexts>
<context position="1296" citStr="Earle et al., 2010" startWordPosition="193" endWordPosition="196"> — we investigate user-level sense distributions, and detect strong support for “one sense per tweeter”. As part of this, we construct a novel sense-tagged lexical sample dataset based on Twitter and a web corpus. 1 Introduction Social media applications such as Twitter enable users from all over the world to create and share web content spontaneously. The resulting usergenerated content has been identified as having potential in a myriad of applications including real-time event detection (Petrovi´c et al., 2010), trend analysis (Lau et al., 2012) and natural disaster response co-ordination (Earle et al., 2010). However, the dynamism and conversational nature of the text contained in social media can cause problems for traditional NLP approaches such as parsing (Baldwin et al., 2013), meaning that most content-based approaches use simple keyword search or a bag-of-words representation of the text. This paper is a first step towards full lexical semantic analysis of social media text, in investigating the sense distribution of a range of polysemous words in Twitter and a generalpurpose web corpus. The primary finding of this paper is that there are strong user-level lexical semantic priors in Twitter</context>
</contexts>
<marker>Earle, Guy, Buckmaster, Ostrum, Horvath, Vaughan, 2010</marker>
<rawString>Paul Earle, Michelle Guy, Richard Buckmaster, Chris Ostrum, Scott Horvath, and Amy Vaughan. 2010. OMG earthquake! can Twitter improve earthquake response? Seismological Research Letters, 81(2):246–251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Diana McCarthy</author>
<author>Nicholas Gaylord</author>
</authors>
<title>Investigations on word senses and word usages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP</booktitle>
<pages>10--18</pages>
<contexts>
<context position="4650" citStr="Erk et al., 2009" startWordPosition="734" endWordPosition="737"> et al., 1992). Gale et al. established the heuristic on the basis of 9 ambiguous words using a coarsegrained sense inventory, finding that the probability of a given pair of usages of a word taken from a given document having the same sense was 94%. However, Krovetz (1998) found that for a finegrained sense inventory, only 67% of words exhibited the single-sense-per-discourse property for all documents in a corpus. A radically different view on WSD is word usage similarity, whereby two usages of a given word are rated on a continuous scale for similarity, in isolation of any sense inventory (Erk et al., 2009). Gella et al. (2013) constructed a word usage similarity dataset for Twitter messages, and developed a topic modelling approach to the task, building on the work of Lui et al. (2012). To the best of our knowledge, this has been the only attempt to carry out explicit word-level lexical semantic analysis of Twitter text. 3 Dataset Construction In order to study sense distributions of words in Twitter, we need a sense inventory to annotate against, and also a set of Twitter messages to annotate. Further, as a point of comparison for the sense distributions in Twitter, we require a second corpus;</context>
</contexts>
<marker>Erk, McCarthy, Gaylord, 2009</marker>
<rawString>Katrin Erk, Diana McCarthy, and Nicholas Gaylord. 2009. Investigations on word senses and word usages. In Proceedings of the Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP 2009), pages 10–18, Singapore.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, USA.</location>
<contexts>
<context position="4307" citStr="(1998)" startWordPosition="677" endWordPosition="677"> observation that a given word will often occur with a single sense across multiple usages in a single document (Gale 215 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 215–220, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics et al., 1992). Gale et al. established the heuristic on the basis of 9 ambiguous words using a coarsegrained sense inventory, finding that the probability of a given pair of usages of a word taken from a given document having the same sense was 94%. However, Krovetz (1998) found that for a finegrained sense inventory, only 67% of words exhibited the single-sense-per-discourse property for all documents in a corpus. A radically different view on WSD is word usage similarity, whereby two usages of a given word are rated on a continuous scale for similarity, in isolation of any sense inventory (Erk et al., 2009). Gella et al. (2013) constructed a word usage similarity dataset for Twitter messages, and developed a topic modelling approach to the task, building on the work of Lui et al. (2012). To the best of our knowledge, this has been the only attempt to carry ou</context>
</contexts>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
</authors>
<title>Introducing and evaluating ukWaC, a very large web-derived corpus of English.</title>
<date>2008</date>
<booktitle>In Proceedings of the 4th Web as Corpus Workshop: Can we beat Google,</booktitle>
<pages>47--54</pages>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="5297" citStr="Ferraresi et al., 2008" startWordPosition="848" endWordPosition="851">nstructed a word usage similarity dataset for Twitter messages, and developed a topic modelling approach to the task, building on the work of Lui et al. (2012). To the best of our knowledge, this has been the only attempt to carry out explicit word-level lexical semantic analysis of Twitter text. 3 Dataset Construction In order to study sense distributions of words in Twitter, we need a sense inventory to annotate against, and also a set of Twitter messages to annotate. Further, as a point of comparison for the sense distributions in Twitter, we require a second corpus; here we use the ukWaC (Ferraresi et al., 2008), a corpus built from web documents. For the sense inventory, we chose the Macmillan English Dictionary Online1 (MACMILLAN, hereafter), on the basis of: (1) its coarse-grained general-purpose sense distinctions, and (2) its regular update cycle (i.e. it contains many recentlyemerged senses). These criteria are important in terms of inter-annotator agreement (especially as we crowdsourced the sense annotation, as described below) and also sense coverage. The other obvious candidate sense inventory which potentially satisfied these criteria was ONTONOTES (Hovy et al., 2006), but a preliminary se</context>
<context position="6668" citStr="Ferraresi et al., 2008" startWordPosition="1066" endWordPosition="1069">e of 20 polysemous nouns, as listed in Table 1. Our target nouns were selected to span the high- to mid-frequency range in both Twitter and the web corpus, and have at least 3 MACMILLAN senses. The average sense ambiguity is 5.5. 1http://www.macmillandictionary.com band bar case charge deal degree field form function issue job light match panel paper position post rule sign track Table 1: The 20 target nouns used in this research 3.1 Data Sampling We sampled tweets from a crawl made using the Twitter Streaming API from January 3, 2012 to February 29, 2012. The web corpus was built from ukWaC (Ferraresi et al., 2008), which was based on a crawl of the .uk domain from 2007. In contrast to ukWaC, the tweets are not restricted to documents from any particular country. For both corpora, we first selected only the English documents using langid.py, an off-theshelf language identification tool (Lui and Baldwin, 2012). We next identified documents which contained nominal usages of the target words, based on the POS tags supplied with the corpus in the case of ukWaC, and the output of the CMU ARK Twitter POS tagger v2.0 (Owoputi et al., 2012) in the case of Twitter. For Twitter, we are interested in not just the </context>
</contexts>
<marker>Ferraresi, Zanchetta, Baroni, Bernardini, 2008</marker>
<rawString>Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008. Introducing and evaluating ukWaC, a very large web-derived corpus of English. In Proceedings of the 4th Web as Corpus Workshop: Can we beat Google, pages 47–54, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
<author>David Yarowsky</author>
</authors>
<title>One sense per discourse.</title>
<date>1992</date>
<booktitle>In Proceedings of the workshop on Speech and Natural Language,</booktitle>
<pages>233--237</pages>
<contexts>
<context position="677" citStr="Gale et al. (1992)" startWordPosition="95" endWordPosition="98">of Twitter Spandana Gella, Paul Cook and Timothy Baldwin Department of Computing and Information Systems The University of Melbourne sgella@student.unimelb.edu.au, paulcook@unimelb.edu.au,tb@ldwin.net Abstract In recent years, microblogs such as Twitter have emerged as a new communication channel. Twitter in particular has become the target of a myriad of content-based applications including trend analysis and event detection, but there has been little fundamental work on the analysis of word usage patterns in this text type. In this paper — inspired by the one-sense-perdiscourse heuristic of Gale et al. (1992) — we investigate user-level sense distributions, and detect strong support for “one sense per tweeter”. As part of this, we construct a novel sense-tagged lexical sample dataset based on Twitter and a web corpus. 1 Introduction Social media applications such as Twitter enable users from all over the world to create and share web content spontaneously. The resulting usergenerated content has been identified as having potential in a myriad of applications including real-time event detection (Petrovi´c et al., 2010), trend analysis (Lau et al., 2012) and natural disaster response co-ordination (</context>
<context position="2040" citStr="Gale et al., 1992" startWordPosition="311" endWordPosition="314"> approaches such as parsing (Baldwin et al., 2013), meaning that most content-based approaches use simple keyword search or a bag-of-words representation of the text. This paper is a first step towards full lexical semantic analysis of social media text, in investigating the sense distribution of a range of polysemous words in Twitter and a generalpurpose web corpus. The primary finding of this paper is that there are strong user-level lexical semantic priors in Twitter, equivalent in strength to document-level lexical semantic priors, popularly termed the “one sense per discourse” heuristic (Gale et al., 1992). This has potential implications for future applications over Twitter which attempt to move beyond a simple string-based meaning representation to explicit lexical semantic analysis. 2 Related Work The traditional approach to the analysis of wordlevel lexical semantics is via word sense disambiguation (WSD), where usages of a given word are mapped onto discrete “senses” in a preexisting sense inventory (Navigli, 2009). The most popular sense inventory used in WSD research has been WordNet (Fellbaum, 1998), although its finegrained sense distinctions have proven to be difficult to make for hum</context>
<context position="11980" citStr="Gale et al. (1992)" startWordPosition="1971" endWordPosition="1974">noun (with average polysemy = 5.5 senses) is used with the same sense across 5 separate messages. For UKWACDOC the proportion of documents with a single sense of a given target noun 217 Partition Agreement (%) Table 2: Pairwise agreement for each dataset, based on different partitions of the data (“—” indicates no partitioning, and exhaustive comparison) across all usages ranged from 1/20 for case to 20/20 for band, at an average of 63%. As such, the one sense per tweeter heuristic is at least as strong as the one sense per discourse heuristic in UKWACDOC. Looking back to the original work of Gale et al. (1992), it is important to realise that their reported agreement of 94% was calculated pairwise between usages in a given document. When we recalculate the agreement in TWITTERUSER and UKWACDOC using this methodology, as detailed in Table 2 (calculating pairwise agreement within partitions of the data based on “user” and “document”, respectively), we see that the numbers for our datasets are very close to those of Gale et al. on the basis of more than twice as many nouns, and many more instances per noun. Moreover, the one sense per tweeter trend again appears to be slightly stronger than the one se</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William A Gale, Kenneth W Church, and David Yarowsky. 1992. One sense per discourse. In Proceedings of the workshop on Speech and Natural Language, pages 233–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spandana Gella</author>
<author>Paul Cook</author>
<author>Bo Han</author>
</authors>
<title>Unsupervised word usage similarity in social media texts.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<pages>248--253</pages>
<location>Atlanta, USA.</location>
<contexts>
<context position="4671" citStr="Gella et al. (2013)" startWordPosition="738" endWordPosition="741">e et al. established the heuristic on the basis of 9 ambiguous words using a coarsegrained sense inventory, finding that the probability of a given pair of usages of a word taken from a given document having the same sense was 94%. However, Krovetz (1998) found that for a finegrained sense inventory, only 67% of words exhibited the single-sense-per-discourse property for all documents in a corpus. A radically different view on WSD is word usage similarity, whereby two usages of a given word are rated on a continuous scale for similarity, in isolation of any sense inventory (Erk et al., 2009). Gella et al. (2013) constructed a word usage similarity dataset for Twitter messages, and developed a topic modelling approach to the task, building on the work of Lui et al. (2012). To the best of our knowledge, this has been the only attempt to carry out explicit word-level lexical semantic analysis of Twitter text. 3 Dataset Construction In order to study sense distributions of words in Twitter, we need a sense inventory to annotate against, and also a set of Twitter messages to annotate. Further, as a point of comparison for the sense distributions in Twitter, we require a second corpus; here we use the ukWa</context>
</contexts>
<marker>Gella, Cook, Han, 2013</marker>
<rawString>Spandana Gella, Paul Cook, and Bo Han. 2013. Unsupervised word usage similarity in social media texts. In Proceedings of the Second Joint Conference on Lexical and Computational Semantics (*SEM 2013), pages 248–253, Atlanta, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>OntoNotes: The 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>57--60</pages>
<location>New York City, USA.</location>
<contexts>
<context position="2790" citStr="Hovy et al., 2006" startWordPosition="432" endWordPosition="435">resentation to explicit lexical semantic analysis. 2 Related Work The traditional approach to the analysis of wordlevel lexical semantics is via word sense disambiguation (WSD), where usages of a given word are mapped onto discrete “senses” in a preexisting sense inventory (Navigli, 2009). The most popular sense inventory used in WSD research has been WordNet (Fellbaum, 1998), although its finegrained sense distinctions have proven to be difficult to make for human annotators and WSD systems alike. This has resulted in a move towards more coarse-grained sense inventories (Palmer et al., 2004; Hovy et al., 2006; Navigli et al., 2007), or alternatively away from pre-existing sense inventories altogether, towards joint word sense induction (WSI) and disambiguation (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). Two heuristics that have proven highly powerful in WSD and WSI research are: (1) first sense tagging, and (2) one sense per discourse. First sense tagging is based on the observation that sense distributions tend to be Zipfian, such that if the predominant or “first” sense can be identified, simply tagging all occurrences of a given word with this sense can achieve high WSD accuracy </context>
<context position="5875" citStr="Hovy et al., 2006" startWordPosition="934" endWordPosition="937">e use the ukWaC (Ferraresi et al., 2008), a corpus built from web documents. For the sense inventory, we chose the Macmillan English Dictionary Online1 (MACMILLAN, hereafter), on the basis of: (1) its coarse-grained general-purpose sense distinctions, and (2) its regular update cycle (i.e. it contains many recentlyemerged senses). These criteria are important in terms of inter-annotator agreement (especially as we crowdsourced the sense annotation, as described below) and also sense coverage. The other obvious candidate sense inventory which potentially satisfied these criteria was ONTONOTES (Hovy et al., 2006), but a preliminary sensetagging exercise indicated that MACMILLAN better captured Twitter-specific usages. Rather than annotating all words, we opted for a lexical sample of 20 polysemous nouns, as listed in Table 1. Our target nouns were selected to span the high- to mid-frequency range in both Twitter and the web corpus, and have at least 3 MACMILLAN senses. The average sense ambiguity is 5.5. 1http://www.macmillandictionary.com band bar case charge deal degree field form function issue job light match panel paper position post rule sign track Table 1: The 20 target nouns used in this resea</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: The 90% solution. In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 57–60, New York City, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Ioannis Klapaftis</author>
</authors>
<title>Semeval2013 task 13: Word sense induction for graded and non-graded senses.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>290--299</pages>
<location>Atlanta, USA.</location>
<contexts>
<context position="3002" citStr="Jurgens and Klapaftis, 2013" startWordPosition="462" endWordPosition="465"> word are mapped onto discrete “senses” in a preexisting sense inventory (Navigli, 2009). The most popular sense inventory used in WSD research has been WordNet (Fellbaum, 1998), although its finegrained sense distinctions have proven to be difficult to make for human annotators and WSD systems alike. This has resulted in a move towards more coarse-grained sense inventories (Palmer et al., 2004; Hovy et al., 2006; Navigli et al., 2007), or alternatively away from pre-existing sense inventories altogether, towards joint word sense induction (WSI) and disambiguation (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). Two heuristics that have proven highly powerful in WSD and WSI research are: (1) first sense tagging, and (2) one sense per discourse. First sense tagging is based on the observation that sense distributions tend to be Zipfian, such that if the predominant or “first” sense can be identified, simply tagging all occurrences of a given word with this sense can achieve high WSD accuracy (McCarthy et al., 2007). Unsurprisingly, there are significant differences in sense distributions across domains (cf. cloud in the COMPUTING and METEOROLOGICAL domains), motivating the need for unsupervised first</context>
</contexts>
<marker>Jurgens, Klapaftis, 2013</marker>
<rawString>David Jurgens and Ioannis Klapaftis. 2013. Semeval2013 task 13: Word sense induction for graded and non-graded senses. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), pages 290–299, Atlanta, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Koeling</author>
<author>Diana McCarthy</author>
<author>John Carroll</author>
</authors>
<title>Domain-specific sense distributions and predominant sense acquisition.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>419--426</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="3669" citStr="Koeling et al., 2005" startWordPosition="570" endWordPosition="573">ful in WSD and WSI research are: (1) first sense tagging, and (2) one sense per discourse. First sense tagging is based on the observation that sense distributions tend to be Zipfian, such that if the predominant or “first” sense can be identified, simply tagging all occurrences of a given word with this sense can achieve high WSD accuracy (McCarthy et al., 2007). Unsurprisingly, there are significant differences in sense distributions across domains (cf. cloud in the COMPUTING and METEOROLOGICAL domains), motivating the need for unsupervised first sense learning over domain-specific corpora (Koeling et al., 2005). One sense per discourse is the observation that a given word will often occur with a single sense across multiple usages in a single document (Gale 215 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 215–220, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics et al., 1992). Gale et al. established the heuristic on the basis of 9 ambiguous words using a coarsegrained sense inventory, finding that the probability of a given pair of usages of a word taken from a given document having the same </context>
</contexts>
<marker>Koeling, McCarthy, Carroll, 2005</marker>
<rawString>Rob Koeling, Diana McCarthy, and John Carroll. 2005. Domain-specific sense distributions and predominant sense acquisition. In Proceedings of the 2005 Conference on Empirical Methods in Natural Language Processing (EMNLP 2005), pages 419– 426, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Krovetz</author>
</authors>
<title>More than one sense per discourse.</title>
<date>1998</date>
<journal>NEC Princeton NJ Labs., Research Memorandum.</journal>
<contexts>
<context position="4307" citStr="Krovetz (1998)" startWordPosition="676" endWordPosition="677">e is the observation that a given word will often occur with a single sense across multiple usages in a single document (Gale 215 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 215–220, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics et al., 1992). Gale et al. established the heuristic on the basis of 9 ambiguous words using a coarsegrained sense inventory, finding that the probability of a given pair of usages of a word taken from a given document having the same sense was 94%. However, Krovetz (1998) found that for a finegrained sense inventory, only 67% of words exhibited the single-sense-per-discourse property for all documents in a corpus. A radically different view on WSD is word usage similarity, whereby two usages of a given word are rated on a continuous scale for similarity, in isolation of any sense inventory (Erk et al., 2009). Gella et al. (2013) constructed a word usage similarity dataset for Twitter messages, and developed a topic modelling approach to the task, building on the work of Lui et al. (2012). To the best of our knowledge, this has been the only attempt to carry ou</context>
</contexts>
<marker>Krovetz, 1998</marker>
<rawString>Robert Krovetz. 1998. More than one sense per discourse. NEC Princeton NJ Labs., Research Memorandum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jey Han Lau</author>
<author>Nigel Collier</author>
<author>Timothy Baldwin</author>
</authors>
<title>On-line trend analysis with topic models: #twitter trends detection topic model online.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012),</booktitle>
<pages>1519--1534</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="1231" citStr="Lau et al., 2012" startWordPosition="183" endWordPosition="186">d by the one-sense-perdiscourse heuristic of Gale et al. (1992) — we investigate user-level sense distributions, and detect strong support for “one sense per tweeter”. As part of this, we construct a novel sense-tagged lexical sample dataset based on Twitter and a web corpus. 1 Introduction Social media applications such as Twitter enable users from all over the world to create and share web content spontaneously. The resulting usergenerated content has been identified as having potential in a myriad of applications including real-time event detection (Petrovi´c et al., 2010), trend analysis (Lau et al., 2012) and natural disaster response co-ordination (Earle et al., 2010). However, the dynamism and conversational nature of the text contained in social media can cause problems for traditional NLP approaches such as parsing (Baldwin et al., 2013), meaning that most content-based approaches use simple keyword search or a bag-of-words representation of the text. This paper is a first step towards full lexical semantic analysis of social media text, in investigating the sense distribution of a range of polysemous words in Twitter and a generalpurpose web corpus. The primary finding of this paper is th</context>
</contexts>
<marker>Lau, Collier, Baldwin, 2012</marker>
<rawString>Jey Han Lau, Nigel Collier, and Timothy Baldwin. 2012. On-line trend analysis with topic models: #twitter trends detection topic model online. In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012), pages 1519–1534, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Timothy Baldwin</author>
</authors>
<title>langid.py: An off-the-shelf language identification tool.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012) Demo Session,</booktitle>
<pages>25--30</pages>
<location>Jeju, Republic of</location>
<contexts>
<context position="6968" citStr="Lui and Baldwin, 2012" startWordPosition="1117" endWordPosition="1121">eld form function issue job light match panel paper position post rule sign track Table 1: The 20 target nouns used in this research 3.1 Data Sampling We sampled tweets from a crawl made using the Twitter Streaming API from January 3, 2012 to February 29, 2012. The web corpus was built from ukWaC (Ferraresi et al., 2008), which was based on a crawl of the .uk domain from 2007. In contrast to ukWaC, the tweets are not restricted to documents from any particular country. For both corpora, we first selected only the English documents using langid.py, an off-theshelf language identification tool (Lui and Baldwin, 2012). We next identified documents which contained nominal usages of the target words, based on the POS tags supplied with the corpus in the case of ukWaC, and the output of the CMU ARK Twitter POS tagger v2.0 (Owoputi et al., 2012) in the case of Twitter. For Twitter, we are interested in not just the overall lexical distribution of each target noun, but also per-user lexical distributions. As such, we construct two Twitter-based datasets: (1) TWITTERRAND, a random sample of 100 usages of each target noun; and (2) TWITTERUSER, 5 usages of each target noun from each member of a random sample of 20</context>
</contexts>
<marker>Lui, Baldwin, 2012</marker>
<rawString>Marco Lui and Timothy Baldwin. 2012. langid.py: An off-the-shelf language identification tool. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012) Demo Session, pages 25–30, Jeju, Republic of Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Timothy Baldwin</author>
<author>Diana McCarthy</author>
</authors>
<title>Unsupervised estimation of word usage similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop</booktitle>
<pages>33--41</pages>
<location>Dunedin, New Zealand.</location>
<contexts>
<context position="4833" citStr="Lui et al. (2012)" startWordPosition="766" endWordPosition="769">f a word taken from a given document having the same sense was 94%. However, Krovetz (1998) found that for a finegrained sense inventory, only 67% of words exhibited the single-sense-per-discourse property for all documents in a corpus. A radically different view on WSD is word usage similarity, whereby two usages of a given word are rated on a continuous scale for similarity, in isolation of any sense inventory (Erk et al., 2009). Gella et al. (2013) constructed a word usage similarity dataset for Twitter messages, and developed a topic modelling approach to the task, building on the work of Lui et al. (2012). To the best of our knowledge, this has been the only attempt to carry out explicit word-level lexical semantic analysis of Twitter text. 3 Dataset Construction In order to study sense distributions of words in Twitter, we need a sense inventory to annotate against, and also a set of Twitter messages to annotate. Further, as a point of comparison for the sense distributions in Twitter, we require a second corpus; here we use the ukWaC (Ferraresi et al., 2008), a corpus built from web documents. For the sense inventory, we chose the Macmillan English Dictionary Online1 (MACMILLAN, hereafter), </context>
</contexts>
<marker>Lui, Baldwin, McCarthy, 2012</marker>
<rawString>Marco Lui, Timothy Baldwin, and Diana McCarthy. 2012. Unsupervised estimation of word usage similarity. In Proceedings of the Australasian Language Technology Workshop 2012 (ALTW 2012), pages 33–41, Dunedin, New Zealand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Unsupervised acquisition of predominant word senses.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>4</volume>
<issue>33</issue>
<contexts>
<context position="3413" citStr="McCarthy et al., 2007" startWordPosition="534" endWordPosition="537"> Navigli et al., 2007), or alternatively away from pre-existing sense inventories altogether, towards joint word sense induction (WSI) and disambiguation (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). Two heuristics that have proven highly powerful in WSD and WSI research are: (1) first sense tagging, and (2) one sense per discourse. First sense tagging is based on the observation that sense distributions tend to be Zipfian, such that if the predominant or “first” sense can be identified, simply tagging all occurrences of a given word with this sense can achieve high WSD accuracy (McCarthy et al., 2007). Unsurprisingly, there are significant differences in sense distributions across domains (cf. cloud in the COMPUTING and METEOROLOGICAL domains), motivating the need for unsupervised first sense learning over domain-specific corpora (Koeling et al., 2005). One sense per discourse is the observation that a given word will often occur with a single sense across multiple usages in a single document (Gale 215 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 215–220, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Compu</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2007</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2007. Unsupervised acquisition of predominant word senses. Computational Linguistics, 4(33):553–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Timothy Chklovski</author>
<author>Adam Kilgarriff</author>
</authors>
<title>The Senseval-3 English lexical sample task.</title>
<date>2004</date>
<booktitle>In Proceedings of Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>25--28</pages>
<location>Barcelona,</location>
<contexts>
<context position="9143" citStr="Mihalcea et al., 2004" startWordPosition="1486" endWordPosition="1489">annotation HIT for position 5 sentences. 5 such sentences were selected for annotation, resulting in a total of 20 nouns x 20 documents x 5 sentences = 2000 sentences. 3.2 Annotation Settings We sense-tagged each of the four datasets using Amazon Mechanical Turk (AMT). Each Human Intelligence Task (HIT) comprised 5 occurrences of a given target noun, with the target noun highlighted in each. Sense definitions and an example sentence (where available) were provided from MACMILLAN. Turkers were free to select multiple sense labels where applicable, in line with best practice in sense labelling (Mihalcea et al., 2004). We also provided an “Other” sense option, in cases where none of the MACMILLAN senses were applicable to the current usage of the target noun. A screenshot of the annotation interface for a single usage is provided in Figure 1. Of the five sentences in each HIT, one was a heldout example sentence for one of the senses of the target noun, taken from MACMILLAN. This gold-standard example was used exclusively for quality assurance purposes, and used to filter the annotations as follows: 1. Accept all HITs from Turkers whose goldstandard tagging accuracy was ≥ 80%; 2. Reject all HITs from Turker</context>
</contexts>
<marker>Mihalcea, Chklovski, Kilgarriff, 2004</marker>
<rawString>Rada Mihalcea, Timothy Chklovski, and Adam Kilgarriff. 2004. The Senseval-3 English lexical sample task. In Proceedings of Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 25–28, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Daniele Vannella</author>
</authors>
<title>SemEval-2013 task 11: Word sense induction and disambiguation within an end-user application.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>193--201</pages>
<location>Atlanta, USA.</location>
<contexts>
<context position="2972" citStr="Navigli and Vannella, 2013" startWordPosition="458" endWordPosition="461">SD), where usages of a given word are mapped onto discrete “senses” in a preexisting sense inventory (Navigli, 2009). The most popular sense inventory used in WSD research has been WordNet (Fellbaum, 1998), although its finegrained sense distinctions have proven to be difficult to make for human annotators and WSD systems alike. This has resulted in a move towards more coarse-grained sense inventories (Palmer et al., 2004; Hovy et al., 2006; Navigli et al., 2007), or alternatively away from pre-existing sense inventories altogether, towards joint word sense induction (WSI) and disambiguation (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). Two heuristics that have proven highly powerful in WSD and WSI research are: (1) first sense tagging, and (2) one sense per discourse. First sense tagging is based on the observation that sense distributions tend to be Zipfian, such that if the predominant or “first” sense can be identified, simply tagging all occurrences of a given word with this sense can achieve high WSD accuracy (McCarthy et al., 2007). Unsurprisingly, there are significant differences in sense distributions across domains (cf. cloud in the COMPUTING and METEOROLOGICAL domains), motivating t</context>
</contexts>
<marker>Navigli, Vannella, 2013</marker>
<rawString>Roberto Navigli and Daniele Vannella. 2013. SemEval-2013 task 11: Word sense induction and disambiguation within an end-user application. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), pages 193– 201, Atlanta, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Kenneth C Litkowski</author>
<author>Orin Hargraves</author>
</authors>
<title>SemEval-2007 task 07: Coarsegrained English all-words task.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>30--35</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2813" citStr="Navigli et al., 2007" startWordPosition="436" endWordPosition="439">icit lexical semantic analysis. 2 Related Work The traditional approach to the analysis of wordlevel lexical semantics is via word sense disambiguation (WSD), where usages of a given word are mapped onto discrete “senses” in a preexisting sense inventory (Navigli, 2009). The most popular sense inventory used in WSD research has been WordNet (Fellbaum, 1998), although its finegrained sense distinctions have proven to be difficult to make for human annotators and WSD systems alike. This has resulted in a move towards more coarse-grained sense inventories (Palmer et al., 2004; Hovy et al., 2006; Navigli et al., 2007), or alternatively away from pre-existing sense inventories altogether, towards joint word sense induction (WSI) and disambiguation (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). Two heuristics that have proven highly powerful in WSD and WSI research are: (1) first sense tagging, and (2) one sense per discourse. First sense tagging is based on the observation that sense distributions tend to be Zipfian, such that if the predominant or “first” sense can be identified, simply tagging all occurrences of a given word with this sense can achieve high WSD accuracy (McCarthy et al., 2007)</context>
</contexts>
<marker>Navigli, Litkowski, Hargraves, 2007</marker>
<rawString>Roberto Navigli, Kenneth C. Litkowski, and Orin Hargraves. 2007. SemEval-2007 task 07: Coarsegrained English all-words task. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 30–35, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word sense disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys (CSUR),</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="2462" citStr="Navigli, 2009" startWordPosition="379" endWordPosition="380">ng user-level lexical semantic priors in Twitter, equivalent in strength to document-level lexical semantic priors, popularly termed the “one sense per discourse” heuristic (Gale et al., 1992). This has potential implications for future applications over Twitter which attempt to move beyond a simple string-based meaning representation to explicit lexical semantic analysis. 2 Related Work The traditional approach to the analysis of wordlevel lexical semantics is via word sense disambiguation (WSD), where usages of a given word are mapped onto discrete “senses” in a preexisting sense inventory (Navigli, 2009). The most popular sense inventory used in WSD research has been WordNet (Fellbaum, 1998), although its finegrained sense distinctions have proven to be difficult to make for human annotators and WSD systems alike. This has resulted in a move towards more coarse-grained sense inventories (Palmer et al., 2004; Hovy et al., 2006; Navigli et al., 2007), or alternatively away from pre-existing sense inventories altogether, towards joint word sense induction (WSI) and disambiguation (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). Two heuristics that have proven highly powerful in WSD and</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word sense disambiguation: A survey. ACM Computing Surveys (CSUR), 41(2):10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
</authors>
<title>Partof-speech tagging for Twitter: Word clusters and other advances.</title>
<date>2012</date>
<tech>Technical Report CMU-ML-12-107,</tech>
<institution>Machine Learning Department, Carnegie Mellon University.</institution>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, 2012</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, and Nathan Schneider. 2012. Partof-speech tagging for Twitter: Word clusters and other advances. Technical Report CMU-ML-12-107, Machine Learning Department, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Olga Babko-Malaya</author>
<author>Hoa Trang Dang</author>
</authors>
<title>Different sense granularities for different applications.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT-NAACL 2004 Workshop: 2nd Workshop on Scalable Natural Language Understanding,</booktitle>
<pages>49--56</pages>
<location>Boston, USA.</location>
<contexts>
<context position="2771" citStr="Palmer et al., 2004" startWordPosition="428" endWordPosition="431">ing-based meaning representation to explicit lexical semantic analysis. 2 Related Work The traditional approach to the analysis of wordlevel lexical semantics is via word sense disambiguation (WSD), where usages of a given word are mapped onto discrete “senses” in a preexisting sense inventory (Navigli, 2009). The most popular sense inventory used in WSD research has been WordNet (Fellbaum, 1998), although its finegrained sense distinctions have proven to be difficult to make for human annotators and WSD systems alike. This has resulted in a move towards more coarse-grained sense inventories (Palmer et al., 2004; Hovy et al., 2006; Navigli et al., 2007), or alternatively away from pre-existing sense inventories altogether, towards joint word sense induction (WSI) and disambiguation (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). Two heuristics that have proven highly powerful in WSD and WSI research are: (1) first sense tagging, and (2) one sense per discourse. First sense tagging is based on the observation that sense distributions tend to be Zipfian, such that if the predominant or “first” sense can be identified, simply tagging all occurrences of a given word with this sense can achieve</context>
</contexts>
<marker>Palmer, Babko-Malaya, Dang, 2004</marker>
<rawString>Martha Palmer, Olga Babko-Malaya, and Hoa Trang Dang. 2004. Different sense granularities for different applications. In Proceedings of the HLT-NAACL 2004 Workshop: 2nd Workshop on Scalable Natural Language Understanding, pages 49–56, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasa Petrovi´c</author>
<author>Miles Osborne</author>
<author>Victor Lavrenko</author>
</authors>
<title>Streaming first story detection with application to twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT 2010),</booktitle>
<pages>181--189</pages>
<location>Los Angeles, USA.</location>
<marker>Petrovi´c, Osborne, Lavrenko, 2010</marker>
<rawString>Sasa Petrovi´c, Miles Osborne, and Victor Lavrenko. 2010. Streaming first story detection with application to twitter. In Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT 2010), pages 181–189, Los Angeles, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeroen Vuurens</author>
<author>Arjen P de Vries</author>
<author>Carsten Eickhoff</author>
</authors>
<title>How much spam can you take? an analysis of crowdsourcing results to increase accuracy.</title>
<date>2011</date>
<booktitle>In Proc. ACM SIGIR Workshop on Crowdsourcing for Information Retrieval (CIR</booktitle>
<pages>21--26</pages>
<marker>Vuurens, de Vries, Eickhoff, 2011</marker>
<rawString>Jeroen Vuurens, Arjen P de Vries, and Carsten Eickhoff. 2011. How much spam can you take? an analysis of crowdsourcing results to increase accuracy. In Proc. ACM SIGIR Workshop on Crowdsourcing for Information Retrieval (CIR 2011), pages 21–26.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>