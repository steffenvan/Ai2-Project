<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001062">
<title confidence="0.979525">
Beyond N in N-gram Tagging
</title>
<author confidence="0.744497">
Robbert Prins
</author>
<affiliation confidence="0.5852025">
Alfa-Informatica
University of Groningen
</affiliation>
<address confidence="0.610256">
P.O. Box 716, NL-9700 AS Groningen
The Netherlands
</address>
<email confidence="0.994758">
r.p.prins@let.rug.nl
</email>
<sectionHeader confidence="0.996514" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999536652173913">
The Hidden Markov Model (HMM) for
part-of-speech (POS) tagging is typi-
cally based on tag trigrams. As such
it models local context but not global
context, leaving long-distance syntactic
relations unrepresented. Using n-gram
models for n &gt; 3 in order to incorporate
global context is problematic as the tag
sequences corresponding to higher order
models will become increasingly rare in
training data, leading to incorrect esti-
mations of their probabilities.
The trigram HMM can be extended with
global contextual information, without
making the model infeasible, by incor-
porating the context separately from the
POS tags. The new information incor-
porated in the model is acquired through
the use of a wide-coverage parser. The
model is trained and tested on Dutch text
from two different sources, showing an
increase in tagging accuracy compared
to tagging using the standard model.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99855275">
The Hidden Markov Model (HMM) used for part-
of-speech (POS) tagging is usually a second-order
model, using tag trigrams, implementing the idea
that a limited number of preceding tags provide a
considerable amount of information on the iden-
tity of the current tag. This approach leads to
good results. For example, the TnT trigram HMM
tagger achieves state-of-the-art tagging accuracies
on English and German (Brants, 2000). In gen-
eral, however, as the model does not consider
global context, mistakes are made that concern
long-distance syntactic relations.
</bodyText>
<sectionHeader confidence="0.875665" genericHeader="method">
2 A restriction of HMM tagging
</sectionHeader>
<bodyText confidence="0.9999394">
The simplifying assumption, which is the basis for
HMM tagging, that the context of a given tag can
be fully represented by just the previous two tags,
leads to tagging errors where syntactic features
that fall outside of this range, and that are needed
for determining the identity of the tag at hand, are
ignored.
One such error in tagging Dutch is related to
finiteness of verbs. This is discussed in the next
paragraph and will be used in explaining the pro-
posed approach. Other possible applications of the
technique include assignment of case in German,
and assignment of chunk tags in addition to part-
of-speech tags. These will be briefly discussed at
the end of this paper.
</bodyText>
<subsectionHeader confidence="0.995315">
2.1 An example from Dutch
</subsectionHeader>
<bodyText confidence="0.997151181818182">
In experiments on tagging Dutch text performed
in the context of (Prins and van Noord, 2004), the
most frequent type of error is a typical example
of a mistake caused by a lack of access to global
context. In Dutch, the plural finite form of a verb
is similar in appearance to the infinitive form of
the verb. In example (1-a) the second verb in the
sentence, vliegen, is correctly tagged as an infini-
tive, but in example (1-b) the added adverb creates
a surrounding in which the tagger incorrectly la-
bels the verb as the finite plural form.
</bodyText>
<figure confidence="0.712746333333333">
(1) a. Jan zag–past sg vogels vliegen–inf
Jan saw birds fly
b. *Jan zag–past sg vogels vliegen–pl
Jan saw birds fly
gisteren
yesterday
</figure>
<bodyText confidence="0.998109785714286">
Since a clause normally contains precisely one fi-
nite verb, this mistake could be avoided by re-
membering whether the finite verb for the current
clause has already occurred, and using this infor-
mation in classifying a newly observed verb as
either finite or nonfinite. The trigram tagger has
normally “forgotten” about any finite verb upon
reaching a second verb, and is led into a mistake
by other parts of the context even if the two verbs
are close to each other.
Basing the model on n-grams bigger than tri-
grams is not a solution as the n-grams would often
not occur in the training data, making the associ-
ated probabilities hard to estimate.
</bodyText>
<sectionHeader confidence="0.992959" genericHeader="method">
3 Extending the model
</sectionHeader>
<bodyText confidence="0.99957552">
Instead of considering longer n-grams, the model
can be extended with specific long-distance con-
text information. Analogous to how sequences of
tags can be modeled as a probabilistic network of
events, modeling the probability of a tag given a
number of preceding tags, in the same way we can
model the syntactic context.
For the example problem presented in sec-
tion 2.1, this network would consist of two states:
pre and post. In state pre the finite verb for the
current clause has not yet been seen, while in state
post is has. In general, the context feature C with
values C1...j and its probability distribution is to
be incorporated in the model.
In describing how the extra context information
is added to the HMM, we will first look at how
the standard model for POS tagging is constructed.
Then the probability distribution on which the new
model is based is introduced. A distinction is
made between a naive approach where the extra
context is added to the model by extending the
tagset, and a method where the context is added
separately from the tags which results in a much
smaller increase in the number of probabilities to
be estimated from the training data.
</bodyText>
<subsectionHeader confidence="0.998358">
3.1 Standard model
</subsectionHeader>
<bodyText confidence="0.986335333333333">
In the standard second order HMM used for
POS tagging (as described for example in chap-
ter 10.2 of (Manning and Sch¨utze, 1999)), a sin-
gle state corresponds to two POS tags, and the
observed symbols are words. The transitions be-
tween states are governed by probabilities that
combine the probabilities for state transitions (tag
sequences ti−2, ti−1, ti) and output of observed
symbols (words wi):
</bodyText>
<equation confidence="0.819388">
P(ti, wi|ti−2, ti−1)
</equation>
<bodyText confidence="0.999301666666667">
This probability distribution over tags and words
is factorized into two separate distributions, using
the chain rule P(A,B|C) = P(A|C)·P(B|C,A):
</bodyText>
<equation confidence="0.9996465">
P(ti,wi|ti−2,ti−1) =
P(ti|ti−2, ti−1) · P(wi|ti−2, ti−1, ti)
</equation>
<bodyText confidence="0.9991385">
Finally, the POS tagging assumption that the word
only depends on the current tag is applied:
</bodyText>
<equation confidence="0.63295">
P(ti, wi|ti−2, ti−1)  P(ti|ti−2, ti−1) · P(wi|ti)
</equation>
<bodyText confidence="0.99859425">
If  is the size of the tagset,  the size of the
vocabulary, and n the length of the tag n-grams
used, then the number of parameters in this stan-
dard model is n + .
</bodyText>
<subsectionHeader confidence="0.995261">
3.2 Extended model
</subsectionHeader>
<bodyText confidence="0.968751">
As a starting point in adding the extra feature to
the model, the same probability distribution used
as a basis for the standard model is used:
P(ti, wi|ti−2, ti−1)
Naive method: extending the tagset. The con-
textual information C with j possible values could
be added to the model by extending the set of tags,
so that every tag t in the tagset is replaced by a
set of tags {tc1, tc2, ... , tcj}. If  is the size of
the original tagset, then the number of parameters
in this extended model would be njn + j, the
number of tag n-grams being multiplied by eight
in our example. In experiments this increase in the
number of parameters led to less accurate proba-
bility estimates.
</bodyText>
<equation confidence="0.921322">
C(t1t2)
C(t1t2)+cXD(t1t2)
(0
3 = Sl
if C(t1t2) = 0
if C(t1t2) &gt; 0
</equation>
<bodyText confidence="0.999906538461539">
Better method: adding context to states as a
separate feature. In order to avoid the problem
associated with the naive method, the context fea-
ture is added to the states of the model separately
from the tags. This way it is possible to com-
bine probabilities from the different distributions
in an appropriate manner, restricting the increase
in the number of parameters. For example, it is
now stated that as far as the context feature is con-
cerned, the model is first order. The probabilities
associated with state transitions are defined as fol-
lows, where ci is the value of the new context fea-
ture at position i:
</bodyText>
<equation confidence="0.853394">
P(ti, wi, ci|ti−2, ti−1, ci−1)
</equation>
<bodyText confidence="0.999898">
As before, the probability distribution is factorized
into separate distributions:
</bodyText>
<equation confidence="0.9996455">
P(ti, wi, ci|ti−2, ti−1, ci−1) =
P(ti|ti−2, ti−1, ci−1) ·
P(ci|ti−2, ti−1, ci−1, ti) ·
P(wi|ti−2, ti−1, ci−1, ti, ci)
</equation>
<bodyText confidence="0.9999758">
The assumption made in the standard POS tagging
model that words only depend on the correspond-
ing tag is applied, as well as the assumption that
the current context value only depends on the cur-
rent tag and the previous context value:
</bodyText>
<equation confidence="0.99865125">
P(ti, wi, ci|ti−2, ti−1, ci−1) 
P(ti|ti−2, ti−1, ci−1) ·
P(ci|ci−1, ti) ·
P(wi|ti)
</equation>
<bodyText confidence="0.9999338">
The total numbers of parameters for this model is
nj+j2+. In the case of the example problem
this means the number of tag n-grams is multiplied
by two. The experiments described in section 5
will make use of this model.
</bodyText>
<subsectionHeader confidence="0.993844">
3.3 Training the model
</subsectionHeader>
<bodyText confidence="0.999968217391304">
The model’s probabilities are estimated from an-
notated training data. Since the model is extended
with global context, this has to be part of the an-
notation. The Alpino wide-coverage parser for
Dutch (Bouma et al., 2001) was used to automati-
cally add the extra information to the data. For the
example concerning finite plural verbs and infini-
tives, this means the parser labels every word in
the sentence with one of the two possible context
values. When the parser encounters a root clause
(including imperative clauses and questions) or a
subordinate clause (including relative clauses), it
assigns the context value pre. When a finite verb
is encountered, the value post is assigned. Past the
end of a root clause or subordinate clause the con-
text is reset to the value used before the embedded
clause began. In all other cases, the value assigned
to the previous position is continued.
From the text annotated with POS tags and con-
text labels the n-gram probabilities and lexical
probabilities needed by the model are estimated
based on the frequencies of the corresponding se-
quences.
</bodyText>
<sectionHeader confidence="0.998755" genericHeader="method">
4 The tagger
</sectionHeader>
<subsectionHeader confidence="0.998269">
4.1 Tagging method
</subsectionHeader>
<bodyText confidence="0.999973">
The trigram HMM tagger used in the experiments
of section 5 computes the a posteriori probability
for every tag. This value is composed of the for-
ward and backward probability of the tag at hand
as defined in the forward-backward algorithm for
HMM-training. This idea is also described in (Je-
linek, 1998) and (Charniak et al., 1996). The
trigram data is combined with bigram and uni-
gram data through linear interpolation to reduce
the problem of sparse data.
</bodyText>
<subsectionHeader confidence="0.740669">
4.1.1 Smoothing
</subsectionHeader>
<bodyText confidence="0.999933473684211">
Applying the method known as linear inter-
polation, probabilities of unigrams, bigrams and
trigrams are combined in a weighted sum using
weights 1, 2 and 3 respectively. The weights
are computed for every individual case using the
notion of n-gram diversity (Collins, 1999). The di-
versity of an n-gram is the number of different tags
that appear in the position following this n-gram
in the training data. The weight 3 assigned to
the trigram t1t2t3 is computed on the basis of the
diversity and frequency of the prefix bigram t1t2,
using the following equation, where c regulates the
importance of diversity (c = 6 was used in the ex-
periments described below), and C(x) and D(x)
are respectively the count and diversity of x:
The bigram weight A2 is computed as a fraction
of 1 − A3 using the bigram version of the above
equation. The remaining weight 1 − A3 − A2 is
used as the unigram weight A1.
</bodyText>
<subsubsectionHeader confidence="0.647406">
4.1.2 Unknown words
</subsubsectionHeader>
<bodyText confidence="0.999987444444445">
The tagger uses a lexicon that has been created
from the training data to assign an initial set of
possible tags to every word. Words that were not
seen during training are not in the lexicon, so that
another method has to be used to assign initial tags
to these words. A technique described and imple-
mented by Jan Daciuk (Daciuk, 1999) was used
to create automata for associating words with tags
based on suffixes of those words.
</bodyText>
<sectionHeader confidence="0.998074" genericHeader="method">
5 Tagging experiment
</sectionHeader>
<subsectionHeader confidence="0.9890435">
5.1 Experiment setup
5.1.1 Method
</subsectionHeader>
<bodyText confidence="0.999914444444445">
An extended model was created featuring con-
text information on the occurrence of the finite
verb form. The tagger is used to tag a set of sen-
tences, assigning one tag to each word, first using
the standard model and then using the extended
model. The results are compared in terms of tag-
ging accuracy. The experiment is conducted twice
with different data sets used for both training and
testing.
</bodyText>
<subsubsectionHeader confidence="0.640343">
5.1.2 Data
</subsubsectionHeader>
<bodyText confidence="0.999976257142857">
The first set consists of a large amount of Dutch
newspaper text that was annotated with syntactical
tags by the Alpino parser. This is referred to as
the “Alpino” data. The second and much smaller
set of data is the Eindhoven corpus tagged with
the Wotan tagset (Berghmans, 1994). This data
set was also used in (van Halteren et al., 2001),
therefore the second experiment will allow for a
comparison of the results with previous work on
tagging Dutch. This data will be referred to as the
“Wotan” data.
For both sets the contextual information con-
cerning finite verbs is added to the training data by
the Alpino parser as described in section 3.3. Due
to memory restrictions, the parser was not able to
parse 265 of the 36K sentences of Wotan training
data. These sentences received no contextual la-
bels and thus not all of the training data used in
(van Halteren et al., 2001) could be used in the
Wotan experiment.
Training data for the Alpino experiment is four
years of daily newspaper text, amounting to about
2M sentences (25M words). Test data is a col-
lection of 3686 sentences (59K words) from the
Parool newspaper. The data is annotated with a
tagset consisting of 2825 tags. (The large size
of the Alpino tagset is mainly due to a large
number of infrequent tags representing specific
uses of prepositions.) In the Wotan experiment,
36K sentences (628K words) are used for training
(compared to 640K words in (van Halteren et al.,
2001)), and 4176 sentences (72K words) are used
for testing. The Wotan data is annotated with a
tagset consisting of 345 tags (although a number
of 341 is reported in (van Halteren et al., 2001)).
</bodyText>
<subsubsectionHeader confidence="0.758743">
5.1.3 Baseline method
</subsubsectionHeader>
<bodyText confidence="0.999917625">
As a baseline method every word is assigned the
tag it was most often seen with in the training data.
Thus the baseline method is to tag each word w
with a tag t such that P(t|w) is maximized. Un-
known words are represented by all words that
occurred only once. The baseline accuracies are
85.9% on the Alpino data and 84.3% on the Wotan
data.
</bodyText>
<sectionHeader confidence="0.574827" genericHeader="evaluation">
5.2 Results
</sectionHeader>
<subsectionHeader confidence="0.526501">
5.2.1 “Alpino” experiment
</subsectionHeader>
<bodyText confidence="0.999829769230769">
The results on the Alpino data are shown in
table 1. Using the standard model, accuracy is
93.34% (3946 mistakes). Using the extended
model, accuracy is 93.62% (3779 mistakes). This
amounts to an overall error reduction of 4.23%. In
table 2 and 3 the 6 most frequent tagging errors are
listed for tagging using the standard and extended
model respectively. Mistakes where verb(pl)
is mixed up with verb(inf) sum up to 241 in-
stances (6.11% of all mistakes) when using the
standard model, as opposed to 82 cases (2.17%)
using the extended model, an error reduction of
65.98%.
</bodyText>
<subsubsectionHeader confidence="0.780713">
5.2.2 “Wotan” experiment
</subsubsectionHeader>
<bodyText confidence="0.952060666666667">
The results on the Wotan data can be seen in
table 4. Using the standard model, accuracy is
92.05% (5715 mistakes). This result is very simi-
</bodyText>
<table confidence="0.998567875">
baseline accuracy 85.9%
model standard extended
bigram accuracy 92.49% 92.94%
trigram accuracy 93.34% 93.62%
errors 3946 3779
error reduction 167 = 4.23%
pl/inf errors 241 (6.11%) 82 (2.17%)
pl/inf error red. 159 = 65.98%
</table>
<tableCaption confidence="0.996666">
Table 1: Tagging results on Alpino data
</tableCaption>
<table confidence="0.999409857142857">
freq assigned correct
159 verb(inf) verb(pl)
82 verb(pl) verb(inf)
68 proper name(both) 1-proper name(both)
57 proper name(both) noun(de,sg)
53 verb(psp) adjective(no e,adv)
45 proper name(both) 2-proper name(both)
</table>
<tableCaption confidence="0.9906105">
Table 2: Most frequent tagging mistakes on
Alpino data, using standard model
</tableCaption>
<bodyText confidence="0.999969818181818">
lar to the 92.06% reported by Van Halteren, Zavrel
and Daelemans in (van Halteren et al., 2001) who
used the TnT trigram tagger (Brants, 2000) on the
same training and testing data. Using the extended
model, accuracy is 92.26% (5564 mistakes). This
amounts to an overall error reduction of 2.64%.
Mistakes where the plural verb is mixed up with
the infinitive sum up to 316 instances (5.53% of
all mistakes) when using the standard model, as
opposed to 199 cases (3.58%) using the extended
model, an error reduction of 37.03%.
</bodyText>
<subsectionHeader confidence="0.999647">
5.3 Discussion of results
</subsectionHeader>
<bodyText confidence="0.999349">
Extending the standard trigram tagging model
with syntactical information aimed at resolving
the most frequent type of tagging error led to
a considerable reduction of this type of error in
stand-alone POS tagging experiments on two dif-
</bodyText>
<table confidence="0.974810571428572">
freq assigned correct
69 proper name(both) 1-proper name(both)
57 proper name(both) noun(de,sg)
53 verb(inf) verb(pl)
47 verb(psp) adjective(no e,adv)
45 proper name(both) 2-proper name(both)
42 punct(ligg streep) skip
</table>
<tableCaption confidence="0.992959">
Table 3: Most frequent tagging mistakes on
Alpino data, using extended model
</tableCaption>
<table confidence="0.999899875">
baseline accuracy 84.3%
model standard extended
bigram accuracy 91.45% 91.73%
trigram accuracy 92.05% 92.26%
errors 5715 5564
error reduction 151 = 2.64%
pl/inf errors 316 (5.53%) 199 (3.58%)
pl/inf error red. 117 = 37.03%
</table>
<tableCaption confidence="0.999791">
Table 4: Tagging results on Wotan data
</tableCaption>
<bodyText confidence="0.996972382352941">
ferent data sets. At the same time, other types of
errors were also reduced.
The relative error reduction for the specific type
of error involving finite and infinite verb forms
is almost twice as high in the case of the Alpino
data as in the case of the Wotan data (respectively
65.98% and 37.03%). There are at least two pos-
sible explanations for this difference.
The first is a difference in tagsets. Although
the Wotan tagset is much smaller than the Alpino
tagset, the former features a more detailed treat-
ment of verbs. In the Alpino data, the difference
between plural finite verb forms and nonfinite verb
forms is represented through just two tags. In the
Wotan data, this difference is represented by 20
tags. An extended model that predicts which of
the two forms should be used in a given situation
is therefore more complex in the case of the Wotan
data.
A further important difference between the two
data sets is the available amount of training data
(25 million words for the Alpino experiment com-
pared to 628 thousand words for the Wotan ex-
periment). In general a stochastic model such as
the HMM will become more accurate when more
training data is available. The Wotan experiment
was repeated with increasing amounts of training
data, and the results indicated that using more data
would improve the results of both the standard and
the extended model. The advantage of the ex-
tended model over the standard model increases
slightly as more data is available, suggesting that
the extended model would benefit more from extra
data than the standard model.
</bodyText>
<sectionHeader confidence="0.989514" genericHeader="conclusions">
6 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999930782608696">
This work has presented how the HMM for POS
tagging was extended with global contextual in-
formation without increasing the number of pa-
rameters beyond practical limits. Two tagging ex-
periments, using a model extended with a binary
feature concerning the occurrence of finite verb
forms, resulted in improved accuracies compared
to using the standard model. The annotation of
the training data with context labels was acquired
automatically through the use of a wide-coverage
parser.
The tagger described here is used as a POS tag
filter in wide-coverage parsing of Dutch (Prins and
van Noord, 2004), increasing parsing efficiency as
fewer POS tags have to be considered. In addi-
tion to reducing lexical ambiguity, it would be in-
teresting to see if structural ambiguity can be re-
duced. In the approach under consideration, the
tagger supplies the parser with an initial syntac-
tic structure in the form of a partial bracketing of
the input, based on the recognition of larger syn-
tactic units or ’chunks’. Typically chunk tags will
be assigned on the basis of words and their POS
tags. An alternative approach is to use an extended
model that assigns chunk tags and POS tags simul-
taneously, as was done for finite verb occurrence
and POS tags in the current work. In this way, re-
lations between POS tags and chunk tags can be
modeled in both directions.
Another possible application is tagging of Ger-
man. German features different cases, which can
lead to problems for statistical taggers. This is il-
lustrated in (Hinrichs and Trushkina, 2003) who
point out that the TnT tagger wrongly assigns
nominative case instead of accusative in a given
sentence, resulting in the unlikely combination of
two nominatives. The preference for just one as-
signment of the nominative case might be learned
by including case information in the model.
Acknowledgements. This research was carried
out as part of the PIONIER Project Algorithms
for Linguistic Processing, funded by NWO (Dutch
Organization for Scientific Research) and the Uni-
versity of Groningen. I would like to thank Hans
van Halteren for supplying the Eindhoven corpus
data set as used in (van Halteren et al., 2001).
</bodyText>
<sectionHeader confidence="0.998378" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9996892">
J. Berghmans. 1994. Wotan, een automatische gram-
matikale tagger voor het Nederlands. Master’s the-
sis, Dept. of Language and Speech, University of
Nijmegen.
Gosse Bouma, Gertjan van Noord, and Robert Mal-
ouf. 2001. Wide coverage computational analysis
of Dutch. In Walter Daelemans, Khalil Sima’an,
Jorn Veenstra, and Jakub Zavrel, editors, Compu-
tational Linguistics in the Netherlands, CLIN 2000,
pages 45–59, Amsterdam. Rodopi.
Thorsten Brants. 2000. TnT – a statistical part-of-
speech tagger. In Proceedings of the 6th Applied
Natural Language Processing Conference, Seattle,
WA.
E. Charniak, G. Carroll, J. Adcock, A. Cassandra,
Y. Gotoh, J. Katz, M. Littman, and J. McCann.
1996. Taggers for parsers. Artificial Intelligence,
85(1-2):45–57.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia, Pennsyl-
vania.
Jan Daciuk. 1999. Treatment of unknown words. In
Proceedings of the Workshop on Implementing Au-
tomata WIA’99, pages IX–1 – IX–9, Potsdam, Ger-
many, July.
Erhard W. Hinrichs and Julia Trushkina. 2003. N-
gram and PCFG models for morpho-syntactic tag-
ging of German. In Proceedings of The 2nd Work-
shop on Treebanks and Linguistic Theories (TLT
2003), pages 81–92, V¨axj¨o, Sweden, November.
Frederick Jelinek. 1998. Statistical Methods for
Speech Recognition. MIT Press.
Christopher D. Manning and Hinrich Sch¨utze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. MIT Press, Cambridge Mass.
Robbert Prins and Gertjan van Noord. 2004. Rein-
forcing parser preferences through tagging. Traite-
ment Automatique des Langues (TAL), special issue
on Evolutions in Parsing. Accepted for publication,
2004.
H. van Halteren, J. Zavrel, and W. Daelemans. 2001.
Improving accuracy in word class tagging through
the combination of machine learning systems. Com-
putational Linguistics, 27(2):199–230.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.710384">
<title confidence="0.999375">Beyond N in N-gram Tagging</title>
<author confidence="0.992591">Robbert Prins</author>
<affiliation confidence="0.883083">Alfa-Informatica University of Groningen</affiliation>
<address confidence="0.976881">P.O. Box 716, NL-9700 AS Groningen The Netherlands</address>
<email confidence="0.997931">r.p.prins@let.rug.nl</email>
<abstract confidence="0.9988505">The Hidden Markov Model (HMM) for part-of-speech (POS) tagging is typically based on tag trigrams. As such it models local context but not global context, leaving long-distance syntactic relations unrepresented. Using n-gram for &gt; order to incorporate global context is problematic as the tag sequences corresponding to higher order models will become increasingly rare in training data, leading to incorrect estimations of their probabilities. The trigram HMM can be extended with global contextual information, without making the model infeasible, by incorporating the context separately from the POS tags. The new information incorporated in the model is acquired through the use of a wide-coverage parser. The model is trained and tested on Dutch text from two different sources, showing an increase in tagging accuracy compared to tagging using the standard model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Berghmans</author>
</authors>
<title>Wotan, een automatische grammatikale tagger voor het Nederlands.</title>
<date>1994</date>
<tech>Master’s thesis,</tech>
<institution>Dept. of Language and Speech, University of Nijmegen.</institution>
<contexts>
<context position="11712" citStr="Berghmans, 1994" startWordPosition="2003" endWordPosition="2004">rence of the finite verb form. The tagger is used to tag a set of sentences, assigning one tag to each word, first using the standard model and then using the extended model. The results are compared in terms of tagging accuracy. The experiment is conducted twice with different data sets used for both training and testing. 5.1.2 Data The first set consists of a large amount of Dutch newspaper text that was annotated with syntactical tags by the Alpino parser. This is referred to as the “Alpino” data. The second and much smaller set of data is the Eindhoven corpus tagged with the Wotan tagset (Berghmans, 1994). This data set was also used in (van Halteren et al., 2001), therefore the second experiment will allow for a comparison of the results with previous work on tagging Dutch. This data will be referred to as the “Wotan” data. For both sets the contextual information concerning finite verbs is added to the training data by the Alpino parser as described in section 3.3. Due to memory restrictions, the parser was not able to parse 265 of the 36K sentences of Wotan training data. These sentences received no contextual labels and thus not all of the training data used in (van Halteren et al., 2001) </context>
</contexts>
<marker>Berghmans, 1994</marker>
<rawString>J. Berghmans. 1994. Wotan, een automatische grammatikale tagger voor het Nederlands. Master’s thesis, Dept. of Language and Speech, University of Nijmegen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gosse Bouma</author>
<author>Gertjan van Noord</author>
<author>Robert Malouf</author>
</authors>
<title>Wide coverage computational analysis of Dutch.</title>
<date>2001</date>
<booktitle>Computational Linguistics in the Netherlands, CLIN 2000,</booktitle>
<pages>45--59</pages>
<editor>In Walter Daelemans, Khalil Sima’an, Jorn Veenstra, and Jakub Zavrel, editors,</editor>
<location>Amsterdam. Rodopi.</location>
<marker>Bouma, van Noord, Malouf, 2001</marker>
<rawString>Gosse Bouma, Gertjan van Noord, and Robert Malouf. 2001. Wide coverage computational analysis of Dutch. In Walter Daelemans, Khalil Sima’an, Jorn Veenstra, and Jakub Zavrel, editors, Computational Linguistics in the Netherlands, CLIN 2000, pages 45–59, Amsterdam. Rodopi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT – a statistical part-ofspeech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Applied Natural Language Processing Conference,</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="1483" citStr="Brants, 2000" startWordPosition="225" endWordPosition="226">coverage parser. The model is trained and tested on Dutch text from two different sources, showing an increase in tagging accuracy compared to tagging using the standard model. 1 Introduction The Hidden Markov Model (HMM) used for partof-speech (POS) tagging is usually a second-order model, using tag trigrams, implementing the idea that a limited number of preceding tags provide a considerable amount of information on the identity of the current tag. This approach leads to good results. For example, the TnT trigram HMM tagger achieves state-of-the-art tagging accuracies on English and German (Brants, 2000). In general, however, as the model does not consider global context, mistakes are made that concern long-distance syntactic relations. 2 A restriction of HMM tagging The simplifying assumption, which is the basis for HMM tagging, that the context of a given tag can be fully represented by just the previous two tags, leads to tagging errors where syntactic features that fall outside of this range, and that are needed for determining the identity of the tag at hand, are ignored. One such error in tagging Dutch is related to finiteness of verbs. This is discussed in the next paragraph and will b</context>
<context position="14910" citStr="Brants, 2000" startWordPosition="2547" endWordPosition="2548">am accuracy 93.34% 93.62% errors 3946 3779 error reduction 167 = 4.23% pl/inf errors 241 (6.11%) 82 (2.17%) pl/inf error red. 159 = 65.98% Table 1: Tagging results on Alpino data freq assigned correct 159 verb(inf) verb(pl) 82 verb(pl) verb(inf) 68 proper name(both) 1-proper name(both) 57 proper name(both) noun(de,sg) 53 verb(psp) adjective(no e,adv) 45 proper name(both) 2-proper name(both) Table 2: Most frequent tagging mistakes on Alpino data, using standard model lar to the 92.06% reported by Van Halteren, Zavrel and Daelemans in (van Halteren et al., 2001) who used the TnT trigram tagger (Brants, 2000) on the same training and testing data. Using the extended model, accuracy is 92.26% (5564 mistakes). This amounts to an overall error reduction of 2.64%. Mistakes where the plural verb is mixed up with the infinitive sum up to 316 instances (5.53% of all mistakes) when using the standard model, as opposed to 199 cases (3.58%) using the extended model, an error reduction of 37.03%. 5.3 Discussion of results Extending the standard trigram tagging model with syntactical information aimed at resolving the most frequent type of tagging error led to a considerable reduction of this type of error in</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT – a statistical part-ofspeech tagger. In Proceedings of the 6th Applied Natural Language Processing Conference, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>G Carroll</author>
<author>J Adcock</author>
<author>A Cassandra</author>
<author>Y Gotoh</author>
<author>J Katz</author>
<author>M Littman</author>
<author>J McCann</author>
</authors>
<title>Taggers for parsers.</title>
<date>1996</date>
<journal>Artificial Intelligence,</journal>
<pages>85--1</pages>
<contexts>
<context position="9473" citStr="Charniak et al., 1996" startWordPosition="1608" endWordPosition="1611"> the value assigned to the previous position is continued. From the text annotated with POS tags and context labels the n-gram probabilities and lexical probabilities needed by the model are estimated based on the frequencies of the corresponding sequences. 4 The tagger 4.1 Tagging method The trigram HMM tagger used in the experiments of section 5 computes the a posteriori probability for every tag. This value is composed of the forward and backward probability of the tag at hand as defined in the forward-backward algorithm for HMM-training. This idea is also described in (Jelinek, 1998) and (Charniak et al., 1996). The trigram data is combined with bigram and unigram data through linear interpolation to reduce the problem of sparse data. 4.1.1 Smoothing Applying the method known as linear interpolation, probabilities of unigrams, bigrams and trigrams are combined in a weighted sum using weights 1, 2 and 3 respectively. The weights are computed for every individual case using the notion of n-gram diversity (Collins, 1999). The diversity of an n-gram is the number of different tags that appear in the position following this n-gram in the training data. The weight 3 assigned to the trigram t1t2t3 is c</context>
</contexts>
<marker>Charniak, Carroll, Adcock, Cassandra, Gotoh, Katz, Littman, McCann, 1996</marker>
<rawString>E. Charniak, G. Carroll, J. Adcock, A. Cassandra, Y. Gotoh, J. Katz, M. Littman, and J. McCann. 1996. Taggers for parsers. Artificial Intelligence, 85(1-2):45–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="9891" citStr="Collins, 1999" startWordPosition="1676" endWordPosition="1677">of the forward and backward probability of the tag at hand as defined in the forward-backward algorithm for HMM-training. This idea is also described in (Jelinek, 1998) and (Charniak et al., 1996). The trigram data is combined with bigram and unigram data through linear interpolation to reduce the problem of sparse data. 4.1.1 Smoothing Applying the method known as linear interpolation, probabilities of unigrams, bigrams and trigrams are combined in a weighted sum using weights 1, 2 and 3 respectively. The weights are computed for every individual case using the notion of n-gram diversity (Collins, 1999). The diversity of an n-gram is the number of different tags that appear in the position following this n-gram in the training data. The weight 3 assigned to the trigram t1t2t3 is computed on the basis of the diversity and frequency of the prefix bigram t1t2, using the following equation, where c regulates the importance of diversity (c = 6 was used in the experiments described below), and C(x) and D(x) are respectively the count and diversity of x: The bigram weight A2 is computed as a fraction of 1 − A3 using the bigram version of the above equation. The remaining weight 1 − A3 − A2 is used</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Daciuk</author>
</authors>
<title>Treatment of unknown words.</title>
<date>1999</date>
<booktitle>In Proceedings of the Workshop on Implementing Automata WIA’99, pages IX–1 – IX–9,</booktitle>
<location>Potsdam, Germany,</location>
<contexts>
<context position="10874" citStr="Daciuk, 1999" startWordPosition="1858" endWordPosition="1859">s described below), and C(x) and D(x) are respectively the count and diversity of x: The bigram weight A2 is computed as a fraction of 1 − A3 using the bigram version of the above equation. The remaining weight 1 − A3 − A2 is used as the unigram weight A1. 4.1.2 Unknown words The tagger uses a lexicon that has been created from the training data to assign an initial set of possible tags to every word. Words that were not seen during training are not in the lexicon, so that another method has to be used to assign initial tags to these words. A technique described and implemented by Jan Daciuk (Daciuk, 1999) was used to create automata for associating words with tags based on suffixes of those words. 5 Tagging experiment 5.1 Experiment setup 5.1.1 Method An extended model was created featuring context information on the occurrence of the finite verb form. The tagger is used to tag a set of sentences, assigning one tag to each word, first using the standard model and then using the extended model. The results are compared in terms of tagging accuracy. The experiment is conducted twice with different data sets used for both training and testing. 5.1.2 Data The first set consists of a large amount o</context>
</contexts>
<marker>Daciuk, 1999</marker>
<rawString>Jan Daciuk. 1999. Treatment of unknown words. In Proceedings of the Workshop on Implementing Automata WIA’99, pages IX–1 – IX–9, Potsdam, Germany, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erhard W Hinrichs</author>
<author>Julia Trushkina</author>
</authors>
<title>Ngram and PCFG models for morpho-syntactic tagging of German.</title>
<date>2003</date>
<booktitle>In Proceedings of The 2nd Workshop on Treebanks and Linguistic Theories (TLT</booktitle>
<pages>81--92</pages>
<location>V¨axj¨o, Sweden,</location>
<contexts>
<context position="19256" citStr="Hinrichs and Trushkina, 2003" startWordPosition="3265" endWordPosition="3268">al bracketing of the input, based on the recognition of larger syntactic units or ’chunks’. Typically chunk tags will be assigned on the basis of words and their POS tags. An alternative approach is to use an extended model that assigns chunk tags and POS tags simultaneously, as was done for finite verb occurrence and POS tags in the current work. In this way, relations between POS tags and chunk tags can be modeled in both directions. Another possible application is tagging of German. German features different cases, which can lead to problems for statistical taggers. This is illustrated in (Hinrichs and Trushkina, 2003) who point out that the TnT tagger wrongly assigns nominative case instead of accusative in a given sentence, resulting in the unlikely combination of two nominatives. The preference for just one assignment of the nominative case might be learned by including case information in the model. Acknowledgements. This research was carried out as part of the PIONIER Project Algorithms for Linguistic Processing, funded by NWO (Dutch Organization for Scientific Research) and the University of Groningen. I would like to thank Hans van Halteren for supplying the Eindhoven corpus data set as used in (van </context>
</contexts>
<marker>Hinrichs, Trushkina, 2003</marker>
<rawString>Erhard W. Hinrichs and Julia Trushkina. 2003. Ngram and PCFG models for morpho-syntactic tagging of German. In Proceedings of The 2nd Workshop on Treebanks and Linguistic Theories (TLT 2003), pages 81–92, V¨axj¨o, Sweden, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="9445" citStr="Jelinek, 1998" startWordPosition="1604" endWordPosition="1606"> In all other cases, the value assigned to the previous position is continued. From the text annotated with POS tags and context labels the n-gram probabilities and lexical probabilities needed by the model are estimated based on the frequencies of the corresponding sequences. 4 The tagger 4.1 Tagging method The trigram HMM tagger used in the experiments of section 5 computes the a posteriori probability for every tag. This value is composed of the forward and backward probability of the tag at hand as defined in the forward-backward algorithm for HMM-training. This idea is also described in (Jelinek, 1998) and (Charniak et al., 1996). The trigram data is combined with bigram and unigram data through linear interpolation to reduce the problem of sparse data. 4.1.1 Smoothing Applying the method known as linear interpolation, probabilities of unigrams, bigrams and trigrams are combined in a weighted sum using weights 1, 2 and 3 respectively. The weights are computed for every individual case using the notion of n-gram diversity (Collins, 1999). The diversity of an n-gram is the number of different tags that appear in the position following this n-gram in the training data. The weight 3 assigne</context>
</contexts>
<marker>Jelinek, 1998</marker>
<rawString>Frederick Jelinek. 1998. Statistical Methods for Speech Recognition. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge Mass.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robbert Prins</author>
<author>Gertjan van Noord</author>
</authors>
<title>Reinforcing parser preferences through tagging. Traitement Automatique des Langues (TAL), special issue on Evolutions in Parsing. Accepted for publication,</title>
<date>2004</date>
<marker>Prins, van Noord, 2004</marker>
<rawString>Robbert Prins and Gertjan van Noord. 2004. Reinforcing parser preferences through tagging. Traitement Automatique des Langues (TAL), special issue on Evolutions in Parsing. Accepted for publication, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H van Halteren</author>
<author>J Zavrel</author>
<author>W Daelemans</author>
</authors>
<title>Improving accuracy in word class tagging through the combination of machine learning systems.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<marker>van Halteren, Zavrel, Daelemans, 2001</marker>
<rawString>H. van Halteren, J. Zavrel, and W. Daelemans. 2001. Improving accuracy in word class tagging through the combination of machine learning systems. Computational Linguistics, 27(2):199–230.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>