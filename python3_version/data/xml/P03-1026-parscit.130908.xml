<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000042">
<title confidence="0.999455">
A Tabulation-Based Parsing Method that Reduces Copying
</title>
<author confidence="0.995095">
Gerald Penn and Cosmin Munteanu
</author>
<affiliation confidence="0.844082666666667">
Department of Computer Science
University of Toronto
Toronto M5S 3G4, Canada
</affiliation>
<email confidence="0.996888">
gpenn,mcosmin @cs.toronto.edu
</email>
<sectionHeader confidence="0.995589" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999765">
This paper presents a new bottom-up chart
parsing algorithm for Prolog along with
a compilation procedure that reduces the
amount of copying at run-time to a con-
stant number (2) per edge. It has ap-
plications to unification-based grammars
with very large partially ordered cate-
gories, in which copying is expensive,
and can facilitate the use of more so-
phisticated indexing strategies for retriev-
ing such categories that may otherwise be
overwhelmed by the cost of such copy-
ing. It also provides a new perspective
on “quick-checking” and related heuris-
tics, which seems to confirm that forcing
an early failure (as opposed to seeking
an early guarantee of success) is in fact
the best approach to use. A preliminary
empirical evaluation of its performance is
also provided.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999982658536586">
This paper addresses the cost of copying edges
in memoization-based, all-paths parsers for phrase-
structure grammars. While there have been great ad-
vances in probabilistic parsing methods in the last
five years, which find one or a few most probable
parses for a string relative to some grammar, all-
paths parsing is still widely used in grammar devel-
opment, and as a means of verifying the accuracy of
syntactically more precise grammars, given a corpus
or test suite.
Most if not all efficient all-paths phrase-structure-
based parsers for natural language are chart-based
because of the inherent ambiguity that exists in
large-scale natural language grammars. Within
WAM-based Prolog, memoization can be a fairly
costly operation because, in addition to the cost of
copying an edge into the memoization table, there
is the additional cost of copying an edge out of the
table onto the heap in order to be used as a premise
in further deductions (phrase structure rule applica-
tions). All textbook bottom-up Prolog parsers copy
edges out: once for every attempt to match an edge
to a daughter category, based on a matching end-
point node, which is usually the first-argument on
which the memoization predicate is indexed. De-
pending on the grammar and the empirical distri-
bution of matching mother/lexical and daughter de-
scriptions, this number could approach copies
for an edge added early to the chart, where is the
length of the input to be parsed.
For classical context-free grammars, the category
information that must be copied is normally quite
small in size. For feature-structure-based grammars
and other highly lexicalized grammars with large
categories, however, which have become consider-
ably more popular since the advent of the standard
parsing algorithms, it becomes quite significant. The
ALE system (Carpenter and Penn, 1996) attempts
to reduce this by using an algorithm due to Carpen-
ter that traverses the string breadth-first, right-to-left,
but matches rule daughters rule depth-first, left-to-
right in a failure-driven loop, which eliminates the
need for active edges and keeps the sizes of the heap
and call stack small. It still copies a candidate edge
every time it tries to match it to a daughter descrip-
tion, however, which can approach because
of its lack of active edges. The OVIS system (van
Noord, 1997) employs selective memoization, which
tabulates only maximal projections in a head-corner
parser — partial projections of a head are still re-
computed.
A chart parser with zero copying overhead has
yet to be discovered, of course. This paper presents
one that reduces this worst case to two copies per
non-empty edge, regardless of the length of the in-
put string or when the edge was added to the chart.
Since textbook chart parsers require at least two
copies per edge as well (assertion and potentially
matching the next lexical edge to the left/right), this
algorithm always achieves the best-case number of
copies attainable by them on non-empty edges. It is
thus of some theoretical interest in that it proves that
at least a constant bound is attainable within a Prolog
setting. It does so by invoking a new kind of gram-
mar transformation, called EFD-closure, which en-
sures that a grammar need not match an empty cat-
egory to the leftmost daughter of any rule. This
transformation is similar to many of the myriad of
earlier transformations proposed for exploring the
decidability of recognition under various parsing
control strategies, but the property it establishes is
more conservative than brute-force epsilon elimi-
nation for unification-based grammars (Dymetman,
1994). It also still treats empty categories distinctly
from non-empty ones, unlike the linking tables pro-
posed for treating leftmost daughters in left-corner
parsing (Pereira and Shieber, 1987). Its motivation,
the practical consideration of copying overhead, is
also rather different, of course.
The algorithm will be presented as an improved
version of ALE’s parser, although other standard
bottom-up parsers can be similarly adapted.
</bodyText>
<sectionHeader confidence="0.968497" genericHeader="introduction">
2 Why Prolog?
</sectionHeader>
<bodyText confidence="0.997787535714286">
Apology! This paper is not an attempt to show that
a Prolog-based parser could be as fast as a phrase-
structure parser implemented in an imperative pro-
gramming language such as C. Indeed, if the cat-
egories of a grammar are discretely ordered, chart
edges can be used for further parsing in situ, i.e.,
with no copying out of the table, in an impera-
tive programming language. Nevertheless, when the
categories are partially ordered, as in unification-
based grammars, there are certain breadth-first pars-
ing control strategies that require even imperatively
implemented parsers to copy edges out of their ta-
bles.
What is more important is the tradeoff at stake
between efficiency and expressiveness. By improv-
ing the performance of Prolog-based parsing, the
computational cost of its extra available expres-
sive devices is effectively reduced. The alterna-
tive, simple phrase-structure parsing, or extended
phrase-structure-based parsing with categories such
as typed feature structures, is extremely cumber-
some for large-scale grammar design. Even in
the handful of instances in which it does seem to
have been successful, which includes the recent
HPSG English Resource Grammar and a handful of
Lexical-Functional Grammars, the results are by no
means graceful, not at all modular, and arguably not
reusable by anyone except their designers.
The particular interest in Prolog’s expressiveness
arises, of course, from the interest in generalized
context-free parsing beginning with definite clause
grammars (Pereira and Shieber, 1987), as an in-
stance of a logic programming control strategy. The
connection between logic programming and parsing
is well-known and has also been a very fruitful one
for parsing, particularly with respect to the appli-
cation of logic programming transformations (Sta-
bler, 1993) and constraint logic programming tech-
niques to more recent constraint-based grammati-
cal theories. Relational predicates also make gram-
mars more modular and readable than pure phrase-
structure-based grammars.
Commercial Prolog implementations are quite
difficult to beat with imperative implementations
when it is general logic programming that is re-
quired. This is no less true with respect to more re-
cent data structures in lexicalized grammatical theo-
ries. A recent comparison (Penn, 2000) of a version
between ALE (which is written in Prolog) that re-
duces typed feature structures to Prolog term encod-
ings, and LiLFeS (Makino et al., 1998), the fastest
imperative re-implementation of an ALE-like lan-
guage, showed that ALE was slightly over 10 times
faster on large-scale parses with its HPSG reference
grammar than LiLFeS was with a slightly more effi-
cient version of that grammar.
</bodyText>
<sectionHeader confidence="0.96991" genericHeader="method">
3 Empirical Efficiency
</sectionHeader>
<bodyText confidence="0.9970145">
Whether this algorithm will outperform standard
Prolog parsers is also largely empirical, because:
</bodyText>
<listItem confidence="0.983312411764706">
1. one of the two copies is kept on the heap itself
and not released until the end of the parse. For
large parses over large data structures, that can
increase the size of the heap significantly, and
will result in a greater number of cache misses
and page swaps.
2. the new algorithm also requires an off-line par-
tial evaluation of the grammar rules that in-
creases the number of rules that must be it-
erated through at run-time during depth-first
closure. This can result in redundant opera-
tions being performed among rules and their
partially evaluated instances to match daughter
categories, unless those rules and their partial
evaluations are folded together with local dis-
junctions to share as much compiled code as
possible.
</listItem>
<bodyText confidence="0.9999745">
A preliminary empirical evaluation is presented in
Section 8.
Oepen and Carroll (2000), by far the most com-
prehensive attempt to profile and optimize the per-
formance of feature-structure-based grammars, also
found copying to be a significant issue in their im-
perative implementations of several HPSG parsers
— to the extent that it even warranted recomput-
ing unifications in places, and modifying the man-
ner in which active edges are used in their fastest
attempt (called hyper-active parsing). The results of
the present study can only cautiously be compared to
theirs so far, because of our lack of access to the suc-
cessive stages of their implementations and the lack
of a common grammar ported to all of the systems
involved. Some parallels can be drawn, however,
particularly with respect to the utility of indexing
and the maintenance of active edges, which suggest
that the algorithm presented below makes Prolog be-
have in a more “C-like” manner on parsing tasks.
</bodyText>
<sectionHeader confidence="0.988189" genericHeader="method">
4 Theoretical Benefits
</sectionHeader>
<bodyText confidence="0.730727">
The principal benefits of this algorithm are that:
</bodyText>
<listItem confidence="0.970464555555556">
1. it reduces copying, as mentioned above.
2. it does not suffer from a problem that text-
book algorithms suffer from when running un-
der non-ISO-compatible Prologs (which is to
say most of them). On such Prologs, asserted
empty category edges that can match leftmost
daughter descriptions of rules are not able to
combine with the outputs of those rules.
3. keeping a copy of the chart on the heap allows
</listItem>
<bodyText confidence="0.99967756">
for more sophisticated indexing strategies to
apply to memoized categories that would oth-
erwise be overwhelmed by the cost of copying
an edge before matching it against an index.
Indexing is also briefly considered in Section 8. In-
dexing is not the same thing as filtering (Torisawa
and Tsuji, 1995), which extracts an approximation
grammar to parse with first, in order to increase the
likelihood of early unification failure. If the filter
parse succeeds, the system then proceeds to perform
the entire unification operation, as if the approxima-
tion had never been applied. Malouf et al. (2000)
cite an improvement of 35–45% using a “quick-
check” algorithm that they appear to believe finds
the optimal selection of feature paths for quick-
checking. It is in fact only a greedy approxima-
tion — the optimization problem is exponential in
the number of feature paths used for the check.
Penn (1999) cites an improvement of 15-40% sim-
ply by re-ordering the sister features of only two
types in the signature of the ALE HPSG grammar
during normal unification.
True indexing re-orders required operations with-
out repeating them. Penn and Popescu (1997) build
an automaton-based index for surface realization
with large lexica, and suggest an extension to statis-
tically trained decision trees. Ninomiya et al. (2002)
take a more computationally brute-force approach to
index very large databases of feature structures for
some kind of information retrieval application. Nei-
ther of these is suitable for indexing chart edges dur-
ing parsing, because the edges are discarded after
every sentence, before the expense of building the
index can be satisfactorily amortized. There is a fair
amount of relevant work in the database and pro-
gramming language communities, but many of the
results are negative (Graf, 1996) — very little time
can be spent on constructing the index.
A moment’s thought reveals that the very notion
of an active edge, tabulating the well-formed pre-
fixes of rule right-hand-sides, presumes that copy-
ing is not a significant enough issue to merit the
overhead of more specialized indexing. While the
present paper proceeds from Carpenter’s algorithm,
in which no active edges are used, it will become
clear from our evaluation that active edges or their
equivalent within a more sophisticated indexing
strategy are an issue that should be re-investigated
now that the cost of copying can provably be re-
duced in Prolog.
</bodyText>
<sectionHeader confidence="0.985371" genericHeader="method">
5 The Algorithm
</sectionHeader>
<bodyText confidence="0.944170222222222">
In this section, it will be assumed that the phrase-
structure grammar to be parsed with obeys the fol-
lowing property:
Definition 1 An (extended) context-free grammar,
, is empty-first-daughter-closed (EFD-closed) iff,
for every production rule,
and there are no empty productions (empty
categories) derivable from non-terminal .
The next section will show how to transform any
phrase-structure grammar into an EFD-closed gram-
mar.
This algorithm, like Carpenter’s algorithm, pro-
ceeds breadth-first, right-to-left through the string,
at each step applying the grammar rules depth-
first, matching daughter categories left-to-right.
The first step is then to reverse the input
string, and compute its length (performed by
reverse count/5) and initialize the chart:
</bodyText>
<equation confidence="0.7843855">
rec(Ws,FS) :-
retractall(edge(_,_,_)),
</equation>
<bodyText confidence="0.96960672">
reverse_count(Ws,[],WsRev,0,Length),
CLength is Length - 1,
functor(Chart,chart,CLength),
build(WsRev,Length,Chart),
edge(0,Length,FS).
Two copies of the chart are used in this
presentation. One is represented by a term
chart(E1,...,EL), where the th argument
holds the list of edges whose left node is . Edges at
the beginning of the chart (left node 0) do not need
to be stored in this copy, nor do edges beginning at
the end of the chart (specifically, empty categories
with left node and right node Length). This will
be called the term copy of the chart. The other copy
is kept in a dynamic predicate, edge/3, as a text-
book Prolog chart parser would. This will be called
the asserted copy of the chart.
Neither copy of the chart stores empty categories.
These are assumed to be available in a separate pred-
icate, empty cat/1. Since the grammar is EFD-
closed, no grammar rule can produce a new empty
category. Lexical items are assumed to be available
in the predicate lex/2.
The predicate, build/3, actually builds the
chart:
</bodyText>
<equation confidence="0.985348909090909">
build([W|Ws],R,Chart):-
RMinus1 is R - 1,
(lex(W,FS),
add_edge(RMinus1,R,FS,Chart)
; ( RMinus1 =:= 0 -&gt; true
; rebuild_edges(RMinus1,Edges),
arg(RMinus1,Chart,Edges),
build(Ws,RMinus1,Chart)
)
).
build([],_,_).
</equation>
<bodyText confidence="0.982078666666667">
The precondition upon each call to
build(Ws,R,Chart) is that Chart con-
tains the complete term copy of the non-loop edges
of the parsing chart from node R to the end, while
Ws contains the (reversed) input string from node
R to the beginning. Each pass through the first
clause of build/3 then decrements Right, and
seeds the chart with every category for the lexical
item that spans from R-1 to R. The predicate,
add edge/4 actually adds the lexical edge to the
asserted copy of the chart, and then closes the chart
depth-first under rule applications in a failure-driven
loop. When it has finished, if Ws is not empty
(RMinus1 is not 0), then build/3 retracts all of
the new edges from the asserted copy of the chart
(with rebuild edges/2, described below) and
adds them to the R-1st argument of the term copy
before continuing to the next word.
add edge/4 matches non-leftmost daughter de-
scriptions from either the term copy of the chart,
thus eliminating the need for additional copying of
non-empty edges, or from empty cat/1. When-
ever it adds an edge, however, it adds it to the as-
serted copy of the chart. This is necessary because
add edge/4 works in a failure-driven loop, and
any edges added to the term copy of the chart would
be removed during backtracking:
add_edge(Left,Right,FS,Chart):-
assert(edge(Left,Right,FS)),
rule(FS,Left,Right,Chart).
</bodyText>
<equation confidence="0.978342769230769">
rule(FS,L,R,Chart) :-
(Mother ===&gt; [FS|DtrsRest]), % PS rule
match_rest(DtrsRest,R,Chart,Mother,L).
match_rest([],R,Chart,Mother,L) :-
in ,
% all Dtrs matched
add_edge(L,R,Mother,Chart).
match_rest([Dtr|Dtrs],R,Chart,Mother,L) :-
arg(R,Chart,Edges),
member(edge(Dtr,NewR),Edges),
match_rest(Dtrs,NewR,Chart,Mother,L)
; empty_cat(Dtr),
match_rest(Dtrs,R,Chart,Mother,L).
</equation>
<bodyText confidence="0.952872333333333">
Note that we never need to be concerned with up-
dating the term copy of the chart during the opera-
tion of add edge/4 because EFD-closure guaran-
tees that all non-leftmost daughters must have left
nodes strictly greater than the Left passed as the
first argument to add edge/4.
Moving new edges from the asserted copy to
the term copy is straightforwardly achieved by re-
build edges/2:
</bodyText>
<equation confidence="0.9834086">
rebuild_edges(Left,Edges) :-
retract(edge(Left,R,FS))
-&gt; Edges = [edge(FS,R)|EdgesRest],
rebuild_edges(Left,EdgesRest)
; Edges = [].
</equation>
<bodyText confidence="0.999879090909091">
The two copies required by this algorithm are
thus: 1) copying a new edge to the asserted copy
of the chart by add edge/4, and 2) copying new
edges from the asserted copy of the chart to the term
copy of the chart by rebuild edges/2. The as-
serted copy is only being used to protect the term
copy from being unwound by backtracking.
Asymptotically, this parsing algorithm has the
same cubic complexity as standard chart parsers —
only its memory consumption and copying behavior
are different.
</bodyText>
<sectionHeader confidence="0.969093" genericHeader="method">
6 EFD-closure
</sectionHeader>
<bodyText confidence="0.9971266">
To convert an (extended) context-free grammar to
one in which EFD-closure holds, we must partially
evaluate those rules for which empty categories
could be the first daughter over the available empty
categories. If all daughters can be empty categories
in some rule, then that rule may create new empty
categories, over which rules must be partially evalu-
ated again, and so on. The closure algorithm is pre-
sented in Figure 1 in pseudo-code and assumes the
existence of six auxiliary lists:
</bodyText>
<equation confidence="0.46988925">
Es — a list of empty categories over which par-
tial evaluation is to occur,
Rs — a list of rules to be used in partial evalu-
ation,
</equation>
<bodyText confidence="0.986596928571429">
NEs — new empty categories, created by
partial evaluation (when all daughters have
matched empty categories),
NRs — new rules, created by partial evaluation
(consisting of a rule to the leftmost daughter of
which an empty category has applied, with only
its non-leftmost daughters remaining),
EAs — an accumulator of empty categories al-
ready partially evaluated once on Rs, and
RAs — an accumulator of rules already used in
partial evaluation once on Es.
Initialize Es to empty cats of grammar;
initialize Rs to rules of input grammar;
initialize the other four lists to [];
</bodyText>
<equation confidence="0.916164">
loop:
while Es =/= [] do
for each E in Es do
for each R in Rs do
unify E with the leftmost unmatched
</equation>
<bodyText confidence="0.896836714285714">
category description of R;
if it does not match, continue;
if the leftmost category was rightmost
(unary rule),
then add the new empty category to NEs
otherwise, add the new rule (with leftmost
category marked as matched) to NRs;
</bodyText>
<equation confidence="0.868064916666667">
od
od;
EAs := append(Es,EAs); Rs := append(Rs,RAs);
RAs := []; Es := NEs; NEs := [];
od;
if NRs = [],
then end: EAs are the closed empty cats,
Rs are the closed rules
else
Es := EAs; EAs:= []; RAs := Rs;
Rs := NRs; NRs := []
go to loop
</equation>
<figureCaption confidence="0.99995">
Figure 1: The off-line EFD-closure algorithm.
</figureCaption>
<bodyText confidence="0.999752333333334">
Each pass through the while-loop attempts to
match the empty categories in Es against the left-
most daughter description of every rule in Rs. If
new empty categories are created in the process
(because some rule in Rs is unary and its daugh-
ter matches), they are also attempted — EAs holds
the others until they are done. Every time a rule’s
leftmost daughter matches an empty category, this
effectively creates a new rule consisting only of
the non-leftmost daughters of the old rule. In a
unification-based setting, these non-leftmost daugh-
ters could also have some of their variables instan-
tiated to information from the matching empty cate-
gory.
If the while-loop terminates (see the next section),
then the rules of Rs are stored in an accumulator,
RAs, until the new rules, NRs, have had a chance
to match their leftmost daughters against all of the
empty categories that Rs has. Partial evaluation with
NRs may create new empty categories that Rs have
never seen and therefore must be applied to. This is
taken care of within the while-loop when RAs are
added back to Rs for second and subsequent passes
through the loop.
</bodyText>
<sectionHeader confidence="0.982615" genericHeader="method">
7 Termination Properties
</sectionHeader>
<bodyText confidence="0.999983027777778">
The parsing algorithm itself always terminates be-
cause the leftmost daughter always consumes input.
Off-line EFD-closure may not terminate when in-
finitely many new empty categories can be produced
by the production rules.
We say that an extended context-free grammar, by
which classical CFGs as well as unification-based
phrase-structure grammars are implied, is -offline-
parseable ( -OP) iff the empty string is not infinitely
ambiguous in the grammar. Every -OP grammar
can be converted to a weakly equivalent grammar
which has the EFD-closure property. The proof of
this statement, which establishes the correctness of
the algorithm, is omitted for brevity.
EFD-closure bears some resemblance in its inten-
tions to Greibach Normal Form, but: (1) it is far
more conservative in the number of extra rules it
must create; (2) it is linked directly to the deriv-
able empty categories of the grammar, whereas GNF
conversion proceeds from an already -eliminated
grammar (EFD-closure of any -free grammar, in
fact, is the grammar itself); (3) GNF is rather more
difficult to define in the case of unification-based
grammars than with classical CFGs, and in the one
generalization we are aware of (Dymetman, 1992),
EFD-closure is actually not guaranteed by it; and
Dymetman’s generalization only works for classi-
cally offline-parseable grammars.
In the case of non- -OP grammars, a standard
bottom-up parser without EFD-closure would not
terminate at run-time either. Our new algorithm is
thus neither better nor worse than a textbook bottom-
up parser with respect to termination. A remain-
ing topic for consideration is the adaptation of this
method to strategies with better termination proper-
ties than the pure bottom-up strategy.
</bodyText>
<sectionHeader confidence="0.955353" genericHeader="method">
8 Empirical Evaluation
</sectionHeader>
<bodyText confidence="0.9995012">
The details of how to integrate an indexing strategy
for unification-based grammars into the EFD-based
parsing algorithm are too numerous to present here,
but a few empirical observations can be made. First,
EFD-based parsing is faster than Carpenter’s algo-
rithm even with atomic, CFG-like categories, where
the cost of copying is at a minimum, even with no in-
dexing. We defined several sizes of CFG by extract-
ing local trees from successively increasing portions
of the Penn Treebank II, as shown in Table 1, and
</bodyText>
<table confidence="0.99785425">
WSJ Number of Lexicon Number of
directories WSJ files size Rules
00 4 131 77
00 5 188 124
00 6 274 168
00 8 456 282
00 10 756 473
00 15 1167 736
00 20 1880 1151
00 25 2129 1263
00 30 2335 1369
00 35 2627 1589
00 40 3781 2170
00 50 5645 3196
00–01 100 8948 5246
00–01 129 11242 6853
00–02 200 13164 7984
00–02 250 14730 9008
00–03 300 17555 10834
00–03 350 18861 11750
00–04 400 20359 12696
00–05 481 20037 13159
00–07 700 27404 17682
00–09 901 32422 20999
</table>
<tableCaption confidence="0.998996">
Table 1: The grammars extracted from the Wall
</tableCaption>
<bodyText confidence="0.954979133333333">
Street Journal directories of the PTB II.
then computed the average time to parse a corpus of
sentences (5 times each) drawn from the initial sec-
tion. All of the parsers were written in SICStus Pro-
log. These average times are shown in Figure 2 as a
function of the number of rules. Storing active edges
is always the worst option, followed by Carpenter’s
algorithm, followed by the EFD-based algorithm. In
this atomic case, indexing simply takes on the form
of a hash by phrase structure category. This can be
implemented on top of EFD because the overhead of
copying has been reduced. This fourth option is the
fastest by a factor of approximately 2.18 on average
over EFD without indexing.
One may also refer to Table 2, in which the num-
</bodyText>
<figure confidence="0.978878">
Average parsing times
0 5000 10000 15000 20000 25000
Number of rules
</figure>
<figureCaption confidence="0.996864">
Figure 2: Parsing times for simple CFGs.
</figureCaption>
<table confidence="0.999737235294118">
Number Successful Failed Success
of rules unifications unifications rate (%)
124 104 1,766 5.56
473 968 51,216 1.85
736 2,904 189,528 1.51
1369 7,152 714,202 0.99
3196 25,416 3,574,138 0.71
5246 78,414 14,644,615 0.53
6853 133,205 30,743,123 0.43
7984 158,352 40,479,293 0.39
9008 195,382 56,998,866 0.34
10834 357,319 119,808,018 0.30
11750 441,332 151,226,016 0.29
12696 479,612 171,137,168 0.28
14193 655,403 250,918,711 0.26
17682 911,480 387,453,422 0.23
20999 1,863,523 847,204,674 0.21
</table>
<bodyText confidence="0.998863142857143">
the CFGs extracted from the Penn Treebank. EFD-
index is also slower than EFD without indexing be-
cause of our poor choice of index for that gram-
mar. With subsumption testing (Figure 4), the ac-
tive edge algorithm and Carpenter’s algorithm are
at an even greater disadvantage because edges must
be copied to be compared for subsumption. On a
pre-release version of MERGE (Figure 5),1 a modi-
fication of the English Resource Grammar that uses
more macros and fewer types, the sheer size of the
categories combined with a scarcity of edges seems
to cost EFD due to the loss of locality of reference,
although that loss is more than compensated for by
indexing.
</bodyText>
<figure confidence="0.97992084">
Parsing times over Alvey grammar - no subsumption
0 20 40 60 80 100 120 140 160 180 200
Test cases
Time [log(sec)]
100000
10000
0.001
1000
0.01
100
0.1
10
1
Active
Carpenter
EFD
EFD-index
Time [log(msec)]
10000
1000
100
Active
Carp
EFD-Index
EFD
</figure>
<figureCaption confidence="0.999359">
Figure 3: Alvey grammar with no subsumption.
</figureCaption>
<tableCaption confidence="0.88777">
Table 2: Successful unification rate for the (non-
indexing) EFD parser.
</tableCaption>
<subsectionHeader confidence="0.644775">
Parsing times over Alvey grammar - with subsumption
</subsectionHeader>
<bodyText confidence="0.999917875">
ber of successful and failed unifications (matches)
was counted over the test suite for each rule set.
Asymptotically, the success rate does not decrease
by very much from rule set to rule set. There are so
many more failures early on, however, that the sheer
quantity of failed unifications makes it more impor-
tant to dispense with these quickly.
Of the grammars to which we have access that use
larger categories, this ranking of parsing algorithms
is generally preserved, although we have found no
correlation between category size and the factor of
improvement. John Carroll’s Prolog port of the
Alvey grammar of English (Figure 3), for example,
is EFD-closed, but the improvement of EFD over
Carpenter’s algorithm is much smaller, presumably
because there are so few edges when compared to
</bodyText>
<figure confidence="0.9892065">
0 20 40 60 80 100 120 140 160 180 200
Test cases
</figure>
<figureCaption confidence="0.6205435">
Figure 4: Alvey grammar with subsumption testing.
&apos;We are indebted to Kordula DeKuthy and Detmar Meurers
of Ohio State University, for making this pre-release version
available to us.
</figureCaption>
<figure confidence="0.999033272727273">
Time [log(msec)]
10000
1000
100
Active
Carp
EFD
EFD-index
Parsing times over Merge grammar
0 5 10 15 20
Test cases
</figure>
<figureCaption confidence="0.999883">
Figure 5: MERGE on the CSLI test-set.
</figureCaption>
<sectionHeader confidence="0.994065" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.99963556">
This paper has presented a bottom-up parsing algo-
rithm for Prolog that reduces the copying of edges
from either linear or quadratic to a constant num-
ber of two per non-empty edge. Its termination
properties and asymptotic complexity are the same
as a standard bottom-up chart parser, but in prac-
tice it performs better. Further optimizations can be
incorporated by compiling rules in a way that lo-
calizes the disjunctions that are implicit in the cre-
ation of extra rules in the compile-time EFD-closure
step, and by integrating automaton- or decision-tree-
based indexing with this algorithm. With copying
now being unnecessary for matching a daughter cat-
egory description, these two areas should result in
a substantial improvement to parse times for highly
lexicalized grammars. The adaptation of this algo-
rithm to active edges, other control strategies, and to
scheduling concerns such as finding the first parse as
quickly as possible remain interesting areas of fur-
ther extension.
Apart from this empirical issue, this algorithm is
of theoretical interest in that it proves that a con-
stant number of edge copies can be attained by an
all-paths parser, even in the presence of partially or-
dered categories.
</bodyText>
<sectionHeader confidence="0.999101" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999917170212766">
B. Carpenter and G. Penn. 1996. Compiling typed
attribute-value logic grammars. In H. Bunt and
M. Tomita, editors, Recent Advances in Parsing Tech-
nologies, pages 145–168. Kluwer.
M. Dymetman. 1992. A generalized greibach normal
form for definite clause grammars. In Proceedings of
the International Conference on Computational Lin-
guistics.
M. Dymetman. 1994. A simple transformation for
offline-parsable gramamrs and its termination proper-
ties. In Proceedings of the International Conference
on Computational Linguistics.
P. Graf. 1996. Term Indexing. Springer Verlag.
T. Makino, K. Torisawa, and J. Tsuji. 1998. LiL-
FeS — practical unification-based programming sys-
tem for typed feature structures. In Proceedings of
COLING/ACL-98, volume 2, pages 807–811.
R. Malouf, J. Carroll, and A. Copestake. 2000. Efficient
feature structure operations without compilation. Nat-
ural Language Engineering, 6(1):29–46.
T. Ninomiya, T. Makino, and J. Tsuji. 2002. An indexing
scheme for typed feature structures. In Proceedings of
the 19th International Conference on Computational
Linguistics (COLING-02).
S. Oepen and J. Carroll. 2000. Parser engineering and
performance profiling. Natural Language Engineer-
ing.
G. Penn and O. Popescu. 1997. Head-driven genera-
tion and indexing in ALE. In Proceedings of the EN-
VGRAMworkshop; ACL/EACL-97.
G. Penn. 1999. Optimising don’t-care non-determinism
with statistical information. Technical Report 140,
Sonderforschungsbereich 340, T¨ubingen.
G. Penn. 2000. The Algebraic Structure of Attributed
Type Signatures. Ph.D. thesis, Carnegie Mellon Uni-
versity.
F. C. N. Pereira and S. M. Shieber. 1987. Prolog and
Natural-Language Analysis, volume 10 of CSLI Lec-
ture Notes. University of Chicago Press.
E. Stabler. 1993. The Logical Approach to Syntax: Foun-
dations, Specifications, and implementations of Theo-
ries of Government and Binding. MIT Press.
K. Torisawa and J. Tsuji. 1995. Compiling HPSG-style
grammar to object-oriented language. In Proceedings
ofNLPRS-1995, pages 568–573.
G. van Noord. 1997. An efficient implementation of the
head-corner parser. Computational Linguistics.
</reference>
<figure confidence="0.999030714285714">
Time [log(msec)]
1000
100
Active
EFD
Carp
EFD-index
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.988227">
<title confidence="0.999972">A Tabulation-Based Parsing Method that Reduces Copying</title>
<author confidence="0.999733">Gerald Penn</author>
<author confidence="0.999733">Cosmin Munteanu</author>
<affiliation confidence="0.999984">Department of Computer Science University of Toronto</affiliation>
<address confidence="0.998012">Toronto M5S 3G4, Canada</address>
<abstract confidence="0.999535476190476">This paper presents a new bottom-up chart parsing algorithm for Prolog along with a compilation procedure that reduces the amount of copying at run-time to a constant number (2) per edge. It has applications to unification-based grammars with very large partially ordered categories, in which copying is expensive, and can facilitate the use of more sophisticated indexing strategies for retrieving such categories that may otherwise be overwhelmed by the cost of such copying. It also provides a new perspective on “quick-checking” and related heuristics, which seems to confirm that forcing an early failure (as opposed to seeking an early guarantee of success) is in fact the best approach to use. A preliminary empirical evaluation of its performance is also provided.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Carpenter</author>
<author>G Penn</author>
</authors>
<title>Compiling typed attribute-value logic grammars.</title>
<date>1996</date>
<booktitle>Recent Advances in Parsing Technologies,</booktitle>
<pages>145--168</pages>
<editor>In H. Bunt and M. Tomita, editors,</editor>
<publisher>Kluwer.</publisher>
<contexts>
<context position="2827" citStr="Carpenter and Penn, 1996" startWordPosition="445" endWordPosition="448">exed. Depending on the grammar and the empirical distribution of matching mother/lexical and daughter descriptions, this number could approach copies for an edge added early to the chart, where is the length of the input to be parsed. For classical context-free grammars, the category information that must be copied is normally quite small in size. For feature-structure-based grammars and other highly lexicalized grammars with large categories, however, which have become considerably more popular since the advent of the standard parsing algorithms, it becomes quite significant. The ALE system (Carpenter and Penn, 1996) attempts to reduce this by using an algorithm due to Carpenter that traverses the string breadth-first, right-to-left, but matches rule daughters rule depth-first, left-toright in a failure-driven loop, which eliminates the need for active edges and keeps the sizes of the heap and call stack small. It still copies a candidate edge every time it tries to match it to a daughter description, however, which can approach because of its lack of active edges. The OVIS system (van Noord, 1997) employs selective memoization, which tabulates only maximal projections in a head-corner parser — partial pr</context>
</contexts>
<marker>Carpenter, Penn, 1996</marker>
<rawString>B. Carpenter and G. Penn. 1996. Compiling typed attribute-value logic grammars. In H. Bunt and M. Tomita, editors, Recent Advances in Parsing Technologies, pages 145–168. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dymetman</author>
</authors>
<title>A generalized greibach normal form for definite clause grammars.</title>
<date>1992</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="21434" citStr="Dymetman, 1992" startWordPosition="3439" endWordPosition="3440">establishes the correctness of the algorithm, is omitted for brevity. EFD-closure bears some resemblance in its intentions to Greibach Normal Form, but: (1) it is far more conservative in the number of extra rules it must create; (2) it is linked directly to the derivable empty categories of the grammar, whereas GNF conversion proceeds from an already -eliminated grammar (EFD-closure of any -free grammar, in fact, is the grammar itself); (3) GNF is rather more difficult to define in the case of unification-based grammars than with classical CFGs, and in the one generalization we are aware of (Dymetman, 1992), EFD-closure is actually not guaranteed by it; and Dymetman’s generalization only works for classically offline-parseable grammars. In the case of non- -OP grammars, a standard bottom-up parser without EFD-closure would not terminate at run-time either. Our new algorithm is thus neither better nor worse than a textbook bottomup parser with respect to termination. A remaining topic for consideration is the adaptation of this method to strategies with better termination properties than the pure bottom-up strategy. 8 Empirical Evaluation The details of how to integrate an indexing strategy for u</context>
</contexts>
<marker>Dymetman, 1992</marker>
<rawString>M. Dymetman. 1992. A generalized greibach normal form for definite clause grammars. In Proceedings of the International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dymetman</author>
</authors>
<title>A simple transformation for offline-parsable gramamrs and its termination properties.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="4597" citStr="Dymetman, 1994" startWordPosition="737" endWordPosition="738">us of some theoretical interest in that it proves that at least a constant bound is attainable within a Prolog setting. It does so by invoking a new kind of grammar transformation, called EFD-closure, which ensures that a grammar need not match an empty category to the leftmost daughter of any rule. This transformation is similar to many of the myriad of earlier transformations proposed for exploring the decidability of recognition under various parsing control strategies, but the property it establishes is more conservative than brute-force epsilon elimination for unification-based grammars (Dymetman, 1994). It also still treats empty categories distinctly from non-empty ones, unlike the linking tables proposed for treating leftmost daughters in left-corner parsing (Pereira and Shieber, 1987). Its motivation, the practical consideration of copying overhead, is also rather different, of course. The algorithm will be presented as an improved version of ALE’s parser, although other standard bottom-up parsers can be similarly adapted. 2 Why Prolog? Apology! This paper is not an attempt to show that a Prolog-based parser could be as fast as a phrasestructure parser implemented in an imperative progra</context>
</contexts>
<marker>Dymetman, 1994</marker>
<rawString>M. Dymetman. 1994. A simple transformation for offline-parsable gramamrs and its termination properties. In Proceedings of the International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Graf</author>
</authors>
<title>Term Indexing.</title>
<date>1996</date>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="11839" citStr="Graf, 1996" startWordPosition="1891" endWordPosition="1892">lization with large lexica, and suggest an extension to statistically trained decision trees. Ninomiya et al. (2002) take a more computationally brute-force approach to index very large databases of feature structures for some kind of information retrieval application. Neither of these is suitable for indexing chart edges during parsing, because the edges are discarded after every sentence, before the expense of building the index can be satisfactorily amortized. There is a fair amount of relevant work in the database and programming language communities, but many of the results are negative (Graf, 1996) — very little time can be spent on constructing the index. A moment’s thought reveals that the very notion of an active edge, tabulating the well-formed prefixes of rule right-hand-sides, presumes that copying is not a significant enough issue to merit the overhead of more specialized indexing. While the present paper proceeds from Carpenter’s algorithm, in which no active edges are used, it will become clear from our evaluation that active edges or their equivalent within a more sophisticated indexing strategy are an issue that should be re-investigated now that the cost of copying can prova</context>
</contexts>
<marker>Graf, 1996</marker>
<rawString>P. Graf. 1996. Term Indexing. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Makino</author>
<author>K Torisawa</author>
<author>J Tsuji</author>
</authors>
<title>LiLFeS — practical unification-based programming system for typed feature structures.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL-98,</booktitle>
<volume>2</volume>
<pages>807--811</pages>
<contexts>
<context position="7488" citStr="Makino et al., 1998" startWordPosition="1176" endWordPosition="1179">programming techniques to more recent constraint-based grammatical theories. Relational predicates also make grammars more modular and readable than pure phrasestructure-based grammars. Commercial Prolog implementations are quite difficult to beat with imperative implementations when it is general logic programming that is required. This is no less true with respect to more recent data structures in lexicalized grammatical theories. A recent comparison (Penn, 2000) of a version between ALE (which is written in Prolog) that reduces typed feature structures to Prolog term encodings, and LiLFeS (Makino et al., 1998), the fastest imperative re-implementation of an ALE-like language, showed that ALE was slightly over 10 times faster on large-scale parses with its HPSG reference grammar than LiLFeS was with a slightly more efficient version of that grammar. 3 Empirical Efficiency Whether this algorithm will outperform standard Prolog parsers is also largely empirical, because: 1. one of the two copies is kept on the heap itself and not released until the end of the parse. For large parses over large data structures, that can increase the size of the heap significantly, and will result in a greater number of</context>
</contexts>
<marker>Makino, Torisawa, Tsuji, 1998</marker>
<rawString>T. Makino, K. Torisawa, and J. Tsuji. 1998. LiLFeS — practical unification-based programming system for typed feature structures. In Proceedings of COLING/ACL-98, volume 2, pages 807–811.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Malouf</author>
<author>J Carroll</author>
<author>A Copestake</author>
</authors>
<title>Efficient feature structure operations without compilation.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="10630" citStr="Malouf et al. (2000)" startWordPosition="1695" endWordPosition="1698">hart on the heap allows for more sophisticated indexing strategies to apply to memoized categories that would otherwise be overwhelmed by the cost of copying an edge before matching it against an index. Indexing is also briefly considered in Section 8. Indexing is not the same thing as filtering (Torisawa and Tsuji, 1995), which extracts an approximation grammar to parse with first, in order to increase the likelihood of early unification failure. If the filter parse succeeds, the system then proceeds to perform the entire unification operation, as if the approximation had never been applied. Malouf et al. (2000) cite an improvement of 35–45% using a “quickcheck” algorithm that they appear to believe finds the optimal selection of feature paths for quickchecking. It is in fact only a greedy approximation — the optimization problem is exponential in the number of feature paths used for the check. Penn (1999) cites an improvement of 15-40% simply by re-ordering the sister features of only two types in the signature of the ALE HPSG grammar during normal unification. True indexing re-orders required operations without repeating them. Penn and Popescu (1997) build an automaton-based index for surface reali</context>
</contexts>
<marker>Malouf, Carroll, Copestake, 2000</marker>
<rawString>R. Malouf, J. Carroll, and A. Copestake. 2000. Efficient feature structure operations without compilation. Natural Language Engineering, 6(1):29–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Ninomiya</author>
<author>T Makino</author>
<author>J Tsuji</author>
</authors>
<title>An indexing scheme for typed feature structures.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics (COLING-02).</booktitle>
<contexts>
<context position="11344" citStr="Ninomiya et al. (2002)" startWordPosition="1811" endWordPosition="1814">nds the optimal selection of feature paths for quickchecking. It is in fact only a greedy approximation — the optimization problem is exponential in the number of feature paths used for the check. Penn (1999) cites an improvement of 15-40% simply by re-ordering the sister features of only two types in the signature of the ALE HPSG grammar during normal unification. True indexing re-orders required operations without repeating them. Penn and Popescu (1997) build an automaton-based index for surface realization with large lexica, and suggest an extension to statistically trained decision trees. Ninomiya et al. (2002) take a more computationally brute-force approach to index very large databases of feature structures for some kind of information retrieval application. Neither of these is suitable for indexing chart edges during parsing, because the edges are discarded after every sentence, before the expense of building the index can be satisfactorily amortized. There is a fair amount of relevant work in the database and programming language communities, but many of the results are negative (Graf, 1996) — very little time can be spent on constructing the index. A moment’s thought reveals that the very noti</context>
</contexts>
<marker>Ninomiya, Makino, Tsuji, 2002</marker>
<rawString>T. Ninomiya, T. Makino, and J. Tsuji. 2002. An indexing scheme for typed feature structures. In Proceedings of the 19th International Conference on Computational Linguistics (COLING-02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
<author>J Carroll</author>
</authors>
<title>Parser engineering and performance profiling.</title>
<date>2000</date>
<journal>Natural Language Engineering.</journal>
<contexts>
<context position="8671" citStr="Oepen and Carroll (2000)" startWordPosition="1369" endWordPosition="1372">y, and will result in a greater number of cache misses and page swaps. 2. the new algorithm also requires an off-line partial evaluation of the grammar rules that increases the number of rules that must be iterated through at run-time during depth-first closure. This can result in redundant operations being performed among rules and their partially evaluated instances to match daughter categories, unless those rules and their partial evaluations are folded together with local disjunctions to share as much compiled code as possible. A preliminary empirical evaluation is presented in Section 8. Oepen and Carroll (2000), by far the most comprehensive attempt to profile and optimize the performance of feature-structure-based grammars, also found copying to be a significant issue in their imperative implementations of several HPSG parsers — to the extent that it even warranted recomputing unifications in places, and modifying the manner in which active edges are used in their fastest attempt (called hyper-active parsing). The results of the present study can only cautiously be compared to theirs so far, because of our lack of access to the successive stages of their implementations and the lack of a common gra</context>
</contexts>
<marker>Oepen, Carroll, 2000</marker>
<rawString>S. Oepen and J. Carroll. 2000. Parser engineering and performance profiling. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Penn</author>
<author>O Popescu</author>
</authors>
<title>Head-driven generation and indexing in ALE.</title>
<date>1997</date>
<booktitle>In Proceedings of the ENVGRAMworkshop; ACL/EACL-97.</booktitle>
<contexts>
<context position="11181" citStr="Penn and Popescu (1997)" startWordPosition="1787" endWordPosition="1790">on, as if the approximation had never been applied. Malouf et al. (2000) cite an improvement of 35–45% using a “quickcheck” algorithm that they appear to believe finds the optimal selection of feature paths for quickchecking. It is in fact only a greedy approximation — the optimization problem is exponential in the number of feature paths used for the check. Penn (1999) cites an improvement of 15-40% simply by re-ordering the sister features of only two types in the signature of the ALE HPSG grammar during normal unification. True indexing re-orders required operations without repeating them. Penn and Popescu (1997) build an automaton-based index for surface realization with large lexica, and suggest an extension to statistically trained decision trees. Ninomiya et al. (2002) take a more computationally brute-force approach to index very large databases of feature structures for some kind of information retrieval application. Neither of these is suitable for indexing chart edges during parsing, because the edges are discarded after every sentence, before the expense of building the index can be satisfactorily amortized. There is a fair amount of relevant work in the database and programming language comm</context>
</contexts>
<marker>Penn, Popescu, 1997</marker>
<rawString>G. Penn and O. Popescu. 1997. Head-driven generation and indexing in ALE. In Proceedings of the ENVGRAMworkshop; ACL/EACL-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Penn</author>
</authors>
<title>Optimising don’t-care non-determinism with statistical information.</title>
<date>1999</date>
<tech>Technical Report 140, Sonderforschungsbereich 340, T¨ubingen.</tech>
<contexts>
<context position="10930" citStr="Penn (1999)" startWordPosition="1749" endWordPosition="1750">d Tsuji, 1995), which extracts an approximation grammar to parse with first, in order to increase the likelihood of early unification failure. If the filter parse succeeds, the system then proceeds to perform the entire unification operation, as if the approximation had never been applied. Malouf et al. (2000) cite an improvement of 35–45% using a “quickcheck” algorithm that they appear to believe finds the optimal selection of feature paths for quickchecking. It is in fact only a greedy approximation — the optimization problem is exponential in the number of feature paths used for the check. Penn (1999) cites an improvement of 15-40% simply by re-ordering the sister features of only two types in the signature of the ALE HPSG grammar during normal unification. True indexing re-orders required operations without repeating them. Penn and Popescu (1997) build an automaton-based index for surface realization with large lexica, and suggest an extension to statistically trained decision trees. Ninomiya et al. (2002) take a more computationally brute-force approach to index very large databases of feature structures for some kind of information retrieval application. Neither of these is suitable for</context>
</contexts>
<marker>Penn, 1999</marker>
<rawString>G. Penn. 1999. Optimising don’t-care non-determinism with statistical information. Technical Report 140, Sonderforschungsbereich 340, T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Penn</author>
</authors>
<title>The Algebraic Structure of Attributed Type Signatures.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="7337" citStr="Penn, 2000" startWordPosition="1151" endWordPosition="1152">itful one for parsing, particularly with respect to the application of logic programming transformations (Stabler, 1993) and constraint logic programming techniques to more recent constraint-based grammatical theories. Relational predicates also make grammars more modular and readable than pure phrasestructure-based grammars. Commercial Prolog implementations are quite difficult to beat with imperative implementations when it is general logic programming that is required. This is no less true with respect to more recent data structures in lexicalized grammatical theories. A recent comparison (Penn, 2000) of a version between ALE (which is written in Prolog) that reduces typed feature structures to Prolog term encodings, and LiLFeS (Makino et al., 1998), the fastest imperative re-implementation of an ALE-like language, showed that ALE was slightly over 10 times faster on large-scale parses with its HPSG reference grammar than LiLFeS was with a slightly more efficient version of that grammar. 3 Empirical Efficiency Whether this algorithm will outperform standard Prolog parsers is also largely empirical, because: 1. one of the two copies is kept on the heap itself and not released until the end </context>
</contexts>
<marker>Penn, 2000</marker>
<rawString>G. Penn. 2000. The Algebraic Structure of Attributed Type Signatures. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F C N Pereira</author>
<author>S M Shieber</author>
</authors>
<title>Prolog and Natural-Language Analysis,</title>
<date>1987</date>
<volume>10</volume>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="4786" citStr="Pereira and Shieber, 1987" startWordPosition="762" endWordPosition="765">n, called EFD-closure, which ensures that a grammar need not match an empty category to the leftmost daughter of any rule. This transformation is similar to many of the myriad of earlier transformations proposed for exploring the decidability of recognition under various parsing control strategies, but the property it establishes is more conservative than brute-force epsilon elimination for unification-based grammars (Dymetman, 1994). It also still treats empty categories distinctly from non-empty ones, unlike the linking tables proposed for treating leftmost daughters in left-corner parsing (Pereira and Shieber, 1987). Its motivation, the practical consideration of copying overhead, is also rather different, of course. The algorithm will be presented as an improved version of ALE’s parser, although other standard bottom-up parsers can be similarly adapted. 2 Why Prolog? Apology! This paper is not an attempt to show that a Prolog-based parser could be as fast as a phrasestructure parser implemented in an imperative programming language such as C. Indeed, if the categories of a grammar are discretely ordered, chart edges can be used for further parsing in situ, i.e., with no copying out of the table, in an i</context>
<context position="6573" citStr="Pereira and Shieber, 1987" startWordPosition="1034" endWordPosition="1037">rase-structure-based parsing with categories such as typed feature structures, is extremely cumbersome for large-scale grammar design. Even in the handful of instances in which it does seem to have been successful, which includes the recent HPSG English Resource Grammar and a handful of Lexical-Functional Grammars, the results are by no means graceful, not at all modular, and arguably not reusable by anyone except their designers. The particular interest in Prolog’s expressiveness arises, of course, from the interest in generalized context-free parsing beginning with definite clause grammars (Pereira and Shieber, 1987), as an instance of a logic programming control strategy. The connection between logic programming and parsing is well-known and has also been a very fruitful one for parsing, particularly with respect to the application of logic programming transformations (Stabler, 1993) and constraint logic programming techniques to more recent constraint-based grammatical theories. Relational predicates also make grammars more modular and readable than pure phrasestructure-based grammars. Commercial Prolog implementations are quite difficult to beat with imperative implementations when it is general logic </context>
</contexts>
<marker>Pereira, Shieber, 1987</marker>
<rawString>F. C. N. Pereira and S. M. Shieber. 1987. Prolog and Natural-Language Analysis, volume 10 of CSLI Lecture Notes. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Stabler</author>
</authors>
<title>The Logical Approach to Syntax: Foundations, Specifications, and implementations of Theories of Government and Binding.</title>
<date>1993</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6846" citStr="Stabler, 1993" startWordPosition="1078" endWordPosition="1080">of Lexical-Functional Grammars, the results are by no means graceful, not at all modular, and arguably not reusable by anyone except their designers. The particular interest in Prolog’s expressiveness arises, of course, from the interest in generalized context-free parsing beginning with definite clause grammars (Pereira and Shieber, 1987), as an instance of a logic programming control strategy. The connection between logic programming and parsing is well-known and has also been a very fruitful one for parsing, particularly with respect to the application of logic programming transformations (Stabler, 1993) and constraint logic programming techniques to more recent constraint-based grammatical theories. Relational predicates also make grammars more modular and readable than pure phrasestructure-based grammars. Commercial Prolog implementations are quite difficult to beat with imperative implementations when it is general logic programming that is required. This is no less true with respect to more recent data structures in lexicalized grammatical theories. A recent comparison (Penn, 2000) of a version between ALE (which is written in Prolog) that reduces typed feature structures to Prolog term e</context>
</contexts>
<marker>Stabler, 1993</marker>
<rawString>E. Stabler. 1993. The Logical Approach to Syntax: Foundations, Specifications, and implementations of Theories of Government and Binding. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Torisawa</author>
<author>J Tsuji</author>
</authors>
<title>Compiling HPSG-style grammar to object-oriented language.</title>
<date>1995</date>
<booktitle>In Proceedings ofNLPRS-1995,</booktitle>
<pages>568--573</pages>
<contexts>
<context position="10333" citStr="Torisawa and Tsuji, 1995" startWordPosition="1648" endWordPosition="1651">m that textbook algorithms suffer from when running under non-ISO-compatible Prologs (which is to say most of them). On such Prologs, asserted empty category edges that can match leftmost daughter descriptions of rules are not able to combine with the outputs of those rules. 3. keeping a copy of the chart on the heap allows for more sophisticated indexing strategies to apply to memoized categories that would otherwise be overwhelmed by the cost of copying an edge before matching it against an index. Indexing is also briefly considered in Section 8. Indexing is not the same thing as filtering (Torisawa and Tsuji, 1995), which extracts an approximation grammar to parse with first, in order to increase the likelihood of early unification failure. If the filter parse succeeds, the system then proceeds to perform the entire unification operation, as if the approximation had never been applied. Malouf et al. (2000) cite an improvement of 35–45% using a “quickcheck” algorithm that they appear to believe finds the optimal selection of feature paths for quickchecking. It is in fact only a greedy approximation — the optimization problem is exponential in the number of feature paths used for the check. Penn (1999) ci</context>
</contexts>
<marker>Torisawa, Tsuji, 1995</marker>
<rawString>K. Torisawa and J. Tsuji. 1995. Compiling HPSG-style grammar to object-oriented language. In Proceedings ofNLPRS-1995, pages 568–573.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G van Noord</author>
</authors>
<title>An efficient implementation of the head-corner parser. Computational Linguistics.</title>
<date>1997</date>
<marker>van Noord, 1997</marker>
<rawString>G. van Noord. 1997. An efficient implementation of the head-corner parser. Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>