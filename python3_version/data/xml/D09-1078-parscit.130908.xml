<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000647">
<title confidence="0.985379">
Less is More: Significance-Based N-gram Selection
for Smaller, Better Language Models
</title>
<author confidence="0.98751">
Robert C. Moore Chris Quirk
</author>
<affiliation confidence="0.961774">
Microsoft Research
</affiliation>
<address confidence="0.926763">
Redmond, WA 98052, USA
</address>
<email confidence="0.999675">
{bobmoore,chrisq}@microsoft.com
</email>
<sectionHeader confidence="0.99482" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999662578947368">
The recent availability of large corpora
for training N-gram language models has
shown the utility of models of higher or-
der than just trigrams. In this paper, we
investigate methods to control the increase
in model size resulting from applying stan-
dard methods at higher orders. We in-
troduce significance-based N-gram selec-
tion, which not only reduces model size,
but also improves perplexity for several
smoothing methods, including Katz back-
off and absolute discounting. We also
show that, when combined with a new
smoothing method and a novel variant of
weighted-difference pruning, our selection
method performs better in the trade-offbe-
tween model size and perplexity than the
best pruning method we found for modi-
fied Kneser-Ney smoothing.
</bodyText>
<sectionHeader confidence="0.998788" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999173181818182">
Statistical language models are potentially useful
for any language technology task that produces
natural-language text as a final (or intermediate)
output. In particular, they are extensively used in
speech recognition and machine translation. De-
spite the criticism that they ignore the structure of
natural language, simple N-gram models, which
estimate the probability of each word in a text
string based on the N −1 preceding words, remain
the most widely-used type of model.
Until the late 1990s, N-gram language models
of order higher than trigrams were seldom used.
This was due, at least in part, to the fact the
amounts of training data available did not produce
significantly better results from higher-order mod-
els. Since that time, however, increasingly large
amounts of language model training data have be-
come available ranging from approximately one
billion words (the Gigaword corpora from the
Linguistic Data Consortium) to trillions of words
(Brants et al., 2007). With these larger resources,
the use of language models based on 5-grams to
7-grams is becoming increasingly common.
The problem we address here is that, even when
relatively modest amounts of training data are
used, high-order N-gram language models esti-
mated by standard techniques can be impractically
large. Hence, we investigate ways of building
high-order N-gram language models without dra-
matically increasing model size. This is, of course,
the same goal behind much previous work on lan-
guage model pruning, including that of Seymore
and Rosenfeld (1996), Stolcke (1998), and Good-
man and Gao (2000). We take a novel approach,
however, which we refer to as significance-based
N-gram selection. We reject a higher-order esti-
mate of the probability of a particular word in a
particular context whenever the distribution of ob-
servations for the higher-order estimate provides
no evidence that the higher-order estimate is bet-
ter than our backoff estimate.
Perhaps our most surprising result is that
significance-based N-gram selection not only re-
duces language model size, but it also improves
perplexity when applied to a number of widely-
used smoothing methods, including Katz backoff
and several variants of absolute discounting.1 In
contrast, experiments applying previous pruning
methods to Katz backoff (Seymore and Rosen-
feld, 1996; Stolcke, 1998) and absolute discount-
ing (Goodman and Gao, 2000) always found the
lowest perplexity model to be the unpruned model.
We tested significance-based selection on only
one smoothing method without obtaining im-
proved perplexity: modified Kneser-Ney (KN)
</bodyText>
<footnote confidence="0.85363075">
1For most of the standard smoothing methods mentioned
here, we refer the reader to the excellent comparative study
of smoothing methods by Chen and Goodman (1998). Refer-
ences to the original sources may be found there.
</footnote>
<page confidence="0.931191">
746
</page>
<note confidence="0.996636">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 746–755,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999572172413793">
smoothing (Chen and Goodman, 1998). This
is unfortunate, because modified KN smoothing
generally seems to have the lowest perplexity of
any known smoothing method for N-gram lan-
guage models; in our tests it had a lower perplex-
ity than any of the other models, with or with-
out significance-based N-gram selection. How-
ever, when we compared modified KN smooth-
ing to our best results applying N-gram selection
to other smoothing methods for multiple N-gram
orders, two of our models outperformed modified
KN in terms of perplexity for a given model size.
Of course, the trade-off between perplexity and
model size for modified KN can also be im-
proved by pruning. So, in a final set of ex-
periments we found the best combinations we
could for pruned modified KN models, and we did
the same for our best model using significance-
based selection. The best pruning method for
the latter turned out to be a novel modifica-
tion of weighted-difference pruning (Seymore and
Rosenfeld, 1996) that was especially convenient
to compute given our method for performing
significance-based N-gram selection. The final re-
sult is that our best model using significance-based
selection and modified weighted difference prun-
ing always had a better size/perplexity trade-off
than pruned modified KN, with up to about 8%
perplexity reduction for a given model size.
</bodyText>
<sectionHeader confidence="0.972904" genericHeader="introduction">
2 Significance-Based N-gram Selection
</sectionHeader>
<bodyText confidence="0.999257736842105">
The idea of using a statistical test to decide
whether to use a higher- or lower-order estimate of
an N-gram probablity is not new. It was perhaps
first proposed by Ron, et al. (1996), who suggested
using a threshold on relative entropy (Kullback-
Liebler divergence) as an appropriate test to de-
cide whether to extend the context used to predict
the next token in a sequence. Stolcke (1998) used
the same metric in his work on language model
pruning, and he also pointed out that weighted dif-
ference pruning is, in fact, an approximation of
relative entropy pruning. However, while relative
entropy pruning is based on a statistical test, it is
not a significance test. The difference in probabil-
ity represented by a certain relative entropy value
can be statistically significant when measured on
a large corpus, but not significant when measured
on a small corpus.
The primary test we use to choose between
higher- or lower-order estimates of an N-gram
probablity is inspired by an insight of Jedynak and
Khudanpur (2005). They note that, given a set
of y observations of a multinomial distribution,
the observed counts will have the highest proba-
bilty of any possible set of y observations for the
maximum likelihood estimate (MLE) model de-
rived from the relative frequencies of those obser-
vations. In general, however, the MLE model will
not be the only model for which this set of obser-
vations is the most probable set of y observations.
Jedynak and Khudanpur call the set of such mod-
els the maximum likelihood set (MLS) for the ob-
servations.
Jedynak and Khudanpur argue that the obser-
vations alone do not support choosing the MLE
over other members of the MLS. The MLE may
assign the observations a higher probability than
other members of the MLS, but that may be an
accident of what outcomes are possible given the
number of observations. If we flip a coin 9 times
and get 5 heads, is there any reason to believe that
the probability of heads is closer to the MLE 5/9
than it is to 5/10? No, because 5/9 is as close as
we can come to 5/10, given 9 observations.
We apply this insight to the problem of N-
gram selection as follows: For each word wn
in a context w1...wn_1 with a backoff estimate
for the probability of that word in that context
Q p(wn|w2...wn_1),2 we do not include an explicit
estimate of p(wn|w1...wn_1) in our model, if the
backoff estimate is within the MLS of the counts
for w1...wn and w1...wn_1.
This requires finding the MLS of a set of obser-
vations only for binomial distributions (rather than
the general multinomial distributions studied by
Jedynak and Khudanpur), which has a very sim-
ple solution:
</bodyText>
<equation confidence="0.996603">
� �
MLS(x y) = p y +x 1 ≤ p ≤ x + 1
y + 1
</equation>
<bodyText confidence="0.999262111111111">
where x is the count for w1...wn, y is the count for
w1...wn_1, and p is a possible backoff probabilty
estimate for p(wn|w1...wn_1). In this case, the
MLS is the set of binomial distributions that have
x successes as their mode given y trials, which is
well-known to be specified by this formula.
We describe this method as “significance-
based” because we can consider our criterion as
a significance test in which we take the backoff
</bodyText>
<footnote confidence="0.868138">
2p(wn|w2...wn−1) being the next lower-order estimate,
and β being the backoff weight for the context w1...wn−1.
</footnote>
<page confidence="0.997132">
747
</page>
<bodyText confidence="0.99992962962963">
probability estimate as the null hypothesis for the
estimate in the higher-order model, and we set the
rejection threshold to the lowest possible value;
we reject the null hypothesis (the backoffprobabil-
ity) if there are any outcomes for the given number
of trials that are more likely, according to the null
hypothesis, than the one we observed.
We make a few refinements to this basic idea.
First, we never add an explicit higher-order esti-
mate to our model, if the next lower-order estimate
is not explicitly stored in the model. This enables
us to keep only the next lower-order model avail-
able while performing N-gram selection.
Next, we observe that in some cases the higher-
order estimate for p(wn|w1...wn−1) may not fall
within the MLS for the observed counts, due to
smoothing. In this case, we prefer the backoff
probability estimate if it lies within the MLS or be-
tween the smoothed higher-order estimate and the
MLS. Otherwise, we would reject the backoff es-
timate for being outside the MLS, only to replace
it with a higher-order estimate even farther outside
the MLS.
Finally, we note that the backoff probability es-
timate for an N-gram not observed in the train-
ing data sometimes falls outside the corresponding
MLS, which in the 0-count case simplifies to
</bodyText>
<equation confidence="0.996378333333333">
� �
1
MLS(0,y) = p� ��0 &lt; p &lt; y + 1
</equation>
<bodyText confidence="0.999955909090909">
When this happens, we include an explicit higher-
order estimate p(wn|w1...wn−1) = 1/(y + 1),
which is the upper limit of the MLS. This is similar
to Rosenfeld and Huang’s (1993) “confidence in-
terval capping” method for reducing unreasonably
high backoff estimates for unobserved N-grams.
In order to apply this treatment of 0-count N-
grams, we sort the explicitly-stored N-grams for
each backoff context by decreasing probability.
For each higher-order context, to find the 0-count
N-grams subject to the 1/(y + 1) limit, we tra-
verse the sorted list of explicitly-stored N-grams
for its backoff context. When we encounter an N-
gram whose extension to the higher-order context
was not observed in the training data, we give it
an explicit probability of 1/(y+1), if its weighted
backoff probability is greater than that. We stop
the traversal as soon as we encounter an N-gram
for the backoff context that has a weighted backoff
probability less than or equal to 1/(y+1), which in
practice means we actually examine only a small
number of backoff probabilities for each context.
</bodyText>
<sectionHeader confidence="0.695023" genericHeader="method">
3 Finding Backoff Weights by Iterative
Search
</sectionHeader>
<bodyText confidence="0.999994469387755">
The approach described above is very attractive
from a theoretical perspective, but it has one prac-
tical complication. To decide which N-grams for
each context to explicitly include in the higher-
order model, we need to know the backoff weight
for the context, but we cannot compute the backoff
weight until we know exactly which higher-order
N-grams are included in the model.
We address this problem by iteratively solving
for a backoff weight that yields a normalized prob-
ability distribution. For each context, we guess
an initial value for the backoff weight and keep
track of the sum of the probabilites resulting from
applying our N-gram selection method with that
backoff weight. If the sum is greater than 1.0, by
more than a convergence threshold, we reduce the
estimated backoff weight and iterate. If the sum
is less than 1.0, by more than the threshold, we
increase the estimated weight and iterate.
It is easy to see that, for all standard smooth-
ing methods, the function from backoff weights
to probability sums is piece-wise linear. Within
a region where no decision changes about which
N-grams to include in the model, the probability
sum is a linear function of the backoff weight. At
values of the backoff weight where the set of se-
lected N-grams changes, the function can be dis-
continous. With a little more effort, one can see
that the linear segments overlap with respect to the
probability sum in such a way that there will al-
ways be one or more values of the backoff weight
that make the probability sum equal 1.0, with one
specific exception.
The exception arises because of the capping of
backoff probabilites for unobserved N-grams. It
is possible for there to be a context for which
all observed N-grams are included in the higher-
order model, the probabilities for all unobserved
N-grams are either capped at 1/(y + 1) or effec-
tively 0 due to arithmetic underflow, and the prob-
ability sum is less than 1.0. For some smoothing
methods, the probability sum cannot be increased
in this situation by increasing the backoff weight.
We check for this situation, and if it arises, we
increase the cap on the 0-count probability just
enough to make the probability sum equal 1.0.
That exception aside, we iteratively find back-
off weights as follows: For an initial estimate
of the backoff weight for a context, we compute
</bodyText>
<page confidence="0.98669">
748
</page>
<bodyText confidence="0.999990425531915">
what the backoff weight would be for the base
smoothing method without N-gram selection. If
that value is less than 1.0, we use it as our ini-
tial estimate, otherwise we use 1.0, which annec-
dotally seems to produce better models than ini-
tial estimates greater than 1.0, in situations where
there are multiple solutions. If the first iteration of
N-gram selection produces a probability sum less
than 1.0, we repeatedly double the estimated back-
off weight until we obtain a sum greater than or
equal to 1.0, or we encounter the special situation
previously described. If the initial probability sum
is greater than 1.0, we repeatedly halve the esti-
mated backoff weight until we obtain a sum less
than or equal to 1.0.
Once we have values for the backoff weight that
produce probability sums on both sides of 1.0, we
have a solution bracketed, and we can use standard
numerical search techniques to find that solution.
At every subsequent iteration, we try a value for
the backoff weight between the largest value we
have tried that produces a sum less than 1.0 and
the smallest value we have tried that produces a
sum greater than 1.0. We stop when the difference
between these values of the backoff weight is less
than a convergence threshold.
We use a combination of simple techniques to
choose the next value of the backoff weight to try.
The primary technique we use is called the “false
position method”, which basically solves the lin-
ear equation defined by the two current bracketing
values and corresponding probability sums. The
advantage of this method is that, if our bracket-
ing points lie on the same linear segment of our
function, we obtain a solution in one step. The
disadvantage of the method is that it sometimes
approaches the solution by a long sequence of tiny
steps from the same side.
We try to detect the latter situation by keeping
track of the number of consecutive iterations that
make a step in the same direction. If this num-
ber reaches 10, we take the next step by the bi-
section method, which simply tries the value of
the backoff weight halfway between our two cur-
rent bracketing values. In practice, this combined
search method works very well, taking an average
of less than four iterations per backoff weight.
</bodyText>
<sectionHeader confidence="0.978756" genericHeader="method">
4 Modified Weighted-Difference Pruning
</sectionHeader>
<bodyText confidence="0.99944580952381">
While the N-gram selection method described
above considerably reduces the number of para-
meters in a high-order language model, we may
wish to reduce language model size even more.
The concept of significance-based N-gram selec-
tion to produce smaller models could be extended
by relaxing our criterion for using backoff distrib-
utions in place of explicit higher-order probability
estimates, but true significance tests at more re-
laxed thresholds that are accurate for small counts
are expensive to compute; so we resort to more
conventional language model pruning methods.
In our experiments, we tried four methods for
additional pruning: simple count cutoffs, relative
entropy pruning (REP) (Stolcke, 1998), and two
modified versions of Seymore and Rosenfeld’s
(1996) weighted-difference pruning (WDP). In the
notation we have been using, Seymore and Rosen-
feld’s WDP criterion for using a backoff estimate,
in place of an explicit higher-order estimate, is that
the quantity
</bodyText>
<equation confidence="0.99617">
K×
�
log(p(wn|w1...wn−1))−
log(Qu p(wn|w2...wn−1))
</equation>
<bodyText confidence="0.9984905">
be less than a pruning threshold, where K is
the Good-Turing-discounted training set count for
w1...wn, and Qu is the backoff weight for the un-
pruned model.
The first of our modified version of WDP uses
the following quantity instead:
</bodyText>
<equation confidence="0.684763333333333">
p(w1...wn)×
log(p(wn|w1...wn−1))−
log(Qp p(wn|w2...wn−1))
</equation>
<bodyText confidence="0.999984777777778">
where p(w1...wn) is an estimate of the probability
of w1...wn and Qp is the backoff weight for the
pruned model.
We make three modifications to WDP in this
formula. First, we follow a suggestion of Stol-
cke (1998) by replacing the discounted training
set count K of w1...wn with an estimate the joint
probability of w1...wn, computed by chaining the
explicit probability estimates, according to our
model, for all N-gram lengths up to n.
The second modification to WDP is that we use
the absolute value of the difference of the log prob-
abilities. By using the signed difference of the log
probabilities, Seymore and Rosenfeld will always
prune a higher-order probability estimate if it is
less than the backoff estimate. But the backoff es-
timate may well be too high. Using the absolute
value of the difference avoids this problem.
</bodyText>
<equation confidence="0.91781175">
�����
�����
749
C(w1...wn)−Dn,C(w1...wn)
C(w1...wn−1)
+ Ow1...wn−1p(wn|w2 ... wn−1) if C(w1 ... wn) &gt; 0
γw1...wn−1p(wn|w2 ... wn−1) if C(w1 ... wn) = 0
p(wn|w1 ... wn−1) = I
αw1...wn−1
Ow1...wn−1 = S |{w0|C(w1...wn−1w0)&gt;0}|
C(w1...wn−1)
αw1...wn−1 = 1 − Ow1...wn−1
</equation>
<figureCaption confidence="0.94761">
Figure 1: New language model smoothing method
</figureCaption>
<bodyText confidence="0.99995012">
The final modification is that we compute the
difference in log probability with respect to the
backoff weight for the pruned model rather than
the unpruned model, which we are able to do by
performing the pruning inside our iterative search
for the value of the backoff weight. We do this
because, if the backoff weight is changed signifi-
cantly by pruning, backoff estimates that meet the
pruning criterion with the old backoff weight may
no longer meet the criterion with the new back-
off weight, and vice versa. Since the new backoff
weight is the one that will be used in the pruned
model, that seems to be the one that should be used
to make pruning decisions.
Our second variant of modified WDP is like the
first, but it estimates p(w1...wn) simply by divid-
ing Seymore and Rosenfeld’s discounted N-gram
count K by the total number of highest-order N-
grams in the training corpus. This is equivalent to
smoothing only the highest-order conditional N-
gram model in estimating p(w1...wn), estimating
all the lower-order probabilities in the chain by the
corresponding MLE model. We refer to this joint
probability estimate as “partially-smoothed”, and
the one suggested by Stolcke as “fully-smoothed”.
</bodyText>
<sectionHeader confidence="0.998548" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.995432666666667">
We carried out three sets of evaluations to test
the new techniques described above. First we
compared the perplexity of full models and mod-
els reduced by significance-based N-gram selec-
tion for seven language model smoothing meth-
ods. For the best three results in that comparison,
we looked at the trade-off between perplexity and
model size over a range of N-gram orders. Finally,
we tried various pruning methods to further reduce
model size, and then compared the best result we
obtained using previous techniques with the best
result we obtained using our new techniques.
</bodyText>
<subsectionHeader confidence="0.925008">
5.1 Data and Base Smoothing Methods
</subsectionHeader>
<bodyText confidence="0.999970818181818">
For training, parameter optimzation, and test data
we used English text from the WMT-06 Europarl
corpus (Koehn and Monz, 2006). We trained on
the designated 1,003,349 sentences (27,493,499
words) of English language model training data,
and used 2000 sentences each for testing and pa-
rameter optimization, from the English half of the
English-French dev and devtest data sets.
We conducted our experiments on seven lan-
guage model smoothing methods. Five of these
are well-known: (1) interpolated absolute dis-
counting with one discount per N-gram length, es-
timated according to the formula derived by Ney
et al. (1994); (2) Katz backoff with Good-Turing
discounts for N-grams occurring 5 times or less;
(3) backoff absolute discounting with Ney et al.
formula discounts; (4) backoff absolute discount-
ing with one discount used for all N-gram lengths,
optimized on held-out data; (5) modified interpo-
lated Kneser-Ney smoothing with three discounts
per N-gram length, estimated according to the for-
mulas suggested by Chen and Goodman (1998).
We also experimented with two variants of a
new smoothing method that we have recently de-
veloped. Full details of the new method are given
elsewhere (Moore and Quirk, 2009), but since it is
not well-known, we summarize the method here.
Smoothed N-gram probabilities are defined by the
formulas shown in Figure 1, for all n such that
N &gt; n &gt; 2,3 where N is the greatest N-gram
length used in the model. The novelty of this
model is that, while it is an interpolated model, the
interpolation weights O for the lower-order model
</bodyText>
<footnote confidence="0.873647">
3For n = 2, we take the expression p(wn|w2 ... wn−1)
to denote a unigram probability estimate p(w2).
</footnote>
<page confidence="0.988234">
750
</page>
<table confidence="0.992076">
Method base select percent
PP PP change
1 interp-AD-fix 62.6 61.6 -1.6
2 Katz backoff 59.8 56.1 -7.9
3 backoff-AD-fix 59.9 54.3 -9.3
4 backoff-AD-opt 58.8 54.4 -7.5
5 KN-mod-fix 51.6 54.6 +5.8
6 new-fix 56.1 52.1 -7.1
7 new-opt 53.7 52.0 -3.3
</table>
<tableCaption confidence="0.999767">
Table 1: Perplexity results for N-gram selection
</tableCaption>
<bodyText confidence="0.99995574">
are not constrained to match the backoff weights
γ for the lower-order model. This allows the in-
terpolation weights to be set independently of the
discounts D, with the backoff weights being ad-
justed to normalize the resulting distributions.
The motivation for this is to let the D para-
meters correct for potential overestimation of the
probabilities for observed N-grams, while the S
parameter (which determines the α and Q interpo-
lation parameters) somewhat independently cor-
rects for quantization errors caused by the fact that
only certain probabilities can be derived from in-
teger observed counts, even after discounting. S is
interpretable as the estimated mean quantization
error for each distinct count for a given context.
We tested two variants of the new method, (6)
one in which the D parameters and the S parameter
are set by fixed criteria, and (7) one in which a sin-
gle value for all D parameters and the value of the
S parameter are optimized on held-out data. For
the fixed value of S, we assume that, since the dis-
tance between possible N-gram counts, after dis-
counting, is approximately 1.0, their mean quan-
tization error would be approximately 0.5. For
the fixed discount parameters, we use three values
for each N-gram length: D1 for N-grams whose
count is 1, D2 for N-grams whose count is 2, and
D3 for N-grams whose count is 3 or more. We
set these values to be the discounts for 1-counts,
2-counts, and 3-counts estimated by the Good-
Turing method. This yields the formula
for 1 &lt; r &lt; 3, where Nr is the number of distinct
N-grams of the length in question occuring r times
in the training set.
In all experiments, the unigram language
model is an un-smoothed, closed-vocabulary MLE
model. We use this unigram model, because there
is no simple, principled way of assigning prob-
abilities to individual out-of-vocabulary (OOV)
words. The only principled solution to this prob-
lem that we are aware of is to use a character-
based model, but this seems overly complicated
for something that is orthogonal to the main points
of this study, and of minor practical importance.
Since we make no provision for OOV words in the
models, OOV words are also omitted from all per-
plexity measurements. Thus, the perplexity num-
bers are systematically lower than they would be
if OOVs were taken into account, but they are all
comparable in this regard.
</bodyText>
<sectionHeader confidence="0.9083705" genericHeader="method">
5.2 Results for Significance-Based N-gram
Selection
</sectionHeader>
<bodyText confidence="0.999967794117647">
Table 1 shows the minimum perplexity (with re-
spect to N-gram order) of language models up to
7-grams for each of the seven smoothing methods
discussed above, with and without significance-
based N-gram selection. N-gram selection im-
proved the perplexity of all models, except for
modified KN. The lowest overall perplexity re-
mains that of the base modified KN method, but
with N-gram selection, the two variants of the new
smoothing method come very close to it.
If we cared only about perplexity, that would be
the end of the story, but we also care about lan-
guage model size. The results in Table 1 were ob-
tained on models estimated using just the counts
needed to cover the parameter optimization and
test sets; so to accurately measure model size, we
trained full language models using base modifed
KN, and the two variants of the new method with
N-gram selection. The resulting sizes of the mod-
els represented in backoff form (in terms of total
number of probability and backoff parameters) are
shown in Figure 2 as function of N-gram length,
from trigrams up to 7-grams for KN and up to
10-grams for the two new models. We see that
beyond 4-grams the model sizes diverge dramati-
cally, with the new models incorporating N-gram
selection leveling off, but the modified KN model
(or any standard model) continuing to grow in size,
apparently linearly in the N-gram order.
In Figure 3, we show the relationship between
perplexity and model size for the same three
models, varying N-gram order. We see that be-
tween about 20 million and 45 million parameters,
both of the new models incorporating significance-
</bodyText>
<equation confidence="0.706405333333333">
Dr = r − (r + 1) ,
Nr
Nr+1
</equation>
<page confidence="0.868917">
751
</page>
<figure confidence="0.514483">
N-gram length
</figure>
<figureCaption confidence="0.9999895">
Figure 2: Model size vs. N-gram length
Figure 3: Perplexity vs. model size
</figureCaption>
<figure confidence="0.979385162162162">
3 4 5 6 7 8 9 10
Millions of model parameters
180
160
140
120
100
80
40
60
20
0
No N-gram
selection
new-opt +
N-gram selection
new-fix +
N-gram selection
Millions of Model Parameters
KN-mod-fix
new-opt +
N-gram selection
new-fix +
N-gram selection
0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160
Perplexity
61
60
59
58
57
56
55
54
53
52
51
</figure>
<bodyText confidence="0.971406666666667">
based N-gram selection seem to outperform mod-
ified KN, and that the best of the three is, in fact,
the new model with fixed parameter values.
</bodyText>
<subsectionHeader confidence="0.534673">
5.3 Results for Additional Pruning
</subsectionHeader>
<bodyText confidence="0.999228454545455">
We further tested modified KN smoothing, and our
new smoothing method with fixed parameter val-
ues and significance-based N-gram selection, with
additional pruning. We compared several pruning
methods on trigram models: count cutoffs, REP,4
and our two modified versions of WDP.
Figure 4 shows the resulting combinations of
perplexity and model size for REP and modified
WDP at various pruning thresholds, and for count
cutoffs of 1, 2, and 3 for both bigrams and trigrams
(n &gt; 1) and for trigrams only (n &gt; 2), applied to
</bodyText>
<footnote confidence="0.931585">
4Thanks to Asela Gunawardana for the use of his REP
tool.
</footnote>
<bodyText confidence="0.9992345">
our new smoothing method with fixed parameter
values, together with significance-based N-gram
selection. Overall, modified WDP with fully-
smoothed joint probability estimates performs the
best. It is has lower perplexity than count cut-
offs at all model sizes tested, and is about equal
to REP at very severe pruning levels and superior
to REP with less pruning. Modified WDP with
fully-smoothed joint probabilities is about equal
to modified WDP with partially-smoothed joint
probabilities at the highest and lowest pruning lev-
els tested, but superior in between.
Figure 4 also shows the result of applying
modified WDP with fully-smoothed joint prob-
abilities to our new smoothing method with-
out significance-based N-gram selection, to test
whether the former subsumes the gains from the
latter. We see that modified WDP does not render
</bodyText>
<page confidence="0.986419">
752
</page>
<figure confidence="0.998266">
Perplexity
73
72
71
70
69
68
67
66
65
64
63
62
61
60
0 1 2 3 4 5 6 7 8 9 10
Millions of Model Parameters
modifed WDP w/fully-
smoothed joint probs,
wo/N-gram selection
modified WDP w/partially-
smoothed joint probs
count cutoffs for n &gt; 1
count cutoffs for n &gt; 2
relative entropy pruning
modified WDP w/fully-
smoothed joint probs
</figure>
<figureCaption confidence="0.999983">
Figure 4: Pruning methods for new smoothing technique with N-gram selection
Figure 5: Pruning methods for modified KN smoothing
</figureCaption>
<figure confidence="0.99785547826087">
Perplexity
72
71
70
69
68
67
66
65
64
63
62
61
60
0 1 2 3 4 5 6 7 8 9 10 11
Millions of Model Parameters
modified WDP w/fully-
smoothed joint probs
relative entropy pruning
modified WDP w/partially-
smoothed joint probs
count cutoffs for n &gt; 1
count cutoffs for n &gt; 2
</figure>
<bodyText confidence="0.998801805555556">
N-gram selection redundant except at very severe
pruning levels, much like REP.
Figure 5 shows the results of applying the
same four pruning methods to KN smoothing.
Count cutoffs clearly perform the best with KN
smoothing. It is interesting to note, however,
that—contrary to the results for our new smooth-
ing method—with KN smoothing, modified WDP
with partially-smoothed joint probabilities is sig-
nificantly better than either REP or modified WDP
with fully-smoothed joint probabilities. We be-
lieve this is due to the fact that the latter two meth-
ods both estimate the joint probabilities by chain-
ing the lower-order conditional probabilities from
the fully-smoothed model, which in the case of
KN smoothing are designed specifically to cover
N-grams that have not been observed, and are poor
estimates for the probabilities of lower-order N-
grams that do occcur in the training data.
Finally, we compared the new smoothing
method with N-gram selection and modified WDP
with fully-smoothed joint probabilities against
modified KN smoothing with count cutoffs, us-
ing combinations of pruning parameter values and
N-gram order that yielded the best size/perplexity
trade-offs. The results are shown in Figure 6. At
all model sizes within the range of these experi-
ments, the new method with significance-based N-
gram selection and modified WDP had lower per-
plexity than modifed KN with count cutoffs—up
to about 8% lower at greater pruning levels.
This experiment also suggests that the
size/perplexity trade-off is easier to optimize
for our new combination of smoothing, N-gram
selection, and modified WDP, than for KN
smoothing with count cut-offs. Table 2 shows the
</bodyText>
<page confidence="0.99414">
753
</page>
<figure confidence="0.994212272727273">
Perplexity 71 KN-mod-fix with
70 count cutoffs
69
68
67
66
65
64
63
62
61 new-fix with N-gram
60 selection and
59 modified WDP
58
57
56
55
54
53
52
0 1 2 3 4 5 6 7 8 9 10 11 12 13
Millions of Model Parameters
</figure>
<figureCaption confidence="0.999324">
Figure 6: Comparison of two best pruned language models
</figureCaption>
<figure confidence="0.9620236">
PP N CC n &gt;
69.9 3 4 1
64.7 4 4 1
62.1 4 3 1
59.0 4 2 1
</figure>
<bodyText confidence="0.963582739130435">
56.5 4 2 2
54.4 4 1 2
53.6 5 1 2
53.4 6 1 2
53.3 7 1 2
Table 2: Optimal pruning parameters for KN-
mod-fix with count cutoffs
perplexity (PP), maximum N-gram length (N) ,
count cutoff (CC), and N-gram lengths to which
the count cutoffs are applied (n &gt;) for the points
on the curve for pruned KN in Figure 6. Although
some tendencies are discernable, it seems clear
that a significant part of the space of combinations
of N, CC, and “n &gt;” parameter values must be
searched to find the best points for trading off
perplexity against model size. Table 3 shows
maximum N-gram length and pruning threshold
values for the points on the corresponding curve
for our new approach. Here the situation is much
simpler. The best trade-off points are found by
varying the pruning threshold, and including
in the model all N-grams that pass the pruning
threshold, regardless of N-gram length.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9868845">
We have shown that significance-based N-gram
selection can simultaneously reduce both model
</bodyText>
<figure confidence="0.83567525">
PP N threshold
67.2 10 10−6.5
62.7 10 10−6.75
59.3 10 10−7.0
56.4 10 10−7.25
54.6 10 10−7.5
53.7 10 10−7.75
53.2 10 10−8.0
</figure>
<tableCaption confidence="0.7771885">
Table 3: Optimal pruning parameters for new-fix
with N-gram selection and modified WDP
</tableCaption>
<bodyText confidence="0.999791857142857">
size and perplexity when applied to a number of
language model smoothing methods, including the
widely-used Katz backoff and absolute discount-
ing methods. We are not aware of any other tech-
nique that does this. We also found that, when
combined with a new smoothing method and a
novel variant of weighted difference pruning, our
N-gram selection method outperformed modified
Kneser-Ney smoothing—using the best form of
pruning we found for that approach—with respect
to the trade-off between model size and model
quality.
As our next steps, first, we need to verify that
the results obtained on a moderate-sized train-
ing corpus are repeatable on much larger corpora.
Second, we plan to extend this work to incorpo-
rate language model size reduction by word clus-
tering, which has been shown by Goodman and
Gao (2000) to produce additional gains when com-
bined with previous methods of language model
pruning.
</bodyText>
<page confidence="0.99808">
754
</page>
<sectionHeader confidence="0.993905" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999553809523809">
Brants, Thorsten, Ashok C. Popat, Peng Xu, Franz
J. Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings
ofEMNLP 2007, 858–867.
Chen, Stanley F., and Joshua Goodman. 1998.
An empirical study of smoothing techniques for
language modeling. Technical Report TR-10-
98, Harvard University.
Goodman, Joshua, and Jianfeng Gao. 2000. Lan-
guage model size reduction by pruning and
clustering. In Proceedings ofICSLP 2000, 110–
113.
Jedynak, Bruno M., and Sanjeev Khudanpur.
2005. Maximum likelihood set for estimating a
probability mass function. Neural Computation
17, 1–23.
Koehn, Philipp, and Christof Monz. 2006. Manual
and automatic evaluation of machine translation
between European languages. In Proceedings
of WMT 2006, 102–121.
Moore, Robert C., and Chris Quirk. 2009. Im-
proved smoothing for N-gram language mod-
els based on ordinary counts. In Proceedings
ofACL-IJCNLP 2009.
Ney, Hermann, Ute Essen, and Reinhard Kneser.
1994. On structuring probabilistic dependen-
cies in stochastic language modelling. Com-
puter Speech and Language, 8, 1–38.
Ron, Dana, Yoram Singer, and Naftali Tishby.
1996. The power of amnesia: learning proba-
bilistic automata with variable memory length.
Machine Learning, 25, 117–149.
Rosenfeld, Ronald, and Xuedong Huang. 1993.
Improvements in stochastic language modeling.
In Proceedings ofHLT 1993, 107–111.
Seymore, Kristie, and Ronald Rosenfeld. 1996.
Scalable Trigram Backoff Language Models. In
Proceedings ofICSLP 1996. 232–235.
Stolcke, Andreas. 1998. Entropy-based pruning
of backoff language models. In Proceedings,
DARPA News Transcription and Understanding
Workshop 1998, 270–274.
</reference>
<page confidence="0.998705">
755
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.768014">
<title confidence="0.988826">Less is More: Significance-Based N-gram for Smaller, Better Language Models</title>
<author confidence="0.999655">Robert C Moore Chris</author>
<affiliation confidence="0.95243">Microsoft</affiliation>
<address confidence="0.998529">Redmond, WA 98052,</address>
<abstract confidence="0.9912183">The recent availability of large corpora for training N-gram language models has shown the utility of models of higher order than just trigrams. In this paper, we investigate methods to control the increase in model size resulting from applying standard methods at higher orders. We introduce significance-based N-gram selection, which not only reduces model size, but also improves perplexity for several smoothing methods, including Katz backoff and absolute discounting. We also show that, when combined with a new smoothing method and a novel variant of weighted-difference pruning, our selection method performs better in the trade-offbetween model size and perplexity than the best pruning method we found for modified Kneser-Ney smoothing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofEMNLP 2007,</booktitle>
<pages>858--867</pages>
<contexts>
<context position="1940" citStr="Brants et al., 2007" startWordPosition="291" endWordPosition="294">robability of each word in a text string based on the N −1 preceding words, remain the most widely-used type of model. Until the late 1990s, N-gram language models of order higher than trigrams were seldom used. This was due, at least in part, to the fact the amounts of training data available did not produce significantly better results from higher-order models. Since that time, however, increasingly large amounts of language model training data have become available ranging from approximately one billion words (the Gigaword corpora from the Linguistic Data Consortium) to trillions of words (Brants et al., 2007). With these larger resources, the use of language models based on 5-grams to 7-grams is becoming increasingly common. The problem we address here is that, even when relatively modest amounts of training data are used, high-order N-gram language models estimated by standard techniques can be impractically large. Hence, we investigate ways of building high-order N-gram language models without dramatically increasing model size. This is, of course, the same goal behind much previous work on language model pruning, including that of Seymore and Rosenfeld (1996), Stolcke (1998), and Goodman and Ga</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Brants, Thorsten, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings ofEMNLP 2007, 858–867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="3709" citStr="Chen and Goodman (1998)" startWordPosition="561" endWordPosition="564">sed smoothing methods, including Katz backoff and several variants of absolute discounting.1 In contrast, experiments applying previous pruning methods to Katz backoff (Seymore and Rosenfeld, 1996; Stolcke, 1998) and absolute discounting (Goodman and Gao, 2000) always found the lowest perplexity model to be the unpruned model. We tested significance-based selection on only one smoothing method without obtaining improved perplexity: modified Kneser-Ney (KN) 1For most of the standard smoothing methods mentioned here, we refer the reader to the excellent comparative study of smoothing methods by Chen and Goodman (1998). References to the original sources may be found there. 746 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 746–755, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP smoothing (Chen and Goodman, 1998). This is unfortunate, because modified KN smoothing generally seems to have the lowest perplexity of any known smoothing method for N-gram language models; in our tests it had a lower perplexity than any of the other models, with or without significance-based N-gram selection. However, when we compared modified KN smoothing to our best results applyi</context>
<context position="20905" citStr="Chen and Goodman (1998)" startWordPosition="3445" endWordPosition="3448"> language model smoothing methods. Five of these are well-known: (1) interpolated absolute discounting with one discount per N-gram length, estimated according to the formula derived by Ney et al. (1994); (2) Katz backoff with Good-Turing discounts for N-grams occurring 5 times or less; (3) backoff absolute discounting with Ney et al. formula discounts; (4) backoff absolute discounting with one discount used for all N-gram lengths, optimized on held-out data; (5) modified interpolated Kneser-Ney smoothing with three discounts per N-gram length, estimated according to the formulas suggested by Chen and Goodman (1998). We also experimented with two variants of a new smoothing method that we have recently developed. Full details of the new method are given elsewhere (Moore and Quirk, 2009), but since it is not well-known, we summarize the method here. Smoothed N-gram probabilities are defined by the formulas shown in Figure 1, for all n such that N &gt; n &gt; 2,3 where N is the greatest N-gram length used in the model. The novelty of this model is that, while it is an interpolated model, the interpolation weights O for the lower-order model 3For n = 2, we take the expression p(wn|w2 ... wn−1) to denote a unigram</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Chen, Stanley F., and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
<author>Jianfeng Gao</author>
</authors>
<title>Language model size reduction by pruning and clustering.</title>
<date>2000</date>
<booktitle>In Proceedings ofICSLP</booktitle>
<pages>110--113</pages>
<contexts>
<context position="2548" citStr="Goodman and Gao (2000)" startWordPosition="385" endWordPosition="389"> et al., 2007). With these larger resources, the use of language models based on 5-grams to 7-grams is becoming increasingly common. The problem we address here is that, even when relatively modest amounts of training data are used, high-order N-gram language models estimated by standard techniques can be impractically large. Hence, we investigate ways of building high-order N-gram language models without dramatically increasing model size. This is, of course, the same goal behind much previous work on language model pruning, including that of Seymore and Rosenfeld (1996), Stolcke (1998), and Goodman and Gao (2000). We take a novel approach, however, which we refer to as significance-based N-gram selection. We reject a higher-order estimate of the probability of a particular word in a particular context whenever the distribution of observations for the higher-order estimate provides no evidence that the higher-order estimate is better than our backoff estimate. Perhaps our most surprising result is that significance-based N-gram selection not only reduces language model size, but it also improves perplexity when applied to a number of widelyused smoothing methods, including Katz backoff and several vari</context>
</contexts>
<marker>Goodman, Gao, 2000</marker>
<rawString>Goodman, Joshua, and Jianfeng Gao. 2000. Language model size reduction by pruning and clustering. In Proceedings ofICSLP 2000, 110– 113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno M Jedynak</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Maximum likelihood set for estimating a probability mass function.</title>
<date>2005</date>
<journal>Neural Computation</journal>
<volume>17</volume>
<pages>1--23</pages>
<contexts>
<context position="6322" citStr="Jedynak and Khudanpur (2005)" startWordPosition="989" endWordPosition="992"> the same metric in his work on language model pruning, and he also pointed out that weighted difference pruning is, in fact, an approximation of relative entropy pruning. However, while relative entropy pruning is based on a statistical test, it is not a significance test. The difference in probability represented by a certain relative entropy value can be statistically significant when measured on a large corpus, but not significant when measured on a small corpus. The primary test we use to choose between higher- or lower-order estimates of an N-gram probablity is inspired by an insight of Jedynak and Khudanpur (2005). They note that, given a set of y observations of a multinomial distribution, the observed counts will have the highest probabilty of any possible set of y observations for the maximum likelihood estimate (MLE) model derived from the relative frequencies of those observations. In general, however, the MLE model will not be the only model for which this set of observations is the most probable set of y observations. Jedynak and Khudanpur call the set of such models the maximum likelihood set (MLS) for the observations. Jedynak and Khudanpur argue that the observations alone do not support choo</context>
</contexts>
<marker>Jedynak, Khudanpur, 2005</marker>
<rawString>Jedynak, Bruno M., and Sanjeev Khudanpur. 2005. Maximum likelihood set for estimating a probability mass function. Neural Computation 17, 1–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
</authors>
<title>Manual and automatic evaluation of machine translation between European languages.</title>
<date>2006</date>
<booktitle>In Proceedings of WMT 2006,</booktitle>
<pages>102--121</pages>
<contexts>
<context position="19994" citStr="Koehn and Monz, 2006" startWordPosition="3304" endWordPosition="3307">of full models and models reduced by significance-based N-gram selection for seven language model smoothing methods. For the best three results in that comparison, we looked at the trade-off between perplexity and model size over a range of N-gram orders. Finally, we tried various pruning methods to further reduce model size, and then compared the best result we obtained using previous techniques with the best result we obtained using our new techniques. 5.1 Data and Base Smoothing Methods For training, parameter optimzation, and test data we used English text from the WMT-06 Europarl corpus (Koehn and Monz, 2006). We trained on the designated 1,003,349 sentences (27,493,499 words) of English language model training data, and used 2000 sentences each for testing and parameter optimization, from the English half of the English-French dev and devtest data sets. We conducted our experiments on seven language model smoothing methods. Five of these are well-known: (1) interpolated absolute discounting with one discount per N-gram length, estimated according to the formula derived by Ney et al. (1994); (2) Katz backoff with Good-Turing discounts for N-grams occurring 5 times or less; (3) backoff absolute dis</context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>Koehn, Philipp, and Christof Monz. 2006. Manual and automatic evaluation of machine translation between European languages. In Proceedings of WMT 2006, 102–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>Chris Quirk</author>
</authors>
<title>Improved smoothing for N-gram language models based on ordinary counts.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL-IJCNLP</booktitle>
<contexts>
<context position="21079" citStr="Moore and Quirk, 2009" startWordPosition="3475" endWordPosition="3478">derived by Ney et al. (1994); (2) Katz backoff with Good-Turing discounts for N-grams occurring 5 times or less; (3) backoff absolute discounting with Ney et al. formula discounts; (4) backoff absolute discounting with one discount used for all N-gram lengths, optimized on held-out data; (5) modified interpolated Kneser-Ney smoothing with three discounts per N-gram length, estimated according to the formulas suggested by Chen and Goodman (1998). We also experimented with two variants of a new smoothing method that we have recently developed. Full details of the new method are given elsewhere (Moore and Quirk, 2009), but since it is not well-known, we summarize the method here. Smoothed N-gram probabilities are defined by the formulas shown in Figure 1, for all n such that N &gt; n &gt; 2,3 where N is the greatest N-gram length used in the model. The novelty of this model is that, while it is an interpolated model, the interpolation weights O for the lower-order model 3For n = 2, we take the expression p(wn|w2 ... wn−1) to denote a unigram probability estimate p(w2). 750 Method base select percent PP PP change 1 interp-AD-fix 62.6 61.6 -1.6 2 Katz backoff 59.8 56.1 -7.9 3 backoff-AD-fix 59.9 54.3 -9.3 4 backof</context>
</contexts>
<marker>Moore, Quirk, 2009</marker>
<rawString>Moore, Robert C., and Chris Quirk. 2009. Improved smoothing for N-gram language models based on ordinary counts. In Proceedings ofACL-IJCNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Ney</author>
<author>Ute Essen</author>
<author>Reinhard Kneser</author>
</authors>
<title>On structuring probabilistic dependencies in stochastic language modelling.</title>
<date>1994</date>
<journal>Computer Speech and Language,</journal>
<volume>8</volume>
<pages>1--38</pages>
<contexts>
<context position="20485" citStr="Ney et al. (1994)" startWordPosition="3381" endWordPosition="3384">For training, parameter optimzation, and test data we used English text from the WMT-06 Europarl corpus (Koehn and Monz, 2006). We trained on the designated 1,003,349 sentences (27,493,499 words) of English language model training data, and used 2000 sentences each for testing and parameter optimization, from the English half of the English-French dev and devtest data sets. We conducted our experiments on seven language model smoothing methods. Five of these are well-known: (1) interpolated absolute discounting with one discount per N-gram length, estimated according to the formula derived by Ney et al. (1994); (2) Katz backoff with Good-Turing discounts for N-grams occurring 5 times or less; (3) backoff absolute discounting with Ney et al. formula discounts; (4) backoff absolute discounting with one discount used for all N-gram lengths, optimized on held-out data; (5) modified interpolated Kneser-Ney smoothing with three discounts per N-gram length, estimated according to the formulas suggested by Chen and Goodman (1998). We also experimented with two variants of a new smoothing method that we have recently developed. Full details of the new method are given elsewhere (Moore and Quirk, 2009), but </context>
</contexts>
<marker>Ney, Essen, Kneser, 1994</marker>
<rawString>Ney, Hermann, Ute Essen, and Reinhard Kneser. 1994. On structuring probabilistic dependencies in stochastic language modelling. Computer Speech and Language, 8, 1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dana Ron</author>
<author>Yoram Singer</author>
<author>Naftali Tishby</author>
</authors>
<title>The power of amnesia: learning probabilistic automata with variable memory length.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<volume>25</volume>
<pages>117--149</pages>
<contexts>
<context position="5483" citStr="Ron, et al. (1996)" startWordPosition="851" endWordPosition="854">ing (Seymore and Rosenfeld, 1996) that was especially convenient to compute given our method for performing significance-based N-gram selection. The final result is that our best model using significance-based selection and modified weighted difference pruning always had a better size/perplexity trade-off than pruned modified KN, with up to about 8% perplexity reduction for a given model size. 2 Significance-Based N-gram Selection The idea of using a statistical test to decide whether to use a higher- or lower-order estimate of an N-gram probablity is not new. It was perhaps first proposed by Ron, et al. (1996), who suggested using a threshold on relative entropy (KullbackLiebler divergence) as an appropriate test to decide whether to extend the context used to predict the next token in a sequence. Stolcke (1998) used the same metric in his work on language model pruning, and he also pointed out that weighted difference pruning is, in fact, an approximation of relative entropy pruning. However, while relative entropy pruning is based on a statistical test, it is not a significance test. The difference in probability represented by a certain relative entropy value can be statistically significant whe</context>
</contexts>
<marker>Ron, Singer, Tishby, 1996</marker>
<rawString>Ron, Dana, Yoram Singer, and Naftali Tishby. 1996. The power of amnesia: learning probabilistic automata with variable memory length. Machine Learning, 25, 117–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
<author>Xuedong Huang</author>
</authors>
<title>Improvements in stochastic language modeling.</title>
<date>1993</date>
<booktitle>In Proceedings ofHLT 1993,</booktitle>
<pages>107--111</pages>
<marker>Rosenfeld, Huang, 1993</marker>
<rawString>Rosenfeld, Ronald, and Xuedong Huang. 1993. Improvements in stochastic language modeling. In Proceedings ofHLT 1993, 107–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristie Seymore</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Scalable Trigram Backoff Language Models.</title>
<date>1996</date>
<booktitle>In Proceedings ofICSLP</booktitle>
<pages>232--235</pages>
<contexts>
<context position="2504" citStr="Seymore and Rosenfeld (1996)" startWordPosition="378" endWordPosition="381">tic Data Consortium) to trillions of words (Brants et al., 2007). With these larger resources, the use of language models based on 5-grams to 7-grams is becoming increasingly common. The problem we address here is that, even when relatively modest amounts of training data are used, high-order N-gram language models estimated by standard techniques can be impractically large. Hence, we investigate ways of building high-order N-gram language models without dramatically increasing model size. This is, of course, the same goal behind much previous work on language model pruning, including that of Seymore and Rosenfeld (1996), Stolcke (1998), and Goodman and Gao (2000). We take a novel approach, however, which we refer to as significance-based N-gram selection. We reject a higher-order estimate of the probability of a particular word in a particular context whenever the distribution of observations for the higher-order estimate provides no evidence that the higher-order estimate is better than our backoff estimate. Perhaps our most surprising result is that significance-based N-gram selection not only reduces language model size, but it also improves perplexity when applied to a number of widelyused smoothing meth</context>
<context position="4898" citStr="Seymore and Rosenfeld, 1996" startWordPosition="759" endWordPosition="762">KN smoothing to our best results applying N-gram selection to other smoothing methods for multiple N-gram orders, two of our models outperformed modified KN in terms of perplexity for a given model size. Of course, the trade-off between perplexity and model size for modified KN can also be improved by pruning. So, in a final set of experiments we found the best combinations we could for pruned modified KN models, and we did the same for our best model using significancebased selection. The best pruning method for the latter turned out to be a novel modification of weighted-difference pruning (Seymore and Rosenfeld, 1996) that was especially convenient to compute given our method for performing significance-based N-gram selection. The final result is that our best model using significance-based selection and modified weighted difference pruning always had a better size/perplexity trade-off than pruned modified KN, with up to about 8% perplexity reduction for a given model size. 2 Significance-Based N-gram Selection The idea of using a statistical test to decide whether to use a higher- or lower-order estimate of an N-gram probablity is not new. It was perhaps first proposed by Ron, et al. (1996), who suggested</context>
</contexts>
<marker>Seymore, Rosenfeld, 1996</marker>
<rawString>Seymore, Kristie, and Ronald Rosenfeld. 1996. Scalable Trigram Backoff Language Models. In Proceedings ofICSLP 1996. 232–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Entropy-based pruning of backoff language models.</title>
<date>1998</date>
<booktitle>In Proceedings, DARPA News Transcription and Understanding Workshop</booktitle>
<pages>270--274</pages>
<contexts>
<context position="2520" citStr="Stolcke (1998)" startWordPosition="382" endWordPosition="383">ons of words (Brants et al., 2007). With these larger resources, the use of language models based on 5-grams to 7-grams is becoming increasingly common. The problem we address here is that, even when relatively modest amounts of training data are used, high-order N-gram language models estimated by standard techniques can be impractically large. Hence, we investigate ways of building high-order N-gram language models without dramatically increasing model size. This is, of course, the same goal behind much previous work on language model pruning, including that of Seymore and Rosenfeld (1996), Stolcke (1998), and Goodman and Gao (2000). We take a novel approach, however, which we refer to as significance-based N-gram selection. We reject a higher-order estimate of the probability of a particular word in a particular context whenever the distribution of observations for the higher-order estimate provides no evidence that the higher-order estimate is better than our backoff estimate. Perhaps our most surprising result is that significance-based N-gram selection not only reduces language model size, but it also improves perplexity when applied to a number of widelyused smoothing methods, including K</context>
<context position="5689" citStr="Stolcke (1998)" startWordPosition="887" endWordPosition="888">selection and modified weighted difference pruning always had a better size/perplexity trade-off than pruned modified KN, with up to about 8% perplexity reduction for a given model size. 2 Significance-Based N-gram Selection The idea of using a statistical test to decide whether to use a higher- or lower-order estimate of an N-gram probablity is not new. It was perhaps first proposed by Ron, et al. (1996), who suggested using a threshold on relative entropy (KullbackLiebler divergence) as an appropriate test to decide whether to extend the context used to predict the next token in a sequence. Stolcke (1998) used the same metric in his work on language model pruning, and he also pointed out that weighted difference pruning is, in fact, an approximation of relative entropy pruning. However, while relative entropy pruning is based on a statistical test, it is not a significance test. The difference in probability represented by a certain relative entropy value can be statistically significant when measured on a large corpus, but not significant when measured on a small corpus. The primary test we use to choose between higher- or lower-order estimates of an N-gram probablity is inspired by an insigh</context>
<context position="16285" citStr="Stolcke, 1998" startWordPosition="2705" endWordPosition="2706">rs in a high-order language model, we may wish to reduce language model size even more. The concept of significance-based N-gram selection to produce smaller models could be extended by relaxing our criterion for using backoff distributions in place of explicit higher-order probability estimates, but true significance tests at more relaxed thresholds that are accurate for small counts are expensive to compute; so we resort to more conventional language model pruning methods. In our experiments, we tried four methods for additional pruning: simple count cutoffs, relative entropy pruning (REP) (Stolcke, 1998), and two modified versions of Seymore and Rosenfeld’s (1996) weighted-difference pruning (WDP). In the notation we have been using, Seymore and Rosenfeld’s WDP criterion for using a backoff estimate, in place of an explicit higher-order estimate, is that the quantity K× � log(p(wn|w1...wn−1))− log(Qu p(wn|w2...wn−1)) be less than a pruning threshold, where K is the Good-Turing-discounted training set count for w1...wn, and Qu is the backoff weight for the unpruned model. The first of our modified version of WDP uses the following quantity instead: p(w1...wn)× log(p(wn|w1...wn−1))− log(Qp p(wn</context>
</contexts>
<marker>Stolcke, 1998</marker>
<rawString>Stolcke, Andreas. 1998. Entropy-based pruning of backoff language models. In Proceedings, DARPA News Transcription and Understanding Workshop 1998, 270–274.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>