<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.999231">
An Entity-Mention Model for Coreference Resolution
with Inductive Logic Programming
</title>
<author confidence="0.9649005">
Xiaofeng Yang1 Jian Su1 Jun Lang2
Chew Lim Tan3 Ting Liu2 Sheng Li2
</author>
<affiliation confidence="0.9935">
1Institute for Infocomm Research 2Harbin Institute of Technology
</affiliation>
<email confidence="0.921074">
{xiaofengy,sujian}@i2r.a-star.edu.sg {bill lang,tliu}@ir.hit.edu.cn
lisheng@hit.edu.cn
</email>
<affiliation confidence="0.974245">
3National University of Singapore,
</affiliation>
<email confidence="0.994705">
tancl@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.995598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999634882352941">
The traditional mention-pair model for coref-
erence resolution cannot capture information
beyond mention pairs for both learning and
testing. To deal with this problem, we present
an expressive entity-mention model that per-
forms coreference resolution at an entity level.
The model adopts the Inductive Logic Pro-
gramming (ILP) algorithm, which provides a
relational way to organize different knowledge
of entities and mentions. The solution can
explicitly express relations between an entity
and the contained mentions, and automatically
learn first-order rules important for corefer-
ence decision. The evaluation on the ACE data
set shows that the ILP based entity-mention
model is effective for the coreference resolu-
tion task.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999750979166667">
Coreference resolution is the process of linking mul-
tiple mentions that refer to the same entity. Most
of previous work adopts the mention-pair model,
which recasts coreference resolution to a binary
classification problem of determining whether or not
two mentions in a document are co-referring (e.g.
Aone and Bennett (1995); McCarthy and Lehnert
(1995); Soon et al. (2001); Ng and Cardie (2002)).
Although having achieved reasonable success, the
mention-pair model has a limitation that informa-
tion beyond mention pairs is ignored for training and
testing. As an individual mention usually lacks ad-
equate descriptive information of the referred entity,
it is often difficult to judge whether or not two men-
tions are talking about the same entity simply from
the pair alone.
An alternative learning model that can overcome
this problem performs coreference resolution based
on entity-mention pairs (Luo et al., 2004; Yang et
al., 2004b). Compared with the traditional mention-
pair counterpart, the entity-mention model aims to
make coreference decision at an entity level. Classi-
fication is done to determine whether a mention is a
referent of a partially found entity. A mention to be
resolved (called active mention henceforth) is linked
to an appropriate entity chain (if any), based on clas-
sification results.
One problem that arises with the entity-mention
model is how to represent the knowledge related to
an entity. In a document, an entity may have more
than one mention. It is impractical to enumerate all
the mentions in an entity and record their informa-
tion in a single feature vector, as it would make the
feature space too large. Even worse, the number of
mentions in an entity is not fixed, which would re-
sult in variant-length feature vectors and make trou-
ble for normal machine learning algorithms. A solu-
tion seen in previous work (Luo et al., 2004; Culotta
et al., 2007) is to design a set of first-order features
summarizing the information of the mentions in an
entity, for example, “whether the entity has any men-
tion that is a name alias of the active mention?” or
“whether most of the mentions in the entity have the
same head word as the active mention?” These fea-
tures, nevertheless, are designed in an ad-hoc man-
ner and lack the capability of describing each indi-
vidual mention in an entity.
In this paper, we present a more expressive entity-
</bodyText>
<page confidence="0.986032">
843
</page>
<note confidence="0.696395">
Proceedings ofACL-08: HLT, pages 843–851,
</note>
<page confidence="0.494656">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999860846153846">
mention model for coreference resolution. The
model employs Inductive Logic Programming (ILP)
to represent the relational knowledge of an active
mention, an entity, and the mentions in the entity. On
top of this, a set of first-order rules is automatically
learned, which can capture the information of each
individual mention in an entity, as well as the global
information of the entity, to make coreference deci-
sion. Hence, our model has a more powerful repre-
sentation capability than the traditional mention-pair
or entity-mention model. And our experimental re-
sults on the ACE data set shows the model is effec-
tive for coreference resolution.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999904542372881">
There are plenty of learning-based coreference reso-
lution systems that employ the mention-pair model.
A typical one of them is presented by Soon et al.
(2001). In the system, a training or testing instance
is formed for two mentions in question, with a fea-
ture vector describing their properties and relation-
ships. At a testing time, an active mention is checked
against all its preceding mentions, and is linked with
the closest one that is classified as positive. The
work is further enhanced by Ng and Cardie (2002)
by expanding the feature set and adopting a “best-
first” linking strategy.
Recent years have seen some work on the entity-
mention model. Luo et al. (2004) propose a system
that performs coreference resolution by doing search
in a large space of entities. They train a classifier that
can determine the likelihood that an active mention
should belong to an entity. The entity-level features
are calculated with an “Any-X” strategy: an entity-
mention pair would be assigned a feature X, if any
mention in the entity has the feature X with the ac-
tive mention.
Culotta et al. (2007) present a system which uses
an online learning approach to train a classifier to
judge whether two entities are coreferential or not.
The features describing the relationships between
two entities are obtained based on the information
of every possible pair of mentions from the two en-
tities. Different from (Luo et al., 2004), the entity-
level features are computed using a “Most-X” strat-
egy, that is, two given entities would have a feature
X, if most of the mention pairs from the two entities
have the feature X.
Yang et al. (2004b) suggest an entity-based coref-
erence resolution system. The model adopted in the
system is similar to the mention-pair model, except
that the entity information (e.g., the global num-
ber/gender agreement) is considered as additional
features of a mention in the entity.
McCallum and Wellner (2003) propose several
graphical models for coreference analysis. These
models aim to overcome the limitation that pair-
wise coreference decisions are made independently
of each other. The simplest model conditions coref-
erence on mention pairs, but enforces dependency
by calculating the distance of a node to a partition
(i.e., the probability that an active mention belongs
to an entity) based on the sum of its distances to all
the nodes in the partition (i.e., the sum of the prob-
ability of the active mention co-referring with the
mentions in the entity).
Inductive Logic Programming (ILP) has been ap-
plied to some natural language processing tasks, in-
cluding parsing (Mooney, 1997), POS disambigua-
tion (Cussens, 1996), lexicon construction (Claveau
et al., 2003), WSD (Specia et al., 2007), and so on.
However, to our knowledge, our work is the first ef-
fort to adopt this technique for the coreference reso-
lution task.
</bodyText>
<sectionHeader confidence="0.95284" genericHeader="method">
3 Modelling Coreference Resolution
</sectionHeader>
<bodyText confidence="0.9994435">
Suppose we have a document containing n mentions
{mj : 1 &lt; j &lt; n1, in which mj is the jth mention
occurring in the document. Let eZ be the ith entity in
the document. We define
</bodyText>
<equation confidence="0.92121">
PIL|eZ, mj), (1)
</equation>
<bodyText confidence="0.999902666666667">
the probability that a mention belongs to an entity.
Here the random variable L takes a binary value and
is 1 if mj is a mention of eZ.
By assuming that mentions occurring after mj
have no influence on the decision of linking mj to
an entity, we can approximate (1) as:
</bodyText>
<equation confidence="0.977897">
PIL|eZ, mj)
a PIL|{mk E eZ,1 &lt; k &lt; j − 11, mj) (2)
PIL|mk, mj) (3)
</equation>
<listItem confidence="0.482091">
(3) further assumes that an entity-mention score
can be computed by using the maximum mention-
</listItem>
<figure confidence="0.578418">
a max
</figure>
<page confidence="0.784398">
MkEei,1&lt;k&lt;j−1
844
</page>
<figureCaption confidence="0.8963199">
[Microsoft Corp. ]i announced [ [ its ]2 new CEO ]3 encountered mention mj, a test instance is formed
[yesterday ]4. [ The company ]5 said [ he ]s will .. . for each preceding mention, mk. This instance is
presented to the classifier to determine the corefer-
ence relationship. mj is linked with the mention that
is classified as positive (if any) with the highest con-
fidence value.
Table 1: A sample text
pair score. Both (2) and (1) can be approximated 3.2 Entity-Mention Model
with a machine learning method, leading to the tra- The mention-based solution has a limitation that in-
ditional mention-pair model and the entity-mention formation beyond a mention pair cannot be captured.
</figureCaption>
<bodyText confidence="0.966458833333333">
model for coreference resolution, respectively. As an individual mention usually lacks complete de-
The two models will be described in the next sub- scription about the referred entity, the coreference
sections, with the sample text in Table 1 used for relationship between two mentions may be not clear,
demonstration. In the table, a mention m is high- which would affect classifier learning. Consider
lighted as [ m ]eidmid, where mid and eid are the IDs a document with three coreferential mentions “Mr.
for the mention and the entity to which it belongs, Powell”, “he”, and “Powell”, appearing in that or-
respectively. Three entity chains can be found in the der. The positive training instance i(“he”, “Powell”)
text, that is, is not informative, as the pronoun “he” itself dis-
e1 : Microsoft Corp. - its - The company closes nothing but the gender. However, if the whole
e2 : its new CEO - he entity is considered instead of only one mention, we
e3 : yesterday can know that “he” refers to a male person named
“Powell”. And consequently, the coreference rela-
tionships between the mentions would become more
obvious.
The mention-pair model would also cause errors
at a testing time. Suppose we have three mentions
“Mr. Powell”, “Powell”, and “she” in a document.
The model tends to link “she” with “Powell” be-
cause of their proximity. This error can be avoided,
if we know “Powell” belongs to the entity starting
with “Mr. Powell”, and therefore refers to a male
person and cannot co-refer with “she”.
The entity-mention model based on Eq. (2) per-
forms coreference resolution at an entity-level. For
simplicity, the framework considered for the entity-
mention model adopts similar training and testing
procedures as for the mention-pair model. Specif-
ically, a training or testing instance has the form of
i{ei, mj}, in which mj is an active mention and ei
is a partial entity found before mj. During train-
ing, given each anaphoric mention mj, one single
positive training instance is created for the entity to
which mj belongs. And a group of negative train-
ing instances is created for every partial entity whose
last mention occurs between mj and the closest an-
tecedent of mj.
See the sample in Table 1 again. For the pronoun
“he”, the following three instances are generated for
3.1 Mention-Pair Model
As a baseline, we first describe a learning framework
with the mention-pair model as adopted in the work
by Soon et al. (2001) and Ng and Cardie (2002).
In the learning framework, a training or testing
instance has the form of i{mk, mj}, in which mj is
an active mention and mk is a preceding mention.
An instance is associated with a vector of features,
which is used to describe the properties of the two
mentions as well as their relationships. Table 2 sum-
marizes the features used in our study.
For training, given each encountered anaphoric
mention mj in a document, one single positive train-
ing instance is created for mj and its closest an-
tecedent. And a group of negative training in-
stances is created for every intervening mentions
between mj and the antecedent. Consider the ex-
ample text in Table 1, for the pronoun “he”, three
instances are generated: i(“The company”,“he”),
i(“yesterday”,“he”), and i(“its new CEO”,“he”).
Among them, the first two are labelled as negative
while the last one is labelled as positive.
Based on the training instances, a binary classifier
can be generated using any discriminative learning
algorithm. During resolution, an input document is
processed from the first mention to the last. For each
845
Features describing an active mention, mj
</bodyText>
<equation confidence="0.8499025">
defNP mj 1 if mj is a definite description; else 0
indefNP mj 1 if mj is an indefinite NP; else 0
nameNP mj 1 if mj is a named-entity; else 0
pron mj 1 if mj is a pronoun; else 0
bareNP mj 1 if mj is a bare NP (i.e., NP without determiners) ; else 0
Features describing a previous mention, mk
defNP mk 1 if mk is adefinite description; else 0
indefNP mk 1 if mk is an indefinite NP; else 0
nameNP mk 1 if mk is a named-entity; else 0
pron mk 1 if mk is apronoun; else 0
bareNP mk 1 if mk is a bare NP; else 0
subject mk 1 if mk is an NP in a subject position; else 0
</equation>
<subsectionHeader confidence="0.621182">
Features describing the relationships between mk and mj
</subsectionHeader>
<bodyText confidence="0.793503125">
sentDist sentence distance between two mentions
numAgree 1 if two mentions match in the number agreement; else 0
genderAgree 1 if two mentions match in the gender agreement; else 0
parallelStruct 1 if two mentions have an identical collocation pattern; else 0
semAgree 1 if two mentions have the same semantic category; else 0
nameAlias 1 if two mentions are an alias of the other; else 0
apposition 1 if two mentions are in an appositive structure; else 0
predicative 1 if two mentions are in a predicative structure; else 0
</bodyText>
<table confidence="0.269441666666667">
strMatch Head 1 if two mentions have the same head string; else 0
strMatch Full 1 if two mentions contain the same strings, excluding the determiners; else 0
strMatch Contain 1 if the string of mj is fully contained in that of mk; else 0
</table>
<tableCaption confidence="0.998355">
Table 2: Feature set for coreference resolution
</tableCaption>
<bodyText confidence="0.995848512195122">
entity e1, e3 and e2:
i({“Microsoft Corp.”, “its”, “The company”},“he”),
i({“yesterday”},“he”),
i({“its new CEO”},“he”).
Among them, the first two are labelled as negative,
while the last one is positive.
The resolution is done using a greedy clustering
strategy. Given a test document, the mentions are
processed one by one. For each encountered men-
tion mj, a test instance is formed for each partial en-
tity found so far, ei. This instance is presented to the
classifier. mj is appended to the entity that is classi-
fied as positive (if any) with the highest confidence
value. If no positive entity exists, the active mention
is deemed as non-anaphoric and forms a new entity.
The process continues until the last mention of the
document is reached.
One potential problem with the entity-mention
model is how to represent the entity-level knowl-
edge. As an entity may contain more than one candi-
date and the number is not fixed, it is impractical to
enumerate all the mentions in an entity and put their
properties into a single feature vector. As a base-
line, we follow the solution proposed in (Luo et al.,
2004) to design a set of first-order features. The fea-
tures are similar to those for the mention-pair model
as shown in Table 2, but their values are calculated
at an entity level. Specifically, the lexical and gram-
matical features are computed by testing any men-
tion1 in the entity against the active mention, for ex-
1Linguistically, pronouns usually have the most direct coref-
ample, the feature nameAlias is assigned value 1 if
at least one mention in the entity is a name alias of
the active mention. The distance feature (i.e., sent-
Dist) is the minimum distance between the mentions
in the entity and the active mention.
The above entity-level features are designed in an
ad-hoc way. They cannot capture the detailed infor-
mation of each individual mention in an entity. In
the next section, we will present a more expressive
entity-mention model by using ILP.
</bodyText>
<sectionHeader confidence="0.998304" genericHeader="method">
4 Entity-mention Model with ILP
</sectionHeader>
<subsectionHeader confidence="0.957769">
4.1 Motivation
</subsectionHeader>
<bodyText confidence="0.995380444444445">
The entity-mention model based on Eq. (2) re-
quires relational knowledge that involves informa-
tion of an active mention (mj), an entity (ei), and
the mentions in the entity ({mk E ei}). How-
ever, normal machine learning algorithms work on
attribute-value vectors, which only allows the repre-
sentation of atomic proposition. To learn from rela-
tional knowledge, we need an algorithm that can ex-
press first-order logic. This requirement motivates
our use of Inductive Logic Programming (ILP), a
learning algorithm capable of inferring logic pro-
grams. The relational nature of ILP makes it pos-
sible to explicitly represent relations between an en-
tity and its mentions, and thus provides a powerful
expressiveness for the coreference resolution task.
erence relationship with antecedents in a local discourse.
Hence, if an active mention is a pronoun, we only consider the
mentions in its previous two sentences for feature computation.
</bodyText>
<page confidence="0.991949">
846
</page>
<bodyText confidence="0.999565111111111">
ILP uses logic programming as a uniform repre-
sentation for examples, background knowledge and
hypotheses. Given a set of positive and negative ex-
ample E = E+ U E−, and a set of background
knowledge K of the domain, ILP tries to induce a
set of hypotheses h that covers most of E+ with no
E−, i.e., K ∧ h |= E+ and K ∧ h K E−.
In our study, we choose ALEPH2, an ILP imple-
mentation by Srinivasan (2000) that has been proven
well suited to deal with a large amount of data in
multiple domains. For its routine use, ALEPH fol-
lows a simple procedure to induce rules. It first se-
lects an example and builds the most specific clause
that entertains the example. Next, it tries to search
for a clause more general than the bottom one. The
best clause is added to the current theory and all the
examples made redundant are removed. The proce-
dure repeats until all examples are processed.
</bodyText>
<subsectionHeader confidence="0.946329">
4.2 Apply ILP to coreference resolution
</subsectionHeader>
<bodyText confidence="0.999988133333333">
Given a document, we encode a mention or a par-
tial entity with a unique constant. Specifically, mj
represents the jth mention (e.g., m6 for the pronoun
“he”). ei j represents the partial entity i before the
jth mention. For example, e1 6 denotes the part of
e1 before m6, i.e., {“Microsoft Corp.”, “its”, “the
company”}, while e1 5 denotes the part of e1 be-
fore m5 (“The company”), i.e., {“Microsoft Corp.”,
“its”}.
Training instances are created as described in Sec-
tion 3.2 for the entity-mention model. Each instance
is recorded with a predicate link(ei j, mj), where mj
is an active mention and ei j is a partial entity. For
example, the three training instances formed by the
pronoun “he” are represented as follows:
</bodyText>
<equation confidence="0.998632">
link(e1 6, m6).
link(e3 6, m6).
link(e2 6, m6).
</equation>
<bodyText confidence="0.9997598">
The first two predicates are put into E−, while the
last one is put to E+.
The background knowledge for an instance
link(ei j, mj) is also represented with predicates,
which are divided into the following types:
</bodyText>
<listItem confidence="0.851428">
1. Predicates describing the information related to
ei j and mj. The properties of mj are pre-
</listItem>
<footnote confidence="0.985328">
2http://web.comlab.ox.ac.uk/oucl/
research/areas/machlearn/Aleph/aleph toc.html
</footnote>
<bodyText confidence="0.995953533333333">
sented with predicates like f(m, v), where f
corresponds to a feature in the first part of Ta-
ble 2 (removing the suffix mj), and v is its
value. For example, the pronoun “he” can be
described by the following predicates:
defNP(m6, 0). indefNP(m6, 0).
nameNP(m6, 0). pron(m6,1).
bareNP(m6, 0).
The predicates for the relationships between
ei j and mj take a form of f(e, m, v). In our
study, we consider the number agreement (ent-
NumAgree) and the gender agreement (entGen-
derAgree) between ei j and mj. v is 1 if all
of the mentions in ei j have consistent num-
ber/gender agreement with mj, e.g,
</bodyText>
<equation confidence="0.600271">
entNumAgree(e1 6, m6, 1).
</equation>
<listItem confidence="0.609276833333333">
2. Predicates describing the belonging relations
between ei j and its mentions. A predicate
has mention(e, m) is used for each mention in
e 3. For example, the partial entity e1 6 has
three mentions, m1, m2 and m5, which can be
described as follows:
</listItem>
<equation confidence="0.695962">
has mention(e1 6, m1).
has mention(e1 6,m2).
has mention(e1 6,m5).
</equation>
<bodyText confidence="0.885685333333333">
3. Predicates describing the information related to
mj and each mention mk in ei j. The predi-
cates for the properties of mk correspond to the
features in the second part of Table 2 (removing
the suffix mk), while the predicates for the re-
lationships between mj and mk correspond to
the features in the third part of Table 2. For ex-
ample, given the two mentions m1 (“Microsoft
Corp.) and m6 (“he), the following predicates
can be applied:
nameNP(m1,1).
pron(m1, 0).
</bodyText>
<equation confidence="0.72584">
. . .
nameAlias(m1, m6, 0).
sentDist(m1, m6, 1).
. . .
</equation>
<bodyText confidence="0.946318">
the last two predicates represent that m1 and
</bodyText>
<footnote confidence="0.9860565">
3If an active mention mj is a pronoun, only the previous
mentions in two sentences apart are recorded by has mention,
while the farther ones are ignored as they have less impact on
the resolution of the pronoun.
</footnote>
<page confidence="0.997608">
847
</page>
<bodyText confidence="0.996214631578948">
m6 are not name alias, and are one sentence
apart.
By using the three types of predicates, the dif-
ferent knowledge related to entities and mentions
are integrated. The predicate has mention acts as
a bridge connecting the entity-mention knowledge
and the mention-pair knowledge. As a result, when
evaluating the coreference relationship between an
active mention and an entity, we can make use of
the “global” information about the entity, as well as
the “local” information of each individual mention
in the entity.
From the training instances and the associated
background knowledge, a set of hypotheses can be
automatically learned by ILP. Each hypothesis is
output as a rule that may look like:
link(A,B):-
predi1, predi2, ... , has mention(A,C), ... , prediN.
which corresponds to first-order logic
</bodyText>
<equation confidence="0.792367555555555">
VA, B(predi1 ∧ predi2 ∧ ... ∧
IC(has mention(A, C) ∧ ... ∧ prediN)
—* link(A, B))
Consider an example rule produced in our system:
link(A,B) :-
has
-
has mention(A,C), numAgree(B,C,1),
strMatch Head(B,C,1), bareNP(C,1).
</equation>
<bodyText confidence="0.999902333333334">
Here, variables A and B stand for an entity and an
active mention in question. The first-order logic is
implemented by using non-instantiated arguments C
in the predicate has mention. This rule states that a
mention B should belong to an entity A, if there ex-
ists a mention C in A such that C is a bare noun
phrase with the same head string as B, and matches
in number with B. In this way, the detailed informa-
tion of each individual mention in an entity can be
captured for resolution.
A rule is applicable to an instance link(e, m), if
the background knowledge for the instance can be
described by the predicates in the body of the rule.
Each rule is associated with a score, which is the
accuracy that the rule can produce for the training
instances.
The learned rules are applied to resolution in a
similar way as described in Section 3.2. Given an
active mention m and a partial entity e, a test in-
stance link(e, m) is formed and tested against every
rule in the rule set. The confidence that m should
</bodyText>
<table confidence="0.970418">
Train Test
#entity #mention #entity #mention
NWire 1678 9861 411 2304
NPaper 1528 10277 365 2290
BNews 1695 8986 468 2493
</table>
<tableCaption confidence="0.989962">
Table 3: statistics of entities (length &gt; 1) and contained
mentions
</tableCaption>
<bodyText confidence="0.996981666666667">
belong to e is the maximal score of the applicable
rules. An active mention is linked to the entity with
the highest confidence value (above 0.5), if any.
</bodyText>
<sectionHeader confidence="0.996547" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.929642">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.996537833333333">
In our study, we did evaluation on the ACE-2003
corpus, which contains two data sets, training and
devtest, used for training and testing respectively.
Each of these sets is further divided into three do-
mains: newswire (NWire), newspaper (NPaper), and
broadcast news (BNews). The number of entities
with more than one mention, as well as the number
of the contained mentions, is summarized in Table 3.
For both training and resolution, an input raw
document was processed by a pipeline of NLP
modules including Tokenizer, Part-of-Speech tag-
ger, NP Chunker and Named-Entity (NE) Recog-
nizer. Trained and tested on Penn WSJ TreeBank,
the POS tagger could obtain an accuracy of 97% and
the NP chunker could produce an F-measure above
94% (Zhou and Su, 2000). Evaluated for the MUC-
6 and MUC-7 Named-Entity task, the NER mod-
ule (Zhou and Su, 2002) could provide an F-measure
of 96.6% (MUC-6) and 94.1%(MUC-7). For evalu-
ation, Vilain et al. (1995)’s scoring algorithm was
adopted to compute recall and precision rates.
By default, the ALEPH algorithm only generates
rules that have 100% accuracy for the training data.
And each rule contains at most three predicates. To
accommodate for coreference resolution, we loos-
ened the restrictions to allow rules that have above
50% accuracy and contain up to ten predicates. De-
fault parameters were applied for all the other set-
tings in ALEPH as well as other learning algorithms
used in the experiments.
</bodyText>
<subsectionHeader confidence="0.91176">
5.2 Results and Discussions
</subsectionHeader>
<bodyText confidence="0.9778045">
Table 4 lists the performance of different corefer-
ence resolution systems. For comparison, we first
</bodyText>
<page confidence="0.992545">
848
</page>
<table confidence="0.999972888888889">
R NWire R NPaper R BNews
P F P F P F
C4.5
- Mention-Pair 68.2 54.3 60.4 67.3 50.8 57.9 66.5 59.5 62.9
- Entity-Mention 66.8 55.0 60.3 64.2 53.4 58.3 64.6 60.6 62.5
- Mention-Pair (all mentions in entity) 66.7 49.3 56.7 65.8 48.9 56.1 66.5 47.6 55.4
ILP
- Mention-Pair 66.1 54.8 59.5 65.6 54.8 59.7 63.5 60.8 62.1
- Entity-Mention 65.0 58.9 61.8 63.4 57.1 60.1 61.7 65.4 63.5
</table>
<tableCaption confidence="0.999887">
Table 4: Results of different systems for coreference resolution
</tableCaption>
<bodyText confidence="0.999914542857143">
examined the C4.5 algorithm4 which is widely used
for the coreference resolution task. The first line of
the table shows the baseline system that employs the
traditional mention-pair model (MP) as described in
Section 3.1. From the table, our baseline system
achieves a recall of around 66%-68% and a preci-
sion of around 50%-60%. The overall F-measure
for NWire, NPaper and BNews is 60.4%, 57.9% and
62.9% respectively. The results are comparable to
those reported in (Ng, 2005) which uses similar fea-
tures and gets an F-measure ranging in 50-60% for
the same data set. As our system relies only on sim-
ple and knowledge-poor features, the achieved F-
measure is around 2-4% lower than the state-of-the-
art systems do, like (Ng, 2007) and (Yang and Su,
2007) which utilized sophisticated semantic or real-
world knowledge. Since ILP has a strong capability
in knowledge management, our system could be fur-
ther improved if such helpful knowledge is incorpo-
rated, which will be explored in our future work.
The second line of Table 4 is for the system
that employs the entity-mention model (EM) with
“Any-X” based entity features, as described in Sec-
tion 3.2. We can find that the EM model does not
show superiority over the baseline MP model. It
achieves a higher precision (up to 2.6%), but a lower
recall (2.9%), than MP. As a result, we only see
±0.4% difference between the F-measure. The re-
sults are consistent with the reports by Luo et al.
(2004) that the entity-mention model with the “Any-
X” first-order features performs worse than the nor-
mal mention-pair model. In our study, we also tested
the “Most-X” strategy for the first-order features as
in (Culotta et al., 2007), but got similar results with-
out much difference (±0.5% F-measure) in perfor-
</bodyText>
<footnote confidence="0.760945">
4http://www.rulequest.com/see5-info.html
</footnote>
<bodyText confidence="0.99989425">
mance. Besides, as with our entity-mention predi-
cates described in Section 4.2, we also tried the “All-
X” strategy for the entity-level agreement features,
that is, whether all mentions in a partial entity agree
in number and gender with an active mention. How-
ever, we found this bring no improvement against
the “Any-X” strategy.
As described, given an active mention mj, the MP
model only considers the mentions between mj and
its closest antecedent. By contrast, the EM model
considers not only these mentions, but also their an-
tecedents in the same entity link. We were interested
in examining what if the MP model utilizes all the
mentions in an entity as the EM model does. As
shown in the third line of Table 4, such a solution
damages the performance; while the recall is at the
same level, the precision drops significantly (up to
12%) and as a result, the F-measure is even lower
than the original MP model. This should be because
a mention does not necessarily have direct corefer-
ence relationships with all of its antecedents. As the
MP model treats each mention-pair as an indepen-
dent instance, including all the antecedents would
produce many less-confident positive instances, and
thus adversely affect training.
The second block of the table summarizes the per-
formance of the systems with ILP. We were first con-
cerned with how well ILP works for the mention-
pair model, compared with the normally used algo-
rithm C4.5. From the results shown in the fourth
line of Table 4, ILP exhibits the same capability in
the resolution; it tends to produce a slightly higher
precision but a lower recall than C4.5 does. Overall,
it performs better in F-measure (1.8%) for Npaper,
while slightly worse (&lt;1%) for Nwire and BNews.
These results demonstrate that ILP could be used as
</bodyText>
<page confidence="0.966068">
849
</page>
<equation confidence="0.994118363636364">
link(A,B) :-
bareNP(B,0), has mention(A,C), appositive(C,1).
link(A,B) :-
has mention(A,C), numAgree(B,C,1), strMatch Head(B,C,1), bareNP(C,1).
link(A,B) :-
nameNP(B,0), has mention(A,C), predicative(C,1).
link(A,B) :-
has mention(A,C), strMatch Contain(B,C,1), strMatch Head(B,C,1), bareNP(C,0).
link(A,B) :-
nameNP(B,0), has mention(A,C), nameAlias(C,1), bareNP(C,0).
link(A,B) :-
</equation>
<figureCaption confidence="0.372746">
pron(B,1), has mention(A,C), nameNP(C,1), has mention(A,D), indefNP(D,1),
subject(D, 1).
</figureCaption>
<figure confidence="0.631751">
...
</figure>
<figureCaption confidence="0.997326">
Figure 1: Examples of rules produced by ILP (entity-
mention model)
</figureCaption>
<bodyText confidence="0.999122936170213">
a good classifier learner for the mention-pair model.
The fifth line of Table 4 is for the ILP based entity-
mention model (described in Section 4.2). We can
observe that the model leads to a better performance
than all the other models. Compared with the sys-
tem with the MP model (under ILP), the EM version
is able to achieve a higher precision (up to 4.6% for
BNews). Although the recall drops slightly (up to
1.8% for BNews), the gain in the precision could
compensate it well; it beats the MP model in the
overall F-measure for all three domains (2.3% for
Nwire, 0.4% for Npaper, 1.4% for BNews). Es-
pecially, the improvement in NWire and BNews is
statistically significant under a 2-tailed t test (p &lt;
0.05). Compared with the EM model with the man-
ually designed first-order feature (the second line),
the ILP-based EM solution also yields better perfor-
mance in precision (with a slightly lower recall) as
well as the overall F-measure (1.0% - 1.8%).
The improvement in precision against the
mention-pair model confirms that the global infor-
mation beyond a single mention pair, when being
considered for training, can make coreference rela-
tions clearer and help classifier learning. The bet-
ter performance against the EM model with heuristi-
cally designed features also suggests that ILP is able
to learn effective first-order rules for the coreference
resolution task.
In Figure 1, we illustrate part of the rules pro-
duced by ILP for the entity-mention model (NWire
domain), which shows how the relational knowledge
of entities and mentions is represented for decision
making. An interesting finding, as shown in the last
rule of the table, is that multiple non-instantiated ar-
guments (i.e. C and D) could possibly appear in
the same rule. According to this rule, a pronominal
mention should be linked with a partial entity which
contains a named-entity and contains an indefinite
NP in a subject position. This supports the claims
in (Yang et al., 2004a) that coreferential informa-
tion is an important factor to evaluate a candidate an-
tecedent in pronoun resolution. Such complex logic
makes it possible to capture information of multi-
ple mentions in an entity at the same time, which is
difficult to implemented in the mention-pair model
and the ordinary entity-mention model with heuris-
tic first-order features.
</bodyText>
<sectionHeader confidence="0.999746" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999957233333333">
This paper presented an expressive entity-mention
model for coreference resolution by using Inductive
Logic Programming. In contrast to the traditional
mention-pair model, our model can capture infor-
mation beyond single mention pairs for both training
and testing. The relational nature of ILP enables our
model to explicitly express the relations between an
entity and its mentions, and to automatically learn
the first-order rules effective for the coreference res-
olution task. The evaluation on ACE data set shows
that the ILP based entity-model performs better than
the mention-pair model (with up to 2.3% increase in
F-measure), and also beats the entity-mention model
with heuristically designed first-order features.
Our current work focuses on the learning model
that calculates the probability of a mention be-
longing to an entity. For simplicity, we just use a
greedy clustering strategy for resolution, that is, a
mention is linked to the current best partial entity.
In our future work, we would like to investigate
more sophisticated clustering methods that would
lead to global optimization, e.g., by keeping a large
search space (Luo et al., 2004) or using integer
programming (Denis and Baldridge, 2007).
Acknowledgements This research is supported
by a Specific Targeted Research Project (STREP)
of the European Union’s 6th Framework Programme
within IST call 4, Bootstrapping Of Ontologies and
Terminologies STrategic REsearch Project (BOOT-
Strep).
</bodyText>
<page confidence="0.995229">
850
</page>
<sectionHeader confidence="0.995878" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999844831578948">
C. Aone and S. W. Bennett. 1995. Evaluating automated
and manual acquisition of anaphora resolution strate-
gies. In Proceedings of the 33rd Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 122–129.
V. Claveau, P. Sebillot, C. Fabre, and P. Bouillon. 2003.
Learning semantic lexicons from a part-of-speech and
semantically tagged corpus using inductive logic pro-
gramming. Journal of Machine Learning Research,
4:493–525.
A. Culotta, M. Wick, and A. McCallum. 2007. First-
order probabilistic models for coreference resolution.
In Proceedings of the Annual Meeting of the North
America Chapter ofthe Associationfor Computational
Linguistics (NAACL), pages 81–88.
J. Cussens. 1996. Part-of-speech disambiguation using
ilp. Technical report, Oxford University Computing
Laboratory.
P. Denis and J. Baldridge. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In Proceedings of the Annual Meeting
of the North America Chapter of the Association for
Computational Linguistics (NAACL), pages 236–243.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the bell tree. In
Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
135–142.
A. McCallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In Proceedings of IJCAI-
03 Workshop on Information Integration on the Web,
pages 79–86.
J. McCarthy and W. Lehnert. 1995. Using decision
trees for coreference resolution. In Proceedings of
the 14th International Conference on Artificial Intel-
ligences (IJCAI), pages 1050–1055.
R. Mooney. 1997. Inductive logic programming for nat-
ural language processing. In Proceedings of the sixth
International Inductive Logic Programming Work-
shop, pages 3–24.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Proceed-
ings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 104–111,
Philadelphia.
V. Ng. 2005. Machine learning for coreference resolu-
tion: From local classification to global ranking. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
157–164.
V. Ng. 2007. Semantic class induction and coreference
resolution. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 536–543.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning
approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521–544.
L. Specia, M. Stevenson, and M. V. Nunes. 2007. Learn-
ing expressive models for words sense disambiguation.
In Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
41–48.
A. Srinivasan. 2000. The aleph manual. Technical re-
port, Oxford University Computing Laboratory.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In Proceedings of the Sixth Mes-
sage understanding Conference (MUC-6), pages 45–
52, San Francisco, CA. Morgan Kaufmann Publishers.
X. Yang and J. Su. 2007. Coreference resolution us-
ing semantic relatedness information from automati-
cally discovered patterns. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 528–535.
X. Yang, J. Su, G. Zhou, and C. Tan. 2004a. Improv-
ing pronoun resolution by incorporating coreferential
information of candidates. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 127–134, Barcelona.
X. Yang, J. Su, G. Zhou, and C. Tan. 2004b. An
NP-cluster approach to coreference resolution. In
Proceedings of the 20th International Conference on
Computational Linguistics, pages 219–225, Geneva.
G. Zhou and J. Su. 2000. Error-driven HMM-based
chunk tagger with context-dependent lexicon. In Pro-
ceedings of the Joint Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora, pages 71–79, Hong Kong.
G. Zhou and J. Su. 2002. Named Entity recognition us-
ing a HMM-based chunk tagger. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 473–480, Philadel-
phia.
</reference>
<page confidence="0.998657">
851
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.664729">
<title confidence="0.999746">An Entity-Mention Model for Coreference Resolution with Inductive Logic Programming</title>
<author confidence="0.9952805">Jian Jun Lim Ting Sheng</author>
<affiliation confidence="0.893623333333333">for Infocomm Research Institute of Technology lisheng@hit.edu.cn University of Singapore,</affiliation>
<email confidence="0.993559">tancl@comp.nus.edu.sg</email>
<abstract confidence="0.998533388888889">The traditional mention-pair model for coreference resolution cannot capture information beyond mention pairs for both learning and testing. To deal with this problem, we present an expressive entity-mention model that performs coreference resolution at an entity level. The model adopts the Inductive Logic Programming (ILP) algorithm, which provides a relational way to organize different knowledge of entities and mentions. The solution can explicitly express relations between an entity and the contained mentions, and automatically learn first-order rules important for coreference decision. The evaluation on the ACE data set shows that the ILP based entity-mention model is effective for the coreference resolution task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Aone</author>
<author>S W Bennett</author>
</authors>
<title>Evaluating automated and manual acquisition of anaphora resolution strategies.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>122--129</pages>
<contexts>
<context position="1439" citStr="Aone and Bennett (1995)" startWordPosition="196" endWordPosition="199">plicitly express relations between an entity and the contained mentions, and automatically learn first-order rules important for coreference decision. The evaluation on the ACE data set shows that the ILP based entity-mention model is effective for the coreference resolution task. 1 Introduction Coreference resolution is the process of linking multiple mentions that refer to the same entity. Most of previous work adopts the mention-pair model, which recasts coreference resolution to a binary classification problem of determining whether or not two mentions in a document are co-referring (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et</context>
</contexts>
<marker>Aone, Bennett, 1995</marker>
<rawString>C. Aone and S. W. Bennett. 1995. Evaluating automated and manual acquisition of anaphora resolution strategies. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 122–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Claveau</author>
<author>P Sebillot</author>
<author>C Fabre</author>
<author>P Bouillon</author>
</authors>
<title>Learning semantic lexicons from a part-of-speech and semantically tagged corpus using inductive logic programming.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>4--493</pages>
<contexts>
<context position="6961" citStr="Claveau et al., 2003" startWordPosition="1106" endWordPosition="1109">are made independently of each other. The simplest model conditions coreference on mention pairs, but enforces dependency by calculating the distance of a node to a partition (i.e., the probability that an active mention belongs to an entity) based on the sum of its distances to all the nodes in the partition (i.e., the sum of the probability of the active mention co-referring with the mentions in the entity). Inductive Logic Programming (ILP) has been applied to some natural language processing tasks, including parsing (Mooney, 1997), POS disambiguation (Cussens, 1996), lexicon construction (Claveau et al., 2003), WSD (Specia et al., 2007), and so on. However, to our knowledge, our work is the first effort to adopt this technique for the coreference resolution task. 3 Modelling Coreference Resolution Suppose we have a document containing n mentions {mj : 1 &lt; j &lt; n1, in which mj is the jth mention occurring in the document. Let eZ be the ith entity in the document. We define PIL|eZ, mj), (1) the probability that a mention belongs to an entity. Here the random variable L takes a binary value and is 1 if mj is a mention of eZ. By assuming that mentions occurring after mj have no influence on the decision</context>
</contexts>
<marker>Claveau, Sebillot, Fabre, Bouillon, 2003</marker>
<rawString>V. Claveau, P. Sebillot, C. Fabre, and P. Bouillon. 2003. Learning semantic lexicons from a part-of-speech and semantically tagged corpus using inductive logic programming. Journal of Machine Learning Research, 4:493–525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>M Wick</author>
<author>A McCallum</author>
</authors>
<title>Firstorder probabilistic models for coreference resolution.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting of the North America Chapter ofthe Associationfor Computational Linguistics (NAACL),</booktitle>
<pages>81--88</pages>
<contexts>
<context position="3004" citStr="Culotta et al., 2007" startWordPosition="454" endWordPosition="457">n (if any), based on classification results. One problem that arises with the entity-mention model is how to represent the knowledge related to an entity. In a document, an entity may have more than one mention. It is impractical to enumerate all the mentions in an entity and record their information in a single feature vector, as it would make the feature space too large. Even worse, the number of mentions in an entity is not fixed, which would result in variant-length feature vectors and make trouble for normal machine learning algorithms. A solution seen in previous work (Luo et al., 2004; Culotta et al., 2007) is to design a set of first-order features summarizing the information of the mentions in an entity, for example, “whether the entity has any mention that is a name alias of the active mention?” or “whether most of the mentions in the entity have the same head word as the active mention?” These features, nevertheless, are designed in an ad-hoc manner and lack the capability of describing each individual mention in an entity. In this paper, we present a more expressive entity843 Proceedings ofACL-08: HLT, pages 843–851, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Lingu</context>
<context position="5368" citStr="Culotta et al. (2007)" startWordPosition="849" endWordPosition="852">k is further enhanced by Ng and Cardie (2002) by expanding the feature set and adopting a “bestfirst” linking strategy. Recent years have seen some work on the entitymention model. Luo et al. (2004) propose a system that performs coreference resolution by doing search in a large space of entities. They train a classifier that can determine the likelihood that an active mention should belong to an entity. The entity-level features are calculated with an “Any-X” strategy: an entitymention pair would be assigned a feature X, if any mention in the entity has the feature X with the active mention. Culotta et al. (2007) present a system which uses an online learning approach to train a classifier to judge whether two entities are coreferential or not. The features describing the relationships between two entities are obtained based on the information of every possible pair of mentions from the two entities. Different from (Luo et al., 2004), the entitylevel features are computed using a “Most-X” strategy, that is, two given entities would have a feature X, if most of the mention pairs from the two entities have the feature X. Yang et al. (2004b) suggest an entity-based coreference resolution system. The mode</context>
<context position="26365" citStr="Culotta et al., 2007" startWordPosition="4473" endWordPosition="4476">loys the entity-mention model (EM) with “Any-X” based entity features, as described in Section 3.2. We can find that the EM model does not show superiority over the baseline MP model. It achieves a higher precision (up to 2.6%), but a lower recall (2.9%), than MP. As a result, we only see ±0.4% difference between the F-measure. The results are consistent with the reports by Luo et al. (2004) that the entity-mention model with the “AnyX” first-order features performs worse than the normal mention-pair model. In our study, we also tested the “Most-X” strategy for the first-order features as in (Culotta et al., 2007), but got similar results without much difference (±0.5% F-measure) in perfor4http://www.rulequest.com/see5-info.html mance. Besides, as with our entity-mention predicates described in Section 4.2, we also tried the “AllX” strategy for the entity-level agreement features, that is, whether all mentions in a partial entity agree in number and gender with an active mention. However, we found this bring no improvement against the “Any-X” strategy. As described, given an active mention mj, the MP model only considers the mentions between mj and its closest antecedent. By contrast, the EM model cons</context>
</contexts>
<marker>Culotta, Wick, McCallum, 2007</marker>
<rawString>A. Culotta, M. Wick, and A. McCallum. 2007. Firstorder probabilistic models for coreference resolution. In Proceedings of the Annual Meeting of the North America Chapter ofthe Associationfor Computational Linguistics (NAACL), pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cussens</author>
</authors>
<title>Part-of-speech disambiguation using ilp.</title>
<date>1996</date>
<tech>Technical report,</tech>
<institution>Oxford University Computing Laboratory.</institution>
<contexts>
<context position="6916" citStr="Cussens, 1996" startWordPosition="1102" endWordPosition="1103">n that pairwise coreference decisions are made independently of each other. The simplest model conditions coreference on mention pairs, but enforces dependency by calculating the distance of a node to a partition (i.e., the probability that an active mention belongs to an entity) based on the sum of its distances to all the nodes in the partition (i.e., the sum of the probability of the active mention co-referring with the mentions in the entity). Inductive Logic Programming (ILP) has been applied to some natural language processing tasks, including parsing (Mooney, 1997), POS disambiguation (Cussens, 1996), lexicon construction (Claveau et al., 2003), WSD (Specia et al., 2007), and so on. However, to our knowledge, our work is the first effort to adopt this technique for the coreference resolution task. 3 Modelling Coreference Resolution Suppose we have a document containing n mentions {mj : 1 &lt; j &lt; n1, in which mj is the jth mention occurring in the document. Let eZ be the ith entity in the document. We define PIL|eZ, mj), (1) the probability that a mention belongs to an entity. Here the random variable L takes a binary value and is 1 if mj is a mention of eZ. By assuming that mentions occurri</context>
</contexts>
<marker>Cussens, 1996</marker>
<rawString>J. Cussens. 1996. Part-of-speech disambiguation using ilp. Technical report, Oxford University Computing Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Denis</author>
<author>J Baldridge</author>
</authors>
<title>Joint determination of anaphoricity and coreference resolution using integer programming.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting of the North America Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<pages>236--243</pages>
<marker>Denis, Baldridge, 2007</marker>
<rawString>P. Denis and J. Baldridge. 2007. Joint determination of anaphoricity and coreference resolution using integer programming. In Proceedings of the Annual Meeting of the North America Chapter of the Association for Computational Linguistics (NAACL), pages 236–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>S Roukos</author>
</authors>
<title>A mention-synchronous coreference resolution algorithm based on the bell tree.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>135--142</pages>
<contexts>
<context position="2030" citStr="Luo et al., 2004" startWordPosition="288" endWordPosition="291">. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional mentionpair counterpart, the entity-mention model aims to make coreference decision at an entity level. Classification is done to determine whether a mention is a referent of a partially found entity. A mention to be resolved (called active mention henceforth) is linked to an appropriate entity chain (if any), based on classification results. One problem that arises with the entity-mention model is how to represent the knowledge related to an entity. In a document, an entity may have more than one mention. It is impractical to enumerate all </context>
<context position="4945" citStr="Luo et al. (2004)" startWordPosition="777" endWordPosition="780">ion systems that employ the mention-pair model. A typical one of them is presented by Soon et al. (2001). In the system, a training or testing instance is formed for two mentions in question, with a feature vector describing their properties and relationships. At a testing time, an active mention is checked against all its preceding mentions, and is linked with the closest one that is classified as positive. The work is further enhanced by Ng and Cardie (2002) by expanding the feature set and adopting a “bestfirst” linking strategy. Recent years have seen some work on the entitymention model. Luo et al. (2004) propose a system that performs coreference resolution by doing search in a large space of entities. They train a classifier that can determine the likelihood that an active mention should belong to an entity. The entity-level features are calculated with an “Any-X” strategy: an entitymention pair would be assigned a feature X, if any mention in the entity has the feature X with the active mention. Culotta et al. (2007) present a system which uses an online learning approach to train a classifier to judge whether two entities are coreferential or not. The features describing the relationships </context>
<context position="14624" citStr="Luo et al., 2004" startWordPosition="2457" endWordPosition="2460">entity that is classified as positive (if any) with the highest confidence value. If no positive entity exists, the active mention is deemed as non-anaphoric and forms a new entity. The process continues until the last mention of the document is reached. One potential problem with the entity-mention model is how to represent the entity-level knowledge. As an entity may contain more than one candidate and the number is not fixed, it is impractical to enumerate all the mentions in an entity and put their properties into a single feature vector. As a baseline, we follow the solution proposed in (Luo et al., 2004) to design a set of first-order features. The features are similar to those for the mention-pair model as shown in Table 2, but their values are calculated at an entity level. Specifically, the lexical and grammatical features are computed by testing any mention1 in the entity against the active mention, for ex1Linguistically, pronouns usually have the most direct corefample, the feature nameAlias is assigned value 1 if at least one mention in the entity is a name alias of the active mention. The distance feature (i.e., sentDist) is the minimum distance between the mentions in the entity and t</context>
<context position="26138" citStr="Luo et al. (2004)" startWordPosition="4436" endWordPosition="4439">s a strong capability in knowledge management, our system could be further improved if such helpful knowledge is incorporated, which will be explored in our future work. The second line of Table 4 is for the system that employs the entity-mention model (EM) with “Any-X” based entity features, as described in Section 3.2. We can find that the EM model does not show superiority over the baseline MP model. It achieves a higher precision (up to 2.6%), but a lower recall (2.9%), than MP. As a result, we only see ±0.4% difference between the F-measure. The results are consistent with the reports by Luo et al. (2004) that the entity-mention model with the “AnyX” first-order features performs worse than the normal mention-pair model. In our study, we also tested the “Most-X” strategy for the first-order features as in (Culotta et al., 2007), but got similar results without much difference (±0.5% F-measure) in perfor4http://www.rulequest.com/see5-info.html mance. Besides, as with our entity-mention predicates described in Section 4.2, we also tried the “AllX” strategy for the entity-level agreement features, that is, whether all mentions in a partial entity agree in number and gender with an active mention.</context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos. 2004. A mention-synchronous coreference resolution algorithm based on the bell tree. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 135–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>B Wellner</author>
</authors>
<title>Toward conditional models of identity uncertainty with application to proper noun coreference.</title>
<date>2003</date>
<booktitle>In Proceedings of IJCAI03 Workshop on Information Integration on the Web,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="6200" citStr="McCallum and Wellner (2003)" startWordPosition="986" endWordPosition="989">ained based on the information of every possible pair of mentions from the two entities. Different from (Luo et al., 2004), the entitylevel features are computed using a “Most-X” strategy, that is, two given entities would have a feature X, if most of the mention pairs from the two entities have the feature X. Yang et al. (2004b) suggest an entity-based coreference resolution system. The model adopted in the system is similar to the mention-pair model, except that the entity information (e.g., the global number/gender agreement) is considered as additional features of a mention in the entity. McCallum and Wellner (2003) propose several graphical models for coreference analysis. These models aim to overcome the limitation that pairwise coreference decisions are made independently of each other. The simplest model conditions coreference on mention pairs, but enforces dependency by calculating the distance of a node to a partition (i.e., the probability that an active mention belongs to an entity) based on the sum of its distances to all the nodes in the partition (i.e., the sum of the probability of the active mention co-referring with the mentions in the entity). Inductive Logic Programming (ILP) has been app</context>
</contexts>
<marker>McCallum, Wellner, 2003</marker>
<rawString>A. McCallum and B. Wellner. 2003. Toward conditional models of identity uncertainty with application to proper noun coreference. In Proceedings of IJCAI03 Workshop on Information Integration on the Web, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J McCarthy</author>
<author>W Lehnert</author>
</authors>
<title>Using decision trees for coreference resolution.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Conference on Artificial Intelligences (IJCAI),</booktitle>
<pages>1050--1055</pages>
<contexts>
<context position="1468" citStr="McCarthy and Lehnert (1995)" startWordPosition="200" endWordPosition="203">s between an entity and the contained mentions, and automatically learn first-order rules important for coreference decision. The evaluation on the ACE data set shows that the ILP based entity-mention model is effective for the coreference resolution task. 1 Introduction Coreference resolution is the process of linking multiple mentions that refer to the same entity. Most of previous work adopts the mention-pair model, which recasts coreference resolution to a binary classification problem of determining whether or not two mentions in a document are co-referring (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with t</context>
</contexts>
<marker>McCarthy, Lehnert, 1995</marker>
<rawString>J. McCarthy and W. Lehnert. 1995. Using decision trees for coreference resolution. In Proceedings of the 14th International Conference on Artificial Intelligences (IJCAI), pages 1050–1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mooney</author>
</authors>
<title>Inductive logic programming for natural language processing.</title>
<date>1997</date>
<booktitle>In Proceedings of the sixth International Inductive Logic Programming Workshop,</booktitle>
<pages>3--24</pages>
<contexts>
<context position="6880" citStr="Mooney, 1997" startWordPosition="1097" endWordPosition="1098">odels aim to overcome the limitation that pairwise coreference decisions are made independently of each other. The simplest model conditions coreference on mention pairs, but enforces dependency by calculating the distance of a node to a partition (i.e., the probability that an active mention belongs to an entity) based on the sum of its distances to all the nodes in the partition (i.e., the sum of the probability of the active mention co-referring with the mentions in the entity). Inductive Logic Programming (ILP) has been applied to some natural language processing tasks, including parsing (Mooney, 1997), POS disambiguation (Cussens, 1996), lexicon construction (Claveau et al., 2003), WSD (Specia et al., 2007), and so on. However, to our knowledge, our work is the first effort to adopt this technique for the coreference resolution task. 3 Modelling Coreference Resolution Suppose we have a document containing n mentions {mj : 1 &lt; j &lt; n1, in which mj is the jth mention occurring in the document. Let eZ be the ith entity in the document. We define PIL|eZ, mj), (1) the probability that a mention belongs to an entity. Here the random variable L takes a binary value and is 1 if mj is a mention of e</context>
</contexts>
<marker>Mooney, 1997</marker>
<rawString>R. Mooney. 1997. Inductive logic programming for natural language processing. In Proceedings of the sixth International Inductive Logic Programming Workshop, pages 3–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>104--111</pages>
<location>Philadelphia.</location>
<contexts>
<context position="1510" citStr="Ng and Cardie (2002)" startWordPosition="208" endWordPosition="211">nd automatically learn first-order rules important for coreference decision. The evaluation on the ACE data set shows that the ILP based entity-mention model is effective for the coreference resolution task. 1 Introduction Coreference resolution is the process of linking multiple mentions that refer to the same entity. Most of previous work adopts the mention-pair model, which recasts coreference resolution to a binary classification problem of determining whether or not two mentions in a document are co-referring (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional mentionpair counterpart, th</context>
<context position="4792" citStr="Ng and Cardie (2002)" startWordPosition="750" endWordPosition="753">l results on the ACE data set shows the model is effective for coreference resolution. 2 Related Work There are plenty of learning-based coreference resolution systems that employ the mention-pair model. A typical one of them is presented by Soon et al. (2001). In the system, a training or testing instance is formed for two mentions in question, with a feature vector describing their properties and relationships. At a testing time, an active mention is checked against all its preceding mentions, and is linked with the closest one that is classified as positive. The work is further enhanced by Ng and Cardie (2002) by expanding the feature set and adopting a “bestfirst” linking strategy. Recent years have seen some work on the entitymention model. Luo et al. (2004) propose a system that performs coreference resolution by doing search in a large space of entities. They train a classifier that can determine the likelihood that an active mention should belong to an entity. The entity-level features are calculated with an “Any-X” strategy: an entitymention pair would be assigned a feature X, if any mention in the entity has the feature X with the active mention. Culotta et al. (2007) present a system which </context>
<context position="10943" citStr="Ng and Cardie (2002)" startWordPosition="1801" endWordPosition="1804">ctive mention and ei is a partial entity found before mj. During training, given each anaphoric mention mj, one single positive training instance is created for the entity to which mj belongs. And a group of negative training instances is created for every partial entity whose last mention occurs between mj and the closest antecedent of mj. See the sample in Table 1 again. For the pronoun “he”, the following three instances are generated for 3.1 Mention-Pair Model As a baseline, we first describe a learning framework with the mention-pair model as adopted in the work by Soon et al. (2001) and Ng and Cardie (2002). In the learning framework, a training or testing instance has the form of i{mk, mj}, in which mj is an active mention and mk is a preceding mention. An instance is associated with a vector of features, which is used to describe the properties of the two mentions as well as their relationships. Table 2 summarizes the features used in our study. For training, given each encountered anaphoric mention mj in a document, one single positive training instance is created for mj and its closest antecedent. And a group of negative training instances is created for every intervening mentions between mj</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 104–111, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
</authors>
<title>Machine learning for coreference resolution: From local classification to global ranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>157--164</pages>
<contexts>
<context position="25170" citStr="Ng, 2005" startWordPosition="4267" endWordPosition="4268">2.1 - Entity-Mention 65.0 58.9 61.8 63.4 57.1 60.1 61.7 65.4 63.5 Table 4: Results of different systems for coreference resolution examined the C4.5 algorithm4 which is widely used for the coreference resolution task. The first line of the table shows the baseline system that employs the traditional mention-pair model (MP) as described in Section 3.1. From the table, our baseline system achieves a recall of around 66%-68% and a precision of around 50%-60%. The overall F-measure for NWire, NPaper and BNews is 60.4%, 57.9% and 62.9% respectively. The results are comparable to those reported in (Ng, 2005) which uses similar features and gets an F-measure ranging in 50-60% for the same data set. As our system relies only on simple and knowledge-poor features, the achieved Fmeasure is around 2-4% lower than the state-of-theart systems do, like (Ng, 2007) and (Yang and Su, 2007) which utilized sophisticated semantic or realworld knowledge. Since ILP has a strong capability in knowledge management, our system could be further improved if such helpful knowledge is incorporated, which will be explored in our future work. The second line of Table 4 is for the system that employs the entity-mention mo</context>
</contexts>
<marker>Ng, 2005</marker>
<rawString>V. Ng. 2005. Machine learning for coreference resolution: From local classification to global ranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 157–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
</authors>
<title>Semantic class induction and coreference resolution.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>536--543</pages>
<contexts>
<context position="25422" citStr="Ng, 2007" startWordPosition="4312" endWordPosition="4313">the baseline system that employs the traditional mention-pair model (MP) as described in Section 3.1. From the table, our baseline system achieves a recall of around 66%-68% and a precision of around 50%-60%. The overall F-measure for NWire, NPaper and BNews is 60.4%, 57.9% and 62.9% respectively. The results are comparable to those reported in (Ng, 2005) which uses similar features and gets an F-measure ranging in 50-60% for the same data set. As our system relies only on simple and knowledge-poor features, the achieved Fmeasure is around 2-4% lower than the state-of-theart systems do, like (Ng, 2007) and (Yang and Su, 2007) which utilized sophisticated semantic or realworld knowledge. Since ILP has a strong capability in knowledge management, our system could be further improved if such helpful knowledge is incorporated, which will be explored in our future work. The second line of Table 4 is for the system that employs the entity-mention model (EM) with “Any-X” based entity features, as described in Section 3.2. We can find that the EM model does not show superiority over the baseline MP model. It achieves a higher precision (up to 2.6%), but a lower recall (2.9%), than MP. As a result, </context>
</contexts>
<marker>Ng, 2007</marker>
<rawString>V. Ng. 2007. Semantic class induction and coreference resolution. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), pages 536–543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Soon</author>
<author>H Ng</author>
<author>D Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="1488" citStr="Soon et al. (2001)" startWordPosition="204" endWordPosition="207">ontained mentions, and automatically learn first-order rules important for coreference decision. The evaluation on the ACE data set shows that the ILP based entity-mention model is effective for the coreference resolution task. 1 Introduction Coreference resolution is the process of linking multiple mentions that refer to the same entity. Most of previous work adopts the mention-pair model, which recasts coreference resolution to a binary classification problem of determining whether or not two mentions in a document are co-referring (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional menti</context>
<context position="4432" citStr="Soon et al. (2001)" startWordPosition="688" endWordPosition="691"> of this, a set of first-order rules is automatically learned, which can capture the information of each individual mention in an entity, as well as the global information of the entity, to make coreference decision. Hence, our model has a more powerful representation capability than the traditional mention-pair or entity-mention model. And our experimental results on the ACE data set shows the model is effective for coreference resolution. 2 Related Work There are plenty of learning-based coreference resolution systems that employ the mention-pair model. A typical one of them is presented by Soon et al. (2001). In the system, a training or testing instance is formed for two mentions in question, with a feature vector describing their properties and relationships. At a testing time, an active mention is checked against all its preceding mentions, and is linked with the closest one that is classified as positive. The work is further enhanced by Ng and Cardie (2002) by expanding the feature set and adopting a “bestfirst” linking strategy. Recent years have seen some work on the entitymention model. Luo et al. (2004) propose a system that performs coreference resolution by doing search in a large space</context>
<context position="10918" citStr="Soon et al. (2001)" startWordPosition="1796" endWordPosition="1799">j}, in which mj is an active mention and ei is a partial entity found before mj. During training, given each anaphoric mention mj, one single positive training instance is created for the entity to which mj belongs. And a group of negative training instances is created for every partial entity whose last mention occurs between mj and the closest antecedent of mj. See the sample in Table 1 again. For the pronoun “he”, the following three instances are generated for 3.1 Mention-Pair Model As a baseline, we first describe a learning framework with the mention-pair model as adopted in the work by Soon et al. (2001) and Ng and Cardie (2002). In the learning framework, a training or testing instance has the form of i{mk, mj}, in which mj is an active mention and mk is a preceding mention. An instance is associated with a vector of features, which is used to describe the properties of the two mentions as well as their relationships. Table 2 summarizes the features used in our study. For training, given each encountered anaphoric mention mj in a document, one single positive training instance is created for mj and its closest antecedent. And a group of negative training instances is created for every interv</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>W. Soon, H. Ng, and D. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Specia</author>
<author>M Stevenson</author>
<author>M V Nunes</author>
</authors>
<title>Learning expressive models for words sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>41--48</pages>
<contexts>
<context position="6988" citStr="Specia et al., 2007" startWordPosition="1111" endWordPosition="1114">ch other. The simplest model conditions coreference on mention pairs, but enforces dependency by calculating the distance of a node to a partition (i.e., the probability that an active mention belongs to an entity) based on the sum of its distances to all the nodes in the partition (i.e., the sum of the probability of the active mention co-referring with the mentions in the entity). Inductive Logic Programming (ILP) has been applied to some natural language processing tasks, including parsing (Mooney, 1997), POS disambiguation (Cussens, 1996), lexicon construction (Claveau et al., 2003), WSD (Specia et al., 2007), and so on. However, to our knowledge, our work is the first effort to adopt this technique for the coreference resolution task. 3 Modelling Coreference Resolution Suppose we have a document containing n mentions {mj : 1 &lt; j &lt; n1, in which mj is the jth mention occurring in the document. Let eZ be the ith entity in the document. We define PIL|eZ, mj), (1) the probability that a mention belongs to an entity. Here the random variable L takes a binary value and is 1 if mj is a mention of eZ. By assuming that mentions occurring after mj have no influence on the decision of linking mj to an entity</context>
</contexts>
<marker>Specia, Stevenson, Nunes, 2007</marker>
<rawString>L. Specia, M. Stevenson, and M. V. Nunes. 2007. Learning expressive models for words sense disambiguation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Srinivasan</author>
</authors>
<title>The aleph manual.</title>
<date>2000</date>
<tech>Technical report,</tech>
<institution>Oxford University Computing Laboratory.</institution>
<contexts>
<context position="16863" citStr="Srinivasan (2000)" startWordPosition="2840" endWordPosition="2841">reference resolution task. erence relationship with antecedents in a local discourse. Hence, if an active mention is a pronoun, we only consider the mentions in its previous two sentences for feature computation. 846 ILP uses logic programming as a uniform representation for examples, background knowledge and hypotheses. Given a set of positive and negative example E = E+ U E−, and a set of background knowledge K of the domain, ILP tries to induce a set of hypotheses h that covers most of E+ with no E−, i.e., K ∧ h |= E+ and K ∧ h K E−. In our study, we choose ALEPH2, an ILP implementation by Srinivasan (2000) that has been proven well suited to deal with a large amount of data in multiple domains. For its routine use, ALEPH follows a simple procedure to induce rules. It first selects an example and builds the most specific clause that entertains the example. Next, it tries to search for a clause more general than the bottom one. The best clause is added to the current theory and all the examples made redundant are removed. The procedure repeats until all examples are processed. 4.2 Apply ILP to coreference resolution Given a document, we encode a mention or a partial entity with a unique constant.</context>
</contexts>
<marker>Srinivasan, 2000</marker>
<rawString>A. Srinivasan. 2000. The aleph manual. Technical report, Oxford University Computing Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vilain</author>
<author>J Burger</author>
<author>J Aberdeen</author>
<author>D Connolly</author>
<author>L Hirschman</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message understanding Conference (MUC-6),</booktitle>
<pages>45--52</pages>
<publisher>Morgan Kaufmann Publishers.</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="23619" citStr="Vilain et al. (1995)" startWordPosition="4004" endWordPosition="4007">ntion, as well as the number of the contained mentions, is summarized in Table 3. For both training and resolution, an input raw document was processed by a pipeline of NLP modules including Tokenizer, Part-of-Speech tagger, NP Chunker and Named-Entity (NE) Recognizer. Trained and tested on Penn WSJ TreeBank, the POS tagger could obtain an accuracy of 97% and the NP chunker could produce an F-measure above 94% (Zhou and Su, 2000). Evaluated for the MUC6 and MUC-7 Named-Entity task, the NER module (Zhou and Su, 2002) could provide an F-measure of 96.6% (MUC-6) and 94.1%(MUC-7). For evaluation, Vilain et al. (1995)’s scoring algorithm was adopted to compute recall and precision rates. By default, the ALEPH algorithm only generates rules that have 100% accuracy for the training data. And each rule contains at most three predicates. To accommodate for coreference resolution, we loosened the restrictions to allow rules that have above 50% accuracy and contain up to ten predicates. Default parameters were applied for all the other settings in ALEPH as well as other learning algorithms used in the experiments. 5.2 Results and Discussions Table 4 lists the performance of different coreference resolution syste</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L. Hirschman. 1995. A model-theoretic coreference scoring scheme. In Proceedings of the Sixth Message understanding Conference (MUC-6), pages 45– 52, San Francisco, CA. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yang</author>
<author>J Su</author>
</authors>
<title>Coreference resolution using semantic relatedness information from automatically discovered patterns.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>528--535</pages>
<contexts>
<context position="25446" citStr="Yang and Su, 2007" startWordPosition="4315" endWordPosition="4318">stem that employs the traditional mention-pair model (MP) as described in Section 3.1. From the table, our baseline system achieves a recall of around 66%-68% and a precision of around 50%-60%. The overall F-measure for NWire, NPaper and BNews is 60.4%, 57.9% and 62.9% respectively. The results are comparable to those reported in (Ng, 2005) which uses similar features and gets an F-measure ranging in 50-60% for the same data set. As our system relies only on simple and knowledge-poor features, the achieved Fmeasure is around 2-4% lower than the state-of-theart systems do, like (Ng, 2007) and (Yang and Su, 2007) which utilized sophisticated semantic or realworld knowledge. Since ILP has a strong capability in knowledge management, our system could be further improved if such helpful knowledge is incorporated, which will be explored in our future work. The second line of Table 4 is for the system that employs the entity-mention model (EM) with “Any-X” based entity features, as described in Section 3.2. We can find that the EM model does not show superiority over the baseline MP model. It achieves a higher precision (up to 2.6%), but a lower recall (2.9%), than MP. As a result, we only see ±0.4% differ</context>
</contexts>
<marker>Yang, Su, 2007</marker>
<rawString>X. Yang and J. Su. 2007. Coreference resolution using semantic relatedness information from automatically discovered patterns. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), pages 528–535.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yang</author>
<author>J Su</author>
<author>G Zhou</author>
<author>C Tan</author>
</authors>
<title>Improving pronoun resolution by incorporating coreferential information of candidates.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>127--134</pages>
<location>Barcelona.</location>
<contexts>
<context position="2049" citStr="Yang et al., 2004" startWordPosition="292" endWordPosition="295"> (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional mentionpair counterpart, the entity-mention model aims to make coreference decision at an entity level. Classification is done to determine whether a mention is a referent of a partially found entity. A mention to be resolved (called active mention henceforth) is linked to an appropriate entity chain (if any), based on classification results. One problem that arises with the entity-mention model is how to represent the knowledge related to an entity. In a document, an entity may have more than one mention. It is impractical to enumerate all the mentions in an </context>
<context position="5902" citStr="Yang et al. (2004" startWordPosition="941" endWordPosition="944">tion in the entity has the feature X with the active mention. Culotta et al. (2007) present a system which uses an online learning approach to train a classifier to judge whether two entities are coreferential or not. The features describing the relationships between two entities are obtained based on the information of every possible pair of mentions from the two entities. Different from (Luo et al., 2004), the entitylevel features are computed using a “Most-X” strategy, that is, two given entities would have a feature X, if most of the mention pairs from the two entities have the feature X. Yang et al. (2004b) suggest an entity-based coreference resolution system. The model adopted in the system is similar to the mention-pair model, except that the entity information (e.g., the global number/gender agreement) is considered as additional features of a mention in the entity. McCallum and Wellner (2003) propose several graphical models for coreference analysis. These models aim to overcome the limitation that pairwise coreference decisions are made independently of each other. The simplest model conditions coreference on mention pairs, but enforces dependency by calculating the distance of a node to</context>
<context position="30766" citStr="Yang et al., 2004" startWordPosition="5180" endWordPosition="5183">the coreference resolution task. In Figure 1, we illustrate part of the rules produced by ILP for the entity-mention model (NWire domain), which shows how the relational knowledge of entities and mentions is represented for decision making. An interesting finding, as shown in the last rule of the table, is that multiple non-instantiated arguments (i.e. C and D) could possibly appear in the same rule. According to this rule, a pronominal mention should be linked with a partial entity which contains a named-entity and contains an indefinite NP in a subject position. This supports the claims in (Yang et al., 2004a) that coreferential information is an important factor to evaluate a candidate antecedent in pronoun resolution. Such complex logic makes it possible to capture information of multiple mentions in an entity at the same time, which is difficult to implemented in the mention-pair model and the ordinary entity-mention model with heuristic first-order features. 6 Conclusions This paper presented an expressive entity-mention model for coreference resolution by using Inductive Logic Programming. In contrast to the traditional mention-pair model, our model can capture information beyond single ment</context>
</contexts>
<marker>Yang, Su, Zhou, Tan, 2004</marker>
<rawString>X. Yang, J. Su, G. Zhou, and C. Tan. 2004a. Improving pronoun resolution by incorporating coreferential information of candidates. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 127–134, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yang</author>
<author>J Su</author>
<author>G Zhou</author>
<author>C Tan</author>
</authors>
<title>An NP-cluster approach to coreference resolution.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>219--225</pages>
<location>Geneva.</location>
<contexts>
<context position="2049" citStr="Yang et al., 2004" startWordPosition="292" endWordPosition="295"> (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional mentionpair counterpart, the entity-mention model aims to make coreference decision at an entity level. Classification is done to determine whether a mention is a referent of a partially found entity. A mention to be resolved (called active mention henceforth) is linked to an appropriate entity chain (if any), based on classification results. One problem that arises with the entity-mention model is how to represent the knowledge related to an entity. In a document, an entity may have more than one mention. It is impractical to enumerate all the mentions in an </context>
<context position="5902" citStr="Yang et al. (2004" startWordPosition="941" endWordPosition="944">tion in the entity has the feature X with the active mention. Culotta et al. (2007) present a system which uses an online learning approach to train a classifier to judge whether two entities are coreferential or not. The features describing the relationships between two entities are obtained based on the information of every possible pair of mentions from the two entities. Different from (Luo et al., 2004), the entitylevel features are computed using a “Most-X” strategy, that is, two given entities would have a feature X, if most of the mention pairs from the two entities have the feature X. Yang et al. (2004b) suggest an entity-based coreference resolution system. The model adopted in the system is similar to the mention-pair model, except that the entity information (e.g., the global number/gender agreement) is considered as additional features of a mention in the entity. McCallum and Wellner (2003) propose several graphical models for coreference analysis. These models aim to overcome the limitation that pairwise coreference decisions are made independently of each other. The simplest model conditions coreference on mention pairs, but enforces dependency by calculating the distance of a node to</context>
<context position="30766" citStr="Yang et al., 2004" startWordPosition="5180" endWordPosition="5183">the coreference resolution task. In Figure 1, we illustrate part of the rules produced by ILP for the entity-mention model (NWire domain), which shows how the relational knowledge of entities and mentions is represented for decision making. An interesting finding, as shown in the last rule of the table, is that multiple non-instantiated arguments (i.e. C and D) could possibly appear in the same rule. According to this rule, a pronominal mention should be linked with a partial entity which contains a named-entity and contains an indefinite NP in a subject position. This supports the claims in (Yang et al., 2004a) that coreferential information is an important factor to evaluate a candidate antecedent in pronoun resolution. Such complex logic makes it possible to capture information of multiple mentions in an entity at the same time, which is difficult to implemented in the mention-pair model and the ordinary entity-mention model with heuristic first-order features. 6 Conclusions This paper presented an expressive entity-mention model for coreference resolution by using Inductive Logic Programming. In contrast to the traditional mention-pair model, our model can capture information beyond single ment</context>
</contexts>
<marker>Yang, Su, Zhou, Tan, 2004</marker>
<rawString>X. Yang, J. Su, G. Zhou, and C. Tan. 2004b. An NP-cluster approach to coreference resolution. In Proceedings of the 20th International Conference on Computational Linguistics, pages 219–225, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zhou</author>
<author>J Su</author>
</authors>
<title>Error-driven HMM-based chunk tagger with context-dependent lexicon.</title>
<date>2000</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>71--79</pages>
<location>Hong Kong.</location>
<contexts>
<context position="23432" citStr="Zhou and Su, 2000" startWordPosition="3971" endWordPosition="3974">ng respectively. Each of these sets is further divided into three domains: newswire (NWire), newspaper (NPaper), and broadcast news (BNews). The number of entities with more than one mention, as well as the number of the contained mentions, is summarized in Table 3. For both training and resolution, an input raw document was processed by a pipeline of NLP modules including Tokenizer, Part-of-Speech tagger, NP Chunker and Named-Entity (NE) Recognizer. Trained and tested on Penn WSJ TreeBank, the POS tagger could obtain an accuracy of 97% and the NP chunker could produce an F-measure above 94% (Zhou and Su, 2000). Evaluated for the MUC6 and MUC-7 Named-Entity task, the NER module (Zhou and Su, 2002) could provide an F-measure of 96.6% (MUC-6) and 94.1%(MUC-7). For evaluation, Vilain et al. (1995)’s scoring algorithm was adopted to compute recall and precision rates. By default, the ALEPH algorithm only generates rules that have 100% accuracy for the training data. And each rule contains at most three predicates. To accommodate for coreference resolution, we loosened the restrictions to allow rules that have above 50% accuracy and contain up to ten predicates. Default parameters were applied for all th</context>
</contexts>
<marker>Zhou, Su, 2000</marker>
<rawString>G. Zhou and J. Su. 2000. Error-driven HMM-based chunk tagger with context-dependent lexicon. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 71–79, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zhou</author>
<author>J Su</author>
</authors>
<title>Named Entity recognition using a HMM-based chunk tagger.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>473--480</pages>
<location>Philadelphia.</location>
<contexts>
<context position="23520" citStr="Zhou and Su, 2002" startWordPosition="3988" endWordPosition="3991">re), newspaper (NPaper), and broadcast news (BNews). The number of entities with more than one mention, as well as the number of the contained mentions, is summarized in Table 3. For both training and resolution, an input raw document was processed by a pipeline of NLP modules including Tokenizer, Part-of-Speech tagger, NP Chunker and Named-Entity (NE) Recognizer. Trained and tested on Penn WSJ TreeBank, the POS tagger could obtain an accuracy of 97% and the NP chunker could produce an F-measure above 94% (Zhou and Su, 2000). Evaluated for the MUC6 and MUC-7 Named-Entity task, the NER module (Zhou and Su, 2002) could provide an F-measure of 96.6% (MUC-6) and 94.1%(MUC-7). For evaluation, Vilain et al. (1995)’s scoring algorithm was adopted to compute recall and precision rates. By default, the ALEPH algorithm only generates rules that have 100% accuracy for the training data. And each rule contains at most three predicates. To accommodate for coreference resolution, we loosened the restrictions to allow rules that have above 50% accuracy and contain up to ten predicates. Default parameters were applied for all the other settings in ALEPH as well as other learning algorithms used in the experiments. </context>
</contexts>
<marker>Zhou, Su, 2002</marker>
<rawString>G. Zhou and J. Su. 2002. Named Entity recognition using a HMM-based chunk tagger. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 473–480, Philadelphia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>