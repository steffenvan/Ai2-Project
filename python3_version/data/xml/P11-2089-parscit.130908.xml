<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004054">
<title confidence="0.9959955">
They Can Help: Using Crowdsourcing to Improve the Evaluation of
Grammatical Error Detection Systems
</title>
<author confidence="0.892407">
Nitin Madnania Joel Tetreaulta Martin Chodorowb Alla Rozovskayac
</author>
<affiliation confidence="0.543135">
aEducational Testing Service
</affiliation>
<address confidence="0.692503">
Princeton, NJ
</address>
<email confidence="0.960935">
{nmadnani,jtetreault}@ets.org
</email>
<affiliation confidence="0.67506">
bHunter College of CUNY
</affiliation>
<email confidence="0.977625">
martin.chodorow@hunter.cuny.edu
</email>
<affiliation confidence="0.913511">
cUniversity of Illinois at Urbana-Champaign
</affiliation>
<email confidence="0.990105">
rozovska@illinois.edu
</email>
<sectionHeader confidence="0.995413" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9991035">
Despite the rising interest in developing gram-
matical error detection systems for non-native
speakers of English, progress in the field has
been hampered by a lack of informative met-
rics and an inability to directly compare the
performance of systems developed by differ-
ent researchers. In this paper we address
these problems by presenting two evaluation
methodologies, both based on a novel use of
crowdsourcing.
</bodyText>
<sectionHeader confidence="0.882548" genericHeader="categories and subject descriptors">
1 Motivation and Contributions
</sectionHeader>
<bodyText confidence="0.999975339622641">
One of the fastest growing areas in need of NLP
tools is the field of grammatical error detection for
learners of English as a Second Language (ESL).
According to Guo and Beckett (2007), “over a bil-
lion people speak English as their second or for-
eign language.” This high demand has resulted in
many NLP research papers on the topic, a Synthesis
Series book (Leacock et al., 2010) and a recurring
workshop (Tetreault et al., 2010a), all in the last five
years. In this year’s ACL conference, there are four
long papers devoted to this topic.
Despite the growing interest, two major factors
encumber the growth of this subfield. First, the lack
of consistent and appropriate score reporting is an
issue. Most work reports results in the form of pre-
cision and recall as measured against the judgment
of a single human rater. This is problematic because
most usage errors (such as those in article and prepo-
sition usage) are a matter of degree rather than sim-
ple rule violations such as number agreement. As a
consequence, it is common for two native speakers
to have different judgments of usage. Therefore, an
appropriate evaluation should take this into account
by not only enlisting multiple human judges but also
aggregating these judgments in a graded manner.
Second, systems are hardly ever compared to each
other. In fact, to our knowledge, no two systems
developed by different groups have been compared
directly within the field primarily because there is
no common corpus or shared task—both commonly
found in other NLP areas such as machine transla-
tion.1 For example, Tetreault and Chodorow (2008),
Gamon et al. (2008) and Felice and Pulman (2008)
developed preposition error detection systems, but
evaluated on three different corpora using different
evaluation measures.
The goal of this paper is to address the above
issues by using crowdsourcing, which has been
proven effective for collecting multiple, reliable
judgments in other NLP tasks: machine transla-
tion (Callison-Burch, 2009; Zaidan and Callison-
Burch, 2010), speech recognition (Evanini et al.,
2010; Novotney and Callison-Burch, 2010), au-
tomated paraphrase generation (Madnani, 2010),
anaphora resolution (Chamberlain et al., 2009),
word sense disambiguation (Akkaya et al., 2010),
lexicon construction for less commonly taught lan-
guages (Irvine and Klementiev, 2010), fact min-
ing (Wang and Callison-Burch, 2010) and named
entity recognition (Finin et al., 2010) among several
others.
In particular, we make a significant contribution
to the field by showing how to leverage crowdsourc-
</bodyText>
<footnote confidence="0.983111">
1There has been a recent proposal for a related shared
task (Dale and Kilgarriff, 2010) that shows promise.
</footnote>
<page confidence="0.824698">
508
</page>
<note confidence="0.6278605">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 508–513,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999795">
ing to both address the lack of appropriate evaluation
metrics and to make system comparison easier. Our
solution is general enough for, in the simplest case,
intrinsically evaluating a single system on a single
dataset and, more realistically, comparing two dif-
ferent systems (from same or different groups).
</bodyText>
<sectionHeader confidence="0.921139" genericHeader="method">
2 A Case Study: Extraneous Prepositions
</sectionHeader>
<bodyText confidence="0.99988905">
We consider the problem of detecting an extraneous
preposition error, i.e., incorrectly using a preposi-
tion where none is licensed. In the sentence “They
came to outside”, the preposition to is an extrane-
ous error whereas in the sentence “They arrived
to the town” the preposition to is a confusion er-
ror (cf. arrived in the town). Most work on au-
tomated correction of preposition errors, with the
exception of Gamon (2010), addresses preposition
confusion errors e.g., (Felice and Pulman, 2008;
Tetreault and Chodorow, 2008; Rozovskaya and
Roth, 2010b). One reason is that in addition to the
standard context-based features used to detect con-
fusion errors, identifying extraneous prepositions
also requires actual knowledge of when a preposi-
tion can and cannot be used. Despite this lack of
attention, extraneous prepositions account for a sig-
nificant proportion—as much as 18% in essays by
advanced English learners (Rozovskaya and Roth,
2010a)—of all preposition usage errors.
</bodyText>
<subsectionHeader confidence="0.994845">
2.1 Data and Systems
</subsectionHeader>
<bodyText confidence="0.999981658536585">
For the experiments in this paper, we chose a propri-
etary corpus of about 500,000 essays written by ESL
students for Test of English as a Foreign Language
(TOEFL R�). Despite being common ESL errors,
preposition errors are still infrequent overall, with
over 90% of prepositions being used correctly (Lea-
cock et al., 2010; Rozovskaya and Roth, 2010a).
Given this fact about error sparsity, we needed an ef-
ficient method to extract a good number of error in-
stances (for statistical reliability) from the large es-
say corpus. We found all trigrams in our essays con-
taining prepositions as the middle word (e.g., marry
with her) and then looked up the counts of each tri-
gram and the corresponding bigram with the prepo-
sition removed (marry her) in the Google Web1T
5-gram Corpus. If the trigram was unattested or had
a count much lower than expected based on the bi-
gram count, then we manually inspected the trigram
to see whether it was actually an error. If it was,
we extracted a sentence from the large essay corpus
containing this erroneous trigram. Once we had ex-
tracted 500 sentences containing extraneous prepo-
sition error instances, we added 500 sentences con-
taining correct instances of preposition usage. This
yielded a corpus of 1000 sentences with a 50% error
rate.
These sentences, with the target preposition high-
lighted, were presented to 3 expert annotators who
are native English speakers. They were asked to
annotate the preposition usage instance as one of
the following: extraneous (Error), not extraneous
(OK) or too hard to decide (Unknown); the last cat-
egory was needed for cases where the context was
too messy to make a decision about the highlighted
preposition. On average, the three experts had an
agreement of 0.87 and a kappa of 0.75. For subse-
quent analysis, we only use the classes Error and
OK since Unknown was used extremely rarely and
never by all 3 experts for the same sentence.
We used two different error detection systems to
illustrate our evaluation methodology:2
</bodyText>
<listItem confidence="0.829577777777778">
• LM: A 4-gram language model trained on
the Google Web1T 5-gram Corpus with
SRILM (Stolcke, 2002).
• PERC: An averaged Perceptron (Freund and
Schapire, 1999) classifier— as implemented in
the Learning by Java toolkit (Rizzolo and Roth,
2007)—trained on 7 million examples and us-
ing the same features employed by Tetreault
and Chodorow (2008).
</listItem>
<sectionHeader confidence="0.972168" genericHeader="method">
3 Crowdsourcing
</sectionHeader>
<bodyText confidence="0.999766777777778">
Recently,we showed that Amazon Mechanical Turk
(AMT) is a cheap and effective alternative to expert
raters for annotating preposition errors (Tetreault et
al., 2010b). In other current work, we have extended
this pilot study to show that CrowdFlower, a crowd-
sourcing service that allows for stronger quality con-
trol on untrained human raters (henceforth, Turkers),
is more reliable than AMT on three different error
detection tasks (article errors, confused prepositions
</bodyText>
<footnote confidence="0.9594265">
2Any conclusions drawn in this paper pertain only to these
specific instantiations of the two systems.
</footnote>
<page confidence="0.997026">
509
</page>
<bodyText confidence="0.999985">
&amp; extraneous prepositions). To impose such quality
control, one has to provide “gold” instances, i.e., ex-
amples with known correct judgments that are then
used to root out any Turkers with low performance
on these instances. For all three tasks, we obtained
20 Turkers’ judgments via CrowdFlower for each in-
stance and found that, on average, only 3 Turkers
were required to match the experts.
More specifically, for the extraneous preposition
error task, we used 75 sentences as gold and ob-
tained judgments for the remaining 923 non-gold
sentences.3 We found that if we used 3 Turker judg-
ments in a majority vote, the agreement with any one
of the three expert raters is, on average, 0.87 with a
kappa of 0.76. This is on par with the inter-expert
agreement and kappa found earlier (0.87 and 0.75
respectively).
The extraneous preposition annotation cost only
$325 (923 judgments x 20 Turkers) and was com-
pleted in a single day. The only restriction on the
Turkers was that they be physically located in the
USA. For the analysis in subsequent sections, we
use these 923 sentences and the respective 20 judg-
ments obtained via CrowdFlower. The 3 expert
judgments are not used any further in this analysis.
</bodyText>
<sectionHeader confidence="0.986856" genericHeader="method">
4 Revamping System Evaluation
</sectionHeader>
<bodyText confidence="0.999793166666667">
In this section, we provide details on how crowd-
sourcing can help revamp the evaluation of error de-
tection systems: (a) by providing more informative
measures for the intrinsic evaluation of a single sys-
tem (§ 4.1), and (b) by easily enabling system com-
parison (§ 4.2).
</bodyText>
<subsectionHeader confidence="0.997659">
4.1 Crowd-informed Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.985546457142857">
When evaluating the performance of grammatical
error detection systems against human judgments,
the judgments for each instance are generally re-
duced to the single most frequent category: Error
or OK. This reduction is not an accurate reflection
of a complex phenomenon. It discards valuable in-
formation about the acceptability of usage because
it treats all “bad” uses as equal (and all good ones
as equal), when they are not. Arguably, it would
be fairer to use a continuous scale, such as the pro-
portion of raters who judge an instance as correct or
3We found 2 duplicate sentences and removed them.
incorrect. For example, if 90% of raters agree on a
rating of Error for an instance of preposition usage,
then that is stronger evidence that the usage is an er-
ror than if 56% of Turkers classified it as Error and
44% classified it as OK (the sentence “In addition
classmates play with some game and enjoy” is an ex-
ample). The regular measures of precision and recall
would be fairer if they reflected this reality. Besides
fairness, another reason to use a continuous scale is
that of stability, particularly with a small number of
instances in the evaluation set (quite common in the
field). By relying on majority judgments, precision
and recall measures tend to be unstable (see below).
We modify the measures of precision and re-
call to incorporate distributions of correctness, ob-
tained via crowdsourcing, in order to make them
fairer and more stable indicators of system perfor-
mance. Given an error detection system that classi-
fies a sentence containing a specific preposition as
Error (class 1) if the preposition is extraneous and
OK (class 0) otherwise, we propose the following
weighted versions of hits (Hw), misses (Mw) and
false positives (FPw):
</bodyText>
<equation confidence="0.997074">
N
Hw = (cisys * picrowd) (1)
i
N
Mw = ((1 − cisys) * picrowd) (2)
i
N
FPw = (cisys * (1 − picrowd)) (3)
i
</equation>
<bodyText confidence="0.999582416666667">
In the above equations, N is the total number of
instances, cisys is the class (1 or 0) , and picrowd
indicates the proportion of the crowd that classi-
fied instance i as Error. Note that if we were to
revert to the majority crowd judgment as the sole
judgment for each instance, instead of proportions,
picrowd would always be either 1 or 0 and the above
formulae would simply compute the normal hits,
misses and false positives. Given these definitions,
weighted precision can be defined as Precisionw =
Hw/(Hw + FPw) and weighted recall as Recallw =
Hw/(Hw + Mw).
</bodyText>
<page confidence="0.980724">
510
</page>
<figure confidence="0.998494222222222">
Precision/Recall
500
400
300
200
100
0
50 60 70 80 90 100
agreement
</figure>
<figureCaption confidence="0.98991">
Figure 1: Histogram of Turker agreements for all 923 in-
stances on whether a preposition is extraneous.
</figureCaption>
<table confidence="0.994945666666667">
Precision Recall
Unweighted 0.957 0.384
Weighted 0.900 0.371
</table>
<tableCaption confidence="0.9694645">
Table 1: Comparing commonly used (unweighted) and
proposed (weighted) precision/recall measures for LM.
</tableCaption>
<bodyText confidence="0.999972619047619">
To illustrate the utility of these weighted mea-
sures, we evaluated the LM and PERC systems
on the dataset containing 923 preposition instances,
against all 20 Turker judgments. Figure 1 shows a
histogram of the Turker agreement for the major-
ity rating over the set. Table 1 shows both the un-
weighted (discrete majority judgment) and weighted
(continuous Turker proportion) versions of precision
and recall for this system.
The numbers clearly show that in the unweighted
case, the performance of the system is overesti-
mated simply because the system is getting as much
credit for each contentious case (low agreement)
as for each clear one (high agreement). In the
weighted measure we propose, the contentious cases
are weighted lower and therefore their contribution
to the overall performance is reduced. This is a
fairer representation since the system should not be
expected to perform as well on the less reliable in-
stances as it does on the clear-cut instances. Essen-
tially, if humans cannot consistently decide whether
</bodyText>
<figure confidence="0.702976">
Agreement Bin
</figure>
<figureCaption confidence="0.9989495">
Figure 2: Unweighted precision/recall by agreement bins
for LM &amp; PERC.
</figureCaption>
<bodyText confidence="0.999471428571428">
a case is an error then a system’s output cannot be
considered entirely right or entirely wrong.4
As an added advantage, the weighted measures
are more stable. Consider a contentious instance in
a small dataset where 7 out of 15 Turkers (a minor-
ity) classified it as Error. However, it might easily
have happened that 8 Turkers (a majority) classified
it as Error instead of 7. In that case, the change in
unweighted precision would have been much larger
than is warranted by such a small change in the
data. However, weighted precision is guaranteed to
be more stable. Note that the instability decreases
as the size of the dataset increases but still remains a
problem.
</bodyText>
<subsectionHeader confidence="0.998259">
4.2 Enabling System Comparison
</subsectionHeader>
<bodyText confidence="0.999845444444444">
In this section, we show how to easily compare dif-
ferent systems both on the same data (in the ideal
case of a shared dataset being available) and, more
realistically, on different datasets. Figure 2 shows
(unweighted) precision and recall of LM and PERC
(computed against the majority Turker judgment)
for three agreement bins, where each bin is defined
as containing only the instances with Turker agree-
ment in a specific range. We chose the bins shown
</bodyText>
<footnote confidence="0.9727995">
4The difference between unweighted and weighted mea-
sures can vary depending on the distribution of agreement.
</footnote>
<figure confidence="0.63737475">
50−75% 75−90% 90−100%
[n=93] [n=114] [n=716]
0.0 0.2 0.4 0.6 0.8 1.0
LM Precision
PERC Precision
LM Recall
PERC Recall
count
</figure>
<page confidence="0.991289">
511
</page>
<bodyText confidence="0.997169956521739">
since they are sufficiently large and represent a rea- yield fairer and more stable indicators of perfor-
sonable stratification of the agreement space. Note mance.
that we are not weighting the precision and recall in For system comparison, we argue that the best
this case since we have already used the agreement solution is to use a shared dataset and present the
proportions to create the bins. precision-agreement plot using a set of agreed-upon
This curve enables us to compare the two sys- bins (possibly in conjunction with the weighted pre-
tems easily on different levels of item contentious- cision and recall measures) for a more informative
ness and, therefore, conveys much more information comparison. However, we recognize that shared
than what is usually reported (a single number for datasets are harder to create in this field (as most of
unweighted precision/recall over the whole corpus). the data is proprietary). Therefore, we also provide
For example, from this graph, PERC is seen to have a way to compare multiple systems across differ-
similar performance as LM for the 75-90% agree- ent datasets by using kappa-agreement plots. As for
ment bin. In addition, even though LM precision is agreement bins, we posit that the agreement values
perfect (1.0) for the most contentious instances (the used to define them depend on the task and, there-
50-75% bin), this turns out to be an artifact of the fore, should be determined by the community.
LM classifier’s decision process. When it must de- Note that both of these practices can also be im-
cide between what it views as two equally likely pos- plemented by using 20 experts instead of 20 Turkers.
sibilities, it defaults to OK. Therefore, even though However, we show that crowdsourcing yields judg-
LM has higher unweighted precision (0.957) than ments that are as good but without the cost. To fa-
PERC (0.813), it is only really better on the most cilitate the adoption of these practices, we make all
clear-cut cases (the 90-100% bin). If one were to re- our evaluation code and data available to the com-
port unweighted precision and recall without using munity.5
any bins—as is the norm—this important qualifica- Acknowledgments
tion would have been harder to discover. We would first like to thank our expert annotators
While this example uses the same dataset for eval- Sarah Ohls and Waverely VanWinkle for their hours
uating two systems, the procedure is general enough of hard work. We would also like to acknowledge
to allow two systems to be compared on two dif- Lei Chen, Keelan Evanini, Jennifer Foster, Derrick
ferent datasets by simply examining the two plots. Higgins and the three anonymous reviewers for their
However, two potential issues arise in that case. The helpful comments and feedback.
first is that the bin sizes will likely vary across the
two plots. However, this should not be a significant
problem as long as the bins are sufficiently large. A
second, more serious, issue is that the error rates (the
proportion of instances that are actually erroneous)
in each bin may be different across the two plots. To
handle this, we recommend that a kappa-agreement
plot be used instead of the precision-agreement plot
shown here.
5 Conclusions
Our goal is to propose best practices to address the
two primary problems in evaluating grammatical er-
ror detection systems and we do so by leveraging
crowdsourcing. For system development, we rec-
ommend that rather than compressing multiple judg-
ments down to the majority, it is better to use agree-
ment proportions to weight precision and recall to
</bodyText>
<table confidence="0.970387833333333">
512
References
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon Mechanical Turk
for Subjectivity Word Sense Disambiguation. In Pro-
ceedings of the NAACL Workshop on Creating Speech
and Language Data with Amazon’s Mechanical Turk,
pages 195–203.
Chris Callison-Burch. 2009. Fast, Cheap, and Creative:
Evaluating Translation Quality Using Amazon’s Me-
chanical Turk. In Proceedings of EMNLP, pages 286–
295.
Jon Chamberlain, Massimo Poesio, and Udo Kruschwitz.
2009. A Demonstration of Human Computation Us-
ing the Phrase Detectives Annotation Game. In ACM
SIGKDD Workshop on Human Computation, pages
23–24.
5http://bit.ly/crowdgrammar
</table>
<reference confidence="0.992994875">
Robert Dale and Adam Kilgarriff. 2010. Helping Our
Own: Text Massaging for Computational Linguistics
as a New Shared Task. In Proceedings of INLG.
Keelan Evanini, Derrick Higgins, and Klaus Zechner.
2010. Using Amazon Mechanical Turk for Transcrip-
tion of Non-Native Speech. In Proceedings of the
NAACL Workshop on Creating Speech and Language
Data with Amazon’s Mechanical Turk, pages 53–56.
Rachele De Felice and Stephen Pulman. 2008. A
Classifier-Based Approach to Preposition and Deter-
miner Error Correction in L2 English. In Proceedings
of COLING, pages 169–176.
Tim Finin, William Murnane, Anand Karandikar,
Nicholas Keller, Justin Martineau, and Mark Dredze.
2010. Annotating Named Entities in Twitter Data with
Crowdsourcing. In Proceedings of the NAACL Work-
shop on Creating Speech and Language Data with
Amazon’s Mechanical Turk, pages 80–88.
Yoav Freund and Robert E. Schapire. 1999. Large Mar-
gin Classification Using the Perceptron Algorithm.
Machine Learning, 37(3):277–296.
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
der Klementiev, William Dolan, Dmitriy Belenko, and
Lucy Vanderwende. 2008. Using Contextual Speller
Techniques and Language Modeling for ESL Error
Correction. In Proceedings of IJCNLP.
Michael Gamon. 2010. Using Mostly Native Data to
Correct Errors in Learners’ Writing. In Proceedings
of NAACL, pages 163–171.
Y. Guo and Gulbahar Beckett. 2007. The Hegemony
of English as a Global Language: Reclaiming Local
Knowledge and Culture in China. Convergence: In-
ternational Journal of Adult Education, 1.
Ann Irvine and Alexandre Klementiev. 2010. Using
Mechanical Turk to Annotate Lexicons for Less Com-
monly Used Languages. In Proceedings of the NAACL
Workshop on Creating Speech and Language Data
with Amazon’s Mechanical Turk, pages 108–113.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Morgan
Claypool.
Nitin Madnani. 2010. The Circle of Meaning: From
Translation to Paraphrasing and Back. Ph.D. thesis,
Department of Computer Science, University of Mary-
land College Park.
Scott Novotney and Chris Callison-Burch. 2010. Cheap,
Fast and Good Enough: Automatic Speech Recogni-
tion with Non-Expert Transcription. In Proceedings
of NAACL, pages 207–215.
Nicholas Rizzolo and Dan Roth. 2007. Modeling
Discriminative Global Inference. In Proceedings of
the First IEEE International Conference on Semantic
Computing (ICSC), pages 597–604, Irvine, California,
September.
Alla Rozovskaya and D. Roth. 2010a. Annotating ESL
errors: Challenges and rewards. In Proceedings of the
NAACL Workshop on Innovative Use of NLPfor Build-
ing Educational Applications.
Alla Rozovskaya and D. Roth. 2010b. Generating Con-
fusion Sets for Context-Sensitive Error Correction. In
Proceedings of EMNLP.
Andreas Stolcke. 2002. SRILM: An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 257–286.
Joel Tetreault and Martin Chodorow. 2008. The Ups and
Downs of Preposition Error Detection in ESL Writing.
In Proceedings of COLING, pages 865–872.
Joel Tetreault, Jill Burstein, and Claudia Leacock, edi-
tors. 2010a. Proceedings of the NAACL Workshop on
Innovative Use of NLP for Building Educational Ap-
plications.
Joel Tetreault, Elena Filatova, and Martin Chodorow.
2010b. Rethinking Grammatical Error Annotation and
Evaluation with the Amazon Mechanical Turk. In Pro-
ceedings of the NAACL Workshop on Innovative Use
of NLP for Building Educational Applications, pages
45–48.
Rui Wang and Chris Callison-Burch. 2010. Cheap Facts
and Counter-Facts. In Proceedings of the NAACL
Workshop on Creating Speech and Language Data
with Amazon’s Mechanical Turk, pages 163–167.
Omar F. Zaidan and Chris Callison-Burch. 2010. Pre-
dicting Human-Targeted Translation Edit Rate via Un-
trained Human Annotators. In Proceedings of NAACL,
pages 369–372.
</reference>
<page confidence="0.998856">
513
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.998598">They Can Help: Using Crowdsourcing to Improve the Evaluation Grammatical Error Detection Systems</title>
<author confidence="0.6362125">Joel Martin Alla Testing</author>
<affiliation confidence="0.653425">Princeton,</affiliation>
<address confidence="0.6169285">College of of Illinois at</address>
<email confidence="0.987004">rozovska@illinois.edu</email>
<abstract confidence="0.980793847619048">Despite the rising interest in developing grammatical error detection systems for non-native speakers of English, progress in the field has been hampered by a lack of informative metrics and an inability to directly compare the performance of systems developed by different researchers. In this paper we address these problems by presenting two evaluation methodologies, both based on a novel use of crowdsourcing. 1 Motivation and Contributions One of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in many NLP research papers on the topic, a Synthesis Series book (Leacock et al., 2010) and a recurring workshop (Tetreault et al., 2010a), all in the last five years. In this year’s ACL conference, there are four long papers devoted to this topic. Despite the growing interest, two major factors encumber the growth of this subfield. First, the lack of consistent and appropriate score reporting is an issue. Most work reports results in the form of precision and recall as measured against the judgment of a single human rater. This is problematic because most usage errors (such as those in article and preposition usage) are a matter of degree rather than simple rule violations such as number agreement. As a consequence, it is common for two native speakers to have different judgments of usage. Therefore, an appropriate evaluation should take this into account by not only enlisting multiple human judges but also aggregating these judgments in a graded manner. Second, systems are hardly ever compared to each other. In fact, to our knowledge, no two systems developed by different groups have been compared directly within the field primarily because there is no common corpus or shared task—both commonly found in other NLP areas such as machine transla- For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but on three using evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and Callison- Burch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution the field by showing how to leverage crowdsourchas been a recent proposal for a related shared task (Dale and Kilgarriff, 2010) that shows promise. 508 of the 49th Annual Meeting of the Association for Computational pages 508–513, Oregon, June 19-24, 2011. Association for Computational Linguistics ing to both address the lack of appropriate evaluation metrics and to make system comparison easier. Our solution is general enough for, in the simplest case, intrinsically evaluating a single system on a single dataset and, more realistically, comparing two different systems (from same or different groups). 2 A Case Study: Extraneous Prepositions consider the problem of detecting an i.e., incorrectly using a preposiwhere none is licensed. In the sentence to the preposition an extraneerror whereas in the sentence arrived the town” preposition a confusion er- (cf. in the Most work on automated correction of preposition errors, with the exception of Gamon (2010), addresses preposition confusion errors e.g., (Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010b). One reason is that in addition to the standard context-based features used to detect confusion errors, identifying extraneous prepositions also requires actual knowledge of when a preposition can and cannot be used. Despite this lack of attention, extraneous prepositions account for a significant proportion—as much as 18% in essays by advanced English learners (Rozovskaya and Roth, 2010a)—of all preposition usage errors. 2.1 Data and Systems For the experiments in this paper, we chose a proprietary corpus of about 500,000 essays written by ESL students for Test of English as a Foreign Language Despite being common ESL errors, preposition errors are still infrequent overall, with over 90% of prepositions being used correctly (Leacock et al., 2010; Rozovskaya and Roth, 2010a). Given this fact about error sparsity, we needed an efficient method to extract a good number of error instances (for statistical reliability) from the large essay corpus. We found all trigrams in our essays conprepositions as the middle word (e.g., and then looked up the counts of each trigram and the corresponding bigram with the preporemoved in the Google Web1T 5-gram Corpus. If the trigram was unattested or had count much lower than expected based on the bigram count, then we manually inspected the trigram to see whether it was actually an error. If it was, we extracted a sentence from the large essay corpus containing this erroneous trigram. Once we had extracted 500 sentences containing extraneous preposition error instances, we added 500 sentences containing correct instances of preposition usage. This yielded a corpus of 1000 sentences with a 50% error rate. These sentences, with the target preposition highlighted, were presented to 3 expert annotators who are native English speakers. They were asked to annotate the preposition usage instance as one of following: extraneous not extraneous or too hard to decide the last category was needed for cases where the context was too messy to make a decision about the highlighted preposition. On average, the three experts had an agreement of 0.87 and a kappa of 0.75. For subseanalysis, we only use the classes used extremely rarely and never by all 3 experts for the same sentence. We used two different error detection systems to our evaluation • A 4-gram language model trained on the Google Web1T 5-gram Corpus with SRILM (Stolcke, 2002). • An averaged Perceptron (Freund and Schapire, 1999) classifier— as implemented in the Learning by Java toolkit (Rizzolo and Roth, 2007)—trained on 7 million examples and using the same features employed by Tetreault and Chodorow (2008). 3 Crowdsourcing Recently,we showed that Amazon Mechanical Turk (AMT) is a cheap and effective alternative to expert raters for annotating preposition errors (Tetreault et al., 2010b). In other current work, we have extended this pilot study to show that CrowdFlower, a crowdsourcing service that allows for stronger quality control on untrained human raters (henceforth, Turkers), is more reliable than AMT on three different error detection tasks (article errors, confused prepositions conclusions drawn in this paper pertain only to these specific instantiations of the two systems. 509 &amp; extraneous prepositions). To impose such quality control, one has to provide “gold” instances, i.e., examples with known correct judgments that are then used to root out any Turkers with low performance on these instances. For all three tasks, we obtained 20 Turkers’ judgments via CrowdFlower for each instance and found that, on average, only 3 Turkers were required to match the experts. More specifically, for the extraneous preposition error task, we used 75 sentences as gold and obtained judgments for the remaining 923 non-gold We found that if we used 3 Turker judgments in a majority vote, the agreement with any one of the three expert raters is, on average, 0.87 with a kappa of 0.76. This is on par with the inter-expert agreement and kappa found earlier (0.87 and 0.75 respectively). The extraneous preposition annotation cost only (923 judgments Turkers) and was completed in a single day. The only restriction on the Turkers was that they be physically located in the USA. For the analysis in subsequent sections, we use these 923 sentences and the respective 20 judgments obtained via CrowdFlower. The 3 expert are any further in this analysis. 4 Revamping System Evaluation In this section, we provide details on how crowdsourcing can help revamp the evaluation of error detection systems: (a) by providing more informative measures for the intrinsic evaluation of a single sysand (b) by easily enabling system com- 4.1 Crowd-informed Evaluation Measures When evaluating the performance of grammatical error detection systems against human judgments, the judgments for each instance are generally reto the single most frequent category: This reduction is not an accurate reflection of a complex phenomenon. It discards valuable information about the acceptability of usage because it treats all “bad” uses as equal (and all good ones as equal), when they are not. Arguably, it would be fairer to use a continuous scale, such as the proportion of raters who judge an instance as correct or found 2 duplicate sentences and removed them. incorrect. For example, if 90% of raters agree on a of an instance of preposition usage, then that is stronger evidence that the usage is an erthan if 56% of Turkers classified it as classified it as sentence addition play withsome game and is an example). The regular measures of precision and recall would be fairer if they reflected this reality. Besides fairness, another reason to use a continuous scale is that of stability, particularly with a small number of instances in the evaluation set (quite common in the field). By relying on majority judgments, precision and recall measures tend to be unstable (see below). We modify the measures of precision and recall to incorporate distributions of correctness, obtained via crowdsourcing, in order to make them fairer and more stable indicators of system performance. Given an error detection system that classifies a sentence containing a specific preposition as 1) if the preposition is extraneous and 0) otherwise, we propose the following versions of hits misses and positives N = i N = ((1 i N = i In the above equations, N is the total number of the class (1 or 0) , and indicates the proportion of the crowd that classiinstance Note that if we were to revert to the majority crowd judgment as the sole judgment for each instance, instead of proportions, always be either 1 or 0 and the above formulae would simply compute the normal hits, misses and false positives. Given these definitions, precision can be defined as = + weighted recall as = + 510 Precision/Recall 500 400 300 200 100 0 50 60 70 80 90 100 agreement Figure 1: Histogram of Turker agreements for all 923 instances on whether a preposition is extraneous. Precision Recall Unweighted 0.957 0.384 Weighted 0.900 0.371 Table 1: Comparing commonly used (unweighted) and proposed (weighted) precision/recall measures for LM. To illustrate the utility of these weighted measures, we evaluated the LM and PERC systems on the dataset containing 923 preposition instances, against all 20 Turker judgments. Figure 1 shows a histogram of the Turker agreement for the majority rating over the set. Table 1 shows both the unweighted (discrete majority judgment) and weighted (continuous Turker proportion) versions of precision and recall for this system. The numbers clearly show that in the unweighted case, the performance of the system is overestimated simply because the system is getting as much credit for each contentious case (low agreement) as for each clear one (high agreement). In the weighted measure we propose, the contentious cases are weighted lower and therefore their contribution to the overall performance is reduced. This is a fairer representation since the system should not be expected to perform as well on the less reliable instances as it does on the clear-cut instances. Essentially, if humans cannot consistently decide whether Agreement Bin Figure 2: Unweighted precision/recall by agreement bins for LM &amp; PERC. a case is an error then a system’s output cannot be entirely right or entirely As an added advantage, the weighted measures are more stable. Consider a contentious instance in a small dataset where 7 out of 15 Turkers (a minorclassified it as However, it might easily have happened that 8 Turkers (a majority) classified as of 7. In that case, the change in unweighted precision would have been much larger than is warranted by such a small change in the data. However, weighted precision is guaranteed to be more stable. Note that the instability decreases as the size of the dataset increases but still remains a problem. 4.2 Enabling System Comparison In this section, we show how to easily compare different systems both on the same data (in the ideal case of a shared dataset being available) and, more realistically, on different datasets. Figure 2 shows (unweighted) precision and recall of LM and PERC (computed against the majority Turker judgment) three where each bin is defined as containing only the instances with Turker agreement in a specific range. We chose the bins shown difference between unweighted and weighted measures can vary depending on the distribution of agreement. 50−75% 75−90% 90−100% [n=93] [n=114] [n=716] 0.0 0.2 0.4 0.6 0.8 1.0</abstract>
<title confidence="0.75756125">LM Precision PERC Precision LM Recall PERC Recall</title>
<abstract confidence="0.949872666666667">count 511 since they are sufficiently large and represent a rea-sonable stratification of the agreement space. Note we are the precision and recall in this case since we have already used the agreement proportions to create the bins. yield fairer and more stable indicators of perfor-mance.</abstract>
<note confidence="0.8939189375">This curve enables us to compare the two sys-tems easily on different levels of item contentious-ness and, therefore, conveys much more information than what is usually reported (a single number for unweighted precision/recall over the whole corpus). For example, from this graph, PERC is seen to have similar performance as LM for the 75-90% agree-ment bin. In addition, even though LM precision is perfect (1.0) for the most contentious instances (the 50-75% bin), this turns out to be an artifact of the LM classifier’s decision process. When it must de-cide between what it views as two equally likely posit defaults to Therefore, even though LM has higher unweighted precision (0.957) than PERC (0.813), it is only really better on the most clear-cut cases (the 90-100% bin). If one were to re-port unweighted precision and recall without using any bins—as is the norm—this important qualifica-tion would have been harder to discover. For system comparison, we argue that the best solution is to use a shared dataset and present the precision-agreement plot using a set of agreed-upon bins (possibly in conjunction with the weighted pre-cision and recall measures) for a more informative comparison. However, we recognize that shared datasets are harder to create in this field (as most of the data is proprietary). Therefore, we also provide way to compare multiple systems across differby using kappa-agreement plots. As for agreement bins, we posit that the agreement values used to define them depend on the task and, there-fore, should be determined by the community. While this example uses the same dataset for eval-uating two systems, the procedure is general enough allow two systems to be compared on two difby simply examining the two plots. However, two potential issues arise in that case. The first is that the bin sizes will likely vary across the two plots. However, this should not be a significant problem as long as the bins are sufficiently large. A second, more serious, issue is that the error rates (the proportion of instances that are actually erroneous) in each bin may be different across the two plots. To handle this, we recommend that a kappa-agreement plot be used instead of the precision-agreement plot shown here. Note that both of these practices can also be im-plemented by using 20 experts instead of 20 Turkers. However, we show that crowdsourcing yields judg-ments that are as good but without the cost. To fa-cilitate the adoption of these practices, we make all our evaluation code and data available to the com- 5 Conclusions Acknowledgments Our goal is to propose best practices to address the two primary problems in evaluating grammatical er-ror detection systems and we do so by leveraging crowdsourcing. For system development, we rec-ommend that rather than compressing multiple judg-ments down to the majority, it is better to use agree-ment proportions to weight precision and recall to We would first like to thank our expert annotators Sarah Ohls and Waverely VanWinkle for their hours of hard work. We would also like to acknowledge Lei Chen, Keelan Evanini, Jennifer Foster, Derrick Higgins and the three anonymous reviewers for their helpful comments and feedback. 512 References Cem Akkaya, Alexander Conrad, Janyce Wiebe, and Rada Mihalcea. 2010. Amazon Mechanical Turk Subjectivity Word Sense Disambiguation. In Pro-ceedings of the NAACL Workshop on Creating Speech Language Data with Amazon’s Mechanical pages 195–203. Chris Callison-Burch. 2009. Fast, Cheap, and Creative: Evaluating Translation Quality Using Amazon’s Me- Turk. In of pages 286– 295. Jon Chamberlain, Massimo Poesio, and Udo Kruschwitz. 2009. A Demonstration of Human Computation Usthe Phrase Detectives Annotation Game. In Workshop on Human pages 23–24. Robert Dale and Adam Kilgarriff. 2010. Helping Our Own: Text Massaging for Computational Linguistics a New Shared Task. In of Keelan Evanini, Derrick Higgins, and Klaus Zechner. 2010. Using Amazon Mechanical Turk for Transcripof Non-Native Speech. In of the NAACL Workshop on Creating Speech and Language with Amazon’s Mechanical pages 53–56. Rachele De Felice and Stephen Pulman. 2008. A Classifier-Based Approach to Preposition and Deter- Error Correction in L2 English. In pages 169–176. Tim Finin, William Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Annotating Named Entities in Twitter Data with In of the NAACL Workshop on Creating Speech and Language Data with Mechanical pages 80–88. Yoav Freund and Robert E. Schapire. 1999. Large Margin Classification Using the Perceptron Algorithm. 37(3):277–296. Michael Gamon, Jianfeng Gao, Chris Brockett, Alexander Klementiev, William Dolan, Dmitriy Belenko, and</note>
<title confidence="0.639103">Lucy Vanderwende. 2008. Using Contextual Speller Techniques and Language Modeling for ESL Error In of</title>
<author confidence="0.712269">Using Mostly Native Data to Errors in Learners’ Writing In</author>
<note confidence="0.8984677">pages 163–171. Y. Guo and Gulbahar Beckett. 2007. The Hegemony of English as a Global Language: Reclaiming Local and Culture in China. In- Journal of Adult 1. Ann Irvine and Alexandre Klementiev. 2010. Using Mechanical Turk to Annotate Lexicons for Less Com- Used Languages. In of the NAACL Workshop on Creating Speech and Language Data Amazon’s Mechanical pages 108–113.</note>
<author confidence="0.747718">Grammatical</author>
<title confidence="0.563283">Detection for Language Synthesis</title>
<author confidence="0.36995">Morgan</author>
<affiliation confidence="0.254796">Claypool.</affiliation>
<address confidence="0.455903">Madnani. 2010. Circle of Meaning: From</address>
<author confidence="0.860343">to Paraphrasing</author>
<author confidence="0.860343">Ph D thesis</author>
<affiliation confidence="0.777129666666667">Department of Computer Science, University of Maryland College Park. Scott Novotney and Chris Callison-Burch. 2010. Cheap,</affiliation>
<title confidence="0.559514">Fast and Good Enough: Automatic Speech Recogni-</title>
<author confidence="0.366291">In</author>
<note confidence="0.971730214285714">pages 207–215. Nicholas Rizzolo and Dan Roth. 2007. Modeling Global Inference. In of the First IEEE International Conference on Semantic pages 597–604, Irvine, California, September. Alla Rozovskaya and D. Roth. 2010a. Annotating ESL Challenges and rewards. In of the NAACL Workshop on Innovative Use of NLPfor Build- Educational Alla Rozovskaya and D. Roth. 2010b. Generating Confusion Sets for Context-Sensitive Error Correction. In of Andreas Stolcke. 2002. SRILM: An Extensible Lan-</note>
<title confidence="0.7733595">Modeling Toolkit. In of the Inter- Conference on Spoken Language</title>
<note confidence="0.915747454545454">pages 257–286. Joel Tetreault and Martin Chodorow. 2008. The Ups and Downs of Preposition Error Detection in ESL Writing. of pages 865–872. Joel Tetreault, Jill Burstein, and Claudia Leacock, edi- 2010a. of the NAACL Workshop on Innovative Use of NLP for Building Educational Ap- Joel Tetreault, Elena Filatova, and Martin Chodorow. 2010b. Rethinking Grammatical Error Annotation and with the Amazon Mechanical Turk. In Proceedings of the NAACL Workshop on Innovative Use NLP for Building Educational pages 45–48. Rui Wang and Chris Callison-Burch. 2010. Cheap Facts Counter-Facts. In of the NAACL Workshop on Creating Speech and Language Data Amazon’s Mechanical pages 163–167. Omar F. Zaidan and Chris Callison-Burch. 2010. Predicting Human-Targeted Translation Edit Rate via Un- Human Annotators. In of pages 369–372. 513</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Adam Kilgarriff</author>
</authors>
<title>Helping Our Own: Text Massaging for Computational Linguistics as a New Shared Task.</title>
<date>2010</date>
<booktitle>In Proceedings of INLG.</booktitle>
<contexts>
<context position="3461" citStr="Dale and Kilgarriff, 2010" startWordPosition="527" endWordPosition="530">rch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourc1There has been a recent proposal for a related shared task (Dale and Kilgarriff, 2010) that shows promise. 508 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 508–513, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ing to both address the lack of appropriate evaluation metrics and to make system comparison easier. Our solution is general enough for, in the simplest case, intrinsically evaluating a single system on a single dataset and, more realistically, comparing two different systems (from same or different groups). 2 A Case Study: Extraneous Prepositions We consider the problem </context>
</contexts>
<marker>Dale, Kilgarriff, 2010</marker>
<rawString>Robert Dale and Adam Kilgarriff. 2010. Helping Our Own: Text Massaging for Computational Linguistics as a New Shared Task. In Proceedings of INLG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keelan Evanini</author>
<author>Derrick Higgins</author>
<author>Klaus Zechner</author>
</authors>
<title>Using Amazon Mechanical Turk for Transcription of Non-Native Speech.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>53--56</pages>
<contexts>
<context position="2887" citStr="Evanini et al., 2010" startWordPosition="443" endWordPosition="446">e there is no common corpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourc1There has been a recent proposal for a related shared task (Dale and Kilgarriff, 2010) that shows promise. 508 P</context>
</contexts>
<marker>Evanini, Higgins, Zechner, 2010</marker>
<rawString>Keelan Evanini, Derrick Higgins, and Klaus Zechner. 2010. Using Amazon Mechanical Turk for Transcription of Non-Native Speech. In Proceedings of the NAACL Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 53–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachele De Felice</author>
<author>Stephen Pulman</author>
</authors>
<title>A Classifier-Based Approach to Preposition and Determiner Error Correction in L2 English.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>169--176</pages>
<marker>De Felice, Pulman, 2008</marker>
<rawString>Rachele De Felice and Stephen Pulman. 2008. A Classifier-Based Approach to Preposition and Determiner Error Correction in L2 English. In Proceedings of COLING, pages 169–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Finin</author>
<author>William Murnane</author>
<author>Anand Karandikar</author>
<author>Nicholas Keller</author>
<author>Justin Martineau</author>
<author>Mark Dredze</author>
</authors>
<title>Annotating Named Entities in Twitter Data with Crowdsourcing.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>80--88</pages>
<contexts>
<context position="3251" citStr="Finin et al., 2010" startWordPosition="493" endWordPosition="496">s the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourc1There has been a recent proposal for a related shared task (Dale and Kilgarriff, 2010) that shows promise. 508 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 508–513, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ing to both address the lack of appropriate evaluation metrics and to make system comparison easier. Our solution is general enough for, in the simplest case, intri</context>
</contexts>
<marker>Finin, Murnane, Karandikar, Keller, Martineau, Dredze, 2010</marker>
<rawString>Tim Finin, William Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Annotating Named Entities in Twitter Data with Crowdsourcing. In Proceedings of the NAACL Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 80–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large Margin Classification Using the Perceptron Algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="7199" citStr="Freund and Schapire, 1999" startWordPosition="1131" endWordPosition="1134">hard to decide (Unknown); the last category was needed for cases where the context was too messy to make a decision about the highlighted preposition. On average, the three experts had an agreement of 0.87 and a kappa of 0.75. For subsequent analysis, we only use the classes Error and OK since Unknown was used extremely rarely and never by all 3 experts for the same sentence. We used two different error detection systems to illustrate our evaluation methodology:2 • LM: A 4-gram language model trained on the Google Web1T 5-gram Corpus with SRILM (Stolcke, 2002). • PERC: An averaged Perceptron (Freund and Schapire, 1999) classifier— as implemented in the Learning by Java toolkit (Rizzolo and Roth, 2007)—trained on 7 million examples and using the same features employed by Tetreault and Chodorow (2008). 3 Crowdsourcing Recently,we showed that Amazon Mechanical Turk (AMT) is a cheap and effective alternative to expert raters for annotating preposition errors (Tetreault et al., 2010b). In other current work, we have extended this pilot study to show that CrowdFlower, a crowdsourcing service that allows for stronger quality control on untrained human raters (henceforth, Turkers), is more reliable than AMT on thre</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1999. Large Margin Classification Using the Perceptron Algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Jianfeng Gao</author>
<author>Chris Brockett</author>
<author>Alexander Klementiev</author>
<author>William Dolan</author>
<author>Dmitriy Belenko</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Using Contextual Speller Techniques and Language Modeling for ESL Error Correction.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP.</booktitle>
<contexts>
<context position="2442" citStr="Gamon et al. (2008)" startWordPosition="379" endWordPosition="382">onsequence, it is common for two native speakers to have different judgments of usage. Therefore, an appropriate evaluation should take this into account by not only enlisting multiple human judges but also aggregating these judgments in a graded manner. Second, systems are hardly ever compared to each other. In fact, to our knowledge, no two systems developed by different groups have been compared directly within the field primarily because there is no common corpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambigu</context>
</contexts>
<marker>Gamon, Gao, Brockett, Klementiev, Dolan, Belenko, Vanderwende, 2008</marker>
<rawString>Michael Gamon, Jianfeng Gao, Chris Brockett, Alexander Klementiev, William Dolan, Dmitriy Belenko, and Lucy Vanderwende. 2008. Using Contextual Speller Techniques and Language Modeling for ESL Error Correction. In Proceedings of IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
</authors>
<title>Using Mostly Native Data to Correct Errors in Learners’ Writing.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>163--171</pages>
<contexts>
<context position="4460" citStr="Gamon (2010)" startWordPosition="683" endWordPosition="684">trinsically evaluating a single system on a single dataset and, more realistically, comparing two different systems (from same or different groups). 2 A Case Study: Extraneous Prepositions We consider the problem of detecting an extraneous preposition error, i.e., incorrectly using a preposition where none is licensed. In the sentence “They came to outside”, the preposition to is an extraneous error whereas in the sentence “They arrived to the town” the preposition to is a confusion error (cf. arrived in the town). Most work on automated correction of preposition errors, with the exception of Gamon (2010), addresses preposition confusion errors e.g., (Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010b). One reason is that in addition to the standard context-based features used to detect confusion errors, identifying extraneous prepositions also requires actual knowledge of when a preposition can and cannot be used. Despite this lack of attention, extraneous prepositions account for a significant proportion—as much as 18% in essays by advanced English learners (Rozovskaya and Roth, 2010a)—of all preposition usage errors. 2.1 Data and Systems For the experiments in</context>
</contexts>
<marker>Gamon, 2010</marker>
<rawString>Michael Gamon. 2010. Using Mostly Native Data to Correct Errors in Learners’ Writing. In Proceedings of NAACL, pages 163–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Guo</author>
<author>Gulbahar Beckett</author>
</authors>
<title>The Hegemony of English as a Global Language: Reclaiming Local Knowledge and Culture in China. Convergence:</title>
<date>2007</date>
<journal>International Journal of Adult Education,</journal>
<volume>1</volume>
<contexts>
<context position="1000" citStr="Guo and Beckett (2007)" startWordPosition="138" endWordPosition="141">he rising interest in developing grammatical error detection systems for non-native speakers of English, progress in the field has been hampered by a lack of informative metrics and an inability to directly compare the performance of systems developed by different researchers. In this paper we address these problems by presenting two evaluation methodologies, both based on a novel use of crowdsourcing. 1 Motivation and Contributions One of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in many NLP research papers on the topic, a Synthesis Series book (Leacock et al., 2010) and a recurring workshop (Tetreault et al., 2010a), all in the last five years. In this year’s ACL conference, there are four long papers devoted to this topic. Despite the growing interest, two major factors encumber the growth of this subfield. First, the lack of consistent and appropriate score reporting is an issue. Most work reports results in the form of precision and recall as measured against </context>
</contexts>
<marker>Guo, Beckett, 2007</marker>
<rawString>Y. Guo and Gulbahar Beckett. 2007. The Hegemony of English as a Global Language: Reclaiming Local Knowledge and Culture in China. Convergence: International Journal of Adult Education, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Irvine</author>
<author>Alexandre Klementiev</author>
</authors>
<title>Using Mechanical Turk to Annotate Lexicons for Less Commonly Used Languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>108--113</pages>
<contexts>
<context position="3156" citStr="Irvine and Klementiev, 2010" startWordPosition="478" endWordPosition="481">ated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourc1There has been a recent proposal for a related shared task (Dale and Kilgarriff, 2010) that shows promise. 508 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 508–513, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ing to both address the lack of appropriate evaluation metrics and to</context>
</contexts>
<marker>Irvine, Klementiev, 2010</marker>
<rawString>Ann Irvine and Alexandre Klementiev. 2010. Using Mechanical Turk to Annotate Lexicons for Less Commonly Used Languages. In Proceedings of the NAACL Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 108–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
<author>Michael Gamon</author>
<author>Joel Tetreault</author>
</authors>
<title>Automated Grammatical Error Detection for Language Learners. Synthesis Lectures on Human Language Technologies.</title>
<date>2010</date>
<publisher>Morgan Claypool.</publisher>
<contexts>
<context position="1195" citStr="Leacock et al., 2010" startWordPosition="173" endWordPosition="176">o directly compare the performance of systems developed by different researchers. In this paper we address these problems by presenting two evaluation methodologies, both based on a novel use of crowdsourcing. 1 Motivation and Contributions One of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in many NLP research papers on the topic, a Synthesis Series book (Leacock et al., 2010) and a recurring workshop (Tetreault et al., 2010a), all in the last five years. In this year’s ACL conference, there are four long papers devoted to this topic. Despite the growing interest, two major factors encumber the growth of this subfield. First, the lack of consistent and appropriate score reporting is an issue. Most work reports results in the form of precision and recall as measured against the judgment of a single human rater. This is problematic because most usage errors (such as those in article and preposition usage) are a matter of degree rather than simple rule violations such</context>
<context position="5359" citStr="Leacock et al., 2010" startWordPosition="819" endWordPosition="823">uires actual knowledge of when a preposition can and cannot be used. Despite this lack of attention, extraneous prepositions account for a significant proportion—as much as 18% in essays by advanced English learners (Rozovskaya and Roth, 2010a)—of all preposition usage errors. 2.1 Data and Systems For the experiments in this paper, we chose a proprietary corpus of about 500,000 essays written by ESL students for Test of English as a Foreign Language (TOEFL R�). Despite being common ESL errors, preposition errors are still infrequent overall, with over 90% of prepositions being used correctly (Leacock et al., 2010; Rozovskaya and Roth, 2010a). Given this fact about error sparsity, we needed an efficient method to extract a good number of error instances (for statistical reliability) from the large essay corpus. We found all trigrams in our essays containing prepositions as the middle word (e.g., marry with her) and then looked up the counts of each trigram and the corresponding bigram with the preposition removed (marry her) in the Google Web1T 5-gram Corpus. If the trigram was unattested or had a count much lower than expected based on the bigram count, then we manually inspected the trigram to see wh</context>
</contexts>
<marker>Leacock, Chodorow, Gamon, Tetreault, 2010</marker>
<rawString>Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault. 2010. Automated Grammatical Error Detection for Language Learners. Synthesis Lectures on Human Language Technologies. Morgan Claypool.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
</authors>
<title>The Circle of Meaning: From Translation to Paraphrasing and Back.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, University of Maryland College Park.</institution>
<contexts>
<context position="2972" citStr="Madnani, 2010" startWordPosition="455" endWordPosition="456">achine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourc1There has been a recent proposal for a related shared task (Dale and Kilgarriff, 2010) that shows promise. 508 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistic</context>
</contexts>
<marker>Madnani, 2010</marker>
<rawString>Nitin Madnani. 2010. The Circle of Meaning: From Translation to Paraphrasing and Back. Ph.D. thesis, Department of Computer Science, University of Maryland College Park.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Novotney</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Cheap, Fast and Good Enough: Automatic Speech Recognition with Non-Expert Transcription.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>207--215</pages>
<contexts>
<context position="2923" citStr="Novotney and Callison-Burch, 2010" startWordPosition="447" endWordPosition="450">orpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourc1There has been a recent proposal for a related shared task (Dale and Kilgarriff, 2010) that shows promise. 508 Proceedings of the 49th Annual Meetin</context>
</contexts>
<marker>Novotney, Callison-Burch, 2010</marker>
<rawString>Scott Novotney and Chris Callison-Burch. 2010. Cheap, Fast and Good Enough: Automatic Speech Recognition with Non-Expert Transcription. In Proceedings of NAACL, pages 207–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Rizzolo</author>
<author>Dan Roth</author>
</authors>
<title>Modeling Discriminative Global Inference.</title>
<date>2007</date>
<booktitle>In Proceedings of the First IEEE International Conference on Semantic Computing (ICSC),</booktitle>
<pages>597--604</pages>
<location>Irvine, California,</location>
<contexts>
<context position="7283" citStr="Rizzolo and Roth, 2007" startWordPosition="1144" endWordPosition="1147">too messy to make a decision about the highlighted preposition. On average, the three experts had an agreement of 0.87 and a kappa of 0.75. For subsequent analysis, we only use the classes Error and OK since Unknown was used extremely rarely and never by all 3 experts for the same sentence. We used two different error detection systems to illustrate our evaluation methodology:2 • LM: A 4-gram language model trained on the Google Web1T 5-gram Corpus with SRILM (Stolcke, 2002). • PERC: An averaged Perceptron (Freund and Schapire, 1999) classifier— as implemented in the Learning by Java toolkit (Rizzolo and Roth, 2007)—trained on 7 million examples and using the same features employed by Tetreault and Chodorow (2008). 3 Crowdsourcing Recently,we showed that Amazon Mechanical Turk (AMT) is a cheap and effective alternative to expert raters for annotating preposition errors (Tetreault et al., 2010b). In other current work, we have extended this pilot study to show that CrowdFlower, a crowdsourcing service that allows for stronger quality control on untrained human raters (henceforth, Turkers), is more reliable than AMT on three different error detection tasks (article errors, confused prepositions 2Any conclu</context>
</contexts>
<marker>Rizzolo, Roth, 2007</marker>
<rawString>Nicholas Rizzolo and Dan Roth. 2007. Modeling Discriminative Global Inference. In Proceedings of the First IEEE International Conference on Semantic Computing (ICSC), pages 597–604, Irvine, California, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alla Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Annotating ESL errors: Challenges and rewards.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL Workshop on Innovative Use of NLPfor Building Educational Applications.</booktitle>
<contexts>
<context position="4588" citStr="Rozovskaya and Roth, 2010" startWordPosition="698" endWordPosition="701"> (from same or different groups). 2 A Case Study: Extraneous Prepositions We consider the problem of detecting an extraneous preposition error, i.e., incorrectly using a preposition where none is licensed. In the sentence “They came to outside”, the preposition to is an extraneous error whereas in the sentence “They arrived to the town” the preposition to is a confusion error (cf. arrived in the town). Most work on automated correction of preposition errors, with the exception of Gamon (2010), addresses preposition confusion errors e.g., (Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010b). One reason is that in addition to the standard context-based features used to detect confusion errors, identifying extraneous prepositions also requires actual knowledge of when a preposition can and cannot be used. Despite this lack of attention, extraneous prepositions account for a significant proportion—as much as 18% in essays by advanced English learners (Rozovskaya and Roth, 2010a)—of all preposition usage errors. 2.1 Data and Systems For the experiments in this paper, we chose a proprietary corpus of about 500,000 essays written by ESL students for Test of English as a Foreign Lang</context>
</contexts>
<marker>Rozovskaya, Roth, 2010</marker>
<rawString>Alla Rozovskaya and D. Roth. 2010a. Annotating ESL errors: Challenges and rewards. In Proceedings of the NAACL Workshop on Innovative Use of NLPfor Building Educational Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alla Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Generating Confusion Sets for Context-Sensitive Error Correction.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4588" citStr="Rozovskaya and Roth, 2010" startWordPosition="698" endWordPosition="701"> (from same or different groups). 2 A Case Study: Extraneous Prepositions We consider the problem of detecting an extraneous preposition error, i.e., incorrectly using a preposition where none is licensed. In the sentence “They came to outside”, the preposition to is an extraneous error whereas in the sentence “They arrived to the town” the preposition to is a confusion error (cf. arrived in the town). Most work on automated correction of preposition errors, with the exception of Gamon (2010), addresses preposition confusion errors e.g., (Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010b). One reason is that in addition to the standard context-based features used to detect confusion errors, identifying extraneous prepositions also requires actual knowledge of when a preposition can and cannot be used. Despite this lack of attention, extraneous prepositions account for a significant proportion—as much as 18% in essays by advanced English learners (Rozovskaya and Roth, 2010a)—of all preposition usage errors. 2.1 Data and Systems For the experiments in this paper, we chose a proprietary corpus of about 500,000 essays written by ESL students for Test of English as a Foreign Lang</context>
</contexts>
<marker>Rozovskaya, Roth, 2010</marker>
<rawString>Alla Rozovskaya and D. Roth. 2010b. Generating Confusion Sets for Context-Sensitive Error Correction. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM: An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>257--286</pages>
<contexts>
<context position="7139" citStr="Stolcke, 2002" startWordPosition="1124" endWordPosition="1125"> extraneous (Error), not extraneous (OK) or too hard to decide (Unknown); the last category was needed for cases where the context was too messy to make a decision about the highlighted preposition. On average, the three experts had an agreement of 0.87 and a kappa of 0.75. For subsequent analysis, we only use the classes Error and OK since Unknown was used extremely rarely and never by all 3 experts for the same sentence. We used two different error detection systems to illustrate our evaluation methodology:2 • LM: A 4-gram language model trained on the Google Web1T 5-gram Corpus with SRILM (Stolcke, 2002). • PERC: An averaged Perceptron (Freund and Schapire, 1999) classifier— as implemented in the Learning by Java toolkit (Rizzolo and Roth, 2007)—trained on 7 million examples and using the same features employed by Tetreault and Chodorow (2008). 3 Crowdsourcing Recently,we showed that Amazon Mechanical Turk (AMT) is a cheap and effective alternative to expert raters for annotating preposition errors (Tetreault et al., 2010b). In other current work, we have extended this pilot study to show that CrowdFlower, a crowdsourcing service that allows for stronger quality control on untrained human rat</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM: An Extensible Language Modeling Toolkit. In Proceedings of the International Conference on Spoken Language Processing, pages 257–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>The Ups and Downs of Preposition Error Detection in ESL Writing.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>865--872</pages>
<contexts>
<context position="2421" citStr="Tetreault and Chodorow (2008)" startWordPosition="375" endWordPosition="378">uch as number agreement. As a consequence, it is common for two native speakers to have different judgments of usage. Therefore, an appropriate evaluation should take this into account by not only enlisting multiple human judges but also aggregating these judgments in a graded manner. Second, systems are hardly ever compared to each other. In fact, to our knowledge, no two systems developed by different groups have been compared directly within the field primarily because there is no common corpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009),</context>
<context position="4561" citStr="Tetreault and Chodorow, 2008" startWordPosition="694" endWordPosition="697">omparing two different systems (from same or different groups). 2 A Case Study: Extraneous Prepositions We consider the problem of detecting an extraneous preposition error, i.e., incorrectly using a preposition where none is licensed. In the sentence “They came to outside”, the preposition to is an extraneous error whereas in the sentence “They arrived to the town” the preposition to is a confusion error (cf. arrived in the town). Most work on automated correction of preposition errors, with the exception of Gamon (2010), addresses preposition confusion errors e.g., (Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010b). One reason is that in addition to the standard context-based features used to detect confusion errors, identifying extraneous prepositions also requires actual knowledge of when a preposition can and cannot be used. Despite this lack of attention, extraneous prepositions account for a significant proportion—as much as 18% in essays by advanced English learners (Rozovskaya and Roth, 2010a)—of all preposition usage errors. 2.1 Data and Systems For the experiments in this paper, we chose a proprietary corpus of about 500,000 essays written by ESL students for Test o</context>
<context position="7383" citStr="Tetreault and Chodorow (2008)" startWordPosition="1160" endWordPosition="1163">had an agreement of 0.87 and a kappa of 0.75. For subsequent analysis, we only use the classes Error and OK since Unknown was used extremely rarely and never by all 3 experts for the same sentence. We used two different error detection systems to illustrate our evaluation methodology:2 • LM: A 4-gram language model trained on the Google Web1T 5-gram Corpus with SRILM (Stolcke, 2002). • PERC: An averaged Perceptron (Freund and Schapire, 1999) classifier— as implemented in the Learning by Java toolkit (Rizzolo and Roth, 2007)—trained on 7 million examples and using the same features employed by Tetreault and Chodorow (2008). 3 Crowdsourcing Recently,we showed that Amazon Mechanical Turk (AMT) is a cheap and effective alternative to expert raters for annotating preposition errors (Tetreault et al., 2010b). In other current work, we have extended this pilot study to show that CrowdFlower, a crowdsourcing service that allows for stronger quality control on untrained human raters (henceforth, Turkers), is more reliable than AMT on three different error detection tasks (article errors, confused prepositions 2Any conclusions drawn in this paper pertain only to these specific instantiations of the two systems. 509 &amp; ex</context>
</contexts>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>Joel Tetreault and Martin Chodorow. 2008. The Ups and Downs of Preposition Error Detection in ESL Writing. In Proceedings of COLING, pages 865–872.</rawString>
</citation>
<citation valid="false">
<booktitle>2010a. Proceedings of the NAACL Workshop on Innovative Use of NLP for Building Educational Applications.</booktitle>
<editor>Joel Tetreault, Jill Burstein, and Claudia Leacock, editors.</editor>
<marker></marker>
<rawString>Joel Tetreault, Jill Burstein, and Claudia Leacock, editors. 2010a. Proceedings of the NAACL Workshop on Innovative Use of NLP for Building Educational Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Tetreault</author>
<author>Elena Filatova</author>
<author>Martin Chodorow</author>
</authors>
<title>Rethinking Grammatical Error Annotation and Evaluation with the Amazon Mechanical Turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>45--48</pages>
<contexts>
<context position="1244" citStr="Tetreault et al., 2010" startWordPosition="181" endWordPosition="184">veloped by different researchers. In this paper we address these problems by presenting two evaluation methodologies, both based on a novel use of crowdsourcing. 1 Motivation and Contributions One of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in many NLP research papers on the topic, a Synthesis Series book (Leacock et al., 2010) and a recurring workshop (Tetreault et al., 2010a), all in the last five years. In this year’s ACL conference, there are four long papers devoted to this topic. Despite the growing interest, two major factors encumber the growth of this subfield. First, the lack of consistent and appropriate score reporting is an issue. Most work reports results in the form of precision and recall as measured against the judgment of a single human rater. This is problematic because most usage errors (such as those in article and preposition usage) are a matter of degree rather than simple rule violations such as number agreement. As a consequence, it is com</context>
<context position="7565" citStr="Tetreault et al., 2010" startWordPosition="1186" endWordPosition="1189">entence. We used two different error detection systems to illustrate our evaluation methodology:2 • LM: A 4-gram language model trained on the Google Web1T 5-gram Corpus with SRILM (Stolcke, 2002). • PERC: An averaged Perceptron (Freund and Schapire, 1999) classifier— as implemented in the Learning by Java toolkit (Rizzolo and Roth, 2007)—trained on 7 million examples and using the same features employed by Tetreault and Chodorow (2008). 3 Crowdsourcing Recently,we showed that Amazon Mechanical Turk (AMT) is a cheap and effective alternative to expert raters for annotating preposition errors (Tetreault et al., 2010b). In other current work, we have extended this pilot study to show that CrowdFlower, a crowdsourcing service that allows for stronger quality control on untrained human raters (henceforth, Turkers), is more reliable than AMT on three different error detection tasks (article errors, confused prepositions 2Any conclusions drawn in this paper pertain only to these specific instantiations of the two systems. 509 &amp; extraneous prepositions). To impose such quality control, one has to provide “gold” instances, i.e., examples with known correct judgments that are then used to root out any Turkers wi</context>
</contexts>
<marker>Tetreault, Filatova, Chodorow, 2010</marker>
<rawString>Joel Tetreault, Elena Filatova, and Martin Chodorow. 2010b. Rethinking Grammatical Error Annotation and Evaluation with the Amazon Mechanical Turk. In Proceedings of the NAACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 45–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Wang</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Cheap Facts and Counter-Facts.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>163--167</pages>
<contexts>
<context position="3201" citStr="Wang and Callison-Burch, 2010" startWordPosition="485" endWordPosition="488">rent evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourc1There has been a recent proposal for a related shared task (Dale and Kilgarriff, 2010) that shows promise. 508 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 508–513, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ing to both address the lack of appropriate evaluation metrics and to make system comparison easier. Our solution </context>
</contexts>
<marker>Wang, Callison-Burch, 2010</marker>
<rawString>Rui Wang and Chris Callison-Burch. 2010. Cheap Facts and Counter-Facts. In Proceedings of the NAACL Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 163–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Predicting Human-Targeted Translation Edit Rate via Untrained Human Annotators.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>369--372</pages>
<marker>Zaidan, Callison-Burch, 2010</marker>
<rawString>Omar F. Zaidan and Chris Callison-Burch. 2010. Predicting Human-Targeted Translation Edit Rate via Untrained Human Annotators. In Proceedings of NAACL, pages 369–372.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>