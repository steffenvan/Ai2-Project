<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.999592">
An Automatic Extraction of Key Paragraphs
Based on Context Dependency
</title>
<author confidence="0.9841">
Fumiyo Fukumoto Yoshimi Suzukit Jun&apos;ichi Fukumotot
</author>
<affiliation confidence="0.997543">
Dept. of Electrical Engineering and Kansai Lab., R &amp; D Group
Computer Science, Yamanashi University Oki Electric Industry Co. Ltd.,$
</affiliation>
<address confidence="0.98105">
4-3-11 Takeda, Kofu 400 Japan 1-2-27 Shiromi, Chuo-ku, Osaka 540 Japan
</address>
<email confidence="0.998487">
ffukumoto@skye, ysuzuki@suwatl.esi.yamanashi.ac.iP fukumotoakansai.oki.co.jp
</email>
<sectionHeader confidence="0.997394" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999317583333334">
In this paper, we propose a method for ex-
tracting key paragraphs in articles based
on the degree of context dependency. Like
Luhn&apos;s technique, our method assumes
that the words related to theme in an arti-
cle appear throughout paragraphs. Our ex-
traction technique of keywords is based on
the degree of context dependency that how
strongly a word is related to a given con-
text. The results of experiments demon-
strate the applicability of our proposed
method.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998482388888889">
With increasing numbers of machine readable doc-
uments becoming available, automatic document
summarisation has become one of the major research
topics in IR and NLP studies.
In the field of an automatic summarisation, there
are at least two approaches. One is knowledge-based
approach with particular subject fields (Reimer,
1988), (Jacobs, 1990). This approach, based on deep
knowledge of particular subject fields, is useful for
restricted tasks, such as, for example, the construc-
tion of &apos;weather forecasts&apos; summaries. However,
when unrestricted subject matter must be treated,
as is often the case in practice, the passage retrieval
and text summarisation methods proposed have not
proven equal to the need, since deep knowledge of
particular subject fields is required (Paice, 1990),
(Zechner, 1996).
The other, alternative strategy is the approach
that relies mainly on corpus statistics (Paice, 1990),
(Paice, 1993). The main task of this approach is
the sentence scoring process. Typically, weights
are assigned to the individual words in a text, and
the complete sentence scores are then based on the
occurrence characteristics of highly-weighted terms
(keywords) in the respective sentences.
Term weighting technique has been widely inves-
tigated in information retrieval and lots of tech-
niques such as location heuristics (Baxendale, 1958),
rhetorical relations (Miike, 1994), and title informa-
tion (Edmundson, 1969) have been proposed. These
techniques seem to be less dependent on the domain.
However, Salton claims that it is difficult to pro-
duce high accuracy of retrieval by using these term-
weighting approaches (Salton, 1993).
The other term weighting technique is based on
keyword frequency (Luhn, 1958). Keyword fre-
quency is further less dependent on the domain than
other weighting methods and therefore, well studied.
Major approaches which are based on keyword fre-
quency assume on the fact that the keywords of the
article appear frequently in the article, but appear
seldom in other articles (Luhn, 1958), (Nagao, 1976),
(Salton, 1993), (Zechner, 1996). These approaches
seem to show the effect in entirely different articles,
such as &apos;weather forecasts&apos;, &apos;medical reports&apos;, and
&apos;computer manuals&apos;. Because each different article
is characterised by a larger number of words which
appear frequently in one article, but appear seldom
in other articles. However, in some articles from
the same domain such as &apos;weather forecasts&apos;, one
encounters quite a number of words which appear
frequently over articles. Therefore, how to extract
keyword from these words is a serious problem in
such the restricted subject domain.
In this paper, we propose a method for extract-
ing key paragraphs in articles based on the degree of
context dependency and show how the idea of con-
text dependency can be used effectively to extract
key paragraphs than other related work.
The basic idea of our approach is that whether a
word is a key in an article or not depends on the do-
main to which the article belongs. Let &apos;stake&apos; be a
keyword and &apos;today&apos; not be a keyword in the article.
If the article belongs to a restricted subject domain,
such as &apos;Stock market&apos;, there are other articles which
are related to the article. Therefore, the frequency of
&apos;stake&apos; and &apos;today&apos; in other articles are similar with
each other. Let us consider further a broad coverage
domain such as newspaper articles; i.e. the article
containing the words &apos;stake&apos; and &apos;today&apos; belongs to
a newspaper which consists of different subject do-
mains such as &apos;Stock market&apos; news, &apos;International&apos;
</bodyText>
<page confidence="0.994802">
291
</page>
<bodyText confidence="0.999883916666667">
news, &apos;Weather forecasts&apos; news. &apos;Today&apos; should ap-
pear frequently with every article even in such a do-
main; i.e. newspaper articles, while &apos;stake&apos; should
not. Our technique for extraction of keywords ex-
plicitly exploits this feature of context dependency
of word: how strongly a word is related to a given
context.
In the following sections, we first explain con-
text dependency using newspaper articles, then we
present our term weighting method and a method
for extracting key paragraphs. Finally, we report
some experiments to show the effect of our method.
</bodyText>
<sectionHeader confidence="0.902936" genericHeader="introduction">
2 Context Dependency
</sectionHeader>
<bodyText confidence="0.999744571428571">
Like Luhn&apos;s assumption about keywords, our
method is based on the fact that a writer normally
repeats certain words (keywords) as he advances or
varies his arguments and as he elaborates on an as-
pect of a subject (Luhn, 1958). In this paper, we
focus on newspaper articles. Figure 1 shows the
structure of Wall Street Journal corpus.
</bodyText>
<equation confidence="0.90867525">
Economic
news
Domain • • •
ireww■mweeno.&amp;quot;114ww■ww%
Article • •
110••■•■■••014•••■•■•■•%
Paragraphl Paragraph2 Paragrap43
o: Keyword
</equation>
<figureCaption confidence="0.999759">
Figure 1: The structure of newspaper articles
</figureCaption>
<bodyText confidence="0.999768388888889">
In Figure 1, one day&apos;s newspaper articles consist of
several different topics such as &apos;Economic news&apos;, &apos;In-
ternational news&apos;, etc. We call this Domain, and each
element (`Economic news&apos;, or &apos;International news&apos;)
a context. A particular domain, for example, &apos;Eco-
nomic news&apos;, consists of several articles each of which
has different title name. In Figure 1, &apos;General sig-
nal corp.&apos;, `Safecard services inc.&apos;, and &apos;Jostens inc.&apos;
show title names. We call this Article, and each
element (&apos;General signal corp.&apos; etc) context. Fur-
thermore, a particular article, for example, &apos;General
signal corp.&apos; consists of several paragraphs and key-
words of the &apos;General signal corp.&apos; article appear
throughout paragraphs. We call each paragraph
context in the Paragraph.
We introduce a degree of context dependency into
the structure of newspaper articles shown in Figure
1 in order to extract keywords. A degree of context
dependency is a measure showing how strongly each
word related to a given context, a particular con-
text of Paragraph, Article, or Domain. In Figure 1,
let &apos;0&apos; be a keyword in the article &apos;General signal
corp.&apos;. According to Luhn&apos;s assumption, &apos;0&apos; fre-
quently appears throughout paragraphs. Therefore,
the deviation value of &apos;0&apos; in the Paragraph is small.
On the other hand, the deviation value of &apos;0&apos; in the
Article is larger than that of the Paragraph, since in
Article, &apos;0&apos; appears in a particular element of the
Article, &apos;General signal corp.&apos;. Furthermore, the de-
viation value of &apos;0&apos; in the Domain is larger than
those of the Article and Paragraph, since in the Do-
main, &apos;0&apos; appears frequently in a particular context,
&apos;Economic news&apos;. We extracted keywords using this
feature of the degree of context dependency. In Fig-
ure 1, if a word is a keyword in a given article, it
satisfies the following two conditions:
</bodyText>
<listItem confidence="0.987353">
1. The deviation value of a word in the Paragraph
is smaller than that of the Article.
2. The deviation value of a word in the Article is
smaller than that of the Domain.
</listItem>
<sectionHeader confidence="0.979897" genericHeader="method">
3 Term Weighting
</sectionHeader>
<bodyText confidence="0.999606433333333">
Every sense of words in articles for extracting key
paragraphs is automatically disambiguated in ad-
vance. This is because to disambiguate word-senses
in articles might affect the accuracy of context de-
pendent (domain specific) key paragraphs retrieval,
since the meaning of a word characterises the do-
main in which it is used. Word-sense disambigua-
tion (WSD in short) is a serious problem for NLP,
and a variety of approaches have been proposed for
solving it (Brown, 1991), (Yarowsky, 1992). Our
disambiguation method is based on Niwa&apos;s method
which uses the similarity between a sentence contain-
ing a polysemous noun and a sentence of dictionary-
definition (Niwa, 1994). Furthermore, we linked
nouns which are disambiguated with their seman-
tically similar nouns mainly in order to cope with
the problem of a phrasal lexicon. A phrasal lexicon
such as Atlantic Seaboard, New England gives a neg-
ative influence for keywords retrieval, since it can
not be regarded as units, i.e. each word which is
the element of a phrasal lexicon is assigned to each
semantic code (Fukumoto, 1996).
To the results of WSD and linking methods,
we then applied a term weighting method to ex-
tract keywords. There have been several term
weighting based on word frequencies, such as
TF(Term Frequency), IDF(Inverse Document Fre-
quency), TF*IDF, WIDF(Weighted Inverse Doc-
ument Frequency) (Luhn, 1957), (Sparck, 1973),
(Salton, 1983). (Tokunaga, 1994). We used Watan-
</bodyText>
<figure confidence="0.954635125">
: 0.
0:0
In ernational
• • •
General Safecard
signal corp. services inc. Jostens Inc.
Paragraph
• • •
</figure>
<page confidence="0.993211">
292
</page>
<bodyText confidence="0.9729445">
abe&apos;s x2 method for term weighting which is shown
in formula (1) (Watanabe, 1996).
</bodyText>
<equation confidence="0.987571333333333">
2 (zi2—m2)2 if &gt; mii (1)
Xij =
0 otherwise
</equation>
<bodyText confidence="0.9107945">
Formula (1) shows the value of )(2 of the word i in
the domain j. xii in (1) is the frequency of word i
in the domain j. mii in (1) is shown in formula (2).
Ejl=i Xii
</bodyText>
<equation confidence="0.989774">
/12ij =
T-4* E xi; (2)
L.-i=i1=1
</equation>
<bodyText confidence="0.99987475">
In formula (2), k is the number of different words
and 1 is the number of the domains. A larger value
of 4 means that the word i appears more frequently
in the domain j than in the other.
</bodyText>
<sectionHeader confidence="0.978526" genericHeader="method">
4 An Extraction of Keywords
</sectionHeader>
<bodyText confidence="0.962546125">
The first step to extract keywords is to calculate
x2 for each word in the Paragraph, the Article, and
the Domain. We used formula (1) to calculate the
value of x13.?i, xiqi, and XD, where x13.?j, xAZi, and
x13; indicate which word i appears most frequently
in the context j of Paragraph, Article, and Domain,
respectively. For example, xF% is shown in formula
(3) by using formula (1).
\ 2
2 (xii — Mi3)
= (3)
In formula (3), z, is the frequency of word i in the
context j of Paragraph. mti in formula (3) is shown
in (2) where k is the number of different words and
1 is the number of contexts in Paragraph.
The second step is to calculate the degree of word
i in Paragraph (x1:1), Article (xA?), and Domain
(x13?). We defined the degree of word i in Paragraph,
Article, and Domain as the deviation value of k con-
texts in Paragraph, Article, and Domain, respectively.
Here, k is the number of contexts in Paragraph, Ar-
ticle, and Domain, respectively. For example, the
deviation value of the word i in Paragraph is defined
as follows:
</bodyText>
<equation confidence="0.8328732">
-
\/ k
m)
i
k
</equation>
<bodyText confidence="0.999951125">
In formula (4), k is the number of contexts in Para-
graph, and mi is the mean value of the total fre-
quency of word i in Paragraph which consists of k
contexts.
The last step to extract keywords is to calculate
the context dependency of word i using formula (4).
We recall that if i satisfies both 1 and 2 in section
2, the word i is regarded as a keyword.
</bodyText>
<equation confidence="0.8441865">
&lt;1 (5)
XI)?
</equation>
<bodyText confidence="0.99932075">
Formulae (5) and (6) shows 1, and 2 in section 2,
respectively. In formulae (5) and (6), xP?, xA?, and
xD? are the deviation value of a set of Paragraph,
Article, and Domain, respectively.
</bodyText>
<sectionHeader confidence="0.906187" genericHeader="method">
5 An Extraction of Key Paragraphs
</sectionHeader>
<bodyText confidence="0.894775461538462">
The procedure for extracting key paragraphs has the
following three stages:
Stage One: Representing every paragraph as a vector
The goal of this stage is to represent every para-
graph in an article as a vector. Using a term weight-
ing method, every paragraph in an article would be
represented by vector of the form
P. = (Nii , Ni2 • &amp;quot; I Nin) (7)
where n is the number of nouns in an article and Nii
is as follows;
/
0 Ni does not appear in Pi
=
</bodyText>
<equation confidence="0.671488333333333">
f(Ni) Ni is a keyword and appears in Pi
0 Ni is not a keyword and appears
in Pi
</equation>
<bodyText confidence="0.9691585">
where f(Ni) is a frequency with which the noun Ni
appears in paragraph Pi.
Stage Two: Clustering method
Given a vector representation of paragraphs Pi,
</bodyText>
<listItem confidence="0.9148788">
• • •, Pm as in formula (7), a similarity between two
paragraphs Pi, Pi in an article would be obtained
by using formula (8). The similarity of Pi and Pi is
measured by the inner product of their normalised
vectors and is defined as follows:
</listItem>
<equation confidence="0.998845">
V(Pi) * V(Pi)
Sim(p, Pi) = (8)
V(P1) II V(Pi)
</equation>
<bodyText confidence="0.996069583333333">
The greater the value of Sim(Pi, Pi) is, the more
similar these two paragraphs are. For a set of para-
graphs P1, •, Pm of an article, we calculate the
semantic similarity value of all possible pairs of para-
graphs. The clustering algorithm is applied to the
sets and produces a set of semantic clusters, which
are ordered in the descending order of their seman-
tic similarity values. We adopted non-overlapping,
group average method in our clustering technique
(Jardine, 1968).
Stage Three: Extraction of key paragraphs
The sample results of clustering is shown in Table
</bodyText>
<equation confidence="0.7380045">
1.
=
(4)
&lt;1 (8)
</equation>
<page confidence="0.998595">
293
</page>
<tableCaption confidence="0.8567285">
Table 1: The sample results of clustering
Num Cluster
</tableCaption>
<table confidence="0.937971333333333">
1 (3,4)
2 (1,(3,4))
3 ((1,(3,4)),2)
</table>
<bodyText confidence="0.919914428571429">
`Nurn&apos; in Table 1 shows the order of clusters which
we have obtained and the number shown under
&apos;Cluster&apos; shows the paragraph numbers. In Table
1, if the number of keywords which belonging to the
third paragraph is larger than that of the fourth,
the order of key paragraphs is 3 —+ 4 —4 1 2,
otherwise, 4 3 1 —+ 2.
</bodyText>
<sectionHeader confidence="0.999771" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.99963375">
We have conducted three experiments to examine
the effect of our method. The first experiment, Key-
words Experiment, is concerned with the keywords
extracting technique and with verifying the effect of
our method which introduces context dependency.
The second experiment, Key Paragraphs Experiment,
shows how the extracted keywords can be used to
extract key paragraphs. In the third experiment,
Comparison to Other Related Work, we applied Zech-
ner&apos;s key sentences method (Zechner, 1996) to key
paragraphs extraction (we call this method_A), and
compared it with our method.
</bodyText>
<subsectionHeader confidence="0.976695">
8.1 Data
</subsectionHeader>
<bodyText confidence="0.999793272727273">
The corpus we have used is the 1988, 1989 Wall
Street Journal (Liberman, 1991) in ACL/DCI CD-
ROM which consists of about 280,000 part-of-speech
tagged sentences (Brill, 1992). Wall Street Journal
consists of many articles, and each article has a ti-
tle name. These titles are classified into 76 different
domains. We selected 10 different domains and used
them as Domain. As a test data, we selected 50
articles each of which belongs to one of these 10 do-
mains. The selected domain names and the number
of articles are shown in Table 2.
</bodyText>
<tableCaption confidence="0.998282">
Table 2: The selected data
</tableCaption>
<table confidence="0.995923833333333">
Domain No Domain No
BB : buybacks 6 BVG: beverages 8
DIV: dividends 5 FOD: food products 5
STK: stock market 5 RET: retailing 1
ARO: aerospace 5 ENV: environment 3
PCS: stones, gold 9 CMD: farm products 3
</table>
<bodyText confidence="0.990621666666667">
There are 3,802 different nouns in 50 articles. As a
result of WSD and linking methods for these articles,
we have obtained 3,707 different nouns.
</bodyText>
<subsectionHeader confidence="0.989058">
6.2 Keywords Experiment
</subsectionHeader>
<bodyText confidence="0.9983775">
Formulae (5) and (6) are applied to 50 articles which
are the results of WSD and linking methods, and as a
result, we have obtained 1,047 keywords in all. The
result of keyword extraction is shown in Table 3.
</bodyText>
<tableCaption confidence="0.996574">
Table 3: The results of keyword experiment
</tableCaption>
<figure confidence="0.930191277777778">
Paragraph Recall/Precision
3(1) 88.9/81.2
4(13) 62.7/86.2
5(6) 76.7/86.2
67.3/77.5
6(6)
7(4) 83.2/86.4
8(3) 89.0/80.0
9(4) 80.3/75.4
10(2) 90.2/72.2
11(1) 80.1/87.6
12(1) 100.0/83.7
14(3) 46.5/50.2
15(2) 100.0/73.4
16(2) 89.2/82.0
17(1) 62.4/89.4
22(1) 64.3/70.0
Total(50) 78.7/78.1
</figure>
<bodyText confidence="0.7077316875">
In Table 3, x in `x(y)&apos; of &apos;Paragraph&apos; shows the num-
ber of paragraphs in an article, &apos;y&apos; shows the number
of articles. For example, 3(1) shows that there is one
article which consists of three paragraphs. Recall
and Precision in Table 3 are as follows;
Number of correct keywords
Number of keywords which are se-
lected by human
Number of correct keywords
Number of keywords which are se-
lected in our method
Recall and Precision in Table 3 show the means in
each paragraph. The denominator of Recall is made
by three human judges; i.e. when more than one
human judged the word as a keyword, the word is
regarded as a keyword.
</bodyText>
<subsectionHeader confidence="0.992471">
6.3 Key Paragraphs Experiment
</subsectionHeader>
<bodyText confidence="0.9999341875">
For each article, we extracted 10 ,,,50 % of its para-
graphs as key paragraphs. The results of key para-
graphs experiment are shown in Table 4.
In Table 4, 10 50 % indicates the extraction ratio
used. &apos;Para.&apos; shows the number of paragraphs which
humans judged to be key paragraphs, and &apos;Correct&apos;
shows the number of these paragraphs which the
method obtained correctly. Evaluation is performed
by three human judges. When more than one hu-
man judges a paragraph as a key paragraph, the
paragraph is regarded as a key paragraph. `*&apos; in Ta-
ble 4 shows that the number of the correct data is
smaller than that of an extraction ratio. For exam-
ple, in Table 4, the number of paragraphs of 20 %
out of 22 is 4. However, the number of paragraphs
that more than one human judged the paragraph
</bodyText>
<equation confidence="0.6310345">
Recall
Preci8ion
</equation>
<page confidence="0.997594">
294
</page>
<tableCaption confidence="0.998671">
Table 4: The results of Key Paragraphs Experiment
</tableCaption>
<figure confidence="0.92430084">
Paragraph Percentage(%) Correct
(Article) %
10 20 30 40 50
Para. Correct Para. Correct Para. Correct Para. Correct Para. Correct
3(1) 1 1 1 1 1 1 1 1 2 2 100.0
4(13)
5(6)
6(6)
7(4)
8(3)
9(4)
10(2)
11(1)
12(1)
14(3)
15(2)
16(2)
17(1)
22(1)
13 12 13 12 13 12 13 12 26 21 88.4
6 5 6 5 *11 8 *10 9 18 14 96.0
6 6 6 6 *9 9 12 10 18 14 88.2
4 4 4 4 8 8 12 8 16 11 79.5
3 3 6 6 6 6 *8 6 12 7 80.0
4 4 8 8 *8 8 16 11 *18 9 74.0
</figure>
<table confidence="0.8776641">
2 2 4 2 *4 2 8 6 10 7 67.8
1 1 2 2 3 3 4 3 6 4 81.2
1 1 2 2 *2 2 *3 3 6 3 78.5
3 2 4 3 *6 4 *14 7 *19 10 56.5
*3 *2 *3 2 *3 2 *8 6 *14 10 70.9
*3 *3 *5 5 5 5 12 8 *16 10 75.6
2 2 3 3 *3 3 *7 4 *8 4 69.5
2 2 *2 2 *2 2 *4 2 *8 4 66.6
Total(50) 54 50 69 63 84 75 132 96 215 130
% 92.5 91.3 89.2 72.7 60.4
</table>
<bodyText confidence="0.996598">
as a key paragraph was only two. Therefore, 2 is
marked with a `40.
</bodyText>
<subsectionHeader confidence="0.998858">
6.4 Comparison to Other Related Work
</subsectionHeader>
<bodyText confidence="0.999990068965517">
Zechner proposed a method to extract key sentences
in an article by using simple statistical method; i.e.
TF*IDF term weighting method. In order to show
the applicability of our method, we applied Zech-
ner&apos;s key sentences method to key paragraphs ex-
traction and compared it with our method. In Zech-
ner&apos;s method, the sum over all TF*IDF values of the
content words for each sentence are calculated, and
the sentences are sorted according to their weights.
Finally a particular number of sentences are ex-
tracted as key sentences. The data we used con-
sists of 1.92 sentences par a paragraph and was not
so many sentences within a paragraph. Then, in
order to apply his method to key paragraphs ex-
traction, we calculated the sum over all sentences
for each paragraph, and sorted the paragraphs ac-
cording to their weights. From these, we extracted
a certain number of paragraphs (method_A). In our
method, every sense of words in articles for extract-
ing key paragraphs is disambiguated in advance and
linking method is performed. In order to examine
where the performance comes from, we also com-
pared our method to the method which WSD and
linking method are not applied. The result is shown
in Table 5.
In Table 5, &amp;quot;ro&apos; shows the extraction ratio, 10 50%
and &apos;Para.&apos; shows the number of paragraphs corre-
sponding to each &apos;Percentage&apos;. &apos;Our method&apos;, &apos;not
WSD&apos;, and `method_A&apos; shows the results using our
</bodyText>
<tableCaption confidence="0.996346">
Table 5: The results of comparative experiment
</tableCaption>
<table confidence="0.998427">
% Para. Our not WSD method_A
method(%)
10 54 50(92.5) 43(79.6) - 31(57.4)
20 69 63(91.3) 55(79.7) 35(50.7)
30 84 75(89.3) 66(78.5) 41(48.8)
40 132 96(72.7) 80(60.6) 63(47.7)
50 215 130(60.4) 112(52.8) 99(46.0)
414(74.7) 356(64.2) 269(48.6)
Total 554
</table>
<bodyText confidence="0.611489">
method, the method which WSD and linking are not
applied, and method_A, respectively.
</bodyText>
<sectionHeader confidence="0.999801" genericHeader="method">
7 Discussion
</sectionHeader>
<subsectionHeader confidence="0.999746">
7.1 Keywords Experiment
Effectiveness of the Method
</subsectionHeader>
<bodyText confidence="0.999988222222222">
According to Table 3, Recall and Precision values
range from 46.5/50.2 to 100.0/89.4, the mean being
78.7/78.6. This shows that our method is effective
even in a restricted domain such as financial articles,
e.g. Wall Street Journal, although the test set was
small (50 articles). Furthermore, the correct ratio
does not depend on the number of paragraphs in
an article. This shows that our context dependency
model is applicable for different size of the samples.
</bodyText>
<subsectionHeader confidence="0.914113">
Problem of the Method
</subsectionHeader>
<bodyText confidence="0.995765">
According to Table 3, the worst results of Recall
and Precision was (46.5/50.2) when the number of
</bodyText>
<page confidence="0.995537">
295
</page>
<bodyText confidence="0.96088875">
paragraphs was 14. As a result, the result of the
extraction of key paragraphs shown in Table 4 was
also worst (56.5%). The possible causes of the error
were summarised the following two points:
(1) The formulae of context dependency
The sample results of keywords of the article,
`Abermin sues Geanges in Effort to rescind Joint
Gold Venture&apos; is shown in Table 6.
</bodyText>
<tableCaption confidence="0.989263">
Table 6: Keywords and their )(2 values in the article
</tableCaption>
<table confidence="0.999594294117647">
Keyword Paragraph Article Domain
Abermin 0.582 10.835 663.605
Belzberg 1.468 1.548 94.801
lin 1.468 1.548 94.801
gold5 1.770 2.496 52.865
Granges 0.680 15.478 948.007
Manitoba 1.468 1.548 94.801
mill 1.706 4.925 94.801
ounces 1.765 5.064 284.402
reserves 2.912 3.060 94.801
suit2 1.099 3.096 189.601
supremel 1.468 1.548 94.801
tartanl 0.251 6.191 379.203
word237 4.633 5.132 362.887
word238 1.468 1.548 94.801
others 15 • • • • • • • •
Total average 1.772 2.383 78.161
</table>
<bodyText confidence="0.99751705">
In Table 6, each value of &apos;Paragraph&apos;, &apos;Article&apos;, and
&apos;Domain&apos;, shows each )(2 value. &apos;Total average&apos; shows
the mean of all keywords. `word237&apos; and &apos;word238&apos;
are representative words which are the result of link-
ing noun with their semantically similar nouns. Ac-
cording to Table 6, we can observe that in &apos;Para-
graph&apos;, for example, some words whose x2 values
are slightly higher than the average (1,772) exist.
For example, the x2 value of `word237&apos; is 4.633 and
slightly higher than 1.772. However, `word237&apos; satis-
fies the formulae of context dependency. As a result,
`word237&apos; is regarded as a keyword, while this is not.
When the extracted ratio was 10%, there were four
articles whose correct ratio did not attained 100%.
Of these, three articles are classified into this type
of the error.
From the above observation, we can estimate that
the formulae of context dependency are weak con-
straints in some domains, while they are still effec-
tive even in a restricted domain. In order to get
more accuracy, some other constraints such as loca-
tion heuristics (Baxendale, 1958) or upper-case word
feature (Kupiec, 1995) might be necessary to be in-
troduced into our framework.
(2) The error of WSD
When the extracted ratio was 10%, there was one
article out of four articles which could not be ex-
tracted correctly because of the error of WSD. The
test article and the results of it was shown in Figure
2.
In Figure 2, the headline shows the title name. The
numbers show the paragraph number, and the un-
derlined words are keywords which are extracted in
our method. The bottom shows the result of key
paragraphs extraction. According to Figure 2, when
the extraction ratio was 50%, the paragraphs 3 and
4 were extracted and the paragraph 1 was not ex-
tracted, although it is a key paragraph. The key-
words and their frequencies of appearance in para-
graph 1, 3, and 4 are shown in Table 7.
</bodyText>
<tableCaption confidence="0.998995">
Table 7: The words and their frequencies
</tableCaption>
<table confidence="0.981756">
Para. 1 Para. 3 Para. 4
Fr. Word Fr. Word Fr. Word
1 crystal4 1 concern2 1 american2
1 oil4 1 crystal2 1 crystal2
5 word237 1 energy4 1 oil3
1 word78 1 oil3 5 word237
1 rate5 1 word78
5 word237
word78: Nov., yesterday2
word237: exchangel, offer4, notes, shares,
</table>
<bodyText confidence="0.9819355625">
stock5, amount4, tradingl, stockl,
cents
According to Table 7, &apos;crystal&apos; and &apos;oil in paragraph
1 are disambiguated incorrectly and were replaced
by `crystal4&apos; and `oil4&apos;, respectively, while &apos;crystal&apos;
should have been replaced by `crystal2&apos; and &apos;oil&apos; with
`oil3&apos;. Therefore, the number of words which appear
in both paragraph 3 and 4 was larger than any other
pair of paragraphs. As a result, paragraph 3 and 4
are the most semantically similar paragraphs and 1
was not extracted as a key paragraph.
In our method, the correct ratio of key paragraphs
extraction strongly depends on the results of WSD.
The correct ratio of our WSD was 78.4% (Fukumoto,
1996). In order to get higher accuracy, it is necessary
to improve our WSD method.
</bodyText>
<subsectionHeader confidence="0.999618">
7.2 Key Paragraphs Experiment
Effectiveness of the Method
</subsectionHeader>
<bodyText confidence="0.999722571428571">
In Key Paragraphs Experiment, the overall results
were positive, especially when the ratio of extrac-
tion was 10-30%. The ratios of correct judgements
in these cases were significantly high; i.e. 92.5%,
91.3%, and 89.2%, respectively. This demonstrates
the applicability of the degree of context depen-
dency.
</bodyText>
<subsectionHeader confidence="0.970477">
Limitations of the Method
</subsectionHeader>
<bodyText confidence="0.9999405">
When the ratio of extraction was higher than 30%,
the results was 72.7% and 60.4%. Furthermore, the
more paragraphs are in an article, the smaller the
number of correct judgements. One possible cause
of these results is that the clustering method might
have a negative effect on extracting key paragraphs.
</bodyText>
<page confidence="0.99476">
296
</page>
<figure confidence="0.2825357">
Crystal Oil Co. Extends Offer
1 Crystal4 oil4 co. said it extended to Nov. 17 the exchangel offer4 for all of its non-interest-
bearing convertible secured notes, due 1997, for shares of its common stock5.
2 The offer4 had been set to expire yesterdayl.
3 The companyl said about 65.89% of the notes outstanding have been tendered, under the
plan5, the notes will be exchanged at a rate5 of 65 shares of crystal2 oil3 common for each
$1,000 principal amount4 of the notes, the energy4 concern2 said.
4 In composite tradingl on the american2 stockl exchangel yesterday2, crystal2 oil3 shares closed
at $2.875, up 12.5 cents.
The results of Key Paragraph Extraction: 3 --+ 4 1 2
</figure>
<figureCaption confidence="0.999594">
Figure 2: The sample of the article
</figureCaption>
<bodyText confidence="0.969439777777778">
In the field of text summarisation, a vector model
was often used for extracting key sentence or key
paragraph (Tokunaga, 1994), (Zechner, 1996). In
this model, the sentences with term weighting are
sorted according to their weights and this informa-
tion is used to extract a certain ratio of highest
weighted paragraph in an article. We implemented
this model and compared it with our clustering tech-
nique. The results are shown in Table 8.
</bodyText>
<tableCaption confidence="0.998432">
Table 8: Our method and a vector model
</tableCaption>
<table confidence="0.999451333333333">
% Para. Our method(%) Vector model(%)
10 54 50(92.5) 48(88.9)
20 69 63(91.3) 58(84.1)
30 84 75(89.3) 68(78.6)
40 132 96(72.7) 91(69.0)
50 215 130(60.4) 128(60.6)
</table>
<bodyText confidence="0.999695315789474">
In Table 8, &amp;quot;Yo&apos; shows the extraction ratio, 10 50%
and &apos;Para.&apos; shows the number of total paragraphs
corresponding to each `%&apos;. &apos;Our method&apos;, and &apos;Vec-
tor model&apos; shows the results of our method, and us-
ing vector model, respectively.
Table 8 shows that the results using our method
are highly than those of using the vector model. In
our method, when the extraction ratio was more
than 30%, the correct ratio decreased. This phe-
nomena is also observed in the vector model. From
the observation, we can estimate that the cause of
the results was not our clustering technique. Exam-
ining the results of human judges, when the number
of paragraphs was more than 14, the number of para-
graphs marked with a &apos;40 is large. This shows that
it is too difficult even for a human to judge whether
a paragraph is a key paragraph or not. From the
observation, for these articles, there are limitations
to our method based on context dependency.
</bodyText>
<subsectionHeader confidence="0.662089">
Other Heuristics
</subsectionHeader>
<bodyText confidence="0.938727875">
As we discussed in Keywords Experiment, it might
be considered that some heuristics such as location
of paragraphs are introduced into our method to get
a higher accuracy of keywords and key paragraphs
extraction, even in these articles. Table 9 shows
the location of key paragraphs extracted using our
method and extracted by humans. The extraction
ratio described in Table 9 is 30%.
</bodyText>
<tableCaption confidence="0.999271">
Table 9: The location of key paragraphs
</tableCaption>
<table confidence="0.996854777777778">
Articles
Hum. Method
(a)First 39 37
(b)First and Last 4 4
(c)First, Mid-position, and Last 1 1
(d)First and Mid-position 4 4
(e)Mid-position 0 1
(f)Otherwise 2 3
Total 50 50
</table>
<bodyText confidence="0.9995502">
In Table 9, each paragraph (First, Mid-position, and
Last paragraph) includes the paragraphs around it.
According to Table 9, in human judgement, 39 out
of 50 articles&apos; key paragraphs are located in the first
parts, and the ratio attained 78.0%. This shows
that using only location heuristics (the key para-
graph tends to be located in the first parts) is a weak
constraint in itself, since the results of our method
showed that the correct ratio attained 89.2%. How-
ever, in our method, 2 articles are not extracted cor-
rectly, while the key paragraph is located in the first
parts of these articles. From the observation, in a
corpus such as Wall Street Journal, utilising a lo-
cation heuristics is useful for extracting key para-
graphs.
</bodyText>
<subsectionHeader confidence="0.994509">
7.3 Comparison to Other Related Work
</subsectionHeader>
<bodyText confidence="0.999681444444444">
According to Table 5, the average ratio of our
method and method_A was 74.7%, and 48.6%, re-
spectively. This shows that method_A is not more
effective than our method. This is because most of
nouns do not contribute to showing the characteris-
tic of each domain for given articles. In the test data
which consists of 3,802 different nouns, 2,171 nouns
appeared in only one article and the frequency of
each of them is one. We recall that in method_A,
</bodyText>
<page confidence="0.986763">
297
</page>
<bodyText confidence="0.999842153846154">
when word i appears in only one article and the fre-
quency of i is one, the value of TF*IDF equals to
log50. There are 2,955 out of 3,802 nouns whose
TF*IDF value is less than log50, and the percentage
attained at 77.7%. This causes the fact that most of
nouns do not contribute to showing the characteris-
tic of each domain for given articles.
Comparing the difference ratio of &apos;Our method&apos;
and &apos;not WSD&apos; to that of &apos;not WSD&apos; and method_A,
the former was 10.5% and the latter was 15.6%.
Therefore, our context dependency model con-
tributes the extraction of key paragraphs, although
WSD and linking are still effective.
</bodyText>
<sectionHeader confidence="0.998124" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999949333333333">
We have reported an experimental study for extract-
ing key paragraphs based on the degree of context
dependency for a given article and showed how our
context dependency model can use effectively to ex-
tract key paragraphs, each of which belongs to the
restricted subject domain. In order to cope with
the remaining problems mentioned in section 7 and
apply this work to practical use, we will conduct
further experiments.
</bodyText>
<sectionHeader confidence="0.997869" genericHeader="acknowledgments">
9 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9696155">
The authors would like to thank the reviewers for
their valuable comments.
</bodyText>
<sectionHeader confidence="0.997374" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996594052631579">
P. B. Baxendale, &amp;quot;Man-made index for technical lit-
erature - an experiment&amp;quot;, IBM J. Res. Develop.,
2(1958)4, pp. 354-361, 1958
E. Brill, &amp;quot;A simple rule-based part of speech tag-
ger&amp;quot;, In Proc. of the 3rd conference on applied
natural language processing, pp. 152-155, 1992
P. F. Brown et al., &amp;quot;Word-Sense Disambiguation
Using Statistical Methods&amp;quot;, In Proc. of the 29th
Annual Meeting of the ACL, pp. 264-270, 1991
H. P. Edmundson, &amp;quot;New methods in automatic ab-
stracting&amp;quot;, Journal of ACM, 16(1969)2, pp. 264-
285, 1969
F. Fukumoto and Y. Suzuki, &amp;quot;An Automatic Clus-
tering of Articles Using Dictionary Definitions&amp;quot;,
In Proc. of the 16th COLING, pp. 406-411, 1996
N. Jardine and R. Sibson, &amp;quot;The construction of hi-
erarchic and non-hierarchic classifications&amp;quot;, Com-
puter Journal, pp. 177-184, 1968
P. S. Jacobs and L. F. Rau, &amp;quot;SCISOR: Extracting
information from on-line news&amp;quot;, Communications
of the ACM, 33(1990)11, pp. 88-97, 1990
J. Kupiec et al., &amp;quot;A trainable document summa-
rizer&amp;quot;, In Proc. of SIGIR&apos;95, pp. 68-73, 1995
M. Liberman, &amp;quot;CD-ROM I Association for Com-
putational Linguistics Data Collection Initiative&amp;quot;,
University of Pennsylvania, 1991
H. P. Luhn, &amp;quot;A statistical approach to mechanized
encoding and searching of literary information&amp;quot;,
IBM journal, 1(1957)4, pp. 307-319, 1957
H. P. Luhn, &amp;quot;The Automatic Creation of Literature
Abstracts&amp;quot;, IBM journal, 2(1958)1, pp. 159-165,
1958
S. Miike et al., &amp;quot;A full-text retrieval system with a
dynamic abstract generation function&amp;quot;, In Proc.
of SIGIR&apos;94, Pp. 152-161, 1994
M. Nagao et al., &amp;quot;An Automatic Method of the Ex-
traction of Important Words from Japanese Sci-
entific Documents (in Japanese)&amp;quot;, IPS Japan,
17(1976)2, pp. 110-117, 1976
Y. Niwa and Y. Nitta, &amp;quot;Co-occurrence vectors from
corpora vs. distance vectors from dictionaries&amp;quot;, In
Proc. of the 15th COLING, pp. 304-309, 1994
C. D. Paice, &amp;quot;Constructing literature abstracts
by computer:Techniques and prospects&amp;quot;, Infor-
mation Processing and Management, vol. 26, pp.
171-186, 1990
C. D. Paice and P. A. Jones, &amp;quot;The identification of
important concepts in highly structured technical
papers&amp;quot;, In Proc. of SIGIR&apos;93, pp. 69-78, 1993
U. Reimer and U. Hahn, &amp;quot;Text condensation as
knowledge base abstraction&amp;quot;, IEEE Conference on
Al Applications, pp. 338-344, 1988
G. Salton and M. J. McGill, &amp;quot;Introduction to Mod-
ern Information Retrieval&amp;quot;, McGraw-Hill, 1983
G. Salton et al., &amp;quot;Approaches to passage retrieval
in full text information systems&amp;quot;, In Proc. of SI-
GIR&apos;93, pp. 49-58, 1993
K. J. Sparck, &amp;quot;A statistical interpretation of term
specificity and its application in retrieval&amp;quot;, Jour-
nal of Documentation, 28(1973)1, pp. 11-21, 1973
T. Tokunaga and M. Iwayama, &amp;quot;Text Categoriza-
tion based on Weighted Inverse Document Fre-
quency&amp;quot;, SIG-IPS Japan, 100(1994)5, pp. 33-40,
1994
Y. Watanabe et al., &amp;quot;Document Classification Using
Domain Specific Kanji Characters Extracted by
X2 Method&amp;quot;, In Proc. of the 16th COLING, PP.
794-799, 1996
D. Yarowsky, &amp;quot;Word sense disambiguation using sta-
tistical models of Roget&apos;s categories trained on
large corpora&amp;quot;, In Proc. of the 14th COLING, pp.
454-460, 1992
K. Zechner, &amp;quot;Fast Generation of Abstracts from
General Domain Text Corpora by Extracting Rel-
evant Sentences&amp;quot;, In Proc. of the 16th COLING,
pp. 986-989, 1996
</reference>
<page confidence="0.997116">
298
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.897399">
<title confidence="0.998346">An Automatic Extraction of Key Paragraphs Based on Context Dependency</title>
<author confidence="0.998596">Fumiyo Fukumoto Yoshimi Suzukit Jun&apos;ichi Fukumotot</author>
<affiliation confidence="0.9944995">of Electrical Engineering and Kansai Lab., R &amp; D Group Computer Science, Yamanashi University Oki Electric Industry Co. Ltd.,$</affiliation>
<address confidence="0.973536">4-3-11 Takeda, Kofu 400 Japan 1-2-27 Shiromi, Chuo-ku, Osaka 540 Japan</address>
<email confidence="0.989006">ffukumoto@skye,ysuzuki@suwatl.esi.yamanashi.ac.iPfukumotoakansai.oki.co.jp</email>
<abstract confidence="0.995811769230769">In this paper, we propose a method for extracting key paragraphs in articles based on the degree of context dependency. Like Luhn&apos;s technique, our method assumes that the words related to theme in an article appear throughout paragraphs. Our extraction technique of keywords is based on the degree of context dependency that how strongly a word is related to a given context. The results of experiments demonstrate the applicability of our proposed method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P B Baxendale</author>
</authors>
<title>Man-made index for technical literature - an experiment&amp;quot;,</title>
<date>1958</date>
<journal>IBM J. Res. Develop.,</journal>
<volume>2</volume>
<issue>1958</issue>
<pages>354--361</pages>
<contexts>
<context position="2232" citStr="Baxendale, 1958" startWordPosition="330" endWordPosition="331">of particular subject fields is required (Paice, 1990), (Zechner, 1996). The other, alternative strategy is the approach that relies mainly on corpus statistics (Paice, 1990), (Paice, 1993). The main task of this approach is the sentence scoring process. Typically, weights are assigned to the individual words in a text, and the complete sentence scores are then based on the occurrence characteristics of highly-weighted terms (keywords) in the respective sentences. Term weighting technique has been widely investigated in information retrieval and lots of techniques such as location heuristics (Baxendale, 1958), rhetorical relations (Miike, 1994), and title information (Edmundson, 1969) have been proposed. These techniques seem to be less dependent on the domain. However, Salton claims that it is difficult to produce high accuracy of retrieval by using these termweighting approaches (Salton, 1993). The other term weighting technique is based on keyword frequency (Luhn, 1958). Keyword frequency is further less dependent on the domain than other weighting methods and therefore, well studied. Major approaches which are based on keyword frequency assume on the fact that the keywords of the article appea</context>
<context position="22036" citStr="Baxendale, 1958" startWordPosition="3821" endWordPosition="3822">nd slightly higher than 1.772. However, `word237&apos; satisfies the formulae of context dependency. As a result, `word237&apos; is regarded as a keyword, while this is not. When the extracted ratio was 10%, there were four articles whose correct ratio did not attained 100%. Of these, three articles are classified into this type of the error. From the above observation, we can estimate that the formulae of context dependency are weak constraints in some domains, while they are still effective even in a restricted domain. In order to get more accuracy, some other constraints such as location heuristics (Baxendale, 1958) or upper-case word feature (Kupiec, 1995) might be necessary to be introduced into our framework. (2) The error of WSD When the extracted ratio was 10%, there was one article out of four articles which could not be extracted correctly because of the error of WSD. The test article and the results of it was shown in Figure 2. In Figure 2, the headline shows the title name. The numbers show the paragraph number, and the underlined words are keywords which are extracted in our method. The bottom shows the result of key paragraphs extraction. According to Figure 2, when the extraction ratio was 50</context>
</contexts>
<marker>Baxendale, 1958</marker>
<rawString>P. B. Baxendale, &amp;quot;Man-made index for technical literature - an experiment&amp;quot;, IBM J. Res. Develop., 2(1958)4, pp. 354-361, 1958</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>A simple rule-based part of speech tagger&amp;quot;,</title>
<date>1992</date>
<booktitle>In Proc. of the 3rd conference on applied natural language processing,</booktitle>
<pages>152--155</pages>
<contexts>
<context position="14005" citStr="Brill, 1992" startWordPosition="2362" endWordPosition="2363">s extracting technique and with verifying the effect of our method which introduces context dependency. The second experiment, Key Paragraphs Experiment, shows how the extracted keywords can be used to extract key paragraphs. In the third experiment, Comparison to Other Related Work, we applied Zechner&apos;s key sentences method (Zechner, 1996) to key paragraphs extraction (we call this method_A), and compared it with our method. 8.1 Data The corpus we have used is the 1988, 1989 Wall Street Journal (Liberman, 1991) in ACL/DCI CDROM which consists of about 280,000 part-of-speech tagged sentences (Brill, 1992). Wall Street Journal consists of many articles, and each article has a title name. These titles are classified into 76 different domains. We selected 10 different domains and used them as Domain. As a test data, we selected 50 articles each of which belongs to one of these 10 domains. The selected domain names and the number of articles are shown in Table 2. Table 2: The selected data Domain No Domain No BB : buybacks 6 BVG: beverages 8 DIV: dividends 5 FOD: food products 5 STK: stock market 5 RET: retailing 1 ARO: aerospace 5 ENV: environment 3 PCS: stones, gold 9 CMD: farm products 3 There </context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>E. Brill, &amp;quot;A simple rule-based part of speech tagger&amp;quot;, In Proc. of the 3rd conference on applied natural language processing, pp. 152-155, 1992</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
</authors>
<title>Word-Sense Disambiguation Using Statistical Methods&amp;quot;,</title>
<date>1991</date>
<booktitle>In Proc. of the 29th Annual Meeting of the ACL,</booktitle>
<pages>264--270</pages>
<contexts>
<context position="8021" citStr="Brown, 1991" startWordPosition="1277" endWordPosition="1278"> is smaller than that of the Article. 2. The deviation value of a word in the Article is smaller than that of the Domain. 3 Term Weighting Every sense of words in articles for extracting key paragraphs is automatically disambiguated in advance. This is because to disambiguate word-senses in articles might affect the accuracy of context dependent (domain specific) key paragraphs retrieval, since the meaning of a word characterises the domain in which it is used. Word-sense disambiguation (WSD in short) is a serious problem for NLP, and a variety of approaches have been proposed for solving it (Brown, 1991), (Yarowsky, 1992). Our disambiguation method is based on Niwa&apos;s method which uses the similarity between a sentence containing a polysemous noun and a sentence of dictionarydefinition (Niwa, 1994). Furthermore, we linked nouns which are disambiguated with their semantically similar nouns mainly in order to cope with the problem of a phrasal lexicon. A phrasal lexicon such as Atlantic Seaboard, New England gives a negative influence for keywords retrieval, since it can not be regarded as units, i.e. each word which is the element of a phrasal lexicon is assigned to each semantic code (Fukumoto</context>
</contexts>
<marker>Brown, 1991</marker>
<rawString>P. F. Brown et al., &amp;quot;Word-Sense Disambiguation Using Statistical Methods&amp;quot;, In Proc. of the 29th Annual Meeting of the ACL, pp. 264-270, 1991</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New methods in automatic abstracting&amp;quot;,</title>
<date>1969</date>
<journal>Journal of ACM,</journal>
<volume>16</volume>
<issue>1969</issue>
<pages>264--285</pages>
<contexts>
<context position="2309" citStr="Edmundson, 1969" startWordPosition="340" endWordPosition="341">other, alternative strategy is the approach that relies mainly on corpus statistics (Paice, 1990), (Paice, 1993). The main task of this approach is the sentence scoring process. Typically, weights are assigned to the individual words in a text, and the complete sentence scores are then based on the occurrence characteristics of highly-weighted terms (keywords) in the respective sentences. Term weighting technique has been widely investigated in information retrieval and lots of techniques such as location heuristics (Baxendale, 1958), rhetorical relations (Miike, 1994), and title information (Edmundson, 1969) have been proposed. These techniques seem to be less dependent on the domain. However, Salton claims that it is difficult to produce high accuracy of retrieval by using these termweighting approaches (Salton, 1993). The other term weighting technique is based on keyword frequency (Luhn, 1958). Keyword frequency is further less dependent on the domain than other weighting methods and therefore, well studied. Major approaches which are based on keyword frequency assume on the fact that the keywords of the article appear frequently in the article, but appear seldom in other articles (Luhn, 1958)</context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>H. P. Edmundson, &amp;quot;New methods in automatic abstracting&amp;quot;, Journal of ACM, 16(1969)2, pp. 264-285, 1969</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Fukumoto</author>
<author>Y Suzuki</author>
</authors>
<title>An Automatic Clustering of Articles Using Dictionary Definitions&amp;quot;,</title>
<date>1996</date>
<booktitle>In Proc. of the 16th COLING,</booktitle>
<pages>406--411</pages>
<marker>Fukumoto, Suzuki, 1996</marker>
<rawString>F. Fukumoto and Y. Suzuki, &amp;quot;An Automatic Clustering of Articles Using Dictionary Definitions&amp;quot;, In Proc. of the 16th COLING, pp. 406-411, 1996</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Jardine</author>
<author>R Sibson</author>
</authors>
<title>The construction of hierarchic and non-hierarchic classifications&amp;quot;,</title>
<date>1968</date>
<journal>Computer Journal,</journal>
<pages>177--184</pages>
<marker>Jardine, Sibson, 1968</marker>
<rawString>N. Jardine and R. Sibson, &amp;quot;The construction of hierarchic and non-hierarchic classifications&amp;quot;, Computer Journal, pp. 177-184, 1968</rawString>
</citation>
<citation valid="true">
<authors>
<author>P S Jacobs</author>
<author>L F Rau</author>
</authors>
<title>SCISOR: Extracting information from on-line news&amp;quot;,</title>
<date>1990</date>
<journal>Communications of the ACM,</journal>
<volume>33</volume>
<issue>1990</issue>
<pages>88--97</pages>
<marker>Jacobs, Rau, 1990</marker>
<rawString>P. S. Jacobs and L. F. Rau, &amp;quot;SCISOR: Extracting information from on-line news&amp;quot;, Communications of the ACM, 33(1990)11, pp. 88-97, 1990</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
</authors>
<title>A trainable document summarizer&amp;quot;,</title>
<date>1995</date>
<booktitle>In Proc. of SIGIR&apos;95,</booktitle>
<pages>68--73</pages>
<institution>University of Pennsylvania,</institution>
<contexts>
<context position="22078" citStr="Kupiec, 1995" startWordPosition="3827" endWordPosition="3828">237&apos; satisfies the formulae of context dependency. As a result, `word237&apos; is regarded as a keyword, while this is not. When the extracted ratio was 10%, there were four articles whose correct ratio did not attained 100%. Of these, three articles are classified into this type of the error. From the above observation, we can estimate that the formulae of context dependency are weak constraints in some domains, while they are still effective even in a restricted domain. In order to get more accuracy, some other constraints such as location heuristics (Baxendale, 1958) or upper-case word feature (Kupiec, 1995) might be necessary to be introduced into our framework. (2) The error of WSD When the extracted ratio was 10%, there was one article out of four articles which could not be extracted correctly because of the error of WSD. The test article and the results of it was shown in Figure 2. In Figure 2, the headline shows the title name. The numbers show the paragraph number, and the underlined words are keywords which are extracted in our method. The bottom shows the result of key paragraphs extraction. According to Figure 2, when the extraction ratio was 50%, the paragraphs 3 and 4 were extracted a</context>
</contexts>
<marker>Kupiec, 1995</marker>
<rawString>J. Kupiec et al., &amp;quot;A trainable document summarizer&amp;quot;, In Proc. of SIGIR&apos;95, pp. 68-73, 1995 M. Liberman, &amp;quot;CD-ROM I Association for Computational Linguistics Data Collection Initiative&amp;quot;, University of Pennsylvania, 1991</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>A statistical approach to mechanized encoding and searching of literary information&amp;quot;,</title>
<date>1957</date>
<journal>IBM journal,</journal>
<volume>1</volume>
<issue>1957</issue>
<pages>307--319</pages>
<contexts>
<context position="8923" citStr="Luhn, 1957" startWordPosition="1421" endWordPosition="1422">ainly in order to cope with the problem of a phrasal lexicon. A phrasal lexicon such as Atlantic Seaboard, New England gives a negative influence for keywords retrieval, since it can not be regarded as units, i.e. each word which is the element of a phrasal lexicon is assigned to each semantic code (Fukumoto, 1996). To the results of WSD and linking methods, we then applied a term weighting method to extract keywords. There have been several term weighting based on word frequencies, such as TF(Term Frequency), IDF(Inverse Document Frequency), TF*IDF, WIDF(Weighted Inverse Document Frequency) (Luhn, 1957), (Sparck, 1973), (Salton, 1983). (Tokunaga, 1994). We used Watan: 0. 0:0 In ernational • • • General Safecard signal corp. services inc. Jostens Inc. Paragraph • • • 292 abe&apos;s x2 method for term weighting which is shown in formula (1) (Watanabe, 1996). 2 (zi2—m2)2 if &gt; mii (1) Xij = 0 otherwise Formula (1) shows the value of )(2 of the word i in the domain j. xii in (1) is the frequency of word i in the domain j. mii in (1) is shown in formula (2). Ejl=i Xii /12ij = T-4* E xi; (2) L.-i=i1=1 In formula (2), k is the number of different words and 1 is the number of the domains. A larger value o</context>
</contexts>
<marker>Luhn, 1957</marker>
<rawString>H. P. Luhn, &amp;quot;A statistical approach to mechanized encoding and searching of literary information&amp;quot;, IBM journal, 1(1957)4, pp. 307-319, 1957 H. P. Luhn, &amp;quot;The Automatic Creation of Literature Abstracts&amp;quot;, IBM journal, 2(1958)1, pp. 159-165, 1958</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miike</author>
</authors>
<title>A full-text retrieval system with a dynamic abstract generation function&amp;quot;,</title>
<date>1994</date>
<booktitle>In Proc. of SIGIR&apos;94,</booktitle>
<pages>152--161</pages>
<contexts>
<context position="2268" citStr="Miike, 1994" startWordPosition="334" endWordPosition="335"> (Paice, 1990), (Zechner, 1996). The other, alternative strategy is the approach that relies mainly on corpus statistics (Paice, 1990), (Paice, 1993). The main task of this approach is the sentence scoring process. Typically, weights are assigned to the individual words in a text, and the complete sentence scores are then based on the occurrence characteristics of highly-weighted terms (keywords) in the respective sentences. Term weighting technique has been widely investigated in information retrieval and lots of techniques such as location heuristics (Baxendale, 1958), rhetorical relations (Miike, 1994), and title information (Edmundson, 1969) have been proposed. These techniques seem to be less dependent on the domain. However, Salton claims that it is difficult to produce high accuracy of retrieval by using these termweighting approaches (Salton, 1993). The other term weighting technique is based on keyword frequency (Luhn, 1958). Keyword frequency is further less dependent on the domain than other weighting methods and therefore, well studied. Major approaches which are based on keyword frequency assume on the fact that the keywords of the article appear frequently in the article, but app</context>
</contexts>
<marker>Miike, 1994</marker>
<rawString>S. Miike et al., &amp;quot;A full-text retrieval system with a dynamic abstract generation function&amp;quot;, In Proc. of SIGIR&apos;94, Pp. 152-161, 1994</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nagao</author>
</authors>
<title>An Automatic Method of the Extraction of Important Words from Japanese Scientific Documents (in Japanese)&amp;quot;,</title>
<date>1976</date>
<journal>IPS Japan,</journal>
<volume>17</volume>
<issue>1976</issue>
<pages>110--117</pages>
<contexts>
<context position="2924" citStr="Nagao, 1976" startWordPosition="439" endWordPosition="440">ve been proposed. These techniques seem to be less dependent on the domain. However, Salton claims that it is difficult to produce high accuracy of retrieval by using these termweighting approaches (Salton, 1993). The other term weighting technique is based on keyword frequency (Luhn, 1958). Keyword frequency is further less dependent on the domain than other weighting methods and therefore, well studied. Major approaches which are based on keyword frequency assume on the fact that the keywords of the article appear frequently in the article, but appear seldom in other articles (Luhn, 1958), (Nagao, 1976), (Salton, 1993), (Zechner, 1996). These approaches seem to show the effect in entirely different articles, such as &apos;weather forecasts&apos;, &apos;medical reports&apos;, and &apos;computer manuals&apos;. Because each different article is characterised by a larger number of words which appear frequently in one article, but appear seldom in other articles. However, in some articles from the same domain such as &apos;weather forecasts&apos;, one encounters quite a number of words which appear frequently over articles. Therefore, how to extract keyword from these words is a serious problem in such the restricted subject domain. In</context>
</contexts>
<marker>Nagao, 1976</marker>
<rawString>M. Nagao et al., &amp;quot;An Automatic Method of the Extraction of Important Words from Japanese Scientific Documents (in Japanese)&amp;quot;, IPS Japan, 17(1976)2, pp. 110-117, 1976</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Niwa</author>
<author>Y Nitta</author>
</authors>
<title>Co-occurrence vectors from corpora vs. distance vectors from dictionaries&amp;quot;,</title>
<date>1994</date>
<booktitle>In Proc. of the 15th COLING,</booktitle>
<volume>26</volume>
<pages>304--309</pages>
<marker>Niwa, Nitta, 1994</marker>
<rawString>Y. Niwa and Y. Nitta, &amp;quot;Co-occurrence vectors from corpora vs. distance vectors from dictionaries&amp;quot;, In Proc. of the 15th COLING, pp. 304-309, 1994 C. D. Paice, &amp;quot;Constructing literature abstracts by computer:Techniques and prospects&amp;quot;, Information Processing and Management, vol. 26, pp. 171-186, 1990</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Paice</author>
<author>P A Jones</author>
</authors>
<title>The identification of important concepts in highly structured technical papers&amp;quot;,</title>
<date>1993</date>
<booktitle>In Proc. of SIGIR&apos;93,</booktitle>
<pages>69--78</pages>
<marker>Paice, Jones, 1993</marker>
<rawString>C. D. Paice and P. A. Jones, &amp;quot;The identification of important concepts in highly structured technical papers&amp;quot;, In Proc. of SIGIR&apos;93, pp. 69-78, 1993</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Reimer</author>
<author>U Hahn</author>
</authors>
<title>Text condensation as knowledge base abstraction&amp;quot;,</title>
<date>1988</date>
<booktitle>IEEE Conference on Al Applications,</booktitle>
<pages>338--344</pages>
<marker>Reimer, Hahn, 1988</marker>
<rawString>U. Reimer and U. Hahn, &amp;quot;Text condensation as knowledge base abstraction&amp;quot;, IEEE Conference on Al Applications, pp. 338-344, 1988</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval&amp;quot;, McGraw-Hill,</title>
<date>1983</date>
<booktitle>In Proc. of SIGIR&apos;93,</booktitle>
<pages>49--58</pages>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M. J. McGill, &amp;quot;Introduction to Modern Information Retrieval&amp;quot;, McGraw-Hill, 1983 G. Salton et al., &amp;quot;Approaches to passage retrieval in full text information systems&amp;quot;, In Proc. of SIGIR&apos;93, pp. 49-58, 1993</rawString>
</citation>
<citation valid="true">
<authors>
<author>K J Sparck</author>
</authors>
<title>A statistical interpretation of term specificity and its application in retrieval&amp;quot;,</title>
<date>1973</date>
<journal>Journal of Documentation,</journal>
<volume>28</volume>
<issue>1973</issue>
<pages>11--21</pages>
<contexts>
<context position="8939" citStr="Sparck, 1973" startWordPosition="1423" endWordPosition="1424"> to cope with the problem of a phrasal lexicon. A phrasal lexicon such as Atlantic Seaboard, New England gives a negative influence for keywords retrieval, since it can not be regarded as units, i.e. each word which is the element of a phrasal lexicon is assigned to each semantic code (Fukumoto, 1996). To the results of WSD and linking methods, we then applied a term weighting method to extract keywords. There have been several term weighting based on word frequencies, such as TF(Term Frequency), IDF(Inverse Document Frequency), TF*IDF, WIDF(Weighted Inverse Document Frequency) (Luhn, 1957), (Sparck, 1973), (Salton, 1983). (Tokunaga, 1994). We used Watan: 0. 0:0 In ernational • • • General Safecard signal corp. services inc. Jostens Inc. Paragraph • • • 292 abe&apos;s x2 method for term weighting which is shown in formula (1) (Watanabe, 1996). 2 (zi2—m2)2 if &gt; mii (1) Xij = 0 otherwise Formula (1) shows the value of )(2 of the word i in the domain j. xii in (1) is the frequency of word i in the domain j. mii in (1) is shown in formula (2). Ejl=i Xii /12ij = T-4* E xi; (2) L.-i=i1=1 In formula (2), k is the number of different words and 1 is the number of the domains. A larger value of 4 means that t</context>
</contexts>
<marker>Sparck, 1973</marker>
<rawString>K. J. Sparck, &amp;quot;A statistical interpretation of term specificity and its application in retrieval&amp;quot;, Journal of Documentation, 28(1973)1, pp. 11-21, 1973 T. Tokunaga and M. Iwayama, &amp;quot;Text Categorization based on Weighted Inverse Document Frequency&amp;quot;, SIG-IPS Japan, 100(1994)5, pp. 33-40, 1994</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Watanabe</author>
</authors>
<title>Document Classification Using Domain Specific Kanji Characters Extracted by X2 Method&amp;quot;,</title>
<date>1996</date>
<booktitle>In Proc. of the 16th COLING,</booktitle>
<pages>794--799</pages>
<contexts>
<context position="9175" citStr="Watanabe, 1996" startWordPosition="1465" endWordPosition="1466">a phrasal lexicon is assigned to each semantic code (Fukumoto, 1996). To the results of WSD and linking methods, we then applied a term weighting method to extract keywords. There have been several term weighting based on word frequencies, such as TF(Term Frequency), IDF(Inverse Document Frequency), TF*IDF, WIDF(Weighted Inverse Document Frequency) (Luhn, 1957), (Sparck, 1973), (Salton, 1983). (Tokunaga, 1994). We used Watan: 0. 0:0 In ernational • • • General Safecard signal corp. services inc. Jostens Inc. Paragraph • • • 292 abe&apos;s x2 method for term weighting which is shown in formula (1) (Watanabe, 1996). 2 (zi2—m2)2 if &gt; mii (1) Xij = 0 otherwise Formula (1) shows the value of )(2 of the word i in the domain j. xii in (1) is the frequency of word i in the domain j. mii in (1) is shown in formula (2). Ejl=i Xii /12ij = T-4* E xi; (2) L.-i=i1=1 In formula (2), k is the number of different words and 1 is the number of the domains. A larger value of 4 means that the word i appears more frequently in the domain j than in the other. 4 An Extraction of Keywords The first step to extract keywords is to calculate x2 for each word in the Paragraph, the Article, and the Domain. We used formula (1) to c</context>
</contexts>
<marker>Watanabe, 1996</marker>
<rawString>Y. Watanabe et al., &amp;quot;Document Classification Using Domain Specific Kanji Characters Extracted by X2 Method&amp;quot;, In Proc. of the 16th COLING, PP. 794-799, 1996</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Word sense disambiguation using statistical models of Roget&apos;s categories trained on large corpora&amp;quot;,</title>
<date>1992</date>
<booktitle>In Proc. of the 14th COLING,</booktitle>
<pages>454--460</pages>
<contexts>
<context position="8039" citStr="Yarowsky, 1992" startWordPosition="1279" endWordPosition="1280">n that of the Article. 2. The deviation value of a word in the Article is smaller than that of the Domain. 3 Term Weighting Every sense of words in articles for extracting key paragraphs is automatically disambiguated in advance. This is because to disambiguate word-senses in articles might affect the accuracy of context dependent (domain specific) key paragraphs retrieval, since the meaning of a word characterises the domain in which it is used. Word-sense disambiguation (WSD in short) is a serious problem for NLP, and a variety of approaches have been proposed for solving it (Brown, 1991), (Yarowsky, 1992). Our disambiguation method is based on Niwa&apos;s method which uses the similarity between a sentence containing a polysemous noun and a sentence of dictionarydefinition (Niwa, 1994). Furthermore, we linked nouns which are disambiguated with their semantically similar nouns mainly in order to cope with the problem of a phrasal lexicon. A phrasal lexicon such as Atlantic Seaboard, New England gives a negative influence for keywords retrieval, since it can not be regarded as units, i.e. each word which is the element of a phrasal lexicon is assigned to each semantic code (Fukumoto, 1996). To the re</context>
</contexts>
<marker>Yarowsky, 1992</marker>
<rawString>D. Yarowsky, &amp;quot;Word sense disambiguation using statistical models of Roget&apos;s categories trained on large corpora&amp;quot;, In Proc. of the 14th COLING, pp. 454-460, 1992</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zechner</author>
</authors>
<title>Fast Generation of Abstracts from General Domain Text Corpora by Extracting Relevant Sentences&amp;quot;,</title>
<date>1996</date>
<booktitle>In Proc. of the 16th COLING,</booktitle>
<pages>986--989</pages>
<contexts>
<context position="1687" citStr="Zechner, 1996" startWordPosition="249" endWordPosition="250">an automatic summarisation, there are at least two approaches. One is knowledge-based approach with particular subject fields (Reimer, 1988), (Jacobs, 1990). This approach, based on deep knowledge of particular subject fields, is useful for restricted tasks, such as, for example, the construction of &apos;weather forecasts&apos; summaries. However, when unrestricted subject matter must be treated, as is often the case in practice, the passage retrieval and text summarisation methods proposed have not proven equal to the need, since deep knowledge of particular subject fields is required (Paice, 1990), (Zechner, 1996). The other, alternative strategy is the approach that relies mainly on corpus statistics (Paice, 1990), (Paice, 1993). The main task of this approach is the sentence scoring process. Typically, weights are assigned to the individual words in a text, and the complete sentence scores are then based on the occurrence characteristics of highly-weighted terms (keywords) in the respective sentences. Term weighting technique has been widely investigated in information retrieval and lots of techniques such as location heuristics (Baxendale, 1958), rhetorical relations (Miike, 1994), and title informa</context>
<context position="2957" citStr="Zechner, 1996" startWordPosition="443" endWordPosition="444">ues seem to be less dependent on the domain. However, Salton claims that it is difficult to produce high accuracy of retrieval by using these termweighting approaches (Salton, 1993). The other term weighting technique is based on keyword frequency (Luhn, 1958). Keyword frequency is further less dependent on the domain than other weighting methods and therefore, well studied. Major approaches which are based on keyword frequency assume on the fact that the keywords of the article appear frequently in the article, but appear seldom in other articles (Luhn, 1958), (Nagao, 1976), (Salton, 1993), (Zechner, 1996). These approaches seem to show the effect in entirely different articles, such as &apos;weather forecasts&apos;, &apos;medical reports&apos;, and &apos;computer manuals&apos;. Because each different article is characterised by a larger number of words which appear frequently in one article, but appear seldom in other articles. However, in some articles from the same domain such as &apos;weather forecasts&apos;, one encounters quite a number of words which appear frequently over articles. Therefore, how to extract keyword from these words is a serious problem in such the restricted subject domain. In this paper, we propose a method </context>
<context position="13735" citStr="Zechner, 1996" startWordPosition="2318" endWordPosition="2319">aragraph is larger than that of the fourth, the order of key paragraphs is 3 —+ 4 —4 1 2, otherwise, 4 3 1 —+ 2. 6 Experiments We have conducted three experiments to examine the effect of our method. The first experiment, Keywords Experiment, is concerned with the keywords extracting technique and with verifying the effect of our method which introduces context dependency. The second experiment, Key Paragraphs Experiment, shows how the extracted keywords can be used to extract key paragraphs. In the third experiment, Comparison to Other Related Work, we applied Zechner&apos;s key sentences method (Zechner, 1996) to key paragraphs extraction (we call this method_A), and compared it with our method. 8.1 Data The corpus we have used is the 1988, 1989 Wall Street Journal (Liberman, 1991) in ACL/DCI CDROM which consists of about 280,000 part-of-speech tagged sentences (Brill, 1992). Wall Street Journal consists of many articles, and each article has a title name. These titles are classified into 76 different domains. We selected 10 different domains and used them as Domain. As a test data, we selected 50 articles each of which belongs to one of these 10 domains. The selected domain names and the number of</context>
<context position="25438" citStr="Zechner, 1996" startWordPosition="4405" endWordPosition="4406">esterdayl. 3 The companyl said about 65.89% of the notes outstanding have been tendered, under the plan5, the notes will be exchanged at a rate5 of 65 shares of crystal2 oil3 common for each $1,000 principal amount4 of the notes, the energy4 concern2 said. 4 In composite tradingl on the american2 stockl exchangel yesterday2, crystal2 oil3 shares closed at $2.875, up 12.5 cents. The results of Key Paragraph Extraction: 3 --+ 4 1 2 Figure 2: The sample of the article In the field of text summarisation, a vector model was often used for extracting key sentence or key paragraph (Tokunaga, 1994), (Zechner, 1996). In this model, the sentences with term weighting are sorted according to their weights and this information is used to extract a certain ratio of highest weighted paragraph in an article. We implemented this model and compared it with our clustering technique. The results are shown in Table 8. Table 8: Our method and a vector model % Para. Our method(%) Vector model(%) 10 54 50(92.5) 48(88.9) 20 69 63(91.3) 58(84.1) 30 84 75(89.3) 68(78.6) 40 132 96(72.7) 91(69.0) 50 215 130(60.4) 128(60.6) In Table 8, &amp;quot;Yo&apos; shows the extraction ratio, 10 50% and &apos;Para.&apos; shows the number of total paragraphs c</context>
</contexts>
<marker>Zechner, 1996</marker>
<rawString>K. Zechner, &amp;quot;Fast Generation of Abstracts from General Domain Text Corpora by Extracting Relevant Sentences&amp;quot;, In Proc. of the 16th COLING, pp. 986-989, 1996</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>