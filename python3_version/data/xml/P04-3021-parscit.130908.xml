<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021961">
<title confidence="0.7860038">
Compiling Boostexter Rules into a Finite-state Transducer
Srinivas Bangalore
AT&amp;T Labs–Research
180 Park Avenue
Florham Park, NJ 07932
</title>
<sectionHeader confidence="0.963204" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997125">
A number of NLP tasks have been effectively mod-
eled as classification tasks using a variety of classi-
fication techniques. Most of these tasks have been
pursued in isolation with the classifier assuming un-
ambiguous input. In order for these techniques to be
more broadly applicable, they need to be extended
to apply on weighted packed representations of am-
biguous input. One approach for achieving this is
to represent the classification model as a weighted
finite-state transducer (WFST). In this paper, we
present a compilation procedure to convert the rules
resulting from an AdaBoost classifier into an WFST.
We validate the compilation technique by applying
the resulting WFST on a call-routing application.
</bodyText>
<sectionHeader confidence="0.998982" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999044196721312">
Many problems in Natural Language Processing
(NLP) can be modeled as classification tasks either
at the word or at the sentence level. For example,
part-of-speech tagging, named-entity identification
supertagging1, word sense disambiguation are tasks
that have been modeled as classification problems at
the word level. In addition, there are problems that
classify the entire sentence or document into one of
a set of categories. These problems are loosely char-
acterized as semantic classification and have been
used in many practical applications including call
routing and text classification.
Most of these problems have been addressed in
isolation assuming unambiguous (one-best) input.
Typically, however, in NLP applications these mod-
ules are chained together with each module intro-
ducing some amount of error. In order to alleviate
the errors introduced by a module, it is typical for a
module to provide multiple weighted solutions (ide-
ally as a packed representation) that serve as input
to the next module. For example, a speech recog-
nizer provides a lattice of possible recognition out-
puts that is to be annotated with part-of-speech and
1associating each word with a label that represents the syn-
tactic information of the word given the context of the sentence.
named-entities. Thus classification approaches need
to be extended to be applicable on weighted packed
representations of ambiguous input represented as a
weighted lattice. The research direction we adopt
here is to compile the model of a classifier into a
weighted finite-state transducer (WFST) so that it
can compose with the input lattice.
Finite state models have been extensively ap-
plied to many aspects of language processing in-
cluding, speech recognition (Pereira and Riley,
1997), phonology (Kaplan and Kay, 1994), mor-
phology (Koskenniemi, 1984), chunking (Abney,
1991; Bangalore and Joshi, 1999), parsing (Roche,
1999; Oflazer, 1999) and machine translation (Vilar
et al., 1999; Bangalore and Riccardi, 2000). Finite-
state models are attractive mechanisms for language
processing since they (a) provide an efficient data
structure for representing weighted ambiguous hy-
potheses (b) generally effective for decoding (c)
associated with a calculus for composing models
which allows for straightforward integration of con-
straints from various levels of speech and language
processing.2
In this paper, we describe the compilation pro-
cess for a particular classifier model into an WFST
and validate the accuracy of the compilation pro-
cess on a one-best input in a call-routing task. We
view this as a first step toward using a classification
model on a lattice input. The outline of the paper is
as follows. In Section 2, we review the classifica-
tion approach to resolving ambiguity in NLP tasks
and in Section 3 we discuss the boosting approach
to classification. In Section 4 we describe the com-
pilation of the boosting model into an WFST and
validate the result of this compilation using a call-
routing task.
</bodyText>
<sectionHeader confidence="0.881882" genericHeader="method">
2 Resolving Ambiguity by Classification
</sectionHeader>
<bodyText confidence="0.987740463414634">
In general, we can characterize all these tagging
problems as search problems formulated as shown
2Furthermore, software implementing the finite-state calcu-
lus is available for research purposes.
in Equation (1). We notate to be the input vocab-
ulary, to be the vocabulary of tags, an word
input sequence as ( ) and tag sequence as
( ). We are interested in , the most likely tag
sequence out of the possible tag sequences ( ) that
can be associated to .
Following the techniques of Hidden Markov
Models (HMM) applied to speech recognition, these
tagging problems have been previously modeled in-
directly through the transformation of the Bayes
rule as in Equation 2. The problem is then approx-
imated for sequence classification by a k -order
Markov model as shown in Equation (3).
Although the HMM approach to tagging can eas-
ily be represented as a WFST, it has a drawback in
that the use of large contexts and richer features re-
sults in sparseness leading to unreliable estimation
of the parameters of the model.
An alternate approach to arriving at is to
model Equation 1 directly. There are many exam-
ples in recent literature (Breiman et al., 1984; Fre-
und and Schapire, 1996; Roth, 1998; Lafferty et al.,
2001; McCallum et al., 2000) which take this ap-
proach and are well equipped to handle large num-
ber of features. The general framework for these
approaches is to learn a model from pairs of asso-
ciations of the form ( ) where is a feature
representation of and ( ) is one of the
members of the tag set. Although these approaches
have been more effective than HMMs, there have
not been many attempts to represent these models
as a WFST, with the exception of the work on com-
piling decision trees (Sproat and Riley, 1996). In
this paper, we consider the boosting (Freund and
Schapire, 1996) approach (which outperforms de-
cision trees) to Equation 1 and present a technique
for compiling the classifier model into a WFST.
</bodyText>
<sectionHeader confidence="0.997568" genericHeader="method">
3 Boostexter
</sectionHeader>
<bodyText confidence="0.99978009375">
Boostexter is a machine learning tool which is based
on the boosting family of algorithms first proposed
in (Freund and Schapire, 1996). The basic idea of
boosting is to build a highly accurate classifier by
combining many “weak” or “simple” base learner,
each one of which may only be moderately accurate.
A weak learner or a rule is a triple , which
tests a predicate (❇) of the input ( ) and assigns a
weight ( ) for each member ( ) of if
is true in and assigns a weight ( ) otherwise. It
is assumed that a pool of such weak learners
can be constructed easily.
From the pool of weak learners, the selection
the weak learner to be combined is performed it-
eratively. At each iteration, a weak learner
is selected that minimizes a prediction error loss
function on the training corpus which takes into ac-
count the weight assigned to each training exam-
ple. Intuitively, the weights encode how important
it is that correctly classifies each training exam-
ple. Generally, the examples that were most often
misclassified by the preceding base classifiers will
be given the most weight so as to force the base
learner to focus on the “hardest” examples. As de-
scribed in (Schapire and Singer, 1999), Boostexter
uses confidence rated classifiers that output a
real number whose sign (-1 or +1) is inter-
preted as a prediction, and whose magnitude
is a measure of “confidence”. The iterative algo-
rithm for combining weak learners stops after a pre-
specified number of iterations or when the training
set accuracy saturates.
</bodyText>
<subsectionHeader confidence="0.998073">
3.1 Weak Learners
</subsectionHeader>
<bodyText confidence="0.998701904761905">
In the case of text classification applications, the set
of possible weak learners is instantiated from sim-
ple -grams of the input text ( ). Thus, if is a
function to produce all -grams up to of its argu-
ment, then the set of predicates for the weak learn-
ers is . For word-level classification
problems, which take into account the left and right
context, we extend the set of weak learners created
from the word features with those created from the
left and right context features. Thus features of the
left context ( ), features of the right context ( )
and the features of the word itself ( ) constitute
the features at position. The predicates for the pool
of weak learners are created from these set of fea-
tures and are typically -grams on the feature repre-
sentations. Thus the set of predicates resulting from
the word level features is , from
left context features is and from
right context features is . The set
of predicates for the weak learners for word level
classification problems is: .
</bodyText>
<subsectionHeader confidence="0.989617">
3.2 Decoding
</subsectionHeader>
<bodyText confidence="0.994975428571429">
The result of training is a set of selected rules
( ). The output of the final
classifier is , i.e. the sum
of confidence of all classifiers . The real-valued
predictions of the final classifier can be converted
into probabilities by a logistic function transform;
that is
</bodyText>
<equation confidence="0.568496">
(4)
</equation>
<bodyText confidence="0.9789404">
Thus the most likely tag sequence is deter-
mined as in Equation 5, where is
computed using Equation 4.
(5)
To date, decoding using the boosted rule sets is
restricted to cases where the test input is unambigu-
ous such as strings or words (not word graphs). By
compiling these rule sets into WFSTs, we intend to
extend their applicability to packed representations
of ambiguous input such as word graphs.
</bodyText>
<sectionHeader confidence="0.998432" genericHeader="method">
4 Compilation
</sectionHeader>
<bodyText confidence="0.998659954545455">
We note that the weak learners selected at the end
of the training process can be partitioned into one
of three types based on the features that the learners
test.
We use the representation of context-dependent
rewrite rules (Johnson, 1972; Kaplan and Kay,
1994) and their weighted version (Mohri and
Sproat, 1996) to represent these weak learners. The
(weighted) context-dependent rewrite rules have the
general form
(6)
where , , and are regular expressions on the
alphabet of the rules. The interpretation of these
rules are as follows: Rewrite by when it is
preceded by and followed by. Furthermore,
can be extended to a rational power series which
are weighted regular expressions where the weights
encode preferences over the paths in (Mohri and
Sproat, 1996).
Each weak learner can then be viewed as a set
of weighted rewrite rules mapping the input word
into each member ( ) with a weight when
the predicate of the weak learner is true and with
weight when the predicate of the weak learner
is false. The translation between the three types of
weak learners and the weighted context-dependency
rules is shown in Table 13.
We note that these rules apply left to right on an
input and do not repeatedly apply at the same point
in an input since the output vocabulary would typ-
ically be disjoint from the input vocabulary .
We use the technique described in (Mohri and
Sproat, 1996) to compile each weighted context-
dependency rules into an WFST. The compilation
is accomplished by the introduction of context sym-
bols which are used as markers to identify locations
for rewrites of with . After the rewrites, the
markers are deleted. The compilation process is rep-
resented as a composition of five transducers.
The WFSTs resulting from the compilation of
each selected weak learner ( ) are unioned to cre-
ate the WFST to be used for decoding. The weights
of paths with the same input and output labels are
added during the union operation.
</bodyText>
<equation confidence="0.537648">
(7)
</equation>
<bodyText confidence="0.9876">
We note that the due to the difference in the nature
of the learning algorithm, compiling decision trees
results in a composition of WFSTs representing the
rules on the path from the root to a leaf node (Sproat
and Riley, 1996), while compiling boosted rules re-
sults in a union of WFSTs, which is expected to re-
sult in smaller transducers.
In order to apply the WFST for decoding, we sim-
ply compose the model with the input represented as
an WFST ( ) and search for the best path (if we are
interested in the single best classification result).
(8)
We have compiled the rules resulting from boos-
texter trained on transcriptions of speech utterances
from a call routing task with a vocabulary ( ) of
2912 and 40 classes ( ). There were a to-
tal of 1800 rules comprising of 900 positive rules
and their negative counterparts. The WFST result-
ing from compiling these rules has a 14372 states
and 5.7 million arcs. The accuracy of the WFST on
a random set of 7013 sentences was the same (85%
accuracy) as the accuracy with the decoder that ac-
companies the boostexter program. This validates
the compilation procedure.
</bodyText>
<sectionHeader confidence="0.997275" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.556186714285714">
Classification techniques have been used to effec-
tively resolve ambiguity in many natural language
3For ease of exposition, we show the positive and negative
sides of a rule each resulting in a context dependency rule.
However, we can represent them in the form of a single con-
text dependency rule which is ommitted here due to space con-
straints.
</bodyText>
<table confidence="0.996173">
: test features of the word
: test features of the left context
: test features of the right context
Type of Weak Learner Weak Learner Weighted Context Dependency Rule
: if WORD== then
else
: if LeftContext== then
else
: if RightContext== then
else
</table>
<tableCaption confidence="0.999953">
Table 1: Translation of the three types of weak learners into weighted context-dependency rules.
</tableCaption>
<bodyText confidence="0.999903444444444">
processing tasks. However, most of these tasks have
been solved in isolation and hence assume an un-
ambiguous input. In this paper, we extend the util-
ity of the classification based techniques so as to be
applicable on packed representations such as word
graphs. We do this by compiling the rules resulting
from an AdaBoost classifier into a finite-state trans-
ducer. The resulting finite-state transducer can then
be used as one part of a finite-state decoding chain.
</bodyText>
<sectionHeader confidence="0.998034" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999001521739131">
S. Abney. 1991. Parsing by chunks. In Robert
Berwick, Steven Abney, and Carol Tenny, editors,
Principle-based parsing. Kluwer Academic Pub-
lishers.
S. Bangalore and A. K. Joshi. 1999. Supertagging:
An approach to almost parsing. Computational
Linguistics, 25(2).
S. Bangalore and G. Riccardi. 2000. Stochastic
finite-state models for spoken language machine
translation. In Proceedings of the Workshop on
Embedded Machine Translation Systems.
L. Breiman, J.H. Friedman, R.A. Olshen, and
C.J. Stone. 1984. Classification and Regression
Trees. Wadsworth &amp; Brooks, Pacific Grove, CA.
Y. Freund and R. E. Schapire. 1996. Experi-
ments with a new boosting alogrithm. In Ma-
chine Learning: Proceedings of the Thirteenth
International Conference, pages 148–156.
C.D. Johnson. 1972. Formal Aspects of Phonologi-
cal Description. Mouton, The Hague.
R. M. Kaplan and M. Kay. 1994. Regular models of
phonological rule systems. Computational Lin-
guistics, 20(3):331–378.
K. K. Koskenniemi. 1984. Two-level morphol-
ogy: a general computation model for word-form
recognition and production. Ph.D. thesis, Uni-
versity of Helsinki.
J. Lafferty, A. McCallum, and F. Pereira. 2001.
Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In In
Proceedings of ICML, San Francisco, CA.
A. McCallum, D. Freitag, and F. Pereira. 2000.
Maximum entropy markov models for informa-
tion extraction and segmentation. In In Proceed-
ings of ICML, Stanford, CA.
M. Mohri and R. Sproat. 1996. An efficient com-
piler for weighted rewrite rules. In Proceedings
ofACL, pages 231–238.
K. Oflazer. 1999. Dependency parsing with an
extended finite state approach. In Proceedings
of the 37th Annual Meeting of the Association
for Computational Linguistics, Maryland, USA,
June.
F.C.N. Pereira and M.D. Riley. 1997. Speech
recognition by composition of weighted finite au-
tomata. In E. Roche and Schabes Y., editors,
Finite State Devices for Natural Language Pro-
cessing, pages 431–456. MIT Press, Cambridge,
Massachusetts.
E. Roche. 1999. Finite state transducers: parsing
free and frozen sentences. In Andr´as Kornai, ed-
itor, Extended Finite State Models of Language.
Cambridge University Press.
D. Roth. 1998. Learning to resolve natural lan-
guage ambiguities: A unified approach. In Pro-
ceedings of AAAI.
R.E. Schapire and Y. Singer. 1999. Improved
boosting algorithms using confidence-rated pre-
dictions. Machine Learning, 37(3):297–336, De-
cember.
R. Sproat and M. Riley. 1996. Compilation of
weighted finite-state transducers from decision
trees. In Proceedings of ACL, pages 215–222.
J. Vilar, V.M. Jim´enez, J. Amengual, A. Castellanos,
D. Llorens, and E. Vidal. 1999. Text and speech
translation by means of subsequential transduc-
ers. In Andr´as Kornai, editor, Extened Finite
State Models of Language. Cambridge University
Press.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.972108">
<title confidence="0.999375">Compiling Boostexter Rules into a Finite-state Transducer</title>
<author confidence="0.980069">Srinivas Bangalore</author>
<affiliation confidence="0.999761">AT&amp;T Labs–Research</affiliation>
<address confidence="0.999017">180 Park Avenue Florham Park, NJ 07932</address>
<abstract confidence="0.9995926">A number of NLP tasks have been effectively modeled as classification tasks using a variety of classification techniques. Most of these tasks have been pursued in isolation with the classifier assuming unambiguous input. In order for these techniques to be more broadly applicable, they need to be extended to apply on weighted packed representations of ambiguous input. One approach for achieving this is to represent the classification model as a weighted finite-state transducer (WFST). In this paper, we present a compilation procedure to convert the rules resulting from an AdaBoost classifier into an WFST. We validate the compilation technique by applying the resulting WFST on a call-routing application.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Parsing by chunks.</title>
<date>1991</date>
<editor>In Robert Berwick, Steven Abney, and Carol Tenny, editors, Principle-based parsing.</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="2724" citStr="Abney, 1991" startWordPosition="415" endWordPosition="416">iven the context of the sentence. named-entities. Thus classification approaches need to be extended to be applicable on weighted packed representations of ambiguous input represented as a weighted lattice. The research direction we adopt here is to compile the model of a classifier into a weighted finite-state transducer (WFST) so that it can compose with the input lattice. Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Bangalore and Joshi, 1999), parsing (Roche, 1999; Oflazer, 1999) and machine translation (Vilar et al., 1999; Bangalore and Riccardi, 2000). Finitestate models are attractive mechanisms for language processing since they (a) provide an efficient data structure for representing weighted ambiguous hypotheses (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of speech and language processing.2 In this paper, we describe the compilation process for a particular classifier model </context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>S. Abney. 1991. Parsing by chunks. In Robert Berwick, Steven Abney, and Carol Tenny, editors, Principle-based parsing. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>A K Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="2752" citStr="Bangalore and Joshi, 1999" startWordPosition="417" endWordPosition="420">ext of the sentence. named-entities. Thus classification approaches need to be extended to be applicable on weighted packed representations of ambiguous input represented as a weighted lattice. The research direction we adopt here is to compile the model of a classifier into a weighted finite-state transducer (WFST) so that it can compose with the input lattice. Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Bangalore and Joshi, 1999), parsing (Roche, 1999; Oflazer, 1999) and machine translation (Vilar et al., 1999; Bangalore and Riccardi, 2000). Finitestate models are attractive mechanisms for language processing since they (a) provide an efficient data structure for representing weighted ambiguous hypotheses (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of speech and language processing.2 In this paper, we describe the compilation process for a particular classifier model into an WFST and validate th</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>S. Bangalore and A. K. Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>G Riccardi</author>
</authors>
<title>Stochastic finite-state models for spoken language machine translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the Workshop on Embedded Machine Translation Systems.</booktitle>
<contexts>
<context position="2865" citStr="Bangalore and Riccardi, 2000" startWordPosition="433" endWordPosition="436">ighted packed representations of ambiguous input represented as a weighted lattice. The research direction we adopt here is to compile the model of a classifier into a weighted finite-state transducer (WFST) so that it can compose with the input lattice. Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Bangalore and Joshi, 1999), parsing (Roche, 1999; Oflazer, 1999) and machine translation (Vilar et al., 1999; Bangalore and Riccardi, 2000). Finitestate models are attractive mechanisms for language processing since they (a) provide an efficient data structure for representing weighted ambiguous hypotheses (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of speech and language processing.2 In this paper, we describe the compilation process for a particular classifier model into an WFST and validate the accuracy of the compilation process on a one-best input in a call-routing task. We view this as a first step to</context>
</contexts>
<marker>Bangalore, Riccardi, 2000</marker>
<rawString>S. Bangalore and G. Riccardi. 2000. Stochastic finite-state models for spoken language machine translation. In Proceedings of the Workshop on Embedded Machine Translation Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
<author>J H Friedman</author>
<author>R A Olshen</author>
<author>C J Stone</author>
</authors>
<title>Classification and Regression Trees.</title>
<date>1984</date>
<publisher>Wadsworth &amp; Brooks,</publisher>
<location>Pacific Grove, CA.</location>
<contexts>
<context position="5051" citStr="Breiman et al., 1984" startWordPosition="800" endWordPosition="803">cognition, these tagging problems have been previously modeled indirectly through the transformation of the Bayes rule as in Equation 2. The problem is then approximated for sequence classification by a k -order Markov model as shown in Equation (3). Although the HMM approach to tagging can easily be represented as a WFST, it has a drawback in that the use of large contexts and richer features results in sparseness leading to unreliable estimation of the parameters of the model. An alternate approach to arriving at is to model Equation 1 directly. There are many examples in recent literature (Breiman et al., 1984; Freund and Schapire, 1996; Roth, 1998; Lafferty et al., 2001; McCallum et al., 2000) which take this approach and are well equipped to handle large number of features. The general framework for these approaches is to learn a model from pairs of associations of the form ( ) where is a feature representation of and ( ) is one of the members of the tag set. Although these approaches have been more effective than HMMs, there have not been many attempts to represent these models as a WFST, with the exception of the work on compiling decision trees (Sproat and Riley, 1996). In this paper, we consi</context>
</contexts>
<marker>Breiman, Friedman, Olshen, Stone, 1984</marker>
<rawString>L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone. 1984. Classification and Regression Trees. Wadsworth &amp; Brooks, Pacific Grove, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Experiments with a new boosting alogrithm.</title>
<date>1996</date>
<booktitle>In Machine Learning: Proceedings of the Thirteenth International Conference,</booktitle>
<pages>148--156</pages>
<contexts>
<context position="5078" citStr="Freund and Schapire, 1996" startWordPosition="804" endWordPosition="808">ng problems have been previously modeled indirectly through the transformation of the Bayes rule as in Equation 2. The problem is then approximated for sequence classification by a k -order Markov model as shown in Equation (3). Although the HMM approach to tagging can easily be represented as a WFST, it has a drawback in that the use of large contexts and richer features results in sparseness leading to unreliable estimation of the parameters of the model. An alternate approach to arriving at is to model Equation 1 directly. There are many examples in recent literature (Breiman et al., 1984; Freund and Schapire, 1996; Roth, 1998; Lafferty et al., 2001; McCallum et al., 2000) which take this approach and are well equipped to handle large number of features. The general framework for these approaches is to learn a model from pairs of associations of the form ( ) where is a feature representation of and ( ) is one of the members of the tag set. Although these approaches have been more effective than HMMs, there have not been many attempts to represent these models as a WFST, with the exception of the work on compiling decision trees (Sproat and Riley, 1996). In this paper, we consider the boosting (Freund an</context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Y. Freund and R. E. Schapire. 1996. Experiments with a new boosting alogrithm. In Machine Learning: Proceedings of the Thirteenth International Conference, pages 148–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Johnson</author>
</authors>
<title>Formal Aspects of Phonological Description.</title>
<date>1972</date>
<publisher>Mouton, The Hague.</publisher>
<contexts>
<context position="9315" citStr="Johnson, 1972" startWordPosition="1555" endWordPosition="1556">mined as in Equation 5, where is computed using Equation 4. (5) To date, decoding using the boosted rule sets is restricted to cases where the test input is unambiguous such as strings or words (not word graphs). By compiling these rule sets into WFSTs, we intend to extend their applicability to packed representations of ambiguous input such as word graphs. 4 Compilation We note that the weak learners selected at the end of the training process can be partitioned into one of three types based on the features that the learners test. We use the representation of context-dependent rewrite rules (Johnson, 1972; Kaplan and Kay, 1994) and their weighted version (Mohri and Sproat, 1996) to represent these weak learners. The (weighted) context-dependent rewrite rules have the general form (6) where , , and are regular expressions on the alphabet of the rules. The interpretation of these rules are as follows: Rewrite by when it is preceded by and followed by. Furthermore, can be extended to a rational power series which are weighted regular expressions where the weights encode preferences over the paths in (Mohri and Sproat, 1996). Each weak learner can then be viewed as a set of weighted rewrite rules </context>
</contexts>
<marker>Johnson, 1972</marker>
<rawString>C.D. Johnson. 1972. Formal Aspects of Phonological Description. Mouton, The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
<author>M Kay</author>
</authors>
<title>Regular models of phonological rule systems.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="2669" citStr="Kaplan and Kay, 1994" startWordPosition="406" endWordPosition="409">h a label that represents the syntactic information of the word given the context of the sentence. named-entities. Thus classification approaches need to be extended to be applicable on weighted packed representations of ambiguous input represented as a weighted lattice. The research direction we adopt here is to compile the model of a classifier into a weighted finite-state transducer (WFST) so that it can compose with the input lattice. Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Bangalore and Joshi, 1999), parsing (Roche, 1999; Oflazer, 1999) and machine translation (Vilar et al., 1999; Bangalore and Riccardi, 2000). Finitestate models are attractive mechanisms for language processing since they (a) provide an efficient data structure for representing weighted ambiguous hypotheses (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of speech and language processing.2 In this paper, we describe the</context>
<context position="9338" citStr="Kaplan and Kay, 1994" startWordPosition="1557" endWordPosition="1560">ation 5, where is computed using Equation 4. (5) To date, decoding using the boosted rule sets is restricted to cases where the test input is unambiguous such as strings or words (not word graphs). By compiling these rule sets into WFSTs, we intend to extend their applicability to packed representations of ambiguous input such as word graphs. 4 Compilation We note that the weak learners selected at the end of the training process can be partitioned into one of three types based on the features that the learners test. We use the representation of context-dependent rewrite rules (Johnson, 1972; Kaplan and Kay, 1994) and their weighted version (Mohri and Sproat, 1996) to represent these weak learners. The (weighted) context-dependent rewrite rules have the general form (6) where , , and are regular expressions on the alphabet of the rules. The interpretation of these rules are as follows: Rewrite by when it is preceded by and followed by. Furthermore, can be extended to a rational power series which are weighted regular expressions where the weights encode preferences over the paths in (Mohri and Sproat, 1996). Each weak learner can then be viewed as a set of weighted rewrite rules mapping the input word </context>
</contexts>
<marker>Kaplan, Kay, 1994</marker>
<rawString>R. M. Kaplan and M. Kay. 1994. Regular models of phonological rule systems. Computational Linguistics, 20(3):331–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K K Koskenniemi</author>
</authors>
<title>Two-level morphology: a general computation model for word-form recognition and production.</title>
<date>1984</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Helsinki.</institution>
<contexts>
<context position="2701" citStr="Koskenniemi, 1984" startWordPosition="412" endWordPosition="413">ctic information of the word given the context of the sentence. named-entities. Thus classification approaches need to be extended to be applicable on weighted packed representations of ambiguous input represented as a weighted lattice. The research direction we adopt here is to compile the model of a classifier into a weighted finite-state transducer (WFST) so that it can compose with the input lattice. Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Bangalore and Joshi, 1999), parsing (Roche, 1999; Oflazer, 1999) and machine translation (Vilar et al., 1999; Bangalore and Riccardi, 2000). Finitestate models are attractive mechanisms for language processing since they (a) provide an efficient data structure for representing weighted ambiguous hypotheses (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of speech and language processing.2 In this paper, we describe the compilation process for a parti</context>
</contexts>
<marker>Koskenniemi, 1984</marker>
<rawString>K. K. Koskenniemi. 1984. Two-level morphology: a general computation model for word-form recognition and production. Ph.D. thesis, University of Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>In Proceedings of ICML,</booktitle>
<location>San Francisco, CA.</location>
<contexts>
<context position="5113" citStr="Lafferty et al., 2001" startWordPosition="811" endWordPosition="814">d indirectly through the transformation of the Bayes rule as in Equation 2. The problem is then approximated for sequence classification by a k -order Markov model as shown in Equation (3). Although the HMM approach to tagging can easily be represented as a WFST, it has a drawback in that the use of large contexts and richer features results in sparseness leading to unreliable estimation of the parameters of the model. An alternate approach to arriving at is to model Equation 1 directly. There are many examples in recent literature (Breiman et al., 1984; Freund and Schapire, 1996; Roth, 1998; Lafferty et al., 2001; McCallum et al., 2000) which take this approach and are well equipped to handle large number of features. The general framework for these approaches is to learn a model from pairs of associations of the form ( ) where is a feature representation of and ( ) is one of the members of the tag set. Although these approaches have been more effective than HMMs, there have not been many attempts to represent these models as a WFST, with the exception of the work on compiling decision trees (Sproat and Riley, 1996). In this paper, we consider the boosting (Freund and Schapire, 1996) approach (which o</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In In Proceedings of ICML, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>D Freitag</author>
<author>F Pereira</author>
</authors>
<title>Maximum entropy markov models for information extraction and segmentation. In</title>
<date>2000</date>
<booktitle>In Proceedings of ICML,</booktitle>
<location>Stanford, CA.</location>
<contexts>
<context position="5137" citStr="McCallum et al., 2000" startWordPosition="815" endWordPosition="818">e transformation of the Bayes rule as in Equation 2. The problem is then approximated for sequence classification by a k -order Markov model as shown in Equation (3). Although the HMM approach to tagging can easily be represented as a WFST, it has a drawback in that the use of large contexts and richer features results in sparseness leading to unreliable estimation of the parameters of the model. An alternate approach to arriving at is to model Equation 1 directly. There are many examples in recent literature (Breiman et al., 1984; Freund and Schapire, 1996; Roth, 1998; Lafferty et al., 2001; McCallum et al., 2000) which take this approach and are well equipped to handle large number of features. The general framework for these approaches is to learn a model from pairs of associations of the form ( ) where is a feature representation of and ( ) is one of the members of the tag set. Although these approaches have been more effective than HMMs, there have not been many attempts to represent these models as a WFST, with the exception of the work on compiling decision trees (Sproat and Riley, 1996). In this paper, we consider the boosting (Freund and Schapire, 1996) approach (which outperforms decision tree</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum entropy markov models for information extraction and segmentation. In In Proceedings of ICML, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohri</author>
<author>R Sproat</author>
</authors>
<title>An efficient compiler for weighted rewrite rules.</title>
<date>1996</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>231--238</pages>
<contexts>
<context position="9390" citStr="Mohri and Sproat, 1996" startWordPosition="1565" endWordPosition="1568">o date, decoding using the boosted rule sets is restricted to cases where the test input is unambiguous such as strings or words (not word graphs). By compiling these rule sets into WFSTs, we intend to extend their applicability to packed representations of ambiguous input such as word graphs. 4 Compilation We note that the weak learners selected at the end of the training process can be partitioned into one of three types based on the features that the learners test. We use the representation of context-dependent rewrite rules (Johnson, 1972; Kaplan and Kay, 1994) and their weighted version (Mohri and Sproat, 1996) to represent these weak learners. The (weighted) context-dependent rewrite rules have the general form (6) where , , and are regular expressions on the alphabet of the rules. The interpretation of these rules are as follows: Rewrite by when it is preceded by and followed by. Furthermore, can be extended to a rational power series which are weighted regular expressions where the weights encode preferences over the paths in (Mohri and Sproat, 1996). Each weak learner can then be viewed as a set of weighted rewrite rules mapping the input word into each member ( ) with a weight when the predicat</context>
</contexts>
<marker>Mohri, Sproat, 1996</marker>
<rawString>M. Mohri and R. Sproat. 1996. An efficient compiler for weighted rewrite rules. In Proceedings ofACL, pages 231–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Oflazer</author>
</authors>
<title>Dependency parsing with an extended finite state approach.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Maryland, USA,</location>
<contexts>
<context position="2790" citStr="Oflazer, 1999" startWordPosition="424" endWordPosition="425">cation approaches need to be extended to be applicable on weighted packed representations of ambiguous input represented as a weighted lattice. The research direction we adopt here is to compile the model of a classifier into a weighted finite-state transducer (WFST) so that it can compose with the input lattice. Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Bangalore and Joshi, 1999), parsing (Roche, 1999; Oflazer, 1999) and machine translation (Vilar et al., 1999; Bangalore and Riccardi, 2000). Finitestate models are attractive mechanisms for language processing since they (a) provide an efficient data structure for representing weighted ambiguous hypotheses (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of speech and language processing.2 In this paper, we describe the compilation process for a particular classifier model into an WFST and validate the accuracy of the compilation process </context>
</contexts>
<marker>Oflazer, 1999</marker>
<rawString>K. Oflazer. 1999. Dependency parsing with an extended finite state approach. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, Maryland, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F C N Pereira</author>
<author>M D Riley</author>
</authors>
<title>Speech recognition by composition of weighted finite automata.</title>
<date>1997</date>
<booktitle>Finite State Devices for Natural Language Processing,</booktitle>
<pages>431--456</pages>
<editor>In E. Roche and Schabes Y., editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="2635" citStr="Pereira and Riley, 1997" startWordPosition="401" endWordPosition="404">speech and 1associating each word with a label that represents the syntactic information of the word given the context of the sentence. named-entities. Thus classification approaches need to be extended to be applicable on weighted packed representations of ambiguous input represented as a weighted lattice. The research direction we adopt here is to compile the model of a classifier into a weighted finite-state transducer (WFST) so that it can compose with the input lattice. Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Bangalore and Joshi, 1999), parsing (Roche, 1999; Oflazer, 1999) and machine translation (Vilar et al., 1999; Bangalore and Riccardi, 2000). Finitestate models are attractive mechanisms for language processing since they (a) provide an efficient data structure for representing weighted ambiguous hypotheses (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of speech and language processin</context>
</contexts>
<marker>Pereira, Riley, 1997</marker>
<rawString>F.C.N. Pereira and M.D. Riley. 1997. Speech recognition by composition of weighted finite automata. In E. Roche and Schabes Y., editors, Finite State Devices for Natural Language Processing, pages 431–456. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Roche</author>
</authors>
<title>Finite state transducers: parsing free and frozen sentences.</title>
<date>1999</date>
<editor>In Andr´as Kornai, editor, Extended</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2774" citStr="Roche, 1999" startWordPosition="422" endWordPosition="423">Thus classification approaches need to be extended to be applicable on weighted packed representations of ambiguous input represented as a weighted lattice. The research direction we adopt here is to compile the model of a classifier into a weighted finite-state transducer (WFST) so that it can compose with the input lattice. Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Bangalore and Joshi, 1999), parsing (Roche, 1999; Oflazer, 1999) and machine translation (Vilar et al., 1999; Bangalore and Riccardi, 2000). Finitestate models are attractive mechanisms for language processing since they (a) provide an efficient data structure for representing weighted ambiguous hypotheses (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of speech and language processing.2 In this paper, we describe the compilation process for a particular classifier model into an WFST and validate the accuracy of the comp</context>
</contexts>
<marker>Roche, 1999</marker>
<rawString>E. Roche. 1999. Finite state transducers: parsing free and frozen sentences. In Andr´as Kornai, editor, Extended Finite State Models of Language. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
</authors>
<title>Learning to resolve natural language ambiguities: A unified approach.</title>
<date>1998</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="5090" citStr="Roth, 1998" startWordPosition="809" endWordPosition="810">ously modeled indirectly through the transformation of the Bayes rule as in Equation 2. The problem is then approximated for sequence classification by a k -order Markov model as shown in Equation (3). Although the HMM approach to tagging can easily be represented as a WFST, it has a drawback in that the use of large contexts and richer features results in sparseness leading to unreliable estimation of the parameters of the model. An alternate approach to arriving at is to model Equation 1 directly. There are many examples in recent literature (Breiman et al., 1984; Freund and Schapire, 1996; Roth, 1998; Lafferty et al., 2001; McCallum et al., 2000) which take this approach and are well equipped to handle large number of features. The general framework for these approaches is to learn a model from pairs of associations of the form ( ) where is a feature representation of and ( ) is one of the members of the tag set. Although these approaches have been more effective than HMMs, there have not been many attempts to represent these models as a WFST, with the exception of the work on compiling decision trees (Sproat and Riley, 1996). In this paper, we consider the boosting (Freund and Schapire, </context>
</contexts>
<marker>Roth, 1998</marker>
<rawString>D. Roth. 1998. Learning to resolve natural language ambiguities: A unified approach. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>Improved boosting algorithms using confidence-rated predictions.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="7030" citStr="Schapire and Singer, 1999" startWordPosition="1154" endWordPosition="1157">rom the pool of weak learners, the selection the weak learner to be combined is performed iteratively. At each iteration, a weak learner is selected that minimizes a prediction error loss function on the training corpus which takes into account the weight assigned to each training example. Intuitively, the weights encode how important it is that correctly classifies each training example. Generally, the examples that were most often misclassified by the preceding base classifiers will be given the most weight so as to force the base learner to focus on the “hardest” examples. As described in (Schapire and Singer, 1999), Boostexter uses confidence rated classifiers that output a real number whose sign (-1 or +1) is interpreted as a prediction, and whose magnitude is a measure of “confidence”. The iterative algorithm for combining weak learners stops after a prespecified number of iterations or when the training set accuracy saturates. 3.1 Weak Learners In the case of text classification applications, the set of possible weak learners is instantiated from simple -grams of the input text ( ). Thus, if is a function to produce all -grams up to of its argument, then the set of predicates for the weak learners is</context>
</contexts>
<marker>Schapire, Singer, 1999</marker>
<rawString>R.E. Schapire and Y. Singer. 1999. Improved boosting algorithms using confidence-rated predictions. Machine Learning, 37(3):297–336, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sproat</author>
<author>M Riley</author>
</authors>
<title>Compilation of weighted finite-state transducers from decision trees.</title>
<date>1996</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>215--222</pages>
<contexts>
<context position="5626" citStr="Sproat and Riley, 1996" startWordPosition="907" endWordPosition="910">mples in recent literature (Breiman et al., 1984; Freund and Schapire, 1996; Roth, 1998; Lafferty et al., 2001; McCallum et al., 2000) which take this approach and are well equipped to handle large number of features. The general framework for these approaches is to learn a model from pairs of associations of the form ( ) where is a feature representation of and ( ) is one of the members of the tag set. Although these approaches have been more effective than HMMs, there have not been many attempts to represent these models as a WFST, with the exception of the work on compiling decision trees (Sproat and Riley, 1996). In this paper, we consider the boosting (Freund and Schapire, 1996) approach (which outperforms decision trees) to Equation 1 and present a technique for compiling the classifier model into a WFST. 3 Boostexter Boostexter is a machine learning tool which is based on the boosting family of algorithms first proposed in (Freund and Schapire, 1996). The basic idea of boosting is to build a highly accurate classifier by combining many “weak” or “simple” base learner, each one of which may only be moderately accurate. A weak learner or a rule is a triple , which tests a predicate (❇) of the input </context>
<context position="11250" citStr="Sproat and Riley, 1996" startWordPosition="1889" endWordPosition="1892">identify locations for rewrites of with . After the rewrites, the markers are deleted. The compilation process is represented as a composition of five transducers. The WFSTs resulting from the compilation of each selected weak learner ( ) are unioned to create the WFST to be used for decoding. The weights of paths with the same input and output labels are added during the union operation. (7) We note that the due to the difference in the nature of the learning algorithm, compiling decision trees results in a composition of WFSTs representing the rules on the path from the root to a leaf node (Sproat and Riley, 1996), while compiling boosted rules results in a union of WFSTs, which is expected to result in smaller transducers. In order to apply the WFST for decoding, we simply compose the model with the input represented as an WFST ( ) and search for the best path (if we are interested in the single best classification result). (8) We have compiled the rules resulting from boostexter trained on transcriptions of speech utterances from a call routing task with a vocabulary ( ) of 2912 and 40 classes ( ). There were a total of 1800 rules comprising of 900 positive rules and their negative counterparts. The </context>
</contexts>
<marker>Sproat, Riley, 1996</marker>
<rawString>R. Sproat and M. Riley. 1996. Compilation of weighted finite-state transducers from decision trees. In Proceedings of ACL, pages 215–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Vilar</author>
<author>V M Jim´enez</author>
<author>J Amengual</author>
<author>A Castellanos</author>
<author>D Llorens</author>
<author>E Vidal</author>
</authors>
<title>Text and speech translation by means of subsequential transducers.</title>
<date>1999</date>
<editor>In Andr´as Kornai, editor, Extened</editor>
<publisher>Cambridge University Press.</publisher>
<marker>Vilar, Jim´enez, Amengual, Castellanos, Llorens, Vidal, 1999</marker>
<rawString>J. Vilar, V.M. Jim´enez, J. Amengual, A. Castellanos, D. Llorens, and E. Vidal. 1999. Text and speech translation by means of subsequential transducers. In Andr´as Kornai, editor, Extened Finite State Models of Language. Cambridge University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>