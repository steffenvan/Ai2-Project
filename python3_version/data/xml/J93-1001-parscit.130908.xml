<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998963333333333">
Introduction to the Special Issue
on Computational Linguistics
Using Large Corpora
</title>
<author confidence="0.972386">
Kenneth W. Church* Robert L. Mercert
</author>
<affiliation confidence="0.509601">
AT&amp;T Bell Laboratories IBM T.J. Watson Research Center
</affiliation>
<bodyText confidence="0.821752157894737">
The 1990s have witnessed a resurgence of interest in 1950s-style empirical and statisti-
cal methods of language analysis. Empiricism was at its peak in the 1950s, dominating
a broad set of fields ranging from psychology (behaviorism) to electrical engineering
(information theory). At that time, it was common practice in linguistics to classify
words not only on the basis of their meanings but also on the basis of their co-
occurrence with other words. Firth, a leading figure in British linguistics during the
1950s, summarized the approach with the memorable line: &amp;quot;You shall know a word
by the company it keeps&amp;quot; (Firth 1957). Regrettably, interest in empiricism faded in the
late 1950s and early 1960s with a number of significant events including Chomsky&apos;s
criticism of n-grams in Syntactic Structures (Chomsky 1957) and Minsky and Papert&apos;s
criticism of neural networks in Perceptrons (Minsky and Papert 1969).
Perhaps the most immediate reason for this empirical renaissance is the availabil-
ity of massive quantities of data: more text is available than ever before. Just ten years
ago, the one-million word Brown Corpus (Francis and Ku&apos;&amp;ra, 1982) was considered
large, but even then, there were much larger corpora such as the Birmingham Cor-
pus (Sinclair et al. 1987; Sinclair 1987). Today, many locations have samples of text
running into the hundreds of millions or even billions of words. Collections of this
magnitude are becoming widely available, thanks to data collection efforts such as the
Association for Computational Linguistics&apos; Data Collection Initiative (ACL/DCI), the
European Corpus Initiative (ECI), ICAME, the British National Corpus (BNC), the Lin-
guistic Data Consortium (LDC), the Consortium for Lexical Research (CLR), Electronic
Dictionary Research (EDR), and standardization efforts such as the Text Encoding Ini-
tiative (TED.&apos; The data-intensive approach to language, which is becoming known as
Text Analysis, takes a pragmatic approach that is well suited to meet the recent empha-
sis on numerical evaluations and concrete deliverables. Text Analysis focuses on broad
(though possibly superficial) coverage of unrestricted text, rather than deep analysis
of (artificially) restricted domains.
* AT&amp;T Bell Laboratories, Office 2B-421, 600 Mountain Ave., Murray Hill, NJ 07974.
IBM T.J. Watson Research Center, P.O. Box 704, J2-H24, Yorktown Heights, NY 10598.
1 For more information on the ACL/DCI, contact Felicia Hurewitz, ACL/DCI, Room 619, Williams Hall,
University of Pennsylvania, Philadelphia, PA 19104-6305, USA, (tel) 215-898-0083, (fax) 215-573-2091,
(e-mail) fel@unagi.cis.upenn.edu. For more information on the LDC, contact Elizabeth Hodas,
Linguistic Data Consortium, Room 441, Williams Hall, University of Pennsylvania, Philadelphia, PA,
19104-6305, USA, (tel) 215-898-0464, (fax) 215-573-2175, (e-mail) ehodas@unagi.cis.upenn.edu. Send
e-mail to smbowie@vax.oxford.ac.uk for information on the BNC, to lexical@nrnsu.edu for information
on the CLR, and to eucorp@cogsci.edinburgh.ac.uk for information on the ECI. Information on the
London-Lund Corpus and other corpora available through ICAME can be found in the ICAME
Journal, edited by Stig Johansson, Department of English, University of Oslo, Norway.
</bodyText>
<note confidence="0.625859">
© 1993 Association for Computational Linguistics
Computational Linguistics Volume 19, Number 1
</note>
<bodyText confidence="0.999359875">
The case for the resurgence of empiricism in computational linguistics is nicely
summarized in Susan Warwick-Armstrong&apos;s call-for-papers for this special issue:
The increasing availability of machine-readable corpora has suggested
new methods for studies in a variety of areas such as lexical knowl-
edge acquisition, grammar construction, and machine translation.
Though common in the speech community, the use of statistical and
probabilistic methods to discover and organize data is relatively new
to the field at large. The various initiatives currently under way to lo-
cate and collect machine-readable corpora have recognized the poten-
tial of using this data and are working toward making these materials
available to the research community. Given the growing interest in
corpus studies, it seems timely to devote an issue of CL to this topic.
In Section 1, we review the experience of the speech recognition community. Stochastic
methods based on Shannon&apos;s noisy channel model have become the methods of choice
within the speech community. Knowledge-based approaches were tried during the first
DARPA speech recognition project in the early 1970s, but have largely been abandoned
in favor of stochastic approaches that have become the main focus of DARPA&apos;s more
recent efforts.
In Section 2, we discuss how this experience is influencing the language commu-
nity. Many of the most successful speech techniques are achieving major improvements
in performance in the language area. In particular, probabilistic taggers based on Shan-
non&apos;s noisy channel model are becoming the method of choice because they correctly
tag 95% of the words in a new text, a major improvement over earlier technologies that
ignored lexical probabilities and other preferences that can be estimated statistically
from corpus evidence.
In Section 3, we discuss a number of frequency-based preferences such as collo-
cations and word associations. Although often ignored in the computational linguistics
literature because they are difficult to capture with traditional parsing technology,
they can easily overwhelm syntactic factors (as any psycholinguist knows). Four arti-
cles in this special issue take a first step toward preference-based parsing, an empirical
alternative to the rational tradition of principle-based parsing, ATNs, unification, etc.
In Section 4, we discuss entropy and evaluation issues, which have become rela-
tively important in recent years.
In Section 5, we discuss the application of noisy channel models to bilingual ap-
plications such as machine translation and bilingual lexicography.
In Section 6, we discuss the use of empirical methods in monolingual lexicogra-
phy, contrasting the exploratory data analysis (EDA) view of statistics with other per-
spectives such as hypothesis testing and supervised/unsupervised learning/training.
There are five articles in this special issue on computational lexicography, using both
the exploratory and the self-organizing approaches to statistics.
</bodyText>
<sectionHeader confidence="0.563754" genericHeader="method">
1. The Influence from the Speech Community
</sectionHeader>
<subsectionHeader confidence="0.745376">
1.1 Consensus in Speech Community: Stochastic Methods Are Outperforming
Knowledge-Based Methods
</subsectionHeader>
<bodyText confidence="0.9094015">
Over the past 20 years, the speech community has reached a consensus in favor of em-
pirical methods. As observed by Waibel and Lee in the introduction to their collection
</bodyText>
<page confidence="0.994366">
2
</page>
<note confidence="0.979811">
Kenneth W. Church and Robert L. Mercer Introduction
</note>
<bodyText confidence="0.994018307692308">
of reprints on speech recognition (Waibel and Lee 1990):
Chapter 5 describes the knowledge-based approach, proposed in the 1970s
and early 1980s. The pure knowledge-based approach emulates hu-
man speech knowledge using expert systems. Rule-based systems
have had only limited success... Chapter 6 describes the stochastic
approach. . . Most successful large-scale systems today use a stochastic
approach. (Waibel and Lee 1990; P. 4)
A number of data collection efforts have helped to bring about this change in the
speech community, especially the Texas Instruments&apos; Digit Corpus (Leonard 1984),
TIMIT and the DARPA Resource Management (RM) Database (Price et al. 1988). Ac-
cording to the Linguistic Data Consortium (LDC), the RM database was used by every
paper that reported speech recognition results in the 1988 Proceedings of IEEE ICASSP,
the major technical society meeting where speech recognition results are reported.
This is especially significant given that abstracts for this meeting were due just a few
months after the release of the corpus, attesting to the speech recognition community&apos;s
hunger for standard corpora for development and evaluation.
Back in the 1970s, the more data-intensive methods were probably beyond the
means of many researchers, especially those working in universities. Perhaps some
of these researchers turned to the knowledge-based approach because they couldn&apos;t
afford the alternative. It is an interesting fact that most of the authors of the knowledge-
based papers in Chapter 5 of Waibel and Lee (1990) have a university affiliation
whereas most of the authors of the data-intensive papers in Chapter 6 have an in-
dustrial affiliation. Fortunately, as a result of improvements in computer technology
and the increasing availability of data due to numerous data collection efforts, the
data-intensive methods are no longer restricted to those working in affluent industrial
laboratories.
</bodyText>
<subsectionHeader confidence="0.990014">
1.2 The Anti-Empiricist Period in Speech Research
</subsectionHeader>
<bodyText confidence="0.99986255">
At the time, of course, the knowledge-based approach was not advocated on economic
grounds. Rather, the knowledge-based approach was advocated as necessary in order
to deal with the lack of allophonic invariance. The mapping between phonemes and
their allophonic realizations is highly variable and ambiguous. The phoneme /t/, for
example, may be realized as a released stop in &amp;quot;Tom,&amp;quot; as a flap in &amp;quot;butter,&amp;quot; or as
a glottal stop in &amp;quot;bottle.&amp;quot; Two different phonemes may lead to the same allophonic
variant in some contexts. For example, &amp;quot;writer&amp;quot; and &amp;quot;rider&amp;quot; are nearly identical in
many dialects of American English. Residual differences, such as the length of the
preconsonantal vowel, are easily overwhelmed by the context in which the word ap-
pears. Thus, if one says &amp;quot;Joe is a rider of novels,&amp;quot; listeners hear &amp;quot;Joe is a writer of
novels,&amp;quot; while if one says &amp;quot;Joe is a writer of horses,&amp;quot; listeners hear &amp;quot;Joe is a rider of
horses.&amp;quot; Listeners usually have little problem with the wild variability and ambiguity
of speech because they know what the speaker is likely to say.
In most systems for sentence recognition, such modifications must be
viewed as a kind of &apos;noise&apos; that makes it more difficult to hypothesize
lexical candidates given an input phonetic transcription. To see that
this must be the case, we note that each phonological rule fin the
utterance: &amp;quot;Did you hit it to Tom?&amp;quot;] results in irreversible ambiguity—
the phonological rule does not have a unique inverse that could be
used to recover the underlying phonemic representation for a lexical
</bodyText>
<page confidence="0.997491">
3
</page>
<note confidence="0.845712">
Computational Linguistics Volume 19, Number 1
</note>
<bodyText confidence="0.996776055555556">
item. For example, ... Mlle tongue flap ... could have come from a
/t/ or a /d/. (Klatt 1980; pp. 548-549)
The first DARPA Speech Understanding project emphasized the use of high-level con-
straints (e.g., syntax, semantics, and pragmatics) as a tool to disambiguate the al-
lophonic information in the speech signal by understanding the message. At BBN,
researchers called their system HWIM for (Hear What I Mean). They hoped to use NLP
techniques such as ATNs (Woods 1970) to understand the sentences that they were
trying to recognize even though the output of their front end was highly variable and
ambiguous.
The emphasis today on empirical methods in the speech recognition community
is a reaction to the failure of knowledge-based approaches of the 1970s. It has become
popular once again to focus on high-level natural language constraints in order to
reduce the search space. But this time, n-gram methods have become the methods of
choice because they seem to work better than the alternatives, at least when the search
space is measured in terms of entropy. Ideally, we might hope that someday parsers
might reduce entropy beyond that of n-grams, but right now, parsers seem to be more
useful for other tasks such as understanding who did what to whom, and less useful
for predicting what the speaker is likely to say.
</bodyText>
<subsectionHeader confidence="0.991919">
1.3 The Raleigh System: A Foundation for the Revival of Empiricism
</subsectionHeader>
<bodyText confidence="0.9999169">
In the midst of all of this excitement over high-level, knowledged-based NLP tech-
niques, IBM formed a new speech group around the nucleus of an existing group
that was moved from Raleigh, North Carolina, to Yorktown Heights early in 1972.
The Raleigh group brought to Yorktown a working speech recognition system, that
had been designed in accordance with prevailing anti-empiricist attitudes of the time,
though it would soon serve as a foundation for the revival of empiricism in the speech
and language communities.
The front end of the Raleigh system converted the speech signal (20,000 samples
per second) first into a sequence of 80 filter bank outputs (100 80-dimensional vectors
per second), and then into a sequence of phoneme-like labels (100 labels per second),
using an elaborate set of hand-tuned rules that would soon be replaced with an auto-
matically trained procedure. The back end converted these labels into a sequence of
words using an artificial finite-state grammar that was so small that the finite-state ma-
chine could be written down on a single piece of paper. Today, many systems attempt
to model unrestricted language using methods that will be discussed in Section 3, but
at the time, it was standard practice to work with artificial grammars of this kind.
When it worked perfectly, the front end produced a transcription of the speech
signal such as might be produced by a human phonetician listening carefully to the
original speech. Unfortunately, it almost never worked perfectly, even on so small a
stretch as a single word. Rapid phones, such as flaps, were often missed; long phones,
such as liquids and stressed vowels, were often broken into several separate seg-
ments; and very often phones were simply mislabeled. The back end was designed to
overcome these problems by navigating through the finite-state network, applying a
complicated set of hand-tuned penalties and bonuses to the various paths in order to
favor those paths where the low-level acoustics matched the high-level grammatical
constraints. This system of hand-tuned penalties and bonuses correctly recognized 35%
of the sentences (and 77% of the words) in the test set. At the time, this level of perfor-
mance was actually quite impressive, but these days, one would expect much more,
now that most systems use parameters trained on real data, rather than a complicated
set of hand-tuned penalties and bonuses.
</bodyText>
<page confidence="0.99826">
4
</page>
<note confidence="0.919668">
Kenneth W. Church and Robert L. Mercer Introduction
</note>
<subsectionHeader confidence="0.978017">
1.4 The Noisy Channel Model
</subsectionHeader>
<bodyText confidence="0.9998866875">
Although the penalties and bonuses were sometimes thought of as probabilities, the
early Raleigh system lacked a complete and unified probabilistic framework. In a rad-
ical departure from the prevailing attitudes of the time, the Yorktown group turned
to Shannon&apos;s theory of communication in the presence of noise and recast the speech
recognition problem in terms of transmission through a noisy channel. Shannon&apos;s the-
ory of communication (Shannon 1948), also known as Information Theory, was originally
developed at AT&amp;T Bell Laboratories to model communication along a noisy channel
such as a telephone line. See Fano (1961) for a well-known secondary source on the
subject, or Cover and Thomas (1991) or Bell, Cleary, and Witten (1990) for more recent
treatments.
The noisy channel paradigm can be applied to other recognition applications such
as optical character recognition (OCR) and spelling correction. Imagine a noisy chan-
nel, such as a speech recognition machine that almost hears, an optical character recog-
nition (OCR) machine that almost reads, or a typist who almost types. A sequence of
good text (I) goes into the channel, and a sequence of corrupted text (0) comes out
the other end.
</bodyText>
<equation confidence="0.87418">
I —+ Noisy Channel --* 0
</equation>
<bodyText confidence="0.9804035">
How can an automatic procedure recover the good input text, I, from the corrupted
output, 0? In principle, one can recover the most likely input, I, by hypothesizing all
possible input texts, I, and selecting the input text with the highest score, Pr(I I 0).
Symbolically,
</bodyText>
<equation confidence="0.978962">
I = argmaxPr(/ I 0) = argmaxPr(/) Pr(0 I I)
</equation>
<bodyText confidence="0.999987071428572">
where ARGMAX finds the argument with the maximum score.
The prior probability, Pr(I), is the probability that I will be presented at the input
to the channel. In speech recognition, it is the probability that the talker utters I; in
spelling correction (Damerau 1964; Kukich 1992), it is the probability that the typist
intends to type I. In practice, the prior probability is unavailable, and consequently,
we have to make do with a model of the prior probability, such as the trigram model.
The parameters of the language model are usually estimated by computing various
statistics over a large sample of text.
The channel probability, Pr(0 I I), is the probability that 0 will appear at the
output of the channel when I is presented at the input; it is large if I is similar, in
some appropriate sense, to 0, and small, otherwise. The channel probability depends
on the application. In speech recognition, for example, the output for the word &amp;quot;writer&amp;quot;
may look similar to the word &amp;quot;rider&amp;quot;; in character recognition, this will not be the case.
Other examples are shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.995381">
1.5 Training Is Better than Guessing
</subsectionHeader>
<bodyText confidence="0.999160888888889">
Rather than rely on guesses for the values of the bonuses and penalties as the Raleigh
group had done, the Yorktown group used three levels of hidden Markov models
(HMMs) to compute the conditional probabilities necessary for the noisy channel.
A Markov model is a finite state machine with probabilities governing transitions
between states and controlling the emission of output symbols. If the sequence of state
transitions cannot be determined when the sequence of outputs is known, the Markov
model is said to be hidden. In practice, the Forward—Backward algorithm is often used
to estimate the values of the transition and emission parameters on the basis of corpus
evidence. See Furui (1989; Appendix D.3, pp. 343-347) for a brief description of the
</bodyText>
<page confidence="0.985514">
5
</page>
<table confidence="0.417464">
Computational Linguistics Volume 19, Number 1
</table>
<tableCaption confidence="0.997966">
Table 1
</tableCaption>
<table confidence="0.9800964">
Examples of channel confusions in different applications.
Application Input Output
Speech writer rider
Recognition here hear
Optical all all (A-one-L)
Character of of
Recognition form farm
Spelling government goverment
Correction occurred occured
commercial commerical
</table>
<tableCaption confidence="0.995689">
Table 2
</tableCaption>
<figure confidence="0.96844925">
Performance after training (Bahl et al. 1975)
Training Set Size Test Sentences Correctly Decoded Decoding Problems
0 2/10 8/10
200 77/100 3/100
400 80/100 2/100
600 85/100 1/100
800 82/100 3/100
1070 83/100 3/100
</figure>
<bodyText confidence="0.995851130434783">
Forward—Backward algorithm, and (Rabiner 1989) for a longer tutorial on HMMs. The
general procedure, of which the Forward—Backward algorithm is a special case, was
first published and shown to converge by Baum (1972).
The first level of the Raleigh system converted spelling to phonemic base forms,
rather like a dictionary; the second level dealt with the problems of allophonic variation
mentioned above; the third level modeled the front end. At first, the values of the
parameters in these HMMs were carefully constructed by hand, but eventually they
would all be replaced with estimates obtained by training on real data using statistical
estimation procedures such as the Forward—Backward algorithm.
The advantages of training are apparent in Table 2. Note the astounding improve-
ment in performance. Despite a few decoding problems, which indicate limitations
in the heuristic search procedure employed by the recognizer, sentence accuracy had
improved from 35% to 82-83%.
Moreover, training turned out to be important for speeding up the search. The first
row shows the results for the initial estimates, which were very carefully prepared by
two members of the group over several weeks. Despite all of the careful hand work,
the search was so slow that only 10 of the 100 test sentences could be recognized.
The initial estimates were unusable without at least some training. These days, most
researchers find that they do not need to be nearly so careful in obtaining initial
estimates.
Emboldened by this success, the group began to explore other areas where training
might be helpful. They began by throwing out the phonological rules. Thus, they
accepted only a single pronunciation for each word. B-u-t-t-e-r had to be pronounced
</bodyText>
<page confidence="0.999125">
6
</page>
<note confidence="0.85602">
Kenneth W. Church and Robert L. Mercer Introduction
</note>
<bodyText confidence="0.999933230769231">
butter and s-o-m-e-t-h-i-n-g had to be pronounced something, and that was that. Any
change in these pronunciations was treated as a mislabeling from the front end. After
training, this simplified system correctly decoded 75% of 100 test sentences, which
was very encouraging.
Finally, they removed the dictionary lookup HMM, taking for the pronunciation
of each word its spelling. Thus, a word like t-h-r-o-u-g-h was assumed to have a
pronunciation like tuh huh ruh oh uu guh huh. After training, the system learned that
with words like 1-a-t-e the front end often missed the e. Similarly, it learned that g&apos;s
and h&apos;s were often silent. This crippled system was still able to recognize 43% of 100
test sentences correctly as compared with 35% for the original Raleigh system.
These results firmly established the importance of a coherent, probabilistic ap-
proach to speech recognition and the importance of data for estimating the parameters
of a probabilistic model. One by one, pieces of the system that had been assiduously
assembled by speech experts yielded to probabilistic modeling. Even the elaborate set
of hand-tuned rules for segmenting the frequency bank outputs into phoneme-sized
segments would be replaced with training (Bakis 1976; Bahl et al. 1978). By the sum-
mer of 1977, performance had reached 95% correct by sentence and 99.4% correct by
word, a considerable improvement over the same system with hand-tuned segmenta-
tion rules (73% by sentence and 95% by word).
Progress in speech recognition at Yorktown and almost everywhere else as well has
continued along the lines drawn in these early experiments. As computers increased
in power, ever greater tracts of the heuristic wasteland opened up for colonization by
probabilistic models. As greater quantities of recorded data became available, these
areas were tamed by automatic training techniques. Today, as indicated in the intro-
duction of Waibel and Lee (1990), almost every aspect of most speech recognition
systems is dominated by probabilistic models with parameters determined from data.
</bodyText>
<sectionHeader confidence="0.819263" genericHeader="method">
2. Part-of-Speech Tagging
</sectionHeader>
<bodyText confidence="0.929698809523809">
Many of the very same methods are being applied to problems in natural language
processing by many of the very same researchers. As a result, the empirical approach
has been adopted by almost all contemporary part-of-speech programs: Bahl and Mer-
cer (1976), Leech, Garside, and Atwell (1983), Jelinek (1985), Deroualt and Merialdo
(1986), Garside, Leech, and Sampson (1987), Church (1988), DeRose (1988), Hindle
(1989), Kupiec (1989, 1992), Ayuso et al. (1990), deMarcken (1990), Karlsson (1990),
Boggess, Agarwal, and Davis (1991), Merialdo (1991), and Voutilainen, Heikkila, and
Anttila (1992). These programs input a sequence of words, e.g., The chair will table the
motion, and output a sequence of part-of-speech tags, e.g., art noun modal verb art noun.
Most of these programs correctly tag at least 95% of the words, with practically no
restrictions on the input text, and with very modest space and time requirements.
Perhaps the most important indication of success is that many of these statistical tag-
ging programs are now being used on large volumes of data (hundreds of millions of
words of text) in a number of different application areas including speech synthesis
(Sproat, personal communication; Liberman and Church 1991), speech recognition (Je-
linek 1985; Jelinek, Mercer, and Roukos 1991), information retrieval (Salton, Zhao, and
Buckley 1990; Croft, Turtle, and Lewis 1991), sense disambiguation (Hearst 1991), and
computational lexicography (Klavans and Tzoukermann 1990a, 1990b; Church and
Gale 1991). Apparently, these programs must be addressing some important needs of
the research community or else they wouldn&apos;t be as widely cited as they are. Many of
the papers in this special issue refer to these taggers.
</bodyText>
<page confidence="0.997835">
7
</page>
<note confidence="0.3659">
Computational Linguistics Volume 19, Number 1
</note>
<bodyText confidence="0.9998796">
As in speech recognition, data collection efforts have played a pivotal role in
advancing data-intensive approaches to part-of-speech tagging. The Brown Corpus
(Francis and Kue&apos;era 1982) and similar efforts within the ICAME community, have
created invaluable opportunities. The Penn Treebank (see the paper by Marcus and
Santorini 1983) is currently being distributed by the ACL/DCI. The European Corpus
Initiative (ECI) plans to distribute similar material in a variety of languages. Even
greater resources are expected from the Linguistic Data Consortium (LDC). And the
Consortium for Lexical Research (CLR) is helping to make dictionaries more accessible
to the research community. For information on contacting these organizations, see
footnote 1.
</bodyText>
<subsectionHeader confidence="0.994468">
2.1 Recasting Part-of-Speech Tagging as a Noisy Channel Problem
</subsectionHeader>
<bodyText confidence="0.9997125">
Many of the tagging programs mentioned above are based on Shannon&apos;s Noisy Chan-
nel Model. Imagine that a sequence of parts of speech, P. is presented at the input to
the channel and for some crazy reason, it appears at the output of the channel in a
corrupted form as a sequence of words, W. Our job is to determine P given W.
</bodyText>
<equation confidence="0.755603">
P —&gt; Noisy Channel W
</equation>
<bodyText confidence="0.984367">
By analogy with the noisy channel formulation of the speech recognition problem, the
most probable part-of-speech sequence, 15, is given by:
</bodyText>
<equation confidence="0.959622">
15 = argmaxPr(P) Pr(W P)
</equation>
<bodyText confidence="0.999933909090909">
In theory, with the proper choice for the probability distributions Pr(P) and Pr(W I P),
this algorithm will perform as well as, or better than, any possible alternative that
one could imagine. Unfortunately, the probability distributions Pr(P) and Pr(W P)
are enormously complex: Pr(W I P) is a table giving for every pair W and P of the
same length a number between 0 and 1 that is the probability that a sequence of
words chosen at random from English text and found to have the part-of-speech
sequence P will turn out to be the word sequence W. Changing even a single word
or part-of-speech in a long sequence may change this number by many orders of
magnitude. However, experience has shown that surprisingly high tagging accuracy
can be achieved in practice using very simple approximations to Pr(P) and Pr(W I P).
In particular, it is possible to replace Pr(P) by a trigram approximation:
</bodyText>
<equation confidence="0.917173">
Pr(Pi, P21 • • • ,PN) H Pr (Pi Pi-2Pi-i)
</equation>
<bodyText confidence="0.9918635">
and to replace Pr(W I P) by an approximation in which each word depends only on
its own part of speech:
</bodyText>
<equation confidence="0.84714">
Pr(WI, W2, • • • WN I Pl)P2) • • • PN) HPr(Wi Pi)
i=i
</equation>
<bodyText confidence="0.9999466">
In these equations, Pi is the ith part of speech in the sequence P. and W, is the ith
word in W. The parameters of this model, the lexical probabilities, Pr( W I P1), and
the contextual probabilities, Pr(P t PI-2P1-1), are generally estimated by computing
various statistics over large bodies of text. One can view the first set of parameters as
a dictionary and the second set of parameters as a grammar.
</bodyText>
<page confidence="0.997914">
8
</page>
<note confidence="0.932703">
Kenneth W. Church and Robert L. Mercer Introduction
</note>
<tableCaption confidence="0.96471">
Table 3
</tableCaption>
<figure confidence="0.217710142857143">
Lexical ambiguity is hard (if we ignore preferences).
Word Part-of-Speech Tags
More Likely Less Likely
I pronoun noun (letter of alphabet)
see verb noun (the Holy See)
a article noun (letter of alphabet)
bird noun verb
</figure>
<subsectionHeader confidence="0.929447">
2.2 Why Traditional Methods Have Failed
</subsectionHeader>
<bodyText confidence="0.986249294117647">
Traditional methods have tended to ignore lexical preferences, which are the single-
most important source of constraint for part-of-speech tagging, and are sufficient by
themselves to resolve 90% of the tags. Consider the trivial sentence, &amp;quot;I see a bird,&amp;quot;
where every word is almost unambiguous. In the Brown Corpus (Francis and Kiitera
1982), the word &amp;quot;I&amp;quot; appears as a pronoun in 5,131 times out of 5,132 100%), &amp;quot;see&amp;quot;
appears as a verb in 771 times out of 772 (c-,- 100%), &amp;quot;a&amp;quot; appears as an article in 22,938
times out of 22,944 (P--- 100%) and &amp;quot;bird&amp;quot; appears as a noun in 25 times out of 25
100%). However, in addition to the desired tag, many dictionaries such as Web-
ster&apos;s Ninth New Collegiate Dictionary (Mish 1983) also list a number of extremely rare
alternatives, as illustrated in Table 3. These alternatives can usually be eliminated on
the basis of the statistical preferences, but traditional parsers don&apos;t, and consequently
run into serious difficulties. Attempts to eliminate unwanted tags on syntactic grounds
have not been very successful. For example, Ilnoun see/noun a/noun bird/noun, cannot
be ruled out as syntactically ill-formed, because the parser must accept sequences of
four nouns in other situations: city school committee meeting. Apparently, syntactic rules
are not nearly as effective as lexical preferences, at least for this application.
The tradition of ignoring preferences dates back to Chomsky&apos;s introduction of the
competence approximation (Chomsky 1957, pp. 15-17). Recall that Chomsky was con-
cerned that approximations, such as Shannon&apos;s n-gram approximation, which was very
much in vogue at the time, were inappropriate for his needs, and therefore, he intro-
duced an alternative with complementary strengths and weaknesses. The competence
approximation is more appropriate for modeling long-distance dependences such as
agreement constraints and wh-movement, but at the cost of missing certain crucial
local constraints, especially the kinds of preferences that are extremely important for
part-of-speech tagging.
2.3 Using Statistics to Fit Probabilistic Models to Data
Probabilistic models provide a theoretical abstraction of language, very much like
Chomsky&apos;s competence model. They are designed to capture the more important as-
pects of language and ignore the less important aspects, where what counts as im-
portant depends on the application. Statistics are often used to estimate the values of
the parameters in these probabilistic models. Thus, for example, we might estimate
the probability distribution for the word Kennedy in the Brown Corpus by modeling
the distribution with a binomial, and then use the frequency of Kennedy in the Brown
Corpus (140) to fit the model to the data.
</bodyText>
<page confidence="0.971838">
9
</page>
<note confidence="0.336717">
Computational Linguistics Volume 19, Number 1
</note>
<bodyText confidence="0.998884333333333">
The classic example of a binomial process is coin tossing. Suppose that the coin
comes up heads with probability p. Then the probability that it will come up heads
exactly m times in n tosses is
</bodyText>
<equation confidence="0.9941685">
( mil )
Pm (1 - 19)n-m &apos;
</equation>
<bodyText confidence="0.915299">
Here
which is called the binomial coefficient, is the number of ways the m positions can be
chosen from the n coin tosses. It is equal to
</bodyText>
<equation confidence="0.434056">
n!
</equation>
<bodyText confidence="0.999974857142857">
where n! (n factorial) is equal to 1 x 2 x • • • x n. For example, tossing a fair coin three
times (n = 3, p = 1/2) will result in 0, 1, 2, and 3 heads with probability 1/8, 3/8,
3/8, and 1/8, respectively. This set of probabilities is called the binomial distribution
for n and p. The expected value of the binomial distribution is np and the variance is
o-2 = np(1 — p). Thus, tossing a fair coin three times will produce an average of 3/2
heads with a variance of 3/4.
How can the binomial be used to model the distribution of Kennedy? Let p be the
probability that a word chosen at random in English text is Kennedy. We can think of
a series of words in English text as analogous to tosses of a coin that comes up heads
with probability p: the coin is heads if the word is Kennedy, and is tails otherwise.
Of course, we don&apos;t really know the value of p, but in a sample of n words, we
should expect to find about np occurrences of Kennedy. There are 140 occurrences of
Kennedy in the Brown Corpus, for which n is approximately 1,000,000. Therefore, we
can argue that 1,000, 000p must be about 140 and we can make an estimate, p, of p
equal to 140/1, 000, 000. If we really believe that words in English text come up like
heads when we flip a biased coin, then p is the value of p that makes the Brown
Corpus as probable as possible. Therefore, this method of estimating parameters is
called maximum likelihood estimation (MLE). For simple models, MLE is very easy to
implement and produces reasonable estimates in many cases. More elaborate methods
such as the Good-Turing Method (Good 1953) or Deleted Estimation (Jelinek and Mercer
1980, 1985) should be used when the frequencies are small (e.g., less than 10).
It is often convenient to use these statistical estimates as if they are the same as the
true probabilities, but this practice can lead to trouble, especially when the data don&apos;t
fit the model very well. In fact, content words don&apos;t fit a binomial very well, because
content words tend to appear in &amp;quot;bursts.&amp;quot; That is, content words are like buses in
New York City; they are social animals and like to travel in packs. In particular, if the
word Kennedy appears once in a unit of text (e.g., a paragraph, a discourse, or a genre),
then it is much more likely than chance to appear a second time in the same unit of
text. Function words also deviate from the binomial, though for different reasons (e.g.,
stylistic factors mentioned in Biber&apos;s paper).
These bursts might serve a useful purpose. People seem to be able to use these
bursts to speed up reaction times in various tasks. Psycholinguists use the term priming
to refer to this effect. Bursts might also be useful in a number of practical applications
such as Information Retrieval (IR) (Salton 1989; Frakes and Baeza-Yates 1992). There
have been a number of attempts over the years to model these bursts. The negative
</bodyText>
<page confidence="0.992233">
10
</page>
<note confidence="0.834892">
Kenneth W. Church and Robert L. Mercer Introduction
</note>
<bodyText confidence="0.999963142857143">
binomial distribution, for example, was explored in considerable detail in the classic
study of the authorship of the Federalist Papers, Mosteller and Wallace (1964), a must-
read for anyone interested in statistical analyses of large corpora.
We can show that the distribution of Kennedy is very bursty in the Brown Corpus by
dividing the corpus into k segments and showing that the probability varies radically
from one segment to another. For example, if we divide the Brown Corpus into 10
segments of 100,000 words each, we find that the frequency of Kennedy is: 58, 57, 2, 12,
6, 1, 4, 0, 0, 0. The variance of these 10 numbers is 539. Under the binomial assumption,
we obtain a very different estimate of the variance. In a sample of n =-- 100,000 words,
with fr = 140 per million, we would expect a variance of np(1 — fr) 14. (The variance
of the binomial is approximately the same as the expected value when p is small.) The
large discrepancy between the empirically derived estimate of the variance (539) and
the one based on the binomial assumption (14) indicates that the binomial assumption
does not fit the data very well.
</bodyText>
<sectionHeader confidence="0.955769" genericHeader="method">
3. Preference-Based Parsers
</sectionHeader>
<bodyText confidence="0.965602090909091">
When the data don&apos;t fit the model very well, we may wish to look for alternative
models. Four articles in this special issue propose empirical alternatives to traditional
parsing methods based on the competence model. As we have seen, the competence
model doesn&apos;t fit the part-of-speech application very well because of the model&apos;s fail-
ure to capture certain lexical preferences. The model also runs into trouble in a number
of other NLP applications. Consider, for example, the problem of deciding between
the words form and farm in the OCR application (mentioned in Section 1.4) when they
appear in the context:
( farm ) of
pure
form
Most people would have little difficulty deciding that form was the intended word.
Neither does an OCR system that employs a trigram language model, because prefer-
ences, such as collocations, fall naturally within the scope of the n-gram approximation.
Traditional NLP techniques, on the other hand, fail here because the competence ap-
proximation does not capture the crucial collocational constraints.
Lexicographers use the terms collocation, co-occurrence, and lexis to describe various
constraints on pairs of words. The words strong and powerful are perhaps the canonical
example. Halliday (1966; p. 150) noted that although strong and powerful have similar
syntax and semantics, there are contexts where one is much more appropriate than
the other (e.g., strong tea vs. powerful computers).
Psycholinguists have a similar concept, which they call word associations. Two fre-
quently cited examples of highly associated words are: bread/butter and doctor &apos;nurse.
See Palermo and Jenkins (1964) for tables of associations, measured for 200 words,
factored by grade level and sex. In general, subjects respond more quickly to a word
such as butter when it follows a highly associated word such as bread.
Some results and implications are summarized from reaction-time ex-
periments in which subjects either (a) classified successive strings of
letters as words and nonwords, or (b) pronounced the strings. Both
types of response to words (e.g., BUTTER) were consistently faster
when preceded by associated words (e.g., BREAD) rather than unas-
sociated words (e.g, NURSE). (Meyer, Schvaneveldt, and Ruddy 1975;
p. 98)
</bodyText>
<page confidence="0.992667">
11
</page>
<table confidence="0.528661">
Computational Linguistics Volume 19, Number 1
</table>
<tableCaption confidence="0.953581">
Table 4
The trigram approximation in action (Jelinek 1985).
</tableCaption>
<table confidence="0.9962693">
Word Rank
We 9
need 7
to 1
resolve 85
all 9
of 2
the 1
important 657
issues 14
</table>
<bodyText confidence="0.899102976744186">
More likely alternatives
The This One Two A Three Please In
are will the would also do
have know do ...
the this these problems ...
the
document question first ...
thing point to ...
These constraints are rarely discussed in computational linguistics because they are
not captured very well with traditional NLP techniques, especially those based on the
competence approximation. Of course, it isn&apos;t hard to build computational models that
capture at least some of these preferences. Even the trigram model, despite all of its
obvious shortcomings, does better than many traditional methods in this regard. The
power of the trigram approximation is illustrated in Table 4 for the sentence fragment,
We need to resolve all of the important issues. . . , selected from a 90 million—word corpus of
IBM office correspondences. Each row shows the correct word, the rank of the correct
word as predicted by the trigram model, and then the list of words judged by the
trigram model to be more probable than the correct word. Thus, We is the 9th most
probable word to begin a sentence. At this point in the sentence, in the absence of any
other context, the trigram model is as good as any model we could have. Following We
at the beginning of the sentence, need is the 7th most probable word, ranking behind
are, will, the, would, also, and do. Here, again, the trigram model still accounts for all
of the context there is and so should be doing as well as any model can. Following
We need, to is the most probable word. Although by now, the trigram model has lost
track of the complete context (it no longer realizes that we are at the beginning of a
sentence), it is still doing very well.
Table 4 shows that the trigram model captures a number of important frequency-
based constraints that would be missed by most traditional parsers. For example, the
trigram model captures the fact that issues is particularly predictable in the collocation:
important issues. In general, high-frequency function words like to and the, which are
acoustically short, are more predictable than content words like resolve and important,
which are longer. This is convenient for speech recognition because it means that the
language model provides more powerful constraints just when the acoustic model is
having the toughest time. One suspects that this is not an accident, but rather a natural
result of the evolution of speech to fill the human needs for reliable communication
in the presence of noise.
. The ideal NLP model would combine the strengths of both the competence ap-
proximation and the n-gram approximation. One possible solution might be the Inside—
Outside algorithm (Baker 1979; Lan i and Young 1991), a generalization of the Forward—
Backward algorithm that estimates the parameters of a hidden stochastic context-free
grammar, rather than a hidden Markov model. Four alternatives are proposed in these
special issues: (1) Brent (1993), (2) Briscoe and Carroll (this issue), (3) Hindle and Rooth
(this issue), and (4) Weischedel et al. (1993). Briscoe and Carroll&apos;s contribution is very
</bodyText>
<page confidence="0.995489">
12
</page>
<note confidence="0.938016">
Kenneth W. Church and Robert L. Mercer Introduction
</note>
<bodyText confidence="0.998225647058824">
much in the spirit of the Inside—Outside algorithm, whereas Hindle and Rooth&apos;s con-
tribution, for example, takes an approach that is much closer to the concerns of lexi-
cography, and makes use of preferences involving words, rather than preferences that
ignore words and focus exclusively on syntactic structures. Hindle and Rooth show
how co-occurrence statistics can be used to improve the performance of the parser on
sentences such as:
( )
wanted
She placed the dress on the rack.
put
where lexical preferences are crucial to resolving the ambiguity of prepositional phrase
attachment (Ford, Bresnan, and Kaplan 1982). Hindle and Rooth show that a parser
can enforce these preferences by comparing the statistical association of the verb-
preposition (want.. . on) with the association of the object-preposition (dress. . . on), when
attaching the prepositional phrase. This work is just a first step toward preference-
based parsing, an empirically motivated alternative to traditional rational approaches
such as ATNs, unification parsers, and principle-based parsers.
</bodyText>
<sectionHeader confidence="0.989967" genericHeader="method">
4. Entropy
</sectionHeader>
<bodyText confidence="0.999976870967742">
How do we decide if one language model is better than another? In the 1940s, Shan-
non defined entropy, a measure of the information content of a probabilistic source,
and used it to quantify such concepts as noise, redundancy, the capacity of a commu-
nication channel (e.g., a telephone), and the efficiency of a code. The standard unit of
entropy is the bit or binary digit. See Bell, Cleary, and Witten (1990) for a more dis-
cussion on entropy; Section 2.2.5 shows how to compute the entropy of a model, and
Section 4 discusses how Shannon and others have estimated the entropy of English.
From the point of view of speech recognition or OCR, we would like to be able
to characterize the size of the search space, the number of binary questions that the
recognizer will have to answer on average in order to decode a message. Cross entropy
is a useful yardstick for measuring the ability of a language model to predict a source
of data. If the language model is very good at predicting the future output of the
source, then the cross entropy will be small. No matter how good the language model
is, though, the cross entropy cannot be reduced below a lower bound, known as the
entropy of the source, the cross entropy of the source with itself.
One can also think of the cross entropy between a language model and a proba-
bilistic source as the number of bits that will be needed on average to encode a symbol
from the source when it is assumed, albeit mistakenly, that the language model is a
perfect probabilistic characterization of the source. Thus, there is a close connection
between a language model and a coding scheme. Table 5 below lists a number of
coding schemes along with estimates of their cross entropies with English text.
The standard ASCII code requires 8 bits per character. It would be a perfect code
if the source produced each of the 28 = 256 symbols equally often and independently
of context. However, English is not like this. For an English source, it is possible to
reduce the average length of the code by assigning shorter codes to more frequent
symbols (e.g., e, n, s) and longer codes to less frequent symbols (e.g., j, q, z), using a
coding scheme such as a Huffman code (Bell, Cleary, and Witten 1990; Section 5.1.2).
Other codes, such as Lempel—Ziv (Welch 1984; Bell, Cleary, and Witten, Chapters 8-9)
and n-gram models on words, achieve even better compression by taking advantage of
context, though none of these codes seem to perform as well as people do in predicting
the next letter (Shannon 1951).
</bodyText>
<page confidence="0.994129">
13
</page>
<table confidence="0.529494">
Computational Linguistics Volume 19, Number 1
</table>
<tableCaption confidence="0.994099">
Table 5
</tableCaption>
<table confidence="0.97737925">
Cross entropy of various language models.
Model Bits / Character
ASCII 8
Huffman code each char 5
Lempel-Ziv (Unix Tm compress) 4.43
Unigram (Huffman code each word) 2.1 (Brown, personal communication)
Trigram 1.76 (Brown et al. 1992)
Human Performance 1.25 (Shannon 1951)
</table>
<bodyText confidence="0.3701">
The cross entropy, H, of a code and a source is given by:
</bodyText>
<equation confidence="0.498587">
H(source, code) = - EE Pr(s,h I source) log2 Pr(s I h, code)
s h
</equation>
<bodyText confidence="0.9990094">
where Pr(s,h I source) is the joint probability of a symbol s following a history h given
the source. Pr(s I h, code) is the conditional probability of s given the history (context)
h and the code. In the special case of ASCII, where Pr(s h, ASCII) = 1/256, we can
actually carry out the indicated sum, and find, not surprisingly, that ASCII requires 8
bits per character:
</bodyText>
<equation confidence="0.896606333333333">
256
H(source, ASCII) = - E 256 &apos;°g2 g 256 -
s=1
</equation>
<bodyText confidence="0.999497111111111">
In more difficult cases, cross entropy is estimated by a sampling procedure. Two in-
dependent samples of the source are collected: Si and 52. The first sample, Si, is used
to fit the values of the parameters of the code, and second sample, S2, is used to test
the fit. For example, to determine the value of 5 bits per character for the Huffman
code in Table 5, we counted the number of times that each of the 256 ASCII characters
appeared in Si, a sample of Ni characters selected from the Wall Street Journal text
distributed by the ACL/DCI. These counts were used to determine Pr(s h, code) (or
rather Pr(s I code), since the Huffman code doesn&apos;t depend on h). Then we collected a
second sample, Sz, of N2 characters, and tested the fit with the formula:
</bodyText>
<equation confidence="0.99729625">
N2
H(source, code) - —1 E log2 Pr(S2 [i] code)
N2
i=1
</equation>
<bodyText confidence="0.999992888888889">
where 52 [i] is the ith character in the second sample. It is important in this procedure to
use two different samples of text. If we were to use the same sample for both testing
and training, we would obtain an overly optimistic estimate of how well the code
performs.
The other codes in Table 5 make better use of context (h), and therefore, they
achieve better compression. For example, Huffman coding on words (a unigram model)
is more than twice as compact as Huffman coding on characters (2.1 vs. 5 bits/char.).
The unigram model is also more than twice as good as Lempel-Ziv (2.1 vs. 4.43 bits/
char.), demonstrating that compress, a popular UnixTM tool for compressing files, could
</bodyText>
<page confidence="0.997971">
14
</page>
<note confidence="0.941224">
Kenneth W. Church and Robert L. Mercer Introduction
</note>
<tableCaption confidence="0.984711">
Table 6
</tableCaption>
<table confidence="0.957818272727273">
Summary of two approaches to NLP.
Rationalism Empiricism
Well-known Advocates:
Model:
Contexts of Interest:
Goals:
Linguistic Generalizations:
Parsing Strategies:
Applications:
Chomsky, Minsky
Competence Model
Phrase Structure
All and Only
Explanatory
Theoretical
Agreement and
Wh-movement
Principle-Based
CKY (Chart), ATNs,
Unification
Understanding
Who did what to whom
Shannon, Skinner, Firth, Harris
Noisy Channel Model
N-grams
Minimize Prediction Error (Entropy)
Descriptive
Applied
Collocations and Word Associations
Preference-Based
Forward—Backward, Inside—Outside
Recognition
Noisy Channel Applications
</table>
<bodyText confidence="0.999799647058823">
be improved by a factor of two (when the files are in English). The trigram model, the
method of choice in speech recognition, achieves 1.76 bits per character, outperforming
the practical alternatives in Table 5, but falling half a bit shy of Shannon&apos;s estimate of
human performance.&apos;
Someday parsers might help squeeze out some of this remaining half bit between
the trigram model and Shannon&apos;s bound, but thus far, parsing has had little impact.
Lan i and Young (1991; p. 255), for example, conducted a number of experiments with
stochastic context-free grammars (SCFGs), and concluded that &amp;quot;Mlle experiments on
word recognition showed that although SCFGs are effective, their complex training
routine prohibits them from directly replacing the simpler HMM-based recognizers.&amp;quot;
They then proceeded to argue, quite sensibly, that parsers are probably more appropri-
ate for tasks where phrase structure is more directly relevant than in word recognition.
In general, phrase structure is probably more important for understanding who did
what to whom, than recognizing what was said.&apos; Some tasks are probably more ap-
propriate for Chomsky&apos;s rational approach to language and other tasks are probably
more appropriate for Shannon&apos;s empirical approach to language. Table 6 summarizes
some of the differences between the two approaches.
</bodyText>
<listItem confidence="0.625389">
5. Machine Translation and Bilingual Lexicography
</listItem>
<bodyText confidence="0.9984018">
Is machine translation (MT) more suitable for rationalism or empiricism? Both ap-
proaches have been investigated. Weaver (1949) was the first to propose an information
theoretic approach to MT. The empirical approach was also practiced at Georgetown
during the 1950s and 1960s (Henisz-Dostert, Ross Macdonald, and Zarechnak 1979)
in a system that eventually became known as SYSTRAN. Recently, most work in MT
</bodyText>
<footnote confidence="0.99605">
2 In fact, the trigram model might be even better than suggested in Table 5, since the estimate for the
trigram model in Brown et al. (1992) is computed over a 256-character alphabet, whereas the estimate
for human performance in Shannan (1951) is computed over a 27-character alphabet.
3 Lan and Young actually looked at another task involving phonotactic structure where there is also
good reason to believe that SCFGs might be able to capture crucial linguistic constraints that might be
missed by simpler HMMs.
</footnote>
<page confidence="0.991996">
15
</page>
<note confidence="0.560433">
Computational Linguistics Volume 19, Number 1
</note>
<bodyText confidence="0.9999335">
has tended to favor rationalism, though there are some important exceptions, such as
example-based MT (Sato and Nagao 1990). The issue remains as controversial as ever,
as evidenced by the lively debate on rationalism versus empiricism at TMI-92, a recent
conference on MT.&apos;
The paper by Brown et al. (1990) revives Weaver&apos;s information theoretic approach
to MT. It requires a bit more squeezing and twisting to fit machine translation into the
noisy channel mold: to translate, for example, from French to English, one imagines
that the native speaker of French has thought up what he or she wants to say in
English and then translates mentally into French before actually saying it. The task of
the translation system is to recover the original English, E, from the observed French,
F. While this may seem a bit far-fetched, it differs little in principle from using English
as an interlingua or as a meaning representation language.
</bodyText>
<equation confidence="0.867886">
E —&gt; Noisy Channel —&gt; F
</equation>
<bodyText confidence="0.999745518518519">
As before, one minimizes one&apos;s chance of error by choosing E according to the formula:
= argmaxPr(E) Pr(F E)
As before, the parameters of the model are estimated by computing various statistics
over large samples of text. The prior probability, Pr(E), is estimated in exactly the same
way as discussed above for the speech recognition application. The parameters of the
channel model, Pr(F I E), are estimated from a parallel text that has been aligned by
an automatic procedure that figures out which parts of the source text correspond to
which parts of the target text. See Brown et al. (1993) for more details on the estimation
of the parameters.
The information theoretic approach to MT may fail for reasons advanced by Chom-
sky and others in the 1950s. But regardless of its ultimate success or failure, there is a
growing community of researchers in corpus-based linguistics who believe that it will
produce a number of lexical resources that may be of great value. In particular, there
has been quite a bit of discussion of bilingual concordances recently (e.g., Klavans
and Tzoukermann 1990a, 1990b; Church and Gale 1991), including the 1990 and 1991
lexicography conferences sponsored by Oxford University Press and Waterloo Univer-
sity. A bilingual concordance is like a monolingual concordance except that each line
in the concordance is followed by a line of text in a second language. There are also
some hopes that the approach might produce tools that could be useful for human
translators (Isabelle 1992).
There are three papers in these special issues on aligning bilingual texts such as
the Canadian Hansards (parliamentary debates) that are available in both English and
French: Brown et al. (1993), Gale and Church (this issue), and Kay and Rosenschein
(this issue). Warwick-Armstrong and Russell have also been interested in the alignment
problem (Warwick-Armstrong and Russell 1990). Except for Brown et al., this work is
focused on the less controversial applications in lexicography and human translation,
rather than MT.
</bodyText>
<footnote confidence="0.990087333333333">
4 Requests for a tape of the debate should be sent to the attention of Pierre Isabelle, CCRIT, TMI-92, 1575
boul. Chomedey, Laval (Quebec), H7V 2X2, Canada. Copies of the TMI proceedings can be obtained by
writing to CCRIT or sending e-mail to tmi@ccrit.doc.ca.
</footnote>
<page confidence="0.997675">
16
</page>
<note confidence="0.969229">
Kenneth W. Church and Robert L. Mercer Introduction
</note>
<sectionHeader confidence="0.86177" genericHeader="method">
6. Monolingual Lexicography, Machine-Readable Dictionaries (MRDs),
</sectionHeader>
<subsectionHeader confidence="0.750987">
and Computational Lexicons
</subsectionHeader>
<bodyText confidence="0.999991974358975">
There has been a long tradition of empiricist approaches in lexicography, both bilingual
and monolingual, dating back to Johnson and Murray. As corpus data and machine-
readable dictionaries (MRDs) become more and more available, it is becoming easier
to compile lexicons for computers and dictionaries for people. This is a particularly
exciting area in computational linguistics as evidenced by the large number of con-
tributions in these special issues: Biber (1993), Brent (1993), Hindle and Rooth (this
issue), Pustejovsky et al. (1993), and Smadja (this issue). Starting with the COBUILD
dictionary (Sinclair et al. 1987), it is now becoming more and more common to find
lexicographers working directly with corpus data. Sinclair makes an excellent case for
the use of corpus evidence in the preface to the COBUILD dictionary:
For the first time, a dictionary has been compiled by the thorough
examination of a representative group of English texts, spoken and
written, running to many millions of words. This means that in ad-
dition to all the tools of the conventional dictionary makers—wide
reading and experience of English, other dictionaries and of course
eyes and ears—this dictionary is based on hard, measurable evidence.
(Sinclair et al. 1987; p. xv)
The experience of writing the COBUILD dictionary is documented in Sinclair (1987),
a collection of articles from the COBUILD project; see Boguraev (1990) for a strong
positive review of this collection. At the time, the corpus-based approach to lexicogra-
phy was considered pioneering, even somewhat controversial; today, quite a number
of the major lexicography houses are collecting large amounts of corpus data.
The traditional alternative to corpora are citation indexes, boxes of interesting
citations collected on index cards by large numbers of human readers. Unfortunately,
citation indexes tend to be a bit like butterfly collections, full of rare and unusual
specimens, but severely lacking in ordinary, garden-variety moths. Murray, the editor
of the Oxford English Dictionary, complained:
The editor or his assistants have to search for precious hours for exam-
ples of common words, which readers passed by... Thus, of Abus ion
we found in the slips about 50 instances; of Abuse not five. (James Au-
gustus Henry Murray, Presidential Address, Philological Society Trans-
actions 1877-9, pp. 571-2, quoted by Murray 1977, p. 178)
He then went on to say, &amp;quot;There was not a single quotation for imaginable, a word used
by Chaucer, Sir Thomas More, and Milton.&amp;quot; From a statistical point of view, citation
indexes have serious sampling problems; they tend to produce a sample that is heavily
skewed away from the &amp;quot;central and typical&amp;quot; facts of the language that every speaker
is expected to know. Large corpus studies, such as the COBUILD dictionary, offer
the hope that it might be possible to base a dictionary on a large and representative
sample of the language as it is actually used.
</bodyText>
<subsectionHeader confidence="0.999425">
6.1 Should a Corpus Be Balanced?
</subsectionHeader>
<bodyText confidence="0.999957">
Ideally, we would like to use a large and representative sample of general language,
but if we have to choose between large and representative, which is more important?
There was a debate on a similar question between Prof. John Sinclair and Sir Randolf
</bodyText>
<page confidence="0.997475">
17
</page>
<note confidence="0.454642">
Computational Linguistics Volume 19, Number 1
</note>
<tableCaption confidence="0.993601">
Table 7
</tableCaption>
<table confidence="0.979671230769231">
Coverage of imaginable in various corpora.
Size (in millions) Corpus raw freq freq/million
1 Brown Corpus 0 0
1 Bible 0 0
2 Shakespeare 0 0
7 WSJ 41 5.9
10 Groliers 5 0.5
18 Hansard 15 0.8
29 DOE 5 0.2
46 AP 1988 36 0.8
50 AP 1989 39 0.8
56 AP 1990 21 0.4
47 AP 1991 19 0.4
</table>
<bodyText confidence="0.99986347826087">
Quirk at the 1991 lexicography conference sponsored by Oxford University Press and
Waterloo University, where the house voted, perhaps surprisingly, that a corpus does
not need to be balanced. Although the house was probably predisposed to side with
Quirk&apos;s position, Sinclair was able to point out a number of serious problems with the
balancing position. It may not be possible to properly balance a corpus. And moreover,
if we insist on throwing out idiosyncratic data, we may find it very difficult to collect
any data at all, since all corpora have their quirks.
In some sense, the question comes down to a tradeoff between quality and quan-
tity. American industrial laboratories (e.g., IBM, AT&amp;T) tend to favor quantity, whereas
the BNC, NERC, and many dictionary publishers, especially in Europe, tend to favor
quality. The paper by Biber (1993) argues for quality, suggesting that we ought to use
the same kinds of sampling methods that statisticians use when studying the econ-
omy or predicting the results of an election. Poor sampling methods, inappropriate
assumptions, and other statistical errors can produce misleading results: &amp;quot;There are
lies, damn lies, and statistics.&amp;quot;
Unfortunately, sampling methods can be expensive; it is not clear whether we can
justify the expense for the kinds of applications that we have in mind. Table 7 might
lend some support for the quantity position for Murray&apos;s example of imaginable. Note
that there is plenty of evidence in the larger corpora, but not in the smaller ones. Thus,
it would appear that &amp;quot;more data are better data,&amp;quot; at least for the purpose of finding
exemplars of words like imaginable.
Similar comments hold for collocation studies, as illustrated in Table 8, which
shows mutual information values (Fano 1961; p. 28)
</bodyText>
<equation confidence="0.997382">
Pr(x,y)
I(x;y) = log2 Pr(x) Pr(y)
</equation>
<bodyText confidence="0.999873">
for several collocations in a number of different corpora. Mutual information compares
the probability of observing word x and word y together (the joint probability) to the
probability of observing x and y independently (chance). Most of the mutual informa-
tion values in Table 8 are much larger than zero, indicating, as we would hope, that
the collocations appear much more often in these corpora than one would expect by
chance. The probabilities, Pr(x) and Pr(y), are estimated by counting the number of
observations of x and y in a corpus, f (x) and f (y), respectively, and normalizing by N,
</bodyText>
<page confidence="0.998356">
18
</page>
<note confidence="0.970308">
Kenneth W. Church and Robert L. Mercer Introduction
</note>
<tableCaption confidence="0.997561">
Table 8
</tableCaption>
<table confidence="0.9880405">
Collocations in different corpora.
Size Corpus strong strong strong strong strong
(in millions) support economy winds man enough
1 Brown Corpus 5.1 (1) 8.3 (1) - 7.3 ,
1 Bible 3.4 (7) -
2 Shakespeare - - - - 6.5 (4)
7 WSJ 5.5 4.9 6.5 (7) 4.7 6.5
10 Groliers 5.8 3.8 8.3 4.2 (3) 8.3
18 Hansard 6.2 6.4 - - 7.0
29 DOE 4.5 4.3 7.7 - 7.4
46 AP 1988 6.3 6.3 8.5 4.0 7.0
50 AP 1989 6.3 4.7 8.4 1.8 (7) 7.3
56 AP 1990 6.5 3.7 8.3 2.4 (9) 7.5
47 AP 1991 7.0 3.4 8.7 2.0 (6) 7.3
</table>
<bodyText confidence="0.999647761904762">
the size of the corpus. The joint probability, Pr(x,y), is estimated by counting the num-
ber of times that x is immediately followed by y in the corpus, f (x , y), and normalizing
by N. Unfortunately, mutual information values become unstable if the counts are too
small. For this reason, small counts (less than 10) are shown in parentheses. A dash is
used when there is no evidence for the collocation.
Like Table 7, Table 8 also shows that &amp;quot;more data are better data.&amp;quot; That is, there
is plenty of evidence in the larger corpora, but not in the smaller ones. &amp;quot;Only a large
corpus of natural language enables us to identify recurring patterns in the language
and to observe collocational and lexical restrictions accurately....&amp;quot; (Hanks 1990; p. 36)
However, in order to make use of this evidence we have to find ways to com-
pensate for the obvious problems of working with unbalanced data. For example, in
the Canadian Hansards, there are a number of unwanted phrases such as: &amp;quot;House of
Commons,&amp;quot; &amp;quot;free trade agreement,&amp;quot; &amp;quot;honour and duty to present,&amp;quot; and &amp;quot;Hear! Hear!&amp;quot;
Fortunately, though, it is extremely unlikely that these unwanted phrases will appear
much more often than chance across a range of other corpora such as Department
of Energy (DOE) abstracts or the Associated Press (AP) news. If such a phrase were
to appear relatively often across a range of such diverse corpora, then it is probably
worthy of further investigation. Thus, it is not required that the corpora be balanced,
but rather that their quirks be uncorrelated across a range of different corpora. This
is a much weaker and more realistic requirement than the more standard (and more
idealistic) practice of balancing and purging quirks.
</bodyText>
<subsectionHeader confidence="0.733228">
6.2 Lexicography and Exploratory Data Analysis (EDA)
</subsectionHeader>
<bodyText confidence="0.99921">
Statistics can be used for many different purposes. Traditionally, statistics such as
Student&apos;s t-tests were developed to test a particular hypothesis. For example, suppose
that we were concerned that strong enough shouldn&apos;t be considered a collocation. A t-
test could be used to compare the hypothesis that strong enough appears too often to be
a fluke against the null hypothesis that the observations can be attributed to chance. The
t-score compares the two hypotheses, by taking the difference of the means of the two
probability distributions, and normalizing appropriately by the variances, so that the
result can be interpreted as a number of standard deviations. Theoretically, if the t-score
is larger than 1.65 standard deviations, then we ought to believe that the co-occurrences
</bodyText>
<page confidence="0.996792">
19
</page>
<note confidence="0.661913">
Computational Linguistics Volume 19, Number 1
</note>
<bodyText confidence="0.999715">
are significant and we can reject the null hypothesis with 95% confidence, though in
practice we might look for a t-score of 2 or more standard deviations, since t-scores
are often inflated (due to certain violations of the assumptions behind the model). See
Dunning (this issue) for a critique of the assumption that the probabilities are normally
distributed, and an alternative parameterization of the probability distributions.
</bodyText>
<equation confidence="0.998628">
= mean(Pr(strong, enough)) — mean(Pr (strong)) mean(Pr (enough))
t
a2 (Pr (strong, enough)) + a2(Pr(strong)Pr (enough))
</equation>
<bodyText confidence="0.991488857142857">
In the Brown Corpus, it happens that f (strong, enough) = 11, f (strong) -= 194, f (enough)
= 426, and N = 1, 181, 041. Using these values, we estimate t 3.3, which is larger
than 1.65, and therefore we can confidently reject the null hypothesis, and conclude that
the co-occurrence is significantly larger than chance. The estimation uses the approx-
imation, a2 (Pr (strong, enough)) f (strong, enough) /N2, which can be justified under
appropriate binomial assumptions. It is also assumed that a2 (Pr (strong) Pr(enough)) is
very small and can be omitted.
</bodyText>
<equation confidence="0.98993275">
f(strong,enough) f (strong) f (mough)
t N N N
\f(strong,enough
N2
</equation>
<bodyText confidence="0.9999118">
Although statistics are often used to test a particular hypothesis as we have just seen,
statistics can also be used to explore the space of possible hypotheses, or to discover
new hypotheses (supervised/unsupervised learning/training). See Tukey (1977) and
Mosteller and Tukey (1977) for two textbooks on Exploratory Data Analysis (EDA),
and Jelinek (1985) for a very nice review paper on self-organizing statistics. Both the
exploratory and self-organizing views are represented in these special issues. Puste-
jovsky et al. (1993) use an EDA approach to investigate certain questions in lexical
semantics. Brent (1993), in contrast, adopts a self-organizing approach to identify sub-
categorization features.
Table 9 shows how the t-score can be used in an exploratory mode to extract large
numbers of words from the Associated Press (AP) news that co-occur more often with
strong than with powerful, and vice versa. It is an interesting question whether collo-
cations are simply idiosyncratic as Halliday and many others have generally assumed
(see Smadja [this issue% or whether there might be some general principles that could
account for many of the cases. After looking at Table 9, Hanks, a lexicographer and
one of the authors of Church et al. (1991), hypothesized that strong is an intrinsic
quality whereas powerful is an extrinsic one. Thus, for example, any worthwhile politi-
cian or cause can expect strong supporters, who are enthusiastic, convinced, vociferous,
etc., but far more valuable are powerful supporters, who will bring others with them.
They are also, according to the AP news, much rarer—or at any rate, much less often
mentioned. This is a fascinating hypothesis that deserves further investigation.
Summary statistics such as mutual information and t-scores may have an impor-
tant role to play in helping lexicographers to discover significant patterns of collo-
cations, though the position remains somewhat controversial. Some lexicographers
prefer mutual information, some prefer t-scores, and some are unconvinced that ei-
ther of them is any good. Church et al. (1991) argued that different statistics have
different strengths and weaknesses, and that it requires human judgment and explo-
ration to decide which statistic is best for a particular problem. Others, such as Jelinek
(1985), would prefer a self-organizing approach, where there is no need for human
judgment.
</bodyText>
<page confidence="0.99216">
20
</page>
<note confidence="0.970009">
Kenneth W. Church and Robert L. Mercer Introduction
</note>
<tableCaption confidence="0.99697">
Table 9
</tableCaption>
<table confidence="0.972155785714286">
An example of the t-score (Church et al. 1991).
strong w Strong w w t Powerful w w
powerful w strong w powerful w
12.42 161 0 showing —7.44 1 56 than
11.94 175 2 support —5.60 1 32 figure
10.08 550 68 , —5.37 3 31 minority
9.97 106 0 defense —5.23 1 28 of
9.76 102 0 economy —4.91 0 24 post
9.50 97 0 demand —4.63 5 25
9.40 95 0 gains —4.35 27 36 nmeiwlitary
9.18 91 0 growth —3.89 0 15 figures
8.84 137 5 winds —3.59 6 17 presidency
8.02 83 1 opposition —3.57 27 29 political
7.78 67 0 sales —3.33 0 11 computers
</table>
<sectionHeader confidence="0.336816" genericHeader="conclusions">
7. Conclusion
</sectionHeader>
<bodyText confidence="0.999873366666667">
The flourishing renaissance of empiricism in computational linguistics grew out of the
experience of the speech recognition community during the 1970s and 1980s. Many of
the same statistical techniques (e.g., Shannon&apos;s Noisy Channel Model, n-gram mod-
els, hidden Markov models (HMMs), entropy (H), mutual information (I), Student&apos;s
t-score) have appeared in one form or another, often first in speech, and then soon
thereafter in language. Many of the same researchers have applied these methods to
a variety of application areas ranging from language modeling for noisy channel ap-
plications (e.g., speech recognition, optical character recognition [OCR], and spelling
correction [Damerau 1964; Kukich 1992]), to part-of-speech tagging, parsing, transla-
tion, lexicography, text compression (Bell, Cleary, and Witten 1990) and information
retrieval (IR) (Salton 1989; Frakes and Baeza-Yates 1992).
Empiricism is, of course, a very old tradition. Back in the 1950s and 1960s, long
before the speech work of the 1970s and 1980s, there was Skinner&apos;s Behaviorism in
Psychology, Shannon&apos;s Information Theory in Electrical Engineering, and Harris&apos; Dis-
tributional Hypothesis in American Linguistics and the Firthian approach in British
Linguistics (&amp;quot;You shall know a word by the company it keeps&amp;quot;). It is possible that
much of this work was actually inspired by Turing&apos;s code-breaking efforts during
World War II, but we may never know for sure given the necessity for secrecy.
The recent revival in empiricism has been fueled by three developments. First
computers are much more powerful and more available than they were in the 1950s
when empiricist ideas were first applied to problems in language, or in the 1970s and
1980s, when data-intensive methods were too expensive for researchers working in
universities. Second, data have become much more available than ever before. As a
result of a number of data collection and related efforts such as ACL/DCI, BNC, CLR,
Ed, EDR, LDC, ICAME, NERC, and TEI, most researchers should now be able to
make use of a number of very respectable machine-readable dictionaries (MRDs) and
text corpora. (See footnote 1 for information on contacting many of these organiza-
tions.) Data-intensive methods are no longer restricted to those working in affluent
industrial laboratories. Third, and perhaps most importantly, due to various political
and economic changes around the world, there is a greater emphasis these days on
</bodyText>
<page confidence="0.995234">
21
</page>
<note confidence="0.631533">
Computational Linguistics Volume 19, Number 1
</note>
<bodyText confidence="0.99859825">
deliverables and evaluation. Data collection efforts have been relatively successful in
responding to these pressures by delivering massive quantities of data. Text Analysis
has also prospered because of its tradition of evaluating performance with theoretically
motivated numerical measures such as entropy.
</bodyText>
<sectionHeader confidence="0.974855" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99303618095238">
Ayuso, D.; Bobrow, R.; MacLaughlin, D.;
Meteer, M.; Ramshaw, L.; Schwartz, R.;
and Weischedel, R. (1990). &amp;quot;Toward
understanding text with a very large
vocabulary.&amp;quot; DARPA Speech and Natural
Language Workshop. San Mateo, CA,
354-358. Morgan Kaufmann.
Bahl, L.; Baker, J.; Cohen, P.; Dixon, N.;
Jelinek, F.; Mercer, R.; and Silverman, H.
(1975). &amp;quot;Preliminary results in the
performance of a system for the
automatic recognition of continuous
speech.&amp;quot; IBM Technical Report #RC 5654.
Bahl, L.; Baker, J.; Cohen, P.; Jelinek, F.;
Lewis, B.; and Mercer, R. (1978).
&amp;quot;Recognition of a continuously read
natural corpus.&amp;quot; In Proceedings, IEEE
International Conference on Acoustics, Speech
and Signal Processing (ICASSP).
Bahl, L., and Mercer, R. (1976). &amp;quot;Part of
speech assignment by a statistical
decision algorithm.&amp;quot; In Abstracts of Papers
from the International Symposium on
Information Theory.
Baker, J. (1979). &amp;quot;Trainable grammars for
speech recognition.&amp;quot; In Speech
Communication Papers for the 97th Meeting of
the Acoustical Society of America, edited by
Klatt and Wolf, 547-550.
Bakis, R. (1976). &amp;quot;Continuous speech
recognition via centisecond acoustic
states.&amp;quot; In Proceedings of 91st Meeting of the
Acoustic Society of America.
Baum, L. (1972). &amp;quot;An inequality and
associated maximization technique in
statistical estimation of probabilistic
functions of a Markov process.&amp;quot;
Inequalities, 3,1-8.
Bell, T.; Cleary, J.; and Witten, I. (1990). Text
Compression. Prentice Hall.
Biber, D. (1993). &amp;quot;Representativeness in
corpus design.&amp;quot; Computational Linguistics,
19(2). In press.
Boggess, L.; Agarwal, R.; and Davis, R.
(1991). &amp;quot;Disambiguation of prepositional
phrases in automatically labelled
technical text.&amp;quot; AAAI, 155-159.
Boguraev, B. (1990). &amp;quot;Looking Up: an
account of the COBUILD project in lexical
computing.&amp;quot; Computational Linguistics,
16(3), 184-185.
Brent, M. (1993). &amp;quot;Robust acquisition of
subcategorization features from arbitrary
text: syntactic knowledge meets
unsupervised learning.&amp;quot; Computational
Linguistics, 19(2). In press.
Brown, P.; Cocke, J.; Della Pietra, S.; Della
Pietra, V.; Jelinek, F.; Lafferty, J.; Mercer,
R.; and Rossin, P. (1990). &amp;quot;A statistical
approach to machine translation.&amp;quot;
Computational Linguistics, 16(2), 79-85.
Brown, P.; Della Pietra, S.; Della Pietra, V.;
Lai, J.; and Mercer, R. (1992). &amp;quot;An
estimate of an upper bound for the
entropy of English.&amp;quot; Computational
Linguistics, 18(1), 31-40.
Brown, P.; Della Pietra, S.; Della Pietra, V.;
Mercer, R. (1993). &amp;quot;The mathematics of
machine translation.&amp;quot; Computational
Linguistics, 19(2). In press.
Chomsky, N. (1957). Syntactic Structures.
Mouton.
Church, K. (1988). &amp;quot;A stochastic parts
program and noun phrase parser for
unrestricted text.&amp;quot; In Proceedings, Second
Conference on Applied Natural Language
Processing. (ACL), Austin, TX, 136-143.
Church, K.; Hanks, P.; Hindle, D.; and Gale,
W. (1991). &amp;quot;Using statistics in lexical
analysis.&amp;quot; In Lexical Acquisition: Using
On-Line Resources to Build a Lexicon, edited
by Zernik. Lawrence Erlbaum, 115-164.
Church, K., and Gale, W. (1991).
&amp;quot;Concordances for parallel text.&amp;quot; In
Proceedings, Seventh Annual Conference of the
UW Centre for the New Oxford English
Dictionary and Text Research.
Cohen, P., and Mercer, R. (1974). &amp;quot;The
phonological component of an automatic
speech-recognition system.&amp;quot; In Speech
Recognition, Invited Papers Presented at
the 1974 IEEE Symposium, edited by
R. Reddy, 275-320.
Cover, T., and Thomas, J. (1991). Elements of
Information Theory. John Wiley &amp; Sons.
Croft, W.; Turtle, H.; and Lewis, D. (1991).
&amp;quot;The use of phrases and structured
queries in information retrieval.&amp;quot; In SIGIR
Forum, edited by A. Bookstein,
Y. Chiaramella, G. Salton, and
V. Raghavan, 32-45,
Damerau, F. (1964). &amp;quot;A technique for
computer detection and correction of
spelling errors.&amp;quot; Communications of the
ACM, 7(3), 171-176.
</reference>
<page confidence="0.99935">
22
</page>
<note confidence="0.983587">
Kenneth W. Church and Robert L. Mercer Introduction
</note>
<reference confidence="0.999038614754098">
deMarcken, C. (1990). &amp;quot;Parsing the LOB
corpus.&amp;quot; Association for Computational
Linguistics, 243-251.
DeRose, S. (1988). &amp;quot;Grammatical category
disambiguation by statistical
optimization.&amp;quot; Computational Linguistics,
14(1), 31-39.
Deroualt, A., and Merialdo, B. (1986).
&amp;quot;Natural language modeling for
phoneme-to-text transcription.&amp;quot; IEEE
Transactions on Pattern Analysis and Machine
Intelligence PAMI-8(6), 742-749.
Fano, R. (1961). Transmission of Information.
MIT. Press.
Firth, J. (1957). &amp;quot;A synopsis of linguistic
theory 1930-1955.&amp;quot; In Studies in Linguistic
Analysis, Philological Society, Oxford;
reprinted in Selected Papers of J. R. Firth,
edited by F. Palmer. Longman. 1968.
Ford, M.; Bresnan, J.; and Kaplan, R. (1982).
&amp;quot;A competence based theory of syntactic
closure.&amp;quot; In The Mental Representation of
Grammatical Relations, edited by
J. Bresnan. MIT Press, 727-796.
Frakes, W., and Baeza-Yates, R., eds. (1992).
Information Retrieval: Data Structures and
Algorithms. Prentice Hall.
Francis, W., and Ku&amp;ra, H. (1982).
Frequency Analysis of English Usage,
Houghton Mifflin.
Furui, S. (1989). Digital Speech Processing,
Synthesis, and Recognition. Marcel Dekker.
Garside, R.; Leech, G.; and Sampson, G.
(1987). The Computational Analysis of
English. Longman.
Good, I. (1953). &amp;quot;The population frequencies
of species and the estimation of
population parameters.&amp;quot; Biometrika, 40,
237-264.
Halliday, M. (1966). &amp;quot;Lexis as a linguistic
level,&amp;quot; In In Memory of J. R. Firth, edited by
C. Bazell, J. Catford, M. Halliday, and
R. Robins. Longman.
Hanks, P. (1990). &amp;quot;Evidence and intuition in
lexicography.&amp;quot; In Meaning and
Lexicography, edited by J. Tomaszczyk and
B. Lewandowska-Tomaszczyk, 31-41.
John Benjamins Publishing Company.
Hearst, M. (1991). &amp;quot;Toward noun homonym
disambiguation using local context in
large text corpora.&amp;quot; In Proceedings, Seventh
Annual Conference of the UW Centre for the
New OED and Text Research. University of
Waterloo, Waterloo, Ontario, 1-22.
Henisz-Dostert, B.; Ross Macdonald, R.; and
Zarechnak, M., eds. (1979). Machine
Translation. Mouton.
Hindle, D. (1989). &amp;quot;Acquiring
disambiguation rules from text.&amp;quot; ACL,
118-125.
Isabelle, P. (1992). &amp;quot;Bi-textual aids for
translators.&amp;quot; In Proceedings, Eighth Annual
Conference of the UW Centre for the New
OED and Text Research. University of
Waterloo, Waterloo, Ontario, 76-89.
Jelinek, F. (1985). &amp;quot;Self-organized language
modeling for speech recognition.&amp;quot; IBM
Report. Reprinted in W &amp; L, 450-506.
Jelinek, E, and Mercer, R. (1980).
&amp;quot;Interpolated estimation of Markov
source parameters from sparse data.&amp;quot; In
Proceedings, Workshop on Pattern Recognition
in Practice. North-Holland.
Jelinek, E, and Mercer, R. (1985).
&amp;quot;Probability distribution estimation from
sparse data.&amp;quot; IBM Technical Disclosure
Bulletin, 28,2591-2594.
Jelinek, E; Mercer, R.; and Roukos, S. (1991).
&amp;quot;Principles of lexical language modeling
for speech recognition.&amp;quot; In Advances in
Speech Signal Processing, edited by S. Furui
and M. Mohan. Marcel Dekker, 651-700.
Karlsson, E (1990). &amp;quot;Constraint grammar as
a framework for parsing running text.&amp;quot; In
Proceedings, 15th International Conference on
Computational Linguistics (COLING-90),
168-173.
Klavans, J., and Tzoukermann, E. (1990a).
&amp;quot;The BICORD system.&amp;quot; In Proceedings,
15th International Conference on
Computational Linguistics (COLING-90),
Helsinki, Finland, 174-179.
Klavans, J., and Tzoukermann, E. (1990b).
&amp;quot;Linking bilingual corpora and machine
readable dictionaries with the BICORD
system.&amp;quot; In Proceedings, Sixth Annual
Conference of the UW Centre for the New
Oxford English Dictionary and Text Research,
19-30.
Klatt, D. (1977). &amp;quot;Review of the ARPA
speech understanding project.&amp;quot; Journal of
the Acoustical Society of America. Reprinted
in W&amp; L, 554-575.
Klatt, D. (1980). &amp;quot;Scriber and lafs: Two new
approaches to speech analysis.&amp;quot; In Trends
in Speech Recognition, edited by W. Lea.
Prentice-Hall.
Kukich, K. (1992). &amp;quot;Techniques for
automatically correcting words in text.&amp;quot;
ACM Computing Surveys, 24(4), 377-439.
Kupiec, J. (1989). &amp;quot;Augmenting a hidden
Markov model for phrase-dependent
word tagging.&amp;quot; DARPA Speech and Natural
Language Workshop, San Mateo, CA, 92-98.
Morgan Kaufmann.
Kupiec, J. (1992). &amp;quot;Robust part-of-speech
tagging using a hidden Markov model.&amp;quot;
Computer Speech and Language, 6, 225-242.
Lan, K., and Young, S. (1991). &amp;quot;Applications
of stochastic context-free grammars using
the inside-outside algorithm.&amp;quot; Computer
Speech and Language, 237-258.
</reference>
<page confidence="0.930812">
23
</page>
<reference confidence="0.994355921568627">
Computational Linguistics Volume 19, Number 1
Leech, G.; Garside, R.; and Atwell, E. (1983).
&amp;quot;The automatic grammatical tagging of
the LOB corpus.&amp;quot; ICANIE News 7,13-33.
Leonard, R. (1984). &amp;quot;A database for
speaker-independent digit recognition.&amp;quot;
Proceedings of the IEEE International
Conference on Acoustics, Speech and Signal
Processing (ICASSP), 3,1-4.
Liberman, M., and Church, K. (1991). &amp;quot;Text
analysis and word pronunciation in
text-to-speech synthesis.&amp;quot; In Advances in
Speech Signal Processing, edited by S. Furui
and M. Mohan. Marcel Dekker, 791-832.
Merialdo, B. (1991). &amp;quot;Tagging text with a
probabilistic model.&amp;quot; IEEE International
Conference on Acoustics, Speech and Signal
Processing (ICASSP), 809-812.
Meyer, D.; Schvaneveldt, R.; and Ruddy, M.
(1975). &amp;quot;Loci of contextual effects on
visual word-recognition.&amp;quot; In Attention and
Performance V, edited by P. Rabbitt and
S. Dornie. Academic Press, 98-116.
Minsky, M., and Papert, S. (1969).
Perceptrons; An Introduction to
Computational Geometry. MIT Press.
Mish, F., ed. (1983). Webster&apos;s Ninth New
Collegiate Dictionary. Merriam, Webster.
Mosteller, F., and Tukey, J. (1977). Data
Analysis and Regression. Addison-Wesley.
Mosteller, Fredrick, and Wallace, David
(1964). Inference and Disputed Authorship:
The Federalist. Addison-Wesley.
Murray, K. (1977). Caught in the Web of Words:
James Murray and the Oxford English
Dictionary. Yale University Press.
Palermo, D., and Jenkins, J. (1964). Word
Association Norms. University of
Minnesota Press.
Price, P.; Fisher, W.; Bernstein, J.; and Pallett,
D. (1988). &amp;quot;The DARPA 1000-word
resource management database for
continuous speech recognition.&amp;quot; In
Proceedings, IEEE International Conference on
Acoustics, Speech and Signal Processing
(ICASSP), 1, 651-654.
Pustejovsky, J.; Berger, S.; and Anick, P.
(1993). &amp;quot;Lexical semantic techniques for
corpus analysis.&amp;quot; Computational
Linguistics, 19(2). In press.
Rabiner, L. (1989). &amp;quot;A tutorial on hidden
Markov models and selected applications
in speech recognition.&amp;quot; In Proceedings,
IEEE, 77(2), 257-286. Reprinted in W &amp; L,
267-296.
Salton, G. (1989). Automatic Text Processing.
Addison-Wesley.
Salton, G.; Zhao, Z.; and Buckley, C. (1990).
&amp;quot;A simple syntactic approach for the
generation of indexing phrases.&amp;quot;
Technical Report 90-1137, Department of
Computer Science, Cornell University.
Sato, S., and Nagao, M. (1990). &amp;quot;Towards
memory based translation.&amp;quot; In
Proceedings, 15th International Conference on
Computational Linguistics (COLING-90),
247-252.
Shannon, C. (1948). &amp;quot;The mathematical
theory of communication.&amp;quot; Bell System
Technical Journal, 27,398-403.
Shannon, C. (1951). &amp;quot;Prediction and entropy
of printed English.&amp;quot; Bell Systems Technical
Journal, 30,50-64.
Sinclair, J.; Hanks, P.; Fox, G.; Moon, R.; and
Stock, P., eds. (1987). Collins COBUILD
English Language Dictionary. Collins.
Sinclair, J., ed. (1987). Looking Up: An Account
of the COBUILD Project in Lexical
Computing. Collins.
Tukey, J. (1977). Exploratory Data Analysis.
Addison-Wesley.
Voutilainen, A.; Heikkila, J.; and Anttila, A.
(1992). &amp;quot;Constraint grammar of English:
A performance-oriented introduction.&amp;quot;
Publication No. 21, University of
Helsinki, Department of Linguistics,
Helsinki, Finland.
Waibel, A., and Lee, K., eds. (1990). Readings
in Speech Recognition. Morgan Kaufmann.
Warwick-Armstrong, S., and Russell, G.
(1990). &amp;quot;Bilingual concordancing and
bilingual lexicography.&amp;quot; Euralex 1990.
Weaver, W. (1949). &amp;quot;Translation.&amp;quot;
Reproduced in Machine Translation of
Languages, edited in 1955 by W. Locke
and A. Booth. MIT Press, 15-23.
Welch, T. (1984). &amp;quot;A technique for high
performance data compression.&amp;quot;
Computer, 17(6), 8-19.
Woods, W. (1970). &amp;quot;Transition networks for
natural language analysis.&amp;quot; CACM,
13(10), 591-606.
</reference>
<page confidence="0.999178">
24
</page>
</variant>
</algorithm>

<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Ayuso</author>
<author>R Bobrow</author>
<author>D MacLaughlin</author>
<author>M Meteer</author>
<author>L Ramshaw</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>Toward understanding text with a very large vocabulary.&amp;quot; DARPA Speech and Natural Language Workshop.</title>
<date>1990</date>
<pages>354--358</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, CA,</location>
<contexts>
<context position="22660" citStr="Ayuso et al. (1990)" startWordPosition="3537" endWordPosition="3540">), almost every aspect of most speech recognition systems is dominated by probabilistic models with parameters determined from data. 2. Part-of-Speech Tagging Many of the very same methods are being applied to problems in natural language processing by many of the very same researchers. As a result, the empirical approach has been adopted by almost all contemporary part-of-speech programs: Bahl and Mercer (1976), Leech, Garside, and Atwell (1983), Jelinek (1985), Deroualt and Merialdo (1986), Garside, Leech, and Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al. (1990), deMarcken (1990), Karlsson (1990), Boggess, Agarwal, and Davis (1991), Merialdo (1991), and Voutilainen, Heikkila, and Anttila (1992). These programs input a sequence of words, e.g., The chair will table the motion, and output a sequence of part-of-speech tags, e.g., art noun modal verb art noun. Most of these programs correctly tag at least 95% of the words, with practically no restrictions on the input text, and with very modest space and time requirements. Perhaps the most important indication of success is that many of these statistical tagging programs are now being used on large volume</context>
</contexts>
<marker>Ayuso, Bobrow, MacLaughlin, Meteer, Ramshaw, Schwartz, Weischedel, 1990</marker>
<rawString>Ayuso, D.; Bobrow, R.; MacLaughlin, D.; Meteer, M.; Ramshaw, L.; Schwartz, R.; and Weischedel, R. (1990). &amp;quot;Toward understanding text with a very large vocabulary.&amp;quot; DARPA Speech and Natural Language Workshop. San Mateo, CA, 354-358. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bahl</author>
<author>J Baker</author>
<author>P Cohen</author>
<author>N Dixon</author>
<author>F Jelinek</author>
<author>R Mercer</author>
<author>H Silverman</author>
</authors>
<title>Preliminary results in the performance of a system for the automatic recognition of continuous speech.&amp;quot;</title>
<date>1975</date>
<tech>IBM Technical Report #RC 5654.</tech>
<contexts>
<context position="18143" citStr="Bahl et al. 1975" startWordPosition="2826" endWordPosition="2829">hidden. In practice, the Forward—Backward algorithm is often used to estimate the values of the transition and emission parameters on the basis of corpus evidence. See Furui (1989; Appendix D.3, pp. 343-347) for a brief description of the 5 Computational Linguistics Volume 19, Number 1 Table 1 Examples of channel confusions in different applications. Application Input Output Speech writer rider Recognition here hear Optical all all (A-one-L) Character of of Recognition form farm Spelling government goverment Correction occurred occured commercial commerical Table 2 Performance after training (Bahl et al. 1975) Training Set Size Test Sentences Correctly Decoded Decoding Problems 0 2/10 8/10 200 77/100 3/100 400 80/100 2/100 600 85/100 1/100 800 82/100 3/100 1070 83/100 3/100 Forward—Backward algorithm, and (Rabiner 1989) for a longer tutorial on HMMs. The general procedure, of which the Forward—Backward algorithm is a special case, was first published and shown to converge by Baum (1972). The first level of the Raleigh system converted spelling to phonemic base forms, rather like a dictionary; the second level dealt with the problems of allophonic variation mentioned above; the third level modeled t</context>
</contexts>
<marker>Bahl, Baker, Cohen, Dixon, Jelinek, Mercer, Silverman, 1975</marker>
<rawString>Bahl, L.; Baker, J.; Cohen, P.; Dixon, N.; Jelinek, F.; Mercer, R.; and Silverman, H. (1975). &amp;quot;Preliminary results in the performance of a system for the automatic recognition of continuous speech.&amp;quot; IBM Technical Report #RC 5654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bahl</author>
<author>J Baker</author>
<author>P Cohen</author>
<author>F Jelinek</author>
<author>B Lewis</author>
<author>R Mercer</author>
</authors>
<title>Recognition of a continuously read natural corpus.&amp;quot;</title>
<date>1978</date>
<booktitle>In Proceedings, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).</booktitle>
<contexts>
<context position="21372" citStr="Bahl et al. 1978" startWordPosition="3339" endWordPosition="3342">led system was still able to recognize 43% of 100 test sentences correctly as compared with 35% for the original Raleigh system. These results firmly established the importance of a coherent, probabilistic approach to speech recognition and the importance of data for estimating the parameters of a probabilistic model. One by one, pieces of the system that had been assiduously assembled by speech experts yielded to probabilistic modeling. Even the elaborate set of hand-tuned rules for segmenting the frequency bank outputs into phoneme-sized segments would be replaced with training (Bakis 1976; Bahl et al. 1978). By the summer of 1977, performance had reached 95% correct by sentence and 99.4% correct by word, a considerable improvement over the same system with hand-tuned segmentation rules (73% by sentence and 95% by word). Progress in speech recognition at Yorktown and almost everywhere else as well has continued along the lines drawn in these early experiments. As computers increased in power, ever greater tracts of the heuristic wasteland opened up for colonization by probabilistic models. As greater quantities of recorded data became available, these areas were tamed by automatic training techni</context>
</contexts>
<marker>Bahl, Baker, Cohen, Jelinek, Lewis, Mercer, 1978</marker>
<rawString>Bahl, L.; Baker, J.; Cohen, P.; Jelinek, F.; Lewis, B.; and Mercer, R. (1978). &amp;quot;Recognition of a continuously read natural corpus.&amp;quot; In Proceedings, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bahl</author>
<author>R Mercer</author>
</authors>
<title>Part of speech assignment by a statistical decision algorithm.&amp;quot;</title>
<date>1976</date>
<booktitle>In Abstracts of Papers from the International Symposium on Information Theory.</booktitle>
<contexts>
<context position="22456" citStr="Bahl and Mercer (1976)" startWordPosition="3507" endWordPosition="3511">tion by probabilistic models. As greater quantities of recorded data became available, these areas were tamed by automatic training techniques. Today, as indicated in the introduction of Waibel and Lee (1990), almost every aspect of most speech recognition systems is dominated by probabilistic models with parameters determined from data. 2. Part-of-Speech Tagging Many of the very same methods are being applied to problems in natural language processing by many of the very same researchers. As a result, the empirical approach has been adopted by almost all contemporary part-of-speech programs: Bahl and Mercer (1976), Leech, Garside, and Atwell (1983), Jelinek (1985), Deroualt and Merialdo (1986), Garside, Leech, and Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al. (1990), deMarcken (1990), Karlsson (1990), Boggess, Agarwal, and Davis (1991), Merialdo (1991), and Voutilainen, Heikkila, and Anttila (1992). These programs input a sequence of words, e.g., The chair will table the motion, and output a sequence of part-of-speech tags, e.g., art noun modal verb art noun. Most of these programs correctly tag at least 95% of the words, with practically no restrictions</context>
</contexts>
<marker>Bahl, Mercer, 1976</marker>
<rawString>Bahl, L., and Mercer, R. (1976). &amp;quot;Part of speech assignment by a statistical decision algorithm.&amp;quot; In Abstracts of Papers from the International Symposium on Information Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Baker</author>
</authors>
<title>Trainable grammars for speech recognition.&amp;quot;</title>
<date>1979</date>
<booktitle>In Speech Communication Papers for the 97th Meeting of the Acoustical Society of America, edited by Klatt and Wolf,</booktitle>
<pages>547--550</pages>
<contexts>
<context position="39499" citStr="Baker 1979" startWordPosition="6360" endWordPosition="6361">able than content words like resolve and important, which are longer. This is convenient for speech recognition because it means that the language model provides more powerful constraints just when the acoustic model is having the toughest time. One suspects that this is not an accident, but rather a natural result of the evolution of speech to fill the human needs for reliable communication in the presence of noise. . The ideal NLP model would combine the strengths of both the competence approximation and the n-gram approximation. One possible solution might be the Inside— Outside algorithm (Baker 1979; Lan i and Young 1991), a generalization of the Forward— Backward algorithm that estimates the parameters of a hidden stochastic context-free grammar, rather than a hidden Markov model. Four alternatives are proposed in these special issues: (1) Brent (1993), (2) Briscoe and Carroll (this issue), (3) Hindle and Rooth (this issue), and (4) Weischedel et al. (1993). Briscoe and Carroll&apos;s contribution is very 12 Kenneth W. Church and Robert L. Mercer Introduction much in the spirit of the Inside—Outside algorithm, whereas Hindle and Rooth&apos;s contribution, for example, takes an approach that is mu</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>Baker, J. (1979). &amp;quot;Trainable grammars for speech recognition.&amp;quot; In Speech Communication Papers for the 97th Meeting of the Acoustical Society of America, edited by Klatt and Wolf, 547-550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bakis</author>
</authors>
<title>Continuous speech recognition via centisecond acoustic states.&amp;quot;</title>
<date>1976</date>
<booktitle>In Proceedings of 91st Meeting of the Acoustic</booktitle>
<publisher>Society of America.</publisher>
<contexts>
<context position="21353" citStr="Bakis 1976" startWordPosition="3337" endWordPosition="3338">. This crippled system was still able to recognize 43% of 100 test sentences correctly as compared with 35% for the original Raleigh system. These results firmly established the importance of a coherent, probabilistic approach to speech recognition and the importance of data for estimating the parameters of a probabilistic model. One by one, pieces of the system that had been assiduously assembled by speech experts yielded to probabilistic modeling. Even the elaborate set of hand-tuned rules for segmenting the frequency bank outputs into phoneme-sized segments would be replaced with training (Bakis 1976; Bahl et al. 1978). By the summer of 1977, performance had reached 95% correct by sentence and 99.4% correct by word, a considerable improvement over the same system with hand-tuned segmentation rules (73% by sentence and 95% by word). Progress in speech recognition at Yorktown and almost everywhere else as well has continued along the lines drawn in these early experiments. As computers increased in power, ever greater tracts of the heuristic wasteland opened up for colonization by probabilistic models. As greater quantities of recorded data became available, these areas were tamed by automa</context>
</contexts>
<marker>Bakis, 1976</marker>
<rawString>Bakis, R. (1976). &amp;quot;Continuous speech recognition via centisecond acoustic states.&amp;quot; In Proceedings of 91st Meeting of the Acoustic Society of America.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Baum</author>
</authors>
<title>An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process.&amp;quot;</title>
<date>1972</date>
<journal>Inequalities,</journal>
<pages>3--1</pages>
<contexts>
<context position="18527" citStr="Baum (1972)" startWordPosition="2888" endWordPosition="2889">r rider Recognition here hear Optical all all (A-one-L) Character of of Recognition form farm Spelling government goverment Correction occurred occured commercial commerical Table 2 Performance after training (Bahl et al. 1975) Training Set Size Test Sentences Correctly Decoded Decoding Problems 0 2/10 8/10 200 77/100 3/100 400 80/100 2/100 600 85/100 1/100 800 82/100 3/100 1070 83/100 3/100 Forward—Backward algorithm, and (Rabiner 1989) for a longer tutorial on HMMs. The general procedure, of which the Forward—Backward algorithm is a special case, was first published and shown to converge by Baum (1972). The first level of the Raleigh system converted spelling to phonemic base forms, rather like a dictionary; the second level dealt with the problems of allophonic variation mentioned above; the third level modeled the front end. At first, the values of the parameters in these HMMs were carefully constructed by hand, but eventually they would all be replaced with estimates obtained by training on real data using statistical estimation procedures such as the Forward—Backward algorithm. The advantages of training are apparent in Table 2. Note the astounding improvement in performance. Despite a </context>
</contexts>
<marker>Baum, 1972</marker>
<rawString>Baum, L. (1972). &amp;quot;An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process.&amp;quot; Inequalities, 3,1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Bell</author>
<author>J Cleary</author>
<author>I Witten</author>
</authors>
<title>Text Compression.</title>
<date>1990</date>
<publisher>Prentice Hall.</publisher>
<marker>Bell, Cleary, Witten, 1990</marker>
<rawString>Bell, T.; Cleary, J.; and Witten, I. (1990). Text Compression. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Biber</author>
</authors>
<title>Representativeness in corpus design.&amp;quot;</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<note>In press.</note>
<contexts>
<context position="52867" citStr="Biber (1993)" startWordPosition="8543" endWordPosition="8544">Kenneth W. Church and Robert L. Mercer Introduction 6. Monolingual Lexicography, Machine-Readable Dictionaries (MRDs), and Computational Lexicons There has been a long tradition of empiricist approaches in lexicography, both bilingual and monolingual, dating back to Johnson and Murray. As corpus data and machinereadable dictionaries (MRDs) become more and more available, it is becoming easier to compile lexicons for computers and dictionaries for people. This is a particularly exciting area in computational linguistics as evidenced by the large number of contributions in these special issues: Biber (1993), Brent (1993), Hindle and Rooth (this issue), Pustejovsky et al. (1993), and Smadja (this issue). Starting with the COBUILD dictionary (Sinclair et al. 1987), it is now becoming more and more common to find lexicographers working directly with corpus data. Sinclair makes an excellent case for the use of corpus evidence in the preface to the COBUILD dictionary: For the first time, a dictionary has been compiled by the thorough examination of a representative group of English texts, spoken and written, running to many millions of words. This means that in addition to all the tools of the conven</context>
<context position="56823" citStr="Biber (1993)" startWordPosition="9200" endWordPosition="9201"> side with Quirk&apos;s position, Sinclair was able to point out a number of serious problems with the balancing position. It may not be possible to properly balance a corpus. And moreover, if we insist on throwing out idiosyncratic data, we may find it very difficult to collect any data at all, since all corpora have their quirks. In some sense, the question comes down to a tradeoff between quality and quantity. American industrial laboratories (e.g., IBM, AT&amp;T) tend to favor quantity, whereas the BNC, NERC, and many dictionary publishers, especially in Europe, tend to favor quality. The paper by Biber (1993) argues for quality, suggesting that we ought to use the same kinds of sampling methods that statisticians use when studying the economy or predicting the results of an election. Poor sampling methods, inappropriate assumptions, and other statistical errors can produce misleading results: &amp;quot;There are lies, damn lies, and statistics.&amp;quot; Unfortunately, sampling methods can be expensive; it is not clear whether we can justify the expense for the kinds of applications that we have in mind. Table 7 might lend some support for the quantity position for Murray&apos;s example of imaginable. Note that there is</context>
</contexts>
<marker>Biber, 1993</marker>
<rawString>Biber, D. (1993). &amp;quot;Representativeness in corpus design.&amp;quot; Computational Linguistics, 19(2). In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Boggess</author>
<author>R Agarwal</author>
<author>R Davis</author>
</authors>
<title>Disambiguation of prepositional phrases in automatically labelled technical text.&amp;quot;</title>
<date>1991</date>
<pages>155--159</pages>
<publisher>AAAI,</publisher>
<marker>Boggess, Agarwal, Davis, 1991</marker>
<rawString>Boggess, L.; Agarwal, R.; and Davis, R. (1991). &amp;quot;Disambiguation of prepositional phrases in automatically labelled technical text.&amp;quot; AAAI, 155-159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Boguraev</author>
</authors>
<title>Looking Up: an account of the COBUILD project in lexical computing.&amp;quot;</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>3</issue>
<pages>184--185</pages>
<contexts>
<context position="53817" citStr="Boguraev (1990)" startWordPosition="8694" endWordPosition="8695">e to the COBUILD dictionary: For the first time, a dictionary has been compiled by the thorough examination of a representative group of English texts, spoken and written, running to many millions of words. This means that in addition to all the tools of the conventional dictionary makers—wide reading and experience of English, other dictionaries and of course eyes and ears—this dictionary is based on hard, measurable evidence. (Sinclair et al. 1987; p. xv) The experience of writing the COBUILD dictionary is documented in Sinclair (1987), a collection of articles from the COBUILD project; see Boguraev (1990) for a strong positive review of this collection. At the time, the corpus-based approach to lexicography was considered pioneering, even somewhat controversial; today, quite a number of the major lexicography houses are collecting large amounts of corpus data. The traditional alternative to corpora are citation indexes, boxes of interesting citations collected on index cards by large numbers of human readers. Unfortunately, citation indexes tend to be a bit like butterfly collections, full of rare and unusual specimens, but severely lacking in ordinary, garden-variety moths. Murray, the editor</context>
</contexts>
<marker>Boguraev, 1990</marker>
<rawString>Boguraev, B. (1990). &amp;quot;Looking Up: an account of the COBUILD project in lexical computing.&amp;quot; Computational Linguistics, 16(3), 184-185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brent</author>
</authors>
<title>Robust acquisition of subcategorization features from arbitrary text: syntactic knowledge meets unsupervised learning.&amp;quot;</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<note>In press.</note>
<contexts>
<context position="39758" citStr="Brent (1993)" startWordPosition="6399" endWordPosition="6400">s that this is not an accident, but rather a natural result of the evolution of speech to fill the human needs for reliable communication in the presence of noise. . The ideal NLP model would combine the strengths of both the competence approximation and the n-gram approximation. One possible solution might be the Inside— Outside algorithm (Baker 1979; Lan i and Young 1991), a generalization of the Forward— Backward algorithm that estimates the parameters of a hidden stochastic context-free grammar, rather than a hidden Markov model. Four alternatives are proposed in these special issues: (1) Brent (1993), (2) Briscoe and Carroll (this issue), (3) Hindle and Rooth (this issue), and (4) Weischedel et al. (1993). Briscoe and Carroll&apos;s contribution is very 12 Kenneth W. Church and Robert L. Mercer Introduction much in the spirit of the Inside—Outside algorithm, whereas Hindle and Rooth&apos;s contribution, for example, takes an approach that is much closer to the concerns of lexicography, and makes use of preferences involving words, rather than preferences that ignore words and focus exclusively on syntactic structures. Hindle and Rooth show how co-occurrence statistics can be used to improve the per</context>
<context position="52881" citStr="Brent (1993)" startWordPosition="8545" endWordPosition="8546">rch and Robert L. Mercer Introduction 6. Monolingual Lexicography, Machine-Readable Dictionaries (MRDs), and Computational Lexicons There has been a long tradition of empiricist approaches in lexicography, both bilingual and monolingual, dating back to Johnson and Murray. As corpus data and machinereadable dictionaries (MRDs) become more and more available, it is becoming easier to compile lexicons for computers and dictionaries for people. This is a particularly exciting area in computational linguistics as evidenced by the large number of contributions in these special issues: Biber (1993), Brent (1993), Hindle and Rooth (this issue), Pustejovsky et al. (1993), and Smadja (this issue). Starting with the COBUILD dictionary (Sinclair et al. 1987), it is now becoming more and more common to find lexicographers working directly with corpus data. Sinclair makes an excellent case for the use of corpus evidence in the preface to the COBUILD dictionary: For the first time, a dictionary has been compiled by the thorough examination of a representative group of English texts, spoken and written, running to many millions of words. This means that in addition to all the tools of the conventional diction</context>
<context position="63319" citStr="Brent (1993)" startWordPosition="10281" endWordPosition="10282">h statistics are often used to test a particular hypothesis as we have just seen, statistics can also be used to explore the space of possible hypotheses, or to discover new hypotheses (supervised/unsupervised learning/training). See Tukey (1977) and Mosteller and Tukey (1977) for two textbooks on Exploratory Data Analysis (EDA), and Jelinek (1985) for a very nice review paper on self-organizing statistics. Both the exploratory and self-organizing views are represented in these special issues. Pustejovsky et al. (1993) use an EDA approach to investigate certain questions in lexical semantics. Brent (1993), in contrast, adopts a self-organizing approach to identify subcategorization features. Table 9 shows how the t-score can be used in an exploratory mode to extract large numbers of words from the Associated Press (AP) news that co-occur more often with strong than with powerful, and vice versa. It is an interesting question whether collocations are simply idiosyncratic as Halliday and many others have generally assumed (see Smadja [this issue% or whether there might be some general principles that could account for many of the cases. After looking at Table 9, Hanks, a lexicographer and one of</context>
</contexts>
<marker>Brent, 1993</marker>
<rawString>Brent, M. (1993). &amp;quot;Robust acquisition of subcategorization features from arbitrary text: syntactic knowledge meets unsupervised learning.&amp;quot; Computational Linguistics, 19(2). In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>J Cocke</author>
<author>Della Pietra</author>
<author>Della Pietra S</author>
<author>V Jelinek</author>
<author>F Lafferty</author>
<author>J Mercer</author>
<author>R</author>
<author>P Rossin</author>
</authors>
<title>A statistical approach to machine translation.&amp;quot;</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<pages>79--85</pages>
<contexts>
<context position="49292" citStr="Brown et al. (1990)" startWordPosition="7969" endWordPosition="7972">uted over a 27-character alphabet. 3 Lan and Young actually looked at another task involving phonotactic structure where there is also good reason to believe that SCFGs might be able to capture crucial linguistic constraints that might be missed by simpler HMMs. 15 Computational Linguistics Volume 19, Number 1 has tended to favor rationalism, though there are some important exceptions, such as example-based MT (Sato and Nagao 1990). The issue remains as controversial as ever, as evidenced by the lively debate on rationalism versus empiricism at TMI-92, a recent conference on MT.&apos; The paper by Brown et al. (1990) revives Weaver&apos;s information theoretic approach to MT. It requires a bit more squeezing and twisting to fit machine translation into the noisy channel mold: to translate, for example, from French to English, one imagines that the native speaker of French has thought up what he or she wants to say in English and then translates mentally into French before actually saying it. The task of the translation system is to recover the original English, E, from the observed French, F. While this may seem a bit far-fetched, it differs little in principle from using English as an interlingua or as a mean</context>
</contexts>
<marker>Brown, Cocke, Pietra, S, Jelinek, Lafferty, Mercer, R, Rossin, 1990</marker>
<rawString>Brown, P.; Cocke, J.; Della Pietra, S.; Della Pietra, V.; Jelinek, F.; Lafferty, J.; Mercer, R.; and Rossin, P. (1990). &amp;quot;A statistical approach to machine translation.&amp;quot; Computational Linguistics, 16(2), 79-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>Della Pietra</author>
<author>Della Pietra S</author>
<author>V Lai</author>
<author>J</author>
<author>R Mercer</author>
</authors>
<title>An estimate of an upper bound for the entropy of English.&amp;quot;</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>1</issue>
<pages>31--40</pages>
<contexts>
<context position="43873" citStr="Brown et al. 1992" startWordPosition="7090" endWordPosition="7093">leary, and Witten 1990; Section 5.1.2). Other codes, such as Lempel—Ziv (Welch 1984; Bell, Cleary, and Witten, Chapters 8-9) and n-gram models on words, achieve even better compression by taking advantage of context, though none of these codes seem to perform as well as people do in predicting the next letter (Shannon 1951). 13 Computational Linguistics Volume 19, Number 1 Table 5 Cross entropy of various language models. Model Bits / Character ASCII 8 Huffman code each char 5 Lempel-Ziv (Unix Tm compress) 4.43 Unigram (Huffman code each word) 2.1 (Brown, personal communication) Trigram 1.76 (Brown et al. 1992) Human Performance 1.25 (Shannon 1951) The cross entropy, H, of a code and a source is given by: H(source, code) = - EE Pr(s,h I source) log2 Pr(s I h, code) s h where Pr(s,h I source) is the joint probability of a symbol s following a history h given the source. Pr(s I h, code) is the conditional probability of s given the history (context) h and the code. In the special case of ASCII, where Pr(s h, ASCII) = 1/256, we can actually carry out the indicated sum, and find, not surprisingly, that ASCII requires 8 bits per character: 256 H(source, ASCII) = - E 256 &apos;°g2 g 256 - s=1 In more difficult</context>
<context position="48561" citStr="Brown et al. (1992)" startWordPosition="7854" endWordPosition="7857"> the two approaches. 5. Machine Translation and Bilingual Lexicography Is machine translation (MT) more suitable for rationalism or empiricism? Both approaches have been investigated. Weaver (1949) was the first to propose an information theoretic approach to MT. The empirical approach was also practiced at Georgetown during the 1950s and 1960s (Henisz-Dostert, Ross Macdonald, and Zarechnak 1979) in a system that eventually became known as SYSTRAN. Recently, most work in MT 2 In fact, the trigram model might be even better than suggested in Table 5, since the estimate for the trigram model in Brown et al. (1992) is computed over a 256-character alphabet, whereas the estimate for human performance in Shannan (1951) is computed over a 27-character alphabet. 3 Lan and Young actually looked at another task involving phonotactic structure where there is also good reason to believe that SCFGs might be able to capture crucial linguistic constraints that might be missed by simpler HMMs. 15 Computational Linguistics Volume 19, Number 1 has tended to favor rationalism, though there are some important exceptions, such as example-based MT (Sato and Nagao 1990). The issue remains as controversial as ever, as evid</context>
</contexts>
<marker>Brown, Pietra, S, Lai, J, Mercer, 1992</marker>
<rawString>Brown, P.; Della Pietra, S.; Della Pietra, V.; Lai, J.; and Mercer, R. (1992). &amp;quot;An estimate of an upper bound for the entropy of English.&amp;quot; Computational Linguistics, 18(1), 31-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>Della Pietra</author>
<author>Della Pietra S</author>
<author>V Mercer</author>
<author>R</author>
</authors>
<title>The mathematics of machine translation.&amp;quot;</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<note>In press.</note>
<contexts>
<context position="50541" citStr="Brown et al. (1993)" startWordPosition="8180" endWordPosition="8183"> —&gt; Noisy Channel —&gt; F As before, one minimizes one&apos;s chance of error by choosing E according to the formula: = argmaxPr(E) Pr(F E) As before, the parameters of the model are estimated by computing various statistics over large samples of text. The prior probability, Pr(E), is estimated in exactly the same way as discussed above for the speech recognition application. The parameters of the channel model, Pr(F I E), are estimated from a parallel text that has been aligned by an automatic procedure that figures out which parts of the source text correspond to which parts of the target text. See Brown et al. (1993) for more details on the estimation of the parameters. The information theoretic approach to MT may fail for reasons advanced by Chomsky and others in the 1950s. But regardless of its ultimate success or failure, there is a growing community of researchers in corpus-based linguistics who believe that it will produce a number of lexical resources that may be of great value. In particular, there has been quite a bit of discussion of bilingual concordances recently (e.g., Klavans and Tzoukermann 1990a, 1990b; Church and Gale 1991), including the 1990 and 1991 lexicography conferences sponsored by</context>
</contexts>
<marker>Brown, Pietra, S, Mercer, R, 1993</marker>
<rawString>Brown, P.; Della Pietra, S.; Della Pietra, V.; Mercer, R. (1993). &amp;quot;The mathematics of machine translation.&amp;quot; Computational Linguistics, 19(2). In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Syntactic Structures.</title>
<date>1957</date>
<publisher>Mouton.</publisher>
<contexts>
<context position="995" citStr="Chomsky 1957" startWordPosition="153" endWordPosition="154"> psychology (behaviorism) to electrical engineering (information theory). At that time, it was common practice in linguistics to classify words not only on the basis of their meanings but also on the basis of their cooccurrence with other words. Firth, a leading figure in British linguistics during the 1950s, summarized the approach with the memorable line: &amp;quot;You shall know a word by the company it keeps&amp;quot; (Firth 1957). Regrettably, interest in empiricism faded in the late 1950s and early 1960s with a number of significant events including Chomsky&apos;s criticism of n-grams in Syntactic Structures (Chomsky 1957) and Minsky and Papert&apos;s criticism of neural networks in Perceptrons (Minsky and Papert 1969). Perhaps the most immediate reason for this empirical renaissance is the availability of massive quantities of data: more text is available than ever before. Just ten years ago, the one-million word Brown Corpus (Francis and Ku&apos;&amp;ra, 1982) was considered large, but even then, there were much larger corpora such as the Birmingham Corpus (Sinclair et al. 1987; Sinclair 1987). Today, many locations have samples of text running into the hundreds of millions or even billions of words. Collections of this ma</context>
<context position="28625" citStr="Chomsky 1957" startWordPosition="4514" endWordPosition="4515">eferences, but traditional parsers don&apos;t, and consequently run into serious difficulties. Attempts to eliminate unwanted tags on syntactic grounds have not been very successful. For example, Ilnoun see/noun a/noun bird/noun, cannot be ruled out as syntactically ill-formed, because the parser must accept sequences of four nouns in other situations: city school committee meeting. Apparently, syntactic rules are not nearly as effective as lexical preferences, at least for this application. The tradition of ignoring preferences dates back to Chomsky&apos;s introduction of the competence approximation (Chomsky 1957, pp. 15-17). Recall that Chomsky was concerned that approximations, such as Shannon&apos;s n-gram approximation, which was very much in vogue at the time, were inappropriate for his needs, and therefore, he introduced an alternative with complementary strengths and weaknesses. The competence approximation is more appropriate for modeling long-distance dependences such as agreement constraints and wh-movement, but at the cost of missing certain crucial local constraints, especially the kinds of preferences that are extremely important for part-of-speech tagging. 2.3 Using Statistics to Fit Probabil</context>
</contexts>
<marker>Chomsky, 1957</marker>
<rawString>Chomsky, N. (1957). Syntactic Structures. Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, Second Conference on Applied Natural Language Processing.</booktitle>
<pages>136--143</pages>
<location>(ACL), Austin, TX,</location>
<contexts>
<context position="22588" citStr="Church (1988)" startWordPosition="3528" endWordPosition="3529">s. Today, as indicated in the introduction of Waibel and Lee (1990), almost every aspect of most speech recognition systems is dominated by probabilistic models with parameters determined from data. 2. Part-of-Speech Tagging Many of the very same methods are being applied to problems in natural language processing by many of the very same researchers. As a result, the empirical approach has been adopted by almost all contemporary part-of-speech programs: Bahl and Mercer (1976), Leech, Garside, and Atwell (1983), Jelinek (1985), Deroualt and Merialdo (1986), Garside, Leech, and Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al. (1990), deMarcken (1990), Karlsson (1990), Boggess, Agarwal, and Davis (1991), Merialdo (1991), and Voutilainen, Heikkila, and Anttila (1992). These programs input a sequence of words, e.g., The chair will table the motion, and output a sequence of part-of-speech tags, e.g., art noun modal verb art noun. Most of these programs correctly tag at least 95% of the words, with practically no restrictions on the input text, and with very modest space and time requirements. Perhaps the most important indication of success is that many </context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Church, K. (1988). &amp;quot;A stochastic parts program and noun phrase parser for unrestricted text.&amp;quot; In Proceedings, Second Conference on Applied Natural Language Processing. (ACL), Austin, TX, 136-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>P Hanks</author>
<author>D Hindle</author>
<author>W Gale</author>
</authors>
<title>Using statistics in lexical analysis.&amp;quot; In Lexical Acquisition: Using On-Line Resources to Build a Lexicon, edited by Zernik. Lawrence Erlbaum,</title>
<date>1991</date>
<pages>115--164</pages>
<contexts>
<context position="63955" citStr="Church et al. (1991)" startWordPosition="10384" endWordPosition="10387">dopts a self-organizing approach to identify subcategorization features. Table 9 shows how the t-score can be used in an exploratory mode to extract large numbers of words from the Associated Press (AP) news that co-occur more often with strong than with powerful, and vice versa. It is an interesting question whether collocations are simply idiosyncratic as Halliday and many others have generally assumed (see Smadja [this issue% or whether there might be some general principles that could account for many of the cases. After looking at Table 9, Hanks, a lexicographer and one of the authors of Church et al. (1991), hypothesized that strong is an intrinsic quality whereas powerful is an extrinsic one. Thus, for example, any worthwhile politician or cause can expect strong supporters, who are enthusiastic, convinced, vociferous, etc., but far more valuable are powerful supporters, who will bring others with them. They are also, according to the AP news, much rarer—or at any rate, much less often mentioned. This is a fascinating hypothesis that deserves further investigation. Summary statistics such as mutual information and t-scores may have an important role to play in helping lexicographers to discover</context>
<context position="65205" citStr="Church et al. 1991" startWordPosition="10577" endWordPosition="10580">ations, though the position remains somewhat controversial. Some lexicographers prefer mutual information, some prefer t-scores, and some are unconvinced that either of them is any good. Church et al. (1991) argued that different statistics have different strengths and weaknesses, and that it requires human judgment and exploration to decide which statistic is best for a particular problem. Others, such as Jelinek (1985), would prefer a self-organizing approach, where there is no need for human judgment. 20 Kenneth W. Church and Robert L. Mercer Introduction Table 9 An example of the t-score (Church et al. 1991). strong w Strong w w t Powerful w w powerful w strong w powerful w 12.42 161 0 showing —7.44 1 56 than 11.94 175 2 support —5.60 1 32 figure 10.08 550 68 , —5.37 3 31 minority 9.97 106 0 defense —5.23 1 28 of 9.76 102 0 economy —4.91 0 24 post 9.50 97 0 demand —4.63 5 25 9.40 95 0 gains —4.35 27 36 nmeiwlitary 9.18 91 0 growth —3.89 0 15 figures 8.84 137 5 winds —3.59 6 17 presidency 8.02 83 1 opposition —3.57 27 29 political 7.78 67 0 sales —3.33 0 11 computers 7. Conclusion The flourishing renaissance of empiricism in computational linguistics grew out of the experience of the speech recogn</context>
</contexts>
<marker>Church, Hanks, Hindle, Gale, 1991</marker>
<rawString>Church, K.; Hanks, P.; Hindle, D.; and Gale, W. (1991). &amp;quot;Using statistics in lexical analysis.&amp;quot; In Lexical Acquisition: Using On-Line Resources to Build a Lexicon, edited by Zernik. Lawrence Erlbaum, 115-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>W Gale</author>
</authors>
<title>Concordances for parallel text.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, Seventh Annual Conference of the UW Centre for the New Oxford English Dictionary and Text Research.</booktitle>
<contexts>
<context position="23723" citStr="Church and Gale 1991" startWordPosition="3697" endWordPosition="3700"> and time requirements. Perhaps the most important indication of success is that many of these statistical tagging programs are now being used on large volumes of data (hundreds of millions of words of text) in a number of different application areas including speech synthesis (Sproat, personal communication; Liberman and Church 1991), speech recognition (Jelinek 1985; Jelinek, Mercer, and Roukos 1991), information retrieval (Salton, Zhao, and Buckley 1990; Croft, Turtle, and Lewis 1991), sense disambiguation (Hearst 1991), and computational lexicography (Klavans and Tzoukermann 1990a, 1990b; Church and Gale 1991). Apparently, these programs must be addressing some important needs of the research community or else they wouldn&apos;t be as widely cited as they are. Many of the papers in this special issue refer to these taggers. 7 Computational Linguistics Volume 19, Number 1 As in speech recognition, data collection efforts have played a pivotal role in advancing data-intensive approaches to part-of-speech tagging. The Brown Corpus (Francis and Kue&apos;era 1982) and similar efforts within the ICAME community, have created invaluable opportunities. The Penn Treebank (see the paper by Marcus and Santorini 1983) i</context>
<context position="51074" citStr="Church and Gale 1991" startWordPosition="8267" endWordPosition="8270">of the source text correspond to which parts of the target text. See Brown et al. (1993) for more details on the estimation of the parameters. The information theoretic approach to MT may fail for reasons advanced by Chomsky and others in the 1950s. But regardless of its ultimate success or failure, there is a growing community of researchers in corpus-based linguistics who believe that it will produce a number of lexical resources that may be of great value. In particular, there has been quite a bit of discussion of bilingual concordances recently (e.g., Klavans and Tzoukermann 1990a, 1990b; Church and Gale 1991), including the 1990 and 1991 lexicography conferences sponsored by Oxford University Press and Waterloo University. A bilingual concordance is like a monolingual concordance except that each line in the concordance is followed by a line of text in a second language. There are also some hopes that the approach might produce tools that could be useful for human translators (Isabelle 1992). There are three papers in these special issues on aligning bilingual texts such as the Canadian Hansards (parliamentary debates) that are available in both English and French: Brown et al. (1993), Gale and Ch</context>
</contexts>
<marker>Church, Gale, 1991</marker>
<rawString>Church, K., and Gale, W. (1991). &amp;quot;Concordances for parallel text.&amp;quot; In Proceedings, Seventh Annual Conference of the UW Centre for the New Oxford English Dictionary and Text Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cohen</author>
<author>R Mercer</author>
</authors>
<title>The phonological component of an automatic speech-recognition system.&amp;quot;</title>
<date>1974</date>
<booktitle>In Speech Recognition, Invited Papers Presented at the 1974 IEEE Symposium, edited</booktitle>
<pages>275--320</pages>
<marker>Cohen, Mercer, 1974</marker>
<rawString>Cohen, P., and Mercer, R. (1974). &amp;quot;The phonological component of an automatic speech-recognition system.&amp;quot; In Speech Recognition, Invited Papers Presented at the 1974 IEEE Symposium, edited by R. Reddy, 275-320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cover</author>
<author>J Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="15047" citStr="Cover and Thomas (1991)" startWordPosition="2317" endWordPosition="2320">he early Raleigh system lacked a complete and unified probabilistic framework. In a radical departure from the prevailing attitudes of the time, the Yorktown group turned to Shannon&apos;s theory of communication in the presence of noise and recast the speech recognition problem in terms of transmission through a noisy channel. Shannon&apos;s theory of communication (Shannon 1948), also known as Information Theory, was originally developed at AT&amp;T Bell Laboratories to model communication along a noisy channel such as a telephone line. See Fano (1961) for a well-known secondary source on the subject, or Cover and Thomas (1991) or Bell, Cleary, and Witten (1990) for more recent treatments. The noisy channel paradigm can be applied to other recognition applications such as optical character recognition (OCR) and spelling correction. Imagine a noisy channel, such as a speech recognition machine that almost hears, an optical character recognition (OCR) machine that almost reads, or a typist who almost types. A sequence of good text (I) goes into the channel, and a sequence of corrupted text (0) comes out the other end. I —+ Noisy Channel --* 0 How can an automatic procedure recover the good input text, I, from the corr</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Cover, T., and Thomas, J. (1991). Elements of Information Theory. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Croft</author>
<author>H Turtle</author>
<author>D Lewis</author>
</authors>
<title>The use of phrases and structured queries in information retrieval.&amp;quot;</title>
<date>1991</date>
<booktitle>In SIGIR Forum,</booktitle>
<pages>32--45</pages>
<note>edited by</note>
<marker>Croft, Turtle, Lewis, 1991</marker>
<rawString>Croft, W.; Turtle, H.; and Lewis, D. (1991). &amp;quot;The use of phrases and structured queries in information retrieval.&amp;quot; In SIGIR Forum, edited by A. Bookstein, Y. Chiaramella, G. Salton, and V. Raghavan, 32-45,</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Damerau</author>
</authors>
<title>A technique for computer detection and correction of spelling errors.&amp;quot;</title>
<date>1964</date>
<journal>Communications of the ACM,</journal>
<volume>7</volume>
<issue>3</issue>
<pages>171--176</pages>
<contexts>
<context position="16154" citStr="Damerau 1964" startWordPosition="2507" endWordPosition="2508">nd. I —+ Noisy Channel --* 0 How can an automatic procedure recover the good input text, I, from the corrupted output, 0? In principle, one can recover the most likely input, I, by hypothesizing all possible input texts, I, and selecting the input text with the highest score, Pr(I I 0). Symbolically, I = argmaxPr(/ I 0) = argmaxPr(/) Pr(0 I I) where ARGMAX finds the argument with the maximum score. The prior probability, Pr(I), is the probability that I will be presented at the input to the channel. In speech recognition, it is the probability that the talker utters I; in spelling correction (Damerau 1964; Kukich 1992), it is the probability that the typist intends to type I. In practice, the prior probability is unavailable, and consequently, we have to make do with a model of the prior probability, such as the trigram model. The parameters of the language model are usually estimated by computing various statistics over a large sample of text. The channel probability, Pr(0 I I), is the probability that 0 will appear at the output of the channel when I is presented at the input; it is large if I is similar, in some appropriate sense, to 0, and small, otherwise. The channel probability depends </context>
<context position="66379" citStr="Damerau 1964" startWordPosition="10784" endWordPosition="10785">of the experience of the speech recognition community during the 1970s and 1980s. Many of the same statistical techniques (e.g., Shannon&apos;s Noisy Channel Model, n-gram models, hidden Markov models (HMMs), entropy (H), mutual information (I), Student&apos;s t-score) have appeared in one form or another, often first in speech, and then soon thereafter in language. Many of the same researchers have applied these methods to a variety of application areas ranging from language modeling for noisy channel applications (e.g., speech recognition, optical character recognition [OCR], and spelling correction [Damerau 1964; Kukich 1992]), to part-of-speech tagging, parsing, translation, lexicography, text compression (Bell, Cleary, and Witten 1990) and information retrieval (IR) (Salton 1989; Frakes and Baeza-Yates 1992). Empiricism is, of course, a very old tradition. Back in the 1950s and 1960s, long before the speech work of the 1970s and 1980s, there was Skinner&apos;s Behaviorism in Psychology, Shannon&apos;s Information Theory in Electrical Engineering, and Harris&apos; Distributional Hypothesis in American Linguistics and the Firthian approach in British Linguistics (&amp;quot;You shall know a word by the company it keeps&amp;quot;). It</context>
</contexts>
<marker>Damerau, 1964</marker>
<rawString>Damerau, F. (1964). &amp;quot;A technique for computer detection and correction of spelling errors.&amp;quot; Communications of the ACM, 7(3), 171-176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C deMarcken</author>
</authors>
<title>Parsing the LOB corpus.&amp;quot;</title>
<date>1990</date>
<journal>Association for Computational Linguistics,</journal>
<pages>243--251</pages>
<contexts>
<context position="22678" citStr="deMarcken (1990)" startWordPosition="3541" endWordPosition="3542">t of most speech recognition systems is dominated by probabilistic models with parameters determined from data. 2. Part-of-Speech Tagging Many of the very same methods are being applied to problems in natural language processing by many of the very same researchers. As a result, the empirical approach has been adopted by almost all contemporary part-of-speech programs: Bahl and Mercer (1976), Leech, Garside, and Atwell (1983), Jelinek (1985), Deroualt and Merialdo (1986), Garside, Leech, and Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al. (1990), deMarcken (1990), Karlsson (1990), Boggess, Agarwal, and Davis (1991), Merialdo (1991), and Voutilainen, Heikkila, and Anttila (1992). These programs input a sequence of words, e.g., The chair will table the motion, and output a sequence of part-of-speech tags, e.g., art noun modal verb art noun. Most of these programs correctly tag at least 95% of the words, with practically no restrictions on the input text, and with very modest space and time requirements. Perhaps the most important indication of success is that many of these statistical tagging programs are now being used on large volumes of data (hundred</context>
</contexts>
<marker>deMarcken, 1990</marker>
<rawString>deMarcken, C. (1990). &amp;quot;Parsing the LOB corpus.&amp;quot; Association for Computational Linguistics, 243-251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S DeRose</author>
</authors>
<title>Grammatical category disambiguation by statistical optimization.&amp;quot;</title>
<date>1988</date>
<journal>Computational Linguistics,</journal>
<volume>14</volume>
<issue>1</issue>
<pages>31--39</pages>
<contexts>
<context position="22603" citStr="DeRose (1988)" startWordPosition="3530" endWordPosition="3531">dicated in the introduction of Waibel and Lee (1990), almost every aspect of most speech recognition systems is dominated by probabilistic models with parameters determined from data. 2. Part-of-Speech Tagging Many of the very same methods are being applied to problems in natural language processing by many of the very same researchers. As a result, the empirical approach has been adopted by almost all contemporary part-of-speech programs: Bahl and Mercer (1976), Leech, Garside, and Atwell (1983), Jelinek (1985), Deroualt and Merialdo (1986), Garside, Leech, and Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al. (1990), deMarcken (1990), Karlsson (1990), Boggess, Agarwal, and Davis (1991), Merialdo (1991), and Voutilainen, Heikkila, and Anttila (1992). These programs input a sequence of words, e.g., The chair will table the motion, and output a sequence of part-of-speech tags, e.g., art noun modal verb art noun. Most of these programs correctly tag at least 95% of the words, with practically no restrictions on the input text, and with very modest space and time requirements. Perhaps the most important indication of success is that many of these statis</context>
</contexts>
<marker>DeRose, 1988</marker>
<rawString>DeRose, S. (1988). &amp;quot;Grammatical category disambiguation by statistical optimization.&amp;quot; Computational Linguistics, 14(1), 31-39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Deroualt</author>
<author>B Merialdo</author>
</authors>
<title>Natural language modeling for phoneme-to-text transcription.&amp;quot;</title>
<date>1986</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence</journal>
<volume>8</volume>
<issue>6</issue>
<pages>742--749</pages>
<contexts>
<context position="22537" citStr="Deroualt and Merialdo (1986)" startWordPosition="3519" endWordPosition="3522"> available, these areas were tamed by automatic training techniques. Today, as indicated in the introduction of Waibel and Lee (1990), almost every aspect of most speech recognition systems is dominated by probabilistic models with parameters determined from data. 2. Part-of-Speech Tagging Many of the very same methods are being applied to problems in natural language processing by many of the very same researchers. As a result, the empirical approach has been adopted by almost all contemporary part-of-speech programs: Bahl and Mercer (1976), Leech, Garside, and Atwell (1983), Jelinek (1985), Deroualt and Merialdo (1986), Garside, Leech, and Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al. (1990), deMarcken (1990), Karlsson (1990), Boggess, Agarwal, and Davis (1991), Merialdo (1991), and Voutilainen, Heikkila, and Anttila (1992). These programs input a sequence of words, e.g., The chair will table the motion, and output a sequence of part-of-speech tags, e.g., art noun modal verb art noun. Most of these programs correctly tag at least 95% of the words, with practically no restrictions on the input text, and with very modest space and time requirements. Perhaps the</context>
</contexts>
<marker>Deroualt, Merialdo, 1986</marker>
<rawString>Deroualt, A., and Merialdo, B. (1986). &amp;quot;Natural language modeling for phoneme-to-text transcription.&amp;quot; IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-8(6), 742-749.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fano</author>
</authors>
<title>Transmission of Information.</title>
<date>1961</date>
<publisher>MIT. Press.</publisher>
<contexts>
<context position="14970" citStr="Fano (1961)" startWordPosition="2306" endWordPosition="2307">nalties and bonuses were sometimes thought of as probabilities, the early Raleigh system lacked a complete and unified probabilistic framework. In a radical departure from the prevailing attitudes of the time, the Yorktown group turned to Shannon&apos;s theory of communication in the presence of noise and recast the speech recognition problem in terms of transmission through a noisy channel. Shannon&apos;s theory of communication (Shannon 1948), also known as Information Theory, was originally developed at AT&amp;T Bell Laboratories to model communication along a noisy channel such as a telephone line. See Fano (1961) for a well-known secondary source on the subject, or Cover and Thomas (1991) or Bell, Cleary, and Witten (1990) for more recent treatments. The noisy channel paradigm can be applied to other recognition applications such as optical character recognition (OCR) and spelling correction. Imagine a noisy channel, such as a speech recognition machine that almost hears, an optical character recognition (OCR) machine that almost reads, or a typist who almost types. A sequence of good text (I) goes into the channel, and a sequence of corrupted text (0) comes out the other end. I —+ Noisy Channel --* 0</context>
<context position="57745" citStr="Fano 1961" startWordPosition="9348" endWordPosition="9349">ies, and statistics.&amp;quot; Unfortunately, sampling methods can be expensive; it is not clear whether we can justify the expense for the kinds of applications that we have in mind. Table 7 might lend some support for the quantity position for Murray&apos;s example of imaginable. Note that there is plenty of evidence in the larger corpora, but not in the smaller ones. Thus, it would appear that &amp;quot;more data are better data,&amp;quot; at least for the purpose of finding exemplars of words like imaginable. Similar comments hold for collocation studies, as illustrated in Table 8, which shows mutual information values (Fano 1961; p. 28) Pr(x,y) I(x;y) = log2 Pr(x) Pr(y) for several collocations in a number of different corpora. Mutual information compares the probability of observing word x and word y together (the joint probability) to the probability of observing x and y independently (chance). Most of the mutual information values in Table 8 are much larger than zero, indicating, as we would hope, that the collocations appear much more often in these corpora than one would expect by chance. The probabilities, Pr(x) and Pr(y), are estimated by counting the number of observations of x and y in a corpus, f (x) and f </context>
</contexts>
<marker>Fano, 1961</marker>
<rawString>Fano, R. (1961). Transmission of Information. MIT. Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Firth</author>
</authors>
<title>A synopsis of linguistic theory 1930-1955.&amp;quot;</title>
<date>1957</date>
<booktitle>In Studies in Linguistic Analysis, Philological Society,</booktitle>
<location>Oxford;</location>
<note>reprinted in Selected Papers of</note>
<contexts>
<context position="802" citStr="Firth 1957" startWordPosition="125" endWordPosition="126">itnessed a resurgence of interest in 1950s-style empirical and statistical methods of language analysis. Empiricism was at its peak in the 1950s, dominating a broad set of fields ranging from psychology (behaviorism) to electrical engineering (information theory). At that time, it was common practice in linguistics to classify words not only on the basis of their meanings but also on the basis of their cooccurrence with other words. Firth, a leading figure in British linguistics during the 1950s, summarized the approach with the memorable line: &amp;quot;You shall know a word by the company it keeps&amp;quot; (Firth 1957). Regrettably, interest in empiricism faded in the late 1950s and early 1960s with a number of significant events including Chomsky&apos;s criticism of n-grams in Syntactic Structures (Chomsky 1957) and Minsky and Papert&apos;s criticism of neural networks in Perceptrons (Minsky and Papert 1969). Perhaps the most immediate reason for this empirical renaissance is the availability of massive quantities of data: more text is available than ever before. Just ten years ago, the one-million word Brown Corpus (Francis and Ku&apos;&amp;ra, 1982) was considered large, but even then, there were much larger corpora such a</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>Firth, J. (1957). &amp;quot;A synopsis of linguistic theory 1930-1955.&amp;quot; In Studies in Linguistic Analysis, Philological Society, Oxford; reprinted in Selected Papers of J. R. Firth, edited by F. Palmer. Longman. 1968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ford</author>
<author>J Bresnan</author>
<author>R Kaplan</author>
</authors>
<title>A competence based theory of syntactic closure.&amp;quot;</title>
<date>1982</date>
<booktitle>In The Mental Representation of Grammatical Relations, edited</booktitle>
<pages>727--796</pages>
<publisher>MIT Press,</publisher>
<marker>Ford, Bresnan, Kaplan, 1982</marker>
<rawString>Ford, M.; Bresnan, J.; and Kaplan, R. (1982). &amp;quot;A competence based theory of syntactic closure.&amp;quot; In The Mental Representation of Grammatical Relations, edited by J. Bresnan. MIT Press, 727-796.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Frakes</author>
<author>R Baeza-Yates</author>
<author>eds</author>
</authors>
<title>Information Retrieval: Data Structures and Algorithms.</title>
<date>1992</date>
<publisher>Prentice Hall.</publisher>
<marker>Frakes, Baeza-Yates, eds, 1992</marker>
<rawString>Frakes, W., and Baeza-Yates, R., eds. (1992). Information Retrieval: Data Structures and Algorithms. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Francis</author>
<author>H Ku&amp;ra</author>
</authors>
<title>Frequency Analysis of English Usage,</title>
<date>1982</date>
<location>Houghton Mifflin.</location>
<marker>Francis, Ku&amp;ra, 1982</marker>
<rawString>Francis, W., and Ku&amp;ra, H. (1982). Frequency Analysis of English Usage, Houghton Mifflin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Furui</author>
</authors>
<title>Digital Speech Processing, Synthesis, and Recognition.</title>
<date>1989</date>
<publisher>Marcel Dekker.</publisher>
<contexts>
<context position="17705" citStr="Furui (1989" startWordPosition="2766" endWordPosition="2767"> group had done, the Yorktown group used three levels of hidden Markov models (HMMs) to compute the conditional probabilities necessary for the noisy channel. A Markov model is a finite state machine with probabilities governing transitions between states and controlling the emission of output symbols. If the sequence of state transitions cannot be determined when the sequence of outputs is known, the Markov model is said to be hidden. In practice, the Forward—Backward algorithm is often used to estimate the values of the transition and emission parameters on the basis of corpus evidence. See Furui (1989; Appendix D.3, pp. 343-347) for a brief description of the 5 Computational Linguistics Volume 19, Number 1 Table 1 Examples of channel confusions in different applications. Application Input Output Speech writer rider Recognition here hear Optical all all (A-one-L) Character of of Recognition form farm Spelling government goverment Correction occurred occured commercial commerical Table 2 Performance after training (Bahl et al. 1975) Training Set Size Test Sentences Correctly Decoded Decoding Problems 0 2/10 8/10 200 77/100 3/100 400 80/100 2/100 600 85/100 1/100 800 82/100 3/100 1070 83/100 </context>
</contexts>
<marker>Furui, 1989</marker>
<rawString>Furui, S. (1989). Digital Speech Processing, Synthesis, and Recognition. Marcel Dekker.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Garside</author>
<author>G Leech</author>
<author>G Sampson</author>
</authors>
<title>The Computational Analysis of English.</title>
<date>1987</date>
<publisher>Longman.</publisher>
<marker>Garside, Leech, Sampson, 1987</marker>
<rawString>Garside, R.; Leech, G.; and Sampson, G. (1987). The Computational Analysis of English. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Good</author>
</authors>
<title>The population frequencies of species and the estimation of population parameters.&amp;quot;</title>
<date>1953</date>
<journal>Biometrika,</journal>
<volume>40</volume>
<pages>237--264</pages>
<contexts>
<context position="31872" citStr="Good 1953" startWordPosition="5085" endWordPosition="5086">he Brown Corpus, for which n is approximately 1,000,000. Therefore, we can argue that 1,000, 000p must be about 140 and we can make an estimate, p, of p equal to 140/1, 000, 000. If we really believe that words in English text come up like heads when we flip a biased coin, then p is the value of p that makes the Brown Corpus as probable as possible. Therefore, this method of estimating parameters is called maximum likelihood estimation (MLE). For simple models, MLE is very easy to implement and produces reasonable estimates in many cases. More elaborate methods such as the Good-Turing Method (Good 1953) or Deleted Estimation (Jelinek and Mercer 1980, 1985) should be used when the frequencies are small (e.g., less than 10). It is often convenient to use these statistical estimates as if they are the same as the true probabilities, but this practice can lead to trouble, especially when the data don&apos;t fit the model very well. In fact, content words don&apos;t fit a binomial very well, because content words tend to appear in &amp;quot;bursts.&amp;quot; That is, content words are like buses in New York City; they are social animals and like to travel in packs. In particular, if the word Kennedy appears once in a unit o</context>
</contexts>
<marker>Good, 1953</marker>
<rawString>Good, I. (1953). &amp;quot;The population frequencies of species and the estimation of population parameters.&amp;quot; Biometrika, 40, 237-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Halliday</author>
</authors>
<title>Lexis as a linguistic level,&amp;quot; In</title>
<date>1966</date>
<booktitle>In Memory of</booktitle>
<publisher>Longman.</publisher>
<contexts>
<context position="35624" citStr="Halliday (1966" startWordPosition="5714" endWordPosition="5715"> of pure form Most people would have little difficulty deciding that form was the intended word. Neither does an OCR system that employs a trigram language model, because preferences, such as collocations, fall naturally within the scope of the n-gram approximation. Traditional NLP techniques, on the other hand, fail here because the competence approximation does not capture the crucial collocational constraints. Lexicographers use the terms collocation, co-occurrence, and lexis to describe various constraints on pairs of words. The words strong and powerful are perhaps the canonical example. Halliday (1966; p. 150) noted that although strong and powerful have similar syntax and semantics, there are contexts where one is much more appropriate than the other (e.g., strong tea vs. powerful computers). Psycholinguists have a similar concept, which they call word associations. Two frequently cited examples of highly associated words are: bread/butter and doctor &apos;nurse. See Palermo and Jenkins (1964) for tables of associations, measured for 200 words, factored by grade level and sex. In general, subjects respond more quickly to a word such as butter when it follows a highly associated word such as br</context>
</contexts>
<marker>Halliday, 1966</marker>
<rawString>Halliday, M. (1966). &amp;quot;Lexis as a linguistic level,&amp;quot; In In Memory of J. R. Firth, edited by C. Bazell, J. Catford, M. Halliday, and R. Robins. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hanks</author>
</authors>
<title>Evidence and intuition in lexicography.&amp;quot; In Meaning and Lexicography, edited by</title>
<date>1990</date>
<pages>31--41</pages>
<publisher>John Benjamins Publishing Company.</publisher>
<contexts>
<context position="59659" citStr="Hanks 1990" startWordPosition="9702" endWordPosition="9703">d by y in the corpus, f (x , y), and normalizing by N. Unfortunately, mutual information values become unstable if the counts are too small. For this reason, small counts (less than 10) are shown in parentheses. A dash is used when there is no evidence for the collocation. Like Table 7, Table 8 also shows that &amp;quot;more data are better data.&amp;quot; That is, there is plenty of evidence in the larger corpora, but not in the smaller ones. &amp;quot;Only a large corpus of natural language enables us to identify recurring patterns in the language and to observe collocational and lexical restrictions accurately....&amp;quot; (Hanks 1990; p. 36) However, in order to make use of this evidence we have to find ways to compensate for the obvious problems of working with unbalanced data. For example, in the Canadian Hansards, there are a number of unwanted phrases such as: &amp;quot;House of Commons,&amp;quot; &amp;quot;free trade agreement,&amp;quot; &amp;quot;honour and duty to present,&amp;quot; and &amp;quot;Hear! Hear!&amp;quot; Fortunately, though, it is extremely unlikely that these unwanted phrases will appear much more often than chance across a range of other corpora such as Department of Energy (DOE) abstracts or the Associated Press (AP) news. If such a phrase were to appear relatively oft</context>
</contexts>
<marker>Hanks, 1990</marker>
<rawString>Hanks, P. (1990). &amp;quot;Evidence and intuition in lexicography.&amp;quot; In Meaning and Lexicography, edited by J. Tomaszczyk and B. Lewandowska-Tomaszczyk, 31-41. John Benjamins Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Toward noun homonym disambiguation using local context in large text corpora.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, Seventh Annual Conference of the UW Centre for the New OED and</booktitle>
<pages>1--22</pages>
<institution>Text Research. University of Waterloo,</institution>
<location>Waterloo, Ontario,</location>
<contexts>
<context position="23630" citStr="Hearst 1991" startWordPosition="3687" endWordPosition="3688">ords, with practically no restrictions on the input text, and with very modest space and time requirements. Perhaps the most important indication of success is that many of these statistical tagging programs are now being used on large volumes of data (hundreds of millions of words of text) in a number of different application areas including speech synthesis (Sproat, personal communication; Liberman and Church 1991), speech recognition (Jelinek 1985; Jelinek, Mercer, and Roukos 1991), information retrieval (Salton, Zhao, and Buckley 1990; Croft, Turtle, and Lewis 1991), sense disambiguation (Hearst 1991), and computational lexicography (Klavans and Tzoukermann 1990a, 1990b; Church and Gale 1991). Apparently, these programs must be addressing some important needs of the research community or else they wouldn&apos;t be as widely cited as they are. Many of the papers in this special issue refer to these taggers. 7 Computational Linguistics Volume 19, Number 1 As in speech recognition, data collection efforts have played a pivotal role in advancing data-intensive approaches to part-of-speech tagging. The Brown Corpus (Francis and Kue&apos;era 1982) and similar efforts within the ICAME community, have creat</context>
</contexts>
<marker>Hearst, 1991</marker>
<rawString>Hearst, M. (1991). &amp;quot;Toward noun homonym disambiguation using local context in large text corpora.&amp;quot; In Proceedings, Seventh Annual Conference of the UW Centre for the New OED and Text Research. University of Waterloo, Waterloo, Ontario, 1-22.</rawString>
</citation>
<citation valid="true">
<date>1979</date>
<booktitle>Machine Translation.</booktitle>
<editor>Henisz-Dostert, B.; Ross Macdonald, R.; and Zarechnak, M., eds.</editor>
<publisher>Mouton.</publisher>
<marker>1979</marker>
<rawString>Henisz-Dostert, B.; Ross Macdonald, R.; and Zarechnak, M., eds. (1979). Machine Translation. Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Acquiring disambiguation rules from text.&amp;quot;</title>
<date>1989</date>
<journal>ACL,</journal>
<pages>118--125</pages>
<contexts>
<context position="22618" citStr="Hindle (1989)" startWordPosition="3532" endWordPosition="3533">introduction of Waibel and Lee (1990), almost every aspect of most speech recognition systems is dominated by probabilistic models with parameters determined from data. 2. Part-of-Speech Tagging Many of the very same methods are being applied to problems in natural language processing by many of the very same researchers. As a result, the empirical approach has been adopted by almost all contemporary part-of-speech programs: Bahl and Mercer (1976), Leech, Garside, and Atwell (1983), Jelinek (1985), Deroualt and Merialdo (1986), Garside, Leech, and Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al. (1990), deMarcken (1990), Karlsson (1990), Boggess, Agarwal, and Davis (1991), Merialdo (1991), and Voutilainen, Heikkila, and Anttila (1992). These programs input a sequence of words, e.g., The chair will table the motion, and output a sequence of part-of-speech tags, e.g., art noun modal verb art noun. Most of these programs correctly tag at least 95% of the words, with practically no restrictions on the input text, and with very modest space and time requirements. Perhaps the most important indication of success is that many of these statistical tagging p</context>
</contexts>
<marker>Hindle, 1989</marker>
<rawString>Hindle, D. (1989). &amp;quot;Acquiring disambiguation rules from text.&amp;quot; ACL, 118-125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Isabelle</author>
</authors>
<title>Bi-textual aids for translators.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, Eighth Annual Conference of the UW Centre for the New OED and</booktitle>
<pages>76--89</pages>
<institution>Text Research. University of Waterloo,</institution>
<location>Waterloo, Ontario,</location>
<contexts>
<context position="51464" citStr="Isabelle 1992" startWordPosition="8331" endWordPosition="8332"> produce a number of lexical resources that may be of great value. In particular, there has been quite a bit of discussion of bilingual concordances recently (e.g., Klavans and Tzoukermann 1990a, 1990b; Church and Gale 1991), including the 1990 and 1991 lexicography conferences sponsored by Oxford University Press and Waterloo University. A bilingual concordance is like a monolingual concordance except that each line in the concordance is followed by a line of text in a second language. There are also some hopes that the approach might produce tools that could be useful for human translators (Isabelle 1992). There are three papers in these special issues on aligning bilingual texts such as the Canadian Hansards (parliamentary debates) that are available in both English and French: Brown et al. (1993), Gale and Church (this issue), and Kay and Rosenschein (this issue). Warwick-Armstrong and Russell have also been interested in the alignment problem (Warwick-Armstrong and Russell 1990). Except for Brown et al., this work is focused on the less controversial applications in lexicography and human translation, rather than MT. 4 Requests for a tape of the debate should be sent to the attention of Pie</context>
</contexts>
<marker>Isabelle, 1992</marker>
<rawString>Isabelle, P. (1992). &amp;quot;Bi-textual aids for translators.&amp;quot; In Proceedings, Eighth Annual Conference of the UW Centre for the New OED and Text Research. University of Waterloo, Waterloo, Ontario, 76-89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Self-organized language modeling for speech recognition.&amp;quot;</title>
<date>1985</date>
<journal>IBM Report. Reprinted in W &amp; L,</journal>
<pages>450--506</pages>
<contexts>
<context position="22507" citStr="Jelinek (1985)" startWordPosition="3517" endWordPosition="3518">rded data became available, these areas were tamed by automatic training techniques. Today, as indicated in the introduction of Waibel and Lee (1990), almost every aspect of most speech recognition systems is dominated by probabilistic models with parameters determined from data. 2. Part-of-Speech Tagging Many of the very same methods are being applied to problems in natural language processing by many of the very same researchers. As a result, the empirical approach has been adopted by almost all contemporary part-of-speech programs: Bahl and Mercer (1976), Leech, Garside, and Atwell (1983), Jelinek (1985), Deroualt and Merialdo (1986), Garside, Leech, and Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al. (1990), deMarcken (1990), Karlsson (1990), Boggess, Agarwal, and Davis (1991), Merialdo (1991), and Voutilainen, Heikkila, and Anttila (1992). These programs input a sequence of words, e.g., The chair will table the motion, and output a sequence of part-of-speech tags, e.g., art noun modal verb art noun. Most of these programs correctly tag at least 95% of the words, with practically no restrictions on the input text, and with very modest space and </context>
<context position="36746" citStr="Jelinek 1985" startWordPosition="5887" endWordPosition="5888">d more quickly to a word such as butter when it follows a highly associated word such as bread. Some results and implications are summarized from reaction-time experiments in which subjects either (a) classified successive strings of letters as words and nonwords, or (b) pronounced the strings. Both types of response to words (e.g., BUTTER) were consistently faster when preceded by associated words (e.g., BREAD) rather than unassociated words (e.g, NURSE). (Meyer, Schvaneveldt, and Ruddy 1975; p. 98) 11 Computational Linguistics Volume 19, Number 1 Table 4 The trigram approximation in action (Jelinek 1985). Word Rank We 9 need 7 to 1 resolve 85 all 9 of 2 the 1 important 657 issues 14 More likely alternatives The This One Two A Three Please In are will the would also do have know do ... the this these problems ... the document question first ... thing point to ... These constraints are rarely discussed in computational linguistics because they are not captured very well with traditional NLP techniques, especially those based on the competence approximation. Of course, it isn&apos;t hard to build computational models that capture at least some of these preferences. Even the trigram model, despite all</context>
<context position="63057" citStr="Jelinek (1985)" startWordPosition="10242" endWordPosition="10243">(strong, enough)) f (strong, enough) /N2, which can be justified under appropriate binomial assumptions. It is also assumed that a2 (Pr (strong) Pr(enough)) is very small and can be omitted. f(strong,enough) f (strong) f (mough) t N N N \f(strong,enough N2 Although statistics are often used to test a particular hypothesis as we have just seen, statistics can also be used to explore the space of possible hypotheses, or to discover new hypotheses (supervised/unsupervised learning/training). See Tukey (1977) and Mosteller and Tukey (1977) for two textbooks on Exploratory Data Analysis (EDA), and Jelinek (1985) for a very nice review paper on self-organizing statistics. Both the exploratory and self-organizing views are represented in these special issues. Pustejovsky et al. (1993) use an EDA approach to investigate certain questions in lexical semantics. Brent (1993), in contrast, adopts a self-organizing approach to identify subcategorization features. Table 9 shows how the t-score can be used in an exploratory mode to extract large numbers of words from the Associated Press (AP) news that co-occur more often with strong than with powerful, and vice versa. It is an interesting question whether col</context>
<context position="65010" citStr="Jelinek (1985)" startWordPosition="10546" endWordPosition="10547">rves further investigation. Summary statistics such as mutual information and t-scores may have an important role to play in helping lexicographers to discover significant patterns of collocations, though the position remains somewhat controversial. Some lexicographers prefer mutual information, some prefer t-scores, and some are unconvinced that either of them is any good. Church et al. (1991) argued that different statistics have different strengths and weaknesses, and that it requires human judgment and exploration to decide which statistic is best for a particular problem. Others, such as Jelinek (1985), would prefer a self-organizing approach, where there is no need for human judgment. 20 Kenneth W. Church and Robert L. Mercer Introduction Table 9 An example of the t-score (Church et al. 1991). strong w Strong w w t Powerful w w powerful w strong w powerful w 12.42 161 0 showing —7.44 1 56 than 11.94 175 2 support —5.60 1 32 figure 10.08 550 68 , —5.37 3 31 minority 9.97 106 0 defense —5.23 1 28 of 9.76 102 0 economy —4.91 0 24 post 9.50 97 0 demand —4.63 5 25 9.40 95 0 gains —4.35 27 36 nmeiwlitary 9.18 91 0 growth —3.89 0 15 figures 8.84 137 5 winds —3.59 6 17 presidency 8.02 83 1 opposit</context>
</contexts>
<marker>Jelinek, 1985</marker>
<rawString>Jelinek, F. (1985). &amp;quot;Self-organized language modeling for speech recognition.&amp;quot; IBM Report. Reprinted in W &amp; L, 450-506.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Jelinek</author>
<author>R Mercer</author>
</authors>
<title>Interpolated estimation of Markov source parameters from sparse data.&amp;quot;</title>
<date>1980</date>
<booktitle>In Proceedings, Workshop on Pattern Recognition in Practice.</booktitle>
<publisher>North-Holland.</publisher>
<contexts>
<context position="31919" citStr="Jelinek and Mercer 1980" startWordPosition="5090" endWordPosition="5093">proximately 1,000,000. Therefore, we can argue that 1,000, 000p must be about 140 and we can make an estimate, p, of p equal to 140/1, 000, 000. If we really believe that words in English text come up like heads when we flip a biased coin, then p is the value of p that makes the Brown Corpus as probable as possible. Therefore, this method of estimating parameters is called maximum likelihood estimation (MLE). For simple models, MLE is very easy to implement and produces reasonable estimates in many cases. More elaborate methods such as the Good-Turing Method (Good 1953) or Deleted Estimation (Jelinek and Mercer 1980, 1985) should be used when the frequencies are small (e.g., less than 10). It is often convenient to use these statistical estimates as if they are the same as the true probabilities, but this practice can lead to trouble, especially when the data don&apos;t fit the model very well. In fact, content words don&apos;t fit a binomial very well, because content words tend to appear in &amp;quot;bursts.&amp;quot; That is, content words are like buses in New York City; they are social animals and like to travel in packs. In particular, if the word Kennedy appears once in a unit of text (e.g., a paragraph, a discourse, or a ge</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>Jelinek, E, and Mercer, R. (1980). &amp;quot;Interpolated estimation of Markov source parameters from sparse data.&amp;quot; In Proceedings, Workshop on Pattern Recognition in Practice. North-Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Jelinek</author>
<author>R Mercer</author>
</authors>
<title>Probability distribution estimation from sparse data.&amp;quot;</title>
<date>1985</date>
<journal>IBM Technical Disclosure Bulletin,</journal>
<pages>28--2591</pages>
<marker>Jelinek, Mercer, 1985</marker>
<rawString>Jelinek, E, and Mercer, R. (1985). &amp;quot;Probability distribution estimation from sparse data.&amp;quot; IBM Technical Disclosure Bulletin, 28,2591-2594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Jelinek</author>
<author>R Mercer</author>
<author>S Roukos</author>
</authors>
<title>Principles of lexical language modeling for speech recognition.&amp;quot;</title>
<date>1991</date>
<booktitle>In Advances in Speech Signal Processing,</booktitle>
<pages>651--700</pages>
<publisher>Marcel Dekker,</publisher>
<note>edited by</note>
<marker>Jelinek, Mercer, Roukos, 1991</marker>
<rawString>Jelinek, E; Mercer, R.; and Roukos, S. (1991). &amp;quot;Principles of lexical language modeling for speech recognition.&amp;quot; In Advances in Speech Signal Processing, edited by S. Furui and M. Mohan. Marcel Dekker, 651-700.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Karlsson</author>
</authors>
<title>Constraint grammar as a framework for parsing running text.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 15th International Conference on Computational Linguistics (COLING-90),</booktitle>
<pages>168--173</pages>
<contexts>
<context position="22695" citStr="Karlsson (1990)" startWordPosition="3543" endWordPosition="3544">ecognition systems is dominated by probabilistic models with parameters determined from data. 2. Part-of-Speech Tagging Many of the very same methods are being applied to problems in natural language processing by many of the very same researchers. As a result, the empirical approach has been adopted by almost all contemporary part-of-speech programs: Bahl and Mercer (1976), Leech, Garside, and Atwell (1983), Jelinek (1985), Deroualt and Merialdo (1986), Garside, Leech, and Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al. (1990), deMarcken (1990), Karlsson (1990), Boggess, Agarwal, and Davis (1991), Merialdo (1991), and Voutilainen, Heikkila, and Anttila (1992). These programs input a sequence of words, e.g., The chair will table the motion, and output a sequence of part-of-speech tags, e.g., art noun modal verb art noun. Most of these programs correctly tag at least 95% of the words, with practically no restrictions on the input text, and with very modest space and time requirements. Perhaps the most important indication of success is that many of these statistical tagging programs are now being used on large volumes of data (hundreds of millions of </context>
</contexts>
<marker>Karlsson, 1990</marker>
<rawString>Karlsson, E (1990). &amp;quot;Constraint grammar as a framework for parsing running text.&amp;quot; In Proceedings, 15th International Conference on Computational Linguistics (COLING-90), 168-173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Klavans</author>
<author>E Tzoukermann</author>
</authors>
<title>The BICORD system.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 15th International Conference on Computational Linguistics (COLING-90),</booktitle>
<pages>174--179</pages>
<location>Helsinki, Finland,</location>
<contexts>
<context position="23692" citStr="Klavans and Tzoukermann 1990" startWordPosition="3692" endWordPosition="3695">input text, and with very modest space and time requirements. Perhaps the most important indication of success is that many of these statistical tagging programs are now being used on large volumes of data (hundreds of millions of words of text) in a number of different application areas including speech synthesis (Sproat, personal communication; Liberman and Church 1991), speech recognition (Jelinek 1985; Jelinek, Mercer, and Roukos 1991), information retrieval (Salton, Zhao, and Buckley 1990; Croft, Turtle, and Lewis 1991), sense disambiguation (Hearst 1991), and computational lexicography (Klavans and Tzoukermann 1990a, 1990b; Church and Gale 1991). Apparently, these programs must be addressing some important needs of the research community or else they wouldn&apos;t be as widely cited as they are. Many of the papers in this special issue refer to these taggers. 7 Computational Linguistics Volume 19, Number 1 As in speech recognition, data collection efforts have played a pivotal role in advancing data-intensive approaches to part-of-speech tagging. The Brown Corpus (Francis and Kue&apos;era 1982) and similar efforts within the ICAME community, have created invaluable opportunities. The Penn Treebank (see the paper </context>
<context position="51043" citStr="Klavans and Tzoukermann 1990" startWordPosition="8262" endWordPosition="8265">rocedure that figures out which parts of the source text correspond to which parts of the target text. See Brown et al. (1993) for more details on the estimation of the parameters. The information theoretic approach to MT may fail for reasons advanced by Chomsky and others in the 1950s. But regardless of its ultimate success or failure, there is a growing community of researchers in corpus-based linguistics who believe that it will produce a number of lexical resources that may be of great value. In particular, there has been quite a bit of discussion of bilingual concordances recently (e.g., Klavans and Tzoukermann 1990a, 1990b; Church and Gale 1991), including the 1990 and 1991 lexicography conferences sponsored by Oxford University Press and Waterloo University. A bilingual concordance is like a monolingual concordance except that each line in the concordance is followed by a line of text in a second language. There are also some hopes that the approach might produce tools that could be useful for human translators (Isabelle 1992). There are three papers in these special issues on aligning bilingual texts such as the Canadian Hansards (parliamentary debates) that are available in both English and French: B</context>
</contexts>
<marker>Klavans, Tzoukermann, 1990</marker>
<rawString>Klavans, J., and Tzoukermann, E. (1990a). &amp;quot;The BICORD system.&amp;quot; In Proceedings, 15th International Conference on Computational Linguistics (COLING-90), Helsinki, Finland, 174-179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Klavans</author>
<author>E Tzoukermann</author>
</authors>
<title>Linking bilingual corpora and machine readable dictionaries with the BICORD system.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, Sixth Annual Conference of the UW Centre for the New Oxford English Dictionary and Text Research,</booktitle>
<contexts>
<context position="23692" citStr="Klavans and Tzoukermann 1990" startWordPosition="3692" endWordPosition="3695">input text, and with very modest space and time requirements. Perhaps the most important indication of success is that many of these statistical tagging programs are now being used on large volumes of data (hundreds of millions of words of text) in a number of different application areas including speech synthesis (Sproat, personal communication; Liberman and Church 1991), speech recognition (Jelinek 1985; Jelinek, Mercer, and Roukos 1991), information retrieval (Salton, Zhao, and Buckley 1990; Croft, Turtle, and Lewis 1991), sense disambiguation (Hearst 1991), and computational lexicography (Klavans and Tzoukermann 1990a, 1990b; Church and Gale 1991). Apparently, these programs must be addressing some important needs of the research community or else they wouldn&apos;t be as widely cited as they are. Many of the papers in this special issue refer to these taggers. 7 Computational Linguistics Volume 19, Number 1 As in speech recognition, data collection efforts have played a pivotal role in advancing data-intensive approaches to part-of-speech tagging. The Brown Corpus (Francis and Kue&apos;era 1982) and similar efforts within the ICAME community, have created invaluable opportunities. The Penn Treebank (see the paper </context>
<context position="51043" citStr="Klavans and Tzoukermann 1990" startWordPosition="8262" endWordPosition="8265">rocedure that figures out which parts of the source text correspond to which parts of the target text. See Brown et al. (1993) for more details on the estimation of the parameters. The information theoretic approach to MT may fail for reasons advanced by Chomsky and others in the 1950s. But regardless of its ultimate success or failure, there is a growing community of researchers in corpus-based linguistics who believe that it will produce a number of lexical resources that may be of great value. In particular, there has been quite a bit of discussion of bilingual concordances recently (e.g., Klavans and Tzoukermann 1990a, 1990b; Church and Gale 1991), including the 1990 and 1991 lexicography conferences sponsored by Oxford University Press and Waterloo University. A bilingual concordance is like a monolingual concordance except that each line in the concordance is followed by a line of text in a second language. There are also some hopes that the approach might produce tools that could be useful for human translators (Isabelle 1992). There are three papers in these special issues on aligning bilingual texts such as the Canadian Hansards (parliamentary debates) that are available in both English and French: B</context>
</contexts>
<marker>Klavans, Tzoukermann, 1990</marker>
<rawString>Klavans, J., and Tzoukermann, E. (1990b). &amp;quot;Linking bilingual corpora and machine readable dictionaries with the BICORD system.&amp;quot; In Proceedings, Sixth Annual Conference of the UW Centre for the New Oxford English Dictionary and Text Research, 19-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klatt</author>
</authors>
<title>Review of the ARPA speech understanding project.&amp;quot;</title>
<date>1977</date>
<journal>Journal of the Acoustical Society of America. Reprinted in W&amp; L,</journal>
<pages>554--575</pages>
<marker>Klatt, 1977</marker>
<rawString>Klatt, D. (1977). &amp;quot;Review of the ARPA speech understanding project.&amp;quot; Journal of the Acoustical Society of America. Reprinted in W&amp; L, 554-575.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klatt</author>
</authors>
<title>Scriber and lafs: Two new approaches to speech analysis.&amp;quot;</title>
<date>1980</date>
<journal>In Trends</journal>
<publisher>Prentice-Hall.</publisher>
<note>in Speech Recognition, edited by</note>
<contexts>
<context position="10546" citStr="Klatt 1980" startWordPosition="1590" endWordPosition="1591">for sentence recognition, such modifications must be viewed as a kind of &apos;noise&apos; that makes it more difficult to hypothesize lexical candidates given an input phonetic transcription. To see that this must be the case, we note that each phonological rule fin the utterance: &amp;quot;Did you hit it to Tom?&amp;quot;] results in irreversible ambiguity— the phonological rule does not have a unique inverse that could be used to recover the underlying phonemic representation for a lexical 3 Computational Linguistics Volume 19, Number 1 item. For example, ... Mlle tongue flap ... could have come from a /t/ or a /d/. (Klatt 1980; pp. 548-549) The first DARPA Speech Understanding project emphasized the use of high-level constraints (e.g., syntax, semantics, and pragmatics) as a tool to disambiguate the allophonic information in the speech signal by understanding the message. At BBN, researchers called their system HWIM for (Hear What I Mean). They hoped to use NLP techniques such as ATNs (Woods 1970) to understand the sentences that they were trying to recognize even though the output of their front end was highly variable and ambiguous. The emphasis today on empirical methods in the speech recognition community is a </context>
</contexts>
<marker>Klatt, 1980</marker>
<rawString>Klatt, D. (1980). &amp;quot;Scriber and lafs: Two new approaches to speech analysis.&amp;quot; In Trends in Speech Recognition, edited by W. Lea. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kukich</author>
</authors>
<title>Techniques for automatically correcting words in text.&amp;quot;</title>
<date>1992</date>
<journal>ACM Computing Surveys,</journal>
<volume>24</volume>
<issue>4</issue>
<pages>377--439</pages>
<contexts>
<context position="16168" citStr="Kukich 1992" startWordPosition="2509" endWordPosition="2510"> Channel --* 0 How can an automatic procedure recover the good input text, I, from the corrupted output, 0? In principle, one can recover the most likely input, I, by hypothesizing all possible input texts, I, and selecting the input text with the highest score, Pr(I I 0). Symbolically, I = argmaxPr(/ I 0) = argmaxPr(/) Pr(0 I I) where ARGMAX finds the argument with the maximum score. The prior probability, Pr(I), is the probability that I will be presented at the input to the channel. In speech recognition, it is the probability that the talker utters I; in spelling correction (Damerau 1964; Kukich 1992), it is the probability that the typist intends to type I. In practice, the prior probability is unavailable, and consequently, we have to make do with a model of the prior probability, such as the trigram model. The parameters of the language model are usually estimated by computing various statistics over a large sample of text. The channel probability, Pr(0 I I), is the probability that 0 will appear at the output of the channel when I is presented at the input; it is large if I is similar, in some appropriate sense, to 0, and small, otherwise. The channel probability depends on the applica</context>
<context position="66392" citStr="Kukich 1992" startWordPosition="10786" endWordPosition="10787">nce of the speech recognition community during the 1970s and 1980s. Many of the same statistical techniques (e.g., Shannon&apos;s Noisy Channel Model, n-gram models, hidden Markov models (HMMs), entropy (H), mutual information (I), Student&apos;s t-score) have appeared in one form or another, often first in speech, and then soon thereafter in language. Many of the same researchers have applied these methods to a variety of application areas ranging from language modeling for noisy channel applications (e.g., speech recognition, optical character recognition [OCR], and spelling correction [Damerau 1964; Kukich 1992]), to part-of-speech tagging, parsing, translation, lexicography, text compression (Bell, Cleary, and Witten 1990) and information retrieval (IR) (Salton 1989; Frakes and Baeza-Yates 1992). Empiricism is, of course, a very old tradition. Back in the 1950s and 1960s, long before the speech work of the 1970s and 1980s, there was Skinner&apos;s Behaviorism in Psychology, Shannon&apos;s Information Theory in Electrical Engineering, and Harris&apos; Distributional Hypothesis in American Linguistics and the Firthian approach in British Linguistics (&amp;quot;You shall know a word by the company it keeps&amp;quot;). It is possible </context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>Kukich, K. (1992). &amp;quot;Techniques for automatically correcting words in text.&amp;quot; ACM Computing Surveys, 24(4), 377-439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
</authors>
<title>Augmenting a hidden Markov model for phrase-dependent word tagging.&amp;quot; DARPA Speech and Natural Language Workshop,</title>
<date>1989</date>
<pages>92--98</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, CA,</location>
<contexts>
<context position="22632" citStr="Kupiec (1989" startWordPosition="3534" endWordPosition="3535"> Waibel and Lee (1990), almost every aspect of most speech recognition systems is dominated by probabilistic models with parameters determined from data. 2. Part-of-Speech Tagging Many of the very same methods are being applied to problems in natural language processing by many of the very same researchers. As a result, the empirical approach has been adopted by almost all contemporary part-of-speech programs: Bahl and Mercer (1976), Leech, Garside, and Atwell (1983), Jelinek (1985), Deroualt and Merialdo (1986), Garside, Leech, and Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al. (1990), deMarcken (1990), Karlsson (1990), Boggess, Agarwal, and Davis (1991), Merialdo (1991), and Voutilainen, Heikkila, and Anttila (1992). These programs input a sequence of words, e.g., The chair will table the motion, and output a sequence of part-of-speech tags, e.g., art noun modal verb art noun. Most of these programs correctly tag at least 95% of the words, with practically no restrictions on the input text, and with very modest space and time requirements. Perhaps the most important indication of success is that many of these statistical tagging programs are no</context>
</contexts>
<marker>Kupiec, 1989</marker>
<rawString>Kupiec, J. (1989). &amp;quot;Augmenting a hidden Markov model for phrase-dependent word tagging.&amp;quot; DARPA Speech and Natural Language Workshop, San Mateo, CA, 92-98. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
</authors>
<title>Robust part-of-speech tagging using a hidden Markov model.&amp;quot;</title>
<date>1992</date>
<journal>Computer Speech and Language,</journal>
<volume>6</volume>
<pages>225--242</pages>
<marker>Kupiec, 1992</marker>
<rawString>Kupiec, J. (1992). &amp;quot;Robust part-of-speech tagging using a hidden Markov model.&amp;quot; Computer Speech and Language, 6, 225-242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lan</author>
<author>S Young</author>
</authors>
<title>Applications of stochastic context-free grammars using the inside-outside algorithm.&amp;quot;</title>
<date>1991</date>
<journal>Computer Speech and Language,</journal>
<pages>237--258</pages>
<marker>Lan, Young, 1991</marker>
<rawString>Lan, K., and Young, S. (1991). &amp;quot;Applications of stochastic context-free grammars using the inside-outside algorithm.&amp;quot; Computer Speech and Language, 237-258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leech</author>
<author>R Garside</author>
<author>E Atwell</author>
</authors>
<title>The automatic grammatical tagging of the LOB corpus.&amp;quot;</title>
<date>1983</date>
<journal>ICANIE News</journal>
<pages>7--13</pages>
<marker>Leech, Garside, Atwell, 1983</marker>
<rawString>Leech, G.; Garside, R.; and Atwell, E. (1983). &amp;quot;The automatic grammatical tagging of the LOB corpus.&amp;quot; ICANIE News 7,13-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Leonard</author>
</authors>
<title>A database for speaker-independent digit recognition.&amp;quot;</title>
<date>1984</date>
<booktitle>Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<pages>3--1</pages>
<contexts>
<context position="7464" citStr="Leonard 1984" startWordPosition="1096" endWordPosition="1097">Mercer Introduction of reprints on speech recognition (Waibel and Lee 1990): Chapter 5 describes the knowledge-based approach, proposed in the 1970s and early 1980s. The pure knowledge-based approach emulates human speech knowledge using expert systems. Rule-based systems have had only limited success... Chapter 6 describes the stochastic approach. . . Most successful large-scale systems today use a stochastic approach. (Waibel and Lee 1990; P. 4) A number of data collection efforts have helped to bring about this change in the speech community, especially the Texas Instruments&apos; Digit Corpus (Leonard 1984), TIMIT and the DARPA Resource Management (RM) Database (Price et al. 1988). According to the Linguistic Data Consortium (LDC), the RM database was used by every paper that reported speech recognition results in the 1988 Proceedings of IEEE ICASSP, the major technical society meeting where speech recognition results are reported. This is especially significant given that abstracts for this meeting were due just a few months after the release of the corpus, attesting to the speech recognition community&apos;s hunger for standard corpora for development and evaluation. Back in the 1970s, the more dat</context>
</contexts>
<marker>Leonard, 1984</marker>
<rawString>Leonard, R. (1984). &amp;quot;A database for speaker-independent digit recognition.&amp;quot; Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 3,1-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Liberman</author>
<author>K Church</author>
</authors>
<title>Text analysis and word pronunciation in text-to-speech synthesis.&amp;quot;</title>
<date>1991</date>
<booktitle>In Advances in Speech Signal Processing,</booktitle>
<pages>791--832</pages>
<publisher>Marcel Dekker,</publisher>
<note>edited by</note>
<contexts>
<context position="23438" citStr="Liberman and Church 1991" startWordPosition="3659" endWordPosition="3662">nput a sequence of words, e.g., The chair will table the motion, and output a sequence of part-of-speech tags, e.g., art noun modal verb art noun. Most of these programs correctly tag at least 95% of the words, with practically no restrictions on the input text, and with very modest space and time requirements. Perhaps the most important indication of success is that many of these statistical tagging programs are now being used on large volumes of data (hundreds of millions of words of text) in a number of different application areas including speech synthesis (Sproat, personal communication; Liberman and Church 1991), speech recognition (Jelinek 1985; Jelinek, Mercer, and Roukos 1991), information retrieval (Salton, Zhao, and Buckley 1990; Croft, Turtle, and Lewis 1991), sense disambiguation (Hearst 1991), and computational lexicography (Klavans and Tzoukermann 1990a, 1990b; Church and Gale 1991). Apparently, these programs must be addressing some important needs of the research community or else they wouldn&apos;t be as widely cited as they are. Many of the papers in this special issue refer to these taggers. 7 Computational Linguistics Volume 19, Number 1 As in speech recognition, data collection efforts hav</context>
</contexts>
<marker>Liberman, Church, 1991</marker>
<rawString>Liberman, M., and Church, K. (1991). &amp;quot;Text analysis and word pronunciation in text-to-speech synthesis.&amp;quot; In Advances in Speech Signal Processing, edited by S. Furui and M. Mohan. Marcel Dekker, 791-832.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging text with a probabilistic model.&amp;quot;</title>
<date>1991</date>
<booktitle>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<pages>809--812</pages>
<contexts>
<context position="22748" citStr="Merialdo (1991)" startWordPosition="3550" endWordPosition="3551">ls with parameters determined from data. 2. Part-of-Speech Tagging Many of the very same methods are being applied to problems in natural language processing by many of the very same researchers. As a result, the empirical approach has been adopted by almost all contemporary part-of-speech programs: Bahl and Mercer (1976), Leech, Garside, and Atwell (1983), Jelinek (1985), Deroualt and Merialdo (1986), Garside, Leech, and Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al. (1990), deMarcken (1990), Karlsson (1990), Boggess, Agarwal, and Davis (1991), Merialdo (1991), and Voutilainen, Heikkila, and Anttila (1992). These programs input a sequence of words, e.g., The chair will table the motion, and output a sequence of part-of-speech tags, e.g., art noun modal verb art noun. Most of these programs correctly tag at least 95% of the words, with practically no restrictions on the input text, and with very modest space and time requirements. Perhaps the most important indication of success is that many of these statistical tagging programs are now being used on large volumes of data (hundreds of millions of words of text) in a number of different application a</context>
</contexts>
<marker>Merialdo, 1991</marker>
<rawString>Merialdo, B. (1991). &amp;quot;Tagging text with a probabilistic model.&amp;quot; IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 809-812.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Meyer</author>
<author>R Schvaneveldt</author>
<author>M Ruddy</author>
</authors>
<title>Loci of contextual effects on visual word-recognition.&amp;quot;</title>
<date>1975</date>
<booktitle>In Attention and Performance</booktitle>
<pages>98--116</pages>
<publisher>Academic Press,</publisher>
<marker>Meyer, Schvaneveldt, Ruddy, 1975</marker>
<rawString>Meyer, D.; Schvaneveldt, R.; and Ruddy, M. (1975). &amp;quot;Loci of contextual effects on visual word-recognition.&amp;quot; In Attention and Performance V, edited by P. Rabbitt and S. Dornie. Academic Press, 98-116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Minsky</author>
<author>S Papert</author>
</authors>
<title>Perceptrons; An Introduction to Computational Geometry.</title>
<date>1969</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1088" citStr="Minsky and Papert 1969" startWordPosition="165" endWordPosition="168">ime, it was common practice in linguistics to classify words not only on the basis of their meanings but also on the basis of their cooccurrence with other words. Firth, a leading figure in British linguistics during the 1950s, summarized the approach with the memorable line: &amp;quot;You shall know a word by the company it keeps&amp;quot; (Firth 1957). Regrettably, interest in empiricism faded in the late 1950s and early 1960s with a number of significant events including Chomsky&apos;s criticism of n-grams in Syntactic Structures (Chomsky 1957) and Minsky and Papert&apos;s criticism of neural networks in Perceptrons (Minsky and Papert 1969). Perhaps the most immediate reason for this empirical renaissance is the availability of massive quantities of data: more text is available than ever before. Just ten years ago, the one-million word Brown Corpus (Francis and Ku&apos;&amp;ra, 1982) was considered large, but even then, there were much larger corpora such as the Birmingham Corpus (Sinclair et al. 1987; Sinclair 1987). Today, many locations have samples of text running into the hundreds of millions or even billions of words. Collections of this magnitude are becoming widely available, thanks to data collection efforts such as the Associat</context>
</contexts>
<marker>Minsky, Papert, 1969</marker>
<rawString>Minsky, M., and Papert, S. (1969). Perceptrons; An Introduction to Computational Geometry. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Mish</author>
<author>ed</author>
</authors>
<title>Webster&apos;s Ninth New Collegiate Dictionary.</title>
<date>1983</date>
<location>Merriam, Webster.</location>
<marker>Mish, ed, 1983</marker>
<rawString>Mish, F., ed. (1983). Webster&apos;s Ninth New Collegiate Dictionary. Merriam, Webster.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Mosteller</author>
<author>J Tukey</author>
</authors>
<title>Data Analysis and Regression.</title>
<date>1977</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="62984" citStr="Mosteller and Tukey (1977)" startWordPosition="10229" endWordPosition="10232">e is significantly larger than chance. The estimation uses the approximation, a2 (Pr (strong, enough)) f (strong, enough) /N2, which can be justified under appropriate binomial assumptions. It is also assumed that a2 (Pr (strong) Pr(enough)) is very small and can be omitted. f(strong,enough) f (strong) f (mough) t N N N \f(strong,enough N2 Although statistics are often used to test a particular hypothesis as we have just seen, statistics can also be used to explore the space of possible hypotheses, or to discover new hypotheses (supervised/unsupervised learning/training). See Tukey (1977) and Mosteller and Tukey (1977) for two textbooks on Exploratory Data Analysis (EDA), and Jelinek (1985) for a very nice review paper on self-organizing statistics. Both the exploratory and self-organizing views are represented in these special issues. Pustejovsky et al. (1993) use an EDA approach to investigate certain questions in lexical semantics. Brent (1993), in contrast, adopts a self-organizing approach to identify subcategorization features. Table 9 shows how the t-score can be used in an exploratory mode to extract large numbers of words from the Associated Press (AP) news that co-occur more often with strong than</context>
</contexts>
<marker>Mosteller, Tukey, 1977</marker>
<rawString>Mosteller, F., and Tukey, J. (1977). Data Analysis and Regression. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fredrick Mosteller</author>
<author>David Wallace</author>
</authors>
<title>Inference and Disputed Authorship: The Federalist.</title>
<date>1964</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="33391" citStr="Mosteller and Wallace (1964)" startWordPosition="5341" endWordPosition="5344"> serve a useful purpose. People seem to be able to use these bursts to speed up reaction times in various tasks. Psycholinguists use the term priming to refer to this effect. Bursts might also be useful in a number of practical applications such as Information Retrieval (IR) (Salton 1989; Frakes and Baeza-Yates 1992). There have been a number of attempts over the years to model these bursts. The negative 10 Kenneth W. Church and Robert L. Mercer Introduction binomial distribution, for example, was explored in considerable detail in the classic study of the authorship of the Federalist Papers, Mosteller and Wallace (1964), a mustread for anyone interested in statistical analyses of large corpora. We can show that the distribution of Kennedy is very bursty in the Brown Corpus by dividing the corpus into k segments and showing that the probability varies radically from one segment to another. For example, if we divide the Brown Corpus into 10 segments of 100,000 words each, we find that the frequency of Kennedy is: 58, 57, 2, 12, 6, 1, 4, 0, 0, 0. The variance of these 10 numbers is 539. Under the binomial assumption, we obtain a very different estimate of the variance. In a sample of n =-- 100,000 words, with f</context>
</contexts>
<marker>Mosteller, Wallace, 1964</marker>
<rawString>Mosteller, Fredrick, and Wallace, David (1964). Inference and Disputed Authorship: The Federalist. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Murray</author>
</authors>
<title>Caught in the Web of Words: James Murray and the Oxford English Dictionary.</title>
<date>1977</date>
<publisher>Yale University Press.</publisher>
<contexts>
<context position="54789" citStr="Murray 1977" startWordPosition="8843" endWordPosition="8844">d on index cards by large numbers of human readers. Unfortunately, citation indexes tend to be a bit like butterfly collections, full of rare and unusual specimens, but severely lacking in ordinary, garden-variety moths. Murray, the editor of the Oxford English Dictionary, complained: The editor or his assistants have to search for precious hours for examples of common words, which readers passed by... Thus, of Abus ion we found in the slips about 50 instances; of Abuse not five. (James Augustus Henry Murray, Presidential Address, Philological Society Transactions 1877-9, pp. 571-2, quoted by Murray 1977, p. 178) He then went on to say, &amp;quot;There was not a single quotation for imaginable, a word used by Chaucer, Sir Thomas More, and Milton.&amp;quot; From a statistical point of view, citation indexes have serious sampling problems; they tend to produce a sample that is heavily skewed away from the &amp;quot;central and typical&amp;quot; facts of the language that every speaker is expected to know. Large corpus studies, such as the COBUILD dictionary, offer the hope that it might be possible to base a dictionary on a large and representative sample of the language as it is actually used. 6.1 Should a Corpus Be Balanced? Id</context>
</contexts>
<marker>Murray, 1977</marker>
<rawString>Murray, K. (1977). Caught in the Web of Words: James Murray and the Oxford English Dictionary. Yale University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Palermo</author>
<author>J Jenkins</author>
</authors>
<title>Word Association Norms.</title>
<date>1964</date>
<publisher>University of Minnesota Press.</publisher>
<contexts>
<context position="36020" citStr="Palermo and Jenkins (1964)" startWordPosition="5772" endWordPosition="5775">ucial collocational constraints. Lexicographers use the terms collocation, co-occurrence, and lexis to describe various constraints on pairs of words. The words strong and powerful are perhaps the canonical example. Halliday (1966; p. 150) noted that although strong and powerful have similar syntax and semantics, there are contexts where one is much more appropriate than the other (e.g., strong tea vs. powerful computers). Psycholinguists have a similar concept, which they call word associations. Two frequently cited examples of highly associated words are: bread/butter and doctor &apos;nurse. See Palermo and Jenkins (1964) for tables of associations, measured for 200 words, factored by grade level and sex. In general, subjects respond more quickly to a word such as butter when it follows a highly associated word such as bread. Some results and implications are summarized from reaction-time experiments in which subjects either (a) classified successive strings of letters as words and nonwords, or (b) pronounced the strings. Both types of response to words (e.g., BUTTER) were consistently faster when preceded by associated words (e.g., BREAD) rather than unassociated words (e.g, NURSE). (Meyer, Schvaneveldt, and </context>
</contexts>
<marker>Palermo, Jenkins, 1964</marker>
<rawString>Palermo, D., and Jenkins, J. (1964). Word Association Norms. University of Minnesota Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Price</author>
<author>W Fisher</author>
<author>J Bernstein</author>
<author>D Pallett</author>
</authors>
<title>The DARPA 1000-word resource management database for continuous speech recognition.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<volume>1</volume>
<pages>651--654</pages>
<contexts>
<context position="7539" citStr="Price et al. 1988" startWordPosition="1106" endWordPosition="1109">1990): Chapter 5 describes the knowledge-based approach, proposed in the 1970s and early 1980s. The pure knowledge-based approach emulates human speech knowledge using expert systems. Rule-based systems have had only limited success... Chapter 6 describes the stochastic approach. . . Most successful large-scale systems today use a stochastic approach. (Waibel and Lee 1990; P. 4) A number of data collection efforts have helped to bring about this change in the speech community, especially the Texas Instruments&apos; Digit Corpus (Leonard 1984), TIMIT and the DARPA Resource Management (RM) Database (Price et al. 1988). According to the Linguistic Data Consortium (LDC), the RM database was used by every paper that reported speech recognition results in the 1988 Proceedings of IEEE ICASSP, the major technical society meeting where speech recognition results are reported. This is especially significant given that abstracts for this meeting were due just a few months after the release of the corpus, attesting to the speech recognition community&apos;s hunger for standard corpora for development and evaluation. Back in the 1970s, the more data-intensive methods were probably beyond the means of many researchers, esp</context>
</contexts>
<marker>Price, Fisher, Bernstein, Pallett, 1988</marker>
<rawString>Price, P.; Fisher, W.; Bernstein, J.; and Pallett, D. (1988). &amp;quot;The DARPA 1000-word resource management database for continuous speech recognition.&amp;quot; In Proceedings, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1, 651-654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>S Berger</author>
<author>P Anick</author>
</authors>
<title>Lexical semantic techniques for corpus analysis.&amp;quot;</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<note>In press.</note>
<contexts>
<context position="52939" citStr="Pustejovsky et al. (1993)" startWordPosition="8552" endWordPosition="8555">lingual Lexicography, Machine-Readable Dictionaries (MRDs), and Computational Lexicons There has been a long tradition of empiricist approaches in lexicography, both bilingual and monolingual, dating back to Johnson and Murray. As corpus data and machinereadable dictionaries (MRDs) become more and more available, it is becoming easier to compile lexicons for computers and dictionaries for people. This is a particularly exciting area in computational linguistics as evidenced by the large number of contributions in these special issues: Biber (1993), Brent (1993), Hindle and Rooth (this issue), Pustejovsky et al. (1993), and Smadja (this issue). Starting with the COBUILD dictionary (Sinclair et al. 1987), it is now becoming more and more common to find lexicographers working directly with corpus data. Sinclair makes an excellent case for the use of corpus evidence in the preface to the COBUILD dictionary: For the first time, a dictionary has been compiled by the thorough examination of a representative group of English texts, spoken and written, running to many millions of words. This means that in addition to all the tools of the conventional dictionary makers—wide reading and experience of English, other d</context>
<context position="63231" citStr="Pustejovsky et al. (1993)" startWordPosition="10265" endWordPosition="10269">y small and can be omitted. f(strong,enough) f (strong) f (mough) t N N N \f(strong,enough N2 Although statistics are often used to test a particular hypothesis as we have just seen, statistics can also be used to explore the space of possible hypotheses, or to discover new hypotheses (supervised/unsupervised learning/training). See Tukey (1977) and Mosteller and Tukey (1977) for two textbooks on Exploratory Data Analysis (EDA), and Jelinek (1985) for a very nice review paper on self-organizing statistics. Both the exploratory and self-organizing views are represented in these special issues. Pustejovsky et al. (1993) use an EDA approach to investigate certain questions in lexical semantics. Brent (1993), in contrast, adopts a self-organizing approach to identify subcategorization features. Table 9 shows how the t-score can be used in an exploratory mode to extract large numbers of words from the Associated Press (AP) news that co-occur more often with strong than with powerful, and vice versa. It is an interesting question whether collocations are simply idiosyncratic as Halliday and many others have generally assumed (see Smadja [this issue% or whether there might be some general principles that could ac</context>
</contexts>
<marker>Pustejovsky, Berger, Anick, 1993</marker>
<rawString>Pustejovsky, J.; Berger, S.; and Anick, P. (1993). &amp;quot;Lexical semantic techniques for corpus analysis.&amp;quot; Computational Linguistics, 19(2). In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings, IEEE,</booktitle>
<volume>77</volume>
<issue>2</issue>
<pages>257--286</pages>
<note>Reprinted in W &amp; L,</note>
<contexts>
<context position="18357" citStr="Rabiner 1989" startWordPosition="2860" endWordPosition="2861">f description of the 5 Computational Linguistics Volume 19, Number 1 Table 1 Examples of channel confusions in different applications. Application Input Output Speech writer rider Recognition here hear Optical all all (A-one-L) Character of of Recognition form farm Spelling government goverment Correction occurred occured commercial commerical Table 2 Performance after training (Bahl et al. 1975) Training Set Size Test Sentences Correctly Decoded Decoding Problems 0 2/10 8/10 200 77/100 3/100 400 80/100 2/100 600 85/100 1/100 800 82/100 3/100 1070 83/100 3/100 Forward—Backward algorithm, and (Rabiner 1989) for a longer tutorial on HMMs. The general procedure, of which the Forward—Backward algorithm is a special case, was first published and shown to converge by Baum (1972). The first level of the Raleigh system converted spelling to phonemic base forms, rather like a dictionary; the second level dealt with the problems of allophonic variation mentioned above; the third level modeled the front end. At first, the values of the parameters in these HMMs were carefully constructed by hand, but eventually they would all be replaced with estimates obtained by training on real data using statistical es</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Rabiner, L. (1989). &amp;quot;A tutorial on hidden Markov models and selected applications in speech recognition.&amp;quot; In Proceedings, IEEE, 77(2), 257-286. Reprinted in W &amp; L, 267-296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>Automatic Text Processing.</title>
<date>1989</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="33051" citStr="Salton 1989" startWordPosition="5290" endWordPosition="5291">Kennedy appears once in a unit of text (e.g., a paragraph, a discourse, or a genre), then it is much more likely than chance to appear a second time in the same unit of text. Function words also deviate from the binomial, though for different reasons (e.g., stylistic factors mentioned in Biber&apos;s paper). These bursts might serve a useful purpose. People seem to be able to use these bursts to speed up reaction times in various tasks. Psycholinguists use the term priming to refer to this effect. Bursts might also be useful in a number of practical applications such as Information Retrieval (IR) (Salton 1989; Frakes and Baeza-Yates 1992). There have been a number of attempts over the years to model these bursts. The negative 10 Kenneth W. Church and Robert L. Mercer Introduction binomial distribution, for example, was explored in considerable detail in the classic study of the authorship of the Federalist Papers, Mosteller and Wallace (1964), a mustread for anyone interested in statistical analyses of large corpora. We can show that the distribution of Kennedy is very bursty in the Brown Corpus by dividing the corpus into k segments and showing that the probability varies radically from one segme</context>
<context position="66551" citStr="Salton 1989" startWordPosition="10806" endWordPosition="10807">, hidden Markov models (HMMs), entropy (H), mutual information (I), Student&apos;s t-score) have appeared in one form or another, often first in speech, and then soon thereafter in language. Many of the same researchers have applied these methods to a variety of application areas ranging from language modeling for noisy channel applications (e.g., speech recognition, optical character recognition [OCR], and spelling correction [Damerau 1964; Kukich 1992]), to part-of-speech tagging, parsing, translation, lexicography, text compression (Bell, Cleary, and Witten 1990) and information retrieval (IR) (Salton 1989; Frakes and Baeza-Yates 1992). Empiricism is, of course, a very old tradition. Back in the 1950s and 1960s, long before the speech work of the 1970s and 1980s, there was Skinner&apos;s Behaviorism in Psychology, Shannon&apos;s Information Theory in Electrical Engineering, and Harris&apos; Distributional Hypothesis in American Linguistics and the Firthian approach in British Linguistics (&amp;quot;You shall know a word by the company it keeps&amp;quot;). It is possible that much of this work was actually inspired by Turing&apos;s code-breaking efforts during World War II, but we may never know for sure given the necessity for secr</context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>Salton, G. (1989). Automatic Text Processing. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>Z Zhao</author>
<author>C Buckley</author>
</authors>
<title>A simple syntactic approach for the generation of indexing phrases.&amp;quot;</title>
<date>1990</date>
<tech>Technical Report 90-1137,</tech>
<institution>Department of Computer Science, Cornell University.</institution>
<marker>Salton, Zhao, Buckley, 1990</marker>
<rawString>Salton, G.; Zhao, Z.; and Buckley, C. (1990). &amp;quot;A simple syntactic approach for the generation of indexing phrases.&amp;quot; Technical Report 90-1137, Department of Computer Science, Cornell University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sato</author>
<author>M Nagao</author>
</authors>
<title>Towards memory based translation.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 15th International Conference on Computational Linguistics (COLING-90),</booktitle>
<pages>247--252</pages>
<contexts>
<context position="49108" citStr="Sato and Nagao 1990" startWordPosition="7938" endWordPosition="7941"> Table 5, since the estimate for the trigram model in Brown et al. (1992) is computed over a 256-character alphabet, whereas the estimate for human performance in Shannan (1951) is computed over a 27-character alphabet. 3 Lan and Young actually looked at another task involving phonotactic structure where there is also good reason to believe that SCFGs might be able to capture crucial linguistic constraints that might be missed by simpler HMMs. 15 Computational Linguistics Volume 19, Number 1 has tended to favor rationalism, though there are some important exceptions, such as example-based MT (Sato and Nagao 1990). The issue remains as controversial as ever, as evidenced by the lively debate on rationalism versus empiricism at TMI-92, a recent conference on MT.&apos; The paper by Brown et al. (1990) revives Weaver&apos;s information theoretic approach to MT. It requires a bit more squeezing and twisting to fit machine translation into the noisy channel mold: to translate, for example, from French to English, one imagines that the native speaker of French has thought up what he or she wants to say in English and then translates mentally into French before actually saying it. The task of the translation system is </context>
</contexts>
<marker>Sato, Nagao, 1990</marker>
<rawString>Sato, S., and Nagao, M. (1990). &amp;quot;Towards memory based translation.&amp;quot; In Proceedings, 15th International Conference on Computational Linguistics (COLING-90), 247-252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Shannon</author>
</authors>
<title>The mathematical theory of communication.&amp;quot;</title>
<date>1948</date>
<journal>Bell System Technical Journal,</journal>
<pages>27--398</pages>
<contexts>
<context position="14797" citStr="Shannon 1948" startWordPosition="2279" endWordPosition="2280">real data, rather than a complicated set of hand-tuned penalties and bonuses. 4 Kenneth W. Church and Robert L. Mercer Introduction 1.4 The Noisy Channel Model Although the penalties and bonuses were sometimes thought of as probabilities, the early Raleigh system lacked a complete and unified probabilistic framework. In a radical departure from the prevailing attitudes of the time, the Yorktown group turned to Shannon&apos;s theory of communication in the presence of noise and recast the speech recognition problem in terms of transmission through a noisy channel. Shannon&apos;s theory of communication (Shannon 1948), also known as Information Theory, was originally developed at AT&amp;T Bell Laboratories to model communication along a noisy channel such as a telephone line. See Fano (1961) for a well-known secondary source on the subject, or Cover and Thomas (1991) or Bell, Cleary, and Witten (1990) for more recent treatments. The noisy channel paradigm can be applied to other recognition applications such as optical character recognition (OCR) and spelling correction. Imagine a noisy channel, such as a speech recognition machine that almost hears, an optical character recognition (OCR) machine that almost r</context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>Shannon, C. (1948). &amp;quot;The mathematical theory of communication.&amp;quot; Bell System Technical Journal, 27,398-403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Shannon</author>
</authors>
<title>Prediction and entropy of printed English.&amp;quot;</title>
<date>1951</date>
<journal>Bell Systems Technical Journal,</journal>
<pages>30--50</pages>
<contexts>
<context position="43580" citStr="Shannon 1951" startWordPosition="7046" endWordPosition="7047">r, English is not like this. For an English source, it is possible to reduce the average length of the code by assigning shorter codes to more frequent symbols (e.g., e, n, s) and longer codes to less frequent symbols (e.g., j, q, z), using a coding scheme such as a Huffman code (Bell, Cleary, and Witten 1990; Section 5.1.2). Other codes, such as Lempel—Ziv (Welch 1984; Bell, Cleary, and Witten, Chapters 8-9) and n-gram models on words, achieve even better compression by taking advantage of context, though none of these codes seem to perform as well as people do in predicting the next letter (Shannon 1951). 13 Computational Linguistics Volume 19, Number 1 Table 5 Cross entropy of various language models. Model Bits / Character ASCII 8 Huffman code each char 5 Lempel-Ziv (Unix Tm compress) 4.43 Unigram (Huffman code each word) 2.1 (Brown, personal communication) Trigram 1.76 (Brown et al. 1992) Human Performance 1.25 (Shannon 1951) The cross entropy, H, of a code and a source is given by: H(source, code) = - EE Pr(s,h I source) log2 Pr(s I h, code) s h where Pr(s,h I source) is the joint probability of a symbol s following a history h given the source. Pr(s I h, code) is the conditional probabil</context>
</contexts>
<marker>Shannon, 1951</marker>
<rawString>Shannon, C. (1951). &amp;quot;Prediction and entropy of printed English.&amp;quot; Bell Systems Technical Journal, 30,50-64.</rawString>
</citation>
<citation valid="true">
<date>1987</date>
<journal>Collins COBUILD English Language Dictionary. Collins.</journal>
<editor>Sinclair, J.; Hanks, P.; Fox, G.; Moon, R.; and Stock, P., eds.</editor>
<contexts>
<context position="22573" citStr="(1987)" startWordPosition="3527" endWordPosition="3527">echniques. Today, as indicated in the introduction of Waibel and Lee (1990), almost every aspect of most speech recognition systems is dominated by probabilistic models with parameters determined from data. 2. Part-of-Speech Tagging Many of the very same methods are being applied to problems in natural language processing by many of the very same researchers. As a result, the empirical approach has been adopted by almost all contemporary part-of-speech programs: Bahl and Mercer (1976), Leech, Garside, and Atwell (1983), Jelinek (1985), Deroualt and Merialdo (1986), Garside, Leech, and Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al. (1990), deMarcken (1990), Karlsson (1990), Boggess, Agarwal, and Davis (1991), Merialdo (1991), and Voutilainen, Heikkila, and Anttila (1992). These programs input a sequence of words, e.g., The chair will table the motion, and output a sequence of part-of-speech tags, e.g., art noun modal verb art noun. Most of these programs correctly tag at least 95% of the words, with practically no restrictions on the input text, and with very modest space and time requirements. Perhaps the most important indication of succes</context>
<context position="53745" citStr="(1987)" startWordPosition="8684" endWordPosition="8684"> an excellent case for the use of corpus evidence in the preface to the COBUILD dictionary: For the first time, a dictionary has been compiled by the thorough examination of a representative group of English texts, spoken and written, running to many millions of words. This means that in addition to all the tools of the conventional dictionary makers—wide reading and experience of English, other dictionaries and of course eyes and ears—this dictionary is based on hard, measurable evidence. (Sinclair et al. 1987; p. xv) The experience of writing the COBUILD dictionary is documented in Sinclair (1987), a collection of articles from the COBUILD project; see Boguraev (1990) for a strong positive review of this collection. At the time, the corpus-based approach to lexicography was considered pioneering, even somewhat controversial; today, quite a number of the major lexicography houses are collecting large amounts of corpus data. The traditional alternative to corpora are citation indexes, boxes of interesting citations collected on index cards by large numbers of human readers. Unfortunately, citation indexes tend to be a bit like butterfly collections, full of rare and unusual specimens, bu</context>
</contexts>
<marker>1987</marker>
<rawString>Sinclair, J.; Hanks, P.; Fox, G.; Moon, R.; and Stock, P., eds. (1987). Collins COBUILD English Language Dictionary. Collins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sinclair</author>
<author>ed</author>
</authors>
<title>Looking Up:</title>
<date>1987</date>
<booktitle>An Account of the COBUILD Project in Lexical Computing.</booktitle>
<publisher>Collins.</publisher>
<marker>Sinclair, ed, 1987</marker>
<rawString>Sinclair, J., ed. (1987). Looking Up: An Account of the COBUILD Project in Lexical Computing. Collins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tukey</author>
</authors>
<title>Exploratory Data Analysis.</title>
<date>1977</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="62953" citStr="Tukey (1977)" startWordPosition="10226" endWordPosition="10227"> the co-occurrence is significantly larger than chance. The estimation uses the approximation, a2 (Pr (strong, enough)) f (strong, enough) /N2, which can be justified under appropriate binomial assumptions. It is also assumed that a2 (Pr (strong) Pr(enough)) is very small and can be omitted. f(strong,enough) f (strong) f (mough) t N N N \f(strong,enough N2 Although statistics are often used to test a particular hypothesis as we have just seen, statistics can also be used to explore the space of possible hypotheses, or to discover new hypotheses (supervised/unsupervised learning/training). See Tukey (1977) and Mosteller and Tukey (1977) for two textbooks on Exploratory Data Analysis (EDA), and Jelinek (1985) for a very nice review paper on self-organizing statistics. Both the exploratory and self-organizing views are represented in these special issues. Pustejovsky et al. (1993) use an EDA approach to investigate certain questions in lexical semantics. Brent (1993), in contrast, adopts a self-organizing approach to identify subcategorization features. Table 9 shows how the t-score can be used in an exploratory mode to extract large numbers of words from the Associated Press (AP) news that co-oc</context>
</contexts>
<marker>Tukey, 1977</marker>
<rawString>Tukey, J. (1977). Exploratory Data Analysis. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Voutilainen</author>
<author>J Heikkila</author>
<author>A Anttila</author>
</authors>
<title>Constraint grammar of English: A performance-oriented introduction.&amp;quot;</title>
<date>1992</date>
<journal>Publication</journal>
<volume>21</volume>
<institution>University of Helsinki, Department of Linguistics,</institution>
<location>Helsinki, Finland.</location>
<marker>Voutilainen, Heikkila, Anttila, 1992</marker>
<rawString>Voutilainen, A.; Heikkila, J.; and Anttila, A. (1992). &amp;quot;Constraint grammar of English: A performance-oriented introduction.&amp;quot; Publication No. 21, University of Helsinki, Department of Linguistics, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Waibel</author>
<author>K Lee</author>
<author>eds</author>
</authors>
<date>1990</date>
<booktitle>Readings in Speech Recognition.</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<marker>Waibel, Lee, eds, 1990</marker>
<rawString>Waibel, A., and Lee, K., eds. (1990). Readings in Speech Recognition. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Warwick-Armstrong</author>
<author>G Russell</author>
</authors>
<title>Bilingual concordancing and bilingual lexicography.&amp;quot; Euralex</title>
<date>1990</date>
<contexts>
<context position="51848" citStr="Warwick-Armstrong and Russell 1990" startWordPosition="8386" endWordPosition="8389">e is like a monolingual concordance except that each line in the concordance is followed by a line of text in a second language. There are also some hopes that the approach might produce tools that could be useful for human translators (Isabelle 1992). There are three papers in these special issues on aligning bilingual texts such as the Canadian Hansards (parliamentary debates) that are available in both English and French: Brown et al. (1993), Gale and Church (this issue), and Kay and Rosenschein (this issue). Warwick-Armstrong and Russell have also been interested in the alignment problem (Warwick-Armstrong and Russell 1990). Except for Brown et al., this work is focused on the less controversial applications in lexicography and human translation, rather than MT. 4 Requests for a tape of the debate should be sent to the attention of Pierre Isabelle, CCRIT, TMI-92, 1575 boul. Chomedey, Laval (Quebec), H7V 2X2, Canada. Copies of the TMI proceedings can be obtained by writing to CCRIT or sending e-mail to tmi@ccrit.doc.ca. 16 Kenneth W. Church and Robert L. Mercer Introduction 6. Monolingual Lexicography, Machine-Readable Dictionaries (MRDs), and Computational Lexicons There has been a long tradition of empiricist a</context>
</contexts>
<marker>Warwick-Armstrong, Russell, 1990</marker>
<rawString>Warwick-Armstrong, S., and Russell, G. (1990). &amp;quot;Bilingual concordancing and bilingual lexicography.&amp;quot; Euralex 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Weaver</author>
</authors>
<title>Translation.&amp;quot; Reproduced in Machine Translation of Languages,</title>
<date>1949</date>
<pages>15--23</pages>
<publisher>MIT Press,</publisher>
<note>edited in 1955 by</note>
<contexts>
<context position="48139" citStr="Weaver (1949)" startWordPosition="7785" endWordPosition="7786">ructure is more directly relevant than in word recognition. In general, phrase structure is probably more important for understanding who did what to whom, than recognizing what was said.&apos; Some tasks are probably more appropriate for Chomsky&apos;s rational approach to language and other tasks are probably more appropriate for Shannon&apos;s empirical approach to language. Table 6 summarizes some of the differences between the two approaches. 5. Machine Translation and Bilingual Lexicography Is machine translation (MT) more suitable for rationalism or empiricism? Both approaches have been investigated. Weaver (1949) was the first to propose an information theoretic approach to MT. The empirical approach was also practiced at Georgetown during the 1950s and 1960s (Henisz-Dostert, Ross Macdonald, and Zarechnak 1979) in a system that eventually became known as SYSTRAN. Recently, most work in MT 2 In fact, the trigram model might be even better than suggested in Table 5, since the estimate for the trigram model in Brown et al. (1992) is computed over a 256-character alphabet, whereas the estimate for human performance in Shannan (1951) is computed over a 27-character alphabet. 3 Lan and Young actually looked</context>
</contexts>
<marker>Weaver, 1949</marker>
<rawString>Weaver, W. (1949). &amp;quot;Translation.&amp;quot; Reproduced in Machine Translation of Languages, edited in 1955 by W. Locke and A. Booth. MIT Press, 15-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Welch</author>
</authors>
<title>A technique for high performance data compression.&amp;quot;</title>
<date>1984</date>
<journal>Computer,</journal>
<volume>17</volume>
<issue>6</issue>
<pages>8--19</pages>
<contexts>
<context position="43338" citStr="Welch 1984" startWordPosition="7006" endWordPosition="7007"> estimates of their cross entropies with English text. The standard ASCII code requires 8 bits per character. It would be a perfect code if the source produced each of the 28 = 256 symbols equally often and independently of context. However, English is not like this. For an English source, it is possible to reduce the average length of the code by assigning shorter codes to more frequent symbols (e.g., e, n, s) and longer codes to less frequent symbols (e.g., j, q, z), using a coding scheme such as a Huffman code (Bell, Cleary, and Witten 1990; Section 5.1.2). Other codes, such as Lempel—Ziv (Welch 1984; Bell, Cleary, and Witten, Chapters 8-9) and n-gram models on words, achieve even better compression by taking advantage of context, though none of these codes seem to perform as well as people do in predicting the next letter (Shannon 1951). 13 Computational Linguistics Volume 19, Number 1 Table 5 Cross entropy of various language models. Model Bits / Character ASCII 8 Huffman code each char 5 Lempel-Ziv (Unix Tm compress) 4.43 Unigram (Huffman code each word) 2.1 (Brown, personal communication) Trigram 1.76 (Brown et al. 1992) Human Performance 1.25 (Shannon 1951) The cross entropy, H, of a</context>
</contexts>
<marker>Welch, 1984</marker>
<rawString>Welch, T. (1984). &amp;quot;A technique for high performance data compression.&amp;quot; Computer, 17(6), 8-19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Woods</author>
</authors>
<title>Transition networks for natural language analysis.&amp;quot;</title>
<date>1970</date>
<journal>CACM,</journal>
<volume>13</volume>
<issue>10</issue>
<pages>591--606</pages>
<contexts>
<context position="10924" citStr="Woods 1970" startWordPosition="1650" endWordPosition="1651">e inverse that could be used to recover the underlying phonemic representation for a lexical 3 Computational Linguistics Volume 19, Number 1 item. For example, ... Mlle tongue flap ... could have come from a /t/ or a /d/. (Klatt 1980; pp. 548-549) The first DARPA Speech Understanding project emphasized the use of high-level constraints (e.g., syntax, semantics, and pragmatics) as a tool to disambiguate the allophonic information in the speech signal by understanding the message. At BBN, researchers called their system HWIM for (Hear What I Mean). They hoped to use NLP techniques such as ATNs (Woods 1970) to understand the sentences that they were trying to recognize even though the output of their front end was highly variable and ambiguous. The emphasis today on empirical methods in the speech recognition community is a reaction to the failure of knowledge-based approaches of the 1970s. It has become popular once again to focus on high-level natural language constraints in order to reduce the search space. But this time, n-gram methods have become the methods of choice because they seem to work better than the alternatives, at least when the search space is measured in terms of entropy. Idea</context>
</contexts>
<marker>Woods, 1970</marker>
<rawString>Woods, W. (1970). &amp;quot;Transition networks for natural language analysis.&amp;quot; CACM, 13(10), 591-606.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>