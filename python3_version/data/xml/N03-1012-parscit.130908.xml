<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<note confidence="0.835226714285714">
Semantic Coherence Scoring Using an Ontology
Proceedings o
Main Papers , pp. 9-16
Iryna Gurevych Rainer Malaka Robert Porzel Edmonton, May-June 200
Hans-Peter Zorn
f HLT-NAACL 2003
3
</note>
<title confidence="0.472504333333333">
European Media Lab GmbH
Schloss-Wolfsbrunnenweg 31c
D-69118 Heidelberg, Germany
</title>
<email confidence="0.945382">
gurevych,malaka,porzel,zorn@eml.org
</email>
<sectionHeader confidence="0.981434" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999720357142857">
In this paper we present ONTOSCORE, a sys-
tem for scoring sets of concepts on the basis
of an ontology. We apply our system to the
task of scoring alternative speech recognition
hypotheses (SRH) in terms of their semantic
coherence. We conducted an annotation exper-
iment and showed that human annotators can
reliably differentiate between semantically co-
herent and incoherent speech recognition hy-
potheses. An evaluation of our system against
the annotated data shows that, it successfully
classifies 73.2% in a German corpus of 2.284
SRHs as either coherent or incoherent (given a
baseline of 54.55%).
</bodyText>
<sectionHeader confidence="0.997126" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999134580645161">
Following Allen et al. (2001), we can distinguish be-
tween controlled and conversational dialogue systems.
Since controlled and restricted interactions between the
user and the system increase recognition and understand-
ing accuracy, such systems are reliable enough to be de-
ployed in various real world applications, e.g. public
transportation or cinema information systems. The more
conversational a dialogue system becomes, the less pre-
dictable are the users’ utterances. Recognition and pro-
cessing become increasingly difficult and unreliable.
Today’s dialogue systems employ domain- and
discourse-specific knowledge bases, so-called ontologies,
to represent the individual discourse entities as concepts,
and their relations to each other. In this paper we present
an algorithm for measuring the semantic coherence of
sets of concepts against such an ontology. In the fol-
lowing, we will show how the semantic coherence mea-
surement can be applied to estimate how well a given
speech recognition hypothesis (SRH) fits with respect to
the existing knowledge representation, thereby providing
a mechanism that increases the robustness and reliability
of dialogue systems.
In Section 2 we discuss the problem of scoring and
classifying SRHs in terms of their semantic coherence
followed by a description of our annotation experiment.
Section 3 contains a description of the kind of knowledge
representations employed by ONTOSCORE. We present
the algorithm in Section 4, and an evaluation of the cor-
responding system for scoring SRHs is given in Section
5. A conclusion and additional applications are given in
Section 6.
</bodyText>
<sectionHeader confidence="0.964757" genericHeader="introduction">
2 Semantic Coherence and Speech
Recognition Hypotheses
</sectionHeader>
<subsectionHeader confidence="0.991123">
2.1 The Problem
</subsectionHeader>
<bodyText confidence="0.999931888888889">
While a simple one-best hypothesis interface between au-
tomatic speech recognition (ASR) and natural language
understanding (NLU) suffices for restricted dialogue sys-
tems, more complex systems either operate on n-best lists
as ASR) output or convert ASR word graphs (Oerder
and Ney, 1993) into n-best lists, given the distribution of
acoustic and language model scores (Schwartz and Chow,
1990; Tran et al., 1996). For example, in our data a user
expressed the wish to see a specific city map again, as:&apos;
</bodyText>
<listItem confidence="0.461967">
(1) Ich w¨urde die Karte gerne wiedersehen
</listItem>
<bodyText confidence="0.9829655625">
I would the map like to see again
Looking at two SRHs from the ensuing n-best list we
found that Example (1a) constituted a suitable represen-
tation of the utterance, whereas Example (1b) constituted
a less adequate representation thereof.
(1a) Ich w¨urde die Karte eine wieder sehen
I would the map one again see
(1b) Ich w¨urde die Karte eine Wiedersehen
I would the map one Good Bye
Facing multiple representations of a single utterance
consequently poses the question, which of the different
hypotheses corresponds most likely to the user’s utter-
ance. Several ways of solving this problem have been
&apos;All examples are displayed with the German original on top
and a glossed translation below.
proposed and implemented in various systems. Fre-
quently the scores provided by the ASR system itself
are used, e.g. acoustic and language model probabilities.
More recently also scores provided by the NLU system
have been employed, e.g. parsing scores or discourse
scores (Litman et al., 1999; Engel, 2002; Alexanders-
son and Becker, 2003). However, these methods assign
higher scores to SRHs which are semantically incoher-
ent and lower scores to semantically coherent ones and
disagree with other.
For instance, the acoustic and language model scores
of Example (1b) are actually better than for Example (1a),
which results from the fact that the frequencies and cor-
responding probabilities for important expressions, such
as Good Bye, are rather high, thereby ensuring their reli-
able recognition. Another phenomenon found in our data
consists of hypotheses such as:
</bodyText>
<listItem confidence="0.950788">
(2) Zeige mir alle Vergn¨ugen
Show me all pleasures
(3) Zeige mir alle Filmen
Show me all Films
</listItem>
<bodyText confidence="0.999970565217391">
In these cases language model scores are higher for Ex-
ample (2) than Example (3), as the incorrect inflection on
alle Filmen was less frequent in the training material than
that of the correct inflection on alle Vergn¨ugen.
Our data also shows - as one would intuitively ex-
pect - that the understanding-based scores generally re-
flect how well a given SRH is covered by the grammar
employed. In many less well-formed cases these scores
do not correspond to the correctness of the SRH. Gener-
ally we find instances where all existing scoring methods
disagree with each other, diverge from the actual word er-
ror rate and ignore the semantic coherence.2 Neither of
the aforementioned approaches systematically employs
the system’s knowledge of the domains at hand. This in-
creases the number of times where a suboptimal recogni-
tion hypothesis is passed through the system. This means
that, while there was a better representation of the actual
utterance in the n-best list, the NLU system is processing
an inferior one, thereby causing overall dialogue metrics,
in the sense of Walker et al. (2000), to decrease. We pro-
pose an alternative way to rank SRHs on the basis of their
semantic coherence with respect to a given ontology rep-
resenting the domains of the system.
</bodyText>
<subsectionHeader confidence="0.999691">
2.2 Annotation Experiments
</subsectionHeader>
<bodyText confidence="0.99939609375">
In a previous study (Gurevych et al., 2002), we tested if
human annotators could reliably classify SRHs in terms
2As the numbers evident from large vocabulary speech
recognition performance (Cox et al., 2000), the occurrence of
less well formed and incoherent SRHs increases the more con-
versational a system becomes.
of their semantic coherence. The task of the annotators
was to determine whether a given hypothesis representsa
n internally coherent utterance or not.
In order to test the reliability of such annotations, we
collected a corpus of SRHs. The data collection was con-
ducted by means of a hidden operator test. We had 29
subjects prompted to say certain inputs in 8 dialogues.
1.479 turns were recorded. Each user-turn in the dialogue
corresponded to a single intention, e.g. a route request or
a sight information request. The audio files were then
sent to the speech recognizer and the input to the seman-
tic coherence scoring module, i.e. n-best lists of SRHs
were recorded in log-files. The final corpus consisted of
2.284 SRHs. All hypotheses were then randomly mixed
to avoid contextual influences and given to separate an-
notators. The resulting Kappa statistics (Carletta, 1996)
over the annotated data yields , which seems to
indicate that human annotators can reliably distinguish
between coherent samples (as in Example (1a)) and inco-
herent ones (as in Example (1b)).
The aim of the work presented here, then, was to pro-
vide a knowledge-based score, that can be employed by
any NLU system to select the best hypothesis from a
given n-best list. ONTOSCORE, the resulting system will
be described below, followed by its evaluation against the
human gold standard.
</bodyText>
<sectionHeader confidence="0.974751" genericHeader="method">
3 The Knowledge Base
</sectionHeader>
<bodyText confidence="0.998254095238095">
In this section, we provide a description of the pre-
existing knowledge source employed by ONTOSCORE,
as far as it is necessary to understand the empirical data
generated by the system. It is important to note that the
ontology employed in this evaluation existed already and
was crafted as a general knowledge representation for
various processing modules within the system.3
Ontologies have traditionally been used to represent
general and domain specific knowledge and are employed
for various natural language understanding tasks, e.g. se-
mantic interpretation (Allen, 1987). We propose an addi-
tional way of employing ontologies, i.e. to use the knowl-
edge modeled therein as the basis for evaluating the se-
mantic coherence of sets of concepts.
The system described herein can be employed indepen-
dently of the specific ontology language used, as the un-
derlying algorithm operates only on the nodes and named
edges of the directed graph represented by the ontology.
The specific knowledge base, e.g. written in DAML+OIL
or OWL,4 is converted into a graph, consisting of-
f
</bodyText>
<sectionHeader confidence="0.884008" genericHeader="method">
3 Alternative
</sectionHeader>
<bodyText confidence="0.998317916666667">
3Alternative knowledge representations, such as WORD-
NET, could have been employed in theory as well, however
most of the modern domains of the system, e.g. electronic me-
dia or program guides, are not covered by WORDNET.
4DAML+OIL and OWL are frequently used knowledge
modeling languages originating in W3C and Semantic Web
the class hierarchy, with each class corresponding to
a concept representing either an entity or a process;
the slots, i.e. the named edges of the graph corre-
sponding to the class properties, constraints and re-
strictions.
The ontology employed herein has about 730 concepts
and 200 relations. It includes a generic top-level ontol-
ogy whose purpose is to provide a basic structure of the
world, i.e. abstract classes to divide the universe in dis-
tinct parts as resulting from the ontological analysis. The
top-level was developed following the procedure outlined
in Russell and Norvig (1995).
In the view of the ontology employed herein, Role
is the most general class in the ontology and represents
a role that any entity or process can perform. It is di-
vided into Event and Abstract Event. Event is
used to describe a kind of role any entity or process may
have in a ”real” situation or process, e.g. a building or
an information search. It is contrasted with Abstract
Event, which is abstracted from a set of situations and
processes. It reflects no reality and is used for the gen-
eral categorization and description, e.g. Number, Set,
Spatial Relation. There are two kinds of events:
Physical Object and Process.
The class Physical Object describes any kind of
objects we come in contact with - living as well as non-
living - having a location in space and time in contrast to
abstract objects. These objects refer to different domains,
such as Sight and Route in the tourism domain, Av
Medium and Actor in the TV and cinema domain, etc.,
and can be associated with certain relations in the pro-
cesses via slot constraint definitions.
The modeling of Process as a kind of event
that is continuous and homogeneous in nature, follows
the frame semantic analysis used for generating the
FRAMENET data (Baker et al., 1998). Currently, there
are four groups of processes (see Figure 1):
General Process, a set of the most general pro-
cesses such as duplication, imitation or repetition
processes;
Mental Process, a set of processes such as cog-
nitive, emotional or perceptual processes;
Physical Process, a set of processes such as
motion, transaction or controlling processes;
Social Process, a set of processes such as
communication or instruction processes.
Let us consider the definition of the Information
Search Process in the ontology. It is modeled as a
projects. For more detail, see www.w3c.org.
subclass of the Cognitive Process, which is a sub-
class of the Mental Process and inherits the follow-
ing slot constraints:
begin time, a time expression indicating the starting
time point;
end time, a time expression indicating the time
point when the process is complete;
state, one of the abstract process states, e.g. start,
continue, interrupt, etc.;
cognizer, filled with a class Person including its
subclasses.
Information Search Process features one ad-
ditional slot constraint, piece-of-information. The possi-
ble slot-fillers are a range of domain objects, e.g. Sight,
Performance, or whole sets of those, e.g. Tv
Program, but also processes, e.g. Controlling Tv
Device Process. This way, an utterance such as:
</bodyText>
<listItem confidence="0.651774">
(4) I h¨atte gerne Informationen zum Schloss
</listItem>
<bodyText confidence="0.9901922">
I would like information about castle
can also be mapped onto Information Search
Process, which has an agent of type User and apiece
of information of type Sight. Sight has a name of
type Castle. Analogously, the utterance:
</bodyText>
<listItem confidence="0.630443">
(5) Wie kann ich den Fernseher steuern
</listItem>
<bodyText confidence="0.934981">
How can I the TV control
can be mapped onto Information Search
Process, which has an agent of type User and
has a piece of information of type Controlling Tv
Device Process.
</bodyText>
<sectionHeader confidence="0.98207" genericHeader="method">
4 Ontology-based Scoring of SRNs
</sectionHeader>
<bodyText confidence="0.997549">
ONTOSCORE performs a number of processing steps,
each of them will be described separately in the respec-
tive subsections.
</bodyText>
<subsectionHeader confidence="0.999015">
4.1 Mapping of SRH to Sets of Concepts
</subsectionHeader>
<bodyText confidence="0.999485">
A necessary preprocessing step is to convert each SRH
into a concept representation (CR). For that purpose we
augmented the system’s lexicon with specific concept
mappings. That is, for each entry in the lexicon either
zero, one or many corresponding concepts where added.
A simple vector of the concepts, corresponding to the
words in the SRH for which concepts in the lexicon exist,
constitutes the resulting CR. All other words with empty
concept mappings, e.g. articles, are ignored in the con-
version.
</bodyText>
<table confidence="0.987771488888889">
Process
General Process
Mental Process
Physical Process
Social Process
Controlling Commu−
nication Device
Abstract Duplication
Process
Abstract Imitation Process
Abstract Repetition Process
Abstract Replacement
Process
Abstract Reset Process
Cognitive Process
Information Search
Process Planning Process
Emotion Active Emotion Directed
Process Process
Perceptual Process
Watch Perceptual Process
Controlling Enter−
tainment Device
Controlling Process
Controlling Device
Process
Controlling Presen−
tation Device
Controlling Media
Process
Controlling
Representational
Artifact
Verification Process
Motion Process
Communicative
Process
Instructive Process
Emotion Process
Hear Perceptual Process
Emotion Experiencer Emotion Experiencer
Subject Process Object Process
Presentation Process
Static Spatial Process
Transaction Process
</table>
<figureCaption confidence="0.971107">
Figure 1: Upper part of the process hierarchy.
</figureCaption>
<subsectionHeader confidence="0.994769">
4.2 Mapping of CR to Graphs
</subsectionHeader>
<bodyText confidence="0.9999104">
ONTOSCORE converts the domain model, i.e. an ontol-
ogy, into a directed graph with concepts as nodes and re-
lations as edges. One additional problem that needed to
be solved lies in the fact that the directed subclass-ofrela-
tions enable path algorithms to ascend the class hierarchy
upwards, but do not let them descend, therefore missing
a significant set of possible paths. In order to remedy that
situation the graph was enriched during its conversion by
corresponding parent-of relations, which eliminated the
directionality problems as well as avoids cycles and 0-
paths. In order to find the shortest path between two con-
cepts, ONTOSCORE employs the single source shortest
path algorithm of Dijkstra (Cormen et al., 1990).
Given a concept representation CR , ..., , the
algorithm runs once for each concept. The Dijkstra algo-
rithm calculates minimal paths from a source node to all
other nodes. Then, the minimal paths connecting a given
concept with every other concept in CR (excluding
itself) are selected, resulting in an matrix of the
respective paths.
</bodyText>
<subsectionHeader confidence="0.999233">
4.3 The Scoring Algorithm
</subsectionHeader>
<bodyText confidence="0.999955863636364">
To score the minimal paths connecting all concepts with
each other in a given CR, we first adopted a method pro-
posed by Demetriou and Atwell (1994) to score the se-
mantic coherence of alternative sentence interpretations
against graphs based on the Longman Dictionary of Con-
temporary English (LDOCE). To construct the graph the
dictionary lemmata were represented as nodes in an isa
hierarchy and their semantic relations were represented
as edges, which were extracted automatically from the
LDOCE.
As defined by Demetriou and Atwell (1994),
is the set of direct relations (both isa
and semantic relations) that can connect two nodes (con-
cepts); and is the set of corre-
sponding weights, where the weight of each isa relation
is set to and that of each other relation to . For each
two concepts , the set denotes
the scores of all possible paths that link the two concepts.
The score for path can be given as:
where represents the number of times the relation
exists in path . The ensuing distance between two con-
cepts and is, then, defined as the minimum score
The algorithm selects from the set of all paths between
two concepts the one with the smallest weight, i.e. the
cheapest. The distances between all concept pairs in CR
are summed up to a total score. The set of concepts
with the lowest aggregate score represents the combina-
tion with the highest semantic relatedness.
Demetriou and Atwell (1994) do not provide concrete
evaluation results for the method. Also, their algorithm
only allows for a relative judgment stating which of a set
of interpretations given a single sentence is more seman-
tically related.
Since our objective is to compute semantic coherence
scores of arbitrary CRs on an absolute scale, certain ex-
tensions are necessary. In this application, the CRs to
be scored can differ in terms of their content, the num-
ber of concepts contained therein and their mappings to
the original SRH. Moreover, in order to achieve absolute
values, the final score should be related to the number of
concepts in an individual set and the number of words in
the original SRH. Therefore, the results must be normal-
ized in order to allow for evaluation, comparability and
clearer interpretation of the semantic coherence scores.
</bodyText>
<subsectionHeader confidence="0.99747">
4.4 Scoring Concept Representations
</subsectionHeader>
<bodyText confidence="0.987660641025641">
We modified the algorithm described above to make it
applicable and evaluatable with respect to the task at
hand as well as other possible tasks. The basic idea is
to calculate a score based on the path distances in .
Since short distances indicate coherence and many con-
cept pairs in a given may have no connecting path,
we define the distance between two concepts and
that are only connected via isa relations in the knowledge
base as . This maximum value can also serve as a
maximum for long distances and can thus help to prune
the search tree for long paths. This constant has to be
set according to the structure of the knowledge base. For
example, employing the ontology described above, the
maximum distance between two concepts does not ex-
ceed ten and we chose in that case .
We can now define the semantic coherence score for
as the average path length between all concept pairs
in :
Since the ontology is a directed graph, we have
pairs of concepts with possible directed
connections, i.e., a path from concept to concept
may be completely different to that from to or even
be missing. As a symmetric alternative, we may want to
consider a path from to and a path from to to
be semantically equivalent and thus model every relation
in a bidirectional way. We can then compute a symmetric
score as:
ONTOSCORE implements both options. In the ontol-
ogy currently employed by the system some reverse re-
lations can be found, e.g. given =Broadcast and
=Channel, there exists a path from to via the
relation has-channel and a different path from to
via the relation has-broadcast. However, such reverse
relations are only sporadically represented in the ontol-
ogy. Consequently, it is difficult to account for their in-
fluence on in general. That is why we chose the
function for the evaluation, i.e. only the best path
between a given pair of concepts, regardless of
the direction, is taken into account.
</bodyText>
<subsectionHeader confidence="0.962142">
4.5 Word/Concept Relation
</subsectionHeader>
<bodyText confidence="0.997750833333333">
Given the algorithm proposed above, a significant num-
ber of misclassifications for SRHs would result from the
cases when an SRH contains a high proportion of func-
tion words (having no conceptual mappings in the result-
ing CR) and only a few content words. Let’s consider the
following example:
</bodyText>
<listItem confidence="0.667114">
(6) Wo den Informationen zu das gleiche
</listItem>
<bodyText confidence="0.999718333333333">
Where the information to the same
The corresponding CR is constituted out of a
single concept Information Search Process.
ON TOSCORE would classify the CR as coherent with the
highest possible score, as this is the only concept in the
set. This, however, would often lead to misclassifications.
We, therefore, included a post-processing technique that
takes the relation between the number of ontology con-
cepts in a given CR and the total number of words
in the original SRH into account. This relation is defined
by the ratio . ONTOSCORE automatically
classifies an SRH as being incoherent irrespective of its
semantic coherence score, if is less then the threshold
set. The threshold may be set freely. The corresponding
findings are presented in the evaluation section.
</bodyText>
<subsectionHeader confidence="0.869531">
4.6 ONTOSCORE at Work
</subsectionHeader>
<bodyText confidence="0.956629545454545">
Looking at an example of ONTOSCORE at work, we
will examine the utterance given in Example (1). The
resulting two SRHs - and - are given in
Example (1a) and (1b) respectively. The human annota-
tors considered to be coherent and labeled
as incoherent. According to the concept entries in the
lexicon, the SRHs are transformed into two alternative
derived between and , i.e.:
concept representations. As no ambiguous words are
found in this example, corresponds to and
corresponds to :
</bodyText>
<listItem confidence="0.714801">
: Person; Map; Parting Process .
</listItem>
<bodyText confidence="0.977706666666667">
They are converted into a graph. According to the algo-
rithm shown in Section 4.3, all paths between the con-
cepts of each graph are calculated and weighted. This
yields the following non- paths:
via the relation has-watcher;
via the relation has-watchable object.
via the relation has-agent;
The ensuing results are:
According to According to
In both cases the results are sufficient for a relative judg-
ment, i.e. constitutes a less semantically coherent
structure as . To allow for a binary classification
into semantically coherent vs. incoherent samples, a cut-
off threshold must be set. The results of the correspond-
ing experiments will be presented in Section 5.2.
</bodyText>
<subsectionHeader confidence="0.997683">
4.7 Word Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.9806199">
Due to lexical ambiguity, the process of transforming an
n-best list of SRH to concept representations often re-
sults in a set of CRs that is greater than 1, i.e. a given
SRH could be transformed into a set of CRs , ...,
. Word sense disambiguation could, therefore, also
independently be performed using the semantic coher-
ence scoring described herein as an additional application
of our approach. However, that has not been investigated
thoroughly yet.
For example, lexicon entries for the words:
</bodyText>
<table confidence="0.800045666666667">
I - Person
am - Static Spatial Process,
Self Identification Process, None
on - Two Point Relation, None
the - None
Philosopher’s Walk - Location
</table>
<bodyText confidence="0.425778">
yield a set of interpretations for an SRH such as:
</bodyText>
<equation confidence="0.692104">
(7) Ich bin auf dem Philosophenweg
I am on the Philosopher’s Walk
</equation>
<bodyText confidence="0.992505142857143">
and corresponding final scores:
The examination of the resulting scores allows us to
conclude that constitutes the most semantically co-
herent representation of the initial SRH, and
display a slightly lesser degree of semantic coherence,
whereas , and are much less coherent and
may, thus, be considered inadequate.
</bodyText>
<sectionHeader confidence="0.99162" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.983929">
5.1 Context
</subsectionHeader>
<bodyText confidence="0.999980411764706">
The ONTOSCORE software runs as a module in
SMARTKOM (Wahlster et al., 2001), a multi-modal and
multi-domain spoken dialogue system. The system fea-
tures the combination of speech and gesture as its input
and output modalities. The domains of the system in-
clude cinema and TV program information, home elec-
tronic device control, mobile services for tourists, e.g.
tour planning and sights information.
ONTOSCORE operates on n-best lists of SRHs pro-
duced by the language interpretation module out of the
ASR word graphs. It computes a numerical ranking of
alternative SRH and thus provides an important aid to
the understanding component of the system in determin-
ing the best SRH. The ONTOSCORE software employs
two knowledge sources, an ontology (about 730 concepts
and 200 relations) and a word/concept lexicon (ca. 3.600
words), covering the respective domains of the system.
</bodyText>
<subsectionHeader confidence="0.637142">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.99849">
The evaluation of ONTOSCORE was carried out on a
dataset of 2.284 SRHs. We reformulated the problem of
measuring the semantic coherence in terms of classify-
ing the SRHs into two classes: coherent and incoherent.
To our knowledge, there exists no similar software per-
forming semantic coherence scoring to be used for com-
</bodyText>
<table confidence="0.954046">
Person, Static Spatial Process,
Location
Person, Static Spatial Process, Two
Point Relation, Location
Person, Self Identification
Process, Location
Person, Self Identification
Process, Two Point Relation, Location
Person, Two Point Relation,
Location
Person, Location
;
;
;
;
;
;
: Person; Map; Watch Perceptual
Process ;
</table>
<bodyText confidence="0.999425166666667">
parison in this evaluation. Therefore, we decided to use
the results from human annotation (s. Section 2.2) as the
baseline.
A gold standard for the evaluation of ONTOSCORE
was derived by the annotators agreeing on the correct so-
lution in cases of disagreement. This way, we obtained
1.246 (54.55%) SRH classified as coherent by humans,
which is also assumed to be the baseline for this evalua-
tion.
Additionally, we performed an inverse linear transfor-
mation of the scores (which range from 1 to ), so
that the output produced by ONTOSCORE is a score on
the scale from 0 to 1, where higher scores indicate greater
coherence. In order to obtain a binary classification of
SRHs into coherent versus incoherent with respect to the
knowledge base, we set a cutoff thresh old. The depen-
dency graph of the threshold value and the results of the
program in % is shown in Figure 1.
</bodyText>
<figureCaption confidence="0.6624415">
Figure 2: Finding the optimal threshold for the coherent
versus incoherent classification
</figureCaption>
<bodyText confidence="0.99999025">
The best results are achieved with the threshold 0.29.
With this threshold, ONTOSCORE correctly classifies
1.487 SRH, i.e. 65.11% in the evaluation dataset (the
word/concept relation is not taken into account at this
point).
Figure 3 shows the dependency graph between , rep-
resenting the threshold for the word/concept relation and
the results of ONTOSCORE, given the best cutoff thresh-
old for the classification (i.e. 0.29) derived in the previous
experiments.
The best results are achieved with the . In
other words, the proportion of concepts vs. words must
be no less than 1 to 3. Under these settings, ONTOSCORE
correctly classifies 1.672 SRH, i.e. 73.2% in the evalua-
tion dataset. This way, the technique brings an additional
improvement of 8.09% as compared to initial results.
</bodyText>
<sectionHeader confidence="0.985381" genericHeader="conclusions">
6 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.986299310344828">
The ONTOSCORE system described herein automatically
performs ontology-based scoring of sets of concepts con-
Figure 3: Finding the optimal threshold for the
word/concept relation
stituting an adequate representation of speech recogni-
tion hypotheses. To date, the algorithm has been im-
plemented in a software which is employed by a multi-
domain and multi-modal dialogue system and applied
to the task of scoring n-best lists of SRH, thus produc-
ing a score expressing how well a given SRH fits within
the domain model. For this task, it provides an alterna-
tive knowledge-based score next to the ones provided by
the ASR and the NLU system. In the evaluation of our
system we employed an ontology that was not designed
for this task, but already existed as the system’s internal
knowledge representation.
As future work we will examine how the computa-
tion of a discourse dependent semantic coherence score,
i.e. how well a given SRH fits within domain model
with respect to the previous discourse, can improve the
overall score. Additionally, we intend to calculate the
semantic coherence score with respect to individual do-
mains of the system, thus enabling domain recognition
and domain change detection in complex multi-modal
and multi-domain spoken dialogue systems. Currently,
we are also beginning to investigate whether the proposed
method can be applied to scoring sets of potential candi-
dates for resolving the semantic interpretation of ambigu-
ous, polysemous and metonymic language use.
</bodyText>
<sectionHeader confidence="0.9964" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999894666666667">
This work has been partially funded by the German Fed-
eral Ministry of Research and Technology (BMBF) as
part of the SmartKom project under Grant 01 IL 905C/0
and by the Klaus Tschira Foundation. We would like to
thank Michael Strube for his helpful comments on the
previous versions of this paper.
</bodyText>
<sectionHeader confidence="0.998169" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999801279411765">
Jan Alexandersson and Tilman Becker. 2003. The For-
mal Foundations Underlying Overlay. In Proceedings
of the Fifth International Workshop on Computational
Semantics (IWCS-5), Tilburg, The Netherlands, Febru-
ary.
James F. Allen, George Ferguson, and Amanda Stent.
2001. An architecture for more realistic conversational
system. In Proceedings of Intelligent User Interfaces,
pages 1–8, Santa Fe, NM.
James F. Allen. 1987. NaturalLanguage Understanding.
Menlo Park, Cal.: Benjamin Cummings.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceedings
of COLING-ACL, Montreal, Canada.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249–254.
Thomas H. Cormen, Charles E. Leiserson, and Ronald R.
Rivest. 1990. Introduction to Algorithms. MIT press,
Cambridge, MA.
R.V. Cox, C.A. Kamm, L.R. Rabiner, J. Schroeter, and
J.G. Wilpon. 2000. Speech and language process-
ing for next-millenium communications services. Pro-
ceedings of the IEEE, 88(8):1314–1334.
George Demetriou and Eric Atwell. 1994. A seman-
tic network for large vocabulary speech recognition.
In Lindsay Evett and Tony Rose, editors, Proceed-
ings of AISB workshop on Computational Linguistics
for Speech and Handwriting Recognition, University
of Leeds.
Ralf Engel. 2002. SPIN: Language understanding for
spoken dialogue systems using a production system ap-
proach. In Proceedings ofICSLP 2002.
Iryna Gurevych, Robert Porzel, and Michael Strube.
2002. Annotating the semantic consistency of speech
recognition hypotheses. In Proceedings of the Third
SIGdial Workshop on Discourse and Dialogue, pages
46–49, Philadelphia, USA, July.
Diane J. Litman, Marilyn A. Walker, and Michael S.
Kearns. 1999. Automatic detection of poor speech
recognition at the dialogue level. In Proceedings of
the 37th Annual Meeting of the Association for Com-
putational Linguistics, College Park, Md., 20–26 June
1999, pages 309–316.
Martin Oerder and Hermann Ney. 1993. Word
graphs: An efficient interface between continuous-
speech recognition and language understanding. In
ICASSP Volume 2, pages 119–122.
Stuart J. Russell and Peter Norvig. 1995. Artificial In-
telligence. A Modern Approach. Prentice Hall, Engle-
wood Cliffs, N.J.
R. Schwartz and Y. Chow. 1990. The n-best algo-
rithm: an efficient and exact procedure for finding the
n most likely sentence hypotheses. In Proceedings of
ICASSP’90, Albuquerque, USA.
B-H. Tran, F. Seide, V. Steinbiss, R. Schwartz, and
Y. Chow. 1996. A word graph based n-best search
in continuous speech recognition. In Proceedings of
ICSLP’96.
Wolfgang Wahlster, Norbert Reithinger, and Anselm
Blocher. 2001. Smartkom: Multimodal communica-
tion with a life-like character. In Proceedings of the
7th European Conference on Speech Communication
and Technology, pages 1547–1550.
Marilyn A. Walker, Candace A. Kamm, and Diane J. Lit-
man. 2000. Towards developing general model of us-
ability with PARADISE. Natural Language Engeneer-
ing, 6.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.032336">
<title confidence="0.587585">Semantic Coherence Scoring Using an Ontology</title>
<note confidence="0.859852333333333">Proceedings o Main Papers , pp. 9-16 Gurevych Rainer Malaka Robert Porzel May-June 200</note>
<author confidence="0.581237">Hans-Peter Zorn</author>
<address confidence="0.5097">f HLT-NAACL 2003</address>
<email confidence="0.28283">3</email>
<affiliation confidence="0.883963">European Media Lab GmbH</affiliation>
<address confidence="0.794955">Schloss-Wolfsbrunnenweg 31c D-69118 Heidelberg, Germany</address>
<email confidence="0.999407">gurevych,malaka,porzel,zorn@eml.org</email>
<abstract confidence="0.998856066666667">this paper we present a system for scoring sets of concepts on the basis of an ontology. We apply our system to the task of scoring alternative speech recognition hypotheses (SRH) in terms of their semantic coherence. We conducted an annotation experiment and showed that human annotators can reliably differentiate between semantically coherent and incoherent speech recognition hypotheses. An evaluation of our system against the annotated data shows that, it successfully classifies 73.2% in a German corpus of 2.284 SRHs as either coherent or incoherent (given a baseline of 54.55%).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jan Alexandersson</author>
<author>Tilman Becker</author>
</authors>
<title>The Formal Foundations Underlying Overlay.</title>
<date>2003</date>
<booktitle>In Proceedings of the Fifth International Workshop on Computational Semantics (IWCS-5),</booktitle>
<location>Tilburg, The Netherlands,</location>
<contexts>
<context position="4195" citStr="Alexandersson and Becker, 2003" startWordPosition="640" endWordPosition="644">epresentations of a single utterance consequently poses the question, which of the different hypotheses corresponds most likely to the user’s utterance. Several ways of solving this problem have been &apos;All examples are displayed with the German original on top and a glossed translation below. proposed and implemented in various systems. Frequently the scores provided by the ASR system itself are used, e.g. acoustic and language model probabilities. More recently also scores provided by the NLU system have been employed, e.g. parsing scores or discourse scores (Litman et al., 1999; Engel, 2002; Alexandersson and Becker, 2003). However, these methods assign higher scores to SRHs which are semantically incoherent and lower scores to semantically coherent ones and disagree with other. For instance, the acoustic and language model scores of Example (1b) are actually better than for Example (1a), which results from the fact that the frequencies and corresponding probabilities for important expressions, such as Good Bye, are rather high, thereby ensuring their reliable recognition. Another phenomenon found in our data consists of hypotheses such as: (2) Zeige mir alle Vergn¨ugen Show me all pleasures (3) Zeige mir alle </context>
</contexts>
<marker>Alexandersson, Becker, 2003</marker>
<rawString>Jan Alexandersson and Tilman Becker. 2003. The Formal Foundations Underlying Overlay. In Proceedings of the Fifth International Workshop on Computational Semantics (IWCS-5), Tilburg, The Netherlands, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James F Allen</author>
<author>George Ferguson</author>
<author>Amanda Stent</author>
</authors>
<title>An architecture for more realistic conversational system.</title>
<date>2001</date>
<booktitle>In Proceedings of Intelligent User Interfaces,</booktitle>
<pages>1--8</pages>
<location>Santa Fe, NM.</location>
<contexts>
<context position="954" citStr="Allen et al. (2001)" startWordPosition="138" endWordPosition="141">TOSCORE, a system for scoring sets of concepts on the basis of an ontology. We apply our system to the task of scoring alternative speech recognition hypotheses (SRH) in terms of their semantic coherence. We conducted an annotation experiment and showed that human annotators can reliably differentiate between semantically coherent and incoherent speech recognition hypotheses. An evaluation of our system against the annotated data shows that, it successfully classifies 73.2% in a German corpus of 2.284 SRHs as either coherent or incoherent (given a baseline of 54.55%). 1 Introduction Following Allen et al. (2001), we can distinguish between controlled and conversational dialogue systems. Since controlled and restricted interactions between the user and the system increase recognition and understanding accuracy, such systems are reliable enough to be deployed in various real world applications, e.g. public transportation or cinema information systems. The more conversational a dialogue system becomes, the less predictable are the users’ utterances. Recognition and processing become increasingly difficult and unreliable. Today’s dialogue systems employ domain- and discourse-specific knowledge bases, so-</context>
</contexts>
<marker>Allen, Ferguson, Stent, 2001</marker>
<rawString>James F. Allen, George Ferguson, and Amanda Stent. 2001. An architecture for more realistic conversational system. In Proceedings of Intelligent User Interfaces, pages 1–8, Santa Fe, NM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James F Allen</author>
</authors>
<title>NaturalLanguage Understanding.</title>
<date>1987</date>
<location>Menlo Park, Cal.: Benjamin Cummings.</location>
<contexts>
<context position="8392" citStr="Allen, 1987" startWordPosition="1324" endWordPosition="1325">gold standard. 3 The Knowledge Base In this section, we provide a description of the preexisting knowledge source employed by ONTOSCORE, as far as it is necessary to understand the empirical data generated by the system. It is important to note that the ontology employed in this evaluation existed already and was crafted as a general knowledge representation for various processing modules within the system.3 Ontologies have traditionally been used to represent general and domain specific knowledge and are employed for various natural language understanding tasks, e.g. semantic interpretation (Allen, 1987). We propose an additional way of employing ontologies, i.e. to use the knowledge modeled therein as the basis for evaluating the semantic coherence of sets of concepts. The system described herein can be employed independently of the specific ontology language used, as the underlying algorithm operates only on the nodes and named edges of the directed graph represented by the ontology. The specific knowledge base, e.g. written in DAML+OIL or OWL,4 is converted into a graph, consisting off 3 Alternative 3Alternative knowledge representations, such as WORDNET, could have been employed in theory</context>
</contexts>
<marker>Allen, 1987</marker>
<rawString>James F. Allen. 1987. NaturalLanguage Understanding. Menlo Park, Cal.: Benjamin Cummings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet Project.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="11043" citStr="Baker et al., 1998" startWordPosition="1770" endWordPosition="1773">ct and Process. The class Physical Object describes any kind of objects we come in contact with - living as well as nonliving - having a location in space and time in contrast to abstract objects. These objects refer to different domains, such as Sight and Route in the tourism domain, Av Medium and Actor in the TV and cinema domain, etc., and can be associated with certain relations in the processes via slot constraint definitions. The modeling of Process as a kind of event that is continuous and homogeneous in nature, follows the frame semantic analysis used for generating the FRAMENET data (Baker et al., 1998). Currently, there are four groups of processes (see Figure 1): General Process, a set of the most general processes such as duplication, imitation or repetition processes; Mental Process, a set of processes such as cognitive, emotional or perceptual processes; Physical Process, a set of processes such as motion, transaction or controlling processes; Social Process, a set of processes such as communication or instruction processes. Let us consider the definition of the Information Search Process in the ontology. It is modeled as a projects. For more detail, see www.w3c.org. subclass of the Cog</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of COLING-ACL, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="7308" citStr="Carletta, 1996" startWordPosition="1152" endWordPosition="1153">ucted by means of a hidden operator test. We had 29 subjects prompted to say certain inputs in 8 dialogues. 1.479 turns were recorded. Each user-turn in the dialogue corresponded to a single intention, e.g. a route request or a sight information request. The audio files were then sent to the speech recognizer and the input to the semantic coherence scoring module, i.e. n-best lists of SRHs were recorded in log-files. The final corpus consisted of 2.284 SRHs. All hypotheses were then randomly mixed to avoid contextual influences and given to separate annotators. The resulting Kappa statistics (Carletta, 1996) over the annotated data yields , which seems to indicate that human annotators can reliably distinguish between coherent samples (as in Example (1a)) and incoherent ones (as in Example (1b)). The aim of the work presented here, then, was to provide a knowledge-based score, that can be employed by any NLU system to select the best hypothesis from a given n-best list. ONTOSCORE, the resulting system will be described below, followed by its evaluation against the human gold standard. 3 The Knowledge Base In this section, we provide a description of the preexisting knowledge source employed by ON</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Charles E Leiserson</author>
<author>Ronald R Rivest</author>
</authors>
<title>Introduction to Algorithms.</title>
<date>1990</date>
<publisher>MIT press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="15153" citStr="Cormen et al., 1990" startWordPosition="2393" endWordPosition="2396">elations as edges. One additional problem that needed to be solved lies in the fact that the directed subclass-ofrelations enable path algorithms to ascend the class hierarchy upwards, but do not let them descend, therefore missing a significant set of possible paths. In order to remedy that situation the graph was enriched during its conversion by corresponding parent-of relations, which eliminated the directionality problems as well as avoids cycles and 0- paths. In order to find the shortest path between two concepts, ONTOSCORE employs the single source shortest path algorithm of Dijkstra (Cormen et al., 1990). Given a concept representation CR , ..., , the algorithm runs once for each concept. The Dijkstra algorithm calculates minimal paths from a source node to all other nodes. Then, the minimal paths connecting a given concept with every other concept in CR (excluding itself) are selected, resulting in an matrix of the respective paths. 4.3 The Scoring Algorithm To score the minimal paths connecting all concepts with each other in a given CR, we first adopted a method proposed by Demetriou and Atwell (1994) to score the semantic coherence of alternative sentence interpretations against graphs ba</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, 1990</marker>
<rawString>Thomas H. Cormen, Charles E. Leiserson, and Ronald R. Rivest. 1990. Introduction to Algorithms. MIT press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R V Cox</author>
<author>C A Kamm</author>
<author>L R Rabiner</author>
<author>J Schroeter</author>
<author>J G Wilpon</author>
</authors>
<title>Speech and language processing for next-millenium communications services.</title>
<date>2000</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>88--8</pages>
<contexts>
<context position="6319" citStr="Cox et al., 2000" startWordPosition="989" endWordPosition="992">, while there was a better representation of the actual utterance in the n-best list, the NLU system is processing an inferior one, thereby causing overall dialogue metrics, in the sense of Walker et al. (2000), to decrease. We propose an alternative way to rank SRHs on the basis of their semantic coherence with respect to a given ontology representing the domains of the system. 2.2 Annotation Experiments In a previous study (Gurevych et al., 2002), we tested if human annotators could reliably classify SRHs in terms 2As the numbers evident from large vocabulary speech recognition performance (Cox et al., 2000), the occurrence of less well formed and incoherent SRHs increases the more conversational a system becomes. of their semantic coherence. The task of the annotators was to determine whether a given hypothesis representsa n internally coherent utterance or not. In order to test the reliability of such annotations, we collected a corpus of SRHs. The data collection was conducted by means of a hidden operator test. We had 29 subjects prompted to say certain inputs in 8 dialogues. 1.479 turns were recorded. Each user-turn in the dialogue corresponded to a single intention, e.g. a route request or </context>
</contexts>
<marker>Cox, Kamm, Rabiner, Schroeter, Wilpon, 2000</marker>
<rawString>R.V. Cox, C.A. Kamm, L.R. Rabiner, J. Schroeter, and J.G. Wilpon. 2000. Speech and language processing for next-millenium communications services. Proceedings of the IEEE, 88(8):1314–1334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Demetriou</author>
<author>Eric Atwell</author>
</authors>
<title>A semantic network for large vocabulary speech recognition.</title>
<date>1994</date>
<booktitle>Proceedings of AISB workshop on Computational Linguistics for Speech and Handwriting Recognition,</booktitle>
<editor>In Lindsay Evett and Tony Rose, editors,</editor>
<location>University of Leeds.</location>
<contexts>
<context position="15663" citStr="Demetriou and Atwell (1994)" startWordPosition="2480" endWordPosition="2483">h between two concepts, ONTOSCORE employs the single source shortest path algorithm of Dijkstra (Cormen et al., 1990). Given a concept representation CR , ..., , the algorithm runs once for each concept. The Dijkstra algorithm calculates minimal paths from a source node to all other nodes. Then, the minimal paths connecting a given concept with every other concept in CR (excluding itself) are selected, resulting in an matrix of the respective paths. 4.3 The Scoring Algorithm To score the minimal paths connecting all concepts with each other in a given CR, we first adopted a method proposed by Demetriou and Atwell (1994) to score the semantic coherence of alternative sentence interpretations against graphs based on the Longman Dictionary of Contemporary English (LDOCE). To construct the graph the dictionary lemmata were represented as nodes in an isa hierarchy and their semantic relations were represented as edges, which were extracted automatically from the LDOCE. As defined by Demetriou and Atwell (1994), is the set of direct relations (both isa and semantic relations) that can connect two nodes (concepts); and is the set of corresponding weights, where the weight of each isa relation is set to and that of </context>
<context position="16922" citStr="Demetriou and Atwell (1994)" startWordPosition="2695" endWordPosition="2698">h two concepts , the set denotes the scores of all possible paths that link the two concepts. The score for path can be given as: where represents the number of times the relation exists in path . The ensuing distance between two concepts and is, then, defined as the minimum score The algorithm selects from the set of all paths between two concepts the one with the smallest weight, i.e. the cheapest. The distances between all concept pairs in CR are summed up to a total score. The set of concepts with the lowest aggregate score represents the combination with the highest semantic relatedness. Demetriou and Atwell (1994) do not provide concrete evaluation results for the method. Also, their algorithm only allows for a relative judgment stating which of a set of interpretations given a single sentence is more semantically related. Since our objective is to compute semantic coherence scores of arbitrary CRs on an absolute scale, certain extensions are necessary. In this application, the CRs to be scored can differ in terms of their content, the number of concepts contained therein and their mappings to the original SRH. Moreover, in order to achieve absolute values, the final score should be related to the numb</context>
</contexts>
<marker>Demetriou, Atwell, 1994</marker>
<rawString>George Demetriou and Eric Atwell. 1994. A semantic network for large vocabulary speech recognition. In Lindsay Evett and Tony Rose, editors, Proceedings of AISB workshop on Computational Linguistics for Speech and Handwriting Recognition, University of Leeds.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Engel</author>
</authors>
<title>SPIN: Language understanding for spoken dialogue systems using a production system approach.</title>
<date>2002</date>
<booktitle>In Proceedings ofICSLP</booktitle>
<contexts>
<context position="4162" citStr="Engel, 2002" startWordPosition="638" endWordPosition="639">ng multiple representations of a single utterance consequently poses the question, which of the different hypotheses corresponds most likely to the user’s utterance. Several ways of solving this problem have been &apos;All examples are displayed with the German original on top and a glossed translation below. proposed and implemented in various systems. Frequently the scores provided by the ASR system itself are used, e.g. acoustic and language model probabilities. More recently also scores provided by the NLU system have been employed, e.g. parsing scores or discourse scores (Litman et al., 1999; Engel, 2002; Alexandersson and Becker, 2003). However, these methods assign higher scores to SRHs which are semantically incoherent and lower scores to semantically coherent ones and disagree with other. For instance, the acoustic and language model scores of Example (1b) are actually better than for Example (1a), which results from the fact that the frequencies and corresponding probabilities for important expressions, such as Good Bye, are rather high, thereby ensuring their reliable recognition. Another phenomenon found in our data consists of hypotheses such as: (2) Zeige mir alle Vergn¨ugen Show me </context>
</contexts>
<marker>Engel, 2002</marker>
<rawString>Ralf Engel. 2002. SPIN: Language understanding for spoken dialogue systems using a production system approach. In Proceedings ofICSLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
<author>Robert Porzel</author>
<author>Michael Strube</author>
</authors>
<title>Annotating the semantic consistency of speech recognition hypotheses.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>46--49</pages>
<location>Philadelphia, USA,</location>
<contexts>
<context position="6154" citStr="Gurevych et al., 2002" startWordPosition="964" endWordPosition="967"> the system’s knowledge of the domains at hand. This increases the number of times where a suboptimal recognition hypothesis is passed through the system. This means that, while there was a better representation of the actual utterance in the n-best list, the NLU system is processing an inferior one, thereby causing overall dialogue metrics, in the sense of Walker et al. (2000), to decrease. We propose an alternative way to rank SRHs on the basis of their semantic coherence with respect to a given ontology representing the domains of the system. 2.2 Annotation Experiments In a previous study (Gurevych et al., 2002), we tested if human annotators could reliably classify SRHs in terms 2As the numbers evident from large vocabulary speech recognition performance (Cox et al., 2000), the occurrence of less well formed and incoherent SRHs increases the more conversational a system becomes. of their semantic coherence. The task of the annotators was to determine whether a given hypothesis representsa n internally coherent utterance or not. In order to test the reliability of such annotations, we collected a corpus of SRHs. The data collection was conducted by means of a hidden operator test. We had 29 subjects </context>
</contexts>
<marker>Gurevych, Porzel, Strube, 2002</marker>
<rawString>Iryna Gurevych, Robert Porzel, and Michael Strube. 2002. Annotating the semantic consistency of speech recognition hypotheses. In Proceedings of the Third SIGdial Workshop on Discourse and Dialogue, pages 46–49, Philadelphia, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
<author>Marilyn A Walker</author>
<author>Michael S Kearns</author>
</authors>
<title>Automatic detection of poor speech recognition at the dialogue level.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>309--316</pages>
<location>College Park, Md.,</location>
<contexts>
<context position="4149" citStr="Litman et al., 1999" startWordPosition="634" endWordPosition="637">map one Good Bye Facing multiple representations of a single utterance consequently poses the question, which of the different hypotheses corresponds most likely to the user’s utterance. Several ways of solving this problem have been &apos;All examples are displayed with the German original on top and a glossed translation below. proposed and implemented in various systems. Frequently the scores provided by the ASR system itself are used, e.g. acoustic and language model probabilities. More recently also scores provided by the NLU system have been employed, e.g. parsing scores or discourse scores (Litman et al., 1999; Engel, 2002; Alexandersson and Becker, 2003). However, these methods assign higher scores to SRHs which are semantically incoherent and lower scores to semantically coherent ones and disagree with other. For instance, the acoustic and language model scores of Example (1b) are actually better than for Example (1a), which results from the fact that the frequencies and corresponding probabilities for important expressions, such as Good Bye, are rather high, thereby ensuring their reliable recognition. Another phenomenon found in our data consists of hypotheses such as: (2) Zeige mir alle Vergn¨</context>
</contexts>
<marker>Litman, Walker, Kearns, 1999</marker>
<rawString>Diane J. Litman, Marilyn A. Walker, and Michael S. Kearns. 1999. Automatic detection of poor speech recognition at the dialogue level. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, College Park, Md., 20–26 June 1999, pages 309–316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Oerder</author>
<author>Hermann Ney</author>
</authors>
<title>Word graphs: An efficient interface between continuousspeech recognition and language understanding.</title>
<date>1993</date>
<booktitle>In ICASSP</booktitle>
<volume>2</volume>
<pages>119--122</pages>
<contexts>
<context position="2902" citStr="Oerder and Ney, 1993" startWordPosition="428" endWordPosition="431">cription of the kind of knowledge representations employed by ONTOSCORE. We present the algorithm in Section 4, and an evaluation of the corresponding system for scoring SRHs is given in Section 5. A conclusion and additional applications are given in Section 6. 2 Semantic Coherence and Speech Recognition Hypotheses 2.1 The Problem While a simple one-best hypothesis interface between automatic speech recognition (ASR) and natural language understanding (NLU) suffices for restricted dialogue systems, more complex systems either operate on n-best lists as ASR) output or convert ASR word graphs (Oerder and Ney, 1993) into n-best lists, given the distribution of acoustic and language model scores (Schwartz and Chow, 1990; Tran et al., 1996). For example, in our data a user expressed the wish to see a specific city map again, as:&apos; (1) Ich w¨urde die Karte gerne wiedersehen I would the map like to see again Looking at two SRHs from the ensuing n-best list we found that Example (1a) constituted a suitable representation of the utterance, whereas Example (1b) constituted a less adequate representation thereof. (1a) Ich w¨urde die Karte eine wieder sehen I would the map one again see (1b) Ich w¨urde die Karte e</context>
</contexts>
<marker>Oerder, Ney, 1993</marker>
<rawString>Martin Oerder and Hermann Ney. 1993. Word graphs: An efficient interface between continuousspeech recognition and language understanding. In ICASSP Volume 2, pages 119–122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart J Russell</author>
<author>Peter Norvig</author>
</authors>
<title>Artificial Intelligence. A Modern Approach.</title>
<date>1995</date>
<publisher>Prentice Hall,</publisher>
<location>Englewood Cliffs, N.J.</location>
<contexts>
<context position="9814" citStr="Russell and Norvig (1995)" startWordPosition="1554" endWordPosition="1557">riginating in W3C and Semantic Web the class hierarchy, with each class corresponding to a concept representing either an entity or a process; the slots, i.e. the named edges of the graph corresponding to the class properties, constraints and restrictions. The ontology employed herein has about 730 concepts and 200 relations. It includes a generic top-level ontology whose purpose is to provide a basic structure of the world, i.e. abstract classes to divide the universe in distinct parts as resulting from the ontological analysis. The top-level was developed following the procedure outlined in Russell and Norvig (1995). In the view of the ontology employed herein, Role is the most general class in the ontology and represents a role that any entity or process can perform. It is divided into Event and Abstract Event. Event is used to describe a kind of role any entity or process may have in a ”real” situation or process, e.g. a building or an information search. It is contrasted with Abstract Event, which is abstracted from a set of situations and processes. It reflects no reality and is used for the general categorization and description, e.g. Number, Set, Spatial Relation. There are two kinds of events: Phy</context>
</contexts>
<marker>Russell, Norvig, 1995</marker>
<rawString>Stuart J. Russell and Peter Norvig. 1995. Artificial Intelligence. A Modern Approach. Prentice Hall, Englewood Cliffs, N.J.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schwartz</author>
<author>Y Chow</author>
</authors>
<title>The n-best algorithm: an efficient and exact procedure for finding the n most likely sentence hypotheses.</title>
<date>1990</date>
<booktitle>In Proceedings of ICASSP’90,</booktitle>
<location>Albuquerque, USA.</location>
<contexts>
<context position="3007" citStr="Schwartz and Chow, 1990" startWordPosition="444" endWordPosition="447">ction 4, and an evaluation of the corresponding system for scoring SRHs is given in Section 5. A conclusion and additional applications are given in Section 6. 2 Semantic Coherence and Speech Recognition Hypotheses 2.1 The Problem While a simple one-best hypothesis interface between automatic speech recognition (ASR) and natural language understanding (NLU) suffices for restricted dialogue systems, more complex systems either operate on n-best lists as ASR) output or convert ASR word graphs (Oerder and Ney, 1993) into n-best lists, given the distribution of acoustic and language model scores (Schwartz and Chow, 1990; Tran et al., 1996). For example, in our data a user expressed the wish to see a specific city map again, as:&apos; (1) Ich w¨urde die Karte gerne wiedersehen I would the map like to see again Looking at two SRHs from the ensuing n-best list we found that Example (1a) constituted a suitable representation of the utterance, whereas Example (1b) constituted a less adequate representation thereof. (1a) Ich w¨urde die Karte eine wieder sehen I would the map one again see (1b) Ich w¨urde die Karte eine Wiedersehen I would the map one Good Bye Facing multiple representations of a single utterance conseq</context>
</contexts>
<marker>Schwartz, Chow, 1990</marker>
<rawString>R. Schwartz and Y. Chow. 1990. The n-best algorithm: an efficient and exact procedure for finding the n most likely sentence hypotheses. In Proceedings of ICASSP’90, Albuquerque, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B-H Tran</author>
<author>F Seide</author>
<author>V Steinbiss</author>
<author>R Schwartz</author>
<author>Y Chow</author>
</authors>
<title>A word graph based n-best search in continuous speech recognition.</title>
<date>1996</date>
<booktitle>In Proceedings of ICSLP’96.</booktitle>
<contexts>
<context position="3027" citStr="Tran et al., 1996" startWordPosition="448" endWordPosition="451">n of the corresponding system for scoring SRHs is given in Section 5. A conclusion and additional applications are given in Section 6. 2 Semantic Coherence and Speech Recognition Hypotheses 2.1 The Problem While a simple one-best hypothesis interface between automatic speech recognition (ASR) and natural language understanding (NLU) suffices for restricted dialogue systems, more complex systems either operate on n-best lists as ASR) output or convert ASR word graphs (Oerder and Ney, 1993) into n-best lists, given the distribution of acoustic and language model scores (Schwartz and Chow, 1990; Tran et al., 1996). For example, in our data a user expressed the wish to see a specific city map again, as:&apos; (1) Ich w¨urde die Karte gerne wiedersehen I would the map like to see again Looking at two SRHs from the ensuing n-best list we found that Example (1a) constituted a suitable representation of the utterance, whereas Example (1b) constituted a less adequate representation thereof. (1a) Ich w¨urde die Karte eine wieder sehen I would the map one again see (1b) Ich w¨urde die Karte eine Wiedersehen I would the map one Good Bye Facing multiple representations of a single utterance consequently poses the que</context>
</contexts>
<marker>Tran, Seide, Steinbiss, Schwartz, Chow, 1996</marker>
<rawString>B-H. Tran, F. Seide, V. Steinbiss, R. Schwartz, and Y. Chow. 1996. A word graph based n-best search in continuous speech recognition. In Proceedings of ICSLP’96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Wahlster</author>
<author>Norbert Reithinger</author>
<author>Anselm Blocher</author>
</authors>
<title>Smartkom: Multimodal communication with a life-like character.</title>
<date>2001</date>
<booktitle>In Proceedings of the 7th European Conference on Speech Communication and Technology,</booktitle>
<pages>1547--1550</pages>
<contexts>
<context position="23240" citStr="Wahlster et al., 2001" startWordPosition="3753" endWordPosition="3756">tion Process, None on - Two Point Relation, None the - None Philosopher’s Walk - Location yield a set of interpretations for an SRH such as: (7) Ich bin auf dem Philosophenweg I am on the Philosopher’s Walk and corresponding final scores: The examination of the resulting scores allows us to conclude that constitutes the most semantically coherent representation of the initial SRH, and display a slightly lesser degree of semantic coherence, whereas , and are much less coherent and may, thus, be considered inadequate. 5 Evaluation 5.1 Context The ONTOSCORE software runs as a module in SMARTKOM (Wahlster et al., 2001), a multi-modal and multi-domain spoken dialogue system. The system features the combination of speech and gesture as its input and output modalities. The domains of the system include cinema and TV program information, home electronic device control, mobile services for tourists, e.g. tour planning and sights information. ONTOSCORE operates on n-best lists of SRHs produced by the language interpretation module out of the ASR word graphs. It computes a numerical ranking of alternative SRH and thus provides an important aid to the understanding component of the system in determining the best SR</context>
</contexts>
<marker>Wahlster, Reithinger, Blocher, 2001</marker>
<rawString>Wolfgang Wahlster, Norbert Reithinger, and Anselm Blocher. 2001. Smartkom: Multimodal communication with a life-like character. In Proceedings of the 7th European Conference on Speech Communication and Technology, pages 1547–1550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Candace A Kamm</author>
<author>Diane J Litman</author>
</authors>
<title>Towards developing general model of usability with PARADISE.</title>
<date>2000</date>
<journal>Natural Language Engeneering,</journal>
<volume>6</volume>
<contexts>
<context position="5912" citStr="Walker et al. (2000)" startWordPosition="922" endWordPosition="925">of the SRH. Generally we find instances where all existing scoring methods disagree with each other, diverge from the actual word error rate and ignore the semantic coherence.2 Neither of the aforementioned approaches systematically employs the system’s knowledge of the domains at hand. This increases the number of times where a suboptimal recognition hypothesis is passed through the system. This means that, while there was a better representation of the actual utterance in the n-best list, the NLU system is processing an inferior one, thereby causing overall dialogue metrics, in the sense of Walker et al. (2000), to decrease. We propose an alternative way to rank SRHs on the basis of their semantic coherence with respect to a given ontology representing the domains of the system. 2.2 Annotation Experiments In a previous study (Gurevych et al., 2002), we tested if human annotators could reliably classify SRHs in terms 2As the numbers evident from large vocabulary speech recognition performance (Cox et al., 2000), the occurrence of less well formed and incoherent SRHs increases the more conversational a system becomes. of their semantic coherence. The task of the annotators was to determine whether a g</context>
</contexts>
<marker>Walker, Kamm, Litman, 2000</marker>
<rawString>Marilyn A. Walker, Candace A. Kamm, and Diane J. Litman. 2000. Towards developing general model of usability with PARADISE. Natural Language Engeneering, 6.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>