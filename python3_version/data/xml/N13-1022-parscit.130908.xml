<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000105">
<title confidence="0.997331">
Using Out-of-Domain Data for Lexical Addressee Detection in
Human-Human-Computer Dialog
</title>
<author confidence="0.994209">
Heeyoung Lee&apos;* Andreas Stolcke2 Elizabeth Shriberg2
</author>
<affiliation confidence="0.8001525">
&apos;Dept. of Electrical Engineering, Stanford University, Stanford, California, USA
2Microsoft Research, Mountain View, California, USA
</affiliation>
<email confidence="0.997795">
heeyoung@stanford.edu, {anstolck,elshribe}@microsoft.com
</email>
<sectionHeader confidence="0.998596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99248109375">
Addressee detection (AD) is an important
problem for dialog systems in human-human-
computer scenarios (contexts involving mul-
tiple people and a system) because system-
directed speech must be distinguished from
human-directed speech. Recent work on AD
(Shriberg et al., 2012) showed good results
using prosodic and lexical features trained on
in-domain data. In-domain data, however, is
expensive to collect for each new domain. In
this study we focus on lexical models and in-
vestigate how well out-of-domain data (either
outside the domain, or from single-user sce-
narios) can fill in for matched in-domain data.
We find that human-addressed speech can be
modeled using out-of-domain conversational
speech transcripts, and that human-computer
utterances can be modeled using single-user
data: the resulting AD system outperforms
a system trained only on matched in-domain
data. Further gains (up to a 4% reduction in
equal error rate) are obtained when in-domain
and out-of-domain models are interpolated.
Finally, we examine which parts of an utter-
ance are most useful. We find that the first
1.5 seconds of an utterance contain most of
the lexical information for AD, and analyze
which lexical items convey this. Overall, we
conclude that the H-H-C scenario can be ap-
proximated by combining data from H-C and
H-H scenarios only.
*Work done while first author was an intern with Microsoft.
</bodyText>
<sectionHeader confidence="0.999571" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973625">
Before a spoken dialog system can recognize and in-
terpret a user’s speech, it should ideally determine
if speech was even meant to be interpreted by the
system. We refer to this task as addressee detec-
tion (AD). AD is often overlooked, especially in tra-
ditional single-user scenarios, because with the ex-
ception of self-talk, side-talk or background speech,
the majority of speech is usually system-directed.
As dialog systems expand to more natural contexts
and multiperson environments, however, AD can be-
come a crucial part of the system’s operational re-
quirements. This is particularly true for systems in
which explicit system addressing (e.g., push-to-talk
or required keyword addressing) is undesirable.
Past research on addressee detection has focused
on human-human (H-H) settings, such as meetings,
sometimes with multimodal cues (op den Akker and
Traum, 2009). Early systems relied primarily on re-
jection of H-H utterances either because they could
not be interpreted (Paek et al., 2000), or because they
yielded low speech recognition confidence (Dowd-
ing et al., 2006). Some systems combine gaze
with lexical and syntactic cues to detect H-H speech
(Katzenmaier et al., 2004). Others use relatively
simple prosodic features based on pitch and energy
in addition to those derived from automatic speech
recognition (ASR) (Reich et al., 2011).
With some exceptions (Bohus and Horvitz, 2011;
Shriberg et al., 2012), relatively little work has
looked at the human-human-computer (H-H-C) sce-
nario, i.e. at contexts involving two or more people
who interact both with a system and with each other.
</bodyText>
<page confidence="0.977129">
221
</page>
<note confidence="0.4726425">
Proceedings of NAACL-HLT 2013, pages 221–229,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999661604166667">
Shriberg et al. (2012) found that novel prosodic
features were more accurate than lexical or seman-
tic features based on speech recognition for the ad-
dressee task. The corpus, also used herein, is com-
prised of H-H-C dialog in which roughly half of the
computer-addressed speech consisted of a small set
of fixed commands. While the word-based features
map directly to the commands, they had trouble
distinguishing all other (noncommand) computer-
directed speech from human-directed speech. This
is because addressee detection in the H-H-C sce-
nario becomes even more challenging when the sys-
tem is designed for natural speech, i.e., utterances
that are conversational in form and not limited to
command phrases with restricted syntax. Further-
more, H-H utterances can be about the domain of
the system (e.g., discussing the dialog task), mak-
ing AD based on language content more difficult.
The prosodic features were good at both types of
distinctions—even improving performance signifi-
cantly when combined with true-word (cheating)
lexical features that have 100% accuracy on the
commands. Nevertheless, the prior work showed
that lexical n-grams are useful for addressee detec-
tion in the H-H-C scenario.
A problem with lexical features is that they are
highly task- and domain-dependent. As with other
language modeling tasks, one usually has to collect
matched training data in significant quantities. Data
collection is made more cumbersome and expensive
by the multi-user aspect of the scenario. Thus, for
practical reasons alone, it would be much better if
the language models for AD could be trained on
out-of-domain data, and if whatever in-domain data
is needed could be limited to single-user interac-
tion. We show in this paper that precisely this train-
ing scenario is feasible and achieves results that are
comparable or better than using completely matched
H-H-C training data.
In addition to studying the role of out-of-domain
data for lexical AD models, we also examine which
words are useful, and how soon in elapsed time they
are available. Whereas most prior work in AD has
looked at processing of entire utterances, we con-
sider an online processing version where AD deci-
sions are to be made as soon as possible after an
utterance was initiated. We find that most of the
addressee-relevant lexical information can be found
</bodyText>
<figureCaption confidence="0.995944">
Figure 1: Conversational Browser dialog system en-
vironment with multi-human scenario
</figureCaption>
<bodyText confidence="0.9991805">
in the first 1.5 seconds, and analyze which words
convey this information.
</bodyText>
<sectionHeader confidence="0.987875" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.996770333333333">
We use in-domain and out-of-domain data from var-
ious sources. The corpora used in this work differ in
size, domain, and scenario.
</bodyText>
<subsectionHeader confidence="0.964252">
2.1 In-domain data
</subsectionHeader>
<bodyText confidence="0.999979863636364">
In-domain data is collected from interactions be-
tween two users and a “Conversational Browser”
(CB) spoken dialog system. We used the same
methodology as Shriberg et al. (2012), but using ad-
ditional data. As depicted in Figure 1, the system
shows a browser on a large TV screen and users
are asked to use natural language for a variety of
information-seeking tasks. For more details about
the dialog system and language understanding ap-
proach, see Hakkani-T¨ur et al. (2011a; 2011b).
We split the in-domain data into training, devel-
opment, and test sets, preserving sessions. Each ses-
sion is about 5 to 40 minutes long. Even though
the whole conversation is recorded, only the seg-
ments captured by the speech recognition system
are used in our experiments. Each utterance seg-
ment belongs to one of four types: computer-
command (C-command), comprising navigational
commands to the system, computer-noncommand
(C-noncommand), which are computer-directed ut-
terances other than commands, human-directed (H),
and mixed (M) utterances, which contain a combina-
</bodyText>
<page confidence="0.998696">
222
</page>
<tableCaption confidence="0.997846">
Table 1: In-domain corpus
</tableCaption>
<table confidence="0.71485619047619">
(a) Sizes, distribution, and ASR word error rates of in-
domain utterance types
Table 2: Out-of-domain corpora. “Single-user CB”
is a corpus collected in same environment as the H-
H-C in-domain data, except that only a single user
was present.
Data set Train Dev Test WER
Transcribed words 6,490 11,298 9,486
ASR words 4,649 6,360 5,514 59.3%
H (%) 19.1 48.6 37.0 87.6%
C-noncomm. (%) 38.3 27.8 32.2 32.6%
C-command (%) 39.9 18.7 27.2 19.7%
M (%) 2.7 4.9 3.6 69.6%
(b) Example utterances by type
Type Example
H Do you want to watch a
movie?
C-noncommand How is the weather today?
C-command Scroll down, Go back.
M Show me sandwich shops.
Oh, are you vegetarian?
</table>
<bodyText confidence="0.999805133333334">
tion of human- and computer-directed speech. The
sizes and distribution of all utterance types, as well
as sample utterances are shown in Table 1.
The ASR system used in the system was based on
off-the-shelf acoustic models and had only the lan-
guage model adapted to the domain, using very lim-
ited data. Consequently, as shown in the right-most
column of Table 1(a), the word error rates (WERs)
are quite high, especially for human-directed utter-
ances. While these could be improved with tar-
geted effort, we consider this a realistic application
scenario, where in-domain training data is typically
scarce, at least early in the development process.
Therefore, any lexically based AD methods need to
be robust to poor ASR accuracy.
</bodyText>
<subsectionHeader confidence="0.994199">
2.2 Out-of-domain data
</subsectionHeader>
<bodyText confidence="0.999956666666667">
To replace the hard-to-obtain in-domain H-H-C data
for training, we use the four out-of-domain corpora
(two H-C and two H-H) shown in Table 2.
Single-user CB data comes from the same Con-
versational Browser system as the in-domain data,
but with only one user present. This data can there-
fore be used for modeling H-C speech. Bing anchor
text (Huang et al., 2010) is a large n-gram corpus of
anchor text associated with links on web pages en-
</bodyText>
<table confidence="0.9177404">
Corpus Addressee Size
Single-user CB H-C 21.9k words
Bing anchor text H-C 1.3B bigrams
Fisher H-H 21M words
ICSI meetings H-H 0.7M words
</table>
<figureCaption confidence="0.959311">
Figure 2: Language model-based score computation
for addressee detection
</figureCaption>
<bodyText confidence="0.999753461538462">
countered by the Bing search engine. When users
want to follow a link displayed on screen, they usu-
ally speak a variant of the anchor text for the link.
We hypothesized that this corpus might aid the mod-
eling of computer-noncommand type utterances in
which such “verbal clicks” are frequent. Fisher tele-
phone conversations and ICSI meetings are both cor-
pora of human-directed speech. The Fisher corpus
(Cieri et al., 2004) comprises two-person telephone
conversations between strangers on prescribed top-
ics. The ICSI meeting corpus (Janin et al., 2003)
contains multiparty face-to-face technical discus-
sions among colleagues.
</bodyText>
<sectionHeader confidence="0.997516" genericHeader="method">
3 Method
</sectionHeader>
<subsectionHeader confidence="0.999714">
3.1 Language modeling for addressee detection
</subsectionHeader>
<bodyText confidence="0.999957125">
We use a lexical AD system that is based on mod-
eling word n-grams in the two addressee-based ut-
terance classes, H (for H-H) and C (for H-C utter-
ances). This approach is similar to language model-
based approaches to speaker and language recogni-
tion, and was shown to be quite effective for this
task (Shriberg et al., 2012). Instead of making
hard decisions, the system outputs a score that is
</bodyText>
<figure confidence="0.996659">
in-domain
(HH)
Fisher, ICSI
meeting –
out-of-domain
Single user CB,
Bing –
out-of-domain
Language model for
computer directed
utterances (C)
P(WlH)
�og
W
1
P(WIC)
P(WlH)
P(WIC)
Language model for
human directed
utterances (H)
in-domain
(HC)
</figure>
<page confidence="0.995974">
223
</page>
<bodyText confidence="0.9924785">
the length-normalized likelihood ratio of the two
classes:
</bodyText>
<equation confidence="0.998904">
1 P(w|C)
|w |log P(w|H), (1)
</equation>
<bodyText confidence="0.999976307692308">
where |w |is the number of words in the recognition
output w for an utterance. P(w|C) and P(w|H) are
obtained from class-specific language models. Fig-
ure 2 gives a flow-chart of the score computation.
Class likelihoods are obtained from standard tri-
gram backoff language models, using Witten-Bell
discounting for smoothing (Witten and Bell, 1991).
For combining various training data sources, we use
language model adaptation by interpolation (Bel-
legarda, 2004). First, a separate model is trained
from each source. The probability estimates from
in-domain and out-of-domain models are then aver-
aged in a weighted fashion:
</bodyText>
<equation confidence="0.994315">
P(wk|hk) = APin(wk|hk) + (1 − A)P..t(wk|hk)
</equation>
<bodyText confidence="0.953908">
(2)
where wk is the k-th word, hk is the (n − 1)-gram
history for the word wk. A is the interpolation weight
and is obtained by tuning a task-related metric on the
development set. We investigated optimizing A for
either model perplexity or classification accuracy, as
discussed below.
</bodyText>
<subsectionHeader confidence="0.999269">
3.2 Part-of-speech-based modeling
</subsectionHeader>
<bodyText confidence="0.962307107692308">
So far we have only been modeling the lexical forms
of words in utterances. If we encounter a word never
before seen, it would appear as an out-of-vocabulary
item in all class-specific language models, and not
contribute much to the decision. More generally, if
a word is rare, its n-gram statistics will be unreliable
and poorly modeled by the system. (The sparseness
issue is exacerbated by small amounts of training
data as in our scenario.)
One common approach to deal with data sparse-
ness in language modeling is to model n-grams over
word classes rather than raw words (Brown et al.,
1992). For example, if we have an utterance How
is the weather in Paris?, the addressee probabilities
are likely to be similar had we seen London instead
of Paris. Therefore, replacing words with properly
chosen word class labels can give better generaliza-
tion from the observed training data. Among the
many methods proposed to class words for language
modeling purposes we chose part-of-speech (POS)
tagging over other, purely data-derived classing al-
gorithms (Brown et al., 1992), for two reasons. First,
our goal here is not to minimize the perplexity of the
data, but to enhance discrimination among utterance
classes. Second, a data-driven class inference algo-
rithm would suffer from the same sparseness issues
when it comes to unseen and rare words (as no ro-
bust statistics are available to infer an unseen word’s
best class in the class induction step). A POS tag-
ger, on the other hand, can do quite well on unseen
words, using context and morphological cues.
A hidden Markov model tagger using POS-
trigram statistics and context-independent class
membership probabilities was used for tagging all
LM training data. The tagger itself had been
trained on the Switchboard (conversational tele-
phone speech) transcripts of the Penn Treebank-
3 corpus (Marcus et al., 1999), and used the 39
Treebank POS labels. To strike a compromise be-
tween generalization and discriminative power in
the language model, we retained the top N most fre-
quent word types from the in-domain training data
as distinct tokens, and varied N as a metaparam-
eter. Barzilay and Lee (2003) used a similar idea
to generalize patterns by substituting words with
slots. This strategy will tend to preserve words that
are either generally frequent function and domain-
independent words, capturing stylistic and syntac-
tic patterns, or which are frequent domain-specific
words, and can thus help characterize computer-
directed utterances.
Here is a sample sentence and its transformed ver-
sion:
Original: Let’s find an Italian restaurant
around this area.
POS-tagged: Let’s find an JJ NN around this
area.
The words except Italian and restaurant are un-
changed because they are in the list of N most fre-
quent words. We transformed all training and test
data in this fashion and then modeled n-gram statis-
tics as before. The one exception was the Bing
anchor-text data, which was only available in the
form of word n-grams (the sentence context required
for accurate POS tagging was missing).
</bodyText>
<page confidence="0.998675">
224
</page>
<tableCaption confidence="0.999152">
Table 3: Addressee detection performance (EER) with different training sets
</tableCaption>
<table confidence="0.9998592">
ASR Transcript
Baseline (in-domain only) 31.1 17.3
Fisher+ICSI, Single-user CB+Bing (out-of-domain only) 27.8 14.2
Baseline + Fisher+ICSI, Single CB + Bing (both-all) 26.9 14.0
Baseline + ICSI, Single-user CB (both-small) 26.6 13.0
</table>
<subsectionHeader confidence="0.967075">
3.3 Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.9805395">
Let’s find an Italian restaurant around this area
Typically, an application-dependent threshold would
be applied to the decision score to convert it into a
binary decision. The optimal threshold is a func-
tion of prior class probabilities and error costs. As
in Shriberg et al. (2012), we used equal error rate
(EER) to compare systems, since we are interested
in the discriminative power of the decision score in-
dependent of priors and costs. EER is the probability
of false detections and misses at the operating point
at which the two types of errors are equally proba-
ble. A prior-free metric such as EER is more mean-
ingful than classification accuracy because the utter-
ance type distribution is heavily skewed (Table 1),
and because the rate of human- versus computer-
directed speech can vary widely depending on the
particular people, domain, and context. We also use
classification accuracy (based on data priors) in one
analysis below, because EERs are not comparable
for different test data subdivisions.
</bodyText>
<subsectionHeader confidence="0.607607">
3.4 Online model
</subsectionHeader>
<bodyText confidence="0.999752375">
The actual dialog system used in this work pro-
cesses utterances after receiving an entire segment
of speech from the recognition subsystem. How-
ever, we envision that a future version of the sys-
tem would perform addressee detection in an online
manner, making a decision as soon as enough evi-
dence is gathered. This raises the question how soon
the addressee can be detected once the user starts
speaking. We simulate this processing mode using a
windowed AD model.
As shown in Figure 3, we define windows start-
ing at the beginning of the utterance and investigate
how AD performance changes as a function of win-
dow size. We use only the words and n-grams falling
completely within a given window. For example, the
word find would be excluded from Window 1 in Fig-
</bodyText>
<figureCaption confidence="0.998445">
Figure 3: The window model
</figureCaption>
<bodyText confidence="0.9951384">
ure 3.
The benefit of early detection in this case is that
once speech is classified as human-directed, it does
not need to be sent to the speech recognizer and sub-
sequent semantic processing. This saves processing
time, especially if processing happens on a server.
Based on the window model performance, we can
assess the feasibility of an online AD model, which
can be approached by shifting the detection window
through time and finding addressee changes.
</bodyText>
<sectionHeader confidence="0.999817" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.971240411764706">
Table 3 compares the performance of our system us-
ing various training data sources. For diagnostic pur-
poses we also compare performance based on recog-
nized words (the realistic scenario) to that based on
human transcripts (idealized, best-case word recog-
nition).
Somewhat surprisingly, the system trained on out-
of-domain data alone performs better by 3.3 EER
points on ASR output and 3.1 points on transcripts
compared to the in-domain baseline. Combining
in-domain and out-of-domain data (both-all, both-
small) gives about 1 point additional EER gain. Note
that training on in-domain data plus the smaller-size
out-of-domain corpora (both-small) is better than
using all available data (both-all).
Figure 4 shows the detection error trade-off
(DET) between false alarm and miss errors for the
</bodyText>
<figure confidence="0.583204">
Window 2 ...
Window 1
</figure>
<page confidence="0.940557">
225
</page>
<figureCaption confidence="0.940190166666667">
Figure 4: Detection error trade-off (DET) curves for
the systems in Table 3. Thin lines at the top right
corner use ASR output (1-4); thick lines at the bot-
tom left corner use reference transcripts (5-8). Each
line number represents one of the systems in Table 3:
1,5 = in-domain only, 2,6 = out-of-domain only, 4,7
</figureCaption>
<bodyText confidence="0.991264692307692">
= both-all, 3,8 = both-small.
systems in Table 3. The DET plot depicts perfor-
mance not only at the EER operating point (which
lies on the diagonal), but over the range of possible
trade-offs between false alarm and miss error rates.
As can be seen, replacing or combining in-domain
data with out-of-domain data gives clear perfor-
mance gains, regardless of operating point (score
threshold), and for both reference and recognized
words.
Figure 5 shows H-H vs. H-C classification accu-
racies on each of the four utterance subtypes listed
in Table 1. It is clear that computer-command ut-
terances are the easiest to classify; the accuracy is
more than 90% using transcripts, and more than 85%
using ASR output. This is not surprising, since
commands are from a fixed small set of phrases.
The biggest gain from use of out-of-domain data
is found for computer-directed noncommand utter-
ances. This is helpful, since in general it is the
noncommand computer-directed utterances (rather
than the commands) that are highly confusable with
human-directed utterances: both use unconstrained
natural language. We note that H-H utterance are
very poorly recognized in the ASR condition when
only out-of-domain data is used. This may be be-
</bodyText>
<figureCaption confidence="0.997438">
Figure 5: AD accuracies by utterance type
</figureCaption>
<tableCaption confidence="0.98246375">
Table 4: Perplexities (computed on dev set ASR
words) by utterance type, for different training cor-
pora. Interpolation refers to the combination of the
three models listed in each case.
</tableCaption>
<table confidence="0.9993478">
Training set Test class
H-C H-H
In-domain H-C (ASR) 257 1856
Single-user CB 104 1237
Bing anchor text 356 789
Interpolation 58 370
In-domain H-H (ASR) 887 1483
Fisher 995 795
ICSI meeting 2007 1583
Interpolation 355 442
</table>
<bodyText confidence="0.997116066666667">
cause the human-human corpora used in training
consist of transcripts, whereas the ASR output for
human-directed utterances is very errorful, creating
a severe train-test mismatch.
As for the optimization of the mixing weight A,
we found that minimizing perplexity on the devel-
opment set of each class is effective. This is a
standard optimization approach for interpolated lan-
guage models, and can be carried out efficiently us-
ing an expectation maximization algorithm. We also
tried search-based optimization using the classifica-
tion metric (EER) as the criterion. While this ap-
proach could theoretically give better results (since
perplexity is not a discriminative criterion) we found
no significant improvement in our experiments.
</bodyText>
<figure confidence="0.992700227272728">
3
4
2 1
5
8 7 6
100
40
90
80
70
50
30
60
20
REF
ASR
baseline(in-
domain only)
out-of-domain
only
both-all
both-small
</figure>
<page confidence="0.997929">
226
</page>
<bodyText confidence="0.999342145833333">
Table 4 shows the perplexities by class of lan-
guage models trained on different corpora. We can
take these as an indication of training/test mismatch
(lower perplexity indicating better match). We also
find substantial perplexity reductions from interpo-
lating models. In order to make perplexities compa-
rable, we trained all models using the union of the
vocabularies from the different sources.
In spite of perplexity being a good way to opti-
mize the weighting of sources, it is not clear that it
is a good criterion for selecting data sources. For
example, we see that the Fisher model has a much
lower perplexity on H-H utterances than the ICSI
meeting model. However, as reflected in Table 3,
the H language model that leaves out the Fisher data
actually performed better. The most likely expla-
nation is that the Fisher corpus is an order of mag-
nitude larger than the ICSI corpus, and that sheer
data size, not stylistic similarity, may account for the
lower perplexity of the Fisher model. Further inves-
tigation is needed regarding good criteria for corpus
selection for classification tasks such as AD.
Table 5 shows the EER performance of the POS-
based model, for various sizes N of the most-
frequent word list. We observe that the partial re-
placement of words with POS tags indeed improves
over the baseline model performance, by 1.5 points
on ASR output and by 1.1 points on transcripts.
We also see that the gain over the corresponding
word-only model is largest for the in-domain base-
line model, and less or non-existent for the out-of-
domain model. This is consistent with the notion
that the in-domain model suffers the most from data
sparseness, and therefore has the most to gain from
better generalization.
Interpolating with out-of-domain data still helps
here. The optimal N differs for ASR output versus
transcripts. The POS-based model with N = 300
improves the EER by 0.5 points on ASR output,
and N = 1000 improves the EER by 0.8 points on
transcripts. Here we use relatively large amounts of
training data, thus the performance gain is smaller,
though still meaningful.
Figure 6 shows the performance of the system
using time windows anchored at the beginnings of
utterances. We incrementally increase the window
width from 0.5 seconds to 3 seconds and compare
results to using full utterances. The leveling off of
</bodyText>
<figureCaption confidence="0.942601">
Figure 6: Simulated online performance on incre-
mental windows
</figureCaption>
<tableCaption confidence="0.992078">
Table 6: The top 15 first words in utterances
</tableCaption>
<table confidence="0.9995854375">
ASR H-C Transcript H-C ASR H-H Transcript H-H
go go play I
scroll scroll go ohh
start start is so
show stop it yeah
stop show what it’s
bing find this you
search Bing show uh
find search how okay
play pause bing what
pause play select it
look look okay and
what uh does that’s
select what start is
how how so no
the ohh I we
</table>
<bodyText confidence="0.999000642857143">
the error plots indicates that most addressee infor-
mation is contained in the first 1 to 1.5 seconds,
although some additional information is found in
the later part of utterances (the plots never level off
completely). This pattern holds for both in-domain
and out-of-domain training, as well as for combined
models.
To give an intuitive understanding of where this
early addressee-relevant information comes from,
we tabulated the top 15 word unigrams in each ut-
terance class, are shown in Table 6. Note that
the substantial differences between the third and
fourth columns in the table reflect the high ASR
error rate for human-directed utterances, whereas
</bodyText>
<figure confidence="0.990130888888889">
Equal error rate
0.40
0.35
0.30
0.25
0.20
0.15
0.10
0.5 1 1.5 2 3 full
Window width (seconds)
ASR baseline
ASR outside
ASR both-all
ASR both-small
REF baseline
REF outside
REF both-all
REF both-small
</figure>
<page confidence="0.987821">
227
</page>
<tableCaption confidence="0.99926">
Table 5: Performance of POS-based model with various top-N word lists (EER)
</tableCaption>
<table confidence="0.998192333333333">
Training data top100 top200 top300 top400 top500 top1000 top2000 Original
ASR baseline 31.6 31.0 29.6 30.1 30.2 31.4 31.5 31.1
out-of-domain only 36.5 37.0 37.2 36.9 36.8 36.6 37.3 27.8
both-all 28.2 26.6 26.1 26.7 27.4 26.9 27.6 26.9
both-small 28.0 26.5 26.2 26.6 26.4 26.3 26.5 26.6
REF baseline 17.1 16.2 16.6 17.1 16.7 17.0 17.2 17.3
out-of-domain only 17.6 17.6 17.5 17.2 17.1 17.2 18.1 14.2
both-all 12.5 12.5 12.5 12.7 12.8 13.2 13.5 14.0
both-small 13.0 13.2 12.8 13.2 12.8 12.2 12.7 13.0
</table>
<bodyText confidence="0.999097636363636">
for computer-directed utterances, the frequent first
words are mostly recognized correctly.
In computer-directed utterances we see mostly
command verbs, which, due to the imperative syn-
tax of these commands occur in utterance-initial po-
sition. Human-directed utterances are characterized
by subject pronouns such as I and it, or answer parti-
cles such as yeah and okay, which likewise occur in
initial position. Based on word frequency and syn-
tax alone it is thus clear why the beginnings of utter-
ances contain strong lexical cues.
</bodyText>
<sectionHeader confidence="0.999348" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999996766666667">
We explored the use of outside data for training
lexical addressee detection systems for the human-
human-computer scenario. Advantages include sav-
ing the time and expense of an in-domain data col-
lection, as well as performance gains even when
some in-domain data is available. We show that H-
C training data can be obtained from a single-user
H-C collection, and that H-H speech can be mod-
eled using general conversational speech. Using the
outside training data, we obtain results that are even
better than results using matched (but smaller) H-
H-C training data. Results can be improved consid-
erably by adapting H-C and H-H language models
with small amounts of matched H-H-C data, via in-
terpolation. The main reason for the improvement is
better detection of computer-directed noncommand
utterances, which tend to be confusable with human-
directed utterances. Another effective way to over-
come scarce training data is to replace the less fre-
quent words with part-of-speech labels. In both
baseline and interpolated model, we found that POS-
based models that keep an appropriate number of the
top N most frequent word types can further improve
the system’s performance.
In a second study we found that the most salient
phrases for lexical addressee detection occur within
the first 1 to 1.5 seconds of speech in each utter-
ance. It reflects a syntactic tendency of class-specific
words to occur utterance-initially, which shows the
feasibility of the online AD system.
</bodyText>
<sectionHeader confidence="0.999075" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999907125">
We thank our Microsoft colleagues Madhu
Chinthakunta, Dilek Hakkani-T¨ur, Larry Heck,
Lisa Stiefelman, and Gokhan T¨ur for developing
the dialog system used in this work, as well as for
many valuable discussions. Ashley Fidler was in
charge of much of the data collection and annotation
required for this study. We also thank Dan Jurafsky
for useful feedback.
</bodyText>
<page confidence="0.996865">
228
</page>
<sectionHeader confidence="0.998342" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999953730769231">
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings HLT-NAACL
2003, pages 16–23, Edmonton, Canada.
Jerome R. Bellegarda. 2004. Statistical language model
adaptation: review and perspectives. Speech Commu-
nication, 42:93–108.
Dan Bohus and Eric Horvitz. 2011. Multiparty turn tak-
ing in situated dialog: Study, lessons, and directions.
In Proceedings ACL SIGDIAL, pages 98–109, Port-
land, OR.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jenifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467–479.
Christopher Cieri, David Miller, and Kevin Walker.
2004. The Fisher corpus: a resource for the next gen-
erations of speech-to-text. In Proceedings 4th Interna-
tional Conference on Language Resources and Evalu-
ation, pages 69–71, Lisbon.
John Dowding, Richard Alena, William J. Clancey,
Maarten Sierhuis, and Jeffrey Graham. 2006. Are
you talking to me? dialogue systems supporting mixed
teams of humans and robots. In Proccedings AAAI
Fall Symposium: Aurally Informed Performance: Inte-
grating Machine Listening and Auditory Presentation
in Robotic Systems, Washington, DC.
Dilek Hakkani-T¨ur, Gokhan Tur, and Larry Heck. 2011a.
Research challenges and opportunities in mobile appli-
cations [dsp education]. IEEE Signal Processing Mag-
azine, 28(4):108 –110.
Dilek Z. Hakkani-T¨ur, G¨okhan T¨ur, Larry P. Heck, and
Elizabeth Shriberg. 2011b. Bootstrapping domain de-
tection using query click logs for new domains. In
Proceedings Interspeech, pages 709–712.
Jian Huang, Jianfeng Gao, Jiangbo Miao, Xiaolong Li,
Kuansang Wang, and Fritz Behr. 2010. Exploring web
scale language models for search query processing. In
Proceedings 19th International Conference on World
Wide Web, pages 451–460, Raleigh, NC.
Adam Janin, Don Baron, Jane Edwards, Dan Ellis,
David Gelbart, Nelson Morgan, Barbara Peskin, Thilo
Pfau, Elizabeth Shriberg, Andreas Stolcke, and Chuck
Wooters. 2003. The ICSI meeting corpus. In Pro-
ceedings IEEE ICASSP, volume 1, pages 364–367,
Hong Kong.
Michael Katzenmaier, Rainer Stiefelhagen, and Tanja
Schultz. 2004. Identifying the addressee in human-
human-robot interactions based on head pose and
speech. In Proceedings 6th International Conference
on Multimodal Interfaces, ICMI, pages 144–151, New
York, NY, USA. ACM.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3.
Linguistic Data Consortium, catalog item LDC99T42.
Rieks op den Akker and David Traum. 2009. A com-
parison of addressee detection methods for multiparty
conversations. In Proceedings of Diaholmia, pages
99–106.
Tim Paek, Eric Horvitz, and Eric Ringger. 2000. Con-
tinuous listening for unconstrained spoken dialog. In
Proceedings ICSLP, volume 1, pages 138–141, Bei-
jing.
Daniel Reich, Felix Putze, Dominic Heger, Joris Ijssel-
muiden, Rainer Stiefelhagen, and Tanja Schultz. 2011.
A real-time speech command detector for a smart con-
trol room. In Proceedings Interspeech, pages 2641–
2644, Florence.
Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-T¨ur,
and Larry Heck. 2012. Learning when to listen:
Detecting system-addressed speech in human-human-
computer dialog. In Proceedings Interspeech, Port-
land, OR.
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: Estimating the probabilities of
novel events in adaptive text compression. IEEE
Transactions on Information Theory, 37(4):1085–
1094.
</reference>
<page confidence="0.998914">
229
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.315297">
<title confidence="0.990414">Using Out-of-Domain Data for Lexical Addressee Detection Human-Human-Computer Dialog</title>
<author confidence="0.999117">Andreas Elizabeth</author>
<affiliation confidence="0.674673">of Electrical Engineering, Stanford University, Stanford, California,</affiliation>
<address confidence="0.796481">Research, Mountain View, California,</address>
<abstract confidence="0.984297727272727">Addressee detection (AD) is an important problem for dialog systems in human-humancomputer scenarios (contexts involving multiple people and a system) because systemdirected speech must be distinguished from human-directed speech. Recent work on AD (Shriberg et al., 2012) showed good results using prosodic and lexical features trained on in-domain data. In-domain data, however, is expensive to collect for each new domain. In this study we focus on lexical models and investigate how well out-of-domain data (either outside the domain, or from single-user scenarios) can fill in for matched in-domain data. We find that human-addressed speech can be modeled using out-of-domain conversational speech transcripts, and that human-computer utterances can be modeled using single-user data: the resulting AD system outperforms a system trained only on matched in-domain data. Further gains (up to a 4% reduction in equal error rate) are obtained when in-domain and out-of-domain models are interpolated. Finally, we examine which parts of an utterance are most useful. We find that the first 1.5 seconds of an utterance contain most of the lexical information for AD, and analyze which lexical items convey this. Overall, we conclude that the H-H-C scenario can be approximated by combining data from H-C and H-H scenarios only. done while first author was an intern with Microsoft.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: An unsupervised approach using multiplesequence alignment.</title>
<date>2003</date>
<booktitle>In Proceedings HLT-NAACL 2003,</booktitle>
<pages>16--23</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="13821" citStr="Barzilay and Lee (2003)" startWordPosition="2197" endWordPosition="2200">text and morphological cues. A hidden Markov model tagger using POStrigram statistics and context-independent class membership probabilities was used for tagging all LM training data. The tagger itself had been trained on the Switchboard (conversational telephone speech) transcripts of the Penn Treebank3 corpus (Marcus et al., 1999), and used the 39 Treebank POS labels. To strike a compromise between generalization and discriminative power in the language model, we retained the top N most frequent word types from the in-domain training data as distinct tokens, and varied N as a metaparameter. Barzilay and Lee (2003) used a similar idea to generalize patterns by substituting words with slots. This strategy will tend to preserve words that are either generally frequent function and domainindependent words, capturing stylistic and syntactic patterns, or which are frequent domain-specific words, and can thus help characterize computerdirected utterances. Here is a sample sentence and its transformed version: Original: Let’s find an Italian restaurant around this area. POS-tagged: Let’s find an JJ NN around this area. The words except Italian and restaurant are unchanged because they are in the list of N most</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: An unsupervised approach using multiplesequence alignment. In Proceedings HLT-NAACL 2003, pages 16–23, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome R Bellegarda</author>
</authors>
<title>Statistical language model adaptation: review and perspectives.</title>
<date>2004</date>
<journal>Speech Communication,</journal>
<pages>42--93</pages>
<contexts>
<context position="11143" citStr="Bellegarda, 2004" startWordPosition="1757" endWordPosition="1759">Language model for human directed utterances (H) in-domain (HC) 223 the length-normalized likelihood ratio of the two classes: 1 P(w|C) |w |log P(w|H), (1) where |w |is the number of words in the recognition output w for an utterance. P(w|C) and P(w|H) are obtained from class-specific language models. Figure 2 gives a flow-chart of the score computation. Class likelihoods are obtained from standard trigram backoff language models, using Witten-Bell discounting for smoothing (Witten and Bell, 1991). For combining various training data sources, we use language model adaptation by interpolation (Bellegarda, 2004). First, a separate model is trained from each source. The probability estimates from in-domain and out-of-domain models are then averaged in a weighted fashion: P(wk|hk) = APin(wk|hk) + (1 − A)P..t(wk|hk) (2) where wk is the k-th word, hk is the (n − 1)-gram history for the word wk. A is the interpolation weight and is obtained by tuning a task-related metric on the development set. We investigated optimizing A for either model perplexity or classification accuracy, as discussed below. 3.2 Part-of-speech-based modeling So far we have only been modeling the lexical forms of words in utterances</context>
</contexts>
<marker>Bellegarda, 2004</marker>
<rawString>Jerome R. Bellegarda. 2004. Statistical language model adaptation: review and perspectives. Speech Communication, 42:93–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Bohus</author>
<author>Eric Horvitz</author>
</authors>
<title>Multiparty turn taking in situated dialog: Study, lessons, and directions.</title>
<date>2011</date>
<booktitle>In Proceedings ACL SIGDIAL,</booktitle>
<pages>98--109</pages>
<location>Portland, OR.</location>
<contexts>
<context position="3140" citStr="Bohus and Horvitz, 2011" startWordPosition="466" endWordPosition="469">(H-H) settings, such as meetings, sometimes with multimodal cues (op den Akker and Traum, 2009). Early systems relied primarily on rejection of H-H utterances either because they could not be interpreted (Paek et al., 2000), or because they yielded low speech recognition confidence (Dowding et al., 2006). Some systems combine gaze with lexical and syntactic cues to detect H-H speech (Katzenmaier et al., 2004). Others use relatively simple prosodic features based on pitch and energy in addition to those derived from automatic speech recognition (ASR) (Reich et al., 2011). With some exceptions (Bohus and Horvitz, 2011; Shriberg et al., 2012), relatively little work has looked at the human-human-computer (H-H-C) scenario, i.e. at contexts involving two or more people who interact both with a system and with each other. 221 Proceedings of NAACL-HLT 2013, pages 221–229, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics Shriberg et al. (2012) found that novel prosodic features were more accurate than lexical or semantic features based on speech recognition for the addressee task. The corpus, also used herein, is comprised of H-H-C dialog in which roughly half of the computer-ad</context>
</contexts>
<marker>Bohus, Horvitz, 2011</marker>
<rawString>Dan Bohus and Eric Horvitz. 2011. Multiparty turn taking in situated dialog: Study, lessons, and directions. In Proceedings ACL SIGDIAL, pages 98–109, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jenifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="12264" citStr="Brown et al., 1992" startWordPosition="1942" endWordPosition="1945">f-speech-based modeling So far we have only been modeling the lexical forms of words in utterances. If we encounter a word never before seen, it would appear as an out-of-vocabulary item in all class-specific language models, and not contribute much to the decision. More generally, if a word is rare, its n-gram statistics will be unreliable and poorly modeled by the system. (The sparseness issue is exacerbated by small amounts of training data as in our scenario.) One common approach to deal with data sparseness in language modeling is to model n-grams over word classes rather than raw words (Brown et al., 1992). For example, if we have an utterance How is the weather in Paris?, the addressee probabilities are likely to be similar had we seen London instead of Paris. Therefore, replacing words with properly chosen word class labels can give better generalization from the observed training data. Among the many methods proposed to class words for language modeling purposes we chose part-of-speech (POS) tagging over other, purely data-derived classing algorithms (Brown et al., 1992), for two reasons. First, our goal here is not to minimize the perplexity of the data, but to enhance discrimination among </context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer. 1992. Classbased n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Cieri</author>
<author>David Miller</author>
<author>Kevin Walker</author>
</authors>
<title>The Fisher corpus: a resource for the next generations of speech-to-text.</title>
<date>2004</date>
<booktitle>In Proceedings 4th International Conference on Language Resources and Evaluation,</booktitle>
<pages>69--71</pages>
<location>Lisbon.</location>
<contexts>
<context position="9698" citStr="Cieri et al., 2004" startWordPosition="1530" endWordPosition="1533">see Size Single-user CB H-C 21.9k words Bing anchor text H-C 1.3B bigrams Fisher H-H 21M words ICSI meetings H-H 0.7M words Figure 2: Language model-based score computation for addressee detection countered by the Bing search engine. When users want to follow a link displayed on screen, they usually speak a variant of the anchor text for the link. We hypothesized that this corpus might aid the modeling of computer-noncommand type utterances in which such “verbal clicks” are frequent. Fisher telephone conversations and ICSI meetings are both corpora of human-directed speech. The Fisher corpus (Cieri et al., 2004) comprises two-person telephone conversations between strangers on prescribed topics. The ICSI meeting corpus (Janin et al., 2003) contains multiparty face-to-face technical discussions among colleagues. 3 Method 3.1 Language modeling for addressee detection We use a lexical AD system that is based on modeling word n-grams in the two addressee-based utterance classes, H (for H-H) and C (for H-C utterances). This approach is similar to language modelbased approaches to speaker and language recognition, and was shown to be quite effective for this task (Shriberg et al., 2012). Instead of making </context>
</contexts>
<marker>Cieri, Miller, Walker, 2004</marker>
<rawString>Christopher Cieri, David Miller, and Kevin Walker. 2004. The Fisher corpus: a resource for the next generations of speech-to-text. In Proceedings 4th International Conference on Language Resources and Evaluation, pages 69–71, Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Dowding</author>
<author>Richard Alena</author>
<author>William J Clancey</author>
<author>Maarten Sierhuis</author>
<author>Jeffrey Graham</author>
</authors>
<title>Are you talking to me? dialogue systems supporting mixed teams of humans and robots.</title>
<date>2006</date>
<booktitle>In Proccedings AAAI Fall Symposium: Aurally Informed Performance: Integrating Machine Listening and Auditory Presentation in Robotic Systems,</booktitle>
<location>Washington, DC.</location>
<contexts>
<context position="2822" citStr="Dowding et al., 2006" startWordPosition="416" endWordPosition="420">and multiperson environments, however, AD can become a crucial part of the system’s operational requirements. This is particularly true for systems in which explicit system addressing (e.g., push-to-talk or required keyword addressing) is undesirable. Past research on addressee detection has focused on human-human (H-H) settings, such as meetings, sometimes with multimodal cues (op den Akker and Traum, 2009). Early systems relied primarily on rejection of H-H utterances either because they could not be interpreted (Paek et al., 2000), or because they yielded low speech recognition confidence (Dowding et al., 2006). Some systems combine gaze with lexical and syntactic cues to detect H-H speech (Katzenmaier et al., 2004). Others use relatively simple prosodic features based on pitch and energy in addition to those derived from automatic speech recognition (ASR) (Reich et al., 2011). With some exceptions (Bohus and Horvitz, 2011; Shriberg et al., 2012), relatively little work has looked at the human-human-computer (H-H-C) scenario, i.e. at contexts involving two or more people who interact both with a system and with each other. 221 Proceedings of NAACL-HLT 2013, pages 221–229, Atlanta, Georgia, 9–14 June</context>
</contexts>
<marker>Dowding, Alena, Clancey, Sierhuis, Graham, 2006</marker>
<rawString>John Dowding, Richard Alena, William J. Clancey, Maarten Sierhuis, and Jeffrey Graham. 2006. Are you talking to me? dialogue systems supporting mixed teams of humans and robots. In Proccedings AAAI Fall Symposium: Aurally Informed Performance: Integrating Machine Listening and Auditory Presentation in Robotic Systems, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dilek Hakkani-T¨ur</author>
<author>Gokhan Tur</author>
<author>Larry Heck</author>
</authors>
<title>Research challenges and opportunities in mobile applications [dsp education].</title>
<date>2011</date>
<journal>IEEE Signal Processing Magazine,</journal>
<volume>28</volume>
<issue>4</issue>
<pages>110</pages>
<marker>Hakkani-T¨ur, Tur, Heck, 2011</marker>
<rawString>Dilek Hakkani-T¨ur, Gokhan Tur, and Larry Heck. 2011a. Research challenges and opportunities in mobile applications [dsp education]. IEEE Signal Processing Magazine, 28(4):108 –110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dilek Z Hakkani-T¨ur</author>
<author>G¨okhan T¨ur</author>
<author>Larry P Heck</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Bootstrapping domain detection using query click logs for new domains.</title>
<date>2011</date>
<booktitle>In Proceedings Interspeech,</booktitle>
<pages>709--712</pages>
<marker>Hakkani-T¨ur, T¨ur, Heck, Shriberg, 2011</marker>
<rawString>Dilek Z. Hakkani-T¨ur, G¨okhan T¨ur, Larry P. Heck, and Elizabeth Shriberg. 2011b. Bootstrapping domain detection using query click logs for new domains. In Proceedings Interspeech, pages 709–712.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian Huang</author>
<author>Jianfeng Gao</author>
<author>Jiangbo Miao</author>
<author>Xiaolong Li</author>
<author>Kuansang Wang</author>
<author>Fritz Behr</author>
</authors>
<title>Exploring web scale language models for search query processing.</title>
<date>2010</date>
<booktitle>In Proceedings 19th International Conference on World Wide Web,</booktitle>
<pages>451--460</pages>
<location>Raleigh, NC.</location>
<contexts>
<context position="8988" citStr="Huang et al., 2010" startWordPosition="1412" endWordPosition="1415">ffort, we consider this a realistic application scenario, where in-domain training data is typically scarce, at least early in the development process. Therefore, any lexically based AD methods need to be robust to poor ASR accuracy. 2.2 Out-of-domain data To replace the hard-to-obtain in-domain H-H-C data for training, we use the four out-of-domain corpora (two H-C and two H-H) shown in Table 2. Single-user CB data comes from the same Conversational Browser system as the in-domain data, but with only one user present. This data can therefore be used for modeling H-C speech. Bing anchor text (Huang et al., 2010) is a large n-gram corpus of anchor text associated with links on web pages enCorpus Addressee Size Single-user CB H-C 21.9k words Bing anchor text H-C 1.3B bigrams Fisher H-H 21M words ICSI meetings H-H 0.7M words Figure 2: Language model-based score computation for addressee detection countered by the Bing search engine. When users want to follow a link displayed on screen, they usually speak a variant of the anchor text for the link. We hypothesized that this corpus might aid the modeling of computer-noncommand type utterances in which such “verbal clicks” are frequent. Fisher telephone con</context>
</contexts>
<marker>Huang, Gao, Miao, Li, Wang, Behr, 2010</marker>
<rawString>Jian Huang, Jianfeng Gao, Jiangbo Miao, Xiaolong Li, Kuansang Wang, and Fritz Behr. 2010. Exploring web scale language models for search query processing. In Proceedings 19th International Conference on World Wide Web, pages 451–460, Raleigh, NC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Janin</author>
<author>Don Baron</author>
<author>Jane Edwards</author>
<author>Dan Ellis</author>
<author>David Gelbart</author>
<author>Nelson Morgan</author>
<author>Barbara Peskin</author>
<author>Thilo Pfau</author>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
<author>Chuck Wooters</author>
</authors>
<title>The ICSI meeting corpus.</title>
<date>2003</date>
<booktitle>In Proceedings IEEE ICASSP,</booktitle>
<volume>1</volume>
<pages>364--367</pages>
<location>Hong Kong.</location>
<contexts>
<context position="9828" citStr="Janin et al., 2003" startWordPosition="1548" endWordPosition="1551"> 2: Language model-based score computation for addressee detection countered by the Bing search engine. When users want to follow a link displayed on screen, they usually speak a variant of the anchor text for the link. We hypothesized that this corpus might aid the modeling of computer-noncommand type utterances in which such “verbal clicks” are frequent. Fisher telephone conversations and ICSI meetings are both corpora of human-directed speech. The Fisher corpus (Cieri et al., 2004) comprises two-person telephone conversations between strangers on prescribed topics. The ICSI meeting corpus (Janin et al., 2003) contains multiparty face-to-face technical discussions among colleagues. 3 Method 3.1 Language modeling for addressee detection We use a lexical AD system that is based on modeling word n-grams in the two addressee-based utterance classes, H (for H-H) and C (for H-C utterances). This approach is similar to language modelbased approaches to speaker and language recognition, and was shown to be quite effective for this task (Shriberg et al., 2012). Instead of making hard decisions, the system outputs a score that is in-domain (HH) Fisher, ICSI meeting – out-of-domain Single user CB, Bing – out-</context>
</contexts>
<marker>Janin, Baron, Edwards, Ellis, Gelbart, Morgan, Peskin, Pfau, Shriberg, Stolcke, Wooters, 2003</marker>
<rawString>Adam Janin, Don Baron, Jane Edwards, Dan Ellis, David Gelbart, Nelson Morgan, Barbara Peskin, Thilo Pfau, Elizabeth Shriberg, Andreas Stolcke, and Chuck Wooters. 2003. The ICSI meeting corpus. In Proceedings IEEE ICASSP, volume 1, pages 364–367, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Katzenmaier</author>
<author>Rainer Stiefelhagen</author>
<author>Tanja Schultz</author>
</authors>
<title>Identifying the addressee in humanhuman-robot interactions based on head pose and speech.</title>
<date>2004</date>
<booktitle>In Proceedings 6th International Conference on Multimodal Interfaces, ICMI,</booktitle>
<pages>144--151</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2929" citStr="Katzenmaier et al., 2004" startWordPosition="434" endWordPosition="437">ments. This is particularly true for systems in which explicit system addressing (e.g., push-to-talk or required keyword addressing) is undesirable. Past research on addressee detection has focused on human-human (H-H) settings, such as meetings, sometimes with multimodal cues (op den Akker and Traum, 2009). Early systems relied primarily on rejection of H-H utterances either because they could not be interpreted (Paek et al., 2000), or because they yielded low speech recognition confidence (Dowding et al., 2006). Some systems combine gaze with lexical and syntactic cues to detect H-H speech (Katzenmaier et al., 2004). Others use relatively simple prosodic features based on pitch and energy in addition to those derived from automatic speech recognition (ASR) (Reich et al., 2011). With some exceptions (Bohus and Horvitz, 2011; Shriberg et al., 2012), relatively little work has looked at the human-human-computer (H-H-C) scenario, i.e. at contexts involving two or more people who interact both with a system and with each other. 221 Proceedings of NAACL-HLT 2013, pages 221–229, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics Shriberg et al. (2012) found that novel prosodic fe</context>
</contexts>
<marker>Katzenmaier, Stiefelhagen, Schultz, 2004</marker>
<rawString>Michael Katzenmaier, Rainer Stiefelhagen, and Tanja Schultz. 2004. Identifying the addressee in humanhuman-robot interactions based on head pose and speech. In Proceedings 6th International Conference on Multimodal Interfaces, ICMI, pages 144–151, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Ann Taylor</author>
</authors>
<date>1999</date>
<booktitle>Treebank-3. Linguistic Data Consortium, catalog item LDC99T42.</booktitle>
<contexts>
<context position="13532" citStr="Marcus et al., 1999" startWordPosition="2147" endWordPosition="2150">nference algorithm would suffer from the same sparseness issues when it comes to unseen and rare words (as no robust statistics are available to infer an unseen word’s best class in the class induction step). A POS tagger, on the other hand, can do quite well on unseen words, using context and morphological cues. A hidden Markov model tagger using POStrigram statistics and context-independent class membership probabilities was used for tagging all LM training data. The tagger itself had been trained on the Switchboard (conversational telephone speech) transcripts of the Penn Treebank3 corpus (Marcus et al., 1999), and used the 39 Treebank POS labels. To strike a compromise between generalization and discriminative power in the language model, we retained the top N most frequent word types from the in-domain training data as distinct tokens, and varied N as a metaparameter. Barzilay and Lee (2003) used a similar idea to generalize patterns by substituting words with slots. This strategy will tend to preserve words that are either generally frequent function and domainindependent words, capturing stylistic and syntactic patterns, or which are frequent domain-specific words, and can thus help characteriz</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, Taylor, 1999</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, and Ann Taylor. 1999. Treebank-3. Linguistic Data Consortium, catalog item LDC99T42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rieks op den Akker</author>
<author>David Traum</author>
</authors>
<title>A comparison of addressee detection methods for multiparty conversations.</title>
<date>2009</date>
<booktitle>In Proceedings of Diaholmia,</booktitle>
<pages>99--106</pages>
<marker>den Akker, Traum, 2009</marker>
<rawString>Rieks op den Akker and David Traum. 2009. A comparison of addressee detection methods for multiparty conversations. In Proceedings of Diaholmia, pages 99–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Paek</author>
</authors>
<title>Eric Horvitz, and Eric Ringger.</title>
<date>2000</date>
<booktitle>In Proceedings ICSLP,</booktitle>
<volume>1</volume>
<pages>138--141</pages>
<location>Beijing.</location>
<marker>Paek, 2000</marker>
<rawString>Tim Paek, Eric Horvitz, and Eric Ringger. 2000. Continuous listening for unconstrained spoken dialog. In Proceedings ICSLP, volume 1, pages 138–141, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Reich</author>
<author>Felix Putze</author>
<author>Dominic Heger</author>
<author>Joris Ijsselmuiden</author>
<author>Rainer Stiefelhagen</author>
<author>Tanja Schultz</author>
</authors>
<title>A real-time speech command detector for a smart control room.</title>
<date>2011</date>
<booktitle>In Proceedings Interspeech,</booktitle>
<pages>2641--2644</pages>
<location>Florence.</location>
<contexts>
<context position="3093" citStr="Reich et al., 2011" startWordPosition="459" endWordPosition="462">essee detection has focused on human-human (H-H) settings, such as meetings, sometimes with multimodal cues (op den Akker and Traum, 2009). Early systems relied primarily on rejection of H-H utterances either because they could not be interpreted (Paek et al., 2000), or because they yielded low speech recognition confidence (Dowding et al., 2006). Some systems combine gaze with lexical and syntactic cues to detect H-H speech (Katzenmaier et al., 2004). Others use relatively simple prosodic features based on pitch and energy in addition to those derived from automatic speech recognition (ASR) (Reich et al., 2011). With some exceptions (Bohus and Horvitz, 2011; Shriberg et al., 2012), relatively little work has looked at the human-human-computer (H-H-C) scenario, i.e. at contexts involving two or more people who interact both with a system and with each other. 221 Proceedings of NAACL-HLT 2013, pages 221–229, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics Shriberg et al. (2012) found that novel prosodic features were more accurate than lexical or semantic features based on speech recognition for the addressee task. The corpus, also used herein, is comprised of H-H-C </context>
</contexts>
<marker>Reich, Putze, Heger, Ijsselmuiden, Stiefelhagen, Schultz, 2011</marker>
<rawString>Daniel Reich, Felix Putze, Dominic Heger, Joris Ijsselmuiden, Rainer Stiefelhagen, and Tanja Schultz. 2011. A real-time speech command detector for a smart control room. In Proceedings Interspeech, pages 2641– 2644, Florence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
<author>Dilek Hakkani-T¨ur</author>
<author>Larry Heck</author>
</authors>
<title>Learning when to listen: Detecting system-addressed speech in human-humancomputer dialog.</title>
<date>2012</date>
<booktitle>In Proceedings Interspeech,</booktitle>
<location>Portland, OR.</location>
<marker>Shriberg, Stolcke, Hakkani-T¨ur, Heck, 2012</marker>
<rawString>Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-T¨ur, and Larry Heck. 2012. Learning when to listen: Detecting system-addressed speech in human-humancomputer dialog. In Proceedings Interspeech, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Timothy C Bell</author>
</authors>
<title>The zerofrequency problem: Estimating the probabilities of novel events in adaptive text compression.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>37</volume>
<issue>4</issue>
<pages>1094</pages>
<contexts>
<context position="11028" citStr="Witten and Bell, 1991" startWordPosition="1740" endWordPosition="1743">e user CB, Bing – out-of-domain Language model for computer directed utterances (C) P(WlH) �og W 1 P(WIC) P(WlH) P(WIC) Language model for human directed utterances (H) in-domain (HC) 223 the length-normalized likelihood ratio of the two classes: 1 P(w|C) |w |log P(w|H), (1) where |w |is the number of words in the recognition output w for an utterance. P(w|C) and P(w|H) are obtained from class-specific language models. Figure 2 gives a flow-chart of the score computation. Class likelihoods are obtained from standard trigram backoff language models, using Witten-Bell discounting for smoothing (Witten and Bell, 1991). For combining various training data sources, we use language model adaptation by interpolation (Bellegarda, 2004). First, a separate model is trained from each source. The probability estimates from in-domain and out-of-domain models are then averaged in a weighted fashion: P(wk|hk) = APin(wk|hk) + (1 − A)P..t(wk|hk) (2) where wk is the k-th word, hk is the (n − 1)-gram history for the word wk. A is the interpolation weight and is obtained by tuning a task-related metric on the development set. We investigated optimizing A for either model perplexity or classification accuracy, as discussed </context>
</contexts>
<marker>Witten, Bell, 1991</marker>
<rawString>Ian H. Witten and Timothy C. Bell. 1991. The zerofrequency problem: Estimating the probabilities of novel events in adaptive text compression. IEEE Transactions on Information Theory, 37(4):1085– 1094.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>