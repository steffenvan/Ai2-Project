<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000019">
<title confidence="0.998396">
A Pilot Study of Opinion Summarization in Conversations
</title>
<author confidence="0.998272">
Dong Wang Yang Liu
</author>
<affiliation confidence="0.991811">
The University of Texas at Dallas
</affiliation>
<email confidence="0.998283">
dongwang,yangl@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.994953" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.812464045454545">
This paper presents a pilot study of opinion
summarization on conversations. We create
a corpus containing extractive and abstrac-
tive summaries of speaker’s opinion towards
a given topic using 88 telephone conversa-
tions. We adopt two methods to perform ex-
tractive summarization. The first one is a
sentence-ranking method that linearly com-
bines scores measured from different aspects
including topic relevance, subjectivity, and
sentence importance. The second one is a
graph-based method, which incorporates topic
and sentiment information, as well as addi-
tional information about sentence-to-sentence
relations extracted based on dialogue struc-
ture. Our evaluation results show that both
methods significantly outperform the baseline
approach that extracts the longest utterances.
In particular, we find that incorporating di-
alogue structure in the graph-based method
contributes to the improved system perfor-
mance.
</bodyText>
<sectionHeader confidence="0.999006" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999761555555556">
Both sentiment analysis (opinion recognition) and
summarization have been well studied in recent
years in the natural language processing (NLP) com-
munity. Most of the previous work on sentiment
analysis has been conducted on reviews. Summa-
rization has been applied to different genres, such
as news articles, scientific articles, and speech do-
mains including broadcast news, meetings, conver-
sations and lectures. However, opinion summariza-
tion has not been explored much. This can be use-
ful for many domains, especially for processing the
increasing amount of conversation recordings (tele-
phone conversations, customer service, round-table
discussions or interviews in broadcast programs)
where we often need to find a person’s opinion or
attitude, for example, “how does the speaker think
about capital punishment and why?”. This kind of
questions can be treated as a topic-oriented opin-
ion summarization task. Opinion summarization
was run as a pilot task in Text Analysis Conference
(TAC) in 2008. The task was to produce summaries
of opinions on specified targets from a set of blog
documents. In this study, we investigate this prob-
lem using spontaneous conversations. The problem
is defined as, given a conversation and a topic, a
summarization system needs to generate a summary
of the speaker’s opinion towards the topic.
This task is built upon opinion recognition and
topic or query based summarization. However, this
problem is challenging in that: (a) Summarization in
spontaneous speech is more difficult than well struc-
tured text (Mckeown et al., 2005), because speech
is always less organized and has recognition errors
when using speech recognition output; (b) Senti-
ment analysis in dialogues is also much harder be-
cause of the genre difference compared to other do-
mains like product reviews or news resources, as re-
ported in (Raaijmakers et al., 2008); (c) In conversa-
tional speech, information density is low and there
are often off topic discussions, therefore presenting
a need to identify utterances that are relevant to the
topic.
In this paper we perform an exploratory study
on opinion summarization in conversations. We
compare two unsupervised methods that have been
</bodyText>
<page confidence="0.980972">
331
</page>
<note confidence="0.979664">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 331–339,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999244277777778">
widely used in extractive summarization: sentence-
ranking and graph-based methods. Our system at-
tempts to incorporate more information about topic
relevancy and sentiment scores. Furthermore, in
the graph-based method, we propose to better in-
corporate the dialogue structure information in the
graph in order to select salient summary utterances.
We have created a corpus of reasonable size in this
study. Our experimental results show that both
methods achieve better results compared to the base-
line.
The rest of this paper is organized as follows. Sec-
tion 2 briefly discusses related work. Section 3 de-
scribes the corpus and annotation scheme we used.
We explain our opinion-oriented conversation sum-
marization system in Section 4 and present experi-
mental results and analysis in Section 5. Section 6
concludes the paper.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999616">
Research in document summarization has been well
established over the past decades. Many tasks have
been defined such as single-document summariza-
tion, multi-document summarization, and query-
based summarization. Previous studies have used
various domains, including news articles, scientific
articles, web documents, reviews. Recently there
is an increasing research interest in speech sum-
marization, such as conversational telephone speech
(Zhu and Penn, 2006; Zechner, 2002), broadcast
news (Maskey and Hirschberg, 2005; Lin et al.,
2009), lectures (Zhang et al., 2007; Furui et al.,
2004), meetings (Murray et al., 2005; Xie and Liu,
2010), voice mails (Koumpis and Renals, 2005).
In general speech domains seem to be more diffi-
cult than well written text for summarization. In
previous work, unsupervised methods like Maximal
Marginal Relevance (MMR), Latent Semantic Anal-
ysis (LSA), and supervised methods that cast the ex-
traction problem as a binary classification task have
been adopted. Prior research has also explored using
speech specific information, including prosodic fea-
tures, dialog structure, and speech recognition con-
fidence.
In order to provide a summary over opinions, we
need to find out which utterances in the conversa-
tion contain opinion. Most previous work in senti-
ment analysis has focused on reviews (Pang and Lee,
2004; Popescu and Etzioni, 2005; Ng et al., 2006)
and news resources (Wiebe and Riloff, 2005). Many
kinds of features are explored, such as lexical fea-
tures (unigram, bigram and trigram), part-of-speech
tags, dependency relations. Most of prior work used
classification methods such as naive Bayes or SVMs
to perform the polarity classification or opinion de-
tection. Only a handful studies have used conver-
sational speech for opinion recognition (Murray and
Carenini, 2009; Raaijmakers et al., 2008), in which
some domain-specific features are utilized such as
structural features and prosodic features.
Our work is also related to question answering
(QA), especially opinion question answering. (Stoy-
anov et al., 2005) applies a subjectivity filter based
on traditional QA systems to generate opinionated
answers. (Balahur et al., 2010) answers some spe-
cific opinion questions like “Why do people criti-
cize Richard Branson?” by retrieving candidate sen-
tences using traditional QA methods and selecting
the ones with the same polarity as the question. Our
work is different in that we are not going to an-
swer specific opinion questions, instead, we provide
a summary on the speaker’s opinion towards a given
topic.
There exists some work on opinion summariza-
tion. For example, (Hu and Liu, 2004; Nishikawa et
al., 2010) have explored opinion summarization in
review domain, and (Paul et al., 2010) summarizes
contrastive viewpoints in opinionated text. How-
ever, opinion summarization in spontaneous conver-
sation is seldom studied.
</bodyText>
<sectionHeader confidence="0.983805" genericHeader="method">
3 Corpus Creation
</sectionHeader>
<bodyText confidence="0.997305090909091">
Though there are many annotated data sets for the
research of speech summarization and sentiment
analysis, there is no corpus available for opinion
summarization on spontaneous speech. Thus for this
study, we create a new pilot data set using a sub-
set of the Switchboard corpus (Godfrey and Holli-
man, 1997).1 These are conversational telephone
speech between two strangers that were assigned a
topic to talk about for around 5 minutes. They were
told to find the opinions of the other person. There
are 70 topics in total. From the Switchboard cor-
</bodyText>
<footnote confidence="0.980905">
1Please contact the authors to obtain the data.
</footnote>
<page confidence="0.99811">
332
</page>
<bodyText confidence="0.9993124">
pus, we selected 88 conversations from 6 topics for
this study. Table 1 lists the number of conversations
in each topic, their average length (measured in the
unit of dialogue acts (DA)) and standard deviation
of length.
</bodyText>
<table confidence="0.956115714285714">
topic #Conv. avg len stdev
space flight and exploration 6
capital punishment 24
gun control 15 165.5 71.40
universal health insurance 9
drug testing 12
universal public service 22
</table>
<tableCaption confidence="0.982315666666667">
Table 1: Corpus statistics: topic description, number of
conversations in each topic, average length (number of
dialog acts), and standard deviation.
</tableCaption>
<bodyText confidence="0.99991856">
We recruited 3 annotators that are all undergrad-
uate computer science students. From the 88 con-
versations, we selected 18 (3 from each topic) and
let all three annotators label them in order to study
inter-annotator agreement. The rest of the conversa-
tions has only one annotation.
The annotators have access to both conversation
transcripts and audio files. For each conversation,
the annotator writes an abstractive summary of up
to 100 words for each speaker about his/her opin-
ion or attitude on the given topic. They were told to
use the words in the original transcripts if possible.
Then the annotator selects up to 15 DAs (no mini-
mum limit) in the transcripts for each speaker, from
which their abstractive summary is derived. The se-
lected DAs are used as the human generated extrac-
tive summary. In addition, the annotator is asked
to select an overall opinion towards the topic for
each speaker among five categories: strongly sup-
port, somewhat support, neutral, somewhat against,
strongly against. Therefore for each conversation,
we have an abstractive summary, an extractive sum-
mary, and an overall opinion for each speaker. The
following shows an example of such annotation for
speaker B in a dialogue about “capital punishment”:
</bodyText>
<subsectionHeader confidence="0.326449">
[Extractive Summary]
</subsectionHeader>
<construct confidence="0.877779958333333">
I think I’ve seen some statistics that say that, uh, it’s
more expensive to kill somebody than to keep them in
prison for life.
committing them mostly is, you know, either crimes of
passion or at the moment
or they think they’re not going to get caught
but you also have to think whether it’s worthwhile on
the individual basis, for example, someone like, uh, jeffrey
dahlmer,
by putting him in prison for life, there is still a possi-
bility that he will get out again.
I don’t think he could ever redeem himself,
but if you look at who gets accused and who are the
ones who actually get executed, it’s very racially related
– and ethnically related
[Abstractive Summary]
B is against capital punishment except under certain
circumstances. B finds that crimes deserving of capital
punishment are “crimes of the moment” and as a result
feels that capital punishment is not an effective deterrent.
however, B also recognizes that on an individual basis
some criminals can never “redeem” themselves.
[Overall Opinion]
Somewhat against
</construct>
<bodyText confidence="0.943072875">
Table 2 shows the compression ratio of the extrac-
tive summaries and abstractive summaries as well as
their standard deviation. Because in conversations,
utterance length varies a lot, we use words as units
when calculating the compression ratio.
avg ratio stdev
extractive summaries 0.26 0.13
abstractive summaries 0.13 0.06
</bodyText>
<tableCaption confidence="0.992946">
Table 2: Compression ratio and standard deviation of ex-
tractive and abstractive summaries.
</tableCaption>
<bodyText confidence="0.99264635">
We measured the inter-annotator agreement
among the three annotators for the 18 conversations
(each has two speakers, thus 36 “documents” in to-
tal). Results are shown in Table 3. For the ex-
tractive or abstractive summaries, we use ROUGE
scores (Lin, 2004), a metric used to evaluate auto-
matic summarization performance, to measure the
pairwise agreement of summaries from different an-
notators. ROUGE F-scores are shown in the table
for different matches, unigram (R-1), bigram (R-2),
and longest subsequence (R-L). For the overall opin-
ion category, since it is a multiclass label (not binary
decision), we use Krippendorff’s α coefficient to
measure human agreement, and the difference func-
tion for interval data: 62 _ (c − k)2 (where c, k are
ck
the interval values, on a scale of 1 to 5 corresponding
to the five categories for the overall opinion).
We notice that the inter-annotator agreement for
extractive summaries is comparable to other speech
</bodyText>
<page confidence="0.997498">
333
</page>
<table confidence="0.998240142857143">
R-1 0.61
extractive summaries R-2 0.52
R-L 0.61
R-1 0.32
abstractive summaries R-2 0.13
R-L 0.25
overall opinion α = 0.79
</table>
<tableCaption confidence="0.9712805">
Table 3: Inter-annotator agreement for extractive and ab-
stractive summaries, and overall opinion.
</tableCaption>
<bodyText confidence="0.999613684210526">
summary annotation (Liu and Liu, 2008). The
agreement on abstractive summaries is much lower
than extractive summaries, which is as expected.
Even for the same opinion or sentence, annotators
use different words in the abstractive summaries.
The agreement for the overall opinion annotation
is similar to other opinion/emotion studies (Wil-
son, 2008b), but slightly lower than the level rec-
ommended by Krippendorff for reliable data (α =
0.8) (Hayes and Krippendorff, 2007), which shows
it is even difficult for humans to determine what
opinion a person holds (support or against some-
thing). Often human annotators have different inter-
pretations about the same sentence, and a speaker’s
opinion/attitude is sometimes ambiguous. Therefore
this also demonstrates that it is more appropriate to
provide a summary rather than a simple opinion cat-
egory to answer questions about a person’s opinion
towards something.
</bodyText>
<sectionHeader confidence="0.994356" genericHeader="method">
4 Opinion Summarization Methods
</sectionHeader>
<bodyText confidence="0.99995">
Automatic summarization can be divided into ex-
tractive summarization and abstractive summariza-
tion. Extractive summarization selects sentences
from the original documents to form a summary;
whereas abstractive summarization requires genera-
tion of new sentences that represent the most salient
content in the original documents like humans do.
Often extractive summarization is used as the first
step to generate abstractive summary.
As a pilot study for the problem of opinion sum-
marization in conversations, we treat this problem
as an extractive summarization task. This section
describes two approaches we have explored in gen-
erating extractive summaries. The first one is a
sentence-ranking method, in which we measure the
salience of each sentence according to a linear com-
bination of scores from several dimensions. The sec-
ond one is a graph-based method, which incorpo-
rates the dialogue structure in ranking. We choose to
investigate these two methods since they have been
widely used in text and speech summarization, and
perform competitively. In addition, they do not re-
quire a large labeled data set for modeling training,
as needed in some classification or feature based
summarization approaches.
</bodyText>
<subsectionHeader confidence="0.998193">
4.1 Sentence Ranking
</subsectionHeader>
<bodyText confidence="0.98879">
In this method, we use Equation 1 to assign a score
to each DA s, and select the most highly ranked ones
until the length constriction is satisfied.
</bodyText>
<equation confidence="0.990298">
score(s) = Asimsim(s, D) + ArelREL(s, topic)
+Asentsentiment(s) + Alenlength(s)
� Ai = 1 (1)
i
</equation>
<listItem confidence="0.9974579">
• sim(s, D) is the cosine similarity between DA
s and all the utterances in the dialogue from
the same speaker, D. It measures the rele-
vancy of s to the entire dialogue from the tar-
get speaker. This score is used to represent the
salience of the DA. It has been shown to be an
important indicator in summarization for var-
ious domains. For cosine similarity measure,
we use TF*IDF (term frequency, inverse docu-
ment frequency) term weighting. The IDF val-
ues are obtained using the entire Switchboard
corpus, treating each conversation as a docu-
ment.
• REL(s, topic) measures the topic relevance of
DA s. It is the sum of the topic relevance of all
the words in the DA. We only consider the con-
tent words for this measure. They are identified
using TreeTagger toolkit.2 To measure the rel-
evance of a word to a topic, we use Pairwise
Mutual Information (PMI):
</listItem>
<equation confidence="0.862864">
PMI(w, topic) = log2 p(w&amp;topic)p(w)p(topic) (2)
</equation>
<footnote confidence="0.965398">
2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/De
cisionTreeTagger.html
</footnote>
<page confidence="0.99689">
334
</page>
<bodyText confidence="0.998486714285714">
where all the statistics are collected from the
Switchboard corpus: p(w&amp;topic) denotes the
probability that word w appears in a dialogue
of topic t, and p(w) is the probability of w ap-
pearing in a dialogue of any topic. Since our
goal is to rank DAs in the same dialog, and
the topic is the same for all the DAs, we drop
p(topic) when calculating PMI scores. Be-
cause the value of PMI(w, topic) is negative,
we transform it into a positive one (denoted
by PMI+(w, topic)) by adding the absolute
value of the minimum value. The final rele-
vance score of each sentence is normalized to
[0, 1] using linear normalization:
</bodyText>
<equation confidence="0.9943522">
RELorig(s, topic) = X PMI+(w, topic)
wEs
RELorig(s,topic) − Min
REL(s,topic) =
Max − Min
</equation>
<bodyText confidence="0.999721307692308">
sentiment(s) indicates the probability that ut-
terance s contains opinion. To obtain this,
we trained a maximum entropy classifier with
a bag-of-words model using a combination
of data sets from several domains, including
movie data (Pang and Lee, 2004), news articles
from MPQA corpus (Wilson and Wiebe, 2003),
and meeting transcripts from AMI corpus (Wil-
son, 2008a). Each sentence (or DA) in these
corpora is annotated as “subjective” or “objec-
tive”. We use each utterance’s probability of
being “subjective” predicted by the classifier as
its sentiment score.
</bodyText>
<listItem confidence="0.511752142857143">
• length(s) is the length of the utterance. This
score can effectively penalize the short sen-
tences which typically do not contain much
important content, especially the backchannels
that appear frequently in dialogues. We also
perform linear normalization such that the final
value lies in [0, 1].
</listItem>
<subsectionHeader confidence="0.87469">
4.2 Graph-based Summarization
</subsectionHeader>
<bodyText confidence="0.999884421052632">
Graph-based methods have been widely used in doc-
ument summarization. In this approach, a document
is modeled as an adjacency matrix, where each node
represents a sentence, and the weight of the edge be-
tween each pair of sentences is their similarity (co-
sine similarity is typically used). An iterative pro-
cess is used until the scores for the nodes converge.
Previous studies (Erkan and Radev, 2004) showed
that this method can effectively extract important
sentences from documents. The basic framework we
use in this study is similar to the query-based graph
summarization system in (Zhao et al., 2009). We
also consider sentiment and topic relevance infor-
mation, and propose to incorporate information ob-
tained from dialog structure in this framework. The
score for a DA s is based on its content similarity
with all other DAs in the dialogue, the connection
with other DAs based on the dialogue structure, the
topic relevance, and its subjectivity, that is:
</bodyText>
<equation confidence="0.996918">
P zEC sim(z, v)
sim(s, v) score(v)
REL(s,topic)
+Arel PzEC REL(z, topic)
sentiment(s)
+�sent PzEC sentiment(z)
+�adj
vEC
X PzE l , )
ADJ(s, v) G&apos; ADJ ( )
(z v score (v)
X Ai = 1 (3)
i
</equation>
<bodyText confidence="0.998707888888889">
where C is the set of all DAs in the dialogue;
REL(s, topic) and sentiment(s) are the same
as those in the above sentence ranking method;
sim(s, v) is the cosine similarity between two DAs
s and v. In addition to the standard connection be-
tween two DAs with an edge weight sim(s, v), we
introduce new connections ADJ(s, v) to model di-
alog structure. It is a directed edge from s to v, de-
fined as follows:
</bodyText>
<listItem confidence="0.9336912">
•
• If s and v are from the same speaker and within
the same turn, there is an edge from s to v and
an edge from v to s with weight 1/dis(s, v)
(ADJ(s, v) = ADJ(v, s) = 1/dis(s, v)),
</listItem>
<bodyText confidence="0.9881785">
where dis(s, v) is the distance between s and
v, measured based on their DA indices. This
way the DAs in the same turn can reinforce
each other. For example, if we consider that
</bodyText>
<equation confidence="0.98809">
Xscore(s) = �sim
vEC
</equation>
<page confidence="0.988872">
335
</page>
<bodyText confidence="0.993555">
one DA is important, then the other DAs in the
same turn are also important.
</bodyText>
<listItem confidence="0.6332135">
• If s and v are from the same speaker, and
separated only by one DA from another
speaker with length less than 3 words (usu-
ally backchannel), there is an edge from s to
v as well as an edge from v to s with weight 1
(ADJ(s, v) = ADJ(v, s) = 1).
• If s and v form a question-answer pair from two
speakers, then there is an edge from question s
to answer v with weight 1 (ADJ(s, v) = 1).
We use a simple rule-based method to deter-
mine question-answer pairs — sentence s has
question marks or contains “wh-word” (i.e.,
“what, how, why”), and sentence v is the im-
mediately following one. The motivation for
adding this connection is, if the score of a ques-
tion sentence is high, then the answer’s score is
also boosted.
• If s and v form an agreement or disagreement
pair, then there is an edge from v to s with
weight 1 (ADJ(v, s) = 1). This is also de-
termined by simple rules: sentence v contains
the word “agree” or “disagree”, s is the previ-
ous sentence, and from a different speaker. The
reason for adding this is similar to the above
question-answer pairs.
• If there are multiple edges generated from the
above steps between two nodes, then we use the
highest weight.
</listItem>
<bodyText confidence="0.999939692307692">
Since we are using a directed graph for the sen-
tence connections to model dialog structure, the re-
sulting adjacency matrix is asymmetric. This is dif-
ferent from the widely used graph methods for sum-
marization. Also note that in the first sentence rank-
ing method or the basic graph methods, summariza-
tion is conducted for each speaker separately. Ut-
terances from one speaker have no influence on the
summary decision for the other speaker. Here in our
proposed graph-based method, we introduce con-
nections between the two speakers, so that the adja-
cency pairs between them can be utilized to extract
salient utterances.
</bodyText>
<sectionHeader confidence="0.999551" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99681">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999978727272727">
The 18 conversations annotated by all 3 annotators
are used as test set, and the rest of 70 conversa-
tions are used as development set to tune the param-
eters (determining the best combination weights). In
preprocessing we applied word stemming. We per-
form extractive summarization using different word
compression ratios (ranging from 10% to 25%). We
use human annotated dialogue acts (DA) as the ex-
traction units. The system-generated summaries are
compared to human annotated extractive and ab-
stractive summaries. We use ROUGE as the eval-
uation metrics for summarization performance.
We compare our methods to two systems. The
first one is a baseline system, where we select the
longest utterances for each speaker. This has been
shown to be a relatively strong baseline for speech
summarization (Gillick et al., 2009). The second
one is human performance. We treat each annota-
tor’s extractive summary as a system summary, and
compare to the other two annotators’ extractive and
abstractive summaries. This can be considered as
the upper bound of our system performance.
</bodyText>
<subsectionHeader confidence="0.935865">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999933">
From the development set, we used the grid search
method to obtain the best combination weights for
the two summarization methods. In the sentence-
ranking method, the best parameters found on the
development set are Asim = 0, Arel = 0.3, Asent =
0.3, Alen = 0.4. It is surprising to see that the sim-
ilarity score is not useful for this task. The possible
reason is, in Switchboard conversations, what peo-
ple talk about is diverse and in many cases only topic
words (except stopwords) appear more than once. In
addition, REL score is already able to catch the topic
relevancy of the sentence. Thus, the similarity score
is redundant here.
In the graph-based method, the best parameters
are Asim = 0, Aadj = 0.3, Arel = 0.4, Asent = 0.3.
The similarity between each pair of utterances is
also not useful, which can be explained with similar
reasons as in the sentence-ranking method. This is
different from graph-based summarization systems
for text domains. A similar finding has also been
shown in (Garg et al., 2009), where similarity be-
</bodyText>
<page confidence="0.996904">
336
</page>
<figure confidence="0.727108">
63
58
53
48
43
38
0.1
(a) compare to reference extractive summary
(b) compare to reference abstractive summary
</figure>
<figureCaption confidence="0.99449475">
Figure 1: ROUGE-1 F-scores compared to extractive
and abstractive reference summaries for different sys-
tems: max-length, sentence-ranking method, graph-
based method, and human performance.
</figureCaption>
<bodyText confidence="0.9995745">
tween utterances does not perform well in conversa-
tion summarization.
Figure 1 shows the ROUGE-1 F-scores compar-
ing to human extractive and abstractive summaries
for different compression ratios. Similar patterns are
observed for other ROUGE scores such as ROUGE-
2 or ROUGE-L, therefore they are not shown here.
Both methods improve significantly over the base-
line approach. There is relatively less improvement
using a higher compression ratio, compared to a
lower one. This is reasonable because when the
compression ratio is low, the most salient utterances
are not necessarily the longest ones, thus using more
information sources helps better identify important
sentences; but when the compression ratio is higher,
longer utterances are more likely to be selected since
they contain more content.
There is no significant difference between the two
methods. When compared to extractive reference
summaries, sentence-ranking is slightly better ex-
cept for the compression ratio of 0.1. When com-
pared to abstractive reference summaries, the graph-
based method is slightly better. The two systems
share the same topic relevance score (REL) and
sentiment score, but the sentence-ranking method
prefers longer DAs and the graph-based method
prefers DAs that are emphasized by the ADJ ma-
trix, such as the DA in the middle of a cluster of
utterances from the same speaker, the answer to a
question, etc.
</bodyText>
<subsectionHeader confidence="0.998907">
5.3 Analysis
</subsectionHeader>
<bodyText confidence="0.99996852">
To analyze the effect of dialogue structure we in-
troduce in the graph-based summarization method,
we compare two configurations: Aadj = 0 (only us-
ing REL score and sentiment score in ranking) and
Aadj = 0.3. We generate summaries using these two
setups and compare with human selected sentences.
Table 4 shows the number of false positive instances
(selected by system but not by human) and false neg-
ative ones (selected by human but not by system).
We use all three annotators’ annotation as reference,
and consider an utterance as positive if one annotator
selects it. This results in a large number of reference
summary DAs (because of low human agreement),
and thus the number of false negatives in the system
output is very high. As expected, a smaller compres-
sion ratio (fewer selected DAs in the system output)
yields a higher false negative rate and a lower false
positive rate. From the results, we can see that gen-
erally adding adjacency matrix information is able
to reduce both types of errors except when the com-
pression ratio is 0.15.
The following shows an example, where the third
DA is selected by the system with Aadj = 0.3, but
not by Aadj = 0. This is partly because the weight
of the second DA is enhanced by the the question-
</bodyText>
<figure confidence="0.939786928571428">
0.1 0.15 0.2 0.25
compression ratio
31
29
27
25
23
21
19
17
max-length
sentence-ranking
graph
human
</figure>
<page confidence="0.983528">
337
</page>
<table confidence="0.9990445">
λadj = 0 λadj = 0.3
ratio FP FN FP FN
0.1 37 588 33 581
0.15 60 542 61 546
0.2 100 516 90 511
0.25 137 489 131 482
</table>
<tableCaption confidence="0.959384">
Table 4: The number of false positive (FP) and false neg-
ative (FN) instances using the graph-based method with
λadj = 0 and λadj = 0.3 for different compression ratios.
</tableCaption>
<bodyText confidence="0.852561">
answer pair (the first and the second DA), and thus
subsequently boosting the score of the third DA.
</bodyText>
<figure confidence="0.901978">
A: Well what do you think?
B: Well, I don’t know, I’m thinking about from one to
ten what my no would be.
B: It would probably be somewhere closer to, uh, less
control because I don’t see, -
</figure>
<bodyText confidence="0.972379466666667">
We also examined the system output and human
annotation and found some reasons for the system
errors:
(a) Topic relevance measure. We use the statis-
tics from the Switchboard corpus to measure the rel-
evance of each word to a given topic (PMI score),
therefore only when people use the same word in
different conversations of the topic, the PMI score of
this word and the topic is high. However, since the
size of the corpus is small, some topics only con-
tain a few conversations, and some words only ap-
pear in one conversation even though they are topic-
relevant. Therefore the current PMI measure cannot
properly measure a word’s and a sentence’s topic
relevance. This problem leads to many false neg-
ative errors (relevant sentences are not captured by
our system).
(b) Extraction units. We used DA segments as
units for extractive summarization, which can be
problematic. In conversational speech, sometimes
a DA segment is not a complete sentence because
of overlaps and interruptions. We notice that anno-
tators tend to select consecutive DAs that constitute
a complete sentence, however, since each individual
DA is not quite meaningful by itself, they are often
not selected by the system. The following segment
is extracted from a dialogue about “universal health
insurance”. The two DAs from speaker B are not
selected by our system but selected by human anno-
tators, causing false negative errors.
</bodyText>
<figure confidence="0.747019666666667">
B: and it just can devastate –
A: and your constantly, -
B: – your budget, you know.
</figure>
<sectionHeader confidence="0.838783" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99997932">
This paper investigates two unsupervised methods
in opinion summarization on spontaneous conver-
sations by incorporating topic score and sentiment
score in existing summarization techniques. In the
sentence-ranking method, we linearly combine sev-
eral scores in different aspects to select sentences
with the highest scores. In the graph-based method,
we use an adjacency matrix to model the dialogue
structure and utilize it to find salient utterances in
conversations. Our experiments show that both
methods are able to improve the baseline approach,
and we find that the cosine similarity between utter-
ances or between an utterance and the whole docu-
ment is not as useful as in other document summa-
rization tasks.
In future work, we will address some issues iden-
tified from our error analysis. First, we will in-
vestigate ways to represent a sentence’s topic rel-
evance. Second, we will evaluate using other ex-
traction units, such as applying preprocessing to re-
move disfluencies and concatenate incomplete sen-
tence segments together. In addition, it would be
interesting to test our system on speech recognition
output and automatically generated DA boundaries
to see how robust it is.
</bodyText>
<sectionHeader confidence="0.998742" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998935">
The authors thank Julia Hirschberg and Ani
Nenkova for useful discussions. This research is
supported by NSF awards CNS-1059226 and IIS-
0939966.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99371">
Alexandra Balahur, Ester Boldrini, Andr´es Montoyo, and
Patricio Martinez-Barco. 2010. Going beyond tra-
ditional QA systems: challenges and keys in opinion
question answering. In Proceedings of COLING.
G¨unes Erkan and Dragomir R. Radev. 2004. LexRank:
graph-based lexical centrality as salience in text sum-
marization. Journal of Artificial Intelligence Re-
search.
</reference>
<page confidence="0.993387">
338
</page>
<reference confidence="0.998985761904762">
Sadaoki Furui, Tomonori Kikuchi, Yousuke Shinnaka,
and Chior i Hori. 2004. Speech-to-text and speech-to-
speech summarization of spontaneous speech. IEEE
Transactions on Audio, Speech &amp; Language Process-
ing, 12(4):401–408.
Nikhil Garg, Benoit Favre, Korbinian Reidhammer, and
Dilek Hakkani T¨ur. 2009. ClusterRank: a graph
based method for meeting summarization. In Proceed-
ings of Interspeech.
Dan Gillick, Korbinian Riedhammer, Benoit Favre, and
Dilek Hakkani-Tur. 2009. A global optimization
framework for meeting summarization. In Proceed-
ings of ICASSP.
John J. Godfrey and Edward Holliman. 1997.
Switchboard-1 Release 2. In Linguistic Data Consor-
tium, Philadelphia.
Andrew Hayes and Klaus Krippendorff. 2007. Answer-
ing the call for a standard reliability measure for cod-
ing data. Journal of Communication Methods and
Measures, 1:77–89.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of ACM
SIGKDD.
Konstantinos Koumpis and Steve Renals. 2005. Auto-
matic summarization of voicemail messages using lex-
ical and prosodic features. ACM - Transactions on
Speech and Language Processing.
Shih Hsiang Lin, Berlin Chen, and Hsin min Wang.
2009. A comparative study of probabilistic ranking
models for chinese spoken document summarization.
ACM Transactions on Asian Language Information
Processing, 8(1).
Chin-Yew Lin. 2004. ROUGE: a package for auto-
matic evaluation of summaries. In Proceedings ofACL
workshop on Text Summarization Branches Out.
Fei Liu and Yang Liu. 2008. What are meeting sum-
maries? An analysis of human extractive summaries
in meeting corpus. In Proceedings of SIGDial.
Sameer Maskey and Julia Hirschberg. 2005. Com-
paring lexical, acoustic/prosodic, structural and dis-
course features for speech summarization. In Pro-
ceedings of Interspeech.
Kathleen Mckeown, Julia Hirschberg, Michel Galley, and
Sameer Maskey. 2005. From text to speech summa-
rization. In Proceedings of ICASSP.
Gabriel Murray and Giuseppe Carenini. 2009. Detecting
subjectivity in multiparty speech. In Proceedings of
Interspeech.
Gabriel Murray, Steve Renals, and Jean Carletta. 2005.
Extractive summarization of meeting recordings. In
Proceedings of EUROSPEECH.
Vincent Ng, Sajib Dasgupta, and S.M.Niaz Arifin. 2006.
Examining the role of linguistic knowledge sources in
the automatic identification and classification of re-
views. In Proceedings of the COLING/ACL.
Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Mat-
suo, and Genichiro Kikui. 2010. Opinion summariza-
tion with integer linear programming formulation for
sentence extraction and ordering. In Proceedings of
COLING.
Bo Pang and Lilian Lee. 2004. A sentiment educa-
tion: sentiment analysis using subjectivity summariza-
tion based on minimum cuts. In Proceedings of ACL.
Michael Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opinion-
ated text. In Proceedings of EMNLP.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Pro-
ceedings of HLT-EMNLP.
Stephan Raaijmakers, Khiet Truong, and Theresa Wilson.
2008. Multimodal subjectivity analysis of multiparty
conversation. In Proceedings of EMNLP.
Veselin Stoyanov, Claire Cardie, and Janyce Wiebe.
2005. Multi-perspective question answering using the
OpQA corpus. In Proceedings of EMNLP/HLT.
Janyce Wiebe and Ellen Riloff. 2005. Creating sub-
jective and objective sentence classifiers from unan-
notated texts. In Proceedings of CICLing.
Theresa Wilson and Janyce Wiebe. 2003. Annotating
opinions in the world press. In Proceedings of SIG-
Dial.
Theresa Wilson. 2008a. Annotating subjective content in
meetings. In Proceedings of LREC.
Theresa Wilson. 2008b. Fine-grained subjectivity and
sentiment analysis: recognizing the intensity, polarity,
and attitudes of private states. Ph.D. thesis, University
of Pittsburgh.
Shasha Xie and Yang Liu. 2010. Improving super-
vised learning for meeting summarization using sam-
pling and regression. Computer Speech and Lan-
guage, 24:495–514.
Klaus Zechner. 2002. Automatic summarization of
open-domain multiparty dialogues in dive rse genres.
Computational Linguistics, 28:447–485.
Justin Jian Zhang, Ho Yin Chan, and Pascale Fung. 2007.
Improving lecture speech summarization using rhetor-
ical information. In Proceedings of Biannual IEEE
Workshop on ASRU.
Lin Zhao, Lide Wu, and Xuanjing Huang. 2009. Using
query expansion in graph-based approach for query-
focused multi-document summarization. Journal of
Information Processing and Management.
Xiaodan Zhu and Gerald Penn. 2006. Summarization of
spontaneous conversations. In Proceedings of Inter-
speech.
</reference>
<page confidence="0.999209">
339
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.941314">
<title confidence="0.99966">A Pilot Study of Opinion Summarization in Conversations</title>
<author confidence="0.999489">Dong Wang Yang Liu</author>
<affiliation confidence="0.993319">The University of Texas at</affiliation>
<email confidence="0.998244">dongwang,yangl@hlt.utdallas.edu</email>
<abstract confidence="0.997781347826087">This paper presents a pilot study of opinion summarization on conversations. We create a corpus containing extractive and abstractive summaries of speaker’s opinion towards a given topic using 88 telephone conversations. We adopt two methods to perform extractive summarization. The first one is a sentence-ranking method that linearly combines scores measured from different aspects including topic relevance, subjectivity, and sentence importance. The second one is a graph-based method, which incorporates topic and sentiment information, as well as additional information about sentence-to-sentence relations extracted based on dialogue structure. Our evaluation results show that both methods significantly outperform the baseline approach that extracts the longest utterances. In particular, we find that incorporating dialogue structure in the graph-based method contributes to the improved system performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandra Balahur</author>
<author>Ester Boldrini</author>
<author>Andr´es Montoyo</author>
<author>Patricio Martinez-Barco</author>
</authors>
<title>Going beyond traditional QA systems: challenges and keys in opinion question answering.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="6488" citStr="Balahur et al., 2010" startWordPosition="973" endWordPosition="976">ncy relations. Most of prior work used classification methods such as naive Bayes or SVMs to perform the polarity classification or opinion detection. Only a handful studies have used conversational speech for opinion recognition (Murray and Carenini, 2009; Raaijmakers et al., 2008), in which some domain-specific features are utilized such as structural features and prosodic features. Our work is also related to question answering (QA), especially opinion question answering. (Stoyanov et al., 2005) applies a subjectivity filter based on traditional QA systems to generate opinionated answers. (Balahur et al., 2010) answers some specific opinion questions like “Why do people criticize Richard Branson?” by retrieving candidate sentences using traditional QA methods and selecting the ones with the same polarity as the question. Our work is different in that we are not going to answer specific opinion questions, instead, we provide a summary on the speaker’s opinion towards a given topic. There exists some work on opinion summarization. For example, (Hu and Liu, 2004; Nishikawa et al., 2010) have explored opinion summarization in review domain, and (Paul et al., 2010) summarizes contrastive viewpoints in op</context>
</contexts>
<marker>Balahur, Boldrini, Montoyo, Martinez-Barco, 2010</marker>
<rawString>Alexandra Balahur, Ester Boldrini, Andr´es Montoyo, and Patricio Martinez-Barco. 2010. Going beyond traditional QA systems: challenges and keys in opinion question answering. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>LexRank: graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research.</journal>
<contexts>
<context position="17644" citStr="Erkan and Radev, 2004" startWordPosition="2775" endWordPosition="2778">typically do not contain much important content, especially the backchannels that appear frequently in dialogues. We also perform linear normalization such that the final value lies in [0, 1]. 4.2 Graph-based Summarization Graph-based methods have been widely used in document summarization. In this approach, a document is modeled as an adjacency matrix, where each node represents a sentence, and the weight of the edge between each pair of sentences is their similarity (cosine similarity is typically used). An iterative process is used until the scores for the nodes converge. Previous studies (Erkan and Radev, 2004) showed that this method can effectively extract important sentences from documents. The basic framework we use in this study is similar to the query-based graph summarization system in (Zhao et al., 2009). We also consider sentiment and topic relevance information, and propose to incorporate information obtained from dialog structure in this framework. The score for a DA s is based on its content similarity with all other DAs in the dialogue, the connection with other DAs based on the dialogue structure, the topic relevance, and its subjectivity, that is: P zEC sim(z, v) sim(s, v) score(v) RE</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes Erkan and Dragomir R. Radev. 2004. LexRank: graph-based lexical centrality as salience in text summarization. Journal of Artificial Intelligence Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadaoki Furui</author>
<author>Tomonori Kikuchi</author>
<author>Yousuke Shinnaka</author>
<author>Chior i Hori</author>
</authors>
<title>Speech-to-text and speech-tospeech summarization of spontaneous speech.</title>
<date>2004</date>
<journal>IEEE Transactions on Audio, Speech &amp; Language Processing,</journal>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="4899" citStr="Furui et al., 2004" startWordPosition="729" endWordPosition="732">s the paper. 2 Related Work Research in document summarization has been well established over the past decades. Many tasks have been defined such as single-document summarization, multi-document summarization, and querybased summarization. Previous studies have used various domains, including news articles, scientific articles, web documents, reviews. Recently there is an increasing research interest in speech summarization, such as conversational telephone speech (Zhu and Penn, 2006; Zechner, 2002), broadcast news (Maskey and Hirschberg, 2005; Lin et al., 2009), lectures (Zhang et al., 2007; Furui et al., 2004), meetings (Murray et al., 2005; Xie and Liu, 2010), voice mails (Koumpis and Renals, 2005). In general speech domains seem to be more difficult than well written text for summarization. In previous work, unsupervised methods like Maximal Marginal Relevance (MMR), Latent Semantic Analysis (LSA), and supervised methods that cast the extraction problem as a binary classification task have been adopted. Prior research has also explored using speech specific information, including prosodic features, dialog structure, and speech recognition confidence. In order to provide a summary over opinions, w</context>
</contexts>
<marker>Furui, Kikuchi, Shinnaka, Hori, 2004</marker>
<rawString>Sadaoki Furui, Tomonori Kikuchi, Yousuke Shinnaka, and Chior i Hori. 2004. Speech-to-text and speech-tospeech summarization of spontaneous speech. IEEE Transactions on Audio, Speech &amp; Language Processing, 12(4):401–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikhil Garg</author>
<author>Benoit Favre</author>
</authors>
<title>Korbinian Reidhammer, and Dilek Hakkani T¨ur.</title>
<date>2009</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<marker>Garg, Favre, 2009</marker>
<rawString>Nikhil Garg, Benoit Favre, Korbinian Reidhammer, and Dilek Hakkani T¨ur. 2009. ClusterRank: a graph based method for meeting summarization. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Korbinian Riedhammer</author>
<author>Benoit Favre</author>
<author>Dilek Hakkani-Tur</author>
</authors>
<title>A global optimization framework for meeting summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of ICASSP.</booktitle>
<contexts>
<context position="21906" citStr="Gillick et al., 2009" startWordPosition="3549" endWordPosition="3552">). In preprocessing we applied word stemming. We perform extractive summarization using different word compression ratios (ranging from 10% to 25%). We use human annotated dialogue acts (DA) as the extraction units. The system-generated summaries are compared to human annotated extractive and abstractive summaries. We use ROUGE as the evaluation metrics for summarization performance. We compare our methods to two systems. The first one is a baseline system, where we select the longest utterances for each speaker. This has been shown to be a relatively strong baseline for speech summarization (Gillick et al., 2009). The second one is human performance. We treat each annotator’s extractive summary as a system summary, and compare to the other two annotators’ extractive and abstractive summaries. This can be considered as the upper bound of our system performance. 5.2 Results From the development set, we used the grid search method to obtain the best combination weights for the two summarization methods. In the sentenceranking method, the best parameters found on the development set are Asim = 0, Arel = 0.3, Asent = 0.3, Alen = 0.4. It is surprising to see that the similarity score is not useful for this </context>
</contexts>
<marker>Gillick, Riedhammer, Favre, Hakkani-Tur, 2009</marker>
<rawString>Dan Gillick, Korbinian Riedhammer, Benoit Favre, and Dilek Hakkani-Tur. 2009. A global optimization framework for meeting summarization. In Proceedings of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Godfrey</author>
<author>Edward Holliman</author>
</authors>
<date>1997</date>
<booktitle>Switchboard-1 Release 2. In Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="7506" citStr="Godfrey and Holliman, 1997" startWordPosition="1136" endWordPosition="1140">some work on opinion summarization. For example, (Hu and Liu, 2004; Nishikawa et al., 2010) have explored opinion summarization in review domain, and (Paul et al., 2010) summarizes contrastive viewpoints in opinionated text. However, opinion summarization in spontaneous conversation is seldom studied. 3 Corpus Creation Though there are many annotated data sets for the research of speech summarization and sentiment analysis, there is no corpus available for opinion summarization on spontaneous speech. Thus for this study, we create a new pilot data set using a subset of the Switchboard corpus (Godfrey and Holliman, 1997).1 These are conversational telephone speech between two strangers that were assigned a topic to talk about for around 5 minutes. They were told to find the opinions of the other person. There are 70 topics in total. From the Switchboard cor1Please contact the authors to obtain the data. 332 pus, we selected 88 conversations from 6 topics for this study. Table 1 lists the number of conversations in each topic, their average length (measured in the unit of dialogue acts (DA)) and standard deviation of length. topic #Conv. avg len stdev space flight and exploration 6 capital punishment 24 gun co</context>
</contexts>
<marker>Godfrey, Holliman, 1997</marker>
<rawString>John J. Godfrey and Edward Holliman. 1997. Switchboard-1 Release 2. In Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hayes</author>
<author>Klaus Krippendorff</author>
</authors>
<title>Answering the call for a standard reliability measure for coding data.</title>
<date>2007</date>
<journal>Journal of Communication Methods and Measures,</journal>
<pages>1--77</pages>
<contexts>
<context position="12708" citStr="Hayes and Krippendorff, 2007" startWordPosition="1978" endWordPosition="1981">ctive summaries R-2 0.13 R-L 0.25 overall opinion α = 0.79 Table 3: Inter-annotator agreement for extractive and abstractive summaries, and overall opinion. summary annotation (Liu and Liu, 2008). The agreement on abstractive summaries is much lower than extractive summaries, which is as expected. Even for the same opinion or sentence, annotators use different words in the abstractive summaries. The agreement for the overall opinion annotation is similar to other opinion/emotion studies (Wilson, 2008b), but slightly lower than the level recommended by Krippendorff for reliable data (α = 0.8) (Hayes and Krippendorff, 2007), which shows it is even difficult for humans to determine what opinion a person holds (support or against something). Often human annotators have different interpretations about the same sentence, and a speaker’s opinion/attitude is sometimes ambiguous. Therefore this also demonstrates that it is more appropriate to provide a summary rather than a simple opinion category to answer questions about a person’s opinion towards something. 4 Opinion Summarization Methods Automatic summarization can be divided into extractive summarization and abstractive summarization. Extractive summarization sele</context>
</contexts>
<marker>Hayes, Krippendorff, 2007</marker>
<rawString>Andrew Hayes and Klaus Krippendorff. 2007. Answering the call for a standard reliability measure for coding data. Journal of Communication Methods and Measures, 1:77–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of ACM SIGKDD.</booktitle>
<contexts>
<context position="6945" citStr="Hu and Liu, 2004" startWordPosition="1050" endWordPosition="1053">estion answering. (Stoyanov et al., 2005) applies a subjectivity filter based on traditional QA systems to generate opinionated answers. (Balahur et al., 2010) answers some specific opinion questions like “Why do people criticize Richard Branson?” by retrieving candidate sentences using traditional QA methods and selecting the ones with the same polarity as the question. Our work is different in that we are not going to answer specific opinion questions, instead, we provide a summary on the speaker’s opinion towards a given topic. There exists some work on opinion summarization. For example, (Hu and Liu, 2004; Nishikawa et al., 2010) have explored opinion summarization in review domain, and (Paul et al., 2010) summarizes contrastive viewpoints in opinionated text. However, opinion summarization in spontaneous conversation is seldom studied. 3 Corpus Creation Though there are many annotated data sets for the research of speech summarization and sentiment analysis, there is no corpus available for opinion summarization on spontaneous speech. Thus for this study, we create a new pilot data set using a subset of the Switchboard corpus (Godfrey and Holliman, 1997).1 These are conversational telephone s</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of ACM SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Konstantinos Koumpis</author>
<author>Steve Renals</author>
</authors>
<title>Automatic summarization of voicemail messages using lexical and prosodic features.</title>
<date>2005</date>
<journal>ACM - Transactions on Speech and Language Processing.</journal>
<contexts>
<context position="4990" citStr="Koumpis and Renals, 2005" startWordPosition="744" endWordPosition="747">hed over the past decades. Many tasks have been defined such as single-document summarization, multi-document summarization, and querybased summarization. Previous studies have used various domains, including news articles, scientific articles, web documents, reviews. Recently there is an increasing research interest in speech summarization, such as conversational telephone speech (Zhu and Penn, 2006; Zechner, 2002), broadcast news (Maskey and Hirschberg, 2005; Lin et al., 2009), lectures (Zhang et al., 2007; Furui et al., 2004), meetings (Murray et al., 2005; Xie and Liu, 2010), voice mails (Koumpis and Renals, 2005). In general speech domains seem to be more difficult than well written text for summarization. In previous work, unsupervised methods like Maximal Marginal Relevance (MMR), Latent Semantic Analysis (LSA), and supervised methods that cast the extraction problem as a binary classification task have been adopted. Prior research has also explored using speech specific information, including prosodic features, dialog structure, and speech recognition confidence. In order to provide a summary over opinions, we need to find out which utterances in the conversation contain opinion. Most previous work</context>
</contexts>
<marker>Koumpis, Renals, 2005</marker>
<rawString>Konstantinos Koumpis and Steve Renals. 2005. Automatic summarization of voicemail messages using lexical and prosodic features. ACM - Transactions on Speech and Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shih Hsiang Lin</author>
<author>Berlin Chen</author>
<author>Hsin min Wang</author>
</authors>
<title>A comparative study of probabilistic ranking models for chinese spoken document summarization.</title>
<date>2009</date>
<journal>ACM Transactions on Asian Language Information Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="4848" citStr="Lin et al., 2009" startWordPosition="720" endWordPosition="723">lts and analysis in Section 5. Section 6 concludes the paper. 2 Related Work Research in document summarization has been well established over the past decades. Many tasks have been defined such as single-document summarization, multi-document summarization, and querybased summarization. Previous studies have used various domains, including news articles, scientific articles, web documents, reviews. Recently there is an increasing research interest in speech summarization, such as conversational telephone speech (Zhu and Penn, 2006; Zechner, 2002), broadcast news (Maskey and Hirschberg, 2005; Lin et al., 2009), lectures (Zhang et al., 2007; Furui et al., 2004), meetings (Murray et al., 2005; Xie and Liu, 2010), voice mails (Koumpis and Renals, 2005). In general speech domains seem to be more difficult than well written text for summarization. In previous work, unsupervised methods like Maximal Marginal Relevance (MMR), Latent Semantic Analysis (LSA), and supervised methods that cast the extraction problem as a binary classification task have been adopted. Prior research has also explored using speech specific information, including prosodic features, dialog structure, and speech recognition confide</context>
</contexts>
<marker>Lin, Chen, Wang, 2009</marker>
<rawString>Shih Hsiang Lin, Berlin Chen, and Hsin min Wang. 2009. A comparative study of probabilistic ranking models for chinese spoken document summarization. ACM Transactions on Asian Language Information Processing, 8(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: a package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL workshop on Text Summarization Branches Out.</booktitle>
<contexts>
<context position="11315" citStr="Lin, 2004" startWordPosition="1760" endWordPosition="1761">es and abstractive summaries as well as their standard deviation. Because in conversations, utterance length varies a lot, we use words as units when calculating the compression ratio. avg ratio stdev extractive summaries 0.26 0.13 abstractive summaries 0.13 0.06 Table 2: Compression ratio and standard deviation of extractive and abstractive summaries. We measured the inter-annotator agreement among the three annotators for the 18 conversations (each has two speakers, thus 36 “documents” in total). Results are shown in Table 3. For the extractive or abstractive summaries, we use ROUGE scores (Lin, 2004), a metric used to evaluate automatic summarization performance, to measure the pairwise agreement of summaries from different annotators. ROUGE F-scores are shown in the table for different matches, unigram (R-1), bigram (R-2), and longest subsequence (R-L). For the overall opinion category, since it is a multiclass label (not binary decision), we use Krippendorff’s α coefficient to measure human agreement, and the difference function for interval data: 62 _ (c − k)2 (where c, k are ck the interval values, on a scale of 1 to 5 corresponding to the five categories for the overall opinion). We </context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: a package for automatic evaluation of summaries. In Proceedings ofACL workshop on Text Summarization Branches Out.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Yang Liu</author>
</authors>
<title>What are meeting summaries? An analysis of human extractive summaries in meeting corpus.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGDial.</booktitle>
<contexts>
<context position="12274" citStr="Liu and Liu, 2008" startWordPosition="1912" endWordPosition="1915">sion), we use Krippendorff’s α coefficient to measure human agreement, and the difference function for interval data: 62 _ (c − k)2 (where c, k are ck the interval values, on a scale of 1 to 5 corresponding to the five categories for the overall opinion). We notice that the inter-annotator agreement for extractive summaries is comparable to other speech 333 R-1 0.61 extractive summaries R-2 0.52 R-L 0.61 R-1 0.32 abstractive summaries R-2 0.13 R-L 0.25 overall opinion α = 0.79 Table 3: Inter-annotator agreement for extractive and abstractive summaries, and overall opinion. summary annotation (Liu and Liu, 2008). The agreement on abstractive summaries is much lower than extractive summaries, which is as expected. Even for the same opinion or sentence, annotators use different words in the abstractive summaries. The agreement for the overall opinion annotation is similar to other opinion/emotion studies (Wilson, 2008b), but slightly lower than the level recommended by Krippendorff for reliable data (α = 0.8) (Hayes and Krippendorff, 2007), which shows it is even difficult for humans to determine what opinion a person holds (support or against something). Often human annotators have different interpret</context>
</contexts>
<marker>Liu, Liu, 2008</marker>
<rawString>Fei Liu and Yang Liu. 2008. What are meeting summaries? An analysis of human extractive summaries in meeting corpus. In Proceedings of SIGDial.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Maskey</author>
<author>Julia Hirschberg</author>
</authors>
<title>Comparing lexical, acoustic/prosodic, structural and discourse features for speech summarization.</title>
<date>2005</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<contexts>
<context position="4829" citStr="Maskey and Hirschberg, 2005" startWordPosition="716" endWordPosition="719">and present experimental results and analysis in Section 5. Section 6 concludes the paper. 2 Related Work Research in document summarization has been well established over the past decades. Many tasks have been defined such as single-document summarization, multi-document summarization, and querybased summarization. Previous studies have used various domains, including news articles, scientific articles, web documents, reviews. Recently there is an increasing research interest in speech summarization, such as conversational telephone speech (Zhu and Penn, 2006; Zechner, 2002), broadcast news (Maskey and Hirschberg, 2005; Lin et al., 2009), lectures (Zhang et al., 2007; Furui et al., 2004), meetings (Murray et al., 2005; Xie and Liu, 2010), voice mails (Koumpis and Renals, 2005). In general speech domains seem to be more difficult than well written text for summarization. In previous work, unsupervised methods like Maximal Marginal Relevance (MMR), Latent Semantic Analysis (LSA), and supervised methods that cast the extraction problem as a binary classification task have been adopted. Prior research has also explored using speech specific information, including prosodic features, dialog structure, and speech </context>
</contexts>
<marker>Maskey, Hirschberg, 2005</marker>
<rawString>Sameer Maskey and Julia Hirschberg. 2005. Comparing lexical, acoustic/prosodic, structural and discourse features for speech summarization. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen Mckeown</author>
<author>Julia Hirschberg</author>
<author>Michel Galley</author>
<author>Sameer Maskey</author>
</authors>
<title>From text to speech summarization.</title>
<date>2005</date>
<booktitle>In Proceedings of ICASSP.</booktitle>
<contexts>
<context position="2649" citStr="Mckeown et al., 2005" startWordPosition="390" endWordPosition="393">a pilot task in Text Analysis Conference (TAC) in 2008. The task was to produce summaries of opinions on specified targets from a set of blog documents. In this study, we investigate this problem using spontaneous conversations. The problem is defined as, given a conversation and a topic, a summarization system needs to generate a summary of the speaker’s opinion towards the topic. This task is built upon opinion recognition and topic or query based summarization. However, this problem is challenging in that: (a) Summarization in spontaneous speech is more difficult than well structured text (Mckeown et al., 2005), because speech is always less organized and has recognition errors when using speech recognition output; (b) Sentiment analysis in dialogues is also much harder because of the genre difference compared to other domains like product reviews or news resources, as reported in (Raaijmakers et al., 2008); (c) In conversational speech, information density is low and there are often off topic discussions, therefore presenting a need to identify utterances that are relevant to the topic. In this paper we perform an exploratory study on opinion summarization in conversations. We compare two unsupervi</context>
</contexts>
<marker>Mckeown, Hirschberg, Galley, Maskey, 2005</marker>
<rawString>Kathleen Mckeown, Julia Hirschberg, Michel Galley, and Sameer Maskey. 2005. From text to speech summarization. In Proceedings of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Murray</author>
<author>Giuseppe Carenini</author>
</authors>
<title>Detecting subjectivity in multiparty speech.</title>
<date>2009</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<contexts>
<context position="6123" citStr="Murray and Carenini, 2009" startWordPosition="920" endWordPosition="923">we need to find out which utterances in the conversation contain opinion. Most previous work in sentiment analysis has focused on reviews (Pang and Lee, 2004; Popescu and Etzioni, 2005; Ng et al., 2006) and news resources (Wiebe and Riloff, 2005). Many kinds of features are explored, such as lexical features (unigram, bigram and trigram), part-of-speech tags, dependency relations. Most of prior work used classification methods such as naive Bayes or SVMs to perform the polarity classification or opinion detection. Only a handful studies have used conversational speech for opinion recognition (Murray and Carenini, 2009; Raaijmakers et al., 2008), in which some domain-specific features are utilized such as structural features and prosodic features. Our work is also related to question answering (QA), especially opinion question answering. (Stoyanov et al., 2005) applies a subjectivity filter based on traditional QA systems to generate opinionated answers. (Balahur et al., 2010) answers some specific opinion questions like “Why do people criticize Richard Branson?” by retrieving candidate sentences using traditional QA methods and selecting the ones with the same polarity as the question. Our work is differen</context>
</contexts>
<marker>Murray, Carenini, 2009</marker>
<rawString>Gabriel Murray and Giuseppe Carenini. 2009. Detecting subjectivity in multiparty speech. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Murray</author>
<author>Steve Renals</author>
<author>Jean Carletta</author>
</authors>
<title>Extractive summarization of meeting recordings.</title>
<date>2005</date>
<booktitle>In Proceedings of EUROSPEECH.</booktitle>
<contexts>
<context position="4930" citStr="Murray et al., 2005" startWordPosition="734" endWordPosition="737">earch in document summarization has been well established over the past decades. Many tasks have been defined such as single-document summarization, multi-document summarization, and querybased summarization. Previous studies have used various domains, including news articles, scientific articles, web documents, reviews. Recently there is an increasing research interest in speech summarization, such as conversational telephone speech (Zhu and Penn, 2006; Zechner, 2002), broadcast news (Maskey and Hirschberg, 2005; Lin et al., 2009), lectures (Zhang et al., 2007; Furui et al., 2004), meetings (Murray et al., 2005; Xie and Liu, 2010), voice mails (Koumpis and Renals, 2005). In general speech domains seem to be more difficult than well written text for summarization. In previous work, unsupervised methods like Maximal Marginal Relevance (MMR), Latent Semantic Analysis (LSA), and supervised methods that cast the extraction problem as a binary classification task have been adopted. Prior research has also explored using speech specific information, including prosodic features, dialog structure, and speech recognition confidence. In order to provide a summary over opinions, we need to find out which uttera</context>
</contexts>
<marker>Murray, Renals, Carletta, 2005</marker>
<rawString>Gabriel Murray, Steve Renals, and Jean Carletta. 2005. Extractive summarization of meeting recordings. In Proceedings of EUROSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Sajib Dasgupta</author>
<author>S M Niaz Arifin</author>
</authors>
<title>Examining the role of linguistic knowledge sources in the automatic identification and classification of reviews.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL.</booktitle>
<contexts>
<context position="5700" citStr="Ng et al., 2006" startWordPosition="856" endWordPosition="859">. In previous work, unsupervised methods like Maximal Marginal Relevance (MMR), Latent Semantic Analysis (LSA), and supervised methods that cast the extraction problem as a binary classification task have been adopted. Prior research has also explored using speech specific information, including prosodic features, dialog structure, and speech recognition confidence. In order to provide a summary over opinions, we need to find out which utterances in the conversation contain opinion. Most previous work in sentiment analysis has focused on reviews (Pang and Lee, 2004; Popescu and Etzioni, 2005; Ng et al., 2006) and news resources (Wiebe and Riloff, 2005). Many kinds of features are explored, such as lexical features (unigram, bigram and trigram), part-of-speech tags, dependency relations. Most of prior work used classification methods such as naive Bayes or SVMs to perform the polarity classification or opinion detection. Only a handful studies have used conversational speech for opinion recognition (Murray and Carenini, 2009; Raaijmakers et al., 2008), in which some domain-specific features are utilized such as structural features and prosodic features. Our work is also related to question answerin</context>
</contexts>
<marker>Ng, Dasgupta, Arifin, 2006</marker>
<rawString>Vincent Ng, Sajib Dasgupta, and S.M.Niaz Arifin. 2006. Examining the role of linguistic knowledge sources in the automatic identification and classification of reviews. In Proceedings of the COLING/ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hitoshi Nishikawa</author>
<author>Takaaki Hasegawa</author>
<author>Yoshihiro Matsuo</author>
<author>Genichiro Kikui</author>
</authors>
<title>Opinion summarization with integer linear programming formulation for sentence extraction and ordering.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="6970" citStr="Nishikawa et al., 2010" startWordPosition="1054" endWordPosition="1057">(Stoyanov et al., 2005) applies a subjectivity filter based on traditional QA systems to generate opinionated answers. (Balahur et al., 2010) answers some specific opinion questions like “Why do people criticize Richard Branson?” by retrieving candidate sentences using traditional QA methods and selecting the ones with the same polarity as the question. Our work is different in that we are not going to answer specific opinion questions, instead, we provide a summary on the speaker’s opinion towards a given topic. There exists some work on opinion summarization. For example, (Hu and Liu, 2004; Nishikawa et al., 2010) have explored opinion summarization in review domain, and (Paul et al., 2010) summarizes contrastive viewpoints in opinionated text. However, opinion summarization in spontaneous conversation is seldom studied. 3 Corpus Creation Though there are many annotated data sets for the research of speech summarization and sentiment analysis, there is no corpus available for opinion summarization on spontaneous speech. Thus for this study, we create a new pilot data set using a subset of the Switchboard corpus (Godfrey and Holliman, 1997).1 These are conversational telephone speech between two strange</context>
</contexts>
<marker>Nishikawa, Hasegawa, Matsuo, Kikui, 2010</marker>
<rawString>Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Matsuo, and Genichiro Kikui. 2010. Opinion summarization with integer linear programming formulation for sentence extraction and ordering. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lilian Lee</author>
</authors>
<title>A sentiment education: sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5655" citStr="Pang and Lee, 2004" startWordPosition="848" endWordPosition="851">ficult than well written text for summarization. In previous work, unsupervised methods like Maximal Marginal Relevance (MMR), Latent Semantic Analysis (LSA), and supervised methods that cast the extraction problem as a binary classification task have been adopted. Prior research has also explored using speech specific information, including prosodic features, dialog structure, and speech recognition confidence. In order to provide a summary over opinions, we need to find out which utterances in the conversation contain opinion. Most previous work in sentiment analysis has focused on reviews (Pang and Lee, 2004; Popescu and Etzioni, 2005; Ng et al., 2006) and news resources (Wiebe and Riloff, 2005). Many kinds of features are explored, such as lexical features (unigram, bigram and trigram), part-of-speech tags, dependency relations. Most of prior work used classification methods such as naive Bayes or SVMs to perform the polarity classification or opinion detection. Only a handful studies have used conversational speech for opinion recognition (Murray and Carenini, 2009; Raaijmakers et al., 2008), in which some domain-specific features are utilized such as structural features and prosodic features. </context>
<context position="16606" citStr="Pang and Lee, 2004" startWordPosition="2610" endWordPosition="2613">ulating PMI scores. Because the value of PMI(w, topic) is negative, we transform it into a positive one (denoted by PMI+(w, topic)) by adding the absolute value of the minimum value. The final relevance score of each sentence is normalized to [0, 1] using linear normalization: RELorig(s, topic) = X PMI+(w, topic) wEs RELorig(s,topic) − Min REL(s,topic) = Max − Min sentiment(s) indicates the probability that utterance s contains opinion. To obtain this, we trained a maximum entropy classifier with a bag-of-words model using a combination of data sets from several domains, including movie data (Pang and Lee, 2004), news articles from MPQA corpus (Wilson and Wiebe, 2003), and meeting transcripts from AMI corpus (Wilson, 2008a). Each sentence (or DA) in these corpora is annotated as “subjective” or “objective”. We use each utterance’s probability of being “subjective” predicted by the classifier as its sentiment score. • length(s) is the length of the utterance. This score can effectively penalize the short sentences which typically do not contain much important content, especially the backchannels that appear frequently in dialogues. We also perform linear normalization such that the final value lies in</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lilian Lee. 2004. A sentiment education: sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
<author>ChengXiang Zhai</author>
<author>Roxana Girju</author>
</authors>
<title>Summarizing contrastive viewpoints in opinionated text.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="7048" citStr="Paul et al., 2010" startWordPosition="1066" endWordPosition="1069">ms to generate opinionated answers. (Balahur et al., 2010) answers some specific opinion questions like “Why do people criticize Richard Branson?” by retrieving candidate sentences using traditional QA methods and selecting the ones with the same polarity as the question. Our work is different in that we are not going to answer specific opinion questions, instead, we provide a summary on the speaker’s opinion towards a given topic. There exists some work on opinion summarization. For example, (Hu and Liu, 2004; Nishikawa et al., 2010) have explored opinion summarization in review domain, and (Paul et al., 2010) summarizes contrastive viewpoints in opinionated text. However, opinion summarization in spontaneous conversation is seldom studied. 3 Corpus Creation Though there are many annotated data sets for the research of speech summarization and sentiment analysis, there is no corpus available for opinion summarization on spontaneous speech. Thus for this study, we create a new pilot data set using a subset of the Switchboard corpus (Godfrey and Holliman, 1997).1 These are conversational telephone speech between two strangers that were assigned a topic to talk about for around 5 minutes. They were to</context>
</contexts>
<marker>Paul, Zhai, Girju, 2010</marker>
<rawString>Michael Paul, ChengXiang Zhai, and Roxana Girju. 2010. Summarizing contrastive viewpoints in opinionated text. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP.</booktitle>
<contexts>
<context position="5682" citStr="Popescu and Etzioni, 2005" startWordPosition="852" endWordPosition="855">tten text for summarization. In previous work, unsupervised methods like Maximal Marginal Relevance (MMR), Latent Semantic Analysis (LSA), and supervised methods that cast the extraction problem as a binary classification task have been adopted. Prior research has also explored using speech specific information, including prosodic features, dialog structure, and speech recognition confidence. In order to provide a summary over opinions, we need to find out which utterances in the conversation contain opinion. Most previous work in sentiment analysis has focused on reviews (Pang and Lee, 2004; Popescu and Etzioni, 2005; Ng et al., 2006) and news resources (Wiebe and Riloff, 2005). Many kinds of features are explored, such as lexical features (unigram, bigram and trigram), part-of-speech tags, dependency relations. Most of prior work used classification methods such as naive Bayes or SVMs to perform the polarity classification or opinion detection. Only a handful studies have used conversational speech for opinion recognition (Murray and Carenini, 2009; Raaijmakers et al., 2008), in which some domain-specific features are utilized such as structural features and prosodic features. Our work is also related to</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Raaijmakers</author>
<author>Khiet Truong</author>
<author>Theresa Wilson</author>
</authors>
<title>Multimodal subjectivity analysis of multiparty conversation.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2951" citStr="Raaijmakers et al., 2008" startWordPosition="440" endWordPosition="443">ization system needs to generate a summary of the speaker’s opinion towards the topic. This task is built upon opinion recognition and topic or query based summarization. However, this problem is challenging in that: (a) Summarization in spontaneous speech is more difficult than well structured text (Mckeown et al., 2005), because speech is always less organized and has recognition errors when using speech recognition output; (b) Sentiment analysis in dialogues is also much harder because of the genre difference compared to other domains like product reviews or news resources, as reported in (Raaijmakers et al., 2008); (c) In conversational speech, information density is low and there are often off topic discussions, therefore presenting a need to identify utterances that are relevant to the topic. In this paper we perform an exploratory study on opinion summarization in conversations. We compare two unsupervised methods that have been 331 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 331–339, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics widely used in extractive summarization: sentenceranking and graph-based methods. </context>
<context position="6150" citStr="Raaijmakers et al., 2008" startWordPosition="924" endWordPosition="927">tterances in the conversation contain opinion. Most previous work in sentiment analysis has focused on reviews (Pang and Lee, 2004; Popescu and Etzioni, 2005; Ng et al., 2006) and news resources (Wiebe and Riloff, 2005). Many kinds of features are explored, such as lexical features (unigram, bigram and trigram), part-of-speech tags, dependency relations. Most of prior work used classification methods such as naive Bayes or SVMs to perform the polarity classification or opinion detection. Only a handful studies have used conversational speech for opinion recognition (Murray and Carenini, 2009; Raaijmakers et al., 2008), in which some domain-specific features are utilized such as structural features and prosodic features. Our work is also related to question answering (QA), especially opinion question answering. (Stoyanov et al., 2005) applies a subjectivity filter based on traditional QA systems to generate opinionated answers. (Balahur et al., 2010) answers some specific opinion questions like “Why do people criticize Richard Branson?” by retrieving candidate sentences using traditional QA methods and selecting the ones with the same polarity as the question. Our work is different in that we are not going </context>
</contexts>
<marker>Raaijmakers, Truong, Wilson, 2008</marker>
<rawString>Stephan Raaijmakers, Khiet Truong, and Theresa Wilson. 2008. Multimodal subjectivity analysis of multiparty conversation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Claire Cardie</author>
<author>Janyce Wiebe</author>
</authors>
<title>Multi-perspective question answering using the OpQA corpus.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP/HLT.</booktitle>
<contexts>
<context position="6370" citStr="Stoyanov et al., 2005" startWordPosition="955" endWordPosition="959">ny kinds of features are explored, such as lexical features (unigram, bigram and trigram), part-of-speech tags, dependency relations. Most of prior work used classification methods such as naive Bayes or SVMs to perform the polarity classification or opinion detection. Only a handful studies have used conversational speech for opinion recognition (Murray and Carenini, 2009; Raaijmakers et al., 2008), in which some domain-specific features are utilized such as structural features and prosodic features. Our work is also related to question answering (QA), especially opinion question answering. (Stoyanov et al., 2005) applies a subjectivity filter based on traditional QA systems to generate opinionated answers. (Balahur et al., 2010) answers some specific opinion questions like “Why do people criticize Richard Branson?” by retrieving candidate sentences using traditional QA methods and selecting the ones with the same polarity as the question. Our work is different in that we are not going to answer specific opinion questions, instead, we provide a summary on the speaker’s opinion towards a given topic. There exists some work on opinion summarization. For example, (Hu and Liu, 2004; Nishikawa et al., 2010)</context>
</contexts>
<marker>Stoyanov, Cardie, Wiebe, 2005</marker>
<rawString>Veselin Stoyanov, Claire Cardie, and Janyce Wiebe. 2005. Multi-perspective question answering using the OpQA corpus. In Proceedings of EMNLP/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Ellen Riloff</author>
</authors>
<title>Creating subjective and objective sentence classifiers from unannotated texts.</title>
<date>2005</date>
<booktitle>In Proceedings of CICLing.</booktitle>
<contexts>
<context position="5744" citStr="Wiebe and Riloff, 2005" startWordPosition="863" endWordPosition="866">ods like Maximal Marginal Relevance (MMR), Latent Semantic Analysis (LSA), and supervised methods that cast the extraction problem as a binary classification task have been adopted. Prior research has also explored using speech specific information, including prosodic features, dialog structure, and speech recognition confidence. In order to provide a summary over opinions, we need to find out which utterances in the conversation contain opinion. Most previous work in sentiment analysis has focused on reviews (Pang and Lee, 2004; Popescu and Etzioni, 2005; Ng et al., 2006) and news resources (Wiebe and Riloff, 2005). Many kinds of features are explored, such as lexical features (unigram, bigram and trigram), part-of-speech tags, dependency relations. Most of prior work used classification methods such as naive Bayes or SVMs to perform the polarity classification or opinion detection. Only a handful studies have used conversational speech for opinion recognition (Murray and Carenini, 2009; Raaijmakers et al., 2008), in which some domain-specific features are utilized such as structural features and prosodic features. Our work is also related to question answering (QA), especially opinion question answerin</context>
</contexts>
<marker>Wiebe, Riloff, 2005</marker>
<rawString>Janyce Wiebe and Ellen Riloff. 2005. Creating subjective and objective sentence classifiers from unannotated texts. In Proceedings of CICLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
</authors>
<title>Annotating opinions in the world press.</title>
<date>2003</date>
<booktitle>In Proceedings of SIGDial.</booktitle>
<contexts>
<context position="16663" citStr="Wilson and Wiebe, 2003" startWordPosition="2619" endWordPosition="2622">) is negative, we transform it into a positive one (denoted by PMI+(w, topic)) by adding the absolute value of the minimum value. The final relevance score of each sentence is normalized to [0, 1] using linear normalization: RELorig(s, topic) = X PMI+(w, topic) wEs RELorig(s,topic) − Min REL(s,topic) = Max − Min sentiment(s) indicates the probability that utterance s contains opinion. To obtain this, we trained a maximum entropy classifier with a bag-of-words model using a combination of data sets from several domains, including movie data (Pang and Lee, 2004), news articles from MPQA corpus (Wilson and Wiebe, 2003), and meeting transcripts from AMI corpus (Wilson, 2008a). Each sentence (or DA) in these corpora is annotated as “subjective” or “objective”. We use each utterance’s probability of being “subjective” predicted by the classifier as its sentiment score. • length(s) is the length of the utterance. This score can effectively penalize the short sentences which typically do not contain much important content, especially the backchannels that appear frequently in dialogues. We also perform linear normalization such that the final value lies in [0, 1]. 4.2 Graph-based Summarization Graph-based method</context>
</contexts>
<marker>Wilson, Wiebe, 2003</marker>
<rawString>Theresa Wilson and Janyce Wiebe. 2003. Annotating opinions in the world press. In Proceedings of SIGDial.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
</authors>
<title>Annotating subjective content in meetings.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="12584" citStr="Wilson, 2008" startWordPosition="1959" endWordPosition="1961">summaries is comparable to other speech 333 R-1 0.61 extractive summaries R-2 0.52 R-L 0.61 R-1 0.32 abstractive summaries R-2 0.13 R-L 0.25 overall opinion α = 0.79 Table 3: Inter-annotator agreement for extractive and abstractive summaries, and overall opinion. summary annotation (Liu and Liu, 2008). The agreement on abstractive summaries is much lower than extractive summaries, which is as expected. Even for the same opinion or sentence, annotators use different words in the abstractive summaries. The agreement for the overall opinion annotation is similar to other opinion/emotion studies (Wilson, 2008b), but slightly lower than the level recommended by Krippendorff for reliable data (α = 0.8) (Hayes and Krippendorff, 2007), which shows it is even difficult for humans to determine what opinion a person holds (support or against something). Often human annotators have different interpretations about the same sentence, and a speaker’s opinion/attitude is sometimes ambiguous. Therefore this also demonstrates that it is more appropriate to provide a summary rather than a simple opinion category to answer questions about a person’s opinion towards something. 4 Opinion Summarization Methods Autom</context>
<context position="16718" citStr="Wilson, 2008" startWordPosition="2629" endWordPosition="2631">+(w, topic)) by adding the absolute value of the minimum value. The final relevance score of each sentence is normalized to [0, 1] using linear normalization: RELorig(s, topic) = X PMI+(w, topic) wEs RELorig(s,topic) − Min REL(s,topic) = Max − Min sentiment(s) indicates the probability that utterance s contains opinion. To obtain this, we trained a maximum entropy classifier with a bag-of-words model using a combination of data sets from several domains, including movie data (Pang and Lee, 2004), news articles from MPQA corpus (Wilson and Wiebe, 2003), and meeting transcripts from AMI corpus (Wilson, 2008a). Each sentence (or DA) in these corpora is annotated as “subjective” or “objective”. We use each utterance’s probability of being “subjective” predicted by the classifier as its sentiment score. • length(s) is the length of the utterance. This score can effectively penalize the short sentences which typically do not contain much important content, especially the backchannels that appear frequently in dialogues. We also perform linear normalization such that the final value lies in [0, 1]. 4.2 Graph-based Summarization Graph-based methods have been widely used in document summarization. In t</context>
</contexts>
<marker>Wilson, 2008</marker>
<rawString>Theresa Wilson. 2008a. Annotating subjective content in meetings. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
</authors>
<title>Fine-grained subjectivity and sentiment analysis: recognizing the intensity, polarity, and attitudes of private states.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pittsburgh.</institution>
<contexts>
<context position="12584" citStr="Wilson, 2008" startWordPosition="1959" endWordPosition="1961">summaries is comparable to other speech 333 R-1 0.61 extractive summaries R-2 0.52 R-L 0.61 R-1 0.32 abstractive summaries R-2 0.13 R-L 0.25 overall opinion α = 0.79 Table 3: Inter-annotator agreement for extractive and abstractive summaries, and overall opinion. summary annotation (Liu and Liu, 2008). The agreement on abstractive summaries is much lower than extractive summaries, which is as expected. Even for the same opinion or sentence, annotators use different words in the abstractive summaries. The agreement for the overall opinion annotation is similar to other opinion/emotion studies (Wilson, 2008b), but slightly lower than the level recommended by Krippendorff for reliable data (α = 0.8) (Hayes and Krippendorff, 2007), which shows it is even difficult for humans to determine what opinion a person holds (support or against something). Often human annotators have different interpretations about the same sentence, and a speaker’s opinion/attitude is sometimes ambiguous. Therefore this also demonstrates that it is more appropriate to provide a summary rather than a simple opinion category to answer questions about a person’s opinion towards something. 4 Opinion Summarization Methods Autom</context>
<context position="16718" citStr="Wilson, 2008" startWordPosition="2629" endWordPosition="2631">+(w, topic)) by adding the absolute value of the minimum value. The final relevance score of each sentence is normalized to [0, 1] using linear normalization: RELorig(s, topic) = X PMI+(w, topic) wEs RELorig(s,topic) − Min REL(s,topic) = Max − Min sentiment(s) indicates the probability that utterance s contains opinion. To obtain this, we trained a maximum entropy classifier with a bag-of-words model using a combination of data sets from several domains, including movie data (Pang and Lee, 2004), news articles from MPQA corpus (Wilson and Wiebe, 2003), and meeting transcripts from AMI corpus (Wilson, 2008a). Each sentence (or DA) in these corpora is annotated as “subjective” or “objective”. We use each utterance’s probability of being “subjective” predicted by the classifier as its sentiment score. • length(s) is the length of the utterance. This score can effectively penalize the short sentences which typically do not contain much important content, especially the backchannels that appear frequently in dialogues. We also perform linear normalization such that the final value lies in [0, 1]. 4.2 Graph-based Summarization Graph-based methods have been widely used in document summarization. In t</context>
</contexts>
<marker>Wilson, 2008</marker>
<rawString>Theresa Wilson. 2008b. Fine-grained subjectivity and sentiment analysis: recognizing the intensity, polarity, and attitudes of private states. Ph.D. thesis, University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shasha Xie</author>
<author>Yang Liu</author>
</authors>
<title>Improving supervised learning for meeting summarization using sampling and regression. Computer Speech and Language,</title>
<date>2010</date>
<pages>24--495</pages>
<contexts>
<context position="4950" citStr="Xie and Liu, 2010" startWordPosition="738" endWordPosition="741">marization has been well established over the past decades. Many tasks have been defined such as single-document summarization, multi-document summarization, and querybased summarization. Previous studies have used various domains, including news articles, scientific articles, web documents, reviews. Recently there is an increasing research interest in speech summarization, such as conversational telephone speech (Zhu and Penn, 2006; Zechner, 2002), broadcast news (Maskey and Hirschberg, 2005; Lin et al., 2009), lectures (Zhang et al., 2007; Furui et al., 2004), meetings (Murray et al., 2005; Xie and Liu, 2010), voice mails (Koumpis and Renals, 2005). In general speech domains seem to be more difficult than well written text for summarization. In previous work, unsupervised methods like Maximal Marginal Relevance (MMR), Latent Semantic Analysis (LSA), and supervised methods that cast the extraction problem as a binary classification task have been adopted. Prior research has also explored using speech specific information, including prosodic features, dialog structure, and speech recognition confidence. In order to provide a summary over opinions, we need to find out which utterances in the conversa</context>
</contexts>
<marker>Xie, Liu, 2010</marker>
<rawString>Shasha Xie and Yang Liu. 2010. Improving supervised learning for meeting summarization using sampling and regression. Computer Speech and Language, 24:495–514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Zechner</author>
</authors>
<title>Automatic summarization of open-domain multiparty dialogues in dive rse genres.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<pages>28--447</pages>
<contexts>
<context position="4784" citStr="Zechner, 2002" startWordPosition="712" endWordPosition="713">mmarization system in Section 4 and present experimental results and analysis in Section 5. Section 6 concludes the paper. 2 Related Work Research in document summarization has been well established over the past decades. Many tasks have been defined such as single-document summarization, multi-document summarization, and querybased summarization. Previous studies have used various domains, including news articles, scientific articles, web documents, reviews. Recently there is an increasing research interest in speech summarization, such as conversational telephone speech (Zhu and Penn, 2006; Zechner, 2002), broadcast news (Maskey and Hirschberg, 2005; Lin et al., 2009), lectures (Zhang et al., 2007; Furui et al., 2004), meetings (Murray et al., 2005; Xie and Liu, 2010), voice mails (Koumpis and Renals, 2005). In general speech domains seem to be more difficult than well written text for summarization. In previous work, unsupervised methods like Maximal Marginal Relevance (MMR), Latent Semantic Analysis (LSA), and supervised methods that cast the extraction problem as a binary classification task have been adopted. Prior research has also explored using speech specific information, including pro</context>
</contexts>
<marker>Zechner, 2002</marker>
<rawString>Klaus Zechner. 2002. Automatic summarization of open-domain multiparty dialogues in dive rse genres. Computational Linguistics, 28:447–485.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Jian Zhang</author>
<author>Ho Yin Chan</author>
<author>Pascale Fung</author>
</authors>
<title>Improving lecture speech summarization using rhetorical information.</title>
<date>2007</date>
<booktitle>In Proceedings of Biannual IEEE Workshop on ASRU.</booktitle>
<contexts>
<context position="4878" citStr="Zhang et al., 2007" startWordPosition="725" endWordPosition="728">. Section 6 concludes the paper. 2 Related Work Research in document summarization has been well established over the past decades. Many tasks have been defined such as single-document summarization, multi-document summarization, and querybased summarization. Previous studies have used various domains, including news articles, scientific articles, web documents, reviews. Recently there is an increasing research interest in speech summarization, such as conversational telephone speech (Zhu and Penn, 2006; Zechner, 2002), broadcast news (Maskey and Hirschberg, 2005; Lin et al., 2009), lectures (Zhang et al., 2007; Furui et al., 2004), meetings (Murray et al., 2005; Xie and Liu, 2010), voice mails (Koumpis and Renals, 2005). In general speech domains seem to be more difficult than well written text for summarization. In previous work, unsupervised methods like Maximal Marginal Relevance (MMR), Latent Semantic Analysis (LSA), and supervised methods that cast the extraction problem as a binary classification task have been adopted. Prior research has also explored using speech specific information, including prosodic features, dialog structure, and speech recognition confidence. In order to provide a sum</context>
</contexts>
<marker>Zhang, Chan, Fung, 2007</marker>
<rawString>Justin Jian Zhang, Ho Yin Chan, and Pascale Fung. 2007. Improving lecture speech summarization using rhetorical information. In Proceedings of Biannual IEEE Workshop on ASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Zhao</author>
<author>Lide Wu</author>
<author>Xuanjing Huang</author>
</authors>
<title>Using query expansion in graph-based approach for queryfocused multi-document summarization.</title>
<date>2009</date>
<journal>Journal of Information Processing and Management.</journal>
<contexts>
<context position="17849" citStr="Zhao et al., 2009" startWordPosition="2807" endWordPosition="2810">Summarization Graph-based methods have been widely used in document summarization. In this approach, a document is modeled as an adjacency matrix, where each node represents a sentence, and the weight of the edge between each pair of sentences is their similarity (cosine similarity is typically used). An iterative process is used until the scores for the nodes converge. Previous studies (Erkan and Radev, 2004) showed that this method can effectively extract important sentences from documents. The basic framework we use in this study is similar to the query-based graph summarization system in (Zhao et al., 2009). We also consider sentiment and topic relevance information, and propose to incorporate information obtained from dialog structure in this framework. The score for a DA s is based on its content similarity with all other DAs in the dialogue, the connection with other DAs based on the dialogue structure, the topic relevance, and its subjectivity, that is: P zEC sim(z, v) sim(s, v) score(v) REL(s,topic) +Arel PzEC REL(z, topic) sentiment(s) +�sent PzEC sentiment(z) +�adj vEC X PzE l , ) ADJ(s, v) G&apos; ADJ ( ) (z v score (v) X Ai = 1 (3) i where C is the set of all DAs in the dialogue; REL(s, topi</context>
</contexts>
<marker>Zhao, Wu, Huang, 2009</marker>
<rawString>Lin Zhao, Lide Wu, and Xuanjing Huang. 2009. Using query expansion in graph-based approach for queryfocused multi-document summarization. Journal of Information Processing and Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodan Zhu</author>
<author>Gerald Penn</author>
</authors>
<title>Summarization of spontaneous conversations.</title>
<date>2006</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<contexts>
<context position="4768" citStr="Zhu and Penn, 2006" startWordPosition="708" endWordPosition="711">nted conversation summarization system in Section 4 and present experimental results and analysis in Section 5. Section 6 concludes the paper. 2 Related Work Research in document summarization has been well established over the past decades. Many tasks have been defined such as single-document summarization, multi-document summarization, and querybased summarization. Previous studies have used various domains, including news articles, scientific articles, web documents, reviews. Recently there is an increasing research interest in speech summarization, such as conversational telephone speech (Zhu and Penn, 2006; Zechner, 2002), broadcast news (Maskey and Hirschberg, 2005; Lin et al., 2009), lectures (Zhang et al., 2007; Furui et al., 2004), meetings (Murray et al., 2005; Xie and Liu, 2010), voice mails (Koumpis and Renals, 2005). In general speech domains seem to be more difficult than well written text for summarization. In previous work, unsupervised methods like Maximal Marginal Relevance (MMR), Latent Semantic Analysis (LSA), and supervised methods that cast the extraction problem as a binary classification task have been adopted. Prior research has also explored using speech specific informatio</context>
</contexts>
<marker>Zhu, Penn, 2006</marker>
<rawString>Xiaodan Zhu and Gerald Penn. 2006. Summarization of spontaneous conversations. In Proceedings of Interspeech.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>