<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000016">
<title confidence="0.9984885">
Fast Generation of Translation Forest
for Large-Scale SMT Discriminative Training
</title>
<author confidence="0.99311">
Xinyan Xiao, Yang Liu, Qun Liu, and Shouxun Lin
</author>
<affiliation confidence="0.984147333333333">
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<email confidence="0.998437">
{xiaoxinyan,yliu,liuqun,sxlin}@ict.ac.cn
</email>
<sectionHeader confidence="0.996654" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99965025">
Although discriminative training guarantees to
improve statistical machine translation by in-
corporating a large amount of overlapping fea-
tures, it is hard to scale up to large data due to
decoding complexity. We propose a new al-
gorithm to generate translation forest of train-
ing data in linear time with the help of word
alignment. Our algorithm also alleviates the
oracle selection problem by ensuring that a
forest always contains derivations that exactly
yield the reference translation. With millions
of features trained on 519K sentences in 0.03
second per sentence, our system achieves sig-
nificant improvement by 0.84 BLEU over the
baseline system on the NIST Chinese-English
test sets.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99377306122449">
Discriminative model (Och and Ney, 2002) can
easily incorporate non-independent and overlapping
features, and has been dominating the research field
of statistical machine translation (SMT) in the last
decade. Recent work have shown that SMT benefits
a lot from exploiting large amount of features (Liang
et al., 2006; Tillmann and Zhang, 2006; Watanabe
et al., 2007; Blunsom et al., 2008; Chiang et al.,
2009). However, the training of the large number
of features was always restricted in fairly small data
sets. Some systems limit the number of training ex-
amples, while others use short sentences to maintain
efficiency.
Overfitting problem often comes when training
many features on a small data (Watanabe et al.,
2007; Chiang et al., 2009). Obviously, using much
more data can alleviate such problem. Furthermore,
large data also enables us to globally train millions
of sparse lexical features which offer accurate clues
for SMT. Despite these advantages, to the best of
our knowledge, no previous discriminative training
paradigms scale up to use a large amount of training
data. The main obstacle comes from the complexity
of packed forests or n-best lists generation which
requires to search through all possible translations
of each training example, which is computationally
prohibitive in practice for SMT.
To make normalization efficient, contrastive esti-
mation (Smith and Eisner, 2005; Poon et al., 2009)
introduce neighborhood for unsupervised log-linear
model, and has presented positive results in various
tasks. Motivated by these work, we use a translation
forest (Section 3) which contains both “reference”
derivations that potentially yield the reference trans-
lation and also neighboring “non-reference” deriva-
tions that fail to produce the reference translation.1
However, the complexity of generating this transla-
tion forest is up to 0(n6), because we still need bi-
parsing to create the reference derivations.
Consequently, we propose a method to fast gener-
ate a subset of the forest. The key idea (Section 4)
is to initialize a reference derivation tree with maxi-
mum score by the help of word alignment, and then
traverse the tree to generate the subset forest in lin-
ear time. Besides the efficiency improvement, such
a forest allows us to train the model without resort-
&apos;Exactly, there are no reference derivations, since derivation
is a latent variable in SMT. We call them reference derivation
just for convenience.
</bodyText>
<page confidence="0.956396">
880
</page>
<note confidence="0.973425">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 880–888,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<table confidence="0.985557333333333">
hyper- rule
edge
e1 r1 X (X1 bei X2, X1 was X2)
e2 r2 X (qiangshou bei X1,
the gunman was X1)
e3 r3 X (jingfang X1, X1 by the police)
e4 r4 X (jingfang X1, police X1 )
e5 r5 X (qiangshou, the gunman)
e6 r6 X (jibi, shot dead)
</table>
<figure confidence="0.918998636363636">
0,4
2
1
2,4
4
3,4
6
0 1 2 3 4
3
0,1
5
</figure>
<figureCaption confidence="0.9359192">
Figure 1: A translation forest which is the running example throughout this paper. The reference translation is “the
gunman was killed by the police”. (1) Solid hyperedges denote a “reference” derivation tree t1 which exactly yields
the reference translation. (2) Replacing e3 in t1 with e4 results a competing non-reference derivation t2, which fails to
swap the order of X3,4. (3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3. Generally,
this is done by deleting a node X0,1.
</figureCaption>
<bodyText confidence="0.999673352941177">
ing to constructing the oracle reference (Liang et al.,
2006; Watanabe et al., 2007; Chiang et al., 2009),
which is non-trivial for SMT and needs to be deter-
mined experimentally. Given such forests, we glob-
ally learn a log-linear model using stochastic gradi-
ent descend (Section 5). Overall, both the generation
of forests and the training algorithm are scalable, en-
abling us to train millions of features on large-scale
data.
To show the effect of our framework, we globally
train millions of word level context features moti-
vated by word sense disambiguation (Chan et al.,
2007) together with the features used in traditional
SMT system (Section 6). Training on 519K sentence
pairs in 0.03 seconds per sentence, we achieve sig-
nificantly improvement over the traditional pipeline
by 0.84 BLEU.
</bodyText>
<sectionHeader confidence="0.974069" genericHeader="method">
2 Synchronous Context Free Grammar
</sectionHeader>
<bodyText confidence="0.9563455">
We work on synchronous context free grammar
(SCFG) (Chiang, 2007) based translation. The el-
ementary structures in an SCFG are rewrite rules of
the form:
</bodyText>
<equation confidence="0.95671">
X (-y, α)
</equation>
<bodyText confidence="0.999914904761905">
where -y and α are strings of terminals and nonter-
minals. We call -y and α as the source side and the
target side of rule respectively. Here a rule means a
phrase translation (Koehn et al., 2003) or a transla-
tion pair that contains nonterminals.
We call a sequence of translation steps as a
derivation. In context of SCFG, a derivation is a se-
quence of SCFG rules {rz}. Translation forest (Mi
et al., 2008; Li and Eisner, 2009) is a compact repre-
sentation of all the derivations for a given sentence
under an SCFG (see Figure 1). A tree t in the forest
corresponds to a derivation. In our paper, tree means
the same as derivation.
More formally, a forest is a pair (V, E), where V
is the set of nodes, E is the set of hyperedge. For
a given source sentence f = f&apos;1 , Each node v E V
is in the form XzJ, which denotes the recognition
of nonterminal X spanning the substring from the i
through j (that is fz+1...fj). Each hyperedge e E E
connects a set of antecedent to a single consequent
node and corresponds to an SCFG rule r(e).
</bodyText>
<sectionHeader confidence="0.987825" genericHeader="method">
3 Our Translation Forest
</sectionHeader>
<bodyText confidence="0.999602">
We use a translation forest that contains both “ref-
erence” derivations that potentially yield the refer-
ence translation and also some neighboring “non-
reference” derivations that fail to produce the ref-
erence translation. Therefore, our forest only repre-
sents some of the derivations for a sentence given an
SCFG rule table. The motivation of using such a for-
est is efficiency. However, since this space contains
both “good” and “bad” translations, it still provides
evidences for discriminative training.
First see the example in Figure 1. The derivation
tree t1 represented by solid hyperedges is a reference
derivation. We can construct a non-reference deriva-
tion by making small change to t1. By replacing the
e3 of t1 with e4, we obtain a non-reference deriva-
</bodyText>
<page confidence="0.970092">
881
</page>
<bodyText confidence="0.975841888888889">
tion tree t2. Considering the rules in each derivation, Algorithm 1 Forest Generation
the difference between t1 and t2 lies in r3 and r4. Al-
though r3 has a same source side with r4, it produces
a different translation. While r3 provides a swap-
ping translation, r4 generates a monotone transla-
tion. Thus, the derivation t2 fails to move the sub-
ject “police” to the behind of verb “shot dead”, re-
sulting a wrong translation “the gunman was police
shot dead”. Given such derivations, we hope that
the discriminative model is capable to explain why
should use a reordering rule in this context.
Generally, our forest contains all the reference
derivations RT for a sentence given a rule table, and
some neighboring non-reference derivations NT,
which can be defined from RT.
More formally, we call two hyperedges e1 and e2
are competing hyperedges, if their corresponding
rules r(e1) = (γ1, α1) and r(e2) = (γ2, α2) :
</bodyText>
<listItem confidence="0.972554058823529">
1: procedure GENERATE(t)
2: list +— t
3: for v E t in post order do
4: e +— incoming edge of v
5: append C(t, e) to list;
6: for u E child(v) from left to right do
7: t,,, +— OPERATE(t, u)
8: if t,,, =� t then
9: append t,,, to list
′ ′
10: for eE t,,, ∧ eE/ t do
11: append C(t,,,,e′) to list
12: if SCORE(t) &lt; SCORE(t,,,) then
13: t +— t,,,
14: return t,list
4 Fast Generation
γ1 = γ2 ∧ α1 =� α2 (1) It is still slow to calculate the entire forest defined
</listItem>
<bodyText confidence="0.982332903846154">
in Section 3, therefore we use a greedy decoding for
fast generating a subset of the forest. Starting form
a reference derivation, we try to slightly change the
derivation into a new reference derivation. During
this process, we collect the competing derivations
of reference derivations. We describe the details of
local operators for changing a derivation in section
4.1, and then introduce the creation of initial refer-
ence derivation with max score in Section 4.2.
For example, given derivation t1, we delete the
node X0,1 and the related hyperedge e1 and e5. Fix-
ing the other nodes and edges, we try to add a new
edge e2 to create a new reference translation. In this
case, if rule r2 really exists in our rule table, we get
a new reference derivation t3. After constructing t3,
we first collect the new tree and C(t3, e2). Then, we
will move to t3, if the score of t3 is higher than t2.
Notably, if r2 does not exist in the rule table, we fail
to create a new reference derivation. In such case,
we keep the origin derivation unchanged.
Algorithm 1 shows the process of generation.3
The input is a reference derivation t, and the out-
put is a new derivation and the generated derivations.
This means they give different translations for a
same source side. We use C(e) to represent the set
of competing hyperedges of e.
Two derivations t1 = (V 1, E1) and t2 =
(V 2, E2) are competing derivations if there exists
e1 E E1 and e2 E E2: 2
V 1 = V 2 ∧ E1 − e1 = E2 − e2
∧ e2 E C(e1) (2)
In other words, derivations t1 and t2 only differ in
e1 and e2, and these two hyperedges are competing
hyperedges. We use C(t) to represent the set of com-
peting derivations of tree t, and C(t,e) to represent
the set of competing derivations of t if the competi-
tion occurs in hyperedge e in t.
Given a rule table, the set of reference derivations
RT for a sentence is determined. Then, the set of
non-reference derivations NT can be defined from
RT:
Ut∈RT C(t) (3)
Overall, our forest is the compact representation of
RT and NT .
3For simplicity, we list all the trees, and do not compress
them into a forest in practice. It is straight to extent the algo-
rithm to get a compact forest for those generated derivations.
Actually, instead of storing the derivations, we call the generate
function twice to calculate gradient of log-linear model.
2The definition of derivation tree is similar to forest, except
that the tree contains exactly one tree while forest contains ex-
ponentially trees. In tree, the hyperedge degrades to edge.
</bodyText>
<figure confidence="0.697613">
882
0,4 0,4 0,4
0,1 2,4 2,4 0,2 2,4
</figure>
<figureCaption confidence="0.992076333333333">
Figure 2: Lexicalize and generalize operators over t1 (part) in Figure 1. Although here only shows the nodes, we also
need to change relative edges actually. (1) Applying lexicalize operator on the non-terminal node X0,1 in (a) results a
new derivation shown in (b). (2) When visiting bei in (b), the generalize operator changes the derivation into (c).
</figureCaption>
<bodyText confidence="0.999980681818182">
The list used for storing forest is initialized with the
input tree (line 2). We visit the nodes in t in post-
order (line 3). For each node v, we first append the
competing derivations Qt,e) to list, where e is in-
coming edge of v (lines 4-5). Then, we apply oper-
ators on the child nodes of v from left to right (lines
6-13). The operators returns a reference derivation
tn (line 7). If it is new (line 8), we collect both the tn
(line 9), and also the competing derivations Qtn, e′)
of the new derivation on those edges e′ which only
occur in the new derivation (lines 10-11). Finally, if
the new derivation has a larger score, we will replace
the origin derivation with new one (lines 12-13).
Although there is a two-level loop for visiting
nodes (line 3 and 6), each node is visited only one
time in the inner loops. Thus, the complexity is
linear with the number of nodes #node. Consid-
ering that the number of source word (also leaf node
here) is less than the total number of nodes and is
more than F(#node+1)/2⌉, the time complexity of
the process is also linear with the number of source
word.
</bodyText>
<subsectionHeader confidence="0.998351">
4.1 Lexicalize and Generalize
</subsectionHeader>
<bodyText confidence="0.99996558974359">
The function OPERATE in Algorithm 1 uses two op-
erators to change a node: lexicalize and generalize.
Figure 2 shows the effects of the two operators. The
lexicalize operator works on nonterminal nodes. It
moves away a nonterminal node and attaches the
children of current node to its parent. In Figure 2(b),
the node X0,1 is deleted, requiring a more lexical-
ized rule to be applied to the parent node X0,4 (one
more terminal in the source side). We constrain the
lexicalize operator to apply on pre-terminal nodes
whose children are all terminal nodes. In contrast,
the generalize operator works on terminal nodes and
inserts a nonterminal node between current node and
its parent node. This operator generalizes over the
continuous terminal sibling nodes left to the current
node (including the current node). Generalizing the
node bei in Figure 2(b) results Figure 2(c). A new
node X0,2 is inserted as the parent of node qiang-
shou and node bei.
Notably, there are two steps when apply an oper-
ator. Suppose we want to lexicalize the node X0,1
in t1 of Figure 1, we first delete the node X0,1 and
related edge e1 and e5, then we try to add the new
edge e2. Since rule table is fixed, the second step
is a process of decoding. Therefore, sometimes we
may fail to create a new reference derivation (like
r2 may not exist in the rule table). In such case, we
keep the origin derivation unchanged.
The changes made by the two operators are local.
Considering the change of rules, the lexicalize oper-
ator deletes two rules and adds one new rule, while
the generalize operator deletes one rule and adds two
new rules. Such local changes provide us with a way
to incrementally calculate the scores of new deriva-
tions. We use this method motivated by Gibbs Sam-
pler (Blunsom et al., 2009) which has been used for
efficiently learning rules. The different lies in that
we use the operator for decoding where the rule ta-
ble is fixing.
</bodyText>
<subsectionHeader confidence="0.98745">
4.2 Initialize a Reference Derivation
</subsectionHeader>
<bodyText confidence="0.996712142857143">
The generation starts from an initial reference
derivation with max score. This requires bi-parsing
(Dyer, 2010) over the source sentence f and the ref-
erence translation e. In practice, we may face three
problems.
First is efficiency problem. Exhaustive search
over the space under SCFG requires O(|f|3|e|3).
</bodyText>
<page confidence="0.994561">
883
</page>
<bodyText confidence="0.999888885714286">
To parse quickly, we only visit the tight consistent
(Zhang et al., 2008) bi-spans with the help of word
alignment a. Only visiting tight consistent spans
greatly speeds up bi-parsing. Besides efficiency,
adoption of this constraint receives support from the
fact that heuristic SCFG rule extraction only extracts
tight consistent initial phrases (Chiang, 2007).
Second is degenerate problem. If we only use
the features as traditional SCFG systems, the bi-
parsing may end with a derivation consists of some
giant rules or rules with rare source/target sides,
which is called degenerate solution (DeNero et al.,
2006). That is because the translation rules with rare
source/target sides always receive a very high trans-
lation probability. We add a prior score log(#rule)
for each rule, where #rule is the number of occur-
rence of a rule, to reward frequent reusable rules and
derivations with more rules.
Finally, we may fail to create reference deriva-
tions due to the limitation in rule extraction. We
create minimum trees for (f, e, a) using shift-reduce
(Zhang et al., 2008). Some minimum rules in the
trees may be illegal according to the definition of
Chiang (2007). We also add these rules to the rule
table, so as to make sure every sentence is reachable
given the rule table. A source sentence is reachable
given a rule table if reference derivations exists. We
refer these rules as added rules. However, this may
introduce rules with more than two variables and in-
crease the complexity of bi-parsing. To tackle this
problem, we initialize the chart with minimum par-
allel tree from the Zhang et al. (2008) algorithm,
ensuring that the bi-parsing has at least one path to
create a reference derivation. Then we only need to
consider the traditional rules during bi-parsing.
</bodyText>
<sectionHeader confidence="0.995877" genericHeader="method">
5 Training
</sectionHeader>
<bodyText confidence="0.99939425">
We use the forest to train a log-linear model with a
latent variable as describe in Blunsom et al.(2008).
The probability p(e|f) is the sum over all possible
derivations:
</bodyText>
<equation confidence="0.9945435">
p(e|f) = � p(t, e|f) (4)
tEA(e,f)
</equation>
<bodyText confidence="0.9981675">
where △(e, f) is the set of all possible derivations
that translate f into e and t is one such derivation.4
</bodyText>
<footnote confidence="0.4866395">
4Although the derivation is typically represent as d, we de-
notes it by t since our paper use tree to represent derivation.
</footnote>
<construct confidence="0.233978">
Algorithm 2 Training
</construct>
<listItem confidence="0.821849625">
1: procedure TRAIN(S)
2: Training Data S = {fn, en, an}Nn=1
3: Derivations T = {}N
n=1
4: for n = 1 to N do
5: tn ← INITIAL(fn, en, an)
6: i ← 0
7: for m = 0 to M do
8: for n = 0 to N do
9: 17 ← LEARNRATE(i)
10: (∆L(wi, tn), tn) ←GENERATE(tn)
11: wi ← wi + 17 × ∆L(wi, tn)
12: i ← i + 1
MN i
13: return ∑i=1 w
MN
</listItem>
<bodyText confidence="0.996705">
This model defines the conditional probability of
a derivation t and the corresponding translation e
given a source sentence f as:
</bodyText>
<equation confidence="0.855799">
p(t, e|f) = exp Ei A(hi(t, e, f) (5)
</equation>
<bodyText confidence="0.980373">
where the partition function is
</bodyText>
<equation confidence="0.9936635">
Z(f) = � � �exp Aihi(t,e,f) (6)
e tEA(e,f) i
</equation>
<bodyText confidence="0.990366090909091">
The partition function is approximated by our for-
est, which is labeled as ˜Z(f), and the derivations
that produce reference translation is approximated
by reference derivations in ˜Z(f).
We estimate the parameters in log-linear model
using maximum a posteriori (MAP) estimator. It
maximizes the likelihood of the bilingual corpus
S = {fn, en}Nn=1, penalized using a gaussian prior
(L2 norm) with the probability density function
p0(Ai) ∝ exp(−A2i/2Q2). We set Q2 to 1.0 in our
experiments. This results in the following gradient:
</bodyText>
<equation confidence="0.6912195">
Ai
= Ep(t|e,f)[hi] − Ep(e|f)[hi] − �2 (7)
</equation>
<bodyText confidence="0.9250236">
We use an online learning algorithm to train the
parameters. We implement stochastic gradient de-
scent (SGD) recommended by Bottou.5 The dy-
namic learning rate we use is N
(i+0), where N is the
</bodyText>
<footnote confidence="0.345193333333333">
5http://leon.bottou.org/projects/sgd
aL
aAi
</footnote>
<page confidence="0.993421">
884
</page>
<bodyText confidence="0.9999345">
number of training example, i is the training itera-
tion, and i0 is a constant number used to get a initial
learning rate, which is determined by calibration.
Algorithm 2 shows the entire process. We first
create an initial reference derivation for every train-
ing examples using bi-parsing (lines 4-5), and then
online learn the parameters using SGD (lines 6-12).
We use the GENERATE function to calculate the gra-
dient. In practice, instead of storing all the deriva-
tions in a list, we traverse the tree twice. The first
time is calculating the partition function, and the
second time calculates the gradient normalized by
partition function. During training, we also change
the derivations (line 10). When training is finished
after M epochs, the algorithm returns an averaged
weight vector (Collins, 2002) to avoid overfitting
(line 13). We use a development set to select total
epoch m, which is set as M = 5 in our experiments.
</bodyText>
<sectionHeader confidence="0.999352" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.99998637037037">
Our method is able to train a large number of fea-
tures on large data. We use a set of word context
features motivated by word sense disambiguation
(Chan et al., 2007) to test scalability. A word level
context feature is a triple (f, e, f+1), which counts
the number of time that f is aligned to e and f+1 oc-
curs to the right of f. Triple (f, e, f−1) is similar ex-
cept that f−1 locates to the left of f. We retain word
alignment information in the extracted rules to ex-
ploit such features. To demonstrate the importance
of scaling up the size of training data and the effect
of our method, we compare three types of training
configurations which differ in the size of features
and data.
MERT. We use MERT (Och, 2003) to training 8
features on a small data. The 8 features is the same
as Chiang (2007) including 4 rule scores (direct and
reverse translation scores; direct and reverse lexi-
cal translation scores); 1 target side language model
score; 3 penalties for word counts, extracted rules
and glue rule. Actually, traditional pipeline often
uses such configuration.
Perceptron. We also learn thousands of context
word features together with the 8 traditional features
on a small data using perceptron. Following (Chiang
et al., 2009), we only use 100 most frequent words
for word context feature. This setting use CKY de-
</bodyText>
<table confidence="0.9926544">
TRAIN RTRAIN DEV TEST
#Sent. 519,359 186,810 878 3,789
#Word 8.6M 1.3M 23K 105K
Avg. Len. 16.5 7.3 26.4 28.0
Lon. Len. 99 95 77 116
</table>
<tableCaption confidence="0.998883">
Table 1: Corpus statistics of Chinese side, where Sent.,
Avg., Lon., and Len. are short for sentence, longest,
average, and length respectively. RTRAIN denotes the
reachable (given rule table without added rules) subset of
TRAIN data.
</tableCaption>
<bodyText confidence="0.998427523809524">
coder to generate n-best lists for training. The com-
plexity of CKY decoding limits the training data into
a small size. We fix the 8 traditional feature weights
as MERT to get a comparable results as MERT.
Our Method. Finally, we use our method to train
millions of features on large data. The use of large
data promises us to use full vocabulary of training
data for the context word features, which results mil-
lions of fully lexicalized context features. During
decoding, when a context feature does not exit, we
simply ignore it. The weights of 8 traditional fea-
tures are fixed the same as MERT also. We fix these
weights because the translation feature weights fluc-
tuate intensely during online learning. The main rea-
son may come from the degeneration solution men-
tioned in Section 4.2, where rare rules with very high
translation probability are selected as the reference
derivations. Another reason could be the fact that
translation features are dense intensify the fluctua-
tion. We leave learning without fixing the 8 feature
weights to future work.
</bodyText>
<subsectionHeader confidence="0.996781">
6.1 Data
</subsectionHeader>
<bodyText confidence="0.999892076923077">
We focus on the Chinese-to-English translation task
in this paper. The bilingual corpus we use con-
tains 519, 359 sentence pairs, with an average length
of 16.5 in source side and 20.3 in target side,
where 186,810 sentence pairs (36%) are reach-
able (without added rules in Section 4.2). The
monolingual data includes the Xinhua portion of
the GIGAWORD corpus, which contains 238M En-
glish words. We use the NIST evaluation sets of
2002 (MT02) as our development set, and sets of
MT03/MT04/MT05 as test sets. Table 2 shows the
statistics of all bilingual corpus.
We use GIZA++ (Och and Ney, 2003) to perform
</bodyText>
<page confidence="0.996142">
885
</page>
<table confidence="0.999907166666667">
System #DATA #FEAT MT03 MT04 MT05 ALL
MERT 878 8 33.03 35.12 32.32 33.85
Perceptron 878 2.4K 32.89 34.88 32.55 33.76
Our Method 187K 2.0M 33.64 35.48 32.91* 34.41*
519K 13.9M 34.19* 35.72* 33.09* 34.69*
Improvement over MERT +1.16 +0.60 +0.77 +0.84
</table>
<tableCaption confidence="0.994308">
Table 2: Effect of our method comparing with MERT and perceptron in terms of BLEU. We also compare our fast
generation method with different data (only reachable or full data). #Data is the size of data for training the feature
weights. * means significantly (Koehn, 2004) better than MERT (p &lt; 0.01).
</tableCaption>
<bodyText confidence="0.999581488372093">
word alignment in both directions, and grow-diag-
final-and (Koehn et al., 2003) to generate symmet-
ric word alignment. We extract SCFG rules as de-
scribed in Chiang (2007) and also added rules (Sec-
tion 4.2). Our algorithm runs on the entire training
data, which requires to load all the rules into the
memory. To fit within memory, we cut off those
composed rules which only happen once in the train-
ing data. Here a composed rule is a rule that can be
produced by any other extracted rules. A 4-grams
language model is trained by the SRILM toolkit
(Stolcke, 2002). Case-insensitive NIST BLEU4 (Pa-
pineni et al., 2002) is used to measure translation
performance.
The training data comes from a subset of the
LDC data including LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06. Since the rule ta-
ble of the entire data is too large to be loaded to
the memory (even drop one-count rules), we remove
many sentence pairs to create a much smaller data
yet having a comparable performance with the entire
data. The intuition lies in that if most of the source
words of a sentence need to be translated by the
added rules, then the word alignment may be highly
crossed and the sentence may be useless. We cre-
ate minimum rules from a sentence pair, and count
the number of source words in those minimum rules
that are added rules. For example, suppose the result
minimum rules of a sentence contain r3 which is an
added rule, then we count 1 time for the sentence. If
the number of such source word is more than 10%
of the total number, we will drop the sentence pair.
We compare the performances of MERT setting
on three bilingual data: the entire data that contains
42.3M Chinese and 48.2M English words; 519K
data that contains 8.6M Chinese and 10.6M En-
glish words; FBIS (LDC2003E14) parts that con-
tains 6.9M Chinese and 9.1M English words. They
produce 33.11/32.32/30.47 BLEU tested on MT05
respectively. The performance of 519K data is com-
parable with that of entire data, and much higher
than that of FBIS data.
</bodyText>
<subsectionHeader confidence="0.997374">
6.2 Result
</subsectionHeader>
<bodyText confidence="0.999982925925926">
Table 3 shows the performance of the three different
training configurations. The training of MERT and
perceptron run on MT02. For our method, we com-
pare two different training sets: one is trained on
all 519K sentence pairs, the other only uses 186K
reachable sentences.
Although the perceptron system exploits 2.4K
features, it fails to produce stable improvements
over MERT. The reason may come from overfitting,
since the training data for perceptron contains only
878 sentences. However, when use our method to
learn the word context feature on the 519K data,
we significantly improve the performance by 0.84
points on the entire test sets (ALL). The improve-
ments range from 0.60 to 1.16 points on MT03-
05. Because we use the full vocabulary, the num-
ber of features increased into 13.9 millions, which is
impractical to be trained on the small development
set. These results confirm the necessity of exploiting
more features and learning the parameters on large
data. Meanwhile, such results also demonstrate that
we can benefits from the forest generated by our fast
method instead of traditional CKY algorithm.
Not surprisingly, the improvements are smaller
when only use 186K reachable sentences. Some-
times we even fail to gain significant improvement.
This verifies our motivation to guarantee all sentence
</bodyText>
<page confidence="0.993103">
886
</page>
<figure confidence="0.995089444444444">
180
150
120
90
60
30
0
0 10 20 30 40 50 60 70 80 90
Sentence Length
</figure>
<figureCaption confidence="0.99741425">
Figure 3: Plot of training times (including forest genera-
tion and SGD training) versus sentence length. We ran-
domly select 1000 sentence from the 519K data for plot-
ting.
</figureCaption>
<bodyText confidence="0.920567">
are reachable, so as to use all training data.
</bodyText>
<subsectionHeader confidence="0.993075">
6.3 Speed
</subsectionHeader>
<bodyText confidence="0.997400827586207">
How about the speed of our framework? Our method
learns in 32 mlliseconds/sentence. Figure 3 shows
training times (including forest generation and SGD
training) versus sentence length. The plot confirms
that our training algorithm scales linearly. If we
use n-best lists which generated by CKY decoder
as MERT, it takes about 3105 milliseconds/sentence
for producing 100-best lists. Our method accelerates
the speed about 97 times (even though we search
twice to calculate the gradient). This shows the effi-
ciency of our method.
The procedure of training includes two steps. (1)
Bi-parsing to initialize a reference derivation with
max score. (2) Training procedure which generates
a set of derivations to calculate the gradient and up-
date parameters. Step (1) only runs once. The av-
erage time of processing a sentence for each step
is about 9.5 milliseconds and 30.2 milliseconds re-
spectively.
For simplicity we do not compress the generated
derivations into forests, therefore the size of result-
ing derivations is fairly small, which is about 265.8
for each sentence on average, where 6.1 of them are
reference derivations. Furthermore, we use lexical-
ize operator more often than generalize operator (the
ration between them is 1.5 to 1). Lexicalize operator
is used more frequently mainly dues to that the ref-
erence derivations are initialized with reusable (thus
small) rules.
</bodyText>
<sectionHeader confidence="0.99974" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999778857142857">
Minimum error rate training (Och, 2003) is perhaps
the most popular discriminative training for SMT.
However, it fails to scale to large number of features.
Researchers have propose many learning algorithms
to train many features: perceptron (Shen et al., 2004;
Liang et al., 2006), minimum risk (Smith and Eisner,
2006; Li et al., 2009), MIRA (Watanabe et al., 2007;
Chiang et al., 2009), gradient descent (Blunsom et
al., 2008; Blunsom and Osborne, 2008). The com-
plexity of n-best lists or packed forests generation
hamper these algorithms to scale to a large amount
of data.
For efficiency, we only use neighboring deriva-
tions for training. Such motivation is same as con-
trastive estimation (Smith and Eisner, 2005; Poon et
al., 2009). The difference lies in that the previous
work actually care about their latent variables (pos
tags, segmentation, dependency trees, etc), while
we are only interested in their marginal distribution.
Furthermore, we focus on how to fast generate trans-
lation forest for training.
The local operators lexicalize/generalize are use
for greedy decoding. The idea is related to “peg-
ging” algorithm (Brown et al., 1993) and greedy de-
coding (Germann et al., 2001). Such types of local
operators are also used in Gibbs sampler for syn-
chronous grammar induction (Blunsom et al., 2009;
Cohn and Blunsom, 2009).
</bodyText>
<sectionHeader confidence="0.986981" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.954121666666667">
We have presented a fast generation algorithm for
translation forest which contains both reference
derivations and neighboring non-reference deriva-
tions for large-scale SMT discriminative training.
We have achieved significantly improvement of 0.84
BLEU by incorporate 13.9M feature trained on 519K
data in 0.03 second per sentence.
In this paper, we define the forest based on com-
peting derivations which only differ in one rule.
There may be better classes of forest that can pro-
duce a better performance. It’s interesting to modify
the definition of forest, and use more local operators
to increase the size of forest. Furthermore, since the
generation of forests is quite general, it’s straight to
Training Time(Milliseconds)
</bodyText>
<page confidence="0.968677">
887
</page>
<bodyText confidence="0.999262">
apply our forest on other learning algorithms. Fi-
nally, we hope to exploit more features such as re-
ordering features and syntactic features so as to fur-
ther improve the performance.
</bodyText>
<sectionHeader confidence="0.929854" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9998458">
We would like to thank Yifan He, Xianhua Li, Daqi
Zheng, and the anonymous reviewers for their in-
sightful comments. The authors were supported by
National Natural Science Foundation of China Con-
tracts 60736014, 60873167, and 60903138.
</bodyText>
<sectionHeader confidence="0.999198" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999681945054946">
Phil Blunsom and Miles Osborne. 2008. Probabilistic
inference for machine translation. In Proc. of EMNLP
2008.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. of ACL-08.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proc. of ACL 2009.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, and Robert. L. Mercer. 1993. The mathemat-
ics of statistical machine translation. Computational
Linguistics, 19:263–311.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proc. of ACL 2007, pages 33–40.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proc. of NAACL 2009.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian model
of syntax-directed tree to string grammar induction. In
Proc. of EMNLP 2009.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proc. of EMNLP 2002.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In Proc. of the HLT-NAACL 2006
Workshop on SMT.
Chris Dyer. 2010. Two monolingual parses are better
than one (synchronous parse). In Proc. of NAACL
2010.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proc. of
ACL 2001.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL 2003.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP
2004.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proc. of EMNLP
2009.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proc. of ACL 2009.
Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proc. of ACL 2006.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL 2008.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. of ACL 2002.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ofACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. of ACL 2002.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation with
log-linear models. In Proc. of NAACL 2009.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
Proc. of NAACL 2004.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proc. of ACL 2005.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proc. of
COLING/ACL 2006.
Andreas Stolcke. 2002. Srilm – an extensible language
modeling toolkit. In Proc. of ICSLP 2002.
Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical mt. In
Proc. of ACL 2006.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In Proc. of EMNLP-CoNLL
2007.
Hao Zhang, Daniel Gildea, and David Chiang. 2008. Ex-
tracting synchronous grammar rules from word-level
alignments in linear time. In Proc. of Coling 2008.
</reference>
<page confidence="0.997644">
888
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.569761">
<title confidence="0.9984365">Fast Generation of Translation for Large-Scale SMT Discriminative Training</title>
<author confidence="0.984806">Yang Liu Xiao</author>
<author confidence="0.984806">Qun Liu</author>
<affiliation confidence="0.9466265">Key Laboratory of Intelligent Information Institute of Computing</affiliation>
<address confidence="0.612008">Chinese Academy of</address>
<abstract confidence="0.998928235294118">Although discriminative training guarantees to improve statistical machine translation by incorporating a large amount of overlapping features, it is hard to scale up to large data due to decoding complexity. We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment. Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation. With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves sigimprovement by 0.84 the baseline system on the NIST Chinese-English test sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Miles Osborne</author>
</authors>
<title>Probabilistic inference for machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP</booktitle>
<contexts>
<context position="28927" citStr="Blunsom and Osborne, 2008" startWordPosition="4969" endWordPosition="4972"> is 1.5 to 1). Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus small) rules. 7 Related Work Minimum error rate training (Och, 2003) is perhaps the most popular discriminative training for SMT. However, it fails to scale to large number of features. Researchers have propose many learning algorithms to train many features: perceptron (Shen et al., 2004; Liang et al., 2006), minimum risk (Smith and Eisner, 2006; Li et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008). The complexity of n-best lists or packed forests generation hamper these algorithms to scale to a large amount of data. For efficiency, we only use neighboring derivations for training. Such motivation is same as contrastive estimation (Smith and Eisner, 2005; Poon et al., 2009). The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marginal distribution. Furthermore, we focus on how to fast generate translation forest for training. The local operators lexicalize/generali</context>
</contexts>
<marker>Blunsom, Osborne, 2008</marker>
<rawString>Phil Blunsom and Miles Osborne. 2008. Probabilistic inference for machine translation. In Proc. of EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-08.</booktitle>
<contexts>
<context position="1391" citStr="Blunsom et al., 2008" startWordPosition="202" endWordPosition="205">e translation. With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets. 1 Introduction Discriminative model (Och and Ney, 2002) can easily incorporate non-independent and overlapping features, and has been dominating the research field of statistical machine translation (SMT) in the last decade. Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). However, the training of the large number of features was always restricted in fairly small data sets. Some systems limit the number of training examples, while others use short sentences to maintain efficiency. Overfitting problem often comes when training many features on a small data (Watanabe et al., 2007; Chiang et al., 2009). Obviously, using much more data can alleviate such problem. Furthermore, large data also enables us to globally train millions of sparse lexical features which offer accurate clues for SMT. Despite these advantages, to the best of our knowled</context>
<context position="28899" citStr="Blunsom et al., 2008" startWordPosition="4965" endWordPosition="4968">he ration between them is 1.5 to 1). Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus small) rules. 7 Related Work Minimum error rate training (Och, 2003) is perhaps the most popular discriminative training for SMT. However, it fails to scale to large number of features. Researchers have propose many learning algorithms to train many features: perceptron (Shen et al., 2004; Liang et al., 2006), minimum risk (Smith and Eisner, 2006; Li et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008). The complexity of n-best lists or packed forests generation hamper these algorithms to scale to a large amount of data. For efficiency, we only use neighboring derivations for training. Such motivation is same as contrastive estimation (Smith and Eisner, 2005; Poon et al., 2009). The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marginal distribution. Furthermore, we focus on how to fast generate translation forest for training. The local o</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proc. of ACL-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Chris Dyer</author>
<author>Miles Osborne</author>
</authors>
<title>A gibbs sampler for phrasal synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="14423" citStr="Blunsom et al., 2009" startWordPosition="2499" endWordPosition="2502">ince rule table is fixed, the second step is a process of decoding. Therefore, sometimes we may fail to create a new reference derivation (like r2 may not exist in the rule table). In such case, we keep the origin derivation unchanged. The changes made by the two operators are local. Considering the change of rules, the lexicalize operator deletes two rules and adds one new rule, while the generalize operator deletes one rule and adds two new rules. Such local changes provide us with a way to incrementally calculate the scores of new derivations. We use this method motivated by Gibbs Sampler (Blunsom et al., 2009) which has been used for efficiently learning rules. The different lies in that we use the operator for decoding where the rule table is fixing. 4.2 Initialize a Reference Derivation The generation starts from an initial reference derivation with max score. This requires bi-parsing (Dyer, 2010) over the source sentence f and the reference translation e. In practice, we may face three problems. First is efficiency problem. Exhaustive search over the space under SCFG requires O(|f|3|e|3). 883 To parse quickly, we only visit the tight consistent (Zhang et al., 2008) bi-spans with the help of word</context>
<context position="29783" citStr="Blunsom et al., 2009" startWordPosition="5107" endWordPosition="5110">n (Smith and Eisner, 2005; Poon et al., 2009). The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marginal distribution. Furthermore, we focus on how to fast generate translation forest for training. The local operators lexicalize/generalize are use for greedy decoding. The idea is related to “pegging” algorithm (Brown et al., 1993) and greedy decoding (Germann et al., 2001). Such types of local operators are also used in Gibbs sampler for synchronous grammar induction (Blunsom et al., 2009; Cohn and Blunsom, 2009). 8 Conclusion and Future Work We have presented a fast generation algorithm for translation forest which contains both reference derivations and neighboring non-reference derivations for large-scale SMT discriminative training. We have achieved significantly improvement of 0.84 BLEU by incorporate 13.9M feature trained on 519K data in 0.03 second per sentence. In this paper, we define the forest based on competing derivations which only differ in one rule. There may be better classes of forest that can produce a better performance. It’s interesting to modify the defin</context>
</contexts>
<marker>Blunsom, Cohn, Dyer, Osborne, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Osborne. 2009. A gibbs sampler for phrasal synchronous grammar induction. In Proc. of ACL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation. Computational Linguistics,</title>
<date>1993</date>
<marker>Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della Pietra, and Robert. L. Mercer. 1993. The mathematics of statistical machine translation. Computational Linguistics, 19:263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>David Chiang</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>33--40</pages>
<contexts>
<context position="5001" citStr="Chan et al., 2007" startWordPosition="796" endWordPosition="799">this is done by deleting a node X0,1. ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is non-trivial for SMT and needs to be determined experimentally. Given such forests, we globally learn a log-linear model using stochastic gradient descend (Section 5). Overall, both the generation of forests and the training algorithm are scalable, enabling us to train millions of features on large-scale data. To show the effect of our framework, we globally train millions of word level context features motivated by word sense disambiguation (Chan et al., 2007) together with the features used in traditional SMT system (Section 6). Training on 519K sentence pairs in 0.03 seconds per sentence, we achieve significantly improvement over the traditional pipeline by 0.84 BLEU. 2 Synchronous Context Free Grammar We work on synchronous context free grammar (SCFG) (Chiang, 2007) based translation. The elementary structures in an SCFG are rewrite rules of the form: X (-y, α) where -y and α are strings of terminals and nonterminals. We call -y and α as the source side and the target side of rule respectively. Here a rule means a phrase translation (Koehn et al</context>
<context position="19647" citStr="Chan et al., 2007" startWordPosition="3401" endWordPosition="3404"> the tree twice. The first time is calculating the partition function, and the second time calculates the gradient normalized by partition function. During training, we also change the derivations (line 10). When training is finished after M epochs, the algorithm returns an averaged weight vector (Collins, 2002) to avoid overfitting (line 13). We use a development set to select total epoch m, which is set as M = 5 in our experiments. 6 Experiments Our method is able to train a large number of features on large data. We use a set of word context features motivated by word sense disambiguation (Chan et al., 2007) to test scalability. A word level context feature is a triple (f, e, f+1), which counts the number of time that f is aligned to e and f+1 occurs to the right of f. Triple (f, e, f−1) is similar except that f−1 locates to the left of f. We retain word alignment information in the extracted rules to exploit such features. To demonstrate the importance of scaling up the size of training data and the effect of our method, we compare three types of training configurations which differ in the size of features and data. MERT. We use MERT (Och, 2003) to training 8 features on a small data. The 8 feat</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007. Word sense disambiguation improves statistical machine translation. In Proc. of ACL 2007, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL</booktitle>
<contexts>
<context position="1413" citStr="Chiang et al., 2009" startWordPosition="206" endWordPosition="209">llions of features trained on 519K sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets. 1 Introduction Discriminative model (Och and Ney, 2002) can easily incorporate non-independent and overlapping features, and has been dominating the research field of statistical machine translation (SMT) in the last decade. Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). However, the training of the large number of features was always restricted in fairly small data sets. Some systems limit the number of training examples, while others use short sentences to maintain efficiency. Overfitting problem often comes when training many features on a small data (Watanabe et al., 2007; Chiang et al., 2009). Obviously, using much more data can alleviate such problem. Furthermore, large data also enables us to globally train millions of sparse lexical features which offer accurate clues for SMT. Despite these advantages, to the best of our knowledge, no previous discri</context>
<context position="4526" citStr="Chiang et al., 2009" startWordPosition="718" endWordPosition="721">5 Figure 1: A translation forest which is the running example throughout this paper. The reference translation is “the gunman was killed by the police”. (1) Solid hyperedges denote a “reference” derivation tree t1 which exactly yields the reference translation. (2) Replacing e3 in t1 with e4 results a competing non-reference derivation t2, which fails to swap the order of X3,4. (3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3. Generally, this is done by deleting a node X0,1. ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is non-trivial for SMT and needs to be determined experimentally. Given such forests, we globally learn a log-linear model using stochastic gradient descend (Section 5). Overall, both the generation of forests and the training algorithm are scalable, enabling us to train millions of features on large-scale data. To show the effect of our framework, we globally train millions of word level context features motivated by word sense disambiguation (Chan et al., 2007) together with the features used in traditional SMT system (Section 6). Training on 519K sentence pairs in 0.03 seconds per s</context>
<context position="20718" citStr="Chiang et al., 2009" startWordPosition="3588" endWordPosition="3591">aining configurations which differ in the size of features and data. MERT. We use MERT (Och, 2003) to training 8 features on a small data. The 8 features is the same as Chiang (2007) including 4 rule scores (direct and reverse translation scores; direct and reverse lexical translation scores); 1 target side language model score; 3 penalties for word counts, extracted rules and glue rule. Actually, traditional pipeline often uses such configuration. Perceptron. We also learn thousands of context word features together with the 8 traditional features on a small data using perceptron. Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature. This setting use CKY deTRAIN RTRAIN DEV TEST #Sent. 519,359 186,810 878 3,789 #Word 8.6M 1.3M 23K 105K Avg. Len. 16.5 7.3 26.4 28.0 Lon. Len. 99 95 77 116 Table 1: Corpus statistics of Chinese side, where Sent., Avg., Lon., and Len. are short for sentence, longest, average, and length respectively. RTRAIN denotes the reachable (given rule table without added rules) subset of TRAIN data. coder to generate n-best lists for training. The complexity of CKY decoding limits the training data into a small size. We fix the 8 traditional f</context>
<context position="28859" citStr="Chiang et al., 2009" startWordPosition="4959" endWordPosition="4962">r more often than generalize operator (the ration between them is 1.5 to 1). Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus small) rules. 7 Related Work Minimum error rate training (Och, 2003) is perhaps the most popular discriminative training for SMT. However, it fails to scale to large number of features. Researchers have propose many learning algorithms to train many features: perceptron (Shen et al., 2004; Liang et al., 2006), minimum risk (Smith and Eisner, 2006; Li et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008). The complexity of n-best lists or packed forests generation hamper these algorithms to scale to a large amount of data. For efficiency, we only use neighboring derivations for training. Such motivation is same as contrastive estimation (Smith and Eisner, 2005; Poon et al., 2009). The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marginal distribution. Furthermore, we focus on how to fast generate tran</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In Proc. of NAACL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="5316" citStr="Chiang, 2007" startWordPosition="846" endWordPosition="847"> Overall, both the generation of forests and the training algorithm are scalable, enabling us to train millions of features on large-scale data. To show the effect of our framework, we globally train millions of word level context features motivated by word sense disambiguation (Chan et al., 2007) together with the features used in traditional SMT system (Section 6). Training on 519K sentence pairs in 0.03 seconds per sentence, we achieve significantly improvement over the traditional pipeline by 0.84 BLEU. 2 Synchronous Context Free Grammar We work on synchronous context free grammar (SCFG) (Chiang, 2007) based translation. The elementary structures in an SCFG are rewrite rules of the form: X (-y, α) where -y and α are strings of terminals and nonterminals. We call -y and α as the source side and the target side of rule respectively. Here a rule means a phrase translation (Koehn et al., 2003) or a translation pair that contains nonterminals. We call a sequence of translation steps as a derivation. In context of SCFG, a derivation is a sequence of SCFG rules {rz}. Translation forest (Mi et al., 2008; Li and Eisner, 2009) is a compact representation of all the derivations for a given sentence un</context>
<context position="15280" citStr="Chiang, 2007" startWordPosition="2634" endWordPosition="2635">th max score. This requires bi-parsing (Dyer, 2010) over the source sentence f and the reference translation e. In practice, we may face three problems. First is efficiency problem. Exhaustive search over the space under SCFG requires O(|f|3|e|3). 883 To parse quickly, we only visit the tight consistent (Zhang et al., 2008) bi-spans with the help of word alignment a. Only visiting tight consistent spans greatly speeds up bi-parsing. Besides efficiency, adoption of this constraint receives support from the fact that heuristic SCFG rule extraction only extracts tight consistent initial phrases (Chiang, 2007). Second is degenerate problem. If we only use the features as traditional SCFG systems, the biparsing may end with a derivation consists of some giant rules or rules with rare source/target sides, which is called degenerate solution (DeNero et al., 2006). That is because the translation rules with rare source/target sides always receive a very high translation probability. We add a prior score log(#rule) for each rule, where #rule is the number of occurrence of a rule, to reward frequent reusable rules and derivations with more rules. Finally, we may fail to create reference derivations due t</context>
<context position="20280" citStr="Chiang (2007)" startWordPosition="3525" endWordPosition="3526">. A word level context feature is a triple (f, e, f+1), which counts the number of time that f is aligned to e and f+1 occurs to the right of f. Triple (f, e, f−1) is similar except that f−1 locates to the left of f. We retain word alignment information in the extracted rules to exploit such features. To demonstrate the importance of scaling up the size of training data and the effect of our method, we compare three types of training configurations which differ in the size of features and data. MERT. We use MERT (Och, 2003) to training 8 features on a small data. The 8 features is the same as Chiang (2007) including 4 rule scores (direct and reverse translation scores; direct and reverse lexical translation scores); 1 target side language model score; 3 penalties for word counts, extracted rules and glue rule. Actually, traditional pipeline often uses such configuration. Perceptron. We also learn thousands of context word features together with the 8 traditional features on a small data using perceptron. Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature. This setting use CKY deTRAIN RTRAIN DEV TEST #Sent. 519,359 186,810 878 3,789 #Word 8.6M 1.3M 23K </context>
<context position="23567" citStr="Chiang (2007)" startWordPosition="4075" endWordPosition="4076">2.55 33.76 Our Method 187K 2.0M 33.64 35.48 32.91* 34.41* 519K 13.9M 34.19* 35.72* 33.09* 34.69* Improvement over MERT +1.16 +0.60 +0.77 +0.84 Table 2: Effect of our method comparing with MERT and perceptron in terms of BLEU. We also compare our fast generation method with different data (only reachable or full data). #Data is the size of data for training the feature weights. * means significantly (Koehn, 2004) better than MERT (p &lt; 0.01). word alignment in both directions, and grow-diagfinal-and (Koehn et al., 2003) to generate symmetric word alignment. We extract SCFG rules as described in Chiang (2007) and also added rules (Section 4.2). Our algorithm runs on the entire training data, which requires to load all the rules into the memory. To fit within memory, we cut off those composed rules which only happen once in the training data. Here a composed rule is a rule that can be produced by any other extracted rules. A 4-grams language model is trained by the SRILM toolkit (Stolcke, 2002). Case-insensitive NIST BLEU4 (Papineni et al., 2002) is used to measure translation performance. The training data comes from a subset of the LDC data including LDC2002E18, LDC2003E07, LDC2003E14, Hansards p</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
</authors>
<title>A Bayesian model of syntax-directed tree to string grammar induction.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP</booktitle>
<contexts>
<context position="29808" citStr="Cohn and Blunsom, 2009" startWordPosition="5111" endWordPosition="5114">005; Poon et al., 2009). The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marginal distribution. Furthermore, we focus on how to fast generate translation forest for training. The local operators lexicalize/generalize are use for greedy decoding. The idea is related to “pegging” algorithm (Brown et al., 1993) and greedy decoding (Germann et al., 2001). Such types of local operators are also used in Gibbs sampler for synchronous grammar induction (Blunsom et al., 2009; Cohn and Blunsom, 2009). 8 Conclusion and Future Work We have presented a fast generation algorithm for translation forest which contains both reference derivations and neighboring non-reference derivations for large-scale SMT discriminative training. We have achieved significantly improvement of 0.84 BLEU by incorporate 13.9M feature trained on 519K data in 0.03 second per sentence. In this paper, we define the forest based on competing derivations which only differ in one rule. There may be better classes of forest that can produce a better performance. It’s interesting to modify the definition of forest, and use </context>
</contexts>
<marker>Cohn, Blunsom, 2009</marker>
<rawString>Trevor Cohn and Phil Blunsom. 2009. A Bayesian model of syntax-directed tree to string grammar induction. In Proc. of EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP</booktitle>
<contexts>
<context position="19342" citStr="Collins, 2002" startWordPosition="3344" endWordPosition="3345">ss. We first create an initial reference derivation for every training examples using bi-parsing (lines 4-5), and then online learn the parameters using SGD (lines 6-12). We use the GENERATE function to calculate the gradient. In practice, instead of storing all the derivations in a list, we traverse the tree twice. The first time is calculating the partition function, and the second time calculates the gradient normalized by partition function. During training, we also change the derivations (line 10). When training is finished after M epochs, the algorithm returns an averaged weight vector (Collins, 2002) to avoid overfitting (line 13). We use a development set to select total epoch m, which is set as M = 5 in our experiments. 6 Experiments Our method is able to train a large number of features on large data. We use a set of word context features motivated by word sense disambiguation (Chan et al., 2007) to test scalability. A word level context feature is a triple (f, e, f+1), which counts the number of time that f is aligned to e and f+1 occurs to the right of f. Triple (f, e, f−1) is similar except that f−1 locates to the left of f. We retain word alignment information in the extracted rule</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proc. of EMNLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Gillick</author>
<author>James Zhang</author>
<author>Dan Klein</author>
</authors>
<title>Why generative phrase models underperform surface heuristics.</title>
<date>2006</date>
<booktitle>In Proc. of the HLT-NAACL 2006 Workshop on SMT.</booktitle>
<contexts>
<context position="15535" citStr="DeNero et al., 2006" startWordPosition="2674" endWordPosition="2677">883 To parse quickly, we only visit the tight consistent (Zhang et al., 2008) bi-spans with the help of word alignment a. Only visiting tight consistent spans greatly speeds up bi-parsing. Besides efficiency, adoption of this constraint receives support from the fact that heuristic SCFG rule extraction only extracts tight consistent initial phrases (Chiang, 2007). Second is degenerate problem. If we only use the features as traditional SCFG systems, the biparsing may end with a derivation consists of some giant rules or rules with rare source/target sides, which is called degenerate solution (DeNero et al., 2006). That is because the translation rules with rare source/target sides always receive a very high translation probability. We add a prior score log(#rule) for each rule, where #rule is the number of occurrence of a rule, to reward frequent reusable rules and derivations with more rules. Finally, we may fail to create reference derivations due to the limitation in rule extraction. We create minimum trees for (f, e, a) using shift-reduce (Zhang et al., 2008). Some minimum rules in the trees may be illegal according to the definition of Chiang (2007). We also add these rules to the rule table, so </context>
</contexts>
<marker>DeNero, Gillick, Zhang, Klein, 2006</marker>
<rawString>John DeNero, Dan Gillick, James Zhang, and Dan Klein. 2006. Why generative phrase models underperform surface heuristics. In Proc. of the HLT-NAACL 2006 Workshop on SMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
</authors>
<title>Two monolingual parses are better than one (synchronous parse).</title>
<date>2010</date>
<booktitle>In Proc. of NAACL</booktitle>
<contexts>
<context position="14718" citStr="Dyer, 2010" startWordPosition="2548" endWordPosition="2549">ge of rules, the lexicalize operator deletes two rules and adds one new rule, while the generalize operator deletes one rule and adds two new rules. Such local changes provide us with a way to incrementally calculate the scores of new derivations. We use this method motivated by Gibbs Sampler (Blunsom et al., 2009) which has been used for efficiently learning rules. The different lies in that we use the operator for decoding where the rule table is fixing. 4.2 Initialize a Reference Derivation The generation starts from an initial reference derivation with max score. This requires bi-parsing (Dyer, 2010) over the source sentence f and the reference translation e. In practice, we may face three problems. First is efficiency problem. Exhaustive search over the space under SCFG requires O(|f|3|e|3). 883 To parse quickly, we only visit the tight consistent (Zhang et al., 2008) bi-spans with the help of word alignment a. Only visiting tight consistent spans greatly speeds up bi-parsing. Besides efficiency, adoption of this constraint receives support from the fact that heuristic SCFG rule extraction only extracts tight consistent initial phrases (Chiang, 2007). Second is degenerate problem. If we </context>
</contexts>
<marker>Dyer, 2010</marker>
<rawString>Chris Dyer. 2010. Two monolingual parses are better than one (synchronous parse). In Proc. of NAACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Kenji Yamada</author>
</authors>
<title>Fast decoding and optimal decoding for machine translation.</title>
<date>2001</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="29665" citStr="Germann et al., 2001" startWordPosition="5087" endWordPosition="5090">ata. For efficiency, we only use neighboring derivations for training. Such motivation is same as contrastive estimation (Smith and Eisner, 2005; Poon et al., 2009). The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marginal distribution. Furthermore, we focus on how to fast generate translation forest for training. The local operators lexicalize/generalize are use for greedy decoding. The idea is related to “pegging” algorithm (Brown et al., 1993) and greedy decoding (Germann et al., 2001). Such types of local operators are also used in Gibbs sampler for synchronous grammar induction (Blunsom et al., 2009; Cohn and Blunsom, 2009). 8 Conclusion and Future Work We have presented a fast generation algorithm for translation forest which contains both reference derivations and neighboring non-reference derivations for large-scale SMT discriminative training. We have achieved significantly improvement of 0.84 BLEU by incorporate 13.9M feature trained on 519K data in 0.03 second per sentence. In this paper, we define the forest based on competing derivations which only differ in one r</context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2001</marker>
<rawString>Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2001. Fast decoding and optimal decoding for machine translation. In Proc. of ACL 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL</booktitle>
<contexts>
<context position="5609" citStr="Koehn et al., 2003" startWordPosition="900" endWordPosition="903"> al., 2007) together with the features used in traditional SMT system (Section 6). Training on 519K sentence pairs in 0.03 seconds per sentence, we achieve significantly improvement over the traditional pipeline by 0.84 BLEU. 2 Synchronous Context Free Grammar We work on synchronous context free grammar (SCFG) (Chiang, 2007) based translation. The elementary structures in an SCFG are rewrite rules of the form: X (-y, α) where -y and α are strings of terminals and nonterminals. We call -y and α as the source side and the target side of rule respectively. Here a rule means a phrase translation (Koehn et al., 2003) or a translation pair that contains nonterminals. We call a sequence of translation steps as a derivation. In context of SCFG, a derivation is a sequence of SCFG rules {rz}. Translation forest (Mi et al., 2008; Li and Eisner, 2009) is a compact representation of all the derivations for a given sentence under an SCFG (see Figure 1). A tree t in the forest corresponds to a derivation. In our paper, tree means the same as derivation. More formally, a forest is a pair (V, E), where V is the set of nodes, E is the set of hyperedge. For a given source sentence f = f&apos;1 , Each node v E V is in the fo</context>
<context position="23477" citStr="Koehn et al., 2003" startWordPosition="4057" endWordPosition="4060">TA #FEAT MT03 MT04 MT05 ALL MERT 878 8 33.03 35.12 32.32 33.85 Perceptron 878 2.4K 32.89 34.88 32.55 33.76 Our Method 187K 2.0M 33.64 35.48 32.91* 34.41* 519K 13.9M 34.19* 35.72* 33.09* 34.69* Improvement over MERT +1.16 +0.60 +0.77 +0.84 Table 2: Effect of our method comparing with MERT and perceptron in terms of BLEU. We also compare our fast generation method with different data (only reachable or full data). #Data is the size of data for training the feature weights. * means significantly (Koehn, 2004) better than MERT (p &lt; 0.01). word alignment in both directions, and grow-diagfinal-and (Koehn et al., 2003) to generate symmetric word alignment. We extract SCFG rules as described in Chiang (2007) and also added rules (Section 4.2). Our algorithm runs on the entire training data, which requires to load all the rules into the memory. To fit within memory, we cut off those composed rules which only happen once in the training data. Here a composed rule is a rule that can be produced by any other extracted rules. A 4-grams language model is trained by the SRILM toolkit (Stolcke, 2002). Case-insensitive NIST BLEU4 (Papineni et al., 2002) is used to measure translation performance. The training data co</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of HLT-NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP</booktitle>
<contexts>
<context position="23369" citStr="Koehn, 2004" startWordPosition="4041" endWordPosition="4042">s the statistics of all bilingual corpus. We use GIZA++ (Och and Ney, 2003) to perform 885 System #DATA #FEAT MT03 MT04 MT05 ALL MERT 878 8 33.03 35.12 32.32 33.85 Perceptron 878 2.4K 32.89 34.88 32.55 33.76 Our Method 187K 2.0M 33.64 35.48 32.91* 34.41* 519K 13.9M 34.19* 35.72* 33.09* 34.69* Improvement over MERT +1.16 +0.60 +0.77 +0.84 Table 2: Effect of our method comparing with MERT and perceptron in terms of BLEU. We also compare our fast generation method with different data (only reachable or full data). #Data is the size of data for training the feature weights. * means significantly (Koehn, 2004) better than MERT (p &lt; 0.01). word alignment in both directions, and grow-diagfinal-and (Koehn et al., 2003) to generate symmetric word alignment. We extract SCFG rules as described in Chiang (2007) and also added rules (Section 4.2). Our algorithm runs on the entire training data, which requires to load all the rules into the memory. To fit within memory, we cut off those composed rules which only happen once in the training data. Here a composed rule is a rule that can be produced by any other extracted rules. A 4-grams language model is trained by the SRILM toolkit (Stolcke, 2002). Case-ins</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
</authors>
<title>First- and second-order expectation semirings with applications to minimumrisk training on translation forests.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP</booktitle>
<contexts>
<context position="5841" citStr="Li and Eisner, 2009" startWordPosition="942" endWordPosition="945">chronous Context Free Grammar We work on synchronous context free grammar (SCFG) (Chiang, 2007) based translation. The elementary structures in an SCFG are rewrite rules of the form: X (-y, α) where -y and α are strings of terminals and nonterminals. We call -y and α as the source side and the target side of rule respectively. Here a rule means a phrase translation (Koehn et al., 2003) or a translation pair that contains nonterminals. We call a sequence of translation steps as a derivation. In context of SCFG, a derivation is a sequence of SCFG rules {rz}. Translation forest (Mi et al., 2008; Li and Eisner, 2009) is a compact representation of all the derivations for a given sentence under an SCFG (see Figure 1). A tree t in the forest corresponds to a derivation. In our paper, tree means the same as derivation. More formally, a forest is a pair (V, E), where V is the set of nodes, E is the set of hyperedge. For a given source sentence f = f&apos;1 , Each node v E V is in the form XzJ, which denotes the recognition of nonterminal X spanning the substring from the i through j (that is fz+1...fj). Each hyperedge e E E connects a set of antecedent to a single consequent node and corresponds to an SCFG rule r(</context>
</contexts>
<marker>Li, Eisner, 2009</marker>
<rawString>Zhifei Li and Jason Eisner. 2009. First- and second-order expectation semirings with applications to minimumrisk training on translation forests. In Proc. of EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Variational decoding for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="28808" citStr="Li et al., 2009" startWordPosition="4950" endWordPosition="4953">vations. Furthermore, we use lexicalize operator more often than generalize operator (the ration between them is 1.5 to 1). Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus small) rules. 7 Related Work Minimum error rate training (Och, 2003) is perhaps the most popular discriminative training for SMT. However, it fails to scale to large number of features. Researchers have propose many learning algorithms to train many features: perceptron (Shen et al., 2004; Liang et al., 2006), minimum risk (Smith and Eisner, 2006; Li et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008). The complexity of n-best lists or packed forests generation hamper these algorithms to scale to a large amount of data. For efficiency, we only use neighboring derivations for training. Such motivation is same as contrastive estimation (Smith and Eisner, 2005; Poon et al., 2009). The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marginal distribution.</context>
</contexts>
<marker>Li, Eisner, Khudanpur, 2009</marker>
<rawString>Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009. Variational decoding for statistical machine translation. In Proc. of ACL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of ACL</booktitle>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proc. of ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="5819" citStr="Mi et al., 2008" startWordPosition="938" endWordPosition="941"> 0.84 BLEU. 2 Synchronous Context Free Grammar We work on synchronous context free grammar (SCFG) (Chiang, 2007) based translation. The elementary structures in an SCFG are rewrite rules of the form: X (-y, α) where -y and α are strings of terminals and nonterminals. We call -y and α as the source side and the target side of rule respectively. Here a rule means a phrase translation (Koehn et al., 2003) or a translation pair that contains nonterminals. We call a sequence of translation steps as a derivation. In context of SCFG, a derivation is a sequence of SCFG rules {rz}. Translation forest (Mi et al., 2008; Li and Eisner, 2009) is a compact representation of all the derivations for a given sentence under an SCFG (see Figure 1). A tree t in the forest corresponds to a derivation. In our paper, tree means the same as derivation. More formally, a forest is a pair (V, E), where V is the set of nodes, E is the set of hyperedge. For a given source sentence f = f&apos;1 , Each node v E V is in the form XzJ, which denotes the recognition of nonterminal X spanning the substring from the i through j (that is fz+1...fj). Each hyperedge e E E connects a set of antecedent to a single consequent node and correspo</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proc. of ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="1043" citStr="Och and Ney, 2002" startWordPosition="148" endWordPosition="151">ing features, it is hard to scale up to large data due to decoding complexity. We propose a new algorithm to generate translation forest of training data in linear time with the help of word alignment. Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation. With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets. 1 Introduction Discriminative model (Och and Ney, 2002) can easily incorporate non-independent and overlapping features, and has been dominating the research field of statistical machine translation (SMT) in the last decade. Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). However, the training of the large number of features was always restricted in fairly small data sets. Some systems limit the number of training examples, while others use short sentences to maintain efficiency. Overfitting prob</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz J. Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. of ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="22832" citStr="Och and Ney, 2003" startWordPosition="3948" endWordPosition="3951">o future work. 6.1 Data We focus on the Chinese-to-English translation task in this paper. The bilingual corpus we use contains 519, 359 sentence pairs, with an average length of 16.5 in source side and 20.3 in target side, where 186,810 sentence pairs (36%) are reachable (without added rules in Section 4.2). The monolingual data includes the Xinhua portion of the GIGAWORD corpus, which contains 238M English words. We use the NIST evaluation sets of 2002 (MT02) as our development set, and sets of MT03/MT04/MT05 as test sets. Table 2 shows the statistics of all bilingual corpus. We use GIZA++ (Och and Ney, 2003) to perform 885 System #DATA #FEAT MT03 MT04 MT05 ALL MERT 878 8 33.03 35.12 32.32 33.85 Perceptron 878 2.4K 32.89 34.88 32.55 33.76 Our Method 187K 2.0M 33.64 35.48 32.91* 34.41* 519K 13.9M 34.19* 35.72* 33.09* 34.69* Improvement over MERT +1.16 +0.60 +0.77 +0.84 Table 2: Effect of our method comparing with MERT and perceptron in terms of BLEU. We also compare our fast generation method with different data (only reachable or full data). #Data is the size of data for training the feature weights. * means significantly (Koehn, 2004) better than MERT (p &lt; 0.01). word alignment in both directions</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ofACL</booktitle>
<contexts>
<context position="20196" citStr="Och, 2003" startWordPosition="3508" endWordPosition="3509">es motivated by word sense disambiguation (Chan et al., 2007) to test scalability. A word level context feature is a triple (f, e, f+1), which counts the number of time that f is aligned to e and f+1 occurs to the right of f. Triple (f, e, f−1) is similar except that f−1 locates to the left of f. We retain word alignment information in the extracted rules to exploit such features. To demonstrate the importance of scaling up the size of training data and the effect of our method, we compare three types of training configurations which differ in the size of features and data. MERT. We use MERT (Och, 2003) to training 8 features on a small data. The 8 features is the same as Chiang (2007) including 4 rule scores (direct and reverse translation scores; direct and reverse lexical translation scores); 1 target side language model score; 3 penalties for word counts, extracted rules and glue rule. Actually, traditional pipeline often uses such configuration. Perceptron. We also learn thousands of context word features together with the 8 traditional features on a small data using perceptron. Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature. This setting u</context>
<context position="28510" citStr="Och, 2003" startWordPosition="4904" endWordPosition="4905">ch step is about 9.5 milliseconds and 30.2 milliseconds respectively. For simplicity we do not compress the generated derivations into forests, therefore the size of resulting derivations is fairly small, which is about 265.8 for each sentence on average, where 6.1 of them are reference derivations. Furthermore, we use lexicalize operator more often than generalize operator (the ration between them is 1.5 to 1). Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus small) rules. 7 Related Work Minimum error rate training (Och, 2003) is perhaps the most popular discriminative training for SMT. However, it fails to scale to large number of features. Researchers have propose many learning algorithms to train many features: perceptron (Shen et al., 2004; Liang et al., 2006), minimum risk (Smith and Eisner, 2006; Li et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008). The complexity of n-best lists or packed forests generation hamper these algorithms to scale to a large amount of data. For efficiency, we only use neighboring derivations for train</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ofACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="24012" citStr="Papineni et al., 2002" startWordPosition="4152" endWordPosition="4156">&lt; 0.01). word alignment in both directions, and grow-diagfinal-and (Koehn et al., 2003) to generate symmetric word alignment. We extract SCFG rules as described in Chiang (2007) and also added rules (Section 4.2). Our algorithm runs on the entire training data, which requires to load all the rules into the memory. To fit within memory, we cut off those composed rules which only happen once in the training data. Here a composed rule is a rule that can be produced by any other extracted rules. A 4-grams language model is trained by the SRILM toolkit (Stolcke, 2002). Case-insensitive NIST BLEU4 (Papineni et al., 2002) is used to measure translation performance. The training data comes from a subset of the LDC data including LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. Since the rule table of the entire data is too large to be loaded to the memory (even drop one-count rules), we remove many sentence pairs to create a much smaller data yet having a comparable performance with the entire data. The intuition lies in that if most of the source words of a sentence need to be translated by the added rules, then the word alignment may be highly crossed and the sent</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. of ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Colin Cherry</author>
<author>Kristina Toutanova</author>
</authors>
<title>Unsupervised morphological segmentation with log-linear models.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL</booktitle>
<contexts>
<context position="2421" citStr="Poon et al., 2009" startWordPosition="362" endWordPosition="365">blem. Furthermore, large data also enables us to globally train millions of sparse lexical features which offer accurate clues for SMT. Despite these advantages, to the best of our knowledge, no previous discriminative training paradigms scale up to use a large amount of training data. The main obstacle comes from the complexity of packed forests or n-best lists generation which requires to search through all possible translations of each training example, which is computationally prohibitive in practice for SMT. To make normalization efficient, contrastive estimation (Smith and Eisner, 2005; Poon et al., 2009) introduce neighborhood for unsupervised log-linear model, and has presented positive results in various tasks. Motivated by these work, we use a translation forest (Section 3) which contains both “reference” derivations that potentially yield the reference translation and also neighboring “non-reference” derivations that fail to produce the reference translation.1 However, the complexity of generating this translation forest is up to 0(n6), because we still need biparsing to create the reference derivations. Consequently, we propose a method to fast generate a subset of the forest. The key id</context>
<context position="29208" citStr="Poon et al., 2009" startWordPosition="5016" endWordPosition="5019">to scale to large number of features. Researchers have propose many learning algorithms to train many features: perceptron (Shen et al., 2004; Liang et al., 2006), minimum risk (Smith and Eisner, 2006; Li et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008). The complexity of n-best lists or packed forests generation hamper these algorithms to scale to a large amount of data. For efficiency, we only use neighboring derivations for training. Such motivation is same as contrastive estimation (Smith and Eisner, 2005; Poon et al., 2009). The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marginal distribution. Furthermore, we focus on how to fast generate translation forest for training. The local operators lexicalize/generalize are use for greedy decoding. The idea is related to “pegging” algorithm (Brown et al., 1993) and greedy decoding (Germann et al., 2001). Such types of local operators are also used in Gibbs sampler for synchronous grammar induction (Blunsom et al., 2009; Cohn and Blunsom, 2009)</context>
</contexts>
<marker>Poon, Cherry, Toutanova, 2009</marker>
<rawString>Hoifung Poon, Colin Cherry, and Kristina Toutanova. 2009. Unsupervised morphological segmentation with log-linear models. In Proc. of NAACL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Anoop Sarkar</author>
<author>Franz Josef Och</author>
</authors>
<title>Discriminative reranking for machine translation.</title>
<date>2004</date>
<booktitle>In Proc. of NAACL</booktitle>
<contexts>
<context position="28731" citStr="Shen et al., 2004" startWordPosition="4936" endWordPosition="4939">about 265.8 for each sentence on average, where 6.1 of them are reference derivations. Furthermore, we use lexicalize operator more often than generalize operator (the ration between them is 1.5 to 1). Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus small) rules. 7 Related Work Minimum error rate training (Och, 2003) is perhaps the most popular discriminative training for SMT. However, it fails to scale to large number of features. Researchers have propose many learning algorithms to train many features: perceptron (Shen et al., 2004; Liang et al., 2006), minimum risk (Smith and Eisner, 2006; Li et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008). The complexity of n-best lists or packed forests generation hamper these algorithms to scale to a large amount of data. For efficiency, we only use neighboring derivations for training. Such motivation is same as contrastive estimation (Smith and Eisner, 2005; Poon et al., 2009). The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, depende</context>
</contexts>
<marker>Shen, Sarkar, Och, 2004</marker>
<rawString>Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004. Discriminative reranking for machine translation. In Proc. of NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="2401" citStr="Smith and Eisner, 2005" startWordPosition="358" endWordPosition="361">a can alleviate such problem. Furthermore, large data also enables us to globally train millions of sparse lexical features which offer accurate clues for SMT. Despite these advantages, to the best of our knowledge, no previous discriminative training paradigms scale up to use a large amount of training data. The main obstacle comes from the complexity of packed forests or n-best lists generation which requires to search through all possible translations of each training example, which is computationally prohibitive in practice for SMT. To make normalization efficient, contrastive estimation (Smith and Eisner, 2005; Poon et al., 2009) introduce neighborhood for unsupervised log-linear model, and has presented positive results in various tasks. Motivated by these work, we use a translation forest (Section 3) which contains both “reference” derivations that potentially yield the reference translation and also neighboring “non-reference” derivations that fail to produce the reference translation.1 However, the complexity of generating this translation forest is up to 0(n6), because we still need biparsing to create the reference derivations. Consequently, we propose a method to fast generate a subset of th</context>
<context position="29188" citStr="Smith and Eisner, 2005" startWordPosition="5012" endWordPosition="5015"> SMT. However, it fails to scale to large number of features. Researchers have propose many learning algorithms to train many features: perceptron (Shen et al., 2004; Liang et al., 2006), minimum risk (Smith and Eisner, 2006; Li et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008). The complexity of n-best lists or packed forests generation hamper these algorithms to scale to a large amount of data. For efficiency, we only use neighboring derivations for training. Such motivation is same as contrastive estimation (Smith and Eisner, 2005; Poon et al., 2009). The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marginal distribution. Furthermore, we focus on how to fast generate translation forest for training. The local operators lexicalize/generalize are use for greedy decoding. The idea is related to “pegging” algorithm (Brown et al., 1993) and greedy decoding (Germann et al., 2001). Such types of local operators are also used in Gibbs sampler for synchronous grammar induction (Blunsom et al., 2009; Coh</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proc. of ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Minimum risk annealing for training log-linear models.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL</booktitle>
<contexts>
<context position="28790" citStr="Smith and Eisner, 2006" startWordPosition="4946" endWordPosition="4949"> them are reference derivations. Furthermore, we use lexicalize operator more often than generalize operator (the ration between them is 1.5 to 1). Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus small) rules. 7 Related Work Minimum error rate training (Och, 2003) is perhaps the most popular discriminative training for SMT. However, it fails to scale to large number of features. Researchers have propose many learning algorithms to train many features: perceptron (Shen et al., 2004; Liang et al., 2006), minimum risk (Smith and Eisner, 2006; Li et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008). The complexity of n-best lists or packed forests generation hamper these algorithms to scale to a large amount of data. For efficiency, we only use neighboring derivations for training. Such motivation is same as contrastive estimation (Smith and Eisner, 2005; Poon et al., 2009). The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marg</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>David A. Smith and Jason Eisner. 2006. Minimum risk annealing for training log-linear models. In Proc. of COLING/ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP</booktitle>
<contexts>
<context position="23959" citStr="Stolcke, 2002" startWordPosition="4147" endWordPosition="4148">nificantly (Koehn, 2004) better than MERT (p &lt; 0.01). word alignment in both directions, and grow-diagfinal-and (Koehn et al., 2003) to generate symmetric word alignment. We extract SCFG rules as described in Chiang (2007) and also added rules (Section 4.2). Our algorithm runs on the entire training data, which requires to load all the rules into the memory. To fit within memory, we cut off those composed rules which only happen once in the training data. Here a composed rule is a rule that can be produced by any other extracted rules. A 4-grams language model is trained by the SRILM toolkit (Stolcke, 2002). Case-insensitive NIST BLEU4 (Papineni et al., 2002) is used to measure translation performance. The training data comes from a subset of the LDC data including LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. Since the rule table of the entire data is too large to be loaded to the memory (even drop one-count rules), we remove many sentence pairs to create a much smaller data yet having a comparable performance with the entire data. The intuition lies in that if most of the source words of a sentence need to be translated by the added rules, then </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm – an extensible language modeling toolkit. In Proc. of ICSLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Tong Zhang</author>
</authors>
<title>A discriminative global training algorithm for statistical mt.</title>
<date>2006</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="1346" citStr="Tillmann and Zhang, 2006" startWordPosition="194" endWordPosition="197">tains derivations that exactly yield the reference translation. With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets. 1 Introduction Discriminative model (Och and Ney, 2002) can easily incorporate non-independent and overlapping features, and has been dominating the research field of statistical machine translation (SMT) in the last decade. Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). However, the training of the large number of features was always restricted in fairly small data sets. Some systems limit the number of training examples, while others use short sentences to maintain efficiency. Overfitting problem often comes when training many features on a small data (Watanabe et al., 2007; Chiang et al., 2009). Obviously, using much more data can alleviate such problem. Furthermore, large data also enables us to globally train millions of sparse lexical features which offer accurate clues for SMT. Despite</context>
</contexts>
<marker>Tillmann, Zhang, 2006</marker>
<rawString>Christoph Tillmann and Tong Zhang. 2006. A discriminative global training algorithm for statistical mt. In Proc. of ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL</booktitle>
<contexts>
<context position="1369" citStr="Watanabe et al., 2007" startWordPosition="198" endWordPosition="201">ctly yield the reference translation. With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets. 1 Introduction Discriminative model (Och and Ney, 2002) can easily incorporate non-independent and overlapping features, and has been dominating the research field of statistical machine translation (SMT) in the last decade. Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). However, the training of the large number of features was always restricted in fairly small data sets. Some systems limit the number of training examples, while others use short sentences to maintain efficiency. Overfitting problem often comes when training many features on a small data (Watanabe et al., 2007; Chiang et al., 2009). Obviously, using much more data can alleviate such problem. Furthermore, large data also enables us to globally train millions of sparse lexical features which offer accurate clues for SMT. Despite these advantages, to t</context>
<context position="4504" citStr="Watanabe et al., 2007" startWordPosition="714" endWordPosition="717"> 3,4 6 0 1 2 3 4 3 0,1 5 Figure 1: A translation forest which is the running example throughout this paper. The reference translation is “the gunman was killed by the police”. (1) Solid hyperedges denote a “reference” derivation tree t1 which exactly yields the reference translation. (2) Replacing e3 in t1 with e4 results a competing non-reference derivation t2, which fails to swap the order of X3,4. (3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3. Generally, this is done by deleting a node X0,1. ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is non-trivial for SMT and needs to be determined experimentally. Given such forests, we globally learn a log-linear model using stochastic gradient descend (Section 5). Overall, both the generation of forests and the training algorithm are scalable, enabling us to train millions of features on large-scale data. To show the effect of our framework, we globally train millions of word level context features motivated by word sense disambiguation (Chan et al., 2007) together with the features used in traditional SMT system (Section 6). Training on 519K sentence pairs</context>
<context position="28837" citStr="Watanabe et al., 2007" startWordPosition="4955" endWordPosition="4958"> use lexicalize operator more often than generalize operator (the ration between them is 1.5 to 1). Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus small) rules. 7 Related Work Minimum error rate training (Och, 2003) is perhaps the most popular discriminative training for SMT. However, it fails to scale to large number of features. Researchers have propose many learning algorithms to train many features: perceptron (Shen et al., 2004; Liang et al., 2006), minimum risk (Smith and Eisner, 2006; Li et al., 2009), MIRA (Watanabe et al., 2007; Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008). The complexity of n-best lists or packed forests generation hamper these algorithms to scale to a large amount of data. For efficiency, we only use neighboring derivations for training. Such motivation is same as contrastive estimation (Smith and Eisner, 2005; Poon et al., 2009). The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marginal distribution. Furthermore, we focus on how</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In Proc. of EMNLP-CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
<author>David Chiang</author>
</authors>
<title>Extracting synchronous grammar rules from word-level alignments in linear time.</title>
<date>2008</date>
<booktitle>In Proc. of Coling</booktitle>
<contexts>
<context position="14992" citStr="Zhang et al., 2008" startWordPosition="2591" endWordPosition="2594"> motivated by Gibbs Sampler (Blunsom et al., 2009) which has been used for efficiently learning rules. The different lies in that we use the operator for decoding where the rule table is fixing. 4.2 Initialize a Reference Derivation The generation starts from an initial reference derivation with max score. This requires bi-parsing (Dyer, 2010) over the source sentence f and the reference translation e. In practice, we may face three problems. First is efficiency problem. Exhaustive search over the space under SCFG requires O(|f|3|e|3). 883 To parse quickly, we only visit the tight consistent (Zhang et al., 2008) bi-spans with the help of word alignment a. Only visiting tight consistent spans greatly speeds up bi-parsing. Besides efficiency, adoption of this constraint receives support from the fact that heuristic SCFG rule extraction only extracts tight consistent initial phrases (Chiang, 2007). Second is degenerate problem. If we only use the features as traditional SCFG systems, the biparsing may end with a derivation consists of some giant rules or rules with rare source/target sides, which is called degenerate solution (DeNero et al., 2006). That is because the translation rules with rare source/</context>
<context position="16530" citStr="Zhang et al. (2008)" startWordPosition="2846" endWordPosition="2849">tion. We create minimum trees for (f, e, a) using shift-reduce (Zhang et al., 2008). Some minimum rules in the trees may be illegal according to the definition of Chiang (2007). We also add these rules to the rule table, so as to make sure every sentence is reachable given the rule table. A source sentence is reachable given a rule table if reference derivations exists. We refer these rules as added rules. However, this may introduce rules with more than two variables and increase the complexity of bi-parsing. To tackle this problem, we initialize the chart with minimum parallel tree from the Zhang et al. (2008) algorithm, ensuring that the bi-parsing has at least one path to create a reference derivation. Then we only need to consider the traditional rules during bi-parsing. 5 Training We use the forest to train a log-linear model with a latent variable as describe in Blunsom et al.(2008). The probability p(e|f) is the sum over all possible derivations: p(e|f) = � p(t, e|f) (4) tEA(e,f) where △(e, f) is the set of all possible derivations that translate f into e and t is one such derivation.4 4Although the derivation is typically represent as d, we denotes it by t since our paper use tree to represe</context>
</contexts>
<marker>Zhang, Gildea, Chiang, 2008</marker>
<rawString>Hao Zhang, Daniel Gildea, and David Chiang. 2008. Extracting synchronous grammar rules from word-level alignments in linear time. In Proc. of Coling 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>