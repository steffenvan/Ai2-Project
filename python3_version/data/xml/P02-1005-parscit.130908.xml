<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002289">
<note confidence="0.9048325">
Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 33-40.
</note>
<bodyText confidence="0.9908666">
analysis (Voorhees, 1999). From the combined
set of 1460 evaluation questions, 70% of the par-
ticipating systems answered successfully ques-
tions like Q1013: &amp;quot;Where is Perth?&amp;quot;, but none
could find a correct answer to complex questions
such as Q1165: &amp;quot;What is the difference between
AM radio stations and FM radio stations?&amp;quot;.
Since performance is affected by the complex-
ity of question processing, we first provide a
broad taxonomy of QA systems.
</bodyText>
<sectionHeader confidence="0.473464" genericHeader="abstract">
2.1 Criteria
</sectionHeader>
<bodyText confidence="0.944241428571429">
The taxonomy is based on several criteria that
play an important role in building QA systems:
(1) linguistic and knowledge resources, (2) natu-
ral language processing involved, (3) document
processing, (4) reasoning methods, (5) whether
or not answer is explicitly stated in a document,
(6) whether or not answer fusion is necessary.
</bodyText>
<subsectionHeader confidence="0.901975">
2.2 Classes of questions
</subsectionHeader>
<bodyText confidence="0.96238575">
Class I. QA systems capable of processing fac-
tual questions
These systems extract answers as text snippets
from one or more documents. Often the an-
swer is found verbatim in a text or as a simple
morphological variation. Typically the answers
are extracted using empirical methods relying
on keyword manipulations.
</bodyText>
<construct confidence="0.542106">
Class 2. QA systems enabling simple reason-
ing mechanisms
</construct>
<bodyText confidence="0.962123629629629">
The characteristic of this class is that answers
are found in snippets of text, but unlike in Class
1, inference is necessary to relate the question
with the answer. More elaborate answer detec-
tion methods such as ontologies or codification
of pragmatic knowledge are necessary. Semantic
alternations, world knowledge axioms and sim-
ple reasoning methods are necessary. An exam-
ple is Q198: &amp;quot;How did Socrates died?&amp;quot; where
die has to be linked with drinking poisoned wine.
WordNet and its extensions are sometimes used
as sources of world knowledge.
Class 3. QA systems capable of answer fusion
from different documents
In this class the partial answer information is
scattered throughout several documents and an-
swer fusion is necessary. The complexity here
ranges from assembling simple lists to far more
complex questions like script questions, (e.g.
&amp;quot;How do I assemble a bicycle?&amp;quot;), or template-
like questions ( &amp;quot;What management successions
occurred at IBM in the past year?&amp;quot;).
Class 4. Interactive QA systems
These systems are able to answer questions in
the context of previous interactions with the
user. As reported in (Harabagiu et al., 2001),
processing a list of questions posed in a con-
text involves complex reference resolution. Un-
like typical reference resolution algorithms that
associate anaphore with a referent, the reference
imposed by context questions requires the asso-
ciation of an anaphora from the current ques-
tion with either one of the previous questions,
answers or their anaphora.
Class 5. QA systems capable of analogical rea-
soning
The characteristic of these systems is their abil-
ity to answer speculative questions similar to:
&amp;quot;Is the Fed going to raise interests at their next
meeting?&amp;quot;; &amp;quot;Is the US out of recession?&amp;quot;; &amp;quot;Is the
airline industry in trouble?&amp;quot;.
Since most probably the answer to such ques-
tions is not explicitly stated in documents, sim-
ply because events may not have happened yet,
QA systems from this class decompose the ques-
tion into queries that extract pieces of evidence,
after which answer is formulated using reasoning
by analogy. The resources include ad-hoc knowl-
edge bases generated from mining text docu-
ments clustered by the question topic. Asso-
ciated with these knowledge sources are case-
based reasoning techniques as well as methods
for temporal reasoning, spatial reasoning and
evidential reasoning.
</bodyText>
<tableCaption confidence="0.997428">
Table 1: Distribution of TREC questions
</tableCaption>
<table confidence="0.8563385">
Type Number (%)
Class 1 (factual) 985 (67.5%)
Class 2 (simple-reasoning) 408 (27.9%)
Class 3 (fusion - list) 25 (1.7%)
Class 4 (interactive - context) 42 (2.9%)
Class 5 (speculative) 0 (0.0%)
</table>
<tableCaption confidence="0.883937">
Table 1 illustrates the distribution of TREC
</tableCaption>
<figure confidence="0.996509018867924">
1966
Niagra − Niagara
rented − rent
Money
bug
Examples
Volkswagen
Person bug invented − inventor
How much
rent
Volkswangen −
Volkswagen
rent built − build
Country
Volkswagen
M1 M2 M3 M4 M5
Question
Keyword
selection
Derivation
of expected
answer type
Keyword
expansion
Construction
of question
representation
Keyword
pre−processing
(split/bind/spell)
System
Modules
Identification
of candidate
answers
Answer
Answer
formulation
Answer
ranking
Retrieval
of documents
and passages
Passage
post−filtering
M6 M7 M8 M9 M10
Examples
&amp;quot;in 1966&amp;quot; USD 520
Volkswagen AND bug
60 passages 2 passages
$1 &amp;quot;rent a Volkswagen
USD 520 bug for $1 a day&amp;quot;
$1
</figure>
<bodyText confidence="0.999837859649123">
AND bug), the retrieval is insuccessful since
the passages containing the correct answers are
missed.
M5 Before the construction of Boolean queries
for actual retrieval, the selected keywords are
expanded with morphological, lexical or seman-
tic alternations. The alternations correspond to
other forms in which the question concepts may
occur in the answers. For example, rented is ex-
panded into rent.
M6 The retrieval engine returns the docu-
ments containing all keywords specified in the
Boolean queries. The documents are then fur-
ther restricted to smaller text passages where
all keywords are located in the proximity of one
another. Each retrieved passage includes addi-
tional text (extra lines) before the earliest and
after the latest keyword match. For illustration,
consider Q005: &amp;quot;What is the name of the man-
aging director of Apricot Computer? and the as-
sociated Boolean query Apricot AND Computer
AND director. The relevant text fragment from
the document collection is &amp;quot;Dr Peter Horne,
managing director of Apricot Computers&amp;quot;. Un-
less additional text is included in the passages,
the actual answer Peter Horne would be missed
because it occurs before all matched keywords,
namely director, Apricot and Computer.
M7 The retrieved passages are further refined
for enhanced precision. Passages that do not
satisfy the semantic constraints specified in the
question are discarded. For example, some of
the passages retrieved for Q013 do not satisfy
the date constraint 1966. Out of the 60 passages
returned by the retrieval engine for Q013, 2 pas-
sages are retained after passage post-filtering.
M8 The search for answers within the re-
trieved passages is restricted to those candidates
corresponding to the expected answer type. If
the expected answer type is a named entity such
as MONEY, the candidates ($1, USD 520) are
identified with a named entity recognizer. Con-
versely, if the answer type is a DEFINITION, e.g.
Q903: &amp;quot;What is autism?&amp;quot;, the candidates are
obtained by matching a set of answer patterns
on the passages.
M9 Each candidate answer receives a rele-
vance score according to lexical and proximity
features such as distance between keywords, or
the occurrence of the candidate answer within
an apposition. The candidates are sorted in de-
creasing order of their scores.
M10 The system selects the candidate answers
with the highest relevance scores. The final an-
swers are either fragments of text extracted from
the passages around the best candidate answers,
or they are internally generated.
</bodyText>
<sectionHeader confidence="0.519664" genericHeader="method">
4 Error analysis for the baseline
serial system
</sectionHeader>
<subsectionHeader confidence="0.992904">
4.1 Performance experiments
</subsectionHeader>
<bodyText confidence="0.999990833333334">
The system was tested on 1460 questions col-
lected from TREC-8, 9 and TREC-2001. An-
swers were extracted from a 3 Gbyte text collec-
tion containing about 1 million documents from
sources such as Los Angeles Times and Wall
Street Journal. Each answer has 50 bytes.
The accuracy was measured by the Mean Re-
ciprocal Rate (MRR) metric used by NIST in
the TREC QA evaluations (Voorhees, 1999).
The reciprocal ranking basically assigns a num-
ber equal to 1/R where R is the rank of the
correct answer. Only the first 5 answers are con-
sidered, thus R is less or equal to 5. When the
system does not return a correct answer in top 5,
the precision score for that question is zero. The
overall system precision is the mean of the indi-
vidual scores. System answers were measured
against correct answers provided by NIST.
</bodyText>
<subsectionHeader confidence="0.995689">
4.2 Module errors
</subsectionHeader>
<bodyText confidence="0.999985428571429">
The inspection of internal traces, at various
checkpoints inserted after each module from Fig-
ure 1, reveals the system errors for each evalua-
tion question. The goal in this experiment is to
identify the earliest module in the chain (from
left to right) that prevents the system to find
the right answer, i.e. causes the error.
As shown in Table 2, question pre-processing
is responsible for 7.1% of the errors distributed
among module M1 (1.9%) and M2 (5.3%). Most
errors in module M2 are due to incorrect parsing
(4.5%). Two of the ten modules (M3 and M5)
account for more than half of the errors. The
failure of either module makes it hard (or impos-
</bodyText>
<tableCaption confidence="0.976852">
Table 2: Distribution of errors per system module
</tableCaption>
<table confidence="0.999550454545454">
Module Module definition Errors (%)
Keyword pre-processing (split/bind/spell check) 1.9
Construction of internal question representation 5.2
Derivation of expected answer type 36.4
Keyword selection (incorrectly added or excluded) 8.9
Keyword expansion desirable but missing 25.7
Actual retrieval (limit on passage number or size) 1.6
Passage post-filtering (incorrectly discarded) 1.6
Identification of candidate answers 8.0
Answer ranking 6.3
Answer formulation 4.4
</table>
<bodyText confidence="0.999861564102564">
sible) for subsequent modules to perform their
task. Whenever the derivation of the expected
answer type (module M3) fails, the set of candi-
date answers identified in the retrieved passages
is either empty in 28.2% of the cases (when the
answer type is unknown) or contains the wrong
entities for 8.2% (when the answer type is incor-
rect). If the keywords used for passage retrieval
are not expanded with the semantically related
forms occurring in the answers (module M5), the
relevant passages are missed.
The selection of keywords from the internal
question representation (module M4) coupled
with the keyword expansion (module M5) gen-
erate 34.6% of the errors. Both these modules
affect the output of passage retrieval, since the
set of retrieved passages depends on the Boolean
queries built and submitted to the retrieval en-
gine by the QA system.
Modules M6 and M7 are responsible for the
retrieval of passages where answers may actually
occur. Their combined errors is 3.2%. In module
M6 there are parameters to control the number
of retrieved documents and passages, as well as
the size of each passage.
Answer processing is done in modules M8
through M10. When the expected answer type
is correctly detected, the identification of the
candidate answers (module M8) produces 8.0%
errors. 3.1% errors are due to named entity
recognition (incomplete dictionaries) and 4.9%
are due to spurious answer pattern matching.
Modules M9 and M10 fail to rank the correct
answer within the top 5 returned in 10.7% of
the cases. Module M9 fails if the correct an-
swer candidate is not ranked within the top 5,
whereas M10 fails if the returned answer string
is incomplete, namely it does not fit within 50
bytes.
</bodyText>
<subsectionHeader confidence="0.980782">
4.3 Resource errors
</subsectionHeader>
<bodyText confidence="0.99996204">
The second set of experiments consists of dis-
abling the main natural language resources used
in the QA system, namely the access to Word-
Net and the named entity recognizer, to as-
sess their impact on the overall answer accuracy.
Note that the parser is an integral part of our
question processing model and therefore it is im-
practical to disable it.
Denote with b the baseline system perfor-
mance when all resources are enabled. The pre-
cision score (MRR) drops to 0.59b if WordNet
is disabled. The derivation of the answer type
(module M3) and keyword expansion (module
M5) from Figure 1 are the two modules that
are most influenced by WordNet. For exam-
ple, the WordNet noun hierarchies specify that
the concept pilot is a specialization of aviator,
which in turn is a kind of person. The answer
type for Q037: &amp;quot;What was the name of the US
helicopter pilot shot down over North Korea?&amp;quot;
is Person. The system cannot derive the an-
swer type correctly unless it has access to Word-
Net hierarchies because the ambiguous question
stem What alone does not provide any clue as to
what the expected answer type is. A closer anal-
</bodyText>
<figure confidence="0.996602936170213">
0.376
Precision (MRR)
0.42
0.41
NP
NP
NP
= 50
= 200
= 500
0.400
0.419
� �
0.421
0.40
0.392
0.374
0.3
� �
69
�
20 50 200
ND
0.39
0.38
0.37
0.36
�
0.414
0.400
265
0.421
0.411
0.400
precision
time
110
0.401
59
0.387
43
32
SP
(nr. extra lines)
3 6 10 20 40
Precision (MRR)
Exec time (sec)
</figure>
<bodyText confidence="0.4071785">
does not always rank the correct answers within
the top 5 returned.
</bodyText>
<sectionHeader confidence="0.830211" genericHeader="method">
6 Impact of Feedbacks
</sectionHeader>
<bodyText confidence="0.99994265625">
The results presented in previous sections cor-
respond to the serialized baseline architecture
from Figure 1. That architecture is in fact a
simplified version of our system which uses sev-
eral feedbacks to boost the overall performance.
As shown in Figure 4, the architecture with
feedbacks extends the serialized architecture in
several ways. Keyword expansion (module M5)
is enhanced to include lexico-semantic alterna-
tions from WordNet. A new module for logic
proving and justification of the answers is in-
serted before answer ranking. In addition, three
loops become an integral part of the system:
the passage retrieval loop (loop 1); the lexico-
semantic loop (loop 2); and the logic proving
loop (loop 3).
As part of loop 1, the Q/A system adjusts
Boolean queries before passing them to the re-
trieval engine. If the output from the retrieval
engine is too small, a keyword is dropped and
retrieval resumed. If the output is too large,
a keyword is added and a new iteration started,
until the output size is neither too large, nor too
small. When lexico-semantic connections from
the question to the retrieved passages are not
possible, loop 2 is triggered. Question keywords
are replaced with WordNet-based alternations
and retrieval is resumed. Loop 3 relies on a logic
prover that verifies the unifications between the
question and logic forms. When the unifications
fail, the keywords are expanded with semanti-
cally related alternations and retrieval resumes.
</bodyText>
<tableCaption confidence="0.998193">
Table 3: Impact of feedbacks on precision
</tableCaption>
<table confidence="0.999836125">
Feedback Precision Incremental
added (MRR) enhancement
none 0.421=b 0%
Passage retrieval 0.468=b1 b+11%
(loop 1)
Lexico-semantic 0.542=b2 b1+15%
(loop 2)
Proving (loop 3) 0.572=b3
</table>
<bodyText confidence="0.998233233333333">
Table 3 illustrates the impact of the retrieval
loops on the answer accuracy. The knowledge
brought into the question answering process by
lexico-semantic alternations has the highest in-
dividual contribution, followed by the mecha-
nism of adding/dropping keywords.
The insertion of the logic proving module adds
a new complexity layer to answer processing, en-
abling more trade-offs between processing com-
plexity and answer accuracy. Table 4 shows the
overall precision for four different settings. The
first setting, direct extraction, corresponds to the
simplest QA system that does not use any NLP
techniques or resources. The answers are ex-
tracted from the start of each passage, and re-
turned in the order in which the passages were
retrieved. The precision is only 0.028. When
the NLP techniques are enabled, with the ex-
ception of the derivation of the expected answer
type, the precision improves from 0.028 to 0.150.
The answer accuracy is still limited because the
candidate answers cannot be properly identified
without knowing their semantic category (per-
sons, cities and so forth). If the derivation of
the expected answer type is also enabled, the
precision score changes to 0.468. Finally, when
all feedbacks are enabled the highest overall pre-
cision of 0.572 is achieved. Comparatively, the
answer processing modules of other QA systems
usually span over levels 2 and 3 from Table 4.
</bodyText>
<tableCaption confidence="0.993696">
Table 4: Performance of answer processing
</tableCaption>
<figure confidence="0.669515125">
Answer processing Modules Precision
complexity level used (MRR)
(1) Direct extraction M1-M6, 0.028
M10
(2) Lexical matching M1-M7, 0.150
M9-M10
(3) Semantic matching M1-M10 0.468
(4) Feedbacks enabled all 0.572
</figure>
<bodyText confidence="0.9905798">
The final precision scores for TREC-8, TREC-
9 and TREC-2001 are respectively 0.555, 0.580,
and 0.570. Therefore the precision did not vary
much in spite of the higher degree of difficulty.
This is due to the increased use of natural lan-
</bodyText>
<figure confidence="0.9705616">
M1 + M2
+ M3 + M4
Logic
proving
Loop 3
M5
+ lexico−sem
alternations
Loop 2
M6
Loop 1
M7 + M8
Question
M9 + M10
Answer
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000942">
<note confidence="0.792899">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 33-40. analysis (Voorhees, 1999). From the combined</note>
<abstract confidence="0.998998109756097">set of 1460 evaluation questions, 70% of the participating systems answered successfully queslike Q1013: is Perth?&amp;quot;, none could find a correct answer to complex questions as Q1165: is the difference between AM radio stations and FM radio stations?&amp;quot;. Since performance is affected by the complexity of question processing, we first provide a broad taxonomy of QA systems. 2.1 Criteria The taxonomy is based on several criteria that play an important role in building QA systems: (1) linguistic and knowledge resources, (2) natural language processing involved, (3) document processing, (4) reasoning methods, (5) whether or not answer is explicitly stated in a document, (6) whether or not answer fusion is necessary. 2.2 Classes of questions Class I. QA systems capable of processing factual questions These systems extract answers as text snippets from one or more documents. Often the answer is found verbatim in a text or as a simple morphological variation. Typically the answers are extracted using empirical methods relying on keyword manipulations. systems enabling simple reasoning mechanisms The characteristic of this class is that answers are found in snippets of text, but unlike in Class 1, inference is necessary to relate the question with the answer. More elaborate answer detection methods such as ontologies or codification of pragmatic knowledge are necessary. Semantic alternations, world knowledge axioms and simple reasoning methods are necessary. An examis Q198: did Socrates died?&amp;quot; to be linked with poisoned wine. WordNet and its extensions are sometimes used as sources of world knowledge. Class 3. QA systems capable of answer fusion from different documents In this class the partial answer information is scattered throughout several documents and answer fusion is necessary. The complexity here ranges from assembling simple lists to far more complex questions like script questions, (e.g. do I assemble a bicycle?&amp;quot;), templatequestions ( management successions occurred at IBM in the past year?&amp;quot;). Class 4. Interactive QA systems These systems are able to answer questions in the context of previous interactions with the user. As reported in (Harabagiu et al., 2001), processing a list of questions posed in a context involves complex reference resolution. Unlike typical reference resolution algorithms that associate anaphore with a referent, the reference imposed by context questions requires the association of an anaphora from the current question with either one of the previous questions, answers or their anaphora. Class 5. QA systems capable of analogical reasoning The characteristic of these systems is their ability to answer speculative questions similar to: &amp;quot;Is the Fed going to raise interests at their next meeting?&amp;quot;; &amp;quot;Is the US out of recession?&amp;quot;; &amp;quot;Is the airline industry in trouble?&amp;quot;. Since most probably the answer to such questions is not explicitly stated in documents, simply because events may not have happened yet, QA systems from this class decompose the question into queries that extract pieces of evidence, after which answer is formulated using reasoning by analogy. The resources include ad-hoc knowledge bases generated from mining text documents clustered by the question topic. Associated with these knowledge sources are casebased reasoning techniques as well as methods for temporal reasoning, spatial reasoning and evidential reasoning.</abstract>
<note confidence="0.9684365">Table 1: Distribution of TREC questions Type Number (%) Class 1 (factual) 985 (67.5%) Class 2 (simple-reasoning) 408 (27.9%) Class 3 (fusion list) 25 (1.7%) Class 4 (interactive context) 42 (2.9%) Class 5 (speculative) 0 (0.0%) Table 1 illustrates the distribution of TREC</note>
<date confidence="0.826733">1966</date>
<abstract confidence="0.935728569863013">Niagra − Niagara rented − rent Money bug Examples Volkswagen Person bug invented − inventor How much rent Volkswangen − Volkswagen rent built − build Country Volkswagen M1 M2 M3 M4 M5 Question Keyword selection Derivation of expected answer type Keyword expansion Construction of question representation Keyword pre−processing (split/bind/spell) System Modules Identification of candidate answers Answer Answer formulation Answer ranking Retrieval of documents and passages Passage post−filtering M6 M7 M8 M9 M10 Examples &amp;quot;in 1966&amp;quot; USD 520 Volkswagen AND bug 60 passages 2 passages $1 &amp;quot;rent a Volkswagen USD 520 bug for $1 a day&amp;quot; $1 retrieval is insuccessful since the passages containing the correct answers are missed. M5Before the construction of Boolean queries for actual retrieval, the selected keywords are expanded with morphological, lexical or semantic alternations. The alternations correspond to other forms in which the question concepts may in the answers. For example, exinto M6The retrieval engine returns the documents containing all keywords specified in the Boolean queries. The documents are then further restricted to smaller text passages where all keywords are located in the proximity of one another. Each retrieved passage includes additional text (extra lines) before the earliest and after the latest keyword match. For illustration, Q005: is the name of the mandirector of Apricot Computer? the as- Boolean query relevant text fragment from document collection is Peter Horne, director of Apricot Computers&amp;quot;. Unless additional text is included in the passages, actual answer Horne be missed because it occurs before all matched keywords, Apricot M7The retrieved passages are further refined for enhanced precision. Passages that do not satisfy the semantic constraints specified in the question are discarded. For example, some of the passages retrieved for Q013 do not satisfy date constraint of the 60 passages returned by the retrieval engine for Q013, 2 passages are retained after passage post-filtering. M8The search for answers within the retrieved passages is restricted to those candidates corresponding to the expected answer type. If the expected answer type is a named entity such MONEY, the candidates USD 520) identified with a named entity recognizer. Conversely, if the answer type is a DEFINITION, e.g. is autism?&amp;quot;, candidates are obtained by matching a set of answer patterns on the passages. M9Each candidate answer receives a relevance score according to lexical and proximity features such as distance between keywords, or the occurrence of the candidate answer within an apposition. The candidates are sorted in decreasing order of their scores. M10The system selects the candidate answers with the highest relevance scores. The final answers are either fragments of text extracted from the passages around the best candidate answers, or they are internally generated. 4 Error analysis for the baseline serial system 4.1 Performance experiments The system was tested on 1460 questions collected from TREC-8, 9 and TREC-2001. Answers were extracted from a 3 Gbyte text collection containing about 1 million documents from sources such as Los Angeles Times and Wall Street Journal. Each answer has 50 bytes. The accuracy was measured by the Mean Reciprocal Rate (MRR) metric used by NIST in the TREC QA evaluations (Voorhees, 1999). The reciprocal ranking basically assigns a number equal to 1/R where R is the rank of the correct answer. Only the first 5 answers are considered, thus R is less or equal to 5. When the system does not return a correct answer in top 5, the precision score for that question is zero. The overall system precision is the mean of the individual scores. System answers were measured against correct answers provided by NIST. 4.2 Module errors The inspection of internal traces, at various checkpoints inserted after each module from Figure 1, reveals the system errors for each evaluation question. The goal in this experiment is to identify the earliest module in the chain (from left to right) that prevents the system to find the right answer, i.e. causes the error. As shown in Table 2, question pre-processing is responsible for 7.1% of the errors distributed among module M1 (1.9%) and M2 (5.3%). Most errors in module M2 are due to incorrect parsing (4.5%). Two of the ten modules (M3 and M5) account for more than half of the errors. The of either module makes it hard (or impos- Table 2: Distribution of errors per system module Module Module definition Errors (%) Keyword pre-processing (split/bind/spell check) 1.9 Construction of internal question representation 5.2 Derivation of expected answer type 36.4 Keyword selection (incorrectly added or excluded) 8.9 Keyword expansion desirable but missing 25.7 Actual retrieval (limit on passage number or size) 1.6 Passage post-filtering (incorrectly discarded) 1.6 Identification of candidate answers 8.0 Answer ranking 6.3 Answer formulation 4.4 sible) for subsequent modules to perform their task. Whenever the derivation of the expected answer type (module M3) fails, the set of candidate answers identified in the retrieved passages is either empty in 28.2% of the cases (when the answer type is unknown) or contains the wrong entities for 8.2% (when the answer type is incorrect). If the keywords used for passage retrieval are not expanded with the semantically related forms occurring in the answers (module M5), the relevant passages are missed. The selection of keywords from the internal question representation (module M4) coupled with the keyword expansion (module M5) generate 34.6% of the errors. Both these modules affect the output of passage retrieval, since the set of retrieved passages depends on the Boolean queries built and submitted to the retrieval engine by the QA system. Modules M6 and M7 are responsible for the retrieval of passages where answers may actually occur. Their combined errors is 3.2%. In module M6 there are parameters to control the number of retrieved documents and passages, as well as the size of each passage. Answer processing is done in modules M8 through M10. When the expected answer type is correctly detected, the identification of the candidate answers (module M8) produces 8.0% errors. 3.1% errors are due to named entity recognition (incomplete dictionaries) and 4.9% are due to spurious answer pattern matching. Modules M9 and M10 fail to rank the correct answer within the top 5 returned in 10.7% of the cases. Module M9 fails if the correct answer candidate is not ranked within the top 5, whereas M10 fails if the returned answer string is incomplete, namely it does not fit within 50 bytes. 4.3 Resource errors The second set of experiments consists of disabling the main natural language resources used in the QA system, namely the access to Word- Net and the named entity recognizer, to assess their impact on the overall answer accuracy. Note that the parser is an integral part of our question processing model and therefore it is impractical to disable it. with baseline system performance when all resources are enabled. The prescore (MRR) drops to WordNet is disabled. The derivation of the answer type (module M3) and keyword expansion (module M5) from Figure 1 are the two modules that are most influenced by WordNet. For example, the WordNet noun hierarchies specify that concept a specialization of in turn is a kind of answer for Q037: was the name of the US helicopter pilot shot down over North Korea?&amp;quot; Person. system cannot derive the answer type correctly unless it has access to Word- Net hierarchies because the ambiguous question does not provide any clue as to the expected answer type is. A closer anal- 0.376 Precision (MRR) 0.42 0.41 NP NP = 50 = 200 = 500 0.400 0.419 � � 0.421 0.40 0.392 0.374 0.3 � � 69 � 20 50 200 0.39 0.38 0.37 0.36 � 0.414 0.400 265 0.421 0.411 0.400 precision time 110 0.401 59 0.387 43 32 (nr. extra lines) 3 6 10 20 40 Precision (MRR) Exec time (sec) does not always rank the correct answers within the top 5 returned. 6 Impact of Feedbacks The results presented in previous sections correspond to the serialized baseline architecture from Figure 1. That architecture is in fact a simplified version of our system which uses several feedbacks to boost the overall performance. As shown in Figure 4, the architecture with feedbacks extends the serialized architecture in several ways. Keyword expansion (module M5) is enhanced to include lexico-semantic alternations from WordNet. A new module for logic proving and justification of the answers is inserted before answer ranking. In addition, three loops become an integral part of the system: the passage retrieval loop (loop 1); the lexicosemantic loop (loop 2); and the logic proving loop (loop 3). As part of loop 1, the Q/A system adjusts Boolean queries before passing them to the retrieval engine. If the output from the retrieval engine is too small, a keyword is dropped and retrieval resumed. If the output is too large, a keyword is added and a new iteration started, until the output size is neither too large, nor too small. When lexico-semantic connections from the question to the retrieved passages are not possible, loop 2 is triggered. Question keywords are replaced with WordNet-based alternations and retrieval is resumed. Loop 3 relies on a logic prover that verifies the unifications between the question and logic forms. When the unifications fail, the keywords are expanded with semantically related alternations and retrieval resumes. Table 3: Impact of feedbacks on precision Feedback Precision (MRR) Incremental enhancement added none 0.421=b 0% Passage retrieval (loop 1) 0.468=b1 b+11% Lexico-semantic 0.542=b2 b1+15% (loop 2) Proving (loop 3) 0.572=b3 Table 3 illustrates the impact of the retrieval loops on the answer accuracy. The knowledge brought into the question answering process by lexico-semantic alternations has the highest individual contribution, followed by the mechanism of adding/dropping keywords. The insertion of the logic proving module adds a new complexity layer to answer processing, enabling more trade-offs between processing complexity and answer accuracy. Table 4 shows the overall precision for four different settings. The setting, extraction, to the simplest QA system that does not use any NLP techniques or resources. The answers are extracted from the start of each passage, and returned in the order in which the passages were retrieved. The precision is only 0.028. When the NLP techniques are enabled, with the exception of the derivation of the expected answer type, the precision improves from 0.028 to 0.150. The answer accuracy is still limited because the candidate answers cannot be properly identified without knowing their semantic category (persons, cities and so forth). If the derivation of the expected answer type is also enabled, the precision score changes to 0.468. Finally, when all feedbacks are enabled the highest overall precision of 0.572 is achieved. Comparatively, the answer processing modules of other QA systems usually span over levels 2 and 3 from Table 4. Table 4: Performance of answer processing Answer processing complexity level Modules used Precision (MRR) (1) Direct extraction M1-M6, M10 0.028 (2) Lexical matching M1-M7, M9-M10 0.150 (3) Semantic matching M1-M10 0.468 (4) Feedbacks enabled all 0.572 The final precision scores for TREC-8, TREC- 9 and TREC-2001 are respectively 0.555, 0.580, and 0.570. Therefore the precision did not vary much in spite of the higher degree of difficulty. is due to the increased use of natural lan- M1 + M2 + M3 + M4 Logic proving Loop 3 M5 + lexico−sem alternations Loop 2 M6 Loop 1 M7 + M8 Question M9 + M10</abstract>
<intro confidence="0.811124">Answer</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>