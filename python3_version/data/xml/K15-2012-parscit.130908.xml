<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007228">
<title confidence="0.986781">
Improving a Pipeline Architecture for Shallow Discourse Parsing
</title>
<author confidence="0.994319">
Yangqiu Song Haoruo Peng Parisa Kordjamshidi Mark Sammons Dan Roth
</author>
<affiliation confidence="0.991336">
Cognitive Computation Group, University of Illinois at Urbana-Champaign
</affiliation>
<email confidence="0.993855">
fyqsong,hpeng7,kordjam,mssammon,danrl@illinois.edu
</email>
<sectionHeader confidence="0.993774" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982833333333">
We present a system that implements an
end-to-end discourse parser. The system
uses a pipeline architecture with seven
stages: preprocessing, recognizing ex-
plicit connectives, identifying argument
positions, identifying and labeling argu-
ments, classifying explicit and implicit
connectives, and identifying attribution
structures. The discourse structure of a
document is inferred based on these com-
ponents. For NLP analysis, we use Illinois
NLP software1 and the Stanford Parser.
We use lexical and semantic features based
on function words, sentiment lexicons,
brown clusters, and polarity features. Our
system achieves an F1 score of 0.2492 in
overall performance on the development
set and 0.1798 on the blind test set.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999571583333334">
The Illinois discourse parsing system builds on ex-
isting approaches, using a series of classifiers to
identify different elements of discourse structures
such as argument boundaries and types along with
discourse connectives and senses. In developing
the components of this pipeline, we investigated
different kinds of features to try to improve ab-
straction while retaining sufficient expressivity. To
that end, we investigated a combination of parse
and lexical (function word) features; brown clus-
ters; and relations between verb-argument struc-
tures in consecutive sentences.
</bodyText>
<sectionHeader confidence="0.991933" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.9914525">
In this section, we describe the system we de-
veloped, and introduce the features we used in
</bodyText>
<footnote confidence="0.9747055">
1http://cogcomp.cs.illinois.edu/page/
software
</footnote>
<bodyText confidence="0.999871071428571">
each component. For our starting point, we im-
plemented a pipeline architecture based on the de-
scription in Lin et al. (2014), then investigated
features and inference approaches to improve the
system. The pipeline includes seven components.
After a preprocessing step, the system identifies
explicit connectives, determines the positions of
the arguments relative to the connective, identifies
and labels arguments, classifies explicit and im-
plicit connectives, and identifies attribution struc-
tures. The system architecture is presented in Fig-
ure 1. All the classifiers are built based on LibLin-
ear (Fan et al., 2008) via the interface of Learning
Based Java (LBJava) (Rizzolo and Roth, 2010).
</bodyText>
<subsectionHeader confidence="0.959359">
2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.9998823">
The preprocessing stage identifies tokens,
sentences, part-of-speech (Roth and Zelenko,
1998), shallow parse chunks (Punyakanok and
Roth, 2001), lemmas2, syntactic constituency
parse (Klein and Manning, 2003), and depen-
dency parse (de Marneffe et al., 2006). This stage
also generates a mapping from our own token
indexes to those provided in the gold standard
data, to allow evaluation with the official shared
task scoring code.
</bodyText>
<subsectionHeader confidence="0.999885">
2.2 Recognizing Explicit Connectives
</subsectionHeader>
<bodyText confidence="0.998875818181818">
To recognize explicit connectives, we construct
a list of existing connectives labeled in the Penn
Discourse Treebank (Prasad et al., 2008a). Since
not all the words of the connective list are neces-
sarily true connectives when they appear in text,
we build a binary classifier to determine when a
word matching an entry in the list represents an
actual connective. We only focus on the con-
nectives with consecutive tokens and ignore the
non-consecutive connectives. We generate lexico-
syntactic and path features associated with the
</bodyText>
<footnote confidence="0.9903485">
2http://cogcomp.cs.illinois.edu/page/
software_view/illinois-lemmatizer
</footnote>
<page confidence="0.976573">
78
</page>
<note confidence="0.935765">
Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 78–83,
Beijing, China, July 26-31, 2015. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999919">
Figure 1: System architecture
</figureCaption>
<bodyText confidence="0.890096">
connectives following Lin et al. (2014).
</bodyText>
<subsectionHeader confidence="0.998643">
2.3 Identifying Arguments and Argument
Positions
</subsectionHeader>
<bodyText confidence="0.999980625">
For each explicit connective, we first identify the
relevant argument positions – whether Arg1 is in
the previous sentence relative to the connective or
in the same sentence. We build a classifier incor-
porating the contextual lexico-syntactic features.
We then detect the spans of Arg1 and Arg2 based
on these two decisions. The features used are sim-
iliar to those of Lin et al. (2014).
To generate candidate arguments, we enumer-
ate all subtrees in the parse tree of each sentence.
We then classify the subtrees to be Arg1, Arg2, or
None. In addition to the features used by Lin et al.
(2014), we also add function words to the path fea-
tures. If we detect a word which is in the lexicon
of function words, we replace the corresponding
tag used in the path with the word’s surface string.
</bodyText>
<subsectionHeader confidence="0.999036">
2.4 Classifying Explicit Connectives
</subsectionHeader>
<bodyText confidence="0.999994">
To classify explicit connectives, we build a multi-
class classifier to identify the senses. In addition
to the features used by Lin et al. (2014), we also
incorporate Brown cluster (Brown et al., 1992)
features generated by Liang (2005). The Brown
clustering algorithm produces a binary tree, where
each word can be uniquely identified by its path
from the root, and this path can be compactly rep-
resented with a bit string. Different lengths of
the prefix of this root-to-leaf path provide different
levels of word abstraction. In this implementation,
we set the length of the prefix to 6.
</bodyText>
<subsectionHeader confidence="0.996153">
2.5 Classifying Implicit Connectives
</subsectionHeader>
<bodyText confidence="0.999923555555556">
We classify the sense of implicit connectives based
on four sets of features. We follow Lin et al.
(2014) to generate the product rules of both con-
stituent parse and dependency parse features, and
to generate the word pair features to enumerate all
the word pairs in each pair of sentences. In ad-
dition, similar to Rutherford and Xue (2014), we
incorporate Brown cluster pairs by replacing each
word with its Brown cluster prefixes (length of 4)
in the way described in Section 2.4. We also use
another rich source of information - the polarity
of context, which has been previously shown to
be useful for coreference problems (Peng et al.,
2015). We extract polarity information using the
data provided by Wilson et al. (2005) given the
predicates of two discourse arguments. The data
contains 8221 words, each of which is labeled with
a polarity of positive, negative, or neutral. We use
</bodyText>
<page confidence="0.99145">
79
</page>
<bodyText confidence="0.999976375">
the extracted polarity values to construct three fea-
tures: Two individual polarities of the two predi-
cates and the conjuction of them. We also imple-
ment feature selection to remove the features that
are active in the corpus less than five times. As a
result, we have 16,989 parse tree features, 4,335
dependency tree features, 77,677 word pair fea-
tures, and 67,204 Brown cluster features.
</bodyText>
<subsectionHeader confidence="0.997342">
2.6 Identifying Attribution Structures
</subsectionHeader>
<bodyText confidence="0.999819266666667">
We train two classifiers for attribution identifica-
tion based on the original PDTB data (sections 2-
21). The first classifier is similar to that developed
by Lin et al. (2014). We use the patterns proposed
by Skadhauge and Hardt (2005) to enumerate all
the candidate attribution spans. The coverage of
our implementation is 59.8%, which means that
we can only enumerate the candidates that con-
tain the attribution covering 59.8% of the annota-
tion. We then build a classifier following Lin et al.
(2014) to decide whether the candidate is a valid
attribution or not. Using the sentences that con-
tain the attribution(s), we also train a tagger. The
tagger is designed following the features used in
Illinois Chunker (Punyakanok and Roth, 2001).
</bodyText>
<sectionHeader confidence="0.986653" genericHeader="method">
3 Evaluation and Results
</sectionHeader>
<bodyText confidence="0.999969166666667">
In this section, we present the data we used and
results of the evaluation based on both cross-
validation on the training data (computed within
our software) and using the official CoNLL 2015
Shared Task evaluation framework of Xue et al.
(2015).
</bodyText>
<subsectionHeader confidence="0.996883">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999904">
The data we used is provided through the CoNLL-
2015 shared task (Xue et al., 2015), which
is a modification of Penn Discourse Treebank
(PDTB) (Prasad et al., 2008b) sections 2 through
21. The training data for attribution identification
is obtained from the original PDTB release, also
sections 2 through 21.
</bodyText>
<subsectionHeader confidence="0.999488">
3.2 Cross-Validation Results
</subsectionHeader>
<bodyText confidence="0.999769333333333">
We first present the cross validation results for
each component using the training data (Table 1).
All the results are averaged over 10-fold cross val-
idation of all the examples we generated, using
our own predicted features and our own evaluation
code. Each component is evaluated in isolation,
assuming the inputs are from gold data (for ex-
ample: for connective classification, it is assumed
we have the correct connective and arguments pro-
vided as input). This means that these results are
higher than they would be if evaluated using in-
puts generated by previous stages of the system,
although in this evaluation they still use the pre-
dicted part-of-speech, chunk, and parse informa-
tion.
</bodyText>
<tableCaption confidence="0.9792835">
Table 1: Cross validation results of all the compo-
nents.
</tableCaption>
<table confidence="0.999949454545455">
P R F1
Explicit Connectives 92.97 93.91 93.44
Argument Positions 98.15 98.15 98.15
Exact Arg1 64.41 64.95 64.68
Exact Arg2 87.06 86.06 86.56
Partial Arg1 77.02 77.66 77.34
Partial Arg2 94.74 93.65 94.19
Explicit Sense 83.18 83.18 83.18
Implicit Sense 34.58 34.58 34.58
Attribution Identification 82.94 58.02 68.27
Attribution Tagger 59.75 56.49 58.08
</table>
<subsectionHeader confidence="0.996324">
3.3 CoNLL Results
</subsectionHeader>
<bodyText confidence="0.999984631578947">
We also present the results evaluated by the
CoNLL-2015 shared task scorer on dev, test, and
blind sets in Tables 2, 3, and 4. The results are
consistent with the cross validation results, al-
lowing for the fact that components are working
with predicted inputs rather than gold annotations.
The sense performance reported here is the macro-
average of explicit and implicit senses.
Compared with the best results on the blind set,
which is shown in Table 5, our main weaknesses
lie in the sense classifier and argument detector.
Since we presently ignore the disjoint connectives,
our results can be improved if we incorporate
those missing connectives. We do not presently in-
corporate the argument information in connective
detection and sense classification for the explicit
parser. Connective detection and classification can
be improved if we also incorporate more features
from arguments or perform joint learning.
</bodyText>
<sectionHeader confidence="0.999761" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.998903333333333">
In this section we point to some types of errors in
our system’s predictions and the complications of
working on the provided corpus.
</bodyText>
<page confidence="0.997662">
80
</page>
<tableCaption confidence="0.988352">
Table 2: CoNLL results on dev set.
</tableCaption>
<table confidence="0.999968285714286">
P R F1
Explicit connective 93.27 89.71 91.45
Exact Arg1 50.88 56.62 53.59
Exact Arg2 62.27 69.29 65.59
Exact Arg1 &amp; Arg2 41.24 45.89 43.44
Sense 33.88 17.87 21.27
Parser 23.65 26.32 24.92
</table>
<tableCaption confidence="0.961624">
Table 3: CoNLL results on test set.
</tableCaption>
<table confidence="0.999969571428571">
P R F1
Explicit connective 92.33 91.33 91.83
Exact Arg1 45.93 52.71 49.09
Exact Arg2 58.97 67.66 63.02
Exact Arg1 &amp; Arg2 35.73 41.00 38.18
Sense 27.01 17.54 15.72
Parser 18.97 21.76 20.27
</table>
<tableCaption confidence="0.927772">
Table 4: CoNLL results on blind set.
</tableCaption>
<table confidence="0.999977428571429">
P R F1
Explicit connective 89.11 86.87 87.98
Exact Arg1 49.52 51.61 50.55
Exact Arg2 66.83 69.64 68.21
Exact Arg1 &amp; Arg2 40.48 42.18 41.31
Sense 21.02 16.81 16.49
Parser 17.62 18.36 17.98
</table>
<tableCaption confidence="0.99035">
Table 5: CoNLL best results on blind set.
</tableCaption>
<table confidence="0.999771857142857">
P R F1
Explicit connective 93.48 90.29 91.86
Exact Arg1 55.12 56.58 55.84
Exact Arg2 73.49 75.43 74.45
Exact Arg1 &amp; Arg2 45.77 46.98 46.37
Sense 23.29 20.56 20.27
Parser 23.69 24.32 24.00
</table>
<subsectionHeader confidence="0.80765">
4.1 Error Analysis
</subsectionHeader>
<bodyText confidence="0.998764545454545">
We provide examples of the errors in the systems’s
predictions that show where the system can be im-
proved, but also some that may indicate possible
improvements in the task annotations themselves.
The argument boundaries in our system are pre-
dicted based on parse tree constituents. Some mis-
takes occur when an argument is non-contiguous,
and some extraneous content is included. How-
ever, the UI-CCG system also generated correct
arguments with erroneous token offsets due to to-
kenization differences.
</bodyText>
<subsectionHeader confidence="0.864446">
Predicted Argument:
</subsectionHeader>
<bodyText confidence="0.9947785">
Golden West Financial Corp. , riding above the turbu-
lence that has troubled most of the thrift industry , posted
a 16 % increase of third-quarter earnings to $41.8 millon
, or 66 cents a share .
</bodyText>
<subsectionHeader confidence="0.985385">
Gold Argument:
</subsectionHeader>
<bodyText confidence="0.9930105">
Golden West Financial Corp posted a 16% increase of
third-quarter earnings to $41.8 millon, or 66 cents a share
Although the shared task specification indicates
that attribution detection is not evaluated, our re-
sults suggest that it is helpful for some cases in
order to obtain the correct argument boundaries.
</bodyText>
<subsectionHeader confidence="0.986376">
Predicted Argument:
</subsectionHeader>
<bodyText confidence="0.99835875">
In savings activity , Mr. Sandler said consumer deposits
have enjoyed a steady increase throughout 1989 , and
topped $11 billion at quarter ’s end for the first time in
the company ’s history.
</bodyText>
<subsectionHeader confidence="0.980574">
Gold Argument:
</subsectionHeader>
<bodyText confidence="0.995024428571429">
consumer deposits have enjoyed a steady increase
throughout 1989, and topped $11 billion at quarter’s end
for the first time in the company’s history
In some cases, there seems to be a legitimate
alternative explanation. The issue of whether con-
nectives should be included in arguments seems
somewhat hard to pin down.
</bodyText>
<subsectionHeader confidence="0.595113">
Prediction:
</subsectionHeader>
<bodyText confidence="0.953124">
argument] then giving up some of its gains
connective/sense as (Contingency.Cause.Reason)
argument2 the dollar recovered
</bodyText>
<subsectionHeader confidence="0.355538">
Gold:
</subsectionHeader>
<bodyText confidence="0.846243733333333">
argument] and then giving up some of its gains
connective/sense as (Temporal.Synchrony)
argument2 the dollar recovered
We also found examples where it is hard to
make sense of the gold token offsets.
Prediction – argument 1:
text: The more active December delivery gold settled
with a gain of $3.90 an ounce at $371.20
tokens: 267, 268, 269, 270, 271, 272, 273, 274, 275,
276, 277, 278, 279, 280, 281, 282
Gold – argument 1:
text: The more active December delivery gold settled
with a gain of $3.90 an ounce at $371.20
tokens: 267, 268, 269, 270, 271, 272, 273, 274, 275,
276, 277, 278, 279, 280, 281, 282, 283, 284
</bodyText>
<subsectionHeader confidence="0.96283">
4.2 Task-specific Complications
</subsectionHeader>
<bodyText confidence="0.999970333333333">
Since we wanted to use our own NLP software
and to build a general-purpose system that will
process raw text from new sources, we parsed the
raw text files provided for the PDTB data. How-
ever, the shared task formulation requires outputs
to be specified in terms of token indexes. We
must therefore align our tokenization to that of the
task-provided parse files, which were based on the
gold tokenization of the Penn Treebank. We found
</bodyText>
<page confidence="0.996742">
81
</page>
<bodyText confidence="0.99963575">
some disagreements in tokenization decisions by
our NLP tools with the gold standard which intro-
duced unwelcome complications and consequent
errors. Possibly, character spans from the original
text might provide an accurate but less constrain-
ing basis for evaluating system outputs, although
the gold standard tokenization appears to omit ter-
minal periods from abbreviated words.
</bodyText>
<sectionHeader confidence="0.925478" genericHeader="method">
5 Extending the Submitted System
</sectionHeader>
<bodyText confidence="0.999992555555555">
Comparing the experimental results of other par-
ticipating systems in the shared task to our model,
it seems that sense disambiguation is the critical
step in our pipeline that requires further improve-
ments. We designed an initial model for making
global decisions for the sequence of senses that
can occur in a paragraph. The idea is to con-
sider the label of the neighboring senses when
predicting the sense of any implicit or explicit
discourse relation candidate. To implement this
idea we designed a constrained conditional model
(CCM) (Chang et al., 2012) in LBJava. In this
model two basic classifiers are trained. A first
classifier, C1, is trained to predict the sense of
each candidate explicit/implicit relation and a sec-
ond classifier C2 is trained to predict the cooccur-
rence of the senses of any neighboring pair of can-
didate explicit/implicit relations. These classifiers
are trained independently using features similar to
those in the previous pipeline model. At predic-
tion time, using C1 and C2 we make a global de-
cision for the whole paragraph by modeling the
component decisions as a sequence tagging task
formulated as a CCM. In this model, the sequen-
tial joint prediction is made by adding two global
constraints on the predictions made by C1 and C2:
a) C2 is applied on all pairs of neighboring rela-
tion candidates contained in a paragraph and the
label assignments to any pair should be consistent
with the label assignments made by C1 to the re-
lations in that pair. b) For any two neighboring
pairs in a paragraph that share a relation, the la-
bel assignments should be consistent; that is, if a
pair p1 contains relation i and relation i + 1 and
the next pair p2 contains relation i+1 and relation
i + 2 then the assignments to the shared relation,
i + 1, should be the same. This constraint should
hold for the whole paragraph. Using this model,
we consider both emission and transition factors in
making a global decision for the whole sequence
of senses in a paragraph. However, this initial ex-
periment on joint inference did not yield a signif-
icant improvement when tested on the sense pre-
diction layer given all ground-truth labels of the
previous layers in the pipeline.
</bodyText>
<sectionHeader confidence="0.996177" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999913647058824">
We have built a reasonably effective discourse
parser using a pipeline architecture, and identified
some features that improve performance over the
previous reported state-of-the-art, including fea-
tures based on Brown Clusters and on argument
polarity information. We have also begun inves-
tigating the use of constrained conditional models
for global inference.
Two natural extensions are: a) Improved fea-
tures for sense classification. Our sense classifi-
cation accuracy is relatively low. We need to im-
prove the features we extract from the candidate
arguments, and ideally these will reflect a higher
level of semantic abstraction than the brown clus-
ter features we used here. b) Global inference over
multiple component decisions using Constrained
Conditional Models.
</bodyText>
<sectionHeader confidence="0.997473" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999852">
This work builds on a core implementation devel-
oped by Yee Seng Chan. We thank the reviewers
for their helpful advice, and the Shared Task orga-
nizers for their hard work and support. This work
is supported by DARPA under agreement num-
ber FA8750-13-2-0008; by grant 1U54GM114838
awarded by NIGMS through funds provided by
the trans-NIH Big Data to Knowledge (BD2K)
initiative (www.bd2k. nih.gov); and by the Mul-
timodal Information Access &amp; Synthesis Center
at UIUC, part of CCICADA, a DHS Science and
Technology Center of Excellence. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprints for Governmental purposes notwithstand-
ing any copyright notation thereon. The views and
conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily
representing the official policies or endorsements,
either expressed or implied, of any of the organi-
zations that supported the work.
</bodyText>
<sectionHeader confidence="0.998976" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99678525">
P. Brown, V. Della Pietra, P. deSouza, J. Lai, and
R. Mercer. 1992. Class-based n-gram models
of natural language. Computational Linguistics,
18(4):467–479.
</reference>
<page confidence="0.992079">
82
</page>
<reference confidence="0.999538962025316">
M. Chang, L. Ratinov, and D. Roth. 2012. Structured
learning with constrained conditional models. Ma-
chine Learning, 88(3):399–431, 6.
M. de Marneffe, B. MacCartney, and C. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of the Fifth
International Conference on Language Resources
and Evaluation (LREC’06).
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. Liblinear: A library for large lin-
ear classification. Journal of Machine Learning Re-
search, 9:1871–1874.
D. Klein and C. D. Manning. 2003. Fast exact in-
ference with a factored model for natural language
parsing. In The Conference on Advances in Neural
Information Processing Systems (NIPS), volume 15.
MIT Press.
P. Liang. 2005. Semi-supervised learning for natural
language. Master’s thesis, Massachusetts Institute
of Technology.
Z. Lin, H. T. Ng, and M.-Y. Kan. 2014. A PDTB-
styled end-to-end discourse parser. Natural Lan-
guage Engineering, 20(2):151–184.
H. Peng, D. Khashabi, and D. Roth. 2015. Solving
hard coreference problems. In Proceedings of the
2015 Conference of the North American Chapter of
the Association for Computational Linguistics, Den-
ver, Colorado, June. Association for Computational
Linguistics.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. 2008a. The
penn discourse treebank 2.0. In Proceedings of
the Sixth International Conference on Language Re-
sources and Evaluation (LREC’08).
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. 2008b.
The penn discourse treebank 2.0. In Nico-
letta Calzolari (Conference Chair), Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odijk, Ste-
lios Piperidis, and Daniel Tapias, editors, Pro-
ceedings of the Sixth International Conference on
Language Resources and Evaluation (LREC’08),
Marrakech, Morocco, May. European Language
Resources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
V. Punyakanok and D. Roth. 2001. The use of clas-
sifiers in sequential inference. In Proc. of the Con-
ference on Neural Information Processing Systems
(NIPS), pages 995–1001. MIT Press.
N. Rizzolo and D. Roth. 2010. Learning based java for
rapid development of nlp systems. In Proc. of the In-
ternational Conference on Language Resources and
Evaluation (LREC), Valletta, Malta, 5.
D. Roth and D. Zelenko. 1998. Part of speech tag-
ging using a network of linear separators. In Coling-
Acl, The 17th International Conference on Compu-
tational Linguistics, pages 1136–1142.
A. Rutherford and N. Xue. 2014. Discovering implicit
discourse relations through brown cluster pair rep-
resentation and coreference patterns. In Proceed-
ings of the 14th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 645–654. Association for Computational Lin-
guistics.
P. R. Skadhauge and D. Hardt. 2005. Syntactic iden-
tification of attribution in the rst treebank. In Pro-
ceedings of the Sixth International Workshop on Lin-
guistically Interpreted Corpora (LINC-2005).
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
J. Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Pat-
wardhan. 2005. Opinionfinder: A system for sub-
jectivity analysis. In Proceedings of HLT/EMNLP
on interactive demonstrations, pages 34–35. Asso-
ciation for Computational Linguistics.
N. Xue, H. T. Ng, S. Pradhan, R. Prasad, C. Bryant, and
A. Rutherford. 2015. The conll-2015 shared task
on shallow discourse parsing. In Proceedings of the
Nineteenth Conference on Computational Natural
Language Learning: Shared Task, Beijing, China.
</reference>
<page confidence="0.999299">
83
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.473771">
<title confidence="0.99991">Improving a Pipeline Architecture for Shallow Discourse Parsing</title>
<author confidence="0.997911">Yangqiu Song Haoruo Peng Parisa Kordjamshidi Mark Sammons Dan Roth</author>
<affiliation confidence="0.500408">Cognitive Computation Group, University of Illinois at</affiliation>
<abstract confidence="0.997155947368421">We present a system that implements an end-to-end discourse parser. The system uses a pipeline architecture with seven stages: preprocessing, recognizing explicit connectives, identifying argument positions, identifying and labeling arguments, classifying explicit and implicit connectives, and identifying attribution structures. The discourse structure of a document is inferred based on these components. For NLP analysis, we use Illinois the Stanford Parser. We use lexical and semantic features based on function words, sentiment lexicons, brown clusters, and polarity features. Our system achieves an F1 score of 0.2492 in overall performance on the development set and 0.1798 on the blind test set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>V Della Pietra</author>
<author>P deSouza</author>
<author>J Lai</author>
<author>R Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="4867" citStr="Brown et al., 1992" startWordPosition="717" endWordPosition="720">te arguments, we enumerate all subtrees in the parse tree of each sentence. We then classify the subtrees to be Arg1, Arg2, or None. In addition to the features used by Lin et al. (2014), we also add function words to the path features. If we detect a word which is in the lexicon of function words, we replace the corresponding tag used in the path with the word’s surface string. 2.4 Classifying Explicit Connectives To classify explicit connectives, we build a multiclass classifier to identify the senses. In addition to the features used by Lin et al. (2014), we also incorporate Brown cluster (Brown et al., 1992) features generated by Liang (2005). The Brown clustering algorithm produces a binary tree, where each word can be uniquely identified by its path from the root, and this path can be compactly represented with a bit string. Different lengths of the prefix of this root-to-leaf path provide different levels of word abstraction. In this implementation, we set the length of the prefix to 6. 2.5 Classifying Implicit Connectives We classify the sense of implicit connectives based on four sets of features. We follow Lin et al. (2014) to generate the product rules of both constituent parse and depende</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>P. Brown, V. Della Pietra, P. deSouza, J. Lai, and R. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chang</author>
<author>L Ratinov</author>
<author>D Roth</author>
</authors>
<title>Structured learning with constrained conditional models.</title>
<date>2012</date>
<booktitle>Machine Learning,</booktitle>
<volume>88</volume>
<issue>3</issue>
<pages>6</pages>
<contexts>
<context position="14874" citStr="Chang et al., 2012" startWordPosition="2367" endWordPosition="2370">ds from abbreviated words. 5 Extending the Submitted System Comparing the experimental results of other participating systems in the shared task to our model, it seems that sense disambiguation is the critical step in our pipeline that requires further improvements. We designed an initial model for making global decisions for the sequence of senses that can occur in a paragraph. The idea is to consider the label of the neighboring senses when predicting the sense of any implicit or explicit discourse relation candidate. To implement this idea we designed a constrained conditional model (CCM) (Chang et al., 2012) in LBJava. In this model two basic classifiers are trained. A first classifier, C1, is trained to predict the sense of each candidate explicit/implicit relation and a second classifier C2 is trained to predict the cooccurrence of the senses of any neighboring pair of candidate explicit/implicit relations. These classifiers are trained independently using features similar to those in the previous pipeline model. At prediction time, using C1 and C2 we make a global decision for the whole paragraph by modeling the component decisions as a sequence tagging task formulated as a CCM. In this model,</context>
</contexts>
<marker>Chang, Ratinov, Roth, 2012</marker>
<rawString>M. Chang, L. Ratinov, and D. Roth. 2012. Structured learning with constrained conditional models. Machine Learning, 88(3):399–431, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M de Marneffe</author>
<author>B MacCartney</author>
<author>C Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC’06).</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>M. de Marneffe, B. MacCartney, and C. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC’06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R-E Fan</author>
<author>K-W Chang</author>
<author>C-J Hsieh</author>
<author>X-R Wang</author>
<author>C-J Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="2358" citStr="Fan et al., 2008" startWordPosition="329" endWordPosition="332">ach component. For our starting point, we implemented a pipeline architecture based on the description in Lin et al. (2014), then investigated features and inference approaches to improve the system. The pipeline includes seven components. After a preprocessing step, the system identifies explicit connectives, determines the positions of the arguments relative to the connective, identifies and labels arguments, classifies explicit and implicit connectives, and identifies attribution structures. The system architecture is presented in Figure 1. All the classifiers are built based on LibLinear (Fan et al., 2008) via the interface of Learning Based Java (LBJava) (Rizzolo and Roth, 2010). 2.1 Preprocessing The preprocessing stage identifies tokens, sentences, part-of-speech (Roth and Zelenko, 1998), shallow parse chunks (Punyakanok and Roth, 2001), lemmas2, syntactic constituency parse (Klein and Manning, 2003), and dependency parse (de Marneffe et al., 2006). This stage also generates a mapping from our own token indexes to those provided in the gold standard data, to allow evaluation with the official shared task scoring code. 2.2 Recognizing Explicit Connectives To recognize explicit connectives, we</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. 2008. Liblinear: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In The Conference on Advances in Neural Information Processing Systems (NIPS),</booktitle>
<volume>15</volume>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2661" citStr="Klein and Manning, 2003" startWordPosition="369" endWordPosition="372">it connectives, determines the positions of the arguments relative to the connective, identifies and labels arguments, classifies explicit and implicit connectives, and identifies attribution structures. The system architecture is presented in Figure 1. All the classifiers are built based on LibLinear (Fan et al., 2008) via the interface of Learning Based Java (LBJava) (Rizzolo and Roth, 2010). 2.1 Preprocessing The preprocessing stage identifies tokens, sentences, part-of-speech (Roth and Zelenko, 1998), shallow parse chunks (Punyakanok and Roth, 2001), lemmas2, syntactic constituency parse (Klein and Manning, 2003), and dependency parse (de Marneffe et al., 2006). This stage also generates a mapping from our own token indexes to those provided in the gold standard data, to allow evaluation with the official shared task scoring code. 2.2 Recognizing Explicit Connectives To recognize explicit connectives, we construct a list of existing connectives labeled in the Penn Discourse Treebank (Prasad et al., 2008a). Since not all the words of the connective list are necessarily true connectives when they appear in text, we build a binary classifier to determine when a word matching an entry in the list represen</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. In The Conference on Advances in Neural Information Processing Systems (NIPS), volume 15. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
</authors>
<title>Semi-supervised learning for natural language. Master’s thesis,</title>
<date>2005</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="4902" citStr="Liang (2005)" startWordPosition="724" endWordPosition="725"> the parse tree of each sentence. We then classify the subtrees to be Arg1, Arg2, or None. In addition to the features used by Lin et al. (2014), we also add function words to the path features. If we detect a word which is in the lexicon of function words, we replace the corresponding tag used in the path with the word’s surface string. 2.4 Classifying Explicit Connectives To classify explicit connectives, we build a multiclass classifier to identify the senses. In addition to the features used by Lin et al. (2014), we also incorporate Brown cluster (Brown et al., 1992) features generated by Liang (2005). The Brown clustering algorithm produces a binary tree, where each word can be uniquely identified by its path from the root, and this path can be compactly represented with a bit string. Different lengths of the prefix of this root-to-leaf path provide different levels of word abstraction. In this implementation, we set the length of the prefix to 6. 2.5 Classifying Implicit Connectives We classify the sense of implicit connectives based on four sets of features. We follow Lin et al. (2014) to generate the product rules of both constituent parse and dependency parse features, and to generate</context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>P. Liang. 2005. Semi-supervised learning for natural language. Master’s thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Lin</author>
<author>H T Ng</author>
<author>M-Y Kan</author>
</authors>
<title>A PDTBstyled end-to-end discourse parser.</title>
<date>2014</date>
<journal>Natural Language Engineering,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="1864" citStr="Lin et al. (2014)" startWordPosition="258" endWordPosition="261">oping the components of this pipeline, we investigated different kinds of features to try to improve abstraction while retaining sufficient expressivity. To that end, we investigated a combination of parse and lexical (function word) features; brown clusters; and relations between verb-argument structures in consecutive sentences. 2 System Description In this section, we describe the system we developed, and introduce the features we used in 1http://cogcomp.cs.illinois.edu/page/ software each component. For our starting point, we implemented a pipeline architecture based on the description in Lin et al. (2014), then investigated features and inference approaches to improve the system. The pipeline includes seven components. After a preprocessing step, the system identifies explicit connectives, determines the positions of the arguments relative to the connective, identifies and labels arguments, classifies explicit and implicit connectives, and identifies attribution structures. The system architecture is presented in Figure 1. All the classifiers are built based on LibLinear (Fan et al., 2008) via the interface of Learning Based Java (LBJava) (Rizzolo and Roth, 2010). 2.1 Preprocessing The preproc</context>
<context position="3791" citStr="Lin et al. (2014)" startWordPosition="533" endWordPosition="536">ild a binary classifier to determine when a word matching an entry in the list represents an actual connective. We only focus on the connectives with consecutive tokens and ignore the non-consecutive connectives. We generate lexicosyntactic and path features associated with the 2http://cogcomp.cs.illinois.edu/page/ software_view/illinois-lemmatizer 78 Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 78–83, Beijing, China, July 26-31, 2015. c�2014 Association for Computational Linguistics Figure 1: System architecture connectives following Lin et al. (2014). 2.3 Identifying Arguments and Argument Positions For each explicit connective, we first identify the relevant argument positions – whether Arg1 is in the previous sentence relative to the connective or in the same sentence. We build a classifier incorporating the contextual lexico-syntactic features. We then detect the spans of Arg1 and Arg2 based on these two decisions. The features used are similiar to those of Lin et al. (2014). To generate candidate arguments, we enumerate all subtrees in the parse tree of each sentence. We then classify the subtrees to be Arg1, Arg2, or None. In additio</context>
<context position="5399" citStr="Lin et al. (2014)" startWordPosition="805" endWordPosition="808">res used by Lin et al. (2014), we also incorporate Brown cluster (Brown et al., 1992) features generated by Liang (2005). The Brown clustering algorithm produces a binary tree, where each word can be uniquely identified by its path from the root, and this path can be compactly represented with a bit string. Different lengths of the prefix of this root-to-leaf path provide different levels of word abstraction. In this implementation, we set the length of the prefix to 6. 2.5 Classifying Implicit Connectives We classify the sense of implicit connectives based on four sets of features. We follow Lin et al. (2014) to generate the product rules of both constituent parse and dependency parse features, and to generate the word pair features to enumerate all the word pairs in each pair of sentences. In addition, similar to Rutherford and Xue (2014), we incorporate Brown cluster pairs by replacing each word with its Brown cluster prefixes (length of 4) in the way described in Section 2.4. We also use another rich source of information - the polarity of context, which has been previously shown to be useful for coreference problems (Peng et al., 2015). We extract polarity information using the data provided b</context>
<context position="6791" citStr="Lin et al. (2014)" startWordPosition="1038" endWordPosition="1041">l. We use 79 the extracted polarity values to construct three features: Two individual polarities of the two predicates and the conjuction of them. We also implement feature selection to remove the features that are active in the corpus less than five times. As a result, we have 16,989 parse tree features, 4,335 dependency tree features, 77,677 word pair features, and 67,204 Brown cluster features. 2.6 Identifying Attribution Structures We train two classifiers for attribution identification based on the original PDTB data (sections 2- 21). The first classifier is similar to that developed by Lin et al. (2014). We use the patterns proposed by Skadhauge and Hardt (2005) to enumerate all the candidate attribution spans. The coverage of our implementation is 59.8%, which means that we can only enumerate the candidates that contain the attribution covering 59.8% of the annotation. We then build a classifier following Lin et al. (2014) to decide whether the candidate is a valid attribution or not. Using the sentences that contain the attribution(s), we also train a tagger. The tagger is designed following the features used in Illinois Chunker (Punyakanok and Roth, 2001). 3 Evaluation and Results In this</context>
</contexts>
<marker>Lin, Ng, Kan, 2014</marker>
<rawString>Z. Lin, H. T. Ng, and M.-Y. Kan. 2014. A PDTBstyled end-to-end discourse parser. Natural Language Engineering, 20(2):151–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Peng</author>
<author>D Khashabi</author>
<author>D Roth</author>
</authors>
<title>Solving hard coreference problems.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Denver, Colorado,</location>
<contexts>
<context position="5940" citStr="Peng et al., 2015" startWordPosition="898" endWordPosition="901">licit connectives based on four sets of features. We follow Lin et al. (2014) to generate the product rules of both constituent parse and dependency parse features, and to generate the word pair features to enumerate all the word pairs in each pair of sentences. In addition, similar to Rutherford and Xue (2014), we incorporate Brown cluster pairs by replacing each word with its Brown cluster prefixes (length of 4) in the way described in Section 2.4. We also use another rich source of information - the polarity of context, which has been previously shown to be useful for coreference problems (Peng et al., 2015). We extract polarity information using the data provided by Wilson et al. (2005) given the predicates of two discourse arguments. The data contains 8221 words, each of which is labeled with a polarity of positive, negative, or neutral. We use 79 the extracted polarity values to construct three features: Two individual polarities of the two predicates and the conjuction of them. We also implement feature selection to remove the features that are active in the corpus less than five times. As a result, we have 16,989 parse tree features, 4,335 dependency tree features, 77,677 word pair features,</context>
</contexts>
<marker>Peng, Khashabi, Roth, 2015</marker>
<rawString>H. Peng, D. Khashabi, and D. Roth. 2015. Solving hard coreference problems. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics, Denver, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
<author>N Dinesh</author>
<author>A Lee</author>
<author>E Miltsakaki</author>
<author>L Robaldo</author>
<author>A Joshi</author>
<author>B Webber</author>
</authors>
<title>The penn discourse treebank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08).</booktitle>
<contexts>
<context position="3059" citStr="Prasad et al., 2008" startWordPosition="432" endWordPosition="435">reprocessing The preprocessing stage identifies tokens, sentences, part-of-speech (Roth and Zelenko, 1998), shallow parse chunks (Punyakanok and Roth, 2001), lemmas2, syntactic constituency parse (Klein and Manning, 2003), and dependency parse (de Marneffe et al., 2006). This stage also generates a mapping from our own token indexes to those provided in the gold standard data, to allow evaluation with the official shared task scoring code. 2.2 Recognizing Explicit Connectives To recognize explicit connectives, we construct a list of existing connectives labeled in the Penn Discourse Treebank (Prasad et al., 2008a). Since not all the words of the connective list are necessarily true connectives when they appear in text, we build a binary classifier to determine when a word matching an entry in the list represents an actual connective. We only focus on the connectives with consecutive tokens and ignore the non-consecutive connectives. We generate lexicosyntactic and path features associated with the 2http://cogcomp.cs.illinois.edu/page/ software_view/illinois-lemmatizer 78 Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 78–83, Beijing, China, July</context>
<context position="7800" citStr="Prasad et al., 2008" startWordPosition="1208" endWordPosition="1211"> Using the sentences that contain the attribution(s), we also train a tagger. The tagger is designed following the features used in Illinois Chunker (Punyakanok and Roth, 2001). 3 Evaluation and Results In this section, we present the data we used and results of the evaluation based on both crossvalidation on the training data (computed within our software) and using the official CoNLL 2015 Shared Task evaluation framework of Xue et al. (2015). 3.1 Data The data we used is provided through the CoNLL2015 shared task (Xue et al., 2015), which is a modification of Penn Discourse Treebank (PDTB) (Prasad et al., 2008b) sections 2 through 21. The training data for attribution identification is obtained from the original PDTB release, also sections 2 through 21. 3.2 Cross-Validation Results We first present the cross validation results for each component using the training data (Table 1). All the results are averaged over 10-fold cross validation of all the examples we generated, using our own predicted features and our own evaluation code. Each component is evaluated in isolation, assuming the inputs are from gold data (for example: for connective classification, it is assumed we have the correct connectiv</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo, A. Joshi, and B. Webber. 2008a. The penn discourse treebank 2.0. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
<author>N Dinesh</author>
<author>A Lee</author>
<author>E Miltsakaki</author>
<author>L Robaldo</author>
<author>A Joshi</author>
<author>B Webber</author>
</authors>
<date>2008</date>
<contexts>
<context position="3059" citStr="Prasad et al., 2008" startWordPosition="432" endWordPosition="435">reprocessing The preprocessing stage identifies tokens, sentences, part-of-speech (Roth and Zelenko, 1998), shallow parse chunks (Punyakanok and Roth, 2001), lemmas2, syntactic constituency parse (Klein and Manning, 2003), and dependency parse (de Marneffe et al., 2006). This stage also generates a mapping from our own token indexes to those provided in the gold standard data, to allow evaluation with the official shared task scoring code. 2.2 Recognizing Explicit Connectives To recognize explicit connectives, we construct a list of existing connectives labeled in the Penn Discourse Treebank (Prasad et al., 2008a). Since not all the words of the connective list are necessarily true connectives when they appear in text, we build a binary classifier to determine when a word matching an entry in the list represents an actual connective. We only focus on the connectives with consecutive tokens and ignore the non-consecutive connectives. We generate lexicosyntactic and path features associated with the 2http://cogcomp.cs.illinois.edu/page/ software_view/illinois-lemmatizer 78 Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 78–83, Beijing, China, July</context>
<context position="7800" citStr="Prasad et al., 2008" startWordPosition="1208" endWordPosition="1211"> Using the sentences that contain the attribution(s), we also train a tagger. The tagger is designed following the features used in Illinois Chunker (Punyakanok and Roth, 2001). 3 Evaluation and Results In this section, we present the data we used and results of the evaluation based on both crossvalidation on the training data (computed within our software) and using the official CoNLL 2015 Shared Task evaluation framework of Xue et al. (2015). 3.1 Data The data we used is provided through the CoNLL2015 shared task (Xue et al., 2015), which is a modification of Penn Discourse Treebank (PDTB) (Prasad et al., 2008b) sections 2 through 21. The training data for attribution identification is obtained from the original PDTB release, also sections 2 through 21. 3.2 Cross-Validation Results We first present the cross validation results for each component using the training data (Table 1). All the results are averaged over 10-fold cross validation of all the examples we generated, using our own predicted features and our own evaluation code. Each component is evaluated in isolation, assuming the inputs are from gold data (for example: for connective classification, it is assumed we have the correct connectiv</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo, A. Joshi, and B. Webber. 2008b.</rawString>
</citation>
<citation valid="true">
<title>The penn discourse treebank 2.0. In</title>
<date></date>
<booktitle>Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08),</booktitle>
<editor>Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, and Daniel Tapias, editors,</editor>
<location>Marrakech, Morocco,</location>
<marker></marker>
<rawString>The penn discourse treebank 2.0. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, and Daniel Tapias, editors, Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08), Marrakech, Morocco, May. European Language Resources Association (ELRA). http://www.lrecconf.org/proceedings/lrec2008/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
</authors>
<title>The use of classifiers in sequential inference.</title>
<date>2001</date>
<booktitle>In Proc. of the Conference on Neural Information Processing Systems (NIPS),</booktitle>
<pages>995--1001</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2596" citStr="Punyakanok and Roth, 2001" startWordPosition="361" endWordPosition="364">omponents. After a preprocessing step, the system identifies explicit connectives, determines the positions of the arguments relative to the connective, identifies and labels arguments, classifies explicit and implicit connectives, and identifies attribution structures. The system architecture is presented in Figure 1. All the classifiers are built based on LibLinear (Fan et al., 2008) via the interface of Learning Based Java (LBJava) (Rizzolo and Roth, 2010). 2.1 Preprocessing The preprocessing stage identifies tokens, sentences, part-of-speech (Roth and Zelenko, 1998), shallow parse chunks (Punyakanok and Roth, 2001), lemmas2, syntactic constituency parse (Klein and Manning, 2003), and dependency parse (de Marneffe et al., 2006). This stage also generates a mapping from our own token indexes to those provided in the gold standard data, to allow evaluation with the official shared task scoring code. 2.2 Recognizing Explicit Connectives To recognize explicit connectives, we construct a list of existing connectives labeled in the Penn Discourse Treebank (Prasad et al., 2008a). Since not all the words of the connective list are necessarily true connectives when they appear in text, we build a binary classifie</context>
<context position="7357" citStr="Punyakanok and Roth, 2001" startWordPosition="1131" endWordPosition="1134">classifier is similar to that developed by Lin et al. (2014). We use the patterns proposed by Skadhauge and Hardt (2005) to enumerate all the candidate attribution spans. The coverage of our implementation is 59.8%, which means that we can only enumerate the candidates that contain the attribution covering 59.8% of the annotation. We then build a classifier following Lin et al. (2014) to decide whether the candidate is a valid attribution or not. Using the sentences that contain the attribution(s), we also train a tagger. The tagger is designed following the features used in Illinois Chunker (Punyakanok and Roth, 2001). 3 Evaluation and Results In this section, we present the data we used and results of the evaluation based on both crossvalidation on the training data (computed within our software) and using the official CoNLL 2015 Shared Task evaluation framework of Xue et al. (2015). 3.1 Data The data we used is provided through the CoNLL2015 shared task (Xue et al., 2015), which is a modification of Penn Discourse Treebank (PDTB) (Prasad et al., 2008b) sections 2 through 21. The training data for attribution identification is obtained from the original PDTB release, also sections 2 through 21. 3.2 Cross-</context>
</contexts>
<marker>Punyakanok, Roth, 2001</marker>
<rawString>V. Punyakanok and D. Roth. 2001. The use of classifiers in sequential inference. In Proc. of the Conference on Neural Information Processing Systems (NIPS), pages 995–1001. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Rizzolo</author>
<author>D Roth</author>
</authors>
<title>Learning based java for rapid development of nlp systems.</title>
<date>2010</date>
<booktitle>In Proc. of the International Conference on Language Resources and Evaluation (LREC),</booktitle>
<location>Valletta,</location>
<contexts>
<context position="2433" citStr="Rizzolo and Roth, 2010" startWordPosition="341" endWordPosition="344">itecture based on the description in Lin et al. (2014), then investigated features and inference approaches to improve the system. The pipeline includes seven components. After a preprocessing step, the system identifies explicit connectives, determines the positions of the arguments relative to the connective, identifies and labels arguments, classifies explicit and implicit connectives, and identifies attribution structures. The system architecture is presented in Figure 1. All the classifiers are built based on LibLinear (Fan et al., 2008) via the interface of Learning Based Java (LBJava) (Rizzolo and Roth, 2010). 2.1 Preprocessing The preprocessing stage identifies tokens, sentences, part-of-speech (Roth and Zelenko, 1998), shallow parse chunks (Punyakanok and Roth, 2001), lemmas2, syntactic constituency parse (Klein and Manning, 2003), and dependency parse (de Marneffe et al., 2006). This stage also generates a mapping from our own token indexes to those provided in the gold standard data, to allow evaluation with the official shared task scoring code. 2.2 Recognizing Explicit Connectives To recognize explicit connectives, we construct a list of existing connectives labeled in the Penn Discourse Tre</context>
</contexts>
<marker>Rizzolo, Roth, 2010</marker>
<rawString>N. Rizzolo and D. Roth. 2010. Learning based java for rapid development of nlp systems. In Proc. of the International Conference on Language Resources and Evaluation (LREC), Valletta, Malta, 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>D Zelenko</author>
</authors>
<title>Part of speech tagging using a network of linear separators.</title>
<date>1998</date>
<booktitle>In ColingAcl, The 17th International Conference on Computational Linguistics,</booktitle>
<pages>1136--1142</pages>
<contexts>
<context position="2546" citStr="Roth and Zelenko, 1998" startWordPosition="354" endWordPosition="357">prove the system. The pipeline includes seven components. After a preprocessing step, the system identifies explicit connectives, determines the positions of the arguments relative to the connective, identifies and labels arguments, classifies explicit and implicit connectives, and identifies attribution structures. The system architecture is presented in Figure 1. All the classifiers are built based on LibLinear (Fan et al., 2008) via the interface of Learning Based Java (LBJava) (Rizzolo and Roth, 2010). 2.1 Preprocessing The preprocessing stage identifies tokens, sentences, part-of-speech (Roth and Zelenko, 1998), shallow parse chunks (Punyakanok and Roth, 2001), lemmas2, syntactic constituency parse (Klein and Manning, 2003), and dependency parse (de Marneffe et al., 2006). This stage also generates a mapping from our own token indexes to those provided in the gold standard data, to allow evaluation with the official shared task scoring code. 2.2 Recognizing Explicit Connectives To recognize explicit connectives, we construct a list of existing connectives labeled in the Penn Discourse Treebank (Prasad et al., 2008a). Since not all the words of the connective list are necessarily true connectives whe</context>
</contexts>
<marker>Roth, Zelenko, 1998</marker>
<rawString>D. Roth and D. Zelenko. 1998. Part of speech tagging using a network of linear separators. In ColingAcl, The 17th International Conference on Computational Linguistics, pages 1136–1142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rutherford</author>
<author>N Xue</author>
</authors>
<title>Discovering implicit discourse relations through brown cluster pair representation and coreference patterns.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>645--654</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5634" citStr="Rutherford and Xue (2014)" startWordPosition="846" endWordPosition="849">path from the root, and this path can be compactly represented with a bit string. Different lengths of the prefix of this root-to-leaf path provide different levels of word abstraction. In this implementation, we set the length of the prefix to 6. 2.5 Classifying Implicit Connectives We classify the sense of implicit connectives based on four sets of features. We follow Lin et al. (2014) to generate the product rules of both constituent parse and dependency parse features, and to generate the word pair features to enumerate all the word pairs in each pair of sentences. In addition, similar to Rutherford and Xue (2014), we incorporate Brown cluster pairs by replacing each word with its Brown cluster prefixes (length of 4) in the way described in Section 2.4. We also use another rich source of information - the polarity of context, which has been previously shown to be useful for coreference problems (Peng et al., 2015). We extract polarity information using the data provided by Wilson et al. (2005) given the predicates of two discourse arguments. The data contains 8221 words, each of which is labeled with a polarity of positive, negative, or neutral. We use 79 the extracted polarity values to construct thre</context>
</contexts>
<marker>Rutherford, Xue, 2014</marker>
<rawString>A. Rutherford and N. Xue. 2014. Discovering implicit discourse relations through brown cluster pair representation and coreference patterns. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 645–654. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Skadhauge</author>
<author>D Hardt</author>
</authors>
<title>Syntactic identification of attribution in the rst treebank.</title>
<date>2005</date>
<booktitle>In Proceedings of the Sixth International Workshop on Linguistically Interpreted Corpora (LINC-2005).</booktitle>
<contexts>
<context position="6851" citStr="Skadhauge and Hardt (2005)" startWordPosition="1048" endWordPosition="1051">ruct three features: Two individual polarities of the two predicates and the conjuction of them. We also implement feature selection to remove the features that are active in the corpus less than five times. As a result, we have 16,989 parse tree features, 4,335 dependency tree features, 77,677 word pair features, and 67,204 Brown cluster features. 2.6 Identifying Attribution Structures We train two classifiers for attribution identification based on the original PDTB data (sections 2- 21). The first classifier is similar to that developed by Lin et al. (2014). We use the patterns proposed by Skadhauge and Hardt (2005) to enumerate all the candidate attribution spans. The coverage of our implementation is 59.8%, which means that we can only enumerate the candidates that contain the attribution covering 59.8% of the annotation. We then build a classifier following Lin et al. (2014) to decide whether the candidate is a valid attribution or not. Using the sentences that contain the attribution(s), we also train a tagger. The tagger is designed following the features used in Illinois Chunker (Punyakanok and Roth, 2001). 3 Evaluation and Results In this section, we present the data we used and results of the eva</context>
</contexts>
<marker>Skadhauge, Hardt, 2005</marker>
<rawString>P. R. Skadhauge and D. Hardt. 2005. Syntactic identification of attribution in the rst treebank. In Proceedings of the Sixth International Workshop on Linguistically Interpreted Corpora (LINC-2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>P Hoffmann</author>
<author>S Somasundaran</author>
<author>J Kessler</author>
<author>J Wiebe</author>
<author>Y Choi</author>
<author>C Cardie</author>
<author>E Riloff</author>
<author>S Patwardhan</author>
</authors>
<title>Opinionfinder: A system for subjectivity analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP on interactive demonstrations,</booktitle>
<pages>34--35</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6021" citStr="Wilson et al. (2005)" startWordPosition="911" endWordPosition="914">o generate the product rules of both constituent parse and dependency parse features, and to generate the word pair features to enumerate all the word pairs in each pair of sentences. In addition, similar to Rutherford and Xue (2014), we incorporate Brown cluster pairs by replacing each word with its Brown cluster prefixes (length of 4) in the way described in Section 2.4. We also use another rich source of information - the polarity of context, which has been previously shown to be useful for coreference problems (Peng et al., 2015). We extract polarity information using the data provided by Wilson et al. (2005) given the predicates of two discourse arguments. The data contains 8221 words, each of which is labeled with a polarity of positive, negative, or neutral. We use 79 the extracted polarity values to construct three features: Two individual polarities of the two predicates and the conjuction of them. We also implement feature selection to remove the features that are active in the corpus less than five times. As a result, we have 16,989 parse tree features, 4,335 dependency tree features, 77,677 word pair features, and 67,204 Brown cluster features. 2.6 Identifying Attribution Structures We tra</context>
</contexts>
<marker>Wilson, Hoffmann, Somasundaran, Kessler, Wiebe, Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler, J. Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. 2005. Opinionfinder: A system for subjectivity analysis. In Proceedings of HLT/EMNLP on interactive demonstrations, pages 34–35. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>H T Ng</author>
<author>S Pradhan</author>
<author>R Prasad</author>
<author>C Bryant</author>
<author>A Rutherford</author>
</authors>
<title>The conll-2015 shared task on shallow discourse parsing.</title>
<date>2015</date>
<booktitle>In Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="7628" citStr="Xue et al. (2015)" startWordPosition="1177" endWordPosition="1180">in the attribution covering 59.8% of the annotation. We then build a classifier following Lin et al. (2014) to decide whether the candidate is a valid attribution or not. Using the sentences that contain the attribution(s), we also train a tagger. The tagger is designed following the features used in Illinois Chunker (Punyakanok and Roth, 2001). 3 Evaluation and Results In this section, we present the data we used and results of the evaluation based on both crossvalidation on the training data (computed within our software) and using the official CoNLL 2015 Shared Task evaluation framework of Xue et al. (2015). 3.1 Data The data we used is provided through the CoNLL2015 shared task (Xue et al., 2015), which is a modification of Penn Discourse Treebank (PDTB) (Prasad et al., 2008b) sections 2 through 21. The training data for attribution identification is obtained from the original PDTB release, also sections 2 through 21. 3.2 Cross-Validation Results We first present the cross validation results for each component using the training data (Table 1). All the results are averaged over 10-fold cross validation of all the examples we generated, using our own predicted features and our own evaluation cod</context>
</contexts>
<marker>Xue, Ng, Pradhan, Prasad, Bryant, Rutherford, 2015</marker>
<rawString>N. Xue, H. T. Ng, S. Pradhan, R. Prasad, C. Bryant, and A. Rutherford. 2015. The conll-2015 shared task on shallow discourse parsing. In Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>