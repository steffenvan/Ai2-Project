<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000315">
<title confidence="0.993655">
Supersense Tagging for Arabic: the MT-in-the-Middle Attack
</title>
<author confidence="0.998668">
Nathan Schneider∗ Behrang Mohit† Chris Dyer∗ Kemal Oflazer† Noah A. Smith∗
</author>
<affiliation confidence="0.81920175">
School of Computer Science
Carnegie Mellon University
∗Pittsburgh, PA 15213, USA
†Doha, Qatar
</affiliation>
<email confidence="0.992717">
{nschneid@cs.,behrang@,cdyer@cs.,ko@cs.,nasmith@cs.}cmu.edu
</email>
<note confidence="0.593609">
� �������������� �������� �� ��� � ��� �� ��
</note>
<sectionHeader confidence="0.947297" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.958575692307692">
We consider the task of tagging Arabic nouns
with WordNet supersenses. Three approaches
are evaluated. The first uses an expert-
crafted but limited-coverage lexicon, Arabic
WordNet, and heuristics. The second uses un-
supervised sequence modeling. The third and
most successful approach uses machine trans-
lation to translate the Arabic into English,
which is automatically tagged with English
supersenses, the results of which are then pro-
jected back into Arabic. Analysis shows gains
and remaining obstacles in four Wikipedia
topical domains.
</bodyText>
<sectionHeader confidence="0.998687" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972875">
A taxonomic view of lexical semantics groups word
senses/usages into categories of varying granulari-
ties. WordNet supersense tags denote coarse seman-
tic classes, including and (for nouns)
and and (for verbs); these categories
can be taken as the top level of a taxonomy. Nominal
supersense tagging (Ciaramita and Johnson, 2003)
is the task of identifying lexical chunks in the sen-
tence for common as well as proper nouns, and la-
beling each with one of the 25 nominal supersense
categories. Figure 1 illustrates two such labelings of
an Arabic sentence. Like the narrower problem of
named entity recognition, supersense tagging of text
holds attraction as a way of inferring representations
that move toward language independence. Here we
consider the problem of nominal supersense tagging
for Arabic, a language with ca. 300 million speak-
ers and moderate linguistic resources, including a
WordNet (Elkateb et al., 2006), annotated datasets
(Maamouri et al., 2004; Hovy et al., 2006), monolin-
gual corpora, and large amounts of Arabic-English
parallel data.
The supervised learning approach that is used
in state-of-the-art English supersense taggers (Cia-
</bodyText>
<figure confidence="0.8978344">
���������� ����� �������
Ann-A Gloss Ann-B
controls
manager
the-windows
in
configuration
and-layout
windows
the-applications
</figure>
<figureCaption confidence="0.797655">
‘The window manager controls the configuration and
layout of application windows.’
Figure 1: A sentence from the “X Window System” ar-
ticle with supersense taggings from two annotators and
post hoc English glosses and translation.
</figureCaption>
<bodyText confidence="0.999722947368421">
ramita and Altun, 2006) is problematic for Ara-
bic, since there are supersense annotations for only
a small amount of Arabic text (65,000 words by
Schneider et al., 2012, versus the 360,000 words that
are annotated for English). Here, we reserve that
corpus for development and evaluation, not training.
We explore several approaches in this paper, the
most effective of which is to (1) translate the Arabic
sentence into English, returning the alignment struc-
ture between the source and target, (2) apply En-
glish supersense tagging to the target sentence, and
(3) heuristically project the tags back to the Arabic
sentence across these alignments. This “MT-in-the-
middle” approach has also been successfully used
for mention detection (Zitouni and Florian, 2008)
and coreference resolution (Rahman and Ng, 2012).
We first discuss the task and relevant resources
(§2), then the approaches we explored (§3), and fi-
nally present experimental results and analysis in §4.
</bodyText>
<sectionHeader confidence="0.941871" genericHeader="introduction">
2 Task and Resources
</sectionHeader>
<bodyText confidence="0.9998965">
A gold standard corpus of sentences annotated
with nominal supersenses (as in figure 1) fa-
cilitates automatic evaluation of supersense tag-
gers. For development and evaluation we use
</bodyText>
<page confidence="0.971992">
661
</page>
<subsectionHeader confidence="0.292241">
Proceedings of NAACL-HLT 2013, pages 661–667,
</subsectionHeader>
<bodyText confidence="0.9916432">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
the AQMAR Arabic Wikipedia Supersense Corpus1
(Schneider et al., 2012), which augmented a named
entity corpus (Mohit et al., 2012) with nominal
supersense tags. The corpus consists of 28 ar-
ticles selected from four topical areas: history
(e.g., “Islamic Golden Age”), science (“Atom”),
sports (“Real Madrid”), and technology (“Linux”).
Schneider et al. (2012) found the distributions of
supersense categories in these four topical domains
to be markedly different; e.g., most instances of
(which includes kinds of software)
occurred in the technology domain, whereas most
s were found in the science domain.
The 18 test articles have 1,393 sentences (39,916
tokens) annotated at least once.2 As the corpus
was released with two annotators’ (partially overlap-
ping) taggings, rather than a single gold standard,
we treat the output of each annotator as a separate
test set. Both annotated some of every article; the
first (Ann-A) annotated 759 sentences, the second
(Ann-B) 811 sentences.
Lexicon. What became known as “supersense
tags” arose from a high-level partitioning of synsets
in the original English WordNet (Fellbaum, 1998)
into lexicographer files. Arabic WordNet (AWN)
(Elkateb et al., 2006) allows us to recover super-
sense categories for some 10,500 Arabic nominal
types, since many of the synsets in AWN are cross-
referenced to English WordNet, and can therefore
be associated with supersense categories. Further,
OntoNotes contains named entity annotations for
Arabic (Hovy et al., 2006).
From these, we construct an Arabic supersense
lexicon, mapping Arabic noun lemmas to supersense
tags. This lexicon contains 23,000 types, of which
11,000 are multiword units. Token coverage of the
test set is 18% (see table 1). Lexical units encoun-
tered in the test data were up to 9-ways supersense-
ambiguous; the average ambiguity of in-vocabulary
tokens was 2.0 supersenses.
Unlabeled Arabic text. For unsupervised learn-
ing we collected 100,000 words of Arabic Wikipedia
text, not constrained by topic. The articles in this
sample were subject to a minimum length threshold
</bodyText>
<footnote confidence="0.9900635">
1http://www.ark.cs.cmu.edu/ArabicSST
2Our development/test split of the data follows Mohit et al.
(2012), but we exclude two test set documents—“Light” and
“Ibn Tolun Mosque”—due to preprocessing issues.
</footnote>
<bodyText confidence="0.996133178571428">
and are all cross-linked to corresponding articles in
English, Chinese, and German.
Arabic—�English machine translation. We used
two independently developed Arabic-English MT
systems. One (QCRI) is a phrase-based system
(Koehn et al., 2003), similar to Moses (Koehn et
al., 2007); the other (cdec) is a hierarchical phrase-
based system (Chiang, 2007), as implemented in
cdec (Dyer et al., 2010). Both were trained on
about 370M tokens of parallel data provided by the
LDC (by volume, mostly newswire and UN data).
Each system includes preprocessing for Arabic mor-
phological segmentation and orthographic normal-
ization.3 The QCRI system used a 5-gram modi-
fied Kneser-Ney language model that generated full-
cased forms (Chen and Goodman, 1999). cdec
used a 4-gram KN language model over lowercase
forms and was recased in a post-processing step.
Both language models were trained using the Giga-
word v. 4 corpus. Both systems were tuned to opti-
mize BLEU on a held-out development set (Papineni
et al., 2002).
English supersense tagger. For English super-
sense tagging, an open-source reimplementation of
the approach of Ciaramita and Altun (2006) was
released by Michael Heilman.4 This tagger was
trained on the SemCor corpus (Miller et al., 1993)
and achieves 77% F1 in-domain.
</bodyText>
<sectionHeader confidence="0.998351" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.999686666666667">
We explored 3 approaches to the supersense tagging
of Arabic: heuristic tagging with a lexicon, unsuper-
vised sequence tagging, and MT-in-the-middle.
</bodyText>
<subsectionHeader confidence="0.998546">
3.1 Heuristic Tagging with a Lexicon
</subsectionHeader>
<bodyText confidence="0.9993585">
Using the lexicon built from AWN and OntoNotes
(see §2), our heuristic approach works as follows:
</bodyText>
<listItem confidence="0.999621666666667">
1. Stem and vocalize; we used MADA (Habash
and Rambow, 2005; Roth et al., 2008).
2. Greedily detect word sequences matching lexi-
con entries from left to right.
3. If a lexicon entry has more than one associated
supersense, Arabic WordNet synsets are
</listItem>
<footnote confidence="0.99553125">
3QCRI accomplishes this using MADA (Habash and Ram-
bow, 2005; Roth et al., 2008). cdec includes a custom CRF-
based segmenter and standard normalization rules.
4http://www.ark.cs.cmu.edu/mheilman/questions
</footnote>
<page confidence="0.981777">
662
</page>
<figure confidence="0.999498142857143">
Eˆ 2 2
ˆe P 3
a N
Aˆ
1 3
1
N
Projected supersense tagging
Automatic English supersense tagging
English sentence
Arabic sentence (e.g., token 6 aligns to English tokens 7–9)
Arabic POS tagging
4 5
4
A
6
5
N
7 8 9
N
6
</figure>
<figureCaption confidence="0.9406225">
Figure 2: A hypothetical aligned sentence pair of 9 English words (with their supersense tags) and 6 Ara-
bic words (with their POS tags). Step 4 of the projection procedure constructs the Arabic-to-English mapping
</figureCaption>
<bodyText confidence="0.947763222222222">
{1→ 11, 4→ 43,{5, 6}→ 76}, resulting in the tagging shown in the bottom row.
weighted to favor earlier senses (presumed
by lexicographers to be more frequent) and
then the supersense with the greatest aggregate
weight is selected. Formally: Let senses(w) be
the ordered list of AWN senses of lemma w.
Let senses(w, s) c_ senses(w) be those senses
that map to a given supersense s. We choose
arg maxs(|senses(w, s)|/ mini:senses(w)iEsenses(w,s) i).
</bodyText>
<subsectionHeader confidence="0.999404">
3.2 Unsupervised Sequence Models
</subsectionHeader>
<bodyText confidence="0.999986590909091">
Unsupervised sequence labeling is our second ap-
proach (Merialdo, 1994). Although it was largely
developed for part-of-speech tagging, the hope is
to use in-domain Arabic data (the unannotated
Wikipedia corpus discussed in §2) to infer clus-
ters that correlate well with supersense groupings.
We applied the generative, feature-based model of
Berg-Kirkpatrick et al. (2010), replicating a feature-
set used previously for NER (Mohit et al., 2012)—
including context tokens, character n-grams, and
POS—and adding the vocalized stem and several
stem shape features: 1) ContainsDigit?; 2) dig-
its replaced by #; 3) digit sequences replaced by
# (for stems mixing digits with other characters);
4) YearLike?—true for 4-digit numerals starting with
19 or 20; 5) LatinWord?, per the morphological an-
alysis; 6) the shape feature of Ciaramita and Al-
tun (2006) (Latin words only). We used 50 itera-
tions of learning (tuned on dev data). For evaluation,
a many-to-one mapping from unsupervised clusters
to supersense tags is greedily induced to maximize
their correspondence on evaluation data.
</bodyText>
<subsectionHeader confidence="0.999679">
3.3 MT-in-the-Middle
</subsectionHeader>
<bodyText confidence="0.999526448275862">
A standard approach to using supervised linguistic
resources in a second language is cross-lingual pro-
jection (Yarowsky and Ngai, 2001; Yarowsky et al.,
2001; Smith and Smith, 2004; Hwa et al., 2005; Mi-
halcea et al., 2007; Burkett and Klein, 2008; Burkett
et al., 2010; Das and Petrov, 2011; Kim et al., 2012,
who use parallel sentences extracted from Wikipedia
for NER). The simplest such approach starts with an
aligned parallel corpus, applies supersense tagging
to the English side, and projects the labels through
the word alignments. A supervised monolingual tag-
ger is then trained on the projected labels. Prelimi-
nary experiments, however, showed that this under-
performed even the simple heuristic baseline above
(likely due to domain mismatch), so it was aban-
doned in favor of a technique that we call MT-in-
the-middle projection.
This approach does not depend on having par-
allel data in the training domain, but rather on an
Arabic→English machine translation system that
can be applied to the sentences we wish to tag. The
approach is inspired by token-level pseudo-parallel
data methods of previous work (Zitouni and Flo-
rian, 2008; Rahman and Ng, 2012). MT output for
this language pair is far from perfect—especially for
Wikipedia text, which is distant from the domain
of the translation system’s training data—but, in the
spirit of Church and Hovy (1993), we conjecture that
it may still be useful. The method is as follows:
</bodyText>
<listItem confidence="0.950266117647059">
1. Preprocess the input Arabic sentence a to
match the decoder’s model of Arabic.
2. Translate the sentence, recovering not just
the English output eˆ but also the deriva-
tion/alignment structure z relating words and/or
phrases of the English output to words and/or
phrases of the Arabic input.
3. Apply the English supersense tagger to the En-
glish translation, discarding any verbal super-
sense tags. Call the tagger output ˆE.
4. Project the supersense tags back to the Ara-
bic sentence, yielding ˆA: Each Arabic token
a E a that is (a) a noun, or (b) an adjec-
tive following 0 or more adjectives following a
noun is mapped to the first English supersense
mention in Eˆ containing some word aligned
to a. Then, for each English supersense men-
</listItem>
<page confidence="0.984467">
663
</page>
<table confidence="0.999635428571429">
Coverage
Nouns All Tokens Mentions
Lexicon heuristics (§3.1) 8,058 33% 8,465 18% 8,407
Unsupervised (§3.2)
MT-in-the-middle QCRI 14,401 59% 16,461 35% 12,861
(§3.3) cdec 14,270 58% 15,542 33% 13,704
MTitM + Lex. cdec 16,798 68% 18,461 40% 16,623
Ann-A Ann-B
P R F1 P R F1
32 55 16 29 21.6 37.9 29 53 15 27 19.4 35.6
20 59 16 48 17.5 52.6 14 56 10 39 11.6 45.9
34 65 27 50 29.9 56.4 36 64 28 51 31.6 56.6
37 69 31 57 33.8 62.4 38 67 32 56 34.6 61.0
35 64 36 65 35.5 64.6 36 63 36 63 36.0 63.2
</table>
<tableCaption confidence="0.998375333333333">
Table 1: Supersense tagging results on the test set: coverage measures5 and gold-standard evaluation—exact la-
beled/unlabeled6 mention precision, recall, and F-score against each annotator. The last row is a hybrid: MT-in-the-
middle followed by lexicon heuristics to improve recall. Best single-technique and best hybrid results are bolded.
</tableCaption>
<bodyText confidence="0.999451333333333">
tion, all its mapped Arabic words are grouped
into a single mention and the supersense cat-
egory for that mention is projected. Figure 2
illustrates this procedure. The cdec system
provides word alignments for its translations
derived from the training data; whereas QCRI
only produces phrase-level alignments, so for
every aligned phrase pair (¯a, ¯e) E z, we con-
sider every word in a¯ as aligned to every word
in e¯ (introducing noise when English super-
sense mention boundaries do not line up with
phrase boundaries).
</bodyText>
<sectionHeader confidence="0.996452" genericHeader="evaluation">
4 Experiments and Analysis
</sectionHeader>
<bodyText confidence="0.999859666666667">
Table 1 compares the techniques (§3) for full Arabic
supersense tagging.7 The number of nouns, tokens,
and mentions covered by the automatic tagging is
reported, as is the mention-level evaluation against
human annotations. The evaluation is reported sep-
arately for the two annotators in the dataset.
With heuristic lexicon lookup, 18% of the tokens
are marked as part of a nominal supersense mention.
Both labeled and unlabeled mention recall with this
method are below 30%; labeled precision is about
30%, and unlabeled mention precision is above
50%. From this we conclude that the biggest prob-
lems are (a) out-of-vocabulary items and (b) poor
semantic disambiguation of in-vocabulary items.
The unsupervised sequence tagger does even
worse on the labeled evaluation. It has some success
at detecting supersense mentions—unlabeled recall
is substantially improved, and unlabeled precision is
</bodyText>
<footnote confidence="0.9815005">
5The unsupervised evaluation greedily maps clusters to tags,
separately for each version of the test set; coverage numbers
thus differ and are not shown here.
6Unlabeled tagging refers to noun chunk detection only.
7It was produced in part using the chunkeval.py script: see
https://github.com/nschneid/pyutil
</footnote>
<bodyText confidence="0.997769333333333">
slightly improved. But it seems to be much worse
at assigning semantic categories; the number of la-
beled true positive mentions is actually lower than
with the lexicon-based approach.
MT-in-the-middle is by far the most success-
ful single approach: both systems outperform the
lexicon-only baseline by about 10 F1 points, de-
spite many errors in the automatic translation, En-
glish tagging, and projection, as well as underlying
linguistic differences between English and Arabic.
The baseline’s unlabeled recall is doubled, indicat-
ing substantially more nominal expressions are de-
tected, in addition to the improved labeled scores.
We further tested simple hybrids combining the
lexicon-based and MT-based approaches. Applying
MT-in-the-middle first, then expanding token cover-
age with the lexicon improves recall at a small cost
to precision (table 1, last row). Combining the tech-
niques in the reverse order is slightly worse than MT-
based projection without consulting the lexicon.
MT-in-the middle improves upon the lexicon-only
baseline, yet performance is still dwarfed by the su-
pervised English tagger (at least in the SemCor eval-
uation; see §2), and also well below the 70% inter-
annotator F1 reported by Schneider et al. (2012). We
therefore examine the weaknesses of our approach
for Arabic.
</bodyText>
<subsectionHeader confidence="0.877477">
4.1 MT for Projection
</subsectionHeader>
<bodyText confidence="0.998505428571429">
In analyzing our projection framework, we per-
formed a small-scale MT evaluation with the
Wikipedia data. Reference English translations for
140 Arabic Wikipedia sentences—5 per article in
the corpus—were elicited from a bilingual linguist.
Table 2 compares the two systems under three stan-
dard metrics of overall sentence translation quality.8
</bodyText>
<note confidence="0.6980035">
8BLEU (Papineni et al., 2002); METEOR (Banerjee and
Lavie, 2005; Lavie and Denkowski, 2009), with default options;
</note>
<page confidence="0.992447">
664
</page>
<figure confidence="0.8027235">
.¡ƒñË@ ú�¯ @Yg. b&apos;.,ª“ ::j�‚Ë@ &amp;quot;ñÓ o@ñ;Èñk Ðñm�� ( �HA�KðQ��ºËB� @ ) �éJ.ËA‚Ë@ �HA�Jj ~‚Ë@ áÓ �éK.Am�... áÓ �èP �YË@ vñº=
QCRI: corn consists of a negative shipments (electron hovering around the nucleus of the shipment ) are very small in the center. (3/6)
cdec: The corn is composed of negative shipments ( electronics ) cloud hovering over the nucleus of a very small positive shipment in the
center . (2/6)
�
. �HAJ��®’�JË@ P@ñ ~‚Ó È@ñ£ Ð �Qî�E ÕË ÈA �ª�KQ�.ËA�¯ , �éËñîD„Ë@ ú�æî�D�JÖß. �HAJ��KAî �DÊË ÈA �ª�KQ�.Ë@ �IÊëA�K
QCRI: Portugal qualified for the finals very easily, Portugal defeated throughout the mission liquidations . (3/5)
cdec: Portugal qualified easily for the finals , Portugal unbeaten throughout the journey ... (3/4)
</figure>
<figureCaption confidence="0.999351">
Figure 3: Example Arabic inputs and the outputs of the two MT systems, with lexical projection precision ratios.
</figureCaption>
<bodyText confidence="0.989310303030303">
While the resulting number of sentences with refer-
ences is far from ideal and there is only one refer-
ence translation for each, all three measures favor
QCRI.
For a targeted measure of lexical translation qual-
ity of noun expressions, we elicited acceptability
judgments from a bilingual annotator for translated
and supersense-projected phrases. Given each MT
system output (for the same 140 sentences) in which
mentions predicted by the English supervised tagger
were highlighted, along with the Arabic source sen-
tence, the judge was asked for each English mention
whether it was a valid translation.9 We call this lexi-
cal projection precision. Figure 3 shows examples,
and the last column of table 2 gives overall statistics.
Upwards of 90% of the lexical translations were ac-
cepted; as with the automatic MT measures, QCRI
slightly outperforms cdec (especially in the technol-
ogy and sports domains10). Of the problematic lex-
ical translations, some are almost certainly domain
effects: e.g., corn or maize instead of atom. Others
are more nuanced, e.g., shipments for charges and
electronics for electrons. Transliteration errors in-
cluded IMAX in place of EMACS and genoa lynx for
GNU Linux. However, lexical projection precision
seems to be a relatively small part of the problem,
especially considering that not all translation errors
lead to supersense tagging errors.
Lexical projection recall was not measured, but
noun token coverage (see table 1) is instructive.
Most noun tokens ought to be tagged; 77% is the
noun coverage rate in the gold standard. In table 1,
and translation edit rate (TER) (Snover et al., 2006)
</bodyText>
<footnote confidence="0.995551111111111">
9The judge did not see alignments or supersense categories.
10For technology articles, the differences in F1 scores be-
tween the two systems were 6.1 and 4.2 for Ann-A and Ann-B,
respectively. For sports the respective differences were 4.3 and
4.4. In the other domains the differences never exceeded 3.3. In-
terestingly, this is the only generalization about topical domain
performance we were able to find that holds across annotators
and systems, in contrast with the stark topical effects observed
by Mohit et al. (2012) for NER.
</footnote>
<table confidence="0.998328">
BLEU METEOR TER Lex. Prec.
QCRI 32.86 32.10 0.46 91.9%
cdec 28.84 31.38 0.49 90.0%
</table>
<tableCaption confidence="0.8486085">
Table 2: MT quality measures comparing the two sys-
tems over 140 sentences. The first three are automatic
measures with 1 reference translation. For the fourth, a
bilingual judged the translation acceptability of phrases
that were identified as supersense mentions by the En-
glish tagger (lexical projection precision).
</tableCaption>
<bodyText confidence="0.999560181818182">
noun coverage gains track overall improvements.
If QCRI produces better translations, why is cdec
more useful for supersense tagging? As noted in
§3.3, cdec gives word-level alignments (even when
the decoder uses large phrasal units for translation),
allowing for more precise projections.11 We suspect
this is especially important in light of findings that
larger phrase sizes are indicative of better transla-
tions (Gamon et al., 2005), so these are exactly the
cases where we expect the translations to be valu-
able.
</bodyText>
<sectionHeader confidence="0.994529" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999558142857143">
To our knowledge, this is the first study of automatic
Arabic supersense tagging. We have shown empiri-
cally that an MT-in-the-middle technique is most ef-
fective of several approaches that do not require la-
beled training data. Analysis sheds light on several
challenges that would need to be overcome for better
Arabic lexical semantic tagging.
</bodyText>
<sectionHeader confidence="0.984412" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999517125">
We thank Wajdi Zaghouani for the translation of the Ara-
bic Wikipedia MT set, Francisco Guzman and Preslav
Nakov for the output of QCRI’s MT system, and Waleed
Ammar and anonymous reviewers for their comments.
This publication was made possible by grant NPRP-08-
485-1-083 from the Qatar National Research Fund (a
member of the Qatar Foundation). The statements made
herein are solely the responsibility of the authors.
</bodyText>
<footnote confidence="0.95824825">
11Our experiments use QCRI as an off-the-shelf system. As
a reviewer notes, it could be retrained to produce word-level
alignments, which would likely improve the accuracy of super-
sense tag projection.
</footnote>
<page confidence="0.996635">
665
</page>
<sectionHeader confidence="0.952182" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.982570594339622">
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
an automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65–72, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Côté,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT 2010), pages
582–590, Los Angeles, California, June. Association
for Computational Linguistics.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceed-
ings of the 2008 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2008), pages
877–886, Honolulu, Hawaii, October. Association for
Computational Linguistics.
David Burkett, Slav Petrov, John Blitzer, and Dan Klein.
2010. Learning better monolingual models with unan-
notated bilingual text. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2010), pages 46–54, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Stanley F. Chen and Joshua Goodman. 1999. An empir-
ical study of smoothing techniques for language mod-
eling. Computer Speech fr Language, 13(4):359–393,
October.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228,
June.
Kenneth W. Church and Eduard H. Hovy. 1993. Good
applications for crummy machine translation. Ma-
chine Translation, 8(4):239–258, December.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and information
extraction with a supersense sequence tagger. In Pro-
ceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, pages 594–602,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Massimiliano Ciaramita and Mark Johnson. 2003. Su-
persense tagging of unknown nouns in WordNet. In
Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing, pages 168–
175, Sapporo, Japan, July.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies (ACL-HLT 2011), pages
600–609, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of the ACL 2010 System Demonstrations, pages
7–12, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
Sabri Elkateb, William Black, Horacio Rodríguez, Musa
Alkhalifa, Piek Vossen, Adam Pease, and Christiane
Fellbaum. 2006. Building a WordNet for Arabic.
In Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC 2006),
pages 29–34, Genoa, Italy, May.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press, Cambridge, MA.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-level MT evaluation without refer-
ence translations: Beyond language modeling. In Pro-
ceedings of the 10th European Association for Ma-
chine Translation Conference (EAMT 2005), pages
103–111, Budapest, May.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL’05), pages 573–580, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
the 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL (HLT-
NAACL 2006), pages 57–60, New York City, USA,
June. Association for Computational Linguistics.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrap-
ping parsers via syntactic projection across parallel
texts. Natural Language Engineering, 11(3):311–325,
September.
Sungchul Kim, Kristina Toutanova, and Hwanjo Yu.
2012. Multilingual named entity recognition using
parallel data and metadata from Wikipedia. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2012), pages
694–702, Jeju Island, Korea, July. Association for
Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
</reference>
<page confidence="0.993282">
666
</page>
<reference confidence="0.99330729245283">
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology (HLT-NAACL
2003), pages 48–54, Edmonton, Canada. Association
for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Demo and Poster Sessions,
pages 177–180, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Alon Lavie and Michael J. Denkowski. 2009. The
METEOR metric for automatic evaluation of machine
translation. Machine Translation, 23(2–3):105–115,
September.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
building a large-scale annotated Arabic corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102–109.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2):155–171.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2007.
Learning multilingual subjective language via cross-
lingual projections. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics (ACL 2007), pages 976–983, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proceedings of the Workshop on Human Language
Technology (HLT ’93), pages 303–308, Plainsboro,
NJ, USA, March. Association for Computational Lin-
guistics.
Behrang Mohit, Nathan Schneider, Rishav Bhowmick,
Kemal Oflazer, and Noah A. Smith. 2012.
Recall-oriented learning of named entities in Arabic
Wikipedia. In Proceedings of the 13th Conference of
the European Chapter of the Association for Computa-
tional Linguistics (EACL 2012), pages 162–173, Avi-
gnon, France, April. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics (ACL 2002), pages 311–318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Altaf Rahman and Vincent Ng. 2012. Translation-
based projection for multilingual coreference resolu-
tion. In Proceedings of the 2012 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (NAACL-HLT 2012), pages 720–730, Montréal,
Canada, June. Association for Computational Linguis-
tics.
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,
and Cynthia Rudin. 2008. Arabic morphological tag-
ging, diacritization, and lemmatization using lexeme
models and feature ranking. In Proceedings of ACL-
08: HLT, pages 117–120, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
Nathan Schneider, Behrang Mohit, Kemal Oflazer, and
Noah A. Smith. 2012. Coarse lexical semantic an-
notation with supersenses: an Arabic case study. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (ACL 2012),
pages 253–258, Jeju Island, Korea, July. Association
for Computational Linguistics.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: using English to
parse Korean. In Proceedings of the 2004 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP’04), pages 49–56, Barcelona, Spain.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Ameri-
cas (AMTA 2006), pages 223–231, Cambridge, Mas-
sachusetts, USA, August.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings
of the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL’01), Pittsburgh, Pennsylvania, USA, June.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Proceed-
ings of the First International Conference on Human
Language Technology Research (HLT’01), San Diego,
California, USA, March.
Imed Zitouni and Radu Florian. 2008. Mention detec-
tion crossing the language barrier. In Proceedings of
the 2008 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2008), pages 600–609,
Honolulu, Hawaii, October. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.997159">
667
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.064357">
<title confidence="0.970259">Supersense Tagging for Arabic: the MT-in-the-Middle Attack</title>
<author confidence="0.709617">Behrang Kemal A</author>
<affiliation confidence="0.9998065">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.998158">PA 15213, USA</address>
<author confidence="0.290211">Qatar</author>
<email confidence="0.997721">nschneid@cs.cmu.edu</email>
<email confidence="0.997721">behrang@cmu.edu</email>
<email confidence="0.997721">cdyer@cs.cmu.edu</email>
<email confidence="0.997721">ko@cs.cmu.edu</email>
<email confidence="0.997721">nasmith@cs.cmu.edu</email>
<abstract confidence="0.997227357142857">We consider the task of tagging Arabic nouns with WordNet supersenses. Three approaches are evaluated. The first uses an expertcrafted but limited-coverage lexicon, Arabic WordNet, and heuristics. The second uses unsupervised sequence modeling. The third and most successful approach uses machine translation to translate the Arabic into English, which is automatically tagged with English supersenses, the results of which are then projected back into Arabic. Analysis shows gains and remaining obstacles in four Wikipedia topical domains.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: an automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="16513" citStr="Banerjee and Lavie, 2005" startWordPosition="2584" endWordPosition="2587">er (at least in the SemCor evaluation; see §2), and also well below the 70% interannotator F1 reported by Schneider et al. (2012). We therefore examine the weaknesses of our approach for Arabic. 4.1 MT for Projection In analyzing our projection framework, we performed a small-scale MT evaluation with the Wikipedia data. Reference English translations for 140 Arabic Wikipedia sentences—5 per article in the corpus—were elicited from a bilingual linguist. Table 2 compares the two systems under three standard metrics of overall sentence translation quality.8 8BLEU (Papineni et al., 2002); METEOR (Banerjee and Lavie, 2005; Lavie and Denkowski, 2009), with default options; 664 .¡ƒñË@ ú�¯ @Yg. b&apos;.,ª“ ::j�‚Ë@ &amp;quot;ñÓ o@ñ;Èñk Ðñm�� ( �HA�KðQ��ºËB� @ ) �éJ.ËA‚Ë@ �HA�Jj ~‚Ë@ áÓ �éK.Am�... áÓ �èP �YË@ vñº= QCRI: corn consists of a negative shipments (electron hovering around the nucleus of the shipment ) are very small in the center. (3/6) cdec: The corn is composed of negative shipments ( electronics ) cloud hovering over the nucleus of a very small positive shipment in the center . (2/6) � . �HAJ��®’�JË@ P@ñ ~‚Ó È@ñ£ Ð �Qî�E ÕË ÈA �ª�KQ�.ËA�¯ , �éËñîD„Ë@ ú�æî�D�JÖß. �HAJ��KAî �DÊË ÈA �ª�KQ�.Ë@ �IÊëA�K QCRI: Portugal qu</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: an automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Alexandre Bouchard-Côté</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2010),</booktitle>
<pages>582--590</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="9294" citStr="Berg-Kirkpatrick et al. (2010)" startWordPosition="1420" endWordPosition="1423">elected. Formally: Let senses(w) be the ordered list of AWN senses of lemma w. Let senses(w, s) c_ senses(w) be those senses that map to a given supersense s. We choose arg maxs(|senses(w, s)|/ mini:senses(w)iEsenses(w,s) i). 3.2 Unsupervised Sequence Models Unsupervised sequence labeling is our second approach (Merialdo, 1994). Although it was largely developed for part-of-speech tagging, the hope is to use in-domain Arabic data (the unannotated Wikipedia corpus discussed in §2) to infer clusters that correlate well with supersense groupings. We applied the generative, feature-based model of Berg-Kirkpatrick et al. (2010), replicating a featureset used previously for NER (Mohit et al., 2012)— including context tokens, character n-grams, and POS—and adding the vocalized stem and several stem shape features: 1) ContainsDigit?; 2) digits replaced by #; 3) digit sequences replaced by # (for stems mixing digits with other characters); 4) YearLike?—true for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupe</context>
</contexts>
<marker>Berg-Kirkpatrick, Bouchard-Côté, DeNero, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick, Alexandre Bouchard-Côté, John DeNero, and Dan Klein. 2010. Painless unsupervised learning with features. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2010), pages 582–590, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>Two languages are better than one (for syntactic parsing).</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP 2008),</booktitle>
<pages>877--886</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="10269" citStr="Burkett and Klein, 2008" startWordPosition="1574" endWordPosition="1577">s starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique that we call MT-inthe-middle projection. This app</context>
</contexts>
<marker>Burkett, Klein, 2008</marker>
<rawString>David Burkett and Dan Klein. 2008. Two languages are better than one (for syntactic parsing). In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP 2008), pages 877–886, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Slav Petrov</author>
<author>John Blitzer</author>
<author>Dan Klein</author>
</authors>
<title>Learning better monolingual models with unannotated bilingual text.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL</booktitle>
<pages>46--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="10291" citStr="Burkett et al., 2010" startWordPosition="1578" endWordPosition="1581"> 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique that we call MT-inthe-middle projection. This approach does not depend </context>
</contexts>
<marker>Burkett, Petrov, Blitzer, Klein, 2010</marker>
<rawString>David Burkett, Slav Petrov, John Blitzer, and Dan Klein. 2010. Learning better monolingual models with unannotated bilingual text. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL 2010), pages 46–54, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1999</date>
<journal>Computer Speech fr Language,</journal>
<volume>13</volume>
<issue>4</issue>
<contexts>
<context position="6728" citStr="Chen and Goodman, 1999" startWordPosition="1006" endWordPosition="1009">ion. We used two independently developed Arabic-English MT systems. One (QCRI) is a phrase-based system (Koehn et al., 2003), similar to Moses (Koehn et al., 2007); the other (cdec) is a hierarchical phrasebased system (Chiang, 2007), as implemented in cdec (Dyer et al., 2010). Both were trained on about 370M tokens of parallel data provided by the LDC (by volume, mostly newswire and UN data). Each system includes preprocessing for Arabic morphological segmentation and orthographic normalization.3 The QCRI system used a 5-gram modified Kneser-Ney language model that generated fullcased forms (Chen and Goodman, 1999). cdec used a 4-gram KN language model over lowercase forms and was recased in a post-processing step. Both language models were trained using the Gigaword v. 4 corpus. Both systems were tuned to optimize BLEU on a held-out development set (Papineni et al., 2002). English supersense tagger. For English supersense tagging, an open-source reimplementation of the approach of Ciaramita and Altun (2006) was released by Michael Heilman.4 This tagger was trained on the SemCor corpus (Miller et al., 1993) and achieves 77% F1 in-domain. 3 Methods We explored 3 approaches to the supersense tagging of Ar</context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech fr Language, 13(4):359–393, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="6338" citStr="Chiang, 2007" startWordPosition="946" endWordPosition="947">s in this sample were subject to a minimum length threshold 1http://www.ark.cs.cmu.edu/ArabicSST 2Our development/test split of the data follows Mohit et al. (2012), but we exclude two test set documents—“Light” and “Ibn Tolun Mosque”—due to preprocessing issues. and are all cross-linked to corresponding articles in English, Chinese, and German. Arabic—�English machine translation. We used two independently developed Arabic-English MT systems. One (QCRI) is a phrase-based system (Koehn et al., 2003), similar to Moses (Koehn et al., 2007); the other (cdec) is a hierarchical phrasebased system (Chiang, 2007), as implemented in cdec (Dyer et al., 2010). Both were trained on about 370M tokens of parallel data provided by the LDC (by volume, mostly newswire and UN data). Each system includes preprocessing for Arabic morphological segmentation and orthographic normalization.3 The QCRI system used a 5-gram modified Kneser-Ney language model that generated fullcased forms (Chen and Goodman, 1999). cdec used a 4-gram KN language model over lowercase forms and was recased in a post-processing step. Both language models were trained using the Gigaword v. 4 corpus. Both systems were tuned to optimize BLEU </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>Eduard H Hovy</author>
</authors>
<title>Good applications for crummy machine translation.</title>
<date>1993</date>
<journal>Machine Translation,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="11390" citStr="Church and Hovy (1993)" startWordPosition="1755" endWordPosition="1758">match), so it was abandoned in favor of a technique that we call MT-inthe-middle projection. This approach does not depend on having parallel data in the training domain, but rather on an Arabic→English machine translation system that can be applied to the sentences we wish to tag. The approach is inspired by token-level pseudo-parallel data methods of previous work (Zitouni and Florian, 2008; Rahman and Ng, 2012). MT output for this language pair is far from perfect—especially for Wikipedia text, which is distant from the domain of the translation system’s training data—but, in the spirit of Church and Hovy (1993), we conjecture that it may still be useful. The method is as follows: 1. Preprocess the input Arabic sentence a to match the decoder’s model of Arabic. 2. Translate the sentence, recovering not just the English output eˆ but also the derivation/alignment structure z relating words and/or phrases of the English output to words and/or phrases of the Arabic input. 3. Apply the English supersense tagger to the English translation, discarding any verbal supersense tags. Call the tagger output ˆE. 4. Project the supersense tags back to the Arabic sentence, yielding ˆA: Each Arabic token a E a that </context>
</contexts>
<marker>Church, Hovy, 1993</marker>
<rawString>Kenneth W. Church and Eduard H. Hovy. 1993. Good applications for crummy machine translation. Machine Translation, 8(4):239–258, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Yasemin Altun</author>
</authors>
<title>Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>594--602</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="7129" citStr="Ciaramita and Altun (2006)" startWordPosition="1071" endWordPosition="1074">ach system includes preprocessing for Arabic morphological segmentation and orthographic normalization.3 The QCRI system used a 5-gram modified Kneser-Ney language model that generated fullcased forms (Chen and Goodman, 1999). cdec used a 4-gram KN language model over lowercase forms and was recased in a post-processing step. Both language models were trained using the Gigaword v. 4 corpus. Both systems were tuned to optimize BLEU on a held-out development set (Papineni et al., 2002). English supersense tagger. For English supersense tagging, an open-source reimplementation of the approach of Ciaramita and Altun (2006) was released by Michael Heilman.4 This tagger was trained on the SemCor corpus (Miller et al., 1993) and achieves 77% F1 in-domain. 3 Methods We explored 3 approaches to the supersense tagging of Arabic: heuristic tagging with a lexicon, unsupervised sequence tagging, and MT-in-the-middle. 3.1 Heuristic Tagging with a Lexicon Using the lexicon built from AWN and OntoNotes (see §2), our heuristic approach works as follows: 1. Stem and vocalize; we used MADA (Habash and Rambow, 2005; Roth et al., 2008). 2. Greedily detect word sequences matching lexicon entries from left to right. 3. If a lexic</context>
<context position="9769" citStr="Ciaramita and Altun (2006)" startWordPosition="1495" endWordPosition="1499">n §2) to infer clusters that correlate well with supersense groupings. We applied the generative, feature-based model of Berg-Kirkpatrick et al. (2010), replicating a featureset used previously for NER (Mohit et al., 2012)— including context tokens, character n-grams, and POS—and adding the vocalized stem and several stem shape features: 1) ContainsDigit?; 2) digits replaced by #; 3) digit sequences replaced by # (for stems mixing digits with other characters); 4) YearLike?—true for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, who use parallel sentences extracted</context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>Massimiliano Ciaramita and Yasemin Altun. 2006. Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 594–602, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Mark Johnson</author>
</authors>
<title>Supersense tagging of unknown nouns in WordNet.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>168--175</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="1225" citStr="Ciaramita and Johnson, 2003" startWordPosition="170" endWordPosition="173">and most successful approach uses machine translation to translate the Arabic into English, which is automatically tagged with English supersenses, the results of which are then projected back into Arabic. Analysis shows gains and remaining obstacles in four Wikipedia topical domains. 1 Introduction A taxonomic view of lexical semantics groups word senses/usages into categories of varying granularities. WordNet supersense tags denote coarse semantic classes, including and (for nouns) and and (for verbs); these categories can be taken as the top level of a taxonomy. Nominal supersense tagging (Ciaramita and Johnson, 2003) is the task of identifying lexical chunks in the sentence for common as well as proper nouns, and labeling each with one of the 25 nominal supersense categories. Figure 1 illustrates two such labelings of an Arabic sentence. Like the narrower problem of named entity recognition, supersense tagging of text holds attraction as a way of inferring representations that move toward language independence. Here we consider the problem of nominal supersense tagging for Arabic, a language with ca. 300 million speakers and moderate linguistic resources, including a WordNet (Elkateb et al., 2006), annota</context>
</contexts>
<marker>Ciaramita, Johnson, 2003</marker>
<rawString>Massimiliano Ciaramita and Mark Johnson. 2003. Supersense tagging of unknown nouns in WordNet. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 168– 175, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
</authors>
<title>Unsupervised partof-speech tagging with bilingual graph-based projections.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011),</booktitle>
<pages>600--609</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="10313" citStr="Das and Petrov, 2011" startWordPosition="1582" endWordPosition="1585">e morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique that we call MT-inthe-middle projection. This approach does not depend on having parallel dat</context>
</contexts>
<marker>Das, Petrov, 2011</marker>
<rawString>Dipanjan Das and Slav Petrov. 2011. Unsupervised partof-speech tagging with bilingual graph-based projections. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011), pages 600–609, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Jonathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations,</booktitle>
<pages>7--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="6382" citStr="Dyer et al., 2010" startWordPosition="952" endWordPosition="955">mum length threshold 1http://www.ark.cs.cmu.edu/ArabicSST 2Our development/test split of the data follows Mohit et al. (2012), but we exclude two test set documents—“Light” and “Ibn Tolun Mosque”—due to preprocessing issues. and are all cross-linked to corresponding articles in English, Chinese, and German. Arabic—�English machine translation. We used two independently developed Arabic-English MT systems. One (QCRI) is a phrase-based system (Koehn et al., 2003), similar to Moses (Koehn et al., 2007); the other (cdec) is a hierarchical phrasebased system (Chiang, 2007), as implemented in cdec (Dyer et al., 2010). Both were trained on about 370M tokens of parallel data provided by the LDC (by volume, mostly newswire and UN data). Each system includes preprocessing for Arabic morphological segmentation and orthographic normalization.3 The QCRI system used a 5-gram modified Kneser-Ney language model that generated fullcased forms (Chen and Goodman, 1999). cdec used a 4-gram KN language model over lowercase forms and was recased in a post-processing step. Both language models were trained using the Gigaword v. 4 corpus. Both systems were tuned to optimize BLEU on a held-out development set (Papineni et a</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finitestate and context-free translation models. In Proceedings of the ACL 2010 System Demonstrations, pages 7–12, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabri Elkateb</author>
<author>William Black</author>
<author>Horacio Rodríguez</author>
<author>Musa Alkhalifa</author>
<author>Piek Vossen</author>
<author>Adam Pease</author>
<author>Christiane Fellbaum</author>
</authors>
<title>Building a WordNet for Arabic.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>29--34</pages>
<location>Genoa, Italy,</location>
<contexts>
<context position="1817" citStr="Elkateb et al., 2006" startWordPosition="265" endWordPosition="268">(Ciaramita and Johnson, 2003) is the task of identifying lexical chunks in the sentence for common as well as proper nouns, and labeling each with one of the 25 nominal supersense categories. Figure 1 illustrates two such labelings of an Arabic sentence. Like the narrower problem of named entity recognition, supersense tagging of text holds attraction as a way of inferring representations that move toward language independence. Here we consider the problem of nominal supersense tagging for Arabic, a language with ca. 300 million speakers and moderate linguistic resources, including a WordNet (Elkateb et al., 2006), annotated datasets (Maamouri et al., 2004; Hovy et al., 2006), monolingual corpora, and large amounts of Arabic-English parallel data. The supervised learning approach that is used in state-of-the-art English supersense taggers (Cia���������� ����� ������� Ann-A Gloss Ann-B controls manager the-windows in configuration and-layout windows the-applications ‘The window manager controls the configuration and layout of application windows.’ Figure 1: A sentence from the “X Window System” article with supersense taggings from two annotators and post hoc English glosses and translation. ramita and </context>
<context position="4909" citStr="Elkateb et al., 2006" startWordPosition="730" endWordPosition="733">ound in the science domain. The 18 test articles have 1,393 sentences (39,916 tokens) annotated at least once.2 As the corpus was released with two annotators’ (partially overlapping) taggings, rather than a single gold standard, we treat the output of each annotator as a separate test set. Both annotated some of every article; the first (Ann-A) annotated 759 sentences, the second (Ann-B) 811 sentences. Lexicon. What became known as “supersense tags” arose from a high-level partitioning of synsets in the original English WordNet (Fellbaum, 1998) into lexicographer files. Arabic WordNet (AWN) (Elkateb et al., 2006) allows us to recover supersense categories for some 10,500 Arabic nominal types, since many of the synsets in AWN are crossreferenced to English WordNet, and can therefore be associated with supersense categories. Further, OntoNotes contains named entity annotations for Arabic (Hovy et al., 2006). From these, we construct an Arabic supersense lexicon, mapping Arabic noun lemmas to supersense tags. This lexicon contains 23,000 types, of which 11,000 are multiword units. Token coverage of the test set is 18% (see table 1). Lexical units encountered in the test data were up to 9-ways supersensea</context>
</contexts>
<marker>Elkateb, Black, Rodríguez, Alkhalifa, Vossen, Pease, Fellbaum, 2006</marker>
<rawString>Sabri Elkateb, William Black, Horacio Rodríguez, Musa Alkhalifa, Piek Vossen, Adam Pease, and Christiane Fellbaum. 2006. Building a WordNet for Arabic. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC 2006), pages 29–34, Genoa, Italy, May.</rawString>
</citation>
<citation valid="true">
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: an electronic lexical database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Anthony Aue</author>
<author>Martine Smets</author>
</authors>
<title>Sentence-level MT evaluation without reference translations: Beyond language modeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th European Association for Machine Translation Conference (EAMT</booktitle>
<pages>103--111</pages>
<location>Budapest,</location>
<contexts>
<context position="20420" citStr="Gamon et al., 2005" startWordPosition="3206" endWordPosition="3209">slation. For the fourth, a bilingual judged the translation acceptability of phrases that were identified as supersense mentions by the English tagger (lexical projection precision). noun coverage gains track overall improvements. If QCRI produces better translations, why is cdec more useful for supersense tagging? As noted in §3.3, cdec gives word-level alignments (even when the decoder uses large phrasal units for translation), allowing for more precise projections.11 We suspect this is especially important in light of findings that larger phrase sizes are indicative of better translations (Gamon et al., 2005), so these are exactly the cases where we expect the translations to be valuable. 5 Conclusion To our knowledge, this is the first study of automatic Arabic supersense tagging. We have shown empirically that an MT-in-the-middle technique is most effective of several approaches that do not require labeled training data. Analysis sheds light on several challenges that would need to be overcome for better Arabic lexical semantic tagging. Acknowledgments We thank Wajdi Zaghouani for the translation of the Arabic Wikipedia MT set, Francisco Guzman and Preslav Nakov for the output of QCRI’s MT syste</context>
</contexts>
<marker>Gamon, Aue, Smets, 2005</marker>
<rawString>Michael Gamon, Anthony Aue, and Martine Smets. 2005. Sentence-level MT evaluation without reference translations: Beyond language modeling. In Proceedings of the 10th European Association for Machine Translation Conference (EAMT 2005), pages 103–111, Budapest, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>573--580</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="7615" citStr="Habash and Rambow, 2005" startWordPosition="1149" endWordPosition="1152"> English supersense tagger. For English supersense tagging, an open-source reimplementation of the approach of Ciaramita and Altun (2006) was released by Michael Heilman.4 This tagger was trained on the SemCor corpus (Miller et al., 1993) and achieves 77% F1 in-domain. 3 Methods We explored 3 approaches to the supersense tagging of Arabic: heuristic tagging with a lexicon, unsupervised sequence tagging, and MT-in-the-middle. 3.1 Heuristic Tagging with a Lexicon Using the lexicon built from AWN and OntoNotes (see §2), our heuristic approach works as follows: 1. Stem and vocalize; we used MADA (Habash and Rambow, 2005; Roth et al., 2008). 2. Greedily detect word sequences matching lexicon entries from left to right. 3. If a lexicon entry has more than one associated supersense, Arabic WordNet synsets are 3QCRI accomplishes this using MADA (Habash and Rambow, 2005; Roth et al., 2008). cdec includes a custom CRFbased segmenter and standard normalization rules. 4http://www.ark.cs.cmu.edu/mheilman/questions 662 Eˆ 2 2 ˆe P 3 a N Aˆ 1 3 1 N Projected supersense tagging Automatic English supersense tagging English sentence Arabic sentence (e.g., token 6 aligns to English tokens 7–9) Arabic POS tagging 4 5 4 A 6 </context>
</contexts>
<marker>Habash, Rambow, 2005</marker>
<rawString>Nizar Habash and Owen Rambow. 2005. Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 573–580, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>OntoNotes: the 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL (HLTNAACL</booktitle>
<pages>57--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="1880" citStr="Hovy et al., 2006" startWordPosition="275" endWordPosition="278">hunks in the sentence for common as well as proper nouns, and labeling each with one of the 25 nominal supersense categories. Figure 1 illustrates two such labelings of an Arabic sentence. Like the narrower problem of named entity recognition, supersense tagging of text holds attraction as a way of inferring representations that move toward language independence. Here we consider the problem of nominal supersense tagging for Arabic, a language with ca. 300 million speakers and moderate linguistic resources, including a WordNet (Elkateb et al., 2006), annotated datasets (Maamouri et al., 2004; Hovy et al., 2006), monolingual corpora, and large amounts of Arabic-English parallel data. The supervised learning approach that is used in state-of-the-art English supersense taggers (Cia���������� ����� ������� Ann-A Gloss Ann-B controls manager the-windows in configuration and-layout windows the-applications ‘The window manager controls the configuration and layout of application windows.’ Figure 1: A sentence from the “X Window System” article with supersense taggings from two annotators and post hoc English glosses and translation. ramita and Altun, 2006) is problematic for Arabic, since there are superse</context>
<context position="5207" citStr="Hovy et al., 2006" startWordPosition="776" endWordPosition="779">tated some of every article; the first (Ann-A) annotated 759 sentences, the second (Ann-B) 811 sentences. Lexicon. What became known as “supersense tags” arose from a high-level partitioning of synsets in the original English WordNet (Fellbaum, 1998) into lexicographer files. Arabic WordNet (AWN) (Elkateb et al., 2006) allows us to recover supersense categories for some 10,500 Arabic nominal types, since many of the synsets in AWN are crossreferenced to English WordNet, and can therefore be associated with supersense categories. Further, OntoNotes contains named entity annotations for Arabic (Hovy et al., 2006). From these, we construct an Arabic supersense lexicon, mapping Arabic noun lemmas to supersense tags. This lexicon contains 23,000 types, of which 11,000 are multiword units. Token coverage of the test set is 18% (see table 1). Lexical units encountered in the test data were up to 9-ways supersenseambiguous; the average ambiguity of in-vocabulary tokens was 2.0 supersenses. Unlabeled Arabic text. For unsupervised learning we collected 100,000 words of Arabic Wikipedia text, not constrained by topic. The articles in this sample were subject to a minimum length threshold 1http://www.ark.cs.cmu</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: the 90% solution. In Proceedings of the Human Language Technology Conference of the NAACL (HLTNAACL 2006), pages 57–60, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Clara Cabezas</author>
<author>Okan Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel texts.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="10221" citStr="Hwa et al., 2005" startWordPosition="1565" endWordPosition="1568">s); 4) YearLike?—true for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique t</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. Natural Language Engineering, 11(3):311–325, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sungchul Kim</author>
<author>Kristina Toutanova</author>
<author>Hwanjo Yu</author>
</authors>
<title>Multilingual named entity recognition using parallel data and metadata from Wikipedia.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012),</booktitle>
<pages>694--702</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="10331" citStr="Kim et al., 2012" startWordPosition="1586" endWordPosition="1589">is; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique that we call MT-inthe-middle projection. This approach does not depend on having parallel data in the training </context>
</contexts>
<marker>Kim, Toutanova, Yu, 2012</marker>
<rawString>Sungchul Kim, Kristina Toutanova, and Hwanjo Yu. 2012. Multilingual named entity recognition using parallel data and metadata from Wikipedia. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), pages 694–702, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (HLT-NAACL</booktitle>
<pages>48--54</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Edmonton, Canada.</location>
<contexts>
<context position="6229" citStr="Koehn et al., 2003" startWordPosition="926" endWordPosition="929">or unsupervised learning we collected 100,000 words of Arabic Wikipedia text, not constrained by topic. The articles in this sample were subject to a minimum length threshold 1http://www.ark.cs.cmu.edu/ArabicSST 2Our development/test split of the data follows Mohit et al. (2012), but we exclude two test set documents—“Light” and “Ibn Tolun Mosque”—due to preprocessing issues. and are all cross-linked to corresponding articles in English, Chinese, and German. Arabic—�English machine translation. We used two independently developed Arabic-English MT systems. One (QCRI) is a phrase-based system (Koehn et al., 2003), similar to Moses (Koehn et al., 2007); the other (cdec) is a hierarchical phrasebased system (Chiang, 2007), as implemented in cdec (Dyer et al., 2010). Both were trained on about 370M tokens of parallel data provided by the LDC (by volume, mostly newswire and UN data). Each system includes preprocessing for Arabic morphological segmentation and orthographic normalization.3 The QCRI system used a 5-gram modified Kneser-Ney language model that generated fullcased forms (Chen and Goodman, 1999). cdec used a 4-gram KN language model over lowercase forms and was recased in a post-processing step</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (HLT-NAACL 2003), pages 48–54, Edmonton, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="6268" citStr="Koehn et al., 2007" startWordPosition="933" endWordPosition="936">00,000 words of Arabic Wikipedia text, not constrained by topic. The articles in this sample were subject to a minimum length threshold 1http://www.ark.cs.cmu.edu/ArabicSST 2Our development/test split of the data follows Mohit et al. (2012), but we exclude two test set documents—“Light” and “Ibn Tolun Mosque”—due to preprocessing issues. and are all cross-linked to corresponding articles in English, Chinese, and German. Arabic—�English machine translation. We used two independently developed Arabic-English MT systems. One (QCRI) is a phrase-based system (Koehn et al., 2003), similar to Moses (Koehn et al., 2007); the other (cdec) is a hierarchical phrasebased system (Chiang, 2007), as implemented in cdec (Dyer et al., 2010). Both were trained on about 370M tokens of parallel data provided by the LDC (by volume, mostly newswire and UN data). Each system includes preprocessing for Arabic morphological segmentation and orthographic normalization.3 The QCRI system used a 5-gram modified Kneser-Ney language model that generated fullcased forms (Chen and Goodman, 1999). cdec used a 4-gram KN language model over lowercase forms and was recased in a post-processing step. Both language models were trained usi</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Demo and Poster Sessions, pages 177–180, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Michael J Denkowski</author>
</authors>
<title>The METEOR metric for automatic evaluation of machine translation.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<pages>23--2</pages>
<contexts>
<context position="16541" citStr="Lavie and Denkowski, 2009" startWordPosition="2588" endWordPosition="2591"> evaluation; see §2), and also well below the 70% interannotator F1 reported by Schneider et al. (2012). We therefore examine the weaknesses of our approach for Arabic. 4.1 MT for Projection In analyzing our projection framework, we performed a small-scale MT evaluation with the Wikipedia data. Reference English translations for 140 Arabic Wikipedia sentences—5 per article in the corpus—were elicited from a bilingual linguist. Table 2 compares the two systems under three standard metrics of overall sentence translation quality.8 8BLEU (Papineni et al., 2002); METEOR (Banerjee and Lavie, 2005; Lavie and Denkowski, 2009), with default options; 664 .¡ƒñË@ ú�¯ @Yg. b&apos;.,ª“ ::j�‚Ë@ &amp;quot;ñÓ o@ñ;Èñk Ðñm�� ( �HA�KðQ��ºËB� @ ) �éJ.ËA‚Ë@ �HA�Jj ~‚Ë@ áÓ �éK.Am�... áÓ �èP �YË@ vñº= QCRI: corn consists of a negative shipments (electron hovering around the nucleus of the shipment ) are very small in the center. (3/6) cdec: The corn is composed of negative shipments ( electronics ) cloud hovering over the nucleus of a very small positive shipment in the center . (2/6) � . �HAJ��®’�JË@ P@ñ ~‚Ó È@ñ£ Ð �Qî�E ÕË ÈA �ª�KQ�.ËA�¯ , �éËñîD„Ë@ ú�æî�D�JÖß. �HAJ��KAî �DÊË ÈA �ª�KQ�.Ë@ �IÊëA�K QCRI: Portugal qualified for the finals very </context>
</contexts>
<marker>Lavie, Denkowski, 2009</marker>
<rawString>Alon Lavie and Michael J. Denkowski. 2009. The METEOR metric for automatic evaluation of machine translation. Machine Translation, 23(2–3):105–115, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Maamouri</author>
<author>Ann Bies</author>
<author>Tim Buckwalter</author>
<author>Wigdan Mekki</author>
</authors>
<title>The Penn Arabic Treebank: building a large-scale annotated Arabic corpus.</title>
<date>2004</date>
<booktitle>In NEMLAR Conference on Arabic Language Resources and Tools,</booktitle>
<pages>102--109</pages>
<contexts>
<context position="1860" citStr="Maamouri et al., 2004" startWordPosition="271" endWordPosition="274">f identifying lexical chunks in the sentence for common as well as proper nouns, and labeling each with one of the 25 nominal supersense categories. Figure 1 illustrates two such labelings of an Arabic sentence. Like the narrower problem of named entity recognition, supersense tagging of text holds attraction as a way of inferring representations that move toward language independence. Here we consider the problem of nominal supersense tagging for Arabic, a language with ca. 300 million speakers and moderate linguistic resources, including a WordNet (Elkateb et al., 2006), annotated datasets (Maamouri et al., 2004; Hovy et al., 2006), monolingual corpora, and large amounts of Arabic-English parallel data. The supervised learning approach that is used in state-of-the-art English supersense taggers (Cia���������� ����� ������� Ann-A Gloss Ann-B controls manager the-windows in configuration and-layout windows the-applications ‘The window manager controls the configuration and layout of application windows.’ Figure 1: A sentence from the “X Window System” article with supersense taggings from two annotators and post hoc English glosses and translation. ramita and Altun, 2006) is problematic for Arabic, sin</context>
</contexts>
<marker>Maamouri, Bies, Buckwalter, Mekki, 2004</marker>
<rawString>Mohamed Maamouri, Ann Bies, Tim Buckwalter, and Wigdan Mekki. 2004. The Penn Arabic Treebank: building a large-scale annotated Arabic corpus. In NEMLAR Conference on Arabic Language Resources and Tools, pages 102–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="8993" citStr="Merialdo, 1994" startWordPosition="1378" endWordPosition="1379">e projection procedure constructs the Arabic-to-English mapping {1→ 11, 4→ 43,{5, 6}→ 76}, resulting in the tagging shown in the bottom row. weighted to favor earlier senses (presumed by lexicographers to be more frequent) and then the supersense with the greatest aggregate weight is selected. Formally: Let senses(w) be the ordered list of AWN senses of lemma w. Let senses(w, s) c_ senses(w) be those senses that map to a given supersense s. We choose arg maxs(|senses(w, s)|/ mini:senses(w)iEsenses(w,s) i). 3.2 Unsupervised Sequence Models Unsupervised sequence labeling is our second approach (Merialdo, 1994). Although it was largely developed for part-of-speech tagging, the hope is to use in-domain Arabic data (the unannotated Wikipedia corpus discussed in §2) to infer clusters that correlate well with supersense groupings. We applied the generative, feature-based model of Berg-Kirkpatrick et al. (2010), replicating a featureset used previously for NER (Mohit et al., 2012)— including context tokens, character n-grams, and POS—and adding the vocalized stem and several stem shape features: 1) ContainsDigit?; 2) digits replaced by #; 3) digit sequences replaced by # (for stems mixing digits with oth</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2):155–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Carmen Banea</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning multilingual subjective language via crosslingual projections.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL</booktitle>
<pages>976--983</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="10244" citStr="Mihalcea et al., 2007" startWordPosition="1569" endWordPosition="1573">rue for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique that we call MT-inthe-mi</context>
</contexts>
<marker>Mihalcea, Banea, Wiebe, 2007</marker>
<rawString>Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2007. Learning multilingual subjective language via crosslingual projections. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL 2007), pages 976–983, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Claudia Leacock</author>
<author>Randee Tengi</author>
<author>Ross T Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proceedings of the Workshop on Human Language Technology (HLT ’93),</booktitle>
<pages>303--308</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Plainsboro, NJ, USA,</location>
<contexts>
<context position="7230" citStr="Miller et al., 1993" startWordPosition="1088" endWordPosition="1091">e QCRI system used a 5-gram modified Kneser-Ney language model that generated fullcased forms (Chen and Goodman, 1999). cdec used a 4-gram KN language model over lowercase forms and was recased in a post-processing step. Both language models were trained using the Gigaword v. 4 corpus. Both systems were tuned to optimize BLEU on a held-out development set (Papineni et al., 2002). English supersense tagger. For English supersense tagging, an open-source reimplementation of the approach of Ciaramita and Altun (2006) was released by Michael Heilman.4 This tagger was trained on the SemCor corpus (Miller et al., 1993) and achieves 77% F1 in-domain. 3 Methods We explored 3 approaches to the supersense tagging of Arabic: heuristic tagging with a lexicon, unsupervised sequence tagging, and MT-in-the-middle. 3.1 Heuristic Tagging with a Lexicon Using the lexicon built from AWN and OntoNotes (see §2), our heuristic approach works as follows: 1. Stem and vocalize; we used MADA (Habash and Rambow, 2005; Roth et al., 2008). 2. Greedily detect word sequences matching lexicon entries from left to right. 3. If a lexicon entry has more than one associated supersense, Arabic WordNet synsets are 3QCRI accomplishes this </context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>George A. Miller, Claudia Leacock, Randee Tengi, and Ross T. Bunker. 1993. A semantic concordance. In Proceedings of the Workshop on Human Language Technology (HLT ’93), pages 303–308, Plainsboro, NJ, USA, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Behrang Mohit</author>
<author>Nathan Schneider</author>
<author>Rishav Bhowmick</author>
<author>Kemal Oflazer</author>
<author>Noah A Smith</author>
</authors>
<title>Recall-oriented learning of named entities in Arabic Wikipedia.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2012),</booktitle>
<pages>162--173</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Avignon, France,</location>
<contexts>
<context position="3838" citStr="Mohit et al., 2012" startWordPosition="568" endWordPosition="571">e first discuss the task and relevant resources (§2), then the approaches we explored (§3), and finally present experimental results and analysis in §4. 2 Task and Resources A gold standard corpus of sentences annotated with nominal supersenses (as in figure 1) facilitates automatic evaluation of supersense taggers. For development and evaluation we use 661 Proceedings of NAACL-HLT 2013, pages 661–667, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics the AQMAR Arabic Wikipedia Supersense Corpus1 (Schneider et al., 2012), which augmented a named entity corpus (Mohit et al., 2012) with nominal supersense tags. The corpus consists of 28 articles selected from four topical areas: history (e.g., “Islamic Golden Age”), science (“Atom”), sports (“Real Madrid”), and technology (“Linux”). Schneider et al. (2012) found the distributions of supersense categories in these four topical domains to be markedly different; e.g., most instances of (which includes kinds of software) occurred in the technology domain, whereas most s were found in the science domain. The 18 test articles have 1,393 sentences (39,916 tokens) annotated at least once.2 As the corpus was released with two an</context>
<context position="5889" citStr="Mohit et al. (2012)" startWordPosition="879" endWordPosition="882">g Arabic noun lemmas to supersense tags. This lexicon contains 23,000 types, of which 11,000 are multiword units. Token coverage of the test set is 18% (see table 1). Lexical units encountered in the test data were up to 9-ways supersenseambiguous; the average ambiguity of in-vocabulary tokens was 2.0 supersenses. Unlabeled Arabic text. For unsupervised learning we collected 100,000 words of Arabic Wikipedia text, not constrained by topic. The articles in this sample were subject to a minimum length threshold 1http://www.ark.cs.cmu.edu/ArabicSST 2Our development/test split of the data follows Mohit et al. (2012), but we exclude two test set documents—“Light” and “Ibn Tolun Mosque”—due to preprocessing issues. and are all cross-linked to corresponding articles in English, Chinese, and German. Arabic—�English machine translation. We used two independently developed Arabic-English MT systems. One (QCRI) is a phrase-based system (Koehn et al., 2003), similar to Moses (Koehn et al., 2007); the other (cdec) is a hierarchical phrasebased system (Chiang, 2007), as implemented in cdec (Dyer et al., 2010). Both were trained on about 370M tokens of parallel data provided by the LDC (by volume, mostly newswire a</context>
<context position="9365" citStr="Mohit et al., 2012" startWordPosition="1432" endWordPosition="1435"> senses(w, s) c_ senses(w) be those senses that map to a given supersense s. We choose arg maxs(|senses(w, s)|/ mini:senses(w)iEsenses(w,s) i). 3.2 Unsupervised Sequence Models Unsupervised sequence labeling is our second approach (Merialdo, 1994). Although it was largely developed for part-of-speech tagging, the hope is to use in-domain Arabic data (the unannotated Wikipedia corpus discussed in §2) to infer clusters that correlate well with supersense groupings. We applied the generative, feature-based model of Berg-Kirkpatrick et al. (2010), replicating a featureset used previously for NER (Mohit et al., 2012)— including context tokens, character n-grams, and POS—and adding the vocalized stem and several stem shape features: 1) ContainsDigit?; 2) digits replaced by #; 3) digit sequences replaced by # (for stems mixing digits with other characters); 4) YearLike?—true for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize thei</context>
<context position="19573" citStr="Mohit et al. (2012)" startWordPosition="3075" endWordPosition="3078">age rate in the gold standard. In table 1, and translation edit rate (TER) (Snover et al., 2006) 9The judge did not see alignments or supersense categories. 10For technology articles, the differences in F1 scores between the two systems were 6.1 and 4.2 for Ann-A and Ann-B, respectively. For sports the respective differences were 4.3 and 4.4. In the other domains the differences never exceeded 3.3. Interestingly, this is the only generalization about topical domain performance we were able to find that holds across annotators and systems, in contrast with the stark topical effects observed by Mohit et al. (2012) for NER. BLEU METEOR TER Lex. Prec. QCRI 32.86 32.10 0.46 91.9% cdec 28.84 31.38 0.49 90.0% Table 2: MT quality measures comparing the two systems over 140 sentences. The first three are automatic measures with 1 reference translation. For the fourth, a bilingual judged the translation acceptability of phrases that were identified as supersense mentions by the English tagger (lexical projection precision). noun coverage gains track overall improvements. If QCRI produces better translations, why is cdec more useful for supersense tagging? As noted in §3.3, cdec gives word-level alignments (eve</context>
</contexts>
<marker>Mohit, Schneider, Bhowmick, Oflazer, Smith, 2012</marker>
<rawString>Behrang Mohit, Nathan Schneider, Rishav Bhowmick, Kemal Oflazer, and Noah A. Smith. 2012. Recall-oriented learning of named entities in Arabic Wikipedia. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2012), pages 162–173, Avignon, France, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="6991" citStr="Papineni et al., 2002" startWordPosition="1052" endWordPosition="1055">et al., 2010). Both were trained on about 370M tokens of parallel data provided by the LDC (by volume, mostly newswire and UN data). Each system includes preprocessing for Arabic morphological segmentation and orthographic normalization.3 The QCRI system used a 5-gram modified Kneser-Ney language model that generated fullcased forms (Chen and Goodman, 1999). cdec used a 4-gram KN language model over lowercase forms and was recased in a post-processing step. Both language models were trained using the Gigaword v. 4 corpus. Both systems were tuned to optimize BLEU on a held-out development set (Papineni et al., 2002). English supersense tagger. For English supersense tagging, an open-source reimplementation of the approach of Ciaramita and Altun (2006) was released by Michael Heilman.4 This tagger was trained on the SemCor corpus (Miller et al., 1993) and achieves 77% F1 in-domain. 3 Methods We explored 3 approaches to the supersense tagging of Arabic: heuristic tagging with a lexicon, unsupervised sequence tagging, and MT-in-the-middle. 3.1 Heuristic Tagging with a Lexicon Using the lexicon built from AWN and OntoNotes (see §2), our heuristic approach works as follows: 1. Stem and vocalize; we used MADA </context>
<context position="16479" citStr="Papineni et al., 2002" startWordPosition="2579" endWordPosition="2582">d by the supervised English tagger (at least in the SemCor evaluation; see §2), and also well below the 70% interannotator F1 reported by Schneider et al. (2012). We therefore examine the weaknesses of our approach for Arabic. 4.1 MT for Projection In analyzing our projection framework, we performed a small-scale MT evaluation with the Wikipedia data. Reference English translations for 140 Arabic Wikipedia sentences—5 per article in the corpus—were elicited from a bilingual linguist. Table 2 compares the two systems under three standard metrics of overall sentence translation quality.8 8BLEU (Papineni et al., 2002); METEOR (Banerjee and Lavie, 2005; Lavie and Denkowski, 2009), with default options; 664 .¡ƒñË@ ú�¯ @Yg. b&apos;.,ª“ ::j�‚Ë@ &amp;quot;ñÓ o@ñ;Èñk Ðñm�� ( �HA�KðQ��ºËB� @ ) �éJ.ËA‚Ë@ �HA�Jj ~‚Ë@ áÓ �éK.Am�... áÓ �èP �YË@ vñº= QCRI: corn consists of a negative shipments (electron hovering around the nucleus of the shipment ) are very small in the center. (3/6) cdec: The corn is composed of negative shipments ( electronics ) cloud hovering over the nucleus of a very small positive shipment in the center . (2/6) � . �HAJ��®’�JË@ P@ñ ~‚Ó È@ñ£ Ð �Qî�E ÕË ÈA �ª�KQ�.ËA�¯ , �éËñîD„Ë@ ú�æî�D�JÖß. �HAJ��KAî �DÊË ÈA �</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Translationbased projection for multilingual coreference resolution.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2012),</booktitle>
<pages>720--730</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montréal, Canada,</location>
<contexts>
<context position="3216" citStr="Rahman and Ng, 2012" startWordPosition="474" endWordPosition="477">ords that are annotated for English). Here, we reserve that corpus for development and evaluation, not training. We explore several approaches in this paper, the most effective of which is to (1) translate the Arabic sentence into English, returning the alignment structure between the source and target, (2) apply English supersense tagging to the target sentence, and (3) heuristically project the tags back to the Arabic sentence across these alignments. This “MT-in-themiddle” approach has also been successfully used for mention detection (Zitouni and Florian, 2008) and coreference resolution (Rahman and Ng, 2012). We first discuss the task and relevant resources (§2), then the approaches we explored (§3), and finally present experimental results and analysis in §4. 2 Task and Resources A gold standard corpus of sentences annotated with nominal supersenses (as in figure 1) facilitates automatic evaluation of supersense taggers. For development and evaluation we use 661 Proceedings of NAACL-HLT 2013, pages 661–667, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics the AQMAR Arabic Wikipedia Supersense Corpus1 (Schneider et al., 2012), which augmented a named entity corpu</context>
<context position="11185" citStr="Rahman and Ng, 2012" startWordPosition="1722" endWordPosition="1725">supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique that we call MT-inthe-middle projection. This approach does not depend on having parallel data in the training domain, but rather on an Arabic→English machine translation system that can be applied to the sentences we wish to tag. The approach is inspired by token-level pseudo-parallel data methods of previous work (Zitouni and Florian, 2008; Rahman and Ng, 2012). MT output for this language pair is far from perfect—especially for Wikipedia text, which is distant from the domain of the translation system’s training data—but, in the spirit of Church and Hovy (1993), we conjecture that it may still be useful. The method is as follows: 1. Preprocess the input Arabic sentence a to match the decoder’s model of Arabic. 2. Translate the sentence, recovering not just the English output eˆ but also the derivation/alignment structure z relating words and/or phrases of the English output to words and/or phrases of the Arabic input. 3. Apply the English supersens</context>
</contexts>
<marker>Rahman, Ng, 2012</marker>
<rawString>Altaf Rahman and Vincent Ng. 2012. Translationbased projection for multilingual coreference resolution. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2012), pages 720–730, Montréal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Roth</author>
<author>Owen Rambow</author>
<author>Nizar Habash</author>
<author>Mona Diab</author>
<author>Cynthia Rudin</author>
</authors>
<title>Arabic morphological tagging, diacritization, and lemmatization using lexeme models and feature ranking.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL08: HLT,</booktitle>
<pages>117--120</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="7635" citStr="Roth et al., 2008" startWordPosition="1153" endWordPosition="1156">r. For English supersense tagging, an open-source reimplementation of the approach of Ciaramita and Altun (2006) was released by Michael Heilman.4 This tagger was trained on the SemCor corpus (Miller et al., 1993) and achieves 77% F1 in-domain. 3 Methods We explored 3 approaches to the supersense tagging of Arabic: heuristic tagging with a lexicon, unsupervised sequence tagging, and MT-in-the-middle. 3.1 Heuristic Tagging with a Lexicon Using the lexicon built from AWN and OntoNotes (see §2), our heuristic approach works as follows: 1. Stem and vocalize; we used MADA (Habash and Rambow, 2005; Roth et al., 2008). 2. Greedily detect word sequences matching lexicon entries from left to right. 3. If a lexicon entry has more than one associated supersense, Arabic WordNet synsets are 3QCRI accomplishes this using MADA (Habash and Rambow, 2005; Roth et al., 2008). cdec includes a custom CRFbased segmenter and standard normalization rules. 4http://www.ark.cs.cmu.edu/mheilman/questions 662 Eˆ 2 2 ˆe P 3 a N Aˆ 1 3 1 N Projected supersense tagging Automatic English supersense tagging English sentence Arabic sentence (e.g., token 6 aligns to English tokens 7–9) Arabic POS tagging 4 5 4 A 6 5 N 7 8 9 N 6 Figure</context>
</contexts>
<marker>Roth, Rambow, Habash, Diab, Rudin, 2008</marker>
<rawString>Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab, and Cynthia Rudin. 2008. Arabic morphological tagging, diacritization, and lemmatization using lexeme models and feature ranking. In Proceedings of ACL08: HLT, pages 117–120, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Schneider</author>
<author>Behrang Mohit</author>
<author>Kemal Oflazer</author>
<author>Noah A Smith</author>
</authors>
<title>Coarse lexical semantic annotation with supersenses: an Arabic case study.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012),</booktitle>
<pages>253--258</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="2574" citStr="Schneider et al., 2012" startWordPosition="376" endWordPosition="379">. The supervised learning approach that is used in state-of-the-art English supersense taggers (Cia���������� ����� ������� Ann-A Gloss Ann-B controls manager the-windows in configuration and-layout windows the-applications ‘The window manager controls the configuration and layout of application windows.’ Figure 1: A sentence from the “X Window System” article with supersense taggings from two annotators and post hoc English glosses and translation. ramita and Altun, 2006) is problematic for Arabic, since there are supersense annotations for only a small amount of Arabic text (65,000 words by Schneider et al., 2012, versus the 360,000 words that are annotated for English). Here, we reserve that corpus for development and evaluation, not training. We explore several approaches in this paper, the most effective of which is to (1) translate the Arabic sentence into English, returning the alignment structure between the source and target, (2) apply English supersense tagging to the target sentence, and (3) heuristically project the tags back to the Arabic sentence across these alignments. This “MT-in-themiddle” approach has also been successfully used for mention detection (Zitouni and Florian, 2008) and co</context>
<context position="4067" citStr="Schneider et al. (2012)" startWordPosition="601" endWordPosition="604">nominal supersenses (as in figure 1) facilitates automatic evaluation of supersense taggers. For development and evaluation we use 661 Proceedings of NAACL-HLT 2013, pages 661–667, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics the AQMAR Arabic Wikipedia Supersense Corpus1 (Schneider et al., 2012), which augmented a named entity corpus (Mohit et al., 2012) with nominal supersense tags. The corpus consists of 28 articles selected from four topical areas: history (e.g., “Islamic Golden Age”), science (“Atom”), sports (“Real Madrid”), and technology (“Linux”). Schneider et al. (2012) found the distributions of supersense categories in these four topical domains to be markedly different; e.g., most instances of (which includes kinds of software) occurred in the technology domain, whereas most s were found in the science domain. The 18 test articles have 1,393 sentences (39,916 tokens) annotated at least once.2 As the corpus was released with two annotators’ (partially overlapping) taggings, rather than a single gold standard, we treat the output of each annotator as a separate test set. Both annotated some of every article; the first (Ann-A) annotated 759 sentences, the se</context>
<context position="16018" citStr="Schneider et al. (2012)" startWordPosition="2510" endWordPosition="2513">oved labeled scores. We further tested simple hybrids combining the lexicon-based and MT-based approaches. Applying MT-in-the-middle first, then expanding token coverage with the lexicon improves recall at a small cost to precision (table 1, last row). Combining the techniques in the reverse order is slightly worse than MTbased projection without consulting the lexicon. MT-in-the middle improves upon the lexicon-only baseline, yet performance is still dwarfed by the supervised English tagger (at least in the SemCor evaluation; see §2), and also well below the 70% interannotator F1 reported by Schneider et al. (2012). We therefore examine the weaknesses of our approach for Arabic. 4.1 MT for Projection In analyzing our projection framework, we performed a small-scale MT evaluation with the Wikipedia data. Reference English translations for 140 Arabic Wikipedia sentences—5 per article in the corpus—were elicited from a bilingual linguist. Table 2 compares the two systems under three standard metrics of overall sentence translation quality.8 8BLEU (Papineni et al., 2002); METEOR (Banerjee and Lavie, 2005; Lavie and Denkowski, 2009), with default options; 664 .¡ƒñË@ ú�¯ @Yg. b&apos;.,ª“ ::j�‚Ë@ &amp;quot;ñÓ o@ñ;Èñk Ðñm�� </context>
</contexts>
<marker>Schneider, Mohit, Oflazer, Smith, 2012</marker>
<rawString>Nathan Schneider, Behrang Mohit, Kemal Oflazer, and Noah A. Smith. 2012. Coarse lexical semantic annotation with supersenses: an Arabic case study. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), pages 253–258, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Noah A Smith</author>
</authors>
<title>Bilingual parsing with factored estimation: using English to parse Korean.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP’04),</booktitle>
<pages>49--56</pages>
<location>Barcelona,</location>
<contexts>
<context position="10203" citStr="Smith and Smith, 2004" startWordPosition="1561" endWordPosition="1564">ts with other characters); 4) YearLike?—true for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favo</context>
</contexts>
<marker>Smith, Smith, 2004</marker>
<rawString>David A. Smith and Noah A. Smith. 2004. Bilingual parsing with factored estimation: using English to parse Korean. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP’04), pages 49–56, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA</booktitle>
<pages>223--231</pages>
<location>Cambridge, Massachusetts, USA,</location>
<contexts>
<context position="19050" citStr="Snover et al., 2006" startWordPosition="2991" endWordPosition="2994">e instead of atom. Others are more nuanced, e.g., shipments for charges and electronics for electrons. Transliteration errors included IMAX in place of EMACS and genoa lynx for GNU Linux. However, lexical projection precision seems to be a relatively small part of the problem, especially considering that not all translation errors lead to supersense tagging errors. Lexical projection recall was not measured, but noun token coverage (see table 1) is instructive. Most noun tokens ought to be tagged; 77% is the noun coverage rate in the gold standard. In table 1, and translation edit rate (TER) (Snover et al., 2006) 9The judge did not see alignments or supersense categories. 10For technology articles, the differences in F1 scores between the two systems were 6.1 and 4.2 for Ann-A and Ann-B, respectively. For sports the respective differences were 4.3 and 4.4. In the other domains the differences never exceeded 3.3. Interestingly, this is the only generalization about topical domain performance we were able to find that holds across annotators and systems, in contrast with the stark topical effects observed by Mohit et al. (2012) for NER. BLEU METEOR TER Lex. Prec. QCRI 32.86 32.10 0.46 91.9% cdec 28.84 3</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA 2006), pages 223–231, Cambridge, Massachusetts, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
</authors>
<title>Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL’01),</booktitle>
<location>Pittsburgh, Pennsylvania, USA,</location>
<contexts>
<context position="10157" citStr="Yarowsky and Ngai, 2001" startWordPosition="1553" endWordPosition="1556">t sequences replaced by # (for stems mixing digits with other characters); 4) YearLike?—true for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to</context>
</contexts>
<marker>Yarowsky, Ngai, 2001</marker>
<rawString>David Yarowsky and Grace Ngai. 2001. Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL’01), Pittsburgh, Pennsylvania, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
<author>Richard Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the First International Conference on Human Language Technology Research (HLT’01),</booktitle>
<location>San Diego, California, USA,</location>
<contexts>
<context position="10180" citStr="Yarowsky et al., 2001" startWordPosition="1557" endWordPosition="1560"> (for stems mixing digits with other characters); 4) YearLike?—true for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so i</context>
</contexts>
<marker>Yarowsky, Ngai, Wicentowski, 2001</marker>
<rawString>David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Proceedings of the First International Conference on Human Language Technology Research (HLT’01), San Diego, California, USA, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Imed Zitouni</author>
<author>Radu Florian</author>
</authors>
<title>Mention detection crossing the language barrier.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP 2008),</booktitle>
<pages>600--609</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="3167" citStr="Zitouni and Florian, 2008" startWordPosition="467" endWordPosition="470">0 words by Schneider et al., 2012, versus the 360,000 words that are annotated for English). Here, we reserve that corpus for development and evaluation, not training. We explore several approaches in this paper, the most effective of which is to (1) translate the Arabic sentence into English, returning the alignment structure between the source and target, (2) apply English supersense tagging to the target sentence, and (3) heuristically project the tags back to the Arabic sentence across these alignments. This “MT-in-themiddle” approach has also been successfully used for mention detection (Zitouni and Florian, 2008) and coreference resolution (Rahman and Ng, 2012). We first discuss the task and relevant resources (§2), then the approaches we explored (§3), and finally present experimental results and analysis in §4. 2 Task and Resources A gold standard corpus of sentences annotated with nominal supersenses (as in figure 1) facilitates automatic evaluation of supersense taggers. For development and evaluation we use 661 Proceedings of NAACL-HLT 2013, pages 661–667, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics the AQMAR Arabic Wikipedia Supersense Corpus1 (Schneider et</context>
<context position="11163" citStr="Zitouni and Florian, 2008" startWordPosition="1717" endWordPosition="1721">ugh the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique that we call MT-inthe-middle projection. This approach does not depend on having parallel data in the training domain, but rather on an Arabic→English machine translation system that can be applied to the sentences we wish to tag. The approach is inspired by token-level pseudo-parallel data methods of previous work (Zitouni and Florian, 2008; Rahman and Ng, 2012). MT output for this language pair is far from perfect—especially for Wikipedia text, which is distant from the domain of the translation system’s training data—but, in the spirit of Church and Hovy (1993), we conjecture that it may still be useful. The method is as follows: 1. Preprocess the input Arabic sentence a to match the decoder’s model of Arabic. 2. Translate the sentence, recovering not just the English output eˆ but also the derivation/alignment structure z relating words and/or phrases of the English output to words and/or phrases of the Arabic input. 3. Apply</context>
</contexts>
<marker>Zitouni, Florian, 2008</marker>
<rawString>Imed Zitouni and Radu Florian. 2008. Mention detection crossing the language barrier. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP 2008), pages 600–609, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>