<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.9974735">
Crouching Dirichlet, Hidden Markov Model:
Unsupervised POS Tagging with Context Local Tag Generation
</title>
<author confidence="0.998988">
Taesun Moon, Katrin Erk, and Jason Baldridge
</author>
<affiliation confidence="0.993040666666667">
Department of Linguistics
University of Texas at Austin
1 University Station B5100
</affiliation>
<address confidence="0.743574">
Austin, TX 78712-0198 USA
</address>
<email confidence="0.998598">
{tsmoon,katrin.erk,jbaldrid}@mail.utexas.edu
</email>
<sectionHeader confidence="0.995026" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999680111111111">
We define the crouching Dirichlet, hidden
Markov model (CDHMM), an HMM for part-
of-speech tagging which draws state prior dis-
tributions for each local document context.
This simple modification of the HMM takes
advantage of the dichotomy in natural lan-
guage between content and function words. In
contrast, a standard HMM draws all prior dis-
tributions once over all states and it is known
to perform poorly in unsupervised and semi-
supervised POS tagging. This modification
significantly improves unsupervised POS tag-
ging performance across several measures on
five data sets for four languages. We also show
that simply using different hyperparameter
values for content and function word states in
a standard HMM (which we call HMM+) is
surprisingly effective.
</bodyText>
<sectionHeader confidence="0.998089" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999790653846154">
Hidden Markov Models (HMMs) are simple, ver-
satile, and widely-used generative sequence models.
They have been applied to part-of-speech (POS) tag-
ging in supervised (Brants, 2000), semi-supervised
(Goldwater and Griffiths, 2007; Ravi and Knight,
2009) and unsupervised (Johnson, 2007) training
scenarios. Though discriminative models achieve
better performance in both semi-supervised (Smith
and Eisner, 2005) and supervised (Toutanova et al.,
2003) learning, there has been only limited work on
unsupervised discriminative sequence models (e.g.,
on synthetic data and protein sequences (Xu et al.,
2006)), and none to POS tagging.
The tagging accuracy of purely unsupervised
HMMs is far below that of supervised and semi-
supervised HMMs; this is unsurprising as it is still
not well understood what kind of structure is being
found by an unconstrained HMM (Headden III et al.,
2008). However, HMMs are fairly simple directed
graphical models, and it is straightforward to ex-
tend them to define alternative generative processes.
This also applies to linguistically motivated HMMs
for recovering states and sequences that correspond
more closely to those implicitly defined by linguists
when they label sentences with parts-of-speech.
One way in which a basic HMM’s structure is a
poor model for POS tagging is that there is no inher-
ent distinction between (open-class) content words
and (closed-class) function words. Here, we propose
two extensions to the HMM. The first, HMM+, is a
very simple modification where two different hyper-
parameters are posited for content states and func-
tion states, respectively. The other is the crouch-
ing Dirichlet, hidden Markov model (CDHMM), an
extended HMM that captures this dichotomy based
on the statistical evidence that comes from context.
Content states display greater variance across lo-
cal context (e.g. sentences, paragraphs, documents),
and we capture this variance by adding a component
to the model for content states that is based on la-
tent Dirichlet allocation (Blei et al., 2003). This ex-
tension is in some ways similar to the LDAHMM
of Griffiths et al. (2005). Both models are compos-
ite in that two distributions do not mix with each
other. Unlike the LDAHMM, the generation of con-
tent states is folded into the CDHMM process.
We compare the HMM+ and CDHMM against a
basic HMM and LDAHMM on POS tagging on a
more extensive and diverse set of languages than
previous work in monolingual unsupervised POS
tagging: four languages from three families (Ger-
manic: English and German; Romance: Portuguese;
</bodyText>
<page confidence="0.492892">
196
</page>
<note confidence="0.9623535">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 196–206,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.995952875">
and Mayan: Uspanteko). The CDHMM easily out-
performs all other models, including HMM+, across
three measures (accuracy, F-score, and variation
of information) for unsupervised POS tagging on
most data sets. However, the HMM+ is surpris-
ingly competitive, outperforming the basic HMM
and LDAHMM, and rivaling or even passing the
CDHMM on some measures and data sets.
</bodyText>
<sectionHeader confidence="0.990244" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.985098">
The Bayesian formulation for a basic HMM (Gold-
water and Griffiths, 2007) is:
</bodyText>
<equation confidence="0.99974975">
ψt|ξ — Dir(ξ)
δt|γ — Dir(γ)
wi|ti = t — Mult(ψt)
ti|ti−1 = t — Mult(δt)
</equation>
<bodyText confidence="0.999989047619047">
Dir is the conjugate Dirichlet prior to Mult (a multi-
nomial distribution). The state transitions are gen-
erated by Mult(δt) whose prior δt is generated by
Dir(γ) with a symmetric (i.e. uniform) hyperparam-
eter γ. Emissions are generated by Mult(ψt) with
a prior ψt generated by Dir(ξ) with a symmetric
hyperparameter ξ. Hyperparameter values smaller
than one encourage posteriors that are peaked, with
smaller values increasing this concentration. It is
not necessary that the hyperparameters be symmet-
ric, but this is a common approach when one wants
to be naive about the data. This is particularly ap-
propriate in unsupervised POS tagging with regard
to novel data since there won’t be a priori grounds
for favoring certain distributions over others.
There is considerable work on extensions to
HMM-based unsupervised POS tagging (see §6),
but here we concentrate on the LDAHMM (Grif-
fiths et al., 2005), which models topics and state
sequences jointly. The model is a composite of a
probabilistic topic model and an HMM in which a
single state is allocated for words generated from
the topic model. A strength of this model is that it
is able to use less supervision than previous topic
models since it does not require a stopword list.
While the topic model component still uses the bags-
of-words assumption, the joint model infers which
words are more likely to carry topical content and
which words are more likely to contribute to the
local sequence. This model is competitive with a
standard topic model, and its output is also compet-
itive when compared with a standard HMM. How-
ever, Griffiths et al. (2005) note that the topic model
component inevitably loses some finer distinctions
with respect to parts-of-speech. Though many con-
tent states such as adjectives, verbs, and nouns can
vary a great deal across documents, the topic state
groups these words together. This leads to assign-
ment of word tokens to clusters that are a poorer fit
for POS tagging. This paper shows that a model that
conflates the LDAHMM topics with content states
can significantly improve POS tagging.
</bodyText>
<sectionHeader confidence="0.992104" genericHeader="method">
3 Models
</sectionHeader>
<bodyText confidence="0.986336303030303">
We aim to model the fact that in many languages
words can generally be grouped into function words
and content words and that these groups often
have significantly different distributions. There are
few function words and they appear frequently,
while there are many content words appearing infre-
quently. Another difference in distribution is often
implied in information retrieval by the use of stop-
word filters and tf-idf values to remove or reduce the
influence of words which occur frequently but have
low variance (i.e. their global probability is similar
to their local probability in a document).
A difference in distribution is also revealed when
the parts-of-speech are known. When no smoothing
parameters are added, the joint probability of a word
that is not ‘the’ or ‘a’ occurring with a DT tag (in
the Penn Treebank) is almost always zero. Similarly
peaked distributions are observed for other function
categories such as MD and CC. On the other hand,
the joint probability of any word occurring with NN
is much less likely to be zero and the distribution is
much less likely to be peaked.
We attempt to account for these two distributional
properties—that certain words have higher variance
across contexts (e.g. a document) and that certain
tags have more peaked emission distributions—in a
sequence model. To do this, we define the crouching
Dirichlet, hidden Markov model1 (CDHMM). This
model, like LDAHMM, captures items of high vari-
ance across contexts, but it does so without losing
1We call our model a “crouching Dirichlet” model since it
involves a Dirichlet prior that generates distributions for certain
states as if it were “crouching” on the side.
</bodyText>
<page confidence="0.758582">
197
</page>
<figureCaption confidence="0.804379">
Figure 1: Graphical representation of relevant vari-
</figureCaption>
<bodyText confidence="0.9989045">
ables and dependencies at a given time step i. Ob-
served word wi is dependent on hidden state ti.
Edges to priors θ, φ, ψ may or may not be activated
depending on the value of ti. The edge to transition
prior δ is always activated. Hyperparameters to pri-
ors are represented by dots. See §3.1 for details.
sequence distinctions, namely, a given word’s lo-
cal function via its part-of-speech. We also define
the HMM+, a simple adaptation of a basic HMM
which accounts for the latter property by using dif-
ferent priors for emissions from content and function
states.
</bodyText>
<subsectionHeader confidence="0.987328">
3.1 CDHMM
</subsectionHeader>
<bodyText confidence="0.9999168">
The CDHMM incorporates an LDA-like module to
its graphical structure in order to capture words
and tags which have high variance across contexts.
Such tags correspond to content states. Like the
LDAHMM, the model is composite in that distribu-
tions over a single random variable are composed
of several different distribution functions which de-
pend on the value of the underlying variable.
We posit the following model (see fig. 1 for a dia-
gram of dependencies and all variables involved at a
single time step). We observe a sequence of tokens
w=(w1, ... , wN) that we assume is generated by
an underlying state sequence t=(t1, ... , tN) over a
state alphabet T with first order Markov dependen-
cies. T is a union of disjoint content states C and
function states F. In this composite model, the pri-
ors for the emission and transition for each step in
the sequence depend on whether state t at step i is
tEC or tEF. If tEC, the word emission is depen-
dent on φ (the content word prior) and the state tran-
sition is dependent on θ (the “topic” prior) and δ (the
transition prior). If tEF, the word emission proba-
bility is dependent on ψ (the function word prior)
and the state transition on δ (again, the transition
prior). Therefore, if tEF, the transition and emis-
sion structure is identical to the standard Bayesian
HMM.
To elaborate, three prior distributions are defined
globally for this model: (1) δt, the transition prior
such that p(t|t, δt) = δt|t (2) ψt, the function word
prior such that p(w|t, ψt) = ψw|t (3) φt, the content
word prior such that p(w|t, φt) = φw|t. Locally for
each context d (documents in our case), we define
θd, the topic prior such that p(t|θd) = θt|d for tEC.
The generative story is as follows:
</bodyText>
<listItem confidence="0.914246785714286">
1. For each state tET
(a) Draw a distribution over states δt —
Dir(γ)
(b) If tEC, draw a distribution over words
φt — Dir(β)
(c) If tEF, draw a distribution over words
ψt — Dir(ξ)
2. For each context d
(a) Draw a distribution θd — Dir(α) over
states tEC
(b) For each word wi in d
i. draw ti from δti−1 o θd
ii. if tiEC, then draw wi from φti, else
draw wi from ψti
</listItem>
<bodyText confidence="0.999603777777778">
For each context d, we draw a prior distribution
θd—formally identical to the LDA topic prior—that
is defined only for the states tEC. This prior is then
used to weight the draws for states at each word,
from δti−1 o θd, where we have defined the vector
valued operation o as follows:
where (δti−1 o θd)ti is the element corresponding to
state ti in the vector δti−1 o θd. Z is a normalization
constant such that the probability mass sums to one.
</bodyText>
<figure confidence="0.997964103448276">
β
ξ
φ
wi
ψ
ti
θ
δ
γ
α
�
(δti−1 o θd)ti = 1
Zδti|ti 1 tiEF
Z δti|ti−1 &apos; θti|d tiEC
1
198
Nt +Tγ+I[ti=ti−1] ti E C
(Nti|ti−1+ \Nti+1|ti+I[ti−1=ti=ti+1]+γ”
p(ti|t−i, w) a { Nwi|ti+β
Nti+Wβ
Nwi|ti+ξ
Nti+Wξ
Nti|di+α
Ndi+Cα
r rr
“ ”“ ”
Nti|ti−1+γ Nti+1|ti+I[ti−1=ti=ti+1]+γ
ti E F
Nti+Tγ+I[ti=ti−1]
</figure>
<figureCaption confidence="0.999685">
Figure 2: Conditional distribution for ti in the CDHMM.
</figureCaption>
<bodyText confidence="0.999407588235294">
The important thing to note is that the draw for
states at each word is proportional to a composite
of (a) the product of the individual elements of the
topic and transition priors when tiEC and (b) the
transition priors when tiEF. The draw is propor-
tional to the product of topic and transition priors
when tiEC because we have made a product of ex-
perts (PoE) factorization assumption (Hinton, 2002)
for tractability and to reduce the size of our model.
Without such an assumption, the transition parame-
ters would lie in a partitioned space of size O(|C|&apos;)
as opposed to O(|T |2) for the current model. Fur-
thermore, this combination of a composite hidden
state space with a product of experts assumption al-
lows us to capture high variance for certain states.
To summarize, the CDHMM is a composite
model where both the observed token and the hidden
state variable are composite distributions. For the
hidden state, this means that there is a “topical” ele-
ment with high variance across contexts that is em-
bedded in the state sequence for a subset of events.
We embed this element through a PoE assumption
where transitions into content states are modeled as
a product of the transition probability and the local
probability of the content state.
Inference. We use a Gibbs sampler (Gao and
Johnson, 2008) to learn the parameters of this and
all other models under consideration. In this infer-
ence regime, two distributions are of particular in-
terest. One is the posterior density and the other is
the conditional distribution, neither of which can be
learned in closed form.
Letting A = (θ, δ, φ, ψ) and h = (α, β, γ, ξ), the
posterior density is given as
</bodyText>
<equation confidence="0.9741438">
p(A|w, t; h) a p(w, t|A)p(A; h)
Note that p(w, t|A) is equal to
(φwi|tiθti|dδti|ti−1 )I[tiEC]
(1)
(ψwi|tiδti|ti−1)I[tiEF]
</equation>
<bodyText confidence="0.999972714285714">
where I[] is the indicator function, D is the number
of documents in the corpus and Nd is the number of
tokens in document d.
Another important measure is the conditional dis-
tribution which is conditioned on all the random
variables except the hidden state variable of interest
and which is derived by integrating out the priors:
</bodyText>
<equation confidence="0.980158">
p(ti|t−i, w; h) a p(ti|t−i; h)p(wi|t, w−i; h) (2)
</equation>
<bodyText confidence="0.999886521739131">
where t−i is the joint random variable t without ti
and w−i is w without wi.
There are two well-known approaches to conduct-
ing Gibbs sampling for HMMs. The default method
is to sample A based on the posterior, then sample
each ti based on the conditional distribution. An-
other approach is to sample directly from the con-
ditional distribution without sampling from the pos-
terior since the conditional distribution incorporates
the posterior through integration. This is called a
collapsed Gibbs sampler, which is the method em-
ployed for the models in this study.
The full conditional distribution for tag transitions
for the Gibbs sampler is given in Figure 2. At each
time step, we decrement all counts for the current
value of ti, sample a new value for ti from a multino-
mial proportional to the conditional distribution and
assign that value to ti. β, ξ are the hyperparameters
for the word emission priors of the content states and
function states, respectively. γ is the hyperparame-
ter for the state transition priors. α is the hyperpa-
rameter for the state prior given that it is in some
context d. Note that we have overridden notation so
</bodyText>
<figure confidence="0.866072142857143">
Nd
ri
i
D
ri
d
199
</figure>
<bodyText confidence="0.997102235294117">
that C and T here refer to the size of the alphabet.
W is the size of the vocabulary. Notation such as
Nti|ti_1 refers to the counts of the events indicated
by the subscript, minus the current token and tag un-
der consideration. Nti|ti_1 is the number of times ti
has occurred after ti_1 minus the tag for wi. N,,,i|ti
is the number of times wi has occurred with ti minus
the current value. Nti and Ndi are the counts for the
given tag and document minus the current value.
In its broad outline, the CDHMM is not much
more complicated than an HMM since the decompo-
sition (eqn. 1) is nearly identical to that of an HMM
with the exception that conditional probabilities for
a subset of the states—the content states—are local.
An inference algorithm can be derived that involves
no more than adding a single term to the standard
MCMC algorithm for HMMs (see Figure 2).
</bodyText>
<subsectionHeader confidence="0.998367">
3.2 BMM+
</subsectionHeader>
<bodyText confidence="0.99998805">
The CDHMM explicitly posits two different types
of states: function states and content states. Hav-
ing made this distinction, there is a very simple way
to capture the difference in emission distributions
for function and content states within an otherwise
standard HMM: posit different hyperparameters for
the two types. One type has a small hyperparame-
ter to model a sparse distribution for function words
and the other has a relatively large hyperparameter
to model a distribution with broader support. This
extension, which we refer to as HMM+, provides an
important benchmark to compare with the CDHMM
to see how much is gained by its additional ability to
model the fact that function words occur frequently
but have low variance across contexts.
As with the CDHMM, we use Gibbs sampling to
estimate the model parameters while holding the two
different hyperparameters fixed. The conditional
distribution for tag transitions for this model is iden-
tical to that in fig. 2 except that it does not have the
</bodyText>
<equation confidence="0.989846333333333">
Nti|di+�
second term in the first case where tiEC.
Ndi+Cce
</equation>
<bodyText confidence="0.9985514">
We are not aware of a published instance of such
an extension to the HMM—which our results show
to be surprisingly effective. Goldwater and Griffiths
(2007) posits different hyperparameters for individ-
ual states, but not for different groups of states.
</bodyText>
<table confidence="0.996037">
corpus tokens docs avg. tags
WSJ 974254 1801 541 43
Brown 797328 343 2325 80
Tiger 447079 1090 410 58
Floresta 197422 1956 101 19
Uspanteko 70125 29 2418 83
</table>
<tableCaption confidence="0.74945">
Table 2: Number of tokens, documents, average to-
kens per document and total tag types for each cor-
pus.
</tableCaption>
<sectionHeader confidence="0.973239" genericHeader="method">
4 Data and Experiments
</sectionHeader>
<bodyText confidence="0.995695666666667">
Data. We use five datasets from four languages
(English, German, Portuguese, Uspanteko) for eval-
uating POS tagging performance.
</bodyText>
<listItem confidence="0.999195285714286">
• English: the Brown corpus (Francis et al., 1982)
and the Wall Street Journal portion of the Penn
Treebank (Marcus et al., 1994).
• German: the Tiger corpus (Brants et al., 2002).
• Portuguese: the full Bosque subset of the Floresta
corpus (Afonso et al., 2002).
• Uspanteko (an endangered Mayan language of
</listItem>
<bodyText confidence="0.913720166666667">
Guatemala): morpheme-segmented and POS-
tagged texts collected and annotated by the
OKMA language documentation project (Pixabaj
et al., 2007); we use the cleaned-up version de-
scribed in Palmer et al. (2009).
Table 2 provides the statistics for these corpora.
We lowercase all words, do not remove any punc-
tuation or hapax legomena, and we do not replace
numerals with a single identifier. Due to the nature
of the models, document boundaries are retained.
Evaluation We report values for three evaluation
metrics on all five corpora, using their full tagsets.
</bodyText>
<listItem confidence="0.703445">
• Accuracy: We use a greedy search algorithm to
map each unsupervised tag to a gold label such
that accuracy is maximized. We evaluate on a
</listItem>
<bodyText confidence="0.965230857142857">
1-to-1 mapping between unsupervised tags and
gold labels, as well as many-to-1 (M-to-1), cor-
responding to the evaluation mappings used in
Johnson (2007). The 1-to-1 mapping provides a
stricter evaluation. The many-to-one mapping, on
the other hand, may be more adequate as unsu-
pervised tags tend to be more fine-grained than
</bodyText>
<table confidence="0.993351565217391">
200
Accuracy Model Pairwise P/R Scores VI
1-to-1 M-to-1 P R F
WSJ (50) HMM 0.34 (0.01) 0.49 (0.03) 0.51 (0.03) 0.19 (0.01) 0.28 (0.01) 3.72 (0.08)
LDAHMM 0.30 (0.04) 0.45 (0.04) 0.25 (0.07) 0.27 (0.03) 0.26 (0.04) 3.64 (0.14)
HMM+ 0.42 (0.04) 0.46 (0.05) 0.24 (0.03) 0.49 (0.03) 0.32 (0.03) 2.65 (0.15)
CDHMM 0.44 (0.01) 0.58 (0.02) 0.31 (0.01) 0.43 (0.03) 0.36 (0.02) 2.73 (0.08)
Brown (50) HMM 0.32 (0.01) 0.50 (0.02) 0.60 (0.02) 0.18 (0.00) 0.28 (0.01) 3.82 (0.05)
LDAHMM 0.28 (0.06) 0.41 (0.08) 0.25 (0.10) 0.28 (0.05) 0.25 (0.05) 3.71 (0.21)
HMM+ 0.43 (0.06) 0.48 (0.07) 0.29 (0.05) 0.50 (0.04) 0.37 (0.05) 2.63 (0.19)
CDHMM 0.48 (0.02) 0.62 (0.02) 0.32 (0.03) 0.54 (0.04) 0.40 (0.03) 2.48 (0.06)
Tiger (50) HMM 0.29 (0.02) 0.49 (0.02) 0.49 (0.04) 0.14 (0.01) 0.22 (0.02) 3.91 (0.06)
LDAHMM 0.31 (0.04) 0.50 (0.04) 0.26 (0.07) 0.24 (0.02) 0.25 (0.04) 3.51 (0.11)
HMM+ 0.41 (0.08) 0.44 (0.05) 0.25 (0.05) 0.58 (0.10) 0.35 (0.06) 2.70 (0.25)
CDHMM 0.47 (0.01) 0.61 (0.02) 0.45 (0.01) 0.58 (0.03) 0.50 (0.02) 2.72 (0.04)
Flor. (50) Usp. (50) HMM 0.36 (0.01) 0.49 (0.02) 0.39 (0.01) 0.18 (0.00) 0.25 (0.00) 3.63 (0.04)
LDAHMM 0.35 (0.02) 0.47 (0.02) 0.26 (0.04) 0.23 (0.03) 0.24 (0.02) 3.52 (0.09)
HMM+ 0.32 (0.02) 0.35 (0.03) 0.12 (0.02) 0.52 (0.05) 0.20 (0.02) 3.13 (0.06)
CDHMM 0.39 (0.02) 0.50 (0.02) 0.16 (0.02) 0.39 (0.03) 0.23 (0.02) 3.00 (0.06)
HMM 0.30 (0.01) 0.58 (0.03) 0.62 (0.05) 0.18 (0.01) 0.28 (0.01) 3.51 (0.06)
LDAHMM 0.36 (0.06) 0.59 (0.04) 0.55 (0.10) 0.29 (0.07) 0.38 (0.08) 3.22 (0.15)
HMM+ 0.35 (0.04) 0.52 (0.02) 0.28 (0.04) 0.43 (0.06) 0.34 (0.04) 2.58 (0.07)
CDHMM 0.36 (0.01) 0.64 (0.02) 0.37 (0.02) 0.27 (0.01) 0.31 (0.01) 2.73 (0.05)
</table>
<tableCaption confidence="0.8052275">
Table 1: Evaluation on WSJ, Brown, Tiger, Floresta and Uspanteko for models with 50 states. For VI, lower
is better
</tableCaption>
<bodyText confidence="0.532775666666667">
gold part-of-speech tags. In particular, they tend
to form semantically coherent sub-classes of gold
parts of speech.
</bodyText>
<listItem confidence="0.967706733333333">
• Pairwise Precision and Recall: Viewing tagging
as a clustering task over tokens, we evaluate pair-
wise precision (P) and recall (R) between the
model tag sequence (M) and gold tag sequence
(G) by counting the true positives (tp), false pos-
itives (fp) and false negatives (fn) between the
two and setting P = tp/(tp + fp) and R =
tp/(tp + fn). tp is the number of token pairs that
share a tag in M as well as in G, fp is the number
token pairs that share the same tag in M but have
different tags in G, and fn is the number token
pairs assigned a different tag in M but the same
in G (Meila, 2007). We also provide the f-score
which is the harmonic mean of P and R.
• Variation of Information (VI): The variation of
</listItem>
<bodyText confidence="0.925760142857143">
information is an information theoretic metric
that measures the amount of information lost and
gained in going from tag sequence M to G (Meila,
2007). It is defined as VI(M,G) = H(M) +
H(G) − 2I(M, G) where H denotes entropy and
I mutual information. Goldwater and Griffiths
(2007) noted that this measure can point out mod-
els that have more consistent errors in the form
of lower VI, even when accuracy figures are the
same.
We also report learning curves on M-to-1 with ge-
ometrically increasing training set sizes of 8, 16, 32,
64, 128, 256, 512, 1024, and all documents, or as
many as possible given the corpus.
</bodyText>
<sectionHeader confidence="0.999413" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999045">
In this section we discuss our parameter settings and
experimental results.
</bodyText>
<subsectionHeader confidence="0.918714">
5.1 Models and Parameters
</subsectionHeader>
<bodyText confidence="0.971171">
We compare four different models:
</bodyText>
<listItem confidence="0.97371875">
• HMM: a standard HMM
• HMM+: an HMM in which the hyperparameters
for the word emissions are asymmetric, such that
content states have different word emission priors
compared to function states.
• LDAHMM: an HMM with a distinguished state
that generates words from a topic model (Griffiths
et al., 2005)
</listItem>
<figure confidence="0.976704133333333">
201
HMM+ LDAHMM CDHMM
acc
0.0 0.1 0.2 0.3 0.4 0.5 0.6
20
30
40
50
acc
0.0 0.1 0.2 0.3 0.4 0.5 0.6
acc
0.0 0.1 0.2 0.3 0.4 0.5 0.6
WSJ Brown Tiger Floresta Uspanteko
WSJ Brown Tiger Floresta Uspanteko
WSJ Brown Tiger Floresta Uspanteko
</figure>
<figureCaption confidence="0.9606745">
Figure 3: Averaged many-to-one accuracy on the full tagset for the models HMM+, LDAHMM, CDHMM
when the number of states is set at 20, 30, 40 and 50 states.
</figureCaption>
<bodyText confidence="0.974602571428572">
• CDHMM: our HMM with context-based emis-
sions, where the context used is the document
We implemented all of these models, ensuring per-
formance differences are due to the models them-
selves rather than implementation details.
For all models, the transition hyperparameters -y
are set to 0.1. For the LDAHMM and HMM all emis-
sion hyperparameters are set to 0.0001. These fig-
ures are the MCMC settings that provided the best
results in Johnson (2007). For the models that distin-
guish content and function states (HMM+, CDHMM),
we fixed the number of content states at 5 and set the
function state emission hyperparameters � = 0.0001
and the content state emission hyperparameters Q =
0.1. For the models with an LDA or LDA-like com-
ponent (LDAHMM, CDHMM), we set the topic or
content-state hyperparameter α = 1.
For decoding, we use maximum posterior decod-
ing to obtain a single sample after the required burn-
in, as has been done in other unsupervised HMM
experiments. We use this sample for evaluation.
</bodyText>
<subsectionHeader confidence="0.785192">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.996043611111111">
Results for all models on the full tagset are provided
in table 1.2 Each number is the mean accuracy of
ten randomly initialized samples after a single chain
burn-in of 1000 iterations. The model with a sta-
tistically significant (p &lt; 0.05) best score for each
measure and data set is given in plain bold. In cases
2Similar results are obtained with reduced tagsets, as is com-
monly done in other work on unsupervised POS-tagging.
where the differences for the best models are not sig-
nificantly different from each other, but are signifi-
cantly better from the others, the top model scores
are given in bold italic.
CDHMM is extremely strong on the accuracy met-
ric: it wins or ties for all datasets for both 1-to-1 and
M-to-1 measures. For pairwise f-score, it obtains
the best score for two datasets (WSJ and Tiger), and
ties with HMM+ on Brown (we return to Uspanteko
and Floresta below in an experiment that varies the
number of states). For VI, HMM+ and CDHMM both
easily outperform the other models, with CDHMM
winning Brown and Uspanteko and HMM+ winning
Floresta.
In the case of Uspanteko, the absolute difference
in mean performance between models is smaller
overall but still significant. This is due to the reduced
variance between samples for all models. This is
striking because the non-CDHMM models have much
higher standard deviation on other corpora but have
sharply reduced standard deviation only for Uspan-
teko. The most likely explanation is that the Uspan-
teko corpus is much smaller than the other corpora.3
Nonetheless, CDHMM comes out strongest on most
measures.
A simple baseline for accuracy is to choose the
most frequent tag for all tokens; this gives accura-
cies of 0.14 (WSJ), 0.14 (Brown), 0.21 (Tiger), 0.20
</bodyText>
<footnote confidence="0.7256675">
3which is interesting in itself since the weak law of large
numbers implies that sample standard deviation decreases with
sample size, which in our case is the number of tokens rather
than the 10 samples under discussion
</footnote>
<table confidence="0.963383545454545">
202
Accuracy Model P/R Scores VI
1-to-1 M-to-1 P R F
Usp. (100) HMM 0.36 (0.01) 0.58 (0.01) 0.56 (0.02) 0.16 (0.00) 0.25 (0.01) 3.53 (0.04)
LDAHMM 0.35 (0.01) 0.58 (0.02) 0.45 (0.04) 0.17 (0.01) 0.24 (0.01) 3.46 (0.06)
HMM+ 0.35 (0.02) 0.41 (0.02) 0.18 (0.01) 0.36 (0.03) 0.24 (0.01) 3.25 (0.08)
CDHMM 0.40 (0.01) 0.59 (0.01) 0.25 (0.02) 0.27 (0.02) 0.26 (0.01) 3.05 (0.03)
Flor. (20) HMM 0.31 (0.02) 0.48 (0.03) 0.40 (0.03) 0.21 (0.01) 0.28 (0.02) 3.54 (0.10)
LDAHMM 0.35 (0.06) 0.46 (0.06) 0.27 (0.07) 0.45 (0.08) 0.33 (0.05) 3.10 (0.10)
HMM+ 0.37 (0.04) 0.50 (0.03) 0.30 (0.02) 0.45 (0.06) 0.36 (0.03) 2.62 (0.06)
CDHMM 0.44 (0.02) 0.55 (0.02) 0.30 (0.01) 0.53 (0.03) 0.39 (0.02) 2.39 (0.07)
</table>
<tableCaption confidence="0.993957">
Table 3: Evaluation for Uspanteko and Floresta. Experiments in this table use state sizes that correspond
</tableCaption>
<bodyText confidence="0.910630333333333">
more closely to the size of the tag sets in the respective corpora.
(Floresta), and 0.11 (Uspanteko). Clearly, all of the
models easily outperform this baseline.
</bodyText>
<figure confidence="0.9953219">
F−SCORE VI
WSJ Brown Tiger Floresta Uspanteko WSJ Brown Tiger Floresta Uspanteko
20
30
40
50
f−score
0.0 0.1 0.2 0.3 0.4 0.5
vi
0 1 2 3 4
</figure>
<bodyText confidence="0.99907115625">
Number of states. Figure 3 shows the change in
accuracy for the different models for different cor-
pora when the overall number of states is varied
between 20 and 50. The figure shows results for
M-to-1. All models with the exception of HMM+
show improvements as the number of states is in-
creased. This brings up the valid concern (Clark,
2003; Johnson, 2007) that a model could posit a
very large number of states and obtain high M-to-
1 scores. However, it is neither the case here nor
in any of the studies we cite. Furthermore, as is
strongly suggested with HMM+, it does not seem as
if all models will benefit from assuming a large num-
ber of states.
Looking at the results by number of states on VI
and f-score for CDHMM(Figure 5), it is clear that
Floresta displays the reverse pattern of all other data
sets where performance monotonically deteriorates
as state sizes are increased. Though the exact reason
is unknown, we believe it is partially due to the fact
that Floresta has 19 tags. We therefore wondered
whether positing a state size that more closely ap-
proximated the size of the gold tag set performs bet-
ter. Since the discrepancy is greatest for Uspanteko
and Floresta, we present tabulated results for exper-
iments with state settings of 100 and 20 states re-
spectively (table 3). With the exception of VI (where
lower is better) for Uspanteko, the scores generally
improve when the model state size is closer to the
gold size. M-to-1 goes down for Floresta when 20
states are posited, but this is to be expected since this
score is defined, to a certain extent, to do better with
</bodyText>
<figureCaption confidence="0.771003">
Figure 5: f-score and VI for CDHMM by number of
states
</figureCaption>
<bodyText confidence="0.997671666666667">
larger models.
Variance. As we average performance figures
over ten runs for each model, it is also instructive
to consider standard deviation across runs. Standard
deviation is lowest for the CDHMM models and the
vanilla HMM. Standard deviation is high for HMM+
and LDAHMM. This is not surprising for LDAHMM,
since it has fifty topic parameters in addition to the
number of states posited, and random initial condi-
tions would have greater effect on the outcome than
for the other models. It is unexpected, however, that
HMM+ has high variance over different chains. The
model shares the large content emission hyperpa-
rameter Q = 0.1 with CDHMM. At this point, it can
only be assumed that the additional LDA component
acts as a regularization factor for CDHMM and re-
duced the volatility in having a large emission hy-
perparameter.
</bodyText>
<page confidence="0.757872">
203
</page>
<note confidence="0.551928">
Brown WSJ Tiger
</note>
<figureCaption confidence="0.991354">
Figure 4: Learning curves on M-to-1 evaluation. The staples at each point represent two standard deviations.
</figureCaption>
<figure confidence="0.992619692307692">
hmm
hmm+
ldahmm
0 1 2 3 4 5 6
0.3 0.4 0.5 0.6
0 1 2 3 4 5 6 7 8 0 1 2 3 4 5 6 7 8
Uspanteko
0 1 2 3 4 5 6 7 8 0 1 2
Floresta
0.3 0.4 0.5 0.6
0.3 0.4 0.5 0.6
0.3 0.4 0.5 0.6
0.3 0.4 0.5 0.6
</figure>
<bodyText confidence="0.988761125">
Learning curves We present learning curves on
different sizes of subcorpora in Figure 4. The graphs
are box plots of the full M-1 accuracy figures on
10 randomly initialized training runs for seven sub-
corpora in Brown, nine in WSJ, Tiger, Floresta and
three in Uspanteko.
Comparing the graphs, the performance of HMM+
shows the strongest improvement for English and
German data as the amount of training data in-
creases. Also, it is evident that CDHMM posts con-
sistent performance gains across data sets as it trains
on more data. This stands in opposition to HMM and
LDAHMM which do not seem able to take advantage
of more information for WSJ and Floresta. This
suggests that performance for CDHMM and HMM+
could improve if the training corpora were aug-
mented with out-of-corpus raw data. One exception
to the consistent improvement over increased data is
the performance of the models on Uspanteko, which
uniformly flatline. One reason might be that the tags
are labeled over segmented morphemes instead of
words like the other corpora. Another could be that
Uspanteko has a relatively large number of tags in a
very small corpus.
</bodyText>
<sectionHeader confidence="0.999964" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.99998792">
Unsupervised POS tagging is an active area of re-
search. Most recent work has involved HMMs.
Given that an unconstrained HMM is not well under-
stood in POS tagging, much work has been done on
examining the mechanism and the properties of the
HMM as applied to natural language data (Johnson,
2007; Gao and Johnson, 2008; Headden III et al.,
2008). Conversely, there has also been work focused
on improving the HMM as an inference procedure
that looked at POS tagging as an example (Graca et
al., 2009; Liang and Klein, 2009). Nonparametric
HMMs for unsupervised POS tag induction (Snyder
et al., 2008; Van Gael et al., 2009) have seen partic-
ular activity due to the fact that model size assump-
tions are unnecessary and it lets the data “speak for
itself.”
There is also work on alternative unsupervised
models that are not HMMs (Sch¨utze, 1993; Abend
et al., 2010; Reichart et al., 2010b) as well as re-
search on improving evaluation of unsupervised tag-
gers (Frank et al., 2009; Reichart et al., 2010a).
Though they did not concentrate on unsupervised
methods, Haghighi and Klein (2006) conducted an
unsupervised experiment that utilized certain to-
ken features (e.g. character suffixes of 3 or less,
</bodyText>
<page confidence="0.762795">
204
</page>
<bodyText confidence="0.998727943396227">
has initial capital, etc.; the features themselves are
from Smith and Eisner (2005)) to learn parameters
in an undirected graphical model which was the
equivalent of an HMM in directed models. It was
also the first study to posit the one-to-one evalua-
tion criterion which has been repeated extensively
since (Johnson, 2007; Headden III et al., 2008;
Graca et al., 2009).
Finkel et al. (2007) is an interesting variant of un-
supervised POS tagging where a parse tree is as-
sumed and POS tags are induced from this structure
non-parametrically. It is the converse of unsuper-
vised parsing which assumes access to a tagged cor-
pus and induces a parsing model.
Other models more directly influenced or closely
parallel our work. Griffiths et al. (2005) is the work
that inspired the current approach where a set of
states is designated to capture variance across con-
texts. The primary goal of that model was to induce
a topic model given data that had not been filtered
of noise in the form of function words. As such,
distinguishing between topic states such that they
model different syntactic states was not attempted,
and we have seen in sec. 3 that such an extension is
not entirely straightforward.4 Boyd-Graber and Blei
(2009) has some parallels to our model in that a hid-
den variable over topics is distributed according to
a normalized product between a context prior and a
syntactic prior. However, it assumes a much greater
amount of information than we do in that a parse tree
as well as (possibly) POS tags are taken as observed.
The model has a very different goal from ours as
well, which is to infer a syntactically informed topic
model. Teichert and Daum´e III (2010) is another
study with close similarities to our own. This study
models distinctions between closed class words and
open class words within a modified HMM. It is un-
clear from their formulation how the distinction be-
tween open class and closed class words is learned.
There is also extensive literature on learning se-
quence structure from unlabeled text (Smith and
Eisner, 2005; Goldberg et al., 2008; Ravi and
Knight, 2009) which assume access to a tag dic-
tionary. Goldwater and Griffiths (2007) deserves
mention for examining a semi-supervised model
4We tested a variant of LDAHMM in which more than one
state can generate topics. It did not achieve good results.
that sampled emission hyperparameters for each
state rather than a single symmetric hyperparame-
ter. They showed that this outperformed a symmet-
ric model. An interesting heuristic model is Zhao
and Marcus (2009) that uses a seed set of closed
class words to classify open class words.
</bodyText>
<sectionHeader confidence="0.99844" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999997315789474">
We have shown that a hidden Markov model that
allocates a subset of the states to have distribu-
tions conditioned on localized domains can signif-
icantly improve performance in unsupervised part-
of-speech tagging. We have also demonstrated that
significant performance gains are possible simply
by setting a different emission hyperparameter for
a subgroup of the states. It is encouraging that these
results hold for both models not just on the WSJ but
across a diverse set of languages and measures.
We believe our proposed extensions to the HMM
are a significant contribution to the general HMM
and unsupervised POS tagging literature in that both
can be implemented with minimum modification
of existing MCMC inferred HMMs, have (nearly)
equivalent run times, produce output that is easy to
interpret since they are based on a generative frame-
work, and bring about considerable performance im-
provements at the same time.
</bodyText>
<sectionHeader confidence="0.997714" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99877775">
The authors would like to thank Elias Ponvert and
the anonymous reviewers. This work was supported
by a grant from the Morris Memorial Trust Fund of
the New York Community Trust.
</bodyText>
<sectionHeader confidence="0.996111" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999900727272727">
O. Abend, R. Reichart, and A. Rappoport. 2010. Im-
proved unsupervised POS induction through prototype
discovery. In Proceedings ofACL, pages 1298–1307.
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. Flo-
resta sint´a(c)tica”: a treebank for Portuguese. In Pro-
ceedings ofLREC, pages 1698–1703.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
Dirichlet allocation. The Journal ofMachine Learning
Research, 3:993–1022.
J. L. Boyd-Graber and D. Blei. 2009. Syntactic topic
models. In Proceedings ofNIPS, pages 185–192.
</reference>
<page confidence="0.539007">
205
</page>
<reference confidence="0.999970252427185">
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The TIGER treebank. In Proceedings of the
Workshop on Treebanks and Linguistic Theories.
T. Brants. 2000. TnT: a statistical part-of-speech tag-
ger. In Proceedings of conference on Applied natural
language processing, pages 224–231.
A. Clark. 2003. Combining distributional and morpho-
logical information for part of speech induction. In
Proceedings of EACL, pages 59–66.
J. R. Finkel, T. Grenager, and C. D. Manning. 2007. The
infinite tree. In Proceedings ofACL, pages 272–279.
W.N. Francis, H. Kuˇcera, and A.W. Mackie. 1982. Fre-
quency analysis of English usage: Lexicon and gram-
mar. Houghton Mifflin Harcourt.
S. Frank, S. Goldwater, and F. Keller. 2009. Evaluating
models of syntactic category acquisition without using
a gold standard. In Proceedings of CogSci.
J. Gao and M. Johnson. 2008. A comparison of Bayesian
estimators for unsupervised Hidden Markov Model
POS taggers. In Proceedings of EMNLP, pages 344–
352.
Y. Goldberg, M. Adler, and M. Elhadad. 2008. EM
can find pretty good HMM POS-taggers (when given
a good start). In Proceedings ofACL, pages 746–754.
S. Goldwater and T. L. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Proceedings ofACL, pages 744–751.
J. Graca, K. Ganchev, B. Taskar, and F. Pereira. 2009.
Posterior vs parameter sparsity in latent variable mod-
els. In Proceedings ofNIPS, pages 664–672.
T. L. Griffiths, M. Steyvers, D. M. Blei, and J. M. Tenen-
baum. 2005. Integrating topics and syntax. In Pro-
ceedings ofNIPS, pages 537–544.
A. Haghighi and D. Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
HLT/NAACL, pages 320–327.
W. P. Headden III, D. McClosky, and E. Charniak.
2008. Evaluating unsupervised part-of-speech tagging
for grammar induction. In Proceedings of COLING,
pages 329–336.
G.E. Hinton. 2002. Training products of experts by min-
imizing contrastive divergence. Neural Computation,
14(8):1771–1800.
M. Johnson. 2007. Why doesn’t EM find good HMM
POS-taggers. In Proceedings of EMNLP-CoNLL,
pages 296–305.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In Proceedings of HLT/NAACL, pages
611–619.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1994. Building a large annotated corpus of English:
The Penn Treebank. Comp. ling., 19(2):313–330.
M. Meila. 2007. Comparing clusterings—an informa-
tion based distance. Journal of Multivariate Analysis,
98(5):873–895.
A. Palmer, T. Moon, and J. Baldridge. 2009. Evaluat-
ing automation strategies in language documentation.
In Proceedings of the NAACL-HLT 2009 Workshop
on Active Learning for Natural Language Processing,
pages 36–44.
T. C. Pixabaj, M. A. Vicente M´endez, M. Vicente
M´endez, and O. A. Dami´an. 2007. Text Collections in
Four Mayan Languages. Archived in The Archive of
the Indigenous Languages of Latin America.
S. Ravi and K. Knight. 2009. Minimized models for
unsupervised part-of-speech tagging. In Proceedings
ofACL and AFNLP, pages 504–512.
R. Reichart, O. Abend, and A. Rappoport. 2010a. Type
level clustering evaluation: New measures and a POS
induction case study. In Proceedings of CoNLL, pages
77–87.
R. Reichart, R. Fattal, and A. Rappoport. 2010b. Im-
proved unsupervised POS induction using intrinsic
clustering quality and a Zipfian constraint. In Proceed-
ings of CoNLL, pages 57–66.
H. Sch¨utze. 1993. Part-of-speech induction from scratch.
In Proceedings ofACL, pages 251–258.
N.A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Pro-
ceedings ofACL, pages 354–362.
B. Snyder, T. Naseem, J. Eisenstein, and R. Barzilay.
2008. Unsupervised multilingual learning for POS
tagging. In Proceedings of EMNLP, pages 1041–
1050.
A.R. Teichert and H. Daum´e III. 2010. Unsupervised
Part of Speech Tagging Without a Lexicon. In NIPS
Workshop on Grammar Induction, Representation of
Language and Language Learning 2010.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proceedings of
NAACL, pages 173–180.
J. Van Gael, A. Vlachos, and Z. Ghahramani. 2009. The
infinite HMM for unsupervised PoS tagging. In Pro-
ceedings of EMNLP, pages 678–687.
L. Xu, D. Wilkinson, F. Southey, and D. Schuurmans.
2006. Discriminative unsupervised learning of struc-
tured predictors. In Proceedings of ICML, pages
1057–1064.
Q. Zhao and M. Marcus. 2009. A simple unsuper-
vised learner for POS disambiguation rules given only
a minimal lexicon. In Proceedings of EMNLP, pages
688–697.
</reference>
<page confidence="0.949411">
206
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.956081">
<title confidence="0.9988985">Crouching Dirichlet, Hidden Markov Unsupervised POS Tagging with Context Local Tag Generation</title>
<author confidence="0.996386">Taesun Moon</author>
<author confidence="0.996386">Katrin Erk</author>
<author confidence="0.996386">Jason</author>
<affiliation confidence="0.999518">Department of University of Texas at 1 University Station</affiliation>
<address confidence="0.984396">Austin, TX 78712-0198</address>
<abstract confidence="0.998776631578947">We define the crouching Dirichlet, hidden Markov model (CDHMM), an HMM for partof-speech tagging which draws state prior distributions for each local document context. This simple modification of the HMM takes advantage of the dichotomy in natural language between content and function words. In contrast, a standard HMM draws all prior distributions once over all states and it is known to perform poorly in unsupervised and semisupervised POS tagging. This modification significantly improves unsupervised POS tagging performance across several measures on five data sets for four languages. We also show that simply using different hyperparameter values for content and function word states in a standard HMM (which we call HMM+) is surprisingly effective.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>O Abend</author>
<author>R Reichart</author>
<author>A Rappoport</author>
</authors>
<title>Improved unsupervised POS induction through prototype discovery.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>1298--1307</pages>
<contexts>
<context position="31846" citStr="Abend et al., 2010" startWordPosition="5444" endWordPosition="5447"> as applied to natural language data (Johnson, 2007; Gao and Johnson, 2008; Headden III et al., 2008). Conversely, there has also been work focused on improving the HMM as an inference procedure that looked at POS tagging as an example (Graca et al., 2009; Liang and Klein, 2009). Nonparametric HMMs for unsupervised POS tag induction (Snyder et al., 2008; Van Gael et al., 2009) have seen particular activity due to the fact that model size assumptions are unnecessary and it lets the data “speak for itself.” There is also work on alternative unsupervised models that are not HMMs (Sch¨utze, 1993; Abend et al., 2010; Reichart et al., 2010b) as well as research on improving evaluation of unsupervised taggers (Frank et al., 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment that utilized certain token features (e.g. character suffixes of 3 or less, 204 has initial capital, etc.; the features themselves are from Smith and Eisner (2005)) to learn parameters in an undirected graphical model which was the equivalent of an HMM in directed models. It was also the first study to posit the one-to-one evaluation crit</context>
</contexts>
<marker>Abend, Reichart, Rappoport, 2010</marker>
<rawString>O. Abend, R. Reichart, and A. Rappoport. 2010. Improved unsupervised POS induction through prototype discovery. In Proceedings ofACL, pages 1298–1307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Afonso</author>
<author>E Bick</author>
<author>R Haber</author>
<author>D Santos</author>
</authors>
<title>Floresta sint´a(c)tica”: a treebank for Portuguese.</title>
<date>2002</date>
<booktitle>In Proceedings ofLREC,</booktitle>
<pages>1698--1703</pages>
<contexts>
<context position="17830" citStr="Afonso et al., 2002" startWordPosition="2999" endWordPosition="3002">rown 797328 343 2325 80 Tiger 447079 1090 410 58 Floresta 197422 1956 101 19 Uspanteko 70125 29 2418 83 Table 2: Number of tokens, documents, average tokens per document and total tag types for each corpus. 4 Data and Experiments Data. We use five datasets from four languages (English, German, Portuguese, Uspanteko) for evaluating POS tagging performance. • English: the Brown corpus (Francis et al., 1982) and the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1994). • German: the Tiger corpus (Brants et al., 2002). • Portuguese: the full Bosque subset of the Floresta corpus (Afonso et al., 2002). • Uspanteko (an endangered Mayan language of Guatemala): morpheme-segmented and POStagged texts collected and annotated by the OKMA language documentation project (Pixabaj et al., 2007); we use the cleaned-up version described in Palmer et al. (2009). Table 2 provides the statistics for these corpora. We lowercase all words, do not remove any punctuation or hapax legomena, and we do not replace numerals with a single identifier. Due to the nature of the models, document boundaries are retained. Evaluation We report values for three evaluation metrics on all five corpora, using their full tag</context>
</contexts>
<marker>Afonso, Bick, Haber, Santos, 2002</marker>
<rawString>S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. Floresta sint´a(c)tica”: a treebank for Portuguese. In Proceedings ofLREC, pages 1698–1703.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<date>2003</date>
<booktitle>Latent Dirichlet allocation. The Journal ofMachine Learning Research,</booktitle>
<pages>3--993</pages>
<contexts>
<context position="3113" citStr="Blei et al., 2003" startWordPosition="464" endWordPosition="467">rds. Here, we propose two extensions to the HMM. The first, HMM+, is a very simple modification where two different hyperparameters are posited for content states and function states, respectively. The other is the crouching Dirichlet, hidden Markov model (CDHMM), an extended HMM that captures this dichotomy based on the statistical evidence that comes from context. Content states display greater variance across local context (e.g. sentences, paragraphs, documents), and we capture this variance by adding a component to the model for content states that is based on latent Dirichlet allocation (Blei et al., 2003). This extension is in some ways similar to the LDAHMM of Griffiths et al. (2005). Both models are composite in that two distributions do not mix with each other. Unlike the LDAHMM, the generation of content states is folded into the CDHMM process. We compare the HMM+ and CDHMM against a basic HMM and LDAHMM on POS tagging on a more extensive and diverse set of languages than previous work in monolingual unsupervised POS tagging: four languages from three families (Germanic: English and German; Romance: Portuguese; 196 Proceedings of the 2010 Conference on Empirical Methods in Natural Language</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent Dirichlet allocation. The Journal ofMachine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Boyd-Graber</author>
<author>D Blei</author>
</authors>
<title>Syntactic topic models.</title>
<date>2009</date>
<booktitle>In Proceedings ofNIPS,</booktitle>
<pages>185--192</pages>
<contexts>
<context position="33411" citStr="Boyd-Graber and Blei (2009)" startWordPosition="5706" endWordPosition="5709">access to a tagged corpus and induces a parsing model. Other models more directly influenced or closely parallel our work. Griffiths et al. (2005) is the work that inspired the current approach where a set of states is designated to capture variance across contexts. The primary goal of that model was to induce a topic model given data that had not been filtered of noise in the form of function words. As such, distinguishing between topic states such that they model different syntactic states was not attempted, and we have seen in sec. 3 that such an extension is not entirely straightforward.4 Boyd-Graber and Blei (2009) has some parallels to our model in that a hidden variable over topics is distributed according to a normalized product between a context prior and a syntactic prior. However, it assumes a much greater amount of information than we do in that a parse tree as well as (possibly) POS tags are taken as observed. The model has a very different goal from ours as well, which is to infer a syntactically informed topic model. Teichert and Daum´e III (2010) is another study with close similarities to our own. This study models distinctions between closed class words and open class words within a modifie</context>
</contexts>
<marker>Boyd-Graber, Blei, 2009</marker>
<rawString>J. L. Boyd-Graber and D. Blei. 2009. Syntactic topic models. In Proceedings ofNIPS, pages 185–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brants</author>
<author>S Dipper</author>
<author>S Hansen</author>
<author>W Lezius</author>
<author>G Smith</author>
</authors>
<title>The TIGER treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Treebanks and Linguistic Theories.</booktitle>
<contexts>
<context position="17747" citStr="Brants et al., 2002" startWordPosition="2985" endWordPosition="2988">r different groups of states. corpus tokens docs avg. tags WSJ 974254 1801 541 43 Brown 797328 343 2325 80 Tiger 447079 1090 410 58 Floresta 197422 1956 101 19 Uspanteko 70125 29 2418 83 Table 2: Number of tokens, documents, average tokens per document and total tag types for each corpus. 4 Data and Experiments Data. We use five datasets from four languages (English, German, Portuguese, Uspanteko) for evaluating POS tagging performance. • English: the Brown corpus (Francis et al., 1982) and the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1994). • German: the Tiger corpus (Brants et al., 2002). • Portuguese: the full Bosque subset of the Floresta corpus (Afonso et al., 2002). • Uspanteko (an endangered Mayan language of Guatemala): morpheme-segmented and POStagged texts collected and annotated by the OKMA language documentation project (Pixabaj et al., 2007); we use the cleaned-up version described in Palmer et al. (2009). Table 2 provides the statistics for these corpora. We lowercase all words, do not remove any punctuation or hapax legomena, and we do not replace numerals with a single identifier. Due to the nature of the models, document boundaries are retained. Evaluation We r</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith. 2002. The TIGER treebank. In Proceedings of the Workshop on Treebanks and Linguistic Theories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>TnT: a statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of conference on Applied natural language processing,</booktitle>
<pages>224--231</pages>
<contexts>
<context position="1262" citStr="Brants, 2000" startWordPosition="183" endWordPosition="184">tributions once over all states and it is known to perform poorly in unsupervised and semisupervised POS tagging. This modification significantly improves unsupervised POS tagging performance across several measures on five data sets for four languages. We also show that simply using different hyperparameter values for content and function word states in a standard HMM (which we call HMM+) is surprisingly effective. 1 Introduction Hidden Markov Models (HMMs) are simple, versatile, and widely-used generative sequence models. They have been applied to part-of-speech (POS) tagging in supervised (Brants, 2000), semi-supervised (Goldwater and Griffiths, 2007; Ravi and Knight, 2009) and unsupervised (Johnson, 2007) training scenarios. Though discriminative models achieve better performance in both semi-supervised (Smith and Eisner, 2005) and supervised (Toutanova et al., 2003) learning, there has been only limited work on unsupervised discriminative sequence models (e.g., on synthetic data and protein sequences (Xu et al., 2006)), and none to POS tagging. The tagging accuracy of purely unsupervised HMMs is far below that of supervised and semisupervised HMMs; this is unsurprising as it is still not w</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>T. Brants. 2000. TnT: a statistical part-of-speech tagger. In Proceedings of conference on Applied natural language processing, pages 224–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Clark</author>
</authors>
<title>Combining distributional and morphological information for part of speech induction.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>59--66</pages>
<contexts>
<context position="27383" citStr="Clark, 2003" startWordPosition="4648" endWordPosition="4649"> tag sets in the respective corpora. (Floresta), and 0.11 (Uspanteko). Clearly, all of the models easily outperform this baseline. F−SCORE VI WSJ Brown Tiger Floresta Uspanteko WSJ Brown Tiger Floresta Uspanteko 20 30 40 50 f−score 0.0 0.1 0.2 0.3 0.4 0.5 vi 0 1 2 3 4 Number of states. Figure 3 shows the change in accuracy for the different models for different corpora when the overall number of states is varied between 20 and 50. The figure shows results for M-to-1. All models with the exception of HMM+ show improvements as the number of states is increased. This brings up the valid concern (Clark, 2003; Johnson, 2007) that a model could posit a very large number of states and obtain high M-to1 scores. However, it is neither the case here nor in any of the studies we cite. Furthermore, as is strongly suggested with HMM+, it does not seem as if all models will benefit from assuming a large number of states. Looking at the results by number of states on VI and f-score for CDHMM(Figure 5), it is clear that Floresta displays the reverse pattern of all other data sets where performance monotonically deteriorates as state sizes are increased. Though the exact reason is unknown, we believe it is pa</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>A. Clark. 2003. Combining distributional and morphological information for part of speech induction. In Proceedings of EACL, pages 59–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>T Grenager</author>
<author>C D Manning</author>
</authors>
<title>The infinite tree.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>272--279</pages>
<contexts>
<context position="32577" citStr="Finkel et al. (2007)" startWordPosition="5563" endWordPosition="5566">09; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment that utilized certain token features (e.g. character suffixes of 3 or less, 204 has initial capital, etc.; the features themselves are from Smith and Eisner (2005)) to learn parameters in an undirected graphical model which was the equivalent of an HMM in directed models. It was also the first study to posit the one-to-one evaluation criterion which has been repeated extensively since (Johnson, 2007; Headden III et al., 2008; Graca et al., 2009). Finkel et al. (2007) is an interesting variant of unsupervised POS tagging where a parse tree is assumed and POS tags are induced from this structure non-parametrically. It is the converse of unsupervised parsing which assumes access to a tagged corpus and induces a parsing model. Other models more directly influenced or closely parallel our work. Griffiths et al. (2005) is the work that inspired the current approach where a set of states is designated to capture variance across contexts. The primary goal of that model was to induce a topic model given data that had not been filtered of noise in the form of funct</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2007</marker>
<rawString>J. R. Finkel, T. Grenager, and C. D. Manning. 2007. The infinite tree. In Proceedings ofACL, pages 272–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W N Francis</author>
<author>H Kuˇcera</author>
<author>A W Mackie</author>
</authors>
<title>Frequency analysis of English usage: Lexicon and grammar.</title>
<date>1982</date>
<publisher>Houghton Mifflin Harcourt.</publisher>
<marker>Francis, Kuˇcera, Mackie, 1982</marker>
<rawString>W.N. Francis, H. Kuˇcera, and A.W. Mackie. 1982. Frequency analysis of English usage: Lexicon and grammar. Houghton Mifflin Harcourt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Frank</author>
<author>S Goldwater</author>
<author>F Keller</author>
</authors>
<title>Evaluating models of syntactic category acquisition without using a gold standard.</title>
<date>2009</date>
<booktitle>In Proceedings of CogSci.</booktitle>
<contexts>
<context position="31959" citStr="Frank et al., 2009" startWordPosition="5464" endWordPosition="5467">, there has also been work focused on improving the HMM as an inference procedure that looked at POS tagging as an example (Graca et al., 2009; Liang and Klein, 2009). Nonparametric HMMs for unsupervised POS tag induction (Snyder et al., 2008; Van Gael et al., 2009) have seen particular activity due to the fact that model size assumptions are unnecessary and it lets the data “speak for itself.” There is also work on alternative unsupervised models that are not HMMs (Sch¨utze, 1993; Abend et al., 2010; Reichart et al., 2010b) as well as research on improving evaluation of unsupervised taggers (Frank et al., 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment that utilized certain token features (e.g. character suffixes of 3 or less, 204 has initial capital, etc.; the features themselves are from Smith and Eisner (2005)) to learn parameters in an undirected graphical model which was the equivalent of an HMM in directed models. It was also the first study to posit the one-to-one evaluation criterion which has been repeated extensively since (Johnson, 2007; Headden III et al., 2008; Graca et al., 2009). Fi</context>
</contexts>
<marker>Frank, Goldwater, Keller, 2009</marker>
<rawString>S. Frank, S. Goldwater, and F. Keller. 2009. Evaluating models of syntactic category acquisition without using a gold standard. In Proceedings of CogSci.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>M Johnson</author>
</authors>
<title>A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>344--352</pages>
<contexts>
<context position="12944" citStr="Gao and Johnson, 2008" startWordPosition="2154" endWordPosition="2157"> of experts assumption allows us to capture high variance for certain states. To summarize, the CDHMM is a composite model where both the observed token and the hidden state variable are composite distributions. For the hidden state, this means that there is a “topical” element with high variance across contexts that is embedded in the state sequence for a subset of events. We embed this element through a PoE assumption where transitions into content states are modeled as a product of the transition probability and the local probability of the content state. Inference. We use a Gibbs sampler (Gao and Johnson, 2008) to learn the parameters of this and all other models under consideration. In this inference regime, two distributions are of particular interest. One is the posterior density and the other is the conditional distribution, neither of which can be learned in closed form. Letting A = (θ, δ, φ, ψ) and h = (α, β, γ, ξ), the posterior density is given as p(A|w, t; h) a p(w, t|A)p(A; h) Note that p(w, t|A) is equal to (φwi|tiθti|dδti|ti−1 )I[tiEC] (1) (ψwi|tiδti|ti−1)I[tiEF] where I[] is the indicator function, D is the number of documents in the corpus and Nd is the number of tokens in document d. </context>
<context position="31302" citStr="Gao and Johnson, 2008" startWordPosition="5350" endWordPosition="5353">ased data is the performance of the models on Uspanteko, which uniformly flatline. One reason might be that the tags are labeled over segmented morphemes instead of words like the other corpora. Another could be that Uspanteko has a relatively large number of tags in a very small corpus. 6 Related work Unsupervised POS tagging is an active area of research. Most recent work has involved HMMs. Given that an unconstrained HMM is not well understood in POS tagging, much work has been done on examining the mechanism and the properties of the HMM as applied to natural language data (Johnson, 2007; Gao and Johnson, 2008; Headden III et al., 2008). Conversely, there has also been work focused on improving the HMM as an inference procedure that looked at POS tagging as an example (Graca et al., 2009; Liang and Klein, 2009). Nonparametric HMMs for unsupervised POS tag induction (Snyder et al., 2008; Van Gael et al., 2009) have seen particular activity due to the fact that model size assumptions are unnecessary and it lets the data “speak for itself.” There is also work on alternative unsupervised models that are not HMMs (Sch¨utze, 1993; Abend et al., 2010; Reichart et al., 2010b) as well as research on improvi</context>
</contexts>
<marker>Gao, Johnson, 2008</marker>
<rawString>J. Gao and M. Johnson. 2008. A comparison of Bayesian estimators for unsupervised Hidden Markov Model POS taggers. In Proceedings of EMNLP, pages 344– 352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Goldberg</author>
<author>M Adler</author>
<author>M Elhadad</author>
</authors>
<title>EM can find pretty good HMM POS-taggers (when given a good start).</title>
<date>2008</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>746--754</pages>
<contexts>
<context position="34261" citStr="Goldberg et al., 2008" startWordPosition="5853" endWordPosition="5856">we do in that a parse tree as well as (possibly) POS tags are taken as observed. The model has a very different goal from ours as well, which is to infer a syntactically informed topic model. Teichert and Daum´e III (2010) is another study with close similarities to our own. This study models distinctions between closed class words and open class words within a modified HMM. It is unclear from their formulation how the distinction between open class and closed class words is learned. There is also extensive literature on learning sequence structure from unlabeled text (Smith and Eisner, 2005; Goldberg et al., 2008; Ravi and Knight, 2009) which assume access to a tag dictionary. Goldwater and Griffiths (2007) deserves mention for examining a semi-supervised model 4We tested a variant of LDAHMM in which more than one state can generate topics. It did not achieve good results. that sampled emission hyperparameters for each state rather than a single symmetric hyperparameter. They showed that this outperformed a symmetric model. An interesting heuristic model is Zhao and Marcus (2009) that uses a seed set of closed class words to classify open class words. 7 Conclusion We have shown that a hidden Markov mo</context>
</contexts>
<marker>Goldberg, Adler, Elhadad, 2008</marker>
<rawString>Y. Goldberg, M. Adler, and M. Elhadad. 2008. EM can find pretty good HMM POS-taggers (when given a good start). In Proceedings ofACL, pages 746–754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T L Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>744--751</pages>
<contexts>
<context position="1310" citStr="Goldwater and Griffiths, 2007" startWordPosition="186" endWordPosition="189">and it is known to perform poorly in unsupervised and semisupervised POS tagging. This modification significantly improves unsupervised POS tagging performance across several measures on five data sets for four languages. We also show that simply using different hyperparameter values for content and function word states in a standard HMM (which we call HMM+) is surprisingly effective. 1 Introduction Hidden Markov Models (HMMs) are simple, versatile, and widely-used generative sequence models. They have been applied to part-of-speech (POS) tagging in supervised (Brants, 2000), semi-supervised (Goldwater and Griffiths, 2007; Ravi and Knight, 2009) and unsupervised (Johnson, 2007) training scenarios. Though discriminative models achieve better performance in both semi-supervised (Smith and Eisner, 2005) and supervised (Toutanova et al., 2003) learning, there has been only limited work on unsupervised discriminative sequence models (e.g., on synthetic data and protein sequences (Xu et al., 2006)), and none to POS tagging. The tagging accuracy of purely unsupervised HMMs is far below that of supervised and semisupervised HMMs; this is unsurprising as it is still not well understood what kind of structure is being f</context>
<context position="4283" citStr="Goldwater and Griffiths, 2007" startWordPosition="649" endWordPosition="653">the 2010 Conference on Empirical Methods in Natural Language Processing, pages 196–206, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics and Mayan: Uspanteko). The CDHMM easily outperforms all other models, including HMM+, across three measures (accuracy, F-score, and variation of information) for unsupervised POS tagging on most data sets. However, the HMM+ is surprisingly competitive, outperforming the basic HMM and LDAHMM, and rivaling or even passing the CDHMM on some measures and data sets. 2 Background The Bayesian formulation for a basic HMM (Goldwater and Griffiths, 2007) is: ψt|ξ — Dir(ξ) δt|γ — Dir(γ) wi|ti = t — Mult(ψt) ti|ti−1 = t — Mult(δt) Dir is the conjugate Dirichlet prior to Mult (a multinomial distribution). The state transitions are generated by Mult(δt) whose prior δt is generated by Dir(γ) with a symmetric (i.e. uniform) hyperparameter γ. Emissions are generated by Mult(ψt) with a prior ψt generated by Dir(ξ) with a symmetric hyperparameter ξ. Hyperparameter values smaller than one encourage posteriors that are peaked, with smaller values increasing this concentration. It is not necessary that the hyperparameters be symmetric, but this is a comm</context>
<context position="17060" citStr="Goldwater and Griffiths (2007)" startWordPosition="2868" endWordPosition="2871">h the CDHMM to see how much is gained by its additional ability to model the fact that function words occur frequently but have low variance across contexts. As with the CDHMM, we use Gibbs sampling to estimate the model parameters while holding the two different hyperparameters fixed. The conditional distribution for tag transitions for this model is identical to that in fig. 2 except that it does not have the Nti|di+� second term in the first case where tiEC. Ndi+Cce We are not aware of a published instance of such an extension to the HMM—which our results show to be surprisingly effective. Goldwater and Griffiths (2007) posits different hyperparameters for individual states, but not for different groups of states. corpus tokens docs avg. tags WSJ 974254 1801 541 43 Brown 797328 343 2325 80 Tiger 447079 1090 410 58 Floresta 197422 1956 101 19 Uspanteko 70125 29 2418 83 Table 2: Number of tokens, documents, average tokens per document and total tag types for each corpus. 4 Data and Experiments Data. We use five datasets from four languages (English, German, Portuguese, Uspanteko) for evaluating POS tagging performance. • English: the Brown corpus (Francis et al., 1982) and the Wall Street Journal portion of th</context>
<context position="21797" citStr="Goldwater and Griffiths (2007)" startWordPosition="3682" endWordPosition="3685">n pairs that share a tag in M as well as in G, fp is the number token pairs that share the same tag in M but have different tags in G, and fn is the number token pairs assigned a different tag in M but the same in G (Meila, 2007). We also provide the f-score which is the harmonic mean of P and R. • Variation of Information (VI): The variation of information is an information theoretic metric that measures the amount of information lost and gained in going from tag sequence M to G (Meila, 2007). It is defined as VI(M,G) = H(M) + H(G) − 2I(M, G) where H denotes entropy and I mutual information. Goldwater and Griffiths (2007) noted that this measure can point out models that have more consistent errors in the form of lower VI, even when accuracy figures are the same. We also report learning curves on M-to-1 with geometrically increasing training set sizes of 8, 16, 32, 64, 128, 256, 512, 1024, and all documents, or as many as possible given the corpus. 5 Experiments In this section we discuss our parameter settings and experimental results. 5.1 Models and Parameters We compare four different models: • HMM: a standard HMM • HMM+: an HMM in which the hyperparameters for the word emissions are asymmetric, such that c</context>
<context position="34357" citStr="Goldwater and Griffiths (2007)" startWordPosition="5869" endWordPosition="5872">del has a very different goal from ours as well, which is to infer a syntactically informed topic model. Teichert and Daum´e III (2010) is another study with close similarities to our own. This study models distinctions between closed class words and open class words within a modified HMM. It is unclear from their formulation how the distinction between open class and closed class words is learned. There is also extensive literature on learning sequence structure from unlabeled text (Smith and Eisner, 2005; Goldberg et al., 2008; Ravi and Knight, 2009) which assume access to a tag dictionary. Goldwater and Griffiths (2007) deserves mention for examining a semi-supervised model 4We tested a variant of LDAHMM in which more than one state can generate topics. It did not achieve good results. that sampled emission hyperparameters for each state rather than a single symmetric hyperparameter. They showed that this outperformed a symmetric model. An interesting heuristic model is Zhao and Marcus (2009) that uses a seed set of closed class words to classify open class words. 7 Conclusion We have shown that a hidden Markov model that allocates a subset of the states to have distributions conditioned on localized domains</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>S. Goldwater and T. L. Griffiths. 2007. A fully Bayesian approach to unsupervised part-of-speech tagging. In Proceedings ofACL, pages 744–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Graca</author>
<author>K Ganchev</author>
<author>B Taskar</author>
<author>F Pereira</author>
</authors>
<title>Posterior vs parameter sparsity in latent variable models.</title>
<date>2009</date>
<booktitle>In Proceedings ofNIPS,</booktitle>
<pages>664--672</pages>
<contexts>
<context position="31483" citStr="Graca et al., 2009" startWordPosition="5382" endWordPosition="5385">r corpora. Another could be that Uspanteko has a relatively large number of tags in a very small corpus. 6 Related work Unsupervised POS tagging is an active area of research. Most recent work has involved HMMs. Given that an unconstrained HMM is not well understood in POS tagging, much work has been done on examining the mechanism and the properties of the HMM as applied to natural language data (Johnson, 2007; Gao and Johnson, 2008; Headden III et al., 2008). Conversely, there has also been work focused on improving the HMM as an inference procedure that looked at POS tagging as an example (Graca et al., 2009; Liang and Klein, 2009). Nonparametric HMMs for unsupervised POS tag induction (Snyder et al., 2008; Van Gael et al., 2009) have seen particular activity due to the fact that model size assumptions are unnecessary and it lets the data “speak for itself.” There is also work on alternative unsupervised models that are not HMMs (Sch¨utze, 1993; Abend et al., 2010; Reichart et al., 2010b) as well as research on improving evaluation of unsupervised taggers (Frank et al., 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an u</context>
</contexts>
<marker>Graca, Ganchev, Taskar, Pereira, 2009</marker>
<rawString>J. Graca, K. Ganchev, B. Taskar, and F. Pereira. 2009. Posterior vs parameter sparsity in latent variable models. In Proceedings ofNIPS, pages 664–672.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
<author>D M Blei</author>
<author>J M Tenenbaum</author>
</authors>
<title>Integrating topics and syntax.</title>
<date>2005</date>
<booktitle>In Proceedings ofNIPS,</booktitle>
<pages>537--544</pages>
<contexts>
<context position="3194" citStr="Griffiths et al. (2005)" startWordPosition="480" endWordPosition="483">simple modification where two different hyperparameters are posited for content states and function states, respectively. The other is the crouching Dirichlet, hidden Markov model (CDHMM), an extended HMM that captures this dichotomy based on the statistical evidence that comes from context. Content states display greater variance across local context (e.g. sentences, paragraphs, documents), and we capture this variance by adding a component to the model for content states that is based on latent Dirichlet allocation (Blei et al., 2003). This extension is in some ways similar to the LDAHMM of Griffiths et al. (2005). Both models are composite in that two distributions do not mix with each other. Unlike the LDAHMM, the generation of content states is folded into the CDHMM process. We compare the HMM+ and CDHMM against a basic HMM and LDAHMM on POS tagging on a more extensive and diverse set of languages than previous work in monolingual unsupervised POS tagging: four languages from three families (Germanic: English and German; Romance: Portuguese; 196 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 196–206, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 As</context>
<context position="5262" citStr="Griffiths et al., 2005" startWordPosition="811" endWordPosition="815">(ξ) with a symmetric hyperparameter ξ. Hyperparameter values smaller than one encourage posteriors that are peaked, with smaller values increasing this concentration. It is not necessary that the hyperparameters be symmetric, but this is a common approach when one wants to be naive about the data. This is particularly appropriate in unsupervised POS tagging with regard to novel data since there won’t be a priori grounds for favoring certain distributions over others. There is considerable work on extensions to HMM-based unsupervised POS tagging (see §6), but here we concentrate on the LDAHMM (Griffiths et al., 2005), which models topics and state sequences jointly. The model is a composite of a probabilistic topic model and an HMM in which a single state is allocated for words generated from the topic model. A strength of this model is that it is able to use less supervision than previous topic models since it does not require a stopword list. While the topic model component still uses the bagsof-words assumption, the joint model infers which words are more likely to carry topical content and which words are more likely to contribute to the local sequence. This model is competitive with a standard topic </context>
<context position="22584" citStr="Griffiths et al., 2005" startWordPosition="3816" endWordPosition="3819">ning curves on M-to-1 with geometrically increasing training set sizes of 8, 16, 32, 64, 128, 256, 512, 1024, and all documents, or as many as possible given the corpus. 5 Experiments In this section we discuss our parameter settings and experimental results. 5.1 Models and Parameters We compare four different models: • HMM: a standard HMM • HMM+: an HMM in which the hyperparameters for the word emissions are asymmetric, such that content states have different word emission priors compared to function states. • LDAHMM: an HMM with a distinguished state that generates words from a topic model (Griffiths et al., 2005) 201 HMM+ LDAHMM CDHMM acc 0.0 0.1 0.2 0.3 0.4 0.5 0.6 20 30 40 50 acc 0.0 0.1 0.2 0.3 0.4 0.5 0.6 acc 0.0 0.1 0.2 0.3 0.4 0.5 0.6 WSJ Brown Tiger Floresta Uspanteko WSJ Brown Tiger Floresta Uspanteko WSJ Brown Tiger Floresta Uspanteko Figure 3: Averaged many-to-one accuracy on the full tagset for the models HMM+, LDAHMM, CDHMM when the number of states is set at 20, 30, 40 and 50 states. • CDHMM: our HMM with context-based emissions, where the context used is the document We implemented all of these models, ensuring performance differences are due to the models themselves rather than implemen</context>
<context position="32930" citStr="Griffiths et al. (2005)" startWordPosition="5623" endWordPosition="5626"> graphical model which was the equivalent of an HMM in directed models. It was also the first study to posit the one-to-one evaluation criterion which has been repeated extensively since (Johnson, 2007; Headden III et al., 2008; Graca et al., 2009). Finkel et al. (2007) is an interesting variant of unsupervised POS tagging where a parse tree is assumed and POS tags are induced from this structure non-parametrically. It is the converse of unsupervised parsing which assumes access to a tagged corpus and induces a parsing model. Other models more directly influenced or closely parallel our work. Griffiths et al. (2005) is the work that inspired the current approach where a set of states is designated to capture variance across contexts. The primary goal of that model was to induce a topic model given data that had not been filtered of noise in the form of function words. As such, distinguishing between topic states such that they model different syntactic states was not attempted, and we have seen in sec. 3 that such an extension is not entirely straightforward.4 Boyd-Graber and Blei (2009) has some parallels to our model in that a hidden variable over topics is distributed according to a normalized product</context>
</contexts>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2005</marker>
<rawString>T. L. Griffiths, M. Steyvers, D. M. Blei, and J. M. Tenenbaum. 2005. Integrating topics and syntax. In Proceedings ofNIPS, pages 537–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>D Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>320--327</pages>
<contexts>
<context position="32068" citStr="Haghighi and Klein (2006)" startWordPosition="5480" endWordPosition="5483">agging as an example (Graca et al., 2009; Liang and Klein, 2009). Nonparametric HMMs for unsupervised POS tag induction (Snyder et al., 2008; Van Gael et al., 2009) have seen particular activity due to the fact that model size assumptions are unnecessary and it lets the data “speak for itself.” There is also work on alternative unsupervised models that are not HMMs (Sch¨utze, 1993; Abend et al., 2010; Reichart et al., 2010b) as well as research on improving evaluation of unsupervised taggers (Frank et al., 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment that utilized certain token features (e.g. character suffixes of 3 or less, 204 has initial capital, etc.; the features themselves are from Smith and Eisner (2005)) to learn parameters in an undirected graphical model which was the equivalent of an HMM in directed models. It was also the first study to posit the one-to-one evaluation criterion which has been repeated extensively since (Johnson, 2007; Headden III et al., 2008; Graca et al., 2009). Finkel et al. (2007) is an interesting variant of unsupervised POS tagging where a parse tree is assumed and PO</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>A. Haghighi and D. Klein. 2006. Prototype-driven learning for sequence models. In Proceedings of HLT/NAACL, pages 320–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W P Headden D McClosky</author>
<author>E Charniak</author>
</authors>
<title>Evaluating unsupervised part-of-speech tagging for grammar induction.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>329--336</pages>
<marker>McClosky, Charniak, 2008</marker>
<rawString>W. P. Headden III, D. McClosky, and E. Charniak. 2008. Evaluating unsupervised part-of-speech tagging for grammar induction. In Proceedings of COLING, pages 329–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
</authors>
<title>Training products of experts by minimizing contrastive divergence.</title>
<date>2002</date>
<journal>Neural Computation,</journal>
<volume>14</volume>
<issue>8</issue>
<contexts>
<context position="12040" citStr="Hinton, 2002" startWordPosition="2002" endWordPosition="2003">ti−1=ti=ti+1]+γ” p(ti|t−i, w) a { Nwi|ti+β Nti+Wβ Nwi|ti+ξ Nti+Wξ Nti|di+α Ndi+Cα r rr “ ”“ ” Nti|ti−1+γ Nti+1|ti+I[ti−1=ti=ti+1]+γ ti E F Nti+Tγ+I[ti=ti−1] Figure 2: Conditional distribution for ti in the CDHMM. The important thing to note is that the draw for states at each word is proportional to a composite of (a) the product of the individual elements of the topic and transition priors when tiEC and (b) the transition priors when tiEF. The draw is proportional to the product of topic and transition priors when tiEC because we have made a product of experts (PoE) factorization assumption (Hinton, 2002) for tractability and to reduce the size of our model. Without such an assumption, the transition parameters would lie in a partitioned space of size O(|C|&apos;) as opposed to O(|T |2) for the current model. Furthermore, this combination of a composite hidden state space with a product of experts assumption allows us to capture high variance for certain states. To summarize, the CDHMM is a composite model where both the observed token and the hidden state variable are composite distributions. For the hidden state, this means that there is a “topical” element with high variance across contexts that</context>
</contexts>
<marker>Hinton, 2002</marker>
<rawString>G.E. Hinton. 2002. Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):1771–1800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>296--305</pages>
<contexts>
<context position="1367" citStr="Johnson, 2007" startWordPosition="196" endWordPosition="197"> tagging. This modification significantly improves unsupervised POS tagging performance across several measures on five data sets for four languages. We also show that simply using different hyperparameter values for content and function word states in a standard HMM (which we call HMM+) is surprisingly effective. 1 Introduction Hidden Markov Models (HMMs) are simple, versatile, and widely-used generative sequence models. They have been applied to part-of-speech (POS) tagging in supervised (Brants, 2000), semi-supervised (Goldwater and Griffiths, 2007; Ravi and Knight, 2009) and unsupervised (Johnson, 2007) training scenarios. Though discriminative models achieve better performance in both semi-supervised (Smith and Eisner, 2005) and supervised (Toutanova et al., 2003) learning, there has been only limited work on unsupervised discriminative sequence models (e.g., on synthetic data and protein sequences (Xu et al., 2006)), and none to POS tagging. The tagging accuracy of purely unsupervised HMMs is far below that of supervised and semisupervised HMMs; this is unsurprising as it is still not well understood what kind of structure is being found by an unconstrained HMM (Headden III et al., 2008). </context>
<context position="18728" citStr="Johnson (2007)" startWordPosition="3146" endWordPosition="3147">s for these corpora. We lowercase all words, do not remove any punctuation or hapax legomena, and we do not replace numerals with a single identifier. Due to the nature of the models, document boundaries are retained. Evaluation We report values for three evaluation metrics on all five corpora, using their full tagsets. • Accuracy: We use a greedy search algorithm to map each unsupervised tag to a gold label such that accuracy is maximized. We evaluate on a 1-to-1 mapping between unsupervised tags and gold labels, as well as many-to-1 (M-to-1), corresponding to the evaluation mappings used in Johnson (2007). The 1-to-1 mapping provides a stricter evaluation. The many-to-one mapping, on the other hand, may be more adequate as unsupervised tags tend to be more fine-grained than 200 Accuracy Model Pairwise P/R Scores VI 1-to-1 M-to-1 P R F WSJ (50) HMM 0.34 (0.01) 0.49 (0.03) 0.51 (0.03) 0.19 (0.01) 0.28 (0.01) 3.72 (0.08) LDAHMM 0.30 (0.04) 0.45 (0.04) 0.25 (0.07) 0.27 (0.03) 0.26 (0.04) 3.64 (0.14) HMM+ 0.42 (0.04) 0.46 (0.05) 0.24 (0.03) 0.49 (0.03) 0.32 (0.03) 2.65 (0.15) CDHMM 0.44 (0.01) 0.58 (0.02) 0.31 (0.01) 0.43 (0.03) 0.36 (0.02) 2.73 (0.08) Brown (50) HMM 0.32 (0.01) 0.50 (0.02) 0.60 (0</context>
<context position="23421" citStr="Johnson (2007)" startWordPosition="3969" endWordPosition="3970">ta Uspanteko Figure 3: Averaged many-to-one accuracy on the full tagset for the models HMM+, LDAHMM, CDHMM when the number of states is set at 20, 30, 40 and 50 states. • CDHMM: our HMM with context-based emissions, where the context used is the document We implemented all of these models, ensuring performance differences are due to the models themselves rather than implementation details. For all models, the transition hyperparameters -y are set to 0.1. For the LDAHMM and HMM all emission hyperparameters are set to 0.0001. These figures are the MCMC settings that provided the best results in Johnson (2007). For the models that distinguish content and function states (HMM+, CDHMM), we fixed the number of content states at 5 and set the function state emission hyperparameters � = 0.0001 and the content state emission hyperparameters Q = 0.1. For the models with an LDA or LDA-like component (LDAHMM, CDHMM), we set the topic or content-state hyperparameter α = 1. For decoding, we use maximum posterior decoding to obtain a single sample after the required burnin, as has been done in other unsupervised HMM experiments. We use this sample for evaluation. 5.2 Results Results for all models on the full </context>
<context position="27399" citStr="Johnson, 2007" startWordPosition="4650" endWordPosition="4651">the respective corpora. (Floresta), and 0.11 (Uspanteko). Clearly, all of the models easily outperform this baseline. F−SCORE VI WSJ Brown Tiger Floresta Uspanteko WSJ Brown Tiger Floresta Uspanteko 20 30 40 50 f−score 0.0 0.1 0.2 0.3 0.4 0.5 vi 0 1 2 3 4 Number of states. Figure 3 shows the change in accuracy for the different models for different corpora when the overall number of states is varied between 20 and 50. The figure shows results for M-to-1. All models with the exception of HMM+ show improvements as the number of states is increased. This brings up the valid concern (Clark, 2003; Johnson, 2007) that a model could posit a very large number of states and obtain high M-to1 scores. However, it is neither the case here nor in any of the studies we cite. Furthermore, as is strongly suggested with HMM+, it does not seem as if all models will benefit from assuming a large number of states. Looking at the results by number of states on VI and f-score for CDHMM(Figure 5), it is clear that Floresta displays the reverse pattern of all other data sets where performance monotonically deteriorates as state sizes are increased. Though the exact reason is unknown, we believe it is partially due to t</context>
<context position="31279" citStr="Johnson, 2007" startWordPosition="5348" endWordPosition="5349">ment over increased data is the performance of the models on Uspanteko, which uniformly flatline. One reason might be that the tags are labeled over segmented morphemes instead of words like the other corpora. Another could be that Uspanteko has a relatively large number of tags in a very small corpus. 6 Related work Unsupervised POS tagging is an active area of research. Most recent work has involved HMMs. Given that an unconstrained HMM is not well understood in POS tagging, much work has been done on examining the mechanism and the properties of the HMM as applied to natural language data (Johnson, 2007; Gao and Johnson, 2008; Headden III et al., 2008). Conversely, there has also been work focused on improving the HMM as an inference procedure that looked at POS tagging as an example (Graca et al., 2009; Liang and Klein, 2009). Nonparametric HMMs for unsupervised POS tag induction (Snyder et al., 2008; Van Gael et al., 2009) have seen particular activity due to the fact that model size assumptions are unnecessary and it lets the data “speak for itself.” There is also work on alternative unsupervised models that are not HMMs (Sch¨utze, 1993; Abend et al., 2010; Reichart et al., 2010b) as well</context>
<context position="32508" citStr="Johnson, 2007" startWordPosition="5552" endWordPosition="5553">improving evaluation of unsupervised taggers (Frank et al., 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment that utilized certain token features (e.g. character suffixes of 3 or less, 204 has initial capital, etc.; the features themselves are from Smith and Eisner (2005)) to learn parameters in an undirected graphical model which was the equivalent of an HMM in directed models. It was also the first study to posit the one-to-one evaluation criterion which has been repeated extensively since (Johnson, 2007; Headden III et al., 2008; Graca et al., 2009). Finkel et al. (2007) is an interesting variant of unsupervised POS tagging where a parse tree is assumed and POS tags are induced from this structure non-parametrically. It is the converse of unsupervised parsing which assumes access to a tagged corpus and induces a parsing model. Other models more directly influenced or closely parallel our work. Griffiths et al. (2005) is the work that inspired the current approach where a set of states is designated to capture variance across contexts. The primary goal of that model was to induce a topic mode</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>M. Johnson. 2007. Why doesn’t EM find good HMM POS-taggers. In Proceedings of EMNLP-CoNLL, pages 296–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>D Klein</author>
</authors>
<title>Online EM for unsupervised models.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>611--619</pages>
<contexts>
<context position="31507" citStr="Liang and Klein, 2009" startWordPosition="5386" endWordPosition="5389">ould be that Uspanteko has a relatively large number of tags in a very small corpus. 6 Related work Unsupervised POS tagging is an active area of research. Most recent work has involved HMMs. Given that an unconstrained HMM is not well understood in POS tagging, much work has been done on examining the mechanism and the properties of the HMM as applied to natural language data (Johnson, 2007; Gao and Johnson, 2008; Headden III et al., 2008). Conversely, there has also been work focused on improving the HMM as an inference procedure that looked at POS tagging as an example (Graca et al., 2009; Liang and Klein, 2009). Nonparametric HMMs for unsupervised POS tag induction (Snyder et al., 2008; Van Gael et al., 2009) have seen particular activity due to the fact that model size assumptions are unnecessary and it lets the data “speak for itself.” There is also work on alternative unsupervised models that are not HMMs (Sch¨utze, 1993; Abend et al., 2010; Reichart et al., 2010b) as well as research on improving evaluation of unsupervised taggers (Frank et al., 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment t</context>
</contexts>
<marker>Liang, Klein, 2009</marker>
<rawString>P. Liang and D. Klein. 2009. Online EM for unsupervised models. In Proceedings of HLT/NAACL, pages 611–619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1994</date>
<journal>Comp. ling.,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="17697" citStr="Marcus et al., 1994" startWordPosition="2976" endWordPosition="2979"> hyperparameters for individual states, but not for different groups of states. corpus tokens docs avg. tags WSJ 974254 1801 541 43 Brown 797328 343 2325 80 Tiger 447079 1090 410 58 Floresta 197422 1956 101 19 Uspanteko 70125 29 2418 83 Table 2: Number of tokens, documents, average tokens per document and total tag types for each corpus. 4 Data and Experiments Data. We use five datasets from four languages (English, German, Portuguese, Uspanteko) for evaluating POS tagging performance. • English: the Brown corpus (Francis et al., 1982) and the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1994). • German: the Tiger corpus (Brants et al., 2002). • Portuguese: the full Bosque subset of the Floresta corpus (Afonso et al., 2002). • Uspanteko (an endangered Mayan language of Guatemala): morpheme-segmented and POStagged texts collected and annotated by the OKMA language documentation project (Pixabaj et al., 2007); we use the cleaned-up version described in Palmer et al. (2009). Table 2 provides the statistics for these corpora. We lowercase all words, do not remove any punctuation or hapax legomena, and we do not replace numerals with a single identifier. Due to the nature of the models,</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1994. Building a large annotated corpus of English: The Penn Treebank. Comp. ling., 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Meila</author>
</authors>
<title>Comparing clusterings—an information based distance.</title>
<date>2007</date>
<journal>Journal of Multivariate Analysis,</journal>
<volume>98</volume>
<issue>5</issue>
<contexts>
<context position="21396" citStr="Meila, 2007" startWordPosition="3613" endWordPosition="3614">rts of speech. • Pairwise Precision and Recall: Viewing tagging as a clustering task over tokens, we evaluate pairwise precision (P) and recall (R) between the model tag sequence (M) and gold tag sequence (G) by counting the true positives (tp), false positives (fp) and false negatives (fn) between the two and setting P = tp/(tp + fp) and R = tp/(tp + fn). tp is the number of token pairs that share a tag in M as well as in G, fp is the number token pairs that share the same tag in M but have different tags in G, and fn is the number token pairs assigned a different tag in M but the same in G (Meila, 2007). We also provide the f-score which is the harmonic mean of P and R. • Variation of Information (VI): The variation of information is an information theoretic metric that measures the amount of information lost and gained in going from tag sequence M to G (Meila, 2007). It is defined as VI(M,G) = H(M) + H(G) − 2I(M, G) where H denotes entropy and I mutual information. Goldwater and Griffiths (2007) noted that this measure can point out models that have more consistent errors in the form of lower VI, even when accuracy figures are the same. We also report learning curves on M-to-1 with geometri</context>
</contexts>
<marker>Meila, 2007</marker>
<rawString>M. Meila. 2007. Comparing clusterings—an information based distance. Journal of Multivariate Analysis, 98(5):873–895.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Palmer</author>
<author>T Moon</author>
<author>J Baldridge</author>
</authors>
<title>Evaluating automation strategies in language documentation.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL-HLT 2009 Workshop on Active Learning for Natural Language Processing,</booktitle>
<pages>36--44</pages>
<contexts>
<context position="18082" citStr="Palmer et al. (2009)" startWordPosition="3037" endWordPosition="3040">sets from four languages (English, German, Portuguese, Uspanteko) for evaluating POS tagging performance. • English: the Brown corpus (Francis et al., 1982) and the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1994). • German: the Tiger corpus (Brants et al., 2002). • Portuguese: the full Bosque subset of the Floresta corpus (Afonso et al., 2002). • Uspanteko (an endangered Mayan language of Guatemala): morpheme-segmented and POStagged texts collected and annotated by the OKMA language documentation project (Pixabaj et al., 2007); we use the cleaned-up version described in Palmer et al. (2009). Table 2 provides the statistics for these corpora. We lowercase all words, do not remove any punctuation or hapax legomena, and we do not replace numerals with a single identifier. Due to the nature of the models, document boundaries are retained. Evaluation We report values for three evaluation metrics on all five corpora, using their full tagsets. • Accuracy: We use a greedy search algorithm to map each unsupervised tag to a gold label such that accuracy is maximized. We evaluate on a 1-to-1 mapping between unsupervised tags and gold labels, as well as many-to-1 (M-to-1), corresponding to </context>
</contexts>
<marker>Palmer, Moon, Baldridge, 2009</marker>
<rawString>A. Palmer, T. Moon, and J. Baldridge. 2009. Evaluating automation strategies in language documentation. In Proceedings of the NAACL-HLT 2009 Workshop on Active Learning for Natural Language Processing, pages 36–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T C Pixabaj</author>
<author>M A Vicente M´endez</author>
<author>M Vicente M´endez</author>
<author>O A Dami´an</author>
</authors>
<date>2007</date>
<booktitle>Text Collections in Four Mayan Languages. Archived in The Archive of the Indigenous Languages of Latin</booktitle>
<publisher>America.</publisher>
<marker>Pixabaj, M´endez, M´endez, Dami´an, 2007</marker>
<rawString>T. C. Pixabaj, M. A. Vicente M´endez, M. Vicente M´endez, and O. A. Dami´an. 2007. Text Collections in Four Mayan Languages. Archived in The Archive of the Indigenous Languages of Latin America.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ravi</author>
<author>K Knight</author>
</authors>
<title>Minimized models for unsupervised part-of-speech tagging.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL and AFNLP,</booktitle>
<pages>504--512</pages>
<contexts>
<context position="1334" citStr="Ravi and Knight, 2009" startWordPosition="190" endWordPosition="193">ly in unsupervised and semisupervised POS tagging. This modification significantly improves unsupervised POS tagging performance across several measures on five data sets for four languages. We also show that simply using different hyperparameter values for content and function word states in a standard HMM (which we call HMM+) is surprisingly effective. 1 Introduction Hidden Markov Models (HMMs) are simple, versatile, and widely-used generative sequence models. They have been applied to part-of-speech (POS) tagging in supervised (Brants, 2000), semi-supervised (Goldwater and Griffiths, 2007; Ravi and Knight, 2009) and unsupervised (Johnson, 2007) training scenarios. Though discriminative models achieve better performance in both semi-supervised (Smith and Eisner, 2005) and supervised (Toutanova et al., 2003) learning, there has been only limited work on unsupervised discriminative sequence models (e.g., on synthetic data and protein sequences (Xu et al., 2006)), and none to POS tagging. The tagging accuracy of purely unsupervised HMMs is far below that of supervised and semisupervised HMMs; this is unsurprising as it is still not well understood what kind of structure is being found by an unconstrained</context>
<context position="34285" citStr="Ravi and Knight, 2009" startWordPosition="5857" endWordPosition="5860">ree as well as (possibly) POS tags are taken as observed. The model has a very different goal from ours as well, which is to infer a syntactically informed topic model. Teichert and Daum´e III (2010) is another study with close similarities to our own. This study models distinctions between closed class words and open class words within a modified HMM. It is unclear from their formulation how the distinction between open class and closed class words is learned. There is also extensive literature on learning sequence structure from unlabeled text (Smith and Eisner, 2005; Goldberg et al., 2008; Ravi and Knight, 2009) which assume access to a tag dictionary. Goldwater and Griffiths (2007) deserves mention for examining a semi-supervised model 4We tested a variant of LDAHMM in which more than one state can generate topics. It did not achieve good results. that sampled emission hyperparameters for each state rather than a single symmetric hyperparameter. They showed that this outperformed a symmetric model. An interesting heuristic model is Zhao and Marcus (2009) that uses a seed set of closed class words to classify open class words. 7 Conclusion We have shown that a hidden Markov model that allocates a sub</context>
</contexts>
<marker>Ravi, Knight, 2009</marker>
<rawString>S. Ravi and K. Knight. 2009. Minimized models for unsupervised part-of-speech tagging. In Proceedings ofACL and AFNLP, pages 504–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Reichart</author>
<author>O Abend</author>
<author>A Rappoport</author>
</authors>
<title>Type level clustering evaluation: New measures and a POS induction case study.</title>
<date>2010</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>77--87</pages>
<contexts>
<context position="31869" citStr="Reichart et al., 2010" startWordPosition="5448" endWordPosition="5451">al language data (Johnson, 2007; Gao and Johnson, 2008; Headden III et al., 2008). Conversely, there has also been work focused on improving the HMM as an inference procedure that looked at POS tagging as an example (Graca et al., 2009; Liang and Klein, 2009). Nonparametric HMMs for unsupervised POS tag induction (Snyder et al., 2008; Van Gael et al., 2009) have seen particular activity due to the fact that model size assumptions are unnecessary and it lets the data “speak for itself.” There is also work on alternative unsupervised models that are not HMMs (Sch¨utze, 1993; Abend et al., 2010; Reichart et al., 2010b) as well as research on improving evaluation of unsupervised taggers (Frank et al., 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment that utilized certain token features (e.g. character suffixes of 3 or less, 204 has initial capital, etc.; the features themselves are from Smith and Eisner (2005)) to learn parameters in an undirected graphical model which was the equivalent of an HMM in directed models. It was also the first study to posit the one-to-one evaluation criterion which has been re</context>
</contexts>
<marker>Reichart, Abend, Rappoport, 2010</marker>
<rawString>R. Reichart, O. Abend, and A. Rappoport. 2010a. Type level clustering evaluation: New measures and a POS induction case study. In Proceedings of CoNLL, pages 77–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Reichart</author>
<author>R Fattal</author>
<author>A Rappoport</author>
</authors>
<title>Improved unsupervised POS induction using intrinsic clustering quality and a Zipfian constraint.</title>
<date>2010</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>57--66</pages>
<contexts>
<context position="31869" citStr="Reichart et al., 2010" startWordPosition="5448" endWordPosition="5451">al language data (Johnson, 2007; Gao and Johnson, 2008; Headden III et al., 2008). Conversely, there has also been work focused on improving the HMM as an inference procedure that looked at POS tagging as an example (Graca et al., 2009; Liang and Klein, 2009). Nonparametric HMMs for unsupervised POS tag induction (Snyder et al., 2008; Van Gael et al., 2009) have seen particular activity due to the fact that model size assumptions are unnecessary and it lets the data “speak for itself.” There is also work on alternative unsupervised models that are not HMMs (Sch¨utze, 1993; Abend et al., 2010; Reichart et al., 2010b) as well as research on improving evaluation of unsupervised taggers (Frank et al., 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment that utilized certain token features (e.g. character suffixes of 3 or less, 204 has initial capital, etc.; the features themselves are from Smith and Eisner (2005)) to learn parameters in an undirected graphical model which was the equivalent of an HMM in directed models. It was also the first study to posit the one-to-one evaluation criterion which has been re</context>
</contexts>
<marker>Reichart, Fattal, Rappoport, 2010</marker>
<rawString>R. Reichart, R. Fattal, and A. Rappoport. 2010b. Improved unsupervised POS induction using intrinsic clustering quality and a Zipfian constraint. In Proceedings of CoNLL, pages 57–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<title>Part-of-speech induction from scratch.</title>
<date>1993</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>251--258</pages>
<marker>Sch¨utze, 1993</marker>
<rawString>H. Sch¨utze. 1993. Part-of-speech induction from scratch. In Proceedings ofACL, pages 251–258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>J Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>354--362</pages>
<contexts>
<context position="1492" citStr="Smith and Eisner, 2005" startWordPosition="209" endWordPosition="212">ive data sets for four languages. We also show that simply using different hyperparameter values for content and function word states in a standard HMM (which we call HMM+) is surprisingly effective. 1 Introduction Hidden Markov Models (HMMs) are simple, versatile, and widely-used generative sequence models. They have been applied to part-of-speech (POS) tagging in supervised (Brants, 2000), semi-supervised (Goldwater and Griffiths, 2007; Ravi and Knight, 2009) and unsupervised (Johnson, 2007) training scenarios. Though discriminative models achieve better performance in both semi-supervised (Smith and Eisner, 2005) and supervised (Toutanova et al., 2003) learning, there has been only limited work on unsupervised discriminative sequence models (e.g., on synthetic data and protein sequences (Xu et al., 2006)), and none to POS tagging. The tagging accuracy of purely unsupervised HMMs is far below that of supervised and semisupervised HMMs; this is unsurprising as it is still not well understood what kind of structure is being found by an unconstrained HMM (Headden III et al., 2008). However, HMMs are fairly simple directed graphical models, and it is straightforward to extend them to define alternative gen</context>
<context position="32269" citStr="Smith and Eisner (2005)" startWordPosition="5511" endWordPosition="5514">he fact that model size assumptions are unnecessary and it lets the data “speak for itself.” There is also work on alternative unsupervised models that are not HMMs (Sch¨utze, 1993; Abend et al., 2010; Reichart et al., 2010b) as well as research on improving evaluation of unsupervised taggers (Frank et al., 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment that utilized certain token features (e.g. character suffixes of 3 or less, 204 has initial capital, etc.; the features themselves are from Smith and Eisner (2005)) to learn parameters in an undirected graphical model which was the equivalent of an HMM in directed models. It was also the first study to posit the one-to-one evaluation criterion which has been repeated extensively since (Johnson, 2007; Headden III et al., 2008; Graca et al., 2009). Finkel et al. (2007) is an interesting variant of unsupervised POS tagging where a parse tree is assumed and POS tags are induced from this structure non-parametrically. It is the converse of unsupervised parsing which assumes access to a tagged corpus and induces a parsing model. Other models more directly inf</context>
<context position="34238" citStr="Smith and Eisner, 2005" startWordPosition="5849" endWordPosition="5852">unt of information than we do in that a parse tree as well as (possibly) POS tags are taken as observed. The model has a very different goal from ours as well, which is to infer a syntactically informed topic model. Teichert and Daum´e III (2010) is another study with close similarities to our own. This study models distinctions between closed class words and open class words within a modified HMM. It is unclear from their formulation how the distinction between open class and closed class words is learned. There is also extensive literature on learning sequence structure from unlabeled text (Smith and Eisner, 2005; Goldberg et al., 2008; Ravi and Knight, 2009) which assume access to a tag dictionary. Goldwater and Griffiths (2007) deserves mention for examining a semi-supervised model 4We tested a variant of LDAHMM in which more than one state can generate topics. It did not achieve good results. that sampled emission hyperparameters for each state rather than a single symmetric hyperparameter. They showed that this outperformed a symmetric model. An interesting heuristic model is Zhao and Marcus (2009) that uses a seed set of closed class words to classify open class words. 7 Conclusion We have shown </context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>N.A. Smith and J. Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings ofACL, pages 354–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Snyder</author>
<author>T Naseem</author>
<author>J Eisenstein</author>
<author>R Barzilay</author>
</authors>
<title>Unsupervised multilingual learning for POS tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1041--1050</pages>
<contexts>
<context position="31583" citStr="Snyder et al., 2008" startWordPosition="5397" endWordPosition="5400">rpus. 6 Related work Unsupervised POS tagging is an active area of research. Most recent work has involved HMMs. Given that an unconstrained HMM is not well understood in POS tagging, much work has been done on examining the mechanism and the properties of the HMM as applied to natural language data (Johnson, 2007; Gao and Johnson, 2008; Headden III et al., 2008). Conversely, there has also been work focused on improving the HMM as an inference procedure that looked at POS tagging as an example (Graca et al., 2009; Liang and Klein, 2009). Nonparametric HMMs for unsupervised POS tag induction (Snyder et al., 2008; Van Gael et al., 2009) have seen particular activity due to the fact that model size assumptions are unnecessary and it lets the data “speak for itself.” There is also work on alternative unsupervised models that are not HMMs (Sch¨utze, 1993; Abend et al., 2010; Reichart et al., 2010b) as well as research on improving evaluation of unsupervised taggers (Frank et al., 2009; Reichart et al., 2010a). Though they did not concentrate on unsupervised methods, Haghighi and Klein (2006) conducted an unsupervised experiment that utilized certain token features (e.g. character suffixes of 3 or less, 2</context>
</contexts>
<marker>Snyder, Naseem, Eisenstein, Barzilay, 2008</marker>
<rawString>B. Snyder, T. Naseem, J. Eisenstein, and R. Barzilay. 2008. Unsupervised multilingual learning for POS tagging. In Proceedings of EMNLP, pages 1041– 1050.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Teichert</author>
<author>H Daum´e</author>
</authors>
<title>Unsupervised Part of Speech Tagging Without a Lexicon.</title>
<date>2010</date>
<booktitle>In NIPS Workshop on Grammar Induction, Representation of Language and Language Learning</booktitle>
<marker>Teichert, Daum´e, 2010</marker>
<rawString>A.R. Teichert and H. Daum´e III. 2010. Unsupervised Part of Speech Tagging Without a Lexicon. In NIPS Workshop on Grammar Induction, Representation of Language and Language Learning 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="1532" citStr="Toutanova et al., 2003" startWordPosition="215" endWordPosition="218">o show that simply using different hyperparameter values for content and function word states in a standard HMM (which we call HMM+) is surprisingly effective. 1 Introduction Hidden Markov Models (HMMs) are simple, versatile, and widely-used generative sequence models. They have been applied to part-of-speech (POS) tagging in supervised (Brants, 2000), semi-supervised (Goldwater and Griffiths, 2007; Ravi and Knight, 2009) and unsupervised (Johnson, 2007) training scenarios. Though discriminative models achieve better performance in both semi-supervised (Smith and Eisner, 2005) and supervised (Toutanova et al., 2003) learning, there has been only limited work on unsupervised discriminative sequence models (e.g., on synthetic data and protein sequences (Xu et al., 2006)), and none to POS tagging. The tagging accuracy of purely unsupervised HMMs is far below that of supervised and semisupervised HMMs; this is unsurprising as it is still not well understood what kind of structure is being found by an unconstrained HMM (Headden III et al., 2008). However, HMMs are fairly simple directed graphical models, and it is straightforward to extend them to define alternative generative processes. This also applies to </context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K. Toutanova, D. Klein, C. Manning, and Y. Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of NAACL, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Van Gael</author>
<author>A Vlachos</author>
<author>Z Ghahramani</author>
</authors>
<title>The infinite HMM for unsupervised PoS tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>678--687</pages>
<marker>Van Gael, Vlachos, Ghahramani, 2009</marker>
<rawString>J. Van Gael, A. Vlachos, and Z. Ghahramani. 2009. The infinite HMM for unsupervised PoS tagging. In Proceedings of EMNLP, pages 678–687.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Xu</author>
<author>D Wilkinson</author>
<author>F Southey</author>
<author>D Schuurmans</author>
</authors>
<title>Discriminative unsupervised learning of structured predictors.</title>
<date>2006</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>1057--1064</pages>
<contexts>
<context position="1687" citStr="Xu et al., 2006" startWordPosition="238" endWordPosition="241">Introduction Hidden Markov Models (HMMs) are simple, versatile, and widely-used generative sequence models. They have been applied to part-of-speech (POS) tagging in supervised (Brants, 2000), semi-supervised (Goldwater and Griffiths, 2007; Ravi and Knight, 2009) and unsupervised (Johnson, 2007) training scenarios. Though discriminative models achieve better performance in both semi-supervised (Smith and Eisner, 2005) and supervised (Toutanova et al., 2003) learning, there has been only limited work on unsupervised discriminative sequence models (e.g., on synthetic data and protein sequences (Xu et al., 2006)), and none to POS tagging. The tagging accuracy of purely unsupervised HMMs is far below that of supervised and semisupervised HMMs; this is unsurprising as it is still not well understood what kind of structure is being found by an unconstrained HMM (Headden III et al., 2008). However, HMMs are fairly simple directed graphical models, and it is straightforward to extend them to define alternative generative processes. This also applies to linguistically motivated HMMs for recovering states and sequences that correspond more closely to those implicitly defined by linguists when they label sen</context>
</contexts>
<marker>Xu, Wilkinson, Southey, Schuurmans, 2006</marker>
<rawString>L. Xu, D. Wilkinson, F. Southey, and D. Schuurmans. 2006. Discriminative unsupervised learning of structured predictors. In Proceedings of ICML, pages 1057–1064.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Zhao</author>
<author>M Marcus</author>
</authors>
<title>A simple unsupervised learner for POS disambiguation rules given only a minimal lexicon.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>688--697</pages>
<contexts>
<context position="34737" citStr="Zhao and Marcus (2009)" startWordPosition="5929" endWordPosition="5932">s is learned. There is also extensive literature on learning sequence structure from unlabeled text (Smith and Eisner, 2005; Goldberg et al., 2008; Ravi and Knight, 2009) which assume access to a tag dictionary. Goldwater and Griffiths (2007) deserves mention for examining a semi-supervised model 4We tested a variant of LDAHMM in which more than one state can generate topics. It did not achieve good results. that sampled emission hyperparameters for each state rather than a single symmetric hyperparameter. They showed that this outperformed a symmetric model. An interesting heuristic model is Zhao and Marcus (2009) that uses a seed set of closed class words to classify open class words. 7 Conclusion We have shown that a hidden Markov model that allocates a subset of the states to have distributions conditioned on localized domains can significantly improve performance in unsupervised partof-speech tagging. We have also demonstrated that significant performance gains are possible simply by setting a different emission hyperparameter for a subgroup of the states. It is encouraging that these results hold for both models not just on the WSJ but across a diverse set of languages and measures. We believe our</context>
</contexts>
<marker>Zhao, Marcus, 2009</marker>
<rawString>Q. Zhao and M. Marcus. 2009. A simple unsupervised learner for POS disambiguation rules given only a minimal lexicon. In Proceedings of EMNLP, pages 688–697.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>