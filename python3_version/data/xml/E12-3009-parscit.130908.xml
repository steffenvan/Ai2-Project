<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000056">
<title confidence="0.997786">
Mining Co-Occurrence Matrices for SO-PMI Paradigm Word
Candidates
</title>
<author confidence="0.992186">
Aleksander Wawer
</author>
<affiliation confidence="0.998212">
Institute of Computer Science, Polish Academy of Science
</affiliation>
<address confidence="0.8791975">
ul. Jana Kazimierza 5
01-248 Warszawa, Poland
</address>
<email confidence="0.998637">
axw@ipipan.waw.pl
</email>
<sectionHeader confidence="0.993876" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999835083333333">
This paper is focused on one aspect of SO-
PMI, an unsupervised approach to senti-
ment vocabulary acquisition proposed by
Turney (Turney and Littman, 2003). The
method, originally applied and evaluated
for English, is often used in bootstrap-
ping sentiment lexicons for European lan-
guages where no such resources typically
exist. In general, SO-PMI values are com-
puted from word co-occurrence frequencies
in the neighbourhoods of two small sets of
paradigm words. The goal of this work is
to investigate how lexeme selection affects
the quality of obtained sentiment estima-
tions. This has been achieved by compar-
ing ad hoc random lexeme selection with
two alternative heuristics, based on clus-
tering and SVD decomposition of a word
co-occurrence matrix, demonstrating supe-
riority of the latter methods. The work can
be also interpreted as sensitivity analysis on
SO-PMI with regard to paradigm word se-
lection. The experiments were carried out
for Polish.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999318125">
This paper seeks to improve one of the main meth-
ods of unsupervised lexeme sentiment polarity as-
signment. The method, introduced by (Turney
and Littman, 2003), is described in more detail in
Section 2. It relies on two sets of paradigm words,
positive and negative, which determine the polar-
ity of unseen words.
The method is resource lean and therefore often
used in languages other than English. Recent ex-
amples include Japanese (Wang and Araki, 2007)
and German (Remus et al., 2006).
Unfortunately, the selection of paradigm words
rarely receives sufficient attention and is typically
done in an ad hoc manner. One notable example
of manual paradigm word selection method was
presented in (Read and Carroll, 2009).
In this context, an interesting variation of the
semantic orientation–pointwise mutual informa-
tion (SO-PMI) algorithm for Japanese was sug-
gested by (Wang and Araki, 2007). Authors, mo-
tivated by excessive leaning toward positive opin-
ions, proposed to modify the algorithm by intro-
ducing balancing factor and detecting neutral ex-
pressions. As will be demonstrated, this problem
can be addressed by proper selection of paradigm
pairs.
One not entirely realistic, but nevertheless in-
teresting theoretical possibility is to pick pairs
of opposing adjectives with the highest loadings
identified in Osgood’s experiments on semantic
differential (Osgood et al., 1967). In the exper-
iments, respondents were presented with a noun
and asked to choose its appropriate position on
a scale between two bipolar adjectives (for ex-
ample: adequate-inadequate, valuable-worthless,
hot-cold). Factor analysis of the results revealed
three distinctive factors, called Osgood dimen-
sions. The first of the dimensions, often consid-
ered synonymous with the notion of sentiment,
was called Evaluative because its foundational ad-
jective pair (one with the highest loading) is good-
bad.
The first problem with using adjective pairs as
exemplary for word co-occurrence distributions
on the basis of their loadings, is the fact that fac-
tor loadings as measured by Osgood et al. are not
necessarily reflected in word frequency phenom-
ena.
</bodyText>
<page confidence="0.986797">
74
</page>
<note confidence="0.6305805">
Proceedings of the EACL 2012 Student Research Workshop, pages 74–80,
Avignon, France, 26 April 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999905952380953">
The second problem is translation: an adjective
pair, central in English, may not be as strongly
associated with a dimension (here: Evaluative) in
other languages and cultures.
The approach we suggest in this paper assumes
a latent structure behind word co-occurrence fre-
quencies. The structure may be seen as a mix-
ture of latent variables of unknown distributions
that drives word selection. Some of the vari-
ables are more likely to produce certain types of
highly evaluative words (words with high senti-
ment scores). We do not attempt to model the
structure in a generative way as in for exam-
ple probabilistic latent semantic analysis (PLSA)
or latent Dirichlet allocation (LDA). A gener-
ative approximation is not feasible when using
corpora such as the balanced, 300-million ver-
sion of the National Corpus of Polish (NKJP)
(Przepiórkowski et al., 2008; Przepiórkowski et
al., 2012) 1 applied in the experiments described
in the next sections, which does not enable creat-
ing a word-document matrix and organizing word
occurrences by documents or narrowly specified
topics.
Therefore, we propose different techniques.
We begin with a symmetric matrix of word co-
occurences and attempt to discover as much of
its structure as possible using two well estab-
lished techniques: Singular Value Decomposi-
tion and clustering. The discovered structures are
then used to optimize the selection of words for
paradigm sets used in SO-PMI.
The paper is organized as follows. In Section
2 we define the SO-PMI measure and briefly for-
mulate the problem. Section 3 describes obtaining
the set of sentiment word candidates, which are
then used to generate a symmetric co-occurence
matrix as outlined in Section 4. Section 5 delin-
eates the details of human word scoring, which
serves as a basis for evaluations in 9. Sections
6, 7 and 8 describe three distinct approaches to
paradigm sets generation.
</bodyText>
<sectionHeader confidence="0.800932" genericHeader="introduction">
2 Problem Statement. SO-PMI
</sectionHeader>
<bodyText confidence="0.998317">
When creating a sentiment lexicon, the strength
of association between candidate words and each
of the two polar classes (positive and negative,
for instance) can be calculated using several mea-
</bodyText>
<footnote confidence="0.9553455">
1http://www.nkjp.pl/index.php?page=
0&amp;lang=1
</footnote>
<bodyText confidence="0.998070166666667">
sures. Perhaps most popular of them, employed in
this experiment after (Turney and Littman, 2003)
and (Grefenstette et al., 2006) is Pointwise Mutual
Information (PMI). The Pointwise Mutual Infor-
mation (PMI) between two words, w1 and w2, is
defined as:
</bodyText>
<equation confidence="0.99946">
� p(w�&amp;w2) �
PMI(w�, w2) = log� p(w1)p(w2)
</equation>
<bodyText confidence="0.9995824">
where p(w1 &amp; w2) is the probability of co-
occurrence of (w1) and (w2). For the task of as-
signing evaluative polarity, it is computed as num-
ber of co-occurrences of candidate words with
each of the paradigm positive and negative words,
denoted as pw and nw. Optimal selection of these
two sets of words is the subject of this paper.
Once the words are known, the semantic ori-
entation PMI (SO-PMI) of each candidate word c
can be computed as:
</bodyText>
<equation confidence="0.997598333333333">
SO-PMI(c) =
�= PMI(c, pw)− � PMI(c, nw)
pwEPW nwENW
</equation>
<bodyText confidence="0.999624">
The equation above demonstrates that opti-
mization of both word lists, pw and nw, is of cru-
cial importance for the performance of SO-PMI.
</bodyText>
<sectionHeader confidence="0.9551075" genericHeader="method">
3 Generating Sentiment Word
Candidates
</sectionHeader>
<bodyText confidence="0.99998505">
This section describes the acquisition of senti-
ment word candidates. The method we followed
could be substituted by any other technique which
results in a set of highly sentimental lexemes, pos-
sibly of varying unknown polarity and strength. A
similar experiment for English has been described
by (Grefenstette et al., 2006).
The procedure can be described as follows. In
the first step, a set of semi-manually defined lexi-
cal patterns is submitted to a search engine to find
candidates for evaluatively charged terms. Then,
the downloaded corpus is analyzed for pattern
continuations – lexemes immediately following
pattern matches, which are likely to be candidates
for sentiment words. In the last step, candidate
terms selected this way are tested for their senti-
ment strength and polarity (in other words, how
positive or negative are the conotations). In origi-
nal experiment described in the cited paper, words
were evaluated using the SO-PMI technique.
</bodyText>
<page confidence="0.993208">
75
</page>
<bodyText confidence="0.9996152">
The purpose of using extraction patterns is to
select candidates for evaluative words. In this
experiment, 112 patterns have been created by
generating all combinations of elements from two
manually prepared sets2, A and B:
</bodyText>
<listItem confidence="0.99972225">
• A: [0] wydawa´c si˛e, [1] wydawał si˛e, [2]
wydawała si˛e, [3] czu´c si˛e, [4] czułem si˛e,
[5] czułam si˛e, [6] czułem, [7] by´c 3
• B: [0] nie do´s´c, [1] niewystarczaj ˛aco, [2]
</listItem>
<bodyText confidence="0.886111514285714">
niedostatecznie, [3] za mało, [4] prawie, [5]
niemal, [6] tak, [7] taki, [8] zbyt, [9] zbyt-
nio, [10] za bardzo, [11] przesadnie, [12]
nadmiernie, [13] szczególnie 4
Each pattern (a combination of A and B) has
been wrapped with double quotes (“A B”) and
submitted to Google to narrow the results to texts
with exact phrases. The Web crawl yielded 17657
web pages, stripped from HTML and other web
tags to filter out non-textual content. Two patterns
are grammatically incorrect due to gender dis-
agreement, namely wydawała si˛e taki and czułam
si˛e taki 5, thus did not generate any results.
The corpus of 17657 web pages has been an-
alyzed using Spejd6, originally a tool for par-
tial parsing and rule-based morphosyntactic dis-
ambiguation, adapted in the context of this work
for the purpose of finding pattern continuations.
Again, 112 patterns were constructed by gener-
ating all combinations of elements from the two
sets, A and B above. Spejd rules were written as
“A B *” where the wildcard can be either an ad-
jective or an adverb.
Parsing the web pages using the 112 patterns
resulted in acquiring 1325 distinct base word
forms (lexemes) recognized by the morphologic
analyser and related dictionaries. This list is sub-
sequently used for generating the co-occurrence
2Terms are translations of words listed in (Grefenstette et
al., 2006). Many of the expressions denote either excess or
deficiency, as for example not enough or too much.
3English translations (morphosyntactic tags in parenthe-
ses): [0] seem to (inf), [1] seemed to (sg,pri,perf,m), [2]
seemed to (sg,pri,perf,f), [3] feel (inf), [4] felt (sg,pri,perf,m),
[5] felt (sg,pri,perf,f), [7] to be (inf)
</bodyText>
<footnote confidence="0.852011333333333">
4items [0-3] are various ways of expressing not enough,
items [4-5] almost, items [6-7] such, items [8-12] too much,
item [13] especially
5seemed(f) so(m) and felt(f) so(m)
6http://nlp.ipipan.waw.pl/Spejd/
(Przepiórkowski and Buczy´nski, 2007)
</footnote>
<bodyText confidence="0.994846">
matrix as delineated in the next Section and for
selecting paradigm words.
</bodyText>
<sectionHeader confidence="0.917215" genericHeader="method">
4 Word Co-Occurrence Matrix
</sectionHeader>
<bodyText confidence="0.9988298125">
Each word (base form) from the list was sought
in the balanced, 300 million segments7 version of
the National Corpus of Polish (NKJP). For each
row i and column j of the co-occurrence matrix
m, its value was computed as follows:
fij
fifj
where fij denotes the number of co-occurences
of word i within the window of 20 segments left
and right with word j, fi and fj denote the total
numbers of occurrences of each word. The se-
lection of a window of 20 follows the choice in
(Turney and Littman, 2003).
This design has been found optimal after a
number of experiments with the singular value de-
composition (SVD) technique described further.
Without the denominator part, decompositions are
heavily biased by word frequency. In this defni-
tion, the matrix resembles the PMI form in (Tur-
ney and Pantel, 2010), however we found that the
logarithm transformation flattens the eigenvalue
distribution and is not really necessary.
If the distributions of words i and j are statis-
tically independent, then by the defnition of inde-
pendence fifj = fij. The product fifj is what we
would expect for fij, if i occurs in the contexts of
j by the matter of a random chance. The opposing
situation happens when there exists a relationship
between i and j, for instance when both words
are generated by the same latent topic variable,
and we expect fij to be larger than in the case of
independency.
</bodyText>
<sectionHeader confidence="0.982983" genericHeader="method">
5 Evaluating Word Candidates
</sectionHeader>
<bodyText confidence="0.999705714285714">
In order to evaluate combinations of paradigm
words, one needs to compare the computed SO-
PMI scores against a human made scoring. Ide-
ally, such a scoring should not only inform about
polarity (indication whether a word is positive or
negative), but also about association strength (the
degree of positivity or negativity). Reliable and
</bodyText>
<footnote confidence="0.8748294">
7A segment usually corresponds to a word. Segments
are not longer than orthographic words, but sometimes
shorter. See http://nkjp.pl/poliqarp/help/
ense1.html#x2-10001 for a detailed discussion
mij =
</footnote>
<page confidence="0.857095">
76
</page>
<bodyText confidence="0.98123875">
valid measurement of word associations on a mul-
tipoint scale is not easy: the inter rater agreement
is likely to decrease with the growing complexity
of the scale.
Therefore, we decided that each lexeme was in-
dependently scored by two humans using a five
point scale. Extreme values denoted very nega-
tive or positive words, the central value denoted
neutral words and remaining intermediate values
were interpreted as somehow positive or nega-
tive. Discrepancies between raters were solved
by arithmetic means of conflicting scores rather
than introducing the third human (often called the
Golden Annotator) to select one value of the two.
Consequently, the 5-point scale extended to 10
points.
Human word scores were used in evaluations of
methods described in forthcoming sections.
of co-occurrence matrices was demonstrated by
studies such as (Mukherjee et al., 2009). The fo-
cus was on phonology with the intention to dis-
cover principles governing consonant inventories
and quantify their importance. Our work, as we
believe, is the first to apply SVD in the context of
co-occurrence matrices and SO-PMI.
We suspect that the SVD technique can be help-
ful by selecting lexemes that represent certain
amounts of latent co-occurrence structure. Fur-
thermore, the fact that 20 eigenvalues constitutes
approximately half of the norm of the spectrum
(Horn and Johnson, 1990), as on Table 1, suggests
that there may exist a small number of organiz-
ing principles which could be potentially helpful
to improve the selection of lexemes into paradigm
sets.
c m
</bodyText>
<sectionHeader confidence="0.978813" genericHeader="method">
6 Random Selection
</sectionHeader>
<bodyText confidence="0.999964454545454">
The baseline method to compare against is to se-
lect lexemes in a random fashion. In order to en-
sure highest possible performance of the method,
lexemes were selected only from those with at
least one extreme human score (very positive or
very negative) and at least 500 occurrences in the
corpus. The last condition renders this method
slightly favourable because in the case of SVD, in
many eigenvectors the highly loaded terms were
not as frequent and had to be selected despite rel-
ative rarity.
</bodyText>
<sectionHeader confidence="0.998016" genericHeader="method">
7 SVD
</sectionHeader>
<bodyText confidence="0.99919">
The word co-occurrence matrix m (1325x1325)
was the subject of singular value decomposition
(SVD), a well-known matrix factorization tech-
nique which decomposes a matrix A into three
matrices:
</bodyText>
<equation confidence="0.729603">
A = UEVT
</equation>
<bodyText confidence="0.9998527">
where E is a matrix whose diagonals are the
singular values of A, U and V are left and right
eigenvectors matrices.
The usage of SVD decompositions has a long
and successful history of applications in extract-
ing meaning from word frequencies in word-
document matrices, as for example the well es-
tablished algorithm of latent semantic indexing
(LSI). More recently, the usability of analyzing
the structure of language via spectral analysis
</bodyText>
<table confidence="0.851712">
0.728 0.410
0.797 0.498
0.924 0.720
</table>
<tableCaption confidence="0.975759">
Table 1: Frobenius norm of the spectrum for 10, 20
and 100 first eigenvalues.
Table 1 depicts also the problem of frequency
</tableCaption>
<figureCaption confidence="0.836476666666667">
bias, stronger in case of 10 and 20 eigenvalues
than for 100. The values were computed for two
matrices: c contains only co-occurrence frequen-
cies and m is the matrix described in section 4.
Figure 1 plots the eigenvalue spectrum restricted
to the first 100 values.
</figureCaption>
<figure confidence="0.9989331">
0.0040
0.0035
0.0030
0.0025
0.0020
0.0015
0.0010
0.0005
0.00000 20 40 60 80 100
Eigenvalues
</figure>
<figureCaption confidence="0.994255">
Figure 1: Eigenvalue distribution (limited to the first
100).
</figureCaption>
<bodyText confidence="0.993507">
In order to “discover” the principles behind the
co-occurrences, we examine eigenvectors associ-
</bodyText>
<figure confidence="0.988399">
10
20
100
Norm of the spectrum
llll
</figure>
<page confidence="0.994505">
77
</page>
<bodyText confidence="0.999654909090909">
ated with the largest eigenvalues. Some of the
vectors indeed appear to have their interpretations
or at least one could name common properties of
involved words. The meaning of vectors becomes
usually apparent after examination of the first few
top component weights.
The list below consits of four eigenvectors, top
three and the eighth one (as ordered according
to their eigenvalues), along with five terms with
highest absolute weights and interpretations of
each vector.
</bodyText>
<equation confidence="0.736078333333333">
1 sztuczny (artificial), liryczny (lyrical), upi-
orny (ghastly), zrz˛edliwy (grouchy), prze-
jrzysty (lucid).
</equation>
<bodyText confidence="0.9303862">
==&gt;. abstract properties one could attribute to
an actor or a play.
2 instynktowny (instinctive), odlotowo (su-
per/cool), ostro˙zny (careful), bolesny
(painful), przesadnie (excessively)
</bodyText>
<figure confidence="0.758168375">
==&gt;. physical and sensual experiences
3 wyemancypowa´c (emancipate), opuszczony
(abandoned), przeszywa´c (pierce), w´scibski
(inquisitive), jednakowo (alike)
==&gt;. unpleasant states and behaviours
8 gładki (smooth), kochany (beloved), stara´c
si˛e (make efforts), niedoł˛e˙zny (infirm), in-
tymnie (intimately)
</figure>
<bodyText confidence="0.975336166666667">
==&gt;. intimacy, caring, emotions
As it has been noted before, the eigenvectors
of pure co-occurrence matrix c did not deliver
anything close in terms of conceivable interpreta-
tions. It is also fairly clear that some of the eigen-
vectors, as for example the third one, are more re-
lated to sentiment than the others. This is also evi-
dent by examination of average lexeme sentiment
of top loaded terms of each vector, not disclosed
in the paper.
The heuristic of SVD backed selection of
paradigm words maximizes three factors:
</bodyText>
<listItem confidence="0.998493142857143">
• corpus frequency: avoid rare words where
possible;
• eigenvector component weights: select
words that contribute the most to a given
eigenvector;
• sentiment polarity: select words with the
highest absolute human scores.
</listItem>
<sectionHeader confidence="0.879842" genericHeader="method">
8 Affinity Propagation
</sectionHeader>
<bodyText confidence="0.998996285714286">
Affinity Propagation (Frey and Dueck, 2007)
method was selected because of two distinct ad-
vantages for our task. First is the fact that it
clusters data by diffusion in the similarity matrix,
therefore does not require finding representations
in Euclidean space. Second advantage, especially
over cluster analysis algorithms such as k-means,
is that the algorithm automatically sets its number
of clusters and does not depend on initialization.
Affinity Propagation clusters data by exchang-
ing real-valued messages between data points un-
til a high-quality set of exemplars (representative
examples, lexemes in our case) and corresponding
clusters gradually emerges.
Interestingly, in each parameter setting the al-
gorithm found exactly 156 clusters. It hints at
the fact that the number of “latent” variables be-
hind the co-occurrences could indeed be over 100.
This is further confirmed by the percentage of
norm of the spectrum covered by top 100 eigen-
values.
</bodyText>
<figure confidence="0.973928125">
30
25
20
15
10
5
00 20 40 60 80 100 120 140
Clusters
</figure>
<figureCaption confidence="0.999929">
Figure 2: Histogram of cluster counts.
</figureCaption>
<bodyText confidence="0.898440636363636">
The five most frequent clusters cover only 116
words. We restrict the selection of paradigm
words to the same frequency and polarity condi-
tions as in the case of random method. We pick
one paradigm word from each most frequent clus-
ter because we assume that it is sufficient to ap-
proximate the principle which organizes that clus-
ter. The heuristic is very similar to the one used
in case of SVD.
Frequency
llll
</bodyText>
<page confidence="0.992211">
78
</page>
<sectionHeader confidence="0.982797" genericHeader="evaluation">
9 Evaluation
</sectionHeader>
<bodyText confidence="0.9993">
Using continous SO-PMI and multi point scales
for human scoring facilitates formulating the
problem as a regression one, where goodness of
fit of the estimations can be computed using dif-
ferent measures than in the case of classification.
This, however, demands a mapping such that
ranges of the continuous SO-PMI scale corre-
spond to discrete human scores. We propose to
base such a mapping on dividing the SO-PMI
range into 10 segments {s0, ..., s10} of various
length, each of which corresponds to one discrete
human value.
The choice of values (locations) of specific
points is a subject of minimization where the error
function E over a set of words W is as follows:
</bodyText>
<equation confidence="0.955192">
�E = dist(sc,se)
WEW
</equation>
<bodyText confidence="0.999450909090909">
For each word w, the distance function dist re-
turns the number of segments between the correct
segment sc and the estimated segment se using
the SO-PMI. We minimize E and find optimum
locations for points separating each segment us-
ing Powell’s conjugate direction method, deter-
mined the most effective for this task. Powell’s
algorithm is a non-gradient numerical optimiza-
tion technique, applicable to a real valued func-
tion which does not need not be differentiable
(Powell, 1964).
</bodyText>
<sectionHeader confidence="0.999841" genericHeader="evaluation">
10 Results
</sectionHeader>
<bodyText confidence="0.999698647058823">
Table 2 presents E errors and extreme (min and
max) SO-PMI values computed over two indepen-
dent samples of 500 lexemes. Error columns indi-
cated as E denote errors computed either on non-
optimized default (def) or optimized segments
(min). Each combination of paradigm words and
each sample required re-computing optimum val-
ues of points dividing the SO-PMI scale into seg-
ments.
Generally, the randomized selection method
performs surprisingly well – most likely due to
the fact that the frequency and polarity conditions
are the key factors. In either case, the best re-
sult was obtained using the selection of paradigm
words using the heuristic based on svd, closely
followed by aff. In one case, random selection
performed better than the aff.
</bodyText>
<table confidence="0.9964975">
sample SO-PMI E min
min max def
S1 r1 -14 29 1226 908
r2 -15 23 1131 765
r3 -18 8.6 844 710
aff -9 25 1057 716
svd -13 26 1002 701
S2 r1 -18 19 983 812
r2 -17 15 910 756
r3 -11 20 1016 789
aff -13 28 1033 732
svd -13 35 1028 724
</table>
<tableCaption confidence="0.984728">
Table 2: SO-PMI ranges and error (E) values on two
independent random samples of N=500. 3 randomized
selections (r1 − r3), Affinity Propagation (aff) and
SVD (svd).
</tableCaption>
<bodyText confidence="0.999949230769231">
The small margin of a victory could be ex-
plained by the fact that the size of each set of
paradigm SO-PMI words is limited to five lex-
emes. Consequently, it is very difficult to repre-
sent a space of over one hundred latent variables
– because such appears to be the number indicated
by the distribution of eigenvalues in SVD and the
number of clusters.
The ranges of SO-PMI values (in the columns
min and max) were often non symmetric and
leaned towards positive. This shift did not nec-
essarily translate to higher error rates, especially
after optimizations.
</bodyText>
<sectionHeader confidence="0.973487" genericHeader="discussions">
11 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999945555555556">
The methods presented in this article, based on the
assumption of latent word co-occurrence struc-
tures, performed moderately better than the base-
line of random selections. The result is ambigu-
ous because it still requires a more in-depth un-
derstanding of underlying mechanims.
The work will be continued in several aspects.
One is to pre-determine lexeme type before it is
actually evaluated against particular members of
paradigm word sets. This could be acheved us-
ing a two-step model consisting of lexeme type
classification (with regard to over one hundred
latent variables) followed by SO-PMI computa-
tion, where the selection of paradigm words is not
fixed, as in this paper, but dependens on previ-
ously selected latent variables. Another promis-
ing direction is to focus on explanations and
word features: how adding or removing particu-
</bodyText>
<page confidence="0.993954">
79
</page>
<bodyText confidence="0.9999375">
lar words change the SO-PMI, and more impor-
tantly, why (in terms of features involved)? What
are the features that change SO-PMI in specific
directions? How to extract them?
</bodyText>
<sectionHeader confidence="0.954019" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.999142">
This research is supported by the POIG.01.01.02-
14-013/09 project which is co-financed by the Eu-
ropean Union under the European Regional De-
velopment Fund
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999925924242424">
Brendan J. Frey and Delbert Dueck. 2007. Clustering
by passing messages between data points. Science,
315:972–976.
Gregory Grefenstette, Yan Qu, David A. Evans, and
James G. Shanahan, 2006. Validating the Cover-
age ofLexical Resources for Affect Analysis and Au-
tomatically Classifying New Words along Semantic
Axes. Springer. Netherlands.
Roger A. Horn and Charles R. Johnson. 1990. Matrix
Analysis. Cambridge University Press.
Animesh Mukherjee, Monojit Choudhury, and Ravi
Kannan. 2009. Discovering global patterns in lin-
guistic networks through spectral analysis: a case
study of the consonant inventories. In Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
EACL ’09, pages 585–593, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Charles E. Osgood, George J. Suci, and Percy H. Tan-
nenbaum. 1967. The Measurement of Meaning.
University of Illinois Press.
M. J. D. Powell. 1964. An efficient method for finding
the minimum of a function of several variables with-
out calculating derivatives. The Computer Journal,
7(2):155–162, January.
Adam Przepiórkowski and Aleksander Buczy´nski.
2007.
spade: Shallow parsing and disambiguation engine.
In Proceedings of the 3rd Language &amp; Technology
Conference, Pozna´n.
Adam Przepiórkowski, Rafał L. Górski, Barbara
Lewandowska-Tomaszczyk, and Marek Łazi´nski.
2008. Towards the national corpus of polish. In
The proceedings of the 6th Language Resources and
Evaluation Conference (LREC 2008), Marrakesh,
Morocco.
Adam Przepiórkowski, Mirosław Ba´nko, Rafał L.
Górski, and Barbara Lewandowska-Tomaszczyk,
editors. 2012. Narodowy Korpus J˛ezyka Polskiego.
Wydawnictwo Naukowe PWN, Warsaw. Forthcom-
ing.
J. Read and J. Carroll. 2009. Weakly supervised
techniques for domain-independent sentiment clas-
sification. In Proceedings of the 1st international
CIKM workshop on Topic-sentiment analysis for
mass opinion, pages 45–52. ACM.
Robert Remus, Uwe Quasthoff, and Gerhard Heyer.
2006. Sentiws: a publicly available german-
language resource for sentiment analysis. In Pro-
ceedings of LREC.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on Infor-
mation Systems, 21:315–346.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: vector space models of seman-
tics. J. Artif. Int. Res., 37:141–188, January.
Guangwei Wang and Kenji Araki. 2007. Modifying
so-pmi for japanese weblog opinion mining by us-
ing a balancing factor and detecting neutral expres-
sions. In Human Language Technologies 2007: The
Conference of the North American Chapter of the
Association for Computational Linguistics; Com-
panion Volume, Short Papers, NAACL-Short ’07,
pages 189–192, Stroudsburg, PA, USA. Association
for Computational Linguistics.
</reference>
<page confidence="0.998235">
80
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.216024">
<title confidence="0.988937">Mining Co-Occurrence Matrices for SO-PMI Paradigm Word Candidates</title>
<author confidence="0.650199">Aleksander</author>
<affiliation confidence="0.591365">Institute of Computer Science, Polish Academy of ul. Jana Kazimierza</affiliation>
<address confidence="0.958953">01-248 Warszawa,</address>
<email confidence="0.995364">axw@ipipan.waw.pl</email>
<abstract confidence="0.97998336">This paper is focused on one aspect of SO- PMI, an unsupervised approach to sentiment vocabulary acquisition proposed by Turney (Turney and Littman, 2003). The method, originally applied and evaluated for English, is often used in bootstrapping sentiment lexicons for European languages where no such resources typically exist. In general, SO-PMI values are computed from word co-occurrence frequencies in the neighbourhoods of two small sets of paradigm words. The goal of this work is to investigate how lexeme selection affects the quality of obtained sentiment estimations. This has been achieved by comparing ad hoc random lexeme selection with two alternative heuristics, based on clustering and SVD decomposition of a word co-occurrence matrix, demonstrating superiority of the latter methods. The work can be also interpreted as sensitivity analysis on SO-PMI with regard to paradigm word selection. The experiments were carried out for Polish.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Brendan J Frey</author>
<author>Delbert Dueck</author>
</authors>
<title>Clustering by passing messages between data points.</title>
<date>2007</date>
<journal>Science,</journal>
<pages>315--972</pages>
<contexts>
<context position="17239" citStr="Frey and Dueck, 2007" startWordPosition="2744" endWordPosition="2747">o fairly clear that some of the eigenvectors, as for example the third one, are more related to sentiment than the others. This is also evident by examination of average lexeme sentiment of top loaded terms of each vector, not disclosed in the paper. The heuristic of SVD backed selection of paradigm words maximizes three factors: • corpus frequency: avoid rare words where possible; • eigenvector component weights: select words that contribute the most to a given eigenvector; • sentiment polarity: select words with the highest absolute human scores. 8 Affinity Propagation Affinity Propagation (Frey and Dueck, 2007) method was selected because of two distinct advantages for our task. First is the fact that it clusters data by diffusion in the similarity matrix, therefore does not require finding representations in Euclidean space. Second advantage, especially over cluster analysis algorithms such as k-means, is that the algorithm automatically sets its number of clusters and does not depend on initialization. Affinity Propagation clusters data by exchanging real-valued messages between data points until a high-quality set of exemplars (representative examples, lexemes in our case) and corresponding clust</context>
</contexts>
<marker>Frey, Dueck, 2007</marker>
<rawString>Brendan J. Frey and Delbert Dueck. 2007. Clustering by passing messages between data points. Science, 315:972–976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
<author>Yan Qu</author>
<author>David A Evans</author>
<author>James G Shanahan</author>
</authors>
<title>Validating the Coverage ofLexical Resources for Affect Analysis and Automatically Classifying New Words along Semantic Axes.</title>
<date>2006</date>
<publisher>Springer. Netherlands.</publisher>
<contexts>
<context position="5736" citStr="Grefenstette et al., 2006" startWordPosition="887" endWordPosition="890">-occurence matrix as outlined in Section 4. Section 5 delineates the details of human word scoring, which serves as a basis for evaluations in 9. Sections 6, 7 and 8 describe three distinct approaches to paradigm sets generation. 2 Problem Statement. SO-PMI When creating a sentiment lexicon, the strength of association between candidate words and each of the two polar classes (positive and negative, for instance) can be calculated using several mea1http://www.nkjp.pl/index.php?page= 0&amp;lang=1 sures. Perhaps most popular of them, employed in this experiment after (Turney and Littman, 2003) and (Grefenstette et al., 2006) is Pointwise Mutual Information (PMI). The Pointwise Mutual Information (PMI) between two words, w1 and w2, is defined as: � p(w�&amp;w2) � PMI(w�, w2) = log� p(w1)p(w2) where p(w1 &amp; w2) is the probability of cooccurrence of (w1) and (w2). For the task of assigning evaluative polarity, it is computed as number of co-occurrences of candidate words with each of the paradigm positive and negative words, denoted as pw and nw. Optimal selection of these two sets of words is the subject of this paper. Once the words are known, the semantic orientation PMI (SO-PMI) of each candidate word c can be comput</context>
<context position="9279" citStr="Grefenstette et al., 2006" startWordPosition="1480" endWordPosition="1483">ntactic disambiguation, adapted in the context of this work for the purpose of finding pattern continuations. Again, 112 patterns were constructed by generating all combinations of elements from the two sets, A and B above. Spejd rules were written as “A B *” where the wildcard can be either an adjective or an adverb. Parsing the web pages using the 112 patterns resulted in acquiring 1325 distinct base word forms (lexemes) recognized by the morphologic analyser and related dictionaries. This list is subsequently used for generating the co-occurrence 2Terms are translations of words listed in (Grefenstette et al., 2006). Many of the expressions denote either excess or deficiency, as for example not enough or too much. 3English translations (morphosyntactic tags in parentheses): [0] seem to (inf), [1] seemed to (sg,pri,perf,m), [2] seemed to (sg,pri,perf,f), [3] feel (inf), [4] felt (sg,pri,perf,m), [5] felt (sg,pri,perf,f), [7] to be (inf) 4items [0, 1, 2, 3] are various ways of expressing not enough, items [4, 5] almost, items [6, 7] such, items [8, 9, 10, 11, 12] too much, item [13] especially 5seemed(f) so(m) and felt(f) so(m) 6http://nlp.ipipan.waw.pl/Spejd/ (Przepiórkowski and Buczy´nski, 2007) matrix a</context>
</contexts>
<marker>Grefenstette, Qu, Evans, Shanahan, 2006</marker>
<rawString>Gregory Grefenstette, Yan Qu, David A. Evans, and James G. Shanahan, 2006. Validating the Coverage ofLexical Resources for Affect Analysis and Automatically Classifying New Words along Semantic Axes. Springer. Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger A Horn</author>
<author>Charles R Johnson</author>
</authors>
<title>Matrix Analysis.</title>
<date>1990</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="13292" citStr="Horn and Johnson, 1990" startWordPosition="2121" endWordPosition="2124">ds described in forthcoming sections. of co-occurrence matrices was demonstrated by studies such as (Mukherjee et al., 2009). The focus was on phonology with the intention to discover principles governing consonant inventories and quantify their importance. Our work, as we believe, is the first to apply SVD in the context of co-occurrence matrices and SO-PMI. We suspect that the SVD technique can be helpful by selecting lexemes that represent certain amounts of latent co-occurrence structure. Furthermore, the fact that 20 eigenvalues constitutes approximately half of the norm of the spectrum (Horn and Johnson, 1990), as on Table 1, suggests that there may exist a small number of organizing principles which could be potentially helpful to improve the selection of lexemes into paradigm sets. c m 6 Random Selection The baseline method to compare against is to select lexemes in a random fashion. In order to ensure highest possible performance of the method, lexemes were selected only from those with at least one extreme human score (very positive or very negative) and at least 500 occurrences in the corpus. The last condition renders this method slightly favourable because in the case of SVD, in many eigenve</context>
</contexts>
<marker>Horn, Johnson, 1990</marker>
<rawString>Roger A. Horn and Charles R. Johnson. 1990. Matrix Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Animesh Mukherjee</author>
<author>Monojit Choudhury</author>
<author>Ravi Kannan</author>
</authors>
<title>Discovering global patterns in linguistic networks through spectral analysis: a case study of the consonant inventories.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’09,</booktitle>
<pages>585--593</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="12793" citStr="Mukherjee et al., 2009" startWordPosition="2041" endWordPosition="2044">g a five point scale. Extreme values denoted very negative or positive words, the central value denoted neutral words and remaining intermediate values were interpreted as somehow positive or negative. Discrepancies between raters were solved by arithmetic means of conflicting scores rather than introducing the third human (often called the Golden Annotator) to select one value of the two. Consequently, the 5-point scale extended to 10 points. Human word scores were used in evaluations of methods described in forthcoming sections. of co-occurrence matrices was demonstrated by studies such as (Mukherjee et al., 2009). The focus was on phonology with the intention to discover principles governing consonant inventories and quantify their importance. Our work, as we believe, is the first to apply SVD in the context of co-occurrence matrices and SO-PMI. We suspect that the SVD technique can be helpful by selecting lexemes that represent certain amounts of latent co-occurrence structure. Furthermore, the fact that 20 eigenvalues constitutes approximately half of the norm of the spectrum (Horn and Johnson, 1990), as on Table 1, suggests that there may exist a small number of organizing principles which could be</context>
</contexts>
<marker>Mukherjee, Choudhury, Kannan, 2009</marker>
<rawString>Animesh Mukherjee, Monojit Choudhury, and Ravi Kannan. 2009. Discovering global patterns in linguistic networks through spectral analysis: a case study of the consonant inventories. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’09, pages 585–593, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles E Osgood</author>
<author>George J Suci</author>
<author>Percy H Tannenbaum</author>
</authors>
<title>The Measurement of Meaning.</title>
<date>1967</date>
<publisher>University of Illinois Press.</publisher>
<contexts>
<context position="2557" citStr="Osgood et al., 1967" startWordPosition="390" endWordPosition="393">n of the semantic orientation–pointwise mutual information (SO-PMI) algorithm for Japanese was suggested by (Wang and Araki, 2007). Authors, motivated by excessive leaning toward positive opinions, proposed to modify the algorithm by introducing balancing factor and detecting neutral expressions. As will be demonstrated, this problem can be addressed by proper selection of paradigm pairs. One not entirely realistic, but nevertheless interesting theoretical possibility is to pick pairs of opposing adjectives with the highest loadings identified in Osgood’s experiments on semantic differential (Osgood et al., 1967). In the experiments, respondents were presented with a noun and asked to choose its appropriate position on a scale between two bipolar adjectives (for example: adequate-inadequate, valuable-worthless, hot-cold). Factor analysis of the results revealed three distinctive factors, called Osgood dimensions. The first of the dimensions, often considered synonymous with the notion of sentiment, was called Evaluative because its foundational adjective pair (one with the highest loading) is goodbad. The first problem with using adjective pairs as exemplary for word co-occurrence distributions on the</context>
</contexts>
<marker>Osgood, Suci, Tannenbaum, 1967</marker>
<rawString>Charles E. Osgood, George J. Suci, and Percy H. Tannenbaum. 1967. The Measurement of Meaning. University of Illinois Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J D Powell</author>
</authors>
<title>An efficient method for finding the minimum of a function of several variables without calculating derivatives.</title>
<date>1964</date>
<journal>The Computer Journal,</journal>
<volume>7</volume>
<issue>2</issue>
<contexts>
<context position="19848" citStr="Powell, 1964" startWordPosition="3181" endWordPosition="3182">f specific points is a subject of minimization where the error function E over a set of words W is as follows: �E = dist(sc,se) WEW For each word w, the distance function dist returns the number of segments between the correct segment sc and the estimated segment se using the SO-PMI. We minimize E and find optimum locations for points separating each segment using Powell’s conjugate direction method, determined the most effective for this task. Powell’s algorithm is a non-gradient numerical optimization technique, applicable to a real valued function which does not need not be differentiable (Powell, 1964). 10 Results Table 2 presents E errors and extreme (min and max) SO-PMI values computed over two independent samples of 500 lexemes. Error columns indicated as E denote errors computed either on nonoptimized default (def) or optimized segments (min). Each combination of paradigm words and each sample required re-computing optimum values of points dividing the SO-PMI scale into segments. Generally, the randomized selection method performs surprisingly well – most likely due to the fact that the frequency and polarity conditions are the key factors. In either case, the best result was obtained u</context>
</contexts>
<marker>Powell, 1964</marker>
<rawString>M. J. D. Powell. 1964. An efficient method for finding the minimum of a function of several variables without calculating derivatives. The Computer Journal, 7(2):155–162, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Przepiórkowski</author>
<author>Aleksander Buczy´nski</author>
</authors>
<date>2007</date>
<marker>Przepiórkowski, Buczy´nski, 2007</marker>
<rawString>Adam Przepiórkowski and Aleksander Buczy´nski. 2007.</rawString>
</citation>
<citation valid="false">
<authors>
<author>spade Shallow</author>
</authors>
<title>parsing and disambiguation engine.</title>
<booktitle>In Proceedings of the 3rd Language &amp; Technology Conference,</booktitle>
<location>Pozna´n.</location>
<marker>Shallow, </marker>
<rawString>spade: Shallow parsing and disambiguation engine. In Proceedings of the 3rd Language &amp; Technology Conference, Pozna´n.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Przepiórkowski</author>
<author>Rafał L Górski</author>
<author>Barbara Lewandowska-Tomaszczyk</author>
<author>Marek Łazi´nski</author>
</authors>
<title>Towards the national corpus of polish.</title>
<date>2008</date>
<booktitle>In The proceedings of the 6th Language Resources and Evaluation Conference (LREC</booktitle>
<location>Marrakesh, Morocco.</location>
<marker>Przepiórkowski, Górski, Lewandowska-Tomaszczyk, Łazi´nski, 2008</marker>
<rawString>Adam Przepiórkowski, Rafał L. Górski, Barbara Lewandowska-Tomaszczyk, and Marek Łazi´nski. 2008. Towards the national corpus of polish. In The proceedings of the 6th Language Resources and Evaluation Conference (LREC 2008), Marrakesh, Morocco.</rawString>
</citation>
<citation valid="true">
<date>2012</date>
<booktitle>Narodowy Korpus J˛ezyka Polskiego. Wydawnictwo Naukowe PWN,</booktitle>
<editor>Adam Przepiórkowski, Mirosław Ba´nko, Rafał L. Górski, and Barbara Lewandowska-Tomaszczyk, editors.</editor>
<location>Warsaw. Forthcoming.</location>
<marker>2012</marker>
<rawString>Adam Przepiórkowski, Mirosław Ba´nko, Rafał L. Górski, and Barbara Lewandowska-Tomaszczyk, editors. 2012. Narodowy Korpus J˛ezyka Polskiego. Wydawnictwo Naukowe PWN, Warsaw. Forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Read</author>
<author>J Carroll</author>
</authors>
<title>Weakly supervised techniques for domain-independent sentiment classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion,</booktitle>
<pages>45--52</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1895" citStr="Read and Carroll, 2009" startWordPosition="293" endWordPosition="296"> assignment. The method, introduced by (Turney and Littman, 2003), is described in more detail in Section 2. It relies on two sets of paradigm words, positive and negative, which determine the polarity of unseen words. The method is resource lean and therefore often used in languages other than English. Recent examples include Japanese (Wang and Araki, 2007) and German (Remus et al., 2006). Unfortunately, the selection of paradigm words rarely receives sufficient attention and is typically done in an ad hoc manner. One notable example of manual paradigm word selection method was presented in (Read and Carroll, 2009). In this context, an interesting variation of the semantic orientation–pointwise mutual information (SO-PMI) algorithm for Japanese was suggested by (Wang and Araki, 2007). Authors, motivated by excessive leaning toward positive opinions, proposed to modify the algorithm by introducing balancing factor and detecting neutral expressions. As will be demonstrated, this problem can be addressed by proper selection of paradigm pairs. One not entirely realistic, but nevertheless interesting theoretical possibility is to pick pairs of opposing adjectives with the highest loadings identified in Osgoo</context>
</contexts>
<marker>Read, Carroll, 2009</marker>
<rawString>J. Read and J. Carroll. 2009. Weakly supervised techniques for domain-independent sentiment classification. In Proceedings of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion, pages 45–52. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Remus</author>
<author>Uwe Quasthoff</author>
<author>Gerhard Heyer</author>
</authors>
<title>Sentiws: a publicly available germanlanguage resource for sentiment analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="1664" citStr="Remus et al., 2006" startWordPosition="258" endWordPosition="261">as sensitivity analysis on SO-PMI with regard to paradigm word selection. The experiments were carried out for Polish. 1 Introduction This paper seeks to improve one of the main methods of unsupervised lexeme sentiment polarity assignment. The method, introduced by (Turney and Littman, 2003), is described in more detail in Section 2. It relies on two sets of paradigm words, positive and negative, which determine the polarity of unseen words. The method is resource lean and therefore often used in languages other than English. Recent examples include Japanese (Wang and Araki, 2007) and German (Remus et al., 2006). Unfortunately, the selection of paradigm words rarely receives sufficient attention and is typically done in an ad hoc manner. One notable example of manual paradigm word selection method was presented in (Read and Carroll, 2009). In this context, an interesting variation of the semantic orientation–pointwise mutual information (SO-PMI) algorithm for Japanese was suggested by (Wang and Araki, 2007). Authors, motivated by excessive leaning toward positive opinions, proposed to modify the algorithm by introducing balancing factor and detecting neutral expressions. As will be demonstrated, this</context>
</contexts>
<marker>Remus, Quasthoff, Heyer, 2006</marker>
<rawString>Robert Remus, Uwe Quasthoff, and Gerhard Heyer. 2006. Sentiws: a publicly available germanlanguage resource for sentiment analysis. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Michael Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems,</journal>
<pages>21--315</pages>
<contexts>
<context position="1337" citStr="Turney and Littman, 2003" startWordPosition="202" endWordPosition="205"> how lexeme selection affects the quality of obtained sentiment estimations. This has been achieved by comparing ad hoc random lexeme selection with two alternative heuristics, based on clustering and SVD decomposition of a word co-occurrence matrix, demonstrating superiority of the latter methods. The work can be also interpreted as sensitivity analysis on SO-PMI with regard to paradigm word selection. The experiments were carried out for Polish. 1 Introduction This paper seeks to improve one of the main methods of unsupervised lexeme sentiment polarity assignment. The method, introduced by (Turney and Littman, 2003), is described in more detail in Section 2. It relies on two sets of paradigm words, positive and negative, which determine the polarity of unseen words. The method is resource lean and therefore often used in languages other than English. Recent examples include Japanese (Wang and Araki, 2007) and German (Remus et al., 2006). Unfortunately, the selection of paradigm words rarely receives sufficient attention and is typically done in an ad hoc manner. One notable example of manual paradigm word selection method was presented in (Read and Carroll, 2009). In this context, an interesting variatio</context>
<context position="5704" citStr="Turney and Littman, 2003" startWordPosition="882" endWordPosition="885">used to generate a symmetric co-occurence matrix as outlined in Section 4. Section 5 delineates the details of human word scoring, which serves as a basis for evaluations in 9. Sections 6, 7 and 8 describe three distinct approaches to paradigm sets generation. 2 Problem Statement. SO-PMI When creating a sentiment lexicon, the strength of association between candidate words and each of the two polar classes (positive and negative, for instance) can be calculated using several mea1http://www.nkjp.pl/index.php?page= 0&amp;lang=1 sures. Perhaps most popular of them, employed in this experiment after (Turney and Littman, 2003) and (Grefenstette et al., 2006) is Pointwise Mutual Information (PMI). The Pointwise Mutual Information (PMI) between two words, w1 and w2, is defined as: � p(w�&amp;w2) � PMI(w�, w2) = log� p(w1)p(w2) where p(w1 &amp; w2) is the probability of cooccurrence of (w1) and (w2). For the task of assigning evaluative polarity, it is computed as number of co-occurrences of candidate words with each of the paradigm positive and negative words, denoted as pw and nw. Optimal selection of these two sets of words is the subject of this paper. Once the words are known, the semantic orientation PMI (SO-PMI) of eac</context>
<context position="10473" citStr="Turney and Littman, 2003" startWordPosition="1670" endWordPosition="1673"> and Buczy´nski, 2007) matrix as delineated in the next Section and for selecting paradigm words. 4 Word Co-Occurrence Matrix Each word (base form) from the list was sought in the balanced, 300 million segments7 version of the National Corpus of Polish (NKJP). For each row i and column j of the co-occurrence matrix m, its value was computed as follows: fij fifj where fij denotes the number of co-occurences of word i within the window of 20 segments left and right with word j, fi and fj denote the total numbers of occurrences of each word. The selection of a window of 20 follows the choice in (Turney and Littman, 2003). This design has been found optimal after a number of experiments with the singular value decomposition (SVD) technique described further. Without the denominator part, decompositions are heavily biased by word frequency. In this defnition, the matrix resembles the PMI form in (Turney and Pantel, 2010), however we found that the logarithm transformation flattens the eigenvalue distribution and is not really necessary. If the distributions of words i and j are statistically independent, then by the defnition of independence fifj = fij. The product fifj is what we would expect for fij, if i occ</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Peter Turney and Michael Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems, 21:315–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: vector space models of semantics.</title>
<date>2010</date>
<journal>J. Artif. Int. Res.,</journal>
<pages>37--141</pages>
<contexts>
<context position="10777" citStr="Turney and Pantel, 2010" startWordPosition="1717" endWordPosition="1721">ence matrix m, its value was computed as follows: fij fifj where fij denotes the number of co-occurences of word i within the window of 20 segments left and right with word j, fi and fj denote the total numbers of occurrences of each word. The selection of a window of 20 follows the choice in (Turney and Littman, 2003). This design has been found optimal after a number of experiments with the singular value decomposition (SVD) technique described further. Without the denominator part, decompositions are heavily biased by word frequency. In this defnition, the matrix resembles the PMI form in (Turney and Pantel, 2010), however we found that the logarithm transformation flattens the eigenvalue distribution and is not really necessary. If the distributions of words i and j are statistically independent, then by the defnition of independence fifj = fij. The product fifj is what we would expect for fij, if i occurs in the contexts of j by the matter of a random chance. The opposing situation happens when there exists a relationship between i and j, for instance when both words are generated by the same latent topic variable, and we expect fij to be larger than in the case of independency. 5 Evaluating Word Can</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: vector space models of semantics. J. Artif. Int. Res., 37:141–188, January.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Guangwei Wang</author>
<author>Kenji Araki</author>
</authors>
<title>Modifying so-pmi for japanese weblog opinion mining by using a balancing factor and detecting neutral expressions.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers, NAACL-Short ’07,</booktitle>
<pages>189--192</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1632" citStr="Wang and Araki, 2007" startWordPosition="252" endWordPosition="255"> The work can be also interpreted as sensitivity analysis on SO-PMI with regard to paradigm word selection. The experiments were carried out for Polish. 1 Introduction This paper seeks to improve one of the main methods of unsupervised lexeme sentiment polarity assignment. The method, introduced by (Turney and Littman, 2003), is described in more detail in Section 2. It relies on two sets of paradigm words, positive and negative, which determine the polarity of unseen words. The method is resource lean and therefore often used in languages other than English. Recent examples include Japanese (Wang and Araki, 2007) and German (Remus et al., 2006). Unfortunately, the selection of paradigm words rarely receives sufficient attention and is typically done in an ad hoc manner. One notable example of manual paradigm word selection method was presented in (Read and Carroll, 2009). In this context, an interesting variation of the semantic orientation–pointwise mutual information (SO-PMI) algorithm for Japanese was suggested by (Wang and Araki, 2007). Authors, motivated by excessive leaning toward positive opinions, proposed to modify the algorithm by introducing balancing factor and detecting neutral expression</context>
</contexts>
<marker>Wang, Araki, 2007</marker>
<rawString>Guangwei Wang and Kenji Araki. 2007. Modifying so-pmi for japanese weblog opinion mining by using a balancing factor and detecting neutral expressions. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers, NAACL-Short ’07, pages 189–192, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>