<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000038">
<title confidence="0.99696">
Answer Credibility: A Language Modeling Approach
to Answer Validation
</title>
<author confidence="0.917374">
Protima Banerjee Hyoil Han
</author>
<affiliation confidence="0.9591">
College of Information Science and Technology
Drexel University
</affiliation>
<address confidence="0.936377">
Philadelphia, PA 191 04
</address>
<email confidence="0.989455">
pb66@drexel.edu, hyoil.han@acm.org
</email>
<sectionHeader confidence="0.993634" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9961772">
Answer Validation is a topic of significant in-
terest within the Question Answering commu-
nity. In this paper, we propose the use of
language modeling methodologies for Answer
Validation, using corpus-based methods that do
not require the use of external sources. Specifi-
cally, we propose a model for Answer Credibil-
ity which quantifies the reliability of a source
document that contains a candidate answer and
the Question&apos;s Context Model.
</bodyText>
<sectionHeader confidence="0.998678" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999898484848485">
In recent years, Answer Validation has become a
topic of significant interest within the Question
Answering community. In the general case, one
can describe Answer Validation as the process that
decides whether a Question is correctly answered
by an Answer according to a given segment of sup-
porting Text. Magnini et al. (Magnini, 2 002) pre-
sents an approach to Answer Validation that uses
redundant information sources on the Web; they
propose that the number of Web documents in
which the question and the answer co-occurred can
serve as an indicator of answer validity. Other re-
cent approaches to Answer Validation Exercise in
the Cross-Language Evaluation Forum (CLEF)
(Peters, 2 008) make use of textual entailment
methodologies for the purposes of Answer Valida-
tion.
In this paper, we propose the use of language mod-
eling methodologies for Answer Validation, using
corpus-based methods that do not require the use
of external sources. Specifically, we propose the
development of an Answer Credibility score which
quantifies reliability of a source document that
contains a candidate answer with respect to the
Question&apos;s Context Model. Unlike many textual
entailment methods, our methodology has the ad-
vantage of being applicable to question types for
which hypothesis generation is not easily accom-
plished.
The remainder of this paper describes our work in
progress, including our model for Answer Credi-
bility, our experiments and results to date, and fu-
ture work.
</bodyText>
<sectionHeader confidence="0.825195" genericHeader="method">
2 Answer Credibility
</sectionHeader>
<bodyText confidence="0.99918385">
Credibility has been extensively studied in the field
of information science (Metzger, 2 002). Credibil-
ity in the computational sciences has been charac-
terized as being synonymous with believability,
and has been broken down into the dimensions of
trustworthiness and expertise.
Our mathematical model of Answer Credibility
attempts to quantify the reliability of a source us-
ing the semantic Question Context. The semantic
Question Context is built using the Aspect-Based
Relevance Language Model that was presented in
(Banerjee, 2 008) and (Banerjee, 2 009). This model
builds upon the Relevance Based Language Model
(Lavrenko, 2 001) and Probabilisitic Latent Seman-
tic Analysis (PLSA) (Hofmann, 1999) to provide a
mechanism for relating sense disambiguated Con-
cept Terms (CT) to a query by their likelihood of
relevance.
The Aspect-Based Relevance Language Model
assumes that for every question there exists an un-
</bodyText>
<page confidence="0.971912">
157
</page>
<note confidence="0.358607">
Proceedings of NAACL HLT 2009: Short Papers, pages 157–160,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.994712962962963">
derlying relevance model R, which is assigned
probabilities P(zlR) where z is a latent aspect of the
information need, as defined by PLSA. Thus, we
can obtain a distribution of aspects according to
their likelihood of relevancy to the user&apos;s informa-
tion need. By considering terms from the aspects
that have the highest likelihood of relevance (eg.
highest P(zlR) values), we can build a distribution
that models a semantic Question Context.
We define Answer Credibility to be a similarity
measure between the Question Context (QC) and
the source document from which the answer was
derived. We consider the Question Context to be a
document, which has a corresponding document
language model. We then use the well-known
Kullback-Leibler divergence method (Lafferty,
2 001) to compute the similarity between the Ques-
tion Context document model and the document
model for a document containing a candidate an-
swer:
Here,
is the language model of the Ques-
tion Context, P(wld) is the language model o the
document containing the candidate answer. To
insert this model into the Answer Validation proc-
ess, we propose an interpolation technique that
modulates the answer score duri
</bodyText>
<equation confidence="0.752031">
P(wlQC)
</equation>
<bodyText confidence="0.762182">
ng the process us-
</bodyText>
<sectionHeader confidence="0.645302" genericHeader="method">
ing Answer Credibility.
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.487079">
guage models.
</bodyText>
<figureCaption confidence="0.997997">
Figure 1: Experiment Methodology
</figureCaption>
<bodyText confidence="0.699773666666667">
lates the OpenEphyra an
swer score according to
the following formula:
</bodyText>
<figure confidence="0.638787666666667">
+A
* score
* AnswerCredibility
</figure>
<bodyText confidence="0.829791857142857">
158 gine (Ogilvie, 2001) to construct the Question
Context and document lan
set using the average of the P(zl
R) values for those
aspects that are included in the Question Context.
Mean Reciprocal Rank (MRR) metri
cs (Voorhees,
</bodyText>
<sectionHeader confidence="0.691538" genericHeader="method">
2 005).
4 Results
</sectionHeader>
<figure confidence="0.978809606060606">
Pw QC
( � )
AnswerCredibility = P
E
w�Cr
TREC 2008, CIIIe�IBe
ClweSme
0~.
�����&amp;quot;���#�$�����
a
����������
�� ������
2008
/hHpe!enb
TRK
MUTANT
������
%&amp;&apos;A
Clwryta!e
E.a#istlm
~t-��!�����
Rd�
A�e(�re
*genEP+ is
Setl
CN Ana(x
&apos;�#������ ���
)�#�������
CN TBBt
�� ��������
���# ���
CN Ana(x
E&amp;quot;��ectlon
</figure>
<bodyText confidence="0.4916536">
from the Text Retri
eval Conference (TREC) 2 006
Question Answering Track (Voorhees, 2 006).
has been included as a part
of the OpenEphyra
</bodyText>
<tableCaption confidence="0.997708">
pipeline. Our results are presented in Table 1 and
Table 2.
</tableCaption>
<table confidence="0.909237">
d the Indri search en- ( � ) log
w QC
P w d
( � )
The experimental methodology we used is shown
</table>
<bodyText confidence="0.988540893617021">
as a block diagram in Figure 1. To validate our
approach, we used the set of all factoid questions
The OpenEphyra Question Answering testbed
(Schlaefer, 2006) was then used as the framework
for our Answer Credibility implementation.
OpenEphyra uses a baseline Answer Validation
mechanism which uses documents retrieved using
Yahoo! search to support candidate answers found
in retrieved passages. In our experiments, we con-
structed the Question Context according to the
methodology described in (Banerjee, 2008). Our
experiments used the Lemur Language Modeling
toolkit (Strohman, 2005) an
a
�Wti.
���������
,
�
We then inserted an Answer Credibility filter into
the OpenEphyra processing pipeline which modu-
score&apos;= (1— A)
Here score is the original OpenEphyra answer
score and score&apos; is the modulated answer score. In
this model, A is an interpolation constant which we
For the purposes of evaluating the effectiveness of
our theoretical model, we use the accuracy and
We compare the results of the baseline
OpenEphyra Answer Validation approach against
the results after our Answer Credibility processing
To facilitate interpretation of our results, we sub-
divided the set of factoid questions into categories
by their question words, following the example of
(Murdock, 2006). The light grey shaded cells in
both tables indicate categories for which improve-
ments were observed after our Answer Credibility
model was applied. The dark grey shaded cells in
both tables indicate categories for which no chan
ge
was observed. The paired Wilcoxon signed rank
test was used to measure significance in improve-
ments for MRR; the shaded cells in Table 2 indi-
cate results for which the results were significant
(p&lt; 0. 05). Due to the binary results for accuracy at
the question level (eg. a question is either correct
or incorrect), the Wilcoxon test was found to be
inappropriate for measuring statistical significance
in accuracy.
</bodyText>
<tableCaption confidence="0.999669">
Table 1: Average MRR of Baseline vs. Baseline Including
</tableCaption>
<table confidence="0.984624529411765">
Answer Credibility
Question Question Baseline Baseline + An-
Category Count MRR swer Credibil-
ity MRR
How 2 0 0.33 0.28
how many 58 0.21 0.16
how much 6 0. 08 0. 02
in what 47 0.68 0.6 0
What 114 0.3 0 0.33
what is 28 0.26 0.26
When 29 0.3 0 0.19
Where 23 0.37 0.37
where is 6 0.40 0.4 0
Which 17 0.38 0.26
Who 17 0.51 0.63
who is 14 0.60 0.74
who was 24 0.43 0.55
</table>
<tableCaption confidence="0.86358">
Table 2: Average Accuracy of Baseline vs. Baseline In-
cluding Answer Credibility
</tableCaption>
<table confidence="0.998167">
Question Question Baseline Baseline +
Category Count Accuracy Answer
Credibility
Accuracy
How 2 0 0.25 0.2 0
how many 58 0.12 0. 07
how much 6 0. 00 0. 00
in what 47 0.64 0.55
What 114 0.23 0.28
what is 28 0.18 0.18
When 29 0.21 0.1 0
Where 23 0.3 0 0.3 0
where is 6 0.33 0.33
Which 17 0.29 0.18
Who 17 0.47 0.59
who is 14 0.57 0.71
who was 24 0.38 0.5 0
</table>
<bodyText confidence="0.896705">
Our results show the following:
</bodyText>
<listItem confidence="0.987546307692308">
• A 5% improvement in accuracy over the base-
line for &amp;quot;what&amp;quot;-type questions.
• An overall improvement of 13% in accuracy
for &amp;quot;who&amp;quot;-type questions, which include the
&amp;quot;who,&amp;quot; &amp;quot;who is&amp;quot; and &amp;quot;who was&amp;quot; categories
• A 9% improvements in MRR for &amp;quot;what&amp;quot; type
questions
• An overall improvement of 25% in MRR for
&amp;quot;who&amp;quot;-type questions, which include the
&amp;quot;who,&amp;quot; &amp;quot;who is&amp;quot; and &amp;quot;who was&amp;quot; categories
• Overall, 7 out of 13 categories (58%) per-
formed at the same level or better than the
baseline
</listItem>
<sectionHeader confidence="0.984519" genericHeader="discussions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999942473684211">
In this section, we examine some examples of
questions that showed improvement to better un-
derstand and interpret our results.
First, we examine a &amp;quot;who&amp;quot; type question which
was not correctly answered by the baseline system,
but which was correctly answered after including
Answer Credibility. For the question &amp;quot;Who is the
host of the Daily Show?&amp;quot; the baseline system cor-
rectly determined the answer was &amp;quot;Jon Stewart&amp;quot;
but incorrectly identified the document that this
answer was derived from. For this question, the
Question Context included the terms &amp;quot;stewart,&amp;quot;
&amp;quot;comedy,&amp;quot; &amp;quot;television,&amp;quot; &amp;quot;news,&amp;quot; and &amp;quot;kilborn.&amp;quot;
(Craig Kilborn was the host of Daily Show until
1999, which makes his name a logical candidate
for inclusion in the Question Context since the
AQUAINT corpus spans 1996-2 000). In this case,
the correct document that the answer was derived
from was actually ranked third in the list. The An-
swer Credibility filter was able to correctly in-
crease the answer score of that document so that it
was ranked as the most reliable source for the an-
swer and chosen as the correct final result.
Next, we consider a case where the correct answer
was ranked at a lower position in the answer list in
the baseline results and correctly raised higher,
though not to the top rank, after the application of
our Answer Credibility filter. For the question
&amp;quot;What position did Janet Reno assume in 1993?&amp;quot;
the correct answer (&amp;quot;attorney general&amp;quot;) was ranked
5 in the list in the baseline results. However, in
this case the score associated with the answer was
lower than the top-ranked answer by an order of
magnitude. The Question Context for this question
included the terms &amp;quot;miami,&amp;quot; &amp;quot;elian,&amp;quot; &amp;quot;gonzales,&amp;quot;
&amp;quot;boy,&amp;quot; &amp;quot;attorney&amp;quot; and &amp;quot;justice.&amp;quot; After the applica-
tion of our Answer Credibility filter, the score and
rank of the correct answer did increase (which con-
</bodyText>
<page confidence="0.99649">
159
</page>
<bodyText confidence="0.999774833333333">
Hofmann, T. 1999. &amp;quot;Probabilistic latent semantic index-
ing,&amp;quot; Proceedings of the 22nd Annual International
SIGIR.
tributed to an increase in MRR), but the increase
was not enough to overshoot the original top-
ranked answer.
Categories for which the Answer Credibility had
negative effect included &amp;quot;how much&amp;quot; and &amp;quot;how
many&amp;quot; questions. For these question types, the
correct answer or correct document was frequently
not present in the answer list. In this case, the An-
swer Credibility filter had no opportunity to in-
crease the rank of correct answers or correct
documents in the answer list. This same reasoning
also limits our applicability to questions that re-
quire a date in response.
Finally, it is important to note here that the very
nature of news data makes our methodology appli-
cable to some categories of questions more than
others. Since our methodology relies on the ability
to derive semantic relationships via a statistical
examination of text, it performs best on those ques-
tions for which some amount of supporting infor-
mation is available.
</bodyText>
<sectionHeader confidence="0.998308" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999989833333333">
In conclusion, we have presented a work in pro-
gress that uses statistical language modeling meth-
ods to create a novel measure called Answer
Credibility for the purpose of Answer Validation.
Our results show performance increases in both
accuracy and MRR for &amp;quot;what&amp;quot; and &amp;quot;who&amp;quot; type
questions when Answer Credibility is included as a
part of the Answer Validation process. Our goals
for the future include further development of the
Answer Credibility model to include not only
terms from a Question Context, but terms that can
be deduced to be in an Answer Context.
</bodyText>
<sectionHeader confidence="0.9989" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999785816326531">
Banerjee, P., Han, H. 2 008. &amp;quot;Incorporation of Corpus-
Specific Semantic Information into Question Answering
Context,&amp;quot; CIKM 2 008 - Ontologies and Information
Systems for the Semantic Web Workshop, Napa Valley,
CA.
Banerjee, P., Han, H 2 009. &amp;quot;Modeling Semantic Ques-
tion Context for Question Answering,&amp;quot; To appear in
FLAIRS 2009.
Lafferty, J. and Zhai, C. 2 001. &amp;quot;Document language
models, query models, and risk minimization for infor-
mation retrieval,&amp;quot; in Proceedings of the 24th Annual
International ACM SIGIR, New Orleans, Louisiana: pp.
111-119.
Lavrenko, V. and Croft, W. B. 2 001. &amp;quot;Relevance based
language models,&amp;quot; Proceedings of the 24th annual inter-
national ACM SIGIR, pp. 12 0-127.
Magnini, B., Negri, M., Prevete, R. Tanev, H. 2 002.
&amp;quot;Is It the Right Answer? Exploiting Web Redundancy
for Answer Validation,&amp;quot; in Association for Computa-
tional Lingustistics (ACL) 2 002, Philadelphia, PA, pp.
425-432.
Metzger, M. 2 007. &amp;quot;Making sense of credibility on the
Web: Models for evaluating online information and
recommendations for future research,&amp;quot; Journal of the
American Society of Information Science and Technol-
ogy (JASIST), vol. 58, p. 2 078.
Murdock, V. 2 006. Exploring Sentence Retrieval.
VDM Verlag.
Ogilvie, P. and Callan, J. P. 2 001. &amp;quot;Experiments Using
the Lemur Toolkit,&amp;quot; in Online Proceedings of the 2 001
Text Retrieval Conference (TREC).
Peters, C. 2 008. &amp;quot;What happened in CLEF 2 008: In-
troduction to the Working Notes.&amp;quot; http://www.clef-
campaign.org/2008/working_notes.
Schlaefer, N., Gieselmann, P., Schaaf, T., &amp; A., W.
2 006. A Pattern Learning Approach to Question An-
swering within the Ephyra Framework, In Proceedings
of the Ninth International Conference on Text, Speech
and Dialogue (TSD).
Strohman, T., Metzler, D., Turtle, H., and Croft, W. B.
2 005. &amp;quot;Indri: A language model-based search engine for
complex queries,&amp;quot; International Conference on Intelli-
gence Analysis McLean, VA.
Voorhees, E. M. and Harman, D. K. 2 005. TREC: Ex-
periment and Evaluation in Information Retrieval (Digi-
tal Libraries and Electronic Publishing): The MIT Press.
Voorhees, E. M. 2 006. &amp;quot;Overview of the TREC 2 006
Question Answering Track,&amp;quot; in Online Proceedings of
2 006 Text Retrieval Conference (TREC).
</reference>
<page confidence="0.997472">
160
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.504088">
<title confidence="0.9912975">Answer Credibility: A Language Modeling to Answer Validation</title>
<author confidence="0.991823">Protima Banerjee Hyoil</author>
<affiliation confidence="0.8990415">College of Information Science and Drexel</affiliation>
<address confidence="0.992681">Philadelphia, PA 191</address>
<email confidence="0.966103">hyoil.han@acm.org</email>
<abstract confidence="0.969270090909091">Answer Validation is a topic of significant interest within the Question Answering community. In this paper, we propose the use of language modeling methodologies for Answer Validation, using corpus-based methods that do not require the use of external sources. Specifically, we propose a model for Answer Credibility which quantifies the reliability of a source document that contains a candidate answer and the Question&apos;s Context Model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>P Banerjee</author>
<author>H Han</author>
</authors>
<title>2 008. &amp;quot;Incorporation of CorpusSpecific Semantic Information into Question Answering Context,&amp;quot;</title>
<journal>CIKM</journal>
<volume>2</volume>
<pages>008</pages>
<location>Napa Valley, CA.</location>
<marker>Banerjee, Han, </marker>
<rawString>Banerjee, P., Han, H. 2 008. &amp;quot;Incorporation of CorpusSpecific Semantic Information into Question Answering Context,&amp;quot; CIKM 2 008 - Ontologies and Information Systems for the Semantic Web Workshop, Napa Valley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Banerjee</author>
<author>Han</author>
</authors>
<title>H 2 009. &amp;quot;Modeling Semantic Question Context for Question Answering,&amp;quot;</title>
<date>2009</date>
<note>To appear in FLAIRS</note>
<marker>Banerjee, Han, 2009</marker>
<rawString>Banerjee, P., Han, H 2 009. &amp;quot;Modeling Semantic Question Context for Question Answering,&amp;quot; To appear in FLAIRS 2009.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Lafferty</author>
<author>C Zhai</author>
</authors>
<title>2 001. &amp;quot;Document language models, query models, and risk minimization for information retrieval,&amp;quot;</title>
<booktitle>in Proceedings of the 24th Annual International ACM SIGIR,</booktitle>
<pages>111--119</pages>
<location>New Orleans, Louisiana:</location>
<marker>Lafferty, Zhai, </marker>
<rawString>Lafferty, J. and Zhai, C. 2 001. &amp;quot;Document language models, query models, and risk minimization for information retrieval,&amp;quot; in Proceedings of the 24th Annual International ACM SIGIR, New Orleans, Louisiana: pp. 111-119.</rawString>
</citation>
<citation valid="false">
<authors>
<author>V Lavrenko</author>
<author>W B Croft</author>
</authors>
<title>2 001. &amp;quot;Relevance based language models,&amp;quot;</title>
<booktitle>Proceedings of the 24th annual international ACM SIGIR,</booktitle>
<pages>12--0</pages>
<marker>Lavrenko, Croft, </marker>
<rawString>Lavrenko, V. and Croft, W. B. 2 001. &amp;quot;Relevance based language models,&amp;quot; Proceedings of the 24th annual international ACM SIGIR, pp. 12 0-127.</rawString>
</citation>
<citation valid="false">
<authors>
<author>B Magnini</author>
<author>M Negri</author>
<author>R Tanev Prevete</author>
<author>H</author>
</authors>
<title>2 002. &amp;quot;Is It the Right Answer? Exploiting Web Redundancy for Answer Validation,&amp;quot;</title>
<booktitle>in Association for Computational Lingustistics (ACL) 2 002,</booktitle>
<pages>425--432</pages>
<location>Philadelphia, PA,</location>
<marker>Magnini, Negri, Prevete, H, </marker>
<rawString>Magnini, B., Negri, M., Prevete, R. Tanev, H. 2 002. &amp;quot;Is It the Right Answer? Exploiting Web Redundancy for Answer Validation,&amp;quot; in Association for Computational Lingustistics (ACL) 2 002, Philadelphia, PA, pp. 425-432.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Metzger</author>
</authors>
<title>2 007. &amp;quot;Making sense of credibility on the Web: Models for evaluating online information and recommendations for future research,&amp;quot;</title>
<journal>Journal of the American Society of Information Science and Technology (JASIST),</journal>
<volume>58</volume>
<pages>2--078</pages>
<marker>Metzger, </marker>
<rawString>Metzger, M. 2 007. &amp;quot;Making sense of credibility on the Web: Models for evaluating online information and recommendations for future research,&amp;quot; Journal of the American Society of Information Science and Technology (JASIST), vol. 58, p. 2 078.</rawString>
</citation>
<citation valid="false">
<authors>
<author>V Murdock</author>
</authors>
<title>2 006. Exploring Sentence Retrieval.</title>
<publisher>VDM Verlag.</publisher>
<marker>Murdock, </marker>
<rawString>Murdock, V. 2 006. Exploring Sentence Retrieval. VDM Verlag.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Ogilvie</author>
<author>J P Callan</author>
</authors>
<title>2 001. &amp;quot;Experiments Using the Lemur Toolkit,&amp;quot;</title>
<booktitle>in Online Proceedings of the 2 001 Text Retrieval Conference (TREC).</booktitle>
<marker>Ogilvie, Callan, </marker>
<rawString>Ogilvie, P. and Callan, J. P. 2 001. &amp;quot;Experiments Using the Lemur Toolkit,&amp;quot; in Online Proceedings of the 2 001 Text Retrieval Conference (TREC).</rawString>
</citation>
<citation valid="false">
<authors>
<author>C Peters</author>
</authors>
<title>2 008. &amp;quot;What happened in CLEF 2 008: Introduction to the Working Notes.&amp;quot; http://www.clefcampaign.org/2008/working_notes.</title>
<marker>Peters, </marker>
<rawString>Peters, C. 2 008. &amp;quot;What happened in CLEF 2 008: Introduction to the Working Notes.&amp;quot; http://www.clefcampaign.org/2008/working_notes.</rawString>
</citation>
<citation valid="false">
<authors>
<author>N Schlaefer</author>
<author>P Gieselmann</author>
<author>T Schaaf</author>
<author>W A</author>
</authors>
<title>2 006. A Pattern Learning Approach to Question Answering within the Ephyra Framework,</title>
<booktitle>In Proceedings of the Ninth International Conference on Text, Speech and Dialogue (TSD).</booktitle>
<marker>Schlaefer, Gieselmann, Schaaf, A, </marker>
<rawString>Schlaefer, N., Gieselmann, P., Schaaf, T., &amp; A., W. 2 006. A Pattern Learning Approach to Question Answering within the Ephyra Framework, In Proceedings of the Ninth International Conference on Text, Speech and Dialogue (TSD).</rawString>
</citation>
<citation valid="false">
<authors>
<author>T Strohman</author>
<author>D Metzler</author>
<author>H Turtle</author>
<author>W B Croft</author>
</authors>
<title>2 005. &amp;quot;Indri: A language model-based search engine for complex queries,&amp;quot;</title>
<booktitle>International Conference on Intelligence Analysis</booktitle>
<location>McLean, VA.</location>
<marker>Strohman, Metzler, Turtle, Croft, </marker>
<rawString>Strohman, T., Metzler, D., Turtle, H., and Croft, W. B. 2 005. &amp;quot;Indri: A language model-based search engine for complex queries,&amp;quot; International Conference on Intelligence Analysis McLean, VA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E M Voorhees</author>
<author>D K Harman</author>
</authors>
<booktitle>005. TREC: Experiment and Evaluation in Information Retrieval (Digital Libraries and Electronic Publishing):</booktitle>
<volume>2</volume>
<publisher>The MIT Press.</publisher>
<marker>Voorhees, Harman, </marker>
<rawString>Voorhees, E. M. and Harman, D. K. 2 005. TREC: Experiment and Evaluation in Information Retrieval (Digital Libraries and Electronic Publishing): The MIT Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E M Voorhees</author>
</authors>
<title>2 006. &amp;quot;Overview of the TREC 2 006 Question Answering Track,&amp;quot;</title>
<booktitle>in Online Proceedings of 2 006 Text Retrieval Conference (TREC).</booktitle>
<marker>Voorhees, </marker>
<rawString>Voorhees, E. M. 2 006. &amp;quot;Overview of the TREC 2 006 Question Answering Track,&amp;quot; in Online Proceedings of 2 006 Text Retrieval Conference (TREC).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>