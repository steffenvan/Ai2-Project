<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.661093">
Machine Translation System Combination by Confusion Forest
</title>
<author confidence="0.632897">
Taro Watanabe and Eiichiro Sumita
</author>
<affiliation confidence="0.494307">
National Institute of Information and Communications Technology
3-5 Hikaridai, Keihanna Science City, 619-0289 JAPAN
</affiliation>
<email confidence="0.964293">
{taro.watanabe,eiichiro.sumita}@nict.go.jp
</email>
<sectionHeader confidence="0.983411" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999979761904762">
The state-of-the-art system combination
method for machine translation (MT) is
based on confusion networks constructed
by aligning hypotheses with regard to word
similarities. We introduce a novel system
combination framework in which hypotheses
are encoded as a confusion forest, a packed
forest representing alternative trees. The
forest is generated using syntactic consensus
among parsed hypotheses: First, MT outputs
are parsed. Second, a context free grammar is
learned by extracting a set of rules that con-
stitute the parse trees. Third, a packed forest
is generated starting from the root symbol of
the extracted grammar through non-terminal
rewriting. The new hypothesis is produced
by searching the best derivation in the forest.
Experimental results on the WMT10 system
combination shared task yield comparable
performance to the conventional confusion
network based method with smaller space.
</bodyText>
<sectionHeader confidence="0.992541" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.985983565217391">
System combination techniques take the advantages
of consensus among multiple systems and have been
widely used in fields, such as speech recognition
(Fiscus, 1997; Mangu et al., 2000) or parsing (Hen-
derson and Brill, 1999). One of the state-of-the-art
system combination methods for MT is based on
confusion networks, which are compact graph-based
structures representing multiple hypotheses (Banga-
lore et al., 2001).
Confusion networks are constructed based on
string similarity information. First, one skeleton or
backbone sentence is selected. Then, other hypothe-
ses are aligned against the skeleton, forming a lattice
with each arc representing alternative word candi-
dates. The alignment method is either model-based
(Matusov et al., 2006; He et al., 2008) in which a
statistical word aligner is used to compute hypothe-
sis alignment, or edit-based (Jayaraman and Lavie,
2005; Sim et al., 2007) in which alignment is mea-
sured by an evaluation metric, such as translation er-
ror rate (TER) (Snover et al., 2006). The new trans-
lation hypothesis is generated by selecting the best
path through the network.
We present a novel method for system combina-
tion which exploits the syntactic similarity of system
outputs. Instead of constructing a string-based con-
fusion network, we generate a packed forest (Billot
and Lang, 1989; Mi et al., 2008) which encodes ex-
ponentially many parse trees in a polynomial space.
The packed forest, or confusion forest, is constructed
by merging the MT outputs with regard to their
syntactic consensus. We employ a grammar-based
method to generate the confusion forest: First, sys-
tem outputs are parsed. Second, a set of rules are
extracted from the parse trees. Third, a packed for-
est is generated using a variant of Earley’s algorithm
(Earley, 1970) starting from the unique root symbol.
New hypotheses are selected by searching the best
derivation in the forest. The grammar, a set of rules,
is limited to those found in the parse trees. Spuri-
ous ambiguity during the generation step is further
reduced by encoding the tree local contextual infor-
mation in each non-terminal symbol, such as parent
and sibling labels, using the state representation in
Earley’s algorithm.
1249
</bodyText>
<note confidence="0.9947335">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1249–1257,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999953176470588">
Experiments were carried out for the system
combination task of the fifth workshop on sta-
tistical machine translation (WMT10) in four di-
rections, {Czech, French, German, Spanish}-to-
English (Callison-Burch et al., 2010), and we found
comparable performance to the conventional con-
fusion network based system combination in two
language pairs, and statistically significant improve-
ments in the others.
First, we will review the state-of-the-art method
which is a system combination framework based on
confusion networks (§2). Then, we will introduce
a novel system combination method based on con-
fusion forest (§3) and present related work in con-
sensus translations (§4). Experiments are presented
in Section 5 followed by discussion and our conclu-
sion.
</bodyText>
<sectionHeader confidence="0.748759" genericHeader="method">
2 Combination by Confusion Network
</sectionHeader>
<bodyText confidence="0.95635534375">
The system combination framework based on confu-
sion network starts from computing pairwise align-
ment between hypotheses by taking one hypothe-
sis as a reference. Matusov et al. (2006) employs
a model based approach in which a statistical word
aligner, such as GIZA++ (Och and Ney, 2003), is
used to align the hypotheses. Sim et al. (2007) in-
troduced TER (Snover et al., 2006) to measure the
edit-based alignment.
Then, one hypothesis is selected, for example by
employing a minimum Bayes risk criterion (Sim et
al., 2007), as a skeleton, or a backbone, which serves
as a building block for aligning the rest of the hy-
potheses. Other hypotheses are aligned against the
skeleton using the pairwise alignment. Figure 1(b)
illustrates an example of a confusion network con-
structed from the four hypotheses in Figure 1(a), as-
suming the first hypothesis is selected as our skele-
ton. The network consists of several arcs, each of
which represents an alternative word at that position,
including the empty symbol, ϵ.
This pairwise alignment strategy is prone to spu-
rious insertions and repetitions due to alignment er-
rors such as in Figure 1(a) in which “green” in the
third hypothesis is aligned with “forest” in the skele-
ton. Rosti et al. (2008) introduces an incremental
method so that hypotheses are aligned incremen-
tally to the growing confusion network, not only the
* I saw the forest
I walked the blue forest
I saw the green trees
the forest was found
</bodyText>
<figure confidence="0.99903325">
(a) Pairwise alignment using the first starred hypothesis as a
skeleton.
I saw blue forest trees found
the
ϵ
ϵ walked
(b) Confusion network from (a)
I saw blue forest was found
the
ϵ
ϵ walked
(c) Incrementally constructed confusion network
</figure>
<figureCaption confidence="0.8582205">
Figure 1: An example confusion network construc-
tion
</figureCaption>
<bodyText confidence="0.999722533333333">
skeleton hypothesis. In our example, “green trees”
is aligned with “blue forest” in Figure 1(c).
The confusion network construction is largely in-
fluenced by the skeleton selection, which determines
the global word reordering of a new hypothesis. For
example, the last hypothesis in Figure 1(a) has a pas-
sive voice grammatical construction while the others
are active voice. This large grammatical difference
may produce a longer sentence with spuriously in-
serted words, as in “I saw the blue trees was found”
in Figure 1(c). Rosti et al. (2007b) partially re-
solved the problem by constructing a large network
in which each hypothesis was treated as a skeleton
and the multiple networks were merged into a single
network.
</bodyText>
<sectionHeader confidence="0.886819" genericHeader="method">
3 Combination by Confusion Forest
</sectionHeader>
<bodyText confidence="0.999809375">
The confusion network approach to system com-
bination encodes multiple hypotheses into a com-
pact lattice structure by using word-level consensus.
Likewise, we propose to encode multiple hypothe-
ses into a confusion forest, which is a packed forest
which represents multiple parse trees in a polyno-
mial space (Billot and Lang, 1989; Mi et al., 2008)
Syntactic consensus is realized by sharing tree frag-
</bodyText>
<equation confidence="0.856290875">
ϵ green
ϵ
was
ϵ
green trees ϵ ϵ
1250
alization:
[TOP →
•S,0] : ¯1 Scan:
[X →α •
xβ, h] : u[ X → α x
•β , h] : uP re d i
ct:
[X →α •
Yβ , h ] [Y→ •γ
Gm p l
,h + 1] : u Y1→ γ ∈
ete:[X→α •
Yβ, h] : u [Y → γ•,h + 1] : v [X → αY
•β,h] : u ⊗ v
Goal:
gur
P@1
D
f
3 V
was
VBN
@P
Fi
V1
walke
PRP
T
th
re
VBD@
P@4
oun
dsawNP@2V2
</equation>
<page confidence="0.154367">
DT
</page>
<figure confidence="0.98670375">
the
NI
NN
efo
st
VBD@2d
blu
@2V
reen NN
fores
tVP@2S
JJ
2V2
f or es
eg
2V1 NN
the
DT@2V
t trees
[TOP →
</figure>
<bodyText confidence="0.985179931034483">
e2:Ane xa mp lepacke d fores trepre sentinghy-po the
sesin Fig ur e1(a). ments
among parse trees .The fo res tisrep re sentedasahy
pe r graphwhich isexp lo ited inpar si ng(Klei nandMa
nni ng,2001; Huang andCh ian g,2005) andma chi
netrans lation (Chia ng,2007; Huang andCh i-a ng,2
007) .More f
orma lly,ahype r graphisapa ir ⟨ V,E⟩ whe re
Visth e se tof nod es andE i sth e se tof hy- pe red
ges.Each n odei nV is re p re sented asX@d wh ere
X∈Nis a n o n- t erminalsymbo land pi san a
dd re ss(Shie beretal. ,1 995) that e ncap su-lates
each n odei drel at ivetoits pa ren t.Thero otn odei
sgiv en thead dre ssϵand t h ead dre ssofthe fir stc
hild ofnod ep is gi v en p.1.E ach h yper edgee∈Eis
r e p re sented asapa ir ⟨ head (e),tails (e)⟩where
head( e) ∈ V is a h e ad n odea ndta ils (e) ∈V ( is a
li st o ftai ln odes ,corre spondingtothe le ft-
handsidea ndth eri ght -handsideo f ani n- st anc
eofaru le i naCF G, r espe ctively.Figur e2pres e
ntsanexa mp lepacke dfores tfor th epa rse dhypot
hesesin Fig ur e1(a). Forex amp le,VP@2h astw
ohy per edges,⟨VP@2 ,VVBD@ 3,VP@4P ⟩and⟨ V P@2
,VVBD@ 2V1,NP@2V 2P⟩, lea di ngtodif fe rentderiv
ationswhere thefo rme rtakes thegr amm aticalconst
ructioninpas si ve voice while thela tte r inac- ti vev
oice .Given
syste moutpu ts,weemp lo ythefo llo winggramm
arbased appro achforco nst ructing aconf u -sionf
ores t:First ,MTout pu tsarepa rse d.Secon d,Initi
S•,0] Fi g ur
e3:The de duc tive syste m forEa rle y’sgener a-tiona
lgor ithmagram
m arislea rn edbytre at ing eachh yper edgeasani ns
ta nceofaCF Gr u le. Third ,afore s tisgen -e rate
dfromt heun iqu eroot s ymbo l ofthe ex tra ctedgramm
arthrou ghnon-t erminalrewri ting.3.1Fo
res tGener ationGiven
theex tra ctedgramm ar,weapp ly avari a ntofEar le
y’salgor ithm(Earl ey,1970) which cange ner -atest
rin gsinale ft - to-right manne rfromt heun iqu eroots
ymbo l,TOP. F igur e3 pres e nts thede duc tiveinfer
encerules (Good man,1999) for ou rge ner ationalgor
ithm.Weuse ca pit allette rsX∈Nto d e n ot enon-t
erminalsandx ∈ Tfo r t e rmi nals.Lower caseGreek
lette rsα,βan d γ a res t rin gsofter mi nalsandno n-t
erminals(T∪N) (. u and v a rew e igh tsasso- ciate
dwithe achi tem. Thema
jor diffe rencecompa redtoEar le y’spars- ingal
gor ithmistha t w eign or ethete rmi nalspani n-fo rma
tioneachn on-t erminalcover s andke ept rack ofthe
he igh tof der iv ationsby h. T he sc ann ingstepw
illa lway ssucce ed by mov in gthedo tto the ri ght
.Combi nedwitht hepr edi ctionand co mpl etionsteps
,our al gor ithmmaypo ten tiallygener ateaspu- r ious
lydeepf ores t.Thus, the he igh tof the fo res tiscon st
rainedinthe pr edi ction stepn otto exc ee dH,whi ch
isemp ir ically setto 1.5 ti mes thema xim umuimum
</bodyText>
<equation confidence="0.741760571428571">
S
P
PR
PPV
N
P
B
</equation>
<bodyText confidence="0.889165">
height of the parsed system outputs.
</bodyText>
<subsectionHeader confidence="0.998761">
3.2 Tree Annotation
</subsectionHeader>
<bodyText confidence="0.999990128205128">
The grammar compiled from the parsed trees is lo-
cal in that it can represent a finite number of sen-
tences translated from a specific input sentence. Al-
though its coverage is limited, our generation algo-
rithm may yield a spuriously large forest. As a way
to reduce spurious ambiguities, we relabel the non-
terminal symbols assigned to each parse tree before
extracting rules.
Here, we replace each non-terminal symbol by
the state representation of Earley’s algorithm corre-
sponding to the sequence of prediction steps starting
from TOP. Figure 4(a) presents an example parse
tree with each symbol replaced by the Earley’s state
in Figure 4(b). For example, the label for VBD is
replaced by •S + NP : •VP + •VBD : NP which
corresponds to the prediction steps of TOP → •S,
S → NP • VP and VP → •VBD NP. The context
represented in the Earley’s state is further limited by
the vertical and horizontal Markovization (Klein and
Manning, 2003). We define the vertical order v in
which the label is limited to memorize only v pre-
vious prediction steps. For instance, setting v = 1
yields NP : •VP + •VBD : NP in our example.
Likewise, we introduce the horizontal order h which
limits the number of sibling labels memorized on the
left and the right of the dotted label. Limiting h = 1
implies that each deductive step is encoded with at
most three symbols.
No limits in the horizontal and vertical
Markovization orders implies memorizing of
all the deductions and yields a confusion forest
representing the union of parse trees through the
grammar collection and the generation processes.
More relaxed horizontal orders allow more reorder-
ing of subtrees in a confusion forest by discarding
the sibling context in each prediction step. Like-
wise, constraining the vertical order generates a
deeper forest by ignoring the sequence of symbols
leading to a particular node.
</bodyText>
<subsectionHeader confidence="0.999667">
3.3 Forest Rescoring
</subsectionHeader>
<bodyText confidence="0.99997456">
From the packed forest F, new k-best derivations
are extracted from all possible derivations D by
efficient forest-based algorithms for k-best parsing
(Huang and Chiang, 2005). We use a linear combi-
proposed a phrasal combination by merging hy-
potheses in a chart structure, while others depended
on confusion networks, or similar structures, as a
building block for merging hypotheses at the word
level (Bangalore et al., 2001; Matusov et al., 2006;
He et al., 2008; Jayaraman and Lavie, 2005; Sim
et al., 2007). Our work is the first to explicitly ex-
ploit syntactic similarity for system combination by
merging hypotheses into a syntactic packed forest.
The confusion forest approach may suffer from pars-
ing errors such as the confusion network construc-
tion influenced by alignment errors. Even with pars-
ing errors, we can still take a tree fragment-level
consensus as long as a parser is consistent in that
similar syntactic mistakes would be made for simi-
lar hypotheses.
Rosti et al. (2007a) describe a re-generation ap-
proach to consensus translation in which a phrasal
translation table is constructed from the MT outputs
aligned with an input source sentence. New transla-
tions are generated by decoding the source sentence
again using the newly extracted phrase table. Our
grammar-based approach can be regarded as a re-
generation approach in which an off-the-shelf mono-
lingual parser, instead of a word aligner, is used to
annotate syntactic information to each hypothesis,
then, a new translation is generated from the merged
forest, not from the input source sentence through
decoding. In terms of generation, our approach is
an instance of statistical generation (Langkilde and
Knight, 1998; Langkilde, 2000). Instead of gener-
ating forests from semantic representations (Langk-
ilde, 2000), we generate forests from a CFG encod-
ing the consensus among parsed hypotheses.
Liu et al. (2009) present joint decoding in which
a translation forest is constructed from two distinct
MT systems, tree-to-string and string-to-string, by
merging forest outputs. Their merging method is ei-
ther translation-level in which no new translation is
generated, or derivation-level in that the rules shar-
ing the same left-hand-side are used in both sys-
tems. While our work is similar in that a new forest
is constructed by sharing rules among systems, al-
though their work involves no consensus translation
and requires structures internal to each system such
as model combinations (DeNero et al., 2010).
</bodyText>
<table confidence="0.998571666666667">
cz-en de-en es-en fr-en
# of systems 6 16 8 14
avg. words tune 10.6K 10.9K 10.9K 11.0K
test 50.5K 52.1K 52.1K 52.4K
sentences tune 455
test 2,034
</table>
<tableCaption confidence="0.869436">
Table 1: WMT10 system combination tuning/testing
data
</tableCaption>
<sectionHeader confidence="0.993176" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.9609">
5.1 Setup
</subsectionHeader>
<bodyText confidence="0.999984">
We ran our experiments for the WMT10 sys-
tem combination task usinge four language pairs,
{Czech, French, German, Spanish}-to-English
(Callison-Burch et al., 2010). The data is summa-
rized in Table 1. The system outputs are retok-
enized to match the Penn-treebank standard, parsed
by the Stanford Parser (Klein and Manning, 2003),
and lower-cased.
We implemented our confusion forest sys-
tem combination using an in-house developed
hypergraph-based toolkit cicada which is motivated
by generic weighted logic programming (Lopez,
2009), originally developed for a synchronous-CFG
based machine translation system (Chiang, 2007).
Input to our system is a collection of hypergraphs,
a set of parsed hypotheses, from which rules are ex-
tracted and a new forest is generated as described
in Section 3. Our baseline, also implemented in ci-
cada, is a confusion network-based system combi-
nation method (§2) which incrementally aligns hy-
potheses to the growing network using TER (Rosti
et al., 2008) and merges multiple networks into a
large single network. After performing epsilon re-
moval, the network is transformed into a forest by
parsing with monotone rules of S —* X, S —* S X
and X —* x. k-best translations are extracted from
the forest using the forest-based algorithms in Sec-
tion 3.3.
</bodyText>
<subsectionHeader confidence="0.976398">
5.2 Features
</subsectionHeader>
<bodyText confidence="0.970928">
The feature weight vector w in Equation 1 is tuned
by MERT over hypergraphs (Kumar et al., 2009).
We use three lower-cased 5-gram language mod-
1253
els hilm(d): English Gigaword Fourth edition1, the
English side of French-English 109 corpus and the
news commentary English data2. The count based
features ht(d) and he(d) count the number of ter-
minals and the number of hyperedges in d, respec-
tively. We employ M confidence measures hms (d)
for M systems, which basically count the number of
rules used in d originally extracted from mth system
hypothesis (Rosti et al., 2007a).
Following Macherey and Och (2007), BLEU (Pa-
pineni et al., 2002) correlations are also incorporated
in our system combination. Given M system outputs
e1...eM, M BLEU scores are computed for d using
each of the system outputs em as a reference
</bodyText>
<equation confidence="0.873524">
� 4
hmb (d) = BP(e, em) · exp 4 � log pn (e, em)
</equation>
<bodyText confidence="0.9991674">
where e = yield(d) is a terminal yield of d,
and
respectively denote brevity penalty and
n-gram precision. Here, we use approximated un-
clipped n-gram counts (Dreyer et al., 2007) for com-
puting
with a compact state representation (Li
and Khudanpur, 2009).
Our baseline confusion network system has an ad-
ditional penalty feature, hp
</bodyText>
<equation confidence="0.989692666666667">
BP(·)
pn(·)
pn(·)
</equation>
<bodyText confidence="0.9959728">
(m), which is the total
edits required to construct a confusion network us-
ing the mth system hypothesis as a skeleton, normal-
ized by the number of nodes in the network (Rosti et
al., 2007b).
</bodyText>
<subsectionHeader confidence="0.65904">
5.3 Results
</subsectionHeader>
<table confidence="0.992182657142857">
CFv=∞,h=∞
CFv=∞,h=2
CFv=∞,h=1
CFv=4,h=∞
CFv=4,h=2
CFv=4,h=1
CFv=3,h=∞ CFv=3,h=2 23.30 23.95 30.02 28.19
CFv=3,h=1 23.23 21.43 29.27 26.53
forest with different vertical (v) and hori
zontal (h)
Markovization order.
language cz-en de-en es-en fr-en
rerank 29.40 32.32 36.83 36.59
CN 38.52 34.97 47.65 46.37
CFv=∞,h=∞ language cz-en de-en es-en fr-en
CFv=∞,h=2 system min 14.09 15.62 21.79 16.79
CFv=∞,h=1 30.51 34.07 38.69 38.94
30.61 34.25 38.87 39.10
31.09 34.65 39.27 39.51
CFv=4,h=∞ max 23.44 24.10 29.97 29.17
CFv=4,h=2 CN 23.70 24.09 30.45 29.15
CFv=4,h=1 24.13 24.18 30.41 29.57
24.14 24.58 30.52 28.84
24.01 23.91 30.46 29.32
23.93 23.57 29.88 28.71
30.86 34.19 39.17 39.39
30.96 34.32 39.35 39.57
31.44 34.62 39.69 39.90
CFv=3,h=∞ 23.82 22.68 29.92 28.83
23.77 21.42 30.10 28.32
23.38 23.34 29.81 27.34
31.03 34.30 39.29 39.57
CFv=3,h=2 31.25 34.97 39.61 40.00
CFv=3,h=1 31.55 34.60 39.72 39.97
n=1
</table>
<tableCaption confidence="0.99786">
Table 3: Oracle lower-case BLEU
</tableCaption>
<equation confidence="0.926283">
h = 1, 2,
</equation>
<bodyText confidence="0.974832272727273">
with vertical orders of v = 3, 4,
Systems without statistically significant differences
from the best result (p &lt; 0.05) are indicated by bold
face. Setting v =
and h =
achieves compa-
rable performance to CN. Our best results in three
languages come from setting v =
and h = 2,
which favors little reordering of phrasal structures.
In general, lower horizontal and vert
</bodyText>
<figure confidence="0.768633615384615">
∞
∞.
∞
∞
∞
ical order leads
to lower BLEU.
catalog No. LDC2009T13
data are available fr
1LDC
2Those
om http://www.statmt.
org/wmt10/.
</figure>
<tableCaption confidence="0.842126166666667">
Table 2 compares our confusion forest approach
(CF) with different orders, a confusion network
(CN) and max/min systems measured by BLEU (Pa-
pineni et al., 2002). We vary the horizontal orders,
Table 2: Translation results in lower-case BLEU.
CN for confusion network and CF for confusion
</tableCaption>
<bodyText confidence="0.989594769230769">
Table 3 presents oracle BLEU achievable by each
combination method. The gains achievable by the
CF over simple reranking are small, at most 2-3
points, indicating that small variations are encoded
in confusion forests. We also observed that a lower
horizontal and vertical order leads to better BLEU
potentials. As briefly pointed out in Section 3.2,
the higher horizontal and vertical order implies more
faithfulness to the original parse trees. Introducing
new tree fragments to confusion forests leads to new
phrasal tran
slations with enlarged forests, as pre-
sented in Table 4, measured by the average number
</bodyText>
<table confidence="0.969373571428571">
1254
lang cz-en de-en es-en fr-en
CN 2,222.68 47,231.20 2,932.24 11,969.40
lattice 1,723.91 41,403.90 2,330.04 10,119.10
CFv=,,,, 230.08 540.03 262.30 386.79
CFv=4 254.45 651.10 302.01 477.51
CFv=3 286.01 802.79 349.21 575.17
</table>
<tableCaption confidence="0.988806">
Table 4: Hypegraph size measured by the average
</tableCaption>
<bodyText confidence="0.999699709677419">
number of hyperedges (h = 1 for CF). “lattice” is
the average number of edges in the original CN.
of hyperedges3. The larger potentials do not imply
better translations, probably due to the larger search
space with increased search errors. We also conjec-
ture that syntactic variations were not captured by
the n-gram like string-based features in Section 5.2,
therefore resulting in BLEU loss, which will be in-
vestigated in future work.
In contrast, CN has more potential for generat-
ing better translations, with the exception of the
German-to-English direction, with scores that are
usually 10 points better than simple sentence-wise
reranking. The low potential in German should be
interpreted in the light of the extremely large confu-
sion network in Table 4. We postulate that the di-
vergence in German hypotheses yields wrong align-
ments, and therefore amounts to larger networks
with incorrect hypotheses. Table 4 also shows that
CN produces a forest that is an order of magnitude
larger than those created by CFs. Although we can-
not directly relate the runtime and the number of
hyperedges in CN and CFs, since the shape of the
forests are different, CN requires more space to en-
code the hypotheses than those by CFs.
Table 5 compares the average length of the min-
imum/maximum hypothesis that each method can
produce. CN may generate shorter hypotheses,
whereby CF prefers longer hypotheses as we de-
crease the vertical order. Large divergence is also
observed for German, such as for hypergraph size.
</bodyText>
<sectionHeader confidence="0.990008" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999552">
We presented a confusion forest based method for
system combination in which system outputs are
merged into a packed forest using their syntactic
</bodyText>
<footnote confidence="0.5419705">
3We measure the hypergraph size before intersecting with
non-local features, like n-gram language models.
</footnote>
<table confidence="0.9997621">
language cz-en de-en es-en fr-en
system avg. 24.84 25.62 25.63 25.75
CN min 11.09 3.39 12.27 7.94
max 33.69 40.65 33.22 36.27
CFv=,,,, min 15.97 10.88 17.67 16.62
max 35.20 47.20 35.28 37.94
CFv=4 min 15.52 10.58 17.02 15.85
max 37.11 53.67 38.56 42.64
CFv=3 min 15.15 10.34 16.54 15.30
max 39.88 68.45 42.85 49.55
</table>
<tableCaption confidence="0.94862">
Table 5: Average min/max hypothesis length pro-
ducible by each method (h = 1 for CF).
</tableCaption>
<bodyText confidence="0.999287454545455">
similarity. The forest construction is treated as a
generation from a CFG compiled from the parsed
outputs. Our experiments indicate comparable per-
formance to a strong confusion network baseline
with smaller space, and statistically significant gains
in some language pairs.
To our knowledge, this is the first work to directly
introduce syntactic consensus to system combina-
tion by encoding multiple system outputs into a sin-
gle forest structure. We believe that the confusion
forest based approach to system combination has
future exploration potential. For instance, we did
not employ syntactic features in Section 5.2 which
would be helpful in discriminating hypotheses in
larger forests. We would also like to analyze the
trade-offs, if any, between parsing errors and confu-
sion forest constructions by controlling the parsing
qualities. As an alternative to the grammar-based
forest generation, we are investigating an edit dis-
tance measure for tree alignment, such as tree edit
distance (Bille, 2005) which basically computes in-
sertion/deletion/replacement of nodes in trees.
</bodyText>
<sectionHeader confidence="0.995469" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999837">
We would like to thank anonymous reviewers and
our colleagues for helpful comments and discussion.
</bodyText>
<sectionHeader confidence="0.982362" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996562768041238">
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proceedings
of Automatic Speech Recognition and Understanding
(ASRU), 2001, pages 351 – 354.
1255
Philip Bille. 2005. A survey on tree edit distance and
related problems. Theor. Comput. Sci., 337:217–239,
June.
Sylvie Billot and Bernard Lang. 1989. The structure
of shared forests in ambiguous parsing. In Proceed-
ings of the 27th Annual Meeting of the Association for
Computational Linguistics, pages 143–151, Vancou-
ver, British Columbia, Canada, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17–53, Uppsala, Sweden, July. Revised August
2010.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
John DeNero, Shankar Kumar, Ciprian Chelba, and Franz
Och. 2010. Model combination for machine trans-
lation. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
975–983, Los Angeles, California, June.
Markus Dreyer, Keith Hall, and Sanjeev Khudanpur.
2007. Comparing reordering constraints for smt us-
ing efficient bleu oracle computation. In Proceedings
of SSST, NAACL-HLT 2007 / AMTA Workshop on Syn-
tax and Structure in Statistical Translation, pages 103–
110, Rochester, New York, April.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the Association for Com-
puting Machinery, 13:94–102, February.
J.G. Fiscus. 1997. A post-processing system to yield re-
duced word error rates: Recognizer output voting error
reduction (rover). In Proceedings ofAutomatic Speech
Recognition and Understanding (ASRU), 1997, pages
347 –354, December.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proceedings of the fourth
conference on Applied natural language processing,
pages 95–100, Morristown, NJ, USA.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25:573–605, December.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-HMM-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 98–107, Honolulu, Hawaii,
October.
John C. Henderson and Eric Brill. 1999. Exploiting
diversity in natural language processing: Combining
parsers. In Proceedings of the Fourth Conference on
Empirical Methods in Natural Language Processing,
pages 187–194.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technology, pages 53–64, Van-
couver, British Columbia, October.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 144–151,
Prague, Czech Republic, June.
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word
matching. In Proceedings of the ACL 2005 on In-
teractive poster and demonstration sessions, ACL ’05,
pages 101–104, Morristown, NJ, USA.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of the Seventh In-
ternational Workshop on Parsing Technologies (IWPT-
2001), pages 123–134.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423–430, Sapporo, Japan, July.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 163–171, Sun-
tec, Singapore, August.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics and 17th In-
ternational Conference on Computational Linguistics
- Volume 1, ACL-36, pages 704–710, Morristown, NJ,
USA.
Irene Langkilde. 2000. Forest-based statistical sentence
generation. In Proceedings of the 1st North American
chapter of the Association for Computational Linguis-
tics conference, pages 170–177, San Francisco, CA,
USA.
Zhifei Li and Sanjeev Khudanpur. 2009. Efficient extrac-
tion of oracle-best translations from hypergraphs. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Companion Volume: Short Papers, pages 9–12, Boul-
der, Colorado, June.
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009.
Joint decoding with multiple translation models. In
1256
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 576–584, Suntec, Singapore, Au-
gust.
Adam Lopez. 2009. Translation as weighted deduction.
In Proceedings of the 12th Conference of the Euro-
pean Chapter of the ACL (EACL 2009), pages 532–
540, Athens, Greece, March.
Wolfgang Macherey and Franz J. Och. 2007. An empir-
ical study on computing consensus translations from
multiple machine translation systems. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
986–995, Prague, Czech Republic, June.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: word error
minimization and other applications of confusion net-
works. Computer Speech &amp; Language, 14(4):373 –
400.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Proceedings of the 11th Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 33–40.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT,
pages 192–199, Columbus, Ohio, June.
Tadashi Nomoto. 2004. Multi-engine machine transla-
tion with voted language model. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL’04), Main Volume, pages 494–501,
Barcelona, Spain, July.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311–318, Philadelphia, Pennsylva-
nia, USA, July.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie Dorr.
2007a. Combining outputs from multiple machine
translation systems. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference, pages 228–
235, Rochester, New York, April.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007b. Improved word-level system com-
bination for machine translation. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 312–319, Prague, Czech
Republic, June.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and
Richard Schwartz. 2008. Incremental hypothesis
alignment for building confusion networks with appli-
cation to machine translation system combination. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 183–186, Columbus, Ohio,
June.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24(1–2):3–36, July–August.
K.C. Sim, W.J. Byrne, M.J.F. Gales, H. Sahbi, and P.C.
Woodland. 2007. Consensus network decoding for
statistical machine translation system combination. In
Proceedings of Acoustics, Speech and Signal Process-
ing (ICASSP), 2007, volume 4, pages IV–105 –IV–
108, April.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In In
Proceedings ofAssociation for Machine Translation in
the Americas, pages 223–231.
</reference>
<page confidence="0.758325">
1257
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.763550">
<title confidence="0.992304">Machine Translation System Combination by Confusion Forest</title>
<author confidence="0.81102">Watanabe</author>
<affiliation confidence="0.961158">National Institute of Information and Communications</affiliation>
<address confidence="0.973635">3-5 Hikaridai, Keihanna Science City, 619-0289</address>
<email confidence="0.980591">taro.watanabe@nict.go.jp</email>
<email confidence="0.980591">eiichiro.sumita@nict.go.jp</email>
<abstract confidence="0.998958045454545">The state-of-the-art system combination method for machine translation (MT) is based on confusion networks constructed by aligning hypotheses with regard to word similarities. We introduce a novel system combination framework in which hypotheses are encoded as a confusion forest, a packed forest representing alternative trees. The forest is generated using syntactic consensus among parsed hypotheses: First, MT outputs are parsed. Second, a context free grammar is learned by extracting a set of rules that constitute the parse trees. Third, a packed forest is generated starting from the root symbol of the extracted grammar through non-terminal rewriting. The new hypothesis is produced by searching the best derivation in the forest. Experimental results on the WMT10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>German Bordel</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems.</title>
<date>2001</date>
<booktitle>In Proceedings of Automatic Speech Recognition and Understanding (ASRU),</booktitle>
<pages>351--354</pages>
<contexts>
<context position="1599" citStr="Bangalore et al., 2001" startWordPosition="219" endWordPosition="223">derivation in the forest. Experimental results on the WMT10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space. 1 Introduction System combination techniques take the advantages of consensus among multiple systems and have been widely used in fields, such as speech recognition (Fiscus, 1997; Mangu et al., 2000) or parsing (Henderson and Brill, 1999). One of the state-of-the-art system combination methods for MT is based on confusion networks, which are compact graph-based structures representing multiple hypotheses (Bangalore et al., 2001). Confusion networks are constructed based on string similarity information. First, one skeleton or backbone sentence is selected. Then, other hypotheses are aligned against the skeleton, forming a lattice with each arc representing alternative word candidates. The alignment method is either model-based (Matusov et al., 2006; He et al., 2008) in which a statistical word aligner is used to compute hypothesis alignment, or edit-based (Jayaraman and Lavie, 2005; Sim et al., 2007) in which alignment is measured by an evaluation metric, such as translation error rate (TER) (Snover et al., 2006). Th</context>
<context position="12798" citStr="Bangalore et al., 2001" startWordPosition="2189" endWordPosition="2192"> sibling context in each prediction step. Likewise, constraining the vertical order generates a deeper forest by ignoring the sequence of symbols leading to a particular node. 3.3 Forest Rescoring From the packed forest F, new k-best derivations are extracted from all possible derivations D by efficient forest-based algorithms for k-best parsing (Huang and Chiang, 2005). We use a linear combiproposed a phrasal combination by merging hypotheses in a chart structure, while others depended on confusion networks, or similar structures, as a building block for merging hypotheses at the word level (Bangalore et al., 2001; Matusov et al., 2006; He et al., 2008; Jayaraman and Lavie, 2005; Sim et al., 2007). Our work is the first to explicitly exploit syntactic similarity for system combination by merging hypotheses into a syntactic packed forest. The confusion forest approach may suffer from parsing errors such as the confusion network construction influenced by alignment errors. Even with parsing errors, we can still take a tree fragment-level consensus as long as a parser is consistent in that similar syntactic mistakes would be made for similar hypotheses. Rosti et al. (2007a) describe a re-generation approa</context>
</contexts>
<marker>Bangalore, Bordel, Riccardi, 2001</marker>
<rawString>Srinivas Bangalore, German Bordel, and Giuseppe Riccardi. 2001. Computing consensus translation from multiple machine translation systems. In Proceedings of Automatic Speech Recognition and Understanding (ASRU), 2001, pages 351 – 354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Bille</author>
</authors>
<title>A survey on tree edit distance and related problems.</title>
<date>2005</date>
<journal>Theor. Comput. Sci.,</journal>
<pages>337--217</pages>
<marker>Bille, 2005</marker>
<rawString>Philip Bille. 2005. A survey on tree edit distance and related problems. Theor. Comput. Sci., 337:217–239, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvie Billot</author>
<author>Bernard Lang</author>
</authors>
<title>The structure of shared forests in ambiguous parsing.</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>143--151</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="2505" citStr="Billot and Lang, 1989" startWordPosition="364" endWordPosition="367"> either model-based (Matusov et al., 2006; He et al., 2008) in which a statistical word aligner is used to compute hypothesis alignment, or edit-based (Jayaraman and Lavie, 2005; Sim et al., 2007) in which alignment is measured by an evaluation metric, such as translation error rate (TER) (Snover et al., 2006). The new translation hypothesis is generated by selecting the best path through the network. We present a novel method for system combination which exploits the syntactic similarity of system outputs. Instead of constructing a string-based confusion network, we generate a packed forest (Billot and Lang, 1989; Mi et al., 2008) which encodes exponentially many parse trees in a polynomial space. The packed forest, or confusion forest, is constructed by merging the MT outputs with regard to their syntactic consensus. We employ a grammar-based method to generate the confusion forest: First, system outputs are parsed. Second, a set of rules are extracted from the parse trees. Third, a packed forest is generated using a variant of Earley’s algorithm (Earley, 1970) starting from the unique root symbol. New hypotheses are selected by searching the best derivation in the forest. The grammar, a set of rules</context>
<context position="7195" citStr="Billot and Lang, 1989" startWordPosition="1121" endWordPosition="1124"> words, as in “I saw the blue trees was found” in Figure 1(c). Rosti et al. (2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network. 3 Combination by Confusion Forest The confusion network approach to system combination encodes multiple hypotheses into a compact lattice structure by using word-level consensus. Likewise, we propose to encode multiple hypotheses into a confusion forest, which is a packed forest which represents multiple parse trees in a polynomial space (Billot and Lang, 1989; Mi et al., 2008) Syntactic consensus is realized by sharing tree fragϵ green ϵ was ϵ green trees ϵ ϵ 1250 alization: [TOP → •S,0] : ¯1 Scan: [X →α • xβ, h] : u[ X → α x •β , h] : uP re d i ct: [X →α • Yβ , h ] [Y→ •γ Gm p l ,h + 1] : u Y1→ γ ∈ ete:[X→α • Yβ, h] : u [Y → γ•,h + 1] : v [X → αY •β,h] : u ⊗ v Goal: gur P@1 D f 3 V was VBN @P Fi V1 walke PRP T th re VBD@ P@4 oun dsawNP@2V2 DT the NI NN efo st VBD@2d blu @2V reen NN fores tVP@2S JJ 2V2 f or es eg 2V1 NN the DT@2V t trees [TOP → e2:Ane xa mp lepacke d fores trepre sentinghy-po the sesin Fig ur e1(a). ments among parse trees .The fo</context>
</contexts>
<marker>Billot, Lang, 1989</marker>
<rawString>Sylvie Billot and Bernard Lang. 1989. The structure of shared forests in ambiguous parsing. In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, pages 143–151, Vancouver, British Columbia, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Kay Peterson</author>
<author>Mark Przybocki</author>
<author>Omar Zaidan</author>
</authors>
<title>Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>17--53</pages>
<publisher>Revised</publisher>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="3800" citStr="Callison-Burch et al., 2010" startWordPosition="565" endWordPosition="568">during the generation step is further reduced by encoding the tree local contextual information in each non-terminal symbol, such as parent and sibling labels, using the state representation in Earley’s algorithm. 1249 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1249–1257, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Experiments were carried out for the system combination task of the fifth workshop on statistical machine translation (WMT10) in four directions, {Czech, French, German, Spanish}-toEnglish (Callison-Burch et al., 2010), and we found comparable performance to the conventional confusion network based system combination in two language pairs, and statistically significant improvements in the others. First, we will review the state-of-the-art method which is a system combination framework based on confusion networks (§2). Then, we will introduce a novel system combination method based on confusion forest (§3) and present related work in consensus translations (§4). Experiments are presented in Section 5 followed by discussion and our conclusion. 2 Combination by Confusion Network The system combination framewor</context>
<context position="15247" citStr="Callison-Burch et al., 2010" startWordPosition="2578" endWordPosition="2581">e our work is similar in that a new forest is constructed by sharing rules among systems, although their work involves no consensus translation and requires structures internal to each system such as model combinations (DeNero et al., 2010). cz-en de-en es-en fr-en # of systems 6 16 8 14 avg. words tune 10.6K 10.9K 10.9K 11.0K test 50.5K 52.1K 52.1K 52.4K sentences tune 455 test 2,034 Table 1: WMT10 system combination tuning/testing data 5 Experiments 5.1 Setup We ran our experiments for the WMT10 system combination task usinge four language pairs, {Czech, French, German, Spanish}-to-English (Callison-Burch et al., 2010). The data is summarized in Table 1. The system outputs are retokenized to match the Penn-treebank standard, parsed by the Stanford Parser (Klein and Manning, 2003), and lower-cased. We implemented our confusion forest system combination using an in-house developed hypergraph-based toolkit cicada which is motivated by generic weighted logic programming (Lopez, 2009), originally developed for a synchronous-CFG based machine translation system (Chiang, 2007). Input to our system is a collection of hypergraphs, a set of parsed hypotheses, from which rules are extracted and a new forest is generat</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Peterson, Przybocki, Zaidan, 2010</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Peterson, Mark Przybocki, and Omar Zaidan. 2010. Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 17–53, Uppsala, Sweden, July. Revised August 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="15707" citStr="Chiang, 2007" startWordPosition="2646" endWordPosition="2647">r experiments for the WMT10 system combination task usinge four language pairs, {Czech, French, German, Spanish}-to-English (Callison-Burch et al., 2010). The data is summarized in Table 1. The system outputs are retokenized to match the Penn-treebank standard, parsed by the Stanford Parser (Klein and Manning, 2003), and lower-cased. We implemented our confusion forest system combination using an in-house developed hypergraph-based toolkit cicada which is motivated by generic weighted logic programming (Lopez, 2009), originally developed for a synchronous-CFG based machine translation system (Chiang, 2007). Input to our system is a collection of hypergraphs, a set of parsed hypotheses, from which rules are extracted and a new forest is generated as described in Section 3. Our baseline, also implemented in cicada, is a confusion network-based system combination method (§2) which incrementally aligns hypotheses to the growing network using TER (Rosti et al., 2008) and merges multiple networks into a large single network. After performing epsilon removal, the network is transformed into a forest by parsing with monotone rules of S —* X, S —* S X and X —* x. k-best translations are extracted from t</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Shankar Kumar</author>
<author>Ciprian Chelba</author>
<author>Franz Och</author>
</authors>
<title>Model combination for machine translation. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>975--983</pages>
<location>Los Angeles, California,</location>
<contexts>
<context position="14859" citStr="DeNero et al., 2010" startWordPosition="2516" endWordPosition="2519">otheses. Liu et al. (2009) present joint decoding in which a translation forest is constructed from two distinct MT systems, tree-to-string and string-to-string, by merging forest outputs. Their merging method is either translation-level in which no new translation is generated, or derivation-level in that the rules sharing the same left-hand-side are used in both systems. While our work is similar in that a new forest is constructed by sharing rules among systems, although their work involves no consensus translation and requires structures internal to each system such as model combinations (DeNero et al., 2010). cz-en de-en es-en fr-en # of systems 6 16 8 14 avg. words tune 10.6K 10.9K 10.9K 11.0K test 50.5K 52.1K 52.1K 52.4K sentences tune 455 test 2,034 Table 1: WMT10 system combination tuning/testing data 5 Experiments 5.1 Setup We ran our experiments for the WMT10 system combination task usinge four language pairs, {Czech, French, German, Spanish}-to-English (Callison-Burch et al., 2010). The data is summarized in Table 1. The system outputs are retokenized to match the Penn-treebank standard, parsed by the Stanford Parser (Klein and Manning, 2003), and lower-cased. We implemented our confusion </context>
</contexts>
<marker>DeNero, Kumar, Chelba, Och, 2010</marker>
<rawString>John DeNero, Shankar Kumar, Ciprian Chelba, and Franz Och. 2010. Model combination for machine translation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 975–983, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Keith Hall</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Comparing reordering constraints for smt using efficient bleu oracle computation.</title>
<date>2007</date>
<booktitle>In Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>103--110</pages>
<location>Rochester, New York,</location>
<contexts>
<context position="17426" citStr="Dreyer et al., 2007" startWordPosition="2944" endWordPosition="2947">ures hms (d) for M systems, which basically count the number of rules used in d originally extracted from mth system hypothesis (Rosti et al., 2007a). Following Macherey and Och (2007), BLEU (Papineni et al., 2002) correlations are also incorporated in our system combination. Given M system outputs e1...eM, M BLEU scores are computed for d using each of the system outputs em as a reference � 4 hmb (d) = BP(e, em) · exp 4 � log pn (e, em) where e = yield(d) is a terminal yield of d, and respectively denote brevity penalty and n-gram precision. Here, we use approximated unclipped n-gram counts (Dreyer et al., 2007) for computing with a compact state representation (Li and Khudanpur, 2009). Our baseline confusion network system has an additional penalty feature, hp BP(·) pn(·) pn(·) (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b). 5.3 Results CFv=∞,h=∞ CFv=∞,h=2 CFv=∞,h=1 CFv=4,h=∞ CFv=4,h=2 CFv=4,h=1 CFv=3,h=∞ CFv=3,h=2 23.30 23.95 30.02 28.19 CFv=3,h=1 23.23 21.43 29.27 26.53 forest with different vertical (v) and hori zontal (h) Markovization order. language cz-</context>
</contexts>
<marker>Dreyer, Hall, Khudanpur, 2007</marker>
<rawString>Markus Dreyer, Keith Hall, and Sanjeev Khudanpur. 2007. Comparing reordering constraints for smt using efficient bleu oracle computation. In Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 103– 110, Rochester, New York, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the Association for Computing Machinery,</journal>
<pages>13--94</pages>
<contexts>
<context position="2963" citStr="Earley, 1970" startWordPosition="442" endWordPosition="443">ts the syntactic similarity of system outputs. Instead of constructing a string-based confusion network, we generate a packed forest (Billot and Lang, 1989; Mi et al., 2008) which encodes exponentially many parse trees in a polynomial space. The packed forest, or confusion forest, is constructed by merging the MT outputs with regard to their syntactic consensus. We employ a grammar-based method to generate the confusion forest: First, system outputs are parsed. Second, a set of rules are extracted from the parse trees. Third, a packed forest is generated using a variant of Earley’s algorithm (Earley, 1970) starting from the unique root symbol. New hypotheses are selected by searching the best derivation in the forest. The grammar, a set of rules, is limited to those found in the parse trees. Spurious ambiguity during the generation step is further reduced by encoding the tree local contextual information in each non-terminal symbol, such as parent and sibling labels, using the state representation in Earley’s algorithm. 1249 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1249–1257, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computa</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Jay Earley. 1970. An efficient context-free parsing algorithm. Communications of the Association for Computing Machinery, 13:94–102, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Fiscus</author>
</authors>
<title>A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (rover).</title>
<date>1997</date>
<booktitle>In Proceedings ofAutomatic Speech Recognition and Understanding (ASRU),</booktitle>
<pages>347--354</pages>
<contexts>
<context position="1345" citStr="Fiscus, 1997" startWordPosition="184" endWordPosition="185">rned by extracting a set of rules that constitute the parse trees. Third, a packed forest is generated starting from the root symbol of the extracted grammar through non-terminal rewriting. The new hypothesis is produced by searching the best derivation in the forest. Experimental results on the WMT10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space. 1 Introduction System combination techniques take the advantages of consensus among multiple systems and have been widely used in fields, such as speech recognition (Fiscus, 1997; Mangu et al., 2000) or parsing (Henderson and Brill, 1999). One of the state-of-the-art system combination methods for MT is based on confusion networks, which are compact graph-based structures representing multiple hypotheses (Bangalore et al., 2001). Confusion networks are constructed based on string similarity information. First, one skeleton or backbone sentence is selected. Then, other hypotheses are aligned against the skeleton, forming a lattice with each arc representing alternative word candidates. The alignment method is either model-based (Matusov et al., 2006; He et al., 2008) i</context>
</contexts>
<marker>Fiscus, 1997</marker>
<rawString>J.G. Fiscus. 1997. A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (rover). In Proceedings ofAutomatic Speech Recognition and Understanding (ASRU), 1997, pages 347 –354, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Frederking</author>
<author>Sergei Nirenburg</author>
</authors>
<title>Three heads are better than one.</title>
<date>1994</date>
<booktitle>In Proceedings of the fourth conference on Applied natural language processing,</booktitle>
<pages>95--100</pages>
<location>Morristown, NJ, USA.</location>
<marker>Frederking, Nirenburg, 1994</marker>
<rawString>Robert Frederking and Sergei Nirenburg. 1994. Three heads are better than one. In Proceedings of the fourth conference on Applied natural language processing, pages 95–100, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Semiring parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<pages>25--573</pages>
<marker>Goodman, 1999</marker>
<rawString>Joshua Goodman. 1999. Semiring parsing. Computational Linguistics, 25:573–605, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Mei Yang</author>
<author>Jianfeng Gao</author>
<author>Patrick Nguyen</author>
<author>Robert Moore</author>
</authors>
<title>Indirect-HMM-based hypothesis alignment for combining outputs from machine translation systems.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>98--107</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="1943" citStr="He et al., 2008" startWordPosition="271" endWordPosition="274">tion (Fiscus, 1997; Mangu et al., 2000) or parsing (Henderson and Brill, 1999). One of the state-of-the-art system combination methods for MT is based on confusion networks, which are compact graph-based structures representing multiple hypotheses (Bangalore et al., 2001). Confusion networks are constructed based on string similarity information. First, one skeleton or backbone sentence is selected. Then, other hypotheses are aligned against the skeleton, forming a lattice with each arc representing alternative word candidates. The alignment method is either model-based (Matusov et al., 2006; He et al., 2008) in which a statistical word aligner is used to compute hypothesis alignment, or edit-based (Jayaraman and Lavie, 2005; Sim et al., 2007) in which alignment is measured by an evaluation metric, such as translation error rate (TER) (Snover et al., 2006). The new translation hypothesis is generated by selecting the best path through the network. We present a novel method for system combination which exploits the syntactic similarity of system outputs. Instead of constructing a string-based confusion network, we generate a packed forest (Billot and Lang, 1989; Mi et al., 2008) which encodes expon</context>
<context position="12837" citStr="He et al., 2008" startWordPosition="2197" endWordPosition="2200">wise, constraining the vertical order generates a deeper forest by ignoring the sequence of symbols leading to a particular node. 3.3 Forest Rescoring From the packed forest F, new k-best derivations are extracted from all possible derivations D by efficient forest-based algorithms for k-best parsing (Huang and Chiang, 2005). We use a linear combiproposed a phrasal combination by merging hypotheses in a chart structure, while others depended on confusion networks, or similar structures, as a building block for merging hypotheses at the word level (Bangalore et al., 2001; Matusov et al., 2006; He et al., 2008; Jayaraman and Lavie, 2005; Sim et al., 2007). Our work is the first to explicitly exploit syntactic similarity for system combination by merging hypotheses into a syntactic packed forest. The confusion forest approach may suffer from parsing errors such as the confusion network construction influenced by alignment errors. Even with parsing errors, we can still take a tree fragment-level consensus as long as a parser is consistent in that similar syntactic mistakes would be made for similar hypotheses. Rosti et al. (2007a) describe a re-generation approach to consensus translation in which a </context>
</contexts>
<marker>He, Yang, Gao, Nguyen, Moore, 2008</marker>
<rawString>Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen, and Robert Moore. 2008. Indirect-HMM-based hypothesis alignment for combining outputs from machine translation systems. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 98–107, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Henderson</author>
<author>Eric Brill</author>
</authors>
<title>Exploiting diversity in natural language processing: Combining parsers.</title>
<date>1999</date>
<booktitle>In Proceedings of the Fourth Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>187--194</pages>
<contexts>
<context position="1405" citStr="Henderson and Brill, 1999" startWordPosition="192" endWordPosition="196">ute the parse trees. Third, a packed forest is generated starting from the root symbol of the extracted grammar through non-terminal rewriting. The new hypothesis is produced by searching the best derivation in the forest. Experimental results on the WMT10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space. 1 Introduction System combination techniques take the advantages of consensus among multiple systems and have been widely used in fields, such as speech recognition (Fiscus, 1997; Mangu et al., 2000) or parsing (Henderson and Brill, 1999). One of the state-of-the-art system combination methods for MT is based on confusion networks, which are compact graph-based structures representing multiple hypotheses (Bangalore et al., 2001). Confusion networks are constructed based on string similarity information. First, one skeleton or backbone sentence is selected. Then, other hypotheses are aligned against the skeleton, forming a lattice with each arc representing alternative word candidates. The alignment method is either model-based (Matusov et al., 2006; He et al., 2008) in which a statistical word aligner is used to compute hypoth</context>
</contexts>
<marker>Henderson, Brill, 1999</marker>
<rawString>John C. Henderson and Eric Brill. 1999. Exploiting diversity in natural language processing: Combining parsers. In Proceedings of the Fourth Conference on Empirical Methods in Natural Language Processing, pages 187–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technology,</booktitle>
<pages>53--64</pages>
<location>Vancouver, British Columbia,</location>
<contexts>
<context position="12548" citStr="Huang and Chiang, 2005" startWordPosition="2148" endWordPosition="2151">ll the deductions and yields a confusion forest representing the union of parse trees through the grammar collection and the generation processes. More relaxed horizontal orders allow more reordering of subtrees in a confusion forest by discarding the sibling context in each prediction step. Likewise, constraining the vertical order generates a deeper forest by ignoring the sequence of symbols leading to a particular node. 3.3 Forest Rescoring From the packed forest F, new k-best derivations are extracted from all possible derivations D by efficient forest-based algorithms for k-best parsing (Huang and Chiang, 2005). We use a linear combiproposed a phrasal combination by merging hypotheses in a chart structure, while others depended on confusion networks, or similar structures, as a building block for merging hypotheses at the word level (Bangalore et al., 2001; Matusov et al., 2006; He et al., 2008; Jayaraman and Lavie, 2005; Sim et al., 2007). Our work is the first to explicitly exploit syntactic similarity for system combination by merging hypotheses into a syntactic packed forest. The confusion forest approach may suffer from parsing errors such as the confusion network construction influenced by ali</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of the Ninth International Workshop on Parsing Technology, pages 53–64, Vancouver, British Columbia, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>144--151</pages>
<location>Prague, Czech Republic,</location>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144–151, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shyamsundar Jayaraman</author>
<author>Alon Lavie</author>
</authors>
<title>Multiengine machine translation guided by explicit word matching.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL 2005 on Interactive poster and demonstration sessions, ACL ’05,</booktitle>
<pages>101--104</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2061" citStr="Jayaraman and Lavie, 2005" startWordPosition="290" endWordPosition="293">ystem combination methods for MT is based on confusion networks, which are compact graph-based structures representing multiple hypotheses (Bangalore et al., 2001). Confusion networks are constructed based on string similarity information. First, one skeleton or backbone sentence is selected. Then, other hypotheses are aligned against the skeleton, forming a lattice with each arc representing alternative word candidates. The alignment method is either model-based (Matusov et al., 2006; He et al., 2008) in which a statistical word aligner is used to compute hypothesis alignment, or edit-based (Jayaraman and Lavie, 2005; Sim et al., 2007) in which alignment is measured by an evaluation metric, such as translation error rate (TER) (Snover et al., 2006). The new translation hypothesis is generated by selecting the best path through the network. We present a novel method for system combination which exploits the syntactic similarity of system outputs. Instead of constructing a string-based confusion network, we generate a packed forest (Billot and Lang, 1989; Mi et al., 2008) which encodes exponentially many parse trees in a polynomial space. The packed forest, or confusion forest, is constructed by merging the</context>
<context position="12864" citStr="Jayaraman and Lavie, 2005" startWordPosition="2201" endWordPosition="2204">g the vertical order generates a deeper forest by ignoring the sequence of symbols leading to a particular node. 3.3 Forest Rescoring From the packed forest F, new k-best derivations are extracted from all possible derivations D by efficient forest-based algorithms for k-best parsing (Huang and Chiang, 2005). We use a linear combiproposed a phrasal combination by merging hypotheses in a chart structure, while others depended on confusion networks, or similar structures, as a building block for merging hypotheses at the word level (Bangalore et al., 2001; Matusov et al., 2006; He et al., 2008; Jayaraman and Lavie, 2005; Sim et al., 2007). Our work is the first to explicitly exploit syntactic similarity for system combination by merging hypotheses into a syntactic packed forest. The confusion forest approach may suffer from parsing errors such as the confusion network construction influenced by alignment errors. Even with parsing errors, we can still take a tree fragment-level consensus as long as a parser is consistent in that similar syntactic mistakes would be made for similar hypotheses. Rosti et al. (2007a) describe a re-generation approach to consensus translation in which a phrasal translation table i</context>
</contexts>
<marker>Jayaraman, Lavie, 2005</marker>
<rawString>Shyamsundar Jayaraman and Alon Lavie. 2005. Multiengine machine translation guided by explicit word matching. In Proceedings of the ACL 2005 on Interactive poster and demonstration sessions, ACL ’05, pages 101–104, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing and hypergraphs.</title>
<date>2001</date>
<booktitle>In Proceedings of the Seventh International Workshop on Parsing Technologies (IWPT2001),</booktitle>
<pages>123--134</pages>
<marker>Klein, Manning, 2001</marker>
<rawString>Dan Klein and Christopher D. Manning. 2001. Parsing and hypergraphs. In Proceedings of the Seventh International Workshop on Parsing Technologies (IWPT2001), pages 123–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="11427" citStr="Klein and Manning, 2003" startWordPosition="1963" endWordPosition="1966">al symbols assigned to each parse tree before extracting rules. Here, we replace each non-terminal symbol by the state representation of Earley’s algorithm corresponding to the sequence of prediction steps starting from TOP. Figure 4(a) presents an example parse tree with each symbol replaced by the Earley’s state in Figure 4(b). For example, the label for VBD is replaced by •S + NP : •VP + •VBD : NP which corresponds to the prediction steps of TOP → •S, S → NP • VP and VP → •VBD NP. The context represented in the Earley’s state is further limited by the vertical and horizontal Markovization (Klein and Manning, 2003). We define the vertical order v in which the label is limited to memorize only v previous prediction steps. For instance, setting v = 1 yields NP : •VP + •VBD : NP in our example. Likewise, we introduce the horizontal order h which limits the number of sibling labels memorized on the left and the right of the dotted label. Limiting h = 1 implies that each deductive step is encoded with at most three symbols. No limits in the horizontal and vertical Markovization orders implies memorizing of all the deductions and yields a confusion forest representing the union of parse trees through the gram</context>
<context position="15411" citStr="Klein and Manning, 2003" startWordPosition="2606" endWordPosition="2609">internal to each system such as model combinations (DeNero et al., 2010). cz-en de-en es-en fr-en # of systems 6 16 8 14 avg. words tune 10.6K 10.9K 10.9K 11.0K test 50.5K 52.1K 52.1K 52.4K sentences tune 455 test 2,034 Table 1: WMT10 system combination tuning/testing data 5 Experiments 5.1 Setup We ran our experiments for the WMT10 system combination task usinge four language pairs, {Czech, French, German, Spanish}-to-English (Callison-Burch et al., 2010). The data is summarized in Table 1. The system outputs are retokenized to match the Penn-treebank standard, parsed by the Stanford Parser (Klein and Manning, 2003), and lower-cased. We implemented our confusion forest system combination using an in-house developed hypergraph-based toolkit cicada which is motivated by generic weighted logic programming (Lopez, 2009), originally developed for a synchronous-CFG based machine translation system (Chiang, 2007). Input to our system is a collection of hypergraphs, a set of parsed hypotheses, from which rules are extracted and a new forest is generated as described in Section 3. Our baseline, also implemented in cicada, is a confusion network-based system combination method (§2) which incrementally aligns hypot</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>Wolfgang Macherey</author>
<author>Chris Dyer</author>
<author>Franz Och</author>
</authors>
<title>Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>163--171</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="16476" citStr="Kumar et al., 2009" startWordPosition="2779" endWordPosition="2782">ribed in Section 3. Our baseline, also implemented in cicada, is a confusion network-based system combination method (§2) which incrementally aligns hypotheses to the growing network using TER (Rosti et al., 2008) and merges multiple networks into a large single network. After performing epsilon removal, the network is transformed into a forest by parsing with monotone rules of S —* X, S —* S X and X —* x. k-best translations are extracted from the forest using the forest-based algorithms in Section 3.3. 5.2 Features The feature weight vector w in Equation 1 is tuned by MERT over hypergraphs (Kumar et al., 2009). We use three lower-cased 5-gram language mod1253 els hilm(d): English Gigaword Fourth edition1, the English side of French-English 109 corpus and the news commentary English data2. The count based features ht(d) and he(d) count the number of terminals and the number of hyperedges in d, respectively. We employ M confidence measures hms (d) for M systems, which basically count the number of rules used in d originally extracted from mth system hypothesis (Rosti et al., 2007a). Following Macherey and Och (2007), BLEU (Papineni et al., 2002) correlations are also incorporated in our system combin</context>
</contexts>
<marker>Kumar, Macherey, Dyer, Och, 2009</marker>
<rawString>Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz Och. 2009. Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 163–171, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
<author>Kevin Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 1, ACL-36,</booktitle>
<pages>704--710</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="14070" citStr="Langkilde and Knight, 1998" startWordPosition="2393" endWordPosition="2396"> translation table is constructed from the MT outputs aligned with an input source sentence. New translations are generated by decoding the source sentence again using the newly extracted phrase table. Our grammar-based approach can be regarded as a regeneration approach in which an off-the-shelf monolingual parser, instead of a word aligner, is used to annotate syntactic information to each hypothesis, then, a new translation is generated from the merged forest, not from the input source sentence through decoding. In terms of generation, our approach is an instance of statistical generation (Langkilde and Knight, 1998; Langkilde, 2000). Instead of generating forests from semantic representations (Langkilde, 2000), we generate forests from a CFG encoding the consensus among parsed hypotheses. Liu et al. (2009) present joint decoding in which a translation forest is constructed from two distinct MT systems, tree-to-string and string-to-string, by merging forest outputs. Their merging method is either translation-level in which no new translation is generated, or derivation-level in that the rules sharing the same left-hand-side are used in both systems. While our work is similar in that a new forest is const</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Irene Langkilde and Kevin Knight. 1998. Generation that exploits corpus-based statistical knowledge. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 1, ACL-36, pages 704–710, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
</authors>
<title>Forest-based statistical sentence generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference,</booktitle>
<pages>170--177</pages>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="14088" citStr="Langkilde, 2000" startWordPosition="2397" endWordPosition="2398">ucted from the MT outputs aligned with an input source sentence. New translations are generated by decoding the source sentence again using the newly extracted phrase table. Our grammar-based approach can be regarded as a regeneration approach in which an off-the-shelf monolingual parser, instead of a word aligner, is used to annotate syntactic information to each hypothesis, then, a new translation is generated from the merged forest, not from the input source sentence through decoding. In terms of generation, our approach is an instance of statistical generation (Langkilde and Knight, 1998; Langkilde, 2000). Instead of generating forests from semantic representations (Langkilde, 2000), we generate forests from a CFG encoding the consensus among parsed hypotheses. Liu et al. (2009) present joint decoding in which a translation forest is constructed from two distinct MT systems, tree-to-string and string-to-string, by merging forest outputs. Their merging method is either translation-level in which no new translation is generated, or derivation-level in that the rules sharing the same left-hand-side are used in both systems. While our work is similar in that a new forest is constructed by sharing </context>
</contexts>
<marker>Langkilde, 2000</marker>
<rawString>Irene Langkilde. 2000. Forest-based statistical sentence generation. In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, pages 170–177, San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Efficient extraction of oracle-best translations from hypergraphs.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,</booktitle>
<pages>9--12</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="17501" citStr="Li and Khudanpur, 2009" startWordPosition="2956" endWordPosition="2959">ed in d originally extracted from mth system hypothesis (Rosti et al., 2007a). Following Macherey and Och (2007), BLEU (Papineni et al., 2002) correlations are also incorporated in our system combination. Given M system outputs e1...eM, M BLEU scores are computed for d using each of the system outputs em as a reference � 4 hmb (d) = BP(e, em) · exp 4 � log pn (e, em) where e = yield(d) is a terminal yield of d, and respectively denote brevity penalty and n-gram precision. Here, we use approximated unclipped n-gram counts (Dreyer et al., 2007) for computing with a compact state representation (Li and Khudanpur, 2009). Our baseline confusion network system has an additional penalty feature, hp BP(·) pn(·) pn(·) (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b). 5.3 Results CFv=∞,h=∞ CFv=∞,h=2 CFv=∞,h=1 CFv=4,h=∞ CFv=4,h=2 CFv=4,h=1 CFv=3,h=∞ CFv=3,h=2 23.30 23.95 30.02 28.19 CFv=3,h=1 23.23 21.43 29.27 26.53 forest with different vertical (v) and hori zontal (h) Markovization order. language cz-en de-en es-en fr-en rerank 29.40 32.32 36.83 36.59 CN 38.52 34.97 47.65 46</context>
</contexts>
<marker>Li, Khudanpur, 2009</marker>
<rawString>Zhifei Li and Sanjeev Khudanpur. 2009. Efficient extraction of oracle-best translations from hypergraphs. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, pages 9–12, Boulder, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Haitao Mi</author>
<author>Yang Feng</author>
<author>Qun Liu</author>
</authors>
<title>Joint decoding with multiple translation models.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>576--584</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="14265" citStr="Liu et al. (2009)" startWordPosition="2423" endWordPosition="2426">r grammar-based approach can be regarded as a regeneration approach in which an off-the-shelf monolingual parser, instead of a word aligner, is used to annotate syntactic information to each hypothesis, then, a new translation is generated from the merged forest, not from the input source sentence through decoding. In terms of generation, our approach is an instance of statistical generation (Langkilde and Knight, 1998; Langkilde, 2000). Instead of generating forests from semantic representations (Langkilde, 2000), we generate forests from a CFG encoding the consensus among parsed hypotheses. Liu et al. (2009) present joint decoding in which a translation forest is constructed from two distinct MT systems, tree-to-string and string-to-string, by merging forest outputs. Their merging method is either translation-level in which no new translation is generated, or derivation-level in that the rules sharing the same left-hand-side are used in both systems. While our work is similar in that a new forest is constructed by sharing rules among systems, although their work involves no consensus translation and requires structures internal to each system such as model combinations (DeNero et al., 2010). cz-e</context>
</contexts>
<marker>Liu, Mi, Feng, Liu, 2009</marker>
<rawString>Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009. Joint decoding with multiple translation models. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 576–584, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Translation as weighted deduction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>532--540</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="15615" citStr="Lopez, 2009" startWordPosition="2635" endWordPosition="2636">034 Table 1: WMT10 system combination tuning/testing data 5 Experiments 5.1 Setup We ran our experiments for the WMT10 system combination task usinge four language pairs, {Czech, French, German, Spanish}-to-English (Callison-Burch et al., 2010). The data is summarized in Table 1. The system outputs are retokenized to match the Penn-treebank standard, parsed by the Stanford Parser (Klein and Manning, 2003), and lower-cased. We implemented our confusion forest system combination using an in-house developed hypergraph-based toolkit cicada which is motivated by generic weighted logic programming (Lopez, 2009), originally developed for a synchronous-CFG based machine translation system (Chiang, 2007). Input to our system is a collection of hypergraphs, a set of parsed hypotheses, from which rules are extracted and a new forest is generated as described in Section 3. Our baseline, also implemented in cicada, is a confusion network-based system combination method (§2) which incrementally aligns hypotheses to the growing network using TER (Rosti et al., 2008) and merges multiple networks into a large single network. After performing epsilon removal, the network is transformed into a forest by parsing </context>
</contexts>
<marker>Lopez, 2009</marker>
<rawString>Adam Lopez. 2009. Translation as weighted deduction. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 532– 540, Athens, Greece, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Macherey</author>
<author>Franz J Och</author>
</authors>
<title>An empirical study on computing consensus translations from multiple machine translation systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>986--995</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="16990" citStr="Macherey and Och (2007)" startWordPosition="2863" endWordPosition="2866">5.2 Features The feature weight vector w in Equation 1 is tuned by MERT over hypergraphs (Kumar et al., 2009). We use three lower-cased 5-gram language mod1253 els hilm(d): English Gigaword Fourth edition1, the English side of French-English 109 corpus and the news commentary English data2. The count based features ht(d) and he(d) count the number of terminals and the number of hyperedges in d, respectively. We employ M confidence measures hms (d) for M systems, which basically count the number of rules used in d originally extracted from mth system hypothesis (Rosti et al., 2007a). Following Macherey and Och (2007), BLEU (Papineni et al., 2002) correlations are also incorporated in our system combination. Given M system outputs e1...eM, M BLEU scores are computed for d using each of the system outputs em as a reference � 4 hmb (d) = BP(e, em) · exp 4 � log pn (e, em) where e = yield(d) is a terminal yield of d, and respectively denote brevity penalty and n-gram precision. Here, we use approximated unclipped n-gram counts (Dreyer et al., 2007) for computing with a compact state representation (Li and Khudanpur, 2009). Our baseline confusion network system has an additional penalty feature, hp BP(·) pn(·)</context>
</contexts>
<marker>Macherey, Och, 2007</marker>
<rawString>Wolfgang Macherey and Franz J. Och. 2007. An empirical study on computing consensus translations from multiple machine translation systems. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 986–995, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidia Mangu</author>
<author>Eric Brill</author>
<author>Andreas Stolcke</author>
</authors>
<title>Finding consensus in speech recognition: word error minimization and other applications of confusion networks.</title>
<date>2000</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>14</volume>
<issue>4</issue>
<pages>400</pages>
<contexts>
<context position="1366" citStr="Mangu et al., 2000" startWordPosition="186" endWordPosition="189">ting a set of rules that constitute the parse trees. Third, a packed forest is generated starting from the root symbol of the extracted grammar through non-terminal rewriting. The new hypothesis is produced by searching the best derivation in the forest. Experimental results on the WMT10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space. 1 Introduction System combination techniques take the advantages of consensus among multiple systems and have been widely used in fields, such as speech recognition (Fiscus, 1997; Mangu et al., 2000) or parsing (Henderson and Brill, 1999). One of the state-of-the-art system combination methods for MT is based on confusion networks, which are compact graph-based structures representing multiple hypotheses (Bangalore et al., 2001). Confusion networks are constructed based on string similarity information. First, one skeleton or backbone sentence is selected. Then, other hypotheses are aligned against the skeleton, forming a lattice with each arc representing alternative word candidates. The alignment method is either model-based (Matusov et al., 2006; He et al., 2008) in which a statistical</context>
</contexts>
<marker>Mangu, Brill, Stolcke, 2000</marker>
<rawString>Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000. Finding consensus in speech recognition: word error minimization and other applications of confusion networks. Computer Speech &amp; Language, 14(4):373 – 400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="1925" citStr="Matusov et al., 2006" startWordPosition="267" endWordPosition="270">such as speech recognition (Fiscus, 1997; Mangu et al., 2000) or parsing (Henderson and Brill, 1999). One of the state-of-the-art system combination methods for MT is based on confusion networks, which are compact graph-based structures representing multiple hypotheses (Bangalore et al., 2001). Confusion networks are constructed based on string similarity information. First, one skeleton or backbone sentence is selected. Then, other hypotheses are aligned against the skeleton, forming a lattice with each arc representing alternative word candidates. The alignment method is either model-based (Matusov et al., 2006; He et al., 2008) in which a statistical word aligner is used to compute hypothesis alignment, or edit-based (Jayaraman and Lavie, 2005; Sim et al., 2007) in which alignment is measured by an evaluation metric, such as translation error rate (TER) (Snover et al., 2006). The new translation hypothesis is generated by selecting the best path through the network. We present a novel method for system combination which exploits the syntactic similarity of system outputs. Instead of constructing a string-based confusion network, we generate a packed forest (Billot and Lang, 1989; Mi et al., 2008) w</context>
<context position="4551" citStr="Matusov et al. (2006)" startWordPosition="680" endWordPosition="683">atistically significant improvements in the others. First, we will review the state-of-the-art method which is a system combination framework based on confusion networks (§2). Then, we will introduce a novel system combination method based on confusion forest (§3) and present related work in consensus translations (§4). Experiments are presented in Section 5 followed by discussion and our conclusion. 2 Combination by Confusion Network The system combination framework based on confusion network starts from computing pairwise alignment between hypotheses by taking one hypothesis as a reference. Matusov et al. (2006) employs a model based approach in which a statistical word aligner, such as GIZA++ (Och and Ney, 2003), is used to align the hypotheses. Sim et al. (2007) introduced TER (Snover et al., 2006) to measure the edit-based alignment. Then, one hypothesis is selected, for example by employing a minimum Bayes risk criterion (Sim et al., 2007), as a skeleton, or a backbone, which serves as a building block for aligning the rest of the hypotheses. Other hypotheses are aligned against the skeleton using the pairwise alignment. Figure 1(b) illustrates an example of a confusion network constructed from t</context>
<context position="12820" citStr="Matusov et al., 2006" startWordPosition="2193" endWordPosition="2196"> prediction step. Likewise, constraining the vertical order generates a deeper forest by ignoring the sequence of symbols leading to a particular node. 3.3 Forest Rescoring From the packed forest F, new k-best derivations are extracted from all possible derivations D by efficient forest-based algorithms for k-best parsing (Huang and Chiang, 2005). We use a linear combiproposed a phrasal combination by merging hypotheses in a chart structure, while others depended on confusion networks, or similar structures, as a building block for merging hypotheses at the word level (Bangalore et al., 2001; Matusov et al., 2006; He et al., 2008; Jayaraman and Lavie, 2005; Sim et al., 2007). Our work is the first to explicitly exploit syntactic similarity for system combination by merging hypotheses into a syntactic packed forest. The confusion forest approach may suffer from parsing errors such as the confusion network construction influenced by alignment errors. Even with parsing errors, we can still take a tree fragment-level consensus as long as a parser is consistent in that similar syntactic mistakes would be made for similar hypotheses. Rosti et al. (2007a) describe a re-generation approach to consensus transl</context>
</contexts>
<marker>Matusov, Ueffing, Ney, 2006</marker>
<rawString>Evgeny Matusov, Nicola Ueffing, and Hermann Ney. 2006. Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>192--199</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2523" citStr="Mi et al., 2008" startWordPosition="368" endWordPosition="371">tusov et al., 2006; He et al., 2008) in which a statistical word aligner is used to compute hypothesis alignment, or edit-based (Jayaraman and Lavie, 2005; Sim et al., 2007) in which alignment is measured by an evaluation metric, such as translation error rate (TER) (Snover et al., 2006). The new translation hypothesis is generated by selecting the best path through the network. We present a novel method for system combination which exploits the syntactic similarity of system outputs. Instead of constructing a string-based confusion network, we generate a packed forest (Billot and Lang, 1989; Mi et al., 2008) which encodes exponentially many parse trees in a polynomial space. The packed forest, or confusion forest, is constructed by merging the MT outputs with regard to their syntactic consensus. We employ a grammar-based method to generate the confusion forest: First, system outputs are parsed. Second, a set of rules are extracted from the parse trees. Third, a packed forest is generated using a variant of Earley’s algorithm (Earley, 1970) starting from the unique root symbol. New hypotheses are selected by searching the best derivation in the forest. The grammar, a set of rules, is limited to th</context>
<context position="7213" citStr="Mi et al., 2008" startWordPosition="1125" endWordPosition="1128">e blue trees was found” in Figure 1(c). Rosti et al. (2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network. 3 Combination by Confusion Forest The confusion network approach to system combination encodes multiple hypotheses into a compact lattice structure by using word-level consensus. Likewise, we propose to encode multiple hypotheses into a confusion forest, which is a packed forest which represents multiple parse trees in a polynomial space (Billot and Lang, 1989; Mi et al., 2008) Syntactic consensus is realized by sharing tree fragϵ green ϵ was ϵ green trees ϵ ϵ 1250 alization: [TOP → •S,0] : ¯1 Scan: [X →α • xβ, h] : u[ X → α x •β , h] : uP re d i ct: [X →α • Yβ , h ] [Y→ •γ Gm p l ,h + 1] : u Y1→ γ ∈ ete:[X→α • Yβ, h] : u [Y → γ•,h + 1] : v [X → αY •β,h] : u ⊗ v Goal: gur P@1 D f 3 V was VBN @P Fi V1 walke PRP T th re VBD@ P@4 oun dsawNP@2V2 DT the NI NN efo st VBD@2d blu @2V reen NN fores tVP@2S JJ 2V2 f or es eg 2V1 NN the DT@2V t trees [TOP → e2:Ane xa mp lepacke d fores trepre sentinghy-po the sesin Fig ur e1(a). ments among parse trees .The fo res tisrep re sen</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proceedings of ACL-08: HLT, pages 192–199, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadashi Nomoto</author>
</authors>
<title>Multi-engine machine translation with voted language model.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>494--501</pages>
<location>Barcelona, Spain,</location>
<marker>Nomoto, 2004</marker>
<rawString>Tadashi Nomoto. 2004. Multi-engine machine translation with voted language model. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 494–501, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="4654" citStr="Och and Ney, 2003" startWordPosition="698" endWordPosition="701">h is a system combination framework based on confusion networks (§2). Then, we will introduce a novel system combination method based on confusion forest (§3) and present related work in consensus translations (§4). Experiments are presented in Section 5 followed by discussion and our conclusion. 2 Combination by Confusion Network The system combination framework based on confusion network starts from computing pairwise alignment between hypotheses by taking one hypothesis as a reference. Matusov et al. (2006) employs a model based approach in which a statistical word aligner, such as GIZA++ (Och and Ney, 2003), is used to align the hypotheses. Sim et al. (2007) introduced TER (Snover et al., 2006) to measure the edit-based alignment. Then, one hypothesis is selected, for example by employing a minimum Bayes risk criterion (Sim et al., 2007), as a skeleton, or a backbone, which serves as a building block for aligning the rest of the hypotheses. Other hypotheses are aligned against the skeleton using the pairwise alignment. Figure 1(b) illustrates an example of a confusion network constructed from the four hypotheses in Figure 1(a), assuming the first hypothesis is selected as our skeleton. The netwo</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="17020" citStr="Papineni et al., 2002" startWordPosition="2868" endWordPosition="2872"> vector w in Equation 1 is tuned by MERT over hypergraphs (Kumar et al., 2009). We use three lower-cased 5-gram language mod1253 els hilm(d): English Gigaword Fourth edition1, the English side of French-English 109 corpus and the news commentary English data2. The count based features ht(d) and he(d) count the number of terminals and the number of hyperedges in d, respectively. We employ M confidence measures hms (d) for M systems, which basically count the number of rules used in d originally extracted from mth system hypothesis (Rosti et al., 2007a). Following Macherey and Och (2007), BLEU (Papineni et al., 2002) correlations are also incorporated in our system combination. Given M system outputs e1...eM, M BLEU scores are computed for d using each of the system outputs em as a reference � 4 hmb (d) = BP(e, em) · exp 4 � log pn (e, em) where e = yield(d) is a terminal yield of d, and respectively denote brevity penalty and n-gram precision. Here, we use approximated unclipped n-gram counts (Dreyer et al., 2007) for computing with a compact state representation (Li and Khudanpur, 2009). Our baseline confusion network system has an additional penalty feature, hp BP(·) pn(·) pn(·) (m), which is the total</context>
<context position="19410" citStr="Papineni et al., 2002" startWordPosition="3268" endWordPosition="3272">s without statistically significant differences from the best result (p &lt; 0.05) are indicated by bold face. Setting v = and h = achieves comparable performance to CN. Our best results in three languages come from setting v = and h = 2, which favors little reordering of phrasal structures. In general, lower horizontal and vert ∞ ∞. ∞ ∞ ∞ ical order leads to lower BLEU. catalog No. LDC2009T13 data are available fr 1LDC 2Those om http://www.statmt. org/wmt10/. Table 2 compares our confusion forest approach (CF) with different orders, a confusion network (CN) and max/min systems measured by BLEU (Papineni et al., 2002). We vary the horizontal orders, Table 2: Translation results in lower-case BLEU. CN for confusion network and CF for confusion Table 3 presents oracle BLEU achievable by each combination method. The gains achievable by the CF over simple reranking are small, at most 2-3 points, indicating that small variations are encoded in confusion forests. We also observed that a lower horizontal and vertical order leads to better BLEU potentials. As briefly pointed out in Section 3.2, the higher horizontal and vertical order implies more faithfulness to the original parse trees. Introducing new tree frag</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Necip Fazil Ayan</author>
<author>Bing Xiang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
<author>Bonnie Dorr</author>
</authors>
<title>Combining outputs from multiple machine translation systems.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>228--235</pages>
<location>Rochester, New York,</location>
<contexts>
<context position="6655" citStr="Rosti et al. (2007" startWordPosition="1035" endWordPosition="1038">cted confusion network Figure 1: An example confusion network construction skeleton hypothesis. In our example, “green trees” is aligned with “blue forest” in Figure 1(c). The confusion network construction is largely influenced by the skeleton selection, which determines the global word reordering of a new hypothesis. For example, the last hypothesis in Figure 1(a) has a passive voice grammatical construction while the others are active voice. This large grammatical difference may produce a longer sentence with spuriously inserted words, as in “I saw the blue trees was found” in Figure 1(c). Rosti et al. (2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network. 3 Combination by Confusion Forest The confusion network approach to system combination encodes multiple hypotheses into a compact lattice structure by using word-level consensus. Likewise, we propose to encode multiple hypotheses into a confusion forest, which is a packed forest which represents multiple parse trees in a polynomial space (Billot and Lang, 1989; Mi et al., 2008) Syntactic consensus is realized by sharin</context>
<context position="13364" citStr="Rosti et al. (2007" startWordPosition="2284" endWordPosition="2287">hypotheses at the word level (Bangalore et al., 2001; Matusov et al., 2006; He et al., 2008; Jayaraman and Lavie, 2005; Sim et al., 2007). Our work is the first to explicitly exploit syntactic similarity for system combination by merging hypotheses into a syntactic packed forest. The confusion forest approach may suffer from parsing errors such as the confusion network construction influenced by alignment errors. Even with parsing errors, we can still take a tree fragment-level consensus as long as a parser is consistent in that similar syntactic mistakes would be made for similar hypotheses. Rosti et al. (2007a) describe a re-generation approach to consensus translation in which a phrasal translation table is constructed from the MT outputs aligned with an input source sentence. New translations are generated by decoding the source sentence again using the newly extracted phrase table. Our grammar-based approach can be regarded as a regeneration approach in which an off-the-shelf monolingual parser, instead of a word aligner, is used to annotate syntactic information to each hypothesis, then, a new translation is generated from the merged forest, not from the input source sentence through decoding.</context>
<context position="16953" citStr="Rosti et al., 2007" startWordPosition="2858" endWordPosition="2861">ased algorithms in Section 3.3. 5.2 Features The feature weight vector w in Equation 1 is tuned by MERT over hypergraphs (Kumar et al., 2009). We use three lower-cased 5-gram language mod1253 els hilm(d): English Gigaword Fourth edition1, the English side of French-English 109 corpus and the news commentary English data2. The count based features ht(d) and he(d) count the number of terminals and the number of hyperedges in d, respectively. We employ M confidence measures hms (d) for M systems, which basically count the number of rules used in d originally extracted from mth system hypothesis (Rosti et al., 2007a). Following Macherey and Och (2007), BLEU (Papineni et al., 2002) correlations are also incorporated in our system combination. Given M system outputs e1...eM, M BLEU scores are computed for d using each of the system outputs em as a reference � 4 hmb (d) = BP(e, em) · exp 4 � log pn (e, em) where e = yield(d) is a terminal yield of d, and respectively denote brevity penalty and n-gram precision. Here, we use approximated unclipped n-gram counts (Dreyer et al., 2007) for computing with a compact state representation (Li and Khudanpur, 2009). Our baseline confusion network system has an addit</context>
</contexts>
<marker>Rosti, Ayan, Xiang, Matsoukas, Schwartz, Dorr, 2007</marker>
<rawString>Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spyros Matsoukas, Richard Schwartz, and Bonnie Dorr. 2007a. Combining outputs from multiple machine translation systems. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 228– 235, Rochester, New York, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Improved word-level system combination for machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>312--319</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="6655" citStr="Rosti et al. (2007" startWordPosition="1035" endWordPosition="1038">cted confusion network Figure 1: An example confusion network construction skeleton hypothesis. In our example, “green trees” is aligned with “blue forest” in Figure 1(c). The confusion network construction is largely influenced by the skeleton selection, which determines the global word reordering of a new hypothesis. For example, the last hypothesis in Figure 1(a) has a passive voice grammatical construction while the others are active voice. This large grammatical difference may produce a longer sentence with spuriously inserted words, as in “I saw the blue trees was found” in Figure 1(c). Rosti et al. (2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network. 3 Combination by Confusion Forest The confusion network approach to system combination encodes multiple hypotheses into a compact lattice structure by using word-level consensus. Likewise, we propose to encode multiple hypotheses into a confusion forest, which is a packed forest which represents multiple parse trees in a polynomial space (Billot and Lang, 1989; Mi et al., 2008) Syntactic consensus is realized by sharin</context>
<context position="13364" citStr="Rosti et al. (2007" startWordPosition="2284" endWordPosition="2287">hypotheses at the word level (Bangalore et al., 2001; Matusov et al., 2006; He et al., 2008; Jayaraman and Lavie, 2005; Sim et al., 2007). Our work is the first to explicitly exploit syntactic similarity for system combination by merging hypotheses into a syntactic packed forest. The confusion forest approach may suffer from parsing errors such as the confusion network construction influenced by alignment errors. Even with parsing errors, we can still take a tree fragment-level consensus as long as a parser is consistent in that similar syntactic mistakes would be made for similar hypotheses. Rosti et al. (2007a) describe a re-generation approach to consensus translation in which a phrasal translation table is constructed from the MT outputs aligned with an input source sentence. New translations are generated by decoding the source sentence again using the newly extracted phrase table. Our grammar-based approach can be regarded as a regeneration approach in which an off-the-shelf monolingual parser, instead of a word aligner, is used to annotate syntactic information to each hypothesis, then, a new translation is generated from the merged forest, not from the input source sentence through decoding.</context>
<context position="16953" citStr="Rosti et al., 2007" startWordPosition="2858" endWordPosition="2861">ased algorithms in Section 3.3. 5.2 Features The feature weight vector w in Equation 1 is tuned by MERT over hypergraphs (Kumar et al., 2009). We use three lower-cased 5-gram language mod1253 els hilm(d): English Gigaword Fourth edition1, the English side of French-English 109 corpus and the news commentary English data2. The count based features ht(d) and he(d) count the number of terminals and the number of hyperedges in d, respectively. We employ M confidence measures hms (d) for M systems, which basically count the number of rules used in d originally extracted from mth system hypothesis (Rosti et al., 2007a). Following Macherey and Och (2007), BLEU (Papineni et al., 2002) correlations are also incorporated in our system combination. Given M system outputs e1...eM, M BLEU scores are computed for d using each of the system outputs em as a reference � 4 hmb (d) = BP(e, em) · exp 4 � log pn (e, em) where e = yield(d) is a terminal yield of d, and respectively denote brevity penalty and n-gram precision. Here, we use approximated unclipped n-gram counts (Dreyer et al., 2007) for computing with a compact state representation (Li and Khudanpur, 2009). Our baseline confusion network system has an addit</context>
</contexts>
<marker>Rosti, Matsoukas, Schwartz, 2007</marker>
<rawString>Antti-Veikko Rosti, Spyros Matsoukas, and Richard Schwartz. 2007b. Improved word-level system combination for machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312–319, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Incremental hypothesis alignment for building confusion networks with application to machine translation system combination.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>183--186</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="5607" citStr="Rosti et al. (2008)" startWordPosition="860" endWordPosition="863">heses. Other hypotheses are aligned against the skeleton using the pairwise alignment. Figure 1(b) illustrates an example of a confusion network constructed from the four hypotheses in Figure 1(a), assuming the first hypothesis is selected as our skeleton. The network consists of several arcs, each of which represents an alternative word at that position, including the empty symbol, ϵ. This pairwise alignment strategy is prone to spurious insertions and repetitions due to alignment errors such as in Figure 1(a) in which “green” in the third hypothesis is aligned with “forest” in the skeleton. Rosti et al. (2008) introduces an incremental method so that hypotheses are aligned incrementally to the growing confusion network, not only the * I saw the forest I walked the blue forest I saw the green trees the forest was found (a) Pairwise alignment using the first starred hypothesis as a skeleton. I saw blue forest trees found the ϵ ϵ walked (b) Confusion network from (a) I saw blue forest was found the ϵ ϵ walked (c) Incrementally constructed confusion network Figure 1: An example confusion network construction skeleton hypothesis. In our example, “green trees” is aligned with “blue forest” in Figure 1(c)</context>
<context position="16070" citStr="Rosti et al., 2008" startWordPosition="2706" endWordPosition="2709">nfusion forest system combination using an in-house developed hypergraph-based toolkit cicada which is motivated by generic weighted logic programming (Lopez, 2009), originally developed for a synchronous-CFG based machine translation system (Chiang, 2007). Input to our system is a collection of hypergraphs, a set of parsed hypotheses, from which rules are extracted and a new forest is generated as described in Section 3. Our baseline, also implemented in cicada, is a confusion network-based system combination method (§2) which incrementally aligns hypotheses to the growing network using TER (Rosti et al., 2008) and merges multiple networks into a large single network. After performing epsilon removal, the network is transformed into a forest by parsing with monotone rules of S —* X, S —* S X and X —* x. k-best translations are extracted from the forest using the forest-based algorithms in Section 3.3. 5.2 Features The feature weight vector w in Equation 1 is tuned by MERT over hypergraphs (Kumar et al., 2009). We use three lower-cased 5-gram language mod1253 els hilm(d): English Gigaword Fourth edition1, the English side of French-English 109 corpus and the news commentary English data2. The count b</context>
</contexts>
<marker>Rosti, Zhang, Matsoukas, Schwartz, 2008</marker>
<rawString>Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and Richard Schwartz. 2008. Incremental hypothesis alignment for building confusion networks with application to machine translation system combination. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 183–186, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>Journal of Logic Programming,</journal>
<pages>24--1</pages>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Stuart M. Shieber, Yves Schabes, and Fernando C. N. Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24(1–2):3–36, July–August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K C Sim</author>
<author>W J Byrne</author>
<author>M J F Gales</author>
<author>H Sahbi</author>
<author>P C Woodland</author>
</authors>
<title>Consensus network decoding for statistical machine translation system combination.</title>
<date>2007</date>
<booktitle>In Proceedings of Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<volume>4</volume>
<pages>105--108</pages>
<contexts>
<context position="2080" citStr="Sim et al., 2007" startWordPosition="294" endWordPosition="297">or MT is based on confusion networks, which are compact graph-based structures representing multiple hypotheses (Bangalore et al., 2001). Confusion networks are constructed based on string similarity information. First, one skeleton or backbone sentence is selected. Then, other hypotheses are aligned against the skeleton, forming a lattice with each arc representing alternative word candidates. The alignment method is either model-based (Matusov et al., 2006; He et al., 2008) in which a statistical word aligner is used to compute hypothesis alignment, or edit-based (Jayaraman and Lavie, 2005; Sim et al., 2007) in which alignment is measured by an evaluation metric, such as translation error rate (TER) (Snover et al., 2006). The new translation hypothesis is generated by selecting the best path through the network. We present a novel method for system combination which exploits the syntactic similarity of system outputs. Instead of constructing a string-based confusion network, we generate a packed forest (Billot and Lang, 1989; Mi et al., 2008) which encodes exponentially many parse trees in a polynomial space. The packed forest, or confusion forest, is constructed by merging the MT outputs with re</context>
<context position="4706" citStr="Sim et al. (2007)" startWordPosition="708" endWordPosition="711">n networks (§2). Then, we will introduce a novel system combination method based on confusion forest (§3) and present related work in consensus translations (§4). Experiments are presented in Section 5 followed by discussion and our conclusion. 2 Combination by Confusion Network The system combination framework based on confusion network starts from computing pairwise alignment between hypotheses by taking one hypothesis as a reference. Matusov et al. (2006) employs a model based approach in which a statistical word aligner, such as GIZA++ (Och and Ney, 2003), is used to align the hypotheses. Sim et al. (2007) introduced TER (Snover et al., 2006) to measure the edit-based alignment. Then, one hypothesis is selected, for example by employing a minimum Bayes risk criterion (Sim et al., 2007), as a skeleton, or a backbone, which serves as a building block for aligning the rest of the hypotheses. Other hypotheses are aligned against the skeleton using the pairwise alignment. Figure 1(b) illustrates an example of a confusion network constructed from the four hypotheses in Figure 1(a), assuming the first hypothesis is selected as our skeleton. The network consists of several arcs, each of which represent</context>
<context position="12883" citStr="Sim et al., 2007" startWordPosition="2205" endWordPosition="2208">tes a deeper forest by ignoring the sequence of symbols leading to a particular node. 3.3 Forest Rescoring From the packed forest F, new k-best derivations are extracted from all possible derivations D by efficient forest-based algorithms for k-best parsing (Huang and Chiang, 2005). We use a linear combiproposed a phrasal combination by merging hypotheses in a chart structure, while others depended on confusion networks, or similar structures, as a building block for merging hypotheses at the word level (Bangalore et al., 2001; Matusov et al., 2006; He et al., 2008; Jayaraman and Lavie, 2005; Sim et al., 2007). Our work is the first to explicitly exploit syntactic similarity for system combination by merging hypotheses into a syntactic packed forest. The confusion forest approach may suffer from parsing errors such as the confusion network construction influenced by alignment errors. Even with parsing errors, we can still take a tree fragment-level consensus as long as a parser is consistent in that similar syntactic mistakes would be made for similar hypotheses. Rosti et al. (2007a) describe a re-generation approach to consensus translation in which a phrasal translation table is constructed from </context>
</contexts>
<marker>Sim, Byrne, Gales, Sahbi, Woodland, 2007</marker>
<rawString>K.C. Sim, W.J. Byrne, M.J.F. Gales, H. Sahbi, and P.C. Woodland. 2007. Consensus network decoding for statistical machine translation system combination. In Proceedings of Acoustics, Speech and Signal Processing (ICASSP), 2007, volume 4, pages IV–105 –IV– 108, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation. In</title>
<date>2006</date>
<booktitle>In Proceedings ofAssociation for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="2195" citStr="Snover et al., 2006" startWordPosition="315" endWordPosition="318">(Bangalore et al., 2001). Confusion networks are constructed based on string similarity information. First, one skeleton or backbone sentence is selected. Then, other hypotheses are aligned against the skeleton, forming a lattice with each arc representing alternative word candidates. The alignment method is either model-based (Matusov et al., 2006; He et al., 2008) in which a statistical word aligner is used to compute hypothesis alignment, or edit-based (Jayaraman and Lavie, 2005; Sim et al., 2007) in which alignment is measured by an evaluation metric, such as translation error rate (TER) (Snover et al., 2006). The new translation hypothesis is generated by selecting the best path through the network. We present a novel method for system combination which exploits the syntactic similarity of system outputs. Instead of constructing a string-based confusion network, we generate a packed forest (Billot and Lang, 1989; Mi et al., 2008) which encodes exponentially many parse trees in a polynomial space. The packed forest, or confusion forest, is constructed by merging the MT outputs with regard to their syntactic consensus. We employ a grammar-based method to generate the confusion forest: First, system</context>
<context position="4743" citStr="Snover et al., 2006" startWordPosition="715" endWordPosition="718">roduce a novel system combination method based on confusion forest (§3) and present related work in consensus translations (§4). Experiments are presented in Section 5 followed by discussion and our conclusion. 2 Combination by Confusion Network The system combination framework based on confusion network starts from computing pairwise alignment between hypotheses by taking one hypothesis as a reference. Matusov et al. (2006) employs a model based approach in which a statistical word aligner, such as GIZA++ (Och and Ney, 2003), is used to align the hypotheses. Sim et al. (2007) introduced TER (Snover et al., 2006) to measure the edit-based alignment. Then, one hypothesis is selected, for example by employing a minimum Bayes risk criterion (Sim et al., 2007), as a skeleton, or a backbone, which serves as a building block for aligning the rest of the hypotheses. Other hypotheses are aligned against the skeleton using the pairwise alignment. Figure 1(b) illustrates an example of a confusion network constructed from the four hypotheses in Figure 1(a), assuming the first hypothesis is selected as our skeleton. The network consists of several arcs, each of which represents an alternative word at that positio</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In In Proceedings ofAssociation for Machine Translation in the Americas, pages 223–231.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>