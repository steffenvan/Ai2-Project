<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001989">
<title confidence="0.998882">
QuickSet: Multimodal Interaction for
Simulation Set-up and Control
</title>
<author confidence="0.9879705">
Philip R. Cohen, Michael Johnston, David McGee, Sharon Oviatt, Jay Pittman,
Ira Smith, Liang Chen and Josh Clow
</author>
<affiliation confidence="0.9852345">
Center for Human Computer Communication
Oregon Graduate Institute of Science and Technology
</affiliation>
<address confidence="0.872057666666667">
P.O.Box 91000
Portland, OR 97291-1000 USA
Tel: 1-503-690-1326
</address>
<email confidence="0.8453775">
E-mail: pcohen@cse.ogi.edu
http://www.cse.ogi.edu/CHCC
</email>
<sectionHeader confidence="0.96717" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.991426083333333">
This paper presents a novel multimodal system applied to
the setup and control of distributed interactive simulations.
We have developed the QuickSet prototype, a pen/voice
system running on a hand-held PC, communicating through
a distributed agent architecture to NRaD&apos;s&apos; LeatherNet
system, a distributed interactive training simulator built for
the US Marine Corps (USMC). The paper briefly describes
the system and illustrates its use in multimodal simulation
setup.
KEYWORDS: multimodal interfaces, agent architecture,
gesture recognition, speech recognition, natural language
processing, distributed interactive simulation.
</bodyText>
<sectionHeader confidence="0.99816" genericHeader="keywords">
1. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999977666666667">
In order to train personnel more effectively, the US military
is developing large-scale distributed simulation capabilities.
Begun as SIMNET in the 1980&apos;s [23], these distributed,
interactive environments attempt to provide a high degree of
fidelity in simulating combat, including simulations of the
individual combatants, the equipment, entity movements,
atmospheric effects, etc. There are four general phases of
user interaction with these simulations: Creating entities,
supplying their initial behavior, interacting with the entities
during a running simulation, and reviewing the results. The
present research concentrates on the first two of these
stages.
Our contribution to the distributed interactive simulation
(DIS) effort is to rethink the nature of the user interaction.
As with most modern simulators, DISs are controlled via
graphical user interfaces (GUIs). However, the simulation
GUI is showing signs of strain, since even for a small-scale
scenario, it requires users to choose from hundreds of
entities in order to select the desired ones to place on a map.
To compound these interface problems, the military is
intending to increase the scale of the simulations
</bodyText>
<note confidence="0.4212285">
NRaD = US Navy Command and Control Ocean Systems Center
Research Development Test and Evaluation (San Diego).
</note>
<bodyText confidence="0.99939812">
dramatically, while at the same time, for reasons of
mobility and affordability, desiring that simulations should
be creatable from small devices (e.g., PDAs). This
impending collision of trends for smaller screen size and for
more entities requires a different paradigm for human-
computer interaction.
We have argued generically that GUI technologies offer
advantages in allowing users to manipulate objects that are
on the screen, in reminding users of their options, and in
minimizing errors [7]. However, GUIs are often weak in
supporting interactions with many objects, or objects not
on the screen. In contrast, it was argued that linguistically-
based interface technologies offer the potential to describe
large sets of objects, which may not all be present on a
screen, and can be used to create more complex behaviors
through specification of rule invocation conditions.
Simulation is one type of application for which these
limitations of GUIs, as well as the strengths of natural
language, especially spoken language, are apparent [6].
It has become clear, however, that speech-only interaction is
not optimal for spatial tasks. Using a high-fidelity
&amp;quot;Wizard-of-Oz&amp;quot; methodology [20], recent empirical results
demonstrate clear language processing and task performance
advantages for multimodal (pen/voice) input over speech-
only input for map-based systems [17,18].
</bodyText>
<sectionHeader confidence="0.99008" genericHeader="introduction">
3. OUICKSET
</sectionHeader>
<bodyText confidence="0.97476675">
To address these simulation interface problems, and
motivated by the above results, we have developed
QuickSet (see Figure 1) a collaborative, handheld,
multimodal system for configuring military simulations
based on LeatherNet [5], a system used in training platoon
leaders and company commanders at the USMC base at 29
Palms, California. LeatherNet simulations are created using
the ModSAF simulator [10] and can be visualized in a
CAVE-based virtual reality environment [11, 26] called
CommandVu (see Figure 2 â€” QuickSet systems are on
the soldiers&apos; tables). In addition to LeatherNet, QuickSet is
being used in a second effort called ExInit (Exercise
</bodyText>
<page confidence="0.990129">
20
</page>
<bodyText confidence="0.9986026">
Initialization), that will enable users to create division-sized
exercises. Because of the use of OAA, QuickSet can
interoperate with agents from CommandTalk [14], which
provides a speech-only interface to ModSAF.
QuickSet runs on both desktop and hand-held PC&apos;s,
communicating over wired and wireless LAN&apos;s, or modem
links. The system combines speech and pen-based gesture
input on multiple 3-lb hand-held PCs (Fujitsu Stylistic
1000), which communicate via wireless LAN through the
Open Agent Architecture (OAA)2 [8], to ModSAF, and also
to CommandVu. With this highly portable device, a user
can create entities, establish &amp;quot;control measures&amp;quot; (e.g.,
objectives, checkpoints, etc.), draw and label various lines
and areas, (e.g., landing zones) and give the entities
behavior.
</bodyText>
<figureCaption confidence="0.998199">
Figure 1: QuickSet running on a wireless handheld PC.
</figureCaption>
<bodyText confidence="0.934545">
In the remainder of the paper, we illustrate the system
briefly, describe its components, and discuss its application.
</bodyText>
<figureCaption confidence="0.773976666666667">
Figure 2: Artist&apos;s rendition of QuickSet used with
CommandVu virtual display of distributed
interactive simulation.
</figureCaption>
<sectionHeader confidence="0.975525" genericHeader="method">
4. SYSTEM ARCHITECTURE
</sectionHeader>
<bodyText confidence="0.998671666666667">
Architecturally, QuickSet uses distributed agent
technologies based on the Open Agent Architecture for
interoperation, information brokering and distribution. An
</bodyText>
<page confidence="0.451971">
2 Open Agent Architecture is a trademark of SRI International.
</page>
<bodyText confidence="0.988780339622642">
agent-based architecture was chosen to support this
application because it offers easy connection to legacy
applications, and the ability to run the same set of software
components in a variety of hardware configurations, ranging
from stand-alone on the handheld PC, to distributed
operation across numerous workstations and PCs.
Additionally, the architecture supports mobility in that
lighter weight agents can run on the handheld, while more
computationally-intensive processing can be migrated
elsewhere on the network. The agents may be written in
any programming language (here, Quintus Prolog, Visual
C++, Visual Basic, and Java), as long as they communicate
via an interagent communication language. The
configuration of agents used in the Quickset system is
illustrated in Figure 3. A brief description of each agent
follows:
QuickSet interface: On the handheld PC is a geo-
referenced map of the region such that entities displayed on
the map are registered to their positions on the actual
terrain, and thereby to their positions on each of the various
user interfaces connected to the simulation. The map
interface agent provides the usual pan and zoom capabilities,
multiple overlays, icons, etc. The user can draw directly on
the map, in order to create points, lines, and areas. The user
can create entities, give them behavior, and watch the
simulation unfold from the handheld. When the pen is
placed on the screen, the speech recognizer is activated,
thereby allowing users to speak and gesture simultaneously.
Speech recognition agent: The speech recognition
agent used in QuickSet employs either IBM&apos;s VoiceType
Application Factory or VoiceType 3.0 recognizers. The
recognizers use an HMM-based continuous speaker-
independent speech recognition technology for PC&apos;s under
Windows 95/NT. Currently, the system has a vocabulary of
450 words. It produces a single most likely interpretation
of an utterance.
Gesture recognition agent: OGI&apos;s gesture recognition
agent processes all pen input from a PC screen or tablet.
The agent weights the results of both HMM and neural net
recognizers, producing a combined score for each of the
possible recognition results. Currently, 45 gestures can be
recognized, resulting in the creation of 21 military symbols,
irregular shapes, and various types of lines.
Natural language agent: The natural language agent
currently employs a definite clause grammar and produces
typed feature structures as a representation of the utterance
meaning. Currently, for this task, the language consists of
noun phrases that label entities, as well as a variety of
imperative constructs for supplying behavior.
Multimodal integration agent: The multimodal
interpretation agent accepts typed feature structure meaning
representations from the language and gesture recognition
agents, and produces a unified multimodal interpretation.
</bodyText>
<page confidence="0.995764">
21
</page>
<figure confidence="0.40414">
QuickSet Brokered Architecture
</figure>
<figureCaption confidence="0.992344333333333">
Figure 3: A blackboard is used by a facilitator
agent, who routes queries to appropriate agents for
solution.
</figureCaption>
<bodyText confidence="0.988013315789474">
Simulation agent: The simulation agent, developed
primarily by SRI International, but modified by us for
multimodal interaction, serves as the communication
channel between the OAA-brokered agents and the ModSAF
simulation system. This agent offers an API for ModSAF
that other agents can use.
Web display agent: The Web display agent can be used
to create entities, points, lines, and areas. It posts queries
for updates to the state of the simulation via Java code that
interacts with the blackboard and facilitator. The queries are
routed to the running ModSAF simulation, and the
available entities can be viewed over a WWW connection
using a suitable browser.
Other user interfaces: When another user interface
connected to the facilitator subscribes to and produces the
same set of events as others, it immediately becomes part of
a collaboration. One can view this as human-human
collaboration mediated by the agent architecture, or as agent-
agent collaboration.
CommandVu agent: Since the CommandVu virtual
reality system is an agent, the same multimodal interface on
the handheld PC can be used to create entities and to fly the
user through the 3-D terrain. For example, the user can ask
&amp;quot;CommandVu, fly me to this platoon &lt;gesture on the
map&gt;.&amp;quot;
Application bridge agent: The bridge agent
generalizes the underlying applications&apos; API to typed feature
structures, thereby providing an interface to the various
applications such as ModSAF, ConunandVu, and Exinit.
This allows for a domain-independent integration
architecture in which constraints on multimodal
interpretation are stated in terms of higher-level constructs
such as typed feature structures, greatly facilitating reuse.
CORBA bridge agent: This agent converts OAA
messages to COREA IDL (Interface Definition Language)
for the Exercise Initialization project.
More detail on the architecture and the individual agents are
provided in [12, 22].
</bodyText>
<sectionHeader confidence="0.989989" genericHeader="method">
5. EXAMPLE
</sectionHeader>
<bodyText confidence="0.999632714285714">
Holding QuickSet in hand, the user views a map from the
ModSAF simulation, and with spoken language coupled
with pen gestures, issues commands to ModSAF. In order
to create a unit in QuickSet, the user would hold the pen at
the desired location and utter (for instance): &amp;quot;red T72
platoon&amp;quot; resulting in a new platoon of the specified type
being created at that location.
</bodyText>
<figureCaption confidence="0.874419">
Figure 4: The QuickSet interface as the user
</figureCaption>
<bodyText confidence="0.9559074375">
establishes two platoons, a barbed-wire fence, a
breached minefield, and then issues a command to one
platoon to follow a traced route.
The user then adds a barbed-wire fence to the simulation by
drawing a line at the desired location while uttering &amp;quot;barbed
wire.&amp;quot; Similarly a fortified line is added. A minefield of an
amorphous shape is drawn and is labeled verbally, and
finally an M IA1 platoon is created as above. Then the user
can assign a task to the new platoon by saying &amp;quot;M1A1
platoon follow this route&amp;quot; while drawing the route with the
pen. The results of these commands are visible on the
QuickSet screen, as seen in Figure 4, in the ModSAF
simulation, and in the CommandVu 3D rendering of the
scene. In addition to multimodal input, unimodal spoken
language and gestural commands can be given at any time,
depending on the user&apos;s task and preference.
</bodyText>
<sectionHeader confidence="0.998361" genericHeader="method">
6. MULTIMODAL INTEGRATION
</sectionHeader>
<bodyText confidence="0.999831615384615">
Since any unimodal recognizer will make mistakes, the
output of the gesture recognizer is not accepted as a simple
unilateral decision. Instead the recognizer produces a set of
probabilities, one for each possible interpretation of the
gesture. The recognized entities, as well as their
recognition probabilities, are sent to the facilitator, which
forwards them to the multimodal interpretation agent. In
combining the meanings of the gestural and spoken
interpretations, we attempt to satisfy an important design
consideration, namely that the communicative modalities
should compensate for each other&apos;s weaknesses [7, 16].
This is accomplished by selecting the highest scoring
unified interpretation of speech and gesture. Importantly,
</bodyText>
<page confidence="0.991807">
22
</page>
<bodyText confidence="0.999689">
the unified interpretation might not include the highest
scoring gestural (or spoken language) interpretation because
it might not be semantically compatible with the other
mode. The key to this interpretation process is the use of a
typed feature structure [1, 3] as a meaning representation
language that is common to the natural language and
gestural interpretation agents. Johnston et al. [12] present
the details of multimodal integration of continuous speech
and pen-based gesture, guided by research in users&apos;
multimodal integration and synchronization strategies [19].
Unlike many previous approaches to multimodal integration
(e.g, [2, 9, 12, 15, 25]) speech is not &amp;quot;in charge,&amp;quot; in the
sense of relegating gesture a secondary and dependent role.
This mutually-compensatory interpretation process is
capable of analyzing multimodal constructions, as well as
speech-only and pen-only constructions when they occur.
Vo and Wood&apos;s system [24] is similar to the one reported
here, though we believe the use of typed feature structures
provides a more generally usable and formal integration
mechanism than their frame-merging strategy. Cheyer and
Julia [4] sketch a system based on Oviatt&apos;s [17] results and
the OAA [8], but do not discuss the integration strategy nor
multimodal compensation.
</bodyText>
<sectionHeader confidence="0.965598" genericHeader="conclusions">
7. CONCLUDING REMARKS
</sectionHeader>
<bodyText confidence="0.993702787234043">
QuickSet has been delivered to the US Navy (NRaD) and
US Marine Corps. for use at 29 Palms, California, where it
is primarily used to set up training scenarios and to control
the virtual environment. It is also installed at NRaD&apos;s
Command Center of the Future. The system was used by
the US Army&apos;s 82nd Airborne Corps. at Ft. Bragg during
the Royal Dragon Exercise. There, QuickSet was deployed
in a tent, where it was subjected to an extreme noise
environment, including explosions, low-flying jet aircraft,
generators, and the like. Not surprisingly, spoken
interaction with QuickSet was not feasible, although users
gestured successfully. Instead, users wanted to gesture.
Although we had provided a multimodal interface for use in
less hostile conditions, nevertheless we needed to
provide,and in fact have provided, a complete overlap in
functionality, such that any task can be accomplished just
with pen or just with speech when necessary. Finally,
QuickSet is now being extended for use in the ExInit
simulation initialization system for DARPA&apos;s STOW-97
Advanced Concept Demonstration that is intended for
creation of division-sized exercises.
Regarding the multimodal interface itself, QuickSet has
undergone a &amp;quot;proactive&amp;quot; interface evaluation in that the
studies that were performed in advance of building the
system predicted the utility of multimodal over unimodal
speech as an input to map-based systems [17, 18]. In
particular, it was discovered in this research that multimodal
interaction generates simpler language than unimodal
spoken commands to maps. For example, to create a
&amp;quot;phase line&amp;quot; between two three-digit &lt;x,y&gt; grid coordinates,
a user would have to say: &amp;quot;create a line from nine four three
nine six one to nine five seven nine six eight and call it
phase line green&amp;quot; [14]. In contrast, a QuickSet user would
say &amp;quot;phase line green&amp;quot; while drawing a line. Creation of area
features with unimodal speech would be more complex still,
if not infeasible. Given that numerous difficult-to-process
linguistic phenomena (such as utterance disfluencies) are
known to be elevated in lengthy utterances, and also to be
elevated when people speak locative constituents [17, 18],
multimodal interaction that permits pen input to specify
locations and that results in brevity offers the possibility of
more robust recognition.
Further development of QuickSet&apos;s spoken, gestural, and
multimodal integration capabilites are continuing. Research
is also ongoing to examine and quantify the benefits of
multimodal interaction in general, and our architecture in
particular.
</bodyText>
<sectionHeader confidence="0.974827" genericHeader="acknowledgments">
ACKNOWLEDGMENTS
</sectionHeader>
<bodyText confidence="0.982742857142857">
This work is supported in part by the Information
Technology and Information Systems offices of DARPA
under contract number DABT63-95-C-007, in part by ONR
grant number N00014-95-1-1164, and has been done in
collaboration with the US Navy&apos;s NCCOSC RDT&amp;E
Division (NRaD), Ascent Technologies, Mitre Corp., MRJ
Corp., and SRI International.
</bodyText>
<sectionHeader confidence="0.99139" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999158103448276">
1. Calder, J. Typed unification for natural language
processing. In E. Klein and J. van Benthem (Eds.),
Categories, Polymorphisms, and Unification. Centre
for Cognitive Science, University of Edinburgh,
Edinburgh, 1987, 65-72.
2. Brison, E. and N. Vigouroux. (unpublished ms.).
Multimodal references: A generic fusion process.
URIT-URA CNRS. Universitd Paul Sabatier,
Toulouse, France.
3. Carpenter, R. The logic of typed feature structures.
Cambridge University Press, Cambridge, 1992.
4. Cheyer, A., and L. Julia. Multimodal maps: An agent-
based approach. International Conference on
Cooperative Multimodal Communication (CMC/95),
May 1995. Eindhoven, The Netherlands, 1995, 24-26.
5. Clarkson, J. D., and Yi., J., LeatherNet: A synthetic
forces tactical training system for the USMC
commander. Proceedings of the Sixth Conference on
Computer Generated Forces and Behavioral
Representation. Institute for simulation and training.
Technical Report IST-TR-96-18, 1996, 275-281.
6. Cohen, P. R. Integrated Interfaces for Decision Support
with Simulation, Proceedings of the Winter Simulation
Conference, Nelson, B. and Kelton, W. D. and Clark,
G. M., (eds.), ACM, New York, December, 1991,
1066-1072.
7. Cohen, P. R. The Role of Natural Language in a
Multimodal Interface. Proceedings of UIST&apos;92, ACM
Press, New York, 1992, 143-149.
</reference>
<page confidence="0.988018">
23
</page>
<reference confidence="0.999763176470589">
8. Cohen, P.R., Cheyer, A., Wang, M., and Baeg, S.C.
An Open Agent Architecture. Working notes of the
AAAI Spring Symposium Series on Software Agents
Stanford Univ., CA, March, 1994, 1-8.
9. Cohen, P. R., Dalrymple, M., Moran, D.B., Pereira,
F. C. N., Sullivan, J. W., Gargan, R. A., Schlossberg,
J. L., and Tyler, S.W. Synergistic Use of Direct
Manipulation and Natural Language, Human Factors
in Computing Systems: CH189 Conference
Proceedings, ACM, Addison Wesley Publishing Co
New York, 227-234, 1989.
10. Courtemanche, A.J. and Ceranowicz, A. ModSAF
Development Status. Proceedings of the Fifth
Conference on Computer Generated Forces old
Behavioral Representation, Univ. Central Florida,
Orlando, 1995, 3-13.
11. Cruz-Neira, C. D.J. Sandin, T.A. DeFanti, &amp;quot;Surround-
Screen Projection-Based Virtual Reality: The Design
and Implementation of the CAVE,&amp;quot; Computer Graphics
(Proceedings of SIGGRAH&apos;93), ACM SIGGRAPH,
August 1993, 135-142.
12. Johnston, M., Cohen, P. R., McGee, D., Oviatt, S.
L., Pittman, J., and Smith, I.. Unification-based
multimodal integration, in submission.
13. Koons, D.B., C.J. Sparrell and K.R. Thorisson. 1993.
Integrating simultaneous input from speech, gaze, and
hand gestures. In Mark T. Maybury (ed.) Intelligent
Multimedia Interfaces. AAAI Press/ MIT Press,
Cambridge, MA, 257-276.
14. Moore, R., Dowding, J. Bratt, H. Gawron, J. M., and
Cheyer, A., CommandTalk: A Spoken-Language
Interface for Battlefield Simulations, 1997, (this
volume).
15. Neal, J.G. and Shapiro, S.C. Intelligent multi-media
interface technology. In J.W. Sullivan and S.W. Tyler,
editors, Intelligent User Interfaces, chapter 3, pages 45-
68. ACM Press Frontier Series, Addison Wesley
Publishing Co., New York, New York, 1991.
16. Oviatt, S. L., Pen/Voice: Complementary multimodal
communication, Proceedings of SpeechTech&apos;92, New
York, February, 1992, 238-241.
17. Oviatt, S.L. Multimodal interfaces for dynamic
interactive maps. Proceedings of CHI&apos;96 Human
Factors in Computing Systems (April 13-18,
Vancouver, Canada), ACM Press, NY, 1996, 95-102.
18. Oviatt, S. L., Multimodal interactive maps: Designing
for human performance, Human-Computer Interaction,
in press.
19. Oviatt, S. L, A. DeAngeli, and K. Kuhn. In press.
Integration and synchronization of input modes during
multimodal human-computer interaction. Proceedings
of the Conference on Human Factors in Computing
Systems (CHI &apos;97), ACM Press, New York.
20. Oviatt, S. L., Cohen, P. R, Fong, M. W. and Frank,
M. P., A rapid semi-automatic simulation technique for
interactive speech and handwriting, Proceedings of the
1992 International Conference Spoken Language
Processing, vol. 2, University of Alberta, J. Ohala
(ed.), October, 1992, 1351-1354.
21. Oviatt, S. L., Cohen, P. R., Wang, M. Q.,Toward
interface design for human language technology:
Modality and structure as determinants of linguistic
complexity, Speech Communication, 15 (3-4), 1994.
22. Pittman, J.A., Smith, I.A., Cohen, P.R., Oviatt, S.L.,
and Yang, T.C. QuickSet: A Multimodal Interface for
Military Simulation. in Proceedings of the Sixth
Conference on Computer-Generated Forces and
Behavioral Representation, Orlando, Florida, 1996.
23. Thorpe, J. A., The new technology of large scale
simulator networking: Implications for mastering the
art of warfighting. Proceedings of the 9th
Interservice/industry Training Systems Conference,
Orlando, Florida, December, 1987, 492-501.
24. Vo, M. T. and C. Wood. Building an application
framework for speech and pen input integration in
multimodal learning interfaces. International
Conference on Acoustics, Speech, and Signal
Processing, Atlanta, GA, 1996.
25. Wauchope, K. Eucalyptus: Integrating natural language
input with a graphical user interface. Naval Research
Laboratory, Report NRL/FR/5510--94-9711, 1994.
26. Zyda, M. J., Pratt, D. R., Monahan, J. G., and
Wilson, K. P., NPSNET: Constructing a 3-D virtual
world, Proceedings of the 1992 Symposium on
Interactive 3-D Graphics, March, 1992.
</reference>
<page confidence="0.999179">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.921087">
<title confidence="0.9998205">QuickSet: Multimodal Interaction for Simulation Set-up and Control</title>
<author confidence="0.998735">Philip R Cohen</author>
<author confidence="0.998735">Michael Johnston</author>
<author confidence="0.998735">David McGee</author>
<author confidence="0.998735">Sharon Oviatt</author>
<author confidence="0.998735">Jay Pittman</author>
<author confidence="0.998735">Ira Smith</author>
<author confidence="0.998735">Liang Chen</author>
<author confidence="0.998735">Josh Clow</author>
<affiliation confidence="0.9996135">Center for Human Computer Communication Oregon Graduate Institute of Science and Technology</affiliation>
<address confidence="0.9996055">P.O.Box 91000 Portland, OR 97291-1000 USA</address>
<phone confidence="0.998628">Tel: 1-503-690-1326</phone>
<email confidence="0.999979">E-mail:pcohen@cse.ogi.edu</email>
<web confidence="0.998101">http://www.cse.ogi.edu/CHCC</web>
<abstract confidence="0.992144384615384">This paper presents a novel multimodal system applied to the setup and control of distributed interactive simulations. We have developed the QuickSet prototype, a pen/voice system running on a hand-held PC, communicating through a distributed agent architecture to NRaD&apos;s&apos; LeatherNet system, a distributed interactive training simulator built for the US Marine Corps (USMC). The paper briefly describes the system and illustrates its use in multimodal simulation setup. interfaces, agent architecture, gesture recognition, speech recognition, natural language processing, distributed interactive simulation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Calder</author>
</authors>
<title>Typed unification for natural language processing. In</title>
<date>1987</date>
<pages>65--72</pages>
<institution>and Unification. Centre for Cognitive Science, University of Edinburgh,</institution>
<location>Edinburgh,</location>
<contexts>
<context position="12946" citStr="[1, 3]" startWordPosition="1931" endWordPosition="1932">t. In combining the meanings of the gestural and spoken interpretations, we attempt to satisfy an important design consideration, namely that the communicative modalities should compensate for each other&apos;s weaknesses [7, 16]. This is accomplished by selecting the highest scoring unified interpretation of speech and gesture. Importantly, 22 the unified interpretation might not include the highest scoring gestural (or spoken language) interpretation because it might not be semantically compatible with the other mode. The key to this interpretation process is the use of a typed feature structure [1, 3] as a meaning representation language that is common to the natural language and gestural interpretation agents. Johnston et al. [12] present the details of multimodal integration of continuous speech and pen-based gesture, guided by research in users&apos; multimodal integration and synchronization strategies [19]. Unlike many previous approaches to multimodal integration (e.g, [2, 9, 12, 15, 25]) speech is not &amp;quot;in charge,&amp;quot; in the sense of relegating gesture a secondary and dependent role. This mutually-compensatory interpretation process is capable of analyzing multimodal constructions, as well a</context>
</contexts>
<marker>1.</marker>
<rawString>Calder, J. Typed unification for natural language processing. In E. Klein and J. van Benthem (Eds.), Categories, Polymorphisms, and Unification. Centre for Cognitive Science, University of Edinburgh, Edinburgh, 1987, 65-72.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E Brison</author>
<author>N Vigouroux</author>
</authors>
<title>(unpublished ms.). Multimodal references: A generic fusion process. URIT-URA CNRS. Universitd Paul Sabatier,</title>
<location>Toulouse, France.</location>
<contexts>
<context position="13341" citStr="[2, 9, 12, 15, 25]" startWordPosition="1984" endWordPosition="1988">ude the highest scoring gestural (or spoken language) interpretation because it might not be semantically compatible with the other mode. The key to this interpretation process is the use of a typed feature structure [1, 3] as a meaning representation language that is common to the natural language and gestural interpretation agents. Johnston et al. [12] present the details of multimodal integration of continuous speech and pen-based gesture, guided by research in users&apos; multimodal integration and synchronization strategies [19]. Unlike many previous approaches to multimodal integration (e.g, [2, 9, 12, 15, 25]) speech is not &amp;quot;in charge,&amp;quot; in the sense of relegating gesture a secondary and dependent role. This mutually-compensatory interpretation process is capable of analyzing multimodal constructions, as well as speech-only and pen-only constructions when they occur. Vo and Wood&apos;s system [24] is similar to the one reported here, though we believe the use of typed feature structures provides a more generally usable and formal integration mechanism than their frame-merging strategy. Cheyer and Julia [4] sketch a system based on Oviatt&apos;s [17] results and the OAA [8], but do not discuss the integration</context>
</contexts>
<marker>2.</marker>
<rawString>Brison, E. and N. Vigouroux. (unpublished ms.). Multimodal references: A generic fusion process. URIT-URA CNRS. Universitd Paul Sabatier, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Carpenter</author>
</authors>
<title>The logic of typed feature structures.</title>
<date>1992</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge,</location>
<contexts>
<context position="12946" citStr="[1, 3]" startWordPosition="1931" endWordPosition="1932">t. In combining the meanings of the gestural and spoken interpretations, we attempt to satisfy an important design consideration, namely that the communicative modalities should compensate for each other&apos;s weaknesses [7, 16]. This is accomplished by selecting the highest scoring unified interpretation of speech and gesture. Importantly, 22 the unified interpretation might not include the highest scoring gestural (or spoken language) interpretation because it might not be semantically compatible with the other mode. The key to this interpretation process is the use of a typed feature structure [1, 3] as a meaning representation language that is common to the natural language and gestural interpretation agents. Johnston et al. [12] present the details of multimodal integration of continuous speech and pen-based gesture, guided by research in users&apos; multimodal integration and synchronization strategies [19]. Unlike many previous approaches to multimodal integration (e.g, [2, 9, 12, 15, 25]) speech is not &amp;quot;in charge,&amp;quot; in the sense of relegating gesture a secondary and dependent role. This mutually-compensatory interpretation process is capable of analyzing multimodal constructions, as well a</context>
</contexts>
<marker>3.</marker>
<rawString>Carpenter, R. The logic of typed feature structures. Cambridge University Press, Cambridge, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cheyer</author>
<author>L Julia</author>
</authors>
<title>Multimodal maps: An agentbased approach.</title>
<date>1995</date>
<booktitle>International Conference on Cooperative Multimodal Communication (CMC/95),</booktitle>
<pages>24--26</pages>
<location>Eindhoven, The</location>
<contexts>
<context position="13842" citStr="[4]" startWordPosition="2062" endWordPosition="2062">on strategies [19]. Unlike many previous approaches to multimodal integration (e.g, [2, 9, 12, 15, 25]) speech is not &amp;quot;in charge,&amp;quot; in the sense of relegating gesture a secondary and dependent role. This mutually-compensatory interpretation process is capable of analyzing multimodal constructions, as well as speech-only and pen-only constructions when they occur. Vo and Wood&apos;s system [24] is similar to the one reported here, though we believe the use of typed feature structures provides a more generally usable and formal integration mechanism than their frame-merging strategy. Cheyer and Julia [4] sketch a system based on Oviatt&apos;s [17] results and the OAA [8], but do not discuss the integration strategy nor multimodal compensation. 7. CONCLUDING REMARKS QuickSet has been delivered to the US Navy (NRaD) and US Marine Corps. for use at 29 Palms, California, where it is primarily used to set up training scenarios and to control the virtual environment. It is also installed at NRaD&apos;s Command Center of the Future. The system was used by the US Army&apos;s 82nd Airborne Corps. at Ft. Bragg during the Royal Dragon Exercise. There, QuickSet was deployed in a tent, where it was subjected to an extre</context>
</contexts>
<marker>4.</marker>
<rawString>Cheyer, A., and L. Julia. Multimodal maps: An agentbased approach. International Conference on Cooperative Multimodal Communication (CMC/95), May 1995. Eindhoven, The Netherlands, 1995, 24-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Clarkson</author>
<author>J Yi</author>
</authors>
<title>LeatherNet: A synthetic forces tactical training system for the USMC commander.</title>
<date>1996</date>
<booktitle>Proceedings of the Sixth Conference on Computer Generated Forces and Behavioral Representation. Institute</booktitle>
<tech>Technical Report IST-TR-96-18,</tech>
<pages>275--281</pages>
<contexts>
<context position="3954" citStr="[5]" startWordPosition="562" endWordPosition="562">en language, are apparent [6]. It has become clear, however, that speech-only interaction is not optimal for spatial tasks. Using a high-fidelity &amp;quot;Wizard-of-Oz&amp;quot; methodology [20], recent empirical results demonstrate clear language processing and task performance advantages for multimodal (pen/voice) input over speechonly input for map-based systems [17,18]. 3. OUICKSET To address these simulation interface problems, and motivated by the above results, we have developed QuickSet (see Figure 1) a collaborative, handheld, multimodal system for configuring military simulations based on LeatherNet [5], a system used in training platoon leaders and company commanders at the USMC base at 29 Palms, California. LeatherNet simulations are created using the ModSAF simulator [10] and can be visualized in a CAVE-based virtual reality environment [11, 26] called CommandVu (see Figure 2 â€” QuickSet systems are on the soldiers&apos; tables). In addition to LeatherNet, QuickSet is being used in a second effort called ExInit (Exercise 20 Initialization), that will enable users to create division-sized exercises. Because of the use of OAA, QuickSet can interoperate with agents from CommandTalk [14], which pro</context>
</contexts>
<marker>5.</marker>
<rawString>Clarkson, J. D., and Yi., J., LeatherNet: A synthetic forces tactical training system for the USMC commander. Proceedings of the Sixth Conference on Computer Generated Forces and Behavioral Representation. Institute for simulation and training. Technical Report IST-TR-96-18, 1996, 275-281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
</authors>
<title>Integrated Interfaces for Decision Support with Simulation,</title>
<date>1991</date>
<booktitle>Proceedings of the Winter Simulation Conference,</booktitle>
<pages>1066--1072</pages>
<editor>Nelson, B. and Kelton, W. D. and Clark, G. M., (eds.),</editor>
<publisher>ACM,</publisher>
<location>New York,</location>
<contexts>
<context position="3380" citStr="[6]" startWordPosition="485" endWordPosition="485">ding users of their options, and in minimizing errors [7]. However, GUIs are often weak in supporting interactions with many objects, or objects not on the screen. In contrast, it was argued that linguisticallybased interface technologies offer the potential to describe large sets of objects, which may not all be present on a screen, and can be used to create more complex behaviors through specification of rule invocation conditions. Simulation is one type of application for which these limitations of GUIs, as well as the strengths of natural language, especially spoken language, are apparent [6]. It has become clear, however, that speech-only interaction is not optimal for spatial tasks. Using a high-fidelity &amp;quot;Wizard-of-Oz&amp;quot; methodology [20], recent empirical results demonstrate clear language processing and task performance advantages for multimodal (pen/voice) input over speechonly input for map-based systems [17,18]. 3. OUICKSET To address these simulation interface problems, and motivated by the above results, we have developed QuickSet (see Figure 1) a collaborative, handheld, multimodal system for configuring military simulations based on LeatherNet [5], a system used in trainin</context>
</contexts>
<marker>6.</marker>
<rawString>Cohen, P. R. Integrated Interfaces for Decision Support with Simulation, Proceedings of the Winter Simulation Conference, Nelson, B. and Kelton, W. D. and Clark, G. M., (eds.), ACM, New York, December, 1991, 1066-1072.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
</authors>
<title>The Role of Natural Language in a Multimodal Interface.</title>
<date>1992</date>
<booktitle>Proceedings of UIST&apos;92,</booktitle>
<pages>143--149</pages>
<publisher>ACM Press,</publisher>
<location>New York,</location>
<contexts>
<context position="2834" citStr="[7]" startWordPosition="399" endWordPosition="399"> = US Navy Command and Control Ocean Systems Center Research Development Test and Evaluation (San Diego). dramatically, while at the same time, for reasons of mobility and affordability, desiring that simulations should be creatable from small devices (e.g., PDAs). This impending collision of trends for smaller screen size and for more entities requires a different paradigm for humancomputer interaction. We have argued generically that GUI technologies offer advantages in allowing users to manipulate objects that are on the screen, in reminding users of their options, and in minimizing errors [7]. However, GUIs are often weak in supporting interactions with many objects, or objects not on the screen. In contrast, it was argued that linguisticallybased interface technologies offer the potential to describe large sets of objects, which may not all be present on a screen, and can be used to create more complex behaviors through specification of rule invocation conditions. Simulation is one type of application for which these limitations of GUIs, as well as the strengths of natural language, especially spoken language, are apparent [6]. It has become clear, however, that speech-only inter</context>
<context position="12564" citStr="[7, 16]" startWordPosition="1874" endWordPosition="1875">l recognizer will make mistakes, the output of the gesture recognizer is not accepted as a simple unilateral decision. Instead the recognizer produces a set of probabilities, one for each possible interpretation of the gesture. The recognized entities, as well as their recognition probabilities, are sent to the facilitator, which forwards them to the multimodal interpretation agent. In combining the meanings of the gestural and spoken interpretations, we attempt to satisfy an important design consideration, namely that the communicative modalities should compensate for each other&apos;s weaknesses [7, 16]. This is accomplished by selecting the highest scoring unified interpretation of speech and gesture. Importantly, 22 the unified interpretation might not include the highest scoring gestural (or spoken language) interpretation because it might not be semantically compatible with the other mode. The key to this interpretation process is the use of a typed feature structure [1, 3] as a meaning representation language that is common to the natural language and gestural interpretation agents. Johnston et al. [12] present the details of multimodal integration of continuous speech and pen-based ges</context>
</contexts>
<marker>7.</marker>
<rawString>Cohen, P. R. The Role of Natural Language in a Multimodal Interface. Proceedings of UIST&apos;92, ACM Press, New York, 1992, 143-149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
<author>A Cheyer</author>
<author>M Wang</author>
<author>S C Baeg</author>
</authors>
<title>An Open Agent Architecture. Working notes of the AAAI Spring</title>
<date>1994</date>
<booktitle>Symposium Series on Software Agents</booktitle>
<pages>1--8</pages>
<location>Stanford Univ., CA,</location>
<contexts>
<context position="4899" citStr="[8]" startWordPosition="705" endWordPosition="705"> to LeatherNet, QuickSet is being used in a second effort called ExInit (Exercise 20 Initialization), that will enable users to create division-sized exercises. Because of the use of OAA, QuickSet can interoperate with agents from CommandTalk [14], which provides a speech-only interface to ModSAF. QuickSet runs on both desktop and hand-held PC&apos;s, communicating over wired and wireless LAN&apos;s, or modem links. The system combines speech and pen-based gesture input on multiple 3-lb hand-held PCs (Fujitsu Stylistic 1000), which communicate via wireless LAN through the Open Agent Architecture (OAA)2 [8], to ModSAF, and also to CommandVu. With this highly portable device, a user can create entities, establish &amp;quot;control measures&amp;quot; (e.g., objectives, checkpoints, etc.), draw and label various lines and areas, (e.g., landing zones) and give the entities behavior. Figure 1: QuickSet running on a wireless handheld PC. In the remainder of the paper, we illustrate the system briefly, describe its components, and discuss its application. Figure 2: Artist&apos;s rendition of QuickSet used with CommandVu virtual display of distributed interactive simulation. 4. SYSTEM ARCHITECTURE Architecturally, QuickSet us</context>
<context position="13905" citStr="[8]" startWordPosition="2074" endWordPosition="2074">al integration (e.g, [2, 9, 12, 15, 25]) speech is not &amp;quot;in charge,&amp;quot; in the sense of relegating gesture a secondary and dependent role. This mutually-compensatory interpretation process is capable of analyzing multimodal constructions, as well as speech-only and pen-only constructions when they occur. Vo and Wood&apos;s system [24] is similar to the one reported here, though we believe the use of typed feature structures provides a more generally usable and formal integration mechanism than their frame-merging strategy. Cheyer and Julia [4] sketch a system based on Oviatt&apos;s [17] results and the OAA [8], but do not discuss the integration strategy nor multimodal compensation. 7. CONCLUDING REMARKS QuickSet has been delivered to the US Navy (NRaD) and US Marine Corps. for use at 29 Palms, California, where it is primarily used to set up training scenarios and to control the virtual environment. It is also installed at NRaD&apos;s Command Center of the Future. The system was used by the US Army&apos;s 82nd Airborne Corps. at Ft. Bragg during the Royal Dragon Exercise. There, QuickSet was deployed in a tent, where it was subjected to an extreme noise environment, including explosions, low-flying jet airc</context>
</contexts>
<marker>8.</marker>
<rawString>Cohen, P.R., Cheyer, A., Wang, M., and Baeg, S.C. An Open Agent Architecture. Working notes of the AAAI Spring Symposium Series on Software Agents Stanford Univ., CA, March, 1994, 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
<author>M Dalrymple</author>
<author>D B Moran</author>
<author>F C N Pereira</author>
<author>J W Sullivan</author>
<author>R A Gargan</author>
<author>J L Schlossberg</author>
<author>S W Tyler</author>
</authors>
<date>1989</date>
<booktitle>Synergistic Use of Direct Manipulation and Natural Language, Human Factors in Computing Systems: CH189 Conference Proceedings,</booktitle>
<pages>227--234</pages>
<publisher>ACM, Addison Wesley Publishing Co</publisher>
<location>New York,</location>
<contexts>
<context position="13341" citStr="[2, 9, 12, 15, 25]" startWordPosition="1984" endWordPosition="1988">ude the highest scoring gestural (or spoken language) interpretation because it might not be semantically compatible with the other mode. The key to this interpretation process is the use of a typed feature structure [1, 3] as a meaning representation language that is common to the natural language and gestural interpretation agents. Johnston et al. [12] present the details of multimodal integration of continuous speech and pen-based gesture, guided by research in users&apos; multimodal integration and synchronization strategies [19]. Unlike many previous approaches to multimodal integration (e.g, [2, 9, 12, 15, 25]) speech is not &amp;quot;in charge,&amp;quot; in the sense of relegating gesture a secondary and dependent role. This mutually-compensatory interpretation process is capable of analyzing multimodal constructions, as well as speech-only and pen-only constructions when they occur. Vo and Wood&apos;s system [24] is similar to the one reported here, though we believe the use of typed feature structures provides a more generally usable and formal integration mechanism than their frame-merging strategy. Cheyer and Julia [4] sketch a system based on Oviatt&apos;s [17] results and the OAA [8], but do not discuss the integration</context>
</contexts>
<marker>9.</marker>
<rawString>Cohen, P. R., Dalrymple, M., Moran, D.B., Pereira, F. C. N., Sullivan, J. W., Gargan, R. A., Schlossberg, J. L., and Tyler, S.W. Synergistic Use of Direct Manipulation and Natural Language, Human Factors in Computing Systems: CH189 Conference Proceedings, ACM, Addison Wesley Publishing Co New York, 227-234, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Courtemanche</author>
<author>A Ceranowicz</author>
</authors>
<title>ModSAF Development Status.</title>
<date>1995</date>
<booktitle>Proceedings of the Fifth Conference on Computer Generated Forces old Behavioral Representation,</booktitle>
<pages>3--13</pages>
<location>Univ. Central Florida, Orlando,</location>
<contexts>
<context position="4129" citStr="[10]" startWordPosition="589" endWordPosition="589">20], recent empirical results demonstrate clear language processing and task performance advantages for multimodal (pen/voice) input over speechonly input for map-based systems [17,18]. 3. OUICKSET To address these simulation interface problems, and motivated by the above results, we have developed QuickSet (see Figure 1) a collaborative, handheld, multimodal system for configuring military simulations based on LeatherNet [5], a system used in training platoon leaders and company commanders at the USMC base at 29 Palms, California. LeatherNet simulations are created using the ModSAF simulator [10] and can be visualized in a CAVE-based virtual reality environment [11, 26] called CommandVu (see Figure 2 â€” QuickSet systems are on the soldiers&apos; tables). In addition to LeatherNet, QuickSet is being used in a second effort called ExInit (Exercise 20 Initialization), that will enable users to create division-sized exercises. Because of the use of OAA, QuickSet can interoperate with agents from CommandTalk [14], which provides a speech-only interface to ModSAF. QuickSet runs on both desktop and hand-held PC&apos;s, communicating over wired and wireless LAN&apos;s, or modem links. The system combines spe</context>
</contexts>
<marker>10.</marker>
<rawString>Courtemanche, A.J. and Ceranowicz, A. ModSAF Development Status. Proceedings of the Fifth Conference on Computer Generated Forces old Behavioral Representation, Univ. Central Florida, Orlando, 1995, 3-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D J Sandin Cruz-Neira</author>
<author>T A DeFanti</author>
</authors>
<title>SurroundScreen Projection-Based Virtual Reality: The Design and Implementation of the CAVE,&amp;quot;</title>
<date>1993</date>
<booktitle>Computer Graphics (Proceedings of SIGGRAH&apos;93), ACM SIGGRAPH,</booktitle>
<pages>135--142</pages>
<contexts>
<context position="4204" citStr="[11, 26]" startWordPosition="600" endWordPosition="601"> task performance advantages for multimodal (pen/voice) input over speechonly input for map-based systems [17,18]. 3. OUICKSET To address these simulation interface problems, and motivated by the above results, we have developed QuickSet (see Figure 1) a collaborative, handheld, multimodal system for configuring military simulations based on LeatherNet [5], a system used in training platoon leaders and company commanders at the USMC base at 29 Palms, California. LeatherNet simulations are created using the ModSAF simulator [10] and can be visualized in a CAVE-based virtual reality environment [11, 26] called CommandVu (see Figure 2 â€” QuickSet systems are on the soldiers&apos; tables). In addition to LeatherNet, QuickSet is being used in a second effort called ExInit (Exercise 20 Initialization), that will enable users to create division-sized exercises. Because of the use of OAA, QuickSet can interoperate with agents from CommandTalk [14], which provides a speech-only interface to ModSAF. QuickSet runs on both desktop and hand-held PC&apos;s, communicating over wired and wireless LAN&apos;s, or modem links. The system combines speech and pen-based gesture input on multiple 3-lb hand-held PCs (Fujitsu Sty</context>
</contexts>
<marker>11.</marker>
<rawString>Cruz-Neira, C. D.J. Sandin, T.A. DeFanti, &amp;quot;SurroundScreen Projection-Based Virtual Reality: The Design and Implementation of the CAVE,&amp;quot; Computer Graphics (Proceedings of SIGGRAH&apos;93), ACM SIGGRAPH, August 1993, 135-142.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Johnston</author>
<author>P R Cohen</author>
<author>D McGee</author>
<author>S L Oviatt</author>
<author>J Pittman</author>
<author>I Smith</author>
</authors>
<title>Unification-based multimodal integration, in submission.</title>
<contexts>
<context position="10622" citStr="[12, 22]" startWordPosition="1557" endWordPosition="1558">izes the underlying applications&apos; API to typed feature structures, thereby providing an interface to the various applications such as ModSAF, ConunandVu, and Exinit. This allows for a domain-independent integration architecture in which constraints on multimodal interpretation are stated in terms of higher-level constructs such as typed feature structures, greatly facilitating reuse. CORBA bridge agent: This agent converts OAA messages to COREA IDL (Interface Definition Language) for the Exercise Initialization project. More detail on the architecture and the individual agents are provided in [12, 22]. 5. EXAMPLE Holding QuickSet in hand, the user views a map from the ModSAF simulation, and with spoken language coupled with pen gestures, issues commands to ModSAF. In order to create a unit in QuickSet, the user would hold the pen at the desired location and utter (for instance): &amp;quot;red T72 platoon&amp;quot; resulting in a new platoon of the specified type being created at that location. Figure 4: The QuickSet interface as the user establishes two platoons, a barbed-wire fence, a breached minefield, and then issues a command to one platoon to follow a traced route. The user then adds a barbed-wire fen</context>
<context position="13079" citStr="[12]" startWordPosition="1952" endWordPosition="1952">y that the communicative modalities should compensate for each other&apos;s weaknesses [7, 16]. This is accomplished by selecting the highest scoring unified interpretation of speech and gesture. Importantly, 22 the unified interpretation might not include the highest scoring gestural (or spoken language) interpretation because it might not be semantically compatible with the other mode. The key to this interpretation process is the use of a typed feature structure [1, 3] as a meaning representation language that is common to the natural language and gestural interpretation agents. Johnston et al. [12] present the details of multimodal integration of continuous speech and pen-based gesture, guided by research in users&apos; multimodal integration and synchronization strategies [19]. Unlike many previous approaches to multimodal integration (e.g, [2, 9, 12, 15, 25]) speech is not &amp;quot;in charge,&amp;quot; in the sense of relegating gesture a secondary and dependent role. This mutually-compensatory interpretation process is capable of analyzing multimodal constructions, as well as speech-only and pen-only constructions when they occur. Vo and Wood&apos;s system [24] is similar to the one reported here, though we be</context>
</contexts>
<marker>12.</marker>
<rawString>Johnston, M., Cohen, P. R., McGee, D., Oviatt, S. L., Pittman, J., and Smith, I.. Unification-based multimodal integration, in submission.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D B Koons</author>
<author>C J Sparrell</author>
<author>K R Thorisson</author>
</authors>
<title>Integrating simultaneous input from speech, gaze, and hand gestures.</title>
<date>1993</date>
<pages>257--276</pages>
<editor>In Mark T. Maybury (ed.)</editor>
<publisher>AAAI Press/ MIT Press,</publisher>
<location>Cambridge, MA,</location>
<marker>13.</marker>
<rawString>Koons, D.B., C.J. Sparrell and K.R. Thorisson. 1993. Integrating simultaneous input from speech, gaze, and hand gestures. In Mark T. Maybury (ed.) Intelligent Multimedia Interfaces. AAAI Press/ MIT Press, Cambridge, MA, 257-276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
<author>J Bratt Dowding</author>
<author>H Gawron</author>
<author>J M</author>
<author>A Cheyer</author>
</authors>
<title>CommandTalk: A Spoken-Language Interface for Battlefield Simulations,</title>
<date>1997</date>
<contexts>
<context position="4543" citStr="[14]" startWordPosition="653" endWordPosition="653"> LeatherNet [5], a system used in training platoon leaders and company commanders at the USMC base at 29 Palms, California. LeatherNet simulations are created using the ModSAF simulator [10] and can be visualized in a CAVE-based virtual reality environment [11, 26] called CommandVu (see Figure 2 â€” QuickSet systems are on the soldiers&apos; tables). In addition to LeatherNet, QuickSet is being used in a second effort called ExInit (Exercise 20 Initialization), that will enable users to create division-sized exercises. Because of the use of OAA, QuickSet can interoperate with agents from CommandTalk [14], which provides a speech-only interface to ModSAF. QuickSet runs on both desktop and hand-held PC&apos;s, communicating over wired and wireless LAN&apos;s, or modem links. The system combines speech and pen-based gesture input on multiple 3-lb hand-held PCs (Fujitsu Stylistic 1000), which communicate via wireless LAN through the Open Agent Architecture (OAA)2 [8], to ModSAF, and also to CommandVu. With this highly portable device, a user can create entities, establish &amp;quot;control measures&amp;quot; (e.g., objectives, checkpoints, etc.), draw and label various lines and areas, (e.g., landing zones) and give the ent</context>
<context position="15802" citStr="[14]" startWordPosition="2370" endWordPosition="2370">uickSet has undergone a &amp;quot;proactive&amp;quot; interface evaluation in that the studies that were performed in advance of building the system predicted the utility of multimodal over unimodal speech as an input to map-based systems [17, 18]. In particular, it was discovered in this research that multimodal interaction generates simpler language than unimodal spoken commands to maps. For example, to create a &amp;quot;phase line&amp;quot; between two three-digit &lt;x,y&gt; grid coordinates, a user would have to say: &amp;quot;create a line from nine four three nine six one to nine five seven nine six eight and call it phase line green&amp;quot; [14]. In contrast, a QuickSet user would say &amp;quot;phase line green&amp;quot; while drawing a line. Creation of area features with unimodal speech would be more complex still, if not infeasible. Given that numerous difficult-to-process linguistic phenomena (such as utterance disfluencies) are known to be elevated in lengthy utterances, and also to be elevated when people speak locative constituents [17, 18], multimodal interaction that permits pen input to specify locations and that results in brevity offers the possibility of more robust recognition. Further development of QuickSet&apos;s spoken, gestural, and mult</context>
</contexts>
<marker>14.</marker>
<rawString>Moore, R., Dowding, J. Bratt, H. Gawron, J. M., and Cheyer, A., CommandTalk: A Spoken-Language Interface for Battlefield Simulations, 1997, (this volume).</rawString>
</citation>
<citation valid="false">
<authors>
<author>J G Neal</author>
<author>S C Shapiro</author>
</authors>
<title>Intelligent multi-media interface technology.</title>
<booktitle>Intelligent User Interfaces, chapter 3,</booktitle>
<pages>45</pages>
<editor>In J.W. Sullivan and S.W. Tyler, editors,</editor>
<contexts>
<context position="13341" citStr="[2, 9, 12, 15, 25]" startWordPosition="1984" endWordPosition="1988">ude the highest scoring gestural (or spoken language) interpretation because it might not be semantically compatible with the other mode. The key to this interpretation process is the use of a typed feature structure [1, 3] as a meaning representation language that is common to the natural language and gestural interpretation agents. Johnston et al. [12] present the details of multimodal integration of continuous speech and pen-based gesture, guided by research in users&apos; multimodal integration and synchronization strategies [19]. Unlike many previous approaches to multimodal integration (e.g, [2, 9, 12, 15, 25]) speech is not &amp;quot;in charge,&amp;quot; in the sense of relegating gesture a secondary and dependent role. This mutually-compensatory interpretation process is capable of analyzing multimodal constructions, as well as speech-only and pen-only constructions when they occur. Vo and Wood&apos;s system [24] is similar to the one reported here, though we believe the use of typed feature structures provides a more generally usable and formal integration mechanism than their frame-merging strategy. Cheyer and Julia [4] sketch a system based on Oviatt&apos;s [17] results and the OAA [8], but do not discuss the integration</context>
</contexts>
<marker>15.</marker>
<rawString>Neal, J.G. and Shapiro, S.C. Intelligent multi-media interface technology. In J.W. Sullivan and S.W. Tyler, editors, Intelligent User Interfaces, chapter 3, pages 45-</rawString>
</citation>
<citation valid="true">
<title>Frontier Series,</title>
<date>1991</date>
<publisher>ACM Press</publisher>
<location>New York, New York,</location>
<marker>68.</marker>
<rawString>ACM Press Frontier Series, Addison Wesley Publishing Co., New York, New York, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Oviatt</author>
</authors>
<title>Pen/Voice: Complementary multimodal communication,</title>
<date>1992</date>
<booktitle>Proceedings of SpeechTech&apos;92,</booktitle>
<pages>238--241</pages>
<location>New York,</location>
<contexts>
<context position="12564" citStr="[7, 16]" startWordPosition="1874" endWordPosition="1875">l recognizer will make mistakes, the output of the gesture recognizer is not accepted as a simple unilateral decision. Instead the recognizer produces a set of probabilities, one for each possible interpretation of the gesture. The recognized entities, as well as their recognition probabilities, are sent to the facilitator, which forwards them to the multimodal interpretation agent. In combining the meanings of the gestural and spoken interpretations, we attempt to satisfy an important design consideration, namely that the communicative modalities should compensate for each other&apos;s weaknesses [7, 16]. This is accomplished by selecting the highest scoring unified interpretation of speech and gesture. Importantly, 22 the unified interpretation might not include the highest scoring gestural (or spoken language) interpretation because it might not be semantically compatible with the other mode. The key to this interpretation process is the use of a typed feature structure [1, 3] as a meaning representation language that is common to the natural language and gestural interpretation agents. Johnston et al. [12] present the details of multimodal integration of continuous speech and pen-based ges</context>
</contexts>
<marker>16.</marker>
<rawString>Oviatt, S. L., Pen/Voice: Complementary multimodal communication, Proceedings of SpeechTech&apos;92, New York, February, 1992, 238-241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Oviatt</author>
</authors>
<title>Multimodal interfaces for dynamic interactive maps.</title>
<date></date>
<booktitle>Proceedings of CHI&apos;96 Human Factors in Computing Systems</booktitle>
<pages>95--102</pages>
<publisher>ACM Press,</publisher>
<location>Vancouver, Canada),</location>
<contexts>
<context position="3709" citStr="[17,18]" startWordPosition="528" endWordPosition="528"> a screen, and can be used to create more complex behaviors through specification of rule invocation conditions. Simulation is one type of application for which these limitations of GUIs, as well as the strengths of natural language, especially spoken language, are apparent [6]. It has become clear, however, that speech-only interaction is not optimal for spatial tasks. Using a high-fidelity &amp;quot;Wizard-of-Oz&amp;quot; methodology [20], recent empirical results demonstrate clear language processing and task performance advantages for multimodal (pen/voice) input over speechonly input for map-based systems [17,18]. 3. OUICKSET To address these simulation interface problems, and motivated by the above results, we have developed QuickSet (see Figure 1) a collaborative, handheld, multimodal system for configuring military simulations based on LeatherNet [5], a system used in training platoon leaders and company commanders at the USMC base at 29 Palms, California. LeatherNet simulations are created using the ModSAF simulator [10] and can be visualized in a CAVE-based virtual reality environment [11, 26] called CommandVu (see Figure 2 â€” QuickSet systems are on the soldiers&apos; tables). In addition to LeatherNe</context>
<context position="13881" citStr="[17]" startWordPosition="2069" endWordPosition="2069">us approaches to multimodal integration (e.g, [2, 9, 12, 15, 25]) speech is not &amp;quot;in charge,&amp;quot; in the sense of relegating gesture a secondary and dependent role. This mutually-compensatory interpretation process is capable of analyzing multimodal constructions, as well as speech-only and pen-only constructions when they occur. Vo and Wood&apos;s system [24] is similar to the one reported here, though we believe the use of typed feature structures provides a more generally usable and formal integration mechanism than their frame-merging strategy. Cheyer and Julia [4] sketch a system based on Oviatt&apos;s [17] results and the OAA [8], but do not discuss the integration strategy nor multimodal compensation. 7. CONCLUDING REMARKS QuickSet has been delivered to the US Navy (NRaD) and US Marine Corps. for use at 29 Palms, California, where it is primarily used to set up training scenarios and to control the virtual environment. It is also installed at NRaD&apos;s Command Center of the Future. The system was used by the US Army&apos;s 82nd Airborne Corps. at Ft. Bragg during the Royal Dragon Exercise. There, QuickSet was deployed in a tent, where it was subjected to an extreme noise environment, including explosi</context>
<context position="15427" citStr="[17, 18]" startWordPosition="2306" endWordPosition="2307">ed, a complete overlap in functionality, such that any task can be accomplished just with pen or just with speech when necessary. Finally, QuickSet is now being extended for use in the ExInit simulation initialization system for DARPA&apos;s STOW-97 Advanced Concept Demonstration that is intended for creation of division-sized exercises. Regarding the multimodal interface itself, QuickSet has undergone a &amp;quot;proactive&amp;quot; interface evaluation in that the studies that were performed in advance of building the system predicted the utility of multimodal over unimodal speech as an input to map-based systems [17, 18]. In particular, it was discovered in this research that multimodal interaction generates simpler language than unimodal spoken commands to maps. For example, to create a &amp;quot;phase line&amp;quot; between two three-digit &lt;x,y&gt; grid coordinates, a user would have to say: &amp;quot;create a line from nine four three nine six one to nine five seven nine six eight and call it phase line green&amp;quot; [14]. In contrast, a QuickSet user would say &amp;quot;phase line green&amp;quot; while drawing a line. Creation of area features with unimodal speech would be more complex still, if not infeasible. Given that numerous difficult-to-process linguis</context>
</contexts>
<marker>17.</marker>
<rawString>Oviatt, S.L. Multimodal interfaces for dynamic interactive maps. Proceedings of CHI&apos;96 Human Factors in Computing Systems (April 13-18, Vancouver, Canada), ACM Press, NY, 1996, 95-102.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S L Oviatt</author>
</authors>
<title>Multimodal interactive maps: Designing for human performance, Human-Computer Interaction,</title>
<note>in press.</note>
<contexts>
<context position="3709" citStr="[17,18]" startWordPosition="528" endWordPosition="528"> a screen, and can be used to create more complex behaviors through specification of rule invocation conditions. Simulation is one type of application for which these limitations of GUIs, as well as the strengths of natural language, especially spoken language, are apparent [6]. It has become clear, however, that speech-only interaction is not optimal for spatial tasks. Using a high-fidelity &amp;quot;Wizard-of-Oz&amp;quot; methodology [20], recent empirical results demonstrate clear language processing and task performance advantages for multimodal (pen/voice) input over speechonly input for map-based systems [17,18]. 3. OUICKSET To address these simulation interface problems, and motivated by the above results, we have developed QuickSet (see Figure 1) a collaborative, handheld, multimodal system for configuring military simulations based on LeatherNet [5], a system used in training platoon leaders and company commanders at the USMC base at 29 Palms, California. LeatherNet simulations are created using the ModSAF simulator [10] and can be visualized in a CAVE-based virtual reality environment [11, 26] called CommandVu (see Figure 2 â€” QuickSet systems are on the soldiers&apos; tables). In addition to LeatherNe</context>
<context position="15427" citStr="[17, 18]" startWordPosition="2306" endWordPosition="2307">ed, a complete overlap in functionality, such that any task can be accomplished just with pen or just with speech when necessary. Finally, QuickSet is now being extended for use in the ExInit simulation initialization system for DARPA&apos;s STOW-97 Advanced Concept Demonstration that is intended for creation of division-sized exercises. Regarding the multimodal interface itself, QuickSet has undergone a &amp;quot;proactive&amp;quot; interface evaluation in that the studies that were performed in advance of building the system predicted the utility of multimodal over unimodal speech as an input to map-based systems [17, 18]. In particular, it was discovered in this research that multimodal interaction generates simpler language than unimodal spoken commands to maps. For example, to create a &amp;quot;phase line&amp;quot; between two three-digit &lt;x,y&gt; grid coordinates, a user would have to say: &amp;quot;create a line from nine four three nine six one to nine five seven nine six eight and call it phase line green&amp;quot; [14]. In contrast, a QuickSet user would say &amp;quot;phase line green&amp;quot; while drawing a line. Creation of area features with unimodal speech would be more complex still, if not infeasible. Given that numerous difficult-to-process linguis</context>
</contexts>
<marker>18.</marker>
<rawString>Oviatt, S. L., Multimodal interactive maps: Designing for human performance, Human-Computer Interaction, in press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S L Oviatt</author>
<author>A DeAngeli</author>
<author>K Kuhn</author>
</authors>
<title>In press. Integration and synchronization of input modes during multimodal human-computer interaction.</title>
<booktitle>Proceedings of the Conference on Human Factors in Computing Systems (CHI &apos;97),</booktitle>
<publisher>ACM Press,</publisher>
<location>New York.</location>
<contexts>
<context position="13257" citStr="[19]" startWordPosition="1975" endWordPosition="1975">and gesture. Importantly, 22 the unified interpretation might not include the highest scoring gestural (or spoken language) interpretation because it might not be semantically compatible with the other mode. The key to this interpretation process is the use of a typed feature structure [1, 3] as a meaning representation language that is common to the natural language and gestural interpretation agents. Johnston et al. [12] present the details of multimodal integration of continuous speech and pen-based gesture, guided by research in users&apos; multimodal integration and synchronization strategies [19]. Unlike many previous approaches to multimodal integration (e.g, [2, 9, 12, 15, 25]) speech is not &amp;quot;in charge,&amp;quot; in the sense of relegating gesture a secondary and dependent role. This mutually-compensatory interpretation process is capable of analyzing multimodal constructions, as well as speech-only and pen-only constructions when they occur. Vo and Wood&apos;s system [24] is similar to the one reported here, though we believe the use of typed feature structures provides a more generally usable and formal integration mechanism than their frame-merging strategy. Cheyer and Julia [4] sketch a syste</context>
</contexts>
<marker>19.</marker>
<rawString>Oviatt, S. L, A. DeAngeli, and K. Kuhn. In press. Integration and synchronization of input modes during multimodal human-computer interaction. Proceedings of the Conference on Human Factors in Computing Systems (CHI &apos;97), ACM Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Oviatt</author>
<author>P R Cohen</author>
<author>M W Fong</author>
<author>M P Frank</author>
</authors>
<title>A rapid semi-automatic simulation technique for interactive speech and handwriting,</title>
<date>1992</date>
<booktitle>Proceedings of the 1992 International Conference Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>1351--1354</pages>
<editor>J. Ohala (ed.),</editor>
<institution>University of Alberta,</institution>
<contexts>
<context position="3528" citStr="[20]" startWordPosition="505" endWordPosition="505">t on the screen. In contrast, it was argued that linguisticallybased interface technologies offer the potential to describe large sets of objects, which may not all be present on a screen, and can be used to create more complex behaviors through specification of rule invocation conditions. Simulation is one type of application for which these limitations of GUIs, as well as the strengths of natural language, especially spoken language, are apparent [6]. It has become clear, however, that speech-only interaction is not optimal for spatial tasks. Using a high-fidelity &amp;quot;Wizard-of-Oz&amp;quot; methodology [20], recent empirical results demonstrate clear language processing and task performance advantages for multimodal (pen/voice) input over speechonly input for map-based systems [17,18]. 3. OUICKSET To address these simulation interface problems, and motivated by the above results, we have developed QuickSet (see Figure 1) a collaborative, handheld, multimodal system for configuring military simulations based on LeatherNet [5], a system used in training platoon leaders and company commanders at the USMC base at 29 Palms, California. LeatherNet simulations are created using the ModSAF simulator [10</context>
</contexts>
<marker>20.</marker>
<rawString>Oviatt, S. L., Cohen, P. R, Fong, M. W. and Frank, M. P., A rapid semi-automatic simulation technique for interactive speech and handwriting, Proceedings of the 1992 International Conference Spoken Language Processing, vol. 2, University of Alberta, J. Ohala (ed.), October, 1992, 1351-1354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Oviatt</author>
<author>P R Cohen</author>
<author>M Wang</author>
</authors>
<title>Q.,Toward interface design for human language technology: Modality and structure as determinants of linguistic complexity,</title>
<date>1994</date>
<journal>Speech Communication,</journal>
<volume>15</volume>
<pages>3--4</pages>
<marker>21.</marker>
<rawString>Oviatt, S. L., Cohen, P. R., Wang, M. Q.,Toward interface design for human language technology: Modality and structure as determinants of linguistic complexity, Speech Communication, 15 (3-4), 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Pittman</author>
<author>I A Smith</author>
<author>P R Cohen</author>
<author>S L Oviatt</author>
<author>T C Yang</author>
</authors>
<title>QuickSet: A Multimodal Interface for Military Simulation.</title>
<date>1996</date>
<booktitle>in Proceedings of the Sixth Conference on Computer-Generated Forces and Behavioral Representation,</booktitle>
<location>Orlando, Florida,</location>
<contexts>
<context position="10622" citStr="[12, 22]" startWordPosition="1557" endWordPosition="1558">izes the underlying applications&apos; API to typed feature structures, thereby providing an interface to the various applications such as ModSAF, ConunandVu, and Exinit. This allows for a domain-independent integration architecture in which constraints on multimodal interpretation are stated in terms of higher-level constructs such as typed feature structures, greatly facilitating reuse. CORBA bridge agent: This agent converts OAA messages to COREA IDL (Interface Definition Language) for the Exercise Initialization project. More detail on the architecture and the individual agents are provided in [12, 22]. 5. EXAMPLE Holding QuickSet in hand, the user views a map from the ModSAF simulation, and with spoken language coupled with pen gestures, issues commands to ModSAF. In order to create a unit in QuickSet, the user would hold the pen at the desired location and utter (for instance): &amp;quot;red T72 platoon&amp;quot; resulting in a new platoon of the specified type being created at that location. Figure 4: The QuickSet interface as the user establishes two platoons, a barbed-wire fence, a breached minefield, and then issues a command to one platoon to follow a traced route. The user then adds a barbed-wire fen</context>
</contexts>
<marker>22.</marker>
<rawString>Pittman, J.A., Smith, I.A., Cohen, P.R., Oviatt, S.L., and Yang, T.C. QuickSet: A Multimodal Interface for Military Simulation. in Proceedings of the Sixth Conference on Computer-Generated Forces and Behavioral Representation, Orlando, Florida, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Thorpe</author>
</authors>
<title>The new technology of large scale simulator networking: Implications for mastering the art of warfighting.</title>
<date>1987</date>
<booktitle>Proceedings of the 9th Interservice/industry Training Systems Conference,</booktitle>
<pages>492--501</pages>
<location>Orlando, Florida,</location>
<contexts>
<context position="1201" citStr="[23]" startWordPosition="156" endWordPosition="156">ld PC, communicating through a distributed agent architecture to NRaD&apos;s&apos; LeatherNet system, a distributed interactive training simulator built for the US Marine Corps (USMC). The paper briefly describes the system and illustrates its use in multimodal simulation setup. KEYWORDS: multimodal interfaces, agent architecture, gesture recognition, speech recognition, natural language processing, distributed interactive simulation. 1. INTRODUCTION In order to train personnel more effectively, the US military is developing large-scale distributed simulation capabilities. Begun as SIMNET in the 1980&apos;s [23], these distributed, interactive environments attempt to provide a high degree of fidelity in simulating combat, including simulations of the individual combatants, the equipment, entity movements, atmospheric effects, etc. There are four general phases of user interaction with these simulations: Creating entities, supplying their initial behavior, interacting with the entities during a running simulation, and reviewing the results. The present research concentrates on the first two of these stages. Our contribution to the distributed interactive simulation (DIS) effort is to rethink the natur</context>
</contexts>
<marker>23.</marker>
<rawString>Thorpe, J. A., The new technology of large scale simulator networking: Implications for mastering the art of warfighting. Proceedings of the 9th Interservice/industry Training Systems Conference, Orlando, Florida, December, 1987, 492-501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T Vo</author>
<author>C Wood</author>
</authors>
<title>Building an application framework for speech and pen input integration in multimodal learning interfaces.</title>
<date>1996</date>
<booktitle>International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<location>Atlanta, GA,</location>
<contexts>
<context position="13629" citStr="[24]" startWordPosition="2029" endWordPosition="2029">d gestural interpretation agents. Johnston et al. [12] present the details of multimodal integration of continuous speech and pen-based gesture, guided by research in users&apos; multimodal integration and synchronization strategies [19]. Unlike many previous approaches to multimodal integration (e.g, [2, 9, 12, 15, 25]) speech is not &amp;quot;in charge,&amp;quot; in the sense of relegating gesture a secondary and dependent role. This mutually-compensatory interpretation process is capable of analyzing multimodal constructions, as well as speech-only and pen-only constructions when they occur. Vo and Wood&apos;s system [24] is similar to the one reported here, though we believe the use of typed feature structures provides a more generally usable and formal integration mechanism than their frame-merging strategy. Cheyer and Julia [4] sketch a system based on Oviatt&apos;s [17] results and the OAA [8], but do not discuss the integration strategy nor multimodal compensation. 7. CONCLUDING REMARKS QuickSet has been delivered to the US Navy (NRaD) and US Marine Corps. for use at 29 Palms, California, where it is primarily used to set up training scenarios and to control the virtual environment. It is also installed at NRa</context>
</contexts>
<marker>24.</marker>
<rawString>Vo, M. T. and C. Wood. Building an application framework for speech and pen input integration in multimodal learning interfaces. International Conference on Acoustics, Speech, and Signal Processing, Atlanta, GA, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wauchope</author>
</authors>
<title>Eucalyptus: Integrating natural language input with a graphical user interface.</title>
<date>1994</date>
<journal>Naval Research Laboratory, Report</journal>
<pages>5510--94</pages>
<contexts>
<context position="13341" citStr="[2, 9, 12, 15, 25]" startWordPosition="1984" endWordPosition="1988">ude the highest scoring gestural (or spoken language) interpretation because it might not be semantically compatible with the other mode. The key to this interpretation process is the use of a typed feature structure [1, 3] as a meaning representation language that is common to the natural language and gestural interpretation agents. Johnston et al. [12] present the details of multimodal integration of continuous speech and pen-based gesture, guided by research in users&apos; multimodal integration and synchronization strategies [19]. Unlike many previous approaches to multimodal integration (e.g, [2, 9, 12, 15, 25]) speech is not &amp;quot;in charge,&amp;quot; in the sense of relegating gesture a secondary and dependent role. This mutually-compensatory interpretation process is capable of analyzing multimodal constructions, as well as speech-only and pen-only constructions when they occur. Vo and Wood&apos;s system [24] is similar to the one reported here, though we believe the use of typed feature structures provides a more generally usable and formal integration mechanism than their frame-merging strategy. Cheyer and Julia [4] sketch a system based on Oviatt&apos;s [17] results and the OAA [8], but do not discuss the integration</context>
</contexts>
<marker>25.</marker>
<rawString>Wauchope, K. Eucalyptus: Integrating natural language input with a graphical user interface. Naval Research Laboratory, Report NRL/FR/5510--94-9711, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Zyda</author>
<author>D R Pratt</author>
<author>J G Monahan</author>
<author>K P Wilson</author>
</authors>
<title>NPSNET: Constructing a 3-D virtual world,</title>
<date>1992</date>
<booktitle>Proceedings of the 1992 Symposium on Interactive 3-D Graphics,</booktitle>
<contexts>
<context position="4204" citStr="[11, 26]" startWordPosition="600" endWordPosition="601"> task performance advantages for multimodal (pen/voice) input over speechonly input for map-based systems [17,18]. 3. OUICKSET To address these simulation interface problems, and motivated by the above results, we have developed QuickSet (see Figure 1) a collaborative, handheld, multimodal system for configuring military simulations based on LeatherNet [5], a system used in training platoon leaders and company commanders at the USMC base at 29 Palms, California. LeatherNet simulations are created using the ModSAF simulator [10] and can be visualized in a CAVE-based virtual reality environment [11, 26] called CommandVu (see Figure 2 â€” QuickSet systems are on the soldiers&apos; tables). In addition to LeatherNet, QuickSet is being used in a second effort called ExInit (Exercise 20 Initialization), that will enable users to create division-sized exercises. Because of the use of OAA, QuickSet can interoperate with agents from CommandTalk [14], which provides a speech-only interface to ModSAF. QuickSet runs on both desktop and hand-held PC&apos;s, communicating over wired and wireless LAN&apos;s, or modem links. The system combines speech and pen-based gesture input on multiple 3-lb hand-held PCs (Fujitsu Sty</context>
</contexts>
<marker>26.</marker>
<rawString>Zyda, M. J., Pratt, D. R., Monahan, J. G., and Wilson, K. P., NPSNET: Constructing a 3-D virtual world, Proceedings of the 1992 Symposium on Interactive 3-D Graphics, March, 1992.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>