<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015428">
<note confidence="0.858688">
Computational Linguistics Volume 20, Number 3
</note>
<title confidence="0.9968535">
Statistically-Driven Computer Grammars of English:
The IBM/Lancaster Approach
</title>
<note confidence="0.6026895">
Ezra Black, Roger Garside, and Geoffrey Leech (editors)
(IBM T.J. Watson Research Center and Lancaster University)
</note>
<bodyText confidence="0.406036285714286">
Amsterdam: Editions Rodopi
(Language and Computers: Studies in
Practical Linguistics, edited by Jan
Aarts and Willem Meijs, Volume 8),
1993, ix + 248 pp.
Paperbound, ISBN 90-5183-478-0,
$58.50, Dfl 100.00
</bodyText>
<figure confidence="0.710469">
Reviewed by
Dekai Wu
</figure>
<affiliation confidence="0.779429">
Hong Kong University of Science and Technology
</affiliation>
<bodyText confidence="0.99978296875">
Statistical computational linguistics is entering a consolidation phase, signaled by the
appearance of book-length tracts devoted to single, coherent projects. After several
years of vigorous research in the area, gains along a number of dimensions have so-
lidified into established methodology. Although Statistically-Driven Computer Grammars
of English (henceforth SDCGE) is an edited collection, its presentation of a broad range
of issues, spanning methodologies from linguistic evaluation to statistical training, is
tightly and lucidly integrated.
SDCGE is the product of a five-year collaboration between IBM T.J. Watson Re-
search Center&apos;s Continuous Speech Recognition Group and Lancaster University&apos;s Unit
for Computer Research on the English Language, to produce a wide-coverage English
parser with the aid of statistical grammar-training techniques. The project initially tar-
geted the AP newswire, but eventually lowered its aim to IBM computer manuals.
As motivation, Ezra Black opens the book with an introduction summarizing three
experiments demonstrating the poor accuracy of &amp;quot;traditional&amp;quot; parsing systems, most
notably a 1992 evaluation of 35 parsers against the Penn Treebank, which yielded
only 22% average correctness (by a generous criterion) by the only seven participants
willing to report their results.
The project incorporates two complementary statistical methodologies. Grammar
development is driven by ranking error types by frequency over the corpus, and
fixing the most frequent ones first. Evaluation is performed by taking the maximum-
likelihood parse for each sentence as the grammar&apos;s prediction, rather than the &amp;quot;tra-
ditional&amp;quot; approach of allowing many possible parses, as with chart parsers. This con-
dition is crucial, since otherwise the accuracy (recall) of any parser can be arbitrarily
inflated by simply outputting more parses per sentence.
One of the most important services of SDCGE to its readers is its assemblage of
invaluable anecdotes for those seeking to avoid the costly pitfalls that are easily en-
countered in this field, especially where human evaluation and processing of corpora
are involved. The authors&apos; experience with various levels of treebanking detail, along
with the tradeoffs involved, is described in chapters by Roger Garside and Anthony
McEnery, and Elizabeth Eyes and Geoffrey Leech. Treebanking proceeds by verifying
the output from the CLAWS automatic tagging system, followed by human &amp;quot;skeleton
parsing,&amp;quot; where sentences are bracketed into flat parse trees labeled from a minimal
</bodyText>
<page confidence="0.993587">
498
</page>
<subsectionHeader confidence="0.814969">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.999981666666667">
set of nonterminal types. User interfaces for post-editing tags and creating skeleton
parses, optimized for throughput, are both described. The authors set out four cri-
teria to be traded off in designing tagsets and nonterminals: the productivity of the
human evaluators, attainable accuracy level, attainable uniformity of analysis across
evaluators, and linguistic validity.
The authors&apos; own design decisions on the CLAWS2a tagset, the 17 nonterminals,
and the approximately 500 English grammar rules are described in concrete detail,
largely in the book&apos;s longest chapter, in which Black covers the IBM grammar. Along
the way, numerous examples extracted from various corpora are used to illustrate how
inadequacies in the coverage of current grammatical theory (e.g., ellipsis, coordination,
argument structure) are dealt with in practical ways. The grammar is sufficiently broad
as to provide a good departure point for those wishing to construct their own systems.
The one unfortunate omission is that the chapter describes the entire grammar
in standard rewrite-rule notation, but goes on in the subsequent chapter (again by
Black) to explain that their system employed a different, more compact feature-based
representation instead. The feature representation reduces the number of grammar rule
variations that must be remembered or explicitly stored, and provides the linguistic
motivation for the equivalence classes (&amp;quot;mnemonics&amp;quot;) that are later used to reduce the
number of parameters to be trained. The features themselves are listed in an appendix,
but the grammar is not given in this form, leaving the reader to reconstruct the feature-
based grammar from the rewrite rules.
The book then moves on to a nice treatment of the training algorithm and issues,
by John Lafferty. Between fields as (formerly) disparate as pattern recognition and lin-
guistics, it is always a difficult balancing act to treat topics in detail comprehensible to
a novice of one area while not burdening specialists with too much to sift through. The
inside-outside algorithm is introduced here at a level suitable for novices; its relation-
ship to the general EM (expectation-maximization) algorithm is omitted. The essential
ideas needed to extend the algorithm to handle feature-based mnemonics are out-
lined, though again the inexperienced reader will encounter difficulty reconstructing
the details.
The concluding chapter by Black, Garside, David Magerman, and Salim Roukos
draws up the results. For sentences of 1-23 words (the authors&apos; primary objective), the
maximum-likelihood parse for 64-69% of the sentences is &amp;quot;correct,&amp;quot; meaning there are
no constituent crossings compared against the treebank parse, measured using ran-
domly extracted test sets from the treebank. A &amp;quot;correct&amp;quot; parse, but not necessarily the
maximum-likelihood parse, is produced for about 94% of the sentences. The chapter
also describes some current directions, notably the idea of a history-based grammar,
in which parse tree derivation probabilities are conditioned on variables other than
a constituent&apos;s immediate parent (most importantly the parent&apos;s head). Because the
resulting parameter space is too large in its full form, a decision tree is induced to
capture the most significant conditioning factors. The preliminary experiment signif-
icantly improved the prediction accuracy on labeling of nonterminals, which is not
included in the above definition of &amp;quot;correct.&amp;quot;
SDCGE displays a refreshing honesty about its empirical results. The authors are
forthcoming about the relative modesty of their &amp;quot;correctness&amp;quot; criteria. The many de-
sign errors—most remedied, some not—and their consequences are described. An
enormous amount of labor has gone into the project, allowing the results to stand by
themselves.
As implied by its title, the book firmly restricts its claims and attention to English.
However, computational linguists interested in other languages would do well to
ignore this focus, since much of the material is applicable to other languages. Only
</bodyText>
<page confidence="0.997341">
499
</page>
<note confidence="0.801223">
Computational Linguistics Volume 20, Number 3
</note>
<bodyText confidence="0.937032692307692">
the specific rules must be overlooked (particularly Chapter 4), leaving an intact thread
on tagging, treebanking, and probabilistic context-free grammars.
SDCGE succeeds as an introductory guide to the methodology of building prob-
abilistic context-free grammars, both for linguists moving toward corpus-based tech-
niques and for computer scientists seeking advice on the tagging and treebanking end.
It is not a textbook, but may be the closest thing so far. Its polemics in favor of sta-
tistical methodology may be too much for some linguists, but the wealth of concrete
detail more than makes up for this.
Dekai Wu received his Ph.D. from the University of California at Berkeley in 1992, working in
probabilistic computational linguistics and natural language processing. He is assistant professor
at the Hong Kong University of Science and Technology, where he directs the SILC project
on statistical Chinese—English translation. Wu&apos;s address is: Department of Computer Science,
University of Science and Technology, Clear Water Bay, Hong Kong; e-mail: dekai@cs.ust.hk
</bodyText>
<page confidence="0.991806">
500
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.097024">
<title confidence="0.923672666666667">Computational Linguistics Volume 20, Number 3 Statistically-Driven Computer Grammars of English: The IBM/Lancaster Approach</title>
<author confidence="0.998692">Ezra Black</author>
<author confidence="0.998692">Roger Garside</author>
<author confidence="0.998692">Geoffrey Leech</author>
<affiliation confidence="0.990058">(IBM T.J. Watson Research Center and Lancaster University)</affiliation>
<address confidence="0.290113">Amsterdam: Editions Rodopi</address>
<note confidence="0.794755428571429">(Language and Computers: Studies in Practical Linguistics, edited by Jan Aarts and Willem Meijs, Volume 8), 1993, ix + 248 pp. Paperbound, ISBN 90-5183-478-0, $58.50, Dfl 100.00 Reviewed by</note>
<author confidence="0.8838035">Dekai Wu Hong Kong University of Science</author>
<author confidence="0.8838035">Technology</author>
<abstract confidence="0.98994475">Statistical computational linguistics is entering a consolidation phase, signaled by the appearance of book-length tracts devoted to single, coherent projects. After several years of vigorous research in the area, gains along a number of dimensions have sointo established methodology. Although Computer Grammars English an edited collection, its presentation of a broad range of issues, spanning methodologies from linguistic evaluation to statistical training, is tightly and lucidly integrated. the product of a five-year collaboration between IBM T.J. Watson Research Center&apos;s Continuous Speech Recognition Group and Lancaster University&apos;s Unit for Computer Research on the English Language, to produce a wide-coverage English parser with the aid of statistical grammar-training techniques. The project initially targeted the AP newswire, but eventually lowered its aim to IBM computer manuals.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>