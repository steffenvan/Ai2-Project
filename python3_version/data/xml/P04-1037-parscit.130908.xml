<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<title confidence="0.988341">
Unsupervised Sense Disambiguation Using Bilingual Probabilistic Models
</title>
<author confidence="0.939631">
Indrajit Bhattacharya
</author>
<affiliation confidence="0.976956">
Dept. of Computer Science
University of Maryland
</affiliation>
<address confidence="0.758002">
College Park, MD,
USA
</address>
<email confidence="0.996438">
indrajit@cs.umd.edu
</email>
<author confidence="0.810079">
Lise Getoor
</author>
<affiliation confidence="0.976843">
Dept. of Computer Science
University of Maryland
</affiliation>
<address confidence="0.7579495">
College Park, MD,
USA
</address>
<email confidence="0.99513">
getoor@cs.umd.edu
</email>
<note confidence="0.572696333333333">
Yoshua Bengio
Dept. IRO
Universit´e de Montr´eal
</note>
<address confidence="0.4520135">
Montr´eal, Qu´ebec,
Canada
</address>
<email confidence="0.982188">
bengioy@IRO.UMontreal.CA
</email>
<sectionHeader confidence="0.996802" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999987875">
We describe two probabilistic models for unsuper-
vised word-sense disambiguation using parallel cor-
pora. The first model, which we call the Sense
model, builds on the work of Diab and Resnik
(2002) that uses both parallel text and a sense in-
ventory for the target language, and recasts their ap-
proach in a probabilistic framework. The second
model, which we call the Concept model, is a hier-
archical model that uses a concept latent variable to
relate different language specific sense labels. We
show that both models improve performance on the
word sense disambiguation task over previous unsu-
pervised approaches, with the Concept model show-
ing the largest improvement. Furthermore, in learn-
ing the Concept model, as a by-product, we learn a
sense inventory for the parallel language.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956877358491">
Word sense disambiguation (WSD) has been a cen-
tral question in the computational linguistics com-
munity since its inception. WSD is fundamental to
natural language understanding and is a useful in-
termediate step for many other language process-
ing tasks (Ide and Veronis, 1998). Many recent
approaches make use of ideas from statistical ma-
chine learning; the availability of shared sense defi-
nitions (e.g. WordNet (Fellbaum, 1998)) and recent
international competitions (Kilgarrif and Rosen-
zweig, 2000) have enabled researchers to compare
their results. Supervised approaches which make
use of a small hand-labeled training set (Bruce
and Wiebe, 1994; Yarowsky, 1993) typically out-
perform unsupervised approaches (Agirre et al.,
2000; Litkowski, 2000; Lin, 2000; Resnik, 1997;
Yarowsky, 1992; Yarowsky, 1995), but tend to be
tuned to a specific corpus and are constrained by
scarcity of labeled data.
In an effort to overcome the difficulty of find-
ing sense-labeled training data, researchers have be-
gun investigating unsupervised approaches to word-
sense disambiguation. For example, the use of par-
allel corpora for sense tagging can help with word
sense disambiguation (Brown et al., 1991; Dagan,
1991; Dagan and Itai, 1994; Ide, 2000; Resnik and
Yarowsky, 1999). As an illustration of sense disam-
biguation from translation data, when the English
word bank is translated to Spanish as orilla, it is
clear that we are referring to the shore sense of bank,
rather than the financial institution sense.
The main inspiration for our work is Diab and
Resnik (2002), who use translations and linguistic
knowledge for disambiguation and automatic sense
tagging. Bengio and Kermorvant (2003) present
a graphical model that is an attempt to formalize
probabilistically the main ideas in Diab and Resnik
(2002). They assume the same semantic hierarchy
(in particular, WordNet) for both the languages and
assign English words as well as their translations
to WordNet synsets. Here we present two variants
of the graphical model in Bengio and Kermorvant
(2003), along with a method to discover a cluster
structure for the Spanish senses. We also present
empirical word sense disambiguation results which
demonstrate the gain brought by this probabilistic
approach, even while only using the translated word
to provide disambiguation information.
Our first generative model, the Sense Model,
groups semantically related words from the two
languages into senses, and translations are gener-
ated by probabilistically choosing a sense and then
words from the sense. We show that this improves
on the results of Diab and Resnik (2002).
Our next model, which we call the Concept
Model, aims to improve on the above sense struc-
ture by modeling the senses of the two languages
separately and relating senses from both languages
through a higher-level, semantically less precise
concept. The intuition here is that not all of the
senses that are possible for a word will be relevant
for a concept. In other words, the distribution over
the senses of a word given a concept can be expected
to have a lower entropy than the distribution over
the senses of the word in the language as a whole.
In this paper, we look at translation data as a re-
source for identification of semantic concepts. Note
that actual translated word pairs are not always good
matches semantically, because the translation pro-
cess is not on a word by word basis. This intro-
duces a kind of noise in the translation, and an addi-
tional hidden variable to represent the shared mean-
ing helps to take it into account. Improved perfor-
mance over the Sense Model validates the use of
concepts in modeling translations.
An interesting by-product of the Concept Model
is a semantic structure for the secondary language.
This is automatically constructed using background
knowledge of the structure for the primary language
and the observed translation pairs. In the model,
words sharing the same sense are synonyms while
senses under the same concept are semantically re-
lated in the corpus. An investigation of the model
trained over real data reveals that it can indeed
group related words together.
It may be noted that predicting senses from trans-
lations need not necessarily be an end result in it-
self. As we have already mentioned, lack of labeled
data is a severe hindrance for supervised approaches
to word sense disambiguation. At the same time,
there is an abundance of bilingual documents and
many more can potentially be mined from the web.
It should be possible using our approach to (noisily)
assign sense tags to words in such documents, thus
providing huge resources of labeled data for super-
vised approaches to make use of.
For the rest of this paper, for simplicity we will
refer to the primary language of the parallel docu-
ment as English and to the secondary as Spanish.
The paper is organized as follows. We begin by for-
mally describing the models in Section 2. We de-
scribe our approach for constructing the senses and
concepts in Section 3. Our algorithm for learning
the model parameters is described in Section 4. We
present experimental results in Section 5 and our
analysis in Section 6. We conclude in Section 7.
</bodyText>
<sectionHeader confidence="0.965084" genericHeader="introduction">
2 Probabilistic Models for Parallel
Corpora
</sectionHeader>
<bodyText confidence="0.990227275862069">
We motivate the use of a probabilistic model by il-
lustrating that disambiguation using translations is
possible even when a word has a unique transla-
tion. For example, according to WordNet, the word
prevention has two senses in English, which may
be abbreviated as hindrance (the act of hindering
or obstruction) and control (by prevention, e.g. the
control of a disease). It has a single translation in
our corpus, that being prevenci´on. The first En-
glish sense, hindrance, also has other words like
bar that occur in the corpus and all of these other
words are observed to be translated in Spanish as
the word obstrucci´on. In addition, none of these
other words translate to prevenci´on. So it is not
unreasonable to suppose that the intended sense for
prevention when translated as prevenci´on is differ-
ent from that of bar. Therefore, the intended sense
is most likely to be control. At the very heart of
the reasoning is probabilistic analysis and indepen-
dence assumptions. We are assuming that senses
and words have certain occurrence probabilities and
that the choice of the word can be made indepen-
dently once the sense has been decided. This is the
flavor that we look to add to modeling parallel doc-
uments for sense disambiguation. We formally de-
scribe the two generative models that use these ideas
in Subsections 2.2 and 2.3.
Figure 1: Graphical Representations of the a) Sense
Model and the b) Concept Model
</bodyText>
<subsectionHeader confidence="0.915">
2.1 Notation
</subsectionHeader>
<bodyText confidence="0.995804125">
Throughout, we use uppercase letters to denote ran-
dom variables and lowercase letters to denote spe-
cific instances of the random variables. A transla-
tion pair is ( , ) where the subscript and
indicate the primary language (English) and the sec-
ondary language (Spanish).
and . We use the shorthand
for .
</bodyText>
<subsectionHeader confidence="0.997661">
2.2 The Sense Model
</subsectionHeader>
<bodyText confidence="0.999834857142857">
The Sense Model makes the assumption, inspired
by ideas in Diab and Resnik (2002) and Ben-
gio and Kermorvant (2003), that the English word
and the Spanish word in a translation pair
share the same precise sense. In other words, the
set of sense labels for the words in the two lan-
guages is the same and may be collapsed into one
set of senses that is responsible for both English
and Spanish words and the single latent variable
in the model is the sense label
for both words and . We also make the as-
sumption that the words in both languages are con-
ditionally independent given the sense label. The
generative parameters for the model are the prior
</bodyText>
<figure confidence="0.9939798">
a) Sense Model b) Concept Model
concept
sense
word
We Ws
T
Te Ts
We
C
Ws
</figure>
<bodyText confidence="0.984248176470588">
probability of each sense and the conditional
probabilities and of each word
and in the two languages given the sense. The
generation of a translation pair by this model may
be viewed as a two-step process that first selects
a sense according to the priors on the senses and
then selects a word from each language using the
conditional probabilities for that sense. This may
be imagined as a factoring of the joint distribution:
. Note
that in the absence of labeled training data, two
of the random variables and are observed,
while the sense variable is not. However, we can
derive the possible values for our sense labels from
WordNet, which gives us the possible senses for
each English word . The Sense model is shown
in Figure 1(a).
</bodyText>
<subsectionHeader confidence="0.98023">
2.3 The Concept Model
</subsectionHeader>
<bodyText confidence="0.993204657894737">
The assumption of a one-to-one association be-
tween sense labels made in the Sense Model may be
too simplistic to hold for arbitrary languages. In par-
ticular, it does not take into account that translation
is from sentence to sentence (with a shared mean-
ing), while the data we are modeling are aligned
single-word translations , in which the in-
tended meaning of does not always match per-
fectly with the intended meaning of . Generally,
a set of related senses in one language may be
translated by one of related senses in the other.
This many-to-many mapping is captured in our al-
ternative model using a second level hidden vari-
able called a concept. Thus we have three hid-
den variables in the Concept Model — the English
sense , the Spanish sense and the concept ,
where
.
We make the assumption that the senses and
are independent of each other given the shared
concept . The generative parameters in the
model are the prior probabilities over the
concepts, the conditional probabilities and
for the English and Spanish senses given the
concept, and the conditional probabilities
and for the words and in each
language given their senses. We can now imag-
ine the generative process of a translation pair by
the Concept Model as first selecting a concept ac-
cording to the priors, then a sense for each lan-
guage given the concept, and finally a word for
each sense using the conditional probabilities of the
words. As in Bengio and Kermorvant (2003), this
generative procedure may be captured by factor-
ing the joint distribution using the conditional inde-
pendence assumptions as
. The
Concept model is shown in Figure 1(b).
</bodyText>
<sectionHeader confidence="0.710705" genericHeader="method">
3 Constructing the Senses and Concepts
</sectionHeader>
<bodyText confidence="0.999765962962963">
Building the structure of the model is crucial for
our task. Choosing the dimensionality of the hidden
variables by selecting the number of senses and con-
cepts, as well as taking advantage of prior knowl-
edge to impose constraints, are very important as-
pects of building the structure.
If certain words are not possible for a given sense,
or certain senses are not possible for a given con-
cept, their corresponding parameters should be 0.
For instance, for all words that do not belong to a
sense , the corresponding parameter would
be permanently set to 0. Only the remaining param-
eters need to be modeled explicitly.
While model selection is an extremely difficult
problem in general, an important and interesting op-
tion is the use of world knowledge. Semantic hi-
erarchies for some languages have been built. We
should be able to make use of these known tax-
onomies in constructing our model. We make heavy
use of the WordNet ontology to assign structure to
both our models, as we discuss in the following sub-
sections. There are two major tasks in building the
structure — determining the possible sense labels
for each word, both English and Spanish, and con-
structing the concepts, which involves choosing the
number of concepts and the probable senses for each
concept.
</bodyText>
<subsectionHeader confidence="0.999875">
3.1 Building the Sense Model
</subsectionHeader>
<bodyText confidence="0.981318">
Each word in WordNet can belong to multiple
synsets in the hierarchy, which are its possible
senses. In both of our models, we directly use the
WordNet senses as the English sense labels. All
WordNet senses for which a word has been ob-
served in the corpus form our set of English sense
labels. The Sense Model holds that the sense labels
for the two domains are the same. So we must use
the same WordNet labels for the Spanish words as
well. We include a Spanish word for a sense if
is the translation of any English word in .
</bodyText>
<subsectionHeader confidence="0.998851">
3.2 Building the Concept Model
</subsectionHeader>
<bodyText confidence="0.999654666666667">
Unlike the Sense Model, the Concept Model does
not constrain the Spanish senses to be the same as
the English ones. So the two major tasks in build-
ing the Concept Model are constructing the Spanish
senses and then clustering the English and Spanish
senses to build the concepts.
</bodyText>
<figure confidence="0.988768833333333">
,
and
c20
c6118
te1 te2
bar prevention
obstruccio’n
prevencio’n
te2 te1 ts1
ts2
prevention bar prevencio’n obstruccio’n
Sense Model Concept Model
</figure>
<figureCaption confidence="0.999784">
Figure 2: The Sense and Concept models for prevention, bar, prevenci´on and obstrucci´on
</figureCaption>
<subsectionHeader confidence="0.729385">
For each Spanish word , we have its set of En-
</subsectionHeader>
<bodyText confidence="0.98521295">
glish translations . One possibility is
to group Spanish words looking at their translations.
However, a more robust approach is to consider the
relevant English senses for . Each English trans-
lation for has its set of English sense labels
drawn from WordNet. So the relevant English sense
labels for may be defined as .
We call this the English sense map or for
. We use the s to define the Spanish senses.
We may imagine each Spanish word to come from
one or more Spanish senses. If each word has a
single sense, then we add a Spanish sense for
each and all Spanish words that share that
belong to that sense. Otherwise, the s
have to be split into frequently occurring subgroups.
Frequently co-occurring subsets of s can de-
fine more refined Spanish senses. We identify these
subsets by looking at pairs of s and comput-
ing their intersections. An intersection is consid-
ered to be a Spanish sense if it occurs for a signifi-
cant number of pairs of s. We consider both
ways of building Spanish senses. In either case, a
constructed Spanish sense comes with its rele-
vant set of English senses, which we denote
as .
Once we have the Spanish senses, we cluster
them to form concepts. We use the corre-
sponding to each Spanish sense to define a measure
of similarity for a pair of Spanish senses. There
are many options to choose from here. We use a
simple measure that counts the number of common
items in the two s.1 The similarity measure is
now used to cluster the Spanish senses . Since
this measure is not transitive, it does not directly
define equivalence classes over . Instead, we get
a similarity graph where the vertices are the Span-
ish senses and we add an edge between two senses
if their similarity is above a threshold. We now
pick each connected component from this graph as
a cluster of similar Spanish senses.
</bodyText>
<tableCaption confidence="0.6354405">
&apos;Another option would be to use a measure of similarity for
English senses, proposed in Resnik (1995) for two synsets in
a concept hierarchy like WordNet. Our initial results with this
measure were not favorable.
</tableCaption>
<bodyText confidence="0.9990764">
Now we build the concepts from the Spanish
sense clusters. We recall that a concept is defined by
a set of English senses and a set of Spanish senses
that are related. Each cluster represents a concept.
A particular concept is formed by the set of Spanish
senses in the cluster and the English senses relevant
for them. The relevant English senses for any Span-
ish sense is given by its . Therefore, the union
of the s of all the Spanish senses in the cluster
forms the set of English senses for each concept.
</bodyText>
<sectionHeader confidence="0.994888" genericHeader="method">
4 Learning the Model Parameters
</sectionHeader>
<bodyText confidence="0.9999545">
Once the model is built, we use the popular EM al-
gorithm (Dempster et al., 1977) for hidden vari-
ables to learn the parameters for both models. The
algorithm repeatedly iterates over two steps. The
first step maximizes the expected log-likelihood of
the joint probability of the observed data with the
current parameter settings . The next step then re-
estimates the values of the parameters of the model.
Below we summarize the re-estimation steps for
each model.
</bodyText>
<subsectionHeader confidence="0.592593">
4.1 EM for the Sense Model
</subsectionHeader>
<bodyText confidence="0.937355">
follows similarly.
</bodyText>
<page confidence="0.36593">
4.2 EM for the Concept Model
</page>
<bodyText confidence="0.859806">
follow similarly.
</bodyText>
<subsectionHeader confidence="0.998219">
4.3 Initialization of Model Probabilities
</subsectionHeader>
<bodyText confidence="0.993399212121213">
Since the EM algorithm performs gradient ascent
as it iteratively improves the log-likelihood, it is
prone to getting caught in local maxima, and se-
lection of the initial conditions is crucial for the
learning procedure. Instead of opting for a uni-
form or random initialization of the probabilities,
we make use of prior knowledge about the English
words and senses available from WordNet. Word-
Net provides occurrence frequencies for each synset
in the SemCor Corpus that may be normalized to
derive probabilities for each English sense
. For the Sense Model, these probabilities form
the initial priors over the senses, while all English
(and Spanish) words belonging to a sense are ini-
tially assumed to be equally likely. However, ini-
tialization of the Concept Model using the same
knowledge is trickier. We would like each En-
glish sense to have . But
the fact that each sense belongs to multiple con-
cepts and the constraint makes
the solution non-trivial. Instead, we settle for a
compromise. We set and
. Subsequent normalization
takes care of the sum constraints. For a Spanish
⑦
sense, we set . Once
we have the Spanish sense probabilities, we follow
the same procedure for setting for each con-
cept. All the Spanish and English words for a sense
are set to be equally likely, as in the Sense Model.
It turned out in our experiments on real data that
this initialization makes a significant difference in
model performance.
</bodyText>
<sectionHeader confidence="0.998371" genericHeader="method">
5 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.9717220625">
Both the models are generative probabilistic models
learned from parallel corpora and are expected to
fit the training and subsequent test data. A good fit
should be reflected in good prediction accuracy over
a test set. The prediction task of interest is the sense
of an English word when its translation is provided.
We estimate the prediction accuracy and recall of
our models on Senseval data.2 In addition, the Con-
cept Model learns a sense structure for the Spanish
2Accuracy is the ratio of the number of correct predictions
and the number of attempted predictions. Recall is the ratio of
the number of correct predictions and the size of the test set.
language. While it is hard to objectively evaluate
the quality of such a structure, we present some in-
teresting concepts that are learned as an indication
of the potential of our approach.
</bodyText>
<subsectionHeader confidence="0.99762">
5.1 Evaluation with Senseval Data
</subsectionHeader>
<bodyText confidence="0.996860666666667">
In our experiments with real data, we make use of
the parallel corpora constructed by Diab and Resnik
(2002) for evaluation purposes. We chose to work
on these corpora in order to permit a direct compar-
ison with their results. The sense-tagged portion of
the English corpus is comprised of the English “all-
words” section of the SENSEVAL-2 test data. The
remainder of this corpus is constructed by adding
the Brown Corpus, the SENSEVAL-1 corpus, the
SENSEVAL-2 English Lexical Sample test, trial
and training corpora and the Wall Street Journal sec-
tions 18-24 from the Penn Treebank. This English
corpus is translated into Spanish using two com-
mercially available MT systems: Globalink Pro 6.4
and Systran Professional Premium. The GIZA++
implementation of the IBM statistical MT models
was used to derive the most-likely word-level align-
ments, and these define the English/Spanish word
co-occurrences. To take into account variability of
translation, we combine the translations from the
two systems for each English word, following in the
footsteps of Diab and Resnik (2002). For our ex-
periments, we focus only on nouns, of which there
are 875 occurrences in our tagged data. The sense
tags for the English domain are derived from the
WordNet 1.7 inventory. After pruning stopwords,
we end up with 16,186 English words, 31,862 Span-
ish words and 2,385,574 instances of 41,850 distinct
translation pairs. The English words come from
20,361 WordNet senses.
</bodyText>
<tableCaption confidence="0.999941">
Table 1: Comparison with Diab’s Model
</tableCaption>
<table confidence="0.99017175">
Model Accuracy Recall Parameters
Diab 0.618 0.572 -
Sense M. 0.624 0.616 154,947
Concept M. 0.672 0.651 120,268
</table>
<bodyText confidence="0.983166083333333">
As can be seen from the following table, both our
models clearly outperform Diab (2003), which is
an improvement over Diab and Resnik (2002), in
both accuracy and recall, while the Concept Model
does significantly better than the Sense Model with
fewer parameters. The comparison is restricted to
the same subset of the test data. For our best re-
sults, the Sense Model has 20,361 senses, while the
Concept Model has 20,361 English senses, 11,961
Spanish senses and 7,366 concepts. The Concept
Model results are for the version that allows mul-
tiple senses for a Spanish word. Results for the
</bodyText>
<figure confidence="0.879268333333333">
and
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Accuracy
</figure>
<figureCaption confidence="0.999524">
Figure 3: Comparison with Senseval2 Systems
</figureCaption>
<bodyText confidence="0.998798230769231">
single-sense model are similar.
In Figure 3, we compare the prediction accuracy
and recall against those of the 21 Senseval-2 English
All Words participants and that of Diab (2003),
when restricted to the same set of noun instances
from the gold standard. It can be seen that our mod-
els outperform all the unsupervised approaches in
recall and many supervised ones as well. No un-
supervised approach is better in both accuracy and
recall. It needs to be kept in mind that we take into
account only bilingual data for our predictions, and
not monolingual features like context of the word as
most other WSD approaches do.
</bodyText>
<subsectionHeader confidence="0.999656">
5.2 Semantic Grouping of Spanish Senses
</subsectionHeader>
<bodyText confidence="0.999945125">
Table 2 shows some interesting examples of differ-
ent Spanish senses for discovered concepts.3 The
context of most concepts, like the ones shown, can
be easily understood. For example, the first concept
is about government actions and the second deals
with murder and accidental deaths. The penulti-
mate concept is interesting because it deals with dif-
ferent kinds of association and involves three dif-
ferent senses containing the word conexi´on. The
other words in two of these senses suggest that
they are about union and relation respectively. The
third probably involves the link sense of connection.
Conciseness of the concepts depends on the simi-
larity threshold that is selected. Some may bring
together loosely-related topics, which can be sepa-
rated by a higher threshold.
</bodyText>
<sectionHeader confidence="0.920147" genericHeader="method">
6 Model Analysis
</sectionHeader>
<bodyText confidence="0.9908164">
In this section, we back up our experimental results
with an in-depth analysis of the performance of our
two models.
Our Sense Model was motivated by Diab and
Resnik (2002) but the flavors of the two are quite
</bodyText>
<footnote confidence="0.749168333333333">
3Some English words are found to occur in the Spanish
Senses. This is because the machine translation system used
to create the Spanish document left certain words untranslated.
</footnote>
<bodyText confidence="0.994743551724138">
different. The most important distinction is that the
Sense Model is a probabilistic generative model for
parallel corpora, where interaction between differ-
ent words stemming from the same sense comes
into play, even if the words are not related through
translations, and this interdependence of the senses
through common words plays a role in sense disam-
biguation.
We started off with our discussions on semantic
ambiguity with the intuition that identification of
semantic concepts in the corpus that relate multi-
ple senses should help disambiguate senses. The
Sense Model falls short of this target since it only
brings together a single sense from each language.
We will now revisit the motivating example from
Section 2 and see how concepts help in disambigua-
tion by grouping multiple related senses together.
For the Sense Model,
since it is the only word that
can generate. However, this difference is com-
pensated for by the higher prior probability ,
which is strengthened by both the translation pairs.
Since the probability of joint occurrence is given by
the product for any sense ,
the model does not develop a clear preference for
any of the two senses.
The critical difference in the Concept Model can
be appreciated directly from the corresponding joint
probability
</bodyText>
<sectionHeader confidence="0.548684" genericHeader="method">
. In
</sectionHeader>
<bodyText confidence="0.984722666666667">
our example, since bar, obstrucci´on can be
generated only through concept , is
the only English sense conditional boosted by it.
prevention, prevenci´on is generated through a
different concept , where the higher condi-
tional gradually strengthens one
of the possible instantiations for it, and the other
one becomes increasingly unlikely as the iterations
progress. The inference is that only one sense of
prevention is possible in the context of the parallel
corpus. The key factor in this disambiguation was
that two senses of prevention separated out in two
different concepts.
The other significant difference between the mod-
els is in the constraints on the parameters and the
effect that they have on sense disambiguation. In
the Sense Model, , while in the Con-
cept Model, separately for each
concept . Now for two relevant senses for an En-
glish word, a slight difference in their priors will
tend to get ironed out when normalized over the en-
</bodyText>
<figure confidence="0.9868152">
unsup.
sup.
diab
concept model
sense model
Recall 0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
,
</figure>
<bodyText confidence="0.99688375">
where is the relevant concept in the model.
The preference for a particular instantiation in the
model is dependent not on the prior over
a sense, but on the sense conditional
</bodyText>
<tableCaption confidence="0.9145815">
Table 2: Example Spanish Senses in a Concept. For each concept, each row is a separate sense. Dictionary
senses of Spanish words are provided in English within parenthesis where necessary.
</tableCaption>
<table confidence="0.999570205128205">
actos accidente accidentes
supremas muertes(deaths)
decisi´on decisiones casualty
gobernando gobernante matar(to kill) matanzas(slaughter) muertes-le
gubernamentales slaying
gobernaci´on gobierno-proporciona derramamiento-de-sangre (spilling-of-blood)
prohibir prohibiendo prohibitivo prohibitiva cachiporra(bludgeon) obligar(force) obligando(forcing)
gubernamental gobiernos asesinato(murder) asesinatos
linterna-el´ectrica linterna(lantern) man´ia craze
faros-autom´ovil(headlight) culto(cult) cultos proto-senility
linternas-portuarias(harbor-light) delirio delirium
antorcha(torch) antorchas antorchas-pino-nudo rabias(fury) rabia farfulla(do hastily)
oportunidad oportunidades diferenciaci´on
ocasi´on ocasiones distinci´on distinciones
riesgo(risk) riesgos peligro(danger) especializaci´on
destino sino(fate) maestr´ia (mastery)
fortuna suerte(fate) peculiaridades particularidades peculiaridades-inglesas
probabilidad probabilidades especialidad especialidades
diablo(devil) diablos modelo parang´on
dickens ideal ideales
heller santo(saint) santos san
lucifer satan satan´as idol idols ´idolo
deslumbra(dazzle) dios god dioses
cromo(chromium) divinidad divinity
meteoro meteoros meteor meteoros-blue inmortal(immortal) inmortales
meteorito meteoritos teolog´ia teolog
pedregosos(rocky) deidad deity deidades
variaci´on variaciones minutos minuto
discordancia desacuerdo(discord) discordancias momento momentos un-momento
desviaci´on(deviation) desviaciones desviaciones-normales minutos momentos momento segundos
discrepancia discrepancias fugaces(fieeting) variaci´on diferencia instante momento
disensi´on pesta˜neo(blink) gui˜na(wink) pesta˜nean
adhesi´on adherencia ataduras(tying) pasillo(corridor)
enlace(connection) ataduras aisle
atadura ataduras pasarela(footbridge)
conexi´on conexiones hall vest´ibulos
conexi´on une(to unite) pasaje(passage)
relaci´on conexi´on callej´on(alley) callejas-ciegas (blind alley) callejones-ocultos
implicaci´on (complicity) envolvimiento
</table>
<bodyText confidence="0.9982316875">
tire set of senses for the corpus. In contrast, if these
two senses belong to the same concept in the Con-
cept Model, the difference in the sense conditionals
will be highlighted since the normalization occurs
over a very small set of senses — the senses for
only that concept, which in the best possible sce-
nario will contain only the two contending senses,
as in concept of our example.
As can be seen from Table 1, the Concept Model
not only outperforms the Sense Model, it does so
with significantly fewer parameters. This may be
counter-intuitive since Concept Model involves an
extra concept variable. However, the dissociation of
Spanish and English senses can significantly reduce
the parameter space. Imagine two Spanish words
that are associated with ten English senses and ac-
cordingly each of them has a probability for belong-
ing to each of these ten senses. Aided with a con-
cept variable, it is possible to model the same re-
lationship by creating a separate Spanish sense that
contains these two words and relating this Spanish
sense with the ten English senses through a concept
variable. Thus these words now need to belong to
only one sense as opposed to ten. Of course, now
there are new transition probabilities for each of the
eleven senses from the new concept node. The exact
reduction in the parameter space will depend on the
frequent subsets discovered for the s of the
Spanish words. Longer and more frequent subsets
will lead to larger reductions. It must also be borne
in mind that this reduction comes with the indepen-
dence assumptions made in the Concept Model.
</bodyText>
<sectionHeader confidence="0.991291" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999997923076923">
We have presented two novel probabilistic models
for unsupervised word sense disambiguation using
parallel corpora and have shown that both models
outperform existing unsupervised approaches. In
addition, we have shown that our second model,
the Concept model, can be used to learn a sense
inventory for the secondary language. An advan-
tage of the probabilistic models is that they can eas-
ily incorporate additional information, such as con-
text information. In future work, we plan to investi-
gate the use of additional monolingual context. We
would also like to perform additional validation of
the learned secondary language sense inventory.
</bodyText>
<sectionHeader confidence="0.99925" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998996">
The authors would like to thank Mona Diab and
Philip Resnik for many helpful discussions and in-
sightful comments for improving the paper and also
for making their data available for our experiments.
This study was supported by NSF Grant 0308030.
</bodyText>
<sectionHeader confidence="0.999038" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999251134146341">
E. Agirre, J. Atserias, L. Padr, and G. Rigau. 2000.
Combining supervised and unsupervised lexical
knowledge methods for word sense disambigua-
tion. In Computers and the Humanities, Special
Double Issue on SensEval. Eds. Martha Palmer
and Adam Kilgarriff. 34:1,2.
Yoshua Bengio and Christopher Kermorvant. 2003.
Extracting hidden sense probabilities from bi-
texts. Technical report, TR 1231, Departement
d’informatique et recherche operationnelle, Uni-
versite de Montreal.
Peter F. Brown, Stephen Della Pietra, Vin-
cent J. Della Pietra, and Robert L. Mercer.
1991. Word-sense disambiguation using statisti-
cal methods. In Meeting of the Association for
Computational Linguistics, pages 264–270.
Rebecca Bruce and Janyce Wiebe. 1994. A new
approach to sense identification. In ARPA Work-
shop on Human Language Technology.
Ido Dagan and Alon Itai. 1994. Word sense disam-
biguation using a second language monolingual
corpus. Computational Linguistics, 20(4):563–
596.
Ido Dagan. 1991. Lexical disambiguation: Sources
of information and their statistical realization. In
Meeting of the Association for Computational
Linguistics, pages 341–342.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977.
Maximum likelihood from incomplete data via
the EM algorithm. Journal of the Royal Statis-
tical Society, B 39:1–38.
Mona Diab and Philip Resnik. 2002. An unsuper-
vised method for word sense tagging using paral-
lel corpora. In Proceedings of the 40th Anniver-
sary Meeting of the Association for Computa-
tional Linguistics (ACL-02).
Mona Diab. 2003. Word Sense Disambiguation
Within a Multilingual Framework. Ph.D. thesis,
University of Maryland, College Park.
Christiane Fellbaum. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Nancy Ide and Jean Veronis. 1998. Word sense dis-
ambiguation: The state of the art. Computational
Linguistics, 28(1):1–40.
Nancy Ide. 2000. Cross-lingual sense determina-
tion: Can it work? In Computers and the Hu-
manities: Special Issue on Senseval, 34:147-152.
Adam Kilgarrif and Joseph Rosenzweig. 2000.
Framework and results for english senseval.
Computers and the Humanities, 34(1):15–48.
Dekang Lin. 2000. Word sense disambiguation
with a similarity based smoothed library. In
Computers and the Humanities: Special Issue on
Senseval, 34:147-152.
K. C. Litkowski. 2000. Senseval: The cl research
experience. In Computers and the Humanities,
34(1-2), pp. 153-8.
Philip Resnik and David Yarowsky. 1999. Distin-
guishing systems and distinguishing senses: new
evaluation methods for word sense disambigua-
tion. Natural Language Engineering, 5(2).
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In
Proceedings of the International Joint Confer-
ence on Artificial Intelligence, pages 448–453.
Philip Resnik. 1997. Selectional preference and
sense disambiguation. In Proceedings of ACL
Siglex Workshop on Tagging Text with Lexical
Semantics, Why, What and How?, Washington,
April 4-5.
David Yarowsky. 1992. Word-sense disambigua-
tion using statistical models of Roget’s cate-
gories trained on large corpora. In Proceedings
of COLING-92, pages 454–460, Nantes, France,
July.
David Yarowsky. 1993. One sense per collocation.
In Proceedings, ARPA Human Language Tech-
nology Workshop, Princeton.
David Yarowsky. 1995. Unsupervised word sense
disambiguation rivaling supervised methods. In
Meeting of the Association for Computational
Linguistics, pages 189–196.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.847255">
<title confidence="0.99878">Unsupervised Sense Disambiguation Using Bilingual Probabilistic Models</title>
<author confidence="0.993563">Indrajit Bhattacharya</author>
<affiliation confidence="0.999982">Dept. of Computer Science University of Maryland</affiliation>
<address confidence="0.9969085">College Park, MD, USA</address>
<email confidence="0.999281">indrajit@cs.umd.edu</email>
<author confidence="0.984505">Lise Getoor</author>
<affiliation confidence="0.9999665">Dept. of Computer Science University of Maryland</affiliation>
<address confidence="0.9974175">College Park, MD, USA</address>
<email confidence="0.999821">getoor@cs.umd.edu</email>
<author confidence="0.956842">Yoshua Bengio</author>
<affiliation confidence="0.9947475">Dept. IRO Universit´e de Montr´eal</affiliation>
<address confidence="0.974865">Montr´eal, Qu´ebec, Canada</address>
<email confidence="0.989747">bengioy@IRO.UMontreal.CA</email>
<abstract confidence="0.999162352941176">We describe two probabilistic models for unsupervised word-sense disambiguation using parallel corpora. The first model, which we call the Sense model, builds on the work of Diab and Resnik (2002) that uses both parallel text and a sense inventory for the target language, and recasts their approach in a probabilistic framework. The second model, which we call the Concept model, is a hierarchical model that uses a concept latent variable to relate different language specific sense labels. We show that both models improve performance on the word sense disambiguation task over previous unsupervised approaches, with the Concept model showing the largest improvement. Furthermore, in learning the Concept model, as a by-product, we learn a sense inventory for the parallel language.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>J Atserias</author>
<author>L Padr</author>
<author>G Rigau</author>
</authors>
<title>Combining supervised and unsupervised lexical knowledge methods for word sense disambiguation.</title>
<date>2000</date>
<booktitle>In Computers and the Humanities, Special Double Issue on SensEval. Eds. Martha Palmer and Adam Kilgarriff.</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1927" citStr="Agirre et al., 2000" startWordPosition="282" endWordPosition="285"> inception. WSD is fundamental to natural language understanding and is a useful intermediate step for many other language processing tasks (Ide and Veronis, 1998). Many recent approaches make use of ideas from statistical machine learning; the availability of shared sense definitions (e.g. WordNet (Fellbaum, 1998)) and recent international competitions (Kilgarrif and Rosenzweig, 2000) have enabled researchers to compare their results. Supervised approaches which make use of a small hand-labeled training set (Bruce and Wiebe, 1994; Yarowsky, 1993) typically outperform unsupervised approaches (Agirre et al., 2000; Litkowski, 2000; Lin, 2000; Resnik, 1997; Yarowsky, 1992; Yarowsky, 1995), but tend to be tuned to a specific corpus and are constrained by scarcity of labeled data. In an effort to overcome the difficulty of finding sense-labeled training data, researchers have begun investigating unsupervised approaches to wordsense disambiguation. For example, the use of parallel corpora for sense tagging can help with word sense disambiguation (Brown et al., 1991; Dagan, 1991; Dagan and Itai, 1994; Ide, 2000; Resnik and Yarowsky, 1999). As an illustration of sense disambiguation from translation data, wh</context>
</contexts>
<marker>Agirre, Atserias, Padr, Rigau, 2000</marker>
<rawString>E. Agirre, J. Atserias, L. Padr, and G. Rigau. 2000. Combining supervised and unsupervised lexical knowledge methods for word sense disambiguation. In Computers and the Humanities, Special Double Issue on SensEval. Eds. Martha Palmer and Adam Kilgarriff. 34:1,2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Christopher Kermorvant</author>
</authors>
<title>Extracting hidden sense probabilities from bitexts.</title>
<date>2003</date>
<tech>Technical report, TR 1231,</tech>
<institution>Departement</institution>
<contexts>
<context position="2878" citStr="Bengio and Kermorvant (2003)" startWordPosition="433" endWordPosition="436">isambiguation. For example, the use of parallel corpora for sense tagging can help with word sense disambiguation (Brown et al., 1991; Dagan, 1991; Dagan and Itai, 1994; Ide, 2000; Resnik and Yarowsky, 1999). As an illustration of sense disambiguation from translation data, when the English word bank is translated to Spanish as orilla, it is clear that we are referring to the shore sense of bank, rather than the financial institution sense. The main inspiration for our work is Diab and Resnik (2002), who use translations and linguistic knowledge for disambiguation and automatic sense tagging. Bengio and Kermorvant (2003) present a graphical model that is an attempt to formalize probabilistically the main ideas in Diab and Resnik (2002). They assume the same semantic hierarchy (in particular, WordNet) for both the languages and assign English words as well as their translations to WordNet synsets. Here we present two variants of the graphical model in Bengio and Kermorvant (2003), along with a method to discover a cluster structure for the Spanish senses. We also present empirical word sense disambiguation results which demonstrate the gain brought by this probabilistic approach, even while only using the tran</context>
<context position="8289" citStr="Bengio and Kermorvant (2003)" startWordPosition="1344" endWordPosition="1348">e formally describe the two generative models that use these ideas in Subsections 2.2 and 2.3. Figure 1: Graphical Representations of the a) Sense Model and the b) Concept Model 2.1 Notation Throughout, we use uppercase letters to denote random variables and lowercase letters to denote specific instances of the random variables. A translation pair is ( , ) where the subscript and indicate the primary language (English) and the secondary language (Spanish). and . We use the shorthand for . 2.2 The Sense Model The Sense Model makes the assumption, inspired by ideas in Diab and Resnik (2002) and Bengio and Kermorvant (2003), that the English word and the Spanish word in a translation pair share the same precise sense. In other words, the set of sense labels for the words in the two languages is the same and may be collapsed into one set of senses that is responsible for both English and Spanish words and the single latent variable in the model is the sense label for both words and . We also make the assumption that the words in both languages are conditionally independent given the sense label. The generative parameters for the model are the prior a) Sense Model b) Concept Model concept sense word We Ws T Te Ts </context>
<context position="11106" citStr="Bengio and Kermorvant (2003)" startWordPosition="1851" endWordPosition="1854"> independent of each other given the shared concept . The generative parameters in the model are the prior probabilities over the concepts, the conditional probabilities and for the English and Spanish senses given the concept, and the conditional probabilities and for the words and in each language given their senses. We can now imagine the generative process of a translation pair by the Concept Model as first selecting a concept according to the priors, then a sense for each language given the concept, and finally a word for each sense using the conditional probabilities of the words. As in Bengio and Kermorvant (2003), this generative procedure may be captured by factoring the joint distribution using the conditional independence assumptions as . The Concept model is shown in Figure 1(b). 3 Constructing the Senses and Concepts Building the structure of the model is crucial for our task. Choosing the dimensionality of the hidden variables by selecting the number of senses and concepts, as well as taking advantage of prior knowledge to impose constraints, are very important aspects of building the structure. If certain words are not possible for a given sense, or certain senses are not possible for a given c</context>
</contexts>
<marker>Bengio, Kermorvant, 2003</marker>
<rawString>Yoshua Bengio and Christopher Kermorvant. 2003. Extracting hidden sense probabilities from bitexts. Technical report, TR 1231, Departement d’informatique et recherche operationnelle, Universite de Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>Word-sense disambiguation using statistical methods.</title>
<date>1991</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>264--270</pages>
<contexts>
<context position="2383" citStr="Brown et al., 1991" startWordPosition="354" endWordPosition="357">ches which make use of a small hand-labeled training set (Bruce and Wiebe, 1994; Yarowsky, 1993) typically outperform unsupervised approaches (Agirre et al., 2000; Litkowski, 2000; Lin, 2000; Resnik, 1997; Yarowsky, 1992; Yarowsky, 1995), but tend to be tuned to a specific corpus and are constrained by scarcity of labeled data. In an effort to overcome the difficulty of finding sense-labeled training data, researchers have begun investigating unsupervised approaches to wordsense disambiguation. For example, the use of parallel corpora for sense tagging can help with word sense disambiguation (Brown et al., 1991; Dagan, 1991; Dagan and Itai, 1994; Ide, 2000; Resnik and Yarowsky, 1999). As an illustration of sense disambiguation from translation data, when the English word bank is translated to Spanish as orilla, it is clear that we are referring to the shore sense of bank, rather than the financial institution sense. The main inspiration for our work is Diab and Resnik (2002), who use translations and linguistic knowledge for disambiguation and automatic sense tagging. Bengio and Kermorvant (2003) present a graphical model that is an attempt to formalize probabilistically the main ideas in Diab and R</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1991</marker>
<rawString>Peter F. Brown, Stephen Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1991. Word-sense disambiguation using statistical methods. In Meeting of the Association for Computational Linguistics, pages 264–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Bruce</author>
<author>Janyce Wiebe</author>
</authors>
<title>A new approach to sense identification.</title>
<date>1994</date>
<booktitle>In ARPA Workshop on Human Language Technology.</booktitle>
<contexts>
<context position="1844" citStr="Bruce and Wiebe, 1994" startWordPosition="271" endWordPosition="274">WSD) has been a central question in the computational linguistics community since its inception. WSD is fundamental to natural language understanding and is a useful intermediate step for many other language processing tasks (Ide and Veronis, 1998). Many recent approaches make use of ideas from statistical machine learning; the availability of shared sense definitions (e.g. WordNet (Fellbaum, 1998)) and recent international competitions (Kilgarrif and Rosenzweig, 2000) have enabled researchers to compare their results. Supervised approaches which make use of a small hand-labeled training set (Bruce and Wiebe, 1994; Yarowsky, 1993) typically outperform unsupervised approaches (Agirre et al., 2000; Litkowski, 2000; Lin, 2000; Resnik, 1997; Yarowsky, 1992; Yarowsky, 1995), but tend to be tuned to a specific corpus and are constrained by scarcity of labeled data. In an effort to overcome the difficulty of finding sense-labeled training data, researchers have begun investigating unsupervised approaches to wordsense disambiguation. For example, the use of parallel corpora for sense tagging can help with word sense disambiguation (Brown et al., 1991; Dagan, 1991; Dagan and Itai, 1994; Ide, 2000; Resnik and Ya</context>
</contexts>
<marker>Bruce, Wiebe, 1994</marker>
<rawString>Rebecca Bruce and Janyce Wiebe. 1994. A new approach to sense identification. In ARPA Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Alon Itai</author>
</authors>
<title>Word sense disambiguation using a second language monolingual corpus.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<pages>596</pages>
<contexts>
<context position="2418" citStr="Dagan and Itai, 1994" startWordPosition="360" endWordPosition="363">nd-labeled training set (Bruce and Wiebe, 1994; Yarowsky, 1993) typically outperform unsupervised approaches (Agirre et al., 2000; Litkowski, 2000; Lin, 2000; Resnik, 1997; Yarowsky, 1992; Yarowsky, 1995), but tend to be tuned to a specific corpus and are constrained by scarcity of labeled data. In an effort to overcome the difficulty of finding sense-labeled training data, researchers have begun investigating unsupervised approaches to wordsense disambiguation. For example, the use of parallel corpora for sense tagging can help with word sense disambiguation (Brown et al., 1991; Dagan, 1991; Dagan and Itai, 1994; Ide, 2000; Resnik and Yarowsky, 1999). As an illustration of sense disambiguation from translation data, when the English word bank is translated to Spanish as orilla, it is clear that we are referring to the shore sense of bank, rather than the financial institution sense. The main inspiration for our work is Diab and Resnik (2002), who use translations and linguistic knowledge for disambiguation and automatic sense tagging. Bengio and Kermorvant (2003) present a graphical model that is an attempt to formalize probabilistically the main ideas in Diab and Resnik (2002). They assume the same </context>
</contexts>
<marker>Dagan, Itai, 1994</marker>
<rawString>Ido Dagan and Alon Itai. 1994. Word sense disambiguation using a second language monolingual corpus. Computational Linguistics, 20(4):563– 596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
</authors>
<title>Lexical disambiguation: Sources of information and their statistical realization.</title>
<date>1991</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>341--342</pages>
<contexts>
<context position="2396" citStr="Dagan, 1991" startWordPosition="358" endWordPosition="359">of a small hand-labeled training set (Bruce and Wiebe, 1994; Yarowsky, 1993) typically outperform unsupervised approaches (Agirre et al., 2000; Litkowski, 2000; Lin, 2000; Resnik, 1997; Yarowsky, 1992; Yarowsky, 1995), but tend to be tuned to a specific corpus and are constrained by scarcity of labeled data. In an effort to overcome the difficulty of finding sense-labeled training data, researchers have begun investigating unsupervised approaches to wordsense disambiguation. For example, the use of parallel corpora for sense tagging can help with word sense disambiguation (Brown et al., 1991; Dagan, 1991; Dagan and Itai, 1994; Ide, 2000; Resnik and Yarowsky, 1999). As an illustration of sense disambiguation from translation data, when the English word bank is translated to Spanish as orilla, it is clear that we are referring to the shore sense of bank, rather than the financial institution sense. The main inspiration for our work is Diab and Resnik (2002), who use translations and linguistic knowledge for disambiguation and automatic sense tagging. Bengio and Kermorvant (2003) present a graphical model that is an attempt to formalize probabilistically the main ideas in Diab and Resnik (2002).</context>
</contexts>
<marker>Dagan, 1991</marker>
<rawString>Ido Dagan. 1991. Lexical disambiguation: Sources of information and their statistical realization. In Meeting of the Association for Computational Linguistics, pages 341–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, B</journal>
<pages>39--1</pages>
<contexts>
<context position="16395" citStr="Dempster et al., 1977" startWordPosition="2792" endWordPosition="2795"> Now we build the concepts from the Spanish sense clusters. We recall that a concept is defined by a set of English senses and a set of Spanish senses that are related. Each cluster represents a concept. A particular concept is formed by the set of Spanish senses in the cluster and the English senses relevant for them. The relevant English senses for any Spanish sense is given by its . Therefore, the union of the s of all the Spanish senses in the cluster forms the set of English senses for each concept. 4 Learning the Model Parameters Once the model is built, we use the popular EM algorithm (Dempster et al., 1977) for hidden variables to learn the parameters for both models. The algorithm repeatedly iterates over two steps. The first step maximizes the expected log-likelihood of the joint probability of the observed data with the current parameter settings . The next step then reestimates the values of the parameters of the model. Below we summarize the re-estimation steps for each model. 4.1 EM for the Sense Model follows similarly. 4.2 EM for the Concept Model follow similarly. 4.3 Initialization of Model Probabilities Since the EM algorithm performs gradient ascent as it iteratively improves the log</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, B 39:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Philip Resnik</author>
</authors>
<title>An unsupervised method for word sense tagging using parallel corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics (ACL-02).</booktitle>
<contexts>
<context position="2754" citStr="Diab and Resnik (2002)" startWordPosition="417" endWordPosition="420">ty of finding sense-labeled training data, researchers have begun investigating unsupervised approaches to wordsense disambiguation. For example, the use of parallel corpora for sense tagging can help with word sense disambiguation (Brown et al., 1991; Dagan, 1991; Dagan and Itai, 1994; Ide, 2000; Resnik and Yarowsky, 1999). As an illustration of sense disambiguation from translation data, when the English word bank is translated to Spanish as orilla, it is clear that we are referring to the shore sense of bank, rather than the financial institution sense. The main inspiration for our work is Diab and Resnik (2002), who use translations and linguistic knowledge for disambiguation and automatic sense tagging. Bengio and Kermorvant (2003) present a graphical model that is an attempt to formalize probabilistically the main ideas in Diab and Resnik (2002). They assume the same semantic hierarchy (in particular, WordNet) for both the languages and assign English words as well as their translations to WordNet synsets. Here we present two variants of the graphical model in Bengio and Kermorvant (2003), along with a method to discover a cluster structure for the Spanish senses. We also present empirical word se</context>
<context position="8256" citStr="Diab and Resnik (2002)" startWordPosition="1339" endWordPosition="1342">for sense disambiguation. We formally describe the two generative models that use these ideas in Subsections 2.2 and 2.3. Figure 1: Graphical Representations of the a) Sense Model and the b) Concept Model 2.1 Notation Throughout, we use uppercase letters to denote random variables and lowercase letters to denote specific instances of the random variables. A translation pair is ( , ) where the subscript and indicate the primary language (English) and the secondary language (Spanish). and . We use the shorthand for . 2.2 The Sense Model The Sense Model makes the assumption, inspired by ideas in Diab and Resnik (2002) and Bengio and Kermorvant (2003), that the English word and the Spanish word in a translation pair share the same precise sense. In other words, the set of sense labels for the words in the two languages is the same and may be collapsed into one set of senses that is responsible for both English and Spanish words and the single latent variable in the model is the sense label for both words and . We also make the assumption that the words in both languages are conditionally independent given the sense label. The generative parameters for the model are the prior a) Sense Model b) Concept Model </context>
<context position="19358" citStr="Diab and Resnik (2002)" startWordPosition="3288" endWordPosition="3291"> our models on Senseval data.2 In addition, the Concept Model learns a sense structure for the Spanish 2Accuracy is the ratio of the number of correct predictions and the number of attempted predictions. Recall is the ratio of the number of correct predictions and the size of the test set. language. While it is hard to objectively evaluate the quality of such a structure, we present some interesting concepts that are learned as an indication of the potential of our approach. 5.1 Evaluation with Senseval Data In our experiments with real data, we make use of the parallel corpora constructed by Diab and Resnik (2002) for evaluation purposes. We chose to work on these corpora in order to permit a direct comparison with their results. The sense-tagged portion of the English corpus is comprised of the English “allwords” section of the SENSEVAL-2 test data. The remainder of this corpus is constructed by adding the Brown Corpus, the SENSEVAL-1 corpus, the SENSEVAL-2 English Lexical Sample test, trial and training corpora and the Wall Street Journal sections 18-24 from the Penn Treebank. This English corpus is translated into Spanish using two commercially available MT systems: Globalink Pro 6.4 and Systran Pro</context>
<context position="20995" citStr="Diab and Resnik (2002)" startWordPosition="3551" endWordPosition="3554">f which there are 875 occurrences in our tagged data. The sense tags for the English domain are derived from the WordNet 1.7 inventory. After pruning stopwords, we end up with 16,186 English words, 31,862 Spanish words and 2,385,574 instances of 41,850 distinct translation pairs. The English words come from 20,361 WordNet senses. Table 1: Comparison with Diab’s Model Model Accuracy Recall Parameters Diab 0.618 0.572 - Sense M. 0.624 0.616 154,947 Concept M. 0.672 0.651 120,268 As can be seen from the following table, both our models clearly outperform Diab (2003), which is an improvement over Diab and Resnik (2002), in both accuracy and recall, while the Concept Model does significantly better than the Sense Model with fewer parameters. The comparison is restricted to the same subset of the test data. For our best results, the Sense Model has 20,361 senses, while the Concept Model has 20,361 English senses, 11,961 Spanish senses and 7,366 concepts. The Concept Model results are for the version that allows multiple senses for a Spanish word. Results for the and 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Accuracy Figure 3: Comparison with Senseval2 Systems single-sense model are similar. In Figure 3, we compare the </context>
<context position="23163" citStr="Diab and Resnik (2002)" startWordPosition="3913" endWordPosition="3916"> different kinds of association and involves three different senses containing the word conexi´on. The other words in two of these senses suggest that they are about union and relation respectively. The third probably involves the link sense of connection. Conciseness of the concepts depends on the similarity threshold that is selected. Some may bring together loosely-related topics, which can be separated by a higher threshold. 6 Model Analysis In this section, we back up our experimental results with an in-depth analysis of the performance of our two models. Our Sense Model was motivated by Diab and Resnik (2002) but the flavors of the two are quite 3Some English words are found to occur in the Spanish Senses. This is because the machine translation system used to create the Spanish document left certain words untranslated. different. The most important distinction is that the Sense Model is a probabilistic generative model for parallel corpora, where interaction between different words stemming from the same sense comes into play, even if the words are not related through translations, and this interdependence of the senses through common words plays a role in sense disambiguation. We started off wit</context>
</contexts>
<marker>Diab, Resnik, 2002</marker>
<rawString>Mona Diab and Philip Resnik. 2002. An unsupervised method for word sense tagging using parallel corpora. In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics (ACL-02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
</authors>
<title>Word Sense Disambiguation Within a Multilingual Framework.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Maryland, College Park.</institution>
<contexts>
<context position="20942" citStr="Diab (2003)" startWordPosition="3544" endWordPosition="3545">our experiments, we focus only on nouns, of which there are 875 occurrences in our tagged data. The sense tags for the English domain are derived from the WordNet 1.7 inventory. After pruning stopwords, we end up with 16,186 English words, 31,862 Spanish words and 2,385,574 instances of 41,850 distinct translation pairs. The English words come from 20,361 WordNet senses. Table 1: Comparison with Diab’s Model Model Accuracy Recall Parameters Diab 0.618 0.572 - Sense M. 0.624 0.616 154,947 Concept M. 0.672 0.651 120,268 As can be seen from the following table, both our models clearly outperform Diab (2003), which is an improvement over Diab and Resnik (2002), in both accuracy and recall, while the Concept Model does significantly better than the Sense Model with fewer parameters. The comparison is restricted to the same subset of the test data. For our best results, the Sense Model has 20,361 senses, while the Concept Model has 20,361 English senses, 11,961 Spanish senses and 7,366 concepts. The Concept Model results are for the version that allows multiple senses for a Spanish word. Results for the and 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Accuracy Figure 3: Comparison with Senseval2 Systems single-</context>
</contexts>
<marker>Diab, 2003</marker>
<rawString>Mona Diab. 2003. Word Sense Disambiguation Within a Multilingual Framework. Ph.D. thesis, University of Maryland, College Park.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1624" citStr="Fellbaum, 1998" startWordPosition="242" endWordPosition="243"> with the Concept model showing the largest improvement. Furthermore, in learning the Concept model, as a by-product, we learn a sense inventory for the parallel language. 1 Introduction Word sense disambiguation (WSD) has been a central question in the computational linguistics community since its inception. WSD is fundamental to natural language understanding and is a useful intermediate step for many other language processing tasks (Ide and Veronis, 1998). Many recent approaches make use of ideas from statistical machine learning; the availability of shared sense definitions (e.g. WordNet (Fellbaum, 1998)) and recent international competitions (Kilgarrif and Rosenzweig, 2000) have enabled researchers to compare their results. Supervised approaches which make use of a small hand-labeled training set (Bruce and Wiebe, 1994; Yarowsky, 1993) typically outperform unsupervised approaches (Agirre et al., 2000; Litkowski, 2000; Lin, 2000; Resnik, 1997; Yarowsky, 1992; Yarowsky, 1995), but tend to be tuned to a specific corpus and are constrained by scarcity of labeled data. In an effort to overcome the difficulty of finding sense-labeled training data, researchers have begun investigating unsupervised</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Jean Veronis</author>
</authors>
<title>Word sense disambiguation: The state of the art.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="1471" citStr="Ide and Veronis, 1998" startWordPosition="217" endWordPosition="220">ferent language specific sense labels. We show that both models improve performance on the word sense disambiguation task over previous unsupervised approaches, with the Concept model showing the largest improvement. Furthermore, in learning the Concept model, as a by-product, we learn a sense inventory for the parallel language. 1 Introduction Word sense disambiguation (WSD) has been a central question in the computational linguistics community since its inception. WSD is fundamental to natural language understanding and is a useful intermediate step for many other language processing tasks (Ide and Veronis, 1998). Many recent approaches make use of ideas from statistical machine learning; the availability of shared sense definitions (e.g. WordNet (Fellbaum, 1998)) and recent international competitions (Kilgarrif and Rosenzweig, 2000) have enabled researchers to compare their results. Supervised approaches which make use of a small hand-labeled training set (Bruce and Wiebe, 1994; Yarowsky, 1993) typically outperform unsupervised approaches (Agirre et al., 2000; Litkowski, 2000; Lin, 2000; Resnik, 1997; Yarowsky, 1992; Yarowsky, 1995), but tend to be tuned to a specific corpus and are constrained by sc</context>
</contexts>
<marker>Ide, Veronis, 1998</marker>
<rawString>Nancy Ide and Jean Veronis. 1998. Word sense disambiguation: The state of the art. Computational Linguistics, 28(1):1–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
</authors>
<title>Cross-lingual sense determination: Can it work?</title>
<date>2000</date>
<booktitle>In Computers and the Humanities: Special Issue on Senseval,</booktitle>
<pages>34--147</pages>
<contexts>
<context position="2429" citStr="Ide, 2000" startWordPosition="364" endWordPosition="365">t (Bruce and Wiebe, 1994; Yarowsky, 1993) typically outperform unsupervised approaches (Agirre et al., 2000; Litkowski, 2000; Lin, 2000; Resnik, 1997; Yarowsky, 1992; Yarowsky, 1995), but tend to be tuned to a specific corpus and are constrained by scarcity of labeled data. In an effort to overcome the difficulty of finding sense-labeled training data, researchers have begun investigating unsupervised approaches to wordsense disambiguation. For example, the use of parallel corpora for sense tagging can help with word sense disambiguation (Brown et al., 1991; Dagan, 1991; Dagan and Itai, 1994; Ide, 2000; Resnik and Yarowsky, 1999). As an illustration of sense disambiguation from translation data, when the English word bank is translated to Spanish as orilla, it is clear that we are referring to the shore sense of bank, rather than the financial institution sense. The main inspiration for our work is Diab and Resnik (2002), who use translations and linguistic knowledge for disambiguation and automatic sense tagging. Bengio and Kermorvant (2003) present a graphical model that is an attempt to formalize probabilistically the main ideas in Diab and Resnik (2002). They assume the same semantic hi</context>
</contexts>
<marker>Ide, 2000</marker>
<rawString>Nancy Ide. 2000. Cross-lingual sense determination: Can it work? In Computers and the Humanities: Special Issue on Senseval, 34:147-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarrif</author>
<author>Joseph Rosenzweig</author>
</authors>
<title>Framework and results for english senseval.</title>
<date>2000</date>
<journal>Computers and the Humanities,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="1696" citStr="Kilgarrif and Rosenzweig, 2000" startWordPosition="248" endWordPosition="252"> Furthermore, in learning the Concept model, as a by-product, we learn a sense inventory for the parallel language. 1 Introduction Word sense disambiguation (WSD) has been a central question in the computational linguistics community since its inception. WSD is fundamental to natural language understanding and is a useful intermediate step for many other language processing tasks (Ide and Veronis, 1998). Many recent approaches make use of ideas from statistical machine learning; the availability of shared sense definitions (e.g. WordNet (Fellbaum, 1998)) and recent international competitions (Kilgarrif and Rosenzweig, 2000) have enabled researchers to compare their results. Supervised approaches which make use of a small hand-labeled training set (Bruce and Wiebe, 1994; Yarowsky, 1993) typically outperform unsupervised approaches (Agirre et al., 2000; Litkowski, 2000; Lin, 2000; Resnik, 1997; Yarowsky, 1992; Yarowsky, 1995), but tend to be tuned to a specific corpus and are constrained by scarcity of labeled data. In an effort to overcome the difficulty of finding sense-labeled training data, researchers have begun investigating unsupervised approaches to wordsense disambiguation. For example, the use of paralle</context>
</contexts>
<marker>Kilgarrif, Rosenzweig, 2000</marker>
<rawString>Adam Kilgarrif and Joseph Rosenzweig. 2000. Framework and results for english senseval. Computers and the Humanities, 34(1):15–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Word sense disambiguation with a similarity based smoothed library.</title>
<date>2000</date>
<booktitle>In Computers and the Humanities: Special Issue on Senseval,</booktitle>
<pages>34--147</pages>
<contexts>
<context position="1955" citStr="Lin, 2000" startWordPosition="288" endWordPosition="289">ral language understanding and is a useful intermediate step for many other language processing tasks (Ide and Veronis, 1998). Many recent approaches make use of ideas from statistical machine learning; the availability of shared sense definitions (e.g. WordNet (Fellbaum, 1998)) and recent international competitions (Kilgarrif and Rosenzweig, 2000) have enabled researchers to compare their results. Supervised approaches which make use of a small hand-labeled training set (Bruce and Wiebe, 1994; Yarowsky, 1993) typically outperform unsupervised approaches (Agirre et al., 2000; Litkowski, 2000; Lin, 2000; Resnik, 1997; Yarowsky, 1992; Yarowsky, 1995), but tend to be tuned to a specific corpus and are constrained by scarcity of labeled data. In an effort to overcome the difficulty of finding sense-labeled training data, researchers have begun investigating unsupervised approaches to wordsense disambiguation. For example, the use of parallel corpora for sense tagging can help with word sense disambiguation (Brown et al., 1991; Dagan, 1991; Dagan and Itai, 1994; Ide, 2000; Resnik and Yarowsky, 1999). As an illustration of sense disambiguation from translation data, when the English word bank is </context>
</contexts>
<marker>Lin, 2000</marker>
<rawString>Dekang Lin. 2000. Word sense disambiguation with a similarity based smoothed library. In Computers and the Humanities: Special Issue on Senseval, 34:147-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K C Litkowski</author>
</authors>
<title>Senseval: The cl research experience.</title>
<date>2000</date>
<booktitle>In Computers and the Humanities,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1944" citStr="Litkowski, 2000" startWordPosition="286" endWordPosition="287">ndamental to natural language understanding and is a useful intermediate step for many other language processing tasks (Ide and Veronis, 1998). Many recent approaches make use of ideas from statistical machine learning; the availability of shared sense definitions (e.g. WordNet (Fellbaum, 1998)) and recent international competitions (Kilgarrif and Rosenzweig, 2000) have enabled researchers to compare their results. Supervised approaches which make use of a small hand-labeled training set (Bruce and Wiebe, 1994; Yarowsky, 1993) typically outperform unsupervised approaches (Agirre et al., 2000; Litkowski, 2000; Lin, 2000; Resnik, 1997; Yarowsky, 1992; Yarowsky, 1995), but tend to be tuned to a specific corpus and are constrained by scarcity of labeled data. In an effort to overcome the difficulty of finding sense-labeled training data, researchers have begun investigating unsupervised approaches to wordsense disambiguation. For example, the use of parallel corpora for sense tagging can help with word sense disambiguation (Brown et al., 1991; Dagan, 1991; Dagan and Itai, 1994; Ide, 2000; Resnik and Yarowsky, 1999). As an illustration of sense disambiguation from translation data, when the English wo</context>
</contexts>
<marker>Litkowski, 2000</marker>
<rawString>K. C. Litkowski. 2000. Senseval: The cl research experience. In Computers and the Humanities, 34(1-2), pp. 153-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>David Yarowsky</author>
</authors>
<title>Distinguishing systems and distinguishing senses: new evaluation methods for word sense disambiguation.</title>
<date>1999</date>
<journal>Natural Language Engineering,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="2457" citStr="Resnik and Yarowsky, 1999" startWordPosition="366" endWordPosition="369">d Wiebe, 1994; Yarowsky, 1993) typically outperform unsupervised approaches (Agirre et al., 2000; Litkowski, 2000; Lin, 2000; Resnik, 1997; Yarowsky, 1992; Yarowsky, 1995), but tend to be tuned to a specific corpus and are constrained by scarcity of labeled data. In an effort to overcome the difficulty of finding sense-labeled training data, researchers have begun investigating unsupervised approaches to wordsense disambiguation. For example, the use of parallel corpora for sense tagging can help with word sense disambiguation (Brown et al., 1991; Dagan, 1991; Dagan and Itai, 1994; Ide, 2000; Resnik and Yarowsky, 1999). As an illustration of sense disambiguation from translation data, when the English word bank is translated to Spanish as orilla, it is clear that we are referring to the shore sense of bank, rather than the financial institution sense. The main inspiration for our work is Diab and Resnik (2002), who use translations and linguistic knowledge for disambiguation and automatic sense tagging. Bengio and Kermorvant (2003) present a graphical model that is an attempt to formalize probabilistically the main ideas in Diab and Resnik (2002). They assume the same semantic hierarchy (in particular, Word</context>
</contexts>
<marker>Resnik, Yarowsky, 1999</marker>
<rawString>Philip Resnik and David Yarowsky. 1999. Distinguishing systems and distinguishing senses: new evaluation methods for word sense disambiguation. Natural Language Engineering, 5(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence,</booktitle>
<pages>448--453</pages>
<contexts>
<context position="15662" citStr="Resnik (1995)" startWordPosition="2660" endWordPosition="2661">ns to choose from here. We use a simple measure that counts the number of common items in the two s.1 The similarity measure is now used to cluster the Spanish senses . Since this measure is not transitive, it does not directly define equivalence classes over . Instead, we get a similarity graph where the vertices are the Spanish senses and we add an edge between two senses if their similarity is above a threshold. We now pick each connected component from this graph as a cluster of similar Spanish senses. &apos;Another option would be to use a measure of similarity for English senses, proposed in Resnik (1995) for two synsets in a concept hierarchy like WordNet. Our initial results with this measure were not favorable. Now we build the concepts from the Spanish sense clusters. We recall that a concept is defined by a set of English senses and a set of Spanish senses that are related. Each cluster represents a concept. A particular concept is formed by the set of Spanish senses in the cluster and the English senses relevant for them. The relevant English senses for any Spanish sense is given by its . Therefore, the union of the s of all the Spanish senses in the cluster forms the set of English sens</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the International Joint Conference on Artificial Intelligence, pages 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional preference and sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL Siglex Workshop on Tagging Text with Lexical Semantics, Why, What and How?,</booktitle>
<pages>4--5</pages>
<location>Washington,</location>
<contexts>
<context position="1969" citStr="Resnik, 1997" startWordPosition="290" endWordPosition="291">e understanding and is a useful intermediate step for many other language processing tasks (Ide and Veronis, 1998). Many recent approaches make use of ideas from statistical machine learning; the availability of shared sense definitions (e.g. WordNet (Fellbaum, 1998)) and recent international competitions (Kilgarrif and Rosenzweig, 2000) have enabled researchers to compare their results. Supervised approaches which make use of a small hand-labeled training set (Bruce and Wiebe, 1994; Yarowsky, 1993) typically outperform unsupervised approaches (Agirre et al., 2000; Litkowski, 2000; Lin, 2000; Resnik, 1997; Yarowsky, 1992; Yarowsky, 1995), but tend to be tuned to a specific corpus and are constrained by scarcity of labeled data. In an effort to overcome the difficulty of finding sense-labeled training data, researchers have begun investigating unsupervised approaches to wordsense disambiguation. For example, the use of parallel corpora for sense tagging can help with word sense disambiguation (Brown et al., 1991; Dagan, 1991; Dagan and Itai, 1994; Ide, 2000; Resnik and Yarowsky, 1999). As an illustration of sense disambiguation from translation data, when the English word bank is translated to </context>
</contexts>
<marker>Resnik, 1997</marker>
<rawString>Philip Resnik. 1997. Selectional preference and sense disambiguation. In Proceedings of ACL Siglex Workshop on Tagging Text with Lexical Semantics, Why, What and How?, Washington, April 4-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Word-sense disambiguation using statistical models of Roget’s categories trained on large corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING-92,</booktitle>
<pages>454--460</pages>
<location>Nantes, France,</location>
<contexts>
<context position="1985" citStr="Yarowsky, 1992" startWordPosition="292" endWordPosition="293">g and is a useful intermediate step for many other language processing tasks (Ide and Veronis, 1998). Many recent approaches make use of ideas from statistical machine learning; the availability of shared sense definitions (e.g. WordNet (Fellbaum, 1998)) and recent international competitions (Kilgarrif and Rosenzweig, 2000) have enabled researchers to compare their results. Supervised approaches which make use of a small hand-labeled training set (Bruce and Wiebe, 1994; Yarowsky, 1993) typically outperform unsupervised approaches (Agirre et al., 2000; Litkowski, 2000; Lin, 2000; Resnik, 1997; Yarowsky, 1992; Yarowsky, 1995), but tend to be tuned to a specific corpus and are constrained by scarcity of labeled data. In an effort to overcome the difficulty of finding sense-labeled training data, researchers have begun investigating unsupervised approaches to wordsense disambiguation. For example, the use of parallel corpora for sense tagging can help with word sense disambiguation (Brown et al., 1991; Dagan, 1991; Dagan and Itai, 1994; Ide, 2000; Resnik and Yarowsky, 1999). As an illustration of sense disambiguation from translation data, when the English word bank is translated to Spanish as orill</context>
</contexts>
<marker>Yarowsky, 1992</marker>
<rawString>David Yarowsky. 1992. Word-sense disambiguation using statistical models of Roget’s categories trained on large corpora. In Proceedings of COLING-92, pages 454–460, Nantes, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>One sense per collocation.</title>
<date>1993</date>
<booktitle>In Proceedings, ARPA Human Language Technology Workshop,</booktitle>
<location>Princeton.</location>
<contexts>
<context position="1861" citStr="Yarowsky, 1993" startWordPosition="275" endWordPosition="276"> question in the computational linguistics community since its inception. WSD is fundamental to natural language understanding and is a useful intermediate step for many other language processing tasks (Ide and Veronis, 1998). Many recent approaches make use of ideas from statistical machine learning; the availability of shared sense definitions (e.g. WordNet (Fellbaum, 1998)) and recent international competitions (Kilgarrif and Rosenzweig, 2000) have enabled researchers to compare their results. Supervised approaches which make use of a small hand-labeled training set (Bruce and Wiebe, 1994; Yarowsky, 1993) typically outperform unsupervised approaches (Agirre et al., 2000; Litkowski, 2000; Lin, 2000; Resnik, 1997; Yarowsky, 1992; Yarowsky, 1995), but tend to be tuned to a specific corpus and are constrained by scarcity of labeled data. In an effort to overcome the difficulty of finding sense-labeled training data, researchers have begun investigating unsupervised approaches to wordsense disambiguation. For example, the use of parallel corpora for sense tagging can help with word sense disambiguation (Brown et al., 1991; Dagan, 1991; Dagan and Itai, 1994; Ide, 2000; Resnik and Yarowsky, 1999). As</context>
</contexts>
<marker>Yarowsky, 1993</marker>
<rawString>David Yarowsky. 1993. One sense per collocation. In Proceedings, ARPA Human Language Technology Workshop, Princeton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="2002" citStr="Yarowsky, 1995" startWordPosition="294" endWordPosition="295">l intermediate step for many other language processing tasks (Ide and Veronis, 1998). Many recent approaches make use of ideas from statistical machine learning; the availability of shared sense definitions (e.g. WordNet (Fellbaum, 1998)) and recent international competitions (Kilgarrif and Rosenzweig, 2000) have enabled researchers to compare their results. Supervised approaches which make use of a small hand-labeled training set (Bruce and Wiebe, 1994; Yarowsky, 1993) typically outperform unsupervised approaches (Agirre et al., 2000; Litkowski, 2000; Lin, 2000; Resnik, 1997; Yarowsky, 1992; Yarowsky, 1995), but tend to be tuned to a specific corpus and are constrained by scarcity of labeled data. In an effort to overcome the difficulty of finding sense-labeled training data, researchers have begun investigating unsupervised approaches to wordsense disambiguation. For example, the use of parallel corpora for sense tagging can help with word sense disambiguation (Brown et al., 1991; Dagan, 1991; Dagan and Itai, 1994; Ide, 2000; Resnik and Yarowsky, 1999). As an illustration of sense disambiguation from translation data, when the English word bank is translated to Spanish as orilla, it is clear th</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Meeting of the Association for Computational Linguistics, pages 189–196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>