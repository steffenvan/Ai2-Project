<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001288">
<title confidence="0.954371">
Position Specific Posterior Lattices for Indexing Speech
</title>
<author confidence="0.975993">
Ciprian Chelba and Alex Acero
</author>
<affiliation confidence="0.96633">
Microsoft Research
Microsoft Corporation
</affiliation>
<address confidence="0.948538">
One Microsoft Way
Redmond, WA 98052
</address>
<email confidence="0.997153">
{chelba, alexac}@microsoft.com
</email>
<sectionHeader confidence="0.99986" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998075">
Ever increasing computing power and connectivity
bandwidth together with falling storage costs re-
sult in an overwhelming amount of data of vari-
ous types being produced, exchanged, and stored.
Consequently, search has emerged as a key applica-
tion as more and more data is being saved (Church,
2003). Text search in particular is the most active
area, with applications that range from web and in-
tranet search to searching for private information re-
siding on one’s hard-drive.
Speech search has not received much attention
due to the fact that large collections of untranscribed
spoken material have not been available, mostly
due to storage constraints. As storage is becoming
cheaper, the availability and usefulness of large col-
lections of spoken documents is limited strictly by
the lack of adequate technology to exploit them.
Manually transcribing speech is expensive and
sometimes outright impossible due to privacy con-
cerns. This leads us to exploring an automatic ap-
proach to searching and navigating spoken docu-
ment collections.
Our current work aims at extending the standard
keyword search paradigm from text documents to
spoken documents. In order to deal with limitations
of current automatic speech recognition (ASR) tech-
nology we propose an approach that uses recogni-
tion lattices — which are considerably more accu-
rate than the ASR 1-best output.
A novel contribution is the use of a representation
of ASR lattices which retains only position informa-
tion for each word. The Position Specific Posterior
</bodyText>
<sectionHeader confidence="0.666881" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.9979629">
The paper presents the Position Specific
Posterior Lattice, a novel representation
of automatic speech recognition lattices
that naturally lends itself to efficient in-
dexing of position information and subse-
quent relevance ranking of spoken docu-
ments using proximity.
In experiments performed on a collec-
tion of lecture recordings — MIT iCam-
pus data — the spoken document rank-
ing accuracy was improved by 20% rela-
tive over the commonly used baseline of
indexing the 1-best output from an auto-
matic speech recognizer. The Mean Aver-
age Precision (MAP) increased from 0.53
when using 1-best output to 0.62 when us-
ing the new lattice representation. The ref-
erence used for evaluation is the output of
a standard retrieval engine working on the
manual transcription of the speech collec-
tion.
Albeit lossy, the PSPL lattice is also much
more compact than the ASR 3-gram lat-
tice from which it is computed — which
translates in reduced inverted index size
as well — at virtually no degradation in
word-error-rate performance. Since new
paths are introduced in the lattice, the OR-
ACLE accuracy increases over the origi-
nal ASR lattice.
</bodyText>
<page confidence="0.990334">
443
</page>
<note confidence="0.9916265">
Proceedings of the 43rd Annual Meeting of the ACL, pages 443–450,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.9985273">
Lattice (PSPL) is a lossy but compact representa-
tion of a speech recognition lattice that lends itself
to the standard inverted indexing done in text search
— which retains the position as well as other con-
textual information for each hit.
Since our aim is to bridge the gap between text
and speech -grade search technology, we take as our
reference the output of a text retrieval engine that
runs on the manual transcription.
The rest of the paper is structured as follows: in
the next section we review previous work in the
area, followed by Section 3 which presents a brief
overview of state-of-the-art text search technology.
We then introduce the PSPL representation in Sec-
tion 4 and explain its use for indexing and searching
speech in the next section. Experiments evaluating
ASR accuracy on iCampus, highlighting empirical
aspects of PSPL lattices as well as search accuracy
results are reported in Section 6. We conclude by
outlining future work.
</bodyText>
<sectionHeader confidence="0.996144" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999927136986302">
The main research effort aiming at spoken docu-
ment retrieval (SDR) was centered around the SDR-
TREC evaluations (Garofolo et al., 2000), although
there is a large body of work in this area prior to
the SDR-TREC evaluations, as well as more recent
work outside this community. Most notable are the
contributions of (Brown et al., 1996) and (James,
1995).
One problem encountered in work published prior
or outside the SDR-TREC community is that it
doesn’t always evaluate performance from a doc-
ument retrieval point of view — using a metric
like Mean Average Precision (MAP) or similar, see
trec_eval (NIST, www) — but rather uses word-
spotting measures, which are more technology-
rather than user- centric. We believe that ultimately
it is the document retrieval performance that matters
and the word-spotting accuracy is just an indicator
for how a SDR system might be improved.
The TREC-SDR 8/9 evaluations — (Garofolo et
al., 2000) Section 6 — focused on using Broadcast
News speech from various sources: CNN, ABC,
PRI, Voice of America. About 550 hrs of speech
were segmented manually into 21,574 stories each
comprising about 250 words on the average. The
approximate manual transcriptions — closed cap-
tioning for video — used for SDR system compar-
ison with text-only retrieval performance had fairly
high WER: 14.5% video and 7.5% radio broadcasts.
ASR systems tuned to the Broadcast News domain
were evaluated on detailed manual transcriptions
and were able to achieve 15-20% WER, not far from
the accuracy of the approximate manual transcrip-
tions. In order to evaluate the accuracy of retrieval
systems, search queries —“topics” — along with bi-
nary relevance judgments were compiled by human
assessors.
SDR systems indexed the ASR 1-best output and
their retrieval performance — measured in terms of
MAP — was found to be flat with respect to ASR
WER variations in the range of 15%-30%. Simply
having a common task and an evaluation-driven col-
laborative research effort represents a huge gain for
the community. There are shortcomings however to
the SDR-TREC framework.
It is well known that ASR systems are very brit-
tle to mismatched training/test conditions and it is
unrealistic to expect error rates in the range 10-15%
when decoding speech mismatched with respect to
the training data. It is thus very important to con-
sider ASR operating points which have higher WER.
Also, the out-of-vocabulary (OOV) rate was very
low, below 1%. Since the “topics”/queries were
long and stated in plain English rather than using
the keyword search paradigm, the query-side OOV
(Q-OOV) was very low as well, an unrealistic situ-
ation in practice. (Woodland et al., 2000) evaluates
the effect of Q-OOV rate on retrieval performance
by reducing the ASR vocabulary size such that the
Q-OOV rate comes closer to 15%, a much more re-
alistic figure since search keywords are typically rare
words. They show severe degradation in MAP per-
formance — 50% relative, from 44 to 22.
The most common approach to dealing with OOV
query words is to represent both the query and the
spoken document using sub-word units — typically
phones or phone n-grams — and then match se-
quences of such units. In his thesis, (Ng, 2000)
shows the feasibility of sub-word SDR and advo-
cates for tighter integration between ASR and IR
technology. Similar conclusions are drawn by the
excellent work in (Siegler, 1999).
As pointed out in (Logan et al., 2002), word level
</bodyText>
<page confidence="0.998389">
444
</page>
<bodyText confidence="0.999977846153846">
indexing and querying is still more accurate, were
it not for the OOV problem. The authors argue in
favor of a combination of word and sub-word level
indexing. Another problem pointed out by the pa-
per is the abundance of word-spotting false-positives
in the sub-word retrieval case, somewhat masked by
the MAP measure.
Similar approaches are taken by (Seide and Yu,
2004). One interesting feature of this work is a two-
pass system whereby an approximate match is car-
ried out at the document level after which the costly
detailed phonetic match is carried out on only 15%
of the documents in the collection.
More recently, (Saraclar and Sproat, 2004) shows
improvement in word-spotting accuracy by using
lattices instead of 1-best. An inverted index from
symbols — word or phone — to links allows to
evaluate adjacency of query words but more gen-
eral proximity information is harder to obtain — see
Section 4. Although no formal comparison has been
carried out, we believe our approach should yield a
more compact index.
Before discussing our architectural design deci-
sions it is probably useful to give a brief presentation
of a state-of-the-art text document retrieval engine
that is using the keyword search paradigm.
</bodyText>
<sectionHeader confidence="0.99733" genericHeader="method">
3 Text Document Retrieval
</sectionHeader>
<bodyText confidence="0.999444285714286">
Probably the most widespread text retrieval model is
the TF-IDF vector model (Baeza-Yates and Ribeiro-
Neto, 1999). For a given query 2 = q1 ... qi ... qQ
and document Dj one calculates a similarity mea-
sure by accumulating the TF-IDF score wij for each
query term qi, possibly weighted by a document spe-
cific weight:
</bodyText>
<equation confidence="0.989389">
S(Dj, 2) =
wij = fij · idfi
</equation>
<bodyText confidence="0.99993855">
where fij is the normalized frequency of word qi in
document Dj and the inverse document frequency
for query term qi is idfi = log n where N is the
total number of documents in the collection and ni
is the number of documents containing qi.
The main criticism to the TF-IDF relevance score
is the fact that the query terms are assumed to be
independent. Proximity information is not taken into
account at all, e.g. whether the words LANGUAGE
and MODELING occur next to each other or not in
a document is not used for relevance scoring.
Another issue is that query terms may be encoun-
tered in different contexts in a given document: ti-
tle, abstract, author name, font size, etc. For hy-
pertext document collections even more context in-
formation is available: anchor text, as well as other
mark-up tags designating various parts of a given
document being just a few examples. The TF-IDF
ranking scheme completely discards such informa-
tion although it is clearly important in practice.
</bodyText>
<subsectionHeader confidence="0.999234">
3.1 Early Google Approach
</subsectionHeader>
<bodyText confidence="0.99998456">
Aside from the use of PageRank for relevance rank-
ing, (Brin and Page, 1998) also uses both proxim-
ity and context information heavily when assigning
a relevance score to a given document — see Sec-
tion 4.5.1 of (Brin and Page, 1998) for details.
For each given query term qi one retrieves the list
of hits corresponding to qi in document D. Hits
can be of various types depending on the context in
which the hit occurred: title, anchor text, etc. Each
type of hit has its own type-weight and the type-
weights are indexed by type.
For a single word query, their ranking algorithm
takes the inner-product between the type-weight
vector and a vector consisting of count-weights (ta-
pered counts such that the effect of large counts is
discounted) and combines the resulting score with
PageRank in a final relevance score.
For multiple word queries, terms co-occurring in a
given document are considered as forming different
proximity-types based on their proximity, from adja-
cent to “not even close”. Each proximity type comes
with a proximity-weight and the relevance score in-
cludes the contribution of proximity information by
taking the inner product over all types, including the
proximity ones.
</bodyText>
<subsectionHeader confidence="0.991191">
3.2 Inverted Index
</subsectionHeader>
<bodyText confidence="0.999991714285714">
Of essence to fast retrieval on static document col-
lections of medium to large size is the use of an in-
verted index. The inverted index stores a list of hits
for each word in a given vocabulary. The hits are
grouped by document. For each document, the list
of hits for a given query term must include position
— needed to evaluate counts of proximity types —
</bodyText>
<equation confidence="0.989171666666667">
� Q
i=1
wij
</equation>
<page confidence="0.988185">
445
</page>
<bodyText confidence="0.999935">
as well as all the context information needed to cal-
culate the relevance score of a given document us-
ing the scheme outlined previously. For details, the
reader is referred to (Brin and Page, 1998), Sec-
tion 4.
</bodyText>
<sectionHeader confidence="0.868995" genericHeader="method">
4 Position Specific Posterior Lattices
</sectionHeader>
<bodyText confidence="0.979792392857143">
As highlighted in the previous section, position in-
formation is crucial for being able to evaluate prox-
imity information when assigning a relevance score
to a given document.
In the spoken document case however, we are
faced with a dilemma. On one hand, using 1-best
ASR output as the transcription to be indexed is sub-
optimal due to the high WER, which is likely to lead
to low recall — query terms that were in fact spo-
ken are wrongly recognized and thus not retrieved.
On the other hand, ASR lattices do have much bet-
ter WER — in our case the 1-best WER was 55%
whereas the lattice WER was 30% — but the posi-
tion information is not readily available: it is easy to
evaluate whether two words are adjacent but ques-
tions about the distance in number of links between
the occurrences of two query words in the lattice are
very hard to answer.
The position information needed for recording a
given word hit is not readily available in ASR lat-
tices — for details on the format of typical ASR
lattices and the information stored in such lattices
the reader is referred to (Young et al., 2002). To
simplify the discussion let’s consider that a tradi-
tional text-document hit for given word consists of
just (document id, position).
The occurrence of a given word in a lattice ob-
tained from a given spoken document is uncertain
and so is the position at which the word occurs in
the document.
The ASR lattices do contain the information
needed to evaluate proximity information, since on a
given path through the lattice we can easily assign a
position index to each link/word in the normal way.
Each path occurs with a given posterior probability,
easily computable from the lattice, so in principle
one could index soft-hits which specify
(document id, position,
posterior probability)
for each word in the lattice. Since it is likely that
more than one path contains the same word in the
same position, one would need to sum over all pos-
sible paths in a lattice that contain a given word at a
given position.
A simple dynamic programming algorithm which
is a variation on the standard forward-backward al-
gorithm can be employed for performing this com-
putation. The computation for the backward pass
stays unchanged, whereas during the forward pass
one needs to split the forward probability arriving
at a given node n, αn, according to the length l —
measured in number of links along the partial path
that contain a word; null (E) links are not counted
when calculating path length — of the partial paths
that start at the start node of the lattice and end at
node n:
</bodyText>
<equation confidence="0.9101195">
αn[l] . = � P(7r)
7r:end(7r)=n,length(7r)=l
</equation>
<bodyText confidence="0.999711285714286">
The backward probability on has the standard defi-
nition (Rabiner, 1989).
To formalize the calculation of the position-
specific forward-backward pass, the initialization,
and one elementary forward step in the forward pass
are carried out using Eq. (1), respectively— see Fig-
ure 1 for notation:
</bodyText>
<equation confidence="0.9850766">
αn[l + 1] = � q αsa[l + S(li, E)] · P(li)
i=1
1.0, l = 0
αstart[l] = {0.0 l =6 0 (1)
,
</equation>
<bodyText confidence="0.986378">
The “probability” P(li) of a given link li is stored
as a log-probability and commonly evaluated in
ASR using:
</bodyText>
<equation confidence="0.980897666666667">
log P(li) = FLATw · [1/LMw · log PAM(li)+
log PLM(word(li)) − 1/LMw · logPIP] (2)
s_ 1
</equation>
<figureCaption confidence="0.644122">
Figure 1: State Transitions
</figureCaption>
<equation confidence="0.985065166666667">
s_ i
P (l_1 )
P ( l_i )
s_q
P (l_q )
n
</equation>
<page confidence="0.977024">
446
</page>
<bodyText confidence="0.998814">
where log PAM(li) is the acoustic model score,
log PLM(word(li)) is the language model score,
LMw &gt; 0 is the language model weight, logPIP &gt;
0 is the “insertion penalty” and FLATw is a flat-
tening weight. In N-gram lattices where N ≥ 2,
all links ending at a given node n must contain the
same word word(n), so the posterior probability of
a given word w occurring at a given position l can
be easily calculated using:
</bodyText>
<equation confidence="0.94590475">
P(w,l|LAT) =
EnR
αn[l] ·βn · δ w Word (n))
/ten s.t. αn[l]·Nn &gt;0 βstart ( , ( ))
</equation>
<bodyText confidence="0.993583">
The Position Specific Posterior Lattice (PSPL) is a
representation of the P(w, l|LAT) distribution: for
each position bin l store the words w along with their
posterior probability P(w, l|LAT).
</bodyText>
<sectionHeader confidence="0.968892" genericHeader="method">
5 Spoken Document Indexing and Search
Using PSPL
</sectionHeader>
<bodyText confidence="0.999906125">
Spoken documents rarely contain only speech. Of-
ten they have a title, author and creation date. There
might also be a text abstract associated with the
speech, video or even slides in some standard for-
mat. The idea of saving context information when
indexing HTML documents and web pages can thus
be readily used for indexing spoken documents, al-
though the context information is of a different na-
ture.
As for the actual speech content of a spoken doc-
ument, the previous section showed how ASR tech-
nology and PSPL lattices can be used to automati-
cally convert it to a format that allows the indexing
of soft hits — a soft index stores posterior proba-
bility along with the position information for term
occurrences in a given document.
</bodyText>
<subsectionHeader confidence="0.986311">
5.1 Speech Content Indexing Using PSPL
</subsectionHeader>
<bodyText confidence="0.999820083333333">
Speech content can be very long. In our case the
speech content of a typical spoken document was ap-
proximately 1 hr long; it is customary to segment a
given speech file in shorter segments.
A spoken document thus consists of an ordered
list of segments. For each segment we generate a
corresponding PSPL lattice. Each document and
each segment in a given collection are mapped to an
integer value using a collection descriptorfile which
lists all documents and segments. Each soft hit in
our index will store the PSPL position and posterior
probability.
</bodyText>
<subsectionHeader confidence="0.9622865">
5.2 Speech Content Relevance Ranking Using
PSPL Representation
</subsectionHeader>
<bodyText confidence="0.99777435">
Consider a given query Q = q1 ... qi ... qQ and
a spoken document D represented as a PSPL. Our
ranking scheme follows the description in Sec-
tion 3.1.
The words in the document D clearly belong to
the ASR vocabulary V whereas the words in the
query may be out-of-vocabulary (OOV). As argued
in Section 2, the query-OOV rate is an important
factor in evaluating the impact of having a finite
ASR vocabulary on the retrieval accuracy. We as-
sume that the words in the query are all contained
in V; OOV words are mapped to UNK and cannot be
matched in any document D.
For all query terms, a 1-gram score is calculated
by summing the PSPL posterior probability across
all segments s and positions k. This is equivalent
to calculating the expected count of a given query
term qi according to the PSPL probability distribu-
tion P(wk(s)|D) for each segment s of document
D. The results are aggregated in a common value
</bodyText>
<equation confidence="0.9416324">
S1−gram(D, Q):
S(D, qi) = log L1 + E E P(wk(s) = qi|D)J
s k
S1−gram(D, Q) = � Q S(D, qi) (3)
i=1
</equation>
<bodyText confidence="0.9927452">
Similar to (Brin and Page, 1998), the logarithmic ta-
pering off is used for discounting the effect of large
counts in a given document.
Our current ranking scheme takes into account
proximity in the form of matching N-grams present
in the query. Similar to the 1-gram case, we cal-
culate an expected tapered-count for each N-gram
qi ... qi+N−1 in the query and then aggregate the re-
sults in a common value SN−gram(D, Q) for each
order N:
</bodyText>
<equation confidence="0.972726">
S(D, qi ... qi+N−1) = (4)
log−[1 + Ks Ek FI1 01 P(wk+l (s) = qi+l  |D)J
S(D, qi ... qi+N−1)
SN−gram(D, Q) = Q−N+1
</equation>
<page confidence="0.771581333333333">
�
i=1
447
</page>
<bodyText confidence="0.997898">
The different proximity types, one for each N-
gram order allowed by the query length, are com-
bined by taking the inner product with a vector of
weights.
</bodyText>
<equation confidence="0.996384">
Q
S(D, 2) _ L WN - SN−g,..(D, 2) (5)
N=1
</equation>
<bodyText confidence="0.999829166666667">
Only documents containing all the terms in the
query are returned. In the current implementation
the weights increase linearly with the N-gram order.
Clearly, better weight assignments must exist, and
as the hit types are enriched beyond using just N-
grams, the weights will have to be determined using
machine learning techniques.
It is worth noting that the transcription for any
given segment can also be represented as a PSPL
with exactly one word per position bin. It is easy to
see that in this case the relevance scores calculated
according to Eq. (3-4) are the ones specified by 3.1.
</bodyText>
<sectionHeader confidence="0.999049" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999935571428571">
We have carried all our experiments on the iCampus
corpus prepared by MIT CSAIL. The main advan-
tages of the corpus are: realistic speech recording
conditions — all lectures are recorded using a lapel
microphone — and the availability of accurate man-
ual transcriptions — which enables the evaluation of
a SDR system against its text counterpart.
</bodyText>
<subsectionHeader confidence="0.99747">
6.1 iCampus Corpus
</subsectionHeader>
<bodyText confidence="0.999985641025641">
The iCampus corpus (Glass et al., 2004) consists
of about 169 hours of lecture materials: 20 Intro-
duction to Computer Programming Lectures (21.7
hours), 35 Linear Algebra Lectures (27.7 hours), 35
Electro-magnetic Physics Lectures (29.1 hours), 79
Assorted MIT World seminars covering a wide vari-
ety of topics (89.9 hours). Each lecture comes with
a word-level manual transcription that segments the
text into semantic units that could be thought of as
sentences; word-level time-alignments between the
transcription and the speech are also provided. The
speech style is in between planned and spontaneous.
The speech is recorded at a sampling rate of 16kHz
(wide-band) using a lapel microphone.
The speech was segmented at the sentence level
based on the time alignments; each lecture is consid-
ered to be a spoken document consisting of a set of
one-sentence long segments determined this way —
see Section 5.1. The final collection consists of 169
documents, 66,102 segments and an average docu-
ment length of 391 segments.
We have then used a standard large vocabulary
ASR system for generating 3-gram ASR lattices and
PSPL lattices. The 3-gram language model used for
decoding is trained on a large amount of text data,
primarily newswire text. The vocabulary of the ASR
system consisted of 110kwds, selected based on fre-
quency in the training data. The acoustic model
is trained on a variety of wide-band speech and it
is a standard clustered tri-phone, 3-states-per-phone
model. Neither model has been tuned in any way to
the iCampus scenario.
On the first lecture L01 of the Introduction to
Computer Programming Lectures the WER of the
ASR system was 44.7%; the OOV rate was 3.3%.
For the entire set of lectures in the Introduction
to Computer Programming Lectures, the WER was
54.8%, with a maximum value of 74% and a mini-
mum value of 44%.
</bodyText>
<subsectionHeader confidence="0.997969">
6.2 PSPL lattices
</subsectionHeader>
<bodyText confidence="0.9997516">
We have then proceeded to generate 3-gram lattices
and PSPL lattices using the above ASR system. Ta-
ble 1 compares the accuracy/size of the 3-gram lat-
tices and the resulting PSPL lattices for the first lec-
ture L01. As it can be seen the PSPL represen-
</bodyText>
<table confidence="0.999657833333333">
Lattice Type 3-gram PSPL
Size on disk 11.3MB 3.2MB
Link density 16.3 14.6
Node density 7.4 1.1
1-best WER 44.7% 45%
ORACLE WER 26.4% 21.7%
</table>
<tableCaption confidence="0.999913">
Table 1: Comparison between 3-gram and PSPL lat-
</tableCaption>
<bodyText confidence="0.9128277">
tices for lecture L01 (iCampus corpus): node and
link density, 1-best and ORACLE WER, size on disk
tation is much more compact than the original 3-
gram lattices at a very small loss in accuracy: the
1-best path through the PSPL lattice is only 0.3%
absolute worse than the one through the original 3-
gram lattice. As expected, the main reduction comes
from the drastically smaller node density — 7 times
smaller, measured in nodes per word in the refer-
ence transcription. Since the PSPL representation
</bodyText>
<page confidence="0.997185">
448
</page>
<bodyText confidence="0.999921166666667">
introduces new paths compared to the original 3-
gram lattice, the ORACLE WER path — least error-
ful path in the lattice — is also about 20% relative
better than in the original 3-gram lattice — 5% ab-
solute. Also to be noted is the much better WER in
both PSPL/3-gram lattices versus 1-best.
</bodyText>
<subsectionHeader confidence="0.990004">
6.3 Spoken Document Retrieval
</subsectionHeader>
<bodyText confidence="0.999990588235294">
Our aim is to narrow the gap between speech and
text document retrieval. We have thus taken as our
reference the output of a standard retrieval engine
working according to one of the TF-IDF flavors, see
Section 3. The engine indexes the manual transcrip-
tion using an unlimited vocabulary. All retrieval re-
sults presented in this section have used the stan-
dard trec_eval package used by the TREC eval-
uations.
The PSPL lattices for each segment in the spo-
ken document collection were indexed as explained
in 5.1. In addition, we generated the PSPL repre-
sentation of the manual transcript and of the 1-best
ASR output and indexed those as well. This allows
us to compare our retrieval results against the results
obtained using the reference engine when working
on the same text document collection.
</bodyText>
<subsectionHeader confidence="0.779646">
6.3.1 Query Collection and Retrieval Setup
</subsectionHeader>
<bodyText confidence="0.977389315789474">
The missing ingredient for performing retrieval
experiments are the queries. We have asked a few
colleagues to issue queries against a demo shell us-
ing the index built from the manual transcription.
The only information1 provided to them was the
same as the summary description in Section 6.1.
We have collected 116 queries in this manner. The
query out-of-vocabulary rate (Q-OOV) was 5.2%
and the average query length was 1.97 words. Since
our approach so far does not index sub-word units,
we cannot deal with OOV query words. We have
thus removed the queries which contained OOV
words — resulting in a set of 96 queries — which
clearly biases the evaluation. On the other hand, the
results on both the 1-best and the lattice indexes are
equally favored by this.
1Arguably, more motivated users that are also more famil-
iar with the document collection would provide a better query
collection framework
</bodyText>
<subsectionHeader confidence="0.635075">
6.3.2 Retrieval Experiments
</subsectionHeader>
<bodyText confidence="0.998515">
We have carried out retrieval experiments in the
above setup. Indexes have been built from:
</bodyText>
<listItem confidence="0.999923">
• trans: manual transcription filtered through
ASR vocabulary
• 1-best: ASR 1-best output
• lat: PSPL lattices.
</listItem>
<bodyText confidence="0.998667090909091">
No tuning of retrieval weights, see Eq. (5), or link
scoring weights, see Eq. (2) has been performed. Ta-
ble 2 presents the results. As a sanity check, the re-
trieval results on transcription — trans — match
almost perfectly the reference. The small difference
comes from stemming rules that the baseline engine
is using for query enhancement which are not repli-
cated in our retrieval engine. The results on lat-
tices (lat) improve significantly on (1-best) —
20% relative improvement in mean average preci-
sion (MAP).
</bodyText>
<table confidence="0.9987875">
trans 1-best lat
# docs retrieved 1411 3206 4971
# relevant docs 1416 1416 1416
# rel retrieved 1411 1088 1301
MAP 0.99 0.53 0.62
R-precision 0.99 0.53 0.58
</table>
<tableCaption confidence="0.808256333333333">
Table 2: Retrieval performance on indexes built
from transcript, ASR 1-best and PSPL lattices, re-
spectively
</tableCaption>
<subsectionHeader confidence="0.621287">
6.3.3 Why Would This Work?
</subsectionHeader>
<bodyText confidence="0.9999110625">
A legitimate question at this point is: why would
anyone expect this to work when the 1-best ASR ac-
curacy is so poor?
In favor of our approach, the ASR lattice WER is
much lower than the 1-best WER, and PSPL have
even lower WER than the ASR lattices. As re-
ported in Table 1, the PSPL WER for L01 was
22% whereas the 1-best WER was 45%. Consider
matching a 2-gram in the PSPL —the average query
length is indeed 2 wds so this is a representative sit-
uation. A simple calculation reveals that it is twice
— (1 − 0.22)2/(1 − 0.45)2 = 2 — more likely to
find a query match in the PSPL than in the 1-best —
if the query 2-gram was indeed spoken at that posi-
tion. According to this heuristic argument one could
expect a dramatic increase in Recall. Another aspect
</bodyText>
<page confidence="0.997748">
449
</page>
<bodyText confidence="0.9999794">
is that people enter typical N-grams as queries. The
contents of adjacent PSPL bins are fairly random in
nature so if a typical 2-gram is found in the PSPL,
chances are it was actually spoken. This translates
in little degradation in Precision.
</bodyText>
<sectionHeader confidence="0.995849" genericHeader="conclusions">
7 Conclusions and Future work
</sectionHeader>
<bodyText confidence="0.999987958333333">
We have developed a new representation for ASR
lattices — the Position Specific Posterior Lattice
(PSPL) — that lends itself naturally to indexing
speech content and integrating state-of-the-art IR
techniques that make use of proximity and context
information. In addition, the PSPL representation is
also much more compact at no loss in WER — both
1-best and ORACLE.
The retrieval results obtained by indexing the
PSPL and performing adequate relevance ranking
are 20% better than when using the ASR 1-best out-
put, although still far from the performance achieved
on text data.
The experiments presented in this paper are truly
a first step. We plan to gather a much larger num-
ber of queries. The binary relevance judgments — a
given document is deemed either relevant or irrele-
vant to a given query in the reference “ranking” —
assumed by the standard trec_eval tool are also
a serious shortcoming; a distance measure between
rankings of documents needs to be used. Finally, us-
ing a baseline engine that in fact makes use of prox-
imity and context information is a priority if such
information is to be used in our algorithms.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999989">
We would like to thank Jim Glass and T J Hazen at
MIT for providing the iCampus data. We would also
like to thank Frank Seide for offering valuable sug-
gestions and our colleagues for providing queries.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999825322033898">
Ricardo Baeza-Yates and Berthier Ribeiro-Neto, 1999.
Modern Information Retrieval, chapter 2, pages 27–
30. Addison Wesley, New York.
Sergey Brin and Lawrence Page. 1998. The anatomy of
a large-scale hypertextual Web search engine. Com-
puter Networks andISDNSystems, 30(1–7):107–117.
M. G. Brown, J. T. Foote, G. J. F. Jones, K. Sp¨arck Jones,
and S. J. Young. 1996. Open-vocabulary speech in-
dexing for voice and video mail retrieval. In Proc.
ACMMultimedia 96, pages 307–316, Boston, Novem-
ber.
Kenneth Ward Church. 2003. Speech and language pro-
cessing: Where have we been and where are we going?
In Proceedings of Eurospeech, Geneva, Switzerland.
J. Garofolo, G. Auzanne, and E. Voorhees. 2000. The
TREC spoken document retrieval track: A success
story. In Proceedings of the Recherche d’Informations
Assiste par Ordinateur: ContentBased Multimedia In-
formation Access Conference, April.
James Glass, T. J. Hazen, Lee Hetherington, and Chao
Wang. 2004. Analysis and processing of lecture audio
data: Preliminary investigations. In HLT-NAACL 2004
Workshop: Interdisciplinary Approaches to Speech
Indexing and Retrieval, pages 9–12, Boston, Mas-
sachusetts, May.
David Anthony James. 1995. The Application of Classi-
cal Information Retrieval Techniques to Spoken Docu-
ments. Ph.D. thesis, University of Cambridge, Down-
ing College.
B. Logan, P. Moreno, and O. Deshmukh. 2002. Word
and sub-word indexing approaches for reducing the ef-
fects of OOV queries on spoken audio. In Proc. HLT.
Kenney Ng. 2000. Subword-Based Approaches for Spo-
ken Document Retrieval. Ph.D. thesis, Massachusetts
Institute of Technology.
NIST. www. The TREC evaluation package. In www-
nlpir.nist.gov/projects/trecvid/trecvid.tools/trec eval.
L. R. Rabiner. 1989. A tutorial on hidden markov mod-
els and selected applications in speech recognition. In
Proceedings IEEE, volume 77(2), pages 257–285.
Murat Saraclar and Richard Sproat. 2004. Lattice-based
search for spoken utterance retrieval. In HLT-NAACL
2004, pages 129–136, Boston, Massachusetts, May.
F. Seide and P. Yu. 2004. Vocabulary-independent search
in spontaneous speech. In Proceedings of ICASSP,
Montreal, Canada.
Matthew A. Siegler. 1999. Integration of Continuous
Speech Recognition and Information Retrievalfor Mu-
tually Optimal Performance. Ph.D. thesis, Carnegie
Mellon University.
P. C. Woodland, S. E. Johnson, P. Jourlin, and K. Sp¨arck
Jones. 2000. Effects of out of vocabulary words in
spoken document retrieval. In Proceedings of SIGIR,
pages 372–374, Athens, Greece.
Steve Young, Gunnar Evermann, Thomas Hain, Dan
Kershaw, Gareth Moore, Julian Odell, Dan Povey
Dave Ollason, Valtcho Valtchev, and Phil Woodland.
2002. The HTK Book. Cambridge University Engi-
neering Department, Cambridge, England, December.
</reference>
<page confidence="0.997869">
450
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.986947">
<title confidence="0.999839">Position Specific Posterior Lattices for Indexing Speech</title>
<author confidence="0.993599">Chelba Acero</author>
<affiliation confidence="0.999743">Microsoft Research Microsoft Corporation</affiliation>
<address confidence="0.996933">One Microsoft Way Redmond, WA 98052</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ricardo Baeza-Yates</author>
<author>Berthier Ribeiro-Neto</author>
</authors>
<title>Modern Information Retrieval,</title>
<date>1999</date>
<volume>2</volume>
<pages>27--30</pages>
<publisher>Addison Wesley,</publisher>
<location>New York.</location>
<marker>Baeza-Yates, Ribeiro-Neto, 1999</marker>
<rawString>Ricardo Baeza-Yates and Berthier Ribeiro-Neto, 1999. Modern Information Retrieval, chapter 2, pages 27– 30. Addison Wesley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual Web search engine.</title>
<date>1998</date>
<journal>Computer Networks andISDNSystems,</journal>
<pages>30--1</pages>
<contexts>
<context position="10079" citStr="Brin and Page, 1998" startWordPosition="1663" endWordPosition="1666">xt to each other or not in a document is not used for relevance scoring. Another issue is that query terms may be encountered in different contexts in a given document: title, abstract, author name, font size, etc. For hypertext document collections even more context information is available: anchor text, as well as other mark-up tags designating various parts of a given document being just a few examples. The TF-IDF ranking scheme completely discards such information although it is clearly important in practice. 3.1 Early Google Approach Aside from the use of PageRank for relevance ranking, (Brin and Page, 1998) also uses both proximity and context information heavily when assigning a relevance score to a given document — see Section 4.5.1 of (Brin and Page, 1998) for details. For each given query term qi one retrieves the list of hits corresponding to qi in document D. Hits can be of various types depending on the context in which the hit occurred: title, anchor text, etc. Each type of hit has its own type-weight and the typeweights are indexed by type. For a single word query, their ranking algorithm takes the inner-product between the type-weight vector and a vector consisting of count-weights (ta</context>
<context position="11788" citStr="Brin and Page, 1998" startWordPosition="1959" endWordPosition="1962">ncluding the proximity ones. 3.2 Inverted Index Of essence to fast retrieval on static document collections of medium to large size is the use of an inverted index. The inverted index stores a list of hits for each word in a given vocabulary. The hits are grouped by document. For each document, the list of hits for a given query term must include position — needed to evaluate counts of proximity types — � Q i=1 wij 445 as well as all the context information needed to calculate the relevance score of a given document using the scheme outlined previously. For details, the reader is referred to (Brin and Page, 1998), Section 4. 4 Position Specific Posterior Lattices As highlighted in the previous section, position information is crucial for being able to evaluate proximity information when assigning a relevance score to a given document. In the spoken document case however, we are faced with a dilemma. On one hand, using 1-best ASR output as the transcription to be indexed is suboptimal due to the high WER, which is likely to lead to low recall — query terms that were in fact spoken are wrongly recognized and thus not retrieved. On the other hand, ASR lattices do have much better WER — in our case the 1-</context>
<context position="18270" citStr="Brin and Page, 1998" startWordPosition="3124" endWordPosition="3127">val accuracy. We assume that the words in the query are all contained in V; OOV words are mapped to UNK and cannot be matched in any document D. For all query terms, a 1-gram score is calculated by summing the PSPL posterior probability across all segments s and positions k. This is equivalent to calculating the expected count of a given query term qi according to the PSPL probability distribution P(wk(s)|D) for each segment s of document D. The results are aggregated in a common value S1−gram(D, Q): S(D, qi) = log L1 + E E P(wk(s) = qi|D)J s k S1−gram(D, Q) = � Q S(D, qi) (3) i=1 Similar to (Brin and Page, 1998), the logarithmic tapering off is used for discounting the effect of large counts in a given document. Our current ranking scheme takes into account proximity in the form of matching N-grams present in the query. Similar to the 1-gram case, we calculate an expected tapered-count for each N-gram qi ... qi+N−1 in the query and then aggregate the results in a common value SN−gram(D, Q) for each order N: S(D, qi ... qi+N−1) = (4) log−[1 + Ks Ek FI1 01 P(wk+l (s) = qi+l |D)J S(D, qi ... qi+N−1) SN−gram(D, Q) = Q−N+1 � i=1 447 The different proximity types, one for each Ngram order allowed by the qu</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual Web search engine. Computer Networks andISDNSystems, 30(1–7):107–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M G Brown</author>
<author>J T Foote</author>
<author>G J F Jones</author>
<author>K Sp¨arck Jones</author>
<author>S J Young</author>
</authors>
<title>Open-vocabulary speech indexing for voice and video mail retrieval.</title>
<date>1996</date>
<booktitle>In Proc. ACMMultimedia 96,</booktitle>
<pages>307--316</pages>
<location>Boston,</location>
<contexts>
<context position="4308" citStr="Brown et al., 1996" startWordPosition="690" endWordPosition="693">nd explain its use for indexing and searching speech in the next section. Experiments evaluating ASR accuracy on iCampus, highlighting empirical aspects of PSPL lattices as well as search accuracy results are reported in Section 6. We conclude by outlining future work. 2 Previous Work The main research effort aiming at spoken document retrieval (SDR) was centered around the SDRTREC evaluations (Garofolo et al., 2000), although there is a large body of work in this area prior to the SDR-TREC evaluations, as well as more recent work outside this community. Most notable are the contributions of (Brown et al., 1996) and (James, 1995). One problem encountered in work published prior or outside the SDR-TREC community is that it doesn’t always evaluate performance from a document retrieval point of view — using a metric like Mean Average Precision (MAP) or similar, see trec_eval (NIST, www) — but rather uses wordspotting measures, which are more technologyrather than user- centric. We believe that ultimately it is the document retrieval performance that matters and the word-spotting accuracy is just an indicator for how a SDR system might be improved. The TREC-SDR 8/9 evaluations — (Garofolo et al., 2000) S</context>
</contexts>
<marker>Brown, Foote, Jones, Jones, Young, 1996</marker>
<rawString>M. G. Brown, J. T. Foote, G. J. F. Jones, K. Sp¨arck Jones, and S. J. Young. 1996. Open-vocabulary speech indexing for voice and video mail retrieval. In Proc. ACMMultimedia 96, pages 307–316, Boston, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
</authors>
<title>Speech and language processing: Where have we been and where are we going?</title>
<date>2003</date>
<booktitle>In Proceedings of Eurospeech,</booktitle>
<location>Geneva, Switzerland.</location>
<marker>Church, 2003</marker>
<rawString>Kenneth Ward Church. 2003. Speech and language processing: Where have we been and where are we going? In Proceedings of Eurospeech, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Garofolo</author>
<author>G Auzanne</author>
<author>E Voorhees</author>
</authors>
<title>The TREC spoken document retrieval track: A success story.</title>
<date>2000</date>
<booktitle>In Proceedings of the Recherche d’Informations Assiste par Ordinateur: ContentBased Multimedia Information Access Conference,</booktitle>
<contexts>
<context position="4109" citStr="Garofolo et al., 2000" startWordPosition="655" endWordPosition="658">xt section we review previous work in the area, followed by Section 3 which presents a brief overview of state-of-the-art text search technology. We then introduce the PSPL representation in Section 4 and explain its use for indexing and searching speech in the next section. Experiments evaluating ASR accuracy on iCampus, highlighting empirical aspects of PSPL lattices as well as search accuracy results are reported in Section 6. We conclude by outlining future work. 2 Previous Work The main research effort aiming at spoken document retrieval (SDR) was centered around the SDRTREC evaluations (Garofolo et al., 2000), although there is a large body of work in this area prior to the SDR-TREC evaluations, as well as more recent work outside this community. Most notable are the contributions of (Brown et al., 1996) and (James, 1995). One problem encountered in work published prior or outside the SDR-TREC community is that it doesn’t always evaluate performance from a document retrieval point of view — using a metric like Mean Average Precision (MAP) or similar, see trec_eval (NIST, www) — but rather uses wordspotting measures, which are more technologyrather than user- centric. We believe that ultimately it </context>
</contexts>
<marker>Garofolo, Auzanne, Voorhees, 2000</marker>
<rawString>J. Garofolo, G. Auzanne, and E. Voorhees. 2000. The TREC spoken document retrieval track: A success story. In Proceedings of the Recherche d’Informations Assiste par Ordinateur: ContentBased Multimedia Information Access Conference, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Glass</author>
<author>T J Hazen</author>
<author>Lee Hetherington</author>
<author>Chao Wang</author>
</authors>
<title>Analysis and processing of lecture audio data: Preliminary investigations.</title>
<date>2004</date>
<booktitle>In HLT-NAACL</booktitle>
<pages>9--12</pages>
<location>Boston, Massachusetts,</location>
<contexts>
<context position="19998" citStr="Glass et al., 2004" startWordPosition="3432" endWordPosition="3435">gment can also be represented as a PSPL with exactly one word per position bin. It is easy to see that in this case the relevance scores calculated according to Eq. (3-4) are the ones specified by 3.1. 6 Experiments We have carried all our experiments on the iCampus corpus prepared by MIT CSAIL. The main advantages of the corpus are: realistic speech recording conditions — all lectures are recorded using a lapel microphone — and the availability of accurate manual transcriptions — which enables the evaluation of a SDR system against its text counterpart. 6.1 iCampus Corpus The iCampus corpus (Glass et al., 2004) consists of about 169 hours of lecture materials: 20 Introduction to Computer Programming Lectures (21.7 hours), 35 Linear Algebra Lectures (27.7 hours), 35 Electro-magnetic Physics Lectures (29.1 hours), 79 Assorted MIT World seminars covering a wide variety of topics (89.9 hours). Each lecture comes with a word-level manual transcription that segments the text into semantic units that could be thought of as sentences; word-level time-alignments between the transcription and the speech are also provided. The speech style is in between planned and spontaneous. The speech is recorded at a samp</context>
</contexts>
<marker>Glass, Hazen, Hetherington, Wang, 2004</marker>
<rawString>James Glass, T. J. Hazen, Lee Hetherington, and Chao Wang. 2004. Analysis and processing of lecture audio data: Preliminary investigations. In HLT-NAACL 2004 Workshop: Interdisciplinary Approaches to Speech Indexing and Retrieval, pages 9–12, Boston, Massachusetts, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Anthony James</author>
</authors>
<title>The Application of Classical Information Retrieval Techniques to Spoken Documents.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge, Downing College.</institution>
<contexts>
<context position="4326" citStr="James, 1995" startWordPosition="695" endWordPosition="696">dexing and searching speech in the next section. Experiments evaluating ASR accuracy on iCampus, highlighting empirical aspects of PSPL lattices as well as search accuracy results are reported in Section 6. We conclude by outlining future work. 2 Previous Work The main research effort aiming at spoken document retrieval (SDR) was centered around the SDRTREC evaluations (Garofolo et al., 2000), although there is a large body of work in this area prior to the SDR-TREC evaluations, as well as more recent work outside this community. Most notable are the contributions of (Brown et al., 1996) and (James, 1995). One problem encountered in work published prior or outside the SDR-TREC community is that it doesn’t always evaluate performance from a document retrieval point of view — using a metric like Mean Average Precision (MAP) or similar, see trec_eval (NIST, www) — but rather uses wordspotting measures, which are more technologyrather than user- centric. We believe that ultimately it is the document retrieval performance that matters and the word-spotting accuracy is just an indicator for how a SDR system might be improved. The TREC-SDR 8/9 evaluations — (Garofolo et al., 2000) Section 6 — focused</context>
</contexts>
<marker>James, 1995</marker>
<rawString>David Anthony James. 1995. The Application of Classical Information Retrieval Techniques to Spoken Documents. Ph.D. thesis, University of Cambridge, Downing College.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Logan</author>
<author>P Moreno</author>
<author>O Deshmukh</author>
</authors>
<title>Word and sub-word indexing approaches for reducing the effects of OOV queries on spoken audio.</title>
<date>2002</date>
<booktitle>In Proc. HLT.</booktitle>
<contexts>
<context position="7391" citStr="Logan et al., 2002" startWordPosition="1201" endWordPosition="1204"> to 15%, a much more realistic figure since search keywords are typically rare words. They show severe degradation in MAP performance — 50% relative, from 44 to 22. The most common approach to dealing with OOV query words is to represent both the query and the spoken document using sub-word units — typically phones or phone n-grams — and then match sequences of such units. In his thesis, (Ng, 2000) shows the feasibility of sub-word SDR and advocates for tighter integration between ASR and IR technology. Similar conclusions are drawn by the excellent work in (Siegler, 1999). As pointed out in (Logan et al., 2002), word level 444 indexing and querying is still more accurate, were it not for the OOV problem. The authors argue in favor of a combination of word and sub-word level indexing. Another problem pointed out by the paper is the abundance of word-spotting false-positives in the sub-word retrieval case, somewhat masked by the MAP measure. Similar approaches are taken by (Seide and Yu, 2004). One interesting feature of this work is a twopass system whereby an approximate match is carried out at the document level after which the costly detailed phonetic match is carried out on only 15% of the docume</context>
</contexts>
<marker>Logan, Moreno, Deshmukh, 2002</marker>
<rawString>B. Logan, P. Moreno, and O. Deshmukh. 2002. Word and sub-word indexing approaches for reducing the effects of OOV queries on spoken audio. In Proc. HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenney Ng</author>
</authors>
<title>Subword-Based Approaches for Spoken Document Retrieval.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="7173" citStr="Ng, 2000" startWordPosition="1167" endWordPosition="1168">low as well, an unrealistic situation in practice. (Woodland et al., 2000) evaluates the effect of Q-OOV rate on retrieval performance by reducing the ASR vocabulary size such that the Q-OOV rate comes closer to 15%, a much more realistic figure since search keywords are typically rare words. They show severe degradation in MAP performance — 50% relative, from 44 to 22. The most common approach to dealing with OOV query words is to represent both the query and the spoken document using sub-word units — typically phones or phone n-grams — and then match sequences of such units. In his thesis, (Ng, 2000) shows the feasibility of sub-word SDR and advocates for tighter integration between ASR and IR technology. Similar conclusions are drawn by the excellent work in (Siegler, 1999). As pointed out in (Logan et al., 2002), word level 444 indexing and querying is still more accurate, were it not for the OOV problem. The authors argue in favor of a combination of word and sub-word level indexing. Another problem pointed out by the paper is the abundance of word-spotting false-positives in the sub-word retrieval case, somewhat masked by the MAP measure. Similar approaches are taken by (Seide and Yu,</context>
</contexts>
<marker>Ng, 2000</marker>
<rawString>Kenney Ng. 2000. Subword-Based Approaches for Spoken Document Retrieval. Ph.D. thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="false">
<authors>
<author>www</author>
</authors>
<title>The TREC evaluation package.</title>
<note>In wwwnlpir.nist.gov/projects/trecvid/trecvid.tools/trec eval.</note>
<marker>www, </marker>
<rawString>NIST. www. The TREC evaluation package. In wwwnlpir.nist.gov/projects/trecvid/trecvid.tools/trec eval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
</authors>
<title>A tutorial on hidden markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>In Proceedings IEEE,</booktitle>
<volume>77</volume>
<issue>2</issue>
<pages>257--285</pages>
<contexts>
<context position="14521" citStr="Rabiner, 1989" startWordPosition="2443" endWordPosition="2444">n the standard forward-backward algorithm can be employed for performing this computation. The computation for the backward pass stays unchanged, whereas during the forward pass one needs to split the forward probability arriving at a given node n, αn, according to the length l — measured in number of links along the partial path that contain a word; null (E) links are not counted when calculating path length — of the partial paths that start at the start node of the lattice and end at node n: αn[l] . = � P(7r) 7r:end(7r)=n,length(7r)=l The backward probability on has the standard definition (Rabiner, 1989). To formalize the calculation of the positionspecific forward-backward pass, the initialization, and one elementary forward step in the forward pass are carried out using Eq. (1), respectively— see Figure 1 for notation: αn[l + 1] = � q αsa[l + S(li, E)] · P(li) i=1 1.0, l = 0 αstart[l] = {0.0 l =6 0 (1) , The “probability” P(li) of a given link li is stored as a log-probability and commonly evaluated in ASR using: log P(li) = FLATw · [1/LMw · log PAM(li)+ log PLM(word(li)) − 1/LMw · logPIP] (2) s_ 1 Figure 1: State Transitions s_ i P (l_1 ) P ( l_i ) s_q P (l_q ) n 446 where log PAM(li) is t</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>L. R. Rabiner. 1989. A tutorial on hidden markov models and selected applications in speech recognition. In Proceedings IEEE, volume 77(2), pages 257–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Murat Saraclar</author>
<author>Richard Sproat</author>
</authors>
<title>Lattice-based search for spoken utterance retrieval.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004,</booktitle>
<pages>129--136</pages>
<location>Boston, Massachusetts,</location>
<contexts>
<context position="8056" citStr="Saraclar and Sproat, 2004" startWordPosition="1314" endWordPosition="1317">is still more accurate, were it not for the OOV problem. The authors argue in favor of a combination of word and sub-word level indexing. Another problem pointed out by the paper is the abundance of word-spotting false-positives in the sub-word retrieval case, somewhat masked by the MAP measure. Similar approaches are taken by (Seide and Yu, 2004). One interesting feature of this work is a twopass system whereby an approximate match is carried out at the document level after which the costly detailed phonetic match is carried out on only 15% of the documents in the collection. More recently, (Saraclar and Sproat, 2004) shows improvement in word-spotting accuracy by using lattices instead of 1-best. An inverted index from symbols — word or phone — to links allows to evaluate adjacency of query words but more general proximity information is harder to obtain — see Section 4. Although no formal comparison has been carried out, we believe our approach should yield a more compact index. Before discussing our architectural design decisions it is probably useful to give a brief presentation of a state-of-the-art text document retrieval engine that is using the keyword search paradigm. 3 Text Document Retrieval Pro</context>
</contexts>
<marker>Saraclar, Sproat, 2004</marker>
<rawString>Murat Saraclar and Richard Sproat. 2004. Lattice-based search for spoken utterance retrieval. In HLT-NAACL 2004, pages 129–136, Boston, Massachusetts, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Seide</author>
<author>P Yu</author>
</authors>
<title>Vocabulary-independent search in spontaneous speech.</title>
<date>2004</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="7779" citStr="Seide and Yu, 2004" startWordPosition="1266" endWordPosition="1269">s, (Ng, 2000) shows the feasibility of sub-word SDR and advocates for tighter integration between ASR and IR technology. Similar conclusions are drawn by the excellent work in (Siegler, 1999). As pointed out in (Logan et al., 2002), word level 444 indexing and querying is still more accurate, were it not for the OOV problem. The authors argue in favor of a combination of word and sub-word level indexing. Another problem pointed out by the paper is the abundance of word-spotting false-positives in the sub-word retrieval case, somewhat masked by the MAP measure. Similar approaches are taken by (Seide and Yu, 2004). One interesting feature of this work is a twopass system whereby an approximate match is carried out at the document level after which the costly detailed phonetic match is carried out on only 15% of the documents in the collection. More recently, (Saraclar and Sproat, 2004) shows improvement in word-spotting accuracy by using lattices instead of 1-best. An inverted index from symbols — word or phone — to links allows to evaluate adjacency of query words but more general proximity information is harder to obtain — see Section 4. Although no formal comparison has been carried out, we believe </context>
</contexts>
<marker>Seide, Yu, 2004</marker>
<rawString>F. Seide and P. Yu. 2004. Vocabulary-independent search in spontaneous speech. In Proceedings of ICASSP, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew A Siegler</author>
</authors>
<title>Integration of Continuous Speech Recognition and Information Retrievalfor Mutually Optimal Performance.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="7351" citStr="Siegler, 1999" startWordPosition="1195" endWordPosition="1196">ch that the Q-OOV rate comes closer to 15%, a much more realistic figure since search keywords are typically rare words. They show severe degradation in MAP performance — 50% relative, from 44 to 22. The most common approach to dealing with OOV query words is to represent both the query and the spoken document using sub-word units — typically phones or phone n-grams — and then match sequences of such units. In his thesis, (Ng, 2000) shows the feasibility of sub-word SDR and advocates for tighter integration between ASR and IR technology. Similar conclusions are drawn by the excellent work in (Siegler, 1999). As pointed out in (Logan et al., 2002), word level 444 indexing and querying is still more accurate, were it not for the OOV problem. The authors argue in favor of a combination of word and sub-word level indexing. Another problem pointed out by the paper is the abundance of word-spotting false-positives in the sub-word retrieval case, somewhat masked by the MAP measure. Similar approaches are taken by (Seide and Yu, 2004). One interesting feature of this work is a twopass system whereby an approximate match is carried out at the document level after which the costly detailed phonetic match </context>
</contexts>
<marker>Siegler, 1999</marker>
<rawString>Matthew A. Siegler. 1999. Integration of Continuous Speech Recognition and Information Retrievalfor Mutually Optimal Performance. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P C Woodland</author>
<author>S E Johnson</author>
<author>P Jourlin</author>
<author>K Sp¨arck Jones</author>
</authors>
<title>Effects of out of vocabulary words in spoken document retrieval.</title>
<date>2000</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>372--374</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="6638" citStr="Woodland et al., 2000" startWordPosition="1070" endWordPosition="1073">owever to the SDR-TREC framework. It is well known that ASR systems are very brittle to mismatched training/test conditions and it is unrealistic to expect error rates in the range 10-15% when decoding speech mismatched with respect to the training data. It is thus very important to consider ASR operating points which have higher WER. Also, the out-of-vocabulary (OOV) rate was very low, below 1%. Since the “topics”/queries were long and stated in plain English rather than using the keyword search paradigm, the query-side OOV (Q-OOV) was very low as well, an unrealistic situation in practice. (Woodland et al., 2000) evaluates the effect of Q-OOV rate on retrieval performance by reducing the ASR vocabulary size such that the Q-OOV rate comes closer to 15%, a much more realistic figure since search keywords are typically rare words. They show severe degradation in MAP performance — 50% relative, from 44 to 22. The most common approach to dealing with OOV query words is to represent both the query and the spoken document using sub-word units — typically phones or phone n-grams — and then match sequences of such units. In his thesis, (Ng, 2000) shows the feasibility of sub-word SDR and advocates for tighter </context>
</contexts>
<marker>Woodland, Johnson, Jourlin, Jones, 2000</marker>
<rawString>P. C. Woodland, S. E. Johnson, P. Jourlin, and K. Sp¨arck Jones. 2000. Effects of out of vocabulary words in spoken document retrieval. In Proceedings of SIGIR, pages 372–374, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Young</author>
<author>Gunnar Evermann</author>
<author>Thomas Hain</author>
<author>Dan Kershaw</author>
<author>Gareth Moore</author>
<author>Julian Odell</author>
<author>Dan Povey Dave Ollason</author>
<author>Valtcho Valtchev</author>
<author>Phil Woodland</author>
</authors>
<title>The HTK Book.</title>
<date>2002</date>
<institution>Cambridge University Engineering Department,</institution>
<location>Cambridge, England,</location>
<contexts>
<context position="12929" citStr="Young et al., 2002" startWordPosition="2168" endWordPosition="2171">. On the other hand, ASR lattices do have much better WER — in our case the 1-best WER was 55% whereas the lattice WER was 30% — but the position information is not readily available: it is easy to evaluate whether two words are adjacent but questions about the distance in number of links between the occurrences of two query words in the lattice are very hard to answer. The position information needed for recording a given word hit is not readily available in ASR lattices — for details on the format of typical ASR lattices and the information stored in such lattices the reader is referred to (Young et al., 2002). To simplify the discussion let’s consider that a traditional text-document hit for given word consists of just (document id, position). The occurrence of a given word in a lattice obtained from a given spoken document is uncertain and so is the position at which the word occurs in the document. The ASR lattices do contain the information needed to evaluate proximity information, since on a given path through the lattice we can easily assign a position index to each link/word in the normal way. Each path occurs with a given posterior probability, easily computable from the lattice, so in prin</context>
</contexts>
<marker>Young, Evermann, Hain, Kershaw, Moore, Odell, Ollason, Valtchev, Woodland, 2002</marker>
<rawString>Steve Young, Gunnar Evermann, Thomas Hain, Dan Kershaw, Gareth Moore, Julian Odell, Dan Povey Dave Ollason, Valtcho Valtchev, and Phil Woodland. 2002. The HTK Book. Cambridge University Engineering Department, Cambridge, England, December.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>