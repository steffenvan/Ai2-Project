<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.989099">
Dependency Tree Abstraction for Long-Distance Reordering in
Statistical Machine Translation
</title>
<author confidence="0.995689">
Chenchen Ding
</author>
<affiliation confidence="0.998693">
Department of Computer Science
University of Tsukuba
</affiliation>
<address confidence="0.762515">
1-1-1 Tennodai, Tsukuba, Ibaraki , Japan
</address>
<email confidence="0.998157">
tei@mibel.cs.tsukuba.ac.jp
</email>
<sectionHeader confidence="0.994779" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999667263157895">
Word reordering is a crucial technique
in statistical machine translation in which
syntactic information plays an important
role. Synchronous context-free gram-
mar has typically been used for this pur-
pose with various modifications for adding
flexibilities to its synchronized tree gen-
eration. We permit further flexibilities
in the synchronous context-free grammar
in order to translate between languages
with drastically different word order. Our
method pre-processes a parallel corpus by
abstracting source-side dependency trees,
and performs long-distance reordering on
top of an off-the-shelf phrase-based sys-
tem. Experimental results show that our
method significantly outperforms previous
phrase-based and syntax-based models for
translation between English and Japanese.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999933833333333">
Since the inception of statistical machine trans-
lation (SMT), long-distance word reordering has
been a notable challenge, particularly when trans-
lating between languages with drastically different
word orders, such as subject-verb-object (SVO)
and subject-object-verb (SOV) languages like En-
glish and Japanese, respectively. Phrase-based
models (Koehn et al., 2003; Och and Ney, 2004;
Xiong et al., 2006) have been strong in local
translation and reordering. However, phrase-based
models cannot effectively conduct long-distance
reordering because they are based purely on statis-
tics of syntax-independent phrases. As a comple-
mentary approach to phrase-based models, some
researchers have incorporated syntactic informa-
tion into an SMT framework (Wu, 1997; Yamada
and Knight, 2001; Liu et al., 2006) using syn-
chronous context-free grammar (SCFG) (Aho and
</bodyText>
<note confidence="0.91778375">
Yuki Arase
Microsoft Research
No. 5 Danling St., Haidian Dist.
Beijing, P.R. China
</note>
<email confidence="0.883367">
yukiar@microsoft.com
</email>
<figureCaption confidence="0.999218">
Figure 1: English abstraction tree example
</figureCaption>
<bodyText confidence="0.9992721875">
Ullman, 1972). The original SCFG assumes that
the syntactic trees of the source and target lan-
guages can be derived synchronously. However,
this assumption is too strict for handling paral-
lel sentences that are often comparable rather than
parallel. For alleviating this assumption, some re-
searchers have added flexibilities in synchronized
tree generation (Wu, 1997; Burkett et al., 2010).
In addition, in the SMT framework, there is an
approach that alleviates the assumption by only
generating the source-side syntactic tree and pro-
jecting it to the target-side sentence (Yamada and
Knight, 2001; Liu et al., 2006).
In practice, these existing methods are not flex-
ible enough to handle parallel sentence pairs, es-
pecially those of SVO and SOV languages. There-
fore, we permit further flexibility in SCFG aiming
to effectively conduct long-distance reordering.
We design our method as a pre-processing proce-
dure so that we can use a well-developed phrase-
based system without adding heavy computational
complexity to the system. Specifically, we propose
an abstraction tree that is a shallow and nested
representation, i.e., abstraction of the dependency
tree as Fig. 1 depicts. Our method pre-processes a
parallel corpus by generating source-side abstrac-
tion trees and projecting the trees onto the target-
side sentences. It then decomposes the corpus
by collecting corresponding node pairs as a new
corpus, and finally trains the phrase-based model.
In this manner, the source-side grammar is deter-
mined on the fly for each sentence based on a de-
</bodyText>
<figure confidence="0.932324">
when the fluid pressure cylinder 31 is used, fluid is gradually applied.
the
fluid
pressure
cylinder
31
[X]
(root)
is
gradually
applied
[X]
[N]
,
when
is
used
fluid
[N]
</figure>
<page confidence="0.987732">
424
</page>
<note confidence="0.9930105">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 424–433,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9994931">
pendency parse of the source sentence. The target
side of each production in the grammar is deter-
mined by running the phrase-based decoder.
We empirically show effectiveness of our
method for English-to-Japanese and Japanese-to-
English translations by comparing it to phrase-
based and syntax-based models. Experimental re-
sults show that our method significantly outper-
forms the previous methods with respect to the
BLEU (Papineni et al., 2002) metric.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.991275534883721">
For adding flexibilities to SCFG under an SMT
scenario, previous studies generate only a source-
side syntactic tree and project it to the target-side
sentence regardless of the true target-side syntactic
structure. Liu et al. (2006) propose a tree-to-string
model using a source-side constituency tree to ex-
tract correspondences between the source-side tree
and the target-side sentence. Quirk et al. (2005)
and Xie et al. (2011) use a dependency tree for the
same purpose. Since these methods project a fine-
grained source-side syntax tree, an accurate pro-
jection is possible only when the target-side sen-
tence has a syntactic structure that is similar to
the source-side. Zhu and Xiao (2011) and Huang
and Pendus (2013) generalize rules obtained by
the tree-to-string model to increase the chance of
rule matching at decoding. Despite their merits,
none of these methods resolves the problem of tree
projection to the target-side.
The hierarchical phrase-based model (HIERO)
proposed by Chiang (2007) is independent of any
syntactic information and generates SCFG rules
only from parallel sentence pairs. Li et al. (2012)
and Feng et al. (2012) incorporate syntactic infor-
mation into HIERO as soft constraints. Since these
methods are bound by the original HIERO rules
that are independent of syntactic information, their
rules cannot represent the global syntactic struc-
ture of a sentence.
There are also pre-reordering methods for long-
distance reordering in SVO-to-SOV translations
using heuristics designed based on source-side
syntactic structures (Xu et al., 2009; Isozaki et al.,
2010; Isozaki et al., 2012). They are fine-tuned to
handle only specific reordering problems in a pre-
determined language pair. Another approach is to
statistically learn pre-reordering rules from a cor-
pus; however, this requires a highly parallel train-
ing corpus consisting of literal translations to learn
Algorithm 1 CKY-style decoding
Input: Input sentence u and its dependency tree ru, transla-
tion model TM, block-LM bLM, sentence-LM sLM,
size of m-best m
</bodyText>
<listItem confidence="0.972746285714286">
1: τu ← generate abstraction tree of u using ru
2: NodeTrans[][] ← ∅
3: for all node in τu do
4: m-best ← Decode(node, TM, bLM, m)
5: (start, end) ← start and end indices of node in u
6: NodeTrans[start][end] ← m-best
7: end for
8: for start := 1 to |u |do
9: for span := 0 to |u |− 1 do
10: end ← start + span
11: ChildTrans[] ← ∅
12: for all (i, j) such that start ≤ i ≤ j ≤ end do
13: if NodeT rans[i][j] =6 ∅ then
14: add NodeTrans[i][j] to ChildTrans
</listItem>
<reference confidence="0.7944735">
15: end if
16: end for
17: CubePruning(NodeTrans[start][end],
ChildTrans, sLM, m)
18: end for
19: end for
</reference>
<bodyText confidence="0.888099333333333">
effective rules (Neubig et al., 2012; Navratil et al.,
2012). Such a training dataset is not widely avail-
able in many languages.
</bodyText>
<sectionHeader confidence="0.741456" genericHeader="method">
3 Overview of the Proposed Method
</sectionHeader>
<bodyText confidence="0.999946555555556">
Our method pre-processes sentences in a paral-
lel corpus based on source-side abstraction trees.
It first generates an abstraction tree τs of a source-
side sentence s by abstracting its dependency tree
rs: (s, rs) → τs. It then projects the tree to the
target-side sentence t for generating a target-side
abstraction tree τt that has exactly the same struc-
ture to τs, i.e., (τs, t) → τt. The abstraction-
tree generation process can be adapted to translat-
ing languages by specifying source-side part-of-
speeches (POSs) as input. Abstraction tree struc-
tures also depend on the dependency grammar that
a parser uses. In this study, we assume commonly
used Stanford typed dependency (de Marneffe et
al., 2006) for English and the chunk-based depen-
dency with ipadic (Asahara and Matsumoto, 2003)
for Japanese. Investigation of effects of different
dependency grammars is our future work.
We decompose the sentence pair into node pairs
according to correspondences between the source
and target abstraction trees, and generate a new
corpus referred to as a block-corpus (Fig. 6). Us-
ing the block-corpus and the original corpus, we
train a phrase-based model. Its translation model
is trained with the block-corpus, and two target-
side language models (LMs) are trained with the
block-corpus (referred to as block-LM) and the
</bodyText>
<page confidence="0.998472">
425
</page>
<figure confidence="0.959431">
a high pressure hot water cylinder used
is
</figure>
<figureCaption confidence="0.998441">
Figure 2: [N] node detection example
</figureCaption>
<bodyText confidence="0.9999713">
original corpus (referred to as sentence-LM), re-
spectively. Thus the sentence-LM can be trained
using a larger-scale monolingual corpus. Com-
pared to previous methods that also decompose
sentence pairs (Xu et al., 2005; Sudoh et al., 2010),
our method is more syntax-oriented.
In decoding, we adopt the parsing algorithm
with cube-pruning (Huang and Chiang, 2005) into
a phrase-based decoder to translate the abstrac-
tion tree of an input sentence efficiently. As
Algorithm 1 shows, our decoder first generates
the abstraction tree of the input sentence (line
1), and independently translates each node us-
ing the block-LM that models ordering among
non-terminals and lexical words (line 3–7). It
then combines the m-best translation hypotheses
of each node to construct a sentence-level trans-
lation (line 8–19). Specifically, we insert sets of
the m-best translation hypotheses of child nodes
into the m-best hypotheses of their parent node
by replacing the corresponding non-terminals us-
ing the cube-pruning (line 17). The ordering of
these child nodes has been determined in their
parent node by the phrase-based model that re-
gards non-terminals only as single words. By
doing so, long-distance reordering is solved con-
sidering the global syntactic structure and con-
texts (lexical strings) preserved in the node. In
cube-pruning, we use the sentence-LM to com-
pose fluent sentence-level translation. The block-
LM and sentence-LM scores are treated as inde-
pendent features.
The computational complexity of our decoder
(line 3–19) is O(|N|C), where |N |is the number
of nodes in the abstraction-tree and C is a con-
stant representing the complexity of phrase-based
decoder and cube-pruning. Since combinations of
hypotheses in cube-pruning are determined by the
abstraction-tree in our method, the computational
cost is significantly smaller than HIERO’s case.
</bodyText>
<sectionHeader confidence="0.944005" genericHeader="method">
4 Abstraction Tree Generation
</sectionHeader>
<bodyText confidence="0.9997035">
In this section, we provide the formal definition of
an abstraction tree and the generation method.
</bodyText>
<subsectionHeader confidence="0.999311">
4.1 Definition of Abstraction Tree
</subsectionHeader>
<bodyText confidence="0.999968">
We define an abstraction tree as T = {N, £},
where N is a set of nodes and £ is a set of
</bodyText>
<figureCaption confidence="0.795724">
Figure 3: [X] and [P] node detection. Since the word
“used” is a head, it and its governing span are detached from
the root “applied” as a child node.
</figureCaption>
<bodyText confidence="0.999935714285714">
edges. For conducting abstraction based on syn-
tactic structures, we merge a span governed by a
dependency head as a node and represent it by a
non-terminal in a parent node. As a result, the i-th
node Ni consists of a sequence of lexical words w
and non-terminals L that replace spans governed
by heads in the corresponding child nodes:
</bodyText>
<equation confidence="0.689117">
Ni = {Ψ|ψ1,...,ψ|Ni|},ψk E {w, L}.
</equation>
<bodyText confidence="0.9999912">
The edge £ij between a parent node Ni and
its child node Nj corresponds to a governor-
dependent relationship from the head in Ni to its
dependent wx in Nj. wx is another head and gov-
erns other words in Nj. The span covered by Nj
is replaced by a non-terminal in Ni.
We use three kinds of labels to represent L for
explicitly using syntactic information that is use-
ful for long-distance reordering; [N], [P], and [X]
according to the head in the corresponding node.
We label a child node [N] when its head word is
a noun and forms a base noun phrase, [P] when
its head word is an adposition1, and [X] for others
like verb phrases, conjunctive phrases, and rela-
tive phrases. These nodes play different roles in a
sentence. An [N] node, i.e., a base noun phrase,
adds context to a sentence. A [P] node depends
on other phrases and generally appears relatively
freely in a sentence. Thus, we assume that the [P]
node requires special reordering.
The abstraction tree depicted in Fig. 1 has a par-
ent [X] node “when [N] is used” and its child [N]
node “the fluid pressure cylinder 31.” The word
“used” governs “cylinder” in the [N] node, and the
[N] node folds the context in the [X] node.
</bodyText>
<subsectionHeader confidence="0.990621">
4.2 Tree Construction
</subsectionHeader>
<bodyText confidence="0.999974444444444">
We begin with detecting [N] nodes, then pro-
ceed to [P] and [X] nodes. These processes require
to specify source-side POSs as input for adapting
to translating languages. We finally flatten frag-
mented nodes.
For detecting [N] nodes, we take a POS of noun
as input and identify each noun and its governing
span, i.e., a string of all governed words, using the
source-side dependency tree as Fig. 2 shows. We
</bodyText>
<figure confidence="0.785662333333333">
1A preposition in English and a postposition in Japanese.
[N]
is gradually applied.
when
[N]
is used,
</figure>
<page confidence="0.990095">
426
</page>
<bodyText confidence="0.86253125">
Algorithm 2 [P] and [X] node detection
Input: Source-side sentence s and its dependency tree r3,
POS list of adpositions PPos, [N] node list N-Nodes
Output: [P] and [X] nodes P-Nodes, X-Nodes
</bodyText>
<listItem confidence="0.980651714285714">
1: P-Nodes[] ← ∅, X-Nodes[] ← ∅
2: HeadList[] ← root of r3
3: repeat
4: head ← pop node from HeadList
5: ChildList[] ← all dependents of head
6: for all child in ChildList do
7: if child ∈/ N-Nodes and child has dependents
then
8: add child to HeadList
9: remove child from ChildList
10: end if
11: end for
12: start ← smallest start index of nodes in ChildList
13: end ← largest end index of nodes in ChildList
14: if POS of head ∈ PPos then
15: add span [start, end] of s to P-Nodes
16: else
17: add span [start, end] of s to X-Nodes
18: end if
19: remove head from HeadList
20: until HeadList = ∅
</listItem>
<bodyText confidence="0.999791653846154">
regard descendant dependents as being governed
by the noun for detecting a noun phrase of a com-
plete form. We extract the span as a node and re-
place it by an [N] label in the sentence.
Next, we identify [P] and [X] nodes given a list
of source-side POSs of adpositions as input. As
Algorithm 2 shows, after [N] node detection, we
trace the dependency tree from its root to leaves
(line 3–20). We find all the dependents to the root,
then check if each dependent is a head. If a de-
pendent of the root is a head and governs other
words, we detach the dependent to process later
(line 6–11). We then find the smallest start index
and largest end index of dependent words and set
the corresponding span as a node (if a dependent
is in an [N] node, we use the start and end indices
of the [N] node). Each node is labeled accord-
ing to the POS of its head as [P] or [X] (line 14–
18). We then take the detached dependent as a new
root and repeat the process until no more detach-
ment is possible. The computational complexity
is O(|s|2). Through this process, a span with di-
rect dependencies is extracted as a node, and other
spans with descendant dependencies become de-
scendant nodes, replaced by non-terminals in their
parent node as shown in Fig. 3.
</bodyText>
<subsectionHeader confidence="0.9998">
4.3 Handling Complex Noun Phrase
</subsectionHeader>
<bodyText confidence="0.99991625">
As described in Sec. 4.2, we detect a noun phrase
as an [N] node. However, an [N] node be-
comes more complex than a base noun phrase
when the head governs a clause, such as a relative
</bodyText>
<figureCaption confidence="0.844245">
Figure 4: Handling a complex noun phrase
</figureCaption>
<bodyText confidence="0.999980277777778">
clause. Such a complex node may require long-
distance reordering of an inside clause when trans-
lating. Therefore, we separate the noun phrase and
clause. We take a POS list whose word can be a
head of [P] and [X] nodes (preposition, verb, to,
Wh-determiner/pronoun/adverb, and coordinating
conjunction for English) as input. If the POS of a
noun’s dependent is in the list, we detach the de-
pendency arc, and then re-attach the dependency
arc to the head of the noun. As a result, the base
noun phrase becomes an [N] node and its clause
becomes a [P] or [X] node that is transformed to a
sibling of the [N] node.
In Fig. 4, the word “cylinder” in the original [N]
node has a relative clause and governs “is.” We de-
tach the dependency arc and re-attach it to “uses”
(the head of “cylinder”), so that the noun phrase
and the clause become sibling [N] and [X] nodes.
</bodyText>
<subsectionHeader confidence="0.999708">
4.4 Flattening Fragmented Nodes
</subsectionHeader>
<bodyText confidence="0.999978066666667">
The above processes are independent of the size
of each node, meaning they produce fragmented
nodes of only a few words. Such fragmented
nodes make the tree projection to the target-side
difficult. To solve this problem, we flatten the ab-
straction tree as shown in Algorithm 3. We pro-
cess an internal node in τ3 from bottom to top. If
the covering span of an internal node is less than a
threshold -y ∈ N, its child nodes are merged (line 3
and 4, Algorithm 3). Specifically, we reinsert the
child nodes by replacing the corresponding non-
terminals with lexical strings that the child nodes
have been covered by. The computational cost is
O(|N|). We investigate the effect of -y in the fol-
lowing evaluation section (Table 2).
</bodyText>
<sectionHeader confidence="0.965054" genericHeader="method">
5 Abstraction Tree Projection
</sectionHeader>
<bodyText confidence="0.999569666666667">
In this section, we describe a method for project-
ing the obtained source-side abstraction tree onto
the target-side sentence.
</bodyText>
<subsectionHeader confidence="0.99671">
5.1 Tree Structure Projection
</subsectionHeader>
<bodyText confidence="0.9998952">
We use word alignment results for tree structure
projection. However, accurate word alignment
is challenging when handling language pairs in
which long-distance reordering is needed, and the
alignment noise propagates to the tree projection.
</bodyText>
<figure confidence="0.9970645">
...
machine uses
the cylinder that is
...
original [N]
[N] [X]
</figure>
<page confidence="0.596813">
427
</page>
<bodyText confidence="0.378051666666667">
Algorithm 3 Tree flattening
Input: Abstraction tree T3, threshold -y
Output: Flattened tree T03
</bodyText>
<listItem confidence="0.604258333333333">
1: for all internal node in T3, from bottom to top do
2: (start, end) ← start and end indices of node
3: if end − start + 1 &lt; -y then
4: T03 ← MergeChildNodes(node, T3)
5: end if
6: end for
</listItem>
<bodyText confidence="0.985333666666667">
To avoid this problem, we first omit alignment
links of function words whose alignment quality
tends to be lower than that of the content words.
We then complement the quality of word align-
ment by adapting the syntactic cohesion assump-
tion (Yamada and Knight, 2001) that assumes a
word string covered by a sub-tree of the source-
side syntactic tree corresponds to a string of con-
tiguous words in the target-side sentence. Follow-
ing the assumption, we project the k-th node of
the source-side abstraction tree N(s)
k to a string of
</bodyText>
<equation confidence="0.965691333333333">
contiguous words in the target-side:
N(s)
k 7→ ti, ... , tj, s.t. 1 ≤ i ≤ j ≤ |t|,
</equation>
<bodyText confidence="0.999936428571429">
where ti is the i-th word in the target-side sentence
and |t |is the number of words in the sentence.
For each node of the source-side abstraction
tree, we first obtain its covering span. We then
define a vector c ∈ {0,1}n whose elements repre-
sent word alignment links in a binary manner. If
and only if the i-th target word is aligned to a word
in the span, the i-th element of c becomes 1, oth-
erwise it is 0. Since the original word alignment
represented by the vector c may be noisy, we find
a vector c* ∈ {0,1}n that maximizes the syntac-
tic cohesion assumption. In c*, only consecutive
elements between two indices i and j are 1, and
others are 0. We derive such c* as follows:
</bodyText>
<equation confidence="0.998227714285714">
Cmin(c) = {c&apos; |argmin
c0
s.t. ∃ i, j, 1 ≤ i ≤ j ≤ n and
&apos; _ r 1 i ≤ k ≤ j,
ck Sl 0 otherwise,
c* = argmax kc&apos;k. (2)
c0EC.in(c)
</equation>
<bodyText confidence="0.99741875">
The operator k · k computes the Euclidean norm
of a vector and c&apos; k is the k-th element of a vector
c&apos;. Finally, c* represents the best possible word
links that maximize the syntactic cohesion as-
sumption, i.e., the longest contiguous word string
in the target-side, and that are closest to the orig-
inal word alignment. Specifically, Eq. (1) deter-
mines vectors that have the smallest distance to
</bodyText>
<table confidence="0.58307075">
Algorithm 4 Tree projection
Input: Source-side abstraction tree T3, target-side sentence
t, word alignment A,,, between s and t
Output: Target-side abstraction tree Tt
</table>
<listItem confidence="0.8992625">
1: Tt[] ← ∅
2: remove links of function words in A,,,
3: for span := |s |− 1 to 0 do
4: for start := 1 to |s |do
5: end ← start + span
6: if span [start, end] ∈ T3 then
7: c ← GenerateVector([start, end], A,,,)
8: c∗ ← Solve(c) ✁ Eq. (1) and Eq. (2)
9: (i, j) ← start and end indices of c∗
10: add span [i, j] of t as a node into Tt
11: A,,, ← UpdateWordAlignment(c, c∗)
12: end if
13: end for
14: end for
</listItem>
<bodyText confidence="0.999892">
the original vector c while satisfying the hard con-
straint, and Eq. (2) selects the one whose norm is
largest, i.e., a vector that has longest contiguous
word links to the target-side. For computational
efficiency, we use the greedy-search so that the
computational cost is O(|t|). When Eq. (2) has
multiple solutions, word links in these solutions
are equally likely, and thus we merge them into a
unique solution. Specifically, we take the union
of the solutions and find the smallest index il and
largest index ir whose elements are 1. We then set
all elements between il and ir to 1.
As Algorithm 4 shows, we conduct this pro-
cess in a top-down manner throughout the abstrac-
tion tree (line 3–14). When processing each node,
word alignment links are updated by overwriting
links in c with the ones in c* (line 11). The com-
putational cost is O(|N(s)||t|), where |N(s) |is the
number of nodes in τs. Figure 5 shows a projection
example of a node. A node of “when [N] is used”
covers a span of “when the fluid pressure cylin-
der 31 is used.” The words in the span are aligned
to the 1st, 2nd, and 5th target words (chunks)2;
however, the link to the 5th target word (chunk)
is a mis-alignment. With the alignment vector c
of [1, 1, 0, 0, 1, 0, 0], we can remove this misalign-
ment and derive c* of [1, 1, 0, 0, 0, 0, 0].
</bodyText>
<subsectionHeader confidence="0.996424">
5.2 Fixed-Expression Recovery
</subsectionHeader>
<bodyText confidence="0.9982535">
The abstraction tree generation and projection are
based on a dependency tree, and thus may over-
segment a fixed-expression, such as idioms and
multi-word expressions. Since a fixed-expression
composes a complete meaning using a contiguous
word string, splitting it into different nodes results
</bodyText>
<footnote confidence="0.9881545">
2In Japanese, a unit of dependency is a chunk in general,
and thus we conduct chunking before projection.
</footnote>
<equation confidence="0.897964">
kc&apos; − ck}, (1)
</equation>
<page confidence="0.861766">
428
</page>
<figure confidence="0.598047714285714">
Node: when [NJ is used Input parallel corpus
When the fluid pressure cylinder 31
is used , fluid is gradually applied .
流体 圧 シリンダ 31 の 場合
流体 が 徐々 に 排出 さ れる
こと と なる 。
Block-Corpus
</figure>
<figureCaption confidence="0.999203">
Figure 5: Abstraction tree projection
</figureCaption>
<bodyText confidence="0.997889857142857">
in poor translation. To avoid this issue, we gener-
ate a list of fixed-expressions using conventional
methods (Evert, 2008) and force them to remain in
one node. On both the source and target abstrac-
tion trees, we recursively reinsert nodes to their
parent node when such a fixed-expression is over-
segmented and spread over multiple nodes.
</bodyText>
<subsectionHeader confidence="0.965368">
5.3 Block-Corpus Construction
</subsectionHeader>
<bodyText confidence="0.999980380952381">
After tree structure projection, we extract cor-
responding node pairs as a block-corpus. Each
node pair has a form (Ψ,, Ψt, AL), where Ψ, E
IQ,, LJ&apos; represents the source-side node of
length n. It consists of a sequence of lexical
words in the source-side vocabulary Q, and non-
terminals L. Ψt E JQt, LJ&apos; similarly represents
the target-side node of length m. AL preserves
correspondences between the non-terminals in the
source and target nodes.
Specifically, we extract a pair of leaf nodes as
a pair of lexical strings. As for internal nodes,
we use the same non-terminal labels appearing
in the source-side node at the target-side node.
Namely, the span covered by child nodes are re-
placed by corresponding non-terminal labels in the
source-side node. At the same time, we record the
correspondence between the non-terminals. Fig-
ure 6 shows an example of the block-corpus, in
which the boxed indices indicate correspondences
of non-terminals in the source and target nodes.
</bodyText>
<sectionHeader confidence="0.998342" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.99999225">
We evaluate our method in English-to-Japanese
(EJ) and Japanese-to-English (JE) translation
tasks, since long-distance reordering is a serious
problem in this language pair.
</bodyText>
<subsectionHeader confidence="0.985043">
6.1 Experiment Corpus
</subsectionHeader>
<bodyText confidence="0.999783142857143">
We use NTCIR-7 PATMT (Fujii et al., 2008), a
publicly available standard evaluation dataset, for
EJ and JE machine translation. The dataset is con-
structed using English and Japanese patents and
consists of 1.8 million parallel sentence pairs for
training, 915 sentence pairs for development, and
1, 381 sentence pairs for testing. The development
</bodyText>
<figure confidence="0.795781285714286">
[X] , [N] is gradually applied . [X] [N] が 徐々 に 排出 さ
0 0
0 0 れる こと と なる 。
when [N] is used [N] の 場合 は
1 1
the fluid pressure cylinder 31 流体 圧 シリンダ 31
fluid 流体
</figure>
<figureCaption confidence="0.996003">
Figure 6: Block-corpus example. Boxed indices link non-
terminals in the source and target exemplars.
</figureCaption>
<bodyText confidence="0.842723333333333">
and test sets have one reference per sentence. This
dataset is bidirectional and can be used for both EJ
and JE translation evaluation.
</bodyText>
<subsectionHeader confidence="0.999771">
6.2 Implementation of Proposed Method
</subsectionHeader>
<bodyText confidence="0.994322108108108">
We implement our method for EJ and JE
translation tasks. In both cases, we use an
in-house implementation of English POS tag-
ger (Collins, 2002) and a Japanese morpholog-
ical analyzer (Kudo et al., 2004) for tokeniza-
tion and POS tagging. As for EJ translation,
we use the Stanford parser (de Marneffe et al.,
2006) to obtain English abstraction trees. We
also use an in-house implementation of a Japanese
chunker (Kudo and Matsumoto, 2002) to obtain
chunks in Japanese sentences. We apply the chun-
ker just before tree projection for using a chunk
as a projection unit, since a chunk is the basic
unit in Japanese. As for JE translation, we use
a popular Japanese dependency parser (Kudo and
Matsumoto, 2002) to obtain Japanese abstraction
trees. We convert Japanese chunk-level depen-
dency tree to a word-level using a simple heuris-
tic. We use GIZA++ (Och and Ney, 2003) with the
grow-diag-final-and heuristic for word alignment.
We use an in-house implementation of
the bracketing transduction grammar (BTG)
model (Xiong et al., 2006) as the phrase-based
model that our method relies on for translation.
Non-terminals in our block-corpus are regarded
as a single word, and their alignments AL deter-
mined in the block-corpus are exclusively used to
align them. We set the maximum phrase length to
5 when training the translation model, since we
find that the performance is stable even setting
larger values as in (Koehn et al., 2003). We then
train the sentence-LM and block-LM using the
original corpus and the obtained block-corpus,
respectively. We ignore a sentence-end tag (&lt;/s&gt;)
in the block-LM. With each corpus, we train a
5-gram LM using the SRI toolkit (Stolcke, 2002).
Span: when the fluid pressure cylinder 31 is used
</bodyText>
<figure confidence="0.629053">
Target �# E iJ&gt;9 31 OD &apos;1J: A# bV 4rz r Ift a ;1.3
sentence: ZL L U3 .
𝒄= 1,1, 0, 0,1, 0, 0
</figure>
<page confidence="0.995782">
429
</page>
<subsectionHeader confidence="0.992709">
6.3 Comparison Method
</subsectionHeader>
<bodyText confidence="0.99996966">
Since our method pre-processes the parallel cor-
pus based on SCFG with increased flexibility and
trains a BTG model using the processed corpus,
we compare our method to another BTG model
trained only with the original corpus (simply re-
ferred to as the BTG model). We also com-
pare to the tree-to-string model and HIERO using
state-of-the-art implementations available in the
Moses system (Koehn et al., 2007), since they are
based on SCFG. The tree-to-string model requires
source-side constituency trees. For EJ transla-
tion, we use a state-of-the-art English constituency
parser (Miyao and Tsujii, 2005; Miyao and Tsujii,
2008). For JE translation, we transform a Japanese
dependency tree into a constituency tree using a
simple heuristic because there is no publicly avail-
able constituency parser. During the translation
model training, we use the same setting as our
method. In addition, we set the maximum span
of rule extraction to infinity for the tree-to-string
model and 10 for HIERO following Moses’ de-
fault. We use the sentence-LM in these models as
they assume.
In addition, we compare our method to Head-
Finalization (Isozaki et al., 2010; Isozaki et al.,
2012) because it has achieved the best BLEU score
in EJ translation by handling long-distance re-
ordering. It is a specialized method to EJ trans-
lation, where a syntactic head in an English sen-
tence is reordered behind its constituents for com-
plying with the head-final nature of the Japanese
language. We pre-process the parallel corpus us-
ing the Head-Finalization and train a BTG model
using the same setting with our method to observe
the effect of different pre-processing methods.
During decoding, we set the translation table
size to 10 for each source string, and the stack
and beam sizes in the cube pruning to 100 for our
method (i.e., m-best = 100) and all other mod-
els. The maximum reordering span in the tree-to-
string model and HIERO is the same as the rule
extraction setting (infinity and 10, respectively).
We set the word reordering limit to infinity for our
method and the BTG model, while we set it to 3
for Head-Finalization as their papers report.
We tune feature weights by the minimum error
rate training (Och, 2003) to maximize the BLEU
score using the development set. As an evaluation
metric, we compute the BLEU score using the test
set, and all the scores discussed in Sec. 6.4 are the
</bodyText>
<table confidence="0.993863833333333">
Method EJ JE
Proposed method (γ = 10) 31.78 28.55
BTG 28.82** 26.98**
HIERO 29.27** 27.96*
Tree-to-string 30.97** 26.28**
Head-Finalization 29.52** NA
</table>
<tableCaption confidence="0.773720666666667">
Table 1: Test-set BLEU scores. The symbol ** represents
a significant difference at the p &lt; .01 level and * indicates a
significant difference at the p &lt; .05 level against our method.
</tableCaption>
<bodyText confidence="0.8529445">
test-set BLEU scores. Significance tests are con-
ducted using bootstrap sampling (Koehn, 2004).
</bodyText>
<subsectionHeader confidence="0.992282">
6.4 Result and Discussion
</subsectionHeader>
<bodyText confidence="0.999872942857143">
In this section, we present experimental results
and discuss them in detail.
Overall Performance Table 1 shows the BLEU
scores, in which our method significantly outper-
forms all other models for both EJ and JE transla-
tion tasks. These results indicate that our method
effectively incorporates syntactic information into
the phrase-based model and improves the transla-
tion quality.
For EJ translation, our method outperforms
the BTG model by 2.96, the HIERO by 2.51,
the tree-to-string model by 0.81, and the Head-
Finalization3 by 2.26 in terms of BLEU score.
When we compare our method to the Head-
Finalization, both of them improve the BTG model
by pre-processing the parallel corpus. Moreover,
our method outperforms the Head-Finalization us-
ing richer syntactic information.
For JE translation, our method outperforms the
BTG model by 1.57, the HIERO by 0.59, and the
tree-to-string model by 2.27 in terms of BLEU
score. Our method and the tree-to-string model,
which depend on syntactic information, largely
outperform the BTG model and HIERO in EJ
translation. While the BTG model and HIERO,
which are independent of syntactic information,
outperform the tree-to-string model in JE trans-
lation. One reason for this phenomenon is that
English is a strongly configurational language that
has rigid word order while Japanese is an agglu-
tinative language that has relatively free word or-
der. A rigid syntactic structure provides solid clues
for word reordering when translated into a flexible
language, while a flexible structure provides weak
clues for fitting it to a rigid structure.
</bodyText>
<footnote confidence="0.994709666666667">
3The BLEU score reported in this experiment differs from
their papers. This may be because they use a phrase-based
model in the Moses system, while we use the BTG model.
</footnote>
<page confidence="0.998026">
430
</page>
<tableCaption confidence="0.998109">
Table 2: Effect of threshold γ
</tableCaption>
<bodyText confidence="0.996045181818182">
Effect of Flattening Threshold Table 2 shows
BLEU scores when changing the flattening thresh-
old -y in our method, and averages and standard
deviations of the abstraction tree heights (-y = ∞
is equal to the BTG model). The performance im-
proves as we increase the threshold, i.e., increas-
ing the level of abstraction. Our method achieves
the best BLEU score when -y = 10 for both EJ and
JE translation, with the performance degrading as
we further increase the threshold.
This trend shows the trade-off between phrase-
based and syntax-based approaches. When the
threshold is too small, an abstraction tree be-
comes closer to the dependency tree and the tree-
projection becomes difficult. In addition, con-
text information becomes unavailable when con-
ducting long-distance reordering with a deep tree.
On the other hand, when setting the threshold too
large, the abstraction tree becomes too abstracted
and syntactic structures useful for long-distance
word reordering are lost. We need to balance these
effects by setting an appropriate threshold.
</bodyText>
<subsectionHeader confidence="0.945276">
Effect of Non-Terminals and Fixed-Expressions
</subsectionHeader>
<bodyText confidence="0.99975825">
We change the kinds of non-terminal labels in an
abstraction tree to investigate their effect on the
translation quality. When we merge the [P] label
to the [X] label, i.e., use only [N] and [X] labels,
the BLEU score drops 0.40 in EJ translation while
the score is unaffected in JE translation. This is
because flexible Japanese syntax does not differ-
entiate postpositional phrases with others, while
English syntax prohibits such a flexibility.
When we merge all labels and only use the [X]
label, the BLEU score drops 0.57 in EJ transla-
tion and 0.43 in JE translation. This result sup-
ports our design of the abstraction tree that distin-
guishes non-terminals according to their different
functionalities in a sentence.
We also evaluate the effect of fixed-expressions
as described in Sec. 5.2. Results show a significant
change when over-splitting fixed-expressions; the
BLEU score drops 1.13 for EJ and 0.36 for JE
translation without reinserting fixed-expressions.
</bodyText>
<table confidence="0.9995425">
Method acceptable I global 1 local 1
Proposed 52 30 4
BTG 34 38 7
Tree-to-string 47 32 7
</table>
<tableCaption confidence="0.999776">
Table 3: Error distribution in 100 samples of EJ translation
</tableCaption>
<bodyText confidence="0.999846342857143">
Error Analysis We randomly sample 100 trans-
lation outputs per our method (-y = 10), BTG, and
tree-to-string models for each EJ and JE transla-
tion tasks, and manually categorize errors based
on (Vilar et al., 2006). We focus primarily on
reordering errors and exclusively categorize the
samples into acceptable translations, translations
with only global or local reordering errors, as well
as others that are complicated combinations of var-
ious errors. An acceptable translation correctly
conveys the information in a source sentence even
if it contains minor grammatical errors.
Table 3 shows the distribution of acceptable
translations and those with global/local reordering
errors in the EJ task (results of JE task are omitted
due to the severe space limitation, but their trend
is similar). It confirms that our method reduces re-
ordering errors, not only for long-distance but for
local reordering, and increases the ratio of accept-
able translations compared to the BTG and tree-
to-string models. We also find that long-distance
reordering was attempted in 85, 66, and 70 sen-
tences by our method, BTG, and tree-to-string, re-
spectively, among these translations. The results
show that our method performs long-distance re-
ordering more frequently than others.
When we compare translations performed by
our method to those performed by the tree-to-
string model, we observe that their effectiveness
depends on a range of reordering. Our method is
effective in long-distance reordering like those of
clauses, while the tree-to-string model performs
middle-range reordering well. This is due to the
trade-off regarding the level of abstraction as dis-
cussed in the flattening threshold experiment.
</bodyText>
<sectionHeader confidence="0.991862" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9998957">
We have proposed an abstraction tree for effec-
tively conducting long-distance reordering using
an off-the-shelf phrase-based model. Evaluation
results show that our method outperforms conven-
tional phrase-based and syntax-based models.
We plan to investigate the effect of translating
language pairs and dependency grammars in ab-
straction tree generation. In addition, we will ap-
ply a structure-aware word aligner (Neubig et al.,
2011) to improve the tree projection.
</bodyText>
<figure confidence="0.999498684210526">
γ EJ
JE
BLEU
0 31.15
3 30.88
5 31.21
8 31.61
10 31.78
12 31.76
15 31.25
∞ 28.82
height
4.1 (1.5)
3.8 (1.7)
3.7 (1.5)
3.4 (1.4)
3.1 (1.3)
2.9 (1.3)
2.6 (1.2)
1.0 (–)
BLEU
28.41
28.34
28.39
28.52
28.55
28.54
28.21
26.98
height
4.2 (1.4)
3.9 (1.6)
3.8 (1.5)
3.4 (1.4)
3.2 (1.3)
3.0 (1.3)
2.7 (1.2)
1.0 (–)
</figure>
<page confidence="0.995539">
431
</page>
<note confidence="0.725826818181818">
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2012. HPSG-based preprocessing
for English-to-Japanese translation. ACM Trans-
actions on Asian Language Information Processing
(TALIP), 11(3):8:1–8:16.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The the-
ory ofparsing, translation, and compiling. Prentice-
Hall Inc.
Masayuki Asahara and Yuji Matsumoto. 2003.
ipadic version 2.7.0 user’s manual. http:
</note>
<reference confidence="0.991375081632653">
//sourceforge.jp/projects/ipadic/
docs/ipadic-2.7.0-manual-en.pdf.
David Burkett, John Blitzer, and Dan Klein. 2010.
Joint parsing and alignment with weakly synchro-
nized grammars. In Proceedings of Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology (NAACL-HLT 2010), pages 127–135.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and ex-
periments with perceptron algorithms. In Proceed-
ings of Conference on Empirical Methods in Natural
Language Processing (EMNLP 2002), pages 1–8.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of International Conference on Lan-
guage Resources and Evaluation (LREC 2006),
pages 449–454.
Stefan Evert. 2008. Corpora and collocations. In Anke
L¨udeling and Merja Kyt¨o, editors, Corpus Linguis-
tics. An International Handbook, volume 2, chap-
ter 58. Mouton de Gruyter.
Yang Feng, Dongdong Zhang, Mu Li, Ming Zhou,
and Qun Liu. 2012. Hierarchical chunk-to-string
translation. In Proceedings of Annual Meeting of
the Association for Computational Linguistics (ACL
2012), pages 950–958.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent
translation task at the NTCIR-7 workshop. In Pro-
ceedings of NTCIR-7 Workshop Meeting (NTCIR),
pages 389–400.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of International Workshop
on Parsing Technology (IWPT 2005), pages 53–64.
Fei Huang and Cezar Pendus. 2013. Generalized re-
ordering rules for improved SMT. In Proceedings
of Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2013), pages 387–392.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010. Head Finalization: A simple re-
ordering rule for SOV languages. In Proceedings
of Joint Workshop on Statistical Machine Transla-
tion and Metrics MATR (WMT-MetricsMATR 2010),
pages 244–251.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of Conference of the North American Chapter of
the Association for Computational Linguistics on
Human Language Technology (NAACL-HLT 2003),
pages 48–54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings ofAnnual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2007),
pages 177–180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), pages 388–395.
Taku Kudo and Yuji Matsumoto. 2002. Japanese de-
pendency analysis using cascaded chunking. In Pro-
ceedings of Conference on Natural Language Learn-
ing (CoNLL 2002), pages 1–7.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
Japanese morphological analysis. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), pages 230–237.
Junhui Li, Zhaopeng Tu, Guodong Zhou, and Josef van
Genabith. 2012. Head-driven hierarchical phrase-
based translation. In Proceedings of Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2012), pages 33–37.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of International Con-
ference on Computational Linguistics and Annual
Meeting of the Association for Computational Lin-
guistics (COLING-ACL 2006), pages 609–616.
Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proceedings ofAnnual Meeting on Asso-
ciation for Computational Linguistics (ACL 2005),
pages 83–90.
Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35–80.
Jiri Navratil, Karthik Visweswariah, and Ananthakrish-
nan Ramanathan. 2012. A comparison of syntac-
tic reordering methods for English-German machine
</reference>
<page confidence="0.981953">
432
</page>
<reference confidence="0.999667707865169">
translation. In Proceedings of International Confer-
ence on Computational Linguistics (COLING 2012),
pages 2043–2058.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori, and Tatsuya Kawahara. 2011.
An unsupervised model for joint phrase alignment
and extraction. In Proceedings of Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies (ACL-HLT 2011),
pages 632–641.
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a discriminative parser to optimize
machine translation reordering. In Proceedings of
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL 2012), pages
843–853.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19–51.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417–449.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
Annual Meeting on Association for Computational
Linguistics (ACL 2003), pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
Annual Meeting on Association for Computational
Linguistics (ACL 2002), pages 311–318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proceedings of Annual
Meeting on Association for Computational Linguis-
tics (ACL 2005), pages 271–279.
Andreas Stolcke. 2002. SRILM - An extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing
(ICSLP 2002), pages 901–904.
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Tsu-
tomu Hirao, and Masaaki Nagata. 2010. Divide
and translate: Improving long distance reordering
in statistical machine translation. In Proceedings
of Joint Workshop on Statistical Machine Transla-
tion and Metrics MATR (WMT-MetricsMATR 2010),
pages 418–427.
David Vilar, Jia Xu, Luis Fernando d’Haro, and Her-
mann Ney. 2006. Error analysis of statistical ma-
chine translation output. In Proceedings of Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2006), pages 697–702.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel
dependency-to-string model for statistical machine
translation. In Proceedings of Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP 2011), pages 216–226.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of In-
ternational Conference on Computational Linguis-
tics and Annual Meeting on Association for Com-
putational Linguistics (COLING-ACL 2006), pages
521–528.
Jia Xu, Richard Zens, and Hermann Ney. 2005.
Sentence segmentation using IBM word alignment
model 1. In Proceedings of Annual Conference of
the European Association for Machine Translation
(EAMT 2005), pages 280–287.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz J.
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In Proceed-
ings of Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology (NAACL-HLT 2009),
pages 245–253.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of Annual Meeting on Association for Computa-
tional Linguistics (ACL 2001), pages 523–530.
Jingbo Zhu and Tong Xiao. 2011. Improving decoding
generalization for tree-to-string translation. In Pro-
ceedings of Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies (ACL-HLT 2001), pages 418–423.
</reference>
<page confidence="0.999428">
433
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.618482">
<title confidence="0.9987035">Dependency Tree Abstraction for Long-Distance Reordering Statistical Machine Translation</title>
<author confidence="0.964256">Chenchen</author>
<affiliation confidence="0.9996605">Department of Computer University of</affiliation>
<address confidence="0.691229">1-1-1 Tennodai, Tsukuba, Ibaraki ,</address>
<email confidence="0.865018">tei@mibel.cs.tsukuba.ac.jp</email>
<abstract confidence="0.99773305">Word reordering is a crucial technique in statistical machine translation in which syntactic information plays an important role. Synchronous context-free grammar has typically been used for this purpose with various modifications for adding flexibilities to its synchronized tree generation. We permit further flexibilities in the synchronous context-free grammar in order to translate between languages with drastically different word order. Our method pre-processes a parallel corpus by abstracting source-side dependency trees, and performs long-distance reordering on top of an off-the-shelf phrase-based system. Experimental results show that our method significantly outperforms previous phrase-based and syntax-based models for translation between English and Japanese.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>15: end if 16: end for 17: CubePruning(NodeTrans[start][end], ChildTrans, sLM, m) 18: end for 19: end for //sourceforge.jp/projects/ipadic/</title>
<pages>2--7</pages>
<marker></marker>
<rawString>15: end if 16: end for 17: CubePruning(NodeTrans[start][end], ChildTrans, sLM, m) 18: end for 19: end for //sourceforge.jp/projects/ipadic/ docs/ipadic-2.7.0-manual-en.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>John Blitzer</author>
<author>Dan Klein</author>
</authors>
<title>Joint parsing and alignment with weakly synchronized grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL-HLT</booktitle>
<pages>127--135</pages>
<contexts>
<context position="2420" citStr="Burkett et al., 2010" startWordPosition="328" endWordPosition="331">ight, 2001; Liu et al., 2006) using synchronous context-free grammar (SCFG) (Aho and Yuki Arase Microsoft Research No. 5 Danling St., Haidian Dist. Beijing, P.R. China yukiar@microsoft.com Figure 1: English abstraction tree example Ullman, 1972). The original SCFG assumes that the syntactic trees of the source and target languages can be derived synchronously. However, this assumption is too strict for handling parallel sentences that are often comparable rather than parallel. For alleviating this assumption, some researchers have added flexibilities in synchronized tree generation (Wu, 1997; Burkett et al., 2010). In addition, in the SMT framework, there is an approach that alleviates the assumption by only generating the source-side syntactic tree and projecting it to the target-side sentence (Yamada and Knight, 2001; Liu et al., 2006). In practice, these existing methods are not flexible enough to handle parallel sentence pairs, especially those of SVO and SOV languages. Therefore, we permit further flexibility in SCFG aiming to effectively conduct long-distance reordering. We design our method as a pre-processing procedure so that we can use a well-developed phrasebased system without adding heavy </context>
</contexts>
<marker>Burkett, Blitzer, Klein, 2010</marker>
<rawString>David Burkett, John Blitzer, and Dan Klein. 2010. Joint parsing and alignment with weakly synchronized grammars. In Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL-HLT 2010), pages 127–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="5427" citStr="Chiang (2007)" startWordPosition="799" endWordPosition="800">Quirk et al. (2005) and Xie et al. (2011) use a dependency tree for the same purpose. Since these methods project a finegrained source-side syntax tree, an accurate projection is possible only when the target-side sentence has a syntactic structure that is similar to the source-side. Zhu and Xiao (2011) and Huang and Pendus (2013) generalize rules obtained by the tree-to-string model to increase the chance of rule matching at decoding. Despite their merits, none of these methods resolves the problem of tree projection to the target-side. The hierarchical phrase-based model (HIERO) proposed by Chiang (2007) is independent of any syntactic information and generates SCFG rules only from parallel sentence pairs. Li et al. (2012) and Feng et al. (2012) incorporate syntactic information into HIERO as soft constraints. Since these methods are bound by the original HIERO rules that are independent of syntactic information, their rules cannot represent the global syntactic structure of a sentence. There are also pre-reordering methods for longdistance reordering in SVO-to-SOV translations using heuristics designed based on source-side syntactic structures (Xu et al., 2009; Isozaki et al., 2010; Isozaki </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>1--8</pages>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>449--454</pages>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of International Conference on Language Resources and Evaluation (LREC 2006), pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
</authors>
<title>Corpora and collocations.</title>
<date>2008</date>
<booktitle>In Anke L¨udeling and Merja Kyt¨o, editors, Corpus Linguistics. An International Handbook, volume 2, chapter 58. Mouton de Gruyter.</booktitle>
<marker>Evert, 2008</marker>
<rawString>Stefan Evert. 2008. Corpora and collocations. In Anke L¨udeling and Merja Kyt¨o, editors, Corpus Linguistics. An International Handbook, volume 2, chapter 58. Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Feng</author>
<author>Dongdong Zhang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Qun Liu</author>
</authors>
<title>Hierarchical chunk-to-string translation.</title>
<date>2012</date>
<booktitle>In Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>950--958</pages>
<contexts>
<context position="5571" citStr="Feng et al. (2012)" startWordPosition="821" endWordPosition="824"> syntax tree, an accurate projection is possible only when the target-side sentence has a syntactic structure that is similar to the source-side. Zhu and Xiao (2011) and Huang and Pendus (2013) generalize rules obtained by the tree-to-string model to increase the chance of rule matching at decoding. Despite their merits, none of these methods resolves the problem of tree projection to the target-side. The hierarchical phrase-based model (HIERO) proposed by Chiang (2007) is independent of any syntactic information and generates SCFG rules only from parallel sentence pairs. Li et al. (2012) and Feng et al. (2012) incorporate syntactic information into HIERO as soft constraints. Since these methods are bound by the original HIERO rules that are independent of syntactic information, their rules cannot represent the global syntactic structure of a sentence. There are also pre-reordering methods for longdistance reordering in SVO-to-SOV translations using heuristics designed based on source-side syntactic structures (Xu et al., 2009; Isozaki et al., 2010; Isozaki et al., 2012). They are fine-tuned to handle only specific reordering problems in a predetermined language pair. Another approach is to statisti</context>
</contexts>
<marker>Feng, Zhang, Li, Zhou, Liu, 2012</marker>
<rawString>Yang Feng, Dongdong Zhang, Mu Li, Ming Zhou, and Qun Liu. 2012. Hierarchical chunk-to-string translation. In Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL 2012), pages 950–958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
<author>Masao Utiyama</author>
<author>Mikio Yamamoto</author>
<author>Takehito Utsuro</author>
</authors>
<title>Overview of the patent translation task at the NTCIR-7 workshop.</title>
<date>2008</date>
<booktitle>In Proceedings of NTCIR-7 Workshop Meeting (NTCIR),</booktitle>
<pages>389--400</pages>
<marker>Fujii, Utiyama, Yamamoto, Utsuro, 2008</marker>
<rawString>Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and Takehito Utsuro. 2008. Overview of the patent translation task at the NTCIR-7 workshop. In Proceedings of NTCIR-7 Workshop Meeting (NTCIR), pages 389–400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of International Workshop on Parsing Technology (IWPT</booktitle>
<pages>53--64</pages>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of International Workshop on Parsing Technology (IWPT 2005), pages 53–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Cezar Pendus</author>
</authors>
<title>Generalized reordering rules for improved SMT.</title>
<date>2013</date>
<booktitle>In Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>387--392</pages>
<contexts>
<context position="5146" citStr="Huang and Pendus (2013)" startWordPosition="756" endWordPosition="759">ide syntactic tree and project it to the target-side sentence regardless of the true target-side syntactic structure. Liu et al. (2006) propose a tree-to-string model using a source-side constituency tree to extract correspondences between the source-side tree and the target-side sentence. Quirk et al. (2005) and Xie et al. (2011) use a dependency tree for the same purpose. Since these methods project a finegrained source-side syntax tree, an accurate projection is possible only when the target-side sentence has a syntactic structure that is similar to the source-side. Zhu and Xiao (2011) and Huang and Pendus (2013) generalize rules obtained by the tree-to-string model to increase the chance of rule matching at decoding. Despite their merits, none of these methods resolves the problem of tree projection to the target-side. The hierarchical phrase-based model (HIERO) proposed by Chiang (2007) is independent of any syntactic information and generates SCFG rules only from parallel sentence pairs. Li et al. (2012) and Feng et al. (2012) incorporate syntactic information into HIERO as soft constraints. Since these methods are bound by the original HIERO rules that are independent of syntactic information, the</context>
</contexts>
<marker>Huang, Pendus, 2013</marker>
<rawString>Fei Huang and Cezar Pendus. 2013. Generalized reordering rules for improved SMT. In Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL 2013), pages 387–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
<author>Kevin Duh</author>
</authors>
<title>Head Finalization: A simple reordering rule for SOV languages.</title>
<date>2010</date>
<booktitle>In Proceedings of Joint Workshop on Statistical Machine Translation and Metrics MATR (WMT-MetricsMATR</booktitle>
<pages>244--251</pages>
<contexts>
<context position="6017" citStr="Isozaki et al., 2010" startWordPosition="886" endWordPosition="889">ERO) proposed by Chiang (2007) is independent of any syntactic information and generates SCFG rules only from parallel sentence pairs. Li et al. (2012) and Feng et al. (2012) incorporate syntactic information into HIERO as soft constraints. Since these methods are bound by the original HIERO rules that are independent of syntactic information, their rules cannot represent the global syntactic structure of a sentence. There are also pre-reordering methods for longdistance reordering in SVO-to-SOV translations using heuristics designed based on source-side syntactic structures (Xu et al., 2009; Isozaki et al., 2010; Isozaki et al., 2012). They are fine-tuned to handle only specific reordering problems in a predetermined language pair. Another approach is to statistically learn pre-reordering rules from a corpus; however, this requires a highly parallel training corpus consisting of literal translations to learn Algorithm 1 CKY-style decoding Input: Input sentence u and its dependency tree ru, translation model TM, block-LM bLM, sentence-LM sLM, size of m-best m 1: τu ← generate abstraction tree of u using ru 2: NodeTrans[][] ← ∅ 3: for all node in τu do 4: m-best ← Decode(node, TM, bLM, m) 5: (start, en</context>
</contexts>
<marker>Isozaki, Sudoh, Tsukada, Duh, 2010</marker>
<rawString>Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and Kevin Duh. 2010. Head Finalization: A simple reordering rule for SOV languages. In Proceedings of Joint Workshop on Statistical Machine Translation and Metrics MATR (WMT-MetricsMATR 2010), pages 244–251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL-HLT</booktitle>
<pages>48--54</pages>
<contexts>
<context position="1393" citStr="Koehn et al., 2003" startWordPosition="177" endWordPosition="180">forms long-distance reordering on top of an off-the-shelf phrase-based system. Experimental results show that our method significantly outperforms previous phrase-based and syntax-based models for translation between English and Japanese. 1 Introduction Since the inception of statistical machine translation (SMT), long-distance word reordering has been a notable challenge, particularly when translating between languages with drastically different word orders, such as subject-verb-object (SVO) and subject-object-verb (SOV) languages like English and Japanese, respectively. Phrase-based models (Koehn et al., 2003; Och and Ney, 2004; Xiong et al., 2006) have been strong in local translation and reordering. However, phrase-based models cannot effectively conduct long-distance reordering because they are based purely on statistics of syntax-independent phrases. As a complementary approach to phrase-based models, some researchers have incorporated syntactic information into an SMT framework (Wu, 1997; Yamada and Knight, 2001; Liu et al., 2006) using synchronous context-free grammar (SCFG) (Aho and Yuki Arase Microsoft Research No. 5 Danling St., Haidian Dist. Beijing, P.R. China yukiar@microsoft.com Figur</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL-HLT 2003), pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofAnnual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings ofAnnual Meeting of the Association for Computational Linguistics (ACL 2007), pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP 2004),</booktitle>
<pages>388--395</pages>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP 2004), pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency analysis using cascaded chunking.</title>
<date>2002</date>
<booktitle>In Proceedings of Conference on Natural Language Learning (CoNLL</booktitle>
<pages>1--7</pages>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In Proceedings of Conference on Natural Language Learning (CoNLL 2002), pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Kaoru Yamamoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Applying conditional random fields to Japanese morphological analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP 2004),</booktitle>
<pages>230--237</pages>
<marker>Kudo, Yamamoto, Matsumoto, 2004</marker>
<rawString>Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to Japanese morphological analysis. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP 2004), pages 230–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junhui Li</author>
<author>Zhaopeng Tu</author>
<author>Guodong Zhou</author>
<author>Josef van Genabith</author>
</authors>
<title>Head-driven hierarchical phrasebased translation.</title>
<date>2012</date>
<booktitle>In Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>33--37</pages>
<marker>Li, Tu, Zhou, van Genabith, 2012</marker>
<rawString>Junhui Li, Zhaopeng Tu, Guodong Zhou, and Josef van Genabith. 2012. Head-driven hierarchical phrasebased translation. In Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL 2012), pages 33–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of International Conference on Computational Linguistics and Annual Meeting of the Association for Computational Linguistics (COLING-ACL</booktitle>
<pages>609--616</pages>
<contexts>
<context position="1828" citStr="Liu et al., 2006" startWordPosition="241" endWordPosition="244">ally different word orders, such as subject-verb-object (SVO) and subject-object-verb (SOV) languages like English and Japanese, respectively. Phrase-based models (Koehn et al., 2003; Och and Ney, 2004; Xiong et al., 2006) have been strong in local translation and reordering. However, phrase-based models cannot effectively conduct long-distance reordering because they are based purely on statistics of syntax-independent phrases. As a complementary approach to phrase-based models, some researchers have incorporated syntactic information into an SMT framework (Wu, 1997; Yamada and Knight, 2001; Liu et al., 2006) using synchronous context-free grammar (SCFG) (Aho and Yuki Arase Microsoft Research No. 5 Danling St., Haidian Dist. Beijing, P.R. China yukiar@microsoft.com Figure 1: English abstraction tree example Ullman, 1972). The original SCFG assumes that the syntactic trees of the source and target languages can be derived synchronously. However, this assumption is too strict for handling parallel sentences that are often comparable rather than parallel. For alleviating this assumption, some researchers have added flexibilities in synchronized tree generation (Wu, 1997; Burkett et al., 2010). In add</context>
<context position="4658" citStr="Liu et al. (2006)" startWordPosition="677" endWordPosition="680">e grammar is determined by running the phrase-based decoder. We empirically show effectiveness of our method for English-to-Japanese and Japanese-toEnglish translations by comparing it to phrasebased and syntax-based models. Experimental results show that our method significantly outperforms the previous methods with respect to the BLEU (Papineni et al., 2002) metric. 2 Related Work For adding flexibilities to SCFG under an SMT scenario, previous studies generate only a sourceside syntactic tree and project it to the target-side sentence regardless of the true target-side syntactic structure. Liu et al. (2006) propose a tree-to-string model using a source-side constituency tree to extract correspondences between the source-side tree and the target-side sentence. Quirk et al. (2005) and Xie et al. (2011) use a dependency tree for the same purpose. Since these methods project a finegrained source-side syntax tree, an accurate projection is possible only when the target-side sentence has a syntactic structure that is similar to the source-side. Zhu and Xiao (2011) and Huang and Pendus (2013) generalize rules obtained by the tree-to-string model to increase the chance of rule matching at decoding. Desp</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment template for statistical machine translation. In Proceedings of International Conference on Computational Linguistics and Annual Meeting of the Association for Computational Linguistics (COLING-ACL 2006), pages 609–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic disambiguation models for wide-coverage HPSG parsing.</title>
<date>2005</date>
<booktitle>In Proceedings ofAnnual Meeting on Association for Computational Linguistics (ACL</booktitle>
<pages>83--90</pages>
<marker>Miyao, Tsujii, 2005</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilistic disambiguation models for wide-coverage HPSG parsing. In Proceedings ofAnnual Meeting on Association for Computational Linguistics (ACL 2005), pages 83–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Feature forest models for probabilistic HPSG parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<marker>Miyao, Tsujii, 2008</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature forest models for probabilistic HPSG parsing. Computational Linguistics, 34(1):35–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiri Navratil</author>
<author>Karthik Visweswariah</author>
<author>Ananthakrishnan Ramanathan</author>
</authors>
<title>A comparison of syntactic reordering methods for English-German machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of International Conference on Computational Linguistics (COLING</booktitle>
<pages>2043--2058</pages>
<marker>Navratil, Visweswariah, Ramanathan, 2012</marker>
<rawString>Jiri Navratil, Karthik Visweswariah, and Ananthakrishnan Ramanathan. 2012. A comparison of syntactic reordering methods for English-German machine translation. In Proceedings of International Conference on Computational Linguistics (COLING 2012), pages 2043–2058.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
<author>Shinsuke Mori</author>
<author>Tatsuya Kawahara</author>
</authors>
<title>An unsupervised model for joint phrase alignment and extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT</booktitle>
<pages>632--641</pages>
<marker>Neubig, Watanabe, Sumita, Mori, Kawahara, 2011</marker>
<rawString>Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shinsuke Mori, and Tatsuya Kawahara. 2011. An unsupervised model for joint phrase alignment and extraction. In Proceedings of Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011), pages 632–641.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Taro Watanabe</author>
<author>Shinsuke Mori</author>
</authors>
<title>Inducing a discriminative parser to optimize machine translation reordering.</title>
<date>2012</date>
<booktitle>In Proceedings of Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<pages>843--853</pages>
<marker>Neubig, Watanabe, Mori, 2012</marker>
<rawString>Graham Neubig, Taro Watanabe, and Shinsuke Mori. 2012. Inducing a discriminative parser to optimize machine translation reordering. In Proceedings of Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2012), pages 843–853.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1412" citStr="Och and Ney, 2004" startWordPosition="181" endWordPosition="184">reordering on top of an off-the-shelf phrase-based system. Experimental results show that our method significantly outperforms previous phrase-based and syntax-based models for translation between English and Japanese. 1 Introduction Since the inception of statistical machine translation (SMT), long-distance word reordering has been a notable challenge, particularly when translating between languages with drastically different word orders, such as subject-verb-object (SVO) and subject-object-verb (SOV) languages like English and Japanese, respectively. Phrase-based models (Koehn et al., 2003; Och and Ney, 2004; Xiong et al., 2006) have been strong in local translation and reordering. However, phrase-based models cannot effectively conduct long-distance reordering because they are based purely on statistics of syntax-independent phrases. As a complementary approach to phrase-based models, some researchers have incorporated syntactic information into an SMT framework (Wu, 1997; Yamada and Knight, 2001; Liu et al., 2006) using synchronous context-free grammar (SCFG) (Aho and Yuki Arase Microsoft Research No. 5 Danling St., Haidian Dist. Beijing, P.R. China yukiar@microsoft.com Figure 1: English abstra</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz J. Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of Annual Meeting on Association for Computational Linguistics (ACL</booktitle>
<pages>160--167</pages>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of Annual Meeting on Association for Computational Linguistics (ACL 2003), pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of Annual Meeting on Association for Computational Linguistics (ACL</booktitle>
<pages>311--318</pages>
<contexts>
<context position="4403" citStr="Papineni et al., 2002" startWordPosition="637" endWordPosition="640">ence of the European Chapter of the Association for Computational Linguistics, pages 424–433, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics pendency parse of the source sentence. The target side of each production in the grammar is determined by running the phrase-based decoder. We empirically show effectiveness of our method for English-to-Japanese and Japanese-toEnglish translations by comparing it to phrasebased and syntax-based models. Experimental results show that our method significantly outperforms the previous methods with respect to the BLEU (Papineni et al., 2002) metric. 2 Related Work For adding flexibilities to SCFG under an SMT scenario, previous studies generate only a sourceside syntactic tree and project it to the target-side sentence regardless of the true target-side syntactic structure. Liu et al. (2006) propose a tree-to-string model using a source-side constituency tree to extract correspondences between the source-side tree and the target-side sentence. Quirk et al. (2005) and Xie et al. (2011) use a dependency tree for the same purpose. Since these methods project a finegrained source-side syntax tree, an accurate projection is possible o</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of Annual Meeting on Association for Computational Linguistics (ACL 2002), pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of Annual Meeting on Association for Computational Linguistics (ACL</booktitle>
<pages>271--279</pages>
<contexts>
<context position="4833" citStr="Quirk et al. (2005)" startWordPosition="702" endWordPosition="705">omparing it to phrasebased and syntax-based models. Experimental results show that our method significantly outperforms the previous methods with respect to the BLEU (Papineni et al., 2002) metric. 2 Related Work For adding flexibilities to SCFG under an SMT scenario, previous studies generate only a sourceside syntactic tree and project it to the target-side sentence regardless of the true target-side syntactic structure. Liu et al. (2006) propose a tree-to-string model using a source-side constituency tree to extract correspondences between the source-side tree and the target-side sentence. Quirk et al. (2005) and Xie et al. (2011) use a dependency tree for the same purpose. Since these methods project a finegrained source-side syntax tree, an accurate projection is possible only when the target-side sentence has a syntactic structure that is similar to the source-side. Zhu and Xiao (2011) and Huang and Pendus (2013) generalize rules obtained by the tree-to-string model to increase the chance of rule matching at decoding. Despite their merits, none of these methods resolves the problem of tree projection to the target-side. The hierarchical phrase-based model (HIERO) proposed by Chiang (2007) is in</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In Proceedings of Annual Meeting on Association for Computational Linguistics (ACL 2005), pages 271–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of International Conference on Spoken Language Processing (ICSLP</booktitle>
<pages>901--904</pages>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - An extensible language modeling toolkit. In Proceedings of International Conference on Spoken Language Processing (ICSLP 2002), pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsuhito Sudoh</author>
<author>Kevin Duh</author>
<author>Hajime Tsukada</author>
<author>Tsutomu Hirao</author>
<author>Masaaki Nagata</author>
</authors>
<title>Divide and translate: Improving long distance reordering in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of Joint Workshop on Statistical Machine Translation and Metrics MATR (WMT-MetricsMATR</booktitle>
<pages>418--427</pages>
<marker>Sudoh, Duh, Tsukada, Hirao, Nagata, 2010</marker>
<rawString>Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Tsutomu Hirao, and Masaaki Nagata. 2010. Divide and translate: Improving long distance reordering in statistical machine translation. In Proceedings of Joint Workshop on Statistical Machine Translation and Metrics MATR (WMT-MetricsMATR 2010), pages 418–427.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Jia Xu</author>
<author>Luis Fernando d’Haro</author>
<author>Hermann Ney</author>
</authors>
<title>Error analysis of statistical machine translation output.</title>
<date>2006</date>
<booktitle>In Proceedings of International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>697--702</pages>
<marker>Vilar, Xu, d’Haro, Ney, 2006</marker>
<rawString>David Vilar, Jia Xu, Luis Fernando d’Haro, and Hermann Ney. 2006. Error analysis of statistical machine translation output. In Proceedings of International Conference on Language Resources and Evaluation (LREC 2006), pages 697–702.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1784" citStr="Wu, 1997" startWordPosition="235" endWordPosition="236">ting between languages with drastically different word orders, such as subject-verb-object (SVO) and subject-object-verb (SOV) languages like English and Japanese, respectively. Phrase-based models (Koehn et al., 2003; Och and Ney, 2004; Xiong et al., 2006) have been strong in local translation and reordering. However, phrase-based models cannot effectively conduct long-distance reordering because they are based purely on statistics of syntax-independent phrases. As a complementary approach to phrase-based models, some researchers have incorporated syntactic information into an SMT framework (Wu, 1997; Yamada and Knight, 2001; Liu et al., 2006) using synchronous context-free grammar (SCFG) (Aho and Yuki Arase Microsoft Research No. 5 Danling St., Haidian Dist. Beijing, P.R. China yukiar@microsoft.com Figure 1: English abstraction tree example Ullman, 1972). The original SCFG assumes that the syntactic trees of the source and target languages can be derived synchronously. However, this assumption is too strict for handling parallel sentences that are often comparable rather than parallel. For alleviating this assumption, some researchers have added flexibilities in synchronized tree generat</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Xie</author>
<author>Haitao Mi</author>
<author>Qun Liu</author>
</authors>
<title>A novel dependency-to-string model for statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>216--226</pages>
<contexts>
<context position="4855" citStr="Xie et al. (2011)" startWordPosition="707" endWordPosition="710">ed and syntax-based models. Experimental results show that our method significantly outperforms the previous methods with respect to the BLEU (Papineni et al., 2002) metric. 2 Related Work For adding flexibilities to SCFG under an SMT scenario, previous studies generate only a sourceside syntactic tree and project it to the target-side sentence regardless of the true target-side syntactic structure. Liu et al. (2006) propose a tree-to-string model using a source-side constituency tree to extract correspondences between the source-side tree and the target-side sentence. Quirk et al. (2005) and Xie et al. (2011) use a dependency tree for the same purpose. Since these methods project a finegrained source-side syntax tree, an accurate projection is possible only when the target-side sentence has a syntactic structure that is similar to the source-side. Zhu and Xiao (2011) and Huang and Pendus (2013) generalize rules obtained by the tree-to-string model to increase the chance of rule matching at decoding. Despite their merits, none of these methods resolves the problem of tree projection to the target-side. The hierarchical phrase-based model (HIERO) proposed by Chiang (2007) is independent of any synta</context>
</contexts>
<marker>Xie, Mi, Liu, 2011</marker>
<rawString>Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel dependency-to-string model for statistical machine translation. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP 2011), pages 216–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of International Conference on Computational Linguistics and Annual Meeting on Association for Computational Linguistics (COLING-ACL</booktitle>
<pages>521--528</pages>
<contexts>
<context position="1433" citStr="Xiong et al., 2006" startWordPosition="185" endWordPosition="188">f an off-the-shelf phrase-based system. Experimental results show that our method significantly outperforms previous phrase-based and syntax-based models for translation between English and Japanese. 1 Introduction Since the inception of statistical machine translation (SMT), long-distance word reordering has been a notable challenge, particularly when translating between languages with drastically different word orders, such as subject-verb-object (SVO) and subject-object-verb (SOV) languages like English and Japanese, respectively. Phrase-based models (Koehn et al., 2003; Och and Ney, 2004; Xiong et al., 2006) have been strong in local translation and reordering. However, phrase-based models cannot effectively conduct long-distance reordering because they are based purely on statistics of syntax-independent phrases. As a complementary approach to phrase-based models, some researchers have incorporated syntactic information into an SMT framework (Wu, 1997; Yamada and Knight, 2001; Liu et al., 2006) using synchronous context-free grammar (SCFG) (Aho and Yuki Arase Microsoft Research No. 5 Danling St., Haidian Dist. Beijing, P.R. China yukiar@microsoft.com Figure 1: English abstraction tree example Ul</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In Proceedings of International Conference on Computational Linguistics and Annual Meeting on Association for Computational Linguistics (COLING-ACL 2006), pages 521–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Sentence segmentation using IBM word alignment model 1.</title>
<date>2005</date>
<booktitle>In Proceedings of Annual Conference of the European Association for Machine Translation (EAMT</booktitle>
<pages>280--287</pages>
<marker>Xu, Zens, Ney, 2005</marker>
<rawString>Jia Xu, Richard Zens, and Hermann Ney. 2005. Sentence segmentation using IBM word alignment model 1. In Proceedings of Annual Conference of the European Association for Machine Translation (EAMT 2005), pages 280–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Jaeho Kang</author>
<author>Michael Ringgaard</author>
<author>Franz J Och</author>
</authors>
<title>Using a dependency parser to improve SMT for subject-object-verb languages.</title>
<date>2009</date>
<booktitle>In Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL-HLT</booktitle>
<pages>245--253</pages>
<contexts>
<context position="5995" citStr="Xu et al., 2009" startWordPosition="882" endWordPosition="885">e-based model (HIERO) proposed by Chiang (2007) is independent of any syntactic information and generates SCFG rules only from parallel sentence pairs. Li et al. (2012) and Feng et al. (2012) incorporate syntactic information into HIERO as soft constraints. Since these methods are bound by the original HIERO rules that are independent of syntactic information, their rules cannot represent the global syntactic structure of a sentence. There are also pre-reordering methods for longdistance reordering in SVO-to-SOV translations using heuristics designed based on source-side syntactic structures (Xu et al., 2009; Isozaki et al., 2010; Isozaki et al., 2012). They are fine-tuned to handle only specific reordering problems in a predetermined language pair. Another approach is to statistically learn pre-reordering rules from a corpus; however, this requires a highly parallel training corpus consisting of literal translations to learn Algorithm 1 CKY-style decoding Input: Input sentence u and its dependency tree ru, translation model TM, block-LM bLM, sentence-LM sLM, size of m-best m 1: τu ← generate abstraction tree of u using ru 2: NodeTrans[][] ← ∅ 3: for all node in τu do 4: m-best ← Decode(node, TM,</context>
</contexts>
<marker>Xu, Kang, Ringgaard, Och, 2009</marker>
<rawString>Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz J. Och. 2009. Using a dependency parser to improve SMT for subject-object-verb languages. In Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL-HLT 2009), pages 245–253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntaxbased statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of Annual Meeting on Association for Computational Linguistics (ACL</booktitle>
<pages>523--530</pages>
<contexts>
<context position="1809" citStr="Yamada and Knight, 2001" startWordPosition="237" endWordPosition="240">en languages with drastically different word orders, such as subject-verb-object (SVO) and subject-object-verb (SOV) languages like English and Japanese, respectively. Phrase-based models (Koehn et al., 2003; Och and Ney, 2004; Xiong et al., 2006) have been strong in local translation and reordering. However, phrase-based models cannot effectively conduct long-distance reordering because they are based purely on statistics of syntax-independent phrases. As a complementary approach to phrase-based models, some researchers have incorporated syntactic information into an SMT framework (Wu, 1997; Yamada and Knight, 2001; Liu et al., 2006) using synchronous context-free grammar (SCFG) (Aho and Yuki Arase Microsoft Research No. 5 Danling St., Haidian Dist. Beijing, P.R. China yukiar@microsoft.com Figure 1: English abstraction tree example Ullman, 1972). The original SCFG assumes that the syntactic trees of the source and target languages can be derived synchronously. However, this assumption is too strict for handling parallel sentences that are often comparable rather than parallel. For alleviating this assumption, some researchers have added flexibilities in synchronized tree generation (Wu, 1997; Burkett et</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntaxbased statistical translation model. In Proceedings of Annual Meeting on Association for Computational Linguistics (ACL 2001), pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingbo Zhu</author>
<author>Tong Xiao</author>
</authors>
<title>Improving decoding generalization for tree-to-string translation.</title>
<date>2011</date>
<booktitle>In Proceedings of Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT</booktitle>
<pages>418--423</pages>
<contexts>
<context position="5118" citStr="Zhu and Xiao (2011)" startWordPosition="751" endWordPosition="754"> generate only a sourceside syntactic tree and project it to the target-side sentence regardless of the true target-side syntactic structure. Liu et al. (2006) propose a tree-to-string model using a source-side constituency tree to extract correspondences between the source-side tree and the target-side sentence. Quirk et al. (2005) and Xie et al. (2011) use a dependency tree for the same purpose. Since these methods project a finegrained source-side syntax tree, an accurate projection is possible only when the target-side sentence has a syntactic structure that is similar to the source-side. Zhu and Xiao (2011) and Huang and Pendus (2013) generalize rules obtained by the tree-to-string model to increase the chance of rule matching at decoding. Despite their merits, none of these methods resolves the problem of tree projection to the target-side. The hierarchical phrase-based model (HIERO) proposed by Chiang (2007) is independent of any syntactic information and generates SCFG rules only from parallel sentence pairs. Li et al. (2012) and Feng et al. (2012) incorporate syntactic information into HIERO as soft constraints. Since these methods are bound by the original HIERO rules that are independent o</context>
</contexts>
<marker>Zhu, Xiao, 2011</marker>
<rawString>Jingbo Zhu and Tong Xiao. 2011. Improving decoding generalization for tree-to-string translation. In Proceedings of Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2001), pages 418–423.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>