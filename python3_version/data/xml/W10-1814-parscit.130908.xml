<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000375">
<title confidence="0.96498">
Cross-lingual Validity of PropBank in the Manual Annotation of French
</title>
<author confidence="0.498264">
Lonneke van der Plas Tanja Samardii´c Paola Merlo
</author>
<affiliation confidence="0.739785">
Linguistics Department
University of Geneva
</affiliation>
<address confidence="0.6473985">
Rue de Candolle 5, 1204 Geneva
Switzerland
</address>
<email confidence="0.997004">
{Lonneke.vanderPlas,Tanja.Samardzic,Paola.Merlo}@unige.ch
</email>
<sectionHeader confidence="0.997357" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99947025">
Methods that re-use existing mono-lingual
semantic annotation resources to annotate
a new language rely on the hypothesis that
the semantic annotation scheme used is
cross-lingually valid. We test this hypoth-
esis in an annotation agreement study. We
show that the annotation scheme can be
applied cross-lingually.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999877814285714">
It is hardly a controversial statement that elegant
language subtleties and powerful linguistic im-
agery found in literary writing are lost in trans-
lation. Yet, translation preserves enough meaning
across language pairs to be useful in many appli-
cations and for many text genres.
The belief that this layer of meaning which is
preserved across languages can be formally rep-
resented and automatically calculated underlies
methods that use parallel corpora for the automatic
generation of semantic annotations through cross-
lingual transfer (Pad´o, 2007; Basili et al., 2009).
A methodology similar in spirit — re-use of the
existing resources in a different language — has
also been applied in developing manually anno-
tated resources. Monachesi et al. (2007) annotate
Dutch sentences using the PropBank annotation
scheme (Palmer et al., 2005), while Burchardt et
al. (2009) use the FrameNet framework (Fillmore
et al., 2003) to annotate a German corpus. In-
stead of building special lexicons containing the
specific semantic information needed for the an-
notation for each language separately, which is a
complex and time-consuming endeavour in itself,
these approaches rely on the lexicons already de-
veloped for English.
In this paper, we hypothesize that the level
of abstraction that is necessary to develop a se-
mantic lexicon/ontology for a single language
based on observable linguistic behaviour — that
is a mono-lingual, item-specific annotation — is
cross-linguistically valid. We test this hypothe-
sis by manually annotating French sentences using
the PropBank frame files developed for English.
It has been claimed that semantic parallelism
across languages is smaller when using the
PropBank semantic annotations instead of the
FrameNet scheme, because FrameNet is more ab-
stract and less verb-specific (Pad´o, 2007). We are
working with the PropBank annotation scheme,
contrary to other works that use the FrameNet
scheme, such as Pad´o (2007) and Basili et al.
(2009). We choose this annotation for two main
reasons. First, the primary use of our annotation is
to serve as a gold standard in the task of syntactic-
semantic parsing. FrameNet does not have a prop-
erly sampled hand-annotated corpus of English,
by design. So we cannot use it for this task. Sec-
ond, in Merlo and Van der Plas (2009), the seman-
tic annotations schemes of PropBank and VerbNet
(Kipper, 2005) are compared, based on annotation
of the SemLink project (Loper et al., 2007). The
authors conclude that PropBank is the preferred
annotation for a joint syntactic-semantic setting.
If the PropBank annotation scheme is cross-
lingually valid, annotators can reach a consensus
and can do so swiftly. Thus, cross-lingual valid-
ity is measured by how well-defined the manual
annotation task is (inter-annotator agreement) and
by how hard it is to reach an agreement (pre- and
post-consensus inter-annotator agreement). In ad-
dition, we measure the impact of the level of ab-
straction of the predicate labels. Conversely, how
often labels do not transfer and distributions of dis-
agreements are indicators of lack of parallelism
across languages that we study both by quantita-
tive and qualitative analysis.
To preview the results, we find that the Prop-
Bank annotation scheme developed for English
can be applied for a large portion of French sen-
</bodyText>
<page confidence="0.989182">
113
</page>
<note confidence="0.3918025">
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 113–117,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.992467333333333">
tences without adjustments, which confirms its
cross-lingual validity. A high level of inter-
annotator agreement is reached when the verb-
specific PropBank labels are replaced by less fine-
grained verb classes after annotating. Non-parallel
cases are mostly due to idioms and collocations.
</bodyText>
<sectionHeader confidence="0.972224" genericHeader="introduction">
2 Materials and Methods
</sectionHeader>
<bodyText confidence="0.999514142857143">
Our choices of formal representation and of la-
belling scheme are driven by the goal of produc-
ing useful annotations for syntactic-semantic pars-
ing in a setting based on an aligned corpus. In the
following subsections we describe the annotation
scheme and procedure, the corpus, and phases of
annotation.
</bodyText>
<subsectionHeader confidence="0.970674">
2.1 The PropBank Annotation Framework
</subsectionHeader>
<bodyText confidence="0.9999534">
We use the PropBank scheme for the manual anno-
tations. PropBank is a linguistic resource that con-
tains information on the semantic structure of sen-
tences. It consists of a one-million-word corpus
of naturally occurring sentences annotated with
semantic structures and a lexicon (the PropBank
frame files) that lists all the predicates (verbs) that
can be found in the annotated sentences and the
sets of semantic roles they introduce.
Predicates are marked with labels that specify
the sense of the verb in the particular sentence. Ar-
guments are marked with the labels A0 to A5. The
labels A0 and A1 have approximately the same
value with all verbs. They are used to mark in-
stances of typical AGENTS (A0) and PATIENTS
(A1). The value of other numbers varies across
verbs. Modifiers are annotated in PropBank with
the label AM. This label can have different exten-
sions depending on the semantic type of the con-
stituent, for example locatives and adverbials.
</bodyText>
<subsectionHeader confidence="0.999458">
2.2 Annotation Procedure
</subsectionHeader>
<bodyText confidence="0.854539916666667">
Annotators have access to PropBank frame files
and guidelines adapted for the current task. The
frame files provide verb-specific descriptions of all
possible semantic roles and illustrate these roles
with examples as shown for the verb paid in (1)
and the verb senses of pay in Table 1. Annotators
need to look up each verb in the frame files to be
able to label it with the right verb sense and to be
able to allocate the arguments consistently.
(1) [A0 The Latin American nation] has
[REL−PAY01 paid] [A1 very little] [A3 on its
debt] [AM−TMP since early last year].
</bodyText>
<table confidence="0.99936494117647">
Frame Semantic roles
pay.01 A0: payer or buyer
A1: money or attention
A2: person being paid, destination of attention
A3: commodity, paid for what
pay.02 A0: payer
pay off A1: debt
A2: owed to whom, person paid
pay.03 A0: payer or buyer
pay out A1: money or attention
A2: person being paid, destination of attention
A3: commodity, paid for what
pay.04 A1: thing succeeding or working out
pay.05 A1: thing succeeding or working out
pay off
pay.06 A0: payer
pay down A1: debt
</table>
<tableCaption confidence="0.999877">
Table 1: The PropBank lexicon entry for pay.
</tableCaption>
<bodyText confidence="0.9999885">
In our cross-lingual setting, annotators used
the English PropBank frame files to annotate the
French sentences. This means that for every pred-
icate they find in the French sentence, they need
to translate it, and find an English verb sense that
is applicable to the French verb. If an appropri-
ate entry cannot be found in the frame files for a
given predicate, the annotator is instructed to use
the “dummy” label for the predicate and fill in the
roles according to their own insights.
For the annotation of sentences we use an adap-
tation of the user-friendly, freely available Tree
Editor (TrEd, Pajas and ˘St˘ep´anek, 2008). The tool
shows the syntactic analysis and the plain sentence
in the same window allowing the user to add se-
mantic arcs and labels to the nodes in the syntactic
dependency tree.
The decision to show syntactic information is
merely driven by the fact that we want to guide the
annotator in selecting the heads of phrases during
the annotation process. The sentences are parsed
by a syntactic parser (Titov and Henderson, 2007)
that we trained on syntactic dependency annota-
tions for French (Candito et al., 2009). Although
the parser is state-of-the-art (87.2% Labelled At-
tachment Score), in case of parse errors, we ask
annotators to ignore the errors of the parser and
put the label on the actual head.
</bodyText>
<subsectionHeader confidence="0.990386">
2.3 Corpus
</subsectionHeader>
<bodyText confidence="0.999854166666667">
We selected the French sentences for the man-
ual annotation from the parallel Europarl corpus
(Koehn, 2005). Because translation shifts are
known to pose problems for the automatic cross-
lingual transfer of semantic roles (Pad´o, 2007)
and for machine translation (Ozdowska and Way,
</bodyText>
<page confidence="0.994593">
114
</page>
<bodyText confidence="0.99990475">
2009), and these are more likely to appear in in-
direct translations, we decided to select only those
parallel sentences, for which we can infer from the
labels used in Europarl that they are direct trans-
lations from English to French, or vice versa. We
selected 1040 sentences for annotation (40 in to-
tal for the two training phases, 100 for calibration,
and 900 for the main annotation phase.)1
</bodyText>
<subsectionHeader confidence="0.998391">
2.4 Annotation Phases
</subsectionHeader>
<bodyText confidence="0.999794">
The training procedure described in Figure 1
is inspired by the methodology indicated in
Pad´o (2007). A set of 130 sentences were anno-
tated manually by four annotators with very good
proficiency in both French and English for the
training and the calibration phase. The remaining
900 sentences are annotated by one annotator (out
of those four), a trained linguist. Inter-annotator
agreement was measured at several points in the
annotation process marked with an arrow in Fig-
ure 1. The guidelines were adjusted after the train-
ing phase.
</bodyText>
<listItem confidence="0.939704">
• Training phase
</listItem>
<bodyText confidence="0.465830666666667">
-TrainingA: 10 sentences, all annotators together
-TrainingB: 30 sentences, all annotators individually⇐
-Reach consensus on Training B ⇐
</bodyText>
<listItem confidence="0.77236575">
• Calibration phase
-100 sentences by main annotator, one third of those by
each of the other 3 annotators ⇐
• Main annotation phase
</listItem>
<figure confidence="0.468854">
-900 sentences by main annotator
</figure>
<figureCaption confidence="0.999754">
Figure 1: The annotation phases.
</figureCaption>
<sectionHeader confidence="0.999929" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.9983552">
Cross-lingual validity is measured by comparing
inter-annotator agreement at several stages in the
annotation, by measuring the agreement on less
specific predicate labelling, and by a quantitative
and qualitative analysis of non-parallel cases.
</bodyText>
<subsectionHeader confidence="0.946141">
3.1 Inter-annotator Agreement for Several
Annotation Phases
</subsectionHeader>
<bodyText confidence="0.99990025">
To assess the quality of the manual annotations we
measured the agreement between annotators as the
average F-measure of all pairs of annotators after
each phase of the annotation procedure.2 The first
</bodyText>
<footnote confidence="0.837153714285714">
1As usual practice in preprocessing for automatic align-
ment, the datasets were tokenised and lowercased and only
sentence pairs corresponding to a 1-to-1 alignment with
lengths ranging from 1 to 40 tokens on both French and En-
glish sides were considered.
2It is a known fact that measuring annotator agreement us-
ing the kappa score is problematic in categorisation tasks that
</footnote>
<table confidence="0.997819">
Predicates Unl. F Arguments Unl. F
Lab. F Lab. F
TrainingB 46 85 62 75
TrainingB(cons.) 95 97 91 95
Calibration 59 93 69 84
</table>
<tableCaption confidence="0.730729">
Table 2: Percent inter-annotator agreement (F-
measure) for labelled/unlabelled predicates and
for labelled/unlabelled arguments
</tableCaption>
<bodyText confidence="0.99997715">
row of Table 2 shows that the task is hard. But
the difference between the first row and the sec-
ond row shows that there were many differences
between annotators that could be resolved. After
discussions and individual corrections the scores
are between 91% and 95%. This indicates that
the task is well-defined. Row three shows that the
agreement in the calibration phase increases a lot
compared to the last training phase (row 1). This
might in part be due to the fact that the guidelines
were adjusted by the end of the training phase, but
could also be because the annotators are getting
more acquainted to the task and the software.
As expected, because annotators used the En-
glish PropBank frame files to annotate French
verbs, the task of labelling predicates proved more
difficult than labelling semantic roles. It results in
the lowest agreement scores overall. In the follow-
ing subsections we study the sources of disagree-
ment in predicate labelling in more detail.
</bodyText>
<subsectionHeader confidence="0.9912305">
3.2 Inter-annotator Agreement in Predicate
Labellings
</subsectionHeader>
<bodyText confidence="0.973582761904762">
Predicate labels in PropBank apply to particular
verb senses, for example walk.01 for the first sense
of the verb walk. Even though the senses are
coarser than, for example, the senses in Word-
Net (Fellbaum, 1998), the labels are rather spe-
cific. This specificity possibly poses problems
when working in a cross-lingual setting.
We compare the agreement reached using Prop-
Bank verb sense labels with the agreement reached
using the verb classifications from VerbNet (Kip-
per, 2005) and the mapping to PropBank labels
as provided in the type mappings of the SemLink
project3 (Loper et al., 2007). If two annotators
used two different predicate labels to annotate the
do not have a fixed number of items and categories (Burchardt
et al., 2006). The F-measure is a well-known measure used
for the evaluation of many task such as syntactic-semantic
parsing, the task that is the motivation for this paper. The
choice of the F-measure makes the comparison to the perfor-
mance of the future parser easier.
3(http://verbs.colorado.edu/semlink/)
</bodyText>
<page confidence="0.997618">
115
</page>
<bodyText confidence="0.999910866666667">
same verb, but those verb senses belong to the
same verb class, we count those as correct4.
The average inter-annotator agreement is rela-
tively low when we compare the annotations on
the PropBank verb sense level: 59%. However, at
the level of verb classes, the inter-annotator agree-
ment increases to 81%. This raises the issue of
whether we should not label the predicates with
verb classes instead of verb senses. By using Prop-
Bank labels for the manual annotation and replac-
ing these with verb classes in post-processing, the
benefits are two-fold: We are able to reach a high
level of cross-lingual parallelism on the annota-
tions, while keeping the manual annotation task as
specific and less abstract as possible.
</bodyText>
<subsectionHeader confidence="0.999959">
3.3 Analysis of Non-Parallel Cases
</subsectionHeader>
<bodyText confidence="0.999982407407408">
For a single annotator, the main measure of cross-
lingual validity is the percentage of dummy pred-
icates in the annotation. In the sentences from the
calibration and the main annotation phase from the
main annotator (1000 sentences in total), we find
130 predicates (tokens) for which the annotator
used the “dummy” label.
Manual inspection reveals that the “dummy” la-
bel is mainly used for French multi-word expres-
sions (82%), most of which can be translated by
a single English verb (47%), whereas others can-
not, because they are translated by a combination
that includes a form of ‘be’ that is not annotated
in PropBank (25%). The 47% of multi-word ex-
pressions that receive the “dummy” label show the
annotator’s reluctance to put a single verb label on
a French multi-word expression. The annotation
guidelines could be adapted to instruct annotators
not to hesitate in such cases.
Similarly, collocations and idiomatic expres-
sions are the main sources of disagreement in
predicate labellings among annotators. We can
conclude that, as shown in studies on other lan-
guage pairs (Burchardt et al., 2009), collocations
and idiomatic expressions were identified as verb
uses where the verb’s predicate label cannot be
transferred directly from one language to another.
</bodyText>
<sectionHeader confidence="0.993411" genericHeader="method">
4 Discussion and Related Work
</sectionHeader>
<bodyText confidence="0.857267">
Burchardt et al. (2009) use English FrameNet to
</bodyText>
<footnote confidence="0.9552198">
4The mappings from PropBank verb sense labels to Verb-
Net verb classes are one-to-many and not complete. We
counted a pair as matching if there exists a class to which
both verb senses belong. We found a verb class for both verb
senses in about 78% of the cases and discarded the rest.
</footnote>
<bodyText confidence="0.999884709677419">
annotate a corpus of German sentences manually.
They find that the vast majority of frames can be
applied to German directly. However, around one
third of the verb senses identified in the German
corpus were not covered by FrameNet. Also, a
number of German verbs were found to be under-
specified. Finally, some problems related to treat-
ing particular verb uses were identified, such as id-
ioms, metaphors, and support verb constructions.
Monachesi et al. (2007) use PropBank labels for
semi-automatic annotation of a corpus of Dutch
sentences. Semantic roles were first annotated
using a rule-based semantic parser and then cor-
rected by one annotator. Although not all Dutch
verbs could be translated to an equivalent verb
sense in English, these cases were assessed as rel-
atively rare. What proved to be problematic was
identifying the correct label for modifiers.
Bittar (2009) makes use of cross-lingual lexi-
cal transfer in annotating French verbs with event
types, by adapting a small-scale English verb lex-
icon with specified event structure (TimeML).
The inter-annotator agreement in labelling pred-
icates reported in Burchardt et al. (2009) reaches
85%, while our best score (when falling back to
verb classes) is 81%. However, unlike Burchardt
et al. (2009) we did not introduce any new French
labels. We find, like Monachesi et al. (2007), that
non-parallel cases are less frequent than what is re-
ported in Burchardt et al. (2009), which could be
due to the properties of the annotations schemes.
</bodyText>
<sectionHeader confidence="0.999652" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99997025">
We can conclude that the general task of anno-
tating French sentences using English PropBank
frame files is well-defined. Nevertheless, it is a
hard task that requires linguistic training. With re-
spect to the disagreements on labelling predicates,
we can conclude that a large part can be resolved
if we compare the annotations at the level of verb
classes instead of at the very fine-grained level of
verb senses. Non-parallel cases are mostly due to
idioms and collocations. Their rate is relatively
low and can be further reduced by adapting anno-
tation guidelines.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9911838">
The research leading to these results has received fund-
ing from the EU FP7 programme (FP7/2007-2013) under
grant agreement nr 216594 (CLASSIC project: www.classic-
project.org). We would like to thank Goljihan Kashaeva and
James Henderson for valuable comments.
</bodyText>
<page confidence="0.998479">
116
</page>
<sectionHeader confidence="0.996401" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999915955882353">
R. Basili, D. De Cao, D. Croce, B. Coppola, and A. Moschitti,
2009. Computational Linguistics and Intelligent Text Pro-
cessing, chapter Cross-Language Frame Semantics Trans-
fer in Bilingual Corpora, pages 332–345. Springer Berlin
/ Heidelberg.
A. Bittar. 2009. Annotation of events and temporal expres-
sions in French texts. In Proceedings of the third Linguis-
tic Annotation Workshop (LAW III), pages 48–51, Suntec,
Singapore.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pad´o, and
M. Pinkal. 2006. The SALSA corpus: a German cor-
pus resource for lexical semantics. In Proceedings of the
5th International Conference on Language Resources and
Evaluation (LREC 2006), pages 969–974, Genoa, Italy.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado, and
M. Pinkal, 2009. Multilingual FrameNets in Computa-
tional Lexicography: Methods and Applications, chapter
FrameNet for the semantic analysis of German: Annota-
tion, representation and automation, pages 209–244. De
Gruyter Mouton, Berlin.
M.-H. Candito, B. Crabb´e, P. Denis, and F. Gu´erin.
2009. Analyse syntaxique du franc¸ais : des constitu-
ants aux d´ependances. In Proceedings of la Conf´erence
sur le Traitement Automatique des Langues Naturelles
(TALN’09), Senlis, France.
C. Fellbaum. 1998. WordNet, an electronic lexical database.
MIT Press.
C. J. Fillmore, R. Johnson, and M.R.L. Petruck. 2003. Back-
ground to FrameNet. International journal of lexicogra-
phy, 16.3:235–250.
K. Kipper. 2005. VerbNet: A broad-coverage, comprehen-
sive verb lexicon. Ph.D. thesis, University of Pennsylvnia.
P. Koehn. 2005. Europarl: A parallel corpus for statistical
machine translation. In Proceedings of the MT Summit,
pages 79–86, Phuket, Thailand.
E. Loper, S-T Yi, and M. Palmer. 2007. Combining lexical
resources: Mapping between PropBank and VerbNet. In
Proceedings of the 7th International Workshop on Com-
putational Semantics (IWCS-7), pages 118–129, Tilburg,
The Netherlands.
P. Merlo and L. van der Plas. 2009. Abstraction and gen-
eralisation in semantic role labels: PropBank, VerbNet
or both? In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of the
AFNLP, pages 288–296, Suntec, Singapore.
P. Monachesi, G. Stevens, and J. Trapman. 2007. Adding
semantic role annotation to a corpus of written Dutch.
In Proceedings of the Linguistic Annotation Workshop
(LAW), pages 77–84, Prague, Czech republic.
S. Ozdowska and A. Way. 2009. Optimal bilingual data for
French-English PB-SMT. In Proceedings of the 13th An-
nual Conference of the European Association for Machine
Translation (EAMT’09), pages 96–103, Barcelona, Spain.
S. Pad´o. 2007. Cross-lingual Annotation Projection Mod-
els for Role-Semantic Information. Ph.D. thesis, Saarland
University.
P. Pajas and J. ˘St˘ep´anek. 2008. Recent advances in a feature-
rich framework for treebank annotation. In Proceedings of
the 22nd International Conference on Computational Lin-
guistics (Coling 2008), pages 673–680, Manchester, UK.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The Proposi-
tion Bank: An annotated corpus of semantic roles. Com-
putational Linguistics, 31:71–105.
I. Titov and J. Henderson. 2007. A latent variable model
for generative dependency parsing. In Proceedings of the
International Conference on Parsing Technologies (IWPT-
07), pages 144–155, Prague, Czech Republic.
</reference>
<page confidence="0.998126">
117
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.693390">
<title confidence="0.987064">Cross-lingual Validity of PropBank in the Manual Annotation of French</title>
<author confidence="0.987079">Lonneke van_der_Plas Tanja Samardii´c Paola Merlo</author>
<affiliation confidence="0.988388">Linguistics University of</affiliation>
<address confidence="0.737076">Rue de Candolle 5, 1204</address>
<abstract confidence="0.997508888888889">Methods that re-use existing mono-lingual semantic annotation resources to annotate a new language rely on the hypothesis that the semantic annotation scheme used is cross-lingually valid. We test this hypothesis in an annotation agreement study. We show that the annotation scheme can be applied cross-lingually.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Basili</author>
<author>D De Cao</author>
<author>D Croce</author>
<author>B Coppola</author>
<author>A Moschitti</author>
</authors>
<date>2009</date>
<booktitle>Computational Linguistics and Intelligent Text Processing, chapter Cross-Language Frame Semantics Transfer in Bilingual Corpora,</booktitle>
<pages>332--345</pages>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<marker>Basili, De Cao, Croce, Coppola, Moschitti, 2009</marker>
<rawString>R. Basili, D. De Cao, D. Croce, B. Coppola, and A. Moschitti, 2009. Computational Linguistics and Intelligent Text Processing, chapter Cross-Language Frame Semantics Transfer in Bilingual Corpora, pages 332–345. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bittar</author>
</authors>
<title>Annotation of events and temporal expressions in French texts.</title>
<date>2009</date>
<booktitle>In Proceedings of the third Linguistic Annotation Workshop (LAW III),</booktitle>
<pages>48--51</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="16222" citStr="Bittar (2009)" startWordPosition="2614" endWordPosition="2615">ere found to be underspecified. Finally, some problems related to treating particular verb uses were identified, such as idioms, metaphors, and support verb constructions. Monachesi et al. (2007) use PropBank labels for semi-automatic annotation of a corpus of Dutch sentences. Semantic roles were first annotated using a rule-based semantic parser and then corrected by one annotator. Although not all Dutch verbs could be translated to an equivalent verb sense in English, these cases were assessed as relatively rare. What proved to be problematic was identifying the correct label for modifiers. Bittar (2009) makes use of cross-lingual lexical transfer in annotating French verbs with event types, by adapting a small-scale English verb lexicon with specified event structure (TimeML). The inter-annotator agreement in labelling predicates reported in Burchardt et al. (2009) reaches 85%, while our best score (when falling back to verb classes) is 81%. However, unlike Burchardt et al. (2009) we did not introduce any new French labels. We find, like Monachesi et al. (2007), that non-parallel cases are less frequent than what is reported in Burchardt et al. (2009), which could be due to the properties of</context>
</contexts>
<marker>Bittar, 2009</marker>
<rawString>A. Bittar. 2009. Annotation of events and temporal expressions in French texts. In Proceedings of the third Linguistic Annotation Workshop (LAW III), pages 48–51, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Burchardt</author>
<author>K Erk</author>
<author>A Frank</author>
<author>A Kowalski</author>
<author>S Pad´o</author>
<author>M Pinkal</author>
</authors>
<title>The SALSA corpus: a German corpus resource for lexical semantics.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>969--974</pages>
<location>Genoa, Italy.</location>
<marker>Burchardt, Erk, Frank, Kowalski, Pad´o, Pinkal, 2006</marker>
<rawString>A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pad´o, and M. Pinkal. 2006. The SALSA corpus: a German corpus resource for lexical semantics. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006), pages 969–974, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Burchardt</author>
<author>K Erk</author>
<author>A Frank</author>
<author>A Kowalski</author>
<author>S Pado</author>
<author>M Pinkal</author>
</authors>
<title>Multilingual FrameNets in Computational Lexicography: Methods and Applications, chapter FrameNet for the semantic analysis of German: Annotation, representation and automation,</title>
<date>2009</date>
<pages>209--244</pages>
<institution>De Gruyter Mouton,</institution>
<location>Berlin.</location>
<contexts>
<context position="1473" citStr="Burchardt et al. (2009)" startWordPosition="210" endWordPosition="213">tions and for many text genres. The belief that this layer of meaning which is preserved across languages can be formally represented and automatically calculated underlies methods that use parallel corpora for the automatic generation of semantic annotations through crosslingual transfer (Pad´o, 2007; Basili et al., 2009). A methodology similar in spirit — re-use of the existing resources in a different language — has also been applied in developing manually annotated resources. Monachesi et al. (2007) annotate Dutch sentences using the PropBank annotation scheme (Palmer et al., 2005), while Burchardt et al. (2009) use the FrameNet framework (Fillmore et al., 2003) to annotate a German corpus. Instead of building special lexicons containing the specific semantic information needed for the annotation for each language separately, which is a complex and time-consuming endeavour in itself, these approaches rely on the lexicons already developed for English. In this paper, we hypothesize that the level of abstraction that is necessary to develop a semantic lexicon/ontology for a single language based on observable linguistic behaviour — that is a mono-lingual, item-specific annotation — is cross-linguistica</context>
<context position="14817" citStr="Burchardt et al., 2009" startWordPosition="2381" endWordPosition="2384">rb (47%), whereas others cannot, because they are translated by a combination that includes a form of ‘be’ that is not annotated in PropBank (25%). The 47% of multi-word expressions that receive the “dummy” label show the annotator’s reluctance to put a single verb label on a French multi-word expression. The annotation guidelines could be adapted to instruct annotators not to hesitate in such cases. Similarly, collocations and idiomatic expressions are the main sources of disagreement in predicate labellings among annotators. We can conclude that, as shown in studies on other language pairs (Burchardt et al., 2009), collocations and idiomatic expressions were identified as verb uses where the verb’s predicate label cannot be transferred directly from one language to another. 4 Discussion and Related Work Burchardt et al. (2009) use English FrameNet to 4The mappings from PropBank verb sense labels to VerbNet verb classes are one-to-many and not complete. We counted a pair as matching if there exists a class to which both verb senses belong. We found a verb class for both verb senses in about 78% of the cases and discarded the rest. annotate a corpus of German sentences manually. They find that the vast m</context>
<context position="16489" citStr="Burchardt et al. (2009)" startWordPosition="2652" endWordPosition="2655"> of Dutch sentences. Semantic roles were first annotated using a rule-based semantic parser and then corrected by one annotator. Although not all Dutch verbs could be translated to an equivalent verb sense in English, these cases were assessed as relatively rare. What proved to be problematic was identifying the correct label for modifiers. Bittar (2009) makes use of cross-lingual lexical transfer in annotating French verbs with event types, by adapting a small-scale English verb lexicon with specified event structure (TimeML). The inter-annotator agreement in labelling predicates reported in Burchardt et al. (2009) reaches 85%, while our best score (when falling back to verb classes) is 81%. However, unlike Burchardt et al. (2009) we did not introduce any new French labels. We find, like Monachesi et al. (2007), that non-parallel cases are less frequent than what is reported in Burchardt et al. (2009), which could be due to the properties of the annotations schemes. 5 Conclusions We can conclude that the general task of annotating French sentences using English PropBank frame files is well-defined. Nevertheless, it is a hard task that requires linguistic training. With respect to the disagreements on la</context>
</contexts>
<marker>Burchardt, Erk, Frank, Kowalski, Pado, Pinkal, 2009</marker>
<rawString>A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado, and M. Pinkal, 2009. Multilingual FrameNets in Computational Lexicography: Methods and Applications, chapter FrameNet for the semantic analysis of German: Annotation, representation and automation, pages 209–244. De Gruyter Mouton, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-H Candito</author>
<author>B Crabb´e</author>
<author>P Denis</author>
<author>F Gu´erin</author>
</authors>
<title>Analyse syntaxique du franc¸ais : des constituants aux d´ependances.</title>
<date>2009</date>
<booktitle>In Proceedings of la Conf´erence sur le Traitement Automatique des Langues Naturelles (TALN’09),</booktitle>
<location>Senlis, France.</location>
<marker>Candito, Crabb´e, Denis, Gu´erin, 2009</marker>
<rawString>M.-H. Candito, B. Crabb´e, P. Denis, and F. Gu´erin. 2009. Analyse syntaxique du franc¸ais : des constituants aux d´ependances. In Proceedings of la Conf´erence sur le Traitement Automatique des Langues Naturelles (TALN’09), Senlis, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet, an electronic lexical database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="12132" citStr="Fellbaum, 1998" startWordPosition="1946" endWordPosition="1947"> software. As expected, because annotators used the English PropBank frame files to annotate French verbs, the task of labelling predicates proved more difficult than labelling semantic roles. It results in the lowest agreement scores overall. In the following subsections we study the sources of disagreement in predicate labelling in more detail. 3.2 Inter-annotator Agreement in Predicate Labellings Predicate labels in PropBank apply to particular verb senses, for example walk.01 for the first sense of the verb walk. Even though the senses are coarser than, for example, the senses in WordNet (Fellbaum, 1998), the labels are rather specific. This specificity possibly poses problems when working in a cross-lingual setting. We compare the agreement reached using PropBank verb sense labels with the agreement reached using the verb classifications from VerbNet (Kipper, 2005) and the mapping to PropBank labels as provided in the type mappings of the SemLink project3 (Loper et al., 2007). If two annotators used two different predicate labels to annotate the do not have a fixed number of items and categories (Burchardt et al., 2006). The F-measure is a well-known measure used for the evaluation of many t</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet, an electronic lexical database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Fillmore</author>
<author>R Johnson</author>
<author>M R L Petruck</author>
</authors>
<title>Background to FrameNet. International journal of lexicography,</title>
<date>2003</date>
<pages>16--3</pages>
<contexts>
<context position="1524" citStr="Fillmore et al., 2003" startWordPosition="218" endWordPosition="221"> layer of meaning which is preserved across languages can be formally represented and automatically calculated underlies methods that use parallel corpora for the automatic generation of semantic annotations through crosslingual transfer (Pad´o, 2007; Basili et al., 2009). A methodology similar in spirit — re-use of the existing resources in a different language — has also been applied in developing manually annotated resources. Monachesi et al. (2007) annotate Dutch sentences using the PropBank annotation scheme (Palmer et al., 2005), while Burchardt et al. (2009) use the FrameNet framework (Fillmore et al., 2003) to annotate a German corpus. Instead of building special lexicons containing the specific semantic information needed for the annotation for each language separately, which is a complex and time-consuming endeavour in itself, these approaches rely on the lexicons already developed for English. In this paper, we hypothesize that the level of abstraction that is necessary to develop a semantic lexicon/ontology for a single language based on observable linguistic behaviour — that is a mono-lingual, item-specific annotation — is cross-linguistically valid. We test this hypothesis by manually anno</context>
</contexts>
<marker>Fillmore, Johnson, Petruck, 2003</marker>
<rawString>C. J. Fillmore, R. Johnson, and M.R.L. Petruck. 2003. Background to FrameNet. International journal of lexicography, 16.3:235–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kipper</author>
</authors>
<title>VerbNet: A broad-coverage, comprehensive verb lexicon.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvnia.</institution>
<contexts>
<context position="2975" citStr="Kipper, 2005" startWordPosition="452" endWordPosition="453">rameNet is more abstract and less verb-specific (Pad´o, 2007). We are working with the PropBank annotation scheme, contrary to other works that use the FrameNet scheme, such as Pad´o (2007) and Basili et al. (2009). We choose this annotation for two main reasons. First, the primary use of our annotation is to serve as a gold standard in the task of syntacticsemantic parsing. FrameNet does not have a properly sampled hand-annotated corpus of English, by design. So we cannot use it for this task. Second, in Merlo and Van der Plas (2009), the semantic annotations schemes of PropBank and VerbNet (Kipper, 2005) are compared, based on annotation of the SemLink project (Loper et al., 2007). The authors conclude that PropBank is the preferred annotation for a joint syntactic-semantic setting. If the PropBank annotation scheme is crosslingually valid, annotators can reach a consensus and can do so swiftly. Thus, cross-lingual validity is measured by how well-defined the manual annotation task is (inter-annotator agreement) and by how hard it is to reach an agreement (pre- and post-consensus inter-annotator agreement). In addition, we measure the impact of the level of abstraction of the predicate labels</context>
<context position="12399" citStr="Kipper, 2005" startWordPosition="1986" endWordPosition="1988">ons we study the sources of disagreement in predicate labelling in more detail. 3.2 Inter-annotator Agreement in Predicate Labellings Predicate labels in PropBank apply to particular verb senses, for example walk.01 for the first sense of the verb walk. Even though the senses are coarser than, for example, the senses in WordNet (Fellbaum, 1998), the labels are rather specific. This specificity possibly poses problems when working in a cross-lingual setting. We compare the agreement reached using PropBank verb sense labels with the agreement reached using the verb classifications from VerbNet (Kipper, 2005) and the mapping to PropBank labels as provided in the type mappings of the SemLink project3 (Loper et al., 2007). If two annotators used two different predicate labels to annotate the do not have a fixed number of items and categories (Burchardt et al., 2006). The F-measure is a well-known measure used for the evaluation of many task such as syntactic-semantic parsing, the task that is the motivation for this paper. The choice of the F-measure makes the comparison to the performance of the future parser easier. 3(http://verbs.colorado.edu/semlink/) 115 same verb, but those verb senses belong </context>
</contexts>
<marker>Kipper, 2005</marker>
<rawString>K. Kipper. 2005. VerbNet: A broad-coverage, comprehensive verb lexicon. Ph.D. thesis, University of Pennsylvnia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the MT Summit,</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand.</location>
<contexts>
<context position="8254" citStr="Koehn, 2005" startWordPosition="1323" endWordPosition="1324">information is merely driven by the fact that we want to guide the annotator in selecting the heads of phrases during the annotation process. The sentences are parsed by a syntactic parser (Titov and Henderson, 2007) that we trained on syntactic dependency annotations for French (Candito et al., 2009). Although the parser is state-of-the-art (87.2% Labelled Attachment Score), in case of parse errors, we ask annotators to ignore the errors of the parser and put the label on the actual head. 2.3 Corpus We selected the French sentences for the manual annotation from the parallel Europarl corpus (Koehn, 2005). Because translation shifts are known to pose problems for the automatic crosslingual transfer of semantic roles (Pad´o, 2007) and for machine translation (Ozdowska and Way, 114 2009), and these are more likely to appear in indirect translations, we decided to select only those parallel sentences, for which we can infer from the labels used in Europarl that they are direct translations from English to French, or vice versa. We selected 1040 sentences for annotation (40 in total for the two training phases, 100 for calibration, and 900 for the main annotation phase.)1 2.4 Annotation Phases The</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the MT Summit, pages 79–86, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Loper</author>
<author>S-T Yi</author>
<author>M Palmer</author>
</authors>
<title>Combining lexical resources: Mapping between PropBank and VerbNet.</title>
<date>2007</date>
<booktitle>In Proceedings of the 7th International Workshop on Computational Semantics (IWCS-7),</booktitle>
<pages>118--129</pages>
<location>Tilburg, The Netherlands.</location>
<contexts>
<context position="3053" citStr="Loper et al., 2007" startWordPosition="463" endWordPosition="466">rking with the PropBank annotation scheme, contrary to other works that use the FrameNet scheme, such as Pad´o (2007) and Basili et al. (2009). We choose this annotation for two main reasons. First, the primary use of our annotation is to serve as a gold standard in the task of syntacticsemantic parsing. FrameNet does not have a properly sampled hand-annotated corpus of English, by design. So we cannot use it for this task. Second, in Merlo and Van der Plas (2009), the semantic annotations schemes of PropBank and VerbNet (Kipper, 2005) are compared, based on annotation of the SemLink project (Loper et al., 2007). The authors conclude that PropBank is the preferred annotation for a joint syntactic-semantic setting. If the PropBank annotation scheme is crosslingually valid, annotators can reach a consensus and can do so swiftly. Thus, cross-lingual validity is measured by how well-defined the manual annotation task is (inter-annotator agreement) and by how hard it is to reach an agreement (pre- and post-consensus inter-annotator agreement). In addition, we measure the impact of the level of abstraction of the predicate labels. Conversely, how often labels do not transfer and distributions of disagreeme</context>
<context position="12512" citStr="Loper et al., 2007" startWordPosition="2005" endWordPosition="2008">nt in Predicate Labellings Predicate labels in PropBank apply to particular verb senses, for example walk.01 for the first sense of the verb walk. Even though the senses are coarser than, for example, the senses in WordNet (Fellbaum, 1998), the labels are rather specific. This specificity possibly poses problems when working in a cross-lingual setting. We compare the agreement reached using PropBank verb sense labels with the agreement reached using the verb classifications from VerbNet (Kipper, 2005) and the mapping to PropBank labels as provided in the type mappings of the SemLink project3 (Loper et al., 2007). If two annotators used two different predicate labels to annotate the do not have a fixed number of items and categories (Burchardt et al., 2006). The F-measure is a well-known measure used for the evaluation of many task such as syntactic-semantic parsing, the task that is the motivation for this paper. The choice of the F-measure makes the comparison to the performance of the future parser easier. 3(http://verbs.colorado.edu/semlink/) 115 same verb, but those verb senses belong to the same verb class, we count those as correct4. The average inter-annotator agreement is relatively low when </context>
</contexts>
<marker>Loper, Yi, Palmer, 2007</marker>
<rawString>E. Loper, S-T Yi, and M. Palmer. 2007. Combining lexical resources: Mapping between PropBank and VerbNet. In Proceedings of the 7th International Workshop on Computational Semantics (IWCS-7), pages 118–129, Tilburg, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Merlo</author>
<author>L van der Plas</author>
</authors>
<title>Abstraction and generalisation in semantic role labels: PropBank, VerbNet or both?</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>288--296</pages>
<location>Suntec, Singapore.</location>
<marker>Merlo, van der Plas, 2009</marker>
<rawString>P. Merlo and L. van der Plas. 2009. Abstraction and generalisation in semantic role labels: PropBank, VerbNet or both? In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 288–296, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Monachesi</author>
<author>G Stevens</author>
<author>J Trapman</author>
</authors>
<title>Adding semantic role annotation to a corpus of written Dutch.</title>
<date>2007</date>
<booktitle>In Proceedings of the Linguistic Annotation Workshop (LAW),</booktitle>
<pages>77--84</pages>
<location>Prague, Czech republic.</location>
<contexts>
<context position="1358" citStr="Monachesi et al. (2007)" startWordPosition="193" endWordPosition="196">e lost in translation. Yet, translation preserves enough meaning across language pairs to be useful in many applications and for many text genres. The belief that this layer of meaning which is preserved across languages can be formally represented and automatically calculated underlies methods that use parallel corpora for the automatic generation of semantic annotations through crosslingual transfer (Pad´o, 2007; Basili et al., 2009). A methodology similar in spirit — re-use of the existing resources in a different language — has also been applied in developing manually annotated resources. Monachesi et al. (2007) annotate Dutch sentences using the PropBank annotation scheme (Palmer et al., 2005), while Burchardt et al. (2009) use the FrameNet framework (Fillmore et al., 2003) to annotate a German corpus. Instead of building special lexicons containing the specific semantic information needed for the annotation for each language separately, which is a complex and time-consuming endeavour in itself, these approaches rely on the lexicons already developed for English. In this paper, we hypothesize that the level of abstraction that is necessary to develop a semantic lexicon/ontology for a single language</context>
<context position="15804" citStr="Monachesi et al. (2007)" startWordPosition="2546" endWordPosition="2549">as matching if there exists a class to which both verb senses belong. We found a verb class for both verb senses in about 78% of the cases and discarded the rest. annotate a corpus of German sentences manually. They find that the vast majority of frames can be applied to German directly. However, around one third of the verb senses identified in the German corpus were not covered by FrameNet. Also, a number of German verbs were found to be underspecified. Finally, some problems related to treating particular verb uses were identified, such as idioms, metaphors, and support verb constructions. Monachesi et al. (2007) use PropBank labels for semi-automatic annotation of a corpus of Dutch sentences. Semantic roles were first annotated using a rule-based semantic parser and then corrected by one annotator. Although not all Dutch verbs could be translated to an equivalent verb sense in English, these cases were assessed as relatively rare. What proved to be problematic was identifying the correct label for modifiers. Bittar (2009) makes use of cross-lingual lexical transfer in annotating French verbs with event types, by adapting a small-scale English verb lexicon with specified event structure (TimeML). The </context>
</contexts>
<marker>Monachesi, Stevens, Trapman, 2007</marker>
<rawString>P. Monachesi, G. Stevens, and J. Trapman. 2007. Adding semantic role annotation to a corpus of written Dutch. In Proceedings of the Linguistic Annotation Workshop (LAW), pages 77–84, Prague, Czech republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ozdowska</author>
<author>A Way</author>
</authors>
<title>Optimal bilingual data for French-English PB-SMT.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Annual Conference of the European Association for Machine Translation (EAMT’09),</booktitle>
<pages>96--103</pages>
<location>Barcelona,</location>
<marker>Ozdowska, Way, 2009</marker>
<rawString>S. Ozdowska and A. Way. 2009. Optimal bilingual data for French-English PB-SMT. In Proceedings of the 13th Annual Conference of the European Association for Machine Translation (EAMT’09), pages 96–103, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pad´o</author>
</authors>
<title>Cross-lingual Annotation Projection Models for Role-Semantic Information.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Saarland University.</institution>
<marker>Pad´o, 2007</marker>
<rawString>S. Pad´o. 2007. Cross-lingual Annotation Projection Models for Role-Semantic Information. Ph.D. thesis, Saarland University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pajas</author>
<author>J ˘St˘ep´anek</author>
</authors>
<title>Recent advances in a featurerich framework for treebank annotation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>673--680</pages>
<location>Manchester, UK.</location>
<marker>Pajas, ˘St˘ep´anek, 2008</marker>
<rawString>P. Pajas and J. ˘St˘ep´anek. 2008. Recent advances in a featurerich framework for treebank annotation. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 673–680, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<pages>31--71</pages>
<contexts>
<context position="1442" citStr="Palmer et al., 2005" startWordPosition="205" endWordPosition="208">to be useful in many applications and for many text genres. The belief that this layer of meaning which is preserved across languages can be formally represented and automatically calculated underlies methods that use parallel corpora for the automatic generation of semantic annotations through crosslingual transfer (Pad´o, 2007; Basili et al., 2009). A methodology similar in spirit — re-use of the existing resources in a different language — has also been applied in developing manually annotated resources. Monachesi et al. (2007) annotate Dutch sentences using the PropBank annotation scheme (Palmer et al., 2005), while Burchardt et al. (2009) use the FrameNet framework (Fillmore et al., 2003) to annotate a German corpus. Instead of building special lexicons containing the specific semantic information needed for the annotation for each language separately, which is a complex and time-consuming endeavour in itself, these approaches rely on the lexicons already developed for English. In this paper, we hypothesize that the level of abstraction that is necessary to develop a semantic lexicon/ontology for a single language based on observable linguistic behaviour — that is a mono-lingual, item-specific an</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>M. Palmer, D. Gildea, and P. Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31:71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>J Henderson</author>
</authors>
<title>A latent variable model for generative dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Parsing Technologies (IWPT07),</booktitle>
<pages>144--155</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="7858" citStr="Titov and Henderson, 2007" startWordPosition="1255" endWordPosition="1258">icate and fill in the roles according to their own insights. For the annotation of sentences we use an adaptation of the user-friendly, freely available Tree Editor (TrEd, Pajas and ˘St˘ep´anek, 2008). The tool shows the syntactic analysis and the plain sentence in the same window allowing the user to add semantic arcs and labels to the nodes in the syntactic dependency tree. The decision to show syntactic information is merely driven by the fact that we want to guide the annotator in selecting the heads of phrases during the annotation process. The sentences are parsed by a syntactic parser (Titov and Henderson, 2007) that we trained on syntactic dependency annotations for French (Candito et al., 2009). Although the parser is state-of-the-art (87.2% Labelled Attachment Score), in case of parse errors, we ask annotators to ignore the errors of the parser and put the label on the actual head. 2.3 Corpus We selected the French sentences for the manual annotation from the parallel Europarl corpus (Koehn, 2005). Because translation shifts are known to pose problems for the automatic crosslingual transfer of semantic roles (Pad´o, 2007) and for machine translation (Ozdowska and Way, 114 2009), and these are more</context>
</contexts>
<marker>Titov, Henderson, 2007</marker>
<rawString>I. Titov and J. Henderson. 2007. A latent variable model for generative dependency parsing. In Proceedings of the International Conference on Parsing Technologies (IWPT07), pages 144–155, Prague, Czech Republic.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>