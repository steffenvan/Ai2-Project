<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000034">
<title confidence="0.909041">
Qme! : A Speech-based Question-Answering system on Mobile Devices
</title>
<author confidence="0.912295">
Taniya Mishra
</author>
<affiliation confidence="0.720849">
AT&amp;T Labs-Research
</affiliation>
<address confidence="0.794624">
180 Park Ave
Florham Park, NJ
</address>
<email confidence="0.992329">
taniya@research.att.com
</email>
<sectionHeader confidence="0.993733" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998306">
Mobile devices are becoming the dominant
mode of information access despite being
cumbersome to input text using small key-
boards and browsing web pages on small
screens. We present Qme!, a speech-based
question-answering system that allows for
spoken queries and retrieves answers to the
questions instead of web pages. We present
bootstrap methods to distinguish dynamic
questions from static questions and we show
the benefits of tight coupling of speech recog-
nition and retrieval components of the system.
</bodyText>
<sectionHeader confidence="0.998786" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997325">
Access to information has moved from desktop and
laptop computers in office and home environments
to be an any place, any time activity due to mo-
bile devices. Although mobile devices have small
keyboards that make typing text input cumbersome
compared to conventional desktop and laptops, the
ability to access unlimited amount of information,
almost everywhere, through the Internet, using these
devices have made them pervasive.
Even so, information access using text input on
mobile devices with small screens and soft/small
keyboards is tedious and unnatural. In addition, by
the mobile nature of these devices, users often like
to use them in hands-busy environments, ruling out
the possibility of typing text. We address this issue
by allowing the user to query an information repos-
itory using speech. We expect that spoken language
queries to be a more natural and less cumbersome
way of information access using mobile devices.
A second issue we address is related to directly
and precisely answering the user’s query beyond
serving web pages. This is in contrast to the current
approach where a user types in a query using key-
words to a search engine, browses the returned re-
sults on the small screen to select a potentially rele-
vant document, suitably magnifies the screen to view
the document and searches for the answer to her
question in the document. By providing a method
</bodyText>
<page confidence="0.985466">
55
</page>
<note confidence="0.8607815">
Srinivas Bangalore
AT&amp;T Labs-Research
180 Park Ave
Florham Park, NJ
</note>
<email confidence="0.820853">
srini@research.att.com
</email>
<bodyText confidence="0.999972838709678">
for the user to pose her query in natural language and
presenting the relevant answer(s) to her question, we
expect the user’s information need to be fulfilled in
a shorter period of time.
We present a speech-driven question answering
system, Qme!, as a solution toward addressing these
two issues. The system provides a natural input
modality – spoken language input – for the users
to pose their information need and presents a col-
lection of answers that potentially address the infor-
mation need directly. For a subclass of questions
that we term static questions, the system retrieves
the answers from an archive of human generated an-
swers to questions. This ensures higher accuracy
for the answers retrieved (if found in the archive)
and also allows us to retrieve related questions on
the user’s topic of interest. For a second subclass of
questions that we term dynamic questions, the sys-
tem retrieves the answer from information databases
accessible over the Internet using web forms.
The layout of the paper is as follows. In Section 2,
we review the related literature. In Section 3, we
illustrate the system for speech-driven question an-
swering. We present the retrieval methods we used
to implement the system in Section 4. In Section 5,
we discuss and evaluate our approach to tight cou-
pling of speech recognition and search components.
In Section 6, we present bootstrap techniques to dis-
tinguish dynamic questions from static questions,
and evaluate the efficacy of these techniques on a
test corpus. We conclude in Section 7.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9996298">
Early question-answering (QA) systems, such as
Baseball (Green et al., 1961) and Lunar (Woods,
1973) were carefully hand-crafted to answer ques-
tions in a limited domain, similar to the QA
components of ELIZA (Weizenbaum, 1966) and
SHRDLU (Winograd, 1972). However, there has
been a resurgence of QA systems following the
TREC conferences with an emphasis on answering
factoid questions. This work on text-based question-
answering which is comprehensively summarized
</bodyText>
<note confidence="0.6370115">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 55–63,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999921879310345">
in (Maybury, 2004), range widely in terms of lin-
guistic sophistication. At one end of the spectrum,
There are linguistically motivated systems (Katz,
1997; Waldinger et al., 2004) that analyze the user’s
question and attempt to synthesize a coherent an-
swer by aggregating the relevant facts. At the other
end of the spectrum, there are data intensive sys-
tems (Dumais et al., 2002) that attempt to use the
redundancy of the web to arrive at an answer for
factoid style questions. There are also variants of
such QA techniques that involve an interaction and
use context to resolve ambiguity (Yang et al., 2006).
In contrast to these approaches, our method matches
the user’s query against the questions in a large cor-
pus of question-answer pairs and retrieves the asso-
ciated answer.
In the information retrieval community, QA sys-
tems attempt to retrieve precise segments of a doc-
ument instead of the entire document. In (To-
muro and Lytinen, 2004), the authors match the
user’s query against a frequently-asked-questions
(FAQ) database and select the answer whose ques-
tion matches most closely to the user’s question.
An extension of this idea is explored in (Xue et al.,
2008; Jeon et al., 2005), where the authors match the
user’s query to a community collected QA archive
such as (Yahoo!, 2009; MSN-QnA, 2009). Our ap-
proach is similar to both these lines of work in spirit,
although the user’s query for our system originates
as a spoken query, in contrast to the text queries in
previous work. We also address the issue of noisy
speech recognition and assess the value of tight in-
tegration of speech recognition and search in terms
of improving the overall performance of the system.
A novelty in this paper is our method to address dy-
namic questions as a seamless extension to answer-
ing static questions.
Also related is the literature on voice-search ap-
plications (Microsoft, 2009; Google, 2009; Yellow-
Pages, 2009; vlingo.com, 2009) that provide a spo-
ken language interface to business directories and
return phone numbers, addresses and web sites of
businesses. User input is typically not a free flowing
natural language query and is limited to expressions
with a business name and a location. In our system,
users can avail of the full range of natural language
expressions to express their information need.
And finally, our method of retrieving answers to
dynamic questions has relevance to the database and
meta search community. There is growing interest
in this community to mine the “hidden” web – infor-
mation repositories that are behind web forms – and
provide a unified meta-interface to such informa-
tion sources, for example, web sites related travel,
or car dealerships. Dynamic questions can be seen
as providing a natural language interface (NLI) to
such web forms, similar to early work on NLI to
databases (Androutsopoulos, 1995).
</bodyText>
<sectionHeader confidence="0.895048" genericHeader="method">
3 Speech-driven Question Retrieval
System
</sectionHeader>
<bodyText confidence="0.929275111111111">
We describe the speech-driven query retrieval appli-
cation in this section. The user of this application
provides a spoken language query to a mobile device
intending to find an answer to the question. Some
example users’ inputs are1 what is the fastest ani-
mal in water, how do I fix a leaky dishwasher, why
is the sky blue. The result of the speech recognizer
is used to search a large corpus of question-answer
pairs to retrieve the answers pertinent to the user’s
static questions. For the dynamic questions, the an-
swers are retrieved by querying a web form from
the appropriate web site (e.g www.fandango.com for
movie information). The result from the speech rec-
ognizer can be a single-best string or a weighted
word lattice.2 The retrieved results are ranked using
different metrics discussed in the next section. In
Figure 2, we illustrate the answers that Qme!returns
for static and dynamic quesitons.
</bodyText>
<figureCaption confidence="0.9600535">
Figure 1: The architecture of the speech-driven question-
answering system
</figureCaption>
<sectionHeader confidence="0.947689" genericHeader="method">
4 Methods of Retrieval
</sectionHeader>
<bodyText confidence="0.988963666666667">
We formulate the problem of answering static
questions as follows. Given a question-answer
archive QA = {(q1, a1), (q2, a2), ... , (qN, aN)}
</bodyText>
<footnote confidence="0.995197333333333">
1The query is not constrained to be of any specific question
type (for example, what, where, when, how).
2For this paper, the ASR used to recognize these utterances
incorporates an acoustic model adapted to speech collected
from mobile devices and a four-gram language model that is
built from the corpus of questions.
</footnote>
<figure confidence="0.994306181818182">
Q&amp;A corpus
Search
Rank
Classify
Match
Dynamic
Retrieve
from Web
Speech ASR 1−best
Lattice
Ranked Results
</figure>
<page confidence="0.767289">
56
</page>
<figureCaption confidence="0.9608915">
Figure 2: Retrieval results for static and dynamic ques-
tions using Qme!
</figureCaption>
<bodyText confidence="0.97214125">
of N question-answer pairs, and a user’s ques-
tion q,,, the task is to retrieve a subset QA&apos; =
{(qr1, ar1), (qr2, ar2), ... , (qr�, ar�)} M « N us-
ing a selection function Select and rank the mem-
bers of QA&apos; using a scoring function Score such
that Score(q,,, (qr� , ar� )) &gt; Score(q,,, (qr�+1, ar �+1)).
Here, we assume
Score(q,,, (qr� , ar�)) = Score(q,,, qr� ).
The Select function is intended to select the
matching questions that have high “semantic” simi-
larity to the user’s question. However, given there is
no objective function that measures semantic simi-
larity, we approximate it using different metrics dis-
cussed below.
Ranking of the members of the retrieved set can
be based on the scores computed during the selec-
tion step or can be independently computed based
on other criteria such as popularity of the question,
credibility of the source, temporal recency of the an-
swer, geographical proximity to the answer origin.
</bodyText>
<subsectionHeader confidence="0.999567">
4.1 Question Retrieval Metrics
</subsectionHeader>
<bodyText confidence="0.993802047619048">
We retrieve QA pairs from the data repository based
on the similarity of match between the user’s query
and each of the set of questions (d) in the repos-
itory. To measure the similarity, we have experi-
mented with the following metrics.
1. TF-IDF metric: The user input query and the
document (in our case, questions in the repos-
itory) are represented as bag-of-n-grams (aka
terms). The term weights are computed using a
combination of term frequency (tf) and inverse
document frequency (idf) (Robertson, 2004).
If Q = q1, q2, . . . , qn is a user query, then the
aggregated score for a document d using a un-
igram model of the query and the document is
given as in Equation 1. For a given query, the
documents with the highest total term weight
are presented as retrieved results. Terms can
also be defined as n-gram sequences of a query
and a document. In our experiments, we have
used up to 4-grams as terms to retrieve and rank
documents.
</bodyText>
<equation confidence="0.99424">
�Score(d) = tfw,d X idfw (1)
wEQ
</equation>
<bodyText confidence="0.831027111111111">
2. String Comparison Metrics: Since the length
of the user query and the query to be retrieved
are similar in length, we use string compar-
ison methods such as Levenshtein edit dis-
tance (Levenshtein, 1966) and n-gram overlap
(BLEU-score) (Papineni et al., 2002) as simi-
larity metrics.
We compare the search effectiveness of these sim-
ilarity metrics in Section 5.3.
</bodyText>
<sectionHeader confidence="0.70968" genericHeader="method">
5 Tightly coupling ASR and Search
</sectionHeader>
<bodyText confidence="0.999852647058824">
Most of the speech-driven search systems use the
1-best output from the ASR as the query for the
search component. Given that ASR 1-best output
is likely to be erroneous, this serialization of the
ASR and search components might result in sub-
optimal search accuracy. A lattice representation
of the ASR output, in particular, a word-confusion
network (WCN) transformation of the lattice, com-
pactly encodes the n-best hypothesis with the flexi-
bility of pruning alternatives at each word position.
An example of a WCN is shown in Figure 3. The
weights on the arcs are to be interpreted as costs and
the best path in the WCN is the lowest cost path
from the start state (0) to the final state (4). Note
that the 1-best path is how old is mama, while the
input speech was how old is obama which also is in
the WCN, but at a higher cost.
</bodyText>
<figureCaption confidence="0.9975515">
Figure 3: A sample word confusion network with arc
costs as negative logarithm of the posterior probabilities.
</figureCaption>
<figure confidence="0.981781888888889">
old/0.006 obama/7.796
how/0.001
0 1
who/6.292
_epsilon/5.010
does/12.63
late/14.14
was/14.43
2
_epsilon/8.369
is/0.000
a/12.60
obamas/13.35
3
mama/0.000
bottle/12.60
lil/7.796
4/1
</figure>
<page confidence="0.804571">
57
</page>
<figureCaption confidence="0.9980975">
Figure 4: Example of an FST representing the search in-
dex.
</figureCaption>
<subsectionHeader confidence="0.996657">
5.1 Representing Search Index as an FST
</subsectionHeader>
<bodyText confidence="0.999941444444445">
Lucene (Hatcher and Gospodnetic., 2004) is an off-
the-shelf search engine that implements the TF-IDF
metric. But, we have implemented our own search
engine using finite-state transducers (FST) for this
reason. The oracle word/phrase accuracy using n-
best hypotheses of an ASR is usually far greater than
the 1-best output. However, using each of the n-best
(n &gt; 1) hypothesis as a separate query to the search
component is computationally sub-optimal since the
strings in the n-best hypotheses usually share large
subsequences with each other. The FST representa-
tion of the search index allows us to efficiently con-
sider lattices/WCNs as input queries.
The FST search index is built as follows. We in-
dex each question-answer (QA) pair from our repos-
itory ((qi, ai), qai for short) using the words (wqi) in
question qi. This index is represented as a weighted
finite-state transducer (SearchFST) as shown in Fig-
ure 4. Here a word wqi (e.g old) is the input symbol
for a set of arcs whose output symbol is the index
of the QA pairs where old appears in the question.
The weight of the arc c(wqi,qi) is one of the simi-
larity based weights discussed in Section 4.1. As
can be seen from Figure 4, the words how, old, is
and obama contribute a score to the question-answer
pair qa25; while other pairs, qa150, qa12, qa450 are
scored by only one of these words.
</bodyText>
<subsectionHeader confidence="0.988245">
5.2 Search Process using FSTs
</subsectionHeader>
<bodyText confidence="0.999988315789474">
A user’s speech query, after speech recognition, is
represented as an FSA (either 1-best or WCN), a
QueryFSA. The QueryFSA (denoted as q) is then
transformed into another FSA (NgramFSA(q)) that
represents the set of n-grams of the QueryFSA.
Due to the arc costs from WCNs, the NgramFSA
for a WCN is a weighted FSA. The NgramFSA is
composed with the SearchFST and we obtain all
the arcs (wq, qawq, c(wq,qawq )) where wq is a query
term, qawq is a QA index with the query term and,
c(wq,qawq) is the weight associated with that pair. Us-
ing this information, we aggregate the weight for a
QA pair (qaq) across all query words and rank the
retrieved QAs in the descending order of this aggre-
gated weight. We select the top N QA pairs from
this ranked list. The query composition, QA weight
aggregation and selection of top N QA pairs are
computed with finite-state transducer operations as
shown in Equations 2 to 5.3
</bodyText>
<equation confidence="0.99955675">
D1 = 7r2(NgramFSA(q) o SearchFST)
R1 = fsmbestpath(D1, 1)
D2 = 7r2(NgramFSA(R1) o SearchFST)
TopN = fsmbestpath(fsmdeterminize(D2), N)
</equation>
<bodyText confidence="0.999796666666667">
The process of retrieving documents using the
Levenshtein-based string similarity metric can also
be encoded as a composition of FSTs.
</bodyText>
<subsectionHeader confidence="0.971641">
5.3 Experiments and Results
</subsectionHeader>
<bodyText confidence="0.999985578947368">
We have a fairly large data set consisting of over a
million question-answer pairs collected by harvest-
ing the web. In order to evaluate the retrieval meth-
ods discussed earlier, we use two test sets of QA
pairs: a Seen set of 450 QA pairs and an Unseen set
of 645 QA pairs. The queries in the Seen set have
an exact match with some question in the database,
while the queries in the Unseen set may not match
any question in the database exactly. 4 The questions
in the Unseen set, however, like those in the Seen set,
also have a human generated answer that is used in
our evaluations.
For each query, we retrieve the twenty most rel-
evant QA pairs, ranked in descending order of the
value of the particular metric under consideration.
However, depending on whether the user query is a
seen or an unseen query, the evaluation of the rele-
vance of the retrieved question-answer pairs is dif-
ferent as discussed below.5
</bodyText>
<footnote confidence="0.797118">
3We have dropped the need to convert the weights into the
real semiring for aggregation, to simplify the discussion.
4There may however be semantically matching questions.
5The reason it is not a recall and precision curve is that, for
the “seen” query set, the retrieval for the questions is a zero/one
boolean accuracy. For the “unseen” query set there is no perfect
match with the input question in the query database, and so we
determine the closeness of the questions based on the closeness
of the answers. Coherence attempts to capture the homogen-
ity of the questions retrieved, with the assumption that the user
might want to see similar questions as the returned results.
</footnote>
<figure confidence="0.989656625">
obama:qa450/c7
obama:qa25/c4
how:qa12/c6
old:qa150/c5
how:qa25/c1
old:qa25/c2
is:qa25/c3
0
</figure>
<page confidence="0.974779">
58
</page>
<subsubsectionHeader confidence="0.46204">
5.3.1 Evaluation Metrics
</subsubsectionHeader>
<bodyText confidence="0.999908666666667">
For the set of Seen queries, we evaluate the rele-
vance of the retrieved top-20 question-answer pairs
in two ways:
</bodyText>
<listItem confidence="0.9985345">
1. Retrieval Accuracy of Top-N results: We eval-
uate whether the question that matches the user
query exactly is located in the top-1, top-5,
top-10, top-20 or not in top-20 of the retrieved
questions.
2. Coherence metric: We compute the coherence
of the retrieved set as the mean of the BLEU-
score between the input query and the set of
top-5 retrieved questions. The intuition is that
we do not want the top-5 retrieved QA pairs
to distract the user by not being relevant to the
user’s query.
</listItem>
<bodyText confidence="0.999906583333333">
For the set of Unseen queries, since there are no
questions in the database that exactly match the in-
put query, we evaluate the relevance of the top-20 re-
trieved question-answer pairs in the following way.
For each of the 645 Unseen queries, we know the
human-generated answer. We manually annotated
each unseen query with the Best-Matched QA pair
whose answer was the closest semantic match to the
human-generated answer for that unseen query. We
evaluate the position of the Best-Matched QA in the
list of top twenty retrieved QA pairs for each re-
trieval method.
</bodyText>
<sectionHeader confidence="0.925853" genericHeader="evaluation">
5.3.2 Results
</sectionHeader>
<bodyText confidence="0.998496277777778">
On the Seen set of queries, as expected the re-
trieval accuracy scores for the various retrieval tech-
niques performed exceedingly well. The unigram
based tf.idf method retrieved 93% of the user’s query
in the first position, 97% in one of top-5 positions
and 100% in one of top-10 positions. All the other
retrieval methods retrieved the user’s query in the
first position for all the Seen queries (100% accu-
racy).
In Table 1, we tabulate the results of the Coher-
ence scores for the top-5 questions retrieved using
the different retrieval techniques for the Seen set of
queries. Here, the higher the n-gram the more co-
herent is the set of the results to the user’s query. It
is interesting to note that the BLEU-score and Lev-
enshtein similarity driven retrieval methods do not
differ significantly in their scores from the n-gram
tf.idf based metrics.
</bodyText>
<table confidence="0.998963625">
Method Coherence Metric
for top-5 results
TF-IDF unigram 61.58
bigram 66.23
trigram 66.23
4-gram 69.74
BLEU-score 66.29
Levenshtein 67.36
</table>
<tableCaption confidence="0.978791">
Table 1: Coherence metric results for top-5 queries re-
trieved using different retrieval techniques for the seen
set.
</tableCaption>
<bodyText confidence="0.999967833333333">
In Table 2, we present the retrieval results using
different methods on the Unseen queries. For 240 of
the 645 unseen queries, the human expert found that
that there was no answer in the data repository that
could be considered semantically equivalent to the
human-generated response to that query. So, these
240 queries cannot be answered using the current
database. For the remaining 405 unseen queries,
over 60% have their Best-Matched question-answer
pair retrieved in the top-1 position. We expect the
coverage to improve considerably by increasing the
size of the QA archive.
</bodyText>
<table confidence="0.9997535">
Method Top-1 Top-20
TFIDF Unigram 69.13 75.81
Bigram 62.46 67.41
Trigram 61.97 65.93
4-gram 56.54 58.77
WCN 70.12 78.52
Levenshtein 67.9 77.29
BLEU-score 72.0 75.31
</table>
<tableCaption confidence="0.995378">
Table 2: Retrieval results for the Unseen queries
</tableCaption>
<subsectionHeader confidence="0.796797">
5.3.3 Speech-driven query retrieval
</subsectionHeader>
<bodyText confidence="0.999783777777778">
In Equation 6, we show the tight integration of
WCNs and SearchFST using the FST composition
operation (o). A is used to scale the weights6 from
the acoustic/language models on the WCNs against
the weights on the SearchFST. As before, we use
Equation 3 to retrieve the top N QA pairs. The tight
integration is expected to improve both the ASR and
Search accuracies by co-constraining both compo-
nents.
</bodyText>
<equation confidence="0.998705">
D = 7r2(Unigrams(WCN)AoSearchFST) (6)
</equation>
<bodyText confidence="0.960482666666667">
For this experiment, we use the speech utterances
corresponding to the Unseen set as the test set. We
use a different set of 250 speech queries as the
</bodyText>
<footnote confidence="0.919823">
6fixed using the development set
</footnote>
<page confidence="0.998913">
59
</page>
<bodyText confidence="0.998908285714286">
development set. In Table 3, we show the Word
and Sentence Accuracy measures for the best path
in the WCN before and after the composition of
SearchFST with the WCN on the development and
test sets. We note that by integrating the constraints
from the search index, the ASR accuracies can be
improved by about 1% absolute.
</bodyText>
<table confidence="0.99336725">
Set # of Word Sentence
utterances Accuracy Accuracy
Dev Set 250 77.1(78.2) 54(54)
Test Set 645 70.8(72.1) 36.7(37.1)
</table>
<tableCaption confidence="0.9756675">
Table 3: ASR accuracies of the best path before and after
(in parenthesis) the composition of SearchFST
</tableCaption>
<bodyText confidence="0.999972857142857">
Since we have the speech utterances of the Un-
seen set, we were also able to compute the search
results obtained by integrating the ASR WCNs with
the SearchFST, as shown in line 5 of Table 2. These
results show that the the integration of the ASR
WCNs with the SearchFST produces higher search
accuracy compared to ASR 1-best.
</bodyText>
<sectionHeader confidence="0.988757" genericHeader="evaluation">
6 Dynamic and Static Questions
</sectionHeader>
<bodyText confidence="0.999952784313726">
Storing previously answered questions and their an-
swers allows Qme!to retrieve the answers to a sub-
class of questions quickly and accurately. We term
this subclass as static questions since the answers
to these questions remain the same irrespective of
when and where the questions are asked. Examples
of such questions are What is the speed of light?,
When is George Washington’s birthday?. In con-
trast, there is a subclass of questions, which we term
dynamic questions, for which the answers depend
on when and where they are asked. For such ques-
tions the above method results in less than satisfac-
tory and sometimes inaccurate answers. Examples
of such questions are What is the stock price of Gen-
eral Motors?, Who won the game last night?, What
is playing at the theaters near me?.
We define dynamic questions as questions whose
answers change more frequently than once a year.
In dynamic questions, there may be no explicit ref-
erence to time, unlike the questions in the TERQAS
corpus (Radev and Sundheim., 2002) which explic-
itly refer to the temporal properties of the entities
being questioned or the relative ordering of past and
future events. The time-dependency of a dynamic
question lies in the temporal nature of its answer.
For example, consider the dynamic question, “What
is the address of the theater ‘White Christmas’ is
playing at in New York?”. White Christmas is a sea-
sonal play that plays in New York every year for a
few weeks in December and January, but it does not
necessarily at the same theater every year. So, de-
pending when this question is asked, the answer will
be different.
Interest in temporal analysis for question-
answering has been growing since the late 1990’s.
Early work on temporal expressions identifica-
tion using a tagger led to the development of
TimeML (Pustejovsky et al., 2001), a markup
language for annotating temporal expressions and
events in text. Other examples include QA-by-
Dossier with Constraints (Prager et al., 2004), a
method of improving QA accuracy by asking auxil-
iary questions related to the original question in or-
der to temporally verify and restrict the original an-
swer. (Moldovan et al., 2005) detect and represent
temporally related events in natural language using
logical form representation. (Saquete et al., 2009)
use the temporal relations in a question to decom-
pose it into simpler questions, the answers of which
are recomposed to produce the answers to the origi-
nal question.
</bodyText>
<subsectionHeader confidence="0.982518">
6.1 Dynamic/Static Classification
</subsectionHeader>
<bodyText confidence="0.999979615384615">
We automatically classify questions as dynamic and
static questions. Answers to static questions can be
retrieved from the QA archive. To answer dynamic
questions, we query the database(s) associated with
the topic of the question through web forms on the
Internet. We use a topic classifier to detect the topic
of a question followed by a dynamic/static classifier
trained on questions related to a topic, as shown in
figure 5. Given the question what movies are play-
ing around me?, we detect it is a movie related dy-
namic question and query a movie information web
site (e.g. www.fandango.com) to retrieve the results
based on the user’s GPS information.
</bodyText>
<figureCaption confidence="0.992429">
Figure 5: Chaining two classifiers
</figureCaption>
<bodyText confidence="0.829236">
We used supervised learning to train the topic
</bodyText>
<page confidence="0.996632">
60
</page>
<bodyText confidence="0.999919263157895">
classifier, since our entire dataset is annotated by hu-
man experts with topic labels. In contrast, to train a
dynamic/static classifier, we experimented with the
following three different techniques.
Baseline: We treat questions as dynamic if they
contain temporal indexicals, e.g. today, now, this
week, two summers ago, currently, recently, which
were based on the TimeML corpus. We also in-
cluded spatial indexicals such as here, and other sub-
strings such as cost of and how much is. A question
is considered static if it does not contain any such
words/phrases.
Self-training with bagging: The general self-
training with bagging algorithm (Banko and Brill,
2001) is presented in Table 6 and illustrated in Fig-
ure 7(a). The benefit of self-training is that we can
build a better classifier than that built from the small
seed corpus by simply adding in the large unlabeled
corpus without requiring hand-labeling.
</bodyText>
<listItem confidence="0.985135625">
1. Create k bags of data, each of size ILI, by sampling
with replacement from labeled set L.
2. Train k classifiers; one classifier on each of k bags.
3. Each classifier predicts labels of the unlabeled set.
4. The N labeled instances that j of k classifiers agree
on with the highest average confidence is added to the
labeled set L, to produce a new labeled set L&apos;.
5. Repeat all 5 steps until stopping criteria is reached.
</listItem>
<figureCaption confidence="0.970248">
Figure 6: Self-training with bagging
Figure 7: (a) Self-training with bagging (b) Committee-
based active-learning
</figureCaption>
<bodyText confidence="0.999867105263158">
In order to prevent a bias towards the majority
class, in step 4, we ensure that the distribution of
the static and dynamic questions remains the same
as in the annotated seed corpus. The benefit of bag-
ging (Breiman, 1996) is to present different views of
the same training set, and thus have a way to assess
the certainty with which a potential training instance
can be labeled.
Active-learning: This is another popular method for
training classifiers when not much annotated data is
available. The key idea in active learning is to anno-
tate only those instances of the dataset that are most
difficult for the classifier to learn to classify. It is
expected that training classifiers using this method
shows better performance than if samples were cho-
sen randomly for the same human annotation effort.
Figure 7(b) illustrates the algorithm and Figure 8
describes the algorithm, also known as committee-
based active-learning (Banko and Brill, 2001).
</bodyText>
<listItem confidence="0.972961">
1. Create k bags of data, each of size ILI, by sampling
with replacement from the labeled set L.
2. Train k classifiers, one on each bag of the k bags.
3. Each classifier predicts the labels of the unlabeled set.
4. Choose N instances from the unlabeled set for human
labeling. N/2 of the instances are those whose labels the
</listItem>
<bodyText confidence="0.915802666666667">
committee of classifiers have highest vote entropy (un-
certainity). The other N/2 of the instances are selected
randomly from the unlabeled set.
</bodyText>
<listItem confidence="0.303765">
5. Repeat all 5 steps until stopping criteria is reached.
</listItem>
<figureCaption confidence="0.999221">
Figure 8: Active Learning algorithm
</figureCaption>
<bodyText confidence="0.999805333333333">
We used the maximum entropy classifier in
Llama (Haffner, 2006) for all of the above classi-
fication tasks.
</bodyText>
<subsectionHeader confidence="0.8874685">
6.2 Experiments and Results
6.2.1 Topic Classification
</subsectionHeader>
<bodyText confidence="0.999986230769231">
The topic classifier was trained using a training
set consisted of over one million questions down-
loaded from the web which were manually labeled
by human experts as part of answering the questions.
The test set consisted of 15,000 randomly selected
questions. Word trigrams of the question are used
as features for a MaxEnt classifier which outputs a
score distribution on all of the 104 possible topic
labels. The error rate results for models selecting
the top topic and the top two topics according to the
score distribution are shown in Table 4. As can be
seen these error rates are far lower than the baseline
model of selecting the most frequent topic.
</bodyText>
<table confidence="0.9882235">
Model Error Rate
Baseline 98.79%
Top topic 23.9%
Top-two topics 12.23%
</table>
<tableCaption confidence="0.99989">
Table 4: Results of topic classification
</tableCaption>
<figure confidence="0.988956">
(a) (b)
</figure>
<page confidence="0.80365">
61
</page>
<figureCaption confidence="0.999296">
Figure 9: Change in classification results
</figureCaption>
<subsubsectionHeader confidence="0.583177">
6.2.2 Dynamic/static Classification
</subsubsectionHeader>
<bodyText confidence="0.999863071428572">
As mentioned before, we experimented with
three different approaches to bootstrapping a dy-
namic/static question classifier. We evaluate these
methods on a 250 question test set drawn from the
broad topic of Movies. For the baseline model, we
used the words/phrases discussed earlier based on
temporal and spatial indexicals. For the “super-
vised” model, we use the baseline model to tag 500K
examples and use the machine-annotated corpus to
train a MaxEnt binary classifier with word trigrams
as features. The error rate in Table 5 shows that it
performs better than the baseline model mostly due
to better lexical coverage contributed by the 500K
examples.
</bodyText>
<table confidence="0.9992618">
Training approach Lowest Error rate
Baseline 27.70%
“Supervised” learning 22.09%
Self-training 8.84%
Active-learning 4.02%
</table>
<tableCaption confidence="0.997998">
Table 5: Best Results of dynamic/static classification
</tableCaption>
<bodyText confidence="0.999977555555556">
In the self-training approach, we start with a small
seed corpus of 250 hand-labeled examples from the
Movies topic annotated with dynamic or static tags.
We used the same set of 500K unlabeled examples
as before and word trigrams from the question were
used as the features for a MaxEnt classifier. We used
11 bags in the bagging phase of this approach and
required that all 11 classifiers agree unanimously
about the label of a new instance. Of all such in-
stances, we randomly selected N instances to be
added to the training set of the next iteration, while
maintaining the distribution of the static and dy-
namic questions to be the same as that in the seed
corpus. We experimented with various values of N,
the number of newly labeled instances added at each
iteration. The error rate at initialization is 10.4%
compared to 22.1% of the “supervised” approach
which can be directly attributed to the 250 hand-
labeled questions. The lowest error rate of the self-
training approach, obtained at N=100, is 8.84%, as
shown in Table 5. In Figure 9, we show the change
in error rate for N=40 (line S1 in the graph) and
N=100 (line S2 in the graph).
For the active learning approach, we used the
same set of 250 questions as the seed corpus, the
same set of 500K unlabeled examples, the same test
set, and the same set of word trigrams features as in
the self-training approach. We used 11 bags for the
bagging phase and selected top 20 new unlabeled in-
stances on which the 11 classifiers had the greatest
vote entropy to be presented to the human labeler for
annotation. We also randomly selected 20 instances
from the rest of the unlabeled set to be presented for
annotation. The best error rate of this classifier on
the test set is 4.02%, as shown in Table 5. The error
rate over successive iterations is shown by line A1
in Figure 9.
In order to illustrate the benefits of selecting the
examples actively, we repeated the experiment de-
scribed above but with all 40 unlabeled instances se-
lected randomly for annotation. The error rate over
successive iterations is shown by line R1 in Fig-
ure 9. Comparing A1 to R1, we see that the error de-
creases faster when we select some of the unlabeled
instances for annotation actively at each iteration.
</bodyText>
<sectionHeader confidence="0.998485" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999918833333333">
In this paper, we have presented a system Qme!,
a speech-driven question-answering system for mo-
bile devices. We have proposed a query retrieval
model for question-answering and demonstrated the
mutual benefits of tightly coupling the ASR and
Search components of the system. We have pre-
sented a novel concept of distinguishing questions
that need dynamic information to be answered from
those questions whose answers can be retrieved from
an archive. We have shown results on bootstrap-
ping such a classifier using semi-supervised learning
techniques.
</bodyText>
<page confidence="0.998979">
62
</page>
<sectionHeader confidence="0.995899" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99978203960396">
L. Androutsopoulos. 1995. Natural language interfaces
to databases - an introduction. Journal of Natural Lan-
guage Engineering, 1:29–81.
M. Banko and E. Brill. 2001. Scaling to very very large
corpora for natural language disambiguation. In Pro-
ceedings of the 39th annual meeting of the association
for computational linguistics: ACL 2001, pages 26–
33.
L. Breiman. 1996. Bagging predictors. Machine Learn-
ing, 24(2):123–140.
S. Dumais, M. Banko, E. Brill, J. Lin, and A. Ng. 2002.
Web question answering: is more always better? In
SIGIR ’02: Proceedings of the 25th annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 291–298, New
York, NY, USA. ACM.
Google, 2009. http://www.google.com/mobile.
B.F. Green, A.K. Wolf, C. Chomsky, and K. Laughery.
1961. Baseball, an automatic question answerer. In
Proceedings of the Western Joint Computer Confer-
ence, pages 219–224.
P. Haffner. 2006. Scaling large margin classifiers for spo-
ken language understanding. Speech Communication,
48(iv):239–261.
E. Hatcher and O. Gospodnetic. 2004. Lucene in Action
(In Action series). Manning Publications Co., Green-
wich, CT, USA.
J. Jeon, W. B. Croft, and J. H. Lee. 2005. Finding sim-
ilar questions in large question and answer archives.
In CIKM ’05: Proceedings of the 14th ACM interna-
tional conference on Information and knowledge man-
agement, pages 84–90, New York, NY, USA. ACM.
B. Katz. 1997. Annotating the world wide web using
natural language. In Proceedings of RIAO.
V.I. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertion and reversals. Soviet Physics
Doklady, 10:707–710.
M. T. Maybury, editor. 2004. New Directions in Question
Answering. AAAI Press.
Microsoft, 2009. http://www.live.com.
D. Moldovan, C. Clark, and S. Harabagiu. 2005. Tem-
poral context representation and reasoning. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence, pages 1009–1104.
MSN-QnA, 2009. http://qna.live.com/.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: A method for automatic evaluation of machine
translation. In Proceedings of 401h Annual Meeting
of the Association of Computational Linguistics, pages
313–318, Philadelphia, PA, July.
J. Prager, J. Chu-Carroll, and K. Czuba. 2004. Ques-
tion answering using constraint satisfaction: Qa-by-
dossier-with-contraints. In Proceedings of the 42nd
annual meeting of the association for computational
linguistics: ACL 2004, pages 574–581.
J. Pustejovsky, R. Ingria, R. Sauri, J. Casta no, J. Littman,
and R. Gaizauskas., 2001. The language of time: A
reader, chapter The specification languae – TimeML.
Oxford University Press.
D. Radev and B. Sundheim. 2002. Using timeml in ques-
tion answering. Technical report, Brandies University.
S. Robertson. 2004. Understanding inverse document
frequency: On theoretical arguments for idf. Journal
of Documentation, 60.
E. Saquete, J. L. Vicedo, P. Martinez-Barco, R. Mu
noz, and H. Llorens. 2009. Enhancing qa sys-
tems with complex temporal question processing ca-
pabilities. Journal of Artificial Intelligence Research,
35:775–811.
N. Tomuro and S. L. Lytinen. 2004. Retrieval models
and Q and A learning with FAQ files. In New Direc-
tions in Question Answering, pages 183–202.
vlingo.com, 2009. http://www.vlingomobile.com/downloads.html.
R. J. Waldinger, D. E. Appelt, J. L. Dungan, J. Fry, J. R.
Hobbs, D. J. Israel, P. Jarvis, D. L. Martin, S. Riehe-
mann, M. E. Stickel, and M. Tyson. 2004. Deductive
question answering from multiple resources. In New
Directions in Question Answering, pages 253–262.
J. Weizenbaum. 1966. ELIZA - a computer program
for the study of natural language communication be-
tween man and machine. Communications of the
ACM, 1:36–45.
T. Winograd. 1972. Understanding Natural Language.
Academic Press.
W. A. Woods. 1973. Progress in natural language un-
derstanding - an application to lunar geology. In Pro-
ceedings of American Federation of Information Pro-
cessing Societies (AFIPS) Conference.
X. Xue, J. Jeon, and W. B. Croft. 2008. Retrieval models
for question and answer archives. In SIGIR ’08: Pro-
ceedings of the 31st annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 475–482, New York, NY, USA.
ACM.
Yahoo!, 2009. http://answers.yahoo.com/.
F. Yang, J. Feng, and G. DiFabbrizio. 2006. A data
driven approach to relevancy recognition for contex-
tual question answering. In HLT-NAACL 2006 Work-
shop on Interactive Question Answering, New York,
USA, June 8-9.
YellowPages, 2009. http://www.speak4it.com.
</reference>
<page confidence="0.999461">
63
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.444994">
<title confidence="0.999588">Qme! : A Speech-based Question-Answering system on Mobile Devices</title>
<author confidence="0.949608">Taniya</author>
<affiliation confidence="0.991781">AT&amp;T</affiliation>
<address confidence="0.742951">180 Park Florham Park,</address>
<email confidence="0.999866">taniya@research.att.com</email>
<abstract confidence="0.995825923076923">Mobile devices are becoming the dominant mode of information access despite being cumbersome to input text using small keyboards and browsing web pages on small We present a speech-based question-answering system that allows for queries and retrieves the questions instead of web pages. We present bootstrap methods to distinguish dynamic questions from static questions and we show the benefits of tight coupling of speech recognition and retrieval components of the system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Androutsopoulos</author>
</authors>
<title>Natural language interfaces to databases - an introduction.</title>
<date>1995</date>
<journal>Journal of Natural Language Engineering,</journal>
<pages>1--29</pages>
<contexts>
<context position="7235" citStr="Androutsopoulos, 1995" startWordPosition="1166" endWordPosition="1167">l of the full range of natural language expressions to express their information need. And finally, our method of retrieving answers to dynamic questions has relevance to the database and meta search community. There is growing interest in this community to mine the “hidden” web – information repositories that are behind web forms – and provide a unified meta-interface to such information sources, for example, web sites related travel, or car dealerships. Dynamic questions can be seen as providing a natural language interface (NLI) to such web forms, similar to early work on NLI to databases (Androutsopoulos, 1995). 3 Speech-driven Question Retrieval System We describe the speech-driven query retrieval application in this section. The user of this application provides a spoken language query to a mobile device intending to find an answer to the question. Some example users’ inputs are1 what is the fastest animal in water, how do I fix a leaky dishwasher, why is the sky blue. The result of the speech recognizer is used to search a large corpus of question-answer pairs to retrieve the answers pertinent to the user’s static questions. For the dynamic questions, the answers are retrieved by querying a web f</context>
</contexts>
<marker>Androutsopoulos, 1995</marker>
<rawString>L. Androutsopoulos. 1995. Natural language interfaces to databases - an introduction. Journal of Natural Language Engineering, 1:29–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>E Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th annual</booktitle>
<pages>26--33</pages>
<contexts>
<context position="25337" citStr="Banko and Brill, 2001" startWordPosition="4220" endWordPosition="4223">notated by human experts with topic labels. In contrast, to train a dynamic/static classifier, we experimented with the following three different techniques. Baseline: We treat questions as dynamic if they contain temporal indexicals, e.g. today, now, this week, two summers ago, currently, recently, which were based on the TimeML corpus. We also included spatial indexicals such as here, and other substrings such as cost of and how much is. A question is considered static if it does not contain any such words/phrases. Self-training with bagging: The general selftraining with bagging algorithm (Banko and Brill, 2001) is presented in Table 6 and illustrated in Figure 7(a). The benefit of self-training is that we can build a better classifier than that built from the small seed corpus by simply adding in the large unlabeled corpus without requiring hand-labeling. 1. Create k bags of data, each of size ILI, by sampling with replacement from labeled set L. 2. Train k classifiers; one classifier on each of k bags. 3. Each classifier predicts labels of the unlabeled set. 4. The N labeled instances that j of k classifiers agree on with the highest average confidence is added to the labeled set L, to produce a ne</context>
<context position="27073" citStr="Banko and Brill, 2001" startWordPosition="4513" endWordPosition="4516">th which a potential training instance can be labeled. Active-learning: This is another popular method for training classifiers when not much annotated data is available. The key idea in active learning is to annotate only those instances of the dataset that are most difficult for the classifier to learn to classify. It is expected that training classifiers using this method shows better performance than if samples were chosen randomly for the same human annotation effort. Figure 7(b) illustrates the algorithm and Figure 8 describes the algorithm, also known as committeebased active-learning (Banko and Brill, 2001). 1. Create k bags of data, each of size ILI, by sampling with replacement from the labeled set L. 2. Train k classifiers, one on each bag of the k bags. 3. Each classifier predicts the labels of the unlabeled set. 4. Choose N instances from the unlabeled set for human labeling. N/2 of the instances are those whose labels the committee of classifiers have highest vote entropy (uncertainity). The other N/2 of the instances are selected randomly from the unlabeled set. 5. Repeat all 5 steps until stopping criteria is reached. Figure 8: Active Learning algorithm We used the maximum entropy classi</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>M. Banko and E. Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Proceedings of the 39th annual meeting of the association for computational linguistics: ACL 2001, pages 26– 33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="26348" citStr="Breiman, 1996" startWordPosition="4398" endWordPosition="4399">gs. 3. Each classifier predicts labels of the unlabeled set. 4. The N labeled instances that j of k classifiers agree on with the highest average confidence is added to the labeled set L, to produce a new labeled set L&apos;. 5. Repeat all 5 steps until stopping criteria is reached. Figure 6: Self-training with bagging Figure 7: (a) Self-training with bagging (b) Committeebased active-learning In order to prevent a bias towards the majority class, in step 4, we ensure that the distribution of the static and dynamic questions remains the same as in the annotated seed corpus. The benefit of bagging (Breiman, 1996) is to present different views of the same training set, and thus have a way to assess the certainty with which a potential training instance can be labeled. Active-learning: This is another popular method for training classifiers when not much annotated data is available. The key idea in active learning is to annotate only those instances of the dataset that are most difficult for the classifier to learn to classify. It is expected that training classifiers using this method shows better performance than if samples were chosen randomly for the same human annotation effort. Figure 7(b) illustr</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>L. Breiman. 1996. Bagging predictors. Machine Learning, 24(2):123–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dumais</author>
<author>M Banko</author>
<author>E Brill</author>
<author>J Lin</author>
<author>A Ng</author>
</authors>
<title>Web question answering: is more always better?</title>
<date>2002</date>
<booktitle>In SIGIR ’02: Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>291--298</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4764" citStr="Dumais et al., 2002" startWordPosition="754" endWordPosition="757">swering which is comprehensively summarized Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 55–63, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics in (Maybury, 2004), range widely in terms of linguistic sophistication. At one end of the spectrum, There are linguistically motivated systems (Katz, 1997; Waldinger et al., 2004) that analyze the user’s question and attempt to synthesize a coherent answer by aggregating the relevant facts. At the other end of the spectrum, there are data intensive systems (Dumais et al., 2002) that attempt to use the redundancy of the web to arrive at an answer for factoid style questions. There are also variants of such QA techniques that involve an interaction and use context to resolve ambiguity (Yang et al., 2006). In contrast to these approaches, our method matches the user’s query against the questions in a large corpus of question-answer pairs and retrieves the associated answer. In the information retrieval community, QA systems attempt to retrieve precise segments of a document instead of the entire document. In (Tomuro and Lytinen, 2004), the authors match the user’s quer</context>
</contexts>
<marker>Dumais, Banko, Brill, Lin, Ng, 2002</marker>
<rawString>S. Dumais, M. Banko, E. Brill, J. Lin, and A. Ng. 2002. Web question answering: is more always better? In SIGIR ’02: Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 291–298, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Google</author>
</authors>
<date>2009</date>
<note>http://www.google.com/mobile.</note>
<contexts>
<context position="6282" citStr="Google, 2009" startWordPosition="1013" endWordPosition="1014">o!, 2009; MSN-QnA, 2009). Our approach is similar to both these lines of work in spirit, although the user’s query for our system originates as a spoken query, in contrast to the text queries in previous work. We also address the issue of noisy speech recognition and assess the value of tight integration of speech recognition and search in terms of improving the overall performance of the system. A novelty in this paper is our method to address dynamic questions as a seamless extension to answering static questions. Also related is the literature on voice-search applications (Microsoft, 2009; Google, 2009; YellowPages, 2009; vlingo.com, 2009) that provide a spoken language interface to business directories and return phone numbers, addresses and web sites of businesses. User input is typically not a free flowing natural language query and is limited to expressions with a business name and a location. In our system, users can avail of the full range of natural language expressions to express their information need. And finally, our method of retrieving answers to dynamic questions has relevance to the database and meta search community. There is growing interest in this community to mine the “h</context>
</contexts>
<marker>Google, 2009</marker>
<rawString>Google, 2009. http://www.google.com/mobile.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B F Green</author>
<author>A K Wolf</author>
<author>C Chomsky</author>
<author>K Laughery</author>
</authors>
<title>Baseball, an automatic question answerer.</title>
<date>1961</date>
<booktitle>In Proceedings of the Western Joint Computer Conference,</booktitle>
<pages>219--224</pages>
<contexts>
<context position="3799" citStr="Green et al., 1961" startWordPosition="606" endWordPosition="609">er is as follows. In Section 2, we review the related literature. In Section 3, we illustrate the system for speech-driven question answering. We present the retrieval methods we used to implement the system in Section 4. In Section 5, we discuss and evaluate our approach to tight coupling of speech recognition and search components. In Section 6, we present bootstrap techniques to distinguish dynamic questions from static questions, and evaluate the efficacy of these techniques on a test corpus. We conclude in Section 7. 2 Related Work Early question-answering (QA) systems, such as Baseball (Green et al., 1961) and Lunar (Woods, 1973) were carefully hand-crafted to answer questions in a limited domain, similar to the QA components of ELIZA (Weizenbaum, 1966) and SHRDLU (Winograd, 1972). However, there has been a resurgence of QA systems following the TREC conferences with an emphasis on answering factoid questions. This work on text-based questionanswering which is comprehensively summarized Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 55–63, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics in (Maybury, 20</context>
</contexts>
<marker>Green, Wolf, Chomsky, Laughery, 1961</marker>
<rawString>B.F. Green, A.K. Wolf, C. Chomsky, and K. Laughery. 1961. Baseball, an automatic question answerer. In Proceedings of the Western Joint Computer Conference, pages 219–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Haffner</author>
</authors>
<title>Scaling large margin classifiers for spoken language understanding. Speech Communication,</title>
<date>2006</date>
<contexts>
<context position="27702" citStr="Haffner, 2006" startWordPosition="4623" endWordPosition="4624">ags of data, each of size ILI, by sampling with replacement from the labeled set L. 2. Train k classifiers, one on each bag of the k bags. 3. Each classifier predicts the labels of the unlabeled set. 4. Choose N instances from the unlabeled set for human labeling. N/2 of the instances are those whose labels the committee of classifiers have highest vote entropy (uncertainity). The other N/2 of the instances are selected randomly from the unlabeled set. 5. Repeat all 5 steps until stopping criteria is reached. Figure 8: Active Learning algorithm We used the maximum entropy classifier in Llama (Haffner, 2006) for all of the above classification tasks. 6.2 Experiments and Results 6.2.1 Topic Classification The topic classifier was trained using a training set consisted of over one million questions downloaded from the web which were manually labeled by human experts as part of answering the questions. The test set consisted of 15,000 randomly selected questions. Word trigrams of the question are used as features for a MaxEnt classifier which outputs a score distribution on all of the 104 possible topic labels. The error rate results for models selecting the top topic and the top two topics accordin</context>
</contexts>
<marker>Haffner, 2006</marker>
<rawString>P. Haffner. 2006. Scaling large margin classifiers for spoken language understanding. Speech Communication, 48(iv):239–261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hatcher</author>
<author>O Gospodnetic</author>
</authors>
<title>Lucene in Action (In Action series).</title>
<date>2004</date>
<publisher>Manning Publications Co.,</publisher>
<location>Greenwich, CT, USA.</location>
<marker>Hatcher, Gospodnetic, 2004</marker>
<rawString>E. Hatcher and O. Gospodnetic. 2004. Lucene in Action (In Action series). Manning Publications Co., Greenwich, CT, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jeon</author>
<author>W B Croft</author>
<author>J H Lee</author>
</authors>
<title>Finding similar questions in large question and answer archives.</title>
<date>2005</date>
<booktitle>In CIKM ’05: Proceedings of the 14th ACM international conference on Information and knowledge management,</booktitle>
<pages>84--90</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5578" citStr="Jeon et al., 2005" startWordPosition="891" endWordPosition="894">lve ambiguity (Yang et al., 2006). In contrast to these approaches, our method matches the user’s query against the questions in a large corpus of question-answer pairs and retrieves the associated answer. In the information retrieval community, QA systems attempt to retrieve precise segments of a document instead of the entire document. In (Tomuro and Lytinen, 2004), the authors match the user’s query against a frequently-asked-questions (FAQ) database and select the answer whose question matches most closely to the user’s question. An extension of this idea is explored in (Xue et al., 2008; Jeon et al., 2005), where the authors match the user’s query to a community collected QA archive such as (Yahoo!, 2009; MSN-QnA, 2009). Our approach is similar to both these lines of work in spirit, although the user’s query for our system originates as a spoken query, in contrast to the text queries in previous work. We also address the issue of noisy speech recognition and assess the value of tight integration of speech recognition and search in terms of improving the overall performance of the system. A novelty in this paper is our method to address dynamic questions as a seamless extension to answering stat</context>
</contexts>
<marker>Jeon, Croft, Lee, 2005</marker>
<rawString>J. Jeon, W. B. Croft, and J. H. Lee. 2005. Finding similar questions in large question and answer archives. In CIKM ’05: Proceedings of the 14th ACM international conference on Information and knowledge management, pages 84–90, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Katz</author>
</authors>
<title>Annotating the world wide web using natural language.</title>
<date>1997</date>
<booktitle>In Proceedings of RIAO.</booktitle>
<contexts>
<context position="4538" citStr="Katz, 1997" startWordPosition="717" endWordPosition="718">ZA (Weizenbaum, 1966) and SHRDLU (Winograd, 1972). However, there has been a resurgence of QA systems following the TREC conferences with an emphasis on answering factoid questions. This work on text-based questionanswering which is comprehensively summarized Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 55–63, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics in (Maybury, 2004), range widely in terms of linguistic sophistication. At one end of the spectrum, There are linguistically motivated systems (Katz, 1997; Waldinger et al., 2004) that analyze the user’s question and attempt to synthesize a coherent answer by aggregating the relevant facts. At the other end of the spectrum, there are data intensive systems (Dumais et al., 2002) that attempt to use the redundancy of the web to arrive at an answer for factoid style questions. There are also variants of such QA techniques that involve an interaction and use context to resolve ambiguity (Yang et al., 2006). In contrast to these approaches, our method matches the user’s query against the questions in a large corpus of question-answer pairs and retri</context>
</contexts>
<marker>Katz, 1997</marker>
<rawString>B. Katz. 1997. Annotating the world wide web using natural language. In Proceedings of RIAO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertion and reversals. Soviet Physics Doklady,</title>
<date>1966</date>
<pages>10--707</pages>
<contexts>
<context position="11064" citStr="Levenshtein, 1966" startWordPosition="1822" endWordPosition="1823">he aggregated score for a document d using a unigram model of the query and the document is given as in Equation 1. For a given query, the documents with the highest total term weight are presented as retrieved results. Terms can also be defined as n-gram sequences of a query and a document. In our experiments, we have used up to 4-grams as terms to retrieve and rank documents. �Score(d) = tfw,d X idfw (1) wEQ 2. String Comparison Metrics: Since the length of the user query and the query to be retrieved are similar in length, we use string comparison methods such as Levenshtein edit distance (Levenshtein, 1966) and n-gram overlap (BLEU-score) (Papineni et al., 2002) as similarity metrics. We compare the search effectiveness of these similarity metrics in Section 5.3. 5 Tightly coupling ASR and Search Most of the speech-driven search systems use the 1-best output from the ASR as the query for the search component. Given that ASR 1-best output is likely to be erroneous, this serialization of the ASR and search components might result in suboptimal search accuracy. A lattice representation of the ASR output, in particular, a word-confusion network (WCN) transformation of the lattice, compactly encodes </context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>V.I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertion and reversals. Soviet Physics Doklady, 10:707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T Maybury</author>
<author>editor</author>
</authors>
<date>2004</date>
<booktitle>New Directions in Question Answering.</booktitle>
<publisher>AAAI Press. Microsoft,</publisher>
<note>http://www.live.com.</note>
<marker>Maybury, editor, 2004</marker>
<rawString>M. T. Maybury, editor. 2004. New Directions in Question Answering. AAAI Press. Microsoft, 2009. http://www.live.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>C Clark</author>
<author>S Harabagiu</author>
</authors>
<title>Temporal context representation and reasoning.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1009--1104</pages>
<note>http://qna.live.com/.</note>
<contexts>
<context position="23605" citStr="Moldovan et al., 2005" startWordPosition="3943" endWordPosition="3946"> depending when this question is asked, the answer will be different. Interest in temporal analysis for questionanswering has been growing since the late 1990’s. Early work on temporal expressions identification using a tagger led to the development of TimeML (Pustejovsky et al., 2001), a markup language for annotating temporal expressions and events in text. Other examples include QA-byDossier with Constraints (Prager et al., 2004), a method of improving QA accuracy by asking auxiliary questions related to the original question in order to temporally verify and restrict the original answer. (Moldovan et al., 2005) detect and represent temporally related events in natural language using logical form representation. (Saquete et al., 2009) use the temporal relations in a question to decompose it into simpler questions, the answers of which are recomposed to produce the answers to the original question. 6.1 Dynamic/Static Classification We automatically classify questions as dynamic and static questions. Answers to static questions can be retrieved from the QA archive. To answer dynamic questions, we query the database(s) associated with the topic of the question through web forms on the Internet. We use a</context>
</contexts>
<marker>Moldovan, Clark, Harabagiu, 2005</marker>
<rawString>D. Moldovan, C. Clark, and S. Harabagiu. 2005. Temporal context representation and reasoning. In Proceedings of the 19th International Joint Conference on Artificial Intelligence, pages 1009–1104. MSN-QnA, 2009. http://qna.live.com/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 401h Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>313--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="11120" citStr="Papineni et al., 2002" startWordPosition="1828" endWordPosition="1831"> model of the query and the document is given as in Equation 1. For a given query, the documents with the highest total term weight are presented as retrieved results. Terms can also be defined as n-gram sequences of a query and a document. In our experiments, we have used up to 4-grams as terms to retrieve and rank documents. �Score(d) = tfw,d X idfw (1) wEQ 2. String Comparison Metrics: Since the length of the user query and the query to be retrieved are similar in length, we use string comparison methods such as Levenshtein edit distance (Levenshtein, 1966) and n-gram overlap (BLEU-score) (Papineni et al., 2002) as similarity metrics. We compare the search effectiveness of these similarity metrics in Section 5.3. 5 Tightly coupling ASR and Search Most of the speech-driven search systems use the 1-best output from the ASR as the query for the search component. Given that ASR 1-best output is likely to be erroneous, this serialization of the ASR and search components might result in suboptimal search accuracy. A lattice representation of the ASR output, in particular, a word-confusion network (WCN) transformation of the lattice, compactly encodes the n-best hypothesis with the flexibility of pruning al</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of 401h Annual Meeting of the Association of Computational Linguistics, pages 313–318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
<author>J Chu-Carroll</author>
<author>K Czuba</author>
</authors>
<title>Question answering using constraint satisfaction: Qa-bydossier-with-contraints.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd annual</booktitle>
<pages>574--581</pages>
<contexts>
<context position="23419" citStr="Prager et al., 2004" startWordPosition="3911" endWordPosition="3914">ew York?”. White Christmas is a seasonal play that plays in New York every year for a few weeks in December and January, but it does not necessarily at the same theater every year. So, depending when this question is asked, the answer will be different. Interest in temporal analysis for questionanswering has been growing since the late 1990’s. Early work on temporal expressions identification using a tagger led to the development of TimeML (Pustejovsky et al., 2001), a markup language for annotating temporal expressions and events in text. Other examples include QA-byDossier with Constraints (Prager et al., 2004), a method of improving QA accuracy by asking auxiliary questions related to the original question in order to temporally verify and restrict the original answer. (Moldovan et al., 2005) detect and represent temporally related events in natural language using logical form representation. (Saquete et al., 2009) use the temporal relations in a question to decompose it into simpler questions, the answers of which are recomposed to produce the answers to the original question. 6.1 Dynamic/Static Classification We automatically classify questions as dynamic and static questions. Answers to static q</context>
</contexts>
<marker>Prager, Chu-Carroll, Czuba, 2004</marker>
<rawString>J. Prager, J. Chu-Carroll, and K. Czuba. 2004. Question answering using constraint satisfaction: Qa-bydossier-with-contraints. In Proceedings of the 42nd annual meeting of the association for computational linguistics: ACL 2004, pages 574–581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>R Ingria</author>
<author>R Sauri</author>
<author>J Casta no</author>
<author>J Littman</author>
<author>R Gaizauskas</author>
</authors>
<title>The language of time: A reader, chapter The specification languae – TimeML.</title>
<date>2001</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="23269" citStr="Pustejovsky et al., 2001" startWordPosition="3889" endWordPosition="3892"> in the temporal nature of its answer. For example, consider the dynamic question, “What is the address of the theater ‘White Christmas’ is playing at in New York?”. White Christmas is a seasonal play that plays in New York every year for a few weeks in December and January, but it does not necessarily at the same theater every year. So, depending when this question is asked, the answer will be different. Interest in temporal analysis for questionanswering has been growing since the late 1990’s. Early work on temporal expressions identification using a tagger led to the development of TimeML (Pustejovsky et al., 2001), a markup language for annotating temporal expressions and events in text. Other examples include QA-byDossier with Constraints (Prager et al., 2004), a method of improving QA accuracy by asking auxiliary questions related to the original question in order to temporally verify and restrict the original answer. (Moldovan et al., 2005) detect and represent temporally related events in natural language using logical form representation. (Saquete et al., 2009) use the temporal relations in a question to decompose it into simpler questions, the answers of which are recomposed to produce the answer</context>
</contexts>
<marker>Pustejovsky, Ingria, Sauri, no, Littman, Gaizauskas, 2001</marker>
<rawString>J. Pustejovsky, R. Ingria, R. Sauri, J. Casta no, J. Littman, and R. Gaizauskas., 2001. The language of time: A reader, chapter The specification languae – TimeML. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Radev</author>
<author>B Sundheim</author>
</authors>
<title>Using timeml in question answering.</title>
<date>2002</date>
<tech>Technical report,</tech>
<institution>Brandies University.</institution>
<marker>Radev, Sundheim, 2002</marker>
<rawString>D. Radev and B. Sundheim. 2002. Using timeml in question answering. Technical report, Brandies University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Robertson</author>
</authors>
<title>Understanding inverse document frequency: On theoretical arguments for idf.</title>
<date>2004</date>
<journal>Journal of Documentation,</journal>
<volume>60</volume>
<contexts>
<context position="10395" citStr="Robertson, 2004" startWordPosition="1694" endWordPosition="1695">emporal recency of the answer, geographical proximity to the answer origin. 4.1 Question Retrieval Metrics We retrieve QA pairs from the data repository based on the similarity of match between the user’s query and each of the set of questions (d) in the repository. To measure the similarity, we have experimented with the following metrics. 1. TF-IDF metric: The user input query and the document (in our case, questions in the repository) are represented as bag-of-n-grams (aka terms). The term weights are computed using a combination of term frequency (tf) and inverse document frequency (idf) (Robertson, 2004). If Q = q1, q2, . . . , qn is a user query, then the aggregated score for a document d using a unigram model of the query and the document is given as in Equation 1. For a given query, the documents with the highest total term weight are presented as retrieved results. Terms can also be defined as n-gram sequences of a query and a document. In our experiments, we have used up to 4-grams as terms to retrieve and rank documents. �Score(d) = tfw,d X idfw (1) wEQ 2. String Comparison Metrics: Since the length of the user query and the query to be retrieved are similar in length, we use string com</context>
</contexts>
<marker>Robertson, 2004</marker>
<rawString>S. Robertson. 2004. Understanding inverse document frequency: On theoretical arguments for idf. Journal of Documentation, 60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Saquete</author>
<author>J L Vicedo</author>
<author>P Martinez-Barco</author>
<author>R Mu noz</author>
<author>H Llorens</author>
</authors>
<title>Enhancing qa systems with complex temporal question processing capabilities.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>35--775</pages>
<contexts>
<context position="23730" citStr="Saquete et al., 2009" startWordPosition="3960" endWordPosition="3963">been growing since the late 1990’s. Early work on temporal expressions identification using a tagger led to the development of TimeML (Pustejovsky et al., 2001), a markup language for annotating temporal expressions and events in text. Other examples include QA-byDossier with Constraints (Prager et al., 2004), a method of improving QA accuracy by asking auxiliary questions related to the original question in order to temporally verify and restrict the original answer. (Moldovan et al., 2005) detect and represent temporally related events in natural language using logical form representation. (Saquete et al., 2009) use the temporal relations in a question to decompose it into simpler questions, the answers of which are recomposed to produce the answers to the original question. 6.1 Dynamic/Static Classification We automatically classify questions as dynamic and static questions. Answers to static questions can be retrieved from the QA archive. To answer dynamic questions, we query the database(s) associated with the topic of the question through web forms on the Internet. We use a topic classifier to detect the topic of a question followed by a dynamic/static classifier trained on questions related to a</context>
</contexts>
<marker>Saquete, Vicedo, Martinez-Barco, noz, Llorens, 2009</marker>
<rawString>E. Saquete, J. L. Vicedo, P. Martinez-Barco, R. Mu noz, and H. Llorens. 2009. Enhancing qa systems with complex temporal question processing capabilities. Journal of Artificial Intelligence Research, 35:775–811.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Tomuro</author>
<author>S L Lytinen</author>
</authors>
<title>Retrieval models and Q and A learning with FAQ files.</title>
<date>2004</date>
<booktitle>In New Directions in Question Answering,</booktitle>
<pages>183--202</pages>
<contexts>
<context position="5329" citStr="Tomuro and Lytinen, 2004" startWordPosition="849" endWordPosition="853">trum, there are data intensive systems (Dumais et al., 2002) that attempt to use the redundancy of the web to arrive at an answer for factoid style questions. There are also variants of such QA techniques that involve an interaction and use context to resolve ambiguity (Yang et al., 2006). In contrast to these approaches, our method matches the user’s query against the questions in a large corpus of question-answer pairs and retrieves the associated answer. In the information retrieval community, QA systems attempt to retrieve precise segments of a document instead of the entire document. In (Tomuro and Lytinen, 2004), the authors match the user’s query against a frequently-asked-questions (FAQ) database and select the answer whose question matches most closely to the user’s question. An extension of this idea is explored in (Xue et al., 2008; Jeon et al., 2005), where the authors match the user’s query to a community collected QA archive such as (Yahoo!, 2009; MSN-QnA, 2009). Our approach is similar to both these lines of work in spirit, although the user’s query for our system originates as a spoken query, in contrast to the text queries in previous work. We also address the issue of noisy speech recogni</context>
</contexts>
<marker>Tomuro, Lytinen, 2004</marker>
<rawString>N. Tomuro and S. L. Lytinen. 2004. Retrieval models and Q and A learning with FAQ files. In New Directions in Question Answering, pages 183–202.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R J Waldinger</author>
<author>D E Appelt</author>
<author>J L Dungan</author>
<author>J Fry</author>
<author>J R Hobbs</author>
<author>D J Israel</author>
<author>P Jarvis</author>
<author>D L Martin</author>
<author>S Riehe-</author>
</authors>
<marker>Waldinger, Appelt, Dungan, Fry, Hobbs, Israel, Jarvis, Martin, Riehe-, </marker>
<rawString>vlingo.com, 2009. http://www.vlingomobile.com/downloads.html. R. J. Waldinger, D. E. Appelt, J. L. Dungan, J. Fry, J. R. Hobbs, D. J. Israel, P. Jarvis, D. L. Martin, S. Riehe-</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Stickel mann</author>
<author>M Tyson</author>
</authors>
<title>Deductive question answering from multiple resources.</title>
<date>2004</date>
<booktitle>In New Directions in Question Answering,</booktitle>
<pages>253--262</pages>
<marker>mann, Tyson, 2004</marker>
<rawString>mann, M. E. Stickel, and M. Tyson. 2004. Deductive question answering from multiple resources. In New Directions in Question Answering, pages 253–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weizenbaum</author>
</authors>
<title>ELIZA - a computer program for the study of natural language communication between man and machine.</title>
<date>1966</date>
<journal>Communications of the ACM,</journal>
<pages>1--36</pages>
<contexts>
<context position="3949" citStr="Weizenbaum, 1966" startWordPosition="632" endWordPosition="633">t the retrieval methods we used to implement the system in Section 4. In Section 5, we discuss and evaluate our approach to tight coupling of speech recognition and search components. In Section 6, we present bootstrap techniques to distinguish dynamic questions from static questions, and evaluate the efficacy of these techniques on a test corpus. We conclude in Section 7. 2 Related Work Early question-answering (QA) systems, such as Baseball (Green et al., 1961) and Lunar (Woods, 1973) were carefully hand-crafted to answer questions in a limited domain, similar to the QA components of ELIZA (Weizenbaum, 1966) and SHRDLU (Winograd, 1972). However, there has been a resurgence of QA systems following the TREC conferences with an emphasis on answering factoid questions. This work on text-based questionanswering which is comprehensively summarized Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 55–63, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics in (Maybury, 2004), range widely in terms of linguistic sophistication. At one end of the spectrum, There are linguistically motivated systems (Katz, 1997; Waldinger</context>
</contexts>
<marker>Weizenbaum, 1966</marker>
<rawString>J. Weizenbaum. 1966. ELIZA - a computer program for the study of natural language communication between man and machine. Communications of the ACM, 1:36–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Winograd</author>
</authors>
<title>Understanding Natural Language.</title>
<date>1972</date>
<publisher>Academic Press.</publisher>
<contexts>
<context position="3977" citStr="Winograd, 1972" startWordPosition="636" endWordPosition="637">d to implement the system in Section 4. In Section 5, we discuss and evaluate our approach to tight coupling of speech recognition and search components. In Section 6, we present bootstrap techniques to distinguish dynamic questions from static questions, and evaluate the efficacy of these techniques on a test corpus. We conclude in Section 7. 2 Related Work Early question-answering (QA) systems, such as Baseball (Green et al., 1961) and Lunar (Woods, 1973) were carefully hand-crafted to answer questions in a limited domain, similar to the QA components of ELIZA (Weizenbaum, 1966) and SHRDLU (Winograd, 1972). However, there has been a resurgence of QA systems following the TREC conferences with an emphasis on answering factoid questions. This work on text-based questionanswering which is comprehensively summarized Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 55–63, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics in (Maybury, 2004), range widely in terms of linguistic sophistication. At one end of the spectrum, There are linguistically motivated systems (Katz, 1997; Waldinger et al., 2004) that analyze </context>
</contexts>
<marker>Winograd, 1972</marker>
<rawString>T. Winograd. 1972. Understanding Natural Language. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
</authors>
<title>Progress in natural language understanding - an application to lunar geology.</title>
<date>1973</date>
<booktitle>In Proceedings of American Federation of Information Processing Societies (AFIPS) Conference.</booktitle>
<contexts>
<context position="3823" citStr="Woods, 1973" startWordPosition="612" endWordPosition="613"> we review the related literature. In Section 3, we illustrate the system for speech-driven question answering. We present the retrieval methods we used to implement the system in Section 4. In Section 5, we discuss and evaluate our approach to tight coupling of speech recognition and search components. In Section 6, we present bootstrap techniques to distinguish dynamic questions from static questions, and evaluate the efficacy of these techniques on a test corpus. We conclude in Section 7. 2 Related Work Early question-answering (QA) systems, such as Baseball (Green et al., 1961) and Lunar (Woods, 1973) were carefully hand-crafted to answer questions in a limited domain, similar to the QA components of ELIZA (Weizenbaum, 1966) and SHRDLU (Winograd, 1972). However, there has been a resurgence of QA systems following the TREC conferences with an emphasis on answering factoid questions. This work on text-based questionanswering which is comprehensively summarized Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 55–63, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics in (Maybury, 2004), range widely in ter</context>
</contexts>
<marker>Woods, 1973</marker>
<rawString>W. A. Woods. 1973. Progress in natural language understanding - an application to lunar geology. In Proceedings of American Federation of Information Processing Societies (AFIPS) Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Xue</author>
<author>J Jeon</author>
<author>W B Croft</author>
</authors>
<title>Retrieval models for question and answer archives.</title>
<date>2008</date>
<booktitle>In SIGIR ’08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>475--482</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5558" citStr="Xue et al., 2008" startWordPosition="887" endWordPosition="890">se context to resolve ambiguity (Yang et al., 2006). In contrast to these approaches, our method matches the user’s query against the questions in a large corpus of question-answer pairs and retrieves the associated answer. In the information retrieval community, QA systems attempt to retrieve precise segments of a document instead of the entire document. In (Tomuro and Lytinen, 2004), the authors match the user’s query against a frequently-asked-questions (FAQ) database and select the answer whose question matches most closely to the user’s question. An extension of this idea is explored in (Xue et al., 2008; Jeon et al., 2005), where the authors match the user’s query to a community collected QA archive such as (Yahoo!, 2009; MSN-QnA, 2009). Our approach is similar to both these lines of work in spirit, although the user’s query for our system originates as a spoken query, in contrast to the text queries in previous work. We also address the issue of noisy speech recognition and assess the value of tight integration of speech recognition and search in terms of improving the overall performance of the system. A novelty in this paper is our method to address dynamic questions as a seamless extensi</context>
</contexts>
<marker>Xue, Jeon, Croft, 2008</marker>
<rawString>X. Xue, J. Jeon, and W. B. Croft. 2008. Retrieval models for question and answer archives. In SIGIR ’08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 475–482, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yahoo</author>
</authors>
<date>2009</date>
<note>http://answers.yahoo.com/.</note>
<marker>Yahoo, 2009</marker>
<rawString>Yahoo!, 2009. http://answers.yahoo.com/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Yang</author>
<author>J Feng</author>
<author>G DiFabbrizio</author>
</authors>
<title>A data driven approach to relevancy recognition for contextual question answering.</title>
<date>2006</date>
<booktitle>In HLT-NAACL 2006 Workshop on Interactive Question Answering,</booktitle>
<location>New York, USA,</location>
<contexts>
<context position="4993" citStr="Yang et al., 2006" startWordPosition="794" endWordPosition="797">guistics in (Maybury, 2004), range widely in terms of linguistic sophistication. At one end of the spectrum, There are linguistically motivated systems (Katz, 1997; Waldinger et al., 2004) that analyze the user’s question and attempt to synthesize a coherent answer by aggregating the relevant facts. At the other end of the spectrum, there are data intensive systems (Dumais et al., 2002) that attempt to use the redundancy of the web to arrive at an answer for factoid style questions. There are also variants of such QA techniques that involve an interaction and use context to resolve ambiguity (Yang et al., 2006). In contrast to these approaches, our method matches the user’s query against the questions in a large corpus of question-answer pairs and retrieves the associated answer. In the information retrieval community, QA systems attempt to retrieve precise segments of a document instead of the entire document. In (Tomuro and Lytinen, 2004), the authors match the user’s query against a frequently-asked-questions (FAQ) database and select the answer whose question matches most closely to the user’s question. An extension of this idea is explored in (Xue et al., 2008; Jeon et al., 2005), where the aut</context>
</contexts>
<marker>Yang, Feng, DiFabbrizio, 2006</marker>
<rawString>F. Yang, J. Feng, and G. DiFabbrizio. 2006. A data driven approach to relevancy recognition for contextual question answering. In HLT-NAACL 2006 Workshop on Interactive Question Answering, New York, USA, June 8-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>YellowPages</author>
</authors>
<date>2009</date>
<note>http://www.speak4it.com.</note>
<contexts>
<context position="6301" citStr="YellowPages, 2009" startWordPosition="1015" endWordPosition="1017">QnA, 2009). Our approach is similar to both these lines of work in spirit, although the user’s query for our system originates as a spoken query, in contrast to the text queries in previous work. We also address the issue of noisy speech recognition and assess the value of tight integration of speech recognition and search in terms of improving the overall performance of the system. A novelty in this paper is our method to address dynamic questions as a seamless extension to answering static questions. Also related is the literature on voice-search applications (Microsoft, 2009; Google, 2009; YellowPages, 2009; vlingo.com, 2009) that provide a spoken language interface to business directories and return phone numbers, addresses and web sites of businesses. User input is typically not a free flowing natural language query and is limited to expressions with a business name and a location. In our system, users can avail of the full range of natural language expressions to express their information need. And finally, our method of retrieving answers to dynamic questions has relevance to the database and meta search community. There is growing interest in this community to mine the “hidden” web – inform</context>
</contexts>
<marker>YellowPages, 2009</marker>
<rawString>YellowPages, 2009. http://www.speak4it.com.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>