<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016007">
<figure confidence="0.471860363636364">
Computational Linguistics Volume 26, Number 2
Extended Finite State Models of Language
Andras Kornai (editor)
(BBN Technologies)
Cambridge University Press (Studies in
natural language processing), 1999,
xii+278 pp and CD-ROM; hardbound,
ISBN 0-521-63198-X, $59.95
Reviewed by
Ed Kaiser
Oregon Graduate Institute
</figure>
<bodyText confidence="0.999690314285714">
In this volume, Andras Kornai has put together a collection of articles that strongly
argue his contention that finite-state approaches to natural language processing (NLP)
are now part of the mainstream, both theoretically and computationally. The papers
included were first presented at a 1996 workshop, held in Budapest as part of the
European Conference on Artificial Intelligence (ECAI &apos;96), and have been chosen for
inclusion in this volume to complement previous works on finite-state approaches.
Articles referring to the Xerox regular expression calculus and the AT&amp;T Bell Labs
system of weighted finite-state transducers that appeared in Roche and Schabes&apos;s 1997
Finite State Language Processing are followed up by new articles in this book. In addition
this volume includes a CD-ROM, which, although it does not contain the Xerox and
AT&amp;T Bell Labs toolkits, does have source code and executables for several of the
systems described in the book, including an older version of Bruce Watson&apos;s FIRE Lite
toolkit for constructing and minimizing finite automata.
The attraction of finite-state approaches to NLP is their speed and efficiency. The
question is their adequacy—just how powerful a formal system is needed to describe
natural language? Consider the following sentence:
A doctor (whom a doctor)tm (hired)&apos; hired another nurse.
As Gazdar and Mellish point out in their textbook, from which this example is taken,
this is a legal English sentence only if m and n are equal (Gazdar and Mellish 1989).
Strings of the form an bn are not regular expressions and therefore cannot be represented
by finite-state machines. They require a context-free grammar definition and a machine
with an unbounded memory, such as the stack of a pushdown automaton. Natural
language constructions can require even more complex forms such as anbncn , which in
turn must be defined using indexed grammars and parsed with a machine that has
the equivalent of multiple stacks of memory. However &amp;quot;suppose that we had access
to hardware that would handle FSTNs [finite-state transition networks] ... ultrafast
and observed that in actual occurrences of an bn constructions, the value of n never
exceeded 3; then we might decide to compile our RTN [recursive transition network]
... descriptions down into FSTNs subject to an n = 3 upper bound (such a compilation
is possible for any given finite upper bound on the value of n)&amp;quot; (Gazdar and Mellish
1989). Kornai points out that such finite upper bounds are indeed what is observed in
parsing natural languages.
Kornai argues in his introduction that while the surge of thought and develop-
ment surrounding transformational models in the early 1960s threatened to remove all
credibility from finite-state approaches to NLP, the &amp;quot;extraordinary impact&amp;quot; of Thomp-
</bodyText>
<page confidence="0.993181">
282
</page>
<subsectionHeader confidence="0.949402">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.968444666666667">
son&apos;s (1968) grep family of Unix tools lead to the pervasive belief that if you wanted
to &amp;quot;do something with text you needed to build finite automata.&amp;quot; In this regard, Kor-
nai&apos;s own chapter on vectorized finite-state automata describes an extremely efficient
pattern-matching engine, around which the NewsMonitor system is built. This sys-
tem extracts relational information, such as &amp;quot;who is where&amp;quot; or &amp;quot;who bought what&amp;quot;,
from issues of the Wall Street Journal (source code and sample data are included on the
CD-ROM).
Shortly after the publication of The Sound Pattern of English (Chomsky and Halle
1968), Kornai points out, &amp;quot;Johnson (1970) demonstrated that the context-sensitive ma-
chinery of SPE . . . [could] be replaced by a much simpler one, based on finite-state
transducers (FSTs); the same conclusion was reached independently by Kaplan and
Kay, whose work remained an underground classic until it was finally published in Ka-
plan and Kay (1994).&amp;quot; These works inspired Koskenniemi&apos;s two-level system, and the
Xerox rule compiler (Dalrymple et al. 1987). Both are now dominant tools in the fields
of computational phonology and morphology, as exemplified by Tateno et al. (Chap-
ter 6), &amp;quot;The Japanese lexical transducer based on stem-suffix style forms&amp;quot; and Kim and
Jang (Chapter 7), &amp;quot;Acquiring rules for reducing morphological ambiguity from POS
tagged corpus in Korean.&amp;quot; The latter includes an algorithm for automatically inferring
regular grammar rules for morphological relations directly from part-of-speech tagged
corpora.
Although finite-state approaches to NLP were attempted as early as 1958, Kornai
comments that finite-state syntax &amp;quot;did not really come in from the cold until the
nineties.&amp;quot; Chapter 2 of this work, &amp;quot;A parser from antiquity: An early application of
finite state transducers to natural language parsing,&amp;quot; by Joshi and Hopely, describes
that 1958 parser. In a short commentary on that article, Lauri Karttunen points out
that &amp;quot;many of the currently popular methods for robust parsing are already present,
fully articulated:
</bodyText>
<listItem confidence="0.992127882352941">
• multiword tokens (because of, in front of);
• tagging words by ambiguity classes (cool VA, [i.e., (V)erb (A)djective
class]);
• rule-based disambiguation;
• syntactic markup by finite-state transduction;
• depth-first strategy with backtracking and pushdown store for the
analysis of recursive structures;
• default selection of one structure among alternative analyses with option
for later revision.&amp;quot;
Contemporary systems that incorporate these features in finite-state approaches to
parsing and modeling syntax are described in the following papers in the book:
• Chanod and Tapanainen, Chapter 8, &amp;quot;Finite state based reductionist
parsing for French&amp;quot;;
• Grefenstette, Chapter 9, &amp;quot;Light parsing as finite state filtering&amp;quot;;
• Kornai, Chapter 10, &amp;quot;Vectorized finite state automata&amp;quot;;
• Roche, Chapter 11, &amp;quot;Finite state transducers: Parsing free and frozen
sentences&amp;quot;.
</listItem>
<page confidence="0.992574">
283
</page>
<note confidence="0.636194">
Computational Linguistics Volume 26, Number 2
</note>
<bodyText confidence="0.9906585">
The papers by Vilares et al. (Chapter 12), &amp;quot;Text and speech translation by means of
subsequential transducers,&amp;quot; and Ejerhed (Chapter 13), &amp;quot;Finite state segmentation of
discourse into clauses,&amp;quot; move the application of finite-state techniques into exciting
new areas of machine translation and discourse segmentation.
In Chapter 11, Roche expands on the part-of-speech-tagging and parsing articles of
Roche and Schabes (1997). Specifically, he details a finite-state method for syntactically
parsing light verbs. Such verbs have complements that allow only a certain degree of
variability:
</bodyText>
<listItem confidence="0.999773666666667">
(1) John makes concessions to his friend.
(2) John makes a right turn.
(3) *John makes a right turn to his friend.
</listItem>
<bodyText confidence="0.999877361111111">
In this example the predicative noun concessions is the real head of the sentence, gov-
erning the number and nature of arguments. The light verb makes is in a support role.
Roche&apos;s paper shows that such constructions, for which &amp;quot;rewriting mechanisms such
as context free parsing are at best unnatural,&amp;quot; can be processed by finite-state trans-
ducer parsing. Kornai points out that such light-verb constructions were seen in the
tradition of Chomsky (1970) as &amp;quot;core cases of transformational grammar.&amp;quot; Roche&apos;s arti-
cle argues persuasively that finite-state models are not just &amp;quot;an efficient but somewhat
inaccurate tool,&amp;quot; as might be imagined from a transformational grammar perspec-
tive, &amp;quot;but rather one of the best formalisms at hand to represent accurately complex
linguistic phenomena.&amp;quot;
Kornai suggests also that another &amp;quot;important way in which mainstream syntax is
impacted by finite-state techniques can be called &apos;finite-state to the rescue&apos;. The paper
by Schulz and Mikolajewski [Chapter 14, &amp;quot;Between finite state and Prolog: Constraint-
based automata for efficient recognition of phrases&amp;quot;] ... describes how constraint-
based grammars can be speeded up by finite-state methods, and the paper by Srinivas
[Chapter 15, &amp;quot;Explanation-based learning and finite state transducers: Applications to
parsing lexicalized tree adjoining grammars&amp;quot;] ... shows how corpus-based acquisition
of LTAGs is facilitated by finite-state techniques.&amp;quot;
Speech recognition is dominated by statistical techniques, in particular HMMs
(which are finite-state mechanisms) and statistical n-gram language models. In regards
to syntax parsing, Kornai points out that &amp;quot;an important step in bringing rule-based and
statistical work closer is the framework of weighted finite-state transducers developed
at AT&amp;T Bell Labs, represented in this volume by the Tzoukermann and Radev paper&amp;quot;
(Chapter 16, &amp;quot;Use of weighted finite state transducers in part of speech tagging&amp;quot;). This
article describes a process of classifying words into &amp;quot;genotypes&amp;quot; through the use of
weighted, negative-constraint transducers. All members of a particular genotype share
the same set of possible part-of-speech tags. After tagging, genotype n-gram models
are built. Tzoukermann and Radev state that their system &amp;quot;correctly disambiguates
96% of words in unrestricted texts.&amp;quot;
Since the main algorithms and model-building techniques associated with the Xe-
rox and AT&amp;T Bell Labs approaches to finite-state NLP are well described elsewhere,
this book examines ways of extending the scope of finite-state machinery. For example,
the &amp;quot;more complex formal systems discussed by Csuhaj-Varju [Chapter 17, &amp;quot;Colonies:
A multi-agent approach to language generation&amp;quot;] , Nederhof and Bertsch [Chap-
ter 18, &amp;quot;An innovative finite state concept for recognition and parsing of context free
languages&amp;quot;] , and Ristad [Chapter 19, &amp;quot;Hidden Markov models with finite state
</bodyText>
<page confidence="0.993168">
284
</page>
<subsectionHeader confidence="0.927976">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.999923375">
supervisionl ... are likely to provide a fertile ground for further experimentation
with extended finite state models of language.&amp;quot;
Nederhof and Bertsch&apos;s article defines a new subclass of context-free languages
that combines deterministic languages (also known as the LR(k) languages) within the
sequential framework of a finite-state automaton. The result is a class of languages
that can be recognized and parsed in linear time, which also allows for determinis-
tic center-self-embedding (i.e., rule descriptions that define strings of the form an bn
for unbounded values of n). This approach is in some respects similar to lexicalized
tree adjoining grammars (Srinivas, Chapter 15), which also have a regular level (i.e.,
initial trees) that contain a context-free lower-level (i.e., auxiliary trees—for center-self-
embedding). The notion of combining regular grammars with context-free or restricted
context-free grammars (like the LR(k) grammars) to create new subclasses of gram-
mars and languages that lend themselves to treatment by finite-state approaches is an
exciting area of research that is well represented in this book.
Overall, I believe that this book will be a valued addition to the library of anyone
interested in emerging finite-state approaches to NLP.
</bodyText>
<sectionHeader confidence="0.97641" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.993608741935484">
Chomsky, Noam. 1970. Remarks on
nominalization. In R. Jacobs and P.
Rosenbaum, editors, Readings in English
Transformational Grammar. Blaisdell,
Waltham, MA, pages 184-221.
Chomsky, Noam and Morris Halle. 1968. The
Sound Pattern of English. Harper and Row,
New York.
Dalrymple, Mary, Ronald M. Kaplan, Lauri
Karttunen, Kimmo Koskenniemi, Sami
Shaio, and Michael Wescoat. 1987. Tools for
morphological analysis. Center for the Study
of Language and Information Report
CSLI-87-108. CSLI, Stanford, CA.
Gazdar, Gerald and Chris Mellish. 1989.
Natural Language Processing in Prolog.
Addison Wesley.
Johnson, C. Douglas. 1970. Formal Aspects of
Phonological Representation Ph.D. thesis,
University of California at Berkeley.
Kaplan, Ronald M. and Martin Kay. 1994.
Regular models of phonological rule
systems. Computational Linguistics,
21(3):331-378.
Roche, Emmanuel and Yves Schabes,
editors. 1997. Finite-State Language
Processing. The MIT Press, Cambridge,
MA.
Thompson, Ken. 1968. Regular expression
search algorithm. Communications of the
ACM, 11(6):419-422.
</reference>
<bodyText confidence="0.7705666">
Ed Kaiser is interested in integrating structural language models directly into the speech recogni-
tion process. He has developed a robust, finite-state parser (inspired by CMU&apos;s Phoenix system)
as a first step towards that integration, and published two papers on the compilation techniques
involved. Kaiser&apos;s address is Computer Science and Engineering Department, Oregon Graduate
Institute, P.O. Box 91000, Portland, OR, 97291-1000; e-mail: kaiser@cse.ogi.edu
</bodyText>
<page confidence="0.997712">
285
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.485286">
<title confidence="0.901978">Computational Linguistics Volume 26, Number 2 Extended Finite State Models of Language</title>
<author confidence="0.993973">Andras Kornai</author>
<affiliation confidence="0.8767295">(BBN Technologies) Cambridge University Press (Studies in</affiliation>
<note confidence="0.92750525">natural language processing), 1999, xii+278 pp and CD-ROM; hardbound, ISBN 0-521-63198-X, $59.95 Reviewed by</note>
<author confidence="0.985115">Ed Kaiser</author>
<affiliation confidence="0.999286">Oregon Graduate Institute</affiliation>
<abstract confidence="0.9980275">In this volume, Andras Kornai has put together a collection of articles that strongly argue his contention that finite-state approaches to natural language processing (NLP) are now part of the mainstream, both theoretically and computationally. The papers included were first presented at a 1996 workshop, held in Budapest as part of the European Conference on Artificial Intelligence (ECAI &apos;96), and have been chosen for inclusion in this volume to complement previous works on finite-state approaches. Articles referring to the Xerox regular expression calculus and the AT&amp;T Bell Labs system of weighted finite-state transducers that appeared in Roche and Schabes&apos;s 1997 State Language Processing followed up by new articles in this book. In addition this volume includes a CD-ROM, which, although it does not contain the Xerox and AT&amp;T Bell Labs toolkits, does have source code and executables for several of the systems described in the book, including an older version of Bruce Watson&apos;s FIRE Lite toolkit for constructing and minimizing finite automata. The attraction of finite-state approaches to NLP is their speed and efficiency. The question is their adequacy—just how powerful a formal system is needed to describe natural language? Consider the following sentence: A doctor (whom a doctor)tm (hired)&apos; hired another nurse. As Gazdar and Mellish point out in their textbook, from which this example is taken, is a legal English sentence only if equal (Gazdar and Mellish 1989). of the form bn not regular expressions and therefore cannot be represented by finite-state machines. They require a context-free grammar definition and a machine with an unbounded memory, such as the stack of a pushdown automaton. Natural constructions can require even more complex forms such as , in turn must be defined using indexed grammars and parsed with a machine that has the equivalent of multiple stacks of memory. However &amp;quot;suppose that we had access to hardware that would handle FSTNs [finite-state transition networks] ... ultrafast observed that in actual occurrences of bn the value of exceeded 3; then we might decide to compile our RTN [recursive transition network] descriptions down into FSTNs subject to an = upper bound (such a compilation possible for any given finite upper bound on the value of and Mellish 1989). Kornai points out that such finite upper bounds are indeed what is observed in parsing natural languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Remarks on nominalization.</title>
<date>1970</date>
<booktitle>Readings in English Transformational Grammar.</booktitle>
<pages>184--221</pages>
<editor>In R. Jacobs and P. Rosenbaum, editors,</editor>
<location>Blaisdell, Waltham, MA,</location>
<contexts>
<context position="7252" citStr="Chomsky (1970)" startWordPosition="1110" endWordPosition="1111">at allow only a certain degree of variability: (1) John makes concessions to his friend. (2) John makes a right turn. (3) *John makes a right turn to his friend. In this example the predicative noun concessions is the real head of the sentence, governing the number and nature of arguments. The light verb makes is in a support role. Roche&apos;s paper shows that such constructions, for which &amp;quot;rewriting mechanisms such as context free parsing are at best unnatural,&amp;quot; can be processed by finite-state transducer parsing. Kornai points out that such light-verb constructions were seen in the tradition of Chomsky (1970) as &amp;quot;core cases of transformational grammar.&amp;quot; Roche&apos;s article argues persuasively that finite-state models are not just &amp;quot;an efficient but somewhat inaccurate tool,&amp;quot; as might be imagined from a transformational grammar perspective, &amp;quot;but rather one of the best formalisms at hand to represent accurately complex linguistic phenomena.&amp;quot; Kornai suggests also that another &amp;quot;important way in which mainstream syntax is impacted by finite-state techniques can be called &apos;finite-state to the rescue&apos;. The paper by Schulz and Mikolajewski [Chapter 14, &amp;quot;Between finite state and Prolog: Constraintbased automata</context>
</contexts>
<marker>Chomsky, 1970</marker>
<rawString>Chomsky, Noam. 1970. Remarks on nominalization. In R. Jacobs and P. Rosenbaum, editors, Readings in English Transformational Grammar. Blaisdell, Waltham, MA, pages 184-221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
<author>Morris Halle</author>
</authors>
<title>The Sound Pattern of English. Harper and Row,</title>
<date>1968</date>
<location>New York.</location>
<contexts>
<context position="3729" citStr="Chomsky and Halle 1968" startWordPosition="580" endWordPosition="583">homp282 Book Reviews son&apos;s (1968) grep family of Unix tools lead to the pervasive belief that if you wanted to &amp;quot;do something with text you needed to build finite automata.&amp;quot; In this regard, Kornai&apos;s own chapter on vectorized finite-state automata describes an extremely efficient pattern-matching engine, around which the NewsMonitor system is built. This system extracts relational information, such as &amp;quot;who is where&amp;quot; or &amp;quot;who bought what&amp;quot;, from issues of the Wall Street Journal (source code and sample data are included on the CD-ROM). Shortly after the publication of The Sound Pattern of English (Chomsky and Halle 1968), Kornai points out, &amp;quot;Johnson (1970) demonstrated that the context-sensitive machinery of SPE . . . [could] be replaced by a much simpler one, based on finite-state transducers (FSTs); the same conclusion was reached independently by Kaplan and Kay, whose work remained an underground classic until it was finally published in Kaplan and Kay (1994).&amp;quot; These works inspired Koskenniemi&apos;s two-level system, and the Xerox rule compiler (Dalrymple et al. 1987). Both are now dominant tools in the fields of computational phonology and morphology, as exemplified by Tateno et al. (Chapter 6), &amp;quot;The Japanese</context>
</contexts>
<marker>Chomsky, Halle, 1968</marker>
<rawString>Chomsky, Noam and Morris Halle. 1968. The Sound Pattern of English. Harper and Row, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Dalrymple</author>
<author>Ronald M Kaplan</author>
<author>Lauri Karttunen</author>
<author>Kimmo Koskenniemi</author>
<author>Sami Shaio</author>
<author>Michael Wescoat</author>
</authors>
<title>Tools for morphological analysis. Center for the Study of Language and Information Report CSLI-87-108. CSLI,</title>
<date>1987</date>
<location>Stanford, CA.</location>
<contexts>
<context position="4184" citStr="Dalrymple et al. 1987" startWordPosition="651" endWordPosition="654">he Wall Street Journal (source code and sample data are included on the CD-ROM). Shortly after the publication of The Sound Pattern of English (Chomsky and Halle 1968), Kornai points out, &amp;quot;Johnson (1970) demonstrated that the context-sensitive machinery of SPE . . . [could] be replaced by a much simpler one, based on finite-state transducers (FSTs); the same conclusion was reached independently by Kaplan and Kay, whose work remained an underground classic until it was finally published in Kaplan and Kay (1994).&amp;quot; These works inspired Koskenniemi&apos;s two-level system, and the Xerox rule compiler (Dalrymple et al. 1987). Both are now dominant tools in the fields of computational phonology and morphology, as exemplified by Tateno et al. (Chapter 6), &amp;quot;The Japanese lexical transducer based on stem-suffix style forms&amp;quot; and Kim and Jang (Chapter 7), &amp;quot;Acquiring rules for reducing morphological ambiguity from POS tagged corpus in Korean.&amp;quot; The latter includes an algorithm for automatically inferring regular grammar rules for morphological relations directly from part-of-speech tagged corpora. Although finite-state approaches to NLP were attempted as early as 1958, Kornai comments that finite-state syntax &amp;quot;did not rea</context>
</contexts>
<marker>Dalrymple, Kaplan, Karttunen, Koskenniemi, Shaio, Wescoat, 1987</marker>
<rawString>Dalrymple, Mary, Ronald M. Kaplan, Lauri Karttunen, Kimmo Koskenniemi, Sami Shaio, and Michael Wescoat. 1987. Tools for morphological analysis. Center for the Study of Language and Information Report CSLI-87-108. CSLI, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Chris Mellish</author>
</authors>
<title>Natural Language Processing in Prolog.</title>
<date>1989</date>
<publisher>Addison Wesley.</publisher>
<contexts>
<context position="1827" citStr="Gazdar and Mellish 1989" startWordPosition="274" endWordPosition="277">e and executables for several of the systems described in the book, including an older version of Bruce Watson&apos;s FIRE Lite toolkit for constructing and minimizing finite automata. The attraction of finite-state approaches to NLP is their speed and efficiency. The question is their adequacy—just how powerful a formal system is needed to describe natural language? Consider the following sentence: A doctor (whom a doctor)tm (hired)&apos; hired another nurse. As Gazdar and Mellish point out in their textbook, from which this example is taken, this is a legal English sentence only if m and n are equal (Gazdar and Mellish 1989). Strings of the form an bn are not regular expressions and therefore cannot be represented by finite-state machines. They require a context-free grammar definition and a machine with an unbounded memory, such as the stack of a pushdown automaton. Natural language constructions can require even more complex forms such as anbncn , which in turn must be defined using indexed grammars and parsed with a machine that has the equivalent of multiple stacks of memory. However &amp;quot;suppose that we had access to hardware that would handle FSTNs [finite-state transition networks] ... ultrafast and observed t</context>
</contexts>
<marker>Gazdar, Mellish, 1989</marker>
<rawString>Gazdar, Gerald and Chris Mellish. 1989. Natural Language Processing in Prolog. Addison Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Douglas Johnson</author>
</authors>
<title>Formal Aspects of Phonological Representation Ph.D. thesis,</title>
<date>1970</date>
<institution>University of California at Berkeley.</institution>
<contexts>
<context position="3765" citStr="Johnson (1970)" startWordPosition="587" endWordPosition="588"> of Unix tools lead to the pervasive belief that if you wanted to &amp;quot;do something with text you needed to build finite automata.&amp;quot; In this regard, Kornai&apos;s own chapter on vectorized finite-state automata describes an extremely efficient pattern-matching engine, around which the NewsMonitor system is built. This system extracts relational information, such as &amp;quot;who is where&amp;quot; or &amp;quot;who bought what&amp;quot;, from issues of the Wall Street Journal (source code and sample data are included on the CD-ROM). Shortly after the publication of The Sound Pattern of English (Chomsky and Halle 1968), Kornai points out, &amp;quot;Johnson (1970) demonstrated that the context-sensitive machinery of SPE . . . [could] be replaced by a much simpler one, based on finite-state transducers (FSTs); the same conclusion was reached independently by Kaplan and Kay, whose work remained an underground classic until it was finally published in Kaplan and Kay (1994).&amp;quot; These works inspired Koskenniemi&apos;s two-level system, and the Xerox rule compiler (Dalrymple et al. 1987). Both are now dominant tools in the fields of computational phonology and morphology, as exemplified by Tateno et al. (Chapter 6), &amp;quot;The Japanese lexical transducer based on stem-su</context>
</contexts>
<marker>Johnson, 1970</marker>
<rawString>Johnson, C. Douglas. 1970. Formal Aspects of Phonological Representation Ph.D. thesis, University of California at Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Martin Kay</author>
</authors>
<title>Regular models of phonological rule systems.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>21--3</pages>
<contexts>
<context position="4077" citStr="Kaplan and Kay (1994)" startWordPosition="635" endWordPosition="639">This system extracts relational information, such as &amp;quot;who is where&amp;quot; or &amp;quot;who bought what&amp;quot;, from issues of the Wall Street Journal (source code and sample data are included on the CD-ROM). Shortly after the publication of The Sound Pattern of English (Chomsky and Halle 1968), Kornai points out, &amp;quot;Johnson (1970) demonstrated that the context-sensitive machinery of SPE . . . [could] be replaced by a much simpler one, based on finite-state transducers (FSTs); the same conclusion was reached independently by Kaplan and Kay, whose work remained an underground classic until it was finally published in Kaplan and Kay (1994).&amp;quot; These works inspired Koskenniemi&apos;s two-level system, and the Xerox rule compiler (Dalrymple et al. 1987). Both are now dominant tools in the fields of computational phonology and morphology, as exemplified by Tateno et al. (Chapter 6), &amp;quot;The Japanese lexical transducer based on stem-suffix style forms&amp;quot; and Kim and Jang (Chapter 7), &amp;quot;Acquiring rules for reducing morphological ambiguity from POS tagged corpus in Korean.&amp;quot; The latter includes an algorithm for automatically inferring regular grammar rules for morphological relations directly from part-of-speech tagged corpora. Although finite-sta</context>
</contexts>
<marker>Kaplan, Kay, 1994</marker>
<rawString>Kaplan, Ronald M. and Martin Kay. 1994. Regular models of phonological rule systems. Computational Linguistics, 21(3):331-378.</rawString>
</citation>
<citation valid="true">
<title>Finite-State Language Processing.</title>
<date>1997</date>
<editor>Roche, Emmanuel and Yves Schabes, editors.</editor>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="6520" citStr="(1997)" startWordPosition="993" endWordPosition="993"> Chapter 10, &amp;quot;Vectorized finite state automata&amp;quot;; • Roche, Chapter 11, &amp;quot;Finite state transducers: Parsing free and frozen sentences&amp;quot;. 283 Computational Linguistics Volume 26, Number 2 The papers by Vilares et al. (Chapter 12), &amp;quot;Text and speech translation by means of subsequential transducers,&amp;quot; and Ejerhed (Chapter 13), &amp;quot;Finite state segmentation of discourse into clauses,&amp;quot; move the application of finite-state techniques into exciting new areas of machine translation and discourse segmentation. In Chapter 11, Roche expands on the part-of-speech-tagging and parsing articles of Roche and Schabes (1997). Specifically, he details a finite-state method for syntactically parsing light verbs. Such verbs have complements that allow only a certain degree of variability: (1) John makes concessions to his friend. (2) John makes a right turn. (3) *John makes a right turn to his friend. In this example the predicative noun concessions is the real head of the sentence, governing the number and nature of arguments. The light verb makes is in a support role. Roche&apos;s paper shows that such constructions, for which &amp;quot;rewriting mechanisms such as context free parsing are at best unnatural,&amp;quot; can be processed b</context>
</contexts>
<marker>1997</marker>
<rawString>Roche, Emmanuel and Yves Schabes, editors. 1997. Finite-State Language Processing. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Thompson</author>
</authors>
<title>Regular expression search algorithm.</title>
<date>1968</date>
<journal>Communications of the ACM,</journal>
<pages>11--6</pages>
<marker>Thompson, 1968</marker>
<rawString>Thompson, Ken. 1968. Regular expression search algorithm. Communications of the ACM, 11(6):419-422.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>