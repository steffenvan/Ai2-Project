<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009592">
<title confidence="0.9806185">
Learning to Grade Short Answer Questions using Semantic Similarity
Measures and Dependency Graph Alignments
</title>
<author confidence="0.99829">
Michael Mohler
</author>
<affiliation confidence="0.999436">
Dept. of Computer Science
University of North Texas
</affiliation>
<address confidence="0.774817">
Denton, TX
</address>
<email confidence="0.998247">
mgm0038@unt.edu
</email>
<author confidence="0.985826">
Razvan Bunescu
</author>
<affiliation confidence="0.9967665">
School of EECS
Ohio University
</affiliation>
<address confidence="0.614311">
Athens, Ohio
</address>
<email confidence="0.998365">
bunescu@ohio.edu
</email>
<author confidence="0.994218">
Rada Mihalcea
</author>
<affiliation confidence="0.999421">
Dept. of Computer Science
University of North Texas
</affiliation>
<address confidence="0.775016">
Denton, TX
</address>
<email confidence="0.999172">
rada@cs.unt.edu
</email>
<sectionHeader confidence="0.995644" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995325">
In this work we address the task of computer-
assisted assessment of short student answers.
We combine several graph alignment features
with lexical semantic similarity measures us-
ing machine learning techniques and show
that the student answers can be more accu-
rately graded than if the semantic measures
were used in isolation. We also present a first
attempt to align the dependency graphs of the
student and the instructor answers in order to
make use of a structural component in the au-
tomatic grading of student answers.
</bodyText>
<sectionHeader confidence="0.998983" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999880576923077">
One of the most important aspects of the learning
process is the assessment of the knowledge acquired
by the learner. In a typical classroom assessment
(e.g., an exam, assignment or quiz), an instructor or
a grader provides students with feedback on their
answers to questions related to the subject matter.
However, in certain scenarios, such as a number of
sites worldwide with limited teacher availability, on-
line learning environments, and individual or group
study sessions done outside of class, an instructor
may not be readily available. In these instances, stu-
dents still need some assessment of their knowledge
of the subject, and so, we must turn to computer-
assisted assessment (CAA).
While some forms of CAA do not require sophis-
ticated text understanding (e.g., multiple choice or
true/false questions can be easily graded by a system
if the correct solution is available), there are also stu-
dent answers made up of free text that may require
textual analysis. Research to date has concentrated
on two subtasks of CAA: grading essay responses,
which includes checking the style, grammaticality,
and coherence of the essay (Higgins et al., 2004),
and the assessment of short student answers (Lea-
cock and Chodorow, 2003; Pulman and Sukkarieh,
2005; Mohler and Mihalcea, 2009), which is the fo-
cus of this work.
An automatic short answer grading system is one
that automatically assigns a grade to an answer pro-
vided by a student, usually by comparing it to one
or more correct answers. Note that this is different
from the related tasks of paraphrase detection and
textual entailment, since a common requirement in
student answer grading is to provide a grade on a
certain scale rather than make a simple yes/no deci-
sion.
In this paper, we explore the possibility of im-
proving upon existing bag-of-words (BOW) ap-
proaches to short answer grading by utilizing ma-
chine learning techniques. Furthermore, in an at-
tempt to mirror the ability of humans to understand
structural (e.g. syntactic) differences between sen-
tences, we employ a rudimentary dependency-graph
alignment module, similar to those more commonly
used in the textual entailment community.
Specifically, we seek answers to the following
questions. First, to what extent can machine learn-
ing be leveraged to improve upon existing ap-
proaches to short answer grading. Second, does the
dependency parse structure of a text provide clues
that can be exploited to improve upon existing BOW
methodologies?
</bodyText>
<page confidence="0.955451">
752
</page>
<note confidence="0.9828015">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 752–762,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.62853" genericHeader="related work">
2 Related Work grades.
</sectionHeader>
<bodyText confidence="0.999917934782609">
Several state-of-the-art short answer grading sys- In the dependency-based classification compo-
tems (Sukkarieh et al., 2004; Mitchell et al., 2002) nent of the Intelligent Tutoring System (Nielsen et
require manually crafted patterns which, if matched, al., 2009), instructor answers are parsed, enhanced,
indicate that a question has been answered correctly. and manually converted into a set of content-bearing
If an annotated corpus is available, these patterns dependency triples or facets. For each facet of the
can be supplemented by learning additional pat- instructor answer each student’s answer is labelled
terns semi-automatically. The Oxford-UCLES sys- to indicate whether it has addressed that facet and
tem (Sukkarieh et al., 2004) bootstraps patterns by whether or not the answer was contradictory. The
starting with a set of keywords and synonyms and system uses a decision tree trained on part-of-speech
searching through windows of a text for new pat- tags, dependency types, word count, and other fea-
terns. A later implementation of the Oxford-UCLES tures to attempt to learn how best to classify an an-
system (Pulman and Sukkarieh, 2005) compares swer/facet pair.
several machine learning techniques, including in- Closely related to the task of short answer grading
ductive logic programming, decision tree learning, is the task of textual entailment (Dagan et al., 2005),
and Bayesian learning, to the earlier pattern match- which targets the identification of a directional in-
ing approach, with encouraging results. ferential relation between texts. Given a pair of two
C-Rater (Leacock and Chodorow, 2003) matches texts as input, typically referred to as text and hy-
the syntactical features of a student response (i.e., pothesis, a textual entailment system automatically
subject, object, and verb) to that of a set of correct finds if the hypothesis is entailed by the text.
responses. This method specifically disregards the In particular, the entailment-related works that are
BOW approach to take into account the difference most similar to our own are the graph matching tech-
between “dog bites man” and “man bites dog” while niques proposed by Haghighi et al. (2005) and Rus
still trying to detect changes in voice (i.e., “the man et al. (2007). Both input texts are converted into a
was bitten by the dog”). graph by using the dependency relations obtained
Another short answer grading system, AutoTutor from a parser. Next, a matching score is calculated,
(Wiemer-Hastings et al., 1999), has been designed by combining separate vertex- and edge-matching
as an immersive tutoring environment with a graph- scores. The vertex matching functions use word-
ical “talking head” and speech recognition to im- level lexical and semantic features to determine the
prove the overall experience for students. AutoTutor quality of the match while the the edge matching
eschews the pattern-based approach entirely in favor functions take into account the types of relations and
of a BOW LSA approach (Landauer and Dumais, the difference in lengths between the aligned paths.
1997). Later work on AutoTutor(Wiemer-Hastings Following the same line of work in the textual en-
et al., 2005; Malatesta et al., 2002) seeks to expand tailment world are (Raina et al., 2005), (MacCartney
upon their BOW approach which becomes less use- et al., 2006), (de Marneffe et al., 2007), and (Cham-
ful as causality (and thus word order) becomes more bers et al., 2007), which experiment variously with
important. using diverse knowledge sources, using a perceptron
A text similarity approach was taken in (Mohler to learn alignment decisions, and exploiting natural
and Mihalcea, 2009), where a grade is assigned logic.
based on a measure of relatedness between the stu- 3 Answer Grading System
dent and the instructor answer. Several measures are We use a set of syntax-aware graph alignment fea-
compared, including knowledge-based and corpus- tures in a three-stage pipelined approach to short an-
based measures, with the best results being obtained swer grading, as outlined in Figure 1.
with a corpus-based measure using Wikipedia com- In the first stage (Section 3.1), the system is pro-
bined with a “relevance feedback” approach that it- vided with the dependency graphs for each pair of
eratively augments the instructor answer by inte- instructor (AZ) and student (A3) answers. For each
grating the student answers that receive the highest
</bodyText>
<page confidence="0.570587">
753
</page>
<figureCaption confidence="0.994815">
Figure 1: Pipeline model for scoring short-answer pairs.
</figureCaption>
<bodyText confidence="0.999972592592593">
node in the instructor’s dependency graph, we com-
pute a similarity score for each node in the student’s
dependency graph based upon a set of lexical, se-
mantic, and syntactic features applied to both the
pair of nodes and their corresponding subgraphs.
The scoring function is trained on a small set of man-
ually aligned graphs using the averaged perceptron
algorithm.
In the second stage (Section 3.2), the node simi-
larity scores calculated in the previous stage are used
to weight the edges in a bipartite graph representing
the nodes in Ai on one side and the nodes in A3 on
the other. We then apply the Hungarian algorithm
to find both an optimal matching and the score asso-
ciated with such a matching. In this stage, we also
introduce question demoting in an attempt to reduce
the advantage of parroting back words provided in
the question.
In the final stage (Section 3.4), we produce an
overall grade based upon the alignment scores found
in the previous stage as well as the results of several
semantic BOW similarity measures (Section 3.3).
Using each of these as features, we use Support Vec-
tor Machines (SVM) to produce a combined real-
number grade. Finally, we build an Isotonic Regres-
sion (IR) model to transform our output scores onto
the original [0,5] scale for ease of comparison.
</bodyText>
<subsectionHeader confidence="0.999245">
3.1 Node to Node Matching
</subsectionHeader>
<bodyText confidence="0.999961708333333">
Dependency graphs for both the student and in-
structor answers are generated using the Stanford
Dependency Parser (de Marneffe et al., 2006) in
collapse/propagate mode. The graphs are further
post-processed to propagate dependencies across the
“APPOS” (apposition) relation, to explicitly encode
negation, part-of-speech, and sentence ID within
each node, and to add an overarching ROOT node
governing the main verb or predicate of each sen-
tence of an answer. The final representation is a
list of (relation,governor,dependent) triples, where
governor and dependent are both tokens described
by the tuple (sentenceID:token:POS:wordPosition).
For example: (nsubj, 1:provide:VBZ:4, 1:pro-
gram:NN:3) indicates that the noun “program” is a
subject in sentence 1 whose associated verb is “pro-
vide.”
If we consider the dependency graphs output by
the Stanford parser as directed (minimally cyclic)
graphs,1 we can define for each node x a set of nodes
Nx that are reachable from x using a subset of the
relations (i.e., edge types)2. We variously define
“reachable” in four ways to create four subgraphs
defined for each node. These are as follows:
</bodyText>
<listItem confidence="0.999588125">
• N° : All edge types may be followed
• N1x : All edge types except for subject types,
ADVCL, PURPCL, APPOS, PARATAXIS,
ABBREV, TMOD, and CONJ
• N2x : All edge types except for those in Nx1 plus
object/complement types, PREP, and RCMOD
• Nx3 : No edge types may be followed (This set
is the single starting node x)
</listItem>
<bodyText confidence="0.999280777777778">
Subgraph similarity (as opposed to simple node
similarity) is a means to escape the rigidity involved
in aligning parse trees while making use of as much
of the sentence structure as possible. Humans intu-
itively make use of modifiers, predicates, and subor-
dinate clauses in determining that two sentence en-
tities are similar. For instance, the entity-describing
phrase “men who put out fires” matches well with
“firemen,” but the words “men” and “firemen” have
</bodyText>
<footnote confidence="0.999823142857143">
1The standard output of the Stanford Parser produces rooted
trees. However, the process of collapsing and propagating de-
pendences violates the tree structure which results in a tree
with a few cross-links between distinct branches.
2For more information on the relations used in this experi-
ment, consult the Stanford Typed Dependencies Manual at
http://nlp.stanford.edu/software/dependencies manual.pdf
</footnote>
<page confidence="0.996419">
754
</page>
<bodyText confidence="0.997846493975904">
less inherent similarity. It remains to be determined 0. set w &lt;-- 0, w &lt;-- 0, n &lt;-- 0
how much of a node’s subgraph will positively en- 1. repeat for T epochs:
rich its semantics. In addition to the complete N0 x 2. foreach (Ai; As):
subgraph, we chose to include N&apos;x and N2x as tight- 3. foreach (xi, xs) E Ai x As:
ening the scope of the subtree by first removing 4. if sgn(wTO(xi, xs)) _� sgn(A(xi, xs)):
more abstract relations, then sightly more concrete 5. set w &lt;-- w + A(xi, xs)O(xi, xs)
relations. 6. set w &lt; --w + w, n &lt; --n + 1
We define a total of 68 features to be used to train 7. return w/n.
our machine learning system to compute node-node Table 1: Perceptron Training for Node Matching.
(more specifically, subgraph-subgraph) matches. Of
these, 36 are based upon the semantic similarity
31
of four subgraphs defined by N�0.. . All eight
WordNet-based similarity measures listed in Sec-
tion 3.3 plus the LSA model are used to produce
these features. The remaining 32 features are lexico-
syntactic features3 defined only for Nx3 and are de-
scribed in more detail in Table 2.
We use O(xi, xs) to denote the feature vector as-
sociated with a pair of nodes (xi, xs), where xi is
a node from the instructor answer Ai and xs is a
node from the student answer As. A matching score
can then be computed for any pair (xi, xs) E Ai x
As through a linear scoring function f(xi, xs) _
wTO(xi,xs). In order to learn the parameter vec-
tor w, we use the averaged version of the percep-
tron algorithm (Freund and Schapire, 1999; Collins,
2002).
As training data, we randomly select a subset of
the student answers in such a way that our set was
roughly balanced between good scores, mediocre
scores, and poor scores. We then manually annotate
each node pair (xi, xs) as matching, i.e. A(xi, xs) _
+1, or not matching, i.e. A(xi, xs) _ −1. Overall,
32 student answers in response to 21 questions with
a total of 7303 node pairs (656 matches, 6647 non-
matches) are manually annotated. The pseudocode
for the learning algorithm is shown in Table 1. Af-
ter training the perceptron, these 32 student answers
are removed from the dataset, not used as training
further along in the pipeline, and are not included in
the final results. After training for 50 epochs,4 the
matching score f(xi, xs) is calculated (and cached)
for each node-node pair across all student answers
for all assignments.
3.2 Graph to Graph Alignment
Once a score has been computed for each node-node
pair across all student/instructor answer pairs, we at-
tempt to find an optimal alignment for the answer
pair. We begin with a bipartite graph where each
node in the student answer is represented by a node
on the left side of the bipartite graph and each node
in the instructor answer is represented by a node
on the right side. The score associated with each
edge is the score computed for each node-node pair
in the previous stage. The bipartite graph is then
augmented by adding dummy nodes to both sides
which are allowed to match any node with a score of
zero. An optimal alignment between the two graphs
is then computed efficiently using the Hungarian al-
gorithm. Note that this results in an optimal match-
ing, not a mapping, so that an individual node is as-
sociated with at most one node in the other answer.
At this stage we also compute several alignment-
based scores by applying various transformations to
the input graphs, the node matching function, and
the alignment score itself.
The first and simplest transformation involves the
normalization of the alignment score. While there
are several possible ways to normalize a matching
such that longer answers do not unjustly receive
higher scores, we opted to simply divide the total
alignment score by the number of nodes in the in-
structor answer.
The second transformation scales the node match-
ing score by multiplying it with the idf5 of the in-
structor answer node, i.e., replace f(xi, xs) with
idf(xi) * f(xi, xs).
The third transformation relies upon a certain
real-world intuition associated with grading student
3Note that synonyms include negated antonyms (and vice
versa). Hypernymy and hyponymy are restricted to at most
two steps).
</bodyText>
<table confidence="0.9924575">
4This value was chosen arbitrarily and was not tuned in anyway
5Inverse document frequency, as computed from the British Na-
tional Corpus (BNC)
755
Name Type # features Description
RootMatch binary 5 Is a ROOT node matched to: ROOT, N, V, JJ, or Other
Lexical binary 3 Exact match, Stemmed match, close Levenshtein match
POSMatch binary 2 Exact POS match, Coarse POS match
POSPairs binary 8 Specific X-Y POS matches found
Ontological binary 4 WordNet relationships: synonymy, antonymy, hypernymy, hyponymy
RoleBased binary 3 Has as a child - subject, object, verb
VerbsSubject binary 3 Both are verbs and neither, one, or both have a subject child
VerbsObject binary 3 Both are verbs and neither, one, or both have an object child
Semantic real 36 Nine semantic measures across four subgraphs each
Bias constant 1 A value of 1 for all vectors
Total 68
</table>
<tableCaption confidence="0.999737">
Table 2: Subtree matching features used to train the perceptron
</tableCaption>
<bodyText confidence="0.997171272727273">
answers – repeating words in the question is easy
and is not necessarily an indication of student under-
standing. With this in mind, we remove any words
in the question from both the instructor answer and
the student answer.
In all, the application of the three transforma-
tions leads to eight different transform combina-
tions, and therefore eight different alignment scores.
For a given answer pair (Ai, As), we assemble the
eight graph alignment scores into a feature vector
OG(Ai, As).
</bodyText>
<subsectionHeader confidence="0.999514">
3.3 Lexical Semantic Similarity
</subsectionHeader>
<bodyText confidence="0.999971418604651">
Haghighi et al. (2005), working on the entailment
detection problem, point out that finding a good
alignment is not sufficient to determine that the
aligned texts are in fact entailing. For instance, two
identical sentences in which an adjective from one is
replaced by its antonym will have very similar struc-
tures (which indicates a good alignment). However,
the sentences will have opposite meanings. Further
information is necessary to arrive at an appropriate
score.
In order to address this, we combine the graph
alignment scores, which encode syntactic knowl-
edge, with the scores obtained from semantic sim-
ilarity measures.
Following Mihalcea et al. (2006) and Mohler
and Mihalcea (2009), we use eight knowledge-
based measures of semantic similarity: shortest path
[PATH], Leacock &amp; Chodorow (1998) [LCH], Lesk
(1986), Wu &amp; Palmer(1994) [WUP], Resnik (1995)
[RES], Lin (1998), Jiang &amp; Conrath (1997) [JCN],
Hirst &amp; St. Onge (1998) [HSO], and two corpus-
based measures: Latent Semantic Analysis [LSA]
(Landauer and Dumais, 1997) and Explicit Seman-
tic Analysis [ESA] (Gabrilovich and Markovitch,
2007).
Briefly, for the knowledge-based measures, we
use the maximum semantic similarity – for each
open-class word – that can be obtained by pairing
it up with individual open-class words in the sec-
ond input text. We base our implementation on
the WordNet::Similarity package provided by Ped-
ersen et al. (2004). For the corpus-based measures,
we create a vector for each answer by summing
the vectors associated with each word in the an-
swer – ignoring stopwords. We produce a score in
the range [0..1] based upon the cosine similarity be-
tween the student and instructor answer vectors. The
LSA model used in these experiments was built by
training Infomap6 on a subset of Wikipedia articles
that contain one or more common computer science
terms. Since ESA uses Wikipedia article associa-
tions as vector features, it was trained using a full
Wikipedia dump.
</bodyText>
<subsectionHeader confidence="0.99882">
3.4 Answer Ranking and Grading
</subsectionHeader>
<bodyText confidence="0.999941153846154">
We combine the alignment scores OG(Ai, As) with
the scores OB(Ai, As) from the lexical seman-
tic similarity measures into a single feature vector
O(Ai, As) = [OG(Ai, As)|OB(Ai, As)]. The fea-
ture vector OG(Ai, As) contains the eight alignment
scores found by applying the three transformations
in the graph alignment stage. The feature vector
OB(Ai, As) consists of eleven semantic features –
the eight knowledge-based features plus LSA, ESA
and a vector consisting only of tf*idf weights – both
with and without question demoting. Thus, the en-
tire feature vector O(Ai, As) contains a total of 30
features.
</bodyText>
<footnote confidence="0.984738">
6http://Infomap-nlp.sourceforge.net/
</footnote>
<page confidence="0.995829">
756
</page>
<bodyText confidence="0.999521913043478">
An input pair (Ai, A3) is then associated with a
grade g(Ai, A3) = uTψ(Ai, A3) computed as a lin-
ear combination of features. The weight vector u is
trained to optimize performance in two scenarios:
Regression: An SVM model for regression (SVR)
is trained using as target function the grades as-
signed by the instructors. We use the libSVM 7 im-
plementation of SVR, with tuned parameters.
Ranking: An SVM model for ranking (SVMRank)
is trained using as ranking pairs all pairs of stu-
dent answers (A3, At) such that grade(Ai, A3) &gt;
grade(Ai, At), where Ai is the corresponding in-
structor answer. We use the SVMLight 8 implemen-
tation of SVMRank with tuned parameters.
In both cases, the parameters are tuned using a
grid-search. At each grid point, the training data is
partitioned into 5 folds which are used to train a tem-
porary SVM model with the given parameters. The
regression passage selects the grid point with the
minimal mean square error (MSE), and the SVM-
Rank package tries to minimize the number of dis-
cordant pairs. The parameters found are then used to
score the test set – a set not used in the grid training.
</bodyText>
<subsectionHeader confidence="0.719776">
3.5 Isotonic Regression
</subsectionHeader>
<bodyText confidence="0.999923769230769">
Since the end result of any grading system is to give
a student feedback on their answers, we need to en-
sure that the system’s final score has some mean-
ing. With this in mind, we use isotonic regression
(Zadrozny and Elkan, 2002) to convert the system
scores onto the same [0..5] scale used by the an-
notators. This has the added benefit of making the
system output more directly related to the annotated
grade, which makes it possible to report root mean
square error in addition to correlation. We train the
isotonic regression model on each type of system
output (i.e., alignment scores, SVM output, BOW
scores).
</bodyText>
<sectionHeader confidence="0.994298" genericHeader="method">
4 Data Set
</sectionHeader>
<bodyText confidence="0.9999582">
To evaluate our method for short answer grading,
we created a data set of questions from introductory
computer science assignments with answers pro-
vided by a class of undergraduate students. The as-
signments were administered as part of a Data Struc-
</bodyText>
<footnote confidence="0.999781">
7http://www.csie.ntu.edu.tw/˜cjlin/libsvm/
8http://svmlight.joachims.org/
</footnote>
<bodyText confidence="0.9996852">
tures course at the University of North Texas. For
each assignment, the student answers were collected
via an online learning environment.
The students submitted answers to 80 questions
spread across ten assignments and two examina-
tions.9 Table 3 shows two question-answer pairs
with three sample student answers each. Thirty-one
students were enrolled in the class and submitted an-
swers to these assignments. The data set we work
with consists of a total of 2273 student answers. This
is less than the expected 31 × 80 = 2480 as some
students did not submit answers for a few assign-
ments. In addition, the student answers used to train
the perceptron are removed from the pipeline after
the perceptron training stage.
The answers were independently graded by two
human judges, using an integer scale from 0 (com-
pletely incorrect) to 5 (perfect answer). Both human
judges were graduate students in the computer sci-
ence department; one (grader1) was the teaching as-
sistant assigned to the Data Structures class, while
the other (grader2) is one of the authors of this pa-
per. We treat the average grade of the two annotators
as the gold standard against which we compare our
system output.
</bodyText>
<table confidence="0.999185428571429">
Difference Examples % of examples
0 1294 57.7%
1 514 22.9%
2 231 10.3%
3 123 5.5%
4 70 3.1%
5 9 0.4%
</table>
<tableCaption confidence="0.999454">
Table 4: Annotator Analysis
</tableCaption>
<bodyText confidence="0.999926181818182">
The annotators were given no explicit instructions
on how to assign grades other than the [0..5] scale.
Both annotators gave the same grade 57.7% of the
time and gave a grade only 1 point apart 22.9% of
the time. The full breakdown can be seen in Table
4. In addition, an analysis of the grading patterns
indicate that the two graders operated off of differ-
ent grading policies where one grader (grader1) was
more generous than the other. In fact, when the two
differed, grader1 gave the higher grade 76.6% of the
time. The average grade given by grader1 is 4.43,
</bodyText>
<footnote confidence="0.7516285">
9Note that this is an expanded version of the dataset used by
Mohler and Mihalcea (2009)
</footnote>
<page confidence="0.989312">
757
</page>
<table confidence="0.685229428571429">
Sample questions, correct answers, and student answers Grades
Question: What is the role of a prototype program in problem solving?
Correct answer: To simulate the behavior of portions of the desired software product.
Student answer 1: A prototype program is used in problem solving to collect data for the problem. 1, 2
Student answer 2: It simulates the behavior of portions of the desired software product. 5, 5
Student answer 3: To find problem and errors in a program before it is finalized. 2, 2
Question: What are the main advantages associated with object-oriented programming?
Correct answer: Abstraction and reusability.
Student answer 1: They make it easier to reuse and adapt previously written code and they separate complex
programs into smaller, easier to understand classes. 5, 4
Student answer 2: Object oriented programming allows programmers to use an object with classes that can be
changed and manipulated while not affecting the entire object at once. 1, 1
Student answer 3: Reusable components, Extensibility, Maintainability, it reduces large problems into smaller
more manageable problems. 4, 4
</table>
<tableCaption confidence="0.999299">
Table 3: A sample question with short answers provided by students and the grades assigned by the two human judges
</tableCaption>
<bodyText confidence="0.99979125">
while the average grade given by grader2 is 3.94.
The dataset is biased towards correct answers. We
believe all of these issues correctly mirror real-world
issues associated with the task of grading.
</bodyText>
<sectionHeader confidence="0.999949" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999970533333333">
We independently test two components of our over-
all grading system: the node alignment detection
scores found by training the perceptron, and the
overall grades produced in the final stage. For the
alignment detection, we report the precision, recall,
and F-measure associated with correctly detecting
matches. For the grading stage, we report a single
Pearson’s correlation coefficient tracking the anno-
tator grades (average of the two annotators) and the
output score of each system. In addition, we re-
port the Root Mean Square Error (RMSE) for the
full dataset as well as the median RMSE across each
individual question. This is to give an indication of
the performance of the system for grading a single
question in isolation.10
</bodyText>
<subsectionHeader confidence="0.986201">
5.1 Perceptron Alignment
</subsectionHeader>
<bodyText confidence="0.998110458333333">
For the purpose of this experiment, the scores as-
sociated with a given node-node matching are con-
verted into a simple yes/no matching decision where
positive scores are considered a match and negative
10We initially intended to report an aggregate of question-level
Pearson correlation results, but discovered that the dataset
contained one question for which each student received full
points – leaving the correlation undefined. We believe that
this casts some doubt on the applicability of Pearson’s (or
Spearman’s) correlation coefficient for the short answer grad-
ing task. We have retained its use here alongside RMSE for
ease of comparison.
scores a non-match. The threshold weight learned
from the bias feature strongly influences the point
at which real scores change from non-matches to
matches, and given the threshold weight learned by
the algorithm, we find an F-measure of 0.72, with
precision(P) = 0.85 and recall(R) = 0.62. However,
as the perceptron is designed to minimize error rate,
this may not reflect an optimal objective when seek-
ing to detect matches. By manually varying the
threshold, we find a maximum F-measure of 0.76,
with P=0.79 and R=0.74. Figure 2 shows the full
precision-recall curve with the F-measure overlaid.
</bodyText>
<figure confidence="0.829934">
0 0.2 0.4 0.6 0.8 1
Recall
</figure>
<figureCaption confidence="0.9915655">
Figure 2: Precision, recall, and F-measure on node-level
match detection
</figureCaption>
<subsectionHeader confidence="0.999118">
5.2 Question Demoting
</subsectionHeader>
<bodyText confidence="0.9999584">
One surprise while building this system was the con-
sistency with which the novel technique of question
demoting improved scores for the BOW similarity
measures. With this relatively minor change the av-
erage correlation between the BOW methods’ sim-
</bodyText>
<figure confidence="0.988273444444444">
Score
0.8
0.6
0.4
0.2
0
Precision
F-Measure
Threshold
</figure>
<page confidence="0.994173">
758
</page>
<bodyText confidence="0.999666733333333">
ilarity scores and the student grades improved by
up to 0.046 with an average improvement of 0.019
across all eleven semantic features. Table 5 shows
the results of applying question demoting to our
semantic features. When comparing scores using
RMSE, the difference is less consistent, yielding an
average improvement of 0.002. However, for one
measure (tf*idf), the improvement is 0.063 which
brings its RMSE score close to the lowest of all
BOW metrics. The reasons for this are not entirely
clear. As a baseline, we include here the results of
assigning the average grade (as determined on the
training data) for each question. The average grade
was chosen as it minimizes the RMSE on the train-
ing data.
</bodyText>
<table confidence="0.999397230769231">
p w/ QD RMSE w/ QD Med. RMSE w/ QD
Lesk 0.450 0.462 1.034 1.050 0.930 0.919
JCN 0.443 0.461 1.022 1.026 0.954 0.923
HSO 0.441 0.456 1.036 1.034 0.966 0.935
PATH 0.436 0.457 1.029 1.030 0.940 0.918
RES 0.409 0.431 1.045 1.035 0.996 0.941
Lin 0.382 0.407 1.069 1.056 0.981 0.949
LCH 0.367 0.387 1.068 1.069 0.986 0.958
WUP 0.325 0.343 1.090 1.086 1.027 0.977
ESA 0.395 0.401 1.031 1.086 0.990 0.955
LSA 0.328 0.335 1.065 1.061 0.951 1.000
tf*idf 0.281 0.327 1.085 1.022 0.991 0.918
Avg.grade 1.097 1.097 0.973 0.973
</table>
<tableCaption confidence="0.923748">
Table 5: BOW Features with Question Demoting (QD).
Pearson’s correlation, root mean square error (RMSE),
and median RMSE for all individual questions.
</tableCaption>
<subsectionHeader confidence="0.998245">
5.3 Alignment Score Grading
</subsectionHeader>
<bodyText confidence="0.9996344">
Before applying any machine learning techniques,
we first test the quality of the eight graph alignment
features OG(Ai, A3) independently. Results indicate
that the basic alignment score performs comparably
to most BOW approaches. The introduction of idf
weighting seems to degrade performance somewhat,
while introducing question demoting causes the cor-
relation with the grader to increase while increasing
RMSE somewhat. The four normalized components
of OG(Ai, A3) are reported in Table 6.
</bodyText>
<subsectionHeader confidence="0.992632">
5.4 SVM Score Grading
</subsectionHeader>
<bodyText confidence="0.9997742">
The SVM components of the system are run on the
full dataset using a 12-fold cross validation. Each of
the 10 assignments and 2 examinations (for a total
of 12 folds) is scored independently with ten of the
remaining eleven used to train the machine learn-
</bodyText>
<table confidence="0.99520475">
Standard w/ IDF w/ QD w/ QD+IDF
Pearson’s p 0.411 0.277 0.428 0.291
RMSE 1.018 1.078 1.046 1.076
Median RMSE 0.910 0.970 0.919 0.992
</table>
<tableCaption confidence="0.98860775">
Table 6: Alignment Feature/Grade Correlations using
Pearson’s p. Results are also reported when inverse doc-
ument frequency weighting (IDF) and question demoting
(QD) are used.
</tableCaption>
<bodyText confidence="0.999833307692308">
ing system. For each fold, one additional fold is
held out for later use in the development of an iso-
tonic regression model (see Figure 3). The param-
eters (for cost C and tube width c) were found us-
ing a grid search. At each point on the grid, the data
from the ten training folds was partitioned into 5 sets
which were scored according to the current param-
eters. SVMRank and SVR sought to minimize the
number of discordant pairs and the mean absolute
error, respectively.
Both SVM models are trained using a linear ker-
nel.11 Results from both the SVR and the SVMRank
implementations are reported in Table 7 along with
a selection of other measures. Note that the RMSE
score is computed after performing isotonic regres-
sion on the SVMRank results, but that it was unnec-
essary to perform an isotonic regression on the SVR
results as the system was trained to produce a score
on the correct scale.
We report the results of running the systems on
three subsets of features O(Ai, A3): BOW features
OB(Ai, A3) only, alignment features OG(Ai, A3)
only, or the full feature vector (labeled “Hybrid”).
Finally, three subsets of the alignment features are
used: only unnormalized features, only normalized
features, or the full alignment feature set.
</bodyText>
<figureCaption confidence="0.991666">
Figure 3: Dependencies of the SVM/IR training stages.
</figureCaption>
<footnote confidence="0.907529666666667">
11We also ran the SVR system using quadratic and radial-basis
function (RBF) kernels, but the results did not show signifi-
cant improvement over the simpler linear kernel.
</footnote>
<figure confidence="0.985591222222222">
Features
A − Ten Folds
B C
SVM Model
IR Model
A − Ten Folds
A − Ten Folds
B C
B C
</figure>
<page confidence="0.993998">
759
</page>
<table confidence="0.9964504">
Unnormalized Normalized Both
IAA Avg. grade tf*idf Lesk BOW Align Hybrid Align Hybrid Align Hybrid
SVMRank
Pearson’s ρ 0.586 0.327 0.450 0.480 0.266 0.451 0.447 0.518 0.424 0.493
RMSE 0.659 1.097 1.022 1.050 1.042 1.093 1.038 1.015 0.998 1.029 1.021
Median RMSE 0.605 0.973 0.918 0.919 0.943 0.974 0.903 0.865 0.873 0.904 0.901
SVR
Pearson’s ρ 0.586 0.327 0.450 0.431 0.167 0.437 0.433 0.459 0.434 0.464
RMSE 0.659 1.097 1.022 1.050 0.999 1.133 0.995 1.001 0.982 1.003 0.978
Median RMSE 0.605 0.973 0.918 0.919 0.910 0.987 0.893 0.894 0.877 0.886 0.862
</table>
<tableCaption confidence="0.9852735">
Table 7: The results of the SVM models trained on the full suite of BOW measures, the alignment scores, and the
hybrid model. The terms “normalized”, “unnormalized”, and “both” indicate which subset of the 8 alignment features
were used to train the SVM model. For ease of comparison, we include in both sections the scores for the IAA, the
“Average grade” baseline, and two of the top performing BOW metrics – both with question demoting.
</tableCaption>
<sectionHeader confidence="0.996363" genericHeader="conclusions">
6 Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.999995982758621">
There are three things that we can learn from these
experiments. First, we can see from the results that
several systems appear better when evaluating on a
correlation measure like Pearson’s ρ, while others
appear better when analyzing error rate. The SVM-
Rank system seemed to outperform the SVR sys-
tem when measuring correlation, however the SVR
system clearly had a minimal RMSE. This is likely
due to the different objective function in the corre-
sponding optimization formulations: while the rank-
ing model attempts to ensure a correct ordering be-
tween the grades, the regression model seeks to min-
imize an error objective that is closer to the RMSE.
It is difficult to claim that either system is superior.
Likewise, perhaps the most unexpected result of
this work is the differing analyses of the simple
tf*idf measure – originally included only as a base-
line. Evaluating with a correlative measure yields
predictably poor results, but evaluating the error rate
indicates that it is comparable to (or better than) the
more intelligent BOW metrics. One explanation for
this result is that the skewed nature of this ”natural”
dataset favors systems that tend towards scores in
the 4 to 4.5 range. In fact, 46% of the scores output
by the tf*idf measure (after IR) were within the 4 to
4.5 range and only 6% were below 3.5. Testing on
a more balanced dataset, this tendency to fit to the
average would be less advantageous.
Second, the supervised learning techniques are
clearly able to leverage multiple BOW measures to
yield improvements over individual BOW metrics.
The correlation for the BOW-only SVM model for
SVMRank improved upon the best BOW feature
from .462 to .480. Likewise, using the BOW-only
SVM model for SVR reduces the RMSE by .022
overall compared to the best BOW feature.
Third, the rudimentary alignment features we
have introduced here are not sufficient to act as a
standalone grading system. However, even with a
very primitive attempt at alignment detection, we
show that it is possible to improve upon grade learn-
ing systems that only consider BOW features. The
correlations associated with the hybrid systems (esp.
those using normalized alignment data) frequently
show an improvement over the BOW-only SVM sys-
tems. This is true for both SVM systems when con-
sidering either evaluation metric.
Future work will concentrate on improving the
quality of the answer alignments by training a model
to directly output graph-to-graph alignments. This
learning approach will allow the use of more com-
plex alignment features, for example features that
are defined on pairs of aligned edges or on larger
subtrees in the two input graphs. Furthermore, given
an alignment, we can define several phrase-level
grammatical features such as negation, modality,
tense, person, number, or gender, which make bet-
ter use of the alignment itself.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.98781">
This work was partially supported by a National Sci-
ence Foundation CAREER award #0747340. Any
opinions, findings, and conclusions or recommenda-
tions expressed in this material are those of the au-
thors and do not necessarily reflect the views of the
National Science Foundation.
</bodyText>
<page confidence="0.99217">
760
</page>
<sectionHeader confidence="0.990227" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999890924528302">
N. Chambers, D. Cer, T. Grenager, D. Hall, C. Kid-
don, B. MacCartney, M.C. de Marneffe, D. Ramage,
E. Yeh, and C.D. Manning. 2007. Learning align-
ments and leveraging natural logic. In Proceedings
of the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing, pages 165–170. Association for
Computational Linguistics.
M. Collins. 2002. Discriminative training methods
for hidden Markov models: Theory and experiments
with perceptron algorithms. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP-02), Philadelphia, PA,
July.
I. Dagan, O. Glickman, and B. Magnini. 2005. The PAS-
CAL recognising textual entailment challenge. In Pro-
ceedings of the PASCAL Workshop.
M.C. de Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC 2006.
M.C. de Marneffe, T. Grenager, B. MacCartney, D. Cer,
D. Ramage, C. Kiddon, and C.D. Manning. 2007.
Aligning semantic graphs for textual inference and
machine reading. In Proceedings of the AAAI Spring
Symposium. Citeseer.
Y. Freund and R. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37:277–296.
E. Gabrilovich and S. Markovitch. 2007. Computing
Semantic Relatedness using Wikipedia-based Explicit
Semantic Analysis. Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence,
pages 6–12.
A.D. Haghighi, A.Y. Ng, and C.D. Manning. 2005. Ro-
bust textual inference via graph matching. In Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 387–394. Association for Computa-
tional Linguistics.
D. Higgins, J. Burstein, D. Marcu, and C. Gentile. 2004.
Evaluating multiple aspects of coherence in student
essays. In Proceedings of the annual meeting of the
North American Chapter of the Association for Com-
putational Linguistics, Boston, MA.
G. Hirst and D. St-Onge, 1998. Lexical chains as repre-
sentations of contexts for the detection and correction
of malaproprisms. The MIT Press.
J. Jiang and D. Conrath. 1997. Semantic similarity based
on corpus statistics and lexical taxonomy. In Proceed-
ings of the International Conference on Research in
Computational Linguistics, Taiwan.
T.K. Landauer and S.T. Dumais. 1997. A solution to
plato’s problem: The latent semantic analysis theory
of acquisition, induction, and representation of knowl-
edge. Psychological Review, 104.
C. Leacock and M. Chodorow. 1998. Combining lo-
cal context and WordNet sense similarity for word
sense identification. In WordNet, An Electronic Lex-
ical Database. The MIT Press.
C. Leacock and M. Chodorow. 2003. C-rater: Auto-
mated Scoring of Short-Answer Questions. Comput-
ers and the Humanities, 37(4):389–405.
M.E. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
SIGDOC Conference 1986, Toronto, June.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the 15th International
Conference on Machine Learning, Madison, WI.
B. MacCartney, T. Grenager, M.C. de Marneffe, D. Cer,
and C.D. Manning. 2006. Learning to recognize fea-
tures of valid textual entailments. In Proceedings of
the main conference on Human Language Technology
Conference of the North American Chapter of the As-
sociation of Computational Linguistics, page 48. As-
sociation for Computational Linguistics.
K.I. Malatesta, P. Wiemer-Hastings, and J. Robertson.
2002. Beyond the Short Answer Question with Re-
search Methods Tutor. In Proceedings of the Intelli-
gent Tutoring Systems Conference.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based approaches to text
semantic similarity. In Proceedings of the American
Association for Artificial Intelligence (AAAI 2006),
Boston.
T. Mitchell, T. Russell, P. Broomhead, and N. Aldridge.
2002. Towards robust computerised marking of free-
text responses. Proceedings of the 6th International
Computer Assisted Assessment (CAA) Conference.
M. Mohler and R. Mihalcea. 2009. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the European Association for Compu-
tational Linguistics (EACL 2009), Athens, Greece.
R.D. Nielsen, W. Ward, and J.H. Martin. 2009. Recog-
nizing entailment in intelligent tutoring systems. Nat-
ural Language Engineering, 15(04):479–501.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet:: Similarity-Measuring the Relatedness of
Concepts. Proceedings of the National Conference on
Artificial Intelligence, pages 1024–1025.
S.G. Pulman and J.Z. Sukkarieh. 2005. Automatic Short
Answer Marking. ACL WS Bldg Ed Apps using NLP.
R. Raina, A. Haghighi, C. Cox, J. Finkel, J. Michels,
K. Toutanova, B. MacCartney, M.C. de Marneffe, C.D.
Manning, and A.Y. Ng. 2005. Robust textual infer-
ence using diverse knowledge sources. Recognizing
Textual Entailment, page 57.
</reference>
<page confidence="0.964734">
761
</page>
<reference confidence="0.999696566666666">
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity. In Proceedings of the 14th In-
ternational Joint Conference on Artificial Intelligence,
Montreal, Canada.
V. Rus, A. Graesser, and K. Desai. 2007. Lexico-
syntactic subsumption for textual entailment. Recent
Advances in Natural Language Processing IV: Se-
lected Papers from RANLP 2005, page 187.
J.Z. Sukkarieh, S.G. Pulman, and N. Raikes. 2004. Auto-
Marking 2: An Update on the UCLES-Oxford Univer-
sity research into using Computational Linguistics to
Score Short, Free Text Responses. International Asso-
ciation of Educational Assessment, Philadephia.
P. Wiemer-Hastings, K. Wiemer-Hastings, and
A. Graesser. 1999. Improving an intelligent tu-
tor’s comprehension of students with Latent Semantic
Analysis. Artificial Intelligence in Education, pages
535–542.
P. Wiemer-Hastings, E. Arnott, and D. Allbritton. 2005.
Initial results and mixed directions for research meth-
ods tutor. In AIED2005 - Supplementary Proceedings
of the 12th International Conference on Artificial In-
telligence in Education, Amsterdam.
Z. Wu and M. Palmer. 1994. Verb semantics and lexical
selection. In Proceedings of the 32nd Annual Meeting
of the Association for Computational Linguistics, Las
Cruces, New Mexico.
B. Zadrozny and C. Elkan. 2002. Transforming classifier
scores into accurate multiclass probability estimates.
Edmonton, Alberta.
</reference>
<page confidence="0.996647">
762
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.422875">
<title confidence="0.999759">Learning to Grade Short Answer Questions using Semantic Measures and Dependency Graph Alignments</title>
<author confidence="0.996742">Michael</author>
<affiliation confidence="0.995887666666667">Dept. of Computer University of North Denton,</affiliation>
<email confidence="0.999282">mgm0038@unt.edu</email>
<author confidence="0.867496">Razvan</author>
<affiliation confidence="0.865568333333333">School of Ohio Athens,</affiliation>
<email confidence="0.999817">bunescu@ohio.edu</email>
<author confidence="0.943333">Rada</author>
<affiliation confidence="0.953407">Dept. of Computer University of North Denton,</affiliation>
<email confidence="0.999927">rada@cs.unt.edu</email>
<abstract confidence="0.998668230769231">In this work we address the task of computerassisted assessment of short student answers. We combine several graph alignment features with lexical semantic similarity measures using machine learning techniques and show that the student answers can be more accurately graded than if the semantic measures were used in isolation. We also present a first attempt to align the dependency graphs of the student and the instructor answers in order to make use of a structural component in the automatic grading of student answers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Chambers</author>
<author>D Cer</author>
<author>T Grenager</author>
<author>D Hall</author>
<author>C Kiddon</author>
<author>B MacCartney</author>
<author>M C de Marneffe</author>
<author>D Ramage</author>
<author>E Yeh</author>
<author>C D Manning</author>
</authors>
<title>Learning alignments and leveraging natural logic.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>165--170</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Chambers, Cer, Grenager, Hall, Kiddon, MacCartney, de Marneffe, Ramage, Yeh, Manning, 2007</marker>
<rawString>N. Chambers, D. Cer, T. Grenager, D. Hall, C. Kiddon, B. MacCartney, M.C. de Marneffe, D. Ramage, E. Yeh, and C.D. Manning. 2007. Learning alignments and leveraging natural logic. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 165–170. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP-02),</booktitle>
<location>Philadelphia, PA,</location>
<contexts>
<context position="13291" citStr="Collins, 2002" startWordPosition="2152" endWordPosition="2153">model are used to produce these features. The remaining 32 features are lexicosyntactic features3 defined only for Nx3 and are described in more detail in Table 2. We use O(xi, xs) to denote the feature vector associated with a pair of nodes (xi, xs), where xi is a node from the instructor answer Ai and xs is a node from the student answer As. A matching score can then be computed for any pair (xi, xs) E Ai x As through a linear scoring function f(xi, xs) _ wTO(xi,xs). In order to learn the parameter vector w, we use the averaged version of the perceptron algorithm (Freund and Schapire, 1999; Collins, 2002). As training data, we randomly select a subset of the student answers in such a way that our set was roughly balanced between good scores, mediocre scores, and poor scores. We then manually annotate each node pair (xi, xs) as matching, i.e. A(xi, xs) _ +1, or not matching, i.e. A(xi, xs) _ −1. Overall, 32 student answers in response to 21 questions with a total of 7303 node pairs (656 matches, 6647 nonmatches) are manually annotated. The pseudocode for the learning algorithm is shown in Table 1. After training the perceptron, these 32 student answers are removed from the dataset, not used as </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP-02), Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>B Magnini</author>
</authors>
<title>The PASCAL recognising textual entailment challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Workshop.</booktitle>
<contexts>
<context position="4997" citStr="Dagan et al., 2005" startWordPosition="773" endWordPosition="776">ther or not the answer was contradictory. The starting with a set of keywords and synonyms and system uses a decision tree trained on part-of-speech searching through windows of a text for new pat- tags, dependency types, word count, and other featerns. A later implementation of the Oxford-UCLES tures to attempt to learn how best to classify an ansystem (Pulman and Sukkarieh, 2005) compares swer/facet pair. several machine learning techniques, including in- Closely related to the task of short answer grading ductive logic programming, decision tree learning, is the task of textual entailment (Dagan et al., 2005), and Bayesian learning, to the earlier pattern match- which targets the identification of a directional ining approach, with encouraging results. ferential relation between texts. Given a pair of two C-Rater (Leacock and Chodorow, 2003) matches texts as input, typically referred to as text and hythe syntactical features of a student response (i.e., pothesis, a textual entailment system automatically subject, object, and verb) to that of a set of correct finds if the hypothesis is entailed by the text. responses. This method specifically disregards the In particular, the entailment-related wor</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>I. Dagan, O. Glickman, and B. Magnini. 2005. The PASCAL recognising textual entailment challenge. In Proceedings of the PASCAL Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C de Marneffe</author>
<author>B MacCartney</author>
<author>C D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In LREC</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>M.C. de Marneffe, B. MacCartney, and C.D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In LREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C de Marneffe</author>
<author>T Grenager</author>
<author>B MacCartney</author>
<author>D Cer</author>
<author>D Ramage</author>
<author>C Kiddon</author>
<author>C D Manning</author>
</authors>
<title>Aligning semantic graphs for textual inference and machine reading.</title>
<date>2007</date>
<booktitle>In Proceedings of the AAAI</booktitle>
<publisher>Spring Symposium. Citeseer.</publisher>
<marker>de Marneffe, Grenager, MacCartney, Cer, Ramage, Kiddon, Manning, 2007</marker>
<rawString>M.C. de Marneffe, T. Grenager, B. MacCartney, D. Cer, D. Ramage, C. Kiddon, and C.D. Manning. 2007. Aligning semantic graphs for textual inference and machine reading. In Proceedings of the AAAI Spring Symposium. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>37--277</pages>
<contexts>
<context position="13275" citStr="Freund and Schapire, 1999" startWordPosition="2148" endWordPosition="2151">n Section 3.3 plus the LSA model are used to produce these features. The remaining 32 features are lexicosyntactic features3 defined only for Nx3 and are described in more detail in Table 2. We use O(xi, xs) to denote the feature vector associated with a pair of nodes (xi, xs), where xi is a node from the instructor answer Ai and xs is a node from the student answer As. A matching score can then be computed for any pair (xi, xs) E Ai x As through a linear scoring function f(xi, xs) _ wTO(xi,xs). In order to learn the parameter vector w, we use the averaged version of the perceptron algorithm (Freund and Schapire, 1999; Collins, 2002). As training data, we randomly select a subset of the student answers in such a way that our set was roughly balanced between good scores, mediocre scores, and poor scores. We then manually annotate each node pair (xi, xs) as matching, i.e. A(xi, xs) _ +1, or not matching, i.e. A(xi, xs) _ −1. Overall, 32 student answers in response to 21 questions with a total of 7303 node pairs (656 matches, 6647 nonmatches) are manually annotated. The pseudocode for the learning algorithm is shown in Table 1. After training the perceptron, these 32 student answers are removed from the datas</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37:277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis.</title>
<date>2007</date>
<booktitle>Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>6--12</pages>
<contexts>
<context position="18452" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="3006" endWordPosition="3009"> In order to address this, we combine the graph alignment scores, which encode syntactic knowledge, with the scores obtained from semantic similarity measures. Following Mihalcea et al. (2006) and Mohler and Mihalcea (2009), we use eight knowledgebased measures of semantic similarity: shortest path [PATH], Leacock &amp; Chodorow (1998) [LCH], Lesk (1986), Wu &amp; Palmer(1994) [WUP], Resnik (1995) [RES], Lin (1998), Jiang &amp; Conrath (1997) [JCN], Hirst &amp; St. Onge (1998) [HSO], and two corpusbased measures: Latent Semantic Analysis [LSA] (Landauer and Dumais, 1997) and Explicit Semantic Analysis [ESA] (Gabrilovich and Markovitch, 2007). Briefly, for the knowledge-based measures, we use the maximum semantic similarity – for each open-class word – that can be obtained by pairing it up with individual open-class words in the second input text. We base our implementation on the WordNet::Similarity package provided by Pedersen et al. (2004). For the corpus-based measures, we create a vector for each answer by summing the vectors associated with each word in the answer – ignoring stopwords. We produce a score in the range [0..1] based upon the cosine similarity between the student and instructor answer vectors. The LSA model used</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>E. Gabrilovich and S. Markovitch. 2007. Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis. Proceedings of the 20th International Joint Conference on Artificial Intelligence, pages 6–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A D Haghighi</author>
<author>A Y Ng</author>
<author>C D Manning</author>
</authors>
<title>Robust textual inference via graph matching.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>387--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5800" citStr="Haghighi et al. (2005)" startWordPosition="900" endWordPosition="903">Given a pair of two C-Rater (Leacock and Chodorow, 2003) matches texts as input, typically referred to as text and hythe syntactical features of a student response (i.e., pothesis, a textual entailment system automatically subject, object, and verb) to that of a set of correct finds if the hypothesis is entailed by the text. responses. This method specifically disregards the In particular, the entailment-related works that are BOW approach to take into account the difference most similar to our own are the graph matching techbetween “dog bites man” and “man bites dog” while niques proposed by Haghighi et al. (2005) and Rus still trying to detect changes in voice (i.e., “the man et al. (2007). Both input texts are converted into a was bitten by the dog”). graph by using the dependency relations obtained Another short answer grading system, AutoTutor from a parser. Next, a matching score is calculated, (Wiemer-Hastings et al., 1999), has been designed by combining separate vertex- and edge-matching as an immersive tutoring environment with a graph- scores. The vertex matching functions use wordical “talking head” and speech recognition to im- level lexical and semantic features to determine the prove the </context>
<context position="17370" citStr="Haghighi et al. (2005)" startWordPosition="2839" endWordPosition="2842">vectors Total 68 Table 2: Subtree matching features used to train the perceptron answers – repeating words in the question is easy and is not necessarily an indication of student understanding. With this in mind, we remove any words in the question from both the instructor answer and the student answer. In all, the application of the three transformations leads to eight different transform combinations, and therefore eight different alignment scores. For a given answer pair (Ai, As), we assemble the eight graph alignment scores into a feature vector OG(Ai, As). 3.3 Lexical Semantic Similarity Haghighi et al. (2005), working on the entailment detection problem, point out that finding a good alignment is not sufficient to determine that the aligned texts are in fact entailing. For instance, two identical sentences in which an adjective from one is replaced by its antonym will have very similar structures (which indicates a good alignment). However, the sentences will have opposite meanings. Further information is necessary to arrive at an appropriate score. In order to address this, we combine the graph alignment scores, which encode syntactic knowledge, with the scores obtained from semantic similarity m</context>
</contexts>
<marker>Haghighi, Ng, Manning, 2005</marker>
<rawString>A.D. Haghighi, A.Y. Ng, and C.D. Manning. 2005. Robust textual inference via graph matching. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 387–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Higgins</author>
<author>J Burstein</author>
<author>D Marcu</author>
<author>C Gentile</author>
</authors>
<title>Evaluating multiple aspects of coherence in student essays.</title>
<date>2004</date>
<booktitle>In Proceedings of the annual meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="2077" citStr="Higgins et al., 2004" startWordPosition="322" endWordPosition="325">vailable. In these instances, students still need some assessment of their knowledge of the subject, and so, we must turn to computerassisted assessment (CAA). While some forms of CAA do not require sophisticated text understanding (e.g., multiple choice or true/false questions can be easily graded by a system if the correct solution is available), there are also student answers made up of free text that may require textual analysis. Research to date has concentrated on two subtasks of CAA: grading essay responses, which includes checking the style, grammaticality, and coherence of the essay (Higgins et al., 2004), and the assessment of short student answers (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Mohler and Mihalcea, 2009), which is the focus of this work. An automatic short answer grading system is one that automatically assigns a grade to an answer provided by a student, usually by comparing it to one or more correct answers. Note that this is different from the related tasks of paraphrase detection and textual entailment, since a common requirement in student answer grading is to provide a grade on a certain scale rather than make a simple yes/no decision. In this paper, we explore</context>
</contexts>
<marker>Higgins, Burstein, Marcu, Gentile, 2004</marker>
<rawString>D. Higgins, J. Burstein, D. Marcu, and C. Gentile. 2004. Evaluating multiple aspects of coherence in student essays. In Proceedings of the annual meeting of the North American Chapter of the Association for Computational Linguistics, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
<author>D St-Onge</author>
</authors>
<title>Lexical chains as representations of contexts for the detection and correction of malaproprisms.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>G. Hirst and D. St-Onge, 1998. Lexical chains as representations of contexts for the detection and correction of malaproprisms. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jiang</author>
<author>D Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Conference on Research in Computational Linguistics,</booktitle>
<contexts>
<context position="18253" citStr="Jiang &amp; Conrath (1997)" startWordPosition="2976" endWordPosition="2979">ill have very similar structures (which indicates a good alignment). However, the sentences will have opposite meanings. Further information is necessary to arrive at an appropriate score. In order to address this, we combine the graph alignment scores, which encode syntactic knowledge, with the scores obtained from semantic similarity measures. Following Mihalcea et al. (2006) and Mohler and Mihalcea (2009), we use eight knowledgebased measures of semantic similarity: shortest path [PATH], Leacock &amp; Chodorow (1998) [LCH], Lesk (1986), Wu &amp; Palmer(1994) [WUP], Resnik (1995) [RES], Lin (1998), Jiang &amp; Conrath (1997) [JCN], Hirst &amp; St. Onge (1998) [HSO], and two corpusbased measures: Latent Semantic Analysis [LSA] (Landauer and Dumais, 1997) and Explicit Semantic Analysis [ESA] (Gabrilovich and Markovitch, 2007). Briefly, for the knowledge-based measures, we use the maximum semantic similarity – for each open-class word – that can be obtained by pairing it up with individual open-class words in the second input text. We base our implementation on the WordNet::Similarity package provided by Pedersen et al. (2004). For the corpus-based measures, we create a vector for each answer by summing the vectors asso</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>J. Jiang and D. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the International Conference on Research in Computational Linguistics, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<contexts>
<context position="18380" citStr="Landauer and Dumais, 1997" startWordPosition="2996" endWordPosition="2999">rther information is necessary to arrive at an appropriate score. In order to address this, we combine the graph alignment scores, which encode syntactic knowledge, with the scores obtained from semantic similarity measures. Following Mihalcea et al. (2006) and Mohler and Mihalcea (2009), we use eight knowledgebased measures of semantic similarity: shortest path [PATH], Leacock &amp; Chodorow (1998) [LCH], Lesk (1986), Wu &amp; Palmer(1994) [WUP], Resnik (1995) [RES], Lin (1998), Jiang &amp; Conrath (1997) [JCN], Hirst &amp; St. Onge (1998) [HSO], and two corpusbased measures: Latent Semantic Analysis [LSA] (Landauer and Dumais, 1997) and Explicit Semantic Analysis [ESA] (Gabrilovich and Markovitch, 2007). Briefly, for the knowledge-based measures, we use the maximum semantic similarity – for each open-class word – that can be obtained by pairing it up with individual open-class words in the second input text. We base our implementation on the WordNet::Similarity package provided by Pedersen et al. (2004). For the corpus-based measures, we create a vector for each answer by summing the vectors associated with each word in the answer – ignoring stopwords. We produce a score in the range [0..1] based upon the cosine similari</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T.K. Landauer and S.T. Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<title>Combining local context and WordNet sense similarity for word sense identification. In WordNet, An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="18152" citStr="Leacock &amp; Chodorow (1998)" startWordPosition="2960" endWordPosition="2963">iling. For instance, two identical sentences in which an adjective from one is replaced by its antonym will have very similar structures (which indicates a good alignment). However, the sentences will have opposite meanings. Further information is necessary to arrive at an appropriate score. In order to address this, we combine the graph alignment scores, which encode syntactic knowledge, with the scores obtained from semantic similarity measures. Following Mihalcea et al. (2006) and Mohler and Mihalcea (2009), we use eight knowledgebased measures of semantic similarity: shortest path [PATH], Leacock &amp; Chodorow (1998) [LCH], Lesk (1986), Wu &amp; Palmer(1994) [WUP], Resnik (1995) [RES], Lin (1998), Jiang &amp; Conrath (1997) [JCN], Hirst &amp; St. Onge (1998) [HSO], and two corpusbased measures: Latent Semantic Analysis [LSA] (Landauer and Dumais, 1997) and Explicit Semantic Analysis [ESA] (Gabrilovich and Markovitch, 2007). Briefly, for the knowledge-based measures, we use the maximum semantic similarity – for each open-class word – that can be obtained by pairing it up with individual open-class words in the second input text. We base our implementation on the WordNet::Similarity package provided by Pedersen et al. </context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>C. Leacock and M. Chodorow. 1998. Combining local context and WordNet sense similarity for word sense identification. In WordNet, An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<date>2003</date>
<booktitle>C-rater: Automated Scoring of Short-Answer Questions. Computers and the Humanities,</booktitle>
<pages>37--4</pages>
<contexts>
<context position="2150" citStr="Leacock and Chodorow, 2003" startWordPosition="333" endWordPosition="337"> their knowledge of the subject, and so, we must turn to computerassisted assessment (CAA). While some forms of CAA do not require sophisticated text understanding (e.g., multiple choice or true/false questions can be easily graded by a system if the correct solution is available), there are also student answers made up of free text that may require textual analysis. Research to date has concentrated on two subtasks of CAA: grading essay responses, which includes checking the style, grammaticality, and coherence of the essay (Higgins et al., 2004), and the assessment of short student answers (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Mohler and Mihalcea, 2009), which is the focus of this work. An automatic short answer grading system is one that automatically assigns a grade to an answer provided by a student, usually by comparing it to one or more correct answers. Note that this is different from the related tasks of paraphrase detection and textual entailment, since a common requirement in student answer grading is to provide a grade on a certain scale rather than make a simple yes/no decision. In this paper, we explore the possibility of improving upon existing bag-of-words (BOW) approaches</context>
<context position="5234" citStr="Leacock and Chodorow, 2003" startWordPosition="808" endWordPosition="811">unt, and other featerns. A later implementation of the Oxford-UCLES tures to attempt to learn how best to classify an ansystem (Pulman and Sukkarieh, 2005) compares swer/facet pair. several machine learning techniques, including in- Closely related to the task of short answer grading ductive logic programming, decision tree learning, is the task of textual entailment (Dagan et al., 2005), and Bayesian learning, to the earlier pattern match- which targets the identification of a directional ining approach, with encouraging results. ferential relation between texts. Given a pair of two C-Rater (Leacock and Chodorow, 2003) matches texts as input, typically referred to as text and hythe syntactical features of a student response (i.e., pothesis, a textual entailment system automatically subject, object, and verb) to that of a set of correct finds if the hypothesis is entailed by the text. responses. This method specifically disregards the In particular, the entailment-related works that are BOW approach to take into account the difference most similar to our own are the graph matching techbetween “dog bites man” and “man bites dog” while niques proposed by Haghighi et al. (2005) and Rus still trying to detect ch</context>
</contexts>
<marker>Leacock, Chodorow, 2003</marker>
<rawString>C. Leacock and M. Chodorow. 2003. C-rater: Automated Scoring of Short-Answer Questions. Computers and the Humanities, 37(4):389–405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the SIGDOC Conference</booktitle>
<location>Toronto,</location>
<contexts>
<context position="18171" citStr="Lesk (1986)" startWordPosition="2965" endWordPosition="2966">l sentences in which an adjective from one is replaced by its antonym will have very similar structures (which indicates a good alignment). However, the sentences will have opposite meanings. Further information is necessary to arrive at an appropriate score. In order to address this, we combine the graph alignment scores, which encode syntactic knowledge, with the scores obtained from semantic similarity measures. Following Mihalcea et al. (2006) and Mohler and Mihalcea (2009), we use eight knowledgebased measures of semantic similarity: shortest path [PATH], Leacock &amp; Chodorow (1998) [LCH], Lesk (1986), Wu &amp; Palmer(1994) [WUP], Resnik (1995) [RES], Lin (1998), Jiang &amp; Conrath (1997) [JCN], Hirst &amp; St. Onge (1998) [HSO], and two corpusbased measures: Latent Semantic Analysis [LSA] (Landauer and Dumais, 1997) and Explicit Semantic Analysis [ESA] (Gabrilovich and Markovitch, 2007). Briefly, for the knowledge-based measures, we use the maximum semantic similarity – for each open-class word – that can be obtained by pairing it up with individual open-class words in the second input text. We base our implementation on the WordNet::Similarity package provided by Pedersen et al. (2004). For the cor</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>M.E. Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone. In Proceedings of the SIGDOC Conference 1986, Toronto, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th International Conference on Machine Learning,</booktitle>
<location>Madison, WI.</location>
<contexts>
<context position="18229" citStr="Lin (1998)" startWordPosition="2974" endWordPosition="2975">ts antonym will have very similar structures (which indicates a good alignment). However, the sentences will have opposite meanings. Further information is necessary to arrive at an appropriate score. In order to address this, we combine the graph alignment scores, which encode syntactic knowledge, with the scores obtained from semantic similarity measures. Following Mihalcea et al. (2006) and Mohler and Mihalcea (2009), we use eight knowledgebased measures of semantic similarity: shortest path [PATH], Leacock &amp; Chodorow (1998) [LCH], Lesk (1986), Wu &amp; Palmer(1994) [WUP], Resnik (1995) [RES], Lin (1998), Jiang &amp; Conrath (1997) [JCN], Hirst &amp; St. Onge (1998) [HSO], and two corpusbased measures: Latent Semantic Analysis [LSA] (Landauer and Dumais, 1997) and Explicit Semantic Analysis [ESA] (Gabrilovich and Markovitch, 2007). Briefly, for the knowledge-based measures, we use the maximum semantic similarity – for each open-class word – that can be obtained by pairing it up with individual open-class words in the second input text. We base our implementation on the WordNet::Similarity package provided by Pedersen et al. (2004). For the corpus-based measures, we create a vector for each answer by </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th International Conference on Machine Learning, Madison, WI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacCartney</author>
<author>T Grenager</author>
<author>M C de Marneffe</author>
<author>D Cer</author>
<author>C D Manning</author>
</authors>
<title>Learning to recognize features of valid textual entailments.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>page</pages>
<marker>MacCartney, Grenager, de Marneffe, Cer, Manning, 2006</marker>
<rawString>B. MacCartney, T. Grenager, M.C. de Marneffe, D. Cer, and C.D. Manning. 2006. Learning to recognize features of valid textual entailments. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, page 48. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K I Malatesta</author>
<author>P Wiemer-Hastings</author>
<author>J Robertson</author>
</authors>
<title>Beyond the Short Answer Question with Research Methods Tutor.</title>
<date>2002</date>
<booktitle>In Proceedings of the Intelligent Tutoring Systems Conference.</booktitle>
<contexts>
<context position="6830" citStr="Malatesta et al., 2002" startWordPosition="1063" endWordPosition="1066">utoring environment with a graph- scores. The vertex matching functions use wordical “talking head” and speech recognition to im- level lexical and semantic features to determine the prove the overall experience for students. AutoTutor quality of the match while the the edge matching eschews the pattern-based approach entirely in favor functions take into account the types of relations and of a BOW LSA approach (Landauer and Dumais, the difference in lengths between the aligned paths. 1997). Later work on AutoTutor(Wiemer-Hastings Following the same line of work in the textual enet al., 2005; Malatesta et al., 2002) seeks to expand tailment world are (Raina et al., 2005), (MacCartney upon their BOW approach which becomes less use- et al., 2006), (de Marneffe et al., 2007), and (Chamful as causality (and thus word order) becomes more bers et al., 2007), which experiment variously with important. using diverse knowledge sources, using a perceptron A text similarity approach was taken in (Mohler to learn alignment decisions, and exploiting natural and Mihalcea, 2009), where a grade is assigned logic. based on a measure of relatedness between the stu- 3 Answer Grading System dent and the instructor answer. S</context>
</contexts>
<marker>Malatesta, Wiemer-Hastings, Robertson, 2002</marker>
<rawString>K.I. Malatesta, P. Wiemer-Hastings, and J. Robertson. 2002. Beyond the Short Answer Question with Research Methods Tutor. In Proceedings of the Intelligent Tutoring Systems Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>C Corley</author>
<author>C Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based approaches to text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the American Association for Artificial Intelligence (AAAI 2006),</booktitle>
<location>Boston.</location>
<contexts>
<context position="18011" citStr="Mihalcea et al. (2006)" startWordPosition="2939" endWordPosition="2942">tailment detection problem, point out that finding a good alignment is not sufficient to determine that the aligned texts are in fact entailing. For instance, two identical sentences in which an adjective from one is replaced by its antonym will have very similar structures (which indicates a good alignment). However, the sentences will have opposite meanings. Further information is necessary to arrive at an appropriate score. In order to address this, we combine the graph alignment scores, which encode syntactic knowledge, with the scores obtained from semantic similarity measures. Following Mihalcea et al. (2006) and Mohler and Mihalcea (2009), we use eight knowledgebased measures of semantic similarity: shortest path [PATH], Leacock &amp; Chodorow (1998) [LCH], Lesk (1986), Wu &amp; Palmer(1994) [WUP], Resnik (1995) [RES], Lin (1998), Jiang &amp; Conrath (1997) [JCN], Hirst &amp; St. Onge (1998) [HSO], and two corpusbased measures: Latent Semantic Analysis [LSA] (Landauer and Dumais, 1997) and Explicit Semantic Analysis [ESA] (Gabrilovich and Markovitch, 2007). Briefly, for the knowledge-based measures, we use the maximum semantic similarity – for each open-class word – that can be obtained by pairing it up with ind</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>R. Mihalcea, C. Corley, and C. Strapparava. 2006. Corpus-based and knowledge-based approaches to text semantic similarity. In Proceedings of the American Association for Artificial Intelligence (AAAI 2006), Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mitchell</author>
<author>T Russell</author>
<author>P Broomhead</author>
<author>N Aldridge</author>
</authors>
<title>Towards robust computerised marking of freetext responses.</title>
<date>2002</date>
<booktitle>Proceedings of the 6th International Computer Assisted Assessment (CAA) Conference.</booktitle>
<contexts>
<context position="3753" citStr="Mitchell et al., 2002" startWordPosition="583" endWordPosition="586">stions. First, to what extent can machine learning be leveraged to improve upon existing approaches to short answer grading. Second, does the dependency parse structure of a text provide clues that can be exploited to improve upon existing BOW methodologies? 752 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 752–762, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Related Work grades. Several state-of-the-art short answer grading sys- In the dependency-based classification compotems (Sukkarieh et al., 2004; Mitchell et al., 2002) nent of the Intelligent Tutoring System (Nielsen et require manually crafted patterns which, if matched, al., 2009), instructor answers are parsed, enhanced, indicate that a question has been answered correctly. and manually converted into a set of content-bearing If an annotated corpus is available, these patterns dependency triples or facets. For each facet of the can be supplemented by learning additional pat- instructor answer each student’s answer is labelled terns semi-automatically. The Oxford-UCLES sys- to indicate whether it has addressed that facet and tem (Sukkarieh et al., 2004) b</context>
</contexts>
<marker>Mitchell, Russell, Broomhead, Aldridge, 2002</marker>
<rawString>T. Mitchell, T. Russell, P. Broomhead, and N. Aldridge. 2002. Towards robust computerised marking of freetext responses. Proceedings of the 6th International Computer Assisted Assessment (CAA) Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohler</author>
<author>R Mihalcea</author>
</authors>
<title>Text-to-text semantic similarity for automatic short answer grading.</title>
<date>2009</date>
<booktitle>In Proceedings of the European Association for Computational Linguistics (EACL</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="2206" citStr="Mohler and Mihalcea, 2009" startWordPosition="342" endWordPosition="345"> computerassisted assessment (CAA). While some forms of CAA do not require sophisticated text understanding (e.g., multiple choice or true/false questions can be easily graded by a system if the correct solution is available), there are also student answers made up of free text that may require textual analysis. Research to date has concentrated on two subtasks of CAA: grading essay responses, which includes checking the style, grammaticality, and coherence of the essay (Higgins et al., 2004), and the assessment of short student answers (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Mohler and Mihalcea, 2009), which is the focus of this work. An automatic short answer grading system is one that automatically assigns a grade to an answer provided by a student, usually by comparing it to one or more correct answers. Note that this is different from the related tasks of paraphrase detection and textual entailment, since a common requirement in student answer grading is to provide a grade on a certain scale rather than make a simple yes/no decision. In this paper, we explore the possibility of improving upon existing bag-of-words (BOW) approaches to short answer grading by utilizing machine learning t</context>
<context position="18042" citStr="Mohler and Mihalcea (2009)" startWordPosition="2944" endWordPosition="2947"> point out that finding a good alignment is not sufficient to determine that the aligned texts are in fact entailing. For instance, two identical sentences in which an adjective from one is replaced by its antonym will have very similar structures (which indicates a good alignment). However, the sentences will have opposite meanings. Further information is necessary to arrive at an appropriate score. In order to address this, we combine the graph alignment scores, which encode syntactic knowledge, with the scores obtained from semantic similarity measures. Following Mihalcea et al. (2006) and Mohler and Mihalcea (2009), we use eight knowledgebased measures of semantic similarity: shortest path [PATH], Leacock &amp; Chodorow (1998) [LCH], Lesk (1986), Wu &amp; Palmer(1994) [WUP], Resnik (1995) [RES], Lin (1998), Jiang &amp; Conrath (1997) [JCN], Hirst &amp; St. Onge (1998) [HSO], and two corpusbased measures: Latent Semantic Analysis [LSA] (Landauer and Dumais, 1997) and Explicit Semantic Analysis [ESA] (Gabrilovich and Markovitch, 2007). Briefly, for the knowledge-based measures, we use the maximum semantic similarity – for each open-class word – that can be obtained by pairing it up with individual open-class words in the</context>
<context position="24044" citStr="Mohler and Mihalcea (2009)" startWordPosition="3954" endWordPosition="3957">no explicit instructions on how to assign grades other than the [0..5] scale. Both annotators gave the same grade 57.7% of the time and gave a grade only 1 point apart 22.9% of the time. The full breakdown can be seen in Table 4. In addition, an analysis of the grading patterns indicate that the two graders operated off of different grading policies where one grader (grader1) was more generous than the other. In fact, when the two differed, grader1 gave the higher grade 76.6% of the time. The average grade given by grader1 is 4.43, 9Note that this is an expanded version of the dataset used by Mohler and Mihalcea (2009) 757 Sample questions, correct answers, and student answers Grades Question: What is the role of a prototype program in problem solving? Correct answer: To simulate the behavior of portions of the desired software product. Student answer 1: A prototype program is used in problem solving to collect data for the problem. 1, 2 Student answer 2: It simulates the behavior of portions of the desired software product. 5, 5 Student answer 3: To find problem and errors in a program before it is finalized. 2, 2 Question: What are the main advantages associated with object-oriented programming? Correct a</context>
</contexts>
<marker>Mohler, Mihalcea, 2009</marker>
<rawString>M. Mohler and R. Mihalcea. 2009. Text-to-text semantic similarity for automatic short answer grading. In Proceedings of the European Association for Computational Linguistics (EACL 2009), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R D Nielsen</author>
<author>W Ward</author>
<author>J H Martin</author>
</authors>
<title>Recognizing entailment in intelligent tutoring systems.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>04</issue>
<marker>Nielsen, Ward, Martin, 2009</marker>
<rawString>R.D. Nielsen, W. Ward, and J.H. Martin. 2009. Recognizing entailment in intelligent tutoring systems. Natural Language Engineering, 15(04):479–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>S Patwardhan</author>
<author>J Michelizzi</author>
</authors>
<title>WordNet:: Similarity-Measuring the Relatedness of Concepts.</title>
<date>2004</date>
<booktitle>Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<pages>1024--1025</pages>
<contexts>
<context position="18758" citStr="Pedersen et al. (2004)" startWordPosition="3055" endWordPosition="3059"> Chodorow (1998) [LCH], Lesk (1986), Wu &amp; Palmer(1994) [WUP], Resnik (1995) [RES], Lin (1998), Jiang &amp; Conrath (1997) [JCN], Hirst &amp; St. Onge (1998) [HSO], and two corpusbased measures: Latent Semantic Analysis [LSA] (Landauer and Dumais, 1997) and Explicit Semantic Analysis [ESA] (Gabrilovich and Markovitch, 2007). Briefly, for the knowledge-based measures, we use the maximum semantic similarity – for each open-class word – that can be obtained by pairing it up with individual open-class words in the second input text. We base our implementation on the WordNet::Similarity package provided by Pedersen et al. (2004). For the corpus-based measures, we create a vector for each answer by summing the vectors associated with each word in the answer – ignoring stopwords. We produce a score in the range [0..1] based upon the cosine similarity between the student and instructor answer vectors. The LSA model used in these experiments was built by training Infomap6 on a subset of Wikipedia articles that contain one or more common computer science terms. Since ESA uses Wikipedia article associations as vector features, it was trained using a full Wikipedia dump. 3.4 Answer Ranking and Grading We combine the alignme</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004. WordNet:: Similarity-Measuring the Relatedness of Concepts. Proceedings of the National Conference on Artificial Intelligence, pages 1024–1025.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S G Pulman</author>
<author>J Z Sukkarieh</author>
</authors>
<title>Automatic Short Answer Marking. ACL WS Bldg Ed Apps using</title>
<date>2005</date>
<pages>57</pages>
<contexts>
<context position="2178" citStr="Pulman and Sukkarieh, 2005" startWordPosition="338" endWordPosition="341">ect, and so, we must turn to computerassisted assessment (CAA). While some forms of CAA do not require sophisticated text understanding (e.g., multiple choice or true/false questions can be easily graded by a system if the correct solution is available), there are also student answers made up of free text that may require textual analysis. Research to date has concentrated on two subtasks of CAA: grading essay responses, which includes checking the style, grammaticality, and coherence of the essay (Higgins et al., 2004), and the assessment of short student answers (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Mohler and Mihalcea, 2009), which is the focus of this work. An automatic short answer grading system is one that automatically assigns a grade to an answer provided by a student, usually by comparing it to one or more correct answers. Note that this is different from the related tasks of paraphrase detection and textual entailment, since a common requirement in student answer grading is to provide a grade on a certain scale rather than make a simple yes/no decision. In this paper, we explore the possibility of improving upon existing bag-of-words (BOW) approaches to short answer grading by </context>
<context position="4762" citStr="Pulman and Sukkarieh, 2005" startWordPosition="739" endWordPosition="742">emented by learning additional pat- instructor answer each student’s answer is labelled terns semi-automatically. The Oxford-UCLES sys- to indicate whether it has addressed that facet and tem (Sukkarieh et al., 2004) bootstraps patterns by whether or not the answer was contradictory. The starting with a set of keywords and synonyms and system uses a decision tree trained on part-of-speech searching through windows of a text for new pat- tags, dependency types, word count, and other featerns. A later implementation of the Oxford-UCLES tures to attempt to learn how best to classify an ansystem (Pulman and Sukkarieh, 2005) compares swer/facet pair. several machine learning techniques, including in- Closely related to the task of short answer grading ductive logic programming, decision tree learning, is the task of textual entailment (Dagan et al., 2005), and Bayesian learning, to the earlier pattern match- which targets the identification of a directional ining approach, with encouraging results. ferential relation between texts. Given a pair of two C-Rater (Leacock and Chodorow, 2003) matches texts as input, typically referred to as text and hythe syntactical features of a student response (i.e., pothesis, a t</context>
</contexts>
<marker>Pulman, Sukkarieh, 2005</marker>
<rawString>S.G. Pulman and J.Z. Sukkarieh. 2005. Automatic Short Answer Marking. ACL WS Bldg Ed Apps using NLP. R. Raina, A. Haghighi, C. Cox, J. Finkel, J. Michels, K. Toutanova, B. MacCartney, M.C. de Marneffe, C.D. Manning, and A.Y. Ng. 2005. Robust textual inference using diverse knowledge sources. Recognizing Textual Entailment, page 57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="18211" citStr="Resnik (1995)" startWordPosition="2971" endWordPosition="2972"> one is replaced by its antonym will have very similar structures (which indicates a good alignment). However, the sentences will have opposite meanings. Further information is necessary to arrive at an appropriate score. In order to address this, we combine the graph alignment scores, which encode syntactic knowledge, with the scores obtained from semantic similarity measures. Following Mihalcea et al. (2006) and Mohler and Mihalcea (2009), we use eight knowledgebased measures of semantic similarity: shortest path [PATH], Leacock &amp; Chodorow (1998) [LCH], Lesk (1986), Wu &amp; Palmer(1994) [WUP], Resnik (1995) [RES], Lin (1998), Jiang &amp; Conrath (1997) [JCN], Hirst &amp; St. Onge (1998) [HSO], and two corpusbased measures: Latent Semantic Analysis [LSA] (Landauer and Dumais, 1997) and Explicit Semantic Analysis [ESA] (Gabrilovich and Markovitch, 2007). Briefly, for the knowledge-based measures, we use the maximum semantic similarity – for each open-class word – that can be obtained by pairing it up with individual open-class words in the second input text. We base our implementation on the WordNet::Similarity package provided by Pedersen et al. (2004). For the corpus-based measures, we create a vector f</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik. 1995. Using information content to evaluate semantic similarity. In Proceedings of the 14th International Joint Conference on Artificial Intelligence, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rus</author>
<author>A Graesser</author>
<author>K Desai</author>
</authors>
<title>Lexicosyntactic subsumption for textual entailment.</title>
<date>2007</date>
<booktitle>Recent Advances in Natural Language Processing IV: Selected Papers from RANLP 2005,</booktitle>
<pages>187</pages>
<marker>Rus, Graesser, Desai, 2007</marker>
<rawString>V. Rus, A. Graesser, and K. Desai. 2007. Lexicosyntactic subsumption for textual entailment. Recent Advances in Natural Language Processing IV: Selected Papers from RANLP 2005, page 187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Z Sukkarieh</author>
<author>S G Pulman</author>
<author>N Raikes</author>
</authors>
<title>AutoMarking 2: An Update on the UCLES-Oxford University research into using Computational Linguistics to Score Short, Free Text Responses.</title>
<date>2004</date>
<journal>International Association of Educational Assessment, Philadephia.</journal>
<contexts>
<context position="3729" citStr="Sukkarieh et al., 2004" startWordPosition="579" endWordPosition="582">ers to the following questions. First, to what extent can machine learning be leveraged to improve upon existing approaches to short answer grading. Second, does the dependency parse structure of a text provide clues that can be exploited to improve upon existing BOW methodologies? 752 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 752–762, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Related Work grades. Several state-of-the-art short answer grading sys- In the dependency-based classification compotems (Sukkarieh et al., 2004; Mitchell et al., 2002) nent of the Intelligent Tutoring System (Nielsen et require manually crafted patterns which, if matched, al., 2009), instructor answers are parsed, enhanced, indicate that a question has been answered correctly. and manually converted into a set of content-bearing If an annotated corpus is available, these patterns dependency triples or facets. For each facet of the can be supplemented by learning additional pat- instructor answer each student’s answer is labelled terns semi-automatically. The Oxford-UCLES sys- to indicate whether it has addressed that facet and tem (S</context>
</contexts>
<marker>Sukkarieh, Pulman, Raikes, 2004</marker>
<rawString>J.Z. Sukkarieh, S.G. Pulman, and N. Raikes. 2004. AutoMarking 2: An Update on the UCLES-Oxford University research into using Computational Linguistics to Score Short, Free Text Responses. International Association of Educational Assessment, Philadephia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Wiemer-Hastings</author>
<author>K Wiemer-Hastings</author>
<author>A Graesser</author>
</authors>
<title>Improving an intelligent tutor’s comprehension of students with Latent Semantic Analysis. Artificial Intelligence in Education,</title>
<date>1999</date>
<pages>535--542</pages>
<contexts>
<context position="6122" citStr="Wiemer-Hastings et al., 1999" startWordPosition="953" endWordPosition="956">y the text. responses. This method specifically disregards the In particular, the entailment-related works that are BOW approach to take into account the difference most similar to our own are the graph matching techbetween “dog bites man” and “man bites dog” while niques proposed by Haghighi et al. (2005) and Rus still trying to detect changes in voice (i.e., “the man et al. (2007). Both input texts are converted into a was bitten by the dog”). graph by using the dependency relations obtained Another short answer grading system, AutoTutor from a parser. Next, a matching score is calculated, (Wiemer-Hastings et al., 1999), has been designed by combining separate vertex- and edge-matching as an immersive tutoring environment with a graph- scores. The vertex matching functions use wordical “talking head” and speech recognition to im- level lexical and semantic features to determine the prove the overall experience for students. AutoTutor quality of the match while the the edge matching eschews the pattern-based approach entirely in favor functions take into account the types of relations and of a BOW LSA approach (Landauer and Dumais, the difference in lengths between the aligned paths. 1997). Later work on Auto</context>
</contexts>
<marker>Wiemer-Hastings, Wiemer-Hastings, Graesser, 1999</marker>
<rawString>P. Wiemer-Hastings, K. Wiemer-Hastings, and A. Graesser. 1999. Improving an intelligent tutor’s comprehension of students with Latent Semantic Analysis. Artificial Intelligence in Education, pages 535–542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Wiemer-Hastings</author>
<author>E Arnott</author>
<author>D Allbritton</author>
</authors>
<title>Initial results and mixed directions for research methods tutor.</title>
<date>2005</date>
<booktitle>In AIED2005 - Supplementary Proceedings of the 12th International Conference on Artificial Intelligence in Education,</booktitle>
<location>Amsterdam.</location>
<marker>Wiemer-Hastings, Arnott, Allbritton, 2005</marker>
<rawString>P. Wiemer-Hastings, E. Arnott, and D. Allbritton. 2005. Initial results and mixed directions for research methods tutor. In AIED2005 - Supplementary Proceedings of the 12th International Conference on Artificial Intelligence in Education, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Wu</author>
<author>M Palmer</author>
</authors>
<title>Verb semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Las Cruces, New Mexico.</location>
<marker>Wu, Palmer, 1994</marker>
<rawString>Z. Wu and M. Palmer. 1994. Verb semantics and lexical selection. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, Las Cruces, New Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Zadrozny</author>
<author>C Elkan</author>
</authors>
<title>Transforming classifier scores into accurate multiclass probability estimates.</title>
<date>2002</date>
<location>Edmonton, Alberta.</location>
<contexts>
<context position="21356" citStr="Zadrozny and Elkan, 2002" startWordPosition="3500" endWordPosition="3503">raining data is partitioned into 5 folds which are used to train a temporary SVM model with the given parameters. The regression passage selects the grid point with the minimal mean square error (MSE), and the SVMRank package tries to minimize the number of discordant pairs. The parameters found are then used to score the test set – a set not used in the grid training. 3.5 Isotonic Regression Since the end result of any grading system is to give a student feedback on their answers, we need to ensure that the system’s final score has some meaning. With this in mind, we use isotonic regression (Zadrozny and Elkan, 2002) to convert the system scores onto the same [0..5] scale used by the annotators. This has the added benefit of making the system output more directly related to the annotated grade, which makes it possible to report root mean square error in addition to correlation. We train the isotonic regression model on each type of system output (i.e., alignment scores, SVM output, BOW scores). 4 Data Set To evaluate our method for short answer grading, we created a data set of questions from introductory computer science assignments with answers provided by a class of undergraduate students. The assignme</context>
</contexts>
<marker>Zadrozny, Elkan, 2002</marker>
<rawString>B. Zadrozny and C. Elkan. 2002. Transforming classifier scores into accurate multiclass probability estimates. Edmonton, Alberta.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>