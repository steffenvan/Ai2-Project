<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<note confidence="0.617596">
The Semantics of Grammar Formalisms
Seen as Computer Languages
</note>
<bodyText confidence="0.828173333333333">
Fernando C. N. Pereira and Stuart M. Shieber
Artificial Intelligence Center
SRI International
and
Center for the Study of Language and Information
Stanford University
</bodyText>
<sectionHeader confidence="0.846089" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999963117647059">
The design, implementation, and use of grammar for-
malisms for natural language have constituted a major
branch of computational linguistics throughout its devel-
opment. By viewing grammar formalisms as just a spe-
cial case of computer languages, we can take advantage of
the machinery of denotational semantics to provide a pre-
cise specification of their meaning. Using Dana Scott&apos;s do-
main theory, we elucidate the nature of the feature systems
used in augmented phrase-structure grammar formalisms,
in particular those of recent versions of generalized phrase
structure grammar, lexical functional grammar and PATR-
II, and provide a denotational semantics for a simple gram-
mar formalism. We find that the mathematical structures
developed for this purpose contain an operation of feature
generalization, not. available in those grammar formalisms,
that can be used to give a partial account of the effect of
coordination on syntactic features.
</bodyText>
<sectionHeader confidence="0.996124" genericHeader="keywords">
1. Introduction&apos;
</sectionHeader>
<bodyText confidence="0.999963444444444">
The design, implementation, and use of grammar for-
malisms for natural language have constituted a major
branch of computational linguistics throughout its devel-
opment. however, notwithstanding the obvious superfi-
cial similarity between designing a grammar formalism and
designing a programming language, the design techniques
used for grammar formalisms have almost always fallen
short with respect to those now available for programming
language design.
Formal and computational linguists most often explain
the effect of a grammar formalism construct either by ex-
ample or through its actual operation in a particular im-
plementation. Such practices are frowned upon by most
programming-language designers; they become even more
dubious if one considers that most grammar formalisms
in use are based either on a context-free skeleton with
augmentations or on some closely related device (such as
ATNs), consequently making them obvious candidates for
</bodyText>
<footnote confidence="0.5102775">
&apos;The research reported in this paper has been made possible by a gift
from the System Development Foundation.
</footnote>
<bodyText confidence="0.99993555">
a declarative semantics2 extended in the natural way from
the declarative semantics of context-free grammars.
The last point deserves amplification. Context-free
grammars possess an obvious declarative semantics in
which nonterminals represent sets of strings and rules rep-
resent n-ary relations over strings. This is brought out by
the reinterpretation familiar from formal language theory
of context-free grammars as polynomials over concatena-
tion and set union. The grammar formalisms developed
from the definite-clause subset of first order logic are the
only others used in natural-language analysis that have
been accorded a rigorous declarative semantics—in this
case derived from the declarative semantics of logic pro-
grams 13,12,111.
Much confusion, wasted effort, and dissension have re-
stilted from this state of affairs. In the absence of a rigorous
semantics for a given grammar formalism, the user, critic,
or implementer of the formalism risks misunderstanding the
intended interpretation of a construct, and is in a poor posi-
tion to compare it to alternatives. Likewise, the inventor of
a new formalism can never be sure of how it compares with
existing ones. As an example of these difficulties, two sim-
ple changes in the implementation of the ATN formalism,
the addition of a well-formed substring table and the use
of a bottom-up parsing strategy, required a rather subtle
and unanticipated reinterpretation of the register-testing
and -setting actions, thereby imparting a different meaning
to grammars that had been developed for initial top-down
backtrack implementation 1221.
Rigorous definitions of grammar formalisms can and
should be made available. Looking at grammar formalisms
as just a special case of computer languages, we can take
advantage of the machinery of denotations! semantics 1201
to provide a precise specification of their meaning. This
approach can elucidate the structure of the data objects
manipulated by a formalism and the mathematical rela-
tionships among various formalisms, suggest new possibil-
ities for linguistic analysis (the subject matter of the for-
malisms), and establish connections between grammar for-
malisms and such other fields of research as programming-
</bodyText>
<footnote confidence="0.92619775">
2This use of the term &amp;quot;semantics&amp;quot; should not be confused with the
more common usage denoting that portion of a grammar concerned
with the meaning of object sentences. Here we are concerned with the
meaning of the metalanguage.
</footnote>
<page confidence="0.998688">
123
</page>
<bodyText confidence="0.999984378378379">
language design and theories of abstract data types. This
last point is particularly interesting because it opens up
several possibilities—among them that of imposing a type
discipline on the use of a formalism, with all the attendant
advantages of compile-time error checking, modularity, and
optimized compilation techniques for grammar rules, and
that of relating grammar formalisms to other knowledge
representation languages [1].
As a specific contribution of this study, we elucidate
the nature of the feature systems used in augmented phrase-
structure grammar formalisms, in particular those of recent
versions of generalized phrase structure grammar (GPSG)
15,151, lexical functional grammar (LFG) [2] and PATR-II
118,171; we find that the mathematical structures developed
for this purpose contain an operation of feature generaliza-
tion, not available in those grammar formalisms, that can
be used to give a partial account of the effect of coordina-
tion on syntactic features.
Just as studies in the semantics of programming lan-
guages start by giving semantics for simple languages, so
we will start with simple grammar formalisms that capture
the essence of the method without an excess of obscuring
detail. The present enterprise should be contrasted with
studies of the generative capacity of formalisms using the
techniques of formal language theory. First, a precise defini-
tion of the semantics of a formalism is a prerequisite for such
generative-capacity studies, and this is precisely what we
are trying to provide. Second, generative capacity is a very
coarse gauge: in particular, it does not distinguish among
different formalisms with the same generative capacity that
may, however, have very different semantic accounts. Fi-
nally, the tools of formal language theory are inadequate to
describe at a sufficiently abstract level formalisms that are
based on the simultaneous solution of sets of constraints
[9,101. An abstract analysis of those formalisms requires a
notion of partial information that is precisely captured by
the constructs of denotational semantics.
</bodyText>
<sectionHeader confidence="0.982821" genericHeader="introduction">
2. Denotational Semantics
</sectionHeader>
<bodyText confidence="0.999926117647059">
In broad terms, denotational semantics is the study of
the connection between programs and mathematical enti-
ties that represent their input-output relations. For such
an account to be useful, it must be compositional, in the
sense that the meaning of a program is developed from the
meanings of its parts by a fixed set of mathematical oper-
ations that correspond directly to the ways in which the
parts participate in the whole.
For the purposes of the present work, denotational se-
mantics will mean the semantic domain theory initiated
by Scott and Strachey [201. In accordance with this ap-
proach, the meanings of programming language constructs
are certain partial mappings between objects that represent
partially specified data objects or partially defined states of
computation. The essential idea is that the meaning of a
construct describes what information it adds to a partial
description of a data object or of a state of computation.
Partial descriptions are used because computations in gen-
eral may not terminate and may therefore never produce a
fully defined output, although each individual step may be
adding more and more information to a partial description
of the undeliverable output.
Domain theory is a mathematical theory of consider-
able complexity. Potential nontermination and the use of
functions as &amp;quot;first-class citizens&amp;quot; in computer languages ac-
count for a substantial fraction of that complexity. If, as is
the case in the present work, neither of those two aspects
comes into play, one may be justified in asking why such
a complex apparatus is used. Indeed, both the semantics
of context-free grammars mentioned earlier and the seman-
tics of logic grammars in general can be formulated using
elementary set theory [7,211.
However, using the more complex machinery may be
beneficial for the following reasons:
</bodyText>
<listItem confidence="0.994876533333333">
• Inherent partiality: many grammar formalisms oper-
ate in terms of constraints between elements that do
not fully specify all the possible features of an ele-
ment.
• Technical economy results that require laborious
constructions without utilizing domain theory can be
reached trivially by using standard results of the the-
ory.
• Suggestiveness: domain theory brings with it a rich
mathematical structure that suggests useful opera-
tions one might add to a grammar formalism.
• Extensibility: unlike a domain-theoretic account, a
specialized semantic account, say in terms of sets,
may not be easily extended as new constructs are
added to the formalism.
</listItem>
<sectionHeader confidence="0.6936165" genericHeader="method">
3. The Domain of Feature Struc-
tures
</sectionHeader>
<bodyText confidence="0.999527047619048">
We will start with an abstract denotational description
of a simple feature system which bears a close resemblance
to the feature systems of GPSG, LFG and PATR-ll, al-
though this similarity, because of its abstractness, may not
be apparent at first glance. Such feature systems tend to
use data structures or mathematical objects that are more
or less isomorphic to directed graphs of one sort or an-
other, or, as they are sometimes described, partial func-
tions. Just what the relation is between these two ways
of viewing things will be explained later. In general, these
graph structures are used to encode linguistic information
in the form of attribute-value pairs. Most importantly, par-
tial information is critical to the use of such systems—for
instance, in the variables of definite clause grammars [12]
and in the GPSG analysis of coordination [15]. That is, the
elements of the feature systems, called feature structures
(alternatively, feature bundles, f-structures [21, or terms)
can be partial in some sense. The partial descriptions, be-
ing in a domain of attributes and complex values, tend to be
equational in nature: some feature&apos;s value is equated with
some other value. Partial descriptions can be understood
</bodyText>
<page confidence="0.991971">
124
</page>
<bodyText confidence="0.999909076923077">
in one of two ways: either the descriptions represent sets
of fully specified elements of an underlying domain or they
are regarded as participating in a relationship of partiality
with respect to each other. We will hold to the latter view
here.
What are feature structures from this perspective?
They are repositories of information about linguistic enti-
ties. In domain-theoretic terms, the underlying domain of
feature structures F is a recursive domain of partial func-
tions from a set of labels L (features, attribute names, at-
tributes) to complex values or primitive atomic values taken
from a set C of constants. Expressed formally, we have the
domain equation
</bodyText>
<equation confidence="0.986695">
F = [L F] + C
</equation>
<bodyText confidence="0.997256666666667">
The solution of this domain equation can be understood as
a set of trees (finite or infinite) with branches labeled by
elements of L, and with other trees or constants as nodes.
The branches /1, ... ,1„, from a node n point to the values
(lm) for which the node, as a partial function, is
defined.
</bodyText>
<sectionHeader confidence="0.970102" genericHeader="method">
4. The Domain of Descriptions
</sectionHeader>
<bodyText confidence="0.980228153846154">
What the grammar formalism does is to talk about F,
not in F. That is, the grammar formalism uses a domain of
descriptions of elements of F. From an intuitive standpoint,
this is because, for any given phrase, we may know facts
about it that cannot be encoded in the partial function
associated with it.
A partial description of an element n of F will be a set
of equations that constrain the values of n on certain labels.
In general, to describe an element x E F we have equations
of the following forms:
(• • • (r(ii, )) • •)(li.) = i• • (x(41)) • •
(• • • (x(1i, )) • • -)((i) = ck
which we prefer to write as
</bodyText>
<equation confidence="0.9850585">
(16 • • • li) = (lit • • &apos;
(4, • • • l) = ck
</equation>
<bodyText confidence="0.999717181818182">
with x implicit. The terms of such equations are constants
c E C or paths (li, • • li), which we identify in what follows
with strings in L&apos;. Taken together, constants and paths
comprise the descriptors.
Using Scott&apos;s information systems approach to domain
construction [161, we can now build directly a characteriza-
tion of feature structures in terms of information-bearing
elements, equations, that engender a system complete with
notions of compatibility and partiality of information.
The information system D describing the elements of
F is defined, following Scott, as the tuple
</bodyText>
<page confidence="0.323121">
D (D, A, Con, I-) ,
</page>
<bodyText confidence="0.9936536">
where D is a set of propositions, Con is a set of finite subsets
of D, the consistent subsets, I- is an entailment relation
between elements of Con and elements of D and is a
special least informative element that gives no information
at all. We say that a subset S of D is deductively closed
if every proposition entailed by a consistent subset of S is
in S. The deductive closure S of S C D is the smallest
deductively closed subset of D that contains S.
The descriptor equations discussed earlier are the
propositions of the information system for feature structure
descriptions. Equations express constraints among feature
values in a feature structure and the entailment relation
encodes the reflexivity, symmetry, transitivity and substi-
tutivity of equality. More precisely, we say that a finite set
of equations E entails an equation e if
</bodyText>
<listItem confidence="0.995581">
• Membership: e E E
• Reflexivity e is A or d = d for some descriptor d
• Symmetry e is di = d2 and d2 = di is in E
• Transitivitr e is di = d2 and there is a descriptor d
such that di = d and d = d2 are in E
• Substitutivity: e is di = pi • d2 and both pi = p2 and
= p2 d2 are in E
• Iteration: there is E&apos; c E such that E&apos; F e and for all
e&apos; E&amp;quot; E Fe&apos;
</listItem>
<bodyText confidence="0.999812382352941">
With this notion of entailment, the most natural definition
of the set Con is that a finite subset E of D is consistent if
and only if it does not entail an inconsistent equation, which
has the form ci = c2, with ci and c2 as distinct constants.
An arbitrary subset of D is consistent if and only if all
its finite subsets are consistent in the way defined above.
The consistent and deductively closed subsets of D ordered
by inclusion form a complete partial order or domain D,
our domain of descriptions of feature structures.
Deductive closure is used to define the elements of D
so that elements defined by equivalent sets of equations are
the same. In the rest of this paper, we will specify elements
of D by convenient sets of equations, leaving the equations
in the closure implicit.
The inclusion order C in D provides the notion of
a description being more or less specific than another.
The least-upper-bound operation U combines two descrip-
tions into the least instantiated description that satisfies
the equations in both descriptions, their unification. The
greatest-lower-bound operation ri gives the most instanti-
ated description containing all the equations common to
two descriptions, their generalization.
The foregoing definition of consistency may seem very
natural, but it has the technical disadvantage that, in gen-
eral, the union of two consistent sets is not itself a consistent
set; therefore, the corresponding operation of unification
may not be defined on certain pairs of inputs. Although
this does not cause problems at this stage, it fails to deal
with the fact that failure to unify is not the same as lack of
definition and causes technical difficulties when providing
rule denotations. We therefore need a slightly less natural
definition.
First we add another statement to the specification of
the entailment relation:
</bodyText>
<page confidence="0.988648">
125
</page>
<listItem confidence="0.986577">
• Falsity if e is inconsistent, {e} entails every element
of D.
</listItem>
<bodyText confidence="0.997225176470588">
- That is, falsity entails anything. Next we define Con to be
simply the set of all finite subsets of D. The set Con no
longer corresponds to sets of equations that are consistent
in the usual equational sense.
With the new definitions of Con and I-, the deductive
closure of a set containing an inconsistent equation is the
whole of D. The partial order D is now a lattice with top
element T = D, and the unification operation U is always
defined and returns T on unification failure.
We can now define the description mapping 5: D F
that relates descriptions to the described feature structures.
The idea is that, in proceeding from a description d E D to
a feature structure f E F, we keep only definite informa-
tion about values and discard information that only states
value constraints, but does not specify the values them-
selves. More precisely, seeing d as a set of equations, we
consider only the subset {c1.1 of d with elements of the form
</bodyText>
<equation confidence="0.988952">
(11 • • 1•3) = ck - •
Each e E Ldf defines an element f (e) of F by the equations
</equation>
<bodyText confidence="0.7466075">
with each of the fi undefined for all other labels. Then, we
can define b(d) as
</bodyText>
<equation confidence="0.9766135">
b(d) =U f(e)
eeid1
</equation>
<bodyText confidence="0.993131037037037">
This description mapping can be shown to be continu-
ous in the sense of domain theory, that is, it has the prop-
erties that increasing information in a description leads
to nondecreasing, information in the described structures
(monotonicity) and that if a sequence of descriptions ap-
proximates another description, the same condition holds
for the described structures.
Note that 6 may map several elements of D on to one
element of F. For example, the elements given by the two
sets of equations
Vf c(g i) (of :;) =
describe the same structure, because the description map-
ping ignores the link between (f h) and (g i) in the first
description. Such links are useful only when unifying with
further descriptive elements, not in the completed feature
structure, which merely provides feature-value assignments.
Informally, we can think of elements of D as directed
rooted graphs and of elements of F as their unfoldings as
trees, the unfolding being given by the mapping b. It is
worth noting that if a description is cyclic—that is, if it has
cycles when viewed as a directed graph—then the resulting
feature tree will be infinite.3
Stated more precisely, an element f of a domain is fi-
nite, if for any ascending sequence {di} such that f
there is an i such that f C di. Then the cyclic elements
of D are those finite elements that are mapped by S into
nonfinite elements of F.
</bodyText>
<sectionHeader confidence="0.9276895" genericHeader="method">
5. Providing a Denotation for a
Grammar
</sectionHeader>
<bodyText confidence="0.9997992">
We now move on to the question of how the domain D
is used to provide a denotational semantics for a grammar
formalism.
We take a simple grammar formalism with rules con-
sisting of a context-free part over a nonterminal vocabu-
</bodyText>
<listItem confidence="0.754142">
lary .41 = NO and a set of equations over paths in
(10..00] L&apos;)UC. A sample rule might be
</listItem>
<equation confidence="0.9985015">
S NP VP
(0 subj) = (I)
(0 predicate) = (2)
(1 agr) = (2 agr) .
</equation>
<bodyText confidence="0.997463857142857">
This is a simplification of the rule format used in the PATR-
II formalism [18,17]. The rule can be read as &amp;quot;an S is an
NP followed by a VP, where the subject of the S is the
NP, its predicate the VP, and the agreement of the NP
the same as the agreement of the VP&amp;quot;.
More formally, a grammar is a quintuple G =
(.1,1 , S, L, C, R), where
</bodyText>
<listItem confidence="0.873194428571429">
• .A/ is a finite, nonempty set of nonterminals , Nk
• S is the set of strings over some alphabet (a flat do-
main with an ancillary continuous function concate-
nation, notated with the symbol -).
• R is a set of pairs r = (N,0 Nr, Nr,„, E,),
where Er is a set of equations between elements of
([0..m] V) UC.
</listItem>
<bodyText confidence="0.993799736842105">
As with context-free grammars, local ambiguity of a
grammar means that in general there are several ways of
assembling the same subphrases into phrases. Thus, the
semantics of context-free grammars is given in terms of
sets of strings. The situation is somewhat more compli-
cated in our sample formalism. The objects specified by
the grammar are pairs of a string and a partial description.
Because of partiality, the appropriate construction cannot
be given in terms of sets of string-description pairs, but
rather in terms of the related domain construction of pow-
erdomains [14,19,16]. We will use the Hoare powerdomain
P = Pm(S x D) of the domain S x D of string-description
pairs. Each element of P is an approximation of a transduc-
tion relation, which is an association between strings and
their possible descriptions.
We can get a feeling for what the domain P is doing
by examining our notion of lexicon. A lexicon will be an
&apos;More precisely a rational tree, that is, a tree with a finite number of
distinct subtrees.
</bodyText>
<page confidence="0.997312">
126
</page>
<bodyText confidence="0.9996985">
element of the domain P, associating with each of the k
nonterminals N1, 1 &lt;I &lt; k a transduction relation from the
corresponding coordinate of P. Thus, for each nontermi-
nal, the lexicon tells us what phrases are under that non-
terminal and what possible descriptions each such phrase
has. here is a sample lexicon:
</bodyText>
<equation confidence="0.947440444444444">
I (&amp;quot;lit her&amp;quot; ,
{(agr mint) = sg, (agr per) = 3))
(&amp;quot;many knights&amp;quot; ,
{(agr num) = pl, (agr per) = 3))
1 (&amp;quot;storms Cornwall&amp;quot;,
{(agr num) = sg})
(&amp;quot;sit at the Round Table&amp;quot;,
{(agr num) = pl})
S: {}
</equation>
<bodyText confidence="0.999856444444445">
By decomposing the effect of a rule into appropriate
steps, we can associate with each rule r a denotation
that combines string-description pairs by concatenation
and unification to build new string-description pairs for the
nonterminal on the left-hand side of the rule, leaving all
other nonterminals untouched. By taking the union of the
denotations of the rules in a grammar, (which is a well-
defined and continuous powerdomain operation,) we get a
mapping
</bodyText>
<equation confidence="0.705193">
U 117)(e)
&apos;ER
</equation>
<bodyText confidence="0.998194625">
from Pk to Pk that represents a one-step application of all
the rules of G &amp;quot;in parallel.&amp;quot;
We can now provide a denotation for the entire gram-
mar as a mapping that completes a lexicon with all the
derived phrases and their descriptions. The denotation of
a grammar is the function that maps each lexicon e into the
smallest fixed point of TG containing f. The fixed point is
defined by
</bodyText>
<equation confidence="0.924561">
IG(t) = UT4(e)
1=0
as TG is continuous.
</equation>
<bodyText confidence="0.9999795">
It remains to describe the decomposition of a rule&apos;s ef-
fect into elementary steps. The main technicality to keep in
mind is that rules state constraints among several descrip-
tions (associated with the parent and each child), whereas
a set of equations in D constrains but a single descrip-
tion. This mismatch is solved by embedding the tuple
(do, ..., dm) of descriptions in a single larger description,
as expressed by
</bodyText>
<equation confidence="0.93375">
(i) = di, 0 &lt; i &lt; m
</equation>
<bodyText confidence="0.997306">
and only then applying the rule constraints—now viewed as
constraining parts of a single description. This is done by
the indexing and combination steps described below. The
</bodyText>
<page confidence="0.864039">
5
</page>
<bodyText confidence="0.999911666666667">
rest of the work of applying a rule, extracting the result, is
done by the projection and deindezing steps.
The four steps for applying a rule
</bodyText>
<equation confidence="0.98991">
r = (N,o —+ N,, . N,„„ E,)
</equation>
<bodyText confidence="0.954833538461539">
to string-description pairs (si, di), .. (SA, dA) are as fol-
lows. First, we index each d,1 into d•;, by replacing every
path p in any of its equations with the path j p. We
then combine these indexed descriptions with the rule by
unifying the deductive closure of E„ with all the indexed
descriptions:
In
d = E, U Ud.
We can now project d by removing from it all equations
with paths that do not start with 0. It is clearly evident
that the result d° is still deductively closed. Finally, d° is
deindexed into dio by removing 0 from the front of all paths
0 • p in its equations. The pair associated with N,.0 is then
</bodyText>
<equation confidence="0.96505">
(Sri • • • 8 r ,o) dro)
</equation>
<bodyText confidence="0.986808777777778">
It is not difficult to show that the above operations
can be lifted into operations over elements of Pl` that leave.
untouched the coordinates not. mentioned in the rule and
that the lifted operations are continuous mappings. With
a slight abuse of notation, we can summarize the foregoing
discussion with the equation
iri= deindex o project o combine, o index,
In the case of the sample lexicon and one rule grammar
presented earlier, IG1(e) would be
</bodyText>
<equation confidence="0.766106">
{• • • as before...)
(- • as before...)
1 (&amp;quot;Uther storms Cornwall&amp;quot;,
{(subj agr num) = sg,...})
(&amp;quot;many knights sit at the Round Table&amp;quot;,
{(subj agr num) = pl,...})
</equation>
<bodyText confidence="0.973714">
(&amp;quot;many knights storms Cornwall&amp;quot;, T)
</bodyText>
<sectionHeader confidence="0.983247" genericHeader="evaluation">
6. Applications
</sectionHeader>
<bodyText confidence="0.998213">
We have used the techniques discussed here to analyze
the feature systems of GPSG [15), LFG [2] and PATR-II
117). All of them turn out to be specializations of our do-
main D of descriptions. Figure 1 provides a summary of two
of the most critical formal properties of context-free-based
grammar formalisms, the domains of their feature systems
(full F, finite elements of F, or elements of F based on
nonrecursive domain equations) and whether the context-
free skeletons of grammars are constrained to be off-line
parseable [13) thereby guaranteeing decidability.
</bodyText>
<table confidence="0.831199444444444">
NP:
VP:
NP:
VP:
S :
127
DCG-114 PATR-H LFG GPSGb
FEATURE SYSTEM full finite finite nonrec.
CF SKELETON full full off-line full
</table>
<tableCaption confidence="0.5056335">
°DCGs based on Pro og-11 which allows cyclic terms.
blIPSG, the current Hewlett-Packard implementation derived
from GPSG, would come more accurately under the PATR-I1
classification.
</tableCaption>
<figureCaption confidence="0.999532">
Figure 1: Summary of Grammar System Properties
</figureCaption>
<bodyText confidence="0.999647191489362">
Though notational differences and some grammatical
devices are glossed over here, the comparison is useful as
a first step in unifying the various formalisms under one
semantic umbrella. Furthermore, this analysis elicits the
need to distinguish carefully between the domain of fea-
ture structures F and that of descriptions. This distinction
is not clear in the published accounts of GPSG and LFG,
which imprecision is responsible for a number of uncertain-
ties in the interpretation of operators and conventions in
those formalisms.
In addition to formal insights, linguistic insights have
also been gleaned from this work. First of all, we note
that while the systems make crucial use of unification, gen-
eralization is also a well-defined notion therein and might
indeed be quite useful. In fact, it was this availability of the
generalization operation that suggested a simplified account
of coordination facts in English now being used in GPSG
[15] and in an extension of PATR-II [81. Though the issues
of coordination and agreement are discussed in greater de-
tail in these two works, we present here a simplified view of
the use of generalization in a GPSG coordination analysis.
Circa 1982 GPSG [61 analyzed coordination by using a
special principle, the conjunct realization principle (CRP),
to achieve partial instantiation of head features (including
agreement) on the parent category. This principle, together
with the head feature convention (HFC) and control agree-
ment principle (CAP), guaranteed agreement between the
head noun of a subject and the head verb of a predicate in
English sentences. The HFC, in particular, can be stated
in our notation as (0 head) = (n head) for n the head of 0.
A more recent analysis [4,15] replaced the conjunct re-
alization principle with a modified head feature conven-
tion that required a head to be more instantiated than the
parent, that is: (0 head) C (n head) for all constituents
n which are heads of 0. Making coordinates heads of
their parent achieved the effect of the CRP. Unfortunately,
since the HFC no longer forced identity of agreement, a
new principle—the nominal completeness principle (NCP),
which required that NP&apos;s be fully instantiated—was re-
quired to guarantee that the appropriate agreements were
maintained.
Making use of the order structure of the domains we
have just built, we can achieve straightforwardly the effect
of the CRP and the old HFC without any notion of the
NCP. Our final version of the HFC merely requires that
the parent&apos;s head features be the generalization of the head
features of the head children. Formally, we have:
</bodyText>
<equation confidence="0.5079055">
(0 head) = n (i head) .
ieheads of 0
</equation>
<bodyText confidence="0.99997975">
In the case of parents with one head child, this final HFC
reduces to the old HFC requiring identity; it reduces to the
newer one, however, in cases (like coordinate structures)
where there are several head constituents.
Furthermore, by utilizing an order structure on the do-
main of constants C, it may be possible to model that trou-
blesome coordination phenomenon, number agreement in
coordinated noun phrases [8,15].
</bodyText>
<sectionHeader confidence="0.976626" genericHeader="conclusions">
7. Conclusion
</sectionHeader>
<bodyText confidence="0.9991665">
We have approached the problem of analyzing the
meaning of grammar formalisms by applying the techniques
of denotational semantics taken from work on the semantics
of computer languages. This has enabled us to
</bodyText>
<listItem confidence="0.932786666666667">
• account rigorously for intrinsically partial descrip-
tions,
• derive directly notions of unification, instantiation
and generalization,
• relate feature systems in linguistics with type systems
in computer science,
• show that feature systems in GPSG, LFG and PATR-
H are special cases of a single construction,
• give semantics to a variety of mechanisms in grammar
formalisms, and
• introduce operations for modeling linguistic phenom-
ena that have not previously been considered.
</listItem>
<bodyText confidence="0.9998106">
We plan to develop the approach further to give ac-
counts of negative and disjunctive constraints [8], besides
the simple equational constraints discussed here.
On the basis of these insights alone, it should be clear
that the view of grammar formalisms as programming lan-
guages offers considerable potential for investigation. But,
even more importantly, the linguistic discipline enforced
by a rigorous approach to the design and analysis of gram-
mar formalisms may make possible a hitherto unachievable
standard of research in this area.
</bodyText>
<sectionHeader confidence="0.998629" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999679222222222">
[1] Ait-Kaci, H. &amp;quot;A New Model of Computation Based
on a Calculus of Type Subsumption.&amp;quot; Dept. of Com-
puter and Information Science, University of Penn-
sylvania (November 1983).
[2] Bresnan, J. and R.. Kaplan. &amp;quot;Lexical-Functional
Grammar: A Formal System for Grammatical Repre-
sentation.&amp;quot; In J. Bresnan, Ed., The Mental Represen-
tation of Grammatical Relations, MIT Press, Cam-
bridge, Massachusetts (1982), pp. 173-281.
</reference>
<page confidence="0.975839">
128
</page>
<reference confidence="0.999896371794872">
131 Colmerauer, A. &amp;quot;Metamorphosis Grammars.&amp;quot; In L.
Bole, Ed., Natural Language Communication with
Computers, Springer-Verlag, Berlin (1978). First
appeared as &amp;quot;Les Grammaires de Metamorphose,&amp;quot;
Groupe d&apos;Intellig,ence Artificielle, Universite de Mar-
seille H (November 1975).
141 Farkas, D., D.P. Flickinger, G. Gazdar, W.A. Ladu-
saw, A. Ojeda, J. Pinkham, G.K. Pullum, and P.
Sells. &amp;quot;Some Revisions to the Theory of Features
and Feature Instantiation.&amp;quot; Unpublished manuscript
(August 1983).
Gazdar, Gerald and G. Pullum. &amp;quot;Generalized Phrase
Structure Grammar: A Theoretical Synopsis.&amp;quot; Indi-
ana University Linguistics Club, Bloomington, Indi-
ana (1982).
[6] Gazdar, G., E. Klein, G.K. Pullum, and I.A. Sag.
&amp;quot;Coordinate Structure and Unbounded Dependen-
cies.&amp;quot; In NI. Barlow, D. P. Flickinger and I. A.
Sag, eds., Developments in Generalized Phrase Struc-
ture Grammar. Indiana University Linguistics Club,
Bloomington, Indiana (1982).
Harrison, NI. Introduction to Formal Language The-
ory. Addison-Wesley, Reading, Massachussets (1978).
[8] Kartunnen, Lauri. &amp;quot;Features and Values.&amp;quot; Proceed-
ings of the Tenth International Conference on Com-
putational Linguistics, Stanford University, Stanford,
California (4-7 July, 1984).
[9] Kay, M. &amp;quot;Functional Grammar.&amp;quot; Proceedings of the
Fifth Annual Meeting of the Berkeley Linguistic Soci-
ety, Berkeley Linguistic Society, Berkeley, California
(February 17-19, 1979), pp. 142-158.
[10] Marcus, M., D. Hindle and M. Fleck. &amp;quot;D-Theory:
Talking about Talking about Trees.&amp;quot; Proceedings of
the 21st Annual Meeting of the Association for Com-
putational Linguistics, Boston, Massachusetts (15-17
June, 1982).
[11] Pereira, F. &amp;quot;Extraposition Grammars.&amp;quot; American
Journal of Computational Linguistics 7, 4 (October-
December 1981), 243-256.
[12] Pereira, F. and D. H. D. Warren. &amp;quot;Definite Clause
Grammars for Language Analysis—a Survey of the
Formalism and a Comparison with Augmented Tran-
sition Networks.&amp;quot; Artificial Intelligence 18 (1980),
231-278.
[13] Pereira, F. C. N., and David H. D. Warren &amp;quot;Parsing
as Deduction.&amp;quot; Proceedings of the 21st Annual Meet-
ing of the Association for Computational Linguistics,
Boston, Massachusetts, (15-17 June, 1983), pp. 137-
144.
[14] Plotkin, G. &amp;quot;A Powerdomain Construction.&amp;quot; SIAM
Journal of Computing 5 (1976), 452-487.
[15] Sag, I., G. Gazdar, T. Wasow and S. Weisler. &amp;quot;Coor-
dination and How to Distinguish Categories.&amp;quot; Report
No. CSLI-84-3, Center for the Study of Language
and Information, Stanford University, Stanford, Cal-
ifornia (June, 1982).
1161 Scott, D. &amp;quot;Domains for Deuotational Semantics.&amp;quot; In
ICALP 82, Springer-Verlag, Heidelberg (1982).
[17] Shieber, Stuart. &amp;quot;The Design of a Computer Lan-
guage for Linguistic Information.&amp;quot; Proceedings of
the Tenth International Conference on Computational
Linguistics (4-7 July, 1984)
[18] Shieber, S., H. Uszkoreit, F. Pereira, J. Robinson and
M. Tyson. &amp;quot;The Formalism and Implementation of
PATR-II.&amp;quot; In Research on Interactive Acquisition and
Use of Knowledge, SRI Final Report 1894. SRI In-
ternational, Menlo Park, California (1983).
[19] Smyth, M. &amp;quot;Power Domains.&amp;quot; Journal of Computer
and System Sciences 16 (1978), 23-36.
[20] Stoy, J. Denotational Semantics: The Scott-Strachey
Approach to Programming Language Theory. MIT
Press, Cambridge, Massachusetts (1977).
[21] van Emden, M. and R. A. Kowalski. &amp;quot;The Seman-
tics of Predicate Logic as a Programming Language.&amp;quot;
Journal of the ACM 23, 4 (October 1976), 733-742.
[22] Woods, W. et al. &amp;quot;Speech Understanding Systems:
Final Report.&amp;quot; BBN Report 3438, Bolt Beranek and
Newman, Cambridge, Massachusetts (1976).
</reference>
<figure confidence="0.949535">
[5]
[7]
</figure>
<page confidence="0.955065">
129
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.986813">
<title confidence="0.999598">The Semantics of Grammar Formalisms Seen as Computer Languages</title>
<author confidence="0.999987">Fernando C N Pereira</author>
<author confidence="0.999987">Stuart M Shieber</author>
<affiliation confidence="0.9987746">Artificial Intelligence Center SRI International and Center for the Study of Language and Information Stanford University</affiliation>
<abstract confidence="0.999632222222222">The design, implementation, and use of grammar formalisms for natural language have constituted a major branch of computational linguistics throughout its development. By viewing grammar formalisms as just a special case of computer languages, we can take advantage of the machinery of denotational semantics to provide a precise specification of their meaning. Using Dana Scott&apos;s domain theory, we elucidate the nature of the feature systems used in augmented phrase-structure grammar formalisms, in particular those of recent versions of generalized phrase structure grammar, lexical functional grammar and PATRprovide a denotational semantics for a simple grammar formalism. We find that the mathematical structures developed for this purpose contain an operation of feature generalization, not. available in those grammar formalisms, that can be used to give a partial account of the effect of coordination on syntactic features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Ait-Kaci</author>
</authors>
<title>A New Model of Computation Based on a Calculus of Type Subsumption.&amp;quot;</title>
<date>1983</date>
<institution>Dept. of Computer and Information Science, University of Pennsylvania</institution>
<contexts>
<context position="5138" citStr="[1]" startWordPosition="767" endWordPosition="767"> the more common usage denoting that portion of a grammar concerned with the meaning of object sentences. Here we are concerned with the meaning of the metalanguage. 123 language design and theories of abstract data types. This last point is particularly interesting because it opens up several possibilities—among them that of imposing a type discipline on the use of a formalism, with all the attendant advantages of compile-time error checking, modularity, and optimized compilation techniques for grammar rules, and that of relating grammar formalisms to other knowledge representation languages [1]. As a specific contribution of this study, we elucidate the nature of the feature systems used in augmented phrasestructure grammar formalisms, in particular those of recent versions of generalized phrase structure grammar (GPSG) 15,151, lexical functional grammar (LFG) [2] and PATR-II 118,171; we find that the mathematical structures developed for this purpose contain an operation of feature generalization, not available in those grammar formalisms, that can be used to give a partial account of the effect of coordination on syntactic features. Just as studies in the semantics of programming </context>
</contexts>
<marker>[1]</marker>
<rawString>Ait-Kaci, H. &amp;quot;A New Model of Computation Based on a Calculus of Type Subsumption.&amp;quot; Dept. of Computer and Information Science, University of Pennsylvania (November 1983).</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Bresnan</author>
<author>R Kaplan</author>
</authors>
<title>Lexical-Functional Grammar: A Formal System for Grammatical Representation.&amp;quot; In</title>
<date>1982</date>
<booktitle>Universite de Marseille H</booktitle>
<pages>173--281</pages>
<publisher>MIT Press,</publisher>
<institution>Indiana University Linguistics Club,</institution>
<location>Cambridge, Massachusetts</location>
<contexts>
<context position="5413" citStr="[2]" startWordPosition="807" endWordPosition="807">it opens up several possibilities—among them that of imposing a type discipline on the use of a formalism, with all the attendant advantages of compile-time error checking, modularity, and optimized compilation techniques for grammar rules, and that of relating grammar formalisms to other knowledge representation languages [1]. As a specific contribution of this study, we elucidate the nature of the feature systems used in augmented phrasestructure grammar formalisms, in particular those of recent versions of generalized phrase structure grammar (GPSG) 15,151, lexical functional grammar (LFG) [2] and PATR-II 118,171; we find that the mathematical structures developed for this purpose contain an operation of feature generalization, not available in those grammar formalisms, that can be used to give a partial account of the effect of coordination on syntactic features. Just as studies in the semantics of programming languages start by giving semantics for simple languages, so we will start with simple grammar formalisms that capture the essence of the method without an excess of obscuring detail. The present enterprise should be contrasted with studies of the generative capacity of form</context>
<context position="24153" citStr="[2]" startWordPosition="4098" endWordPosition="4098">in the rule and that the lifted operations are continuous mappings. With a slight abuse of notation, we can summarize the foregoing discussion with the equation iri= deindex o project o combine, o index, In the case of the sample lexicon and one rule grammar presented earlier, IG1(e) would be {• • • as before...) (- • as before...) 1 (&amp;quot;Uther storms Cornwall&amp;quot;, {(subj agr num) = sg,...}) (&amp;quot;many knights sit at the Round Table&amp;quot;, {(subj agr num) = pl,...}) (&amp;quot;many knights storms Cornwall&amp;quot;, T) 6. Applications We have used the techniques discussed here to analyze the feature systems of GPSG [15), LFG [2] and PATR-II 117). All of them turn out to be specializations of our domain D of descriptions. Figure 1 provides a summary of two of the most critical formal properties of context-free-based grammar formalisms, the domains of their feature systems (full F, finite elements of F, or elements of F based on nonrecursive domain equations) and whether the contextfree skeletons of grammars are constrained to be off-line parseable [13) thereby guaranteeing decidability. NP: VP: NP: VP: S : 127 DCG-114 PATR-H LFG GPSGb FEATURE SYSTEM full finite finite nonrec. CF SKELETON full full off-line full °DCGs </context>
</contexts>
<marker>[2]</marker>
<rawString>Bresnan, J. and R.. Kaplan. &amp;quot;Lexical-Functional Grammar: A Formal System for Grammatical Representation.&amp;quot; In J. Bresnan, Ed., The Mental Representation of Grammatical Relations, MIT Press, Cambridge, Massachusetts (1982), pp. 173-281. 131 Colmerauer, A. &amp;quot;Metamorphosis Grammars.&amp;quot; In L. Bole, Ed., Natural Language Communication with Computers, Springer-Verlag, Berlin (1978). First appeared as &amp;quot;Les Grammaires de Metamorphose,&amp;quot; Groupe d&apos;Intellig,ence Artificielle, Universite de Marseille H (November 1975). 141 Farkas, D., D.P. Flickinger, G. Gazdar, W.A. Ladusaw, A. Ojeda, J. Pinkham, G.K. Pullum, and P. Sells. &amp;quot;Some Revisions to the Theory of Features and Feature Instantiation.&amp;quot; Unpublished manuscript (August 1983). Gazdar, Gerald and G. Pullum. &amp;quot;Generalized Phrase Structure Grammar: A Theoretical Synopsis.&amp;quot; Indiana University Linguistics Club, Bloomington, Indiana (1982).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>E Klein</author>
<author>G K Pullum</author>
<author>I A Sag</author>
</authors>
<title>Coordinate Structure and Unbounded Dependencies.&amp;quot;</title>
<date>1982</date>
<booktitle>Developments in Generalized Phrase Structure</booktitle>
<editor>In NI. Barlow, D. P. Flickinger and I. A. Sag, eds.,</editor>
<publisher>Addison-Wesley,</publisher>
<institution>Grammar. Indiana University Linguistics Club,</institution>
<location>Bloomington, Indiana</location>
<marker>[6]</marker>
<rawString>Gazdar, G., E. Klein, G.K. Pullum, and I.A. Sag. &amp;quot;Coordinate Structure and Unbounded Dependencies.&amp;quot; In NI. Barlow, D. P. Flickinger and I. A. Sag, eds., Developments in Generalized Phrase Structure Grammar. Indiana University Linguistics Club, Bloomington, Indiana (1982). Harrison, NI. Introduction to Formal Language Theory. Addison-Wesley, Reading, Massachussets (1978).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Kartunnen</author>
</authors>
<title>Features and Values.&amp;quot;</title>
<date>1984</date>
<booktitle>Proceedings of the Tenth International Conference on Computational Linguistics,</booktitle>
<location>Stanford University, Stanford, California</location>
<contexts>
<context position="28041" citStr="[8,15]" startWordPosition="4730" endWordPosition="4730"> version of the HFC merely requires that the parent&apos;s head features be the generalization of the head features of the head children. Formally, we have: (0 head) = n (i head) . ieheads of 0 In the case of parents with one head child, this final HFC reduces to the old HFC requiring identity; it reduces to the newer one, however, in cases (like coordinate structures) where there are several head constituents. Furthermore, by utilizing an order structure on the domain of constants C, it may be possible to model that troublesome coordination phenomenon, number agreement in coordinated noun phrases [8,15]. 7. Conclusion We have approached the problem of analyzing the meaning of grammar formalisms by applying the techniques of denotational semantics taken from work on the semantics of computer languages. This has enabled us to • account rigorously for intrinsically partial descriptions, • derive directly notions of unification, instantiation and generalization, • relate feature systems in linguistics with type systems in computer science, • show that feature systems in GPSG, LFG and PATRH are special cases of a single construction, • give semantics to a variety of mechanisms in grammar formalis</context>
</contexts>
<marker>[8]</marker>
<rawString>Kartunnen, Lauri. &amp;quot;Features and Values.&amp;quot; Proceedings of the Tenth International Conference on Computational Linguistics, Stanford University, Stanford, California (4-7 July, 1984).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Functional Grammar.&amp;quot;</title>
<date>1979</date>
<booktitle>Proceedings of the Fifth Annual Meeting of the</booktitle>
<pages>142--158</pages>
<institution>Berkeley Linguistic Society, Berkeley Linguistic Society,</institution>
<location>Berkeley, California</location>
<marker>[9]</marker>
<rawString>Kay, M. &amp;quot;Functional Grammar.&amp;quot; Proceedings of the Fifth Annual Meeting of the Berkeley Linguistic Society, Berkeley Linguistic Society, Berkeley, California (February 17-19, 1979), pp. 142-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>D Hindle</author>
<author>M Fleck</author>
</authors>
<title>D-Theory: Talking about Talking about Trees.&amp;quot;</title>
<date>1982</date>
<booktitle>Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Boston, Massachusetts</location>
<marker>[10]</marker>
<rawString>Marcus, M., D. Hindle and M. Fleck. &amp;quot;D-Theory: Talking about Talking about Trees.&amp;quot; Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics, Boston, Massachusetts (15-17 June, 1982).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
</authors>
<title>Extraposition Grammars.&amp;quot;</title>
<date>1981</date>
<journal>American Journal of Computational Linguistics</journal>
<volume>7</volume>
<pages>243--256</pages>
<marker>[11]</marker>
<rawString>Pereira, F. &amp;quot;Extraposition Grammars.&amp;quot; American Journal of Computational Linguistics 7, 4 (OctoberDecember 1981), 243-256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>D H D Warren</author>
</authors>
<title>Definite Clause Grammars for Language Analysis—a Survey of the Formalism and a Comparison with Augmented Transition Networks.&amp;quot;</title>
<date>1980</date>
<journal>Artificial Intelligence</journal>
<volume>18</volume>
<pages>231--278</pages>
<contexts>
<context position="10152" citStr="[12]" startWordPosition="1558" endWordPosition="1558"> abstractness, may not be apparent at first glance. Such feature systems tend to use data structures or mathematical objects that are more or less isomorphic to directed graphs of one sort or another, or, as they are sometimes described, partial functions. Just what the relation is between these two ways of viewing things will be explained later. In general, these graph structures are used to encode linguistic information in the form of attribute-value pairs. Most importantly, partial information is critical to the use of such systems—for instance, in the variables of definite clause grammars [12] and in the GPSG analysis of coordination [15]. That is, the elements of the feature systems, called feature structures (alternatively, feature bundles, f-structures [21, or terms) can be partial in some sense. The partial descriptions, being in a domain of attributes and complex values, tend to be equational in nature: some feature&apos;s value is equated with some other value. Partial descriptions can be understood 124 in one of two ways: either the descriptions represent sets of fully specified elements of an underlying domain or they are regarded as participating in a relationship of partiality</context>
</contexts>
<marker>[12]</marker>
<rawString>Pereira, F. and D. H. D. Warren. &amp;quot;Definite Clause Grammars for Language Analysis—a Survey of the Formalism and a Comparison with Augmented Transition Networks.&amp;quot; Artificial Intelligence 18 (1980), 231-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F C N Pereira</author>
<author>David H D Warren</author>
</authors>
<title>Parsing as Deduction.&amp;quot;</title>
<date>1983</date>
<booktitle>Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>137--144</pages>
<location>Boston, Massachusetts,</location>
<marker>[13]</marker>
<rawString>Pereira, F. C. N., and David H. D. Warren &amp;quot;Parsing as Deduction.&amp;quot; Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics, Boston, Massachusetts, (15-17 June, 1983), pp. 137-144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Plotkin</author>
</authors>
<title>A Powerdomain Construction.&amp;quot;</title>
<date>1976</date>
<journal>SIAM Journal of Computing</journal>
<volume>5</volume>
<pages>452--487</pages>
<contexts>
<context position="20118" citStr="[14,19,16]" startWordPosition="3366" endWordPosition="3366">ements of ([0..m] V) UC. As with context-free grammars, local ambiguity of a grammar means that in general there are several ways of assembling the same subphrases into phrases. Thus, the semantics of context-free grammars is given in terms of sets of strings. The situation is somewhat more complicated in our sample formalism. The objects specified by the grammar are pairs of a string and a partial description. Because of partiality, the appropriate construction cannot be given in terms of sets of string-description pairs, but rather in terms of the related domain construction of powerdomains [14,19,16]. We will use the Hoare powerdomain P = Pm(S x D) of the domain S x D of string-description pairs. Each element of P is an approximation of a transduction relation, which is an association between strings and their possible descriptions. We can get a feeling for what the domain P is doing by examining our notion of lexicon. A lexicon will be an &apos;More precisely a rational tree, that is, a tree with a finite number of distinct subtrees. 126 element of the domain P, associating with each of the k nonterminals N1, 1 &lt;I &lt; k a transduction relation from the corresponding coordinate of P. Thus, for e</context>
</contexts>
<marker>[14]</marker>
<rawString>Plotkin, G. &amp;quot;A Powerdomain Construction.&amp;quot; SIAM Journal of Computing 5 (1976), 452-487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Sag</author>
<author>G Gazdar</author>
<author>T Wasow</author>
<author>S Weisler</author>
</authors>
<title>Coordination and How to Distinguish Categories.&amp;quot;</title>
<date>1982</date>
<booktitle>In ICALP 82,</booktitle>
<tech>Report No. CSLI-84-3,</tech>
<pages>1161</pages>
<publisher>Springer-Verlag,</publisher>
<institution>Center for the Study of Language and Information, Stanford University,</institution>
<location>Stanford, California</location>
<contexts>
<context position="10198" citStr="[15]" startWordPosition="1566" endWordPosition="1566">ance. Such feature systems tend to use data structures or mathematical objects that are more or less isomorphic to directed graphs of one sort or another, or, as they are sometimes described, partial functions. Just what the relation is between these two ways of viewing things will be explained later. In general, these graph structures are used to encode linguistic information in the form of attribute-value pairs. Most importantly, partial information is critical to the use of such systems—for instance, in the variables of definite clause grammars [12] and in the GPSG analysis of coordination [15]. That is, the elements of the feature systems, called feature structures (alternatively, feature bundles, f-structures [21, or terms) can be partial in some sense. The partial descriptions, being in a domain of attributes and complex values, tend to be equational in nature: some feature&apos;s value is equated with some other value. Partial descriptions can be understood 124 in one of two ways: either the descriptions represent sets of fully specified elements of an underlying domain or they are regarded as participating in a relationship of partiality with respect to each other. We will hold to t</context>
<context position="25928" citStr="[15]" startWordPosition="4377" endWordPosition="4377">t clear in the published accounts of GPSG and LFG, which imprecision is responsible for a number of uncertainties in the interpretation of operators and conventions in those formalisms. In addition to formal insights, linguistic insights have also been gleaned from this work. First of all, we note that while the systems make crucial use of unification, generalization is also a well-defined notion therein and might indeed be quite useful. In fact, it was this availability of the generalization operation that suggested a simplified account of coordination facts in English now being used in GPSG [15] and in an extension of PATR-II [81. Though the issues of coordination and agreement are discussed in greater detail in these two works, we present here a simplified view of the use of generalization in a GPSG coordination analysis. Circa 1982 GPSG [61 analyzed coordination by using a special principle, the conjunct realization principle (CRP), to achieve partial instantiation of head features (including agreement) on the parent category. This principle, together with the head feature convention (HFC) and control agreement principle (CAP), guaranteed agreement between the head noun of a subjec</context>
<context position="28041" citStr="[8,15]" startWordPosition="4730" endWordPosition="4730"> version of the HFC merely requires that the parent&apos;s head features be the generalization of the head features of the head children. Formally, we have: (0 head) = n (i head) . ieheads of 0 In the case of parents with one head child, this final HFC reduces to the old HFC requiring identity; it reduces to the newer one, however, in cases (like coordinate structures) where there are several head constituents. Furthermore, by utilizing an order structure on the domain of constants C, it may be possible to model that troublesome coordination phenomenon, number agreement in coordinated noun phrases [8,15]. 7. Conclusion We have approached the problem of analyzing the meaning of grammar formalisms by applying the techniques of denotational semantics taken from work on the semantics of computer languages. This has enabled us to • account rigorously for intrinsically partial descriptions, • derive directly notions of unification, instantiation and generalization, • relate feature systems in linguistics with type systems in computer science, • show that feature systems in GPSG, LFG and PATRH are special cases of a single construction, • give semantics to a variety of mechanisms in grammar formalis</context>
</contexts>
<marker>[15]</marker>
<rawString>Sag, I., G. Gazdar, T. Wasow and S. Weisler. &amp;quot;Coordination and How to Distinguish Categories.&amp;quot; Report No. CSLI-84-3, Center for the Study of Language and Information, Stanford University, Stanford, California (June, 1982). 1161 Scott, D. &amp;quot;Domains for Deuotational Semantics.&amp;quot; In ICALP 82, Springer-Verlag, Heidelberg (1982).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
</authors>
<title>The Design of a Computer Language for Linguistic Information.&amp;quot;</title>
<date>1984</date>
<booktitle>Proceedings of the Tenth International Conference on Computational Linguistics</booktitle>
<pages>4--7</pages>
<contexts>
<context position="18971" citStr="[18,17]" startWordPosition="3152" endWordPosition="3152">. Then the cyclic elements of D are those finite elements that are mapped by S into nonfinite elements of F. 5. Providing a Denotation for a Grammar We now move on to the question of how the domain D is used to provide a denotational semantics for a grammar formalism. We take a simple grammar formalism with rules consisting of a context-free part over a nonterminal vocabulary .41 = NO and a set of equations over paths in (10..00] L&apos;)UC. A sample rule might be S NP VP (0 subj) = (I) (0 predicate) = (2) (1 agr) = (2 agr) . This is a simplification of the rule format used in the PATRII formalism [18,17]. The rule can be read as &amp;quot;an S is an NP followed by a VP, where the subject of the S is the NP, its predicate the VP, and the agreement of the NP the same as the agreement of the VP&amp;quot;. More formally, a grammar is a quintuple G = (.1,1 , S, L, C, R), where • .A/ is a finite, nonempty set of nonterminals , Nk • S is the set of strings over some alphabet (a flat domain with an ancillary continuous function concatenation, notated with the symbol -). • R is a set of pairs r = (N,0 Nr, Nr,„, E,), where Er is a set of equations between elements of ([0..m] V) UC. As with context-free grammars, local a</context>
</contexts>
<marker>[17]</marker>
<rawString>Shieber, Stuart. &amp;quot;The Design of a Computer Language for Linguistic Information.&amp;quot; Proceedings of the Tenth International Conference on Computational Linguistics (4-7 July, 1984)</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
<author>H Uszkoreit</author>
<author>F Pereira</author>
<author>J Robinson</author>
<author>M Tyson</author>
</authors>
<title>The Formalism and Implementation of PATR-II.&amp;quot;</title>
<date>1983</date>
<booktitle>In Research on Interactive Acquisition and Use of Knowledge, SRI Final Report 1894. SRI International,</booktitle>
<location>Menlo Park, California</location>
<contexts>
<context position="18971" citStr="[18,17]" startWordPosition="3152" endWordPosition="3152">. Then the cyclic elements of D are those finite elements that are mapped by S into nonfinite elements of F. 5. Providing a Denotation for a Grammar We now move on to the question of how the domain D is used to provide a denotational semantics for a grammar formalism. We take a simple grammar formalism with rules consisting of a context-free part over a nonterminal vocabulary .41 = NO and a set of equations over paths in (10..00] L&apos;)UC. A sample rule might be S NP VP (0 subj) = (I) (0 predicate) = (2) (1 agr) = (2 agr) . This is a simplification of the rule format used in the PATRII formalism [18,17]. The rule can be read as &amp;quot;an S is an NP followed by a VP, where the subject of the S is the NP, its predicate the VP, and the agreement of the NP the same as the agreement of the VP&amp;quot;. More formally, a grammar is a quintuple G = (.1,1 , S, L, C, R), where • .A/ is a finite, nonempty set of nonterminals , Nk • S is the set of strings over some alphabet (a flat domain with an ancillary continuous function concatenation, notated with the symbol -). • R is a set of pairs r = (N,0 Nr, Nr,„, E,), where Er is a set of equations between elements of ([0..m] V) UC. As with context-free grammars, local a</context>
</contexts>
<marker>[18]</marker>
<rawString>Shieber, S., H. Uszkoreit, F. Pereira, J. Robinson and M. Tyson. &amp;quot;The Formalism and Implementation of PATR-II.&amp;quot; In Research on Interactive Acquisition and Use of Knowledge, SRI Final Report 1894. SRI International, Menlo Park, California (1983).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Smyth</author>
</authors>
<title>Power Domains.&amp;quot;</title>
<date>1978</date>
<journal>Journal of Computer and System Sciences</journal>
<volume>16</volume>
<pages>23--36</pages>
<contexts>
<context position="20118" citStr="[14,19,16]" startWordPosition="3366" endWordPosition="3366">ements of ([0..m] V) UC. As with context-free grammars, local ambiguity of a grammar means that in general there are several ways of assembling the same subphrases into phrases. Thus, the semantics of context-free grammars is given in terms of sets of strings. The situation is somewhat more complicated in our sample formalism. The objects specified by the grammar are pairs of a string and a partial description. Because of partiality, the appropriate construction cannot be given in terms of sets of string-description pairs, but rather in terms of the related domain construction of powerdomains [14,19,16]. We will use the Hoare powerdomain P = Pm(S x D) of the domain S x D of string-description pairs. Each element of P is an approximation of a transduction relation, which is an association between strings and their possible descriptions. We can get a feeling for what the domain P is doing by examining our notion of lexicon. A lexicon will be an &apos;More precisely a rational tree, that is, a tree with a finite number of distinct subtrees. 126 element of the domain P, associating with each of the k nonterminals N1, 1 &lt;I &lt; k a transduction relation from the corresponding coordinate of P. Thus, for e</context>
</contexts>
<marker>[19]</marker>
<rawString>Smyth, M. &amp;quot;Power Domains.&amp;quot; Journal of Computer and System Sciences 16 (1978), 23-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Stoy</author>
</authors>
<title>Denotational Semantics: The Scott-Strachey Approach to Programming Language Theory.</title>
<date>1977</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts</location>
<marker>[20]</marker>
<rawString>Stoy, J. Denotational Semantics: The Scott-Strachey Approach to Programming Language Theory. MIT Press, Cambridge, Massachusetts (1977).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M van Emden</author>
<author>R A Kowalski</author>
</authors>
<title>The Semantics of Predicate Logic as a Programming Language.&amp;quot;</title>
<date>1976</date>
<journal>Journal of the ACM</journal>
<volume>23</volume>
<pages>733--742</pages>
<marker>[21]</marker>
<rawString>van Emden, M. and R. A. Kowalski. &amp;quot;The Semantics of Predicate Logic as a Programming Language.&amp;quot; Journal of the ACM 23, 4 (October 1976), 733-742.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Woods</author>
</authors>
<title>Speech Understanding Systems: Final Report.&amp;quot;</title>
<date>1976</date>
<tech>BBN Report 3438,</tech>
<institution>Bolt Beranek and Newman,</institution>
<location>Cambridge, Massachusetts</location>
<marker>[22]</marker>
<rawString>Woods, W. et al. &amp;quot;Speech Understanding Systems: Final Report.&amp;quot; BBN Report 3438, Bolt Beranek and Newman, Cambridge, Massachusetts (1976).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>