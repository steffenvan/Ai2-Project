<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019155">
<title confidence="0.993706">
Automatic Metrics for Genre-specific Text Quality
</title>
<author confidence="0.989088">
Annie Louis
</author>
<affiliation confidence="0.8154465">
University of Pennsylvania
Philadelphia, PA 19103, USA
</affiliation>
<email confidence="0.998479">
lannie@seas.upenn.edu
</email>
<sectionHeader confidence="0.995634" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998061571428571">
To date, researchers have proposed different
ways to compute the readability and coher-
ence of a text using a variety of lexical, syn-
tax, entity and discourse properties. But these
metrics have not been defined with special rel-
evance to any particular genre but rather pro-
posed as general indicators of writing qual-
ity. In this thesis, we propose and evalu-
ate novel text quality metrics that utilize the
unique properties of different genres. We fo-
cus on three genres: academic publications,
news articles about science, and machine gen-
erated text, in particular the output from auto-
matic text summarization systems.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999769294117647">
Automatic methods to measure the writing quality of
a text can be quite useful for several applications, for
example search and recommendation systems, and
writing support and grading tools. There are two
main categories of prior work on this topic. The first
is studies on ‘readability’ which have proposed met-
rics to select texts appropriate (easy to read) for an
audience of given age and education level (Flesch,
1948; Collins-Thompson and Callan, 2004). These
metrics typically classify texts as suitable for adult
or child, or into a more fine-grained set of 12 ed-
ucational grade levels. The second line of work
are recent computational metrics to predict coher-
ence. These methods identify regularities in words
(Barzilay and Lee, 2004), entity coreference (Barzi-
lay and Lapata, 2008) and discourse relations (Pitler
and Nenkova, 2008) from a large collection of ar-
</bodyText>
<page confidence="0.973753">
54
</page>
<bodyText confidence="0.998451909090909">
ticles and use these patterns to predict the coher-
ence. They assume a particular competency level
(adult educated readers) and also fix the text (typi-
cally news articles, which are appropriate for adult
readers). By removing the focus on age/education
level, these methods compute textual differences be-
tween good and poorly written texts as perceived by
a single audience level.
In my thesis, I propose a new definition – text
quality: the overall well-written characteristic of an
article. It differs from prior work in three respects:
</bodyText>
<listItem confidence="0.936788714285714">
1. We consider a single fixed audience level and
the texts that audience is typically exposed to.
For example, a college educated reader of a
newspaper might find some articles better writ-
ten than others, even though he understands and
can read nearly all of them with ease.
2. It is a holistic property of texts. At a mini-
mum, at least four factors influence quality: the
content/topic that is discussed, sentence level-
grammaticality, discourse coherence and writ-
ing style. Here writing style refers to extra
properties introduced into the text by the au-
thor but do not necessarily interfere with co-
herence if not provided. For example, the use
of metaphors, examples and humour can have
connections with quality. Previous work on co-
herence metrics do not consider these aspects.
3. Such a property would also have genre-specific
dimensions: an academic article should above
all be clear and a thriller-story should be fast-
paced and interesting. Further even if the same
</listItem>
<note confidence="0.296264">
Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 54–59,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998626">
quality aspect is relevant for multiple genres, it
has higher weight in one versus another. Prior
readability and coherence studies were not pro-
posed with relvance to any particular genre.
These aspects make the investigation of text qual-
ity linguistically interesting because by definition
the focus is on a wide range of properties of the text
itself rather than appropriateness for a reader.
In this thesis, we propose computable measures
to capture genre-specific text quality. Our hypoth-
esis is that writing quality is a combination some
generic aspects that matter for most texts, such as
grammatical sentences, and other unique ones which
have high impact in a particular genre.
Specifically, we consider three genres which
have high relevance for writing quality research—
academic writing, science journalism and output of
automatic summarization systems.
Both academic writing and science news articles
describe science, but their audience is quite differ-
ent. Academic writing aims to clearly explain the
details of the research to other experts, while sci-
ence news conveys interesting research findings to
lay readers. This fact creates distinctive content and
writing style in the two genres. There is also a huge
opportunity in these genres for developing applica-
tions involving text quality, for example, authoring
tools for academic writing and information retrival
and recommendation for news articles. We also in-
clude a third genre—automatically generated sum-
maries. Here, when systems produce multi-sentence
text, they must ensure that the text is readable and
coherent. Automatic evaluation of content and lin-
guistic quality is therefore necessary for system de-
velopment in this genre.
</bodyText>
<sectionHeader confidence="0.528374" genericHeader="method">
2 Thesis Summary and Contributions
</sectionHeader>
<bodyText confidence="0.999858117647059">
For this thesis, we only consider the discourse and
style components of text quality, aspects that have
received less focus in prior work. Sentence-level
problems have been widely explored and recently,
even specifically for academic writing (Dale and
Kilgarriff, 2010). We also do not consider content
in our work, for example, academic writing quality
also depends on the ideas and arguments presented
but these aspects are outside the scope of this thesis.
As defined previously, we focus on a fixed audience
level. We assume a reader at the top level of the
competency spectrum: an adult educated reader for
science news and automatic summaries, and for aca-
demic articles, an expert on the topic. This definition
has minimal focus on reader abilities and allows us
to analyze textual differences exclusively.
The specific contributions of this thesis are:
</bodyText>
<listItem confidence="0.9929121">
1. Defining text quality in terms of linguistic as-
pects rather than readability: Our work is the first
to propose a quality definition where well-written
nature is the central focus and including genre-
dependent aspects and writing style.
2. Investigating genre-specific metrics: This study
is also the first to design and evaluate genre-specific
features for text quality prediction. For each genre:
academic writing, science journalism and automatic
summaries, we develop metrics unique to the genre
and evaluate their ability to predict text quality both
individually and in combination with generic fea-
tures put forth in prior work.
3. Proposing new discourse-level features: In
prior work, there are discourse-based features based
on coreference, discourse relations and word co-
occurrence between adjacent sentences. We intro-
duce new features which capture aspects such as
organization of communicative goals and general-
specific nature of sentences.
</listItem>
<bodyText confidence="0.8250074">
Specifically, we introduce the following metrics:
a) Patterns in communicative goals (Section 5):
Every text has a purpose and the author uses a
sequence of communicative goals realized as sen-
tences to convey that purpose. We introduce a met-
ric that predicts coherence based on the size and se-
quence of communicative goals for a genre. This as-
pect is most relevant for research writing: academic
and science journalism because there is a clear goal
and well-defined purpose for these articles.
b) General-specific nature of sentences (Section
6): Some sentences in a text convey only general
content, others provide details and a well-written
text would have a certain balance between the two.
Particularly, while creating summaries, there is a
length contraint, so it cannot include all specific con-
tent but some information must be made more gen-
eral. We introduce a method to predict the specificity
for a sentence and examine how specificity and se-
quence of general-specific sentences is related to the
</bodyText>
<page confidence="0.99129">
55
</page>
<bodyText confidence="0.977801361111111">
quality of automatic summaries.
c) Information cohesiveness (Section 7): This
idea is also proposed for automatic summaries, they
must have a focus and present a small set of ideas
with easy to understand links between them. We
show that cohesiveness properties (computed auto-
matically) of the source text to be summarized can
be linked to the expected content quality of sum-
maries that can be generated for that text. This work
will be extended to analyze the relationship of cohe-
siveness with ratings of focus for the summaries.
d) Aspects of style (Section 8): Here we inves-
tigate metrics beyond coherence and related to ex-
tra features included in the article. We consider the
genre of science journalism and investigate whether
surprise-invoking sentence construction, visual de-
scriptions and emotional content of the articles are
also correlated with perceived quality.
We will evaluate our approaches in two ways:
1. We investigate the extent to which genre-
specific metrics are indicative of text quality and
whether they complement generic features.
2. We also examine how unique these metrics are
for a given genre, for example: are surprising arti-
cles always considered well-written even if they are
not science-news? For this analysis, we will con-
sider a set of randomly selected news texts (no genre
division) with text quality ratings. On this set, we
will test the performance of generic and each set
of genre-specific metrics. We expect that on this
data, the generic features would be best with little
improvement from the genre-specific metrics.
So far, we have designed some of the metrics that
we described above and have found them to be pre-
dictive of writing quality. We will carry out exten-
sive evaluation of these measures in future work.
</bodyText>
<sectionHeader confidence="0.999943" genericHeader="method">
3 Related work
</sectionHeader>
<bodyText confidence="0.999776275862069">
Early readability metrics used sentence length, num-
ber of syllables in words and number of ‘easy’
words to distinguish texts from different grade lev-
els (Flesch, 1948; Gunning, 1952; Dale and Chall,
1948). Other measures are based on word familiarity
(Collins-Thompson and Callan, 2004; Si and Callan,
2001), difficulty of concepts (Zhao and Kan, 2010)
and features of sentence syntax (Schwarm and Os-
tendorf, 2005). There are also readability studies for
audience distinctions other than grade levels. Feng
et al. (2009) consider adult readers with intellectual
disability and therefore introduce features such as
the number of entities a person should keep in work-
ing memory for that text and how far entity links
stretch. Heilman et al. (2007) show that grammati-
cal features make a bigger impact while predicting
readability for second language learners in contrast
to native speakers.
Newer coherence measures do not focus on reader
abilities. They are typically run on news articles
and assume an adult audience. They show that
word co-occurrence (Soricut and Marcu, 2006), sub-
topic structure (Barzilay and Lee, 2004), discourse
relations (Pitler and Nenkova, 2008; Lin et al.,
2011) and coreference patterns (Barzilay and Lap-
ata, 2008) learn from large corpora can be used to
predict coherence.
But prior metrics are not proposed as unique to
any genre. Some metrics using word patterns (Si and
Callan, 2001; Barzilay and Lee, 2004) are domain-
dependent in that they require documents from the
target domain for training. But they can be trained
for any domain in this manner.
However recent work show that genre-specific
indicators could be quite useful for applications.
McIntyre and Lapata (2009) automatically generate
short children’s stories using patterns of event and
entity co-occurrences. They find that people judge
their stories as better when the text is optimized not
only for coherence and but also its interesting nature.
They use a supervised approach to predict the inter-
est value for a story during the generation process.
Burstein et al. (2010) find that for predicting the co-
herence of student essays, better accuracies can be
obtained by augmenting generic coherence metrics
with features related to student writing such as word
variety and spelling errors.
In my own work on automatic evaluation of sum-
maries (Pitler et al., 2010), I have observed the im-
pact of genre. We consider a corpus of summaries
written by people and those produced by automatic
systems. Psycholinguistic metrics previously pro-
posed for analyzing coherence of human texts work
successfully on human summaries but are less ac-
curate for system summaries. Similarly, metrics
which predict the fluency of machine translations
accurately, work barely above baseline for judging
the grammaticality of sentences from human sum-
</bodyText>
<page confidence="0.984978">
56
</page>
<bodyText confidence="0.999742666666667">
maries. But they give high accuracies on machine
summary sentences. So for machine and human gen-
erated text, clearly different features matter.
</bodyText>
<sectionHeader confidence="0.99729" genericHeader="method">
4 Corpora for text quality
</sectionHeader>
<bodyText confidence="0.999982111111111">
For the automatic summarization genre, several
years of evaluation workshops organized by NIST1
have created large-scale datasets of automatic sum-
maries rated manually by people for content and lin-
guistic quality. We utilize this data for our experi-
ments but such corpora do not exist for other genres.
For academic writing, we plan to use a collection
of biology journal articles marked with the impact
factor of the journal. The intuition is that the pop-
ular journals are more competitive and so the writ-
ing is on average better than less impactful venues.
It is however not a direct measure of text quality.
For some of our experiments done so far, we have
taken an approach that is common with prior studies
on coherence (Barzilay and Lee, 2004; Barzilay and
Lapata, 2008; Lin et al., 2011). We take an original
article and create a random permutation of its sen-
tences, the latter we consider as an incoherent article
and the original version as coherent.
For science news, we expect that Amazon Me-
chanical Turk will be a suitable platform for obtain-
ing ratings of popular and interesting articles from
the target audience. We also plan to use proxies such
as lists of most emailed/viewed articles from news
websites. Here the negative examples would be
other articles published during the same day/period
but not appearing in the popular article list.
</bodyText>
<sectionHeader confidence="0.973655" genericHeader="method">
5 Patterns in communicative goals
</sectionHeader>
<bodyText confidence="0.99987275">
Consider the related work section of a conference
paper. One might suppose that a good structure for
this section would contain a description of an at-
tribute of the current work, followed by previous
work on the topic and then reporting how the current
work is different and addresses shortcomings if any
of prior work. In fact, this intuition of seeing texts
as a sequence of semantic zones is well-understood
for the academic writing genre. Prior research has
identified that a small set of argumentative zones ex-
ist in academic articles such as motivation, results,
prior work, speculations and descriptions. They also
</bodyText>
<footnote confidence="0.925421">
1http://www.nist.gov/tac/
</footnote>
<bodyText confidence="0.999993611111111">
found that sentences could be manually annotated
into zones with high agreement and automatically
predicting the zone for a sentence can also be done
with high accuracy (Teufel and Moens, 2000; Li-
akata et al., 2010). We hypothesize that these zones
would also have a certain distribution and sequence
in well-written articles versus others and propose a
metric based on this aspect for the academic writing
and science journalism genres.
Rather than using a predefined set of communica-
tive goals, we develop an unsupervised technique
to identify analogs to semantic zones and use the
patterns in zones to predict coherence (Louis and
Nenkova, 2012a). Our key idea is that the syntax
of a sentence can be a useful proxy for its commu-
nicative goal. For example, questions and definition
sentences have unique syntax. We extend this idea
to a large scale analysis. Our model represents a sen-
tence either using productions from its constituency
parse tree or as a sequence of phrasal nodes. Then
we employ two methods that learn patterns in these
representations from a collection of articles. The
first local method detects patterns in the syntax of
adjacent sentences. The second approach is global,
where sentences are first grouped into clusters based
on syntactic similarity and a Hidden Markov Model
is used to record patterns. Each hidden state is as-
sumed to generate the syntax of sentences from a
particular zone.
We have evaluated our method on conference
publications from the ACL anthology. Our results
indicate that we can distinguish an original introduc-
tion, abstract or related work section from a corre-
sponding perturbed version (where the sentences are
randomly permuted and is therefore incoherent text)
with accuracies of 64 to 74% over a 50% baseline.
</bodyText>
<sectionHeader confidence="0.989999" genericHeader="method">
6 General-specific nature of sentences
</sectionHeader>
<bodyText confidence="0.9993345">
In any article, some sentences convey the topic at
a high level with other sentences providing details
such as justification and examples. The idea is par-
ticularly relevant for summaries. Since summaries
are much shorter than their source documents, they
cannot include all the details from the source. Some
details have to be omitted and others made more
general. So we explore the preferred degree of
general-specific content and its relationship to text
quality for summaries.
</bodyText>
<page confidence="0.996471">
57
</page>
<bodyText confidence="0.99999445945946">
We developed a classifier to distinguish between
general and specific sentences from news articles
(Louis and Nenkova, 2011a; Louis and Nenkova,
2012b). The classifier uses features such as the word
specificity, presence of named entities, word polar-
ity, counts of different phrase types, sentence length,
likelihood under language models and the identities
of the words themselves. For example, sentences
with named entities tended to be specific whereas
sentences with shorter verb phrases and more polar-
ity words were general. This classifier was trained
on sentences multiply annotated by people as gen-
eral or specific and produces an accuracy of about
79%. Further the classifier confidence was found to
be indicative of the annotator agreement on the sen-
tences; when there was high agreement that a sen-
tence was either general or specific, the classifier
also made a very confident prediction for the correct
class. So our system also provides a graded score
for specificity rather than binary predictions.
Using the classifier we analyzed a large corpus of
news summaries created by people and by automatic
systems (Louis and Nenkova, 2011b). We found
that summaries written by people have more general
content than automatic summaries. Similarly, when
people were asked to rate automatic summaries for
content quality, they gave higher scores to general
summaries than specific. On the linguistic quality
side an opposite trend was found. Summaries that
were more specific had higher scores. Our examina-
tions revealed that general sentences, since they are
topic oriented and high level, need to be followed
by proper substantiation and details. But automatic
systems are rather poor at achieving such ordering.
So even though more general content is preferred in
summaries, proper ordering of general-specific sen-
tences is needed to create the right effect.
</bodyText>
<sectionHeader confidence="0.993444" genericHeader="method">
7 Information cohesiveness
</sectionHeader>
<bodyText confidence="0.999990849056604">
If an article has too many ideas it would be difficult
to read. Also if the ideas were not closely related
in the article that would create additional difficulty.
This aspect is important for machine generated text:
an automatic summary should focus on a few main
aspects rather than present a bag of many unrelated
facts. In fact, in large scale evaluation workshops,
automatic summaries are also manually graded for a
‘focus’ aspect. For this purpose, we want to identify
metrics which can indicate cohesiveness and focus
of an article. In our studies so far, we have have
developed cohesiveness metrics for clusters of arti-
cles (Nenkova and Louis, 2008; Louis and Nenkova,
2009). In future work, we will explore how these
metrics work for individual articles.
Information quality also arises in the context of
source documents given for automatic summariza-
tion. Particularly for systems which summarize on-
line news, the input is created by clustering together
news on the same topic from different sources. For
example, a cluster may be created for the Japanese
earthquake and aftermath. When the period covered
is too large or when the documents discuss many
different opinions and ideas it becomes hard for a
system to point out the most relevant facts. So one
proxy for cohesiveness of the input cluster is the av-
erage quality of a number of automatic summaries
produced for it by different methods. If most of
these methods fail to produce a good summary, then
that input can be deemed as difficult and incohesive.
We used a large collection of inputs, their au-
tomatic summaries and summary scores from the
DUC workshops. We computed the average content
quality score given by people to each summary and
computed the average performance on summaries
created for the same input. This value represents the
expected system performance for that input and we
develop features to predict the same. We simplify
the task as binary prediction, average system perfor-
mance above mean value – low difficulty, and high
difficulty otherwise.
One indicative feature was the entropy of the dis-
tribution of words in the input. When the entropy
was low, the difficulty was less since there are few
main ideas to summarize. Another useful feature
was the divergence computed between the word dis-
tribution in an input and that of a random collection
of documents not on any topic. If the input distri-
bution was closer to random documents it indicates
the lack of a coherent topic for the source cluster
and such inputs were under the hard category. We
envision that similar features might help to predict
judgements of focus for automatic summaries.
</bodyText>
<sectionHeader confidence="0.907928" genericHeader="conclusions">
8 Current and future work
</sectionHeader>
<bodyText confidence="0.9484065">
For future work, we want to focus on metrics related
to style of writing. We will do this analysis for sci-
</bodyText>
<page confidence="0.993723">
58
</page>
<bodyText confidence="0.9891561">
ence news articles since journalists employ creative
ways to convey technical research content to non-
experts readers. For example, authors use analogies
and visual language and incorporate a story line. We
also noticed that some of the most emailed articles
are entertaining and even contain humor. Two exam-
ple snippets from such articles are provided below to
demonstrate some of our intuitions about text quality
in this genre. Our aim is to obtain lexical and syn-
tactic correlates that capture some of these unique
factors for this domain.
[1]... caused by defects in the cilia—solitary slivers
that poke out of almost every cell in the body. They are
not the wisps that wave Rockette-like in our airways.
[2] News flash: we’re boring. New research that makes
creative use of sensitive location-tracking data from cell-
phones in Europe suggests that most people can be found
in one of just a few locations at any time.
Future work will also include extensive evaluation
of our proposed models.
</bodyText>
<sectionHeader confidence="0.998113" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999680231707317">
R. Barzilay and M. Lapata. 2008. Modeling local coher-
ence: An entity-based approach. Computational Lin-
guistics, 34(1):1–34.
R. Barzilay and L. Lee. 2004. Catching the drift: Proba-
bilistic content models, with applications to generation
and summarization. In Proceedings of IAACL-HLT,
pages 113–120.
J. Burstein, J. Tetreault, and S. Andreyev. 2010. Using
entity-based features to model coherence in student es-
says. In Proceedings ofHLT-IAACL, pages 681–684.
K. Collins-Thompson and J. Callan. 2004. A language
modeling approach to predicting reading difficulty. In
Proceedings ofHLT-IAACL, pages 193–200.
E. Dale and J. S. Chall. 1948. A formula for predicting
readability. Edu. Research Bulletin, 27(1):11–28.
R. Dale and A. Kilgarriff. 2010. Helping our own:
text massaging for computational linguistics as a new
shared task. In Proceedings ofIILG, pages 263–267.
L. Feng, N. Elhadad, and M. Huenerfauth. 2009. Cog-
nitively motivated features for readability assessment.
In Proceedings of EACL, pages 229–237.
R. Flesch. 1948. A new readability yardstick. Journal of
Applied Psychology, 32:221 – 233.
R. Gunning. 1952. The technique of clear writing.
McGraw-Hill; Fouth Printing edition.
M. Heilman, K. Collins-Thompson, J. Callan, and M. Es-
kenazi. 2007. Combining lexical and grammati-
cal features to improve readability measures for first
and second language texts. In Proceedings of HLT-
IAACL, pages 460–467.
M. Liakata, S. Teufel, A. Siddharthan, and C. Batchelor.
2010. Corpora for the conceptualisation and zoning of
scientific papers. In Proceedings ofLREC.
Z. Lin, H. Ng, and M. Kan. 2011. Automatically evalu-
ating text coherence using discourse relations. In Pro-
ceedings ofACL-HLT, pages 997–1006.
A. Louis and A. Nenkova. 2009. Performance con-
fidence estimation for automatic summarization. In
Proceedings of EACL, pages 541–548.
A. Louis and A. Nenkova. 2011a. Automatic identi-
fication of general and specific sentences by leverag-
ing discourse annotations. In Proceedings ofIJCILP,
pages 605–613.
A. Louis and A. Nenkova. 2011b. Text specificity and
impact on quality of news summaries. In Proceedings
of the Workshop on Monolingual Text-To-Text Genera-
tion, pages 34–42.
A. Louis and A. Nenkova. 2012a. A coherence model
based on syntactic patterns. Technical Report, Univer-
sity of Pennsylvania.
A. Louis and A. Nenkova. 2012b. A corpus of general
and specific sentences from news. In Proceedings of
LREC.
N. McIntyre and M. Lapata. 2009. Learning to tell tales:
A data-driven approach to story generation. In Pro-
ceedings ofACL-IJCILP, pages 217–225.
A. Nenkova and A. Louis. 2008. Can you summa-
rize this? identifying correlates of input difficulty for
multi-document summarization. In Proceedings of
ACL-HLT, pages 825–833.
E. Pitler and A. Nenkova. 2008. Revisiting readabil-
ity: A unified framework for predicting text quality. In
Proceedings ofEMILP, pages 186–195.
E. Pitler, A. Louis, and A. Nenkova. 2010. Automatic
evaluation of linguistic quality in multi-document
summarization. In Proceedings ofACL.
S. Schwarm and M. Ostendorf. 2005. Reading level as-
sessment using support vector machines and statistical
language models. In Proceedings ofACL, pages 523–
530.
L. Si and J. Callan. 2001. A statistical model for scien-
tific readability. In Proceedings of CIKM, pages 574–
576.
R. Soricut and D. Marcu. 2006. Discourse generation us-
ing utility-trained coherence models. In Proceedings
of COLIIG-ACL, pages 803–810.
S. Teufel and M. Moens. 2000. What’s yours and what’s
mine: determining intellectual attribution in scientific
text. In Proceedings of EMILP, pages 9–17.
J. Zhao and M. Kan. 2010. Domain-specific iterative
readability computation. In Proceedings of JDCL,
pages 205–214.
</reference>
<page confidence="0.999261">
59
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.896797">
<title confidence="0.997587">Automatic Metrics for Genre-specific Text Quality</title>
<author confidence="0.97147">Annie</author>
<affiliation confidence="0.999458">University of</affiliation>
<address confidence="0.928814">Philadelphia, PA 19103,</address>
<email confidence="0.999818">lannie@seas.upenn.edu</email>
<abstract confidence="0.999650266666667">To date, researchers have proposed different ways to compute the readability and coherence of a text using a variety of lexical, syntax, entity and discourse properties. But these metrics have not been defined with special relevance to any particular genre but rather proposed as general indicators of writing quality. In this thesis, we propose and evaluate novel text quality metrics that utilize the unique properties of different genres. We focus on three genres: academic publications, news articles about science, and machine generated text, in particular the output from automatic text summarization systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="1569" citStr="Barzilay and Lapata, 2008" startWordPosition="241" endWordPosition="245">support and grading tools. There are two main categories of prior work on this topic. The first is studies on ‘readability’ which have proposed metrics to select texts appropriate (easy to read) for an audience of given age and education level (Flesch, 1948; Collins-Thompson and Callan, 2004). These metrics typically classify texts as suitable for adult or child, or into a more fine-grained set of 12 educational grade levels. The second line of work are recent computational metrics to predict coherence. These methods identify regularities in words (Barzilay and Lee, 2004), entity coreference (Barzilay and Lapata, 2008) and discourse relations (Pitler and Nenkova, 2008) from a large collection of ar54 ticles and use these patterns to predict the coherence. They assume a particular competency level (adult educated readers) and also fix the text (typically news articles, which are appropriate for adult readers). By removing the focus on age/education level, these methods compute textual differences between good and poorly written texts as perceived by a single audience level. In my thesis, I propose a new definition – text quality: the overall well-written characteristic of an article. It differs from prior wo</context>
<context position="10900" citStr="Barzilay and Lapata, 2008" startWordPosition="1717" endWordPosition="1721">the number of entities a person should keep in working memory for that text and how far entity links stretch. Heilman et al. (2007) show that grammatical features make a bigger impact while predicting readability for second language learners in contrast to native speakers. Newer coherence measures do not focus on reader abilities. They are typically run on news articles and assume an adult audience. They show that word co-occurrence (Soricut and Marcu, 2006), subtopic structure (Barzilay and Lee, 2004), discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) and coreference patterns (Barzilay and Lapata, 2008) learn from large corpora can be used to predict coherence. But prior metrics are not proposed as unique to any genre. Some metrics using word patterns (Si and Callan, 2001; Barzilay and Lee, 2004) are domaindependent in that they require documents from the target domain for training. But they can be trained for any domain in this manner. However recent work show that genre-specific indicators could be quite useful for applications. McIntyre and Lapata (2009) automatically generate short children’s stories using patterns of event and entity co-occurrences. They find that people judge their sto</context>
<context position="13429" citStr="Barzilay and Lapata, 2008" startWordPosition="2129" endWordPosition="2132">es rated manually by people for content and linguistic quality. We utilize this data for our experiments but such corpora do not exist for other genres. For academic writing, we plan to use a collection of biology journal articles marked with the impact factor of the journal. The intuition is that the popular journals are more competitive and so the writing is on average better than less impactful venues. It is however not a direct measure of text quality. For some of our experiments done so far, we have taken an approach that is common with prior studies on coherence (Barzilay and Lee, 2004; Barzilay and Lapata, 2008; Lin et al., 2011). We take an original article and create a random permutation of its sentences, the latter we consider as an incoherent article and the original version as coherent. For science news, we expect that Amazon Mechanical Turk will be a suitable platform for obtaining ratings of popular and interesting articles from the target audience. We also plan to use proxies such as lists of most emailed/viewed articles from news websites. Here the negative examples would be other articles published during the same day/period but not appearing in the popular article list. 5 Patterns in comm</context>
</contexts>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>R. Barzilay and M. Lapata. 2008. Modeling local coherence: An entity-based approach. Computational Linguistics, 34(1):1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>L Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of IAACL-HLT,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="1521" citStr="Barzilay and Lee, 2004" startWordPosition="235" endWordPosition="238">arch and recommendation systems, and writing support and grading tools. There are two main categories of prior work on this topic. The first is studies on ‘readability’ which have proposed metrics to select texts appropriate (easy to read) for an audience of given age and education level (Flesch, 1948; Collins-Thompson and Callan, 2004). These metrics typically classify texts as suitable for adult or child, or into a more fine-grained set of 12 educational grade levels. The second line of work are recent computational metrics to predict coherence. These methods identify regularities in words (Barzilay and Lee, 2004), entity coreference (Barzilay and Lapata, 2008) and discourse relations (Pitler and Nenkova, 2008) from a large collection of ar54 ticles and use these patterns to predict the coherence. They assume a particular competency level (adult educated readers) and also fix the text (typically news articles, which are appropriate for adult readers). By removing the focus on age/education level, these methods compute textual differences between good and poorly written texts as perceived by a single audience level. In my thesis, I propose a new definition – text quality: the overall well-written charac</context>
<context position="10781" citStr="Barzilay and Lee, 2004" startWordPosition="1700" endWordPosition="1703">ls. Feng et al. (2009) consider adult readers with intellectual disability and therefore introduce features such as the number of entities a person should keep in working memory for that text and how far entity links stretch. Heilman et al. (2007) show that grammatical features make a bigger impact while predicting readability for second language learners in contrast to native speakers. Newer coherence measures do not focus on reader abilities. They are typically run on news articles and assume an adult audience. They show that word co-occurrence (Soricut and Marcu, 2006), subtopic structure (Barzilay and Lee, 2004), discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) and coreference patterns (Barzilay and Lapata, 2008) learn from large corpora can be used to predict coherence. But prior metrics are not proposed as unique to any genre. Some metrics using word patterns (Si and Callan, 2001; Barzilay and Lee, 2004) are domaindependent in that they require documents from the target domain for training. But they can be trained for any domain in this manner. However recent work show that genre-specific indicators could be quite useful for applications. McIntyre and Lapata (2009) automatically gen</context>
<context position="13402" citStr="Barzilay and Lee, 2004" startWordPosition="2125" endWordPosition="2128">ets of automatic summaries rated manually by people for content and linguistic quality. We utilize this data for our experiments but such corpora do not exist for other genres. For academic writing, we plan to use a collection of biology journal articles marked with the impact factor of the journal. The intuition is that the popular journals are more competitive and so the writing is on average better than less impactful venues. It is however not a direct measure of text quality. For some of our experiments done so far, we have taken an approach that is common with prior studies on coherence (Barzilay and Lee, 2004; Barzilay and Lapata, 2008; Lin et al., 2011). We take an original article and create a random permutation of its sentences, the latter we consider as an incoherent article and the original version as coherent. For science news, we expect that Amazon Mechanical Turk will be a suitable platform for obtaining ratings of popular and interesting articles from the target audience. We also plan to use proxies such as lists of most emailed/viewed articles from news websites. Here the negative examples would be other articles published during the same day/period but not appearing in the popular artic</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>R. Barzilay and L. Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In Proceedings of IAACL-HLT, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Burstein</author>
<author>J Tetreault</author>
<author>S Andreyev</author>
</authors>
<title>Using entity-based features to model coherence in student essays.</title>
<date>2010</date>
<booktitle>In Proceedings ofHLT-IAACL,</booktitle>
<pages>681--684</pages>
<contexts>
<context position="11728" citStr="Burstein et al. (2010)" startWordPosition="1852" endWordPosition="1855">independent in that they require documents from the target domain for training. But they can be trained for any domain in this manner. However recent work show that genre-specific indicators could be quite useful for applications. McIntyre and Lapata (2009) automatically generate short children’s stories using patterns of event and entity co-occurrences. They find that people judge their stories as better when the text is optimized not only for coherence and but also its interesting nature. They use a supervised approach to predict the interest value for a story during the generation process. Burstein et al. (2010) find that for predicting the coherence of student essays, better accuracies can be obtained by augmenting generic coherence metrics with features related to student writing such as word variety and spelling errors. In my own work on automatic evaluation of summaries (Pitler et al., 2010), I have observed the impact of genre. We consider a corpus of summaries written by people and those produced by automatic systems. Psycholinguistic metrics previously proposed for analyzing coherence of human texts work successfully on human summaries but are less accurate for system summaries. Similarly, met</context>
</contexts>
<marker>Burstein, Tetreault, Andreyev, 2010</marker>
<rawString>J. Burstein, J. Tetreault, and S. Andreyev. 2010. Using entity-based features to model coherence in student essays. In Proceedings ofHLT-IAACL, pages 681–684.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Collins-Thompson</author>
<author>J Callan</author>
</authors>
<title>A language modeling approach to predicting reading difficulty.</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT-IAACL,</booktitle>
<pages>193--200</pages>
<contexts>
<context position="1236" citStr="Collins-Thompson and Callan, 2004" startWordPosition="189" endWordPosition="192">three genres: academic publications, news articles about science, and machine generated text, in particular the output from automatic text summarization systems. 1 Introduction Automatic methods to measure the writing quality of a text can be quite useful for several applications, for example search and recommendation systems, and writing support and grading tools. There are two main categories of prior work on this topic. The first is studies on ‘readability’ which have proposed metrics to select texts appropriate (easy to read) for an audience of given age and education level (Flesch, 1948; Collins-Thompson and Callan, 2004). These metrics typically classify texts as suitable for adult or child, or into a more fine-grained set of 12 educational grade levels. The second line of work are recent computational metrics to predict coherence. These methods identify regularities in words (Barzilay and Lee, 2004), entity coreference (Barzilay and Lapata, 2008) and discourse relations (Pitler and Nenkova, 2008) from a large collection of ar54 ticles and use these patterns to predict the coherence. They assume a particular competency level (adult educated readers) and also fix the text (typically news articles, which are ap</context>
<context position="9945" citStr="Collins-Thompson and Callan, 2004" startWordPosition="1568" endWordPosition="1571">fic metrics. We expect that on this data, the generic features would be best with little improvement from the genre-specific metrics. So far, we have designed some of the metrics that we described above and have found them to be predictive of writing quality. We will carry out extensive evaluation of these measures in future work. 3 Related work Early readability metrics used sentence length, number of syllables in words and number of ‘easy’ words to distinguish texts from different grade levels (Flesch, 1948; Gunning, 1952; Dale and Chall, 1948). Other measures are based on word familiarity (Collins-Thompson and Callan, 2004; Si and Callan, 2001), difficulty of concepts (Zhao and Kan, 2010) and features of sentence syntax (Schwarm and Ostendorf, 2005). There are also readability studies for audience distinctions other than grade levels. Feng et al. (2009) consider adult readers with intellectual disability and therefore introduce features such as the number of entities a person should keep in working memory for that text and how far entity links stretch. Heilman et al. (2007) show that grammatical features make a bigger impact while predicting readability for second language learners in contrast to native speaker</context>
</contexts>
<marker>Collins-Thompson, Callan, 2004</marker>
<rawString>K. Collins-Thompson and J. Callan. 2004. A language modeling approach to predicting reading difficulty. In Proceedings ofHLT-IAACL, pages 193–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Dale</author>
<author>J S Chall</author>
</authors>
<title>A formula for predicting readability.</title>
<date>1948</date>
<journal>Edu. Research Bulletin,</journal>
<volume>27</volume>
<issue>1</issue>
<contexts>
<context position="9864" citStr="Dale and Chall, 1948" startWordPosition="1557" endWordPosition="1560">, we will test the performance of generic and each set of genre-specific metrics. We expect that on this data, the generic features would be best with little improvement from the genre-specific metrics. So far, we have designed some of the metrics that we described above and have found them to be predictive of writing quality. We will carry out extensive evaluation of these measures in future work. 3 Related work Early readability metrics used sentence length, number of syllables in words and number of ‘easy’ words to distinguish texts from different grade levels (Flesch, 1948; Gunning, 1952; Dale and Chall, 1948). Other measures are based on word familiarity (Collins-Thompson and Callan, 2004; Si and Callan, 2001), difficulty of concepts (Zhao and Kan, 2010) and features of sentence syntax (Schwarm and Ostendorf, 2005). There are also readability studies for audience distinctions other than grade levels. Feng et al. (2009) consider adult readers with intellectual disability and therefore introduce features such as the number of entities a person should keep in working memory for that text and how far entity links stretch. Heilman et al. (2007) show that grammatical features make a bigger impact while </context>
</contexts>
<marker>Dale, Chall, 1948</marker>
<rawString>E. Dale and J. S. Chall. 1948. A formula for predicting readability. Edu. Research Bulletin, 27(1):11–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>A Kilgarriff</author>
</authors>
<title>Helping our own: text massaging for computational linguistics as a new shared task.</title>
<date>2010</date>
<booktitle>In Proceedings ofIILG,</booktitle>
<pages>263--267</pages>
<contexts>
<context position="5330" citStr="Dale and Kilgarriff, 2010" startWordPosition="830" endWordPosition="833">d recommendation for news articles. We also include a third genre—automatically generated summaries. Here, when systems produce multi-sentence text, they must ensure that the text is readable and coherent. Automatic evaluation of content and linguistic quality is therefore necessary for system development in this genre. 2 Thesis Summary and Contributions For this thesis, we only consider the discourse and style components of text quality, aspects that have received less focus in prior work. Sentence-level problems have been widely explored and recently, even specifically for academic writing (Dale and Kilgarriff, 2010). We also do not consider content in our work, for example, academic writing quality also depends on the ideas and arguments presented but these aspects are outside the scope of this thesis. As defined previously, we focus on a fixed audience level. We assume a reader at the top level of the competency spectrum: an adult educated reader for science news and automatic summaries, and for academic articles, an expert on the topic. This definition has minimal focus on reader abilities and allows us to analyze textual differences exclusively. The specific contributions of this thesis are: 1. Defini</context>
</contexts>
<marker>Dale, Kilgarriff, 2010</marker>
<rawString>R. Dale and A. Kilgarriff. 2010. Helping our own: text massaging for computational linguistics as a new shared task. In Proceedings ofIILG, pages 263–267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Feng</author>
<author>N Elhadad</author>
<author>M Huenerfauth</author>
</authors>
<title>Cognitively motivated features for readability assessment.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>229--237</pages>
<contexts>
<context position="10180" citStr="Feng et al. (2009)" startWordPosition="1605" endWordPosition="1608"> quality. We will carry out extensive evaluation of these measures in future work. 3 Related work Early readability metrics used sentence length, number of syllables in words and number of ‘easy’ words to distinguish texts from different grade levels (Flesch, 1948; Gunning, 1952; Dale and Chall, 1948). Other measures are based on word familiarity (Collins-Thompson and Callan, 2004; Si and Callan, 2001), difficulty of concepts (Zhao and Kan, 2010) and features of sentence syntax (Schwarm and Ostendorf, 2005). There are also readability studies for audience distinctions other than grade levels. Feng et al. (2009) consider adult readers with intellectual disability and therefore introduce features such as the number of entities a person should keep in working memory for that text and how far entity links stretch. Heilman et al. (2007) show that grammatical features make a bigger impact while predicting readability for second language learners in contrast to native speakers. Newer coherence measures do not focus on reader abilities. They are typically run on news articles and assume an adult audience. They show that word co-occurrence (Soricut and Marcu, 2006), subtopic structure (Barzilay and Lee, 2004</context>
</contexts>
<marker>Feng, Elhadad, Huenerfauth, 2009</marker>
<rawString>L. Feng, N. Elhadad, and M. Huenerfauth. 2009. Cognitively motivated features for readability assessment. In Proceedings of EACL, pages 229–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Flesch</author>
</authors>
<title>A new readability yardstick.</title>
<date>1948</date>
<journal>Journal of Applied Psychology,</journal>
<pages>32--221</pages>
<contexts>
<context position="1200" citStr="Flesch, 1948" startWordPosition="187" endWordPosition="188">. We focus on three genres: academic publications, news articles about science, and machine generated text, in particular the output from automatic text summarization systems. 1 Introduction Automatic methods to measure the writing quality of a text can be quite useful for several applications, for example search and recommendation systems, and writing support and grading tools. There are two main categories of prior work on this topic. The first is studies on ‘readability’ which have proposed metrics to select texts appropriate (easy to read) for an audience of given age and education level (Flesch, 1948; Collins-Thompson and Callan, 2004). These metrics typically classify texts as suitable for adult or child, or into a more fine-grained set of 12 educational grade levels. The second line of work are recent computational metrics to predict coherence. These methods identify regularities in words (Barzilay and Lee, 2004), entity coreference (Barzilay and Lapata, 2008) and discourse relations (Pitler and Nenkova, 2008) from a large collection of ar54 ticles and use these patterns to predict the coherence. They assume a particular competency level (adult educated readers) and also fix the text (t</context>
<context position="9826" citStr="Flesch, 1948" startWordPosition="1553" endWordPosition="1554"> quality ratings. On this set, we will test the performance of generic and each set of genre-specific metrics. We expect that on this data, the generic features would be best with little improvement from the genre-specific metrics. So far, we have designed some of the metrics that we described above and have found them to be predictive of writing quality. We will carry out extensive evaluation of these measures in future work. 3 Related work Early readability metrics used sentence length, number of syllables in words and number of ‘easy’ words to distinguish texts from different grade levels (Flesch, 1948; Gunning, 1952; Dale and Chall, 1948). Other measures are based on word familiarity (Collins-Thompson and Callan, 2004; Si and Callan, 2001), difficulty of concepts (Zhao and Kan, 2010) and features of sentence syntax (Schwarm and Ostendorf, 2005). There are also readability studies for audience distinctions other than grade levels. Feng et al. (2009) consider adult readers with intellectual disability and therefore introduce features such as the number of entities a person should keep in working memory for that text and how far entity links stretch. Heilman et al. (2007) show that grammatica</context>
</contexts>
<marker>Flesch, 1948</marker>
<rawString>R. Flesch. 1948. A new readability yardstick. Journal of Applied Psychology, 32:221 – 233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Gunning</author>
</authors>
<title>The technique of clear writing. McGraw-Hill; Fouth Printing edition.</title>
<date>1952</date>
<contexts>
<context position="9841" citStr="Gunning, 1952" startWordPosition="1555" endWordPosition="1556">gs. On this set, we will test the performance of generic and each set of genre-specific metrics. We expect that on this data, the generic features would be best with little improvement from the genre-specific metrics. So far, we have designed some of the metrics that we described above and have found them to be predictive of writing quality. We will carry out extensive evaluation of these measures in future work. 3 Related work Early readability metrics used sentence length, number of syllables in words and number of ‘easy’ words to distinguish texts from different grade levels (Flesch, 1948; Gunning, 1952; Dale and Chall, 1948). Other measures are based on word familiarity (Collins-Thompson and Callan, 2004; Si and Callan, 2001), difficulty of concepts (Zhao and Kan, 2010) and features of sentence syntax (Schwarm and Ostendorf, 2005). There are also readability studies for audience distinctions other than grade levels. Feng et al. (2009) consider adult readers with intellectual disability and therefore introduce features such as the number of entities a person should keep in working memory for that text and how far entity links stretch. Heilman et al. (2007) show that grammatical features make</context>
</contexts>
<marker>Gunning, 1952</marker>
<rawString>R. Gunning. 1952. The technique of clear writing. McGraw-Hill; Fouth Printing edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Heilman</author>
<author>K Collins-Thompson</author>
<author>J Callan</author>
<author>M Eskenazi</author>
</authors>
<title>Combining lexical and grammatical features to improve readability measures for first and second language texts.</title>
<date>2007</date>
<booktitle>In Proceedings of HLTIAACL,</booktitle>
<pages>460--467</pages>
<contexts>
<context position="10405" citStr="Heilman et al. (2007)" startWordPosition="1642" endWordPosition="1645"> from different grade levels (Flesch, 1948; Gunning, 1952; Dale and Chall, 1948). Other measures are based on word familiarity (Collins-Thompson and Callan, 2004; Si and Callan, 2001), difficulty of concepts (Zhao and Kan, 2010) and features of sentence syntax (Schwarm and Ostendorf, 2005). There are also readability studies for audience distinctions other than grade levels. Feng et al. (2009) consider adult readers with intellectual disability and therefore introduce features such as the number of entities a person should keep in working memory for that text and how far entity links stretch. Heilman et al. (2007) show that grammatical features make a bigger impact while predicting readability for second language learners in contrast to native speakers. Newer coherence measures do not focus on reader abilities. They are typically run on news articles and assume an adult audience. They show that word co-occurrence (Soricut and Marcu, 2006), subtopic structure (Barzilay and Lee, 2004), discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) and coreference patterns (Barzilay and Lapata, 2008) learn from large corpora can be used to predict coherence. But prior metrics are not proposed as unique </context>
</contexts>
<marker>Heilman, Collins-Thompson, Callan, Eskenazi, 2007</marker>
<rawString>M. Heilman, K. Collins-Thompson, J. Callan, and M. Eskenazi. 2007. Combining lexical and grammatical features to improve readability measures for first and second language texts. In Proceedings of HLTIAACL, pages 460–467.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Liakata</author>
<author>S Teufel</author>
<author>A Siddharthan</author>
<author>C Batchelor</author>
</authors>
<title>Corpora for the conceptualisation and zoning of scientific papers.</title>
<date>2010</date>
<booktitle>In Proceedings ofLREC.</booktitle>
<contexts>
<context position="14908" citStr="Liakata et al., 2010" startWordPosition="2369" endWordPosition="2373">he current work is different and addresses shortcomings if any of prior work. In fact, this intuition of seeing texts as a sequence of semantic zones is well-understood for the academic writing genre. Prior research has identified that a small set of argumentative zones exist in academic articles such as motivation, results, prior work, speculations and descriptions. They also 1http://www.nist.gov/tac/ found that sentences could be manually annotated into zones with high agreement and automatically predicting the zone for a sentence can also be done with high accuracy (Teufel and Moens, 2000; Liakata et al., 2010). We hypothesize that these zones would also have a certain distribution and sequence in well-written articles versus others and propose a metric based on this aspect for the academic writing and science journalism genres. Rather than using a predefined set of communicative goals, we develop an unsupervised technique to identify analogs to semantic zones and use the patterns in zones to predict coherence (Louis and Nenkova, 2012a). Our key idea is that the syntax of a sentence can be a useful proxy for its communicative goal. For example, questions and definition sentences have unique syntax. </context>
</contexts>
<marker>Liakata, Teufel, Siddharthan, Batchelor, 2010</marker>
<rawString>M. Liakata, S. Teufel, A. Siddharthan, and C. Batchelor. 2010. Corpora for the conceptualisation and zoning of scientific papers. In Proceedings ofLREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Lin</author>
<author>H Ng</author>
<author>M Kan</author>
</authors>
<title>Automatically evaluating text coherence using discourse relations.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL-HLT,</booktitle>
<pages>997--1006</pages>
<contexts>
<context position="10847" citStr="Lin et al., 2011" startWordPosition="1710" endWordPosition="1713">ty and therefore introduce features such as the number of entities a person should keep in working memory for that text and how far entity links stretch. Heilman et al. (2007) show that grammatical features make a bigger impact while predicting readability for second language learners in contrast to native speakers. Newer coherence measures do not focus on reader abilities. They are typically run on news articles and assume an adult audience. They show that word co-occurrence (Soricut and Marcu, 2006), subtopic structure (Barzilay and Lee, 2004), discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) and coreference patterns (Barzilay and Lapata, 2008) learn from large corpora can be used to predict coherence. But prior metrics are not proposed as unique to any genre. Some metrics using word patterns (Si and Callan, 2001; Barzilay and Lee, 2004) are domaindependent in that they require documents from the target domain for training. But they can be trained for any domain in this manner. However recent work show that genre-specific indicators could be quite useful for applications. McIntyre and Lapata (2009) automatically generate short children’s stories using patterns of event and entity </context>
<context position="13448" citStr="Lin et al., 2011" startWordPosition="2133" endWordPosition="2136"> for content and linguistic quality. We utilize this data for our experiments but such corpora do not exist for other genres. For academic writing, we plan to use a collection of biology journal articles marked with the impact factor of the journal. The intuition is that the popular journals are more competitive and so the writing is on average better than less impactful venues. It is however not a direct measure of text quality. For some of our experiments done so far, we have taken an approach that is common with prior studies on coherence (Barzilay and Lee, 2004; Barzilay and Lapata, 2008; Lin et al., 2011). We take an original article and create a random permutation of its sentences, the latter we consider as an incoherent article and the original version as coherent. For science news, we expect that Amazon Mechanical Turk will be a suitable platform for obtaining ratings of popular and interesting articles from the target audience. We also plan to use proxies such as lists of most emailed/viewed articles from news websites. Here the negative examples would be other articles published during the same day/period but not appearing in the popular article list. 5 Patterns in communicative goals Con</context>
</contexts>
<marker>Lin, Ng, Kan, 2011</marker>
<rawString>Z. Lin, H. Ng, and M. Kan. 2011. Automatically evaluating text coherence using discourse relations. In Proceedings ofACL-HLT, pages 997–1006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Louis</author>
<author>A Nenkova</author>
</authors>
<title>Performance confidence estimation for automatic summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>541--548</pages>
<contexts>
<context position="19559" citStr="Louis and Nenkova, 2009" startWordPosition="3111" endWordPosition="3114"> read. Also if the ideas were not closely related in the article that would create additional difficulty. This aspect is important for machine generated text: an automatic summary should focus on a few main aspects rather than present a bag of many unrelated facts. In fact, in large scale evaluation workshops, automatic summaries are also manually graded for a ‘focus’ aspect. For this purpose, we want to identify metrics which can indicate cohesiveness and focus of an article. In our studies so far, we have have developed cohesiveness metrics for clusters of articles (Nenkova and Louis, 2008; Louis and Nenkova, 2009). In future work, we will explore how these metrics work for individual articles. Information quality also arises in the context of source documents given for automatic summarization. Particularly for systems which summarize online news, the input is created by clustering together news on the same topic from different sources. For example, a cluster may be created for the Japanese earthquake and aftermath. When the period covered is too large or when the documents discuss many different opinions and ideas it becomes hard for a system to point out the most relevant facts. So one proxy for cohes</context>
</contexts>
<marker>Louis, Nenkova, 2009</marker>
<rawString>A. Louis and A. Nenkova. 2009. Performance confidence estimation for automatic summarization. In Proceedings of EACL, pages 541–548.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Louis</author>
<author>A Nenkova</author>
</authors>
<title>Automatic identification of general and specific sentences by leveraging discourse annotations.</title>
<date>2011</date>
<booktitle>In Proceedings ofIJCILP,</booktitle>
<pages>605--613</pages>
<contexts>
<context position="17107" citStr="Louis and Nenkova, 2011" startWordPosition="2722" endWordPosition="2725">ure of sentences In any article, some sentences convey the topic at a high level with other sentences providing details such as justification and examples. The idea is particularly relevant for summaries. Since summaries are much shorter than their source documents, they cannot include all the details from the source. Some details have to be omitted and others made more general. So we explore the preferred degree of general-specific content and its relationship to text quality for summaries. 57 We developed a classifier to distinguish between general and specific sentences from news articles (Louis and Nenkova, 2011a; Louis and Nenkova, 2012b). The classifier uses features such as the word specificity, presence of named entities, word polarity, counts of different phrase types, sentence length, likelihood under language models and the identities of the words themselves. For example, sentences with named entities tended to be specific whereas sentences with shorter verb phrases and more polarity words were general. This classifier was trained on sentences multiply annotated by people as general or specific and produces an accuracy of about 79%. Further the classifier confidence was found to be indicative </context>
</contexts>
<marker>Louis, Nenkova, 2011</marker>
<rawString>A. Louis and A. Nenkova. 2011a. Automatic identification of general and specific sentences by leveraging discourse annotations. In Proceedings ofIJCILP, pages 605–613.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Louis</author>
<author>A Nenkova</author>
</authors>
<title>Text specificity and impact on quality of news summaries.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Monolingual Text-To-Text Generation,</booktitle>
<pages>34--42</pages>
<contexts>
<context position="17107" citStr="Louis and Nenkova, 2011" startWordPosition="2722" endWordPosition="2725">ure of sentences In any article, some sentences convey the topic at a high level with other sentences providing details such as justification and examples. The idea is particularly relevant for summaries. Since summaries are much shorter than their source documents, they cannot include all the details from the source. Some details have to be omitted and others made more general. So we explore the preferred degree of general-specific content and its relationship to text quality for summaries. 57 We developed a classifier to distinguish between general and specific sentences from news articles (Louis and Nenkova, 2011a; Louis and Nenkova, 2012b). The classifier uses features such as the word specificity, presence of named entities, word polarity, counts of different phrase types, sentence length, likelihood under language models and the identities of the words themselves. For example, sentences with named entities tended to be specific whereas sentences with shorter verb phrases and more polarity words were general. This classifier was trained on sentences multiply annotated by people as general or specific and produces an accuracy of about 79%. Further the classifier confidence was found to be indicative </context>
</contexts>
<marker>Louis, Nenkova, 2011</marker>
<rawString>A. Louis and A. Nenkova. 2011b. Text specificity and impact on quality of news summaries. In Proceedings of the Workshop on Monolingual Text-To-Text Generation, pages 34–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Louis</author>
<author>A Nenkova</author>
</authors>
<title>A coherence model based on syntactic patterns.</title>
<date>2012</date>
<tech>Technical Report,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="15340" citStr="Louis and Nenkova, 2012" startWordPosition="2438" endWordPosition="2441">ld be manually annotated into zones with high agreement and automatically predicting the zone for a sentence can also be done with high accuracy (Teufel and Moens, 2000; Liakata et al., 2010). We hypothesize that these zones would also have a certain distribution and sequence in well-written articles versus others and propose a metric based on this aspect for the academic writing and science journalism genres. Rather than using a predefined set of communicative goals, we develop an unsupervised technique to identify analogs to semantic zones and use the patterns in zones to predict coherence (Louis and Nenkova, 2012a). Our key idea is that the syntax of a sentence can be a useful proxy for its communicative goal. For example, questions and definition sentences have unique syntax. We extend this idea to a large scale analysis. Our model represents a sentence either using productions from its constituency parse tree or as a sequence of phrasal nodes. Then we employ two methods that learn patterns in these representations from a collection of articles. The first local method detects patterns in the syntax of adjacent sentences. The second approach is global, where sentences are first grouped into clusters b</context>
<context position="17133" citStr="Louis and Nenkova, 2012" startWordPosition="2726" endWordPosition="2729">ticle, some sentences convey the topic at a high level with other sentences providing details such as justification and examples. The idea is particularly relevant for summaries. Since summaries are much shorter than their source documents, they cannot include all the details from the source. Some details have to be omitted and others made more general. So we explore the preferred degree of general-specific content and its relationship to text quality for summaries. 57 We developed a classifier to distinguish between general and specific sentences from news articles (Louis and Nenkova, 2011a; Louis and Nenkova, 2012b). The classifier uses features such as the word specificity, presence of named entities, word polarity, counts of different phrase types, sentence length, likelihood under language models and the identities of the words themselves. For example, sentences with named entities tended to be specific whereas sentences with shorter verb phrases and more polarity words were general. This classifier was trained on sentences multiply annotated by people as general or specific and produces an accuracy of about 79%. Further the classifier confidence was found to be indicative of the annotator agreement</context>
</contexts>
<marker>Louis, Nenkova, 2012</marker>
<rawString>A. Louis and A. Nenkova. 2012a. A coherence model based on syntactic patterns. Technical Report, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Louis</author>
<author>A Nenkova</author>
</authors>
<title>A corpus of general and specific sentences from news.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="15340" citStr="Louis and Nenkova, 2012" startWordPosition="2438" endWordPosition="2441">ld be manually annotated into zones with high agreement and automatically predicting the zone for a sentence can also be done with high accuracy (Teufel and Moens, 2000; Liakata et al., 2010). We hypothesize that these zones would also have a certain distribution and sequence in well-written articles versus others and propose a metric based on this aspect for the academic writing and science journalism genres. Rather than using a predefined set of communicative goals, we develop an unsupervised technique to identify analogs to semantic zones and use the patterns in zones to predict coherence (Louis and Nenkova, 2012a). Our key idea is that the syntax of a sentence can be a useful proxy for its communicative goal. For example, questions and definition sentences have unique syntax. We extend this idea to a large scale analysis. Our model represents a sentence either using productions from its constituency parse tree or as a sequence of phrasal nodes. Then we employ two methods that learn patterns in these representations from a collection of articles. The first local method detects patterns in the syntax of adjacent sentences. The second approach is global, where sentences are first grouped into clusters b</context>
<context position="17133" citStr="Louis and Nenkova, 2012" startWordPosition="2726" endWordPosition="2729">ticle, some sentences convey the topic at a high level with other sentences providing details such as justification and examples. The idea is particularly relevant for summaries. Since summaries are much shorter than their source documents, they cannot include all the details from the source. Some details have to be omitted and others made more general. So we explore the preferred degree of general-specific content and its relationship to text quality for summaries. 57 We developed a classifier to distinguish between general and specific sentences from news articles (Louis and Nenkova, 2011a; Louis and Nenkova, 2012b). The classifier uses features such as the word specificity, presence of named entities, word polarity, counts of different phrase types, sentence length, likelihood under language models and the identities of the words themselves. For example, sentences with named entities tended to be specific whereas sentences with shorter verb phrases and more polarity words were general. This classifier was trained on sentences multiply annotated by people as general or specific and produces an accuracy of about 79%. Further the classifier confidence was found to be indicative of the annotator agreement</context>
</contexts>
<marker>Louis, Nenkova, 2012</marker>
<rawString>A. Louis and A. Nenkova. 2012b. A corpus of general and specific sentences from news. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N McIntyre</author>
<author>M Lapata</author>
</authors>
<title>Learning to tell tales: A data-driven approach to story generation.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL-IJCILP,</booktitle>
<pages>217--225</pages>
<contexts>
<context position="11363" citStr="McIntyre and Lapata (2009)" startWordPosition="1794" endWordPosition="1797"> subtopic structure (Barzilay and Lee, 2004), discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) and coreference patterns (Barzilay and Lapata, 2008) learn from large corpora can be used to predict coherence. But prior metrics are not proposed as unique to any genre. Some metrics using word patterns (Si and Callan, 2001; Barzilay and Lee, 2004) are domaindependent in that they require documents from the target domain for training. But they can be trained for any domain in this manner. However recent work show that genre-specific indicators could be quite useful for applications. McIntyre and Lapata (2009) automatically generate short children’s stories using patterns of event and entity co-occurrences. They find that people judge their stories as better when the text is optimized not only for coherence and but also its interesting nature. They use a supervised approach to predict the interest value for a story during the generation process. Burstein et al. (2010) find that for predicting the coherence of student essays, better accuracies can be obtained by augmenting generic coherence metrics with features related to student writing such as word variety and spelling errors. In my own work on a</context>
</contexts>
<marker>McIntyre, Lapata, 2009</marker>
<rawString>N. McIntyre and M. Lapata. 2009. Learning to tell tales: A data-driven approach to story generation. In Proceedings ofACL-IJCILP, pages 217–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>A Louis</author>
</authors>
<title>Can you summarize this? identifying correlates of input difficulty for multi-document summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT,</booktitle>
<pages>825--833</pages>
<contexts>
<context position="19533" citStr="Nenkova and Louis, 2008" startWordPosition="3107" endWordPosition="3110"> it would be difficult to read. Also if the ideas were not closely related in the article that would create additional difficulty. This aspect is important for machine generated text: an automatic summary should focus on a few main aspects rather than present a bag of many unrelated facts. In fact, in large scale evaluation workshops, automatic summaries are also manually graded for a ‘focus’ aspect. For this purpose, we want to identify metrics which can indicate cohesiveness and focus of an article. In our studies so far, we have have developed cohesiveness metrics for clusters of articles (Nenkova and Louis, 2008; Louis and Nenkova, 2009). In future work, we will explore how these metrics work for individual articles. Information quality also arises in the context of source documents given for automatic summarization. Particularly for systems which summarize online news, the input is created by clustering together news on the same topic from different sources. For example, a cluster may be created for the Japanese earthquake and aftermath. When the period covered is too large or when the documents discuss many different opinions and ideas it becomes hard for a system to point out the most relevant fac</context>
</contexts>
<marker>Nenkova, Louis, 2008</marker>
<rawString>A. Nenkova and A. Louis. 2008. Can you summarize this? identifying correlates of input difficulty for multi-document summarization. In Proceedings of ACL-HLT, pages 825–833.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pitler</author>
<author>A Nenkova</author>
</authors>
<title>Revisiting readability: A unified framework for predicting text quality.</title>
<date>2008</date>
<booktitle>In Proceedings ofEMILP,</booktitle>
<pages>186--195</pages>
<contexts>
<context position="1620" citStr="Pitler and Nenkova, 2008" startWordPosition="249" endWordPosition="252">ries of prior work on this topic. The first is studies on ‘readability’ which have proposed metrics to select texts appropriate (easy to read) for an audience of given age and education level (Flesch, 1948; Collins-Thompson and Callan, 2004). These metrics typically classify texts as suitable for adult or child, or into a more fine-grained set of 12 educational grade levels. The second line of work are recent computational metrics to predict coherence. These methods identify regularities in words (Barzilay and Lee, 2004), entity coreference (Barzilay and Lapata, 2008) and discourse relations (Pitler and Nenkova, 2008) from a large collection of ar54 ticles and use these patterns to predict the coherence. They assume a particular competency level (adult educated readers) and also fix the text (typically news articles, which are appropriate for adult readers). By removing the focus on age/education level, these methods compute textual differences between good and poorly written texts as perceived by a single audience level. In my thesis, I propose a new definition – text quality: the overall well-written characteristic of an article. It differs from prior work in three respects: 1. We consider a single fixed</context>
<context position="10828" citStr="Pitler and Nenkova, 2008" startWordPosition="1706" endWordPosition="1709">with intellectual disability and therefore introduce features such as the number of entities a person should keep in working memory for that text and how far entity links stretch. Heilman et al. (2007) show that grammatical features make a bigger impact while predicting readability for second language learners in contrast to native speakers. Newer coherence measures do not focus on reader abilities. They are typically run on news articles and assume an adult audience. They show that word co-occurrence (Soricut and Marcu, 2006), subtopic structure (Barzilay and Lee, 2004), discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) and coreference patterns (Barzilay and Lapata, 2008) learn from large corpora can be used to predict coherence. But prior metrics are not proposed as unique to any genre. Some metrics using word patterns (Si and Callan, 2001; Barzilay and Lee, 2004) are domaindependent in that they require documents from the target domain for training. But they can be trained for any domain in this manner. However recent work show that genre-specific indicators could be quite useful for applications. McIntyre and Lapata (2009) automatically generate short children’s stories using patterns o</context>
</contexts>
<marker>Pitler, Nenkova, 2008</marker>
<rawString>E. Pitler and A. Nenkova. 2008. Revisiting readability: A unified framework for predicting text quality. In Proceedings ofEMILP, pages 186–195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pitler</author>
<author>A Louis</author>
<author>A Nenkova</author>
</authors>
<title>Automatic evaluation of linguistic quality in multi-document summarization.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="12017" citStr="Pitler et al., 2010" startWordPosition="1899" endWordPosition="1902">dren’s stories using patterns of event and entity co-occurrences. They find that people judge their stories as better when the text is optimized not only for coherence and but also its interesting nature. They use a supervised approach to predict the interest value for a story during the generation process. Burstein et al. (2010) find that for predicting the coherence of student essays, better accuracies can be obtained by augmenting generic coherence metrics with features related to student writing such as word variety and spelling errors. In my own work on automatic evaluation of summaries (Pitler et al., 2010), I have observed the impact of genre. We consider a corpus of summaries written by people and those produced by automatic systems. Psycholinguistic metrics previously proposed for analyzing coherence of human texts work successfully on human summaries but are less accurate for system summaries. Similarly, metrics which predict the fluency of machine translations accurately, work barely above baseline for judging the grammaticality of sentences from human sum56 maries. But they give high accuracies on machine summary sentences. So for machine and human generated text, clearly different feature</context>
</contexts>
<marker>Pitler, Louis, Nenkova, 2010</marker>
<rawString>E. Pitler, A. Louis, and A. Nenkova. 2010. Automatic evaluation of linguistic quality in multi-document summarization. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Schwarm</author>
<author>M Ostendorf</author>
</authors>
<title>Reading level assessment using support vector machines and statistical language models.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="10074" citStr="Schwarm and Ostendorf, 2005" startWordPosition="1588" endWordPosition="1592">So far, we have designed some of the metrics that we described above and have found them to be predictive of writing quality. We will carry out extensive evaluation of these measures in future work. 3 Related work Early readability metrics used sentence length, number of syllables in words and number of ‘easy’ words to distinguish texts from different grade levels (Flesch, 1948; Gunning, 1952; Dale and Chall, 1948). Other measures are based on word familiarity (Collins-Thompson and Callan, 2004; Si and Callan, 2001), difficulty of concepts (Zhao and Kan, 2010) and features of sentence syntax (Schwarm and Ostendorf, 2005). There are also readability studies for audience distinctions other than grade levels. Feng et al. (2009) consider adult readers with intellectual disability and therefore introduce features such as the number of entities a person should keep in working memory for that text and how far entity links stretch. Heilman et al. (2007) show that grammatical features make a bigger impact while predicting readability for second language learners in contrast to native speakers. Newer coherence measures do not focus on reader abilities. They are typically run on news articles and assume an adult audienc</context>
</contexts>
<marker>Schwarm, Ostendorf, 2005</marker>
<rawString>S. Schwarm and M. Ostendorf. 2005. Reading level assessment using support vector machines and statistical language models. In Proceedings ofACL, pages 523– 530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Si</author>
<author>J Callan</author>
</authors>
<title>A statistical model for scientific readability.</title>
<date>2001</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>574--576</pages>
<contexts>
<context position="9967" citStr="Si and Callan, 2001" startWordPosition="1572" endWordPosition="1575"> data, the generic features would be best with little improvement from the genre-specific metrics. So far, we have designed some of the metrics that we described above and have found them to be predictive of writing quality. We will carry out extensive evaluation of these measures in future work. 3 Related work Early readability metrics used sentence length, number of syllables in words and number of ‘easy’ words to distinguish texts from different grade levels (Flesch, 1948; Gunning, 1952; Dale and Chall, 1948). Other measures are based on word familiarity (Collins-Thompson and Callan, 2004; Si and Callan, 2001), difficulty of concepts (Zhao and Kan, 2010) and features of sentence syntax (Schwarm and Ostendorf, 2005). There are also readability studies for audience distinctions other than grade levels. Feng et al. (2009) consider adult readers with intellectual disability and therefore introduce features such as the number of entities a person should keep in working memory for that text and how far entity links stretch. Heilman et al. (2007) show that grammatical features make a bigger impact while predicting readability for second language learners in contrast to native speakers. Newer coherence mea</context>
</contexts>
<marker>Si, Callan, 2001</marker>
<rawString>L. Si and J. Callan. 2001. A statistical model for scientific readability. In Proceedings of CIKM, pages 574– 576.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>D Marcu</author>
</authors>
<title>Discourse generation using utility-trained coherence models.</title>
<date>2006</date>
<booktitle>In Proceedings of COLIIG-ACL,</booktitle>
<pages>803--810</pages>
<contexts>
<context position="10736" citStr="Soricut and Marcu, 2006" startWordPosition="1693" endWordPosition="1696">or audience distinctions other than grade levels. Feng et al. (2009) consider adult readers with intellectual disability and therefore introduce features such as the number of entities a person should keep in working memory for that text and how far entity links stretch. Heilman et al. (2007) show that grammatical features make a bigger impact while predicting readability for second language learners in contrast to native speakers. Newer coherence measures do not focus on reader abilities. They are typically run on news articles and assume an adult audience. They show that word co-occurrence (Soricut and Marcu, 2006), subtopic structure (Barzilay and Lee, 2004), discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) and coreference patterns (Barzilay and Lapata, 2008) learn from large corpora can be used to predict coherence. But prior metrics are not proposed as unique to any genre. Some metrics using word patterns (Si and Callan, 2001; Barzilay and Lee, 2004) are domaindependent in that they require documents from the target domain for training. But they can be trained for any domain in this manner. However recent work show that genre-specific indicators could be quite useful for applications.</context>
</contexts>
<marker>Soricut, Marcu, 2006</marker>
<rawString>R. Soricut and D. Marcu. 2006. Discourse generation using utility-trained coherence models. In Proceedings of COLIIG-ACL, pages 803–810.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>M Moens</author>
</authors>
<title>What’s yours and what’s mine: determining intellectual attribution in scientific text.</title>
<date>2000</date>
<booktitle>In Proceedings of EMILP,</booktitle>
<pages>9--17</pages>
<contexts>
<context position="14885" citStr="Teufel and Moens, 2000" startWordPosition="2365" endWordPosition="2368">and then reporting how the current work is different and addresses shortcomings if any of prior work. In fact, this intuition of seeing texts as a sequence of semantic zones is well-understood for the academic writing genre. Prior research has identified that a small set of argumentative zones exist in academic articles such as motivation, results, prior work, speculations and descriptions. They also 1http://www.nist.gov/tac/ found that sentences could be manually annotated into zones with high agreement and automatically predicting the zone for a sentence can also be done with high accuracy (Teufel and Moens, 2000; Liakata et al., 2010). We hypothesize that these zones would also have a certain distribution and sequence in well-written articles versus others and propose a metric based on this aspect for the academic writing and science journalism genres. Rather than using a predefined set of communicative goals, we develop an unsupervised technique to identify analogs to semantic zones and use the patterns in zones to predict coherence (Louis and Nenkova, 2012a). Our key idea is that the syntax of a sentence can be a useful proxy for its communicative goal. For example, questions and definition sentenc</context>
</contexts>
<marker>Teufel, Moens, 2000</marker>
<rawString>S. Teufel and M. Moens. 2000. What’s yours and what’s mine: determining intellectual attribution in scientific text. In Proceedings of EMILP, pages 9–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zhao</author>
<author>M Kan</author>
</authors>
<title>Domain-specific iterative readability computation.</title>
<date>2010</date>
<booktitle>In Proceedings of JDCL,</booktitle>
<pages>205--214</pages>
<contexts>
<context position="10012" citStr="Zhao and Kan, 2010" startWordPosition="1579" endWordPosition="1582"> little improvement from the genre-specific metrics. So far, we have designed some of the metrics that we described above and have found them to be predictive of writing quality. We will carry out extensive evaluation of these measures in future work. 3 Related work Early readability metrics used sentence length, number of syllables in words and number of ‘easy’ words to distinguish texts from different grade levels (Flesch, 1948; Gunning, 1952; Dale and Chall, 1948). Other measures are based on word familiarity (Collins-Thompson and Callan, 2004; Si and Callan, 2001), difficulty of concepts (Zhao and Kan, 2010) and features of sentence syntax (Schwarm and Ostendorf, 2005). There are also readability studies for audience distinctions other than grade levels. Feng et al. (2009) consider adult readers with intellectual disability and therefore introduce features such as the number of entities a person should keep in working memory for that text and how far entity links stretch. Heilman et al. (2007) show that grammatical features make a bigger impact while predicting readability for second language learners in contrast to native speakers. Newer coherence measures do not focus on reader abilities. They </context>
</contexts>
<marker>Zhao, Kan, 2010</marker>
<rawString>J. Zhao and M. Kan. 2010. Domain-specific iterative readability computation. In Proceedings of JDCL, pages 205–214.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>