<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.164583">
<title confidence="0.980678">
A Multi-Dimensional Bayesian Approach to Lexical Style
</title>
<author confidence="0.9982">
Julian Brooke
</author>
<affiliation confidence="0.9993575">
Department of Computer Science
University of Toronto
</affiliation>
<email confidence="0.993005">
jbrooke@cs.toronto.edu
</email>
<author confidence="0.992717">
Graeme Hirst
</author>
<affiliation confidence="0.999346">
Department of Computer Science
University of Toronto
</affiliation>
<email confidence="0.996541">
gh@cs.toronto.edu
</email>
<sectionHeader confidence="0.99859" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999933666666667">
We adapt the popular LDA topic model (Blei
et al., 2003) to the representation of stylistic
lexical information, evaluating our model on
the basis of human-interpretability at the word
and text level. We show, in particular, that this
model can be applied to the task of inducing
stylistic lexicons, and that a multi-dimensional
approach is warranted given the correlations
among stylistic dimensions.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999982217391304">
In language, stylistic variation is a reflection of var-
ious contextual factors, including the backgrounds
of and relationship between the parties involved.
Although in the context of prescriptive linguistics
(Strunk and White, 1979), style is often assumed to
be a matter of aesthetics, the stylistic intuitions of
language users are inextricably linked to the conven-
tions of register and genre (Biber and Conrad, 2009).
Intentional or not, stylistic differences play a role
in numerous NLP tasks. Examples include genre
classification (Kessler et al., 1997), author profil-
ing (Garera and Yarowsky, 2009; Rosenthal and Mc-
Keown, 2011), social relationship classification (Pe-
terson et al., 2011), sentiment analysis (Wilson et al.,
2005), readability classification (Collins-Thompson
and Callan, 2005), and text generation (Hovy, 1990;
Inkpen and Hirst, 2006). Following the classic work
of Biber (1988), computational modeling of style
has often focused on textual statistics and the fre-
quency of function words and syntactic categories.
When content words are considered, they are of-
ten limited to manually-constructed lists (Argamon
et al., 2007), or used as individual features for su-
pervised classification, which can be confounded by
topic (Petrenz and Webber, 2011) or fail in the face
of lexical variety. Our interest is models that offer
broad lexical coverage of human-identifiable stylis-
tic variation.
Research most similar to ours has focused on clas-
sifying the lexicon in terms of individual aspects rel-
evant to style (e.g. formality, specificity, readability)
(Brooke et al., 2010; Pan and Yang, 2010; Kidwell
et al., 2009) and a large body of research on the in-
duction of polarity lexicons, in particular from large
corpora (Turney, 2002; Kaji and Kitsuregawa, 2007;
Velikovich et al., 2010). Our work is the first to rep-
resent multiple dimensions of style in a single statis-
tical model, adapting latent Dirichlet allocation (Blei
et al., 2003), a Bayesian ‘topic’ model, to our stylis-
tic purposes; as such, our approach also follows on
recent interest in the interpretability of topic-model
topics (Chang et al., 2009; Newman et al., 2011).
We show that our model can be used for acquisition
of stylistic lexicons, and we also evaluate the model
relative to theories of register variation and the ex-
pected stylistic character of particular genres.
</bodyText>
<sectionHeader confidence="0.999177" genericHeader="introduction">
2 Model
</sectionHeader>
<subsectionHeader confidence="0.988675">
2.1 Linguistic foundations
</subsectionHeader>
<bodyText confidence="0.999959166666667">
In English manuals of style and other prescriptivist
texts (Fowler and Fowler, 1906; Gunning, 1952;
Follett, 1966; Strunk and White, 1979; Kane, 1983;
Hayakawa, 1994), writers are urged to pay atten-
tion to various aspects of lexical style, including el-
ements such as clarity, familiarity, readability, for-
</bodyText>
<page confidence="0.990322">
673
</page>
<subsectionHeader confidence="0.293532">
Proceedings of NAACL-HLT 2013, pages 673–679,
</subsectionHeader>
<bodyText confidence="0.991242512195122">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
mality, fanciness, colloquialness, specificity, con-
creteness, objectivity, and naturalness; these stylis-
tic categories reflect common aesthetic judgments
about language. In descriptive studies of register,
some researchers have posited a few fixed styles
(Joos, 1961) or a small, discrete set of situational
constraints which determine style and register (Crys-
tal and Davy, 1969; Halliday and Hasan, 1976); by
contrast, the applied approach of Biber (1988) and
theoretical framework of Leckie-Tarry (1995) offer a
more continuous interpretation of register variation.
In Biber’s approach, functional dimensions such
as Involved vs. Informational, Argumentative vs.
Non-argumentative, and Abstract vs. non-Abstract
are derived in an unsupervised manner from a
mixed-genre corpus, with the labels assigned de-
pending on where features (a small set of known in-
dicators of register) and genres fall on each spec-
trum. The theory of Leckie-Tarry posits a single
main cline of register with one pole (the oral pole)
reflecting a full reliance on the context of the lin-
guistic situation, and the other (the literate pole) re-
flecting a reliance on cultural knowledge. The more
specific elements of register are represented as sub-
clines which are strongly influenced by this main
cline, creating probabilistic relationships between
related dimensions (Birch, 1995).
For the present study, we have chosen 3 dimen-
sions (6 styles) which are clearly represented in the
lexicon, which are discussed often in the relevant lit-
erature, and which fit well into the Leckie-Tarry con-
ception of related subclines: colloquial vs. literary,
concrete vs. abstract, and subjective vs. objective. In
addition to a negative correlation between opposing
styles, we also expect a positive correlation between
stylistic aspects that tend toward the same main pole,
situational (i.e. colloquial, concrete, subjective) or
cultural (i.e. literary, abstract, objective). These cor-
relations can potentially interfere with accurate lex-
ical acquisition.
</bodyText>
<subsectionHeader confidence="0.997739">
2.2 Implementation
</subsectionHeader>
<bodyText confidence="0.999982823529412">
Our main model is an adaption of the popular latent
Dirichlet allocation topic model (Blei et al., 2003),
with each of the 6 styles corresponding to a topic.
Briefly, latent Dirichlet allocation (LDA) is a gener-
ative Bayesian model: for each document d, a dis-
tribution of topics θd is drawn from a Dirichlet prior
(with parameter α). For each topic z, there is a prob-
ability distribution βz1 corresponding to the proba-
bility of that topic generating any given word in the
vocabulary. Words in document d are generated by
first selecting a topic z randomly according to θd,
and then randomly selecting a word w according to
βz. An extension of LDA, the correlated topic model
(CTM) (Blei and Lafferty, 2007), supposes a more
complex representation of topics: given a matrix E
representing the covariance between topics and µ
representing the means, for each document a topic
distribution η (analogous to θ) is drawn from the
logistic normal distribution. Given a corpus, good
estimates for the relevant parameters can be derived
using Bayesian inference.
For both LDA and CTM we use the original
variational Bayes implementation of Blei. Varia-
tional Bayes (VB) works by approximating the true
posterior with a simpler distribution, minimizing
the Kullback-Leibler divergence between the two
through iterative updates of specially-introduced
free variables. The mathematical and algorithmic
details are omitted here; see Blei et al. (2003; 2007).
Our early investigations used an online, batch ver-
sion of LDA (Hoffman et al., 2010), which is more
appropriate for large corpora because it requires
only a single iteration over the dataset. We discov-
ered, however, that batch models were markedly in-
ferior to more traditional models for our purposes
because the influence of the initial model diminishes
too quickly; here, we need particular topics in the
model to correspond to particular styles, and we ac-
complish this by seeding the model with known in-
stances of each style (see Section 3). Specifically,
our initial β consists of distributions where the entire
probability mass is divided amongst the seeds for
each corresponding topic, and a full iteration over
the corpus occurs before β is updated. Typically,
LDA iterates over the corpus until a convergence re-
quirement is met, but in this case this is neither prac-
tical (due to the size of our corpus) nor necessarily
desirable; the diminishing effects of the initial seed-
ing means that the model may not stabilize, in terms
of its likelihood, until after it has shifted away from
our desired stylistic dimensions towards some other
</bodyText>
<footnote confidence="0.850894666666667">
1Some versions of LDA smooth this distribution using a
Dirichlet prior; here, though, we use the original formulation
from Blei (2003), which does not.
</footnote>
<page confidence="0.997939">
674
</page>
<bodyText confidence="0.999979466666667">
variation in the data. Therefore, we treat the optimal
number of iterations as a variable to investigate.
The model is trained on a 1 million text por-
tion of the 2009 version of the ICWSM Spinn3r
dataset (Burton et al., 2009), a corpus of blogs we
have previously used for formality lexicon induction
(Brooke et al., 2010). Since our method relies on co-
occurrence, we followed our earlier work in using
only texts with at least 100 different word types. All
words were tokenized and converted to lower-case,
with no further lemmatization. Following Hoffman
et al. (2010), we initialized the α of our models to
1/k where k is the number of topics. Otherwise we
used the default settings; when they overlap they
were identical for the LDA and CTM models.
</bodyText>
<sectionHeader confidence="0.993783" genericHeader="method">
3 Lexicon Induction
</sectionHeader>
<bodyText confidence="0.999987878048781">
Our primary evaluation is based on the stylistic in-
duction of held-out seed words. The words were
collected from various sources by the first author
and further reviewed by the second; we are both
native speakers of English with significant experi-
ence in English linguistics. Included words had to
be clear, extreme members of their stylistic cate-
gory, with little or no ambiguity with respect to their
style. The colloquial seeds consist of English slang
terms and acronyms, e.g. cuz, gig, asshole, lol. The
literary seeds were primarily drawn from web sites
which explain difficult language in texts such as the
Bible and Lord of the Rings; examples include be-
hold, resplendent, amiss, and thine. The concrete
seeds all denote objects and actions strongly rooted
in the physical world, e.g. shove and lamppost, while
the abstract seeds all involve concepts which require
significant human psychological or cultural knowl-
edge to grasp, for instance patriotism and noncha-
lant. For our subjective seeds, we used an edited
list of strongly positive and negative terms from a
manually-constructed sentiment lexicon (Taboada et
al., 2011), e.g. gorgeous and depraved, and for our
objective set we selected words from sets of near-
synonyms where one was clearly an emotionally-
distant alternative, e.g. residence (for home), jocu-
lar (for funny) and communicable (for contagious).
We filtered initial lists to 150 of each type, remov-
ing words which did not appear in the corpus or
which occurred in multiple lists. For evaluation we
used stratified 3-fold crossvalidation, averaged over
5 different (3-way) splits of the seeds, with the same
splits used for all evaluated conditions.
Given two sets of opposing seeds, we follow our
earlier work in evaluating our performance in terms
of the number of pairings of seeds from each set
which have the expected stylistic relationship rel-
ative to each other (the guessing baseline is 0.5).
Given a word w and two opposing styles (topics) p
and n, we place w on the PN dimension according to
the β of our trained model as follows:
</bodyText>
<equation confidence="0.961277">
_
PNw Pp w − βnw
βpw + βnw
</equation>
<bodyText confidence="0.999627342857143">
The normalization is important because otherwise
more-common words would tend to have higher
PN’s, when in fact the opposite is true (rare words
tend to be more stylistically prominent). We then
calculate pairwise accuracy as the percentage of
pairs (wp,wn) (wp E Pseeds and wn E Nseeds) where
PNwp &gt; PNwn. However, this metric does not address
the case where the degree of a word in one stylistic
dimension is overestimated because of its status on
a parallel dimension. Two more-holistic alternatives
are total accuracy, the percentage of seeds for which
the highest βtw is the topic t for which w is a seed
(guessing baseline is 0.17), and the average rank of
the correct t as ordered by βtw (in the range 1–6,
guessing baseline is 3.5); the latter is more forgiving
of near misses.
We tested a few options which involved straight-
forward modifications to model training. Standard
LDA produces all tokens in the document, but when
dealing with style rather than topic, the number of
times a word appears is much less relevant (Brooke
et al., 2010). Our binary model assumes an LDA
that generates types, not tokens.2 A key comparison
2At the theoretical level, this move is admittedly problem-
atic, since our LDA model is thus being trained under the as-
sumption that texts with multiple instances of the same type can
be generated, when of course such texts cannot by definition ex-
ist. We might address this by moving to Bayesian models with
very different generative assumptions, e.g. the spherical topic
model (Reisinger et al., 2010), but these methods involve a sig-
nificant increase of computational complexity and we believe
that on a practical level there are no real negatives associated
with directly using a binary representation as input to LDA; in
fact, we are avoiding what appears to be a much more serious
problem, burstiness (Doyle and Elkan, 2009), i.e. the fact that
</bodyText>
<page confidence="0.992977">
675
</page>
<table confidence="0.999935076923077">
Model Total Acc. (%) Avg. Rank
Pairwise Accuracy (%)
Lit/Col Abs/Con Obj/Sub All
50.0 50.0 50.0 50.0 16.6. 3.50
94.3 98.8 93.0 95.4 55.0 1.79
96.2 98.9 93.5 96.2 57.7 1.74
95.4 99.2 93.3 96.0 53.1 1.86
96.3 99.0 89.6 95.0 53.0 1.87
guessing baseline
basic LDA (iter 2)
binary LDA (iter 2)
combo binary LDA (iter 1)
binary CTM (iter 1)
</table>
<tableCaption confidence="0.999897">
Table 1: Model performance in lexical induction of seeds. Bold indicates best in column.
</tableCaption>
<bodyText confidence="0.998630732142857">
here is with a combined LDA model (combo), an
amalgamation of three independently trained 2-topic
models, one for each dimension; this tests our key
hypothesis that training dimensions of style together
is beneficial. Finally, we test against the correlated
topic model (CTM), which offers an explicit repre-
sentation of style correlation, but which has done
poorly with respect to interpretability, despite offer-
ing better perplexity (Chang et al., 2009).
The results of the lexicon induction evaluation
are in Table 1. Since the number of optimal iter-
ations varies, we report the result from the best of
the first five iterations, as measured by total accu-
racy; the best iteration is shown in parenthesis. In
general, all the results are high enough—we are re-
liably above 90% for the pairwise task, and above
50% for the 6-way task—for us to conclude with
some confidence that our model is capturing a sig-
nificant amount of stylistic variation. As predicted,
using words as boolean features had a net positive
gain, consistent across all of our metrics, though this
effect was not as marked as we have seen previously.
The model with independent training of each dimen-
sion (combo) did noticeably worse, supporting our
conclusion that a multidimensional approach is war-
ranted here. Particularly striking is the much larger
drop in overall accuracy as compared to pairwise ac-
curacy, which suggests that the combo model is cap-
turing the general trends but not distinguishing cor-
related styles as well. However, the most complex
model, the CTM, actually does slightly worse than
the combo, which was contrary to our expectations
but nonetheless consistent with previous work on the
interpretability of topic models. The performance of
the full LDA models benefited from a second itera-
traditional LDA is influenced too much by multiple instances of
the same word.
tion, but this was not true of combo LDA or CTM,
and the performance of all models dropped after the
second iteration.
An analysis of individual errors reveals, unsur-
prisingly, that most of the errors occur across styles
on the same pole; by far the largest single com-
mon misclassification is objective words to abstract.
Of the words that consistently show this misclas-
sification across the runs, many of them, e.g. ani-
mate, aperture, encircle, and constrain are clearly
errors (if anything, these words tend towards con-
creteness), but in other cases the word in question
is arguably also fairly abstract, e.g. categorize and
predominant, and might not be labeled an error at
all. Other signs that our model might be doing bet-
ter than our total accuracy metric gives it credit for:
many of the subjective words that are consistently
mislabeled as literary have an exaggerated, literary
feel, e.g. jubilant, grievous, and malevolent.
</bodyText>
<sectionHeader confidence="0.998605" genericHeader="method">
4 Text-level Analysis
</sectionHeader>
<bodyText confidence="0.9999434375">
Our secondary analysis involved evaluating the 0’s
of our best configuration (based on average pairwise
and total accuracy) on other texts. After training,
we carried out inference on the BNC corpus, aver-
aging the resulting 0’s to see which styles are asso-
ciated with which genres. Appearances of the seed
terms for each model were disregarded during this
process; only the induced part of the lexicon was
used. The average differences relative to the mean
across the various stylistic dimensions (as measured
by the probabilities in 0) are given for a selection of
genres in Table 2.
The most obvious pattern in table 2 is the domi-
nance of the medium: all written genres are positive
for our styles on the ‘cultural’ pole and negative for
styles on the ‘situational’ pole and the opposite is
</bodyText>
<page confidence="0.998288">
676
</page>
<table confidence="0.999688666666667">
Genre Styles
Literary Abstract Objective Colloquial Concrete Subjective
News +0.67 +0.50 +0.43 −0.31 −0.72 −0.57
Religious texts +0.38 +0.38 +0.28 −0.27 −0.44 −0.32
Academic +0.18 +0.29 +0.26 −0.20 −0.36 −0.18
Fiction +0.31 +0.09 +0.02 −0.05 −0.12 −0.25
Meeting −0.61 −0.54 −0.42 +0.35 +0.69 +0.55
Courtroom −0.63 −0.53 −0.41 +0.32 +0.69 +0.57
Conversation −0.56 −0.63 −0.54 +0.43 +0.80 +0.50
</table>
<tableCaption confidence="0.9735695">
Table 2: Average differences from corpus mean of LDA-derived stylistic dimension probabilities for various genres in
the BNC, in hundredths.
</tableCaption>
<bodyText confidence="0.999893176470588">
true for spoken genres. The magnitude of this ef-
fect is more difficult to interpret: though it is clear
why fiction should sit on the boundary (since it con-
tains spoken dialogue), the appearance of news at
the written extreme is odd, though it might be due to
the fact that news blogs are the most prevalent for-
mal genre in the training corpus.
However, if we ignore magnitude and focus on the
relative ratios of the stylistic differences for styles
on the same pole, we can identify some individ-
ual stylistic effects among genres within the same
medium. Relative to the other written genres, for in-
stance, fiction is, sensibly, more literary and much
less objective, while academic texts are much more
abstract and objective; for the other two written gen-
res, the spread is more even, though relative to re-
ligious texts, news is more objective. At the sit-
uational pole, fiction also stands out, being much
more colloquial and concrete than other written gen-
res. Predictably, if we consider again the ratios
across styles, conversation is the most colloquial
genre here, though the difference is subtle.
We carried out a correlation analysis of the LDA-
reduced styles of all texts in the BNC and, con-
sistent with the genre results in Table 2, found a
strong positive correlation for all styles on the same
main pole, averaging 0.83. The average negative
correlation between opposing poles is even higher,
−0.88. This supports the Leckie-Tarry formulation.
The independence assumptions of the LDA model
did not prevent strong correlations from forming be-
tween these distinct yet clearly interrelated dimen-
sions; if anything, the correlations are stronger than
we would have predicted.
</bodyText>
<sectionHeader confidence="0.997771" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999990666666667">
We have introduced a Bayesian model of stylistic
variation. Topic models like LDA are often evalu-
ated using information-theoretic measures, but our
emphasis has been on interpretibility: at the word
level we can use the model to induce stylistic lex-
icons which correspond to human judgement, and
at the text level we can use it distinguish genres in
expected ways. Another theme has been to offer ev-
idence that indeed a multi-dimensional approach is
strongly warranted: importantly, our results indicate
that separate unidimensional models of style are in-
ferior for identifying the core stylistic character of
each word, and in our secondary analysis we found
strong correlations among styles attributable to the
situational/cultural dichotomy. However, an off-the-
shelf model that integrates correlation among topics
did not outperform basic LDA.
One advantage of a Bayesian approach is in the
flexibility of the model: there are any number of
other interesting possible extensions at both the 0
and P levels of the model, including alternative ap-
proaches to correlation (Li and McCallum, 2006).
Beyond Bayesian models, vector space and graphi-
cal approaches should be compared. More work is
clearly needed to improve evaluation: some of our
seeds could fall into multiple stylistic categories, so
a more detailed annotation would be useful.
</bodyText>
<sectionHeader confidence="0.984592" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.996467333333333">
This work was financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada.
</bodyText>
<page confidence="0.998226">
677
</page>
<sectionHeader confidence="0.983932" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999533028846154">
Shlomo Argamon, Casey Whitelaw, Paul Chase, Sob-
han Raj Hota, Navendu Garg, and Shlomo Levitan.
2007. Stylistic text classification using functional lex-
ical features. Journal of the American Society for In-
formation Science and Technology, 7:91–109.
Douglas Biber and Susan Conrad. 2009. Register, Genre,
and Style. Cambridge University Press.
Douglas Biber. 1988. Variation Across Speech and Writ-
ing. Cambridge University Press.
David Birch. 1995. Introduction. In Helen Leckie-Tarry,
editor, Language and Context: A Functional Linguis-
tic Theory of Register. Pinter.
David M. Blei and John D. Lafferty. 2007. Correlated
topic models. Annals of Applied Statistics, 1(1):17–
35.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent Dirichlet allocation.
Journal of Machine Learning Research, 3:993–1022.
Julian Brooke, Tong Wang, and Graeme Hirst. 2010. Au-
tomatic acquisition of lexical formality. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (COLING ’10), Beijing.
Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The
ICWSM 2009 Spinn3r Dataset. In Proceedings of the
Third Annual Conference on Weblogs and Social Me-
dia (ICWSM 2009), San Jose, CA.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Pro-
ceedings of Neural Information Processing Systems
(NIPS ’09).
Kevyn Collins-Thompson and Jamie Callan. 2005.
Predicting reading difficulty with statistical language
models. Journal of the American Society for Informa-
tion Science Technology, 56(13):1448–1462.
David Crystal and Derek Davy. 1969. Investigating En-
glish Style. Indiana University Press.
Gabriel Doyle and Charles Elkan. 2009. Accounting for
burstiness in topic models. In International Confer-
ence on Machine Learning (ICML ’09).
Wilson Follett. 1966. Modern American Usage. Hill &amp;
Wang, New York.
H. W. Fowler and F. G. Fowler. 1906. The King’s En-
glish. Clarendon Press, Oxford, 2nd edition.
Nikesh Garera and David Yarowsky. 2009. Modeling la-
tent biographic attributes in conversational genres. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP (ACL-IJCNLP ’09), pages 710–718, Sin-
gapore.
Robert Gunning. 1952. The Technique of Clear Writing.
McGraw-Hill, New York.
M.A.K. Halliday and Ruqaiya Hasan. 1976. Cohesion in
English. Longman, London.
S.I. Hayakawa, editor. 1994. Choose the Right Word.
HarperCollins Publishers, second edition. Revised by
Eugene Ehrlich.
Matthew D. Hoffman, David M. Blei, and Francis R.
Bach. 2010. Online learning for latent Dirichlet al-
location. In Neural Information Processing Systems
(NIPS ’10), pages 856–864.
Eduard H. Hovy. 1990. Pragmatics and natural language
generation. Artificial Intelligence, 43:153–197.
Diana Inkpen and Graeme Hirst. 2006. Building and
using a lexical knowledge base of near-synonym dif-
ferences. Computational Linguistics, 32(2):223–262.
Martin Joos. 1961. The Five Clocks. Harcourt, Brace
and World, New York.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing lexicon for sentiment analysis from massive col-
lection of HTML documents. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL ’07).
Thomas S. Kane. 1983. The Oxford Guide to Writing.
Oxford Univeristy Press.
Brett Kessler, Geoffrey Nunberg, and Hinrich Sch¨utze.
1997. Automatic detection of text genre. In Proceed-
ings of the 35th Annual Meeting of the Association
for Computational Linguistics (ACL ’97), pages 32–
38, Madrid, Spain.
Paul Kidwell, Guy Lebanon, and Kevyn Collins-
Thompson. 2009. Statistical estimation of word
acquisition with application to readability predic-
tion. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
(EMNLP’09), pages 900–909, Singapore.
Helen Leckie-Tarry. 1995. Language and Context: A
Functional Linguistic Theory of Register. Pinter.
Wei Li and Andrew McCallum. 2006. Pachinko alloca-
tion: DAG-structured mixture models of topic correla-
tions. In Proceedings of the 23rd International Con-
ference on Machine Learning, ICML ’06, pages 577–
584.
David Newman, Edwin V. Bonilla, and Wray Buntine.
2011. Improving topic coherence with regularized
topic models. In Proceedings of Advances in Neural
Information Processing Systems (NIPS ’11).
Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Transactions on Knowledge
and Data Engineering, 22(10).
Kelly Peterson, Matt Hohensee, and Fei Xia. 2011.
Email formality in the workplace: A case study on
</reference>
<page confidence="0.983782">
678
</page>
<reference confidence="0.999135325">
the Enron corpus. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics (ACL ’11), Portland, Oregon.
Philipp Petrenz and Bonnie Webber. 2011. Stable clas-
sification of text genres. Computational Linguistics,
37(2):385–393, June.
J. Reisinger, A. Waters, B. Silverthorn, and R. Mooney.
2010. Spherical topic models. In International Con-
ference on Machine Learning (ICML ’10).
Sara Rosenthal and Kathleen McKeown. 2011. Age pre-
diction in blogs: A study of style, content, and online
behavior in pre- and post-social media generations. In
Proceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ’11), Port-
land, Oregon.
William Strunk and E.B. White. 1979. The Elements of
Style. Macmillan, 3rd edition.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational Linguis-
tics, 37(2):267–307.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, ACL ’02, pages 417–424, Philadelphia, Pennsyl-
vania.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, HLT ’10, pages 777–785, Los An-
geles, California.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, HLT/EMNLP
’05, pages 347–354.
</reference>
<page confidence="0.999032">
679
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.921354">
<title confidence="0.999869">A Multi-Dimensional Bayesian Approach to Lexical Style</title>
<author confidence="0.998661">Julian</author>
<affiliation confidence="0.9997785">Department of Computer University of</affiliation>
<email confidence="0.998604">jbrooke@cs.toronto.edu</email>
<author confidence="0.931131">Graeme</author>
<affiliation confidence="0.999798">Department of Computer University of</affiliation>
<email confidence="0.999098">gh@cs.toronto.edu</email>
<abstract confidence="0.9993978">We adapt the popular LDA topic model (Blei et al., 2003) to the representation of stylistic lexical information, evaluating our model on the basis of human-interpretability at the word and text level. We show, in particular, that this model can be applied to the task of inducing stylistic lexicons, and that a multi-dimensional approach is warranted given the correlations among stylistic dimensions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shlomo Argamon</author>
<author>Casey Whitelaw</author>
<author>Paul Chase</author>
<author>Sobhan Raj Hota</author>
<author>Navendu Garg</author>
<author>Shlomo Levitan</author>
</authors>
<title>Stylistic text classification using functional lexical features.</title>
<date>2007</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<pages>7--91</pages>
<contexts>
<context position="1802" citStr="Argamon et al., 2007" startWordPosition="257" endWordPosition="260">ification (Kessler et al., 1997), author profiling (Garera and Yarowsky, 2009; Rosenthal and McKeown, 2011), social relationship classification (Peterson et al., 2011), sentiment analysis (Wilson et al., 2005), readability classification (Collins-Thompson and Callan, 2005), and text generation (Hovy, 1990; Inkpen and Hirst, 2006). Following the classic work of Biber (1988), computational modeling of style has often focused on textual statistics and the frequency of function words and syntactic categories. When content words are considered, they are often limited to manually-constructed lists (Argamon et al., 2007), or used as individual features for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of lexical variety. Our interest is models that offer broad lexical coverage of human-identifiable stylistic variation. Research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability) (Brooke et al., 2010; Pan and Yang, 2010; Kidwell et al., 2009) and a large body of research on the induction of polarity lexicons, in particular from large corpora (Turney, 2</context>
</contexts>
<marker>Argamon, Whitelaw, Chase, Hota, Garg, Levitan, 2007</marker>
<rawString>Shlomo Argamon, Casey Whitelaw, Paul Chase, Sobhan Raj Hota, Navendu Garg, and Shlomo Levitan. 2007. Stylistic text classification using functional lexical features. Journal of the American Society for Information Science and Technology, 7:91–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Biber</author>
<author>Susan Conrad</author>
</authors>
<date>2009</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1074" citStr="Biber and Conrad, 2009" startWordPosition="152" endWordPosition="155">particular, that this model can be applied to the task of inducing stylistic lexicons, and that a multi-dimensional approach is warranted given the correlations among stylistic dimensions. 1 Introduction In language, stylistic variation is a reflection of various contextual factors, including the backgrounds of and relationship between the parties involved. Although in the context of prescriptive linguistics (Strunk and White, 1979), style is often assumed to be a matter of aesthetics, the stylistic intuitions of language users are inextricably linked to the conventions of register and genre (Biber and Conrad, 2009). Intentional or not, stylistic differences play a role in numerous NLP tasks. Examples include genre classification (Kessler et al., 1997), author profiling (Garera and Yarowsky, 2009; Rosenthal and McKeown, 2011), social relationship classification (Peterson et al., 2011), sentiment analysis (Wilson et al., 2005), readability classification (Collins-Thompson and Callan, 2005), and text generation (Hovy, 1990; Inkpen and Hirst, 2006). Following the classic work of Biber (1988), computational modeling of style has often focused on textual statistics and the frequency of function words and synt</context>
</contexts>
<marker>Biber, Conrad, 2009</marker>
<rawString>Douglas Biber and Susan Conrad. 2009. Register, Genre, and Style. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Biber</author>
</authors>
<title>Variation Across Speech and Writing.</title>
<date>1988</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1556" citStr="Biber (1988)" startWordPosition="222" endWordPosition="223">e stylistic intuitions of language users are inextricably linked to the conventions of register and genre (Biber and Conrad, 2009). Intentional or not, stylistic differences play a role in numerous NLP tasks. Examples include genre classification (Kessler et al., 1997), author profiling (Garera and Yarowsky, 2009; Rosenthal and McKeown, 2011), social relationship classification (Peterson et al., 2011), sentiment analysis (Wilson et al., 2005), readability classification (Collins-Thompson and Callan, 2005), and text generation (Hovy, 1990; Inkpen and Hirst, 2006). Following the classic work of Biber (1988), computational modeling of style has often focused on textual statistics and the frequency of function words and syntactic categories. When content words are considered, they are often limited to manually-constructed lists (Argamon et al., 2007), or used as individual features for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of lexical variety. Our interest is models that offer broad lexical coverage of human-identifiable stylistic variation. Research most similar to ours has focused on classifying the lexicon in terms of individua</context>
<context position="3946" citStr="Biber (1988)" startWordPosition="590" endWordPosition="591">arity, readability, for673 Proceedings of NAACL-HLT 2013, pages 673–679, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics mality, fanciness, colloquialness, specificity, concreteness, objectivity, and naturalness; these stylistic categories reflect common aesthetic judgments about language. In descriptive studies of register, some researchers have posited a few fixed styles (Joos, 1961) or a small, discrete set of situational constraints which determine style and register (Crystal and Davy, 1969; Halliday and Hasan, 1976); by contrast, the applied approach of Biber (1988) and theoretical framework of Leckie-Tarry (1995) offer a more continuous interpretation of register variation. In Biber’s approach, functional dimensions such as Involved vs. Informational, Argumentative vs. Non-argumentative, and Abstract vs. non-Abstract are derived in an unsupervised manner from a mixed-genre corpus, with the labels assigned depending on where features (a small set of known indicators of register) and genres fall on each spectrum. The theory of Leckie-Tarry posits a single main cline of register with one pole (the oral pole) reflecting a full reliance on the context of the</context>
</contexts>
<marker>Biber, 1988</marker>
<rawString>Douglas Biber. 1988. Variation Across Speech and Writing. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Birch</author>
</authors>
<date>1995</date>
<booktitle>Language and Context: A Functional Linguistic Theory of Register.</booktitle>
<editor>Introduction. In Helen Leckie-Tarry, editor,</editor>
<publisher>Pinter.</publisher>
<contexts>
<context position="4843" citStr="Birch, 1995" startWordPosition="725" endWordPosition="726">sed manner from a mixed-genre corpus, with the labels assigned depending on where features (a small set of known indicators of register) and genres fall on each spectrum. The theory of Leckie-Tarry posits a single main cline of register with one pole (the oral pole) reflecting a full reliance on the context of the linguistic situation, and the other (the literate pole) reflecting a reliance on cultural knowledge. The more specific elements of register are represented as subclines which are strongly influenced by this main cline, creating probabilistic relationships between related dimensions (Birch, 1995). For the present study, we have chosen 3 dimensions (6 styles) which are clearly represented in the lexicon, which are discussed often in the relevant literature, and which fit well into the Leckie-Tarry conception of related subclines: colloquial vs. literary, concrete vs. abstract, and subjective vs. objective. In addition to a negative correlation between opposing styles, we also expect a positive correlation between stylistic aspects that tend toward the same main pole, situational (i.e. colloquial, concrete, subjective) or cultural (i.e. literary, abstract, objective). These correlations</context>
</contexts>
<marker>Birch, 1995</marker>
<rawString>David Birch. 1995. Introduction. In Helen Leckie-Tarry, editor, Language and Context: A Functional Linguistic Theory of Register. Pinter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>John D Lafferty</author>
</authors>
<title>Correlated topic models.</title>
<date>2007</date>
<journal>Annals of Applied Statistics,</journal>
<volume>1</volume>
<issue>1</issue>
<pages>35</pages>
<contexts>
<context position="6230" citStr="Blei and Lafferty, 2007" startWordPosition="945" endWordPosition="948">el (Blei et al., 2003), with each of the 6 styles corresponding to a topic. Briefly, latent Dirichlet allocation (LDA) is a generative Bayesian model: for each document d, a distribution of topics θd is drawn from a Dirichlet prior (with parameter α). For each topic z, there is a probability distribution βz1 corresponding to the probability of that topic generating any given word in the vocabulary. Words in document d are generated by first selecting a topic z randomly according to θd, and then randomly selecting a word w according to βz. An extension of LDA, the correlated topic model (CTM) (Blei and Lafferty, 2007), supposes a more complex representation of topics: given a matrix E representing the covariance between topics and µ representing the means, for each document a topic distribution η (analogous to θ) is drawn from the logistic normal distribution. Given a corpus, good estimates for the relevant parameters can be derived using Bayesian inference. For both LDA and CTM we use the original variational Bayes implementation of Blei. Variational Bayes (VB) works by approximating the true posterior with a simpler distribution, minimizing the Kullback-Leibler divergence between the two through iterativ</context>
</contexts>
<marker>Blei, Lafferty, 2007</marker>
<rawString>David M. Blei and John D. Lafferty. 2007. Correlated topic models. Annals of Applied Statistics, 1(1):17– 35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
<author>John Lafferty</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="2612" citStr="Blei et al., 2003" startWordPosition="389" endWordPosition="392">offer broad lexical coverage of human-identifiable stylistic variation. Research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability) (Brooke et al., 2010; Pan and Yang, 2010; Kidwell et al., 2009) and a large body of research on the induction of polarity lexicons, in particular from large corpora (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). Our work is the first to represent multiple dimensions of style in a single statistical model, adapting latent Dirichlet allocation (Blei et al., 2003), a Bayesian ‘topic’ model, to our stylistic purposes; as such, our approach also follows on recent interest in the interpretability of topic-model topics (Chang et al., 2009; Newman et al., 2011). We show that our model can be used for acquisition of stylistic lexicons, and we also evaluate the model relative to theories of register variation and the expected stylistic character of particular genres. 2 Model 2.1 Linguistic foundations In English manuals of style and other prescriptivist texts (Fowler and Fowler, 1906; Gunning, 1952; Follett, 1966; Strunk and White, 1979; Kane, 1983; Hayakawa,</context>
<context position="5628" citStr="Blei et al., 2003" startWordPosition="840" endWordPosition="843"> which fit well into the Leckie-Tarry conception of related subclines: colloquial vs. literary, concrete vs. abstract, and subjective vs. objective. In addition to a negative correlation between opposing styles, we also expect a positive correlation between stylistic aspects that tend toward the same main pole, situational (i.e. colloquial, concrete, subjective) or cultural (i.e. literary, abstract, objective). These correlations can potentially interfere with accurate lexical acquisition. 2.2 Implementation Our main model is an adaption of the popular latent Dirichlet allocation topic model (Blei et al., 2003), with each of the 6 styles corresponding to a topic. Briefly, latent Dirichlet allocation (LDA) is a generative Bayesian model: for each document d, a distribution of topics θd is drawn from a Dirichlet prior (with parameter α). For each topic z, there is a probability distribution βz1 corresponding to the probability of that topic generating any given word in the vocabulary. Words in document d are generated by first selecting a topic z randomly according to θd, and then randomly selecting a word w according to βz. An extension of LDA, the correlated topic model (CTM) (Blei and Lafferty, 200</context>
<context position="6960" citStr="Blei et al. (2003" startWordPosition="1053" endWordPosition="1056">nd µ representing the means, for each document a topic distribution η (analogous to θ) is drawn from the logistic normal distribution. Given a corpus, good estimates for the relevant parameters can be derived using Bayesian inference. For both LDA and CTM we use the original variational Bayes implementation of Blei. Variational Bayes (VB) works by approximating the true posterior with a simpler distribution, minimizing the Kullback-Leibler divergence between the two through iterative updates of specially-introduced free variables. The mathematical and algorithmic details are omitted here; see Blei et al. (2003; 2007). Our early investigations used an online, batch version of LDA (Hoffman et al., 2010), which is more appropriate for large corpora because it requires only a single iteration over the dataset. We discovered, however, that batch models were markedly inferior to more traditional models for our purposes because the influence of the initial model diminishes too quickly; here, we need particular topics in the model to correspond to particular styles, and we accomplish this by seeding the model with known instances of each style (see Section 3). Specifically, our initial β consists of distri</context>
</contexts>
<marker>Blei, Ng, Jordan, Lafferty, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, Michael I. Jordan, and John Lafferty. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Brooke</author>
<author>Tong Wang</author>
<author>Graeme Hirst</author>
</authors>
<title>Automatic acquisition of lexical formality.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING ’10),</booktitle>
<location>Beijing.</location>
<contexts>
<context position="2247" citStr="Brooke et al., 2010" startWordPosition="327" endWordPosition="330">s and the frequency of function words and syntactic categories. When content words are considered, they are often limited to manually-constructed lists (Argamon et al., 2007), or used as individual features for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of lexical variety. Our interest is models that offer broad lexical coverage of human-identifiable stylistic variation. Research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability) (Brooke et al., 2010; Pan and Yang, 2010; Kidwell et al., 2009) and a large body of research on the induction of polarity lexicons, in particular from large corpora (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). Our work is the first to represent multiple dimensions of style in a single statistical model, adapting latent Dirichlet allocation (Blei et al., 2003), a Bayesian ‘topic’ model, to our stylistic purposes; as such, our approach also follows on recent interest in the interpretability of topic-model topics (Chang et al., 2009; Newman et al., 2011). We show that our model can be used fo</context>
<context position="8590" citStr="Brooke et al., 2010" startWordPosition="1327" endWordPosition="1330">model may not stabilize, in terms of its likelihood, until after it has shifted away from our desired stylistic dimensions towards some other 1Some versions of LDA smooth this distribution using a Dirichlet prior; here, though, we use the original formulation from Blei (2003), which does not. 674 variation in the data. Therefore, we treat the optimal number of iterations as a variable to investigate. The model is trained on a 1 million text portion of the 2009 version of the ICWSM Spinn3r dataset (Burton et al., 2009), a corpus of blogs we have previously used for formality lexicon induction (Brooke et al., 2010). Since our method relies on cooccurrence, we followed our earlier work in using only texts with at least 100 different word types. All words were tokenized and converted to lower-case, with no further lemmatization. Following Hoffman et al. (2010), we initialized the α of our models to 1/k where k is the number of topics. Otherwise we used the default settings; when they overlap they were identical for the LDA and CTM models. 3 Lexicon Induction Our primary evaluation is based on the stylistic induction of held-out seed words. The words were collected from various sources by the first author </context>
<context position="12184" citStr="Brooke et al., 2010" startWordPosition="1933" endWordPosition="1936">use of its status on a parallel dimension. Two more-holistic alternatives are total accuracy, the percentage of seeds for which the highest βtw is the topic t for which w is a seed (guessing baseline is 0.17), and the average rank of the correct t as ordered by βtw (in the range 1–6, guessing baseline is 3.5); the latter is more forgiving of near misses. We tested a few options which involved straightforward modifications to model training. Standard LDA produces all tokens in the document, but when dealing with style rather than topic, the number of times a word appears is much less relevant (Brooke et al., 2010). Our binary model assumes an LDA that generates types, not tokens.2 A key comparison 2At the theoretical level, this move is admittedly problematic, since our LDA model is thus being trained under the assumption that texts with multiple instances of the same type can be generated, when of course such texts cannot by definition exist. We might address this by moving to Bayesian models with very different generative assumptions, e.g. the spherical topic model (Reisinger et al., 2010), but these methods involve a significant increase of computational complexity and we believe that on a practical</context>
</contexts>
<marker>Brooke, Wang, Hirst, 2010</marker>
<rawString>Julian Brooke, Tong Wang, and Graeme Hirst. 2010. Automatic acquisition of lexical formality. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING ’10), Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Burton</author>
<author>Akshay Java</author>
<author>Ian Soboroff</author>
</authors>
<date>2009</date>
<booktitle>The ICWSM 2009 Spinn3r Dataset. In Proceedings of the Third Annual Conference on Weblogs and Social Media (ICWSM 2009),</booktitle>
<location>San Jose, CA.</location>
<contexts>
<context position="8493" citStr="Burton et al., 2009" startWordPosition="1311" endWordPosition="1314">corpus) nor necessarily desirable; the diminishing effects of the initial seeding means that the model may not stabilize, in terms of its likelihood, until after it has shifted away from our desired stylistic dimensions towards some other 1Some versions of LDA smooth this distribution using a Dirichlet prior; here, though, we use the original formulation from Blei (2003), which does not. 674 variation in the data. Therefore, we treat the optimal number of iterations as a variable to investigate. The model is trained on a 1 million text portion of the 2009 version of the ICWSM Spinn3r dataset (Burton et al., 2009), a corpus of blogs we have previously used for formality lexicon induction (Brooke et al., 2010). Since our method relies on cooccurrence, we followed our earlier work in using only texts with at least 100 different word types. All words were tokenized and converted to lower-case, with no further lemmatization. Following Hoffman et al. (2010), we initialized the α of our models to 1/k where k is the number of topics. Otherwise we used the default settings; when they overlap they were identical for the LDA and CTM models. 3 Lexicon Induction Our primary evaluation is based on the stylistic ind</context>
</contexts>
<marker>Burton, Java, Soboroff, 2009</marker>
<rawString>Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The ICWSM 2009 Spinn3r Dataset. In Proceedings of the Third Annual Conference on Weblogs and Social Media (ICWSM 2009), San Jose, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>Jordan Boyd-Graber</author>
<author>Sean Gerrish</author>
<author>Chong Wang</author>
<author>David Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of Neural Information Processing Systems (NIPS ’09).</booktitle>
<contexts>
<context position="2786" citStr="Chang et al., 2009" startWordPosition="417" endWordPosition="420">levant to style (e.g. formality, specificity, readability) (Brooke et al., 2010; Pan and Yang, 2010; Kidwell et al., 2009) and a large body of research on the induction of polarity lexicons, in particular from large corpora (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). Our work is the first to represent multiple dimensions of style in a single statistical model, adapting latent Dirichlet allocation (Blei et al., 2003), a Bayesian ‘topic’ model, to our stylistic purposes; as such, our approach also follows on recent interest in the interpretability of topic-model topics (Chang et al., 2009; Newman et al., 2011). We show that our model can be used for acquisition of stylistic lexicons, and we also evaluate the model relative to theories of register variation and the expected stylistic character of particular genres. 2 Model 2.1 Linguistic foundations In English manuals of style and other prescriptivist texts (Fowler and Fowler, 1906; Gunning, 1952; Follett, 1966; Strunk and White, 1979; Kane, 1983; Hayakawa, 1994), writers are urged to pay attention to various aspects of lexical style, including elements such as clarity, familiarity, readability, for673 Proceedings of NAACL-HLT </context>
<context position="13901" citStr="Chang et al., 2009" startWordPosition="2218" endWordPosition="2221"> (iter 2) binary LDA (iter 2) combo binary LDA (iter 1) binary CTM (iter 1) Table 1: Model performance in lexical induction of seeds. Bold indicates best in column. here is with a combined LDA model (combo), an amalgamation of three independently trained 2-topic models, one for each dimension; this tests our key hypothesis that training dimensions of style together is beneficial. Finally, we test against the correlated topic model (CTM), which offers an explicit representation of style correlation, but which has done poorly with respect to interpretability, despite offering better perplexity (Chang et al., 2009). The results of the lexicon induction evaluation are in Table 1. Since the number of optimal iterations varies, we report the result from the best of the first five iterations, as measured by total accuracy; the best iteration is shown in parenthesis. In general, all the results are high enough—we are reliably above 90% for the pairwise task, and above 50% for the 6-way task—for us to conclude with some confidence that our model is capturing a significant amount of stylistic variation. As predicted, using words as boolean features had a net positive gain, consistent across all of our metrics,</context>
</contexts>
<marker>Chang, Boyd-Graber, Gerrish, Wang, Blei, 2009</marker>
<rawString>Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish, Chong Wang, and David Blei. 2009. Reading tea leaves: How humans interpret topic models. In Proceedings of Neural Information Processing Systems (NIPS ’09).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevyn Collins-Thompson</author>
<author>Jamie Callan</author>
</authors>
<title>Predicting reading difficulty with statistical language models.</title>
<date>2005</date>
<journal>Journal of the American Society for Information Science Technology,</journal>
<volume>56</volume>
<issue>13</issue>
<contexts>
<context position="1454" citStr="Collins-Thompson and Callan, 2005" startWordPosition="204" endWordPosition="207">in the context of prescriptive linguistics (Strunk and White, 1979), style is often assumed to be a matter of aesthetics, the stylistic intuitions of language users are inextricably linked to the conventions of register and genre (Biber and Conrad, 2009). Intentional or not, stylistic differences play a role in numerous NLP tasks. Examples include genre classification (Kessler et al., 1997), author profiling (Garera and Yarowsky, 2009; Rosenthal and McKeown, 2011), social relationship classification (Peterson et al., 2011), sentiment analysis (Wilson et al., 2005), readability classification (Collins-Thompson and Callan, 2005), and text generation (Hovy, 1990; Inkpen and Hirst, 2006). Following the classic work of Biber (1988), computational modeling of style has often focused on textual statistics and the frequency of function words and syntactic categories. When content words are considered, they are often limited to manually-constructed lists (Argamon et al., 2007), or used as individual features for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of lexical variety. Our interest is models that offer broad lexical coverage of human-identifiable stylistic</context>
</contexts>
<marker>Collins-Thompson, Callan, 2005</marker>
<rawString>Kevyn Collins-Thompson and Jamie Callan. 2005. Predicting reading difficulty with statistical language models. Journal of the American Society for Information Science Technology, 56(13):1448–1462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Crystal</author>
<author>Derek Davy</author>
</authors>
<title>Investigating English Style.</title>
<date>1969</date>
<publisher>Indiana University Press.</publisher>
<contexts>
<context position="3868" citStr="Crystal and Davy, 1969" startWordPosition="575" endWordPosition="579">ttention to various aspects of lexical style, including elements such as clarity, familiarity, readability, for673 Proceedings of NAACL-HLT 2013, pages 673–679, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics mality, fanciness, colloquialness, specificity, concreteness, objectivity, and naturalness; these stylistic categories reflect common aesthetic judgments about language. In descriptive studies of register, some researchers have posited a few fixed styles (Joos, 1961) or a small, discrete set of situational constraints which determine style and register (Crystal and Davy, 1969; Halliday and Hasan, 1976); by contrast, the applied approach of Biber (1988) and theoretical framework of Leckie-Tarry (1995) offer a more continuous interpretation of register variation. In Biber’s approach, functional dimensions such as Involved vs. Informational, Argumentative vs. Non-argumentative, and Abstract vs. non-Abstract are derived in an unsupervised manner from a mixed-genre corpus, with the labels assigned depending on where features (a small set of known indicators of register) and genres fall on each spectrum. The theory of Leckie-Tarry posits a single main cline of register </context>
</contexts>
<marker>Crystal, Davy, 1969</marker>
<rawString>David Crystal and Derek Davy. 1969. Investigating English Style. Indiana University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Doyle</author>
<author>Charles Elkan</author>
</authors>
<title>Accounting for burstiness in topic models.</title>
<date>2009</date>
<booktitle>In International Conference on Machine Learning (ICML ’09).</booktitle>
<contexts>
<context position="12998" citStr="Doyle and Elkan, 2009" startWordPosition="2068" endWordPosition="2071">under the assumption that texts with multiple instances of the same type can be generated, when of course such texts cannot by definition exist. We might address this by moving to Bayesian models with very different generative assumptions, e.g. the spherical topic model (Reisinger et al., 2010), but these methods involve a significant increase of computational complexity and we believe that on a practical level there are no real negatives associated with directly using a binary representation as input to LDA; in fact, we are avoiding what appears to be a much more serious problem, burstiness (Doyle and Elkan, 2009), i.e. the fact that 675 Model Total Acc. (%) Avg. Rank Pairwise Accuracy (%) Lit/Col Abs/Con Obj/Sub All 50.0 50.0 50.0 50.0 16.6. 3.50 94.3 98.8 93.0 95.4 55.0 1.79 96.2 98.9 93.5 96.2 57.7 1.74 95.4 99.2 93.3 96.0 53.1 1.86 96.3 99.0 89.6 95.0 53.0 1.87 guessing baseline basic LDA (iter 2) binary LDA (iter 2) combo binary LDA (iter 1) binary CTM (iter 1) Table 1: Model performance in lexical induction of seeds. Bold indicates best in column. here is with a combined LDA model (combo), an amalgamation of three independently trained 2-topic models, one for each dimension; this tests our key hy</context>
</contexts>
<marker>Doyle, Elkan, 2009</marker>
<rawString>Gabriel Doyle and Charles Elkan. 2009. Accounting for burstiness in topic models. In International Conference on Machine Learning (ICML ’09).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilson Follett</author>
</authors>
<date>1966</date>
<publisher>Modern American Usage. Hill &amp; Wang,</publisher>
<location>New York.</location>
<contexts>
<context position="3165" citStr="Follett, 1966" startWordPosition="479" endWordPosition="480">el, adapting latent Dirichlet allocation (Blei et al., 2003), a Bayesian ‘topic’ model, to our stylistic purposes; as such, our approach also follows on recent interest in the interpretability of topic-model topics (Chang et al., 2009; Newman et al., 2011). We show that our model can be used for acquisition of stylistic lexicons, and we also evaluate the model relative to theories of register variation and the expected stylistic character of particular genres. 2 Model 2.1 Linguistic foundations In English manuals of style and other prescriptivist texts (Fowler and Fowler, 1906; Gunning, 1952; Follett, 1966; Strunk and White, 1979; Kane, 1983; Hayakawa, 1994), writers are urged to pay attention to various aspects of lexical style, including elements such as clarity, familiarity, readability, for673 Proceedings of NAACL-HLT 2013, pages 673–679, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics mality, fanciness, colloquialness, specificity, concreteness, objectivity, and naturalness; these stylistic categories reflect common aesthetic judgments about language. In descriptive studies of register, some researchers have posited a few fixed styles (Joos, 1961) or a sm</context>
</contexts>
<marker>Follett, 1966</marker>
<rawString>Wilson Follett. 1966. Modern American Usage. Hill &amp; Wang, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H W Fowler</author>
<author>F G Fowler</author>
</authors>
<title>The King’s English.</title>
<date>1906</date>
<publisher>Clarendon Press,</publisher>
<location>Oxford,</location>
<contexts>
<context position="3135" citStr="Fowler and Fowler, 1906" startWordPosition="473" endWordPosition="476">ons of style in a single statistical model, adapting latent Dirichlet allocation (Blei et al., 2003), a Bayesian ‘topic’ model, to our stylistic purposes; as such, our approach also follows on recent interest in the interpretability of topic-model topics (Chang et al., 2009; Newman et al., 2011). We show that our model can be used for acquisition of stylistic lexicons, and we also evaluate the model relative to theories of register variation and the expected stylistic character of particular genres. 2 Model 2.1 Linguistic foundations In English manuals of style and other prescriptivist texts (Fowler and Fowler, 1906; Gunning, 1952; Follett, 1966; Strunk and White, 1979; Kane, 1983; Hayakawa, 1994), writers are urged to pay attention to various aspects of lexical style, including elements such as clarity, familiarity, readability, for673 Proceedings of NAACL-HLT 2013, pages 673–679, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics mality, fanciness, colloquialness, specificity, concreteness, objectivity, and naturalness; these stylistic categories reflect common aesthetic judgments about language. In descriptive studies of register, some researchers have posited a few fix</context>
</contexts>
<marker>Fowler, Fowler, 1906</marker>
<rawString>H. W. Fowler and F. G. Fowler. 1906. The King’s English. Clarendon Press, Oxford, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikesh Garera</author>
<author>David Yarowsky</author>
</authors>
<title>Modeling latent biographic attributes in conversational genres.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP ’09),</booktitle>
<pages>710--718</pages>
<contexts>
<context position="1258" citStr="Garera and Yarowsky, 2009" startWordPosition="179" endWordPosition="182">sions. 1 Introduction In language, stylistic variation is a reflection of various contextual factors, including the backgrounds of and relationship between the parties involved. Although in the context of prescriptive linguistics (Strunk and White, 1979), style is often assumed to be a matter of aesthetics, the stylistic intuitions of language users are inextricably linked to the conventions of register and genre (Biber and Conrad, 2009). Intentional or not, stylistic differences play a role in numerous NLP tasks. Examples include genre classification (Kessler et al., 1997), author profiling (Garera and Yarowsky, 2009; Rosenthal and McKeown, 2011), social relationship classification (Peterson et al., 2011), sentiment analysis (Wilson et al., 2005), readability classification (Collins-Thompson and Callan, 2005), and text generation (Hovy, 1990; Inkpen and Hirst, 2006). Following the classic work of Biber (1988), computational modeling of style has often focused on textual statistics and the frequency of function words and syntactic categories. When content words are considered, they are often limited to manually-constructed lists (Argamon et al., 2007), or used as individual features for supervised classifi</context>
</contexts>
<marker>Garera, Yarowsky, 2009</marker>
<rawString>Nikesh Garera and David Yarowsky. 2009. Modeling latent biographic attributes in conversational genres. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP ’09), pages 710–718, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Gunning</author>
</authors>
<title>The Technique of Clear Writing.</title>
<date>1952</date>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<contexts>
<context position="3150" citStr="Gunning, 1952" startWordPosition="477" endWordPosition="478">statistical model, adapting latent Dirichlet allocation (Blei et al., 2003), a Bayesian ‘topic’ model, to our stylistic purposes; as such, our approach also follows on recent interest in the interpretability of topic-model topics (Chang et al., 2009; Newman et al., 2011). We show that our model can be used for acquisition of stylistic lexicons, and we also evaluate the model relative to theories of register variation and the expected stylistic character of particular genres. 2 Model 2.1 Linguistic foundations In English manuals of style and other prescriptivist texts (Fowler and Fowler, 1906; Gunning, 1952; Follett, 1966; Strunk and White, 1979; Kane, 1983; Hayakawa, 1994), writers are urged to pay attention to various aspects of lexical style, including elements such as clarity, familiarity, readability, for673 Proceedings of NAACL-HLT 2013, pages 673–679, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics mality, fanciness, colloquialness, specificity, concreteness, objectivity, and naturalness; these stylistic categories reflect common aesthetic judgments about language. In descriptive studies of register, some researchers have posited a few fixed styles (Joos</context>
</contexts>
<marker>Gunning, 1952</marker>
<rawString>Robert Gunning. 1952. The Technique of Clear Writing. McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<date>1976</date>
<booktitle>Cohesion in English. Longman,</booktitle>
<location>London.</location>
<contexts>
<context position="3895" citStr="Halliday and Hasan, 1976" startWordPosition="580" endWordPosition="583">cts of lexical style, including elements such as clarity, familiarity, readability, for673 Proceedings of NAACL-HLT 2013, pages 673–679, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics mality, fanciness, colloquialness, specificity, concreteness, objectivity, and naturalness; these stylistic categories reflect common aesthetic judgments about language. In descriptive studies of register, some researchers have posited a few fixed styles (Joos, 1961) or a small, discrete set of situational constraints which determine style and register (Crystal and Davy, 1969; Halliday and Hasan, 1976); by contrast, the applied approach of Biber (1988) and theoretical framework of Leckie-Tarry (1995) offer a more continuous interpretation of register variation. In Biber’s approach, functional dimensions such as Involved vs. Informational, Argumentative vs. Non-argumentative, and Abstract vs. non-Abstract are derived in an unsupervised manner from a mixed-genre corpus, with the labels assigned depending on where features (a small set of known indicators of register) and genres fall on each spectrum. The theory of Leckie-Tarry posits a single main cline of register with one pole (the oral pol</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>M.A.K. Halliday and Ruqaiya Hasan. 1976. Cohesion in English. Longman, London.</rawString>
</citation>
<citation valid="true">
<title>Choose the Right Word. HarperCollins Publishers, second edition. Revised by Eugene Ehrlich.</title>
<date>1994</date>
<editor>S.I. Hayakawa, editor.</editor>
<marker>1994</marker>
<rawString>S.I. Hayakawa, editor. 1994. Choose the Right Word. HarperCollins Publishers, second edition. Revised by Eugene Ehrlich.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew D Hoffman</author>
<author>David M Blei</author>
<author>Francis R Bach</author>
</authors>
<title>Online learning for latent Dirichlet allocation.</title>
<date>2010</date>
<booktitle>In Neural Information Processing Systems (NIPS ’10),</booktitle>
<pages>856--864</pages>
<contexts>
<context position="7053" citStr="Hoffman et al., 2010" startWordPosition="1069" endWordPosition="1072"> drawn from the logistic normal distribution. Given a corpus, good estimates for the relevant parameters can be derived using Bayesian inference. For both LDA and CTM we use the original variational Bayes implementation of Blei. Variational Bayes (VB) works by approximating the true posterior with a simpler distribution, minimizing the Kullback-Leibler divergence between the two through iterative updates of specially-introduced free variables. The mathematical and algorithmic details are omitted here; see Blei et al. (2003; 2007). Our early investigations used an online, batch version of LDA (Hoffman et al., 2010), which is more appropriate for large corpora because it requires only a single iteration over the dataset. We discovered, however, that batch models were markedly inferior to more traditional models for our purposes because the influence of the initial model diminishes too quickly; here, we need particular topics in the model to correspond to particular styles, and we accomplish this by seeding the model with known instances of each style (see Section 3). Specifically, our initial β consists of distributions where the entire probability mass is divided amongst the seeds for each corresponding</context>
<context position="8838" citStr="Hoffman et al. (2010)" startWordPosition="1367" endWordPosition="1370"> formulation from Blei (2003), which does not. 674 variation in the data. Therefore, we treat the optimal number of iterations as a variable to investigate. The model is trained on a 1 million text portion of the 2009 version of the ICWSM Spinn3r dataset (Burton et al., 2009), a corpus of blogs we have previously used for formality lexicon induction (Brooke et al., 2010). Since our method relies on cooccurrence, we followed our earlier work in using only texts with at least 100 different word types. All words were tokenized and converted to lower-case, with no further lemmatization. Following Hoffman et al. (2010), we initialized the α of our models to 1/k where k is the number of topics. Otherwise we used the default settings; when they overlap they were identical for the LDA and CTM models. 3 Lexicon Induction Our primary evaluation is based on the stylistic induction of held-out seed words. The words were collected from various sources by the first author and further reviewed by the second; we are both native speakers of English with significant experience in English linguistics. Included words had to be clear, extreme members of their stylistic category, with little or no ambiguity with respect to </context>
</contexts>
<marker>Hoffman, Blei, Bach, 2010</marker>
<rawString>Matthew D. Hoffman, David M. Blei, and Francis R. Bach. 2010. Online learning for latent Dirichlet allocation. In Neural Information Processing Systems (NIPS ’10), pages 856–864.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard H Hovy</author>
</authors>
<title>Pragmatics and natural language generation.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<pages>43--153</pages>
<contexts>
<context position="1487" citStr="Hovy, 1990" startWordPosition="211" endWordPosition="212">ite, 1979), style is often assumed to be a matter of aesthetics, the stylistic intuitions of language users are inextricably linked to the conventions of register and genre (Biber and Conrad, 2009). Intentional or not, stylistic differences play a role in numerous NLP tasks. Examples include genre classification (Kessler et al., 1997), author profiling (Garera and Yarowsky, 2009; Rosenthal and McKeown, 2011), social relationship classification (Peterson et al., 2011), sentiment analysis (Wilson et al., 2005), readability classification (Collins-Thompson and Callan, 2005), and text generation (Hovy, 1990; Inkpen and Hirst, 2006). Following the classic work of Biber (1988), computational modeling of style has often focused on textual statistics and the frequency of function words and syntactic categories. When content words are considered, they are often limited to manually-constructed lists (Argamon et al., 2007), or used as individual features for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of lexical variety. Our interest is models that offer broad lexical coverage of human-identifiable stylistic variation. Research most similar</context>
</contexts>
<marker>Hovy, 1990</marker>
<rawString>Eduard H. Hovy. 1990. Pragmatics and natural language generation. Artificial Intelligence, 43:153–197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana Inkpen</author>
<author>Graeme Hirst</author>
</authors>
<title>Building and using a lexical knowledge base of near-synonym differences.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>2</issue>
<contexts>
<context position="1512" citStr="Inkpen and Hirst, 2006" startWordPosition="213" endWordPosition="216">style is often assumed to be a matter of aesthetics, the stylistic intuitions of language users are inextricably linked to the conventions of register and genre (Biber and Conrad, 2009). Intentional or not, stylistic differences play a role in numerous NLP tasks. Examples include genre classification (Kessler et al., 1997), author profiling (Garera and Yarowsky, 2009; Rosenthal and McKeown, 2011), social relationship classification (Peterson et al., 2011), sentiment analysis (Wilson et al., 2005), readability classification (Collins-Thompson and Callan, 2005), and text generation (Hovy, 1990; Inkpen and Hirst, 2006). Following the classic work of Biber (1988), computational modeling of style has often focused on textual statistics and the frequency of function words and syntactic categories. When content words are considered, they are often limited to manually-constructed lists (Argamon et al., 2007), or used as individual features for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of lexical variety. Our interest is models that offer broad lexical coverage of human-identifiable stylistic variation. Research most similar to ours has focused on c</context>
</contexts>
<marker>Inkpen, Hirst, 2006</marker>
<rawString>Diana Inkpen and Graeme Hirst. 2006. Building and using a lexical knowledge base of near-synonym differences. Computational Linguistics, 32(2):223–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Joos</author>
</authors>
<title>The Five Clocks.</title>
<date>1961</date>
<publisher>Harcourt, Brace and World,</publisher>
<location>New York.</location>
<contexts>
<context position="3757" citStr="Joos, 1961" startWordPosition="560" endWordPosition="561">1952; Follett, 1966; Strunk and White, 1979; Kane, 1983; Hayakawa, 1994), writers are urged to pay attention to various aspects of lexical style, including elements such as clarity, familiarity, readability, for673 Proceedings of NAACL-HLT 2013, pages 673–679, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics mality, fanciness, colloquialness, specificity, concreteness, objectivity, and naturalness; these stylistic categories reflect common aesthetic judgments about language. In descriptive studies of register, some researchers have posited a few fixed styles (Joos, 1961) or a small, discrete set of situational constraints which determine style and register (Crystal and Davy, 1969; Halliday and Hasan, 1976); by contrast, the applied approach of Biber (1988) and theoretical framework of Leckie-Tarry (1995) offer a more continuous interpretation of register variation. In Biber’s approach, functional dimensions such as Involved vs. Informational, Argumentative vs. Non-argumentative, and Abstract vs. non-Abstract are derived in an unsupervised manner from a mixed-genre corpus, with the labels assigned depending on where features (a small set of known indicators of</context>
</contexts>
<marker>Joos, 1961</marker>
<rawString>Martin Joos. 1961. The Five Clocks. Harcourt, Brace and World, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobuhiro Kaji</author>
<author>Masaru Kitsuregawa</author>
</authors>
<title>Building lexicon for sentiment analysis from massive collection of HTML documents.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<contexts>
<context position="2433" citStr="Kaji and Kitsuregawa, 2007" startWordPosition="359" endWordPosition="362">used as individual features for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of lexical variety. Our interest is models that offer broad lexical coverage of human-identifiable stylistic variation. Research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability) (Brooke et al., 2010; Pan and Yang, 2010; Kidwell et al., 2009) and a large body of research on the induction of polarity lexicons, in particular from large corpora (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). Our work is the first to represent multiple dimensions of style in a single statistical model, adapting latent Dirichlet allocation (Blei et al., 2003), a Bayesian ‘topic’ model, to our stylistic purposes; as such, our approach also follows on recent interest in the interpretability of topic-model topics (Chang et al., 2009; Newman et al., 2011). We show that our model can be used for acquisition of stylistic lexicons, and we also evaluate the model relative to theories of register variation and the expected stylistic character of particular genres. 2 Model 2.1 Ling</context>
</contexts>
<marker>Kaji, Kitsuregawa, 2007</marker>
<rawString>Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Building lexicon for sentiment analysis from massive collection of HTML documents. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL ’07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas S Kane</author>
</authors>
<title>The Oxford Guide to Writing.</title>
<date>1983</date>
<publisher>Univeristy Press.</publisher>
<location>Oxford</location>
<contexts>
<context position="3201" citStr="Kane, 1983" startWordPosition="485" endWordPosition="486">n (Blei et al., 2003), a Bayesian ‘topic’ model, to our stylistic purposes; as such, our approach also follows on recent interest in the interpretability of topic-model topics (Chang et al., 2009; Newman et al., 2011). We show that our model can be used for acquisition of stylistic lexicons, and we also evaluate the model relative to theories of register variation and the expected stylistic character of particular genres. 2 Model 2.1 Linguistic foundations In English manuals of style and other prescriptivist texts (Fowler and Fowler, 1906; Gunning, 1952; Follett, 1966; Strunk and White, 1979; Kane, 1983; Hayakawa, 1994), writers are urged to pay attention to various aspects of lexical style, including elements such as clarity, familiarity, readability, for673 Proceedings of NAACL-HLT 2013, pages 673–679, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics mality, fanciness, colloquialness, specificity, concreteness, objectivity, and naturalness; these stylistic categories reflect common aesthetic judgments about language. In descriptive studies of register, some researchers have posited a few fixed styles (Joos, 1961) or a small, discrete set of situational con</context>
</contexts>
<marker>Kane, 1983</marker>
<rawString>Thomas S. Kane. 1983. The Oxford Guide to Writing. Oxford Univeristy Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brett Kessler</author>
<author>Geoffrey Nunberg</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic detection of text genre.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL ’97),</booktitle>
<pages>32--38</pages>
<location>Madrid,</location>
<marker>Kessler, Nunberg, Sch¨utze, 1997</marker>
<rawString>Brett Kessler, Geoffrey Nunberg, and Hinrich Sch¨utze. 1997. Automatic detection of text genre. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL ’97), pages 32– 38, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kidwell</author>
<author>Guy Lebanon</author>
<author>Kevyn CollinsThompson</author>
</authors>
<title>Statistical estimation of word acquisition with application to readability prediction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP’09),</booktitle>
<pages>900--909</pages>
<contexts>
<context position="2290" citStr="Kidwell et al., 2009" startWordPosition="335" endWordPosition="338"> syntactic categories. When content words are considered, they are often limited to manually-constructed lists (Argamon et al., 2007), or used as individual features for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of lexical variety. Our interest is models that offer broad lexical coverage of human-identifiable stylistic variation. Research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability) (Brooke et al., 2010; Pan and Yang, 2010; Kidwell et al., 2009) and a large body of research on the induction of polarity lexicons, in particular from large corpora (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). Our work is the first to represent multiple dimensions of style in a single statistical model, adapting latent Dirichlet allocation (Blei et al., 2003), a Bayesian ‘topic’ model, to our stylistic purposes; as such, our approach also follows on recent interest in the interpretability of topic-model topics (Chang et al., 2009; Newman et al., 2011). We show that our model can be used for acquisition of stylistic lexicons, and we</context>
</contexts>
<marker>Kidwell, Lebanon, CollinsThompson, 2009</marker>
<rawString>Paul Kidwell, Guy Lebanon, and Kevyn CollinsThompson. 2009. Statistical estimation of word acquisition with application to readability prediction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP’09), pages 900–909, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helen Leckie-Tarry</author>
</authors>
<title>Language and Context: A Functional Linguistic Theory of Register.</title>
<date>1995</date>
<publisher>Pinter.</publisher>
<contexts>
<context position="3995" citStr="Leckie-Tarry (1995)" startWordPosition="596" endWordPosition="597">NAACL-HLT 2013, pages 673–679, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics mality, fanciness, colloquialness, specificity, concreteness, objectivity, and naturalness; these stylistic categories reflect common aesthetic judgments about language. In descriptive studies of register, some researchers have posited a few fixed styles (Joos, 1961) or a small, discrete set of situational constraints which determine style and register (Crystal and Davy, 1969; Halliday and Hasan, 1976); by contrast, the applied approach of Biber (1988) and theoretical framework of Leckie-Tarry (1995) offer a more continuous interpretation of register variation. In Biber’s approach, functional dimensions such as Involved vs. Informational, Argumentative vs. Non-argumentative, and Abstract vs. non-Abstract are derived in an unsupervised manner from a mixed-genre corpus, with the labels assigned depending on where features (a small set of known indicators of register) and genres fall on each spectrum. The theory of Leckie-Tarry posits a single main cline of register with one pole (the oral pole) reflecting a full reliance on the context of the linguistic situation, and the other (the literat</context>
</contexts>
<marker>Leckie-Tarry, 1995</marker>
<rawString>Helen Leckie-Tarry. 1995. Language and Context: A Functional Linguistic Theory of Register. Pinter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Li</author>
<author>Andrew McCallum</author>
</authors>
<title>Pachinko allocation: DAG-structured mixture models of topic correlations.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd International Conference on Machine Learning, ICML ’06,</booktitle>
<pages>577--584</pages>
<marker>Li, McCallum, 2006</marker>
<rawString>Wei Li and Andrew McCallum. 2006. Pachinko allocation: DAG-structured mixture models of topic correlations. In Proceedings of the 23rd International Conference on Machine Learning, ICML ’06, pages 577– 584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Edwin V Bonilla</author>
<author>Wray Buntine</author>
</authors>
<title>Improving topic coherence with regularized topic models.</title>
<date>2011</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems (NIPS ’11).</booktitle>
<contexts>
<context position="2808" citStr="Newman et al., 2011" startWordPosition="421" endWordPosition="424">. formality, specificity, readability) (Brooke et al., 2010; Pan and Yang, 2010; Kidwell et al., 2009) and a large body of research on the induction of polarity lexicons, in particular from large corpora (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). Our work is the first to represent multiple dimensions of style in a single statistical model, adapting latent Dirichlet allocation (Blei et al., 2003), a Bayesian ‘topic’ model, to our stylistic purposes; as such, our approach also follows on recent interest in the interpretability of topic-model topics (Chang et al., 2009; Newman et al., 2011). We show that our model can be used for acquisition of stylistic lexicons, and we also evaluate the model relative to theories of register variation and the expected stylistic character of particular genres. 2 Model 2.1 Linguistic foundations In English manuals of style and other prescriptivist texts (Fowler and Fowler, 1906; Gunning, 1952; Follett, 1966; Strunk and White, 1979; Kane, 1983; Hayakawa, 1994), writers are urged to pay attention to various aspects of lexical style, including elements such as clarity, familiarity, readability, for673 Proceedings of NAACL-HLT 2013, pages 673–679, A</context>
</contexts>
<marker>Newman, Bonilla, Buntine, 2011</marker>
<rawString>David Newman, Edwin V. Bonilla, and Wray Buntine. 2011. Improving topic coherence with regularized topic models. In Proceedings of Advances in Neural Information Processing Systems (NIPS ’11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sinno Jialin Pan</author>
<author>Qiang Yang</author>
</authors>
<title>A survey on transfer learning.</title>
<date>2010</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>22</volume>
<issue>10</issue>
<contexts>
<context position="2267" citStr="Pan and Yang, 2010" startWordPosition="331" endWordPosition="334">f function words and syntactic categories. When content words are considered, they are often limited to manually-constructed lists (Argamon et al., 2007), or used as individual features for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of lexical variety. Our interest is models that offer broad lexical coverage of human-identifiable stylistic variation. Research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability) (Brooke et al., 2010; Pan and Yang, 2010; Kidwell et al., 2009) and a large body of research on the induction of polarity lexicons, in particular from large corpora (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). Our work is the first to represent multiple dimensions of style in a single statistical model, adapting latent Dirichlet allocation (Blei et al., 2003), a Bayesian ‘topic’ model, to our stylistic purposes; as such, our approach also follows on recent interest in the interpretability of topic-model topics (Chang et al., 2009; Newman et al., 2011). We show that our model can be used for acquisition of sty</context>
</contexts>
<marker>Pan, Yang, 2010</marker>
<rawString>Sinno Jialin Pan and Qiang Yang. 2010. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kelly Peterson</author>
<author>Matt Hohensee</author>
<author>Fei Xia</author>
</authors>
<title>Email formality in the workplace: A case study on the Enron corpus.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL ’11),</booktitle>
<location>Portland, Oregon.</location>
<contexts>
<context position="1348" citStr="Peterson et al., 2011" startWordPosition="191" endWordPosition="195">factors, including the backgrounds of and relationship between the parties involved. Although in the context of prescriptive linguistics (Strunk and White, 1979), style is often assumed to be a matter of aesthetics, the stylistic intuitions of language users are inextricably linked to the conventions of register and genre (Biber and Conrad, 2009). Intentional or not, stylistic differences play a role in numerous NLP tasks. Examples include genre classification (Kessler et al., 1997), author profiling (Garera and Yarowsky, 2009; Rosenthal and McKeown, 2011), social relationship classification (Peterson et al., 2011), sentiment analysis (Wilson et al., 2005), readability classification (Collins-Thompson and Callan, 2005), and text generation (Hovy, 1990; Inkpen and Hirst, 2006). Following the classic work of Biber (1988), computational modeling of style has often focused on textual statistics and the frequency of function words and syntactic categories. When content words are considered, they are often limited to manually-constructed lists (Argamon et al., 2007), or used as individual features for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of</context>
</contexts>
<marker>Peterson, Hohensee, Xia, 2011</marker>
<rawString>Kelly Peterson, Matt Hohensee, and Fei Xia. 2011. Email formality in the workplace: A case study on the Enron corpus. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL ’11), Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Petrenz</author>
<author>Bonnie Webber</author>
</authors>
<title>Stable classification of text genres.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>2</issue>
<contexts>
<context position="1925" citStr="Petrenz and Webber, 2011" startWordPosition="276" endWordPosition="279">ationship classification (Peterson et al., 2011), sentiment analysis (Wilson et al., 2005), readability classification (Collins-Thompson and Callan, 2005), and text generation (Hovy, 1990; Inkpen and Hirst, 2006). Following the classic work of Biber (1988), computational modeling of style has often focused on textual statistics and the frequency of function words and syntactic categories. When content words are considered, they are often limited to manually-constructed lists (Argamon et al., 2007), or used as individual features for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of lexical variety. Our interest is models that offer broad lexical coverage of human-identifiable stylistic variation. Research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability) (Brooke et al., 2010; Pan and Yang, 2010; Kidwell et al., 2009) and a large body of research on the induction of polarity lexicons, in particular from large corpora (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). Our work is the first to represent multiple dimensions of style </context>
</contexts>
<marker>Petrenz, Webber, 2011</marker>
<rawString>Philipp Petrenz and Bonnie Webber. 2011. Stable classification of text genres. Computational Linguistics, 37(2):385–393, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Reisinger</author>
<author>A Waters</author>
<author>B Silverthorn</author>
<author>R Mooney</author>
</authors>
<title>Spherical topic models.</title>
<date>2010</date>
<booktitle>In International Conference on Machine Learning (ICML ’10).</booktitle>
<contexts>
<context position="12671" citStr="Reisinger et al., 2010" startWordPosition="2014" endWordPosition="2017"> document, but when dealing with style rather than topic, the number of times a word appears is much less relevant (Brooke et al., 2010). Our binary model assumes an LDA that generates types, not tokens.2 A key comparison 2At the theoretical level, this move is admittedly problematic, since our LDA model is thus being trained under the assumption that texts with multiple instances of the same type can be generated, when of course such texts cannot by definition exist. We might address this by moving to Bayesian models with very different generative assumptions, e.g. the spherical topic model (Reisinger et al., 2010), but these methods involve a significant increase of computational complexity and we believe that on a practical level there are no real negatives associated with directly using a binary representation as input to LDA; in fact, we are avoiding what appears to be a much more serious problem, burstiness (Doyle and Elkan, 2009), i.e. the fact that 675 Model Total Acc. (%) Avg. Rank Pairwise Accuracy (%) Lit/Col Abs/Con Obj/Sub All 50.0 50.0 50.0 50.0 16.6. 3.50 94.3 98.8 93.0 95.4 55.0 1.79 96.2 98.9 93.5 96.2 57.7 1.74 95.4 99.2 93.3 96.0 53.1 1.86 96.3 99.0 89.6 95.0 53.0 1.87 guessing baselin</context>
</contexts>
<marker>Reisinger, Waters, Silverthorn, Mooney, 2010</marker>
<rawString>J. Reisinger, A. Waters, B. Silverthorn, and R. Mooney. 2010. Spherical topic models. In International Conference on Machine Learning (ICML ’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Kathleen McKeown</author>
</authors>
<title>Age prediction in blogs: A study of style, content, and online behavior in pre- and post-social media generations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL ’11),</booktitle>
<location>Portland, Oregon.</location>
<contexts>
<context position="1288" citStr="Rosenthal and McKeown, 2011" startWordPosition="183" endWordPosition="187">nguage, stylistic variation is a reflection of various contextual factors, including the backgrounds of and relationship between the parties involved. Although in the context of prescriptive linguistics (Strunk and White, 1979), style is often assumed to be a matter of aesthetics, the stylistic intuitions of language users are inextricably linked to the conventions of register and genre (Biber and Conrad, 2009). Intentional or not, stylistic differences play a role in numerous NLP tasks. Examples include genre classification (Kessler et al., 1997), author profiling (Garera and Yarowsky, 2009; Rosenthal and McKeown, 2011), social relationship classification (Peterson et al., 2011), sentiment analysis (Wilson et al., 2005), readability classification (Collins-Thompson and Callan, 2005), and text generation (Hovy, 1990; Inkpen and Hirst, 2006). Following the classic work of Biber (1988), computational modeling of style has often focused on textual statistics and the frequency of function words and syntactic categories. When content words are considered, they are often limited to manually-constructed lists (Argamon et al., 2007), or used as individual features for supervised classification, which can be confounde</context>
</contexts>
<marker>Rosenthal, McKeown, 2011</marker>
<rawString>Sara Rosenthal and Kathleen McKeown. 2011. Age prediction in blogs: A study of style, content, and online behavior in pre- and post-social media generations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL ’11), Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Strunk</author>
<author>E B White</author>
</authors>
<date>1979</date>
<journal>The Elements of Style. Macmillan,</journal>
<volume>3</volume>
<pages>edition.</pages>
<contexts>
<context position="887" citStr="Strunk and White, 1979" startWordPosition="121" endWordPosition="124">opic model (Blei et al., 2003) to the representation of stylistic lexical information, evaluating our model on the basis of human-interpretability at the word and text level. We show, in particular, that this model can be applied to the task of inducing stylistic lexicons, and that a multi-dimensional approach is warranted given the correlations among stylistic dimensions. 1 Introduction In language, stylistic variation is a reflection of various contextual factors, including the backgrounds of and relationship between the parties involved. Although in the context of prescriptive linguistics (Strunk and White, 1979), style is often assumed to be a matter of aesthetics, the stylistic intuitions of language users are inextricably linked to the conventions of register and genre (Biber and Conrad, 2009). Intentional or not, stylistic differences play a role in numerous NLP tasks. Examples include genre classification (Kessler et al., 1997), author profiling (Garera and Yarowsky, 2009; Rosenthal and McKeown, 2011), social relationship classification (Peterson et al., 2011), sentiment analysis (Wilson et al., 2005), readability classification (Collins-Thompson and Callan, 2005), and text generation (Hovy, 1990</context>
<context position="3189" citStr="Strunk and White, 1979" startWordPosition="481" endWordPosition="484">tent Dirichlet allocation (Blei et al., 2003), a Bayesian ‘topic’ model, to our stylistic purposes; as such, our approach also follows on recent interest in the interpretability of topic-model topics (Chang et al., 2009; Newman et al., 2011). We show that our model can be used for acquisition of stylistic lexicons, and we also evaluate the model relative to theories of register variation and the expected stylistic character of particular genres. 2 Model 2.1 Linguistic foundations In English manuals of style and other prescriptivist texts (Fowler and Fowler, 1906; Gunning, 1952; Follett, 1966; Strunk and White, 1979; Kane, 1983; Hayakawa, 1994), writers are urged to pay attention to various aspects of lexical style, including elements such as clarity, familiarity, readability, for673 Proceedings of NAACL-HLT 2013, pages 673–679, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics mality, fanciness, colloquialness, specificity, concreteness, objectivity, and naturalness; these stylistic categories reflect common aesthetic judgments about language. In descriptive studies of register, some researchers have posited a few fixed styles (Joos, 1961) or a small, discrete set of sit</context>
</contexts>
<marker>Strunk, White, 1979</marker>
<rawString>William Strunk and E.B. White. 1979. The Elements of Style. Macmillan, 3rd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Julian Brooke</author>
<author>Milan Tofiloski</author>
<author>Kimberly Voll</author>
<author>Manfred Stede</author>
</authors>
<title>Lexicon-based methods for sentiment analysis.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>2</issue>
<contexts>
<context position="10175" citStr="Taboada et al., 2011" startWordPosition="1585" endWordPosition="1588">rary seeds were primarily drawn from web sites which explain difficult language in texts such as the Bible and Lord of the Rings; examples include behold, resplendent, amiss, and thine. The concrete seeds all denote objects and actions strongly rooted in the physical world, e.g. shove and lamppost, while the abstract seeds all involve concepts which require significant human psychological or cultural knowledge to grasp, for instance patriotism and nonchalant. For our subjective seeds, we used an edited list of strongly positive and negative terms from a manually-constructed sentiment lexicon (Taboada et al., 2011), e.g. gorgeous and depraved, and for our objective set we selected words from sets of nearsynonyms where one was clearly an emotionallydistant alternative, e.g. residence (for home), jocular (for funny) and communicable (for contagious). We filtered initial lists to 150 of each type, removing words which did not appear in the corpus or which occurred in multiple lists. For evaluation we used stratified 3-fold crossvalidation, averaged over 5 different (3-way) splits of the seeds, with the same splits used for all evaluated conditions. Given two sets of opposing seeds, we follow our earlier wo</context>
</contexts>
<marker>Taboada, Brooke, Tofiloski, Voll, Stede, 2011</marker>
<rawString>Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly Voll, and Manfred Stede. 2011. Lexicon-based methods for sentiment analysis. Computational Linguistics, 37(2):267–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>417--424</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="2405" citStr="Turney, 2002" startWordPosition="357" endWordPosition="358">l., 2007), or used as individual features for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of lexical variety. Our interest is models that offer broad lexical coverage of human-identifiable stylistic variation. Research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability) (Brooke et al., 2010; Pan and Yang, 2010; Kidwell et al., 2009) and a large body of research on the induction of polarity lexicons, in particular from large corpora (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). Our work is the first to represent multiple dimensions of style in a single statistical model, adapting latent Dirichlet allocation (Blei et al., 2003), a Bayesian ‘topic’ model, to our stylistic purposes; as such, our approach also follows on recent interest in the interpretability of topic-model topics (Chang et al., 2009; Newman et al., 2011). We show that our model can be used for acquisition of stylistic lexicons, and we also evaluate the model relative to theories of register variation and the expected stylistic character of particu</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 417–424, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonid Velikovich</author>
<author>Sasha Blair-Goldensohn</author>
<author>Kerry Hannan</author>
<author>Ryan McDonald</author>
</authors>
<title>The viability of webderived polarity lexicons.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>777--785</pages>
<location>Los Angeles, California.</location>
<contexts>
<context position="2459" citStr="Velikovich et al., 2010" startWordPosition="363" endWordPosition="366">for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of lexical variety. Our interest is models that offer broad lexical coverage of human-identifiable stylistic variation. Research most similar to ours has focused on classifying the lexicon in terms of individual aspects relevant to style (e.g. formality, specificity, readability) (Brooke et al., 2010; Pan and Yang, 2010; Kidwell et al., 2009) and a large body of research on the induction of polarity lexicons, in particular from large corpora (Turney, 2002; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). Our work is the first to represent multiple dimensions of style in a single statistical model, adapting latent Dirichlet allocation (Blei et al., 2003), a Bayesian ‘topic’ model, to our stylistic purposes; as such, our approach also follows on recent interest in the interpretability of topic-model topics (Chang et al., 2009; Newman et al., 2011). We show that our model can be used for acquisition of stylistic lexicons, and we also evaluate the model relative to theories of register variation and the expected stylistic character of particular genres. 2 Model 2.1 Linguistic foundations In Engl</context>
</contexts>
<marker>Velikovich, Blair-Goldensohn, Hannan, McDonald, 2010</marker>
<rawString>Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Hannan, and Ryan McDonald. 2010. The viability of webderived polarity lexicons. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 777–785, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT/EMNLP ’05,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="1390" citStr="Wilson et al., 2005" startWordPosition="198" endWordPosition="201">lationship between the parties involved. Although in the context of prescriptive linguistics (Strunk and White, 1979), style is often assumed to be a matter of aesthetics, the stylistic intuitions of language users are inextricably linked to the conventions of register and genre (Biber and Conrad, 2009). Intentional or not, stylistic differences play a role in numerous NLP tasks. Examples include genre classification (Kessler et al., 1997), author profiling (Garera and Yarowsky, 2009; Rosenthal and McKeown, 2011), social relationship classification (Peterson et al., 2011), sentiment analysis (Wilson et al., 2005), readability classification (Collins-Thompson and Callan, 2005), and text generation (Hovy, 1990; Inkpen and Hirst, 2006). Following the classic work of Biber (1988), computational modeling of style has often focused on textual statistics and the frequency of function words and syntactic categories. When content words are considered, they are often limited to manually-constructed lists (Argamon et al., 2007), or used as individual features for supervised classification, which can be confounded by topic (Petrenz and Webber, 2011) or fail in the face of lexical variety. Our interest is models t</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT/EMNLP ’05, pages 347–354.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>