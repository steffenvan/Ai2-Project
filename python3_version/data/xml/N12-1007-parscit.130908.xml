<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.813264">
Entity Clustering Across Languages
</title>
<author confidence="0.8894075">
Spence Green*, Nicholas Andrews†, Matthew R. Gormley†,
Mark Dredze†, and Christopher D. Manning*
</author>
<affiliation confidence="0.936563">
*Computer Science Department, Stanford University
</affiliation>
<email confidence="0.976731">
{spenceg,manning}@stanford.edu
</email>
<affiliation confidence="0.6954">
†Human Language Technology Center of Excellence, Johns Hopkins University
</affiliation>
<email confidence="0.997758">
{noa,mrg,mdredze}@cs.jhu.edu
</email>
<sectionHeader confidence="0.994825" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996283176470588">
Standard entity clustering systems commonly
rely on mention (string) matching, syntactic
features, and linguistic resources like English
WordNet. When co-referent text mentions ap-
pear in different languages, these techniques
cannot be easily applied. Consequently, we
develop new methods for clustering text men-
tions across documents and languages simulta-
neously, producing cross-lingual entity clusters.
Our approach extends standard clustering algo-
rithms with cross-lingual mention and context
similarity measures. Crucially, we do not as-
sume a pre-existing entity list (knowledge base),
so entity characteristics are unknown. On an
Arabic-English corpus that contains seven dif-
ferent text genres, our best model yields a 24.3%
F1 gain over the baseline.
</bodyText>
<sectionHeader confidence="0.998776" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958263157895">
This paper introduces techniques for clustering co-
referent text mentions across documents and lan-
guages. On the web today, a breaking news item
may instantly result in mentions to a real-world entity
in multiple text formats: news articles, blog posts,
tweets, etc. Much NLP work has focused on model
adaptation to these diverse text genres. However, the
diversity of languages in which the mentions appear
is a more significant challenge. This was particularly
evident during the 2011 popular uprisings in the Arab
world, in which electronic media played a prominent
role. A key issue for the outside world was the aggre-
gation of information that appeared simultaneously
in English, French, and various Arabic dialects.
To our knowledge, we are the first to consider clus-
tering entity mentions across languages without a pri-
ori knowledge of the quantity or types of real-world
entities (a knowledge base). The cross-lingual set-
ting introduces several challenges. First, we cannot
</bodyText>
<page confidence="0.975972">
60
</page>
<bodyText confidence="0.999816625">
assume a prototypical name format. For example,
the Anglo-centric first/middle/last prototype used in
previous name modeling work (cf. (Charniak, 2001))
does not apply to Arabic names like Abdullah ibn
Abd Al-Aziz Al-Saud or Chinese names like Hu Jin-
tao (referred to as Mr. Hu, not Mr. Jintao). Sec-
ond, organization names often require both translit-
eration and translation. For example, the Arabic
</bodyText>
<equation confidence="0.573297666666667">
PPñ�KñÓ ÈQ��g. �é»Qå�...‘General Motors Corp’ contains
�
transliterations of „ñ�KñÓ ÈQ��g. ‘General Motors’,
</equation>
<bodyText confidence="0.9979165">
but a translation of �é»Qå�... ‘Corporation’.
Our models are organized as a pipeline. First, for
each document, we perform standard mention detec-
tion and coreference resolution. Then, we use pair-
wise cross-lingual similarity models to measure both
mention and context similarity. Finally, we cluster
the mentions based on similarity.
Our work makes the following contributions: (1)
introduction of the task, (2) novel models for cross-
lingual entity clustering of person and organization en-
tities, (3) cross-lingual annotation of the NIST Auto-
matic Content Extraction (ACE) 2008 Arabic-English
evaluation set, and (4) experimental results using both
gold and automatic within-document processing. We
will release our software and annotations to support
future research.
</bodyText>
<subsectionHeader confidence="0.994655">
1.1 Task Description via a Simple Example
</subsectionHeader>
<bodyText confidence="0.9992025">
Consider the toy corpus in Fig. 1. The English docu-
ments contain mentions of two people: Steven Paul
Jobs and Mark Elliot Zuckerberg. Of course, the sur-
face realization of Mr. Jobs’ last name in English is
also an ordinary nominal, hence the ambiguous men-
tion string (absent context) in the second document.
The Arabic document introduces an organization en-
tity (Apple Inc.) along with proper and pronominal
references to Mr. Jobs. Finally, the French document
refers to Mr. Jobs by the honorific ‘Monsieur,’ and to
</bodyText>
<note confidence="0.6687852">
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 60–69,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
doc1:
doc2: Jobs program details delayed
Steve Jobs admired Mark Zuckerberg
</note>
<figure confidence="0.9936805">
A3L?I! At.t j.tj&amp; iJii*3 iJ.tl 715
M. Jobs, le fondateur d&apos;Apple, est mort
=
=
≠
≠
=
E1
E2
E3
</figure>
<figureCaption confidence="0.9825728">
Figure 1: Clustering entity mentions across languages and documents. The toy corpus contains English (doc1 and
doc2), Arabic (doc3), and French (doc4). Together, the documents make reference to three real-world entities, the
identification of which is the primary objective of this work. We use a separately-trained system for within-document
mention detection and coreference (indicated by the text boxes and intra-document links, respectively). Our experimental
results are for Arabic-English only.
</figureCaption>
<bodyText confidence="0.998256148148148">
Apple without its corporate designation.
Our goal is to automatically produce the cross-
lingual entity clusters E1 (Mark Elliot Zuckerberg),
E2 (Apple Inc.), and E3 (Steven Paul Jobs). Both the
true number and characteristics of these entities are
unobserved. Our models require two pre-processing
steps: mention detection and within-document coref-
erence/anaphora resolution, shown in Fig. 1 by the
text boxes and intra-document links, respectively. For
example, in doc3, a within-document coreference sys-
tem would pre-linker joobz ‘Jobs’ with the mascu-
line pronoun o h `his&apos;. In addition, the mention detec-
tor determines that the surface form “Jobs” in doc2
is not an entity reference. For this within-document
pre-processing we use Serif (Ramshaw et al., 2011).1
Our models measure cross-lingual similarity of the
coreference chains to make clustering decisions (•
in Fig. 1). The similarity models (indicated by the
= and =6 operators in Fig. 1) consider both mention
string and context similarity (§2). We use the men-
tion similarities as hard constraints, and the context
similarities as soft constraints. In this work, we inves-
tigate two standard constrained clustering algorithms
(§3). Our methods can be used to extend existing sys-
tems for mono-lingual entity clustering (also known
as “cross-document coreference resolution”) to the
cross-lingual setting.
</bodyText>
<footnote confidence="0.9693694">
1Serif is a commercial system that assumes each document
contains only one language. Currently, there are no publicly avail-
able within-document coreference systems for Arabic and many
other languages. To remedy this problem, the CoNNL-2012
shared task aims to develop multilingual coreference systems.
</footnote>
<sectionHeader confidence="0.854526" genericHeader="introduction">
2 Mention and Context Similarity
</sectionHeader>
<bodyText confidence="0.99985612">
Our goal is to create cross-lingual sets of co-referent
mentions to real-world entities (people, places, orga-
nizations, etc.). In this paper, we adopt the following
notation. Let M be a set of distinct text mentions in a
collection of documents; C is a partitioning of M into
document-level sets of co-referent mentions (called
coreference chains); E is a partitioning of C into sets
of co-referent chains (called entities). Let i, j be non-
negative integers less than or equal to |M |and a, b be
non-negative integers less than or equal to |C|. Our
experiments use a separate within-document corefer-
ence system to create C, which is fixed. We will learn
E, which has size no greater than |C |since the set of
mono-lingual chains is the largest valid partitioning.
We define accessor functions to access properties
of mentions and chains. For any mention mi, define
the following functions: lang(mi) is the language;
doc(mi) is the document containing mi; type(mi) is
the semantic type, which is assigned by the within-
document coreference system. We also extract a set
of mention contexts S, which are the sentences con-
taining each mention (i.e., |S |= |M|).
We learn the partition E by considering mention
and context similarity, which are measured with sep-
arate component models.
</bodyText>
<subsectionHeader confidence="0.998324">
2.1 Mention Similarity
</subsectionHeader>
<bodyText confidence="0.999723">
We use separate methods for within- and cross-
language mention similarity. The pairwise similarity
</bodyText>
<page confidence="0.997252">
61
</page>
<subsectionHeader confidence="0.776026">
Arabic Rules
</subsectionHeader>
<equation confidence="0.940522307692308">
HH~ � t
. � b H�� th h. � j
�
h � h p � kh X � d X� th
P a r P z € � s €:� sh
• e s • ,d es t � � th
� �
¬ � f †� q
1/4 � k È --+ l Ð m à�i n
è o h @ i a ð � w ø i a
o ah ø_ � 0 Z � 0
English Rules
k � c ppb x � ks e,i,o,u-0
</equation>
<tableCaption confidence="0.977985">
Table 1: English-Arabic mapping rules to a common or-
</tableCaption>
<bodyText confidence="0.617783">
thographic representation. “0” indicates a null mapping.
For English, we also lowercase and remove determiners
and punctuation. For Arabic, we remove the determiner
È@ Al `the&apos; and the elongation character tatwil `_&apos;.
of any two mentions mi and mj is:
</bodyText>
<equation confidence="0.905562">
sim(mi,mj) =
� faro-winkler(mi, mj) if lang(mi) = lang(mj)
maxent(mi,mj) otherwise
Jaro-Winkler Distance (within-language) If
lang(mi) = lang(mj), we use the Jaro-Winkler edit
</equation>
<bodyText confidence="0.998012294117647">
distance (Porter and Winkler, 1997). Jaro-Winkler
rewards matching prefixes, the empirical justification
being that less variation typically occurs at the
beginning of names.2 The metric produces a score in
the range [0,1], where 0 indicates equality.
Maxent model (cross-language) When lang(mi)
=� lang(mj), then the two mentions might be in dif-
ferent writing systems. Edit distance calculations
no longer apply directly. One solution would be
full-blown transliteration (Knight and Graehl, 1998),
followed by application of Jaro-Winkler. However,
transliteration systems are complex and require sig-
nificant training resources. We find that a simpler,
low-resource approach works well in practice.
First, we deterministically map both languages to a
common phonetic representation (Tbl. 1).3 Next, we
align the mention pairs with the Hungarian algorithm,
</bodyText>
<footnote confidence="0.9946585">
2For multi-token names, we sort the tokens prior to computing
the score, as suggested by Christen (2006).
3This idea is reminiscent of Soundex, which Freeman et al.
(2006) used for cross-lingual name matching.
</footnote>
<figure confidence="0.864568076923077">
Overlap Active for each bigram in
cbigrams(mi,u) U
cbigrams(mj,v)
Bigram-Diff-mi Active for each bigram in
cbigrams(mi) − cbigrams(mj)
Bigram-Diff-mj Active for each bigram in
cbigrams(mj) − cbigrams(mi)
Bigram-Len-Diff Value of abs(size(cbigrams(mi) −
cbigrams(mj)))
Big-Edit-Dist Count of token pairs with
Lev(mi,u, mj,v) &gt; 3.0
Total-Edit-Dist Sum of aligned token edit distances
Length Active for one of:
</figure>
<equation confidence="0.98457925">
len(mi) &gt; len(mj) or
len(mi) &lt; len(mj) or
len(mi) = len(mj)
Length-Diff abs(len(mi) − len(mj))
</equation>
<table confidence="0.5038265">
Singleton Active if len(mi) = 1
Singleton-Pair Active if len(mi) = len(mj) = 1
</table>
<tableCaption confidence="0.9905865">
Table 2: Cross-language Maxent feature templates for a
whitespace-tokenized mention pair (mi, mj) with align-
</tableCaption>
<subsubsectionHeader confidence="0.465151">
ment A.,,,.,. Let (u, v) E A,,,,,,,., indicate aligned to-
</subsubsectionHeader>
<bodyText confidence="0.964406363636364">
ken indices. Define the following functions for strings:
cbigrams(·) returns the set of character bigrams; len(·) is
the token length; Lev(·, ·) is the Levenshtein edit distance
between two strings. Prior to feature extraction, we add
unique start and end symbols to the mention strings.
which produces a word-to-word alignment Ary,.i�...j.4
Finally, we build a simple binary Maxent classifier
p(y|mi, mj; A) that extracts features from the aligned
mentions (Tbl. 2). We learn the parameters A using a
quasi-Newton procedure with Li (lasso) regulariza-
tion (Andrew and Gao, 2007).
</bodyText>
<subsectionHeader confidence="0.999762">
2.2 Context Mapping and Similarity
</subsectionHeader>
<bodyText confidence="0.999904909090909">
Mention strings alone are not always sufficient for
disambiguation. Consider again the simple exam-
ple in Fig. 1. Both doc3 and doc4 reference “Steve
Jobs” and “Apple” in the same contexts. Context co-
occurence and/or similarity can thus disambiguate
these two entities from other entities with similar ref-
erences (e.g., “Steve Jones” or “Apple Corps”). As
with the mention strings, the contexts may originate
in different writing systems. We consider both high-
and low-resource approaches for mapping contexts to
a common representation.
</bodyText>
<footnote confidence="0.968953333333333">
4The Hungarian algorithm finds an optimal minimum-cost
alignment. For pairwise costs between tokens, we used the Lev-
enshtein edit distance
</footnote>
<note confidence="0.222351">
¨ _+ a ¨�i g
</note>
<page confidence="0.99037">
62
</page>
<bodyText confidence="0.999784772727273">
Machine Translation (MT) For the high-resource
setting, if lang(mi) =� English, then we translate both
mi and its context si to English with an MT system.
We use Phrasal (Cer et al., 2010), a phrase-based
system which, like most public MT systems, lacks a
transliteration module. We believe that this approach
yields the most accurate context mapping for high-
resource language pairs (like English-Arabic).
Polylingual Topic Model (PLTM) The polylin-
gual topic model (PLTM) (Mimno et al., 2009) is
a generative process in which document tuples—
groups of topically-similar documents—share a topic
distribution. The tuples need not be sentence-aligned,
so training data is easier to obtain. For example, one
document tuple might be the set of Wikipedia articles
(in all languages) for Steve Jobs.
Let D be a set of document tuples, where
there is one document in each tuple for each
of L languages. Each language has vocabu-
lary V and each document dlt has Ni tokens.
We specify a fixed-size set of topics K. The
PLTM generates the document tuples as follows:
</bodyText>
<equation confidence="0.966238666666667">
POLYLINGUAL TOPIC MODEL
Bt ti Dir(αK) [cross-lingual tuple-topic prior]
0lk ti Dir(oV�) [word-topic prior]
for each token wlt,n with n = {1, ... , Ni}:
zt,n ti Mult(9t)
wlt,n ti Mult(0lzt,n)
</equation>
<bodyText confidence="0.9999902">
For cross-lingual context mapping, we infer the 1-
best topic assignments for each token in all S mention
contexts. This technique reduces V = k for all l.
Moreover, all languages have a common vocabulary:
the set of K topic indices. Since the PLTM is not
a contribution of this paper, we refer the interested
reader to (Mimno et al., 2009) for more details.
After mapping each mention context to a common
representation, we measure context similarity based
on the choice of clustering algorithm.
</bodyText>
<sectionHeader confidence="0.965032" genericHeader="method">
3 Clustering Algorithms
</sectionHeader>
<bodyText confidence="0.992917857142857">
We incorporate the mention and context similarity
measures into a clustering framework. We consider
two algorithms. The first is hierarchical agglomera-
tive clustering (HAC), with which we assume basic
familiarity (Manning et al., 2008). A shortcoming of
HAC is that a stop threshold must be tuned. To avoid
this requirement, we also consider non-parametric
probabilistic clustering in the form of a Dirichlet pro-
cess mixture model (DPMM) (Antoniak, 1974) .
Both clustering algorithms can be modified to ac-
commodate pairwise constraints. We have observed
better results by encoding mention similarity as a
hard constraint. Context similarity is thus the cluster
distance measure.5
To turn the Jaro-Winkler distance into a hard
boolean constraint, we tuned a threshold q on held-out
data, i.e., jaro-winkler(mi, mj) G q =&gt;. mi = mj.
Likewise, the Maxent model is a binary classifier, so
p(y = 1|mi, mj; A) &gt; 0.5 =&gt;. mi = mj.
In both clustering algorithms, any two chains Ca
and Cb cannot share the same cluster assignment if:
</bodyText>
<listItem confidence="0.940803">
1. Document origin: doc(Ca) = doc(Cb)
2. Semantic type: type(Ca) =� type(Cb)
3. Mention Match: sim(mi, mj) = false,
where mi = repr(Ca) and mj = repr(Cb).
</listItem>
<bodyText confidence="0.9999729">
The deterministic accessor function repr(Ca) returns
the representative mention of a chain. The heuristic
we used was “first mention”: the function returns the
earliest mention that appears in the associated docu-
ment. In many languages, the first mention is typi-
cally more complete than later mentions. This heuris-
tic also makes our system less sensitive to within-
document coreference errors.6 The representative
mention only has special status for mention similar-
ity: context similarity considers all mention contexts.
</bodyText>
<subsectionHeader confidence="0.998742">
3.1 Constrained Hierarchical Clustering
</subsectionHeader>
<bodyText confidence="0.998830333333333">
HAC iteratively merges the “nearest” clusters accord-
ing to context similarity. In our system, each cluster
context is a bag of words W formed from the contexts
of all coreference chains in that cluster. For each word
in W we estimate a unigram Entity Language Model
(ELM) (Raghavan et al., 2004):
</bodyText>
<equation confidence="0.964929">
_ countW(w) + PPV (w)
P(w) Ew, countW (w&apos;) + p
PV (w) is the unigram probability in all contexts in
</equation>
<bodyText confidence="0.792937">
the corpus7 and p is a smoothing parameter. For any
</bodyText>
<footnote confidence="0.96882">
5Specification of a combined similarity measure is an inter-
esting direction for future work.
6These constraints are similar to the pairf�lters of Mayfield
et al. (2009).
7Recall that after context mapping, all languages have a com-
mon vocabulary V .
</footnote>
<page confidence="0.998993">
63
</page>
<bodyText confidence="0.99935075">
two entity clusters Ea and Eb, the distance between
PE. and PEb is given by a metric based on the Jensen-
Shannon Divergence (JSD) (Endres and Schindelin,
2003):
</bodyText>
<equation confidence="0.9918845">
�
dist(PEa, PEb) = 2 · JSD(PEa||PEb)
�
= KL(PEa||M) + KL(M||PEb)
</equation>
<bodyText confidence="0.9915475">
where KL(PE.||M) is the Kullback-Leibler diver-
gence and M =1�(PE. + PEb).
We initialize HAC to E = C, i.e., the initial clus-
tering solution is just the set of all coreference chains.
Then we remove all links in the HAC proximity matrix
that violate pairwise cannot-link constraints. During
clustering, we do not merge Ea and Eb if any pair of
chains violates a cannot-link constraint. This proce-
dure propagates the cannot-link constraints (Klein et
al., 2002). To output E, we stop clustering when the
minimum JSD exceeds a stop threshold &apos;y, which is
tuned on a development set.
</bodyText>
<subsectionHeader confidence="0.984933">
3.2 Constrained Dirichlet Process Mixture
Model (DPMM)
</subsectionHeader>
<bodyText confidence="0.9988855">
Instead of tuning a parameter like -y, it would be prefer-
able to let the data dictate the number of entity clus-
ters. We thus consider a non-parametric Bayesian
mixture model where the mixtures are multinomial
distributions over the entity contexts S. Specifically,
we consider a DPMM, which automatically infers
the number of mixtures. Each Ca has an associated
mixture Ba:
</bodyText>
<equation confidence="0.9998415">
Ca|Ba — Mult(Ba)
Ba|G — G
G|α, G0 — DP(α, G0)
α — Gamma(1,1)
</equation>
<bodyText confidence="0.999955705882353">
where α is the concentration parameter of the DP
prior and G0 is the base distribution with support V .
For our experiments, we set G0 = Dir(7r1,... , 7rV ),
where 7ri = PV (wi).
For inference, we use the Gibbs sampler of Vla-
chos et al. (2009), which can incorporate pairwise
constraints. The sampler is identical to a standard col-
lapsed, token-based sampler, except the conditional
probability p(Ea = E|E_a, Ca) = 0 if Ca cannot
be merged with the chains in cluster E. This property
makes the model non-exchangeable, but in practice
non-exchangeable models are sometimes useful (Blei
and Frazier, 2010). During sampling, we also learn α
using the auxiliary variable procedure of West (1995),
so the only fixed parameters are those of the vague
Gamma prior. However, we found that these hyper-
parameters were not sensitive.
</bodyText>
<sectionHeader confidence="0.78229" genericHeader="method">
4 Training Data and Procedures
</sectionHeader>
<bodyText confidence="0.998735264705882">
We trained our system for Arabic-English cross-
lingual entity clustering.8
Maxent Mention Similarity The Maxent mention
similarity model requires a parallel name list for train-
ing. Name pair lists can be obtained from the LDC
(e.g., LDC2005T34 contains nearly 450,000 parallel
Chinese-English names) or Wikipedia (Irvine et al.,
2010). We extracted 12,860 name pairs from the par-
allel Arabic-English translation treebanks,9 although
our experiments show that the model achieves high
accuracy with significantly fewer training examples.
We generated a uniform distribution of training ex-
amples by running a Bernoulli trial for each aligned
name pair in the corpus. If the coin was heads, we
replaced the English name with another English name
chosen randomly from the corpus.
MT Context Mapping For the MT context map-
ping method, we trained Phrasal with all data permit-
ted under the NIST OpenMT Ar-En 2009 constrained
track evaluation. We built a 5-gram language model
from the Xinhua and AFP sections of the Gigaword
corpus (LDC2007T07), in addition to all of the target
side training data. In addition to the baseline Phrasal
feature set, we used the lexicalized re-ordering model
of Galley and Manning (2008).
PLTM Context Mapping For PLTM training, we
formed a corpus of 19,139 English-Arabic topically-
aligned Wikipedia articles. Cross-lingual links in
Wikipedia are abundant: as of February 2010, there
were 77.07M cross-lingual links among Wikipedia�s
272 language editions (de Melo and Weikum, 2010).
To increase vocabulary coverage for our ACE2008
evaluation corpus, we added 20,000 document sin-
gletons from the ACE2008 training corpus. The
</bodyText>
<footnote confidence="0.9880654">
8We tokenized all English documents with packages from
the Stanford parser (Klein and Manning, 2003). For Arabic
documents, we used Mada (Habash and Rambow, 2005) for
orthographic normalization and clitic segmentation.
9LDC Catalog numbers LDC2009E82 and LDC2009E88.
</footnote>
<page confidence="0.998937">
64
</page>
<bodyText confidence="0.998963833333333">
topically-aligned tuples served as “glue” to share top-
ics between languages, while the ACE documents
distribute those topics over in-domain vocabulary.10
We used the PLTM implementation in Mallet (Mc-
Callum, 2002). We ran the sampler for 10,000 itera-
tions and set the number of topics K = 512.
</bodyText>
<sectionHeader confidence="0.990114" genericHeader="method">
5 Task Evaluation Framework
</sectionHeader>
<bodyText confidence="0.993356">
Our experimental design is a cross-lingual extension
of the standard cross-document coreference resolu-
tion task, which appeared in ACE2008 (Strassel et
al., 2008; NIST, 2008). We evaluate name (NAM)
mentions for cross-lingual person (PER) and organi-
zation (ORG) entities. Neither the number nor the
attributes of the entities are known (i.e., the task does
not include a knowledge base). We report results for
both gold and automatic within-document mention
detection and coreference resolution.
Evaluation Metrics We use entity-level evaluation
metrics, i.e., we evaluate the E entity clusters rather
than the mentions. For the gold setting, we report:
</bodyText>
<listItem confidence="0.999585142857143">
• B3 (Bagga and Baldwin, 1998a): Precision and
recall are computed from the intersection of the
hypothesis and reference clusters.
• CEAF (Luo, 2005): Precision and recall are
computed from a maximum bipartite matching
between hypothesis and reference clusters.
• NVI (Reichart and Rappoport, 2009):
</listItem>
<bodyText confidence="0.780806117647059">
Information-theoretic measure that uti-
lizes the entropy of the clusters and their mutual
information. Unlike the commonly-used Varia-
tion of Information (VI) metric, normalized VI
(NVI) is not sensitive to the size of the data set.
For the automatic setting, we must apply a different
metric since the number of system chains may differ
from the reference. We use B3sys (Cai and Strube,
2010), a variant of B3 that was shown to penalize
both twinless reference chains and spurious system
chains more fairly.
Evaluation Corpus The automatic evaluation of
cross-lingual coreference systems requires annotated
10Mimno et al. (2009) showed that so long as the proportion
of topically-aligned to non-aligned documents exceeded 0.25,
the topic distributions (as measured by mean Jensen-Shannon
Divergence between distributions) did not degrade significantly.
</bodyText>
<table confidence="0.948913666666667">
Docs Tokens Entities Chains Mentions
ARABIC 412 178,269 2,594 4,216 9,222
ENGLISH 414 246,309 2,278 3,950 9,140
</table>
<tableCaption confidence="0.86572375">
Table 3: ACE2008 evaluation corpus PER and ORG entity
statistics. Singleton chains account for 51.4% of the Arabic
data and 46.2% of the English data. Just 216 entities appear
in both languages.
</tableCaption>
<bodyText confidence="0.999683">
multilingual corpora. Cross-document annotation
is expensive (Strassel et al., 2008), so we chose the
ACE2008 Arabic-English evaluation corpus as a start-
ing point for cross-lingual annotation. The corpus
consists of seven genres sampled from independent
sources over the course of a decade (Tbl. 3). The
corpus provides gold mono-lingual cross-document
coreference annotations for both PER and ORG enti-
ties. Using these annotations as a starting point, we
found and annotated 216 cross-lingual entities.11
Because a similar corpus did not exist for develop-
ment, we split the evaluation corpus into development
and test sections. However, the usual method of split-
ting by document would not confine all mentions of
each entity to one side of the split. We thus split the
corpus by global entity id. We assigned one-third of
the entities to development, and the remaining two-
thirds to test.
</bodyText>
<sectionHeader confidence="0.96104" genericHeader="method">
6 Comparison to Related Tasks and Work
</sectionHeader>
<bodyText confidence="0.965821444444444">
Our modeling techniques and task formulation can be
viewed as cross-lingual extensions to cross-document
coreference resolution. The classic work on this task
was by Bagga and Baldwin (1998b), who adapted
the Vector Space Model (VSM) (Salton et al., 1975).
Gooi and Allan (2004) found effective algorithmic
extensions like agglomerative clustering. Successful
feature extensions to the VSM for cross-document
coreference have included biographical information
(Mann and Yarowsky, 2003) and syntactic context
(Chen and Martin, 2007). However, neither of these
feature sets generalize easily to the cross-lingual set-
ting with multiple entity types. Fleischman and Hovy
(2004) added a discriminative pairwise mention clas-
sifier to a VSM-like model, much as we do. More
11The annotators were the first author and another fluent
speaker of Arabic. The annotations, corrections, and corpus
split are available at http://www.spencegreen.com/research/.
</bodyText>
<page confidence="0.999276">
65
</page>
<bodyText confidence="0.999825421052632">
recent work has considered new models for web-scale
corpora (Rao et al., 2010; Singh et al., 2011).
Cross-document work on languages other than En-
glish is scarce. Wang (2005) used a combination of
the VSM and heuristic feature selection strategies to
cluster transliterated Chinese personal names. For
Arabic, Magdy et al. (2007) started with the output of
the mention detection and within-document corefer-
ence system of Florian et al. (2004). They clustered
the entities incrementally using a binary classifier.
Baron and Freedman (2008) used complete-link ag-
glomerative clustering, where merging decisions were
based on a variety of features such as document topic
and name uniqueness. Finally, Sayeed et al. (2009)
translated Arabic name mentions to English and then
formed clusters greedily using pairwise matching.
To our knowledge, the cross-lingual entity cluster-
ing task is novel. However, there is significant prior
work on similar tasks:
</bodyText>
<listItem confidence="0.985982944444445">
• Multilingual coreference resolution: Adapt
English within-document coreference models to
other languages (Harabagiu and Maiorano, 2000;
Florian et al., 2004; Luo and Zitouni, 2005).
• Named entity translation: For a non-English
document, produce an inventory of entities in
English. An ACE2007 pilot task (Song and
Strassel, 2008).
• Named entity clustering: Assign semantic
types to text mentions (Collins and Singer, 1999;
Elsner et al., 2009).
• Cross-language name search / entity linking:
Match a single query name against a list of
known multilingual names (knowledge base). A
track in the 2011 NIST Text Analysis Conference
(TAC-KBP) evaluation (Aktolga et al., 2008;
McCarley, 2009; Udupa and Khapra, 2010; Mc-
Namee et al., 2011).
</listItem>
<bodyText confidence="0.996793666666667">
Our work incorporates elements of the first three tasks.
Most importantly, we avoid the key element of entity
linking: a knowledge base.
</bodyText>
<sectionHeader confidence="0.998089" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.99974375">
We performed intrinsic evaluations for both mention
and context similarity. For context similarity, we
analyzed mono-lingual entity clustering, which also
facilitated comparison to prior work on the ACE2008
</bodyText>
<table confidence="0.9996018">
Genre #Train #Test Accuracy(%)
wb 125 16 87.5
bn 2,720 340 95.6
nw 7,443 930 96.6
all 10,288 1,286 97.1 (+7.55)
</table>
<tableCaption confidence="0.989076833333333">
Table 4: Cross-lingual mention matching accuracy [%].
The training data contains names from three genres: broad-
cast news (bn), newswire (nw), and weblog (wb). We used
the full training corpus (all) for the cross-lingual clustering
experiments, but the model achieved high accuracy with
significantly fewer training examples (e.g., bn).
</tableCaption>
<table confidence="0.854026666666667">
B3 T
#hyp P R F1
Mono-lingual Arabic (#gold=1,721)
HAC 87.2 0.052 1,669 89.8 89.8 89.8
Mono-lingual English (#gold=1,529)
HAC 88.5 0.042 1,536 93.7 89.0 91.4
</table>
<tableCaption confidence="0.978916">
Table 5: Mono-lingual entity clustering evaluation (test
</tableCaption>
<bodyText confidence="0.687464166666667">
set, gold within-document processing). Higher scores (T)
are better for CEAF and B3, whereas lower (].) is better
for NVI. #gold indicates the number of reference entities,
whereas #hyp is the size of E.
evaluation set. Our main results are for the new task:
cross-lingual entity clustering.
</bodyText>
<subsectionHeader confidence="0.97271">
7.1 Intrinsic Evaluations
</subsectionHeader>
<bodyText confidence="0.951788722222222">
Cross-lingual Mention Matching We created a
random 80/10/10 (train, development, test) split of
the Maxent training corpus and evaluated binary clas-
sification accuracy (Tbl. 4). Of the mis-classified
examples, we observed three major error types. First,
the model learns that high edit distance is predictive
of a mismatch. However, singleton strings that do not
match often have a lower edit distance than longer
strings that do match. As a result, singletons often
cause false positives. Second, names that originate in
a third language tend to violate the phonemic corre-
spondences. For example, the model gives a false neg-
ative for a German football team: vim,� , �. , v 1
(phonetic mapping: af s kazrslawtrn) versus “FC
Kaiserslautern.” Finally, names that require trans-
lation are problematic. For example, the classifier
produces a false negative for (God, gd) ? (ail1, allh).
CEAFT NVI].
</bodyText>
<page confidence="0.758708">
66
</page>
<table confidence="0.997061">
#gold = 3,057 CEAFT NVIt #hyp B3 T R F1 B3target T (#gold = 146)
P #hyp P R F1
SINGLEToN 64.9 0.165 5,453 100.0 56.1 71.8 1,587 100.0 9.20 16.9
No-CoNTExT 57.4 0.136 2,216 65.6 75.2 70.1 517 78.3 41.8 54.5
HAC+MT 79.8 0.070 2,783 84.4 86.4 85.4 310 91.7 69.1 78.8
DPMM+MT 74.3 0.122 3,649 89.3 64.1 74.6 634 93.3 24.3 38.6
HAC+PLTM 72.1 0.110 2,746 76.9 77.6 77.3 506 84.4 44.6 58.4
DPMM+PLTM 57.2 0.180 2,609 64.0 62.8 63.4 715 73.9 22.2 34.1
</table>
<tableCaption confidence="0.995782">
Table 6: Cross-lingual entity clustering (test set, gold within-document processing). B3target is the standard B3 metric
</tableCaption>
<bodyText confidence="0.917556181818182">
applied to the subset of target cross-lingual entities in the test set. For CEAF and B3, SINGLEToN is the stronger baseline
due to the high proportion of singleton entities in the corpus. Of course, cross-lingual entities have at least two chains,
so No-CoNTExT is a better baseline for cross-lingual clustering.
Mono-lingual Entity Clustering For comparison,
we also evaluated our system on a standard mono-
lingual cross-document coreference task (Arabic and
English) (Tbl. 5). We configured the system with
HAC clustering and Jaro-Winkler (within-language)
mention similarity. We built mono-lingual ELMs for
context similarity.
We used two baselines:
</bodyText>
<listItem confidence="0.998309333333333">
• SINGLEToN: E = C, i.e., the cross-lingual clus-
tering solution is just the set of mono-lingual
coreference chains. This is a common baseline
for mono-lingual entity clustering (Baron and
Freedman, 2008).
• No-CoNTExT: We run HAC with p = oc. There-
</listItem>
<bodyText confidence="0.959025166666667">
fore, E is the set of fully-connected components
in C subject to the pairwise constraints.
For HAC, we manually tuned the stop threshold -y,
the Jaro-Winkler threshold q, and the ELM smoothing
parameter p on the development set. For the DPMM,
no development tuning was necessary, and we evalu-
ated a single sample of E taken after 3,000 iterations.
To our knowledge, Baron and Freedman (2008)
reported the only previous results on the ACE2008
data set. However, they only gave gold results for
English, and clustered the entire evaluation corpus
(test+development). To control for the effect of
within-document errors, we considered their gold in-
put (mention detection and within-document coref-
erence resolution) results. They reported B3 for the
two entity types separately: ORG (91.5% F1) and
PER (94.3% F1). The different experimental designs
preclude a precise comparison, but the accuracy of
</bodyText>
<table confidence="0.998915">
#gold = 3,057 #hyp B3,�, T F1
P R
SINGLEToN 7,655 100.0 57.1 72.7
No-CoNTExT 2,918 63.3 71.1 67.0
HAC+MT 3,804 75.6 77.8 76.7
DPMM+MT 4,491 77.1 62.5 69.0
HAC+PLTM 6,353 94.1 62.8 75.3
DPMM+PLTM 3,522 64.6 62.0 63.3
</table>
<tableCaption confidence="0.993602">
Table 7: Cross-lingual entity clustering (test set, automatic
</tableCaption>
<bodyText confidence="0.734286">
(Serif) within-document processing). For HAC, we used
the same parameters as the gold setting.
the two systems are at least in the same range.
</bodyText>
<subsectionHeader confidence="0.99775">
7.2 Cross-lingual Entity Clustering
</subsectionHeader>
<bodyText confidence="0.9996723125">
We evaluated four system configurations on the new
task: HAC+MT, HAC+PLTM, DPMM+MT, and
DPMM+PLTM. First, we established an upper bound
by assuming gold within-document mention detection
and coreference resolution (Tbl. 6). This setting iso-
lated the new cross-lingual clustering methods from
within-document processing errors. Then we evalu-
ated with Serif (automatic) within-document process-
ing (Tbl. 7). This second experiment replicated an
application setting. We used the same baselines and
tuning procedures as in the mono-lingual clustering
experiment.
Results In the gold setting, HAC+MT produces the
best results, as expected. The dimensionality reduc-
tion of the vocabulary imposed by PLTM significantly
reduces accuracy, but HAC+PLTM still exceeds the
</bodyText>
<page confidence="0.998206">
67
</page>
<bodyText confidence="0.999991548387097">
baseline. We tried increasing the number of PLTM
topics k, but did not observe an improvement in task
accuracy. For both context-mapping methods, the
DPMM suffers from low-recall. Upon inspection, the
clustering solution of DPMM+MT contains a high
proportion of singleton hypotheses, suggesting that
the model finds lower similarity in the presence of a
larger vocabulary. When the context vocabulary con-
sists of PLTM topics, larger clusters are discovered
(DPMM+PLTM).
The effect of dimensionality reduction is also appar-
ent in the clustering solutions of the PLTM models.
For example, for the Serif output, DPMM+PLTM
produces a cluster consisting of “White House”, “Sen-
ate”, “House of Representatives”, and “Parliament”.
Arabic mentions of the latter three entities pass the
pairwise mention similarity constraints due to the
word 0_4A ‘council’, which appears in text mentions
for all three legislative bodies. A cross-language
matching error resulted in the linking of “White
House”, and the reduced granularity of the contexts
precluded further disambiguation. Of course, these
entities probably appear in similar contexts.
The caveat with the Serif results in Tbl. 7 is that
3,251 of the 7,655 automatic coreference chains are
not in the reference. Consequently, the evaluation is
dominated by the penalty for spurious system coref-
erence chains. Nonetheless, all models except for
DPMM+PLTM exceed the baselines, and the rela-
tionships between models depicted in the gold exper-
iments hold for the this setting.
</bodyText>
<sectionHeader confidence="0.997654" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999228675675676">
Cross-lingual entity clustering is a natural step to-
ward more robust natural language understanding.
We proposed pipeline models that make clustering
decisions based on cross-lingual similarity. We inves-
tigated two methods for mapping documents in differ-
ent languages to a common representation: MT and
the PLTM. Although MT may achieve more accurate
results for some language pairs, the PLTM training
resources (e.g., Wikipedia) are readily available for
many languages. As for the clustering algorithms,
HAC appears to perform better than the DPMM on
our dataset, but this may be due to the small corpus
size. The instance-level constraints represent tenden-
cies that could be learned from larger amounts of data.
With more data, we might be able to relax the con-
straints and use an exchangeable DPMM, which might
be more effective. Finally, we have shown that sig-
nificant quantities of within-document errors cascade
into the cross-lingual clustering phase. As a result,
we plan a model that clusters the mentions directly,
thus removing the dependence on within-document
coreference resolution.
In this paper, we have set baselines and proposed
models that significantly exceeded those baselines.
The best model improved upon the cross-lingual en-
tity baseline by 24.3% F1. This result was achieved
without a knowledge base, which is required by previ-
ous approaches to cross-lingual entity linking. More
importantly, our techniques can be used to extend
existing cross-document entity clustering systems for
the increasingly multilingual web.
Acknowledgments We thank Jason Eisner, David Mimno,
Scott Miller, Jim Mayfield, and Paul McNamee for helpful
discussions. This work was started during the SCALE
2010 summer workshop at Johns Hopkins. The first author
is supported by a National Science Foundation Graduate
Fellowship.
</bodyText>
<sectionHeader confidence="0.997169" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999556884615385">
E. Aktolga, M. Cartright, and J. Allan. 2008. Cross-document
cross-lingual coreference retrieval. In CIKM.
G. Andrew and J. Gao. 2007. Scalable training of L1-regularized
log-linear models. In ICML.
C. E. Antoniak. 1974. Mixtures of Dirichlet processes with
applications to Bayesian nonparametric problems. The Annals
of Statistics, 2(6):1152–1174.
A. Bagga and B. Baldwin. 1998a. Algorithms for scoring coref-
erence chains. In LREC.
A. Bagga and B. Baldwin. 1998b. Entity-based cross-document
coreferencing using the vector space model. In COLING-ACL.
A. Baron and M. Freedman. 2008. Who is Who and What
is What: Experiments in cross-document co-reference. In
EMNLP.
D. Blei and P. Frazier. 2010. Distance dependent Chinese restau-
rant processes. In ICML.
J. Cai and M. Strube. 2010. Evaluation metrics for end-to-
end coreference resolution systems. In Proceedings of the
SIGDIAL 2010 Conference.
D. Cer, M. Galley, D. Jurafsky, and C. D. Manning. 2010. Phrasal:
A statistical machine translation toolkit for exploring new
model features. In HLT-NAACL, Demonstration Session.
E. Charniak. 2001. Unsupervised learning of name structure
from coreference data. In NAACL.
Y. Chen and J. Martin. 2007. Towards robust unsupervised
personal name disambiguation. In EMNLP-CoNLL.
</reference>
<page confidence="0.995199">
68
</page>
<reference confidence="0.999736017857143">
P. Christen. 2006. A comparison of personal name matching:
Techniques and practical issues. Technical Report TR-CS-06-
02, Australian National University.
M. Collins and Y. Singer. 1999. Unsupervised models for named
entity classification. In EMNLP.
G. de Melo and G. Weikum. 2010. Untangling the cross-lingual
link structure of Wikipedia. In ACL.
M. Elsner, E. Charniak, and M. Johnson. 2009. Structured
generative models for unsupervised named-entity clustering.
In HLT-NAACL.
D. M. Endres and J. E. Schindelin. 2003. A new metric for
probability distributions. IEEE Transactions on Information
Theory, 49(7):1858 – 1860.
M. Fleischman and E. Hovy. 2004. Multi-document person name
resolution. In ACL Workshop on Reference Resolution and its
Applications.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, et al.
2004. A statistical model for multilingual entity detection and
tracking. In HLT-NAACL.
A. T. Freeman, S. L. Condon, and C. M. Ackerman. 2006. Cross
linguistic name matching in English and Arabic: a one to
many mapping extension of the Levenshtein edit distance
algorithm. In HLT-NAACL.
M. Galley and C. D. Manning. 2008. A simple and effective
hierarchical phrase reordering model. In EMNLP.
C. H. Gooi and J. Allan. 2004. Cross-document coreference on
a large scale corpus. In HLT-NAACL.
N. Habash and O. Rambow. 2005. Arabic tokenization, part-of-
speech tagging and morphological disambiguation in one fell
swoop. In ACL.
S. M. Harabagiu and S. J. Maiorano. 2000. Multilingual corefer-
ence resolution. In ANLP.
A. Irvine, C. Callison-Burch, and A. Klementiev. 2010. Translit-
erating from all languages. In AMTA.
D. Klein and C. D. Manning. 2003. Accurate unlexicalized
parsing. In ACL.
D. Klein, S. D. Kamvar, and C. D. Manning. 2002. From instance-
level constraints to space-level constraints: Making the most
of prior knowledge in data clustering. In ICML.
K. Knight and J. Graehl. 1998. Machine transliteration. Compu-
tational Linguistics, 24:599–612.
X. Luo and I. Zitouni. 2005. Multi-lingual coreference resolution
with syntactic features. In HLT-EMNLP.
X. Luo. 2005. On coreference resolution performance metrics.
In HLT-EMNLP.
W. Magdy, K. Darwish, O. Emam, and H. Hassan. 2007. Arabic
cross-document person name normalization. In Workshop on
Computational Approaches to Semitic Languages.
G. S. Mann and D. Yarowsky. 2003. Unsupervised personal
name disambiguation. In NAACL.
C. D. Manning, P. Raghavan, and H. Schütze. 2008. Introduction
to Information Retrieval. Cambridge University Press.
J. Mayfield, D. Alexander, B. Dorr, J. Eisner, T. Elsayed, et al.
2009. Cross-document coreference resolution: A key technol-
ogy for learning by reading. In AAAI Spring Symposium on
Learning by Reading and Learning to Read.
A. K. McCallum. 2002. MALLET: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
J. S. McCarley. 2009. Cross language name matching. In SIGIR.
P. McNamee, J. Mayfield, D. Lawrie, D.W. Oard, and D. Doer-
mann. 2011. Cross-language entity linking. In IJCNLP.
D. Mimno, H. M. Wallach, J. Naradowsky, D. A. Smith, and
A. McCallum. 2009. Polylingual topic models. In EMNLP.
NIST. 2008. Automatic Content Extraction 2008 evaluation
plan (ACE2008): Assessment of detection and recognition
of entities and relations within and across documents. Tech-
nical Report rev. 1.2d, National Institute of Standards and
Technology (NIST), 8 August.
E. H. Porter and W. E. Winkler, 1997. Approximate String Com-
parison and its Effect on an Advanced Record Linkage System,
chapter 6, pages 190–199. U.S. Bureau of the Census.
H. Raghavan, J. Allan, and A. McCallum. 2004. An explo-
ration of entity models, collective classification and relation
description. In KDD Workshop on Link Analysis and Group
Detection.
L. Ramshaw, E. Boschee, M. Freedman, J. MacBride,
R. Weischedel, and A. Zamanian. 2011. SERIF language
processing—effective trainable language understanding. In
J. Olive et al., editors, Handbook ofNatural Language Process-
ing and Machine Translation: DARPA Global Autonomous
Language Exploitation, pages 636–644. Springer.
D. Rao, P. McNamee, and M. Dredze. 2010. Streaming cross
document entity coreference resolution. In COLING.
R. Reichart and A. Rappoport. 2009. The NVI clustering evalu-
ation measure. In CoNLL.
G. Salton, A. Wong, and C. S. Yang. 1975. A vector space model
for automatic indexing. CACM, 18:613–620, November.
A. Sayeed, T. Elsayed, N. Garera, D. Alexander, T. Xu, et al.
2009. Arabic cross-document coreference detection. In ACL-
IJCNLP, Short Papers.
S. Singh, A. Subramanya, F. Pereira, and A. McCallum. 2011.
Large-scale cross-document coreference using distributed in-
ference and hierarchical models. In ACL.
Z. Song and S. Strassel. 2008. Entity translation and alignment
in the ACE-07 ET task. In LREC.
S. Strassel, M. Przybocki, K. Peterson, Z. Song, and K. Maeda.
2008. Linguistic resources and evaluation techniques for
evaluation of cross-document automatic content extraction.
In LREC.
R. Udupa and M. M. Khapra. 2010. Improving the multilin-
gual user experience of Wikipedia using cross-language name
search. In HLT-NAACL.
A. Vlachos, A. Korhonen, and Z. Ghahramani. 2009. Unsuper-
vised and constrained Dirichlet process mixture models for
verb clustering. In Proc. of the Workshop on Geometrical
Models of Natural Language Semantics.
H. Wang. 2005. Cross-document transliterated personal name
coreference resolution. In L. Wang and Y. Jin, editors, Fuzzy
Systems and Knowledge Discovery, volume 3614 of Lecture
Notes in Computer Science, pages 11–20. Springer.
M. West. 1995. Hyperparameter estimation in Dirichlet process
mixture models. Technical report, Duke University.
</reference>
<page confidence="0.999313">
69
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.287234">
<title confidence="0.999881">Entity Clustering Across Languages</title>
<author confidence="0.837175">Nicholas Matthew R</author>
<author confidence="0.837175">D Christopher</author>
<affiliation confidence="0.920763">Science Department, Stanford</affiliation>
<email confidence="0.999296">spenceg@stanford.edu</email>
<email confidence="0.999296">manning@stanford.edu</email>
<affiliation confidence="0.515108">Language Technology Center of Excellence, Johns Hopkins</affiliation>
<email confidence="0.999559">noa@cs.jhu.edu</email>
<email confidence="0.999559">mrg@cs.jhu.edu</email>
<email confidence="0.999559">mdredze@cs.jhu.edu</email>
<abstract confidence="0.992745111111111">Standard entity clustering systems commonly rely on mention (string) matching, syntactic features, and linguistic resources like English WordNet. When co-referent text mentions appear in different languages, these techniques cannot be easily applied. Consequently, we develop new methods for clustering text mentions across documents and languages simultaneously, producing cross-lingual entity clusters. Our approach extends standard clustering algorithms with cross-lingual mention and context similarity measures. Crucially, we do not assume a pre-existing entity list (knowledge base), so entity characteristics are unknown. On an Arabic-English corpus that contains seven different text genres, our best model yields a 24.3% F1 gain over the baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Aktolga</author>
<author>M Cartright</author>
<author>J Allan</author>
</authors>
<title>Cross-document cross-lingual coreference retrieval.</title>
<date>2008</date>
<booktitle>In CIKM.</booktitle>
<contexts>
<context position="25955" citStr="Aktolga et al., 2008" startWordPosition="4057" endWordPosition="4060">thin-document coreference models to other languages (Harabagiu and Maiorano, 2000; Florian et al., 2004; Luo and Zitouni, 2005). • Named entity translation: For a non-English document, produce an inventory of entities in English. An ACE2007 pilot task (Song and Strassel, 2008). • Named entity clustering: Assign semantic types to text mentions (Collins and Singer, 1999; Elsner et al., 2009). • Cross-language name search / entity linking: Match a single query name against a list of known multilingual names (knowledge base). A track in the 2011 NIST Text Analysis Conference (TAC-KBP) evaluation (Aktolga et al., 2008; McCarley, 2009; Udupa and Khapra, 2010; McNamee et al., 2011). Our work incorporates elements of the first three tasks. Most importantly, we avoid the key element of entity linking: a knowledge base. 7 Experiments We performed intrinsic evaluations for both mention and context similarity. For context similarity, we analyzed mono-lingual entity clustering, which also facilitated comparison to prior work on the ACE2008 Genre #Train #Test Accuracy(%) wb 125 16 87.5 bn 2,720 340 95.6 nw 7,443 930 96.6 all 10,288 1,286 97.1 (+7.55) Table 4: Cross-lingual mention matching accuracy [%]. The trainin</context>
</contexts>
<marker>Aktolga, Cartright, Allan, 2008</marker>
<rawString>E. Aktolga, M. Cartright, and J. Allan. 2008. Cross-document cross-lingual coreference retrieval. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Andrew</author>
<author>J Gao</author>
</authors>
<title>Scalable training of L1-regularized log-linear models.</title>
<date>2007</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="11020" citStr="Andrew and Gao, 2007" startWordPosition="1702" endWordPosition="1705">Let (u, v) E A,,,,,,,., indicate aligned token indices. Define the following functions for strings: cbigrams(·) returns the set of character bigrams; len(·) is the token length; Lev(·, ·) is the Levenshtein edit distance between two strings. Prior to feature extraction, we add unique start and end symbols to the mention strings. which produces a word-to-word alignment Ary,.i�...j.4 Finally, we build a simple binary Maxent classifier p(y|mi, mj; A) that extracts features from the aligned mentions (Tbl. 2). We learn the parameters A using a quasi-Newton procedure with Li (lasso) regularization (Andrew and Gao, 2007). 2.2 Context Mapping and Similarity Mention strings alone are not always sufficient for disambiguation. Consider again the simple example in Fig. 1. Both doc3 and doc4 reference “Steve Jobs” and “Apple” in the same contexts. Context cooccurence and/or similarity can thus disambiguate these two entities from other entities with similar references (e.g., “Steve Jones” or “Apple Corps”). As with the mention strings, the contexts may originate in different writing systems. We consider both highand low-resource approaches for mapping contexts to a common representation. 4The Hungarian algorithm fi</context>
</contexts>
<marker>Andrew, Gao, 2007</marker>
<rawString>G. Andrew and J. Gao. 2007. Scalable training of L1-regularized log-linear models. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Antoniak</author>
</authors>
<title>Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems.</title>
<date>1974</date>
<journal>The Annals of Statistics,</journal>
<volume>2</volume>
<issue>6</issue>
<contexts>
<context position="13970" citStr="Antoniak, 1974" startWordPosition="2178" endWordPosition="2179">fter mapping each mention context to a common representation, we measure context similarity based on the choice of clustering algorithm. 3 Clustering Algorithms We incorporate the mention and context similarity measures into a clustering framework. We consider two algorithms. The first is hierarchical agglomerative clustering (HAC), with which we assume basic familiarity (Manning et al., 2008). A shortcoming of HAC is that a stop threshold must be tuned. To avoid this requirement, we also consider non-parametric probabilistic clustering in the form of a Dirichlet process mixture model (DPMM) (Antoniak, 1974) . Both clustering algorithms can be modified to accommodate pairwise constraints. We have observed better results by encoding mention similarity as a hard constraint. Context similarity is thus the cluster distance measure.5 To turn the Jaro-Winkler distance into a hard boolean constraint, we tuned a threshold q on held-out data, i.e., jaro-winkler(mi, mj) G q =&gt;. mi = mj. Likewise, the Maxent model is a binary classifier, so p(y = 1|mi, mj; A) &gt; 0.5 =&gt;. mi = mj. In both clustering algorithms, any two chains Ca and Cb cannot share the same cluster assignment if: 1. Document origin: doc(Ca) = </context>
</contexts>
<marker>Antoniak, 1974</marker>
<rawString>C. E. Antoniak. 1974. Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems. The Annals of Statistics, 2(6):1152–1174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="21026" citStr="Bagga and Baldwin, 1998" startWordPosition="3316" endWordPosition="3319">standard cross-document coreference resolution task, which appeared in ACE2008 (Strassel et al., 2008; NIST, 2008). We evaluate name (NAM) mentions for cross-lingual person (PER) and organization (ORG) entities. Neither the number nor the attributes of the entities are known (i.e., the task does not include a knowledge base). We report results for both gold and automatic within-document mention detection and coreference resolution. Evaluation Metrics We use entity-level evaluation metrics, i.e., we evaluate the E entity clusters rather than the mentions. For the gold setting, we report: • B3 (Bagga and Baldwin, 1998a): Precision and recall are computed from the intersection of the hypothesis and reference clusters. • CEAF (Luo, 2005): Precision and recall are computed from a maximum bipartite matching between hypothesis and reference clusters. • NVI (Reichart and Rappoport, 2009): Information-theoretic measure that utilizes the entropy of the clusters and their mutual information. Unlike the commonly-used Variation of Information (VI) metric, normalized VI (NVI) is not sensitive to the size of the data set. For the automatic setting, we must apply a different metric since the number of system chains may </context>
<context position="23573" citStr="Bagga and Baldwin (1998" startWordPosition="3706" endWordPosition="3709">entities.11 Because a similar corpus did not exist for development, we split the evaluation corpus into development and test sections. However, the usual method of splitting by document would not confine all mentions of each entity to one side of the split. We thus split the corpus by global entity id. We assigned one-third of the entities to development, and the remaining twothirds to test. 6 Comparison to Related Tasks and Work Our modeling techniques and task formulation can be viewed as cross-lingual extensions to cross-document coreference resolution. The classic work on this task was by Bagga and Baldwin (1998b), who adapted the Vector Space Model (VSM) (Salton et al., 1975). Gooi and Allan (2004) found effective algorithmic extensions like agglomerative clustering. Successful feature extensions to the VSM for cross-document coreference have included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007). However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types. Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11The annotators were t</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>A. Bagga and B. Baldwin. 1998a. Algorithms for scoring coreference chains. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Entity-based cross-document coreferencing using the vector space model.</title>
<date>1998</date>
<booktitle>In COLING-ACL.</booktitle>
<contexts>
<context position="21026" citStr="Bagga and Baldwin, 1998" startWordPosition="3316" endWordPosition="3319">standard cross-document coreference resolution task, which appeared in ACE2008 (Strassel et al., 2008; NIST, 2008). We evaluate name (NAM) mentions for cross-lingual person (PER) and organization (ORG) entities. Neither the number nor the attributes of the entities are known (i.e., the task does not include a knowledge base). We report results for both gold and automatic within-document mention detection and coreference resolution. Evaluation Metrics We use entity-level evaluation metrics, i.e., we evaluate the E entity clusters rather than the mentions. For the gold setting, we report: • B3 (Bagga and Baldwin, 1998a): Precision and recall are computed from the intersection of the hypothesis and reference clusters. • CEAF (Luo, 2005): Precision and recall are computed from a maximum bipartite matching between hypothesis and reference clusters. • NVI (Reichart and Rappoport, 2009): Information-theoretic measure that utilizes the entropy of the clusters and their mutual information. Unlike the commonly-used Variation of Information (VI) metric, normalized VI (NVI) is not sensitive to the size of the data set. For the automatic setting, we must apply a different metric since the number of system chains may </context>
<context position="23573" citStr="Bagga and Baldwin (1998" startWordPosition="3706" endWordPosition="3709">entities.11 Because a similar corpus did not exist for development, we split the evaluation corpus into development and test sections. However, the usual method of splitting by document would not confine all mentions of each entity to one side of the split. We thus split the corpus by global entity id. We assigned one-third of the entities to development, and the remaining twothirds to test. 6 Comparison to Related Tasks and Work Our modeling techniques and task formulation can be viewed as cross-lingual extensions to cross-document coreference resolution. The classic work on this task was by Bagga and Baldwin (1998b), who adapted the Vector Space Model (VSM) (Salton et al., 1975). Gooi and Allan (2004) found effective algorithmic extensions like agglomerative clustering. Successful feature extensions to the VSM for cross-document coreference have included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007). However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types. Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11The annotators were t</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>A. Bagga and B. Baldwin. 1998b. Entity-based cross-document coreferencing using the vector space model. In COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Baron</author>
<author>M Freedman</author>
</authors>
<title>Who is Who and What is What: Experiments in cross-document co-reference.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="24870" citStr="Baron and Freedman (2008)" startWordPosition="3895" endWordPosition="3898">orrections, and corpus split are available at http://www.spencegreen.com/research/. 65 recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other than English is scarce. Wang (2005) used a combination of the VSM and heuristic feature selection strategies to cluster transliterated Chinese personal names. For Arabic, Magdy et al. (2007) started with the output of the mention detection and within-document coreference system of Florian et al. (2004). They clustered the entities incrementally using a binary classifier. Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name uniqueness. Finally, Sayeed et al. (2009) translated Arabic name mentions to English and then formed clusters greedily using pairwise matching. To our knowledge, the cross-lingual entity clustering task is novel. However, there is significant prior work on similar tasks: • Multilingual coreference resolution: Adapt English within-document coreference models to other languages (Harabagiu and Maiorano, 2000; Florian et al., 2004; Luo and Zitouni, 2005). • Name</context>
<context position="29676" citStr="Baron and Freedman, 2008" startWordPosition="4646" endWordPosition="4649">least two chains, so No-CoNTExT is a better baseline for cross-lingual clustering. Mono-lingual Entity Clustering For comparison, we also evaluated our system on a standard monolingual cross-document coreference task (Arabic and English) (Tbl. 5). We configured the system with HAC clustering and Jaro-Winkler (within-language) mention similarity. We built mono-lingual ELMs for context similarity. We used two baselines: • SINGLEToN: E = C, i.e., the cross-lingual clustering solution is just the set of mono-lingual coreference chains. This is a common baseline for mono-lingual entity clustering (Baron and Freedman, 2008). • No-CoNTExT: We run HAC with p = oc. Therefore, E is the set of fully-connected components in C subject to the pairwise constraints. For HAC, we manually tuned the stop threshold -y, the Jaro-Winkler threshold q, and the ELM smoothing parameter p on the development set. For the DPMM, no development tuning was necessary, and we evaluated a single sample of E taken after 3,000 iterations. To our knowledge, Baron and Freedman (2008) reported the only previous results on the ACE2008 data set. However, they only gave gold results for English, and clustered the entire evaluation corpus (test+deve</context>
</contexts>
<marker>Baron, Freedman, 2008</marker>
<rawString>A. Baron and M. Freedman. 2008. Who is Who and What is What: Experiments in cross-document co-reference. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>P Frazier</author>
</authors>
<title>Distance dependent Chinese restaurant processes.</title>
<date>2010</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="17854" citStr="Blei and Frazier, 2010" startWordPosition="2829" endWordPosition="2832"> G0) α — Gamma(1,1) where α is the concentration parameter of the DP prior and G0 is the base distribution with support V . For our experiments, we set G0 = Dir(7r1,... , 7rV ), where 7ri = PV (wi). For inference, we use the Gibbs sampler of Vlachos et al. (2009), which can incorporate pairwise constraints. The sampler is identical to a standard collapsed, token-based sampler, except the conditional probability p(Ea = E|E_a, Ca) = 0 if Ca cannot be merged with the chains in cluster E. This property makes the model non-exchangeable, but in practice non-exchangeable models are sometimes useful (Blei and Frazier, 2010). During sampling, we also learn α using the auxiliary variable procedure of West (1995), so the only fixed parameters are those of the vague Gamma prior. However, we found that these hyperparameters were not sensitive. 4 Training Data and Procedures We trained our system for Arabic-English crosslingual entity clustering.8 Maxent Mention Similarity The Maxent mention similarity model requires a parallel name list for training. Name pair lists can be obtained from the LDC (e.g., LDC2005T34 contains nearly 450,000 parallel Chinese-English names) or Wikipedia (Irvine et al., 2010). We extracted 1</context>
</contexts>
<marker>Blei, Frazier, 2010</marker>
<rawString>D. Blei and P. Frazier. 2010. Distance dependent Chinese restaurant processes. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cai</author>
<author>M Strube</author>
</authors>
<title>Evaluation metrics for end-toend coreference resolution systems.</title>
<date>2010</date>
<booktitle>In Proceedings of the SIGDIAL 2010 Conference.</booktitle>
<contexts>
<context position="21688" citStr="Cai and Strube, 2010" startWordPosition="3419" endWordPosition="3422">m the intersection of the hypothesis and reference clusters. • CEAF (Luo, 2005): Precision and recall are computed from a maximum bipartite matching between hypothesis and reference clusters. • NVI (Reichart and Rappoport, 2009): Information-theoretic measure that utilizes the entropy of the clusters and their mutual information. Unlike the commonly-used Variation of Information (VI) metric, normalized VI (NVI) is not sensitive to the size of the data set. For the automatic setting, we must apply a different metric since the number of system chains may differ from the reference. We use B3sys (Cai and Strube, 2010), a variant of B3 that was shown to penalize both twinless reference chains and spurious system chains more fairly. Evaluation Corpus The automatic evaluation of cross-lingual coreference systems requires annotated 10Mimno et al. (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly. Docs Tokens Entities Chains Mentions ARABIC 412 178,269 2,594 4,216 9,222 ENGLISH 414 246,309 2,278 3,950 9,140 Table 3: ACE2008 evaluation</context>
</contexts>
<marker>Cai, Strube, 2010</marker>
<rawString>J. Cai and M. Strube. 2010. Evaluation metrics for end-toend coreference resolution systems. In Proceedings of the SIGDIAL 2010 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cer</author>
<author>M Galley</author>
<author>D Jurafsky</author>
<author>C D Manning</author>
</authors>
<title>Phrasal: A statistical machine translation toolkit for exploring new model features. In HLT-NAACL, Demonstration Session.</title>
<date>2010</date>
<contexts>
<context position="11936" citStr="Cer et al., 2010" startWordPosition="1848" endWordPosition="1851">ities from other entities with similar references (e.g., “Steve Jones” or “Apple Corps”). As with the mention strings, the contexts may originate in different writing systems. We consider both highand low-resource approaches for mapping contexts to a common representation. 4The Hungarian algorithm finds an optimal minimum-cost alignment. For pairwise costs between tokens, we used the Levenshtein edit distance ¨ _+ a ¨�i g 62 Machine Translation (MT) For the high-resource setting, if lang(mi) =� English, then we translate both mi and its context si to English with an MT system. We use Phrasal (Cer et al., 2010), a phrase-based system which, like most public MT systems, lacks a transliteration module. We believe that this approach yields the most accurate context mapping for highresource language pairs (like English-Arabic). Polylingual Topic Model (PLTM) The polylingual topic model (PLTM) (Mimno et al., 2009) is a generative process in which document tuples— groups of topically-similar documents—share a topic distribution. The tuples need not be sentence-aligned, so training data is easier to obtain. For example, one document tuple might be the set of Wikipedia articles (in all languages) for Steve </context>
</contexts>
<marker>Cer, Galley, Jurafsky, Manning, 2010</marker>
<rawString>D. Cer, M. Galley, D. Jurafsky, and C. D. Manning. 2010. Phrasal: A statistical machine translation toolkit for exploring new model features. In HLT-NAACL, Demonstration Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Unsupervised learning of name structure from coreference data.</title>
<date>2001</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="2232" citStr="Charniak, 2001" startWordPosition="316" endWordPosition="317">d, in which electronic media played a prominent role. A key issue for the outside world was the aggregation of information that appeared simultaneously in English, French, and various Arabic dialects. To our knowledge, we are the first to consider clustering entity mentions across languages without a priori knowledge of the quantity or types of real-world entities (a knowledge base). The cross-lingual setting introduces several challenges. First, we cannot 60 assume a prototypical name format. For example, the Anglo-centric first/middle/last prototype used in previous name modeling work (cf. (Charniak, 2001)) does not apply to Arabic names like Abdullah ibn Abd Al-Aziz Al-Saud or Chinese names like Hu Jintao (referred to as Mr. Hu, not Mr. Jintao). Second, organization names often require both transliteration and translation. For example, the Arabic PPñ�KñÓ ÈQ��g. �é»Qå�...‘General Motors Corp’ contains � transliterations of „ñ�KñÓ ÈQ��g. ‘General Motors’, but a translation of �é»Qå�... ‘Corporation’. Our models are organized as a pipeline. First, for each document, we perform standard mention detection and coreference resolution. Then, we use pairwise cross-lingual similarity models to measure b</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>E. Charniak. 2001. Unsupervised learning of name structure from coreference data. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chen</author>
<author>J Martin</author>
</authors>
<title>Towards robust unsupervised personal name disambiguation.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<contexts>
<context position="23915" citStr="Chen and Martin, 2007" startWordPosition="3752" endWordPosition="3755">o development, and the remaining twothirds to test. 6 Comparison to Related Tasks and Work Our modeling techniques and task formulation can be viewed as cross-lingual extensions to cross-document coreference resolution. The classic work on this task was by Bagga and Baldwin (1998b), who adapted the Vector Space Model (VSM) (Salton et al., 1975). Gooi and Allan (2004) found effective algorithmic extensions like agglomerative clustering. Successful feature extensions to the VSM for cross-document coreference have included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007). However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types. Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. 65 recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other than English is scarce. Wang (2005) used a c</context>
</contexts>
<marker>Chen, Martin, 2007</marker>
<rawString>Y. Chen and J. Martin. 2007. Towards robust unsupervised personal name disambiguation. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Christen</author>
</authors>
<title>A comparison of personal name matching: Techniques and practical issues.</title>
<date>2006</date>
<tech>Technical Report TR-CS-06-02,</tech>
<institution>Australian National University.</institution>
<contexts>
<context position="9589" citStr="Christen (2006)" startWordPosition="1490" endWordPosition="1491">fferent writing systems. Edit distance calculations no longer apply directly. One solution would be full-blown transliteration (Knight and Graehl, 1998), followed by application of Jaro-Winkler. However, transliteration systems are complex and require significant training resources. We find that a simpler, low-resource approach works well in practice. First, we deterministically map both languages to a common phonetic representation (Tbl. 1).3 Next, we align the mention pairs with the Hungarian algorithm, 2For multi-token names, we sort the tokens prior to computing the score, as suggested by Christen (2006). 3This idea is reminiscent of Soundex, which Freeman et al. (2006) used for cross-lingual name matching. Overlap Active for each bigram in cbigrams(mi,u) U cbigrams(mj,v) Bigram-Diff-mi Active for each bigram in cbigrams(mi) − cbigrams(mj) Bigram-Diff-mj Active for each bigram in cbigrams(mj) − cbigrams(mi) Bigram-Len-Diff Value of abs(size(cbigrams(mi) − cbigrams(mj))) Big-Edit-Dist Count of token pairs with Lev(mi,u, mj,v) &gt; 3.0 Total-Edit-Dist Sum of aligned token edit distances Length Active for one of: len(mi) &gt; len(mj) or len(mi) &lt; len(mj) or len(mi) = len(mj) Length-Diff abs(len(mi) − </context>
</contexts>
<marker>Christen, 2006</marker>
<rawString>P. Christen. 2006. A comparison of personal name matching: Techniques and practical issues. Technical Report TR-CS-06-02, Australian National University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="25705" citStr="Collins and Singer, 1999" startWordPosition="4017" endWordPosition="4020"> to English and then formed clusters greedily using pairwise matching. To our knowledge, the cross-lingual entity clustering task is novel. However, there is significant prior work on similar tasks: • Multilingual coreference resolution: Adapt English within-document coreference models to other languages (Harabagiu and Maiorano, 2000; Florian et al., 2004; Luo and Zitouni, 2005). • Named entity translation: For a non-English document, produce an inventory of entities in English. An ACE2007 pilot task (Song and Strassel, 2008). • Named entity clustering: Assign semantic types to text mentions (Collins and Singer, 1999; Elsner et al., 2009). • Cross-language name search / entity linking: Match a single query name against a list of known multilingual names (knowledge base). A track in the 2011 NIST Text Analysis Conference (TAC-KBP) evaluation (Aktolga et al., 2008; McCarley, 2009; Udupa and Khapra, 2010; McNamee et al., 2011). Our work incorporates elements of the first three tasks. Most importantly, we avoid the key element of entity linking: a knowledge base. 7 Experiments We performed intrinsic evaluations for both mention and context similarity. For context similarity, we analyzed mono-lingual entity cl</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y. Singer. 1999. Unsupervised models for named entity classification. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G de Melo</author>
<author>G Weikum</author>
</authors>
<title>Untangling the cross-lingual link structure of Wikipedia.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<marker>de Melo, Weikum, 2010</marker>
<rawString>G. de Melo and G. Weikum. 2010. Untangling the cross-lingual link structure of Wikipedia. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elsner</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Structured generative models for unsupervised named-entity clustering.</title>
<date>2009</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="25727" citStr="Elsner et al., 2009" startWordPosition="4021" endWordPosition="4024">d clusters greedily using pairwise matching. To our knowledge, the cross-lingual entity clustering task is novel. However, there is significant prior work on similar tasks: • Multilingual coreference resolution: Adapt English within-document coreference models to other languages (Harabagiu and Maiorano, 2000; Florian et al., 2004; Luo and Zitouni, 2005). • Named entity translation: For a non-English document, produce an inventory of entities in English. An ACE2007 pilot task (Song and Strassel, 2008). • Named entity clustering: Assign semantic types to text mentions (Collins and Singer, 1999; Elsner et al., 2009). • Cross-language name search / entity linking: Match a single query name against a list of known multilingual names (knowledge base). A track in the 2011 NIST Text Analysis Conference (TAC-KBP) evaluation (Aktolga et al., 2008; McCarley, 2009; Udupa and Khapra, 2010; McNamee et al., 2011). Our work incorporates elements of the first three tasks. Most importantly, we avoid the key element of entity linking: a knowledge base. 7 Experiments We performed intrinsic evaluations for both mention and context similarity. For context similarity, we analyzed mono-lingual entity clustering, which also f</context>
</contexts>
<marker>Elsner, Charniak, Johnson, 2009</marker>
<rawString>M. Elsner, E. Charniak, and M. Johnson. 2009. Structured generative models for unsupervised named-entity clustering. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Endres</author>
<author>J E Schindelin</author>
</authors>
<title>A new metric for probability distributions.</title>
<date>2003</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>49</volume>
<issue>7</issue>
<pages>1860</pages>
<contexts>
<context position="16114" citStr="Endres and Schindelin, 2003" startWordPosition="2532" endWordPosition="2535">te a unigram Entity Language Model (ELM) (Raghavan et al., 2004): _ countW(w) + PPV (w) P(w) Ew, countW (w&apos;) + p PV (w) is the unigram probability in all contexts in the corpus7 and p is a smoothing parameter. For any 5Specification of a combined similarity measure is an interesting direction for future work. 6These constraints are similar to the pairf�lters of Mayfield et al. (2009). 7Recall that after context mapping, all languages have a common vocabulary V . 63 two entity clusters Ea and Eb, the distance between PE. and PEb is given by a metric based on the JensenShannon Divergence (JSD) (Endres and Schindelin, 2003): � dist(PEa, PEb) = 2 · JSD(PEa||PEb) � = KL(PEa||M) + KL(M||PEb) where KL(PE.||M) is the Kullback-Leibler divergence and M =1�(PE. + PEb). We initialize HAC to E = C, i.e., the initial clustering solution is just the set of all coreference chains. Then we remove all links in the HAC proximity matrix that violate pairwise cannot-link constraints. During clustering, we do not merge Ea and Eb if any pair of chains violates a cannot-link constraint. This procedure propagates the cannot-link constraints (Klein et al., 2002). To output E, we stop clustering when the minimum JSD exceeds a stop thre</context>
</contexts>
<marker>Endres, Schindelin, 2003</marker>
<rawString>D. M. Endres and J. E. Schindelin. 2003. A new metric for probability distributions. IEEE Transactions on Information Theory, 49(7):1858 – 1860.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fleischman</author>
<author>E Hovy</author>
</authors>
<title>Multi-document person name resolution.</title>
<date>2004</date>
<booktitle>In ACL Workshop on Reference Resolution and its Applications.</booktitle>
<contexts>
<context position="24057" citStr="Fleischman and Hovy (2004)" startWordPosition="3773" endWordPosition="3776">n be viewed as cross-lingual extensions to cross-document coreference resolution. The classic work on this task was by Bagga and Baldwin (1998b), who adapted the Vector Space Model (VSM) (Salton et al., 1975). Gooi and Allan (2004) found effective algorithmic extensions like agglomerative clustering. Successful feature extensions to the VSM for cross-document coreference have included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007). However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types. Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. 65 recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other than English is scarce. Wang (2005) used a combination of the VSM and heuristic feature selection strategies to cluster transliterated Chinese personal names. For Arabic, Magdy et al. (2</context>
</contexts>
<marker>Fleischman, Hovy, 2004</marker>
<rawString>M. Fleischman and E. Hovy. 2004. Multi-document person name resolution. In ACL Workshop on Reference Resolution and its Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>H Hassan</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
</authors>
<title>A statistical model for multilingual entity detection and tracking.</title>
<date>2004</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="24774" citStr="Florian et al. (2004)" startWordPosition="3882" endWordPosition="3885">he annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. 65 recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other than English is scarce. Wang (2005) used a combination of the VSM and heuristic feature selection strategies to cluster transliterated Chinese personal names. For Arabic, Magdy et al. (2007) started with the output of the mention detection and within-document coreference system of Florian et al. (2004). They clustered the entities incrementally using a binary classifier. Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name uniqueness. Finally, Sayeed et al. (2009) translated Arabic name mentions to English and then formed clusters greedily using pairwise matching. To our knowledge, the cross-lingual entity clustering task is novel. However, there is significant prior work on similar tasks: • Multilingual coreference resolution: Adapt English within-document coreference models to oth</context>
</contexts>
<marker>Florian, Hassan, Ittycheriah, Jing, Kambhatla, 2004</marker>
<rawString>R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, et al. 2004. A statistical model for multilingual entity detection and tracking. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A T Freeman</author>
<author>S L Condon</author>
<author>C M Ackerman</author>
</authors>
<title>Cross linguistic name matching in English and Arabic: a one to many mapping extension of the Levenshtein edit distance algorithm.</title>
<date>2006</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="9656" citStr="Freeman et al. (2006)" startWordPosition="1499" endWordPosition="1502"> apply directly. One solution would be full-blown transliteration (Knight and Graehl, 1998), followed by application of Jaro-Winkler. However, transliteration systems are complex and require significant training resources. We find that a simpler, low-resource approach works well in practice. First, we deterministically map both languages to a common phonetic representation (Tbl. 1).3 Next, we align the mention pairs with the Hungarian algorithm, 2For multi-token names, we sort the tokens prior to computing the score, as suggested by Christen (2006). 3This idea is reminiscent of Soundex, which Freeman et al. (2006) used for cross-lingual name matching. Overlap Active for each bigram in cbigrams(mi,u) U cbigrams(mj,v) Bigram-Diff-mi Active for each bigram in cbigrams(mi) − cbigrams(mj) Bigram-Diff-mj Active for each bigram in cbigrams(mj) − cbigrams(mi) Bigram-Len-Diff Value of abs(size(cbigrams(mi) − cbigrams(mj))) Big-Edit-Dist Count of token pairs with Lev(mi,u, mj,v) &gt; 3.0 Total-Edit-Dist Sum of aligned token edit distances Length Active for one of: len(mi) &gt; len(mj) or len(mi) &lt; len(mj) or len(mi) = len(mj) Length-Diff abs(len(mi) − len(mj)) Singleton Active if len(mi) = 1 Singleton-Pair Active if l</context>
</contexts>
<marker>Freeman, Condon, Ackerman, 2006</marker>
<rawString>A. T. Freeman, S. L. Condon, and C. M. Ackerman. 2006. Cross linguistic name matching in English and Arabic: a one to many mapping extension of the Levenshtein edit distance algorithm. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>C D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="19314" citStr="Galley and Manning (2008)" startWordPosition="3060" endWordPosition="3063">xamples by running a Bernoulli trial for each aligned name pair in the corpus. If the coin was heads, we replaced the English name with another English name chosen randomly from the corpus. MT Context Mapping For the MT context mapping method, we trained Phrasal with all data permitted under the NIST OpenMT Ar-En 2009 constrained track evaluation. We built a 5-gram language model from the Xinhua and AFP sections of the Gigaword corpus (LDC2007T07), in addition to all of the target side training data. In addition to the baseline Phrasal feature set, we used the lexicalized re-ordering model of Galley and Manning (2008). PLTM Context Mapping For PLTM training, we formed a corpus of 19,139 English-Arabic topicallyaligned Wikipedia articles. Cross-lingual links in Wikipedia are abundant: as of February 2010, there were 77.07M cross-lingual links among Wikipedia�s 272 language editions (de Melo and Weikum, 2010). To increase vocabulary coverage for our ACE2008 evaluation corpus, we added 20,000 document singletons from the ACE2008 training corpus. The 8We tokenized all English documents with packages from the Stanford parser (Klein and Manning, 2003). For Arabic documents, we used Mada (Habash and Rambow, 2005)</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>M. Galley and C. D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Gooi</author>
<author>J Allan</author>
</authors>
<title>Cross-document coreference on a large scale corpus.</title>
<date>2004</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="23662" citStr="Gooi and Allan (2004)" startWordPosition="3721" endWordPosition="3724"> corpus into development and test sections. However, the usual method of splitting by document would not confine all mentions of each entity to one side of the split. We thus split the corpus by global entity id. We assigned one-third of the entities to development, and the remaining twothirds to test. 6 Comparison to Related Tasks and Work Our modeling techniques and task formulation can be viewed as cross-lingual extensions to cross-document coreference resolution. The classic work on this task was by Bagga and Baldwin (1998b), who adapted the Vector Space Model (VSM) (Salton et al., 1975). Gooi and Allan (2004) found effective algorithmic extensions like agglomerative clustering. Successful feature extensions to the VSM for cross-document coreference have included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007). However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types. Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and c</context>
</contexts>
<marker>Gooi, Allan, 2004</marker>
<rawString>C. H. Gooi and J. Allan. 2004. Cross-document coreference on a large scale corpus. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>O Rambow</author>
</authors>
<title>Arabic tokenization, part-ofspeech tagging and morphological disambiguation in one fell swoop.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="19914" citStr="Habash and Rambow, 2005" startWordPosition="3148" endWordPosition="3151">alley and Manning (2008). PLTM Context Mapping For PLTM training, we formed a corpus of 19,139 English-Arabic topicallyaligned Wikipedia articles. Cross-lingual links in Wikipedia are abundant: as of February 2010, there were 77.07M cross-lingual links among Wikipedia�s 272 language editions (de Melo and Weikum, 2010). To increase vocabulary coverage for our ACE2008 evaluation corpus, we added 20,000 document singletons from the ACE2008 training corpus. The 8We tokenized all English documents with packages from the Stanford parser (Klein and Manning, 2003). For Arabic documents, we used Mada (Habash and Rambow, 2005) for orthographic normalization and clitic segmentation. 9LDC Catalog numbers LDC2009E82 and LDC2009E88. 64 topically-aligned tuples served as “glue” to share topics between languages, while the ACE documents distribute those topics over in-domain vocabulary.10 We used the PLTM implementation in Mallet (McCallum, 2002). We ran the sampler for 10,000 iterations and set the number of topics K = 512. 5 Task Evaluation Framework Our experimental design is a cross-lingual extension of the standard cross-document coreference resolution task, which appeared in ACE2008 (Strassel et al., 2008; NIST, 20</context>
</contexts>
<marker>Habash, Rambow, 2005</marker>
<rawString>N. Habash and O. Rambow. 2005. Arabic tokenization, part-ofspeech tagging and morphological disambiguation in one fell swoop. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Harabagiu</author>
<author>S J Maiorano</author>
</authors>
<title>Multilingual coreference resolution.</title>
<date>2000</date>
<booktitle>In ANLP.</booktitle>
<contexts>
<context position="25416" citStr="Harabagiu and Maiorano, 2000" startWordPosition="3972" endWordPosition="3975">ed the entities incrementally using a binary classifier. Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name uniqueness. Finally, Sayeed et al. (2009) translated Arabic name mentions to English and then formed clusters greedily using pairwise matching. To our knowledge, the cross-lingual entity clustering task is novel. However, there is significant prior work on similar tasks: • Multilingual coreference resolution: Adapt English within-document coreference models to other languages (Harabagiu and Maiorano, 2000; Florian et al., 2004; Luo and Zitouni, 2005). • Named entity translation: For a non-English document, produce an inventory of entities in English. An ACE2007 pilot task (Song and Strassel, 2008). • Named entity clustering: Assign semantic types to text mentions (Collins and Singer, 1999; Elsner et al., 2009). • Cross-language name search / entity linking: Match a single query name against a list of known multilingual names (knowledge base). A track in the 2011 NIST Text Analysis Conference (TAC-KBP) evaluation (Aktolga et al., 2008; McCarley, 2009; Udupa and Khapra, 2010; McNamee et al., 201</context>
</contexts>
<marker>Harabagiu, Maiorano, 2000</marker>
<rawString>S. M. Harabagiu and S. J. Maiorano. 2000. Multilingual coreference resolution. In ANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Irvine</author>
<author>C Callison-Burch</author>
<author>A Klementiev</author>
</authors>
<title>Transliterating from all languages.</title>
<date>2010</date>
<booktitle>In AMTA.</booktitle>
<contexts>
<context position="18438" citStr="Irvine et al., 2010" startWordPosition="2919" endWordPosition="2922">imes useful (Blei and Frazier, 2010). During sampling, we also learn α using the auxiliary variable procedure of West (1995), so the only fixed parameters are those of the vague Gamma prior. However, we found that these hyperparameters were not sensitive. 4 Training Data and Procedures We trained our system for Arabic-English crosslingual entity clustering.8 Maxent Mention Similarity The Maxent mention similarity model requires a parallel name list for training. Name pair lists can be obtained from the LDC (e.g., LDC2005T34 contains nearly 450,000 parallel Chinese-English names) or Wikipedia (Irvine et al., 2010). We extracted 12,860 name pairs from the parallel Arabic-English translation treebanks,9 although our experiments show that the model achieves high accuracy with significantly fewer training examples. We generated a uniform distribution of training examples by running a Bernoulli trial for each aligned name pair in the corpus. If the coin was heads, we replaced the English name with another English name chosen randomly from the corpus. MT Context Mapping For the MT context mapping method, we trained Phrasal with all data permitted under the NIST OpenMT Ar-En 2009 constrained track evaluation.</context>
</contexts>
<marker>Irvine, Callison-Burch, Klementiev, 2010</marker>
<rawString>A. Irvine, C. Callison-Burch, and A. Klementiev. 2010. Transliterating from all languages. In AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="19852" citStr="Klein and Manning, 2003" startWordPosition="3138" endWordPosition="3141">al feature set, we used the lexicalized re-ordering model of Galley and Manning (2008). PLTM Context Mapping For PLTM training, we formed a corpus of 19,139 English-Arabic topicallyaligned Wikipedia articles. Cross-lingual links in Wikipedia are abundant: as of February 2010, there were 77.07M cross-lingual links among Wikipedia�s 272 language editions (de Melo and Weikum, 2010). To increase vocabulary coverage for our ACE2008 evaluation corpus, we added 20,000 document singletons from the ACE2008 training corpus. The 8We tokenized all English documents with packages from the Stanford parser (Klein and Manning, 2003). For Arabic documents, we used Mada (Habash and Rambow, 2005) for orthographic normalization and clitic segmentation. 9LDC Catalog numbers LDC2009E82 and LDC2009E88. 64 topically-aligned tuples served as “glue” to share topics between languages, while the ACE documents distribute those topics over in-domain vocabulary.10 We used the PLTM implementation in Mallet (McCallum, 2002). We ran the sampler for 10,000 iterations and set the number of topics K = 512. 5 Task Evaluation Framework Our experimental design is a cross-lingual extension of the standard cross-document coreference resolution ta</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Accurate unlexicalized parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>S D Kamvar</author>
<author>C D Manning</author>
</authors>
<title>From instancelevel constraints to space-level constraints: Making the most of prior knowledge in data clustering.</title>
<date>2002</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="16640" citStr="Klein et al., 2002" startWordPosition="2621" endWordPosition="2624">s given by a metric based on the JensenShannon Divergence (JSD) (Endres and Schindelin, 2003): � dist(PEa, PEb) = 2 · JSD(PEa||PEb) � = KL(PEa||M) + KL(M||PEb) where KL(PE.||M) is the Kullback-Leibler divergence and M =1�(PE. + PEb). We initialize HAC to E = C, i.e., the initial clustering solution is just the set of all coreference chains. Then we remove all links in the HAC proximity matrix that violate pairwise cannot-link constraints. During clustering, we do not merge Ea and Eb if any pair of chains violates a cannot-link constraint. This procedure propagates the cannot-link constraints (Klein et al., 2002). To output E, we stop clustering when the minimum JSD exceeds a stop threshold &apos;y, which is tuned on a development set. 3.2 Constrained Dirichlet Process Mixture Model (DPMM) Instead of tuning a parameter like -y, it would be preferable to let the data dictate the number of entity clusters. We thus consider a non-parametric Bayesian mixture model where the mixtures are multinomial distributions over the entity contexts S. Specifically, we consider a DPMM, which automatically infers the number of mixtures. Each Ca has an associated mixture Ba: Ca|Ba — Mult(Ba) Ba|G — G G|α, G0 — DP(α, G0) α — </context>
</contexts>
<marker>Klein, Kamvar, Manning, 2002</marker>
<rawString>D. Klein, S. D. Kamvar, and C. D. Manning. 2002. From instancelevel constraints to space-level constraints: Making the most of prior knowledge in data clustering. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>J Graehl</author>
</authors>
<date>1998</date>
<booktitle>Machine transliteration. Computational Linguistics,</booktitle>
<pages>24--599</pages>
<contexts>
<context position="9126" citStr="Knight and Graehl, 1998" startWordPosition="1421" endWordPosition="1424"> lang(mj) maxent(mi,mj) otherwise Jaro-Winkler Distance (within-language) If lang(mi) = lang(mj), we use the Jaro-Winkler edit distance (Porter and Winkler, 1997). Jaro-Winkler rewards matching prefixes, the empirical justification being that less variation typically occurs at the beginning of names.2 The metric produces a score in the range [0,1], where 0 indicates equality. Maxent model (cross-language) When lang(mi) =� lang(mj), then the two mentions might be in different writing systems. Edit distance calculations no longer apply directly. One solution would be full-blown transliteration (Knight and Graehl, 1998), followed by application of Jaro-Winkler. However, transliteration systems are complex and require significant training resources. We find that a simpler, low-resource approach works well in practice. First, we deterministically map both languages to a common phonetic representation (Tbl. 1).3 Next, we align the mention pairs with the Hungarian algorithm, 2For multi-token names, we sort the tokens prior to computing the score, as suggested by Christen (2006). 3This idea is reminiscent of Soundex, which Freeman et al. (2006) used for cross-lingual name matching. Overlap Active for each bigram </context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>K. Knight and J. Graehl. 1998. Machine transliteration. Computational Linguistics, 24:599–612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
<author>I Zitouni</author>
</authors>
<title>Multi-lingual coreference resolution with syntactic features.</title>
<date>2005</date>
<booktitle>In HLT-EMNLP.</booktitle>
<contexts>
<context position="25462" citStr="Luo and Zitouni, 2005" startWordPosition="3980" endWordPosition="3983">ier. Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name uniqueness. Finally, Sayeed et al. (2009) translated Arabic name mentions to English and then formed clusters greedily using pairwise matching. To our knowledge, the cross-lingual entity clustering task is novel. However, there is significant prior work on similar tasks: • Multilingual coreference resolution: Adapt English within-document coreference models to other languages (Harabagiu and Maiorano, 2000; Florian et al., 2004; Luo and Zitouni, 2005). • Named entity translation: For a non-English document, produce an inventory of entities in English. An ACE2007 pilot task (Song and Strassel, 2008). • Named entity clustering: Assign semantic types to text mentions (Collins and Singer, 1999; Elsner et al., 2009). • Cross-language name search / entity linking: Match a single query name against a list of known multilingual names (knowledge base). A track in the 2011 NIST Text Analysis Conference (TAC-KBP) evaluation (Aktolga et al., 2008; McCarley, 2009; Udupa and Khapra, 2010; McNamee et al., 2011). Our work incorporates elements of the firs</context>
</contexts>
<marker>Luo, Zitouni, 2005</marker>
<rawString>X. Luo and I. Zitouni. 2005. Multi-lingual coreference resolution with syntactic features. In HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In HLT-EMNLP.</booktitle>
<contexts>
<context position="21146" citStr="Luo, 2005" startWordPosition="3336" endWordPosition="3337">(NAM) mentions for cross-lingual person (PER) and organization (ORG) entities. Neither the number nor the attributes of the entities are known (i.e., the task does not include a knowledge base). We report results for both gold and automatic within-document mention detection and coreference resolution. Evaluation Metrics We use entity-level evaluation metrics, i.e., we evaluate the E entity clusters rather than the mentions. For the gold setting, we report: • B3 (Bagga and Baldwin, 1998a): Precision and recall are computed from the intersection of the hypothesis and reference clusters. • CEAF (Luo, 2005): Precision and recall are computed from a maximum bipartite matching between hypothesis and reference clusters. • NVI (Reichart and Rappoport, 2009): Information-theoretic measure that utilizes the entropy of the clusters and their mutual information. Unlike the commonly-used Variation of Information (VI) metric, normalized VI (NVI) is not sensitive to the size of the data set. For the automatic setting, we must apply a different metric since the number of system chains may differ from the reference. We use B3sys (Cai and Strube, 2010), a variant of B3 that was shown to penalize both twinless</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>X. Luo. 2005. On coreference resolution performance metrics. In HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Magdy</author>
<author>K Darwish</author>
<author>O Emam</author>
<author>H Hassan</author>
</authors>
<title>Arabic cross-document person name normalization.</title>
<date>2007</date>
<booktitle>In Workshop on Computational Approaches to Semitic Languages.</booktitle>
<contexts>
<context position="24661" citStr="Magdy et al. (2007)" startWordPosition="3864" endWordPosition="3867">and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. 65 recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other than English is scarce. Wang (2005) used a combination of the VSM and heuristic feature selection strategies to cluster transliterated Chinese personal names. For Arabic, Magdy et al. (2007) started with the output of the mention detection and within-document coreference system of Florian et al. (2004). They clustered the entities incrementally using a binary classifier. Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name uniqueness. Finally, Sayeed et al. (2009) translated Arabic name mentions to English and then formed clusters greedily using pairwise matching. To our knowledge, the cross-lingual entity clustering task is novel. However, there is significant prior work</context>
</contexts>
<marker>Magdy, Darwish, Emam, Hassan, 2007</marker>
<rawString>W. Magdy, K. Darwish, O. Emam, and H. Hassan. 2007. Arabic cross-document person name normalization. In Workshop on Computational Approaches to Semitic Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G S Mann</author>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised personal name disambiguation.</title>
<date>2003</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="23869" citStr="Mann and Yarowsky, 2003" startWordPosition="3745" endWordPosition="3748">tity id. We assigned one-third of the entities to development, and the remaining twothirds to test. 6 Comparison to Related Tasks and Work Our modeling techniques and task formulation can be viewed as cross-lingual extensions to cross-document coreference resolution. The classic work on this task was by Bagga and Baldwin (1998b), who adapted the Vector Space Model (VSM) (Salton et al., 1975). Gooi and Allan (2004) found effective algorithmic extensions like agglomerative clustering. Successful feature extensions to the VSM for cross-document coreference have included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007). However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types. Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. 65 recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages othe</context>
</contexts>
<marker>Mann, Yarowsky, 2003</marker>
<rawString>G. S. Mann and D. Yarowsky. 2003. Unsupervised personal name disambiguation. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>P Raghavan</author>
<author>H Schütze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="13751" citStr="Manning et al., 2008" startWordPosition="2141" endWordPosition="2144">ces V = k for all l. Moreover, all languages have a common vocabulary: the set of K topic indices. Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al., 2009) for more details. After mapping each mention context to a common representation, we measure context similarity based on the choice of clustering algorithm. 3 Clustering Algorithms We incorporate the mention and context similarity measures into a clustering framework. We consider two algorithms. The first is hierarchical agglomerative clustering (HAC), with which we assume basic familiarity (Manning et al., 2008). A shortcoming of HAC is that a stop threshold must be tuned. To avoid this requirement, we also consider non-parametric probabilistic clustering in the form of a Dirichlet process mixture model (DPMM) (Antoniak, 1974) . Both clustering algorithms can be modified to accommodate pairwise constraints. We have observed better results by encoding mention similarity as a hard constraint. Context similarity is thus the cluster distance measure.5 To turn the Jaro-Winkler distance into a hard boolean constraint, we tuned a threshold q on held-out data, i.e., jaro-winkler(mi, mj) G q =&gt;. mi = mj. Like</context>
</contexts>
<marker>Manning, Raghavan, Schütze, 2008</marker>
<rawString>C. D. Manning, P. Raghavan, and H. Schütze. 2008. Introduction to Information Retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mayfield</author>
<author>D Alexander</author>
<author>B Dorr</author>
<author>J Eisner</author>
<author>T Elsayed</author>
</authors>
<title>Cross-document coreference resolution: A key technology for learning by reading.</title>
<date>2009</date>
<booktitle>In AAAI Spring Symposium on Learning by Reading and Learning to Read.</booktitle>
<contexts>
<context position="15872" citStr="Mayfield et al. (2009)" startWordPosition="2489" endWordPosition="2492">ring HAC iteratively merges the “nearest” clusters according to context similarity. In our system, each cluster context is a bag of words W formed from the contexts of all coreference chains in that cluster. For each word in W we estimate a unigram Entity Language Model (ELM) (Raghavan et al., 2004): _ countW(w) + PPV (w) P(w) Ew, countW (w&apos;) + p PV (w) is the unigram probability in all contexts in the corpus7 and p is a smoothing parameter. For any 5Specification of a combined similarity measure is an interesting direction for future work. 6These constraints are similar to the pairf�lters of Mayfield et al. (2009). 7Recall that after context mapping, all languages have a common vocabulary V . 63 two entity clusters Ea and Eb, the distance between PE. and PEb is given by a metric based on the JensenShannon Divergence (JSD) (Endres and Schindelin, 2003): � dist(PEa, PEb) = 2 · JSD(PEa||PEb) � = KL(PEa||M) + KL(M||PEb) where KL(PE.||M) is the Kullback-Leibler divergence and M =1�(PE. + PEb). We initialize HAC to E = C, i.e., the initial clustering solution is just the set of all coreference chains. Then we remove all links in the HAC proximity matrix that violate pairwise cannot-link constraints. During c</context>
</contexts>
<marker>Mayfield, Alexander, Dorr, Eisner, Elsayed, 2009</marker>
<rawString>J. Mayfield, D. Alexander, B. Dorr, J. Eisner, T. Elsayed, et al. 2009. Cross-document coreference resolution: A key technology for learning by reading. In AAAI Spring Symposium on Learning by Reading and Learning to Read.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K McCallum</author>
</authors>
<title>MALLET: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="20234" citStr="McCallum, 2002" startWordPosition="3193" endWordPosition="3195">ase vocabulary coverage for our ACE2008 evaluation corpus, we added 20,000 document singletons from the ACE2008 training corpus. The 8We tokenized all English documents with packages from the Stanford parser (Klein and Manning, 2003). For Arabic documents, we used Mada (Habash and Rambow, 2005) for orthographic normalization and clitic segmentation. 9LDC Catalog numbers LDC2009E82 and LDC2009E88. 64 topically-aligned tuples served as “glue” to share topics between languages, while the ACE documents distribute those topics over in-domain vocabulary.10 We used the PLTM implementation in Mallet (McCallum, 2002). We ran the sampler for 10,000 iterations and set the number of topics K = 512. 5 Task Evaluation Framework Our experimental design is a cross-lingual extension of the standard cross-document coreference resolution task, which appeared in ACE2008 (Strassel et al., 2008; NIST, 2008). We evaluate name (NAM) mentions for cross-lingual person (PER) and organization (ORG) entities. Neither the number nor the attributes of the entities are known (i.e., the task does not include a knowledge base). We report results for both gold and automatic within-document mention detection and coreference resolut</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>A. K. McCallum. 2002. MALLET: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S McCarley</author>
</authors>
<title>Cross language name matching. In</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="25971" citStr="McCarley, 2009" startWordPosition="4061" endWordPosition="4062">nce models to other languages (Harabagiu and Maiorano, 2000; Florian et al., 2004; Luo and Zitouni, 2005). • Named entity translation: For a non-English document, produce an inventory of entities in English. An ACE2007 pilot task (Song and Strassel, 2008). • Named entity clustering: Assign semantic types to text mentions (Collins and Singer, 1999; Elsner et al., 2009). • Cross-language name search / entity linking: Match a single query name against a list of known multilingual names (knowledge base). A track in the 2011 NIST Text Analysis Conference (TAC-KBP) evaluation (Aktolga et al., 2008; McCarley, 2009; Udupa and Khapra, 2010; McNamee et al., 2011). Our work incorporates elements of the first three tasks. Most importantly, we avoid the key element of entity linking: a knowledge base. 7 Experiments We performed intrinsic evaluations for both mention and context similarity. For context similarity, we analyzed mono-lingual entity clustering, which also facilitated comparison to prior work on the ACE2008 Genre #Train #Test Accuracy(%) wb 125 16 87.5 bn 2,720 340 95.6 nw 7,443 930 96.6 all 10,288 1,286 97.1 (+7.55) Table 4: Cross-lingual mention matching accuracy [%]. The training data contains </context>
</contexts>
<marker>McCarley, 2009</marker>
<rawString>J. S. McCarley. 2009. Cross language name matching. In SIGIR. P. McNamee, J. Mayfield, D. Lawrie, D.W. Oard, and D. Doermann. 2011. Cross-language entity linking. In IJCNLP. D. Mimno, H. M. Wallach, J. Naradowsky, D. A. Smith, and A. McCallum. 2009. Polylingual topic models. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>Automatic Content Extraction</title>
<date>2008</date>
<booktitle>National Institute of Standards and Technology (NIST), 8</booktitle>
<tech>Technical Report rev. 1.2d,</tech>
<contexts>
<context position="20517" citStr="NIST, 2008" startWordPosition="3240" endWordPosition="3241">w, 2005) for orthographic normalization and clitic segmentation. 9LDC Catalog numbers LDC2009E82 and LDC2009E88. 64 topically-aligned tuples served as “glue” to share topics between languages, while the ACE documents distribute those topics over in-domain vocabulary.10 We used the PLTM implementation in Mallet (McCallum, 2002). We ran the sampler for 10,000 iterations and set the number of topics K = 512. 5 Task Evaluation Framework Our experimental design is a cross-lingual extension of the standard cross-document coreference resolution task, which appeared in ACE2008 (Strassel et al., 2008; NIST, 2008). We evaluate name (NAM) mentions for cross-lingual person (PER) and organization (ORG) entities. Neither the number nor the attributes of the entities are known (i.e., the task does not include a knowledge base). We report results for both gold and automatic within-document mention detection and coreference resolution. Evaluation Metrics We use entity-level evaluation metrics, i.e., we evaluate the E entity clusters rather than the mentions. For the gold setting, we report: • B3 (Bagga and Baldwin, 1998a): Precision and recall are computed from the intersection of the hypothesis and reference</context>
</contexts>
<marker>NIST, 2008</marker>
<rawString>NIST. 2008. Automatic Content Extraction 2008 evaluation plan (ACE2008): Assessment of detection and recognition of entities and relations within and across documents. Technical Report rev. 1.2d, National Institute of Standards and Technology (NIST), 8 August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Porter</author>
<author>W E Winkler</author>
</authors>
<title>Approximate String Comparison and its Effect on an Advanced Record Linkage System, chapter 6,</title>
<date>1997</date>
<journal>U.S. Bureau of the Census.</journal>
<pages>190--199</pages>
<contexts>
<context position="8664" citStr="Porter and Winkler, 1997" startWordPosition="1355" endWordPosition="1358"> 1/4 � k È --+ l Ð m à�i n è o h @ i a ð � w ø i a o ah ø_ � 0 Z � 0 English Rules k � c ppb x � ks e,i,o,u-0 Table 1: English-Arabic mapping rules to a common orthographic representation. “0” indicates a null mapping. For English, we also lowercase and remove determiners and punctuation. For Arabic, we remove the determiner È@ Al `the&apos; and the elongation character tatwil `_&apos;. of any two mentions mi and mj is: sim(mi,mj) = � faro-winkler(mi, mj) if lang(mi) = lang(mj) maxent(mi,mj) otherwise Jaro-Winkler Distance (within-language) If lang(mi) = lang(mj), we use the Jaro-Winkler edit distance (Porter and Winkler, 1997). Jaro-Winkler rewards matching prefixes, the empirical justification being that less variation typically occurs at the beginning of names.2 The metric produces a score in the range [0,1], where 0 indicates equality. Maxent model (cross-language) When lang(mi) =� lang(mj), then the two mentions might be in different writing systems. Edit distance calculations no longer apply directly. One solution would be full-blown transliteration (Knight and Graehl, 1998), followed by application of Jaro-Winkler. However, transliteration systems are complex and require significant training resources. We fin</context>
</contexts>
<marker>Porter, Winkler, 1997</marker>
<rawString>E. H. Porter and W. E. Winkler, 1997. Approximate String Comparison and its Effect on an Advanced Record Linkage System, chapter 6, pages 190–199. U.S. Bureau of the Census.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Raghavan</author>
<author>J Allan</author>
<author>A McCallum</author>
</authors>
<title>An exploration of entity models, collective classification and relation description.</title>
<date>2004</date>
<booktitle>In KDD Workshop on Link Analysis and Group Detection.</booktitle>
<contexts>
<context position="15550" citStr="Raghavan et al., 2004" startWordPosition="2432" endWordPosition="2435">es, the first mention is typically more complete than later mentions. This heuristic also makes our system less sensitive to withindocument coreference errors.6 The representative mention only has special status for mention similarity: context similarity considers all mention contexts. 3.1 Constrained Hierarchical Clustering HAC iteratively merges the “nearest” clusters according to context similarity. In our system, each cluster context is a bag of words W formed from the contexts of all coreference chains in that cluster. For each word in W we estimate a unigram Entity Language Model (ELM) (Raghavan et al., 2004): _ countW(w) + PPV (w) P(w) Ew, countW (w&apos;) + p PV (w) is the unigram probability in all contexts in the corpus7 and p is a smoothing parameter. For any 5Specification of a combined similarity measure is an interesting direction for future work. 6These constraints are similar to the pairf�lters of Mayfield et al. (2009). 7Recall that after context mapping, all languages have a common vocabulary V . 63 two entity clusters Ea and Eb, the distance between PE. and PEb is given by a metric based on the JensenShannon Divergence (JSD) (Endres and Schindelin, 2003): � dist(PEa, PEb) = 2 · JSD(PEa||PE</context>
</contexts>
<marker>Raghavan, Allan, McCallum, 2004</marker>
<rawString>H. Raghavan, J. Allan, and A. McCallum. 2004. An exploration of entity models, collective classification and relation description. In KDD Workshop on Link Analysis and Group Detection.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ramshaw</author>
<author>E Boschee</author>
<author>M Freedman</author>
<author>J MacBride</author>
<author>R Weischedel</author>
<author>A Zamanian</author>
</authors>
<title>SERIF language processing—effective trainable language understanding.</title>
<date>2011</date>
<booktitle>Handbook ofNatural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation,</booktitle>
<pages>636--644</pages>
<editor>In J. Olive et al., editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="5569" citStr="Ramshaw et al., 2011" startWordPosition="817" endWordPosition="820">E2 (Apple Inc.), and E3 (Steven Paul Jobs). Both the true number and characteristics of these entities are unobserved. Our models require two pre-processing steps: mention detection and within-document coreference/anaphora resolution, shown in Fig. 1 by the text boxes and intra-document links, respectively. For example, in doc3, a within-document coreference system would pre-linker joobz ‘Jobs’ with the masculine pronoun o h `his&apos;. In addition, the mention detector determines that the surface form “Jobs” in doc2 is not an entity reference. For this within-document pre-processing we use Serif (Ramshaw et al., 2011).1 Our models measure cross-lingual similarity of the coreference chains to make clustering decisions (• in Fig. 1). The similarity models (indicated by the = and =6 operators in Fig. 1) consider both mention string and context similarity (§2). We use the mention similarities as hard constraints, and the context similarities as soft constraints. In this work, we investigate two standard constrained clustering algorithms (§3). Our methods can be used to extend existing systems for mono-lingual entity clustering (also known as “cross-document coreference resolution”) to the cross-lingual setting</context>
</contexts>
<marker>Ramshaw, Boschee, Freedman, MacBride, Weischedel, Zamanian, 2011</marker>
<rawString>L. Ramshaw, E. Boschee, M. Freedman, J. MacBride, R. Weischedel, and A. Zamanian. 2011. SERIF language processing—effective trainable language understanding. In J. Olive et al., editors, Handbook ofNatural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation, pages 636–644. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Rao</author>
<author>P McNamee</author>
<author>M Dredze</author>
</authors>
<title>Streaming cross document entity coreference resolution. In</title>
<date>2010</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="24409" citStr="Rao et al., 2010" startWordPosition="3825" endWordPosition="3828">coreference have included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007). However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types. Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. 65 recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other than English is scarce. Wang (2005) used a combination of the VSM and heuristic feature selection strategies to cluster transliterated Chinese personal names. For Arabic, Magdy et al. (2007) started with the output of the mention detection and within-document coreference system of Florian et al. (2004). They clustered the entities incrementally using a binary classifier. Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name u</context>
</contexts>
<marker>Rao, McNamee, Dredze, 2010</marker>
<rawString>D. Rao, P. McNamee, and M. Dredze. 2010. Streaming cross document entity coreference resolution. In COLING. R. Reichart and A. Rappoport. 2009. The NVI clustering evaluation measure. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>C S Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>CACM,</journal>
<pages>18--613</pages>
<contexts>
<context position="23639" citStr="Salton et al., 1975" startWordPosition="3717" endWordPosition="3720">e split the evaluation corpus into development and test sections. However, the usual method of splitting by document would not confine all mentions of each entity to one side of the split. We thus split the corpus by global entity id. We assigned one-third of the entities to development, and the remaining twothirds to test. 6 Comparison to Related Tasks and Work Our modeling techniques and task formulation can be viewed as cross-lingual extensions to cross-document coreference resolution. The classic work on this task was by Bagga and Baldwin (1998b), who adapted the Vector Space Model (VSM) (Salton et al., 1975). Gooi and Allan (2004) found effective algorithmic extensions like agglomerative clustering. Successful feature extensions to the VSM for cross-document coreference have included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007). However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types. Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11The annotators were the first author and another fluent speaker of Arabic. The annotati</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>G. Salton, A. Wong, and C. S. Yang. 1975. A vector space model for automatic indexing. CACM, 18:613–620, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sayeed</author>
<author>T Elsayed</author>
<author>N Garera</author>
<author>D Alexander</author>
<author>T Xu</author>
</authors>
<title>Arabic cross-document coreference detection.</title>
<date>2009</date>
<booktitle>In ACLIJCNLP, Short Papers.</booktitle>
<contexts>
<context position="25049" citStr="Sayeed et al. (2009)" startWordPosition="3922" endWordPosition="3925">). Cross-document work on languages other than English is scarce. Wang (2005) used a combination of the VSM and heuristic feature selection strategies to cluster transliterated Chinese personal names. For Arabic, Magdy et al. (2007) started with the output of the mention detection and within-document coreference system of Florian et al. (2004). They clustered the entities incrementally using a binary classifier. Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name uniqueness. Finally, Sayeed et al. (2009) translated Arabic name mentions to English and then formed clusters greedily using pairwise matching. To our knowledge, the cross-lingual entity clustering task is novel. However, there is significant prior work on similar tasks: • Multilingual coreference resolution: Adapt English within-document coreference models to other languages (Harabagiu and Maiorano, 2000; Florian et al., 2004; Luo and Zitouni, 2005). • Named entity translation: For a non-English document, produce an inventory of entities in English. An ACE2007 pilot task (Song and Strassel, 2008). • Named entity clustering: Assign s</context>
</contexts>
<marker>Sayeed, Elsayed, Garera, Alexander, Xu, 2009</marker>
<rawString>A. Sayeed, T. Elsayed, N. Garera, D. Alexander, T. Xu, et al. 2009. Arabic cross-document coreference detection. In ACLIJCNLP, Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Singh</author>
<author>A Subramanya</author>
<author>F Pereira</author>
<author>A McCallum</author>
</authors>
<title>Large-scale cross-document coreference using distributed inference and hierarchical models.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="24430" citStr="Singh et al., 2011" startWordPosition="3829" endWordPosition="3832">ncluded biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007). However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types. Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. 65 recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other than English is scarce. Wang (2005) used a combination of the VSM and heuristic feature selection strategies to cluster transliterated Chinese personal names. For Arabic, Magdy et al. (2007) started with the output of the mention detection and within-document coreference system of Florian et al. (2004). They clustered the entities incrementally using a binary classifier. Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name uniqueness. Finally, S</context>
</contexts>
<marker>Singh, Subramanya, Pereira, McCallum, 2011</marker>
<rawString>S. Singh, A. Subramanya, F. Pereira, and A. McCallum. 2011. Large-scale cross-document coreference using distributed inference and hierarchical models. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Song</author>
<author>S Strassel</author>
</authors>
<date>2008</date>
<booktitle>Entity translation and alignment in the ACE-07 ET task. In LREC.</booktitle>
<contexts>
<context position="25612" citStr="Song and Strassel, 2008" startWordPosition="4003" endWordPosition="4006">ment topic and name uniqueness. Finally, Sayeed et al. (2009) translated Arabic name mentions to English and then formed clusters greedily using pairwise matching. To our knowledge, the cross-lingual entity clustering task is novel. However, there is significant prior work on similar tasks: • Multilingual coreference resolution: Adapt English within-document coreference models to other languages (Harabagiu and Maiorano, 2000; Florian et al., 2004; Luo and Zitouni, 2005). • Named entity translation: For a non-English document, produce an inventory of entities in English. An ACE2007 pilot task (Song and Strassel, 2008). • Named entity clustering: Assign semantic types to text mentions (Collins and Singer, 1999; Elsner et al., 2009). • Cross-language name search / entity linking: Match a single query name against a list of known multilingual names (knowledge base). A track in the 2011 NIST Text Analysis Conference (TAC-KBP) evaluation (Aktolga et al., 2008; McCarley, 2009; Udupa and Khapra, 2010; McNamee et al., 2011). Our work incorporates elements of the first three tasks. Most importantly, we avoid the key element of entity linking: a knowledge base. 7 Experiments We performed intrinsic evaluations for bo</context>
</contexts>
<marker>Song, Strassel, 2008</marker>
<rawString>Z. Song and S. Strassel. 2008. Entity translation and alignment in the ACE-07 ET task. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Strassel</author>
<author>M Przybocki</author>
<author>K Peterson</author>
<author>Z Song</author>
<author>K Maeda</author>
</authors>
<title>Linguistic resources and evaluation techniques for evaluation of cross-document automatic content extraction.</title>
<date>2008</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="20504" citStr="Strassel et al., 2008" startWordPosition="3236" endWordPosition="3239"> Mada (Habash and Rambow, 2005) for orthographic normalization and clitic segmentation. 9LDC Catalog numbers LDC2009E82 and LDC2009E88. 64 topically-aligned tuples served as “glue” to share topics between languages, while the ACE documents distribute those topics over in-domain vocabulary.10 We used the PLTM implementation in Mallet (McCallum, 2002). We ran the sampler for 10,000 iterations and set the number of topics K = 512. 5 Task Evaluation Framework Our experimental design is a cross-lingual extension of the standard cross-document coreference resolution task, which appeared in ACE2008 (Strassel et al., 2008; NIST, 2008). We evaluate name (NAM) mentions for cross-lingual person (PER) and organization (ORG) entities. Neither the number nor the attributes of the entities are known (i.e., the task does not include a knowledge base). We report results for both gold and automatic within-document mention detection and coreference resolution. Evaluation Metrics We use entity-level evaluation metrics, i.e., we evaluate the E entity clusters rather than the mentions. For the gold setting, we report: • B3 (Bagga and Baldwin, 1998a): Precision and recall are computed from the intersection of the hypothesis </context>
<context position="22540" citStr="Strassel et al., 2008" startWordPosition="3541" endWordPosition="3544">(2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly. Docs Tokens Entities Chains Mentions ARABIC 412 178,269 2,594 4,216 9,222 ENGLISH 414 246,309 2,278 3,950 9,140 Table 3: ACE2008 evaluation corpus PER and ORG entity statistics. Singleton chains account for 51.4% of the Arabic data and 46.2% of the English data. Just 216 entities appear in both languages. multilingual corpora. Cross-document annotation is expensive (Strassel et al., 2008), so we chose the ACE2008 Arabic-English evaluation corpus as a starting point for cross-lingual annotation. The corpus consists of seven genres sampled from independent sources over the course of a decade (Tbl. 3). The corpus provides gold mono-lingual cross-document coreference annotations for both PER and ORG entities. Using these annotations as a starting point, we found and annotated 216 cross-lingual entities.11 Because a similar corpus did not exist for development, we split the evaluation corpus into development and test sections. However, the usual method of splitting by document woul</context>
</contexts>
<marker>Strassel, Przybocki, Peterson, Song, Maeda, 2008</marker>
<rawString>S. Strassel, M. Przybocki, K. Peterson, Z. Song, and K. Maeda. 2008. Linguistic resources and evaluation techniques for evaluation of cross-document automatic content extraction. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Udupa</author>
<author>M M Khapra</author>
</authors>
<title>Improving the multilingual user experience of Wikipedia using cross-language name search.</title>
<date>2010</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="25995" citStr="Udupa and Khapra, 2010" startWordPosition="4063" endWordPosition="4066">her languages (Harabagiu and Maiorano, 2000; Florian et al., 2004; Luo and Zitouni, 2005). • Named entity translation: For a non-English document, produce an inventory of entities in English. An ACE2007 pilot task (Song and Strassel, 2008). • Named entity clustering: Assign semantic types to text mentions (Collins and Singer, 1999; Elsner et al., 2009). • Cross-language name search / entity linking: Match a single query name against a list of known multilingual names (knowledge base). A track in the 2011 NIST Text Analysis Conference (TAC-KBP) evaluation (Aktolga et al., 2008; McCarley, 2009; Udupa and Khapra, 2010; McNamee et al., 2011). Our work incorporates elements of the first three tasks. Most importantly, we avoid the key element of entity linking: a knowledge base. 7 Experiments We performed intrinsic evaluations for both mention and context similarity. For context similarity, we analyzed mono-lingual entity clustering, which also facilitated comparison to prior work on the ACE2008 Genre #Train #Test Accuracy(%) wb 125 16 87.5 bn 2,720 340 95.6 nw 7,443 930 96.6 all 10,288 1,286 97.1 (+7.55) Table 4: Cross-lingual mention matching accuracy [%]. The training data contains names from three genres:</context>
</contexts>
<marker>Udupa, Khapra, 2010</marker>
<rawString>R. Udupa and M. M. Khapra. 2010. Improving the multilingual user experience of Wikipedia using cross-language name search. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Vlachos</author>
<author>A Korhonen</author>
<author>Z Ghahramani</author>
</authors>
<title>Unsupervised and constrained Dirichlet process mixture models for verb clustering.</title>
<date>2009</date>
<booktitle>In Proc. of the Workshop on Geometrical Models of Natural Language Semantics.</booktitle>
<contexts>
<context position="17494" citStr="Vlachos et al. (2009)" startWordPosition="2773" endWordPosition="2777">able to let the data dictate the number of entity clusters. We thus consider a non-parametric Bayesian mixture model where the mixtures are multinomial distributions over the entity contexts S. Specifically, we consider a DPMM, which automatically infers the number of mixtures. Each Ca has an associated mixture Ba: Ca|Ba — Mult(Ba) Ba|G — G G|α, G0 — DP(α, G0) α — Gamma(1,1) where α is the concentration parameter of the DP prior and G0 is the base distribution with support V . For our experiments, we set G0 = Dir(7r1,... , 7rV ), where 7ri = PV (wi). For inference, we use the Gibbs sampler of Vlachos et al. (2009), which can incorporate pairwise constraints. The sampler is identical to a standard collapsed, token-based sampler, except the conditional probability p(Ea = E|E_a, Ca) = 0 if Ca cannot be merged with the chains in cluster E. This property makes the model non-exchangeable, but in practice non-exchangeable models are sometimes useful (Blei and Frazier, 2010). During sampling, we also learn α using the auxiliary variable procedure of West (1995), so the only fixed parameters are those of the vague Gamma prior. However, we found that these hyperparameters were not sensitive. 4 Training Data and </context>
</contexts>
<marker>Vlachos, Korhonen, Ghahramani, 2009</marker>
<rawString>A. Vlachos, A. Korhonen, and Z. Ghahramani. 2009. Unsupervised and constrained Dirichlet process mixture models for verb clustering. In Proc. of the Workshop on Geometrical Models of Natural Language Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wang</author>
</authors>
<title>Cross-document transliterated personal name coreference resolution.</title>
<date>2005</date>
<booktitle>Fuzzy Systems and Knowledge Discovery,</booktitle>
<volume>3614</volume>
<pages>11--20</pages>
<editor>In L. Wang and Y. Jin, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="24506" citStr="Wang (2005)" startWordPosition="3843" endWordPosition="3844">en and Martin, 2007). However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types. Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. 65 recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other than English is scarce. Wang (2005) used a combination of the VSM and heuristic feature selection strategies to cluster transliterated Chinese personal names. For Arabic, Magdy et al. (2007) started with the output of the mention detection and within-document coreference system of Florian et al. (2004). They clustered the entities incrementally using a binary classifier. Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name uniqueness. Finally, Sayeed et al. (2009) translated Arabic name mentions to English and then form</context>
</contexts>
<marker>Wang, 2005</marker>
<rawString>H. Wang. 2005. Cross-document transliterated personal name coreference resolution. In L. Wang and Y. Jin, editors, Fuzzy Systems and Knowledge Discovery, volume 3614 of Lecture Notes in Computer Science, pages 11–20. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M West</author>
</authors>
<title>Hyperparameter estimation in Dirichlet process mixture models.</title>
<date>1995</date>
<tech>Technical report,</tech>
<institution>Duke University.</institution>
<contexts>
<context position="17942" citStr="West (1995)" startWordPosition="2845" endWordPosition="2846">bution with support V . For our experiments, we set G0 = Dir(7r1,... , 7rV ), where 7ri = PV (wi). For inference, we use the Gibbs sampler of Vlachos et al. (2009), which can incorporate pairwise constraints. The sampler is identical to a standard collapsed, token-based sampler, except the conditional probability p(Ea = E|E_a, Ca) = 0 if Ca cannot be merged with the chains in cluster E. This property makes the model non-exchangeable, but in practice non-exchangeable models are sometimes useful (Blei and Frazier, 2010). During sampling, we also learn α using the auxiliary variable procedure of West (1995), so the only fixed parameters are those of the vague Gamma prior. However, we found that these hyperparameters were not sensitive. 4 Training Data and Procedures We trained our system for Arabic-English crosslingual entity clustering.8 Maxent Mention Similarity The Maxent mention similarity model requires a parallel name list for training. Name pair lists can be obtained from the LDC (e.g., LDC2005T34 contains nearly 450,000 parallel Chinese-English names) or Wikipedia (Irvine et al., 2010). We extracted 12,860 name pairs from the parallel Arabic-English translation treebanks,9 although our e</context>
</contexts>
<marker>West, 1995</marker>
<rawString>M. West. 1995. Hyperparameter estimation in Dirichlet process mixture models. Technical report, Duke University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>