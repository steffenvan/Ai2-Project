<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.991604">
Classifying Arguments by Scheme
</title>
<author confidence="0.997807">
Vanessa Wei Feng
</author>
<affiliation confidence="0.9987035">
Department of Computer Science
University of Toronto
</affiliation>
<address confidence="0.9924">
Toronto, ON, M5S 3G4, Canada
</address>
<email confidence="0.999786">
weifeng@cs.toronto.edu
</email>
<author confidence="0.995554">
Graeme Hirst
</author>
<affiliation confidence="0.998702">
Department of Computer Science
University of Toronto
</affiliation>
<address confidence="0.992397">
Toronto, ON, M5S 3G4, Canada
</address>
<email confidence="0.999785">
gh@cs.toronto.edu
</email>
<sectionHeader confidence="0.995653" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998729444444444">
Argumentation schemes are structures or tem-
plates for various kinds of arguments. Given
the text of an argument with premises and con-
clusion identified, we classify it as an instance
of one of five common schemes, using features
specific to each scheme. We achieve accura-
cies of 63–91% in one-against-others classifi-
cation and 80–94% in pairwise classification
(baseline = 50% in both cases).
</bodyText>
<sectionHeader confidence="0.998792" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997607">
We investigate a new task in the computational anal-
ysis of arguments: the classification of arguments
by the argumentation schemes that they use. An ar-
gumentation scheme, informally, is a framework or
structure for a (possibly defeasible) argument; we
will give a more-formal definition and examples in
Section 3. Our work is motivated by the need to de-
termine the unstated (or implicitly stated) premises
that arguments written in natural language normally
draw on. Such premises are called enthymemes.
For instance, the argument in Example 1 consists
of one explicit premise (the first sentence) and a con-
clusion (the second sentence):
Example 1 [Premise:] The survival of the entire
world is at stake.
[Conclusion:] The treaties and covenants aiming
for a world free of nuclear arsenals and other con-
ventional and biological weapons of mass destruc-
tion should be adhered to scrupulously by all na-
tions.
Another premise is left implicit — “Adhering to
those treaties and covenants is a means of realizing
survival of the entire world”. This proposition is an
enthymeme of this argument.
Our ultimate goal is to reconstruct the en-
thymemes in an argument, because determining
these unstated assumptions is an integral part of un-
derstanding, supporting, or attacking an entire argu-
ment. Hence reconstructing enthymemes is an im-
portant problem in argument understanding. We be-
lieve that first identifying the particular argumenta-
tion scheme that an argument is using will help to
bridge the gap between stated and unstated proposi-
tions in the argument, because each argumentation
scheme is a relatively fixed “template” for arguing.
That is, given an argument, we first classify its ar-
gumentation scheme; then we fit the stated proposi-
tions into the corresponding template; and from this
we infer the enthymemes.
In this paper, we present an argument scheme
classification system as a stage following argument
detection and proposition classification. First in Sec-
tion 2 and Section 3, we introduce the background
to our work, including related work in this field,
the two core concepts of argumentation schemes and
scheme-sets, and the Araucaria dataset. In Section 4
and Section 5 we present our classification system,
including the overall framework, data preprocessing,
feature selection, and the experimental setups. In
the remaining section, we present the essential ap-
proaches to solve the leftover problems of this paper
which we will study in our future work, and discuss
the experimental results, and potential directions for
future work.
</bodyText>
<page confidence="0.873583">
987
</page>
<note confidence="0.9875135">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 987–996,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.402139" genericHeader="introduction">
2 Related work determine how the pieces fit together as an instance
</sectionHeader>
<bodyText confidence="0.991741404255319">
of an argumentation scheme.
Argumentation has not received a great deal of at-
tention in computational linguistics, although it has
been a topic of interest for many years. Cohen
(1987) presented a computational model of argu-
mentative discourse. Dick (1987; 1991a; 1991b) de-
veloped a representation for retrieval of judicial de-
cisions by the structure of their legal argument — a
necessity for finding legal precedents independent of
their domain. However, at that time no corpus of ar-
guments was available, so Dick’s system was purely
theoretical. Recently, the Araucaria project at Uni-
versity of Dundee has developed a software tool for
manual argument analysis, with a point-and-click in-
terface for users to reconstruct and diagram an ar-
gument (Reed and Rowe, 2004; Rowe and Reed,
2008). The project also maintains an online repos-
itory, called AraucariaDB, of marked-up naturally
occurring arguments collected by annotators world-
wide, which can be used as an experimental corpus
for automatic argumentation analysis (for details see
Section 3.2).
Recent work on argument interpretation includes
that of George, Zukerman, and Nieman (2007), who
interpret constructed-example arguments (not natu-
rally occurring text) as Bayesian networks. Other
contemporary research has looked at the automatic
detection of arguments in text and the classification
of premises and conclusions. The work closest to
ours is perhaps that of Mochales and Moens (2007;
2008; 2009a; 2009b). In their early work, they fo-
cused on automatic detection of arguments in legal
texts. With each sentence represented as a vector of
shallow features, they trained a multinomial naive
Bayes classifier and a maximum entropy model on
the Araucaria corpus, and obtained a best average
accuracy of 73.75%. In their follow-up work, they
trained a support vector machine to further classify
each argumentative clause into a premise or a con-
clusion, with an F1 measure of 68.12% and 74.07%
respectively. In addition, their context-free grammar
for argumentation structure parsing obtained around
60% accuracy.
Our work is “downstream” from that of Mochales
and Moens. Assuming the eventual success of their,
or others’, research program on detecting and clas-
sifying the components of an argument, we seek to
</bodyText>
<sectionHeader confidence="0.870909" genericHeader="method">
3 Argumentation schemes, scheme-sets,
and annotation
</sectionHeader>
<subsectionHeader confidence="0.999898">
3.1 Definition and examples
</subsectionHeader>
<bodyText confidence="0.999982409090909">
Argumentation schemes are structures or templates
for forms of arguments. The arguments need not be
deductive or inductive; on the contrary, most argu-
mentation schemes are for presumptive or defeasible
arguments (Walton and Reed, 2002). For example,
argument from cause to effect is a commonly used
scheme in everyday arguments. A list of such argu-
mentation schemes is called a scheme-set.
It has been shown that argumentation schemes
are useful in evaluating common arguments as falla-
cious or not (van Eemeren and Grootendorst, 1992).
In order to judge the weakness of an argument, a set
of critical questions are asked according to the par-
ticular scheme that the argument is using, and the
argument is regarded as valid if it matches all the
requirements imposed by the scheme.
Walton’s set of 65 argumentation schemes (Wal-
ton et al., 2008) is one of the best-developed scheme-
sets in argumentation theory. The five schemes de-
fined in Table 1 are the most commonly used ones,
and they are the focus of the scheme classification
system that we will describe in this paper.
</bodyText>
<subsectionHeader confidence="0.999752">
3.2 Araucaria dataset
</subsectionHeader>
<bodyText confidence="0.999984916666667">
One of the challenges for automatic argumentation
analysis is that suitable annotated corpora are still
very rare, in spite of work by many researchers.
In the work described here, we use the Araucaria
database1, an online repository of arguments, as our
experimental dataset. Araucaria includes approxi-
mately 660 manually annotated arguments from var-
ious sources, such as newspapers and court cases,
and keeps growing. Although Araucaria has sev-
eral limitations, such as rather small size and low
agreement among annotators2, it is nonetheless one
of the best argumentative corpora available to date.
</bodyText>
<footnote confidence="0.999023">
1http://araucaria.computing.dundee.ac.uk/doku.php#
araucaria argumentation corpus
2The developers of Araucaria did not report on inter-
annotator agreement, probably because some arguments are an-
notated by only one commentator.
</footnote>
<page confidence="0.992205">
988
</page>
<subsectionHeader confidence="0.697648">
Argument from example
</subsectionHeader>
<bodyText confidence="0.9421242">
Premise: In this particular case, the individual a
has property F and also property G.
Conclusion: Therefore, generally, if x has prop-
erty F, then it also has property G.
Argument from cause to effect
</bodyText>
<listItem confidence="0.9833305">
Major premise: Generally, if A occurs, then B will
(might) occur.
Minor premise: In this case, A occurs (might oc-
cur).
Conclusion: Therefore, in this case, B will
(might) occur.
</listItem>
<subsectionHeader confidence="0.851718">
Practical reasoning
</subsectionHeader>
<bodyText confidence="0.5255472">
Major premise: I have a goal G.
Minor premise: Carrying out action A is a means
to realize G.
Conclusion: Therefore, I ought (practically
speaking) to carry out this action A.
</bodyText>
<subsectionHeader confidence="0.295486">
Argument from consequences
</subsectionHeader>
<listItem confidence="0.5512545">
Premise: If A is (is not) brought about, good (bad)
consequences will (will not) plausibly occur.
Conclusion: Therefore, A should (should not) be
brought about.
</listItem>
<subsectionHeader confidence="0.914361">
Argument from verbal classification
</subsectionHeader>
<bodyText confidence="0.99151">
Individual premise: a has a particular property F.
Classification premise: For all x, if x has property
F, then x can be classified as having property
G.
Conclusion: Therefore, a has property G.
</bodyText>
<tableCaption confidence="0.996911">
Table 1: The five most frequent schemes and their defini-
tions in Walton’s scheme-set.
</tableCaption>
<bodyText confidence="0.997935692307692">
Arguments in Araucaria are annotated in a XML-
based format called “AML” (Argument Markup
Language). A typical argument (see Example 2)
consists of several AU nodes. Each AU node is a
complete argument unit, composed of a conclusion
proposition followed by optional premise proposi-
tion(s) in a linked or convergent structure. Each of
these propositions can be further defined as a hier-
archical collection of smaller AUs. INSCHEME is
the particular scheme (e.g., “Argument from Con-
sequences”) of which the current proposition is a
member; enthymemes that have been made explicit
are annotated as “missing = yes”.
</bodyText>
<figure confidence="0.963394151515152">
Example 2 Example of argument markup from
Araucaria
&lt;TEXT&gt;If we stop the free creation of art, we will stop
the free viewing of art.&lt;/TEXT&gt;
&lt;AU&gt;
&lt;PROP identifier=&amp;quot;C&amp;quot; missing=&amp;quot;yes&amp;quot;&gt;
&lt;PROPTEXT offset=&amp;quot;-1&amp;quot;&gt;
The prohibition of the free creation of art should
not be brought about.&lt;/PROPTEXT&gt;
&lt;INSCHEME scheme=&amp;quot;Argument from Consequences&amp;quot;
schid=&amp;quot;0&amp;quot; /&gt;
&lt;/PROP&gt;
&lt;LA&gt;
&lt;AU&gt;
&lt;PROP identifier=&amp;quot;A&amp;quot; missing=&amp;quot;no&amp;quot;&gt;
&lt;PROPTEXT offset=&amp;quot;0&amp;quot;&gt;
If we stop the free creation of art, we will
stop the free viewing of art.&lt;/PROPTEXT&gt;
&lt;INSCHEME scheme=&amp;quot;Argument from Consequences&amp;quot;
schid=&amp;quot;0&amp;quot; /&gt;
&lt;/PROP&gt;
&lt;/AU&gt;
&lt;AU&gt;
&lt;PROP identifier=&amp;quot;B&amp;quot; missing=&amp;quot;yes&amp;quot;&gt;
&lt;PROPTEXT offset=&amp;quot;-1&amp;quot;&gt;
The prohibition of free viewing of art is not
acceptable.&lt;/PROPTEXT&gt;
&lt;INSCHEME scheme=&amp;quot;Argument from Consequences&amp;quot;
schid=&amp;quot;0&amp;quot; /&gt;
&lt;/PROP&gt;
&lt;/AU&gt;
&lt;/LA&gt;
&lt;/AU&gt;
</figure>
<bodyText confidence="0.9987345">
There are three scheme-sets used in the anno-
tations in Araucaria: Walton’s scheme-set, Katzav
and Reed’s (2004) scheme-set, and Pollock’s (1995)
scheme-set. Each of these has a different set of
schemes; and most arguments in Araucaria are
marked up according to only one of them. Our
experimental dataset is composed of only those
arguments annotated in accordance with Walton’s
scheme-set, within which the five schemes shown in
Table 1 constitute 61% of the total occurrences.
</bodyText>
<sectionHeader confidence="0.999669" genericHeader="method">
4 Methods
</sectionHeader>
<subsectionHeader confidence="0.999224">
4.1 Overall framework
</subsectionHeader>
<bodyText confidence="0.99998725">
As we noted above, our ultimate goal is to recon-
struct enthymemes, the unstated premises, in an ar-
gument by taking advantage of the stated proposi-
tions; and in order to achieve this goal we need to
first determine the particular argumentation scheme
that the argument is using. This problem is de-
picted in Figure 1. Our scheme classifier is the
dashed round-cornered rectangle portion of this
</bodyText>
<page confidence="0.997311">
989
</page>
<figureCaption confidence="0.9996">
Figure 1: Overall framework of this research.
</figureCaption>
<bodyText confidence="0.999930533333333">
overall framework: its input is the extracted con-
clusion and premise(s) determined by an argument
detector, followed by a premise / conclusion classi-
fier, given an unknown text as the input to the entire
system. And the portion below the dashed round-
rectangle represents our long-term goal — to recon-
struct the implicit premise(s) in an argument, given
its argumentation scheme and its explicit conclusion
and premise(s) as input. Since argument detection
and classification are not the topic of this paper, we
assume here that the input conclusion and premise(s)
have already been retrieved, segmented, and classi-
fied, as for example by the methods of Mochales and
Moens (see Section 2 above). And the scheme tem-
plate fitter is the topic of our on-going work.
</bodyText>
<subsectionHeader confidence="0.999232">
4.2 Data preprocessing
</subsectionHeader>
<bodyText confidence="0.999916235294118">
From all arguments in Araucaria, we first ex-
tract those annotated in accordance with Walton’s
scheme-set. Then we break each complex AU
node into several simple AUs where no conclusion
or premise proposition nodes have embedded AU
nodes. From these generated simple arguments, we
extract those whose scheme falls into one of the five
most frequent schemes as described in Table 1. Fur-
thermore, we remove all enthymemes that have been
inserted by the annotator and ignore any argument
with a missing conclusion, since the input to our pro-
posed classifier, as depicted in Figure 1, cannot have
any access to unstated argumentative propositions.
The resulting preprocessed dataset is composed of
393 arguments, of which 149, 106, 53, 44, and 41
respectively belong to the five schemes in the order
shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.999578">
4.3 Feature selection
</subsectionHeader>
<bodyText confidence="0.999201">
The features used in our work fall into two cat-
egories: general features and scheme-specific fea-
tures.
</bodyText>
<subsubsectionHeader confidence="0.808855">
4.3.1 General features
</subsubsectionHeader>
<bodyText confidence="0.9997905">
General features are applicable to arguments belong-
ing to any of the five schemes (shown in Table 2).
For the features conLoc, premLoc, gap, and
lenRat, we have two versions, differing in terms
of their basic measurement unit: sentence-based
and token-based. The final feature, type, indicates
whether the premises contribute to the conclusion
in a linked or convergent order. A linked argument
(LA) is one that has two or more inter-dependent
premise propositions, all of which are necessary to
make the conclusion valid, whereas in a conver-
gent argument (CA) exactly one premise proposi-
tion is sufficient to do so. Since it is observed that
there exists a strong correlation between type and
the particular scheme employed while arguing, we
believe type can be a good indicator of argumenta-
tion scheme. However, although this feature is avail-
able to us because it is included in the Araucaria an-
notations, its value cannot be obtained from raw text
as easily as other features mentioned above; but it is
possible that we will in the future be able to deter-
mine it automatically by taking advantage of some
scheme-independent cues such as the discourse re-
lation between the conclusion and the premises.
</bodyText>
<subsubsectionHeader confidence="0.714954">
4.3.2 Scheme-specific features
</subsubsectionHeader>
<bodyText confidence="0.9998885">
Scheme-specific features are different for each
scheme, since each scheme has its own cue phrases
or patterns. The features for each scheme are shown
in Table 3 (for complete lists of features see Feng
(2010)). In our experiments in Section 5 below, all
these features are computed for all arguments; but
</bodyText>
<figure confidence="0.997941277777778">
Detecting
argumentative text
TEXT
CONCLUSION
ARGUMENTATIVE
SEGMENT
ARGUMENTATION
SCHEME
Scheme classifier
CONSTRUCTED
ENTHYMEME
Argument
template fitter
Premise /
conclusion
classifier
PREMISE #1
PREMISE #2
</figure>
<page confidence="0.952682">
990
</page>
<bodyText confidence="0.9732328125">
conLoc: the location (in token or sentence) of the
conclusion in the text.
premLoc: the location (in token or sentence) of
the first premise proposition.
conFirst: whether the conclusion appears before
the first premise proposition.
gap: the interval (in token or sentence) between
the conclusion and the first premise proposi-
tion.
lenRat: the ratio of the length (in token or sen-
tence) of the premise(s) to that of the conclu-
sion.
numPrem: the number of explicit premise propo-
sitions (PROP nodes) in the argument.
type: type of argumentation structure, i.e., linked
or convergent.
</bodyText>
<tableCaption confidence="0.986552">
Table 2: List of general features.
</tableCaption>
<bodyText confidence="0.99992508">
the features for any particular scheme are used only
when it is the subject of a particular task. For ex-
ample, when we classify argument from example
in a one-against-others setup, we use the scheme-
specific features of that scheme for all arguments;
when we classify argument from example against
argument from cause to effect, we use the scheme-
specific features of those two schemes.
For the first three schemes (argument from ex-
ample, argument from cause to effect, and practi-
cal reasoning), the scheme-specific features are se-
lected cue phrases or patterns that are believed to be
indicative of each scheme. Since these cue phrases
and patterns have differing qualities in terms of their
precision and recall, we do not treat them all equally.
For each cue phrase or pattern, we compute “confi-
dence”, the degree of belief that the argument of in-
terest belongs to a particular scheme, using the dis-
tribution characteristics of the cue phrase or pattern
in the corpus, as described below.
For each argument A, a vector CV = {c1, c2, c3}
is added to its feature set, where each ci indicates
the “confidence” of the existence of the specific fea-
tures associated with each of the first three schemes,
schemei. This is defined in Equation 1:
</bodyText>
<equation confidence="0.97237">
(P (schemei|cpk) · dik) (1)
</equation>
<bodyText confidence="0.947611444444445">
Argument from example
8 keywords and phrases including for example,
such as, for instance, etc.; 3 punctuation cues: “:”,
“;”, and “—”.
Argument from cause to effect
22 keywords and simple cue phrases including re-
sult, related to, lead to, etc.; 10 causal and non-
causal relation patterns extracted from WordNet
(Girju, 2003).
</bodyText>
<subsectionHeader confidence="0.993123">
Practical reasoning
</subsectionHeader>
<bodyText confidence="0.99993175">
28 keywords and phrases including want, aim, ob-
jective, etc.; 4 modal verbs: should, could, must,
and need; 4 patterns including imperatives and in-
finitives indicating the goal of the speaker.
</bodyText>
<subsectionHeader confidence="0.75244">
Argument from consequences
</subsectionHeader>
<bodyText confidence="0.9993">
The counts of positive and negative propositions
in the conclusion and premises, calculated from
the General Inquirer2.
</bodyText>
<subsectionHeader confidence="0.845228">
Argument from verbal classification
</subsectionHeader>
<bodyText confidence="0.999776666666667">
The maximal similarity between the central word
pairs extracted from the conclusion and the
premise; the counts of copula, expletive, and neg-
ative modifier dependency relations returned by
the Stanford parser3 in the conclusion and the
premise.
</bodyText>
<footnote confidence="0.795212">
2 http://www.wjh.harvard.edu/∼inquirer/
3 http://nlp.stanford.edu/software/lex-parser.shtml
</footnote>
<tableCaption confidence="0.994222">
Table 3: List of scheme-specific features.
</tableCaption>
<bodyText confidence="0.999533933333333">
Here mi is the number of scheme-specific cue
phrases designed for schemei; P (schemei|cpk) is the
prior probability that the argument A actually be-
longs to schemei, given that some particular cue
phrase cpk is found in A; dik is a value indicat-
ing whether cpk is found in A; and the normaliza-
tion factor N is the number of scheme-specific cue
phrase patterns designed for schemei with at least
one support (at least one of the arguments belonging
to schemei contains that cue phrase). There are two
ways to calculate dik, Boolean and count: in Boolean
mode, dik is treated as 1 if A matches cpk; in count
mode, dik equals to the number of times A matches
cpk; and in both modes, dik is treated as 0 if cpk is
not found in A.
</bodyText>
<equation confidence="0.840600833333333">
1
N
ci =
mi
z
k=1
</equation>
<page confidence="0.992814">
991
</page>
<bodyText confidence="0.999830333333333">
For argumentfrom consequences, since the arguer
has an obvious preference for some particular con-
sequence, sentiment orientation can be a good in-
dicator for this scheme, which is quantified by the
counts of positive and negative propositions in the
conclusion and premise.
For argument from verbal classification, there ex-
ists a hypernymy-like relation between some pair of
propositions (entities, concepts, or actions) located
in the conclusion and the premise respectively. The
existence of such a relation is quantified by the max-
imal Jiang-Conrath Similarity (Jiang and Conrath,
1997) between the “central word” pairs extracted
from the conclusion and the premise. We parse each
sentence of the argument with the Stanford depen-
dency parser, and a word or phrase is considered to
be a central word if it is the dependent or governor of
several particular dependency relations, which basi-
cally represents the attribute or the action of an en-
tity in a sentence, or the entity itself. For example,
if a word or phrase is the dependent of the depen-
dency relation agent, it is therefore considered as a
“central word”. In addition, an arguer tends to use
several particular syntactic structures (copula, exple-
tive, and negative modifier) when using this scheme,
which can be quantified by the counts of those spe-
cial relations in the conclusion and the premise(s).
</bodyText>
<sectionHeader confidence="0.999414" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.971068">
5.1 Training
</subsectionHeader>
<bodyText confidence="0.985741733333333">
We experiment with two kinds of classification: one-
against-others and pairwise. We build a pruned
C4.5 decision tree (Quinlan, 1993) for each different
classification setup, implemented by Weka Toolkit
3.65 (Hall et al., 2009).
One-against-others classification A one-against-
others classifier is constructed for each of the five
most frequent schemes, using the general features
and the scheme-specific features for the scheme of
interest. For each classifier, there are two possi-
ble outcomes: target scheme and other; 50% of the
training dataset is arguments associated with tar-
get scheme, while the rest is arguments of all the
other schemes, which are treated as other. One-
against-other classification thus tests the effective-
</bodyText>
<footnote confidence="0.770387">
5http://cs.waikato.ac.nz/ml/weka
</footnote>
<bodyText confidence="0.986139909090909">
ness of each scheme’s specific features.
Pairwise classification A pairwise classifier is
constructed for each of the ten possible pairings
of the five schemes, using the general features and
the scheme-specific features of the two schemes in
the pair. For each of the ten classifiers, the train-
ing dataset is divided equally into arguments be-
longing to scheme1 and arguments belonging to
scheme2, where scheme1 and scheme2 are two dif-
ferent schemes among the five. Only features asso-
ciated with scheme1 and scheme2 are used.
</bodyText>
<subsectionHeader confidence="0.995163">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.9999565">
We experiment with different combinations of gen-
eral features and scheme-specific features (discussed
in Section 4.3). To evaluate each experiment, we
use the average accuracy over 10 pools of randomly
sampled data (each with baseline at 50%6) with 10-
fold cross-validation.
</bodyText>
<sectionHeader confidence="0.999841" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999987777777778">
We first present the best average accuracy (BAA) of
each classification setup. Then we demonstrate the
impact of the feature type (convergent or linked ar-
gument) on BAAs for different classification setups,
since we believe type is strongly correlated with
the particular argumentation scheme and its value is
the only one directly retrieved from the annotations
of the training corpus. For more details, see Feng
(2010).
</bodyText>
<subsectionHeader confidence="0.982074">
6.1 BAAs of each classification setup
</subsectionHeader>
<table confidence="0.995138285714286">
target scheme BAA dik base type
example 90.6 count token yes
cause 70.4 Boolean token no
/ count
reasoning 90.8 count sentence yes
consequences 62.9 – sentence yes
classification 63.2 – token yes
</table>
<tableCaption confidence="0.997916">
Table 4: Best average accuracies (BAAs) (%) of one-
against-others classification.
</tableCaption>
<footnote confidence="0.543540333333333">
6We also experiment with using general features only, but
the results are consistently below or around the sampling base-
line of 50%; therefore, we do not use them as a baseline here.
</footnote>
<page confidence="0.961095">
992
</page>
<table confidence="0.9995855">
example cause reason- conse-
ing quences
cause 80.6
reasoning 93.1 94.2
consequences 86.9 86.7 97.9
classification 86.0 85.6 98.3 64.2
</table>
<tableCaption confidence="0.9940595">
Table 5: Best average accuracies (BAAs) (%) of pairwise
classification.
</tableCaption>
<bodyText confidence="0.824325833333333">
Table 4 presents the best average accuracies of
one-against-others classification for each of the five
schemes. The subsequent three columns list the
particular strategies of features incorporation under
which those BAAs are achieved (the complete set of
possible choices is given in Section 4.3.):
</bodyText>
<listItem confidence="0.999870857142857">
• dik: Boolean or count — the strategy of com-
bining scheme-specific cue phrases or patterns
using either Boolean or count for dik.
• base: sentence or token — the basic unit of ap-
plying location- or length-related general fea-
tures.
• type: yes or no — whether type (convergent or
</listItem>
<bodyText confidence="0.966092607142857">
linked argument) is incorporated into the fea-
ture set.
As Table 4 shows, one-against-others classifica-
tion achieves high accuracy for argument from ex-
ample and practical reasoning: 90.6% and 90.8%.
The BAA of argument from cause to effect is only
just over 70%. However, with the last two schemes
(argument from consequences and argument from
verbal classification), accuracy is only in the low
60s; there is little improvement of our system over
the majority baseline of 50%. This is probably due
at least partly to the fact that these schemes do not
have such obvious cue phrases or patterns as the
other three schemes which therefore may require
more world knowledge encoded, and also because
the available training data for each is relatively small
(44 and 41 instances, respectively). The BAA for
each scheme is achieved with inconsistent choices
of base and dik, but the accuracies that resulted from
different choices vary only by very little.
Table 5 shows that our system is able to correctly
differentiate between most of the different scheme
pairs, with accuracies as high as 98%. It has poor
performance (64.0%) only for the pair argument
from consequences and argument from verbal clas-
sification; perhaps not coincidentally, these are the
two schemes for which performance was poorest in
the one-against-others task.
</bodyText>
<subsectionHeader confidence="0.99954">
6.2 Impact of type on classification accuracy
</subsectionHeader>
<bodyText confidence="0.999551928571429">
As we can see from Table 6, for one-against-others
classifications, incorporating type into the feature
vectors improves classification accuracy in most
cases: the only exception is that the best average ac-
curacy of one-against-others classification between
argument from cause to effect and others is obtained
without involving type into the feature vector —
but the difference is negligible, i.e., 0.5 percent-
age points with respect to the average difference.
Type also has a relatively small impact on argument
from verbal classification (2.6 points), compared to
its impact on argument from example (22.3 points),
practical reasoning (8.1 points), and argument from
consequences (7.5 points), in terms of the maximal
differences.
Similarly, for pairwise classifications, as shown
in Table 7, type has significant impact on BAAs, es-
pecially on the pairs of practical reasoning versus
argument from cause to effect (17.4 points), prac-
tical reasoning versus argument from example (22.6
points), and argumentfrom verbal classification ver-
sus argument from example (20.2 points), in terms
of the maximal differences; but it has a relatively
small impact on argument from consequences ver-
sus argument from cause to effect (0.8 point), and
argumentfrom verbal classification versus argument
from consequences (1.1 points), in terms of average
differences.
</bodyText>
<sectionHeader confidence="0.999564" genericHeader="discussions">
7 Future Work
</sectionHeader>
<bodyText confidence="0.9999027">
In future work, we will look at automatically clas-
sifying type (i.e., whether an argument is linked or
convergent), as type is the only feature directly re-
trieved from annotations in the training corpus that
has a strong impact on improving classification ac-
curacies.
Automatically classifying type will not be easy,
because sometimes it is subjective to say whether a
premise is sufficient by itself to support the conclu-
sion or not, especially when the argument is about
</bodyText>
<page confidence="0.995629">
993
</page>
<table confidence="0.999784666666667">
target scheme BAA-t BAA-no t max di� min di� avg di�
example 90.6 71.6 22.3 10.6 14.7
cause 70.4 70.9 −0.5 −0.6 −0.5
reasoning 90.8 83.2 8.1 7.5 7.7
consequences 62.9 61.9 7.5 −0.6 4.2
classification 63.2 60.7 2.6 0.4 2.0
</table>
<tableCaption confidence="0.96255375">
Table 6: Accuracy (%) with and without type in one-against-others classification. BAA-t is best average accuracy with
type, and BAA-no t is best average accuracy without type. max di�, min di�, and avg di� are maximal, minimal, and
average differences between each experimental setup with type and without type while the remaining conditions are
the same.
</tableCaption>
<table confidence="0.999964363636364">
scheme1 scheme2 BAA-t BAA-no t max di� min di� avg di�
cause example 80.6 69.7 10.9 7.1 8.7
reasoning example 93.1 73.1 22.8 19.1 20.1
reasoning cause 94.2 80.5 17.4 8.7 13.9
consequences example 86.9 76.0 13.8 6.9 10.1
consequences cause 87.7 86.7 3.8 −1.5 −0.1
consequences reasoning 97.9 97.9 10.6 0.0 0.8
classification example 86.0 74.6 20.2 3.7 7.1
classification cause 85.6 76.8 9.0 3.7 7.1
classification reasoning 98.3 89.3 8.9 4.2 8.3
classification consequences 64.0 60.0 6.5 −1.3 1.1
</table>
<tableCaption confidence="0.913391">
Table 7: Accuracy (%) with and without type in pairwise classification. Column headings have the same meanings as
in Table 6.
</tableCaption>
<bodyText confidence="0.999290033333333">
personal opinions or judgments. So for this task,
we will initially focus on arguments that are (or at
least seem to be) empirical or objective rather than
value-based. It will also be non-trivial to deter-
mine whether an argument is convergent or linked
— whether the premises are independent of one an-
other or not. Cue words and discourse relations be-
tween the premises and the conclusion will be one
helpful factor; for example, besides generally flags
an independent premise. And one premise may be
regarded as linked to another if either would become
an enthymeme if deleted; but determining this in the
general case, without circularity, will be difficult.
We will also work on the argument template fitter,
which is the final component in our overall frame-
work. The task of the argument template fitter is to
map each explicitly stated conclusion and premise
into the corresponding position in its scheme tem-
plate and to extract the information necessary for en-
thymeme reconstruction. Here we propose a syntax-
based approach for this stage, which is similar to
tasks in information retrieval. This can be best ex-
plained by the argument in Example 1, which uses
the particular argumentation scheme practical rea-
soning.
We want to fit the Premise and the Conclusion of
this argument into the Major premise and the Con-
clusion slots of the definition of practical reasoning
(see Table 1), and construct the following conceptual
mapping relations:
</bodyText>
<listItem confidence="0.943152">
1. Survival of the entire world −� a goal G
2. Adhering to the treaties and covenants aiming
</listItem>
<bodyText confidence="0.834435">
for a world free of nuclear arsenals and other
conventional and biological weapons of mass
destruction −� action A
Thereby we will be able to reconstruct the missing
Minor premise — the enthymeme in this argument:
Carrying out adhering to the treaties and
covenants aiming for a world free of nuclear
arsenals and other conventional and biological
</bodyText>
<page confidence="0.995369">
994
</page>
<bodyText confidence="0.9720165">
weapons of mass destruction is a means of real-
izing survival of the entire world.
</bodyText>
<sectionHeader confidence="0.994105" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999982714285714">
The argumentation scheme classification system that
we have presented in this paper introduces a new
task in research on argumentation. To the best of
our knowledge, this is the first attempt to classify
argumentation schemes.
In our experiments, we have focused on the five
most frequently used schemes in Walton’s scheme-
set, and conducted two kinds of classification: in
one-against-others classification, we achieved over
90% best average accuracies for two schemes, with
other three schemes in the 60s to 70s; and in pair-
wise classification, we obtained 80% to 90% best
average accuracies for most scheme pairs. The poor
performance of our classification system on other
experimental setups is partly due to the lack of train-
ing examples or to insufficient world knowledge.
Completion of our scheme classification system
will be a step towards our ultimate goal of recon-
structing the enthymemes in an argument by the pro-
cedure depicted in Figure 1. Because of the signifi-
cance of enthymemes in reasoning and arguing, this
is crucial to the goal of understanding arguments.
But given the still-premature state of research of ar-
gumentation in computational linguistics, there are
many practical issues to deal with first, such as the
construction of richer training corpora and improve-
ment of the performance of each step in the proce-
dure.
</bodyText>
<sectionHeader confidence="0.998361" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999648">
This work was financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada and by the University of Toronto. We are
grateful to Suzanne Stevenson for helpful comments
and suggestions.
</bodyText>
<sectionHeader confidence="0.99948" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999889983050848">
Robin Cohen. 1987. Analyzing the structure of ar-
gumentative discourse. Computational Linguistics,
13(1–2):11–24.
Judith Dick. 1987. Conceptual retrieval and case law.
In Proceedings, First International Conference on Ar-
tificial Intelligence and Law, pages 106–115, Boston,
May.
Judith Dick. 1991a. A Conceptual, Case-relation Repre-
sentation of Text for Intelligent Retrieval. Ph.D. thesis,
Faculty of Library and Information Science, Univer-
sity of Toronto, April.
Judith Dick. 1991b. Representation of legal text for con-
ceptual retrieval. In Proceedings, Third International
Conference on Artificial Intelligence and Law, pages
244–252, Oxford, June.
Vanessa Wei Feng. 2010. Classifying argu-
ments by scheme. Technical report, Depart-
ment of Computer Science, University of Toronto,
November. http://ftp.cs.toronto.edu/pub/
gh/Feng-MSc-2010.pdf.
Sarah George, Ingrid Zukerman, and Michael Niemann.
2007. Inferences, suppositions and explanatory exten-
sions in argument interpretation. User Modeling and
User-Adapted Interaction, 17(5):439–474.
Roxana Girju. 2003. Automatic detection of causal re-
lations for question answering. In Proceedings of the
ACL 2003 Workshop on Multilingual Summarization
and Question Answering, pages 76–83, Morristown,
NJ, USA. Association for Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explorations Newsletter, 11(1):10–18.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical taxon-
omy. In International Conference Research on Com-
putational Linguistics (ROCLING X), pages 19–33.
Joel Katzav and Chris Reed. 2004. On argumentation
schemes and the natural classification of arguments.
Argumentation, 18(2):239–259.
Raquel Mochales and Marie-Francine Moens. 2008.
Study on the structure of argumentation in case law. In
Proceedings of the 2008 Conference on Legal Knowl-
edge and Information Systems, pages 11–20, Amster-
dam, The Netherlands. IOS Press.
Raquel Mochales and Marie-Francine Moens. 2009a.
Argumentation mining: the detection, classification
and structure of arguments in text. In ICAIL ’09: Pro-
ceedings of the 12th International Conference on Arti-
ficial Intelligence and Law, pages 98–107, New York,
NY, USA. ACM.
Raquel Mochales and Marie-Francine Moens. 2009b.
Automatic argumentation detection and its role in law
and the semantic web. In Proceedings of the 2009
Conference on Law, Ontologies and the Semantic Web,
pages 115–129, Amsterdam, The Netherlands. IOS
Press.
Marie-Francine Moens, Erik Boiy, Raquel Mochales
Palau, and Chris Reed. 2007. Automatic detection
</reference>
<page confidence="0.98385">
995
</page>
<reference confidence="0.999422592592592">
of arguments in legal texts. In ICAIL ’07: Proceed-
ings of the 11th International Conference on Artificial
Intelligence and Law, pages 225–230, New York, NY,
USA. ACM.
John L. Pollock. 1995. Cognitive Carpentry: A
Blueprint for How to Build a Person. Bradford Books.
The MIT Press, May.
J. Ross Quinlan. 1993. C4.5: Programs for machine
learning. Machine Learning, 16(3):235–240.
Chris Reed and Glenn Rowe. 2004. Araucaria: Software
for argument analysis, diagramming and representa-
tion. International Journal of Artificial Intelligence
Tools, 14:961–980.
Glenn Rowe and Chris Reed. 2008. Argument diagram-
ming: The Araucaria project. In Knowledge Cartog-
raphy, pages 163–181. Springer London.
Frans H. van Eemeren and Rob Grootendorst. 1992.
Argumentation, Communication, and Fallacies: A
Pragma-Dialectical Perspective. Routledge.
Douglas Walton and Chris Reed. 2002. Argumenta-
tion schemes and defeasible inferences. In Workshop
on Computational Models of Natural Argument, 15th
European Conference on Artificial Intelligence, pages
11–20, Amsterdam, The Netherlands. IOS Press.
Douglas Walton, Chris Reed, and Fabrizio Macagno.
2008. Argumentation Schemes. Cambridge University
Press.
</reference>
<page confidence="0.998586">
996
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.903984">
<title confidence="0.998179">Classifying Arguments by Scheme</title>
<author confidence="0.99999">Vanessa Wei</author>
<affiliation confidence="0.9998505">Department of Computer University of</affiliation>
<address confidence="0.956902">Toronto, ON, M5S 3G4,</address>
<email confidence="0.999866">weifeng@cs.toronto.edu</email>
<author confidence="0.99969">Graeme Hirst</author>
<affiliation confidence="0.9999835">Department of Computer Science University of Toronto</affiliation>
<address confidence="0.999711">Toronto, ON, M5S 3G4, Canada</address>
<email confidence="0.999928">gh@cs.toronto.edu</email>
<abstract confidence="0.9941472">Argumentation schemes are structures or templates for various kinds of arguments. Given the text of an argument with premises and conclusion identified, we classify it as an instance of one of five common schemes, using features specific to each scheme. We achieve accuracies of 63–91% in one-against-others classification and 80–94% in pairwise classification in both cases).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Robin Cohen</author>
</authors>
<title>Analyzing the structure of argumentative discourse.</title>
<date>1987</date>
<journal>Computational Linguistics,</journal>
<pages>13--1</pages>
<contexts>
<context position="3676" citStr="Cohen (1987)" startWordPosition="569" endWordPosition="570">s to solve the leftover problems of this paper which we will study in our future work, and discuss the experimental results, and potential directions for future work. 987 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 987–996, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Related work determine how the pieces fit together as an instance of an argumentation scheme. Argumentation has not received a great deal of attention in computational linguistics, although it has been a topic of interest for many years. Cohen (1987) presented a computational model of argumentative discourse. Dick (1987; 1991a; 1991b) developed a representation for retrieval of judicial decisions by the structure of their legal argument — a necessity for finding legal precedents independent of their domain. However, at that time no corpus of arguments was available, so Dick’s system was purely theoretical. Recently, the Araucaria project at University of Dundee has developed a software tool for manual argument analysis, with a point-and-click interface for users to reconstruct and diagram an argument (Reed and Rowe, 2004; Rowe and Reed, 2</context>
</contexts>
<marker>Cohen, 1987</marker>
<rawString>Robin Cohen. 1987. Analyzing the structure of argumentative discourse. Computational Linguistics, 13(1–2):11–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Dick</author>
</authors>
<title>Conceptual retrieval and case law.</title>
<date>1987</date>
<booktitle>In Proceedings, First International Conference on Artificial Intelligence and Law,</booktitle>
<pages>106--115</pages>
<location>Boston,</location>
<contexts>
<context position="3747" citStr="Dick (1987" startWordPosition="579" endWordPosition="580"> future work, and discuss the experimental results, and potential directions for future work. 987 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 987–996, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Related work determine how the pieces fit together as an instance of an argumentation scheme. Argumentation has not received a great deal of attention in computational linguistics, although it has been a topic of interest for many years. Cohen (1987) presented a computational model of argumentative discourse. Dick (1987; 1991a; 1991b) developed a representation for retrieval of judicial decisions by the structure of their legal argument — a necessity for finding legal precedents independent of their domain. However, at that time no corpus of arguments was available, so Dick’s system was purely theoretical. Recently, the Araucaria project at University of Dundee has developed a software tool for manual argument analysis, with a point-and-click interface for users to reconstruct and diagram an argument (Reed and Rowe, 2004; Rowe and Reed, 2008). The project also maintains an online repository, called Araucaria</context>
</contexts>
<marker>Dick, 1987</marker>
<rawString>Judith Dick. 1987. Conceptual retrieval and case law. In Proceedings, First International Conference on Artificial Intelligence and Law, pages 106–115, Boston, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Dick</author>
</authors>
<title>A Conceptual, Case-relation Representation of Text for Intelligent Retrieval.</title>
<date>1991</date>
<tech>Ph.D. thesis,</tech>
<institution>Faculty of Library and Information Science, University of Toronto,</institution>
<marker>Dick, 1991</marker>
<rawString>Judith Dick. 1991a. A Conceptual, Case-relation Representation of Text for Intelligent Retrieval. Ph.D. thesis, Faculty of Library and Information Science, University of Toronto, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Dick</author>
</authors>
<title>Representation of legal text for conceptual retrieval.</title>
<date>1991</date>
<booktitle>In Proceedings, Third International Conference on Artificial Intelligence and Law,</booktitle>
<pages>244--252</pages>
<location>Oxford,</location>
<marker>Dick, 1991</marker>
<rawString>Judith Dick. 1991b. Representation of legal text for conceptual retrieval. In Proceedings, Third International Conference on Artificial Intelligence and Law, pages 244–252, Oxford, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Wei Feng</author>
</authors>
<title>Classifying arguments by scheme.</title>
<date>2010</date>
<tech>Technical report,</tech>
<institution>Department of Computer Science, University of Toronto,</institution>
<note>http://ftp.cs.toronto.edu/pub/ gh/Feng-MSc-2010.pdf.</note>
<contexts>
<context position="14422" citStr="Feng (2010)" startWordPosition="2265" endWordPosition="2266">ure is available to us because it is included in the Araucaria annotations, its value cannot be obtained from raw text as easily as other features mentioned above; but it is possible that we will in the future be able to determine it automatically by taking advantage of some scheme-independent cues such as the discourse relation between the conclusion and the premises. 4.3.2 Scheme-specific features Scheme-specific features are different for each scheme, since each scheme has its own cue phrases or patterns. The features for each scheme are shown in Table 3 (for complete lists of features see Feng (2010)). In our experiments in Section 5 below, all these features are computed for all arguments; but Detecting argumentative text TEXT CONCLUSION ARGUMENTATIVE SEGMENT ARGUMENTATION SCHEME Scheme classifier CONSTRUCTED ENTHYMEME Argument template fitter Premise / conclusion classifier PREMISE #1 PREMISE #2 990 conLoc: the location (in token or sentence) of the conclusion in the text. premLoc: the location (in token or sentence) of the first premise proposition. conFirst: whether the conclusion appears before the first premise proposition. gap: the interval (in token or sentence) between the conclu</context>
<context position="21862" citStr="Feng (2010)" startWordPosition="3455" endWordPosition="3456">cussed in Section 4.3). To evaluate each experiment, we use the average accuracy over 10 pools of randomly sampled data (each with baseline at 50%6) with 10- fold cross-validation. 6 Results We first present the best average accuracy (BAA) of each classification setup. Then we demonstrate the impact of the feature type (convergent or linked argument) on BAAs for different classification setups, since we believe type is strongly correlated with the particular argumentation scheme and its value is the only one directly retrieved from the annotations of the training corpus. For more details, see Feng (2010). 6.1 BAAs of each classification setup target scheme BAA dik base type example 90.6 count token yes cause 70.4 Boolean token no / count reasoning 90.8 count sentence yes consequences 62.9 – sentence yes classification 63.2 – token yes Table 4: Best average accuracies (BAAs) (%) of oneagainst-others classification. 6We also experiment with using general features only, but the results are consistently below or around the sampling baseline of 50%; therefore, we do not use them as a baseline here. 992 example cause reason- conseing quences cause 80.6 reasoning 93.1 94.2 consequences 86.9 86.7 97.</context>
</contexts>
<marker>Feng, 2010</marker>
<rawString>Vanessa Wei Feng. 2010. Classifying arguments by scheme. Technical report, Department of Computer Science, University of Toronto, November. http://ftp.cs.toronto.edu/pub/ gh/Feng-MSc-2010.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah George</author>
<author>Ingrid Zukerman</author>
<author>Michael Niemann</author>
</authors>
<title>Inferences, suppositions and explanatory extensions in argument interpretation. User Modeling and User-Adapted Interaction,</title>
<date>2007</date>
<marker>George, Zukerman, Niemann, 2007</marker>
<rawString>Sarah George, Ingrid Zukerman, and Michael Niemann. 2007. Inferences, suppositions and explanatory extensions in argument interpretation. User Modeling and User-Adapted Interaction, 17(5):439–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
</authors>
<title>Automatic detection of causal relations for question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL 2003 Workshop on Multilingual Summarization and Question Answering,</booktitle>
<pages>76--83</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="16938" citStr="Girju, 2003" startWordPosition="2676" endWordPosition="2677">bed below. For each argument A, a vector CV = {c1, c2, c3} is added to its feature set, where each ci indicates the “confidence” of the existence of the specific features associated with each of the first three schemes, schemei. This is defined in Equation 1: (P (schemei|cpk) · dik) (1) Argument from example 8 keywords and phrases including for example, such as, for instance, etc.; 3 punctuation cues: “:”, “;”, and “—”. Argument from cause to effect 22 keywords and simple cue phrases including result, related to, lead to, etc.; 10 causal and noncausal relation patterns extracted from WordNet (Girju, 2003). Practical reasoning 28 keywords and phrases including want, aim, objective, etc.; 4 modal verbs: should, could, must, and need; 4 patterns including imperatives and infinitives indicating the goal of the speaker. Argument from consequences The counts of positive and negative propositions in the conclusion and premises, calculated from the General Inquirer2. Argument from verbal classification The maximal similarity between the central word pairs extracted from the conclusion and the premise; the counts of copula, expletive, and negative modifier dependency relations returned by the Stanford </context>
</contexts>
<marker>Girju, 2003</marker>
<rawString>Roxana Girju. 2003. Automatic detection of causal relations for question answering. In Proceedings of the ACL 2003 Workshop on Multilingual Summarization and Question Answering, pages 76–83, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDD Explorations Newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="20079" citStr="Hall et al., 2009" startWordPosition="3180" endWordPosition="3183">if a word or phrase is the dependent of the dependency relation agent, it is therefore considered as a “central word”. In addition, an arguer tends to use several particular syntactic structures (copula, expletive, and negative modifier) when using this scheme, which can be quantified by the counts of those special relations in the conclusion and the premise(s). 5 Experiments 5.1 Training We experiment with two kinds of classification: oneagainst-others and pairwise. We build a pruned C4.5 decision tree (Quinlan, 1993) for each different classification setup, implemented by Weka Toolkit 3.65 (Hall et al., 2009). One-against-others classification A one-againstothers classifier is constructed for each of the five most frequent schemes, using the general features and the scheme-specific features for the scheme of interest. For each classifier, there are two possible outcomes: target scheme and other; 50% of the training dataset is arguments associated with target scheme, while the rest is arguments of all the other schemes, which are treated as other. Oneagainst-other classification thus tests the effective5http://cs.waikato.ac.nz/ml/weka ness of each scheme’s specific features. Pairwise classification</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: an update. SIGKDD Explorations Newsletter, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In International Conference Research on Computational Linguistics (ROCLING X),</booktitle>
<pages>pages</pages>
<contexts>
<context position="19051" citStr="Jiang and Conrath, 1997" startWordPosition="3010" endWordPosition="3013">t found in A. 1 N ci = mi z k=1 991 For argumentfrom consequences, since the arguer has an obvious preference for some particular consequence, sentiment orientation can be a good indicator for this scheme, which is quantified by the counts of positive and negative propositions in the conclusion and premise. For argument from verbal classification, there exists a hypernymy-like relation between some pair of propositions (entities, concepts, or actions) located in the conclusion and the premise respectively. The existence of such a relation is quantified by the maximal Jiang-Conrath Similarity (Jiang and Conrath, 1997) between the “central word” pairs extracted from the conclusion and the premise. We parse each sentence of the argument with the Stanford dependency parser, and a word or phrase is considered to be a central word if it is the dependent or governor of several particular dependency relations, which basically represents the attribute or the action of an entity in a sentence, or the entity itself. For example, if a word or phrase is the dependent of the dependency relation agent, it is therefore considered as a “central word”. In addition, an arguer tends to use several particular syntactic struct</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In International Conference Research on Computational Linguistics (ROCLING X), pages 19–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Katzav</author>
<author>Chris Reed</author>
</authors>
<title>On argumentation schemes and the natural classification of arguments.</title>
<date>2004</date>
<journal>Argumentation,</journal>
<volume>18</volume>
<issue>2</issue>
<marker>Katzav, Reed, 2004</marker>
<rawString>Joel Katzav and Chris Reed. 2004. On argumentation schemes and the natural classification of arguments. Argumentation, 18(2):239–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raquel Mochales</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Study on the structure of argumentation in case law.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Legal Knowledge and Information Systems,</booktitle>
<pages>11--20</pages>
<publisher>IOS Press.</publisher>
<location>Amsterdam, The Netherlands.</location>
<marker>Mochales, Moens, 2008</marker>
<rawString>Raquel Mochales and Marie-Francine Moens. 2008. Study on the structure of argumentation in case law. In Proceedings of the 2008 Conference on Legal Knowledge and Information Systems, pages 11–20, Amsterdam, The Netherlands. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raquel Mochales</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Argumentation mining: the detection, classification and structure of arguments in text.</title>
<date>2009</date>
<booktitle>In ICAIL ’09: Proceedings of the 12th International Conference on Artificial Intelligence and Law,</booktitle>
<pages>98--107</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Mochales, Moens, 2009</marker>
<rawString>Raquel Mochales and Marie-Francine Moens. 2009a. Argumentation mining: the detection, classification and structure of arguments in text. In ICAIL ’09: Proceedings of the 12th International Conference on Artificial Intelligence and Law, pages 98–107, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raquel Mochales</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Automatic argumentation detection and its role in law and the semantic web.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Law, Ontologies and the Semantic Web,</booktitle>
<pages>115--129</pages>
<publisher>IOS Press.</publisher>
<location>Amsterdam, The Netherlands.</location>
<marker>Mochales, Moens, 2009</marker>
<rawString>Raquel Mochales and Marie-Francine Moens. 2009b. Automatic argumentation detection and its role in law and the semantic web. In Proceedings of the 2009 Conference on Law, Ontologies and the Semantic Web, pages 115–129, Amsterdam, The Netherlands. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Francine Moens</author>
<author>Erik Boiy</author>
<author>Raquel Mochales Palau</author>
<author>Chris Reed</author>
</authors>
<title>Automatic detection of arguments in legal texts.</title>
<date>2007</date>
<booktitle>In ICAIL ’07: Proceedings of the 11th International Conference on Artificial Intelligence and Law,</booktitle>
<pages>225--230</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Moens, Boiy, Palau, Reed, 2007</marker>
<rawString>Marie-Francine Moens, Erik Boiy, Raquel Mochales Palau, and Chris Reed. 2007. Automatic detection of arguments in legal texts. In ICAIL ’07: Proceedings of the 11th International Conference on Artificial Intelligence and Law, pages 225–230, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John L Pollock</author>
</authors>
<title>Cognitive Carpentry: A Blueprint for How to Build a Person. Bradford Books.</title>
<date>1995</date>
<publisher>The MIT Press,</publisher>
<marker>Pollock, 1995</marker>
<rawString>John L. Pollock. 1995. Cognitive Carpentry: A Blueprint for How to Build a Person. Bradford Books. The MIT Press, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>C4.5: Programs for machine learning.</title>
<date>1993</date>
<booktitle>Machine Learning,</booktitle>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="19985" citStr="Quinlan, 1993" startWordPosition="3168" endWordPosition="3169">he attribute or the action of an entity in a sentence, or the entity itself. For example, if a word or phrase is the dependent of the dependency relation agent, it is therefore considered as a “central word”. In addition, an arguer tends to use several particular syntactic structures (copula, expletive, and negative modifier) when using this scheme, which can be quantified by the counts of those special relations in the conclusion and the premise(s). 5 Experiments 5.1 Training We experiment with two kinds of classification: oneagainst-others and pairwise. We build a pruned C4.5 decision tree (Quinlan, 1993) for each different classification setup, implemented by Weka Toolkit 3.65 (Hall et al., 2009). One-against-others classification A one-againstothers classifier is constructed for each of the five most frequent schemes, using the general features and the scheme-specific features for the scheme of interest. For each classifier, there are two possible outcomes: target scheme and other; 50% of the training dataset is arguments associated with target scheme, while the rest is arguments of all the other schemes, which are treated as other. Oneagainst-other classification thus tests the effective5ht</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. Ross Quinlan. 1993. C4.5: Programs for machine learning. Machine Learning, 16(3):235–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Reed</author>
<author>Glenn Rowe</author>
</authors>
<title>Araucaria: Software for argument analysis, diagramming and representation.</title>
<date>2004</date>
<journal>International Journal of Artificial Intelligence Tools,</journal>
<pages>14--961</pages>
<contexts>
<context position="4258" citStr="Reed and Rowe, 2004" startWordPosition="660" endWordPosition="663">interest for many years. Cohen (1987) presented a computational model of argumentative discourse. Dick (1987; 1991a; 1991b) developed a representation for retrieval of judicial decisions by the structure of their legal argument — a necessity for finding legal precedents independent of their domain. However, at that time no corpus of arguments was available, so Dick’s system was purely theoretical. Recently, the Araucaria project at University of Dundee has developed a software tool for manual argument analysis, with a point-and-click interface for users to reconstruct and diagram an argument (Reed and Rowe, 2004; Rowe and Reed, 2008). The project also maintains an online repository, called AraucariaDB, of marked-up naturally occurring arguments collected by annotators worldwide, which can be used as an experimental corpus for automatic argumentation analysis (for details see Section 3.2). Recent work on argument interpretation includes that of George, Zukerman, and Nieman (2007), who interpret constructed-example arguments (not naturally occurring text) as Bayesian networks. Other contemporary research has looked at the automatic detection of arguments in text and the classification of premises and c</context>
</contexts>
<marker>Reed, Rowe, 2004</marker>
<rawString>Chris Reed and Glenn Rowe. 2004. Araucaria: Software for argument analysis, diagramming and representation. International Journal of Artificial Intelligence Tools, 14:961–980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Rowe</author>
<author>Chris Reed</author>
</authors>
<title>Argument diagramming: The Araucaria project.</title>
<date>2008</date>
<booktitle>In Knowledge Cartography,</booktitle>
<pages>163--181</pages>
<publisher>Springer</publisher>
<location>London.</location>
<contexts>
<context position="4280" citStr="Rowe and Reed, 2008" startWordPosition="664" endWordPosition="667">rs. Cohen (1987) presented a computational model of argumentative discourse. Dick (1987; 1991a; 1991b) developed a representation for retrieval of judicial decisions by the structure of their legal argument — a necessity for finding legal precedents independent of their domain. However, at that time no corpus of arguments was available, so Dick’s system was purely theoretical. Recently, the Araucaria project at University of Dundee has developed a software tool for manual argument analysis, with a point-and-click interface for users to reconstruct and diagram an argument (Reed and Rowe, 2004; Rowe and Reed, 2008). The project also maintains an online repository, called AraucariaDB, of marked-up naturally occurring arguments collected by annotators worldwide, which can be used as an experimental corpus for automatic argumentation analysis (for details see Section 3.2). Recent work on argument interpretation includes that of George, Zukerman, and Nieman (2007), who interpret constructed-example arguments (not naturally occurring text) as Bayesian networks. Other contemporary research has looked at the automatic detection of arguments in text and the classification of premises and conclusions. The work c</context>
</contexts>
<marker>Rowe, Reed, 2008</marker>
<rawString>Glenn Rowe and Chris Reed. 2008. Argument diagramming: The Araucaria project. In Knowledge Cartography, pages 163–181. Springer London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frans H van Eemeren</author>
<author>Rob Grootendorst</author>
</authors>
<title>Argumentation, Communication, and Fallacies: A Pragma-Dialectical Perspective.</title>
<date>1992</date>
<publisher>Routledge.</publisher>
<marker>van Eemeren, Grootendorst, 1992</marker>
<rawString>Frans H. van Eemeren and Rob Grootendorst. 1992. Argumentation, Communication, and Fallacies: A Pragma-Dialectical Perspective. Routledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Walton</author>
<author>Chris Reed</author>
</authors>
<title>Argumentation schemes and defeasible inferences.</title>
<date>2002</date>
<booktitle>In Workshop on Computational Models of Natural Argument, 15th European Conference on Artificial Intelligence,</booktitle>
<pages>11--20</pages>
<publisher>IOS Press.</publisher>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="6077" citStr="Walton and Reed, 2002" startWordPosition="934" endWordPosition="937">. In addition, their context-free grammar for argumentation structure parsing obtained around 60% accuracy. Our work is “downstream” from that of Mochales and Moens. Assuming the eventual success of their, or others’, research program on detecting and classifying the components of an argument, we seek to 3 Argumentation schemes, scheme-sets, and annotation 3.1 Definition and examples Argumentation schemes are structures or templates for forms of arguments. The arguments need not be deductive or inductive; on the contrary, most argumentation schemes are for presumptive or defeasible arguments (Walton and Reed, 2002). For example, argument from cause to effect is a commonly used scheme in everyday arguments. A list of such argumentation schemes is called a scheme-set. It has been shown that argumentation schemes are useful in evaluating common arguments as fallacious or not (van Eemeren and Grootendorst, 1992). In order to judge the weakness of an argument, a set of critical questions are asked according to the particular scheme that the argument is using, and the argument is regarded as valid if it matches all the requirements imposed by the scheme. Walton’s set of 65 argumentation schemes (Walton et al.</context>
</contexts>
<marker>Walton, Reed, 2002</marker>
<rawString>Douglas Walton and Chris Reed. 2002. Argumentation schemes and defeasible inferences. In Workshop on Computational Models of Natural Argument, 15th European Conference on Artificial Intelligence, pages 11–20, Amsterdam, The Netherlands. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Walton</author>
<author>Chris Reed</author>
<author>Fabrizio Macagno</author>
</authors>
<title>Argumentation Schemes.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6684" citStr="Walton et al., 2008" startWordPosition="1037" endWordPosition="1041">d Reed, 2002). For example, argument from cause to effect is a commonly used scheme in everyday arguments. A list of such argumentation schemes is called a scheme-set. It has been shown that argumentation schemes are useful in evaluating common arguments as fallacious or not (van Eemeren and Grootendorst, 1992). In order to judge the weakness of an argument, a set of critical questions are asked according to the particular scheme that the argument is using, and the argument is regarded as valid if it matches all the requirements imposed by the scheme. Walton’s set of 65 argumentation schemes (Walton et al., 2008) is one of the best-developed schemesets in argumentation theory. The five schemes defined in Table 1 are the most commonly used ones, and they are the focus of the scheme classification system that we will describe in this paper. 3.2 Araucaria dataset One of the challenges for automatic argumentation analysis is that suitable annotated corpora are still very rare, in spite of work by many researchers. In the work described here, we use the Araucaria database1, an online repository of arguments, as our experimental dataset. Araucaria includes approximately 660 manually annotated arguments from</context>
</contexts>
<marker>Walton, Reed, Macagno, 2008</marker>
<rawString>Douglas Walton, Chris Reed, and Fabrizio Macagno. 2008. Argumentation Schemes. Cambridge University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>