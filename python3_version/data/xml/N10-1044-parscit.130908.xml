<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014580">
<title confidence="0.964242">
Time-Efficient Creation of an Accurate Sentence Fusion Corpus
</title>
<author confidence="0.999404">
Kathleen McKeown, Sara Rosenthal, Kapil Thadani and Coleman Moore
</author>
<affiliation confidence="0.997925">
Columbia University
</affiliation>
<address confidence="0.97445">
New York, NY 10027, USA
</address>
<email confidence="0.999233">
{kathy,sara,kapil}@cs.columbia.edu, cjm2140@columbia.edu
</email>
<sectionHeader confidence="0.993891" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999717285714286">
Sentence fusion enables summarization and
question-answering systems to produce out-
put by combining fully formed phrases from
different sentences. Yet there is little data
that can be used to develop and evaluate fu-
sion techniques. In this paper, we present a
methodology for collecting fusions of simi-
lar sentence pairs using Amazon’s Mechani-
cal Turk, selecting the input pairs in a semi-
automated fashion. We evaluate the results
using a novel technique for automatically se-
lecting a representative sentence from multi-
ple responses. Our approach allows for rapid
construction of a high accuracy fusion corpus.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999825169811321">
Summarization and question-answering systems
must transform input text to produce useful output
text, condensing an input document or document set
in the case of summarization and selecting text that
meets the question constraints in the case of question
answering. While many systems use sentence ex-
traction to facilitate the task, this approach risks in-
cluding additional, irrelevant or non-salient informa-
tion in the output, and the original sentence wording
may be inappropriate for the new context in which
it appears. Instead, recent research has investigated
methods for generating new sentences using a tech-
nique called sentence fusion (Barzilay and McKe-
own, 2005; Marsi and Krahmer, 2005; Filippova and
Strube, 2008) where output sentences are generated
by fusing together portions of related sentences.
While algorithms for automated fusion have been
developed, there is no corpus of human-generated
fused sentences available to train and evaluate such
systems. The creation of such a dataset could pro-
vide insight into the kinds of fusions that people
produce. Furthermore, since research in the related
task of sentence compression has benefited from
the availability of training data (Jing, 2000; Knight
and Marcu, 2002; McDonald, 2006; Cohn and Za-
pata, 2008), we expect that the creation of this cor-
pus might encourage the development of supervised
learning techniques for automated sentence fusion.
In this work, we present a methodology for cre-
ating such a corpus using Amazon’s Mechanical
Turk1, a widely used online marketplace for crowd-
sourced task completion. Our goal is the generation
of accurate fusions between pairs of sentences that
have some information in common. To ensure that
the task is performed consistently, we abide by the
distinction proposed by Marsi and Krahmer (2005)
between intersection fusion and union fusion. In-
tersection fusion results in a sentence that contains
only the information that the sentences had in com-
mon and is usually shorter than either of the original
sentences. Union fusion, on the other hand, results
in a sentence that contains all information content
from the original two sentences. An example of in-
tersection and union fusion is shown in Figure 1.
We solicit multiple annotations for both union and
intersection tasks separately and leverage the differ-
ent responses to automatically choose a representa-
tive response. Analysis of the responses shows that
our approach yields 95% accuracy on the task of
union fusion. This is a promising first step and indi-
cates that our methodology can be applied towards
efficiently building a highly accurate corpus for sen-
tence fusion.
</bodyText>
<footnote confidence="0.998151">
1https://www.mturk.com
</footnote>
<page confidence="0.909533">
317
</page>
<affiliation confidence="0.190114">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 317–320,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</affiliation>
<listItem confidence="0.998259666666667">
1. Palin actually turned against the bridge project only after it
became a national symbol of wasteful spending.
2. Ms. Palin supported the bridge project while running for
governor, and abandoned it after it became a national scandal.
Intersection: Palin turned against the bridge project after it
became a national scandal.
</listItem>
<bodyText confidence="0.692340333333333">
Union: Ms. Palin supported the bridge project while running
for governor, but turned against it when it became a national
scandal and a symbol of wasteful spending.
</bodyText>
<figureCaption confidence="0.999148">
Figure 1: Examples of intersection and union
</figureCaption>
<sectionHeader confidence="0.999232" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999958194444445">
The combination of fragments of sentences on a
common topic has been studied in the domain of sin-
gle document summarization (Jing, 2000; Daum´e III
and Marcu, 2002; Xie et al., 2008). In contrast to
these approaches, sentence fusion was introduced to
combine fragments of sentences with common infor-
mation for multi-document summarization (Barzilay
and McKeown, 2005). Automated fusion of sen-
tence pairs has since received attention as an inde-
pendent task (Marsi and Krahmer, 2005; Filippova
and Strube, 2008). Although generic fusion of sen-
tence pairs based on importance does not yield high
agreement when performed by humans (Daum´e III
and Marcu, 2004), fusion in the context of a query
has been shown to produce better agreement (Krah-
mer et al., 2008). We examine similar fusion an-
notation tasks in this paper, but we asked workers
to provide two specific types of fusion, intersection
and union, thus avoiding the less specific definition
based on importance. Furthermore, as our goal is
the generation of corpora, our target for evaluation
is accuracy rather than agreement.
This work studies an approach to the automatic
construction of large fusion corpora using workers
through Amazon’s Mechanical Turk service. Previ-
ous studies using this online task marketplace have
shown that the collective judgments of many work-
ers are comparable to those of trained annotators
on labeling tasks (Snow et al., 2008) although these
judgments can be obtained at a fraction of the cost
and effort. However, our task presents an additional
challenge: building a corpus for sentence fusion re-
quires workers to enter free text rather than simply
choose between predefined options; the results are
prone to variation and this makes comparing and ag-
gregating multiple responses problematic.
</bodyText>
<figure confidence="0.706702333333333">
A. After a decade on the job, Gordon had become an experi-
enced cop.
B. Gordon has a lot of experience in the police force.
</figure>
<figureCaption confidence="0.924789">
Figure 2: An example of sentences that were judged to be
too similar for inclusion in the dataset
</figureCaption>
<sectionHeader confidence="0.936122" genericHeader="method">
3 Collection Methodology
</sectionHeader>
<bodyText confidence="0.999927585365853">
Data collection involved the identification of the
types of sentence pairs that would make suitable
candidates for fusion, the development of a sys-
tem to automatically identify good pairs and manual
filtering of the sentence pairs to remove erroneous
choices. The selected sentence pairs were then pre-
sented to workers on Mechanical Turk in an inter-
face that required them to manually type in a fused
sentence (intersection or union) for each case.
Not all pairs of related sentences are useful for the
fusion task. When sentences are too similar, the re-
sult of fusion is simply one of the input sentences.
For example (Fig. 2), if sentence A contains all the
information in sentence B but not vice versa, then
B is also their intersection while A is their union
and no sentence generation is required. On the other
hand, if the two sentences are too dissimilar, then
no intersection is possible and the union is just the
conjunction of the sentences.
We experimented with different similarity metrics
aimed at identifying pairs of sentences that were in-
appropriate for fusion. The sentences in this study
were drawn from clusters of news articles on the
same event from the Newsblaster summarization
system (McKeown et al., 2002). While these clus-
ters are likely to contain similar sentences, they will
contain many more dissimilar than similar pairs and
thus a metric that emphasizes precision over recall
is important. We computed pairwise similarity be-
tween sentences within each cluster using three stan-
dard metrics: word overlap, n-gram overlap and co-
sine similarity. Bigram overlap yielded the best pre-
cision in our experiments. We empirically arrived at
a lower threshold of .35 to remove dissimilar sen-
tences and an upper threshold of .65 to avoid near-
identical sentences, yielding a false-positive rate of
44.4%. The remaining inappropriate pairs were then
manually filtered. This semi-automated procedure
enabled fast selection of suitable sentence pairs: one
person was able to select 30 pairs an hour yielding
the 300 pairs for the full experiment in ten hours.
</bodyText>
<page confidence="0.988843">
318
</page>
<table confidence="0.996683333333333">
Responses Intersection Union
All (1500) 0.49 0.88
Representatives (300) 0.54 0.95
</table>
<tableCaption confidence="0.999555">
Table 1: Union and intersection accuracy
</tableCaption>
<subsectionHeader confidence="0.999013">
3.1 Using Amazon’s Mechanical Turk
</subsectionHeader>
<bodyText confidence="0.9997562">
Based on a pilot study with 20 sentence pairs, we
designed an interface for the full study. For inter-
section tasks, the interface posed the question “How
would you combine the following two sentences into
a single sentence conveying only the information
they have in common?”. For union tasks, the ques-
tion was “How would you combine the following two
sentences into a single sentence that contains ALL of
the information in each?”.
We used all 300 pairs of similar sentences for
both union and intersection and chose to collect five
worker responses per pair, given the diversity of
responses that we found in the pilot study. This
yielded a total of 3000 fused sentences with 1500
intersections and 1500 unions.
</bodyText>
<subsectionHeader confidence="0.997794">
3.2 Representative Responses
</subsectionHeader>
<bodyText confidence="0.999946173913043">
Using multiple workers provides little benefit unless
we are able to harness the collective judgments of
their responses. To this end, we experiment with
a simple technique to select one representative re-
sponse from all responses for a case, hypothesizing
that such a response would have a lower error rate.
We test the hypothesis by comparing the accuracy of
representative responses with the average accuracy
over all responses.
Our strategy for selecting representatives draws
on the common assumption used in human com-
putation that human agreement in independently-
generated labels implies accuracy (von Ahn and
Dabbish, 2004). We approximate agreement be-
tween responses using a simple and transparent
measure for overlap: cosine similarity over stems
weighted by tf-idf where idf values are learned over
the Gigawords corpus2. After comparing all re-
sponses in a pairwise fashion, we need to choose a
representative response. As using the centroid di-
rectly might not be robust to the presence of er-
roneous responses, we first select the pair of re-
sponses with the greatest overlap as candidates and
</bodyText>
<footnote confidence="0.572676">
2LDC Catalog No. LDC2003T05
</footnote>
<table confidence="0.9987975">
Errors Intersection Union
Missing clause 2 7
Union/Intersection 46 6
S1/S2 21 8
Additional clause 10 1
Lexical 3 1
</table>
<tableCaption confidence="0.999523">
Table 2: Errors seen in 30 random cases (150 responses)
</tableCaption>
<bodyText confidence="0.9940145">
then choose the candidate which has the greatest to-
tal overlap with all other responses.
</bodyText>
<sectionHeader confidence="0.998886" genericHeader="evaluation">
4 Results and Error Analysis
</sectionHeader>
<bodyText confidence="0.999985694444445">
For evaluating accuracy, fused sentences were man-
ually compared to the original sentence pairs. Due to
the time-consuming nature of the evaluation, 50% of
the 300 cases were randomly selected for analysis.
10% were initially analyzed by two of the authors; if
a disagreement occurred, the authors discussed their
differences and came to a unified decision. The re-
maining 40% were then analyzed by one author. In
addition to this high-level analysis, we further ana-
lyzed 10% of the cases to identify the the types of
errors made in fusion as well as the techniques used
and the effect of task difficulty on performance.
The accuracy for intersection and union tasks is
shown in Table 1. For both tasks, accuracy of the se-
lected representatives significantly exceeded the av-
erage response accuracy. In our error analysis, we
found that workers often answered the intersection
task by providing a union, possibly due to a misin-
terpretation of the question. This caused intersection
accuracy to be significantly worse than union. We
analyzed the impact of this error by computing ac-
curacy on the first 30 cases (10%) without this error
and the accuracy for intersection increased 22%.
Error types were categorized as “missing clause”,
“using union for intersection and vice versa”,
“choosing an input sentence (S1/S2)”, “additional
clause” and “lexical error”. Table 2 shows the num-
ber of occurrences of each in 10% of the cases.
We binned the sentence pairs according to
the difficulty of the fusion task for each pair
(easy/medium/hard) and found that performance
was not dependent on difficulty level; accuracy was
relatively similar across bins. We also observed that
workers typically performed fusion by selecting one
sentence as a base and removing clauses or merging
in additional clauses from the other sentence.
</bodyText>
<page confidence="0.998871">
319
</page>
<figureCaption confidence="0.9879035">
Figure 3: Number of cases in which x/5 workers pro-
vided accurate responses for fusion
</figureCaption>
<bodyText confidence="0.999984545454546">
In order to determine the benefit of using many
workers, we studied the number of workers who an-
swered correctly for each case. Figure 3 reveals that
2/5 or more workers (summing across columns) re-
sponded accurately in 99% of union cases and 82%
of intersection cases. The intersection results are
skewed due to the question misinterpretation issue
which, though it was the most common error, was
made by 3/5 workers only 17% of the time. Thus, in
the majority of the cases, accurate fusions can still
be found using the representative method.
</bodyText>
<sectionHeader confidence="0.998835" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999998333333333">
We presented a methodology to build a fusion cor-
pus which uses semi-automated techniques to select
similar sentence pairs for annotation on Mechanical
Turk3. Additionally, we showed how multiple re-
sponses for each fusion task can be leveraged by au-
tomatically selecting a representative response. Our
approach yielded 95% accuracy for union tasks, and
while intersection fusion accuracy was much lower,
our analysis showed that workers sometimes pro-
vided unions instead of intersections and we sus-
pect that an improved formulation of the question
could lead to better results. Construction of the fu-
sion dataset was relatively fast; it required only ten
hours of labor on the part of a trained undergraduate
and seven days of active time on Mechanical Turk.
</bodyText>
<sectionHeader confidence="0.996514" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999517333333333">
This material is based on research supported in part
by the U.S. National Science Foundation (NSF) un-
der IIS-05-34871 Any opinions, findings and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not necessarily
reflect the views of the NSF.
</bodyText>
<footnote confidence="0.9488685">
3The corpus described in this work is available at
http://www.cs.columbia.edu/∼kathy/fusioncorpus
</footnote>
<sectionHeader confidence="0.900545" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999625423076923">
Regina Barzilay and Kathleen R. McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297–328.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
COLING, pages 137–144.
Hal Daum´e III and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of
ACL, pages 449–456.
Hal Daum´e III and Daniel Marcu. 2004. Generic sen-
tence fusion is an ill-defined summarization task. In
Proceedings of the ACL Text Summarization Branches
Out Workshop, pages 96–103.
Katja Filippova and Michael Strube. 2008. Sentence fu-
sion via dependency graph compression. In Proceed-
ings of EMNLP, pages 177–185.
Hongyan Jing. 2000. Sentence reduction for automatic
text summarization. In Proceedings of Applied Natu-
ral Language Processing, pages 310–315.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139(1):91–107.
Emiel Krahmer, Erwin Marsi, and Paul van Pelt. 2008.
Query-based sentence fusion is better defined and
leads to more preferred results than generic sentence
fusion. In Proceedings of ACL, pages 193–196.
Erwin Marsi and Emiel Krahmer. 2005. Explorations in
sentence fusion. In Proceedings of the European Work-
shopon Natural Language Generation, pages 109–117.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL, pages 297–304.
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news on
a daily basis with Columbia’s Newsblaster. In Pro-
ceedings of HLT, pages 280–285.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast—but is it
good?: Evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of EMNLP, pages
254–263.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of the
SIGCHI conference on Human Factors in Computing
Systems, pages 319–326.
Zhuli Xie, Barbara Di Eugenio, and Peter C. Nel-
son. 2008. From extracting to abstracting: Gener-
ating quasi-abstractive summaries. In Proceedings of
LREC, May.
</reference>
<page confidence="0.998255">
320
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.487054">
<title confidence="0.975635">Time-Efficient Creation of an Accurate Sentence Fusion Corpus</title>
<author confidence="0.741869">Sara Rosenthal McKeown</author>
<author confidence="0.741869">Kapil Thadani</author>
<affiliation confidence="0.560892">Columbia</affiliation>
<address confidence="0.953779">New York, NY 10027,</address>
<email confidence="0.99435">cjm2140@columbia.edu</email>
<abstract confidence="0.999565333333333">Sentence fusion enables summarization and question-answering systems to produce output by combining fully formed phrases from different sentences. Yet there is little data that can be used to develop and evaluate fusion techniques. In this paper, we present a methodology for collecting fusions of similar sentence pairs using Amazon’s Mechanical Turk, selecting the input pairs in a semiautomated fashion. We evaluate the results using a novel technique for automatically selecting a representative sentence from multiple responses. Our approach allows for rapid construction of a high accuracy fusion corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Sentence fusion for multidocument news summarization.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="1535" citStr="Barzilay and McKeown, 2005" startWordPosition="221" endWordPosition="225">must transform input text to produce useful output text, condensing an input document or document set in the case of summarization and selecting text that meets the question constraints in the case of question answering. While many systems use sentence extraction to facilitate the task, this approach risks including additional, irrelevant or non-salient information in the output, and the original sentence wording may be inappropriate for the new context in which it appears. Instead, recent research has investigated methods for generating new sentences using a technique called sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008) where output sentences are generated by fusing together portions of related sentences. While algorithms for automated fusion have been developed, there is no corpus of human-generated fused sentences available to train and evaluate such systems. The creation of such a dataset could provide insight into the kinds of fusions that people produce. Furthermore, since research in the related task of sentence compression has benefited from the availability of training data (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Cohn and Zapata, 2008</context>
<context position="4649" citStr="Barzilay and McKeown, 2005" startWordPosition="708" endWordPosition="711">fter it became a national scandal. Union: Ms. Palin supported the bridge project while running for governor, but turned against it when it became a national scandal and a symbol of wasteful spending. Figure 1: Examples of intersection and union 2 Related Work The combination of fragments of sentences on a common topic has been studied in the domain of single document summarization (Jing, 2000; Daum´e III and Marcu, 2002; Xie et al., 2008). In contrast to these approaches, sentence fusion was introduced to combine fragments of sentences with common information for multi-document summarization (Barzilay and McKeown, 2005). Automated fusion of sentence pairs has since received attention as an independent task (Marsi and Krahmer, 2005; Filippova and Strube, 2008). Although generic fusion of sentence pairs based on importance does not yield high agreement when performed by humans (Daum´e III and Marcu, 2004), fusion in the context of a query has been shown to produce better agreement (Krahmer et al., 2008). We examine similar fusion annotation tasks in this paper, but we asked workers to provide two specific types of fusion, intersection and union, thus avoiding the less specific definition based on importance. F</context>
</contexts>
<marker>Barzilay, McKeown, 2005</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2005. Sentence fusion for multidocument news summarization. Computational Linguistics, 31(3):297–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Sentence compression beyond word deletion.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>137--144</pages>
<marker>Cohn, Lapata, 2008</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2008. Sentence compression beyond word deletion. In Proceedings of COLING, pages 137–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>A noisy-channel model for document compression.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>449--456</pages>
<marker>Daum´e, Marcu, 2002</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2002. A noisy-channel model for document compression. In Proceedings of ACL, pages 449–456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Generic sentence fusion is an ill-defined summarization task.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL Text Summarization Branches Out Workshop,</booktitle>
<pages>96--103</pages>
<marker>Daum´e, Marcu, 2004</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2004. Generic sentence fusion is an ill-defined summarization task. In Proceedings of the ACL Text Summarization Branches Out Workshop, pages 96–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Sentence fusion via dependency graph compression.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>177--185</pages>
<contexts>
<context position="1589" citStr="Filippova and Strube, 2008" startWordPosition="230" endWordPosition="233">xt, condensing an input document or document set in the case of summarization and selecting text that meets the question constraints in the case of question answering. While many systems use sentence extraction to facilitate the task, this approach risks including additional, irrelevant or non-salient information in the output, and the original sentence wording may be inappropriate for the new context in which it appears. Instead, recent research has investigated methods for generating new sentences using a technique called sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008) where output sentences are generated by fusing together portions of related sentences. While algorithms for automated fusion have been developed, there is no corpus of human-generated fused sentences available to train and evaluate such systems. The creation of such a dataset could provide insight into the kinds of fusions that people produce. Furthermore, since research in the related task of sentence compression has benefited from the availability of training data (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Cohn and Zapata, 2008), we expect that the creation of this corpus might en</context>
<context position="4791" citStr="Filippova and Strube, 2008" startWordPosition="731" endWordPosition="734">ame a national scandal and a symbol of wasteful spending. Figure 1: Examples of intersection and union 2 Related Work The combination of fragments of sentences on a common topic has been studied in the domain of single document summarization (Jing, 2000; Daum´e III and Marcu, 2002; Xie et al., 2008). In contrast to these approaches, sentence fusion was introduced to combine fragments of sentences with common information for multi-document summarization (Barzilay and McKeown, 2005). Automated fusion of sentence pairs has since received attention as an independent task (Marsi and Krahmer, 2005; Filippova and Strube, 2008). Although generic fusion of sentence pairs based on importance does not yield high agreement when performed by humans (Daum´e III and Marcu, 2004), fusion in the context of a query has been shown to produce better agreement (Krahmer et al., 2008). We examine similar fusion annotation tasks in this paper, but we asked workers to provide two specific types of fusion, intersection and union, thus avoiding the less specific definition based on importance. Furthermore, as our goal is the generation of corpora, our target for evaluation is accuracy rather than agreement. This work studies an approa</context>
</contexts>
<marker>Filippova, Strube, 2008</marker>
<rawString>Katja Filippova and Michael Strube. 2008. Sentence fusion via dependency graph compression. In Proceedings of EMNLP, pages 177–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Sentence reduction for automatic text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of Applied Natural Language Processing,</booktitle>
<pages>310--315</pages>
<contexts>
<context position="2072" citStr="Jing, 2000" startWordPosition="305" endWordPosition="306">nces using a technique called sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008) where output sentences are generated by fusing together portions of related sentences. While algorithms for automated fusion have been developed, there is no corpus of human-generated fused sentences available to train and evaluate such systems. The creation of such a dataset could provide insight into the kinds of fusions that people produce. Furthermore, since research in the related task of sentence compression has benefited from the availability of training data (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Cohn and Zapata, 2008), we expect that the creation of this corpus might encourage the development of supervised learning techniques for automated sentence fusion. In this work, we present a methodology for creating such a corpus using Amazon’s Mechanical Turk1, a widely used online marketplace for crowdsourced task completion. Our goal is the generation of accurate fusions between pairs of sentences that have some information in common. To ensure that the task is performed consistently, we abide by the distinction proposed by Marsi and Krahmer (2005) </context>
<context position="4417" citStr="Jing, 2000" startWordPosition="676" endWordPosition="677">ational symbol of wasteful spending. 2. Ms. Palin supported the bridge project while running for governor, and abandoned it after it became a national scandal. Intersection: Palin turned against the bridge project after it became a national scandal. Union: Ms. Palin supported the bridge project while running for governor, but turned against it when it became a national scandal and a symbol of wasteful spending. Figure 1: Examples of intersection and union 2 Related Work The combination of fragments of sentences on a common topic has been studied in the domain of single document summarization (Jing, 2000; Daum´e III and Marcu, 2002; Xie et al., 2008). In contrast to these approaches, sentence fusion was introduced to combine fragments of sentences with common information for multi-document summarization (Barzilay and McKeown, 2005). Automated fusion of sentence pairs has since received attention as an independent task (Marsi and Krahmer, 2005; Filippova and Strube, 2008). Although generic fusion of sentence pairs based on importance does not yield high agreement when performed by humans (Daum´e III and Marcu, 2004), fusion in the context of a query has been shown to produce better agreement (</context>
</contexts>
<marker>Jing, 2000</marker>
<rawString>Hongyan Jing. 2000. Sentence reduction for automatic text summarization. In Proceedings of Applied Natural Language Processing, pages 310–315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Summarization beyond sentence extraction: a probabilistic approach to sentence compression.</title>
<date>2002</date>
<journal>Artificial Intelligence,</journal>
<volume>139</volume>
<issue>1</issue>
<contexts>
<context position="2096" citStr="Knight and Marcu, 2002" startWordPosition="307" endWordPosition="310"> technique called sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008) where output sentences are generated by fusing together portions of related sentences. While algorithms for automated fusion have been developed, there is no corpus of human-generated fused sentences available to train and evaluate such systems. The creation of such a dataset could provide insight into the kinds of fusions that people produce. Furthermore, since research in the related task of sentence compression has benefited from the availability of training data (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Cohn and Zapata, 2008), we expect that the creation of this corpus might encourage the development of supervised learning techniques for automated sentence fusion. In this work, we present a methodology for creating such a corpus using Amazon’s Mechanical Turk1, a widely used online marketplace for crowdsourced task completion. Our goal is the generation of accurate fusions between pairs of sentences that have some information in common. To ensure that the task is performed consistently, we abide by the distinction proposed by Marsi and Krahmer (2005) between intersection fus</context>
</contexts>
<marker>Knight, Marcu, 2002</marker>
<rawString>Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: a probabilistic approach to sentence compression. Artificial Intelligence, 139(1):91–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Erwin Marsi</author>
<author>Paul van Pelt</author>
</authors>
<title>Query-based sentence fusion is better defined and leads to more preferred results than generic sentence fusion.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>193--196</pages>
<marker>Krahmer, Marsi, van Pelt, 2008</marker>
<rawString>Emiel Krahmer, Erwin Marsi, and Paul van Pelt. 2008. Query-based sentence fusion is better defined and leads to more preferred results than generic sentence fusion. In Proceedings of ACL, pages 193–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erwin Marsi</author>
<author>Emiel Krahmer</author>
</authors>
<title>Explorations in sentence fusion.</title>
<date>2005</date>
<booktitle>In Proceedings of the European Workshopon Natural Language Generation,</booktitle>
<pages>109--117</pages>
<contexts>
<context position="1560" citStr="Marsi and Krahmer, 2005" startWordPosition="226" endWordPosition="229"> produce useful output text, condensing an input document or document set in the case of summarization and selecting text that meets the question constraints in the case of question answering. While many systems use sentence extraction to facilitate the task, this approach risks including additional, irrelevant or non-salient information in the output, and the original sentence wording may be inappropriate for the new context in which it appears. Instead, recent research has investigated methods for generating new sentences using a technique called sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008) where output sentences are generated by fusing together portions of related sentences. While algorithms for automated fusion have been developed, there is no corpus of human-generated fused sentences available to train and evaluate such systems. The creation of such a dataset could provide insight into the kinds of fusions that people produce. Furthermore, since research in the related task of sentence compression has benefited from the availability of training data (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Cohn and Zapata, 2008), we expect that the cre</context>
<context position="4762" citStr="Marsi and Krahmer, 2005" startWordPosition="727" endWordPosition="730">ed against it when it became a national scandal and a symbol of wasteful spending. Figure 1: Examples of intersection and union 2 Related Work The combination of fragments of sentences on a common topic has been studied in the domain of single document summarization (Jing, 2000; Daum´e III and Marcu, 2002; Xie et al., 2008). In contrast to these approaches, sentence fusion was introduced to combine fragments of sentences with common information for multi-document summarization (Barzilay and McKeown, 2005). Automated fusion of sentence pairs has since received attention as an independent task (Marsi and Krahmer, 2005; Filippova and Strube, 2008). Although generic fusion of sentence pairs based on importance does not yield high agreement when performed by humans (Daum´e III and Marcu, 2004), fusion in the context of a query has been shown to produce better agreement (Krahmer et al., 2008). We examine similar fusion annotation tasks in this paper, but we asked workers to provide two specific types of fusion, intersection and union, thus avoiding the less specific definition based on importance. Furthermore, as our goal is the generation of corpora, our target for evaluation is accuracy rather than agreement</context>
</contexts>
<marker>Marsi, Krahmer, 2005</marker>
<rawString>Erwin Marsi and Emiel Krahmer. 2005. Explorations in sentence fusion. In Proceedings of the European Workshopon Natural Language Generation, pages 109–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative sentence compression with soft syntactic evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>297--304</pages>
<contexts>
<context position="2112" citStr="McDonald, 2006" startWordPosition="311" endWordPosition="312">ce fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008) where output sentences are generated by fusing together portions of related sentences. While algorithms for automated fusion have been developed, there is no corpus of human-generated fused sentences available to train and evaluate such systems. The creation of such a dataset could provide insight into the kinds of fusions that people produce. Furthermore, since research in the related task of sentence compression has benefited from the availability of training data (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Cohn and Zapata, 2008), we expect that the creation of this corpus might encourage the development of supervised learning techniques for automated sentence fusion. In this work, we present a methodology for creating such a corpus using Amazon’s Mechanical Turk1, a widely used online marketplace for crowdsourced task completion. Our goal is the generation of accurate fusions between pairs of sentences that have some information in common. To ensure that the task is performed consistently, we abide by the distinction proposed by Marsi and Krahmer (2005) between intersection fusion and union fu</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>Ryan McDonald. 2006. Discriminative sentence compression with soft syntactic evidence. In Proceedings of EACL, pages 297–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
<author>Regina Barzilay</author>
<author>David Evans</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Judith L Klavans</author>
<author>Ani Nenkova</author>
<author>Carl Sable</author>
<author>Barry Schiffman</author>
<author>Sergey Sigelman</author>
</authors>
<title>Tracking and summarizing news on a daily basis with Columbia’s Newsblaster.</title>
<date>2002</date>
<booktitle>In Proceedings of HLT,</booktitle>
<pages>280--285</pages>
<contexts>
<context position="7540" citStr="McKeown et al., 2002" startWordPosition="1186" endWordPosition="1189">r example (Fig. 2), if sentence A contains all the information in sentence B but not vice versa, then B is also their intersection while A is their union and no sentence generation is required. On the other hand, if the two sentences are too dissimilar, then no intersection is possible and the union is just the conjunction of the sentences. We experimented with different similarity metrics aimed at identifying pairs of sentences that were inappropriate for fusion. The sentences in this study were drawn from clusters of news articles on the same event from the Newsblaster summarization system (McKeown et al., 2002). While these clusters are likely to contain similar sentences, they will contain many more dissimilar than similar pairs and thus a metric that emphasizes precision over recall is important. We computed pairwise similarity between sentences within each cluster using three standard metrics: word overlap, n-gram overlap and cosine similarity. Bigram overlap yielded the best precision in our experiments. We empirically arrived at a lower threshold of .35 to remove dissimilar sentences and an upper threshold of .65 to avoid nearidentical sentences, yielding a false-positive rate of 44.4%. The rem</context>
</contexts>
<marker>McKeown, Barzilay, Evans, Hatzivassiloglou, Klavans, Nenkova, Sable, Schiffman, Sigelman, 2002</marker>
<rawString>Kathleen R. McKeown, Regina Barzilay, David Evans, Vasileios Hatzivassiloglou, Judith L. Klavans, Ani Nenkova, Carl Sable, Barry Schiffman, and Sergey Sigelman. 2002. Tracking and summarizing news on a daily basis with Columbia’s Newsblaster. In Proceedings of HLT, pages 280–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>254--263</pages>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast—but is it good?: Evaluating non-expert annotations for natural language tasks. In Proceedings of EMNLP, pages 254–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>Labeling images with a computer game.</title>
<date>2004</date>
<booktitle>In Proceedings of the SIGCHI conference on Human Factors in Computing Systems,</booktitle>
<pages>319--326</pages>
<marker>von Ahn, Dabbish, 2004</marker>
<rawString>Luis von Ahn and Laura Dabbish. 2004. Labeling images with a computer game. In Proceedings of the SIGCHI conference on Human Factors in Computing Systems, pages 319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhuli Xie</author>
<author>Barbara Di Eugenio</author>
<author>Peter C Nelson</author>
</authors>
<title>From extracting to abstracting: Generating quasi-abstractive summaries.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC,</booktitle>
<marker>Xie, Di Eugenio, Nelson, 2008</marker>
<rawString>Zhuli Xie, Barbara Di Eugenio, and Peter C. Nelson. 2008. From extracting to abstracting: Generating quasi-abstractive summaries. In Proceedings of LREC, May.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>