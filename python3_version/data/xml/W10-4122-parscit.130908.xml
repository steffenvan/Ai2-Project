<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9961955">
Improving Chinese Word Segmentation by Adopting
Self-Organized Maps of Character N-gram
</title>
<author confidence="0.975626">
Chongyang Zhang Zhigang Chen Guoping Hu
</author>
<affiliation confidence="0.893647">
iFLYTEK Research iFLYTEK Research iFLYTEK Research
</affiliation>
<email confidence="0.994467">
cyzhang@iflytek.com zgchen@iflytek.com gphu@iflytek.com
</email>
<sectionHeader confidence="0.997352" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999850416666667">
Character-based tagging method has
achieved great success in Chinese Word
Segmentation (CWS). This paper
proposes a new approach to improve the
CWS tagging accuracy by combining
Self-Organizing Map (SOM) with
structured support vector machine
(SVM) for utilization of enormous
unlabeled text corpus. First, character
N-grams are clustered and mapped into
a low-dimensional space by adopting
SOM algorithm. Two different maps are
built based on the N-gram’s preceding
and succeeding context respectively.
Then new features are extracted from
these maps and integrated into the
structured SVM methods for CWS.
Experimental results on Bakeoff-2005
database show that SOM-based features
can contribute more than 7% relative
error reduction, and the structured SVM
method for CWS proposed in this paper
also outperforms traditional conditional
random field (CRF) method.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99982247826087">
It is well known that there is no space or any
other separators to indicate the word boundary
in Chinese. But word is the basic unit for most
of Chinese natural language process tasks, such
as Machine Translation, Information Extraction,
Text Categorization and so on. As a result,
Chinese word segmentation (CWS) becomes
one of the most fundamental technologies in
Chinese natural language process.
In the last decade, many statistics-based
methods for automatic CWS have been
proposed with development of machine learning
and statistical method (Huang and Zhao, 2007).
Especially, the character-based tagging method
which was proposed by Nianwen Xue (2003)
achieves great success in the second
International Chinese word segmentation
Bakeoff in 2005 (Low et al., 2005). The
character-based tagging method formulates the
CWS problem as a task of predicting a tag for
each character in the sentence, i.e. every
character is considered as one of four different
types in 4-tag set: B (begin of word), M (middle
of word), E (end of word), and S (single-
character word).
Most of these works train tagging models
only on limited labeled training sets, without
using any unsupervised learning outcomes from
innumerous unlabeled text. But in recent years,
researchers begin to exploit the value of
enormous unlabeled corpus for CWS. Some
statistics information on co-occurrence of sub-
sequences in the whole text has been extracted
from unlabeled data and been employed as input
features for tagging model training (Zhao and
Kit , 2007).
Word clustering is a common method to
utilize unlabeled corpus in language processing
research to enhance the generalization ability,
such as part-of-speech clustering and semantic
clustering (Lee et al., 1999 and B Wang and H
Wang 2006). Character-based tagging method
usually employs N-gram features, where an N-
gram is an N-character segment of a string. We
believe that there are also semantic or
grammatical relationships between most of N-
grams and these relationships will be useful in
CWS. Intuitively, assuming the training data
contains the bigram “ t / 列 ”(The last two
characters of the word “Israel” in Chinese), not
contain the bigram “ 耳 / 其 ”(The last two
characters of the word “Turkey” in Chinese), if
we could cluster the two bigrams together
according to unlabeled corpus and employ it as
a feature for supervised training of tagging
model, maybe we will know that there should
be a word boundary after “耳/其” though we
only find the existence of word boundary after
“t/X” in the training data. So we investigate
how to apply clustering method onto unlabeled
data for the purpose of improving CWS
accuracy in this paper.
This paper proposes a novel method of using
unlabeled data for CWS, which employs Self-
Organizing Map (SOM) (Kohonen 1982) to
organize Chinese character N-grams on a two-
dimensional array, named as “N-gram cluster
map” (NGCM), in which the character N-grams
similar in grammatical structure and semantic
meaning are organized in the same or adjacent
position. Two different arrays are built based on
the N-gram’s preceding context and succeeding
context respectively because sometimes N-gram
is just a part of Chinese word and does not share
similar preceding and succeeding context in the
same time. Then NGCM-based features are
extracted and applied to tagging model of CWS.
Two tagging models are investigated, which are
structured support vector machine (SVM)
(Tsochantaridis et al., 2005) model and
Confidential Random Fields (CRF) (Lafferty et
al., 2001). The experimental results show that
NGCM is really helpful to CWS. In addition,
we find that the structured SVM achieves better
performance than CRF.
The rest of this paper is organized as follows:
Section 2 presents self-organizing map and the
idea of N-gram cluster maps. Section 3
describes structured SVM and how to use the
NGCMs based features in CWS. Section 4
shows experimental results on Bakeoff-2005
database and Section 5 gives our conclusion.
</bodyText>
<sectionHeader confidence="0.930778" genericHeader="method">
2 N-gram cluster maps
</sectionHeader>
<bodyText confidence="0.999743727272727">
Supervised learning method for CWS needs
enough pre-labeled corpus with word boundary
information for training. The final CWS
performance relies heavily on the quality of the
training data. The training data is limited and
cannot cover completely the linguistic
phenomenon. But unlabeled corpus can be
obtained easily from internet. One intuitive
method is to extract information from
unsupervised learning results from enormous
unlabeled data to enhance supervised learning.
</bodyText>
<subsectionHeader confidence="0.995093">
2.1 Self-Organizing Map
</subsectionHeader>
<bodyText confidence="0.941784363636364">
The Self-Organizing Map (SOM) (Kohonen
1982), sometimes called Kohonen map, was
developed by Teuvo Kohonen in the early
1980s. Different from other clustering method,
SOM is a type of artificial neural network on
the basis of competitive learning to visualize
higher dimensional data in a low-dimensional
space (usually 1D or 2D) while preserving the
topological properties of the input space. Figure
1 displays a 2D SOM.
Best matching unit
</bodyText>
<figureCaption confidence="0.993873">
Figure 1: SOM model
</figureCaption>
<bodyText confidence="0.893162">
In SOM, the input is a lot of data samples,
and each sample is represented as a vector
xi, i =1,2,..., M , where M is the number of the
input vectors. SOM will cluster all these
samples into L neurons, and each neuron is
associated with a weight vector wi, i =1,2,..., L ,
where L is the total number of the neurons. wj
is of the same dimensions as the input data
vectors xi. The learning algorithm of SOM is
</bodyText>
<listItem confidence="0.9749315">
as follows:
1. Randomize every neuron’s weight vector
wi ;
2. Randomly select an input vector xr ;
</listItem>
<bodyText confidence="0.904621333333333">
Two-dimensional
array of neurous
Input
</bodyText>
<listItem confidence="0.8834408">
3. Find the winning neuron j , whose
associate weight vector wj has the
minimal distance to xt ;
4. Update the weight vector of all the neurons
according to the following formula:
</listItem>
<equation confidence="0.9774475">
wi &lt;- wi + &apos;70(i, j)(xt − wi )
Where &apos;7 is the learning-rate and 0(i, j)
</equation>
<bodyText confidence="0.9584625">
is the neighborhood function. A simple
choice defines 0(i, j) =1 for all neuron i
in a neighborhood of radius r of neuron
j and 0(i, j) = 0 for all other neurons. &apos;7
and 0(i, j) usually varied dynamically
during learning for best results;
</bodyText>
<listItem confidence="0.684312">
5. Continue step 2 until maximum number of
iterations has been reached or no noticeable
changes are observed.
</listItem>
<subsectionHeader confidence="0.998989">
2.2 SOM-based N-gram cluster maps
</subsectionHeader>
<bodyText confidence="0.999914844444445">
Self-organizing semantic maps (Ritter and
Kohonen 1989, 1990) are SOMs that have been
organized according to word similarities,
measured by the similarity of the short contexts
of the words. Our algorithm of building N-gram
cluster maps is similar to self-organizing
semantic maps. Because sometimes N-gram is
just part of Chinese word and do not share
similar preceding and succeeding context in the
same time, so we build two different maps
according to the preceding context and the
succeeding context of N-gram individually. In
the end we build two NGCMs: NGCMP
(NGCM according to preceding context) and
NGCMS (NGCM according to succeeding
context).
In this paper we only consider bigram cluster
maps. So our purpose is to acquire a 2GCMP
and a 2GCMS. The large-scale unlabeled corpus
we used for training NGCMs is about 3.5G in
size. It was obtained easily from websites like
Sohu, Netease, Sina and People Daily. When
the cut-off threshold is set to 5, we got about 9K
different characters and 380K different bigrams
by counting the corpus. For each bigram, a 9K-
dimensional sparse vector can be derived from
the preceding character of the bigram. Therefore
a collection of 380K vector samples are
generated, which is denoted as P. Another
vector collection S which considers succeeding
character was obtained using the same method.
Our implementation used SOM-PAK
package Version 1.0 (Kohonen et al., 1996). We
set the topology type to rectangular and the map
size to15 x 1 5. In the training process, we used
P and S as input data respectively. After the
training we acquired a 2GCMP and a 2GCMS,
meanwhile each bigram was mapped to one
neuron. Because the number of neurons is much
smaller than the number of bigrams, each
neuron in the map was labeled with multiple
bigrams. The 2GCMP and 2GCMS are shown
in Figure 2 and Figure 3 respectively. The
comment boxes in the figures show some
samples of bigrams mapped in the same neuron.
</bodyText>
<figureCaption confidence="0.954364">
Figure 3: 2GCMS
</figureCaption>
<bodyText confidence="0.999857666666667">
After checking the results, we find that most
of the meaningless bigrams that contain
characters from more than one word, such as the
bigram &amp;quot; 京 天 &amp;quot; in &amp;quot;... 北 京 天 坛 ...&amp;quot; , are
organized into the same neurons in the map, and
most of the first or last bigrams of the country
names are organized into a few adjacent
neurons, such as “色/列”, “耳/其”, “中/国” and
“美/国”in 2GCMS , “巴/基”, “埃/塞”, “英/格”,
“俄/罗” , and “中/国” in 2GCMP. We also tried
to use the preceding and the succeeding context
together in NGCM training just like the method
</bodyText>
<figureCaption confidence="0.973346">
Figure 2: 2GCMP
</figureCaption>
<figure confidence="0.979136586206897">
同/志
士/兵
美/军
台/胞
丈/夫
父/亲
已/婚
女/性
...
0,14 1,14 2,14
0,0
0,1 1,1 2,1
0,2 1,2 2,2
1,0 2,0
14,14
14,0
14,1
14,2
色/列
耳/其
苏/丹
中/国
美/国
日/本
德/国
印/尼
班/牙
...
...
</figure>
<bodyText confidence="0.998783166666667">
used in the self-organizing semantic maps. We
found that the bigrams of “E/�”, “A/�” and
“A/�V” will never be assigned to the same
neuron again, which indicates that we need to
build two NGCMs according to preceding and
succeeding context separately.
</bodyText>
<sectionHeader confidence="0.99859" genericHeader="method">
3 Integrate NGCM into Structured
SVM for CWS
</sectionHeader>
<subsectionHeader confidence="0.999785">
3.1 Structured support vector machine
</subsectionHeader>
<bodyText confidence="0.955491375">
The structured support vector machine can learn
to predict structured y , such as trees sequences
or sets, from x based on large-margin approach.
We employ a structured SVM that can predict a
sequence of labels y = (y1,..., yT) for a given
observation sequence x=(x1,...,xT)
, where
yt ∈ ∑ , ∑ is the label set for y.
There are two types of features in the
structured SVM: transition features (interactions
between neighboring labels along the chain),
emission features (interactions between
attributes of the observation vectors and a
specific label).we can represent the input-output
pairs via joint feature map (JFM)
where
</bodyText>
<equation confidence="0.9973132">
Λc(y) ≡ (δ(y1,y),δ(y2,y),...,δ(yK,y))
∈ {0,1} , { , ,..., }
K y y y y
∈ =
1 2 K ∑
</equation>
<bodyText confidence="0.5403645">
denotes an arbitrary feature representation
of the inputs. The sign
</bodyText>
<figure confidence="0.584510391304348">
expresses tensor
product defined as
Rd
Rk
[
=
is the length of an
observation sequence.
0 is a scaling factor
which balances the two types of contributions.
Note that both transition features and
emission features can
φ(x)
&amp;quot;⊗&amp;quot;
⊗:
×
→R,
dk
] i (j 1)d
a b + −
⊗
[a]i[b]j.T
η≥
</figure>
<bodyText confidence="0.8904605">
be extended by including
higher-order interdependencies of labels (e.g.
</bodyText>
<equation confidence="0.70903">
Λc (yt) ⊗Λc (yt+ 1) ⊗Λc (yt+2) ),by including
current position (e.g. replacing ( )
φ x with
t
</equation>
<bodyText confidence="0.847736">
φ(xt−r, ,xt, xt+r) )or by combining higher-
order output features with input features (e.g.
Training the parameters can
be formulated as
the following optimization problem.
</bodyText>
<equation confidence="0.997060285714286">
I�
2 n
min 1 w,w + C
iw,ξ
i=1
s.t.∀i,∀y∈ Y:
w,ψi(xi,yi)− ψi(xi,y) ≥ Δ(yi,y)
</equation>
<bodyText confidence="0.9999435">
where n is the number of the training samples,
ξi is a slack variable , C ≥ 0 is a constant
controlling the tradeoff between training error
minimization and margin maximization,
</bodyText>
<equation confidence="0.873222">
Δ(y, y) is the loss function ,usually the
1
</equation>
<bodyText confidence="0.929596">
number of misclassified tags in the sentence.
</bodyText>
<subsectionHeader confidence="0.989425">
3.2 Features set for tagging model
</subsectionHeader>
<bodyText confidence="0.949261">
For a training sample denoted as
</bodyText>
<equation confidence="0.994245333333333">
x = x x and
( ,..., )
1 T y = (y1,..., yT) . We chose
</equation>
<bodyText confidence="0.996185">
first-order interdependencies of labels to be
transition features, and dependencies between
labels and N-grams (n=1, 2, 3) at current
position in observed input sequence to be
emission features.
So our JFM is the concatenation of the follow
vectors
</bodyText>
<equation confidence="0.995676153846154">
T
−1
c t 1
∑ Λ ⊗ Λ
( ) ( )
c t
y y + ,
φ(xt+m)⊗Λ c(yt),m∈ {−1,0,1}
φ(xt+m, xt+m+1) ⊗Λ c (yt ), m ∈ {−2,−1,0,1}
φ(xt+m−1, xt+m, xt+m+1) ⊗Λ c (yt ), m ∈ {−1,0,1}
t 1
=
T
⎛ ⎞
⎜∑φ(x t) ⊗Λc(yt) ⎟
t=1
= ⎜ ⎟
⎜ T−1 ⎟
⎜ ηyy+
∑Λ⎝ t=1 ⎠
ψ ( , )
x y
Kronecker delta ,
δ δ =
i j,
{
n
− ξi
1
t=
T
∑
t
=
1
T
∑
t
=
</equation>
<page confidence="0.518355">
1
</page>
<figure confidence="0.82038435">
T
∑
1,i= j
≠j
0
1
∑ ⊗ Λ ⊗ Λ
( ) ( ) ( )
t c t c t
t φ x y y + )
The w-parametrized discriminant function
F: X × Y→ R interpreted as measuring the
compatibility of x and y is defined as:
F ( x , y ; w ) = w , ψ ( x , y )
So we can maximize this function over the
response variable to make a prediction
f (x ) arg max ( , , )
= F x y w
y Y
∈
</figure>
<figureCaption confidence="0.958498">
Figure 4 shows the transition features and the
</figureCaption>
<bodyText confidence="0.63927">
input features from a window centered at the
</bodyText>
<equation confidence="0.739006833333333">
T
∑
t
1
T
∑
</equation>
<bodyText confidence="0.9145295">
emission features of N-grams (n=1, 2) at y3 .
The emission features of 3-grams are not shown
here because of the large number of the
interactions.
</bodyText>
<equation confidence="0.963566">
η2GCMS (xt+m,xt+m+1)⊗Λ c(yt), m∈ {−2,−1}
η 2GCMP (xt+mxt+m+1) ⊗ Λ c (yt ), m ∈ {0,1}
</equation>
<figureCaption confidence="0.978243">
Figure 4: the transition features and the
emission features at y3 for structured SVM
</figureCaption>
<subsectionHeader confidence="0.999394">
3.3 Using NGCM in CWS
</subsectionHeader>
<bodyText confidence="0.999020310344827">
Two methods can be used for extracting the
features from NGCMs to expend features
definition in section 3.2.
One method is to treat NGCM just as a
clustering tool and do not take into account the
similarity between adjacent neurons. So a new
feature with L dimensions can be generated,
where L is the number of the neurons or classes.
Only one value of the L dimension equals to 1
and others equal to 0. We call it NGCM
clustering feature.
Another way of using the NGCM is to adopt
the position of the neurons which current N-
gram mapped in the NGCM as a new feature.
So every feature has D dimensions (D equals to
the dimension of the NGCM, every dimension
is corresponding to the coordinate value in the
NGCM). In this way, N-gram which is
originally represented as a high dimensional
vector based on its context is mapped into a
very low-dimensional space. We call it NGCM
mapping feature.
In this paper, we only consider the NGCM
clustering or mapping features related to the
current label yi. We also extract features from
the quantization error of current N-gram
because the result of the NGCM is very noisy.
Then our previous JFM in section 3.2 is
concatenated with the following features:
</bodyText>
<equation confidence="0.995122666666667">
ϕ 2GCMS (xt+m,xt+m+1)⊗Λ c(yt),m∈ {−2,−1}
2GCMP (xt+m, xt+m+1 ) ⊗ Λ c (yt ), m ∈ {0,1}
t=
</equation>
<bodyText confidence="0.913164857142857">
where ϕ2GCMS( x) denotes the NGCM feature
from 2GCMS, ϕ2GCMP( x) denotes the NGCM
feature from 2GCMP. ηNGCM( x) denotes the
quantization error of current N-gram x on its
NGCM.
In 15 × 1 5 size NGCM, when we use the
NGCM clustering feature ϕ2GCMS( x) and
</bodyText>
<equation confidence="0.99857575">
ϕ2GCMP( x) 15 15
∈ {0,1} × . When we use the
NGCM mapping feature ϕ 2GCMS( x) and
ϕ2GCMP( x) ∈ {0,1,...,14} .
</equation>
<bodyText confidence="0.967935">
2 Notice that the
dimension of the NGCM clustering feature is
much higher than the NGCM mapping feature.
As an example, the process of import features
from NGCMs at y3 is presented in Figure 5.
</bodyText>
<equation confidence="0.934104666666667">
y5
x1 x2 x3 x4
2GCMS 2GCMP
</equation>
<figureCaption confidence="0.954503">
Figure 5: Using 2GCMS and 2GCMP as input
to structured SVM
</figureCaption>
<sectionHeader confidence="0.994117" genericHeader="evaluation">
4 Applications and Experiments
</sectionHeader>
<subsectionHeader confidence="0.999167">
4.1 Corpus
</subsectionHeader>
<bodyText confidence="0.985992">
We use the data adopted by the second
International Chinese Word Segmentation
Bakeoff (Bakeoff-2005). The corpus size
information is listed in Table 1.
</bodyText>
<table confidence="0.989255666666667">
Corpus As CityU MSRA PKU
Training(M) 5.45 1.46 2.37 1.1
Test(K) 122 41 107 104
</table>
<tableCaption confidence="0.8446255">
Table 1: Corpus size of Bakeoff-2005 in
number of words
</tableCaption>
<equation confidence="0.9853396">
y1 y2 y3 y4
x1 x2 x3 x4
y5
x5
T
∑
t
1
T
∑
1
t=
1
y1 y2 y3 y4
x5
</equation>
<subsectionHeader confidence="0.992285">
4.2 Text Preprocessing
</subsectionHeader>
<bodyText confidence="0.999943333333333">
Text is usually mixed up with numerical or
alphabetic characters in Chinese natural
language, such as “我在 office 上班到晚上 9
点”. These numerical or alphabetic characters
are barely segmented in CWS. Hence, we treat
these symbols as a whole “character” according
to the following two preprocessing steps. First
replace one alphabetic character to four
continuous alphabetic characters with E1 to E4
respectively, five or more alphabetic characters
with E5. Then replace one numerical number to
four numerical numbers with N1 to N4 and five
or more numerical numbers with N5. After text
preprocessing, the above examples will be “我
在 E5 上班到晚上 N1 点”.
</bodyText>
<subsectionHeader confidence="0.996241">
4.3 Character-based tagging method
</subsectionHeader>
<bodyText confidence="0.970091454545455">
for CWS
Previous works show that 6-tag set achieved
a better CWS performance (Zhao et al.,
2006). Thus, we opt for this tag set. This 6-
tag set adds ‘B2’ and ’B3’ to 4-tag set
which stand for the type of the second and
the third character in a Chinese word
respectively. For example, the tag sequence
for the sentence “上海世博会/将/持续/半
年(Shanghai World Expo / will / last / six
months)” will be “B B2 B3 M E S B E B E”.
</bodyText>
<subsectionHeader confidence="0.982217">
4.4 Experiments
</subsectionHeader>
<bodyText confidence="0.9173745">
The F-measure is employed for evaluation,
which is defined as follows:
num of correctly segmented words
num of the system output words
num of correctly segmented words
num of total words in test data
</bodyText>
<equation confidence="0.49068">
F-measure: F
P +R
</equation>
<bodyText confidence="0.995312161290322">
To compare with other discriminative
learning methods we first developed a baseline
system using conditional random field (CRF)
without using NGCM feature and then we
developed another CRF system: CFCRF (using
NGCM clustering features). In the end we
developed three structured SVM CWS systems:
SVM (without using NGCM features), CFSVM
(using NGCM clustering features), and
MFSVM (using NGCM mapping features). The
features for the baseline CRF system are the
same with the SVM system. The features for
CFCRF are the same with CFSVM. The result
of the CRF system using NGCM mapping
features cannot be given here, because it is
difficult to support continuous-value features
for CRF method which is based on the
Maximum Entropy Model.
We use CRF++ version 0.5 (Kudu, 2009) to
build our CRF models. The cut-off threshold is
set to 2(using the features that occurs no less
than 2 times in the given training data) and the
hyper-parameter is set to 4.5. We use hmm
svm
version 3.1 to build our structured SVM models.
The cut-off threshold is set to 2. The precision
parameter is set to 0.1. The tradeoff between
training error minimization and margin
maximization is set to 1000.
The comparisons between CRF, CFCRF, SVM,
CFSVM and MFSVM are shown in Table 2.
</bodyText>
<table confidence="0.999493764705882">
Corpus As CityU MSRA PKU
CRF P 0.945 0.943 0.971 0.953
baseline
R 0.955 0.942 0.970 0.946
F 0.950 0.942 0.971 0.950
CFCRF P 0.948 0.956 0.973 0.959
R 0.959 0.961 0.972 0.952
F 0.953 0.958 0.973 0.955
SVM P 0.949 0.957 0.972 0.953
R 0.959 0.959 0.972 0.946
F 0.954 0.958 0.972 0.950
CFSVM P 0.952 0.959 0.974 0.958
R 0.960 0.964 0.974 0.952
F 0.956 0.961 0.974 0.955
MFSVM P 0.950 0.957 0.974 0.958
R 0.961 0.963 0.974 0.951
F 0.956 0.960 0.974 0.954
</table>
<tableCaption confidence="0.999246">
Table 2: The results of our systems
</tableCaption>
<subsectionHeader confidence="0.654776">
4.5 Discussion
</subsectionHeader>
<bodyText confidence="0.9506172">
From Table 2, we can see that:
1) The NGCM feature is useful for CWS. The
feature achieves 13.9% relative error
reduction on CRF method and 7.2% relative
error reduction on structured SVM method;
2) CFSVM and MFSVM achieve similar
performance, differ from the expectation of
MFSVM should be better than CFSVM. We
think that this is because the size of 2GCMs
is too small. Due to the limitation of our
</bodyText>
<equation confidence="0.5252126">
Precision: P
Recall: R =
=
2 P R
x x
</equation>
<bodyText confidence="0.997817846153846">
computer and time we only get two 15 × 1 5
size 2GCMs, similarity between adjacent
neurons on the two small 2GCMs is very
week, NGCM cluster feature performs as
good as NGCM mapping feature on CWS.
But due to the dimensions of the NGCM
cluster feature is much larger than the
NGCM mapping feature, the training time
of the CFSVM is much longer than the
MFSVM;
3) It is obvious that structured SVM performs
better than CRF, demonstrating the benefit
of large margin approach.
</bodyText>
<sectionHeader confidence="0.999065" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999956333333333">
This paper proposes an approach to improve
CWS tagging accuracy by combining SOM with
structured SVM. We use SOM to organize
Chinese character N-grams on a two-
dimensional array, so that the N-grams similar
in grammatical structure and semantic meaning
are organized in the same or adjacent position.
Two different maps are built based on the N-
gram’s preceding and succeeding context
respectively. Then new features are extracted
from these maps and integrated into the
structured SVM methods for CWS.
Experimental results on Bakeoff-2005 database
show that SOM-based features can contribute
more than 7% relative error reduction, and the
structured SVM method for CWS, to our
knowledge, first proposed in this paper also
outperforms traditional CRF method.
In future work, we will try to organizing all
the N-grams on a much larger array, so that
every neuron will be labeled by a single N-gram.
Our ultimate objective is to reduce the
dimension of input features for supervised CWS
learning , such as structured SVM , by replacing
N-gram features with two-dimensional NGCM
mapping features in most of Chinese natural
language process tasks.
</bodyText>
<sectionHeader confidence="0.999453" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999648409090909">
B.Wang, H.Wang 2006.A Comparative Study on
Chinese Word Clustering. Computer Processing
of Oriental Languages. Beyond the Orient: The
Research Challenges Ahead, pages 157-164
Chang-Ning Huang and Hai Zhao. 2007. Chinese
word segmentation: A decade review. Journal of
Chinese Information Processing, 21(3):8–20.
Chung-Hong Lee &amp; Hsin-Chang Yang.1999, A Web
Text Mining Approach Based on Self-Organizing
Map, ACM-library
G.Bakir, T.Hofmann, B.Scholkopf, A.Smola, B.
Taskar, and S. V. N. Vishwanathan, editors. 2007
Predicting Structured Data. MIT Press,
Cambridge, Massachusetts.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-
Liang Lu. 2006. Effective tag set selection
inChinese word segmentation via conditional
random field modeling. In Proceedings of
PACLIC-20, pages 87–94. Wuhan, China.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006.An
improved Chinese word segmentation system with
conditional random field. In SIGHAN-5, pages
162–165, Sydney, Australia, July 22-23.
Hai Zhao and Chunyu Kit. 2007. Incorporating
global information into supervised learning for
Chinese word segmentation. In PACLING-2007,
pages 66–74, Melbourne,Australia, September 19-
21.
H.Ritter, and T.Kohonen, 1989. Self-organizing
semantic maps. Biological Cybernetics, vol. 61,
no. 4, pp. 241-254.
I.Tsochantaridis,T.Joachims,T.Hofmann,and Y.Altun.
2005. Large Margin Methods for Structured and
Interdependent Output Variables, Journal of
Machine Learning Research (JMLR),
6(Sep):1453-1484.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan
Guo.2005. A maximum entropy approach to
Chinese word segmentation. In Proceedings of the
Fourth SIGHAN Workshop on Chinese Language
Processing, pages 161–164. Jeju Island,Korea.
J.Lafferty,A.McCallum, F.Pereira. 2001. Conditional
random fields: Probabilistic models for
segmenting and labeling sequence data. In
Proceedings of the International Conference on
Machine Learning (ICML). San Francisco:
Morgan Kaufmann Publishers, 282−289.
Nianwen Xue and Susan P. Converse., 2002,
Combining Classifiers for Chinese Word
Segmentation, In Proceedings of First SIGHAN
Workshop on Chinese Language Processing.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29–48.
R.Sproat and T.Emerson. 2003.The first
international Chinese word segmentation bakeoff.
In The Second SIGHAN Workshop on Chinese
Language Processing, pages 133–143.Sapporo,
Japan.
S.Haykin, 1994. Neural Networks: A Comprehensive
Foundation. NewYork: MacMillan.
T.Joachims, T.Finley, Chun-Nam Yu. 2009, Cutting-
Plane Training of Structural SVMs, Machine
Learning Journal,77(1):27-59.
T.Joachims. 2008 . svm Sequence Tagging with
hmm
Structural Support Vector Machines,
http://www.cs.cornell.edu/People/tj/svm_light/sv
m_hmm.html
T.Honkela, 1997. Self-Organizing Maps in Natural
Language Processing. PhD thesis, Helsinki
University of Technology, Department of
Computer Science and Engineering, Laboratory of
Computer and Information Science.
T.Kohonen. 1982.Self-organized formation of
topologically correct feature maps. Biological
Cybernetics, 43, pp. 59-69.
T.Kohonen., J.Hynninen, J.Kangas, J.Laaksonen,
1996 ,SOM_PAK: The Self-Organizing Map
Program Package,Technical Report A31,
Helsinki University of Technology ,
http://www.cis.hut.fi/nnrc/nnrc-programs.html
T.Kudu.2009. CRF++: Yet another CRF
toolkit.:http://crfpp.sourceforge.net/.
Y.Altun, I.Tsochantaridis, T.Hofmann. 2003. Hidden
Markov Support Vector Machines. In Proceedings
of International Conference on Machine Learning
(ICML).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.883341">
<title confidence="0.966764">Improving Chinese Word Segmentation by Self-Organized Maps of Character N-gram</title>
<author confidence="0.996905">Chongyang Zhang Zhigang Chen Guoping Hu</author>
<affiliation confidence="0.990917">iFLYTEK Research iFLYTEK Research iFLYTEK Research</affiliation>
<email confidence="0.991317">cyzhang@iflytek.comzgchen@iflytek.comgphu@iflytek.com</email>
<abstract confidence="0.99841552">Character-based tagging method has achieved great success in Chinese Word Segmentation (CWS). This paper proposes a new approach to improve the CWS tagging accuracy by combining Self-Organizing Map (SOM) with structured support vector machine (SVM) for utilization of enormous unlabeled text corpus. First, character N-grams are clustered and mapped into a low-dimensional space by adopting SOM algorithm. Two different maps are built based on the N-gram’s preceding and succeeding context respectively. Then new features are extracted from these maps and integrated into the structured SVM methods for CWS. Experimental results on Bakeoff-2005 database show that SOM-based features can contribute more than 7% relative error reduction, and the structured SVM method for CWS proposed in this paper also outperforms traditional conditional random field (CRF) method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>B Wang</author>
</authors>
<title>H.Wang 2006.A Comparative Study on Chinese Word Clustering. Computer Processing of Oriental Languages. Beyond the Orient: The Research Challenges Ahead,</title>
<pages>157--164</pages>
<marker>Wang, </marker>
<rawString>B.Wang, H.Wang 2006.A Comparative Study on Chinese Word Clustering. Computer Processing of Oriental Languages. Beyond the Orient: The Research Challenges Ahead, pages 157-164</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang-Ning Huang</author>
<author>Hai Zhao</author>
</authors>
<title>Chinese word segmentation: A decade review.</title>
<date>2007</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="1694" citStr="Huang and Zhao, 2007" startWordPosition="238" endWordPosition="241">nditional random field (CRF) method. 1 Introduction It is well known that there is no space or any other separators to indicate the word boundary in Chinese. But word is the basic unit for most of Chinese natural language process tasks, such as Machine Translation, Information Extraction, Text Categorization and so on. As a result, Chinese word segmentation (CWS) becomes one of the most fundamental technologies in Chinese natural language process. In the last decade, many statistics-based methods for automatic CWS have been proposed with development of machine learning and statistical method (Huang and Zhao, 2007). Especially, the character-based tagging method which was proposed by Nianwen Xue (2003) achieves great success in the second International Chinese word segmentation Bakeoff in 2005 (Low et al., 2005). The character-based tagging method formulates the CWS problem as a task of predicting a tag for each character in the sentence, i.e. every character is considered as one of four different types in 4-tag set: B (begin of word), M (middle of word), E (end of word), and S (singlecharacter word). Most of these works train tagging models only on limited labeled training sets, without using any unsup</context>
</contexts>
<marker>Huang, Zhao, 2007</marker>
<rawString>Chang-Ning Huang and Hai Zhao. 2007. Chinese word segmentation: A decade review. Journal of Chinese Information Processing, 21(3):8–20.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Chung-Hong Lee</author>
</authors>
<title>Hsin-Chang Yang.1999, A Web Text Mining Approach Based on Self-Organizing Map,</title>
<location>ACM-library</location>
<marker>Lee, </marker>
<rawString>Chung-Hong Lee &amp; Hsin-Chang Yang.1999, A Web Text Mining Approach Based on Self-Organizing Map, ACM-library</rawString>
</citation>
<citation valid="true">
<title>Predicting Structured Data.</title>
<date>2007</date>
<editor>G.Bakir, T.Hofmann, B.Scholkopf, A.Smola, B. Taskar, and S. V. N. Vishwanathan, editors.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>2007</marker>
<rawString>G.Bakir, T.Hofmann, B.Scholkopf, A.Smola, B. Taskar, and S. V. N. Vishwanathan, editors. 2007 Predicting Structured Data. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
<author>BaoLiang Lu</author>
</authors>
<title>Effective tag set selection inChinese word segmentation via conditional random field modeling.</title>
<date>2006</date>
<booktitle>In Proceedings of PACLIC-20,</booktitle>
<pages>87--94</pages>
<location>Wuhan, China.</location>
<contexts>
<context position="16566" citStr="Zhao et al., 2006" startWordPosition="2888" endWordPosition="2891">aracters are barely segmented in CWS. Hence, we treat these symbols as a whole “character” according to the following two preprocessing steps. First replace one alphabetic character to four continuous alphabetic characters with E1 to E4 respectively, five or more alphabetic characters with E5. Then replace one numerical number to four numerical numbers with N1 to N4 and five or more numerical numbers with N5. After text preprocessing, the above examples will be “我 在 E5 上班到晚上 N1 点”. 4.3 Character-based tagging method for CWS Previous works show that 6-tag set achieved a better CWS performance (Zhao et al., 2006). Thus, we opt for this tag set. This 6- tag set adds ‘B2’ and ’B3’ to 4-tag set which stand for the type of the second and the third character in a Chinese word respectively. For example, the tag sequence for the sentence “上海世博会/将/持续/半 年(Shanghai World Expo / will / last / six months)” will be “B B2 B3 M E S B E B E”. 4.4 Experiments The F-measure is employed for evaluation, which is defined as follows: num of correctly segmented words num of the system output words num of correctly segmented words num of total words in test data F-measure: F P +R To compare with other discriminative learning</context>
</contexts>
<marker>Zhao, Huang, Li, Lu, 2006</marker>
<rawString>Hai Zhao, Chang-Ning Huang, Mu Li, and BaoLiang Lu. 2006. Effective tag set selection inChinese word segmentation via conditional random field modeling. In Proceedings of PACLIC-20, pages 87–94. Wuhan, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
</authors>
<title>improved Chinese word segmentation system with conditional random field.</title>
<date>2006</date>
<booktitle>In SIGHAN-5,</booktitle>
<pages>162--165</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="16566" citStr="Zhao et al., 2006" startWordPosition="2888" endWordPosition="2891">aracters are barely segmented in CWS. Hence, we treat these symbols as a whole “character” according to the following two preprocessing steps. First replace one alphabetic character to four continuous alphabetic characters with E1 to E4 respectively, five or more alphabetic characters with E5. Then replace one numerical number to four numerical numbers with N1 to N4 and five or more numerical numbers with N5. After text preprocessing, the above examples will be “我 在 E5 上班到晚上 N1 点”. 4.3 Character-based tagging method for CWS Previous works show that 6-tag set achieved a better CWS performance (Zhao et al., 2006). Thus, we opt for this tag set. This 6- tag set adds ‘B2’ and ’B3’ to 4-tag set which stand for the type of the second and the third character in a Chinese word respectively. For example, the tag sequence for the sentence “上海世博会/将/持续/半 年(Shanghai World Expo / will / last / six months)” will be “B B2 B3 M E S B E B E”. 4.4 Experiments The F-measure is employed for evaluation, which is defined as follows: num of correctly segmented words num of the system output words num of correctly segmented words num of total words in test data F-measure: F P +R To compare with other discriminative learning</context>
</contexts>
<marker>Zhao, Huang, Li, 2006</marker>
<rawString>Hai Zhao, Chang-Ning Huang, and Mu Li. 2006.An improved Chinese word segmentation system with conditional random field. In SIGHAN-5, pages 162–165, Sydney, Australia, July 22-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Incorporating global information into supervised learning for Chinese word segmentation.</title>
<date>2007</date>
<booktitle>In PACLING-2007,</booktitle>
<pages>66--74</pages>
<location>Melbourne,Australia,</location>
<marker>Zhao, Kit, 2007</marker>
<rawString>Hai Zhao and Chunyu Kit. 2007. Incorporating global information into supervised learning for Chinese word segmentation. In PACLING-2007, pages 66–74, Melbourne,Australia, September 19-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ritter</author>
<author>T Kohonen</author>
</authors>
<title>Self-organizing semantic maps.</title>
<date>1989</date>
<journal>Biological Cybernetics,</journal>
<volume>61</volume>
<pages>241--254</pages>
<contexts>
<context position="7326" citStr="Ritter and Kohonen 1989" startWordPosition="1166" endWordPosition="1169"> the minimal distance to xt ; 4. Update the weight vector of all the neurons according to the following formula: wi &lt;- wi + &apos;70(i, j)(xt − wi ) Where &apos;7 is the learning-rate and 0(i, j) is the neighborhood function. A simple choice defines 0(i, j) =1 for all neuron i in a neighborhood of radius r of neuron j and 0(i, j) = 0 for all other neurons. &apos;7 and 0(i, j) usually varied dynamically during learning for best results; 5. Continue step 2 until maximum number of iterations has been reached or no noticeable changes are observed. 2.2 SOM-based N-gram cluster maps Self-organizing semantic maps (Ritter and Kohonen 1989, 1990) are SOMs that have been organized according to word similarities, measured by the similarity of the short contexts of the words. Our algorithm of building N-gram cluster maps is similar to self-organizing semantic maps. Because sometimes N-gram is just part of Chinese word and do not share similar preceding and succeeding context in the same time, so we build two different maps according to the preceding context and the succeeding context of N-gram individually. In the end we build two NGCMs: NGCMP (NGCM according to preceding context) and NGCMS (NGCM according to succeeding context). </context>
</contexts>
<marker>Ritter, Kohonen, 1989</marker>
<rawString>H.Ritter, and T.Kohonen, 1989. Self-organizing semantic maps. Biological Cybernetics, vol. 61, no. 4, pp. 241-254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims I Tsochantaridis</author>
<author>T Hofmann</author>
<author>Y Altun</author>
</authors>
<title>Large Margin Methods for Structured and Interdependent Output Variables,</title>
<date>2005</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<pages>6--1453</pages>
<contexts>
<context position="4583" citStr="Tsochantaridis et al., 2005" startWordPosition="704" endWordPosition="707">a twodimensional array, named as “N-gram cluster map” (NGCM), in which the character N-grams similar in grammatical structure and semantic meaning are organized in the same or adjacent position. Two different arrays are built based on the N-gram’s preceding context and succeeding context respectively because sometimes N-gram is just a part of Chinese word and does not share similar preceding and succeeding context in the same time. Then NGCM-based features are extracted and applied to tagging model of CWS. Two tagging models are investigated, which are structured support vector machine (SVM) (Tsochantaridis et al., 2005) model and Confidential Random Fields (CRF) (Lafferty et al., 2001). The experimental results show that NGCM is really helpful to CWS. In addition, we find that the structured SVM achieves better performance than CRF. The rest of this paper is organized as follows: Section 2 presents self-organizing map and the idea of N-gram cluster maps. Section 3 describes structured SVM and how to use the NGCMs based features in CWS. Section 4 shows experimental results on Bakeoff-2005 database and Section 5 gives our conclusion. 2 N-gram cluster maps Supervised learning method for CWS needs enough pre-lab</context>
</contexts>
<marker>Tsochantaridis, Hofmann, Altun, 2005</marker>
<rawString>I.Tsochantaridis,T.Joachims,T.Hofmann,and Y.Altun. 2005. Large Margin Methods for Structured and Interdependent Output Variables, Journal of Machine Learning Research (JMLR), 6(Sep):1453-1484.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jin Kiat Low</author>
</authors>
<title>Hwee Tou Ng, and Wenyuan Guo.2005. A maximum entropy approach to Chinese word segmentation.</title>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>161--164</pages>
<note>Jeju Island,Korea.</note>
<marker>Low, </marker>
<rawString>Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo.2005. A maximum entropy approach to Chinese word segmentation. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, pages 161–164. Jeju Island,Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum J Lafferty</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML).</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>San Francisco:</location>
<marker>Lafferty, Pereira, 2001</marker>
<rawString>J.Lafferty,A.McCallum, F.Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Conference on Machine Learning (ICML). San Francisco: Morgan Kaufmann Publishers, 282−289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Susan P Converse</author>
</authors>
<title>Combining Classifiers for Chinese Word Segmentation,</title>
<date>2002</date>
<booktitle>In Proceedings of First SIGHAN Workshop on Chinese Language Processing.</booktitle>
<marker>Xue, Converse, 2002</marker>
<rawString>Nianwen Xue and Susan P. Converse., 2002, Combining Classifiers for Chinese Word Segmentation, In Proceedings of First SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese word segmentation as character tagging.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Chinese Language Processing,</booktitle>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="1783" citStr="Xue (2003)" startWordPosition="252" endWordPosition="253">ther separators to indicate the word boundary in Chinese. But word is the basic unit for most of Chinese natural language process tasks, such as Machine Translation, Information Extraction, Text Categorization and so on. As a result, Chinese word segmentation (CWS) becomes one of the most fundamental technologies in Chinese natural language process. In the last decade, many statistics-based methods for automatic CWS have been proposed with development of machine learning and statistical method (Huang and Zhao, 2007). Especially, the character-based tagging method which was proposed by Nianwen Xue (2003) achieves great success in the second International Chinese word segmentation Bakeoff in 2005 (Low et al., 2005). The character-based tagging method formulates the CWS problem as a task of predicting a tag for each character in the sentence, i.e. every character is considered as one of four different types in 4-tag set: B (begin of word), M (middle of word), E (end of word), and S (singlecharacter word). Most of these works train tagging models only on limited labeled training sets, without using any unsupervised learning outcomes from innumerous unlabeled text. But in recent years, researcher</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Nianwen Xue. 2003. Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing, 8(1):29–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sproat</author>
</authors>
<title>T.Emerson. 2003.The first international Chinese word segmentation bakeoff.</title>
<date></date>
<booktitle>In The Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>133--143</pages>
<marker>Sproat, </marker>
<rawString>R.Sproat and T.Emerson. 2003.The first international Chinese word segmentation bakeoff. In The Second SIGHAN Workshop on Chinese Language Processing, pages 133–143.Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Haykin</author>
</authors>
<title>Neural Networks: A Comprehensive Foundation.</title>
<date>1994</date>
<publisher>NewYork: MacMillan.</publisher>
<marker>Haykin, 1994</marker>
<rawString>S.Haykin, 1994. Neural Networks: A Comprehensive Foundation. NewYork: MacMillan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Finley T Joachims</author>
<author>Chun-Nam Yu</author>
</authors>
<title>CuttingPlane Training of Structural SVMs,</title>
<date>2009</date>
<journal>Machine Learning Journal,77(1):27-59.</journal>
<marker>Joachims, Yu, 2009</marker>
<rawString>T.Joachims, T.Finley, Chun-Nam Yu. 2009, CuttingPlane Training of Structural SVMs, Machine Learning Journal,77(1):27-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>svm Sequence Tagging with hmm Structural Support Vector Machines,</title>
<date>2008</date>
<note>http://www.cs.cornell.edu/People/tj/svm_light/sv m_hmm.html</note>
<marker>Joachims, 2008</marker>
<rawString>T.Joachims. 2008 . svm Sequence Tagging with hmm Structural Support Vector Machines, http://www.cs.cornell.edu/People/tj/svm_light/sv m_hmm.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Honkela</author>
</authors>
<title>Self-Organizing Maps in Natural Language Processing.</title>
<date>1997</date>
<tech>PhD thesis,</tech>
<institution>Helsinki University of Technology, Department of Computer Science and Engineering, Laboratory of Computer and Information Science.</institution>
<marker>Honkela, 1997</marker>
<rawString>T.Honkela, 1997. Self-Organizing Maps in Natural Language Processing. PhD thesis, Helsinki University of Technology, Department of Computer Science and Engineering, Laboratory of Computer and Information Science.</rawString>
</citation>
<citation valid="false">
<authors>
<author>T Kohonen</author>
</authors>
<title>1982.Self-organized formation of topologically correct feature maps.</title>
<journal>Biological Cybernetics,</journal>
<volume>43</volume>
<pages>59--69</pages>
<marker>Kohonen, </marker>
<rawString>T.Kohonen. 1982.Self-organized formation of topologically correct feature maps. Biological Cybernetics, 43, pp. 59-69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hynninen T Kohonen</author>
<author>J Laaksonen J Kangas</author>
</authors>
<title>SOM_PAK: The Self-Organizing Map Program</title>
<date>1996</date>
<tech>Package,Technical Report A31,</tech>
<institution>Helsinki University of Technology</institution>
<note>http://www.cis.hut.fi/nnrc/nnrc-programs.html</note>
<marker>Kohonen, Kangas, 1996</marker>
<rawString>T.Kohonen., J.Hynninen, J.Kangas, J.Laaksonen, 1996 ,SOM_PAK: The Self-Organizing Map Program Package,Technical Report A31, Helsinki University of Technology , http://www.cis.hut.fi/nnrc/nnrc-programs.html</rawString>
</citation>
<citation valid="false">
<booktitle>T.Kudu.2009. CRF++: Yet another CRF toolkit.:http://crfpp.sourceforge.net/.</booktitle>
<marker></marker>
<rawString>T.Kudu.2009. CRF++: Yet another CRF toolkit.:http://crfpp.sourceforge.net/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tsochantaridis Y Altun</author>
<author>T Hofmann</author>
</authors>
<title>Hidden Markov Support Vector Machines.</title>
<date>2003</date>
<booktitle>In Proceedings of International Conference on Machine Learning (ICML).</booktitle>
<marker>Altun, Hofmann, 2003</marker>
<rawString>Y.Altun, I.Tsochantaridis, T.Hofmann. 2003. Hidden Markov Support Vector Machines. In Proceedings of International Conference on Machine Learning (ICML).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>