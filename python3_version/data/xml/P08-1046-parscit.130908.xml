<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000344">
<title confidence="0.991019">
Using adaptor grammars to identify synergies
in the unsupervised acquisition of linguistic structure
</title>
<author confidence="0.9915">
Mark Johnson
</author>
<affiliation confidence="0.975232">
Brown University
</affiliation>
<email confidence="0.946073">
Mark Johnson@Brown.edu
</email>
<sectionHeader confidence="0.997201" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999606285714286">
Adaptor grammars (Johnson et al., 2007b) are
a non-parametric Bayesian extension of Prob-
abilistic Context-Free Grammars (PCFGs)
which in effect learn the probabilities of en-
tire subtrees. In practice, this means that an
adaptor grammar learns the structures useful
for generating the training data as well as
their probabilities. We present several differ-
ent adaptor grammars that learn to segment
phonemic input into words by modeling dif-
ferent linguistic properties of the input. One
of the advantages of a grammar-based frame-
work is that it is easy to combine grammars,
and we use this ability to compare models that
capture different kinds of linguistic structure.
We show that incorporating both unsupervised
syllabification and collocation-finding into the
adaptor grammar significantly improves un-
supervised word-segmentation accuracy over
that achieved by adaptor grammars that model
only one of these linguistic phenomena.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999936318181818">
How humans acquire language is arguably the cen-
tral issue in the scientific study of language. Hu-
man language is richly structured, but it is still hotly
debated as to whether this structure can be learnt,
or whether it must be innately specified. Compu-
tational linguistics can contribute to this debate by
identifying which aspects of language can poten-
tially be learnt from the input available to a child.
Here we try to identify linguistic properties that
convey information useful for learning to segment
streams of phonemes into words. We show that si-
multaneously learning syllable structure and collo-
cations improves word segmentation accuracy com-
pared to models that learn these independently. This
suggests that there might be a synergistic interaction
in learning several aspects of linguistic structure si-
multaneously, as compared to learning each kind of
linguistic structure independently.
Because learning collocations and word-initial
syllable onset clusters requires the learner to be able
to identify word boundaries, it might seem that we
face a chicken-and-egg problem here. One of the im-
portant properties of the adaptor grammar inference
procedure is that it gives us a way of learning these
interacting linguistic structures simultaneously.
Adaptor grammars are also interesting because
they can be viewed as directly inferring linguistic
structure. Most well-known machine-learning and
statistical inference procedures are parameter esti-
mation procedures, i.e., the procedure is designed to
find the values of a finite vector of parameters. Stan-
dard methods for learning linguistic structure typi-
cally try to reduce structure learning to parameter
estimation, say, by using an iterative generate-and-
prune procedure in which each iteration consists of
a rule generation step that proposes new rules ac-
cording to some scheme, a parameter estimation step
that estimates the utility of these rules, and pruning
step that removes low utility rules. For example, the
Bayesian unsupervised PCFG estimation procedure
devised by Stolcke (1994) uses a model-merging
procedure to propose new sets of PCFG rules and
a Bayesian version of the EM procedure to estimate
their weights.
</bodyText>
<page confidence="0.979916">
398
</page>
<note confidence="0.714461">
Proceedings of ACL-08: HLT, pages 398–406,
</note>
<page confidence="0.537595">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999868489361702">
Recently, methods have been developed in the
statistical community for Bayesian inference of
increasingly sophisticated non-parametric models.
(“Non-parametric” here means that the models are
not characterized by a finite vector of parameters,
so the complexity of the model can vary depending
on the data it describes). Adaptor grammars are a
framework for specifying a wide range of such mod-
els for grammatical inference. They can be viewed
as a nonparametric extension of PCFGs.
Informally, there seem to be at least two natu-
ral ways to construct non-parametric extensions of a
PCFG. First, we can construct an infinite number of
more specialized PCFGs by splitting or refining the
PCFG’s nonterminals into increasingly finer states;
this leads to the iPCFG or “infinite PCFG” (Liang et
al., 2007). Second, we can generalize over arbitrary
subtrees rather than local trees in much the way done
in DOP or tree substitution grammar (Bod, 1998;
Joshi, 2003), which leads to adaptor grammars.
Informally, the units of generalization of adap-
tor grammars are entire subtrees, rather than just
local trees, as in PCFGs. Just as in tree substitu-
tion grammars, each of these subtrees behaves as
a new context-free rule that expands the subtree’s
root node to its leaves, but unlike a tree substitu-
tion grammar, in which the subtrees are specified
in advance, in an adaptor grammar the subtrees, as
well as their probabilities, are learnt from the train-
ing data. In order to make parsing and inference
tractable we require the leaves of these subtrees to
be terminals, as explained in section 2. Thus adaptor
grammars are simple models of structure learning,
where the subtrees that constitute the units of gen-
eralization are in effect new context-free rules learnt
during the inference process. (In fact, the inference
procedure for adaptor grammars described in John-
son et al. (2007b) relies on a PCFG approximation
that contains a rule for each subtree generalization
in the adaptor grammar).
This paper applies adaptor grammars to word seg-
mentation and morphological acquisition. Linguis-
tically, these exhibit considerable cross-linguistic
variation, and so are likely to be learned by human
learners. It’s also plausible that semantics and con-
textual information is less important for their acqui-
sition than, say, syntax.
</bodyText>
<sectionHeader confidence="0.476698" genericHeader="method">
2 From PCFGs to Adaptor Grammars
</sectionHeader>
<bodyText confidence="0.999192172413793">
This section introduces adaptor grammars as an ex-
tension of PCFGs; for a more detailed exposition see
Johnson et al. (2007b). Formally, an adaptor gram-
mar is a PCFG in which a subset M of the nonter-
minals are adapted. An adaptor grammar generates
the same set of trees as the CFG with the same rules,
but instead of defining a fixed probability distribu-
tion over these trees as a PCFG does, it defines a
distribution over distributions over trees. An adaptor
grammar can be viewed as a kind of PCFG in which
each subtree of each adapted nonterminal A ∈ M is
a potential rule, with its own probability, so an adap-
tor grammar is nonparametric if there are infinitely
many possible adapted subtrees. (An adaptor gram-
mar can thus be viewed as a tree substitution gram-
mar with infinitely many initial trees). But any finite
set of sample parses for any finite corpus can only in-
volve a finite number of such subtrees, so the corre-
sponding PCFG approximation only involves a finite
number of rules, which permits us to build MCMC
samplers for adaptor grammars.
A PCFG can be viewed as a set of recursively-
defined mixture distributions GA over trees, one for
each nonterminal and terminal in the grammar. If A
is a terminal then GA is the distribution that puts all
of its mass on the unit tree (i.e., tree consisting of a
single node) labeled A. If A is a nonterminal then
GA is the distribution over trees with root labeled A
that satisfies:
</bodyText>
<equation confidence="0.995425">
�GA = BA→B1...BnTDA(GB1, ... , GBn)
A→B1...BnERA
</equation>
<bodyText confidence="0.9996942">
where RA is the set of rules expanding A,
BA→B1,...,Bn is the PCFG “probability” parame-
ter associated with the rule A → B1 ... Bn and
TDA(GB1,..., GBn) is the distribution over trees
with root label A satisfying:
</bodyText>
<equation confidence="0.97224925">
� n
TDA(G1, ... , Gn) 7_
_ ri Gi(ti).
t1 .. to i=1
</equation>
<bodyText confidence="0.908972333333333">
That is, TDA(G1, ... , Gn) is the distribution over
trees whose root node is labeled A and each subtree
ti is generated independently from the distribution
Gi. This independence assumption is what makes
a PCFG “context-free” (i.e., each subtree is inde-
pendent given its label). Adaptor grammars relax
</bodyText>
<page confidence="0.998541">
399
</page>
<bodyText confidence="0.999745346153846">
this independence assumption by in effect learning
the probability of the subtrees rooted in a specified
subset M of the nonterminals known as the adapted
nonterminals.
Adaptor grammars achieve this by associating
each adapted nonterminal A E M with a Dirichlet
Process (DP). A DP is a function of a base distri-
bution H and a concentration parameter α, and it
returns a distribution over distributions DP(α, H).
There are several different ways to define DPs; one
of the most useful is the characterization of the con-
ditional or sampling distribution of a draw from
DP(α, H) in terms of the Polya urn or Chinese
Restaurant Process (Teh et al., 2006). The Polya urn
initially contains αH(x) balls of color x. We sample
a distribution from DP(α, H) by repeatedly drawing
a ball at random from the urn and then returning it
plus an additional ball of the same color to the urn.
In an adaptor grammar there is one DP for each
adapted nonterminal A E M, whose base distribu-
tion HA is the distribution over trees defined using
A’s PCFG rules. This DP “adapts” A’s PCFG distri-
bution by moving mass from the infrequently to the
frequently occuring subtrees. An adaptor grammar
associates a distribution GA that satisfies the follow-
ing constraints with each nonterminal A:
</bodyText>
<equation confidence="0.992929">
GA - DP(αA, HA) if A E M
GA = HA if A E� M
�HA = BA→B1...B,,,TDA(GB1, ... , GBJ
A→B1...B.ERA
</equation>
<bodyText confidence="0.995096230769231">
Unlike a PCFG, an adaptor grammar does not define
a single distribution over trees; rather, each set of
draws from the DPs defines a different distribution.
In the adaptor grammars used in this paper there is
no recursion amongst adapted nonterminals (i.e., an
adapted nonterminal never expands to itself); it is
currently unknown whether there are tree distribu-
tions that satisfy the adaptor grammar constraints for
recursive adaptor grammars.
Inference for an adaptor grammar involves finding
the rule probabilities θ and the adapted distributions
over trees G. We put Dirichlet priors over the rule
probabilities, i.e.:
</bodyText>
<equation confidence="0.744823">
θA - DIR(βA)
</equation>
<bodyText confidence="0.999416041666667">
where θA is the vector of probabilities for the rules
expanding the nonterminal A and βA are the corre-
sponding Dirichlet parameters.
The applications described below require unsu-
pervised estimation, i.e., the training data consists
of terminal strings alone. Johnson et al. (2007b)
describe an MCMC procedure for inferring the
adapted tree distributions GA, and Johnson et al.
(2007a) describe a Bayesian inference procedure for
the PCFG rule parameters θ using a Metropolis-
Hastings MCMC procedure; implementations are
available from the author’s web site.
Informally, the inference procedure proceeds as
follows. We initialize the sampler by randomly as-
signing each string in the training corpus a random
tree generated by the grammar. Then we randomly
select a string to resample, and sample a parse of that
string with a PCFG approximation to the adaptor
grammar. This PCFG contains a production for each
adapted subtree in the parses of the other strings in
the training corpus. A final accept-reject step cor-
rects for the difference in the probability of the sam-
pled tree under the adaptor grammar and the PCFG
approximation.
</bodyText>
<sectionHeader confidence="0.9808785" genericHeader="method">
3 Word segmentation with adaptor
grammars
</sectionHeader>
<bodyText confidence="0.999997714285714">
We now turn to linguistic applications of adap-
tor grammars, specifically, to models of unsu-
pervised word segmentation. We follow previ-
ous work in using the Brent corpus consists of
9790 transcribed utterances (33,399 words) of child-
directed speech from the Bernstein-Ratner corpus
(Bernstein-Ratner, 1987) in the CHILDES database
(MacWhinney and Snow, 1985). The utterances
have been converted to a phonemic representation
using a phonemic dictionary, so that each occur-
rence of a word has the same phonemic transcrip-
tion. Utterance boundaries are given in the input to
the system; other word boundaries are not. We eval-
uated the f-score of the recovered word constituents
(Goldwater et al., 2006b). Using the adaptor gram-
mar software available on the author’s web site, sam-
plers were run for 10,000 epochs (passes through
the training data). We scored the parses assigned
to the training data at the end of sampling, and for
the last two epochs we annealed at temperature 0.5
(i.e., squared the probability) during sampling in or-
</bodyText>
<page confidence="0.987329">
400
</page>
<table confidence="0.999883285714286">
1 10 100 1000
U word 0.55 0.55 0.55 0.53
U morph 0.46 0.46 0.42 0.36
U syll 0.52 0.51 0.49 0.46
C word 0.53 0.64 0.74 0.76
C morph 0.56 0.63 0.73 0.63
C syll 0.77 0.77 0.78 0.74
</table>
<tableCaption confidence="0.99210525">
Table 1: Word segmentation f-score results for all mod-
els, as a function of DP concentration parameter α. “U”
indicates unigram-based grammars, while “C” indicates
collocation-based grammars.
</tableCaption>
<figure confidence="0.8776205">
Sentence → Word+
Word → Phoneme+
</figure>
<figureCaption confidence="0.73143175">
Figure 1: The unigram word adaptor grammar, which
uses a unigram model to generate a sequence of words,
where each word is a sequence of phonemes. Adapted
nonterminals are underlined.
</figureCaption>
<bodyText confidence="0.999967111111111">
der to concentrate mass on high probability parses.
In all experiments below we set ,C3 = 1, which corre-
sponds to a uniform prior on PCFG rule probabilities
0. We tied the Dirichlet Process concentration pa-
rameters α, and performed runs with α = 1,10,100
and 1000; apart from this, no attempt was made to
optimize the hyperparameters. Table 1 summarizes
the word segmentation f-scores for all models de-
scribed in this paper.
</bodyText>
<subsectionHeader confidence="0.996244">
3.1 Unigram word adaptor grammar
</subsectionHeader>
<bodyText confidence="0.999978625">
Johnson et al. (2007a) presented an adaptor gram-
mar that defines a unigram model of word segmen-
tation and showed that it performs as well as the
unigram DP word segmentation model presented by
(Goldwater et al., 2006a). The adaptor grammar that
encodes a unigram word segmentation model shown
in Figure 1.
In this grammar and the grammars below, under-
lining indicates an adapted nonterminal. Phoneme
is a nonterminal that expands to each of the 50 dis-
tinct phonemes present in the Brent corpus. This
grammar defines a Sentence to consist of a sequence
of Words, where a Word consists of a sequence of
Phonemes. The category Word is adapted, which
means that the grammar learns the words that oc-
cur in the training corpus. We present our adap-
</bodyText>
<figure confidence="0.9817505">
Sentence → Words
Words → Word
Words → Word Words
Word → Phonemes
Phonemes → Phoneme
Phonemes → Phoneme Phonemes
</figure>
<figureCaption confidence="0.97446">
Figure 2: The unigram word adaptor grammar of Fig-
ure 1 where regular expressions are expanded using new
unadapted right-branching nonterminals.
</figureCaption>
<figure confidence="0.962197">
Sentence
y u w a n t t u s i D 6 b U k
</figure>
<figureCaption confidence="0.9982858">
Figure 3: A parse of the phonemic representation of “you
want to see the book” produced by unigram word adap-
tor grammar of Figure 1. Only nonterminal nodes la-
beled with adapted nonterminals and the start symbol are
shown.
</figureCaption>
<bodyText confidence="0.999989473684211">
tor grammars using regular expressions for clarity,
but since our implementation does not handle reg-
ular expressions in rules, in the grammars actually
used by the program they are expanded using new
non-adapted nonterminals that rewrite in a uniform
right-branching manner. That is, the adaptor gram-
mar used by the program is shown in Figure 2.
The unigram word adaptor grammar generates
parses such as the one shown in Figure 3. With α =
1 and α = 10 we obtained a word segmentation f-
score of 0.55. Depending on the run, between 1,100
and 1, 400 subtrees (i.e., new rules) were found for
Word. As reported in Goldwater et al. (2006a) and
Goldwater et al. (2007), a unigram word segmen-
tation model tends to undersegment and misanalyse
collocations as individual words. This is presumably
because the unigram model has no way to capture
dependencies between words in collocations except
to make the collocation into a single word.
</bodyText>
<subsectionHeader confidence="0.998796">
3.2 Unigram morphology adaptor grammar
</subsectionHeader>
<bodyText confidence="0.9999544">
This section investigates whether learning mor-
phology together with word segmentation improves
word segmentation accuracy. Johnson et al. (2007a)
presented an adaptor grammar for segmenting verbs
into stems and suffixes that implements the DP-
</bodyText>
<figure confidence="0.886306777777778">
Word
Word
Word
Word
401
Sentence → Word+
Word → Stem (Suffix)
Stem → Phoneme+
Suffix → Phoneme+
</figure>
<figureCaption confidence="0.99892125">
Figure 4: The unigram morphology adaptor grammar,
which generates each Sentence as a sequence of Words,
and each Word as a Stem optionally followed by a Suffix.
Parentheses indicate optional constituents.
</figureCaption>
<figure confidence="0.873854">
Sentence
</figure>
<figureCaption confidence="0.996693285714286">
Figure 5: Parses of “wanna close it” and “you have to tell
me” produced by the unigram morphology grammar of
Figure 4. The first parse was chosen because it demon-
strates how the grammar is intended to analyse “wanna”
into a Stem and Suffix, while the second parse shows how
the grammar tends to use Stem and Suffix to capture col-
locations.
</figureCaption>
<bodyText confidence="0.99995517948718">
based unsupervised morphological analysis model
presented by Goldwater et al. (2006b). Here we
combine that adaptor grammar with the unigram
word segmentation grammar to produce the adap-
tor grammar shown in Figure 4, which is designed
to simultaneously learn both word segmentation and
morphology.
Parentheses indicate optional constituents in these
rules, so this grammar says that a Sentence consists
of a sequence of Words, and each Word consists of a
Stem followed by an optional Suffix. The categories
Word, Stem and Suffix are adapted, which means
that the grammar learns the Words, Stems and Suf-
fixes that occur in the training corpus. Technically
this grammar implements a Hierarchical Dirichlet
Process (HDP) (Teh et al., 2006) because the base
distribution for the Word DP is itself constructed
from the Stem and Suffix distributions, which are
themselves generated by DPs.
This grammar recovers words with an f-score of
only 0.46 with α = 1 or α = 10, which is consid-
erably less accurate than the unigram model of sec-
tion 3.1. Typical parses are shown in Figure 5. The
unigram morphology grammar tends to misanalyse
even longer collocations as words than the unigram
word grammar does. Inspecting the parses shows
that rather than capturing morphological structure,
the Stem and Suffix categories typically expand to
words themselves, so the Word category expands to
a collocation. It may be possible to correct this by
“tuning” the grammar’s hyperparameters, but we did
not attempt this here.
These results are not too surprising, since the kind
of regular stem-suffix morphology that this grammar
can capture is not common in the Brent corpus. It
is possible that a more sophisticated model of mor-
phology, or even a careful tuning of the Bayesian
prior parameters α and ,3, would produce better re-
sults.
</bodyText>
<subsectionHeader confidence="0.9978">
3.3 Unigram syllable adaptor grammar
</subsectionHeader>
<bodyText confidence="0.9999136">
PCFG estimation procedures have been used to
model the supervised and unsupervised acquisition
of syllable structure (M¨uller, 2001; M¨uller, 2002);
and the best performance in unsupervised acquisi-
tion is obtained using a grammar that encodes lin-
guistically detailed properties of syllables whose
rules are inferred using a fairly complex algorithm
(Goldwater and Johnson, 2005). While that work
studied the acquisition of syllable structure from iso-
lated words, here we investigate whether learning
syllable structure together with word segmentation
improves word segmentation accuracy. Modeling
syllable structure is a natural application of adaptor
grammars, since the grammar can learn the possible
onset and coda clusters, rather than requiring them
to be stipulated in the grammar.
In the unigram syllable adaptor grammar shown
in Figure 7, Consonant expands to any consonant
and Vowel expands to any vowel. This gram-
mar defines a Word to consist of up to three Syl-
lables, where each Syllable consists of an Onset
and a Rhyme and a Rhyme consists of a Nucleus
and a Coda. Following Goldwater and Johnson
(2005), the grammar differentiates between OnsetI,
which expands to word-initial onsets, and Onset,
</bodyText>
<figure confidence="0.978817777777778">
y u
h &amp; v
m i
t E l
Word
Word
Stem
Stem
Suffix
Suffix
w a n
6 k l o z
I t
Sentence
Word
Suffix
Word
Suffix
Word
Stem
t u
Stem
Stem
402
Sentence
Word Word
W A t s D I s
</figure>
<figureCaption confidence="0.997085666666667">
Figure 6: A parse of “what’s this” produced by the
unigram syllable adaptor grammar of Figure 7. (Only
adapted non-root nonterminals are shown in the parse).
</figureCaption>
<bodyText confidence="0.99991476">
which expands to non-word-initial onsets, and be-
tween CodaF, which expands to word-final codas,
and Coda, which expands to non-word-final codas.
Note that we do not need to distinguish specific posi-
tions within the Onset and Coda clusters as Goldwa-
ter and Johnson (2005) did, since the adaptor gram-
mar learns these clusters directly. Just like the un-
igram morphology grammar, the unigram syllable
grammar also defines a HDP because the base dis-
tribution for Word is defined in terms of the Onset
and Rhyme distributions.
The unigram syllable grammar achieves a word
segmentation f-score of 0.52 at α = 1, which is also
lower than the unigram word grammar achieves. In-
spection of the parses shows that the unigram sylla-
ble grammar also tends to misanalyse long colloca-
tions as Words. Specifically, it seems to misanalyse
function words as associated with the content words
next to them, perhaps because function words tend
to have simpler initial and final clusters.
We cannot compare our syllabification accuracy
with Goldwater’s and others’ previous work because
that work used different, supervised training data
and phonological representations based on British
rather than American pronunciation.
</bodyText>
<subsectionHeader confidence="0.98595">
3.4 Collocation word adaptor grammar
</subsectionHeader>
<bodyText confidence="0.9998153">
Goldwater et al. (2006a) showed that modeling de-
pendencies between adjacent words dramatically
improves word segmentation accuracy. It is not
possible to write an adaptor grammar that directly
implements Goldwater’s bigram word segmentation
model because an adaptor grammar has one DP per
adapted nonterminal (so the number of DPs is fixed
in advance) while Goldwater’s bigram model has
one DP per word type, and the number of word
types is not known in advance. However it is pos-
</bodyText>
<figure confidence="0.8882636">
Sentence → Word+
Word → SyllableIF
Word → SyllableI SyllableF
Word → SyllableI Syllable SyllableF
Syllable → (Onset) Rhyme
SyllableI → (OnsetI) Rhyme
SyllableF → (Onset) RhymeF
SyllableIF → (OnsetI) RhymeF
Rhyme → Nucleus (Coda)
RhymeF → Nucleus (CodaF)
Onset → Consonant+
OnsetI → Consonant+
Coda → Consonant+
CodaF → Consonant+
Nucleus → Vowel+
</figure>
<figureCaption confidence="0.993871333333333">
Figure 7: The unigram syllable adaptor grammar, which
generates each word as a sequence of up to three Sylla-
bles. Word-initial Onsets and word-final Codas are distin-
guished using the suffixes “I” and “F” respectively; these
are propagated through the grammar to ensure that these
appear in the correct positions.
</figureCaption>
<figure confidence="0.944908333333333">
Sentence → Colloc+
Colloc → Word+
Word → Phoneme+
</figure>
<figureCaption confidence="0.998515">
Figure 8: The collocation word adaptor grammar, which
generates a Sentence as sequence of Colloc(ations), each
of which consists of a sequence of Words.
</figureCaption>
<bodyText confidence="0.999848">
sible for an adaptor grammar to generate a sentence
as a sequence of collocations, each of which con-
sists of a sequence of words. These collocations give
the grammar a way to model dependencies between
words.
With the DP concentration parameters α = 1000
we obtained a f-score of 0.76, which is approxi-
mately the same as the results reported by Goldwa-
ter et al. (2006a) and Goldwater et al. (2007). This
suggests that the collocation word adaptor grammar
can capture inter-word dependencies similar to those
that improve the performance of Goldwater’s bigram
segmentation model.
</bodyText>
<subsectionHeader confidence="0.985586">
3.5 Collocation morphology adaptor grammar
</subsectionHeader>
<bodyText confidence="0.9983825">
One of the advantages of working within a gram-
matical framework is that it is often easy to combine
</bodyText>
<figure confidence="0.996467333333333">
OnsetI
Nucleus
CodaF
OnsetI
Nucleus
CodaF
</figure>
<page confidence="0.67205">
403
</page>
<figure confidence="0.999769705882353">
Sentence
Sentence
Colloc
Colloc
y u
w a n t
t u
s i
D 6
b U k
Colloc
Colloc
Word
Word
Word
Word
Word
Word
OnsetI
h
Nucleus
Word
&amp;
CodaF
v
Word
Nucleus
6
OnsetI
d r
Nucleus
Word
I N k
CodaF
</figure>
<figureCaption confidence="0.9762275">
Figure 9: A parse of “you want to see the book” produced
by the collocation word adaptor grammar of Figure 8.
</figureCaption>
<table confidence="0.336744">
Sentence —* Colloc+
Colloc —* Word+
Word —* Stem (Suffix)
Stem —* Phoneme+
Suffix —* Phoneme+
</table>
<figureCaption confidence="0.87745925">
Figure 10: The collocation morphology adaptor gram-
mar, which generates each Sentence as a sequence of Col-
loc(ations), each Colloc as a sequence of Words, and each
Word as a Stem optionally followed by a Suffix.
</figureCaption>
<bodyText confidence="0.999941714285714">
different grammar fragments into a single grammar.
In this section we combine the collocation aspect
of the previous grammar with the morphology com-
ponent of the grammar presented in section 3.2 to
produce a grammar that generates Sentences as se-
quences of Colloc(ations), where each Colloc con-
sists of a sequence of Words, and each Word consists
of a Stem followed by an optional Suffix, as shown
in Figure 10.
This grammar achieves a word segmentation f-
score of 0.73 at α = 100, which is much better than
the unigram morphology grammar of section 3.2,
but not as good as the collocation word grammar of
the previous section. Inspecting the parses shows
</bodyText>
<figure confidence="0.729118">
Sentence
Colloc Colloc
</figure>
<figureCaption confidence="0.993065166666667">
Figure 11: A parse of the phonemic representation of
“you have to tell me” using the collocation morphology
adaptor grammar of Figure 10.
Figure 12: A parse of “have a drink” produced by the col-
location syllable adaptor grammar. (Only adapted non-
root nonterminals are shown in the parse).
</figureCaption>
<bodyText confidence="0.999967555555556">
that while the ability to directly model collocations
reduces the number of collocations misanalysed as
words, function words still tend to be misanalysed as
morphemes of two-word collocations. In fact, some
of the misanalyses have a certain plausibility to them
(e.g., “to” is often analysed as the suffix of verbs
such as “have”, “want” and “like”, while “me” is of-
ten analysed as a suffix of verbs such as “show” and
“tell”), but they lower the word f-score considerably.
</bodyText>
<subsectionHeader confidence="0.998956">
3.6 Collocation syllable adaptor grammar
</subsectionHeader>
<bodyText confidence="0.99657475">
The collocation syllable adaptor grammar is the
same as the unigram syllable adaptor grammar of
Figure 7, except that the first production is replaced
with the following pair of productions.
</bodyText>
<equation confidence="0.6010135">
Sentence —* Colloc+
Colloc —* Word+
</equation>
<bodyText confidence="0.9999825">
This grammar generates a Sentence as a sequence of
Colloc(ations), each of which is composed of a se-
quence of Words, each of which in turn is composed
of a sequence of Syll(ables).
This grammar achieves a word segmentation f-
score of 0.78 at α = 100, which is the highest f-
score of any of the grammars investigated in this pa-
per, including the collocation word grammar, which
models collocations but not syllables. To confirm
that the difference is significant, we ran a Wilcoxon
test to compare the f-scores obtained from 8 runs of
the collocation syllable grammar with α = 100 and
the collocation word grammar with α = 1000, and
found that the difference is significant at p = 0.006.
</bodyText>
<sectionHeader confidence="0.991917" genericHeader="conclusions">
4 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.9913935">
This paper has shown how adaptor grammars can
be used to study a variety of different linguistic hy-
</bodyText>
<figure confidence="0.997557230769231">
h &amp; v
t u
Word
Stem
y u
Word
Stem
Stem
t E l
Suffix
m i
Word
Suffix
</figure>
<page confidence="0.997995">
404
</page>
<bodyText confidence="0.999989978021978">
potheses about the interaction of morphology and
syllable structure with word segmentation. Techni-
cally, adaptor grammars are a way of specifying a
variety of Hierarchical Dirichlet Processes (HDPs)
that can spread their support over an unbounded
number of distinct subtrees, giving them the abil-
ity to learn which subtrees are most useful for de-
scribing the training corpus. Thus adaptor gram-
mars move beyond simple parameter estimation and
provide a principled approach to the Bayesian es-
timation of at least some types of linguistic struc-
ture. Because of this, less linguistic structure needs
to be “built in” to an adaptor grammar compared to a
comparable PCFG. For example, the adaptor gram-
mars for syllable structure presented in sections 3.3
and 3.6 learn more information about syllable onsets
and codas than the PCFGs presented in Goldwater
and Johnson (2005).
We used adaptor grammars to study the effects
of modeling morphological structure, syllabification
and collocations on the accuracy of a standard unsu-
pervised word segmentation task. We showed how
adaptor grammars can implement a previously in-
vestigated model of unsupervised word segmenta-
tion, the unigram word segmentation model. We
then investigated adaptor grammars that incorpo-
rate one additional kind of information, and found
that modeling collocations provides the greatest im-
provement in word segmentation accuracy, result-
ing in a model that seems to capture many of the
same interword dependencies as the bigram model
of Goldwater et al. (2006b).
We then investigated grammars that combine
these kinds of information. There does not seem
to be a straight forward way to design an adaptor
grammar that models both morphology and sylla-
ble structure, as morpheme boundaries typically do
not align with syllable boundaries. However, we
showed that an adaptor grammar that models col-
locations and syllable structure performs word seg-
mentation more accurately than an adaptor grammar
that models either collocations or syllable structure
alone. This is not surprising, since syllable onsets
and codas that occur word-peripherally are typically
different to those that appear word-internally, and
our results suggest that by tracking these onsets and
codas, it is possible to learn more accurate word seg-
mentation.
There are a number of interesting directions for
future work. In this paper all of the hyperparame-
ters αA were tied and varied simultaneously, but it
is desirable to learn these from data as well. Just
before the camera-ready version of this paper was
due we developed a method for estimating the hyper-
parameters by putting a vague Gamma hyper-prior
on each αA and sampled using Metropolis-Hastings
with a sequence of increasingly narrow Gamma pro-
posal distributions, producing results for each model
that are as good or better than the best ones reported
in Table 1.
The adaptor grammars presented here barely
scratch the surface of the linguistically interesting
models that can be expressed as Hierarchical Dirich-
let Processes. The models of morphology presented
here are particularly naive—they only capture reg-
ular concatenative morphology consisting of one
paradigm class—which may partially explain why
we obtained such poor results using morphology
adaptor grammars. It’s straight forward to design
an adaptor grammar that can capture a finite number
of concatenative paradigm classes (Goldwater et al.,
2006b; Johnson et al., 2007a). We’d like to learn the
number of paradigm classes from the data, but do-
ing this would probably require extending adaptor
grammars to incorporate the kind of adaptive state-
splitting found in the iHMM and iPCFG (Liang et
al., 2007). There is no principled reason why this
could not be done, i.e., why one could not design an
HDP framework that simultaneously learns both the
fragments (as in an adaptor grammar) and the states
(as in an iHMM or iPCFG).
However, inference with these more complex
models will probably itself become more complex.
The MCMC sampler of Johnson et al. (2007a) used
here is satifactory for small and medium-sized prob-
lems, but it would be very useful to have more ef-
ficient inference procedures. It may be possible to
adapt efficient split-merge samplers (Jain and Neal,
2007) and Variational Bayes methods (Teh et al.,
2008) for DPs to adaptor grammars and other lin-
guistic applications of HDPs.
</bodyText>
<sectionHeader confidence="0.999333" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9854245">
This research was funded by NSF awards 0544127
and 0631667.
</bodyText>
<page confidence="0.998836">
405
</page>
<sectionHeader confidence="0.998346" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999917397590361">
N. Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck, editors,
Children’s Language, volume 6. Erlbaum, Hillsdale,
NJ.
Rens Bod. 1998. Beyond grammar: an experience-based
theory of language. CSLI Publications, Stanford, Cal-
ifornia.
Sharon Goldwater and Mark Johnson. 2005. Repre-
sentational bias in unsupervised learning of syllable
structure. In Proceedings of the Ninth Conference on
Computational Natural Language Learning (CoNLL-
2005), pages 112–119, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006a. Contextual dependencies in unsupervised
word segmentation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 673–680, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Sharon Goldwater, Tom Griffiths, and Mark Johnson.
2006b. Interpolating between types and tokens
by estimating power-law generators. In Y. Weiss,
B. Sch¨olkopf, and J. Platt, editors, Advances in Neural
Information Processing Systems 18, pages 459–466,
Cambridge, MA. MIT Press.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2007. Distributional cues to word boundaries:
Context is important. In David Bamman, Tatiana
Magnitskaia, and Colleen Zaller, editors, Proceedings
of the 31st Annual Boston University Conference on
Language Development, pages 239–250, Somerville,
MA. Cascadilla Press.
Sonia Jain and Radford M. Neal. 2007. Splitting and
merging components of a nonconjugate dirichlet pro-
cess mixture model. Bayesian Analysis, 2(3):445–472.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007a. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 139–146,
Rochester, New York, April. Association for Compu-
tational Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007b. Adaptor Grammars: A framework
for specifying compositional nonparametric Bayesian
models. In B. Sch¨olkopf, J. Platt, and T. Hoffman, ed-
itors, Advances in Neural Information Processing Sys-
tems 19, pages 641–648. MIT Press, Cambridge, MA.
Aravind Joshi. 2003. Tree adjoining grammars. In Rus-
lan Mikkov, editor, The Oxford Handbook of Compu-
tational Linguistics, pages 483–501. Oxford Univer-
sity Press, Oxford, England.
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical Dirichlet
processes. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 688–697.
Brian MacWhinney and Catherine Snow. 1985. The
child language data exchange system. Journal of Child
Language, 12:271–296.
Karin M¨uller. 2001. Automatic detection of syllable
boundaries combining the advantages of treebank and
bracketed corpora training. In Proceedings of the 39th
Annual Meeting of the Association for Computational
Linguistics.
Karin M¨uller. 2002. Probabilistic context-free grammars
for phonology. In Proceedings of the 6th Workshop
of the ACL Special Interest Group in Computational
Phonology (SIGPHON), pages 70–80, Philadelphia.
Andreas Stolcke. 1994. Bayesian Learning of Proba-
bilistic Language Models. Ph.D. thesis, University of
California, Berkeley.
Y. W. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hier-
archical Dirichlet processes. Journal of the American
Statistical Association, 101:1566–1581.
Yee Whye Teh, Kenichi Kurihara, and Max Welling.
2008. Collapsed variational inference for hdp. In J.C.
Platt, D. Koller, Y. Singer, and S. Roweis, editors, Ad-
vances in Neural Information Processing Systems 20.
MIT Press, Cambridge, MA.
</reference>
<page confidence="0.999046">
406
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.987221">
<title confidence="0.9978315">Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure</title>
<author confidence="0.999998">Mark Johnson</author>
<affiliation confidence="0.999906">Brown University</affiliation>
<author confidence="0.997902">Mark JohnsonBrown edu</author>
<abstract confidence="0.999708409090909">Adaptor grammars (Johnson et al., 2007b) are a non-parametric Bayesian extension of Probabilistic Context-Free Grammars (PCFGs) which in effect learn the probabilities of entire subtrees. In practice, this means that an adaptor grammar learns the structures useful for generating the training data as well as their probabilities. We present several different adaptor grammars that learn to segment phonemic input into words by modeling different linguistic properties of the input. One of the advantages of a grammar-based framework is that it is easy to combine grammars, and we use this ability to compare models that capture different kinds of linguistic structure. We show that incorporating both unsupervised syllabification and collocation-finding into the adaptor grammar significantly improves unsupervised word-segmentation accuracy over that achieved by adaptor grammars that model only one of these linguistic phenomena.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Bernstein-Ratner</author>
</authors>
<title>The phonology of parentchild speech.</title>
<date>1987</date>
<booktitle>Children’s Language,</booktitle>
<volume>6</volume>
<editor>In K. Nelson and A. van Kleeck, editors,</editor>
<publisher>Erlbaum,</publisher>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="11309" citStr="Bernstein-Ratner, 1987" startWordPosition="1822" endWordPosition="1823">or grammar. This PCFG contains a production for each adapted subtree in the parses of the other strings in the training corpus. A final accept-reject step corrects for the difference in the probability of the sampled tree under the adaptor grammar and the PCFG approximation. 3 Word segmentation with adaptor grammars We now turn to linguistic applications of adaptor grammars, specifically, to models of unsupervised word segmentation. We follow previous work in using the Brent corpus consists of 9790 transcribed utterances (33,399 words) of childdirected speech from the Bernstein-Ratner corpus (Bernstein-Ratner, 1987) in the CHILDES database (MacWhinney and Snow, 1985). The utterances have been converted to a phonemic representation using a phonemic dictionary, so that each occurrence of a word has the same phonemic transcription. Utterance boundaries are given in the input to the system; other word boundaries are not. We evaluated the f-score of the recovered word constituents (Goldwater et al., 2006b). Using the adaptor grammar software available on the author’s web site, samplers were run for 10,000 epochs (passes through the training data). We scored the parses assigned to the training data at the end </context>
</contexts>
<marker>Bernstein-Ratner, 1987</marker>
<rawString>N. Bernstein-Ratner. 1987. The phonology of parentchild speech. In K. Nelson and A. van Kleeck, editors, Children’s Language, volume 6. Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Beyond grammar: an experience-based theory of language.</title>
<date>1998</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, California.</location>
<contexts>
<context position="4373" citStr="Bod, 1998" startWordPosition="658" endWordPosition="659">ars are a framework for specifying a wide range of such models for grammatical inference. They can be viewed as a nonparametric extension of PCFGs. Informally, there seem to be at least two natural ways to construct non-parametric extensions of a PCFG. First, we can construct an infinite number of more specialized PCFGs by splitting or refining the PCFG’s nonterminals into increasingly finer states; this leads to the iPCFG or “infinite PCFG” (Liang et al., 2007). Second, we can generalize over arbitrary subtrees rather than local trees in much the way done in DOP or tree substitution grammar (Bod, 1998; Joshi, 2003), which leads to adaptor grammars. Informally, the units of generalization of adaptor grammars are entire subtrees, rather than just local trees, as in PCFGs. Just as in tree substitution grammars, each of these subtrees behaves as a new context-free rule that expands the subtree’s root node to its leaves, but unlike a tree substitution grammar, in which the subtrees are specified in advance, in an adaptor grammar the subtrees, as well as their probabilities, are learnt from the training data. In order to make parsing and inference tractable we require the leaves of these subtree</context>
</contexts>
<marker>Bod, 1998</marker>
<rawString>Rens Bod. 1998. Beyond grammar: an experience-based theory of language. CSLI Publications, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Mark Johnson</author>
</authors>
<title>Representational bias in unsupervised learning of syllable structure.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL2005),</booktitle>
<pages>112--119</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="18435" citStr="Goldwater and Johnson, 2005" startWordPosition="3012" endWordPosition="3015">ammar can capture is not common in the Brent corpus. It is possible that a more sophisticated model of morphology, or even a careful tuning of the Bayesian prior parameters α and ,3, would produce better results. 3.3 Unigram syllable adaptor grammar PCFG estimation procedures have been used to model the supervised and unsupervised acquisition of syllable structure (M¨uller, 2001; M¨uller, 2002); and the best performance in unsupervised acquisition is obtained using a grammar that encodes linguistically detailed properties of syllables whose rules are inferred using a fairly complex algorithm (Goldwater and Johnson, 2005). While that work studied the acquisition of syllable structure from isolated words, here we investigate whether learning syllable structure together with word segmentation improves word segmentation accuracy. Modeling syllable structure is a natural application of adaptor grammars, since the grammar can learn the possible onset and coda clusters, rather than requiring them to be stipulated in the grammar. In the unigram syllable adaptor grammar shown in Figure 7, Consonant expands to any consonant and Vowel expands to any vowel. This grammar defines a Word to consist of up to three Syllables,</context>
<context position="19863" citStr="Goldwater and Johnson (2005)" startWordPosition="3260" endWordPosition="3264">word-initial onsets, and Onset, y u h &amp; v m i t E l Word Word Stem Stem Suffix Suffix w a n 6 k l o z I t Sentence Word Suffix Word Suffix Word Stem t u Stem Stem 402 Sentence Word Word W A t s D I s Figure 6: A parse of “what’s this” produced by the unigram syllable adaptor grammar of Figure 7. (Only adapted non-root nonterminals are shown in the parse). which expands to non-word-initial onsets, and between CodaF, which expands to word-final codas, and Coda, which expands to non-word-final codas. Note that we do not need to distinguish specific positions within the Onset and Coda clusters as Goldwater and Johnson (2005) did, since the adaptor grammar learns these clusters directly. Just like the unigram morphology grammar, the unigram syllable grammar also defines a HDP because the base distribution for Word is defined in terms of the Onset and Rhyme distributions. The unigram syllable grammar achieves a word segmentation f-score of 0.52 at α = 1, which is also lower than the unigram word grammar achieves. Inspection of the parses shows that the unigram syllable grammar also tends to misanalyse long collocations as Words. Specifically, it seems to misanalyse function words as associated with the content word</context>
<context position="27009" citStr="Goldwater and Johnson (2005)" startWordPosition="4462" endWordPosition="4465">ounded number of distinct subtrees, giving them the ability to learn which subtrees are most useful for describing the training corpus. Thus adaptor grammars move beyond simple parameter estimation and provide a principled approach to the Bayesian estimation of at least some types of linguistic structure. Because of this, less linguistic structure needs to be “built in” to an adaptor grammar compared to a comparable PCFG. For example, the adaptor grammars for syllable structure presented in sections 3.3 and 3.6 learn more information about syllable onsets and codas than the PCFGs presented in Goldwater and Johnson (2005). We used adaptor grammars to study the effects of modeling morphological structure, syllabification and collocations on the accuracy of a standard unsupervised word segmentation task. We showed how adaptor grammars can implement a previously investigated model of unsupervised word segmentation, the unigram word segmentation model. We then investigated adaptor grammars that incorporate one additional kind of information, and found that modeling collocations provides the greatest improvement in word segmentation accuracy, resulting in a model that seems to capture many of the same interword dep</context>
</contexts>
<marker>Goldwater, Johnson, 2005</marker>
<rawString>Sharon Goldwater and Mark Johnson. 2005. Representational bias in unsupervised learning of syllable structure. In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL2005), pages 112–119, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>673--680</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="11700" citStr="Goldwater et al., 2006" startWordPosition="1884" endWordPosition="1887">to models of unsupervised word segmentation. We follow previous work in using the Brent corpus consists of 9790 transcribed utterances (33,399 words) of childdirected speech from the Bernstein-Ratner corpus (Bernstein-Ratner, 1987) in the CHILDES database (MacWhinney and Snow, 1985). The utterances have been converted to a phonemic representation using a phonemic dictionary, so that each occurrence of a word has the same phonemic transcription. Utterance boundaries are given in the input to the system; other word boundaries are not. We evaluated the f-score of the recovered word constituents (Goldwater et al., 2006b). Using the adaptor grammar software available on the author’s web site, samplers were run for 10,000 epochs (passes through the training data). We scored the parses assigned to the training data at the end of sampling, and for the last two epochs we annealed at temperature 0.5 (i.e., squared the probability) during sampling in or400 1 10 100 1000 U word 0.55 0.55 0.55 0.53 U morph 0.46 0.46 0.42 0.36 U syll 0.52 0.51 0.49 0.46 C word 0.53 0.64 0.74 0.76 C morph 0.56 0.63 0.73 0.63 C syll 0.77 0.77 0.78 0.74 Table 1: Word segmentation f-score results for all models, as a function of DP conce</context>
<context position="13299" citStr="Goldwater et al., 2006" startWordPosition="2161" endWordPosition="2164">In all experiments below we set ,C3 = 1, which corresponds to a uniform prior on PCFG rule probabilities 0. We tied the Dirichlet Process concentration parameters α, and performed runs with α = 1,10,100 and 1000; apart from this, no attempt was made to optimize the hyperparameters. Table 1 summarizes the word segmentation f-scores for all models described in this paper. 3.1 Unigram word adaptor grammar Johnson et al. (2007a) presented an adaptor grammar that defines a unigram model of word segmentation and showed that it performs as well as the unigram DP word segmentation model presented by (Goldwater et al., 2006a). The adaptor grammar that encodes a unigram word segmentation model shown in Figure 1. In this grammar and the grammars below, underlining indicates an adapted nonterminal. Phoneme is a nonterminal that expands to each of the 50 distinct phonemes present in the Brent corpus. This grammar defines a Sentence to consist of a sequence of Words, where a Word consists of a sequence of Phonemes. The category Word is adapted, which means that the grammar learns the words that occur in the training corpus. We present our adapSentence → Words Words → Word Words → Word Words Word → Phonemes Phonemes →</context>
<context position="14974" citStr="Goldwater et al. (2006" startWordPosition="2460" endWordPosition="2463">s using regular expressions for clarity, but since our implementation does not handle regular expressions in rules, in the grammars actually used by the program they are expanded using new non-adapted nonterminals that rewrite in a uniform right-branching manner. That is, the adaptor grammar used by the program is shown in Figure 2. The unigram word adaptor grammar generates parses such as the one shown in Figure 3. With α = 1 and α = 10 we obtained a word segmentation fscore of 0.55. Depending on the run, between 1,100 and 1, 400 subtrees (i.e., new rules) were found for Word. As reported in Goldwater et al. (2006a) and Goldwater et al. (2007), a unigram word segmentation model tends to undersegment and misanalyse collocations as individual words. This is presumably because the unigram model has no way to capture dependencies between words in collocations except to make the collocation into a single word. 3.2 Unigram morphology adaptor grammar This section investigates whether learning morphology together with word segmentation improves word segmentation accuracy. Johnson et al. (2007a) presented an adaptor grammar for segmenting verbs into stems and suffixes that implements the DPWord Word Word Word 4</context>
<context position="16286" citStr="Goldwater et al. (2006" startWordPosition="2668" endWordPosition="2671">nigram morphology adaptor grammar, which generates each Sentence as a sequence of Words, and each Word as a Stem optionally followed by a Suffix. Parentheses indicate optional constituents. Sentence Figure 5: Parses of “wanna close it” and “you have to tell me” produced by the unigram morphology grammar of Figure 4. The first parse was chosen because it demonstrates how the grammar is intended to analyse “wanna” into a Stem and Suffix, while the second parse shows how the grammar tends to use Stem and Suffix to capture collocations. based unsupervised morphological analysis model presented by Goldwater et al. (2006b). Here we combine that adaptor grammar with the unigram word segmentation grammar to produce the adaptor grammar shown in Figure 4, which is designed to simultaneously learn both word segmentation and morphology. Parentheses indicate optional constituents in these rules, so this grammar says that a Sentence consists of a sequence of Words, and each Word consists of a Stem followed by an optional Suffix. The categories Word, Stem and Suffix are adapted, which means that the grammar learns the Words, Stems and Suffixes that occur in the training corpus. Technically this grammar implements a Hi</context>
<context position="20853" citStr="Goldwater et al. (2006" startWordPosition="3416" endWordPosition="3419">ram word grammar achieves. Inspection of the parses shows that the unigram syllable grammar also tends to misanalyse long collocations as Words. Specifically, it seems to misanalyse function words as associated with the content words next to them, perhaps because function words tend to have simpler initial and final clusters. We cannot compare our syllabification accuracy with Goldwater’s and others’ previous work because that work used different, supervised training data and phonological representations based on British rather than American pronunciation. 3.4 Collocation word adaptor grammar Goldwater et al. (2006a) showed that modeling dependencies between adjacent words dramatically improves word segmentation accuracy. It is not possible to write an adaptor grammar that directly implements Goldwater’s bigram word segmentation model because an adaptor grammar has one DP per adapted nonterminal (so the number of DPs is fixed in advance) while Goldwater’s bigram model has one DP per word type, and the number of word types is not known in advance. However it is posSentence → Word+ Word → SyllableIF Word → SyllableI SyllableF Word → SyllableI Syllable SyllableF Syllable → (Onset) Rhyme SyllableI → (OnsetI</context>
<context position="22541" citStr="Goldwater et al. (2006" startWordPosition="3692" endWordPosition="3696"> ensure that these appear in the correct positions. Sentence → Colloc+ Colloc → Word+ Word → Phoneme+ Figure 8: The collocation word adaptor grammar, which generates a Sentence as sequence of Colloc(ations), each of which consists of a sequence of Words. sible for an adaptor grammar to generate a sentence as a sequence of collocations, each of which consists of a sequence of words. These collocations give the grammar a way to model dependencies between words. With the DP concentration parameters α = 1000 we obtained a f-score of 0.76, which is approximately the same as the results reported by Goldwater et al. (2006a) and Goldwater et al. (2007). This suggests that the collocation word adaptor grammar can capture inter-word dependencies similar to those that improve the performance of Goldwater’s bigram segmentation model. 3.5 Collocation morphology adaptor grammar One of the advantages of working within a grammatical framework is that it is often easy to combine OnsetI Nucleus CodaF OnsetI Nucleus CodaF 403 Sentence Sentence Colloc Colloc y u w a n t t u s i D 6 b U k Colloc Colloc Word Word Word Word Word Word OnsetI h Nucleus Word &amp; CodaF v Word Nucleus 6 OnsetI d r Nucleus Word I N k CodaF Figure 9: </context>
<context position="27664" citStr="Goldwater et al. (2006" startWordPosition="4561" endWordPosition="4564">dy the effects of modeling morphological structure, syllabification and collocations on the accuracy of a standard unsupervised word segmentation task. We showed how adaptor grammars can implement a previously investigated model of unsupervised word segmentation, the unigram word segmentation model. We then investigated adaptor grammars that incorporate one additional kind of information, and found that modeling collocations provides the greatest improvement in word segmentation accuracy, resulting in a model that seems to capture many of the same interword dependencies as the bigram model of Goldwater et al. (2006b). We then investigated grammars that combine these kinds of information. There does not seem to be a straight forward way to design an adaptor grammar that models both morphology and syllable structure, as morpheme boundaries typically do not align with syllable boundaries. However, we showed that an adaptor grammar that models collocations and syllable structure performs word segmentation more accurately than an adaptor grammar that models either collocations or syllable structure alone. This is not surprising, since syllable onsets and codas that occur word-peripherally are typically diffe</context>
<context position="29545" citStr="Goldwater et al., 2006" startWordPosition="4851" endWordPosition="4854"> model that are as good or better than the best ones reported in Table 1. The adaptor grammars presented here barely scratch the surface of the linguistically interesting models that can be expressed as Hierarchical Dirichlet Processes. The models of morphology presented here are particularly naive—they only capture regular concatenative morphology consisting of one paradigm class—which may partially explain why we obtained such poor results using morphology adaptor grammars. It’s straight forward to design an adaptor grammar that can capture a finite number of concatenative paradigm classes (Goldwater et al., 2006b; Johnson et al., 2007a). We’d like to learn the number of paradigm classes from the data, but doing this would probably require extending adaptor grammars to incorporate the kind of adaptive statesplitting found in the iHMM and iPCFG (Liang et al., 2007). There is no principled reason why this could not be done, i.e., why one could not design an HDP framework that simultaneously learns both the fragments (as in an adaptor grammar) and the states (as in an iHMM or iPCFG). However, inference with these more complex models will probably itself become more complex. The MCMC sampler of Johnson et</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006a. Contextual dependencies in unsupervised word segmentation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 673–680, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power-law generators. In</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems 18,</booktitle>
<pages>459--466</pages>
<editor>Y. Weiss, B. Sch¨olkopf, and J. Platt, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="11700" citStr="Goldwater et al., 2006" startWordPosition="1884" endWordPosition="1887">to models of unsupervised word segmentation. We follow previous work in using the Brent corpus consists of 9790 transcribed utterances (33,399 words) of childdirected speech from the Bernstein-Ratner corpus (Bernstein-Ratner, 1987) in the CHILDES database (MacWhinney and Snow, 1985). The utterances have been converted to a phonemic representation using a phonemic dictionary, so that each occurrence of a word has the same phonemic transcription. Utterance boundaries are given in the input to the system; other word boundaries are not. We evaluated the f-score of the recovered word constituents (Goldwater et al., 2006b). Using the adaptor grammar software available on the author’s web site, samplers were run for 10,000 epochs (passes through the training data). We scored the parses assigned to the training data at the end of sampling, and for the last two epochs we annealed at temperature 0.5 (i.e., squared the probability) during sampling in or400 1 10 100 1000 U word 0.55 0.55 0.55 0.53 U morph 0.46 0.46 0.42 0.36 U syll 0.52 0.51 0.49 0.46 C word 0.53 0.64 0.74 0.76 C morph 0.56 0.63 0.73 0.63 C syll 0.77 0.77 0.78 0.74 Table 1: Word segmentation f-score results for all models, as a function of DP conce</context>
<context position="13299" citStr="Goldwater et al., 2006" startWordPosition="2161" endWordPosition="2164">In all experiments below we set ,C3 = 1, which corresponds to a uniform prior on PCFG rule probabilities 0. We tied the Dirichlet Process concentration parameters α, and performed runs with α = 1,10,100 and 1000; apart from this, no attempt was made to optimize the hyperparameters. Table 1 summarizes the word segmentation f-scores for all models described in this paper. 3.1 Unigram word adaptor grammar Johnson et al. (2007a) presented an adaptor grammar that defines a unigram model of word segmentation and showed that it performs as well as the unigram DP word segmentation model presented by (Goldwater et al., 2006a). The adaptor grammar that encodes a unigram word segmentation model shown in Figure 1. In this grammar and the grammars below, underlining indicates an adapted nonterminal. Phoneme is a nonterminal that expands to each of the 50 distinct phonemes present in the Brent corpus. This grammar defines a Sentence to consist of a sequence of Words, where a Word consists of a sequence of Phonemes. The category Word is adapted, which means that the grammar learns the words that occur in the training corpus. We present our adapSentence → Words Words → Word Words → Word Words Word → Phonemes Phonemes →</context>
<context position="14974" citStr="Goldwater et al. (2006" startWordPosition="2460" endWordPosition="2463">s using regular expressions for clarity, but since our implementation does not handle regular expressions in rules, in the grammars actually used by the program they are expanded using new non-adapted nonterminals that rewrite in a uniform right-branching manner. That is, the adaptor grammar used by the program is shown in Figure 2. The unigram word adaptor grammar generates parses such as the one shown in Figure 3. With α = 1 and α = 10 we obtained a word segmentation fscore of 0.55. Depending on the run, between 1,100 and 1, 400 subtrees (i.e., new rules) were found for Word. As reported in Goldwater et al. (2006a) and Goldwater et al. (2007), a unigram word segmentation model tends to undersegment and misanalyse collocations as individual words. This is presumably because the unigram model has no way to capture dependencies between words in collocations except to make the collocation into a single word. 3.2 Unigram morphology adaptor grammar This section investigates whether learning morphology together with word segmentation improves word segmentation accuracy. Johnson et al. (2007a) presented an adaptor grammar for segmenting verbs into stems and suffixes that implements the DPWord Word Word Word 4</context>
<context position="16286" citStr="Goldwater et al. (2006" startWordPosition="2668" endWordPosition="2671">nigram morphology adaptor grammar, which generates each Sentence as a sequence of Words, and each Word as a Stem optionally followed by a Suffix. Parentheses indicate optional constituents. Sentence Figure 5: Parses of “wanna close it” and “you have to tell me” produced by the unigram morphology grammar of Figure 4. The first parse was chosen because it demonstrates how the grammar is intended to analyse “wanna” into a Stem and Suffix, while the second parse shows how the grammar tends to use Stem and Suffix to capture collocations. based unsupervised morphological analysis model presented by Goldwater et al. (2006b). Here we combine that adaptor grammar with the unigram word segmentation grammar to produce the adaptor grammar shown in Figure 4, which is designed to simultaneously learn both word segmentation and morphology. Parentheses indicate optional constituents in these rules, so this grammar says that a Sentence consists of a sequence of Words, and each Word consists of a Stem followed by an optional Suffix. The categories Word, Stem and Suffix are adapted, which means that the grammar learns the Words, Stems and Suffixes that occur in the training corpus. Technically this grammar implements a Hi</context>
<context position="20853" citStr="Goldwater et al. (2006" startWordPosition="3416" endWordPosition="3419">ram word grammar achieves. Inspection of the parses shows that the unigram syllable grammar also tends to misanalyse long collocations as Words. Specifically, it seems to misanalyse function words as associated with the content words next to them, perhaps because function words tend to have simpler initial and final clusters. We cannot compare our syllabification accuracy with Goldwater’s and others’ previous work because that work used different, supervised training data and phonological representations based on British rather than American pronunciation. 3.4 Collocation word adaptor grammar Goldwater et al. (2006a) showed that modeling dependencies between adjacent words dramatically improves word segmentation accuracy. It is not possible to write an adaptor grammar that directly implements Goldwater’s bigram word segmentation model because an adaptor grammar has one DP per adapted nonterminal (so the number of DPs is fixed in advance) while Goldwater’s bigram model has one DP per word type, and the number of word types is not known in advance. However it is posSentence → Word+ Word → SyllableIF Word → SyllableI SyllableF Word → SyllableI Syllable SyllableF Syllable → (Onset) Rhyme SyllableI → (OnsetI</context>
<context position="22541" citStr="Goldwater et al. (2006" startWordPosition="3692" endWordPosition="3696"> ensure that these appear in the correct positions. Sentence → Colloc+ Colloc → Word+ Word → Phoneme+ Figure 8: The collocation word adaptor grammar, which generates a Sentence as sequence of Colloc(ations), each of which consists of a sequence of Words. sible for an adaptor grammar to generate a sentence as a sequence of collocations, each of which consists of a sequence of words. These collocations give the grammar a way to model dependencies between words. With the DP concentration parameters α = 1000 we obtained a f-score of 0.76, which is approximately the same as the results reported by Goldwater et al. (2006a) and Goldwater et al. (2007). This suggests that the collocation word adaptor grammar can capture inter-word dependencies similar to those that improve the performance of Goldwater’s bigram segmentation model. 3.5 Collocation morphology adaptor grammar One of the advantages of working within a grammatical framework is that it is often easy to combine OnsetI Nucleus CodaF OnsetI Nucleus CodaF 403 Sentence Sentence Colloc Colloc y u w a n t t u s i D 6 b U k Colloc Colloc Word Word Word Word Word Word OnsetI h Nucleus Word &amp; CodaF v Word Nucleus 6 OnsetI d r Nucleus Word I N k CodaF Figure 9: </context>
<context position="27664" citStr="Goldwater et al. (2006" startWordPosition="4561" endWordPosition="4564">dy the effects of modeling morphological structure, syllabification and collocations on the accuracy of a standard unsupervised word segmentation task. We showed how adaptor grammars can implement a previously investigated model of unsupervised word segmentation, the unigram word segmentation model. We then investigated adaptor grammars that incorporate one additional kind of information, and found that modeling collocations provides the greatest improvement in word segmentation accuracy, resulting in a model that seems to capture many of the same interword dependencies as the bigram model of Goldwater et al. (2006b). We then investigated grammars that combine these kinds of information. There does not seem to be a straight forward way to design an adaptor grammar that models both morphology and syllable structure, as morpheme boundaries typically do not align with syllable boundaries. However, we showed that an adaptor grammar that models collocations and syllable structure performs word segmentation more accurately than an adaptor grammar that models either collocations or syllable structure alone. This is not surprising, since syllable onsets and codas that occur word-peripherally are typically diffe</context>
<context position="29545" citStr="Goldwater et al., 2006" startWordPosition="4851" endWordPosition="4854"> model that are as good or better than the best ones reported in Table 1. The adaptor grammars presented here barely scratch the surface of the linguistically interesting models that can be expressed as Hierarchical Dirichlet Processes. The models of morphology presented here are particularly naive—they only capture regular concatenative morphology consisting of one paradigm class—which may partially explain why we obtained such poor results using morphology adaptor grammars. It’s straight forward to design an adaptor grammar that can capture a finite number of concatenative paradigm classes (Goldwater et al., 2006b; Johnson et al., 2007a). We’d like to learn the number of paradigm classes from the data, but doing this would probably require extending adaptor grammars to incorporate the kind of adaptive statesplitting found in the iHMM and iPCFG (Liang et al., 2007). There is no principled reason why this could not be done, i.e., why one could not design an HDP framework that simultaneously learns both the fragments (as in an adaptor grammar) and the states (as in an iHMM or iPCFG). However, inference with these more complex models will probably itself become more complex. The MCMC sampler of Johnson et</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Tom Griffiths, and Mark Johnson. 2006b. Interpolating between types and tokens by estimating power-law generators. In Y. Weiss, B. Sch¨olkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18, pages 459–466, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Distributional cues to word boundaries: Context is important.</title>
<date>2007</date>
<booktitle>Proceedings of the 31st Annual Boston University Conference on Language Development,</booktitle>
<pages>239--250</pages>
<editor>In David Bamman, Tatiana Magnitskaia, and Colleen Zaller, editors,</editor>
<publisher>Cascadilla Press.</publisher>
<location>Somerville, MA.</location>
<contexts>
<context position="15004" citStr="Goldwater et al. (2007)" startWordPosition="2465" endWordPosition="2468">or clarity, but since our implementation does not handle regular expressions in rules, in the grammars actually used by the program they are expanded using new non-adapted nonterminals that rewrite in a uniform right-branching manner. That is, the adaptor grammar used by the program is shown in Figure 2. The unigram word adaptor grammar generates parses such as the one shown in Figure 3. With α = 1 and α = 10 we obtained a word segmentation fscore of 0.55. Depending on the run, between 1,100 and 1, 400 subtrees (i.e., new rules) were found for Word. As reported in Goldwater et al. (2006a) and Goldwater et al. (2007), a unigram word segmentation model tends to undersegment and misanalyse collocations as individual words. This is presumably because the unigram model has no way to capture dependencies between words in collocations except to make the collocation into a single word. 3.2 Unigram morphology adaptor grammar This section investigates whether learning morphology together with word segmentation improves word segmentation accuracy. Johnson et al. (2007a) presented an adaptor grammar for segmenting verbs into stems and suffixes that implements the DPWord Word Word Word 401 Sentence → Word+ Word → Ste</context>
<context position="22571" citStr="Goldwater et al. (2007)" startWordPosition="3698" endWordPosition="3701">the correct positions. Sentence → Colloc+ Colloc → Word+ Word → Phoneme+ Figure 8: The collocation word adaptor grammar, which generates a Sentence as sequence of Colloc(ations), each of which consists of a sequence of Words. sible for an adaptor grammar to generate a sentence as a sequence of collocations, each of which consists of a sequence of words. These collocations give the grammar a way to model dependencies between words. With the DP concentration parameters α = 1000 we obtained a f-score of 0.76, which is approximately the same as the results reported by Goldwater et al. (2006a) and Goldwater et al. (2007). This suggests that the collocation word adaptor grammar can capture inter-word dependencies similar to those that improve the performance of Goldwater’s bigram segmentation model. 3.5 Collocation morphology adaptor grammar One of the advantages of working within a grammatical framework is that it is often easy to combine OnsetI Nucleus CodaF OnsetI Nucleus CodaF 403 Sentence Sentence Colloc Colloc y u w a n t t u s i D 6 b U k Colloc Colloc Word Word Word Word Word Word OnsetI h Nucleus Word &amp; CodaF v Word Nucleus 6 OnsetI d r Nucleus Word I N k CodaF Figure 9: A parse of “you want to see th</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2007</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2007. Distributional cues to word boundaries: Context is important. In David Bamman, Tatiana Magnitskaia, and Colleen Zaller, editors, Proceedings of the 31st Annual Boston University Conference on Language Development, pages 239–250, Somerville, MA. Cascadilla Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonia Jain</author>
<author>Radford M Neal</author>
</authors>
<title>Splitting and merging components of a nonconjugate dirichlet process mixture model.</title>
<date>2007</date>
<journal>Bayesian Analysis,</journal>
<volume>2</volume>
<issue>3</issue>
<marker>Jain, Neal, 2007</marker>
<rawString>Sonia Jain and Radford M. Neal. 2007. Splitting and merging components of a nonconjugate dirichlet process mixture model. Bayesian Analysis, 2(3):445–472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>139--146</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="5307" citStr="Johnson et al. (2007" startWordPosition="809" endWordPosition="813">leaves, but unlike a tree substitution grammar, in which the subtrees are specified in advance, in an adaptor grammar the subtrees, as well as their probabilities, are learnt from the training data. In order to make parsing and inference tractable we require the leaves of these subtrees to be terminals, as explained in section 2. Thus adaptor grammars are simple models of structure learning, where the subtrees that constitute the units of generalization are in effect new context-free rules learnt during the inference process. (In fact, the inference procedure for adaptor grammars described in Johnson et al. (2007b) relies on a PCFG approximation that contains a rule for each subtree generalization in the adaptor grammar). This paper applies adaptor grammars to word segmentation and morphological acquisition. Linguistically, these exhibit considerable cross-linguistic variation, and so are likely to be learned by human learners. It’s also plausible that semantics and contextual information is less important for their acquisition than, say, syntax. 2 From PCFGs to Adaptor Grammars This section introduces adaptor grammars as an extension of PCFGs; for a more detailed exposition see Johnson et al. (2007b)</context>
<context position="10110" citStr="Johnson et al. (2007" startWordPosition="1632" endWordPosition="1635">to itself); it is currently unknown whether there are tree distributions that satisfy the adaptor grammar constraints for recursive adaptor grammars. Inference for an adaptor grammar involves finding the rule probabilities θ and the adapted distributions over trees G. We put Dirichlet priors over the rule probabilities, i.e.: θA - DIR(βA) where θA is the vector of probabilities for the rules expanding the nonterminal A and βA are the corresponding Dirichlet parameters. The applications described below require unsupervised estimation, i.e., the training data consists of terminal strings alone. Johnson et al. (2007b) describe an MCMC procedure for inferring the adapted tree distributions GA, and Johnson et al. (2007a) describe a Bayesian inference procedure for the PCFG rule parameters θ using a MetropolisHastings MCMC procedure; implementations are available from the author’s web site. Informally, the inference procedure proceeds as follows. We initialize the sampler by randomly assigning each string in the training corpus a random tree generated by the grammar. Then we randomly select a string to resample, and sample a parse of that string with a PCFG approximation to the adaptor grammar. This PCFG co</context>
<context position="13103" citStr="Johnson et al. (2007" startWordPosition="2127" endWordPosition="2130">, which uses a unigram model to generate a sequence of words, where each word is a sequence of phonemes. Adapted nonterminals are underlined. der to concentrate mass on high probability parses. In all experiments below we set ,C3 = 1, which corresponds to a uniform prior on PCFG rule probabilities 0. We tied the Dirichlet Process concentration parameters α, and performed runs with α = 1,10,100 and 1000; apart from this, no attempt was made to optimize the hyperparameters. Table 1 summarizes the word segmentation f-scores for all models described in this paper. 3.1 Unigram word adaptor grammar Johnson et al. (2007a) presented an adaptor grammar that defines a unigram model of word segmentation and showed that it performs as well as the unigram DP word segmentation model presented by (Goldwater et al., 2006a). The adaptor grammar that encodes a unigram word segmentation model shown in Figure 1. In this grammar and the grammars below, underlining indicates an adapted nonterminal. Phoneme is a nonterminal that expands to each of the 50 distinct phonemes present in the Brent corpus. This grammar defines a Sentence to consist of a sequence of Words, where a Word consists of a sequence of Phonemes. The categ</context>
<context position="15454" citStr="Johnson et al. (2007" startWordPosition="2530" endWordPosition="2533">f 0.55. Depending on the run, between 1,100 and 1, 400 subtrees (i.e., new rules) were found for Word. As reported in Goldwater et al. (2006a) and Goldwater et al. (2007), a unigram word segmentation model tends to undersegment and misanalyse collocations as individual words. This is presumably because the unigram model has no way to capture dependencies between words in collocations except to make the collocation into a single word. 3.2 Unigram morphology adaptor grammar This section investigates whether learning morphology together with word segmentation improves word segmentation accuracy. Johnson et al. (2007a) presented an adaptor grammar for segmenting verbs into stems and suffixes that implements the DPWord Word Word Word 401 Sentence → Word+ Word → Stem (Suffix) Stem → Phoneme+ Suffix → Phoneme+ Figure 4: The unigram morphology adaptor grammar, which generates each Sentence as a sequence of Words, and each Word as a Stem optionally followed by a Suffix. Parentheses indicate optional constituents. Sentence Figure 5: Parses of “wanna close it” and “you have to tell me” produced by the unigram morphology grammar of Figure 4. The first parse was chosen because it demonstrates how the grammar is in</context>
<context position="29568" citStr="Johnson et al., 2007" startWordPosition="4855" endWordPosition="4858">r better than the best ones reported in Table 1. The adaptor grammars presented here barely scratch the surface of the linguistically interesting models that can be expressed as Hierarchical Dirichlet Processes. The models of morphology presented here are particularly naive—they only capture regular concatenative morphology consisting of one paradigm class—which may partially explain why we obtained such poor results using morphology adaptor grammars. It’s straight forward to design an adaptor grammar that can capture a finite number of concatenative paradigm classes (Goldwater et al., 2006b; Johnson et al., 2007a). We’d like to learn the number of paradigm classes from the data, but doing this would probably require extending adaptor grammars to incorporate the kind of adaptive statesplitting found in the iHMM and iPCFG (Liang et al., 2007). There is no principled reason why this could not be done, i.e., why one could not design an HDP framework that simultaneously learns both the fragments (as in an adaptor grammar) and the states (as in an iHMM or iPCFG). However, inference with these more complex models will probably itself become more complex. The MCMC sampler of Johnson et al. (2007a) used here </context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas Griffiths, and Sharon Goldwater. 2007a. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 139–146, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Adaptor Grammars: A framework for specifying compositional nonparametric Bayesian models.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems 19,</booktitle>
<pages>641--648</pages>
<editor>In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5307" citStr="Johnson et al. (2007" startWordPosition="809" endWordPosition="813">leaves, but unlike a tree substitution grammar, in which the subtrees are specified in advance, in an adaptor grammar the subtrees, as well as their probabilities, are learnt from the training data. In order to make parsing and inference tractable we require the leaves of these subtrees to be terminals, as explained in section 2. Thus adaptor grammars are simple models of structure learning, where the subtrees that constitute the units of generalization are in effect new context-free rules learnt during the inference process. (In fact, the inference procedure for adaptor grammars described in Johnson et al. (2007b) relies on a PCFG approximation that contains a rule for each subtree generalization in the adaptor grammar). This paper applies adaptor grammars to word segmentation and morphological acquisition. Linguistically, these exhibit considerable cross-linguistic variation, and so are likely to be learned by human learners. It’s also plausible that semantics and contextual information is less important for their acquisition than, say, syntax. 2 From PCFGs to Adaptor Grammars This section introduces adaptor grammars as an extension of PCFGs; for a more detailed exposition see Johnson et al. (2007b)</context>
<context position="10110" citStr="Johnson et al. (2007" startWordPosition="1632" endWordPosition="1635">to itself); it is currently unknown whether there are tree distributions that satisfy the adaptor grammar constraints for recursive adaptor grammars. Inference for an adaptor grammar involves finding the rule probabilities θ and the adapted distributions over trees G. We put Dirichlet priors over the rule probabilities, i.e.: θA - DIR(βA) where θA is the vector of probabilities for the rules expanding the nonterminal A and βA are the corresponding Dirichlet parameters. The applications described below require unsupervised estimation, i.e., the training data consists of terminal strings alone. Johnson et al. (2007b) describe an MCMC procedure for inferring the adapted tree distributions GA, and Johnson et al. (2007a) describe a Bayesian inference procedure for the PCFG rule parameters θ using a MetropolisHastings MCMC procedure; implementations are available from the author’s web site. Informally, the inference procedure proceeds as follows. We initialize the sampler by randomly assigning each string in the training corpus a random tree generated by the grammar. Then we randomly select a string to resample, and sample a parse of that string with a PCFG approximation to the adaptor grammar. This PCFG co</context>
<context position="13103" citStr="Johnson et al. (2007" startWordPosition="2127" endWordPosition="2130">, which uses a unigram model to generate a sequence of words, where each word is a sequence of phonemes. Adapted nonterminals are underlined. der to concentrate mass on high probability parses. In all experiments below we set ,C3 = 1, which corresponds to a uniform prior on PCFG rule probabilities 0. We tied the Dirichlet Process concentration parameters α, and performed runs with α = 1,10,100 and 1000; apart from this, no attempt was made to optimize the hyperparameters. Table 1 summarizes the word segmentation f-scores for all models described in this paper. 3.1 Unigram word adaptor grammar Johnson et al. (2007a) presented an adaptor grammar that defines a unigram model of word segmentation and showed that it performs as well as the unigram DP word segmentation model presented by (Goldwater et al., 2006a). The adaptor grammar that encodes a unigram word segmentation model shown in Figure 1. In this grammar and the grammars below, underlining indicates an adapted nonterminal. Phoneme is a nonterminal that expands to each of the 50 distinct phonemes present in the Brent corpus. This grammar defines a Sentence to consist of a sequence of Words, where a Word consists of a sequence of Phonemes. The categ</context>
<context position="15454" citStr="Johnson et al. (2007" startWordPosition="2530" endWordPosition="2533">f 0.55. Depending on the run, between 1,100 and 1, 400 subtrees (i.e., new rules) were found for Word. As reported in Goldwater et al. (2006a) and Goldwater et al. (2007), a unigram word segmentation model tends to undersegment and misanalyse collocations as individual words. This is presumably because the unigram model has no way to capture dependencies between words in collocations except to make the collocation into a single word. 3.2 Unigram morphology adaptor grammar This section investigates whether learning morphology together with word segmentation improves word segmentation accuracy. Johnson et al. (2007a) presented an adaptor grammar for segmenting verbs into stems and suffixes that implements the DPWord Word Word Word 401 Sentence → Word+ Word → Stem (Suffix) Stem → Phoneme+ Suffix → Phoneme+ Figure 4: The unigram morphology adaptor grammar, which generates each Sentence as a sequence of Words, and each Word as a Stem optionally followed by a Suffix. Parentheses indicate optional constituents. Sentence Figure 5: Parses of “wanna close it” and “you have to tell me” produced by the unigram morphology grammar of Figure 4. The first parse was chosen because it demonstrates how the grammar is in</context>
<context position="29568" citStr="Johnson et al., 2007" startWordPosition="4855" endWordPosition="4858">r better than the best ones reported in Table 1. The adaptor grammars presented here barely scratch the surface of the linguistically interesting models that can be expressed as Hierarchical Dirichlet Processes. The models of morphology presented here are particularly naive—they only capture regular concatenative morphology consisting of one paradigm class—which may partially explain why we obtained such poor results using morphology adaptor grammars. It’s straight forward to design an adaptor grammar that can capture a finite number of concatenative paradigm classes (Goldwater et al., 2006b; Johnson et al., 2007a). We’d like to learn the number of paradigm classes from the data, but doing this would probably require extending adaptor grammars to incorporate the kind of adaptive statesplitting found in the iHMM and iPCFG (Liang et al., 2007). There is no principled reason why this could not be done, i.e., why one could not design an HDP framework that simultaneously learns both the fragments (as in an adaptor grammar) and the states (as in an iHMM or iPCFG). However, inference with these more complex models will probably itself become more complex. The MCMC sampler of Johnson et al. (2007a) used here </context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007b. Adaptor Grammars: A framework for specifying compositional nonparametric Bayesian models. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 641–648. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind Joshi</author>
</authors>
<title>Tree adjoining grammars.</title>
<date>2003</date>
<booktitle>The Oxford Handbook of Computational Linguistics,</booktitle>
<pages>483--501</pages>
<editor>In Ruslan Mikkov, editor,</editor>
<publisher>Oxford University Press,</publisher>
<location>Oxford, England.</location>
<contexts>
<context position="4387" citStr="Joshi, 2003" startWordPosition="660" endWordPosition="661">ramework for specifying a wide range of such models for grammatical inference. They can be viewed as a nonparametric extension of PCFGs. Informally, there seem to be at least two natural ways to construct non-parametric extensions of a PCFG. First, we can construct an infinite number of more specialized PCFGs by splitting or refining the PCFG’s nonterminals into increasingly finer states; this leads to the iPCFG or “infinite PCFG” (Liang et al., 2007). Second, we can generalize over arbitrary subtrees rather than local trees in much the way done in DOP or tree substitution grammar (Bod, 1998; Joshi, 2003), which leads to adaptor grammars. Informally, the units of generalization of adaptor grammars are entire subtrees, rather than just local trees, as in PCFGs. Just as in tree substitution grammars, each of these subtrees behaves as a new context-free rule that expands the subtree’s root node to its leaves, but unlike a tree substitution grammar, in which the subtrees are specified in advance, in an adaptor grammar the subtrees, as well as their probabilities, are learnt from the training data. In order to make parsing and inference tractable we require the leaves of these subtrees to be termin</context>
</contexts>
<marker>Joshi, 2003</marker>
<rawString>Aravind Joshi. 2003. Tree adjoining grammars. In Ruslan Mikkov, editor, The Oxford Handbook of Computational Linguistics, pages 483–501. Oxford University Press, Oxford, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Slav Petrov</author>
<author>Michael Jordan</author>
<author>Dan Klein</author>
</authors>
<title>The infinite PCFG using hierarchical Dirichlet processes.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>688--697</pages>
<contexts>
<context position="4230" citStr="Liang et al., 2007" startWordPosition="632" endWordPosition="635">models are not characterized by a finite vector of parameters, so the complexity of the model can vary depending on the data it describes). Adaptor grammars are a framework for specifying a wide range of such models for grammatical inference. They can be viewed as a nonparametric extension of PCFGs. Informally, there seem to be at least two natural ways to construct non-parametric extensions of a PCFG. First, we can construct an infinite number of more specialized PCFGs by splitting or refining the PCFG’s nonterminals into increasingly finer states; this leads to the iPCFG or “infinite PCFG” (Liang et al., 2007). Second, we can generalize over arbitrary subtrees rather than local trees in much the way done in DOP or tree substitution grammar (Bod, 1998; Joshi, 2003), which leads to adaptor grammars. Informally, the units of generalization of adaptor grammars are entire subtrees, rather than just local trees, as in PCFGs. Just as in tree substitution grammars, each of these subtrees behaves as a new context-free rule that expands the subtree’s root node to its leaves, but unlike a tree substitution grammar, in which the subtrees are specified in advance, in an adaptor grammar the subtrees, as well as </context>
<context position="29801" citStr="Liang et al., 2007" startWordPosition="4895" endWordPosition="4898">gy presented here are particularly naive—they only capture regular concatenative morphology consisting of one paradigm class—which may partially explain why we obtained such poor results using morphology adaptor grammars. It’s straight forward to design an adaptor grammar that can capture a finite number of concatenative paradigm classes (Goldwater et al., 2006b; Johnson et al., 2007a). We’d like to learn the number of paradigm classes from the data, but doing this would probably require extending adaptor grammars to incorporate the kind of adaptive statesplitting found in the iHMM and iPCFG (Liang et al., 2007). There is no principled reason why this could not be done, i.e., why one could not design an HDP framework that simultaneously learns both the fragments (as in an adaptor grammar) and the states (as in an iHMM or iPCFG). However, inference with these more complex models will probably itself become more complex. The MCMC sampler of Johnson et al. (2007a) used here is satifactory for small and medium-sized problems, but it would be very useful to have more efficient inference procedures. It may be possible to adapt efficient split-merge samplers (Jain and Neal, 2007) and Variational Bayes metho</context>
</contexts>
<marker>Liang, Petrov, Jordan, Klein, 2007</marker>
<rawString>Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein. 2007. The infinite PCFG using hierarchical Dirichlet processes. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 688–697.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian MacWhinney</author>
<author>Catherine Snow</author>
</authors>
<title>The child language data exchange system.</title>
<date>1985</date>
<journal>Journal of Child Language,</journal>
<pages>12--271</pages>
<contexts>
<context position="11361" citStr="MacWhinney and Snow, 1985" startWordPosition="1828" endWordPosition="1831">ach adapted subtree in the parses of the other strings in the training corpus. A final accept-reject step corrects for the difference in the probability of the sampled tree under the adaptor grammar and the PCFG approximation. 3 Word segmentation with adaptor grammars We now turn to linguistic applications of adaptor grammars, specifically, to models of unsupervised word segmentation. We follow previous work in using the Brent corpus consists of 9790 transcribed utterances (33,399 words) of childdirected speech from the Bernstein-Ratner corpus (Bernstein-Ratner, 1987) in the CHILDES database (MacWhinney and Snow, 1985). The utterances have been converted to a phonemic representation using a phonemic dictionary, so that each occurrence of a word has the same phonemic transcription. Utterance boundaries are given in the input to the system; other word boundaries are not. We evaluated the f-score of the recovered word constituents (Goldwater et al., 2006b). Using the adaptor grammar software available on the author’s web site, samplers were run for 10,000 epochs (passes through the training data). We scored the parses assigned to the training data at the end of sampling, and for the last two epochs we annealed</context>
</contexts>
<marker>MacWhinney, Snow, 1985</marker>
<rawString>Brian MacWhinney and Catherine Snow. 1985. The child language data exchange system. Journal of Child Language, 12:271–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin M¨uller</author>
</authors>
<title>Automatic detection of syllable boundaries combining the advantages of treebank and bracketed corpora training.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>M¨uller, 2001</marker>
<rawString>Karin M¨uller. 2001. Automatic detection of syllable boundaries combining the advantages of treebank and bracketed corpora training. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin M¨uller</author>
</authors>
<title>Probabilistic context-free grammars for phonology.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th Workshop of the ACL Special Interest Group in Computational Phonology (SIGPHON),</booktitle>
<pages>70--80</pages>
<location>Philadelphia.</location>
<marker>M¨uller, 2002</marker>
<rawString>Karin M¨uller. 2002. Probabilistic context-free grammars for phonology. In Proceedings of the 6th Workshop of the ACL Special Interest Group in Computational Phonology (SIGPHON), pages 70–80, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Bayesian Learning of Probabilistic Language Models.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California, Berkeley.</institution>
<contexts>
<context position="3166" citStr="Stolcke (1994)" startWordPosition="469" endWordPosition="470">re parameter estimation procedures, i.e., the procedure is designed to find the values of a finite vector of parameters. Standard methods for learning linguistic structure typically try to reduce structure learning to parameter estimation, say, by using an iterative generate-andprune procedure in which each iteration consists of a rule generation step that proposes new rules according to some scheme, a parameter estimation step that estimates the utility of these rules, and pruning step that removes low utility rules. For example, the Bayesian unsupervised PCFG estimation procedure devised by Stolcke (1994) uses a model-merging procedure to propose new sets of PCFG rules and a Bayesian version of the EM procedure to estimate their weights. 398 Proceedings of ACL-08: HLT, pages 398–406, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics Recently, methods have been developed in the statistical community for Bayesian inference of increasingly sophisticated non-parametric models. (“Non-parametric” here means that the models are not characterized by a finite vector of parameters, so the complexity of the model can vary depending on the data it describes). Adaptor grammar</context>
</contexts>
<marker>Stolcke, 1994</marker>
<rawString>Andreas Stolcke. 1994. Bayesian Learning of Probabilistic Language Models. Ph.D. thesis, University of California, Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
<author>M Jordan</author>
<author>M Beal</author>
<author>D Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<pages>101--1566</pages>
<contexts>
<context position="8487" citStr="Teh et al., 2006" startWordPosition="1361" endWordPosition="1364">in effect learning the probability of the subtrees rooted in a specified subset M of the nonterminals known as the adapted nonterminals. Adaptor grammars achieve this by associating each adapted nonterminal A E M with a Dirichlet Process (DP). A DP is a function of a base distribution H and a concentration parameter α, and it returns a distribution over distributions DP(α, H). There are several different ways to define DPs; one of the most useful is the characterization of the conditional or sampling distribution of a draw from DP(α, H) in terms of the Polya urn or Chinese Restaurant Process (Teh et al., 2006). The Polya urn initially contains αH(x) balls of color x. We sample a distribution from DP(α, H) by repeatedly drawing a ball at random from the urn and then returning it plus an additional ball of the same color to the urn. In an adaptor grammar there is one DP for each adapted nonterminal A E M, whose base distribution HA is the distribution over trees defined using A’s PCFG rules. This DP “adapts” A’s PCFG distribution by moving mass from the infrequently to the frequently occuring subtrees. An adaptor grammar associates a distribution GA that satisfies the following constraints with each </context>
<context position="16939" citStr="Teh et al., 2006" startWordPosition="2772" endWordPosition="2775">mmar with the unigram word segmentation grammar to produce the adaptor grammar shown in Figure 4, which is designed to simultaneously learn both word segmentation and morphology. Parentheses indicate optional constituents in these rules, so this grammar says that a Sentence consists of a sequence of Words, and each Word consists of a Stem followed by an optional Suffix. The categories Word, Stem and Suffix are adapted, which means that the grammar learns the Words, Stems and Suffixes that occur in the training corpus. Technically this grammar implements a Hierarchical Dirichlet Process (HDP) (Teh et al., 2006) because the base distribution for the Word DP is itself constructed from the Stem and Suffix distributions, which are themselves generated by DPs. This grammar recovers words with an f-score of only 0.46 with α = 1 or α = 10, which is considerably less accurate than the unigram model of section 3.1. Typical parses are shown in Figure 5. The unigram morphology grammar tends to misanalyse even longer collocations as words than the unigram word grammar does. Inspecting the parses shows that rather than capturing morphological structure, the Stem and Suffix categories typically expand to words th</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Y. W. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101:1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Kenichi Kurihara</author>
<author>Max Welling</author>
</authors>
<title>Collapsed variational inference for hdp.</title>
<date>2008</date>
<booktitle>Advances in Neural Information Processing Systems 20.</booktitle>
<editor>In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Teh, Kurihara, Welling, 2008</marker>
<rawString>Yee Whye Teh, Kenichi Kurihara, and Max Welling. 2008. Collapsed variational inference for hdp. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20. MIT Press, Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>