<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004788">
<title confidence="0.997954">
Acquiring Vocabulary for Predictive Text Entry
through Dynamic Reuse of a Small User Corpus
</title>
<author confidence="0.96867">
Kumiko TANAKA-Ishii* and Daichi Hayakawat and Masato TAKEICHIt
</author>
<affiliation confidence="0.973359">
The University of Tokyo
</affiliation>
<address confidence="0.838242">
7-3-1 Hongo, Bunkyoku, Tokyo, Japan.
</address>
<email confidence="0.996609">
kumiko,takeichi@mist.i.u-tokyo.ac.jp, daichiftak.dti.ne.jp
</email>
<sectionHeader confidence="0.980043" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999926391304348">
As mobile computing and communica-
tions have become popular, predictive
text entry systems have become an in-
creasingly important technology. Ex-
isting methods still need refinement,
though, with respect to personaliza-
tion, especially how to acquire vocab-
ulary not pre-registered in the system
dictionary. In this paper, we report
on an automatic method that dynami-
cally obtains a user specific vocabulary
from the user&apos;s unanalyzed documents.
When a user makes an entry, the sys-
tem dynamically extracts the corre-
sponding chunks from the user text
and suggests them along with words
suggested by the dictionary. With our
method, texts in a particular style or
concerning a specific domain can be en-
tered using a predictive text entry sys-
tem. We verified that a large amount
of words not registered in the dictio-
nary can be entered using our method.
</bodyText>
<sectionHeader confidence="0.997322" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999619375">
Recent advances in technologies now allow var-
ious means of information entry, such as char-
acter or speech recognition. Still, entry using a
keyboard remains dominant because of its ease
of implementation and its utility.
While various text entry methods can be used
with a keyboard, our concern in this paper is
predictive text entry. Any predictive method
</bodyText>
<affiliation confidence="0.575842">
* Language Informatics Laboratory, Information
Technology Center
tInterfaculty Initiative in Information Studies
t Graduate School of Information Science and Tech-
nology
</affiliation>
<listItem confidence="0.916588545454546">
allows text to be entered continuously in three
stages:
1. The user enters an ambiguous character se-
quence
2. The text entry system looks for correspond-
ing candidates in a dictionary pre-attached
to the software. It sorts the candidates with
regard to the context and displays them to
the user.
3. The user chooses his preferred word from
among the candidates.
</listItem>
<bodyText confidence="0.999630285714286">
For example, Chinese pinyin-hanji conversion is
a predictive text entry having an ambiguous se-
quence being pin yin and target words which are
hanji words.
Historically, such predictive text entry sys-
tems have been popular only in East Asian coun-
tries whose languages use many characters. The
problem with these languages was that the num-
ber of characters to be handled exceeds the num-
ber of keys on a keyboard. Therefore, predictive
text entry was invented to enable projection of a
wide range of characters using the limited num-
ber of keys on a keyboard.
This problem has become more international
as smaller machines have been developed. Cur-
rent devices can be as small as a wristwatch
(IBM, 2001), so the number of characters in
any language will exceed the number of buttons
available for input. As a result, predictive text
entry has been widely discussed in the academic
and industrial domains. The T9 method of en-
tering a digit sequence and predicting words is
an example of one way to deal with this problem
(Tegic, 2000). Research has even shown that an
entry can be made with reasonable efficiency us-
ing only four buttons if a predictive text entry
method is applied (Tanaka-Ishii et al., 2002).
The major drawback to this predictive text
</bodyText>
<tableCaption confidence="0.999539">
Table 1: The Rate of Unknown Words
</tableCaption>
<table confidence="0.997273785714286">
title (English) R1 R2
Adventures Of Sherlock 2.54 7.48
Holmes
Chat 14.78 15.11
The Merchant of Venice 7.34 13.00
Patent 2.94 7.74
RFC1459(Protocol Manual) 3.16 15.55
title (Japanese) R1 R2
Patent 2.76 17.35
RFC1459J 16.36 38.13
Tales of Genii (8-9 th cen- 13.03 2.91
tury)
I am a Cat (19 th century) 6.40 2.35
Chat 13.20 13.97
</table>
<bodyText confidence="0.999766846153846">
entry method is related to the dictionary use.
The user cannot enter words not registered in
the dictionary (what we refer to as unregis-
tered words in the following). Thus, a conven-
tional predictive entry system cannot easily han-
dle text written using a special vocabulary or in
an unusual style; for example, text written in a
particular dialect, or using old words or words
with specific technical meanings.
This problem is currently handled through the
creation of a user dictionary. Users can enter
unregistered words in some way (e.g., charac-
ter by character) and register the words into the
user dictionary. After that, the system ranks
these words highly when the user enters the cor-
responding sequence. However, it is the user&apos;s
responsibility to register the vocabulary into the
user dictionary and this often becomes a cum-
bersome task.
To alleviate this problem, some companies of-
fer dictionaries of vocabularies from specific do-
mains; for example, the chat dictionary or a dic-
tionary of dialect (JustSystems, 2002). How-
ever, the style that any one user enters into a
computer is likely to be unique, or more pre-
cisely, user specific.
We therefore propose a better method that
works by dynamically processing a small user
corpus. This is based on an interesting observa-
tion that a user typically reuses vocabulary at
a 70% rate after entry of only a small amount
of text. Based on this property, we have created
a simple predictive text entry system that dy-
namically extracts unknown words from the user
corpus. In this paper, we show how this system
solves the problem of unregistered words. We
start in the next section by explaining how we
came to observe the property that the typical
reuse rate is 70%.
</bodyText>
<sectionHeader confidence="0.603087" genericHeader="method">
2 The Property of Editorial Behavior
2.1 Data
</sectionHeader>
<bodyText confidence="0.999973117647059">
In this paper, we discuss our experiments re-
garding English and Japanese texts. For our
research, we used texts from diverse domains in
these two languages, as shown in Table 1. Each
text reflects individual&apos;s personal writing style.
We define an unregistered word as any vocab-
ulary not appearing in the 30-Mbyte corpus of
WSJ and Mainichi newspaper. Of course, the
word dictionary used by the predictive text en-
try can be larger. However, this won&apos;t make any
difference regarding our basic argument in this
paper, since the problem of unregistered words
always exists no matter what size of dictionary
we use.
As our concern was the unregistered vocabu-
lary, we first investigated the percentage of un-
registered words. We measured two rates:
</bodyText>
<listItem confidence="0.7913935">
R1 = No. of unknown words
N0. of total words
</listItem>
<bodyText confidence="0.999550529411765">
No. of different unregistered words
Japanese texts were all manually segmented
(because they contained unregistered words),
so statistics were calculated using the first 20
Kbytes of text. To be consistent, we used 20
Kbytes for English texts, too.
Table 1 shows R1 and R2 for our test texts,
with results in the upper half for English texts
and those in the lower half for Japanese. The
rate of unknown words differed according to the
type of text. It was especially high for the tech-
nical, colloquial, or old texts. Thus, both recent
and old texts can contain a large number of un-
registered words. As our registered words were
extracted from newspapers, we would guess that
newspapers usually use a standardized vocabu-
lary based on words that have been used long
</bodyText>
<figure confidence="0.99815403773585">
R2=
No. of different words
100
90
80
Rate of Reuse (English)
70
60
50
40
30
20
10
0
100
80
Rate of Reuse (Japanese)
60
40
20
0
Adventures Of Sherlock Holmes
Chat
The Merchant Of Venice
Patent
RFC1459
0 2 4 6 8 10 12 14 16 18 20
Offset of Text(Kbyes)
Patent
RFC1459J
Genji
Neko
Chat
0 2 4 6 8 10 12 14 16 18 20
Offset inside Text (Kbytes)
80
70
rate of used and unknown
60
50
40
30
20
Japanese Average
English Average
10
0
0 2 4 6 8 10 12 14 16 18 20
text size(kb)
0 1 2 3 4
symbols symbols ABC DEF GHI
5 6 7 8 9
JKL MNO PQRS TUV WXYZ
</figure>
<figureCaption confidence="0.87394">
Figure 3: An Alphabet Assignment on Digits
</figureCaption>
<figure confidence="0.99783728125">
O Romeo, Romeo! &gt; 943733
enter wheref&apos;
(1.wherefore )
(2.wherefore)
(3.wherefores)
O Romeo, Romeo! &gt; +1
choose No.1 candidate
O Romeo, Romeo! wherefore &gt; 278
enter tart&apos;
(1.art thou happy.)
(2.art thou )
(3.art thou happy)
(4.aqua)
(5.art )
O Romeo, Romeo! wherefore &gt; +2
choose No.2 candidate
O Romeo, Romeo! wherefore art thou &gt; 76
enter Ro&apos; of Romeo
(iso)
(2.so )
(3.Romeo)
(4.sought )
(5.some )
O Romeo, Romeo! wherefore art thou &gt; +3
choose No.3 candidate
O Romeo, Romeo! wherefore art thou Romeo&gt; 0
enter the question mark (0 for symbols)
(1.?)
(2. )
O Romeo, Romeo! wherefore art thou Romeo&gt; +1
choose No.1 candidate
O Romeo, Romeo! wherefore art thou Romeo?&gt;
</figure>
<figureCaption confidence="0.936979333333333">
Figure 4: Entering &apos;Romeo and Juliet&apos; with Our
Simple Evaluation System (In a Manner of En-
glish Mobile Phone)
</figureCaption>
<bodyText confidence="0.993666230769231">
discussed in (Nakagawa and Mori, 2002). How-
ever, such methods are designed to obtain a lim-
ited number of strictly defined and specialized
terms from a huge corpus assuming that taggers
work properly. However, in our case, we should
not assume that any language tools will work
properly, because our target is the unregistered
words. Therefore, since our target is to include
non-segmented languages, our method should be
string based.
Intuitively, what we want here is for the text
chunk to be used always together. Under the
constraint that the user corpus is small, what we
may look at is the repetition of strings. How-
ever, if we extract all repeated strings, the num-
ber of candidates explodes and the user will be
forced to look at inadequate chunks (because
when &amp;quot;the&amp;quot; repeats, &amp;quot;th&amp;quot; or &amp;quot;t&amp;quot; are also re-
peated). Therefore, we decided to extract the
maximal repeated prefixes. A maximal re-
peated prefix is a repeated string that does not
occur as the prefix of another prefix string.
For example, if &amp;quot;abracadabra&amp;quot; is the user cor-
pus, and the user entered &amp;quot;a&amp;quot;, then
abra(2), abr(2), ab(2), a(5)
are the repeated strings that can be the can-
didates. Among these, &amp;quot;abr&amp;quot;, &amp;quot;ab&amp;quot;, and &amp;quot;a&amp;quot;
(twice) occur as the prefix of two occurrences
of &amp;quot;abra&amp;quot;. Therefore, we eliminate these, which
leaves:
abra(2), a(3) .
Another two &amp;quot;a&amp;quot; occurred as part of &amp;quot;abra&amp;quot;, but
it occurred as the post fix of &amp;quot;abra&amp;quot;, so, it is
shown among the candidates. As the maximal
repeated prefixes can be quickly extracted using
the suffix array (Manber and Myers, 1993), the
system explained in the previous section trans-
forms the user corpus into a suffix array when it
is initiated.
</bodyText>
<subsectionHeader confidence="0.994407">
4.2 Ranking
</subsectionHeader>
<bodyText confidence="0.973380666666667">
Candidates are displayed in a certain order.
This order is determined by an evaluation func-
tion, and we chose that it be done using the
PPM (prediction by partial match) framework
(Bell et al., 1990). We decided to use the
PPM because the context provides the best
information for selecting adequate candidates.
PPM integrates such a concept by interpolat-
ing the statistics obtained from the user corpus
and the initial language model obtained from a
huge corpus. PPM language models have been
used in much of the earlier works (Ward et al.,
2000)(Tanaka-Ishii et al., 2002), and has been
found to be superior to other language models
such as fixed n-grams and co-occurrence-based
methods (Maruyama et al., 2001).
PPM can be situated as variant n-gram lan-
guage models. It interpolates the n-gram counts
in the user corpus and the statistics in the base
dictionary. The following formula is used to esti-
mate a probability for the ith element wj, P(w):
</bodyText>
<equation confidence="0.990327333333333">
kmax
Pew) Uk Pk (U% i) (3)
k= â€” 1
</equation>
<bodyText confidence="0.99994734375">
Here, k, the order, indicates the number of ele-
ments before wi that are used in the calculation
of Pk (1V). For example, P2 (11% i) is estimated on
the basis of the occurrence of wi_1 and wi_2.
kinax is the length of available context and is
set at 4, in our study (thus maximally 5-grams
are concerned). Pk (1V ) is calculated as:
where C k is the frequency of the current k ele-
ment context, and ck (wi) is the frequency with
which wi occurs in that context. P_1(wi) de-
scribes a base initial probability assuming no
context. In our case, wi is a registered word
or an unregistered chunk. For an unregistered
chunk, the initial probability cannot be obtained
because the word is unregistered. Therefore, we
set the initial probability at a constant value.
For other k, Pk(w) is calculated from statis-
tics obtained from the user corpus. Here, el-
ements cannot correspond to words, because
the user corpus is unanalyzed. Therefore, con-
textual elements are counted by characters,
whereas the current element in question is the
unregistered chunk or the registered dictionary
word.
Finally, uk is a weighting probability that is
multiplied by Pk (1V d to obtain the final P(w).
There have been many studies of uk (Teahan,
2000), and we have chosen to use PPM-A(Bell
et al., 1990), the simplest form, because our pre-
liminary experiments showed no significant dif-
ference in performance among the various meth-
ods we tried.
</bodyText>
<sectionHeader confidence="0.9107165" genericHeader="evaluation">
5 Evaluation
5.1 Settings
</sectionHeader>
<bodyText confidence="0.9999535">
Using the tool described in 9, we examined to
what extent the predictive text entry system was
improved. The experiment was done by auto-
matically entering test text into our system.
The test text was prepared according to the
following stages.
</bodyText>
<listItem confidence="0.924603875">
1. All texts used in V were separated at the
sentence level.
2. Sentences were sorted into a random or-
der to measure the average capability of the
method.
3. First 5000 bytes were taken as the test text.
For Japanese, this test text was all manu-
ally analyzed and cut into words. The rest
</listItem>
<bodyText confidence="0.994325913043478">
of the text was used as the learning cor-
pus by differentiating the size of the corpus.
The smaller learning corpus was included in
the larger corpora.
Stages 2 and 3 were repeated five times so that
five different sets of the test and the learning
corpora were obtained. All of the graphs that
follow show the average of the test results for
these five different sets (five-times cross valida-
tion).
The automatic text entry proceeded as fol-
lows. First, an ambiguous sequence correspond-
ing to the first word was entered. Two lists of
candidates were then shown by the system, one
list obtained from the word dictionary, and the
other from the user corpus. The string that cor-
responded to the prefix of the target test text
was then chosen (the correct candidate). When
there were multiple correct candidates among
the two lists, the highest ranked candidate was
chosen. If two correct candidates were equally
ranked on the two lists, the longer candidate was
chosen.
</bodyText>
<sectionHeader confidence="0.682738" genericHeader="references">
5.2 Results
</sectionHeader>
<bodyText confidence="0.999922833333333">
First, we consider the rate of unregistered words
being successfully entered with our method.
Figure 5 shows the results for English (left)
and Japanese (right), where the horizontal axis
shows the user corpus size and the vertical axis
shows the rate of successfully entered unregis-
</bodyText>
<figure confidence="0.998815875">
Pk etai) = Ck (1V i)
C k
(4)
0 20 40 60 80 100
Kbytes
0 20 40 60 80 100
Kbytes
Rate of SUccessfully Entered Unregistered Words
80
60
40
20
70
50
30
10
0
Adventure Of Sherlock Homes
Chat-E
Patent for CreatingCommunity
The Merchant of Venice
RFC1459
Rate of Successfully Entered Unregistered Words
80
60
40
20
70
50
30
10
0
Genji
Neko
Chat
RFC1459J
Patent
100
90
80
70
from corpus
60
50
40
30
20
10
Adventure Of Sherlock Homes
Chat-E
Patent for CreatingCommunity
The Merchant of Venice
RFC1459
0
0 20 40 60 80 100
Kbytes
1.6
1.55
1.5
emergence position
1.45
1.4
1.35
1.3
Adventure Of Sherlock Homes
Chat-E
Patent for CreatingCommunity
The Merchant of Venice
RFC1459
1.25
0 20 40 60 80 100
Kbytes
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.837037">
<title confidence="0.9431425">Acquiring Vocabulary for Predictive Text Entry through Dynamic Reuse of a Small User Corpus</title>
<author confidence="0.935828">TANAKA-Ishii Hayakawat TAKEICHIt</author>
<affiliation confidence="0.999833">The University of Tokyo</affiliation>
<address confidence="0.99986">7-3-1 Hongo, Bunkyoku, Tokyo, Japan.</address>
<email confidence="0.996743">daichiftak.dti.ne.jp</email>
<abstract confidence="0.999245541666667">As mobile computing and communications have become popular, predictive text entry systems have become an increasingly important technology. Existing methods still need refinement, though, with respect to personalization, especially how to acquire vocabulary not pre-registered in the system dictionary. In this paper, we report on an automatic method that dynamically obtains a user specific vocabulary from the user&apos;s unanalyzed documents. When a user makes an entry, the system dynamically extracts the corresponding chunks from the user text and suggests them along with words suggested by the dictionary. With our method, texts in a particular style or concerning a specific domain can be entered using a predictive text entry system. We verified that a large amount of words not registered in the dictionary can be entered using our method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>