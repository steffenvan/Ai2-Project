<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000036">
<title confidence="0.991822">
Modeling the Translation of Predicate-Argument Structure for SMT
</title>
<author confidence="0.983744">
Deyi Xiong, Min Zhang∗, Haizhou Li
</author>
<affiliation confidence="0.869401666666667">
Human Language Technology
Institute for Infocomm Research
1 Fusionopolis Way, #21-01 Connexis, Singapore 138632
</affiliation>
<email confidence="0.994557">
{dyxiong, mzhang, hli}@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.993438" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.982274409090909">
Predicate-argument structure contains rich se-
mantic information of which statistical ma-
chine translation hasn’t taken full advantage.
In this paper, we propose two discriminative,
feature-based models to exploit predicate-
argument structures for statistical machine
translation: 1) a predicate translation model
and 2) an argument reordering model. The
predicate translation model explores lexical
and semantic contexts surrounding a verbal
predicate to select desirable translations for
the predicate. The argument reordering model
automatically predicts the moving direction
of an argument relative to its predicate af-
ter translation using semantic features. The
two models are integrated into a state-of-the-
art phrase-based machine translation system
and evaluated on Chinese-to-English transla-
tion tasks with large-scale training data. Ex-
perimental results demonstrate that the two
models significantly improve translation accu-
racy.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.988962239130435">
Recent years have witnessed increasing efforts to-
wards integrating predicate-argument structures into
statistical machine translation (SMT) (Wu and Fung,
2009b; Liu and Gildea, 2010). In this paper, we take
a step forward by introducing a novel approach to in-
corporate such semantic structures into SMT. Given
a source side predicate-argument structure, we at-
tempt to translate each semantic frame (predicate
and its associated arguments) into an appropriate tar-
get string. We believe that the translation of predi-
cates and reordering of arguments are the two central
∗Corresponding author
issues concerning the transfer of predicate-argument
structure across languages.
Predicates1 are essential elements in sentences.
Unfortunately they are usually neither correctly
translated nor translated at all in many SMT sys-
tems according to the error study by Wu and Fung
(2009a). This suggests that conventional lexical and
phrasal translation models adopted in those SMT
systems are not sufficient to correctly translate pred-
icates in source sentences. Thus we propose a
discriminative, feature-based predicate translation
model that captures not only lexical information
(i.e., surrounding words) but also high-level seman-
tic contexts to correctly translate predicates.
Arguments contain information for questions of
who, what, when, where, why, and how in sentences
(Xue, 2008). One common error in translating ar-
guments is about their reorderings: arguments are
placed at incorrect positions after translation. In or-
der to reduce such errors, we introduce a discrim-
inative argument reordering model that uses the
position of a predicate as the reference axis to es-
timate positions of its associated arguments on the
target side. In this way, the model predicts moving
directions of arguments relative to their predicates
with semantic features.
We integrate these two discriminative models into
a state-of-the-art phrase-based system. Experimen-
tal results on large-scale Chinese-to-English transla-
tion show that both models are able to obtain signif-
icant improvements over the baseline. Our analysis
on system outputs further reveals that they can in-
deed help reduce errors in predicate translations and
argument reorderings.
</bodyText>
<footnote confidence="0.889146">
1We only consider verbal predicates in this paper.
</footnote>
<page confidence="0.86246">
902
</page>
<note confidence="0.9864735">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 902–911,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999099692307692">
The paper is organized as follows. In Section 2,
we will introduce related work and show the signif-
icant differences between our models and previous
work. In Section 3 and 4, we will elaborate the pro-
posed predicate translation model and argument re-
ordering model respectively, including details about
modeling, features and training procedure. Section
5 will introduce how to integrate these two models
into SMT. Section 6 will describe our experiments
and results. Section 7 will empirically discuss how
the proposed models improve translation accuracy.
Finally we will conclude with future research direc-
tions in Section 8.
</bodyText>
<sectionHeader confidence="0.999811" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999882265306122">
Predicate-argument structures (PAS) are explored
for SMT on both the source and target side in some
previous work. As PAS analysis widely employs
global and sentence-wide features, it is computa-
tionally expensive to integrate target side predicate-
argument structures into the dynamic programming
style of SMT decoding (Wu and Fung, 2009b).
Therefore they either postpone the integration of tar-
get side PASs until the whole decoding procedure is
completed (Wu and Fung, 2009b), or directly project
semantic roles from the source side to the target side
through word alignments during decoding (Liu and
Gildea, 2010).
There are other previous studies that explore only
source side predicate-argument structures. Komachi
and Matsumoto (2006) reorder arguments in source
language (Japanese) sentences using heuristic rules
defined on source side predicate-argument structures
in a pre-processing step. Wu et al. (2011) automate
this procedure by automatically extracting reorder-
ing rules from predicate-argument structures and ap-
plying these rules to reorder source language sen-
tences. Aziz et al. (2011) incorporate source lan-
guage semantic role labels into a tree-to-string SMT
system.
Although we also focus on source side predicate-
argument structures, our models differ from the pre-
vious work in two main aspects: 1) we propose two
separate discriminative models to exploit predicate-
argument structures for predicate translation and ar-
gument reordering respectively; 2) we consider ar-
gument reordering as an argument movement (rel-
ative to its predicate) prediction problem and use
a discriminatively trained classifier for such predic-
tions.
Our predicate translation model is also related to
previous discriminative lexicon translation models
(Berger et al., 1996; Venkatapathy and Bangalore,
2007; Mauser et al., 2009). While previous models
predict translations for all words in vocabulary, we
only focus on verbal predicates. This will tremen-
dously reduce the amount of training data required,
which usually is a problem in discriminative lexi-
con translation models (Mauser et al., 2009). Fur-
thermore, the proposed translation model also dif-
fers from previous lexicon translation models in that
we use both lexical and semantic features. Our ex-
perimental results show that semantic features are
able to further improve translation accuracy.
</bodyText>
<sectionHeader confidence="0.997743" genericHeader="method">
3 Predicate Translation Model
</sectionHeader>
<bodyText confidence="0.9996095">
In this section, we present the features and the train-
ing process of the predicate translation model.
</bodyText>
<subsectionHeader confidence="0.998452">
3.1 Model
</subsectionHeader>
<bodyText confidence="0.999979571428572">
Following the context-dependent word models in
(Berger et al., 1996), we propose a discriminative
predicate translation model. The essential compo-
nent of our model is a maximum entropy classifier
pt(e|C(v)) that predicts the target translation e for
a verbal predicate v given its surrounding context
C(v). The classifier can be formulated as follows.
</bodyText>
<equation confidence="0.994549">
exp(Ei Bifi(e,C(v)))
pt(e|C(v)) = (1)
Ee′ exp(Ei Bifi(e′, C(v)))
</equation>
<bodyText confidence="0.9999245">
where fi are binary features, Bi are weights of these
features. Given a source sentence which contains
N verbal predicates {vi}N1 , our predicate translation
model Mt can be denoted as
</bodyText>
<equation confidence="0.989341">
N
Mt = pt(evi|C(vi)) (2)
i=1
</equation>
<bodyText confidence="0.9997565">
Note that we do not restrict the target translation
e to be a single word. We allow e to be a phrase
of length up to 4 words so as to capture multi-word
translations for a verbal predicate. For example, a
Chinese verb “R(T(issue)” can be translated as “to
be issued” or “have issued” with modality words.
</bodyText>
<page confidence="0.998144">
903
</page>
<bodyText confidence="0.999888166666667">
This will increase the number of classes to be pre-
dicted by the maximum entropy classifier. But ac-
cording to our observation, it is still computation-
ally tractable (see Section 3.3). If a verbal predicate
is not translated, we set e = NULL so that we can
also capture null translations for verbal predicates.
</bodyText>
<subsectionHeader confidence="0.987859">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.9998379">
The apparent advantage of discriminative lexicon
translation models over generative translation mod-
els (e.g., conventional lexical translation model as
described in (Koehn et al., 2003)) is that discrim-
inative models allow us to integrate richer contexts
(lexical, syntactic or semantic) into target translation
prediction. We use two kinds of features to predict
translations for verbal predicates: 1) lexical features
and 2) semantic features. All features are in the fol-
lowing binary form.
</bodyText>
<equation confidence="0.920115">
� 1, if e = * and C(v).Q = ♠
f(e,C(v)) =
0, else
(3)
</equation>
<bodyText confidence="0.983591923076923">
where the symbol * is a placeholder for a possible
target translation (up to 4 words), the symbol Q indi-
cates a contextual (lexical or semantic) element for
the verbal predicate v, and the symbol ♠ represents
the value of Q.
Lexical Features: The lexical element Q is
extracted from the surrounding words of verbal
predicate v. We use the preceding 3 words and
the succeeding 3 words to define the lexical con-
text for the verbal predicate v. Therefore Q E
{w−3, w−2, w−1, v, w1, w2, w31.
Semantic Features: The semantic element Q is
extracted from the surrounding arguments of ver-
bal predicate v. In particular, we define a seman-
tic window centered at the verbal predicate with
6 arguments {A−3, A−2, A−1, A1, A2, A31 where
A−3 − A−1 are arguments on the left side of v
while A1 − A3 are those on the right side. Differ-
ent verbal predicates have different number of argu-
ments in different linguistic scenarios. We observe
on our training data that the number of arguments for
96.5% verbal predicates on each side (left/right) is
not larger than 3. Therefore the defined 6-argument
semantic window is sufficient to describe argument
contexts for predicates.
For each argument Ai in the defined seman-
</bodyText>
<equation confidence="0.972856333333333">
f(e, C(v)) = 1 if and only if
e = adjourn and C(v).Ah−3 = #tom
e = adjourn and C(v).Ar−1 = ARGM-TMP
e = adjourn and C(v).Ah1 =
e = adjourn and C(v).Ar2 = null
e = adjourn and C(v).Ah3 =null
</equation>
<tableCaption confidence="0.993839">
Table 1: Semantic feature examples.
</tableCaption>
<bodyText confidence="0.999969076923077">
tic window, we use its semantic role (i.e., ARG0,
ARGM-TMP and so on) Az and head word AZ to
define semantic context elements Q. If an argument
Ai does not exist for the verbal predicate v 2, we set
the value of both Az and AZ to null.
Figure 1 shows a Chinese sentence with its
predicate-argument structure and English transla-
tion. The verbal predicate “�*c--zP�./adjourn” (in bold)
has 4 arguments: one in an ARG0 agent role, one
in an ARGM-ADV adverbial modifier role, one in
an ARGM-TMP temporal modifier role and the last
one in an ARG1 patient role. Table 1 shows several
semantic feature examples of this verbal predicate.
</bodyText>
<subsectionHeader confidence="0.99785">
3.3 Training
</subsectionHeader>
<bodyText confidence="0.99996055">
In order to train the discriminative predicate transla-
tion model, we first parse source sentences and la-
beled semantic roles for all verbal predicates (see
details in Section 6.1) in our word-aligned bilingual
training data. Then we extract all training events for
verbal predicates which occur at least 10 times in
the training data. A training event for a verbal predi-
cate v consists of all contextual elements C(v) (e.g.,
w1, A�1) defined in the last section and the target
translation e. Using these events, we train one max-
imum entropy classifier per verbal predicate (16,121
verbs in total) via the off-the-shelf MaxEnt toolkit3.
We perform 100 iterations of the L-BFGS algorithm
implemented in the training toolkit for each verbal
predicate with both Gaussian prior and event cutoff
set to 1 to avoid overfitting. After event cutoff, we
have an average of 140 classes (target translations)
per verbal predicate with the maximum number of
classes being 9,226. The training takes an average of
52.6 seconds per verb. In order to expedite the train-
</bodyText>
<footnote confidence="0.954618">
2For example, the verb v has only two arguments on its left
side. Thus argument A−3 doest not exist.
3Available at: http://homepages.inf.ed.ac.uk/lzhang10/
maxent toolkit.html
</footnote>
<page confidence="0.994757">
904
</page>
<figure confidence="0.8770545">
The [Security Council] will adjourn for [4 days] [starting Thursday]
1 *2 [03 )WR4 ffM5] [�7 )c8]
</figure>
<figureCaption confidence="0.9729875">
Figure 1: An example of predicate-argument structure in Chinese and its aligned English translation. The bold word in
Chinese is the verbal predicate. The subscripts on the Chinese sentence show the indexes of words from left to right.
</figureCaption>
<figure confidence="0.99512125">
ARGM-TMP
ARG1
ARGM-ADV
ARG0
</figure>
<bodyText confidence="0.639551">
ing, we run the training toolkit in a parallel manner.
</bodyText>
<sectionHeader confidence="0.984756" genericHeader="method">
4 Argument Reordering Model
</sectionHeader>
<bodyText confidence="0.999828666666667">
In this section we introduce the discriminative ar-
gument reordering model, features and the training
procedure.
</bodyText>
<subsectionHeader confidence="0.989642">
4.1 Model
</subsectionHeader>
<bodyText confidence="0.999985740740741">
Since the predicate determines what arguments are
involved in its semantic frame and semantic frames
tend to be cohesive across languages (Fung et al.,
2006), the movements of predicate and its arguments
across translations are like the motions of a planet
and its satellites. Therefore we consider the reorder-
ing of an argument as the motion of the argument
relative to its predicate. In particular, we use the po-
sition of the predicate as the reference axis. The mo-
tion of associated arguments relative to the reference
axis can be roughly divided into 3 categories4: 1) no
change across languages (NC); 2) moving from the
left side of its predicate to the right side of the predi-
cate after translation (L2R); and 3) moving from the
right side of its predicate to the left side of the pred-
icate after translation (R2L).
Let’s revisit Figure 1. The ARG0, ARGM-ADV
and ARG1 are located at the same side of their predi-
cate after being translated into English, therefore the
reordering category of these three arguments is as-
signed as “NC”. The ARGM-TMP is moved from
the left side of “�* /adjourn” to the right side of
“adjourn” after translation, thus its reordering cate-
gory is L2R.
In order to predict the reordering category for
an argument, we propose a discriminative argu-
ment reordering model that uses a maximum en-
</bodyText>
<footnote confidence="0.628161">
4Here we assume that the translations of arguments are not
interrupted by their predicates, other arguments or any words
outside the arguments in question. We leave for future research
the task of determining whether arguments should be translated
as a unit or not.
</footnote>
<bodyText confidence="0.947717333333333">
tropy classifier to calculate the reordering category
m E {NC, L2R, R2L} for an argument A as fol-
lows.
</bodyText>
<equation confidence="0.985575333333333">
exp(Ei �ifi(m, C(A)))
pr(m|C(A)) = (4)
Em′ exp(Ei Oi fi (m′, C (A)))
</equation>
<bodyText confidence="0.9999385">
where C(A) indicates the surrounding context of A.
The features fi will be introduced in the next sec-
tion. We assume that motions of arguments are in-
dependent on each other. Given a source sentence
with labeled arguments {Ai}N1 , our discriminative
argument reordering model Mr is formulated as
</bodyText>
<equation confidence="0.985402666666667">
N
Mr = pr(mAi|C(Ai)) (5)
i=1
</equation>
<subsectionHeader confidence="0.495848">
4.2 Features
</subsectionHeader>
<bodyText confidence="0.999958">
The features fi used in the argument reordering
model still takes the binary form as in Eq. (3). Table
2 shows the features that are used in the argument
reordering model. We extract features from both the
source and target side. On the source side, the fea-
tures include the verbal predicate, the semantic role
of the argument, the head word and the boundary
words of the argument. On the target side, the trans-
lation of the verbal predicate, the translation of the
head word of the argument, as well as the boundary
words of the translation of the argument are used as
features.
</bodyText>
<subsectionHeader confidence="0.995927">
4.3 Training
</subsectionHeader>
<bodyText confidence="0.999955222222222">
To train the argument reordering model, we first ex-
tract features defined in the last section from our
bilingual training data where source sentences are
annotated with predicate-argument structures. We
also study the distribution of argument reordering
categories (i.e.,NC, L2R and R2L) in the training
data, which is shown in Table 3. Most arguments,
accounting for 82.43%, are on the same side of their
verbal predicates after translation. The remaining
</bodyText>
<page confidence="0.993188">
905
</page>
<bodyText confidence="0.9692465">
Features of an argument A for reordering
its verbal predicate Ap
its semantic role Ar
src its head word Ah
the leftmost word of A
the rightmost word of A
the translation of Ap
tgt the translation of Ah
the leftmost word of the translation of A
the rightmost word of the translation of A
</bodyText>
<tableCaption confidence="0.99733975">
Table 2: Features adopted in the argument reordering
model.
Table 3: Distribution of argument reordering categories
in the training data.
</tableCaption>
<bodyText confidence="0.999727222222222">
arguments (17.57%) are moved either from the left
side of their predicates to the right side after transla-
tion (accounting for 11.19%) or from the right side
to the left side of their translated predicates (ac-
counting for 6.38%).
After all features are extracted, we use the maxi-
mum entropy toolkit in Section 3.3 to train the maxi-
mum entropy classifier as formulated in Eq. (4). We
perform 100 iterations of L-BFGS.
</bodyText>
<sectionHeader confidence="0.9152" genericHeader="method">
5 Integrating the Two Models into SMT
</sectionHeader>
<bodyText confidence="0.9998428">
In this section, we elaborate how to integrate the two
models into phrase-based SMT. In particular, we in-
tegrate the models into a phrase-based system which
uses bracketing transduction grammars (BTG) (Wu,
1997) for phrasal translation (Xiong et al., 2006).
Since the system is based on a CKY-style decoder,
the integration algorithms introduced here can be
easily adapted to other CKY-based decoding sys-
tems such as the hierarchical phrasal system (Chi-
ang, 2007).
</bodyText>
<subsectionHeader confidence="0.745859">
5.1 Integrating the Predicate Translation
Model
</subsectionHeader>
<bodyText confidence="0.9981965">
It is straightforward to integrate the predicate trans-
lation model into phrase-based SMT (Koehn et al.,
</bodyText>
<figure confidence="0.996426375">
NC
L2R
R2L
82.43%
11.19%
6.38%
Reordering Category
Percent
</figure>
<bodyText confidence="0.995931380952381">
2003; Xiong et al., 2006). We maintain word
alignments for each phrase pair in the phrase ta-
ble. Given a source sentence with its predicate-
argument structure, we detect all verbal predicates
and load trained predicate translation classifiers for
these verbs. Whenever a hypothesis covers a new
verbal predicate v, we find the target translation e
for v through word alignments and then calculate its
translation probability pt(e|C(v)) according to Eq.
(1).
The predicate translation model (as formulated in
Eq. (2)) is integrated into the whole log-linear model
just like the conventional lexical translation model
in phrase-based SMT (Koehn et al., 2003). The
two models are independently estimated but comple-
mentary to each other. While the lexical translation
model calculates the probability of a verbal predi-
cate being translated given its local lexical context,
the discriminative predicate translation model is able
to employ both lexical and semantic contexts to pre-
dict translations for verbs.
</bodyText>
<figure confidence="0.6034185">
5.2 Integrating the Argument Reordering
Model
</figure>
<figureCaption confidence="0.837376166666667">
Before we introduce the integration algorithm for
the argument reordering model, we define two
functions A and N on a source sentence and its
predicate-argument structure z as follows.
• A(i, j, z): from the predicate-argument struc-
ture z, the function finds all predicate-argument
pairs which are completely located within the
span from source word i to j. For example, in
Figure 1, A(3,6, z) = {(Wez&apos; :, ARGM-TMP)}
while A(2,3, z) = {}, A(1, 5, z) = {} because
the verbal predicate “��” is located outside
the span (2,3) and (1,5).
</figureCaption>
<bodyText confidence="0.866481">
• N(i, k, j, z): the function finds all predicate-
argument pairs that cross the two neighboring
spans (i, k) and (k+1, j). It can be formulated
as A(i,j,z) − (A(i,k,z) UA(k + 1, j, z)).
We then define another function Pr to calculate
the argument reordering model probability on all ar-
guments which are found by the previous two func-
tions A and N as follows.
</bodyText>
<equation confidence="0.807249">
Pr(B) = 11 pr(mA|C(A)) (6)
A∈B
</equation>
<page confidence="0.989614">
906
</page>
<bodyText confidence="0.999805541666667">
where B denotes either A or N.
Following (Chiang, 2007), we describe the algo-
rithm in a deductive system. It is shown in Figure
2. The algorithm integrates the argument reordering
model into a CKY-style decoder (Xiong et al., 2006).
The item [X, i, j] denotes a BTG node X spanning
from i to j on the source side. For notational con-
venience, we only show the argument reordering
model probability for each item, ignoring all other
sub-model probabilities such as the language model
probability. The Eq. (7) shows how we calculate the
argument reordering model probability when a lex-
ical rule is applied to translate a source phrase c to
a target phrase e. The Eq. (8) shows how we com-
pute the argument reordering model probability for a
span (i, j) in a dynamic programming manner when
a merging rule is applied to combine its two sub-
spans in a straight (X → [X1, X2]) or inverted or-
der (X → (X1, X2)). We directly use the probabili-
ties P,(A(i, k, -r)) and P,(A(k + 1, j, -r)) that have
been already obtained for the two sub-spans (i, k)
and (k + 1, j). In this way, we only need to calcu-
late the probability P,(N(i, k, j, -r)) for predicate-
argument pairs that cross the two sub-spans.
</bodyText>
<sectionHeader confidence="0.997868" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.950589833333333">
In this section, we present our experiments on
Chinese-to-English translation tasks, which are
trained with large-scale data. The experiments are
aimed at measuring the effectiveness of the proposed
discriminative predicate translation model and argu-
ment reordering model.
</bodyText>
<subsectionHeader confidence="0.991546">
6.1 Setup
</subsectionHeader>
<bodyText confidence="0.9999688">
The baseline system is the BTG-based phrasal sys-
tem (Xiong et al., 2006). Our training corpora5
consist of 3.8M sentence pairs with 96.9M Chinese
words and 109.5M English words. We ran GIZA++
on these corpora in both directions and then applied
the “grow-diag-final” refinement rule to obtain word
alignments. We then used all these word-aligned
corpora to generate our phrase table. Our 5-gram
language model was trained on the Xinhua section
of the English Gigaword corpus (306 million words)
</bodyText>
<footnote confidence="0.808367333333333">
5The corpora include LDC2004E12, LDC2004T08,
LDC2005T10, LDC2003E14, LDC2002E18, LDC2005T06,
LDC2003E07 and LDC2004T07.
</footnote>
<bodyText confidence="0.99909347368421">
using the SRILM toolkit (Stolcke, 2002) with modi-
fied Kneser-Ney smoothing.
To train the proposed predicate translation model
and argument reordering model, we first parsed all
source sentences using the Berkeley Chinese parser
(Petrov et al., 2006) and then ran the Chinese se-
mantic role labeler6 (Li et al., 2010) on all source
parse trees to annotate semantic roles for all verbal
predicates. After we obtained semantic roles on the
source side, we extracted features as described in
Section 3.2 and 4.2 and used these features to train
our two models as described in Section 3.3 and 4.3.
We used the NIST MT03 evaluation test data as
our development set, and the NIST MT04, MT05
as the test sets. We adopted the case-insensitive
BLEU-4 (Papineni et al., 2002) as the evaluation
metric. Statistical significance in BLEU differences
was tested by paired bootstrap re-sampling (Koehn,
2004).
</bodyText>
<subsectionHeader confidence="0.941421">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.995309523809524">
Our first group of experiments is to investigate
whether the predicate translation model is able to
improve translation accuracy in terms of BLEU and
whether semantic features are useful. The experi-
mental results are shown in Table 4. From the table,
we have the following two observations.
• The proposed predicate translation models
achieve an average improvement of 0.57 BLEU
points across the two NIST test sets when all
features (lex+sem) are used. Such an improve-
ment is statistically significant (p &lt; 0.01). Ac-
cording to our statistics, there are 5.07 verbal
predicates per sentence in NIST04 and 4.76
verbs per sentence in NIST05, which account
for 18.02% and 16.88% of all words in NIST04
and 05 respectively. This shows that not only
verbal predicates are semantically important,
they also form a major part of the sentences.
Therefore, whether verbal predicates are trans-
lated correctly or not has a great impact on the
translation accuracy of the whole sentence 7.
</bodyText>
<footnote confidence="0.999287">
6Available at: http://nlp.suda.edu.cn/∼jhli/.
7The example in Table 6 shows that the translations of
verbs even influences reorderings and translations of neighbor-
ing words.
</footnote>
<page confidence="0.96601">
907
</page>
<equation confidence="0.994412">
X → c/e (7)
[X,i,j] : Pr(A(i,j,τ))
X → [X1, X2] or (X1, X2) [X1, i, k] : Pr(A(i, k, τ)) [X2, k + 1,j] : Pr(A(k + 1, j, τ)) (8)
[X, i,j] : Pr(A(i, k, τ)) · Pr(A(k + 1,j,τ)) · Pr(N(i, k, j, τ))
</equation>
<figureCaption confidence="0.991817">
Figure 2: Integrating the argument reordering model into a BTG-style decoder.
</figureCaption>
<table confidence="0.9995375">
Model NIST04 NIST05
Base 35.52 33.80
Base+PTM (lex) 35.71+ 34.09+
Base+PTM (lex+sem) 36.10++** 34.35++*
</table>
<tableCaption confidence="0.985264833333333">
Table 4: Effects of the proposed predicate translation
model (PTM). PTM (lex): predicate translation model
with lexical features; PTM (lex+sem): predicate transla-
tion model with both lexical and semantic features; +/++:
better than the baseline (p &lt; 0.05/0.01). */**: better
than Base+PTM (lex) (p &lt; 0.05/0.01).
</tableCaption>
<table confidence="0.9991525">
Model NIST04 NIST05
Base 35.52 33.80
Base+ARM 35.82++ 34.29++
Base+ARM+PTM 36.19++ 34.72++
</table>
<tableCaption confidence="0.994123">
Table 5: Effects of the proposed argument reordering
</tableCaption>
<listItem confidence="0.782252111111111">
model (ARM) and the combination of ARM and PTM.
++: better than the baseline (p &lt; 0.01).
• When we integrate both lexical and semantic
features (lex+sem) described in Section 3.2, we
obtain an improvement of about 0.33 BLEU
points over the system where only lexical fea-
tures (lex) are used. Such a gain, which is sta-
tistically significant, confirms the effectiveness
of semantic features.
</listItem>
<bodyText confidence="0.999859071428572">
Our second group of experiments is to validate
whether the argument reordering model is capable
of improving translation quality. Table 5 shows the
results. We obtain an average improvement of 0.4
BLEU points on the two test sets over the base-
line when we incorporate the proposed argument re-
ordering model into our system. The improvements
on the two test sets are both statistically significant
(p &lt; 0.01).
Finally, we integrate both the predicate translation
model and argument reordering model into the final
system. The two models collectively achieve an im-
provement of up to 0.92 BLEU points over the base-
line, which is shown in Table 5.
</bodyText>
<sectionHeader confidence="0.953413" genericHeader="method">
7 Analysis
</sectionHeader>
<bodyText confidence="0.999840470588235">
In this section, we conduct some case studies to
show how the proposed models improve translation
accuracy by looking into the differences that they
make on translation hypotheses.
Table 6 displays a translation example which
shows the difference between the baseline and
the system enhanced with the predicate translation
model. There are two verbal predicates “ji± #/head
to” and “ë\/attend” in the source sentence. In
order to get the most appropriate translations for
these two verbal predicates, we should adopt differ-
ent ways to translate them. The former should be
translated as a corresponding verb word or phrase
while the latter into a preposition word “for”. Unfor-
tunately, the baseline incorrectly translates the two
verbs. Furthermore, such translation errors even re-
sult in undesirable reorderings of neighboring words
“Ë|ð/Bethlehem and “‘ (/mass”. This indi-
cates that verbal predicate translation errors may
lead to more errors, such as inappropriate reorder-
ings or lexical choices for neighboring words. On
the contrary, we can see that our predicate transla-
tion model is able to help select appropriate words
for both verbs. The correct translations of these two
verbs also avoid incorrect reorderings of neighbor-
ing words.
Table 7 shows another example to demonstrate
how the argument reordering model improve re-
orderings. The verbal predicate “?1/carry out”
has three arguments, ARG0, ARG-ADV and ARG1.
The ARG1 argument should be moved from the
right side of the predicate to its left side after trans-
lation. The ARG0 argument can either stay on the
left side or move to right side of the predicate. Ac-
</bodyText>
<page confidence="0.997882">
908
</page>
<tableCaption confidence="0.956831">
Table 6: A translation example showing the difference between the baseline and the system with the predicate transla-
tion model (PTM). Phrase alignments in the two system outputs are shown with dashed lines. Chinese words in bold
are verbal predicates.
</tableCaption>
<table confidence="0.999643444444444">
PAS ARG0
ARGM-ADV
ARG1
[4A ù @ / ´w &apos;*Ú] 33‡ ?V [• 4; I‡ N FA0]
Base [4A ù] @ /X [´w &apos;*Ú] 33-I‡ [?V • 4;] [i‡ N FA-l]
the more [important consultations] also set disaster [warning system]
Base+ARM 4A [ù @] /X [´w &apos;*Ú] [a‡ ?V] [• 4;] [R‡ N FA-l]
more [important consultations] on [such a] disaster [warning system] [should be carried out]
Ref more important discussions will be held on the disaster warning system
</table>
<tableCaption confidence="0.872590666666667">
Table 7: A translation example showing the difference between the baseline and the system with the argument re-
ordering model (ARM). The predicate-argument structure (PAS) of the source sentence is also displayed in the first
row.
</tableCaption>
<figure confidence="0.631290285714286">
[t -T] iAä ` 4wmv- obn [T!r –] I�Jljft
[thousands of] followers to Mass in Bethlehem [Christmas Eve]
[t -T] iAä ` 4wmv- obn [ r –] ��
Base
[thousands of] devotees [rushed to] Bethlehem for [Christmas Eve] mass
Base+PTM
Ref thousands of worshippers head to Bethlehem for Christmas Midnight mass
</figure>
<bodyText confidence="0.990509090909091">
cording to the phrase alignments of the baseline,
we clearly observe three serious translation errors:
1) the ARG0 argument is translated into separate
groups which are not adjacent on the target side;
2) the predicate is not translated at all; and 3) the
ARG1 argument is not moved to the left side of the
predicate after translation. All of these 3 errors are
avoided in the Base+ARM system output as a re-
sult of the argument reordering model that correctly
identifies arguments and moves them in the right di-
rections.
</bodyText>
<sectionHeader confidence="0.99616" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999987346153846">
We have presented two discriminative models to
incorporate source side predicate-argument struc-
tures into SMT. The two models have been inte-
grated into a phrase-based SMT system and evalu-
ated on Chinese-to-English translation tasks using
large-scale training data. The first model is the pred-
icate translation model which employs both lexical
and semantic contexts to translate verbal predicates.
The second model is the argument reordering model
which estimates the direction of argument move-
ment relative to its predicate after translation. Ex-
perimental results show that both models are able to
significantly improve translation accuracy in terms
of BLEU score.
In the future work, we will extend our predicate
translation model to translate both verbal and nom-
inal predicates. Nominal predicates also frequently
occur in Chinese sentences and thus accurate trans-
lations of them are desirable for SMT. We also want
to address another translation issue of arguments as
shown in Table 7: arguments are wrongly translated
into separate groups instead of a cohesive unit (Wu
and Fung, 2009a). We will build an argument seg-
mentation model that follows (Xiong et al., 2011) to
determine whether arguments should be translated
as a unit or not.
</bodyText>
<page confidence="0.997374">
909
</page>
<sectionHeader confidence="0.989584" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999742764150944">
Wilker Aziz, Miguel Rios, and Lucia Specia. 2011. Shal-
low semantic trees for smt. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
316–322, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39–71.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Pascale Fung, Wu Zhaojun, Yang Yongsheng, and Dekai
Wu. 2006. Automatic learning of chinese english se-
mantic structure mapping. In IEEE/ACL 2006 Work-
shop on Spoken Language Technology (SLT 2006),
Aruba, December.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 58–54, Edmon-
ton, Canada, May-June.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388–395, Barcelona, Spain, July.
Mamoru Komachi and Yuji Matsumoto. 2006. Phrase
reordering for statistical machine translation based on
predicate-argument structure. In In Proceedings of the
International Workshop on Spoken Language Trans-
lation: Evaluation Campaign on Spoken Language
Translation, pages 77–82.
Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010.
Joint syntactic and semantic parsing of chinese. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 1108–
1117, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
Ding Liu and Daniel Gildea. 2010. Semantic role
features for machine translation. In Proceedings of
the 23rd International Conference on Computational
Linguistics (Coling 2010), pages 716–724, Beijing,
China, August. Coling 2010 Organizing Committee.
Arne Mauser, Saˇsa Hasan, and Hermann Ney. 2009. Ex-
tending statistical machine translation with discrimi-
native and trigger-based lexicon models. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing, pages 210–218, Singa-
pore, August. Association for Computational Linguis-
tics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311–318, Philadelphia, Pennsylva-
nia, USA, July.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433–440, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Andreas Stolcke. 2002. Srilm–an extensible language
modeling toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing,
pages 901–904, Denver, Colorado, USA, September.
Sriram Venkatapathy and Srinivas Bangalore. 2007.
Three models for discriminative machine translation
using global lexical selection and sentence reconstruc-
tion. In Proceedings of SSST, NAACL-HLT 2007 /
AMTA Workshop on Syntax and Structure in Statisti-
cal Translation, pages 96–102, Rochester, New York,
April. Association for Computational Linguistics.
Dekai Wu and Pascale Fung. 2009a. Can semantic
role labeling improve smt. In Proceedings of the
13th Annual Conference of the EAMT, pages 218–225,
Barcelona, May.
Dekai Wu and Pascale Fung. 2009b. Semantic roles for
smt: A hybrid two-pass model. In Proceedings ofHu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, Companion Vol-
ume: Short Papers, pages 13–16, Boulder, Colorado,
June. Association for Computational Linguistics.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting pre-
ordering rules from predicate-argument structures. In
Proceedings of 5th International Joint Conference on
Natural Language Processing, pages 29–37, Chiang
Mai, Thailand, November. Asian Federation of Natu-
ral Language Processing.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 521–528, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2011. A
maximum-entropy segmentation model for statistical
machine translation. IEEE Transactions on Audio,
Speech and Language Processing, 19(8):2494–2505.
</reference>
<page confidence="0.970776">
910
</page>
<reference confidence="0.948435666666667">
Nianwen Xue. 2008. Labeling chinese predicates
with semantic roles. Computational Linguistics,
34(2):225–255.
</reference>
<page confidence="0.998066">
911
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.364625">
<title confidence="0.999977">Modeling the Translation of Predicate-Argument Structure for SMT</title>
<author confidence="0.764344">Min Haizhou Human Language Xiong</author>
<affiliation confidence="0.927274">Institute for Infocomm</affiliation>
<address confidence="0.775462">1 Fusionopolis Way, #21-01 Connexis, Singapore</address>
<email confidence="0.988039">mzhang,</email>
<abstract confidence="0.990654478260869">Predicate-argument structure contains rich semantic information of which statistical machine translation hasn’t taken full advantage. In this paper, we propose two discriminative, feature-based models to exploit predicateargument structures for statistical machine translation: 1) a predicate translation model and 2) an argument reordering model. The predicate translation model explores lexical and semantic contexts surrounding a verbal predicate to select desirable translations for the predicate. The argument reordering model automatically predicts the moving direction of an argument relative to its predicate after translation using semantic features. The two models are integrated into a state-of-theart phrase-based machine translation system and evaluated on Chinese-to-English translation tasks with large-scale training data. Experimental results demonstrate that the two models significantly improve translation accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Wilker Aziz</author>
<author>Miguel Rios</author>
<author>Lucia Specia</author>
</authors>
<title>Shallow semantic trees for smt.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>316--322</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="5429" citStr="Aziz et al. (2011)" startWordPosition="780" endWordPosition="783">rectly project semantic roles from the source side to the target side through word alignments during decoding (Liu and Gildea, 2010). There are other previous studies that explore only source side predicate-argument structures. Komachi and Matsumoto (2006) reorder arguments in source language (Japanese) sentences using heuristic rules defined on source side predicate-argument structures in a pre-processing step. Wu et al. (2011) automate this procedure by automatically extracting reordering rules from predicate-argument structures and applying these rules to reorder source language sentences. Aziz et al. (2011) incorporate source language semantic role labels into a tree-to-string SMT system. Although we also focus on source side predicateargument structures, our models differ from the previous work in two main aspects: 1) we propose two separate discriminative models to exploit predicateargument structures for predicate translation and argument reordering respectively; 2) we consider argument reordering as an argument movement (relative to its predicate) prediction problem and use a discriminatively trained classifier for such predictions. Our predicate translation model is also related to previous</context>
</contexts>
<marker>Aziz, Rios, Specia, 2011</marker>
<rawString>Wilker Aziz, Miguel Rios, and Lucia Specia. 2011. Shallow semantic trees for smt. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 316–322, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="6092" citStr="Berger et al., 1996" startWordPosition="878" endWordPosition="881">abels into a tree-to-string SMT system. Although we also focus on source side predicateargument structures, our models differ from the previous work in two main aspects: 1) we propose two separate discriminative models to exploit predicateargument structures for predicate translation and argument reordering respectively; 2) we consider argument reordering as an argument movement (relative to its predicate) prediction problem and use a discriminatively trained classifier for such predictions. Our predicate translation model is also related to previous discriminative lexicon translation models (Berger et al., 1996; Venkatapathy and Bangalore, 2007; Mauser et al., 2009). While previous models predict translations for all words in vocabulary, we only focus on verbal predicates. This will tremendously reduce the amount of training data required, which usually is a problem in discriminative lexicon translation models (Mauser et al., 2009). Furthermore, the proposed translation model also differs from previous lexicon translation models in that we use both lexical and semantic features. Our experimental results show that semantic features are able to further improve translation accuracy. 3 Predicate Transla</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="16982" citStr="Chiang, 2007" startWordPosition="2712" endWordPosition="2714">tion 3.3 to train the maximum entropy classifier as formulated in Eq. (4). We perform 100 iterations of L-BFGS. 5 Integrating the Two Models into SMT In this section, we elaborate how to integrate the two models into phrase-based SMT. In particular, we integrate the models into a phrase-based system which uses bracketing transduction grammars (BTG) (Wu, 1997) for phrasal translation (Xiong et al., 2006). Since the system is based on a CKY-style decoder, the integration algorithms introduced here can be easily adapted to other CKY-based decoding systems such as the hierarchical phrasal system (Chiang, 2007). 5.1 Integrating the Predicate Translation Model It is straightforward to integrate the predicate translation model into phrase-based SMT (Koehn et al., NC L2R R2L 82.43% 11.19% 6.38% Reordering Category Percent 2003; Xiong et al., 2006). We maintain word alignments for each phrase pair in the phrase table. Given a source sentence with its predicateargument structure, we detect all verbal predicates and load trained predicate translation classifiers for these verbs. Whenever a hypothesis covers a new verbal predicate v, we find the target translation e for v through word alignments and then c</context>
<context position="19226" citStr="Chiang, 2007" startWordPosition="3080" endWordPosition="3081">, in Figure 1, A(3,6, z) = {(Wez&apos; :, ARGM-TMP)} while A(2,3, z) = {}, A(1, 5, z) = {} because the verbal predicate “��” is located outside the span (2,3) and (1,5). • N(i, k, j, z): the function finds all predicateargument pairs that cross the two neighboring spans (i, k) and (k+1, j). It can be formulated as A(i,j,z) − (A(i,k,z) UA(k + 1, j, z)). We then define another function Pr to calculate the argument reordering model probability on all arguments which are found by the previous two functions A and N as follows. Pr(B) = 11 pr(mA|C(A)) (6) A∈B 906 where B denotes either A or N. Following (Chiang, 2007), we describe the algorithm in a deductive system. It is shown in Figure 2. The algorithm integrates the argument reordering model into a CKY-style decoder (Xiong et al., 2006). The item [X, i, j] denotes a BTG node X spanning from i to j on the source side. For notational convenience, we only show the argument reordering model probability for each item, ignoring all other sub-model probabilities such as the language model probability. The Eq. (7) shows how we calculate the argument reordering model probability when a lexical rule is applied to translate a source phrase c to a target phrase e.</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Wu Zhaojun</author>
<author>Yang Yongsheng</author>
<author>Dekai Wu</author>
</authors>
<title>Automatic learning of chinese english semantic structure mapping.</title>
<date>2006</date>
<booktitle>In IEEE/ACL 2006 Workshop on Spoken Language Technology (SLT</booktitle>
<location>Aruba,</location>
<contexts>
<context position="12646" citStr="Fung et al., 2006" startWordPosition="1972" endWordPosition="1975">: An example of predicate-argument structure in Chinese and its aligned English translation. The bold word in Chinese is the verbal predicate. The subscripts on the Chinese sentence show the indexes of words from left to right. ARGM-TMP ARG1 ARGM-ADV ARG0 ing, we run the training toolkit in a parallel manner. 4 Argument Reordering Model In this section we introduce the discriminative argument reordering model, features and the training procedure. 4.1 Model Since the predicate determines what arguments are involved in its semantic frame and semantic frames tend to be cohesive across languages (Fung et al., 2006), the movements of predicate and its arguments across translations are like the motions of a planet and its satellites. Therefore we consider the reordering of an argument as the motion of the argument relative to its predicate. In particular, we use the position of the predicate as the reference axis. The motion of associated arguments relative to the reference axis can be roughly divided into 3 categories4: 1) no change across languages (NC); 2) moving from the left side of its predicate to the right side of the predicate after translation (L2R); and 3) moving from the right side of its pred</context>
</contexts>
<marker>Fung, Zhaojun, Yongsheng, Wu, 2006</marker>
<rawString>Pascale Fung, Wu Zhaojun, Yang Yongsheng, and Dekai Wu. 2006. Automatic learning of chinese english semantic structure mapping. In IEEE/ACL 2006 Workshop on Spoken Language Technology (SLT 2006), Aruba, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Joseph Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>58--54</pages>
<location>Edmonton, Canada, May-June.</location>
<contexts>
<context position="8262" citStr="Koehn et al., 2003" startWordPosition="1227" endWordPosition="1230">mple, a Chinese verb “R(T(issue)” can be translated as “to be issued” or “have issued” with modality words. 903 This will increase the number of classes to be predicted by the maximum entropy classifier. But according to our observation, it is still computationally tractable (see Section 3.3). If a verbal predicate is not translated, we set e = NULL so that we can also capture null translations for verbal predicates. 3.2 Features The apparent advantage of discriminative lexicon translation models over generative translation models (e.g., conventional lexical translation model as described in (Koehn et al., 2003)) is that discriminative models allow us to integrate richer contexts (lexical, syntactic or semantic) into target translation prediction. We use two kinds of features to predict translations for verbal predicates: 1) lexical features and 2) semantic features. All features are in the following binary form. � 1, if e = * and C(v).Q = ♠ f(e,C(v)) = 0, else (3) where the symbol * is a placeholder for a possible target translation (up to 4 words), the symbol Q indicates a contextual (lexical or semantic) element for the verbal predicate v, and the symbol ♠ represents the value of Q. Lexical Featur</context>
<context position="17850" citStr="Koehn et al., 2003" startWordPosition="2844" endWordPosition="2847">in word alignments for each phrase pair in the phrase table. Given a source sentence with its predicateargument structure, we detect all verbal predicates and load trained predicate translation classifiers for these verbs. Whenever a hypothesis covers a new verbal predicate v, we find the target translation e for v through word alignments and then calculate its translation probability pt(e|C(v)) according to Eq. (1). The predicate translation model (as formulated in Eq. (2)) is integrated into the whole log-linear model just like the conventional lexical translation model in phrase-based SMT (Koehn et al., 2003). The two models are independently estimated but complementary to each other. While the lexical translation model calculates the probability of a verbal predicate being translated given its local lexical context, the discriminative predicate translation model is able to employ both lexical and semantic contexts to predict translations for verbs. 5.2 Integrating the Argument Reordering Model Before we introduce the integration algorithm for the argument reordering model, we define two functions A and N on a source sentence and its predicate-argument structure z as follows. • A(i, j, z): from th</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 58–54, Edmonton, Canada, May-June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP 2004,</booktitle>
<pages>388--395</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="22160" citStr="Koehn, 2004" startWordPosition="3567" endWordPosition="3568">ntic role labeler6 (Li et al., 2010) on all source parse trees to annotate semantic roles for all verbal predicates. After we obtained semantic roles on the source side, we extracted features as described in Section 3.2 and 4.2 and used these features to train our two models as described in Section 3.3 and 4.3. We used the NIST MT03 evaluation test data as our development set, and the NIST MT04, MT05 as the test sets. We adopted the case-insensitive BLEU-4 (Papineni et al., 2002) as the evaluation metric. Statistical significance in BLEU differences was tested by paired bootstrap re-sampling (Koehn, 2004). 6.2 Results Our first group of experiments is to investigate whether the predicate translation model is able to improve translation accuracy in terms of BLEU and whether semantic features are useful. The experimental results are shown in Table 4. From the table, we have the following two observations. • The proposed predicate translation models achieve an average improvement of 0.57 BLEU points across the two NIST test sets when all features (lex+sem) are used. Such an improvement is statistically significant (p &lt; 0.01). According to our statistics, there are 5.07 verbal predicates per sente</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP 2004, pages 388–395, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mamoru Komachi</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Phrase reordering for statistical machine translation based on predicate-argument structure. In</title>
<date>2006</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation: Evaluation Campaign on Spoken Language Translation,</booktitle>
<pages>77--82</pages>
<contexts>
<context position="5067" citStr="Komachi and Matsumoto (2006)" startWordPosition="729" endWordPosition="732">ork. As PAS analysis widely employs global and sentence-wide features, it is computationally expensive to integrate target side predicateargument structures into the dynamic programming style of SMT decoding (Wu and Fung, 2009b). Therefore they either postpone the integration of target side PASs until the whole decoding procedure is completed (Wu and Fung, 2009b), or directly project semantic roles from the source side to the target side through word alignments during decoding (Liu and Gildea, 2010). There are other previous studies that explore only source side predicate-argument structures. Komachi and Matsumoto (2006) reorder arguments in source language (Japanese) sentences using heuristic rules defined on source side predicate-argument structures in a pre-processing step. Wu et al. (2011) automate this procedure by automatically extracting reordering rules from predicate-argument structures and applying these rules to reorder source language sentences. Aziz et al. (2011) incorporate source language semantic role labels into a tree-to-string SMT system. Although we also focus on source side predicateargument structures, our models differ from the previous work in two main aspects: 1) we propose two separa</context>
</contexts>
<marker>Komachi, Matsumoto, 2006</marker>
<rawString>Mamoru Komachi and Yuji Matsumoto. 2006. Phrase reordering for statistical machine translation based on predicate-argument structure. In In Proceedings of the International Workshop on Spoken Language Translation: Evaluation Campaign on Spoken Language Translation, pages 77–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junhui Li</author>
<author>Guodong Zhou</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Joint syntactic and semantic parsing of chinese.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1108--1117</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="21584" citStr="Li et al., 2010" startWordPosition="3470" endWordPosition="3473">hen used all these word-aligned corpora to generate our phrase table. Our 5-gram language model was trained on the Xinhua section of the English Gigaword corpus (306 million words) 5The corpora include LDC2004E12, LDC2004T08, LDC2005T10, LDC2003E14, LDC2002E18, LDC2005T06, LDC2003E07 and LDC2004T07. using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. To train the proposed predicate translation model and argument reordering model, we first parsed all source sentences using the Berkeley Chinese parser (Petrov et al., 2006) and then ran the Chinese semantic role labeler6 (Li et al., 2010) on all source parse trees to annotate semantic roles for all verbal predicates. After we obtained semantic roles on the source side, we extracted features as described in Section 3.2 and 4.2 and used these features to train our two models as described in Section 3.3 and 4.3. We used the NIST MT03 evaluation test data as our development set, and the NIST MT04, MT05 as the test sets. We adopted the case-insensitive BLEU-4 (Papineni et al., 2002) as the evaluation metric. Statistical significance in BLEU differences was tested by paired bootstrap re-sampling (Koehn, 2004). 6.2 Results Our first </context>
</contexts>
<marker>Li, Zhou, Ng, 2010</marker>
<rawString>Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010. Joint syntactic and semantic parsing of chinese. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1108– 1117, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Semantic role features for machine translation.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>716--724</pages>
<location>Beijing, China,</location>
<contexts>
<context position="1395" citStr="Liu and Gildea, 2010" startWordPosition="179" endWordPosition="182">nt reordering model automatically predicts the moving direction of an argument relative to its predicate after translation using semantic features. The two models are integrated into a state-of-theart phrase-based machine translation system and evaluated on Chinese-to-English translation tasks with large-scale training data. Experimental results demonstrate that the two models significantly improve translation accuracy. 1 Introduction Recent years have witnessed increasing efforts towards integrating predicate-argument structures into statistical machine translation (SMT) (Wu and Fung, 2009b; Liu and Gildea, 2010). In this paper, we take a step forward by introducing a novel approach to incorporate such semantic structures into SMT. Given a source side predicate-argument structure, we attempt to translate each semantic frame (predicate and its associated arguments) into an appropriate target string. We believe that the translation of predicates and reordering of arguments are the two central ∗Corresponding author issues concerning the transfer of predicate-argument structure across languages. Predicates1 are essential elements in sentences. Unfortunately they are usually neither correctly translated no</context>
<context position="4943" citStr="Liu and Gildea, 2010" startWordPosition="713" endWordPosition="716">d Work Predicate-argument structures (PAS) are explored for SMT on both the source and target side in some previous work. As PAS analysis widely employs global and sentence-wide features, it is computationally expensive to integrate target side predicateargument structures into the dynamic programming style of SMT decoding (Wu and Fung, 2009b). Therefore they either postpone the integration of target side PASs until the whole decoding procedure is completed (Wu and Fung, 2009b), or directly project semantic roles from the source side to the target side through word alignments during decoding (Liu and Gildea, 2010). There are other previous studies that explore only source side predicate-argument structures. Komachi and Matsumoto (2006) reorder arguments in source language (Japanese) sentences using heuristic rules defined on source side predicate-argument structures in a pre-processing step. Wu et al. (2011) automate this procedure by automatically extracting reordering rules from predicate-argument structures and applying these rules to reorder source language sentences. Aziz et al. (2011) incorporate source language semantic role labels into a tree-to-string SMT system. Although we also focus on sour</context>
</contexts>
<marker>Liu, Gildea, 2010</marker>
<rawString>Ding Liu and Daniel Gildea. 2010. Semantic role features for machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 716–724, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne Mauser</author>
<author>Saˇsa Hasan</author>
<author>Hermann Ney</author>
</authors>
<title>Extending statistical machine translation with discriminative and trigger-based lexicon models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>210--218</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6148" citStr="Mauser et al., 2009" startWordPosition="886" endWordPosition="889">o focus on source side predicateargument structures, our models differ from the previous work in two main aspects: 1) we propose two separate discriminative models to exploit predicateargument structures for predicate translation and argument reordering respectively; 2) we consider argument reordering as an argument movement (relative to its predicate) prediction problem and use a discriminatively trained classifier for such predictions. Our predicate translation model is also related to previous discriminative lexicon translation models (Berger et al., 1996; Venkatapathy and Bangalore, 2007; Mauser et al., 2009). While previous models predict translations for all words in vocabulary, we only focus on verbal predicates. This will tremendously reduce the amount of training data required, which usually is a problem in discriminative lexicon translation models (Mauser et al., 2009). Furthermore, the proposed translation model also differs from previous lexicon translation models in that we use both lexical and semantic features. Our experimental results show that semantic features are able to further improve translation accuracy. 3 Predicate Translation Model In this section, we present the features and </context>
</contexts>
<marker>Mauser, Hasan, Ney, 2009</marker>
<rawString>Arne Mauser, Saˇsa Hasan, and Hermann Ney. 2009. Extending statistical machine translation with discriminative and trigger-based lexicon models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 210–218, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="22032" citStr="Papineni et al., 2002" startWordPosition="3548" endWordPosition="3551">ordering model, we first parsed all source sentences using the Berkeley Chinese parser (Petrov et al., 2006) and then ran the Chinese semantic role labeler6 (Li et al., 2010) on all source parse trees to annotate semantic roles for all verbal predicates. After we obtained semantic roles on the source side, we extracted features as described in Section 3.2 and 4.2 and used these features to train our two models as described in Section 3.3 and 4.3. We used the NIST MT03 evaluation test data as our development set, and the NIST MT04, MT05 as the test sets. We adopted the case-insensitive BLEU-4 (Papineni et al., 2002) as the evaluation metric. Statistical significance in BLEU differences was tested by paired bootstrap re-sampling (Koehn, 2004). 6.2 Results Our first group of experiments is to investigate whether the predicate translation model is able to improve translation accuracy in terms of BLEU and whether semantic features are useful. The experimental results are shown in Table 4. From the table, we have the following two observations. • The proposed predicate translation models achieve an average improvement of 0.57 BLEU points across the two NIST test sets when all features (lex+sem) are used. Such</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="21518" citStr="Petrov et al., 2006" startWordPosition="3457" endWordPosition="3460"> the “grow-diag-final” refinement rule to obtain word alignments. We then used all these word-aligned corpora to generate our phrase table. Our 5-gram language model was trained on the Xinhua section of the English Gigaword corpus (306 million words) 5The corpora include LDC2004E12, LDC2004T08, LDC2005T10, LDC2003E14, LDC2002E18, LDC2005T06, LDC2003E07 and LDC2004T07. using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. To train the proposed predicate translation model and argument reordering model, we first parsed all source sentences using the Berkeley Chinese parser (Petrov et al., 2006) and then ran the Chinese semantic role labeler6 (Li et al., 2010) on all source parse trees to annotate semantic roles for all verbal predicates. After we obtained semantic roles on the source side, we extracted features as described in Section 3.2 and 4.2 and used these features to train our two models as described in Section 3.3 and 4.3. We used the NIST MT03 evaluation test data as our development set, and the NIST MT04, MT05 as the test sets. We adopted the case-insensitive BLEU-4 (Papineni et al., 2002) as the evaluation metric. Statistical significance in BLEU differences was tested by </context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm–an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the 7th International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, Colorado, USA,</location>
<contexts>
<context position="21308" citStr="Stolcke, 2002" startWordPosition="3428" endWordPosition="3429">asal system (Xiong et al., 2006). Our training corpora5 consist of 3.8M sentence pairs with 96.9M Chinese words and 109.5M English words. We ran GIZA++ on these corpora in both directions and then applied the “grow-diag-final” refinement rule to obtain word alignments. We then used all these word-aligned corpora to generate our phrase table. Our 5-gram language model was trained on the Xinhua section of the English Gigaword corpus (306 million words) 5The corpora include LDC2004E12, LDC2004T08, LDC2005T10, LDC2003E14, LDC2002E18, LDC2005T06, LDC2003E07 and LDC2004T07. using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. To train the proposed predicate translation model and argument reordering model, we first parsed all source sentences using the Berkeley Chinese parser (Petrov et al., 2006) and then ran the Chinese semantic role labeler6 (Li et al., 2010) on all source parse trees to annotate semantic roles for all verbal predicates. After we obtained semantic roles on the source side, we extracted features as described in Section 3.2 and 4.2 and used these features to train our two models as described in Section 3.3 and 4.3. We used the NIST MT03 evaluation test data as o</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm–an extensible language modeling toolkit. In Proceedings of the 7th International Conference on Spoken Language Processing, pages 901–904, Denver, Colorado, USA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sriram Venkatapathy</author>
<author>Srinivas Bangalore</author>
</authors>
<title>Three models for discriminative machine translation using global lexical selection and sentence reconstruction.</title>
<date>2007</date>
<booktitle>In Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>96--102</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="6126" citStr="Venkatapathy and Bangalore, 2007" startWordPosition="882" endWordPosition="885">string SMT system. Although we also focus on source side predicateargument structures, our models differ from the previous work in two main aspects: 1) we propose two separate discriminative models to exploit predicateargument structures for predicate translation and argument reordering respectively; 2) we consider argument reordering as an argument movement (relative to its predicate) prediction problem and use a discriminatively trained classifier for such predictions. Our predicate translation model is also related to previous discriminative lexicon translation models (Berger et al., 1996; Venkatapathy and Bangalore, 2007; Mauser et al., 2009). While previous models predict translations for all words in vocabulary, we only focus on verbal predicates. This will tremendously reduce the amount of training data required, which usually is a problem in discriminative lexicon translation models (Mauser et al., 2009). Furthermore, the proposed translation model also differs from previous lexicon translation models in that we use both lexical and semantic features. Our experimental results show that semantic features are able to further improve translation accuracy. 3 Predicate Translation Model In this section, we pre</context>
</contexts>
<marker>Venkatapathy, Bangalore, 2007</marker>
<rawString>Sriram Venkatapathy and Srinivas Bangalore. 2007. Three models for discriminative machine translation using global lexical selection and sentence reconstruction. In Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 96–102, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Pascale Fung</author>
</authors>
<title>Can semantic role labeling improve smt.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Annual Conference of the EAMT,</booktitle>
<pages>218--225</pages>
<location>Barcelona,</location>
<contexts>
<context position="1371" citStr="Wu and Fung, 2009" startWordPosition="175" endWordPosition="178">redicate. The argument reordering model automatically predicts the moving direction of an argument relative to its predicate after translation using semantic features. The two models are integrated into a state-of-theart phrase-based machine translation system and evaluated on Chinese-to-English translation tasks with large-scale training data. Experimental results demonstrate that the two models significantly improve translation accuracy. 1 Introduction Recent years have witnessed increasing efforts towards integrating predicate-argument structures into statistical machine translation (SMT) (Wu and Fung, 2009b; Liu and Gildea, 2010). In this paper, we take a step forward by introducing a novel approach to incorporate such semantic structures into SMT. Given a source side predicate-argument structure, we attempt to translate each semantic frame (predicate and its associated arguments) into an appropriate target string. We believe that the translation of predicates and reordering of arguments are the two central ∗Corresponding author issues concerning the transfer of predicate-argument structure across languages. Predicates1 are essential elements in sentences. Unfortunately they are usually neither</context>
<context position="4665" citStr="Wu and Fung, 2009" startWordPosition="669" endWordPosition="672">ntroduce how to integrate these two models into SMT. Section 6 will describe our experiments and results. Section 7 will empirically discuss how the proposed models improve translation accuracy. Finally we will conclude with future research directions in Section 8. 2 Related Work Predicate-argument structures (PAS) are explored for SMT on both the source and target side in some previous work. As PAS analysis widely employs global and sentence-wide features, it is computationally expensive to integrate target side predicateargument structures into the dynamic programming style of SMT decoding (Wu and Fung, 2009b). Therefore they either postpone the integration of target side PASs until the whole decoding procedure is completed (Wu and Fung, 2009b), or directly project semantic roles from the source side to the target side through word alignments during decoding (Liu and Gildea, 2010). There are other previous studies that explore only source side predicate-argument structures. Komachi and Matsumoto (2006) reorder arguments in source language (Japanese) sentences using heuristic rules defined on source side predicate-argument structures in a pre-processing step. Wu et al. (2011) automate this procedu</context>
</contexts>
<marker>Wu, Fung, 2009</marker>
<rawString>Dekai Wu and Pascale Fung. 2009a. Can semantic role labeling improve smt. In Proceedings of the 13th Annual Conference of the EAMT, pages 218–225, Barcelona, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Pascale Fung</author>
</authors>
<title>Semantic roles for smt: A hybrid two-pass model.</title>
<date>2009</date>
<booktitle>In Proceedings ofHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,</booktitle>
<pages>13--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="1371" citStr="Wu and Fung, 2009" startWordPosition="175" endWordPosition="178">redicate. The argument reordering model automatically predicts the moving direction of an argument relative to its predicate after translation using semantic features. The two models are integrated into a state-of-theart phrase-based machine translation system and evaluated on Chinese-to-English translation tasks with large-scale training data. Experimental results demonstrate that the two models significantly improve translation accuracy. 1 Introduction Recent years have witnessed increasing efforts towards integrating predicate-argument structures into statistical machine translation (SMT) (Wu and Fung, 2009b; Liu and Gildea, 2010). In this paper, we take a step forward by introducing a novel approach to incorporate such semantic structures into SMT. Given a source side predicate-argument structure, we attempt to translate each semantic frame (predicate and its associated arguments) into an appropriate target string. We believe that the translation of predicates and reordering of arguments are the two central ∗Corresponding author issues concerning the transfer of predicate-argument structure across languages. Predicates1 are essential elements in sentences. Unfortunately they are usually neither</context>
<context position="4665" citStr="Wu and Fung, 2009" startWordPosition="669" endWordPosition="672">ntroduce how to integrate these two models into SMT. Section 6 will describe our experiments and results. Section 7 will empirically discuss how the proposed models improve translation accuracy. Finally we will conclude with future research directions in Section 8. 2 Related Work Predicate-argument structures (PAS) are explored for SMT on both the source and target side in some previous work. As PAS analysis widely employs global and sentence-wide features, it is computationally expensive to integrate target side predicateargument structures into the dynamic programming style of SMT decoding (Wu and Fung, 2009b). Therefore they either postpone the integration of target side PASs until the whole decoding procedure is completed (Wu and Fung, 2009b), or directly project semantic roles from the source side to the target side through word alignments during decoding (Liu and Gildea, 2010). There are other previous studies that explore only source side predicate-argument structures. Komachi and Matsumoto (2006) reorder arguments in source language (Japanese) sentences using heuristic rules defined on source side predicate-argument structures in a pre-processing step. Wu et al. (2011) automate this procedu</context>
</contexts>
<marker>Wu, Fung, 2009</marker>
<rawString>Dekai Wu and Pascale Fung. 2009b. Semantic roles for smt: A hybrid two-pass model. In Proceedings ofHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, pages 13–16, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianchao Wu</author>
<author>Katsuhito Sudoh</author>
<author>Kevin Duh</author>
<author>Hajime Tsukada</author>
<author>Masaaki Nagata</author>
</authors>
<title>Extracting preordering rules from predicate-argument structures.</title>
<date>2011</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>29--37</pages>
<location>Chiang Mai, Thailand,</location>
<contexts>
<context position="5243" citStr="Wu et al. (2011)" startWordPosition="753" endWordPosition="756">yle of SMT decoding (Wu and Fung, 2009b). Therefore they either postpone the integration of target side PASs until the whole decoding procedure is completed (Wu and Fung, 2009b), or directly project semantic roles from the source side to the target side through word alignments during decoding (Liu and Gildea, 2010). There are other previous studies that explore only source side predicate-argument structures. Komachi and Matsumoto (2006) reorder arguments in source language (Japanese) sentences using heuristic rules defined on source side predicate-argument structures in a pre-processing step. Wu et al. (2011) automate this procedure by automatically extracting reordering rules from predicate-argument structures and applying these rules to reorder source language sentences. Aziz et al. (2011) incorporate source language semantic role labels into a tree-to-string SMT system. Although we also focus on source side predicateargument structures, our models differ from the previous work in two main aspects: 1) we propose two separate discriminative models to exploit predicateargument structures for predicate translation and argument reordering respectively; 2) we consider argument reordering as an argume</context>
</contexts>
<marker>Wu, Sudoh, Duh, Tsukada, Nagata, 2011</marker>
<rawString>Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, and Masaaki Nagata. 2011. Extracting preordering rules from predicate-argument structures. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 29–37, Chiang Mai, Thailand, November. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="16730" citStr="Wu, 1997" startWordPosition="2673" endWordPosition="2674"> their predicates to the right side after translation (accounting for 11.19%) or from the right side to the left side of their translated predicates (accounting for 6.38%). After all features are extracted, we use the maximum entropy toolkit in Section 3.3 to train the maximum entropy classifier as formulated in Eq. (4). We perform 100 iterations of L-BFGS. 5 Integrating the Two Models into SMT In this section, we elaborate how to integrate the two models into phrase-based SMT. In particular, we integrate the models into a phrase-based system which uses bracketing transduction grammars (BTG) (Wu, 1997) for phrasal translation (Xiong et al., 2006). Since the system is based on a CKY-style decoder, the integration algorithms introduced here can be easily adapted to other CKY-based decoding systems such as the hierarchical phrasal system (Chiang, 2007). 5.1 Integrating the Predicate Translation Model It is straightforward to integrate the predicate translation model into phrase-based SMT (Koehn et al., NC L2R R2L 82.43% 11.19% 6.38% Reordering Category Percent 2003; Xiong et al., 2006). We maintain word alignments for each phrase pair in the phrase table. Given a source sentence with its predi</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>521--528</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="16775" citStr="Xiong et al., 2006" startWordPosition="2678" endWordPosition="2681"> after translation (accounting for 11.19%) or from the right side to the left side of their translated predicates (accounting for 6.38%). After all features are extracted, we use the maximum entropy toolkit in Section 3.3 to train the maximum entropy classifier as formulated in Eq. (4). We perform 100 iterations of L-BFGS. 5 Integrating the Two Models into SMT In this section, we elaborate how to integrate the two models into phrase-based SMT. In particular, we integrate the models into a phrase-based system which uses bracketing transduction grammars (BTG) (Wu, 1997) for phrasal translation (Xiong et al., 2006). Since the system is based on a CKY-style decoder, the integration algorithms introduced here can be easily adapted to other CKY-based decoding systems such as the hierarchical phrasal system (Chiang, 2007). 5.1 Integrating the Predicate Translation Model It is straightforward to integrate the predicate translation model into phrase-based SMT (Koehn et al., NC L2R R2L 82.43% 11.19% 6.38% Reordering Category Percent 2003; Xiong et al., 2006). We maintain word alignments for each phrase pair in the phrase table. Given a source sentence with its predicateargument structure, we detect all verbal </context>
<context position="19402" citStr="Xiong et al., 2006" startWordPosition="3108" endWordPosition="3111">, k, j, z): the function finds all predicateargument pairs that cross the two neighboring spans (i, k) and (k+1, j). It can be formulated as A(i,j,z) − (A(i,k,z) UA(k + 1, j, z)). We then define another function Pr to calculate the argument reordering model probability on all arguments which are found by the previous two functions A and N as follows. Pr(B) = 11 pr(mA|C(A)) (6) A∈B 906 where B denotes either A or N. Following (Chiang, 2007), we describe the algorithm in a deductive system. It is shown in Figure 2. The algorithm integrates the argument reordering model into a CKY-style decoder (Xiong et al., 2006). The item [X, i, j] denotes a BTG node X spanning from i to j on the source side. For notational convenience, we only show the argument reordering model probability for each item, ignoring all other sub-model probabilities such as the language model probability. The Eq. (7) shows how we calculate the argument reordering model probability when a lexical rule is applied to translate a source phrase c to a target phrase e. The Eq. (8) shows how we compute the argument reordering model probability for a span (i, j) in a dynamic programming manner when a merging rule is applied to combine its two </context>
<context position="20726" citStr="Xiong et al., 2006" startWordPosition="3342" endWordPosition="3345"> P,(A(i, k, -r)) and P,(A(k + 1, j, -r)) that have been already obtained for the two sub-spans (i, k) and (k + 1, j). In this way, we only need to calculate the probability P,(N(i, k, j, -r)) for predicateargument pairs that cross the two sub-spans. 6 Experiments In this section, we present our experiments on Chinese-to-English translation tasks, which are trained with large-scale data. The experiments are aimed at measuring the effectiveness of the proposed discriminative predicate translation model and argument reordering model. 6.1 Setup The baseline system is the BTG-based phrasal system (Xiong et al., 2006). Our training corpora5 consist of 3.8M sentence pairs with 96.9M Chinese words and 109.5M English words. We ran GIZA++ on these corpora in both directions and then applied the “grow-diag-final” refinement rule to obtain word alignments. We then used all these word-aligned corpora to generate our phrase table. Our 5-gram language model was trained on the Xinhua section of the English Gigaword corpus (306 million words) 5The corpora include LDC2004E12, LDC2004T08, LDC2005T10, LDC2003E14, LDC2002E18, LDC2005T06, LDC2003E07 and LDC2004T07. using the SRILM toolkit (Stolcke, 2002) with modified Kne</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 521–528, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>A maximum-entropy segmentation model for statistical machine translation.</title>
<date>2011</date>
<journal>IEEE Transactions on Audio, Speech and Language Processing,</journal>
<volume>19</volume>
<issue>8</issue>
<marker>Xiong, Zhang, Li, 2011</marker>
<rawString>Deyi Xiong, Min Zhang, and Haizhou Li. 2011. A maximum-entropy segmentation model for statistical machine translation. IEEE Transactions on Audio, Speech and Language Processing, 19(8):2494–2505.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Labeling chinese predicates with semantic roles.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="2588" citStr="Xue, 2008" startWordPosition="356" endWordPosition="357">tly translated nor translated at all in many SMT systems according to the error study by Wu and Fung (2009a). This suggests that conventional lexical and phrasal translation models adopted in those SMT systems are not sufficient to correctly translate predicates in source sentences. Thus we propose a discriminative, feature-based predicate translation model that captures not only lexical information (i.e., surrounding words) but also high-level semantic contexts to correctly translate predicates. Arguments contain information for questions of who, what, when, where, why, and how in sentences (Xue, 2008). One common error in translating arguments is about their reorderings: arguments are placed at incorrect positions after translation. In order to reduce such errors, we introduce a discriminative argument reordering model that uses the position of a predicate as the reference axis to estimate positions of its associated arguments on the target side. In this way, the model predicts moving directions of arguments relative to their predicates with semantic features. We integrate these two discriminative models into a state-of-the-art phrase-based system. Experimental results on large-scale Chine</context>
</contexts>
<marker>Xue, 2008</marker>
<rawString>Nianwen Xue. 2008. Labeling chinese predicates with semantic roles. Computational Linguistics, 34(2):225–255.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>