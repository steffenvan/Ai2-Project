<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000417">
<title confidence="0.988764">
A Rule-based Conversation Participant
</title>
<author confidence="0.948145">
Robert E. Frederking
</author>
<affiliation confidence="0.8564015">
Computer Science Department, Carnegie-Mellon University
Pittsburgh, Pennsylvania 15213
</affiliation>
<sectionHeader confidence="0.867103" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999743363636364">
The problem of modeling human understanding and
generation of a coherent dialog is investigated by simulating a
conversation participant. The rule-based system currently
under development attempts to capture the intuitive concept
of &amp;quot;topic&amp;quot; using data structures consisting of declarative
representations of the subjects under discussion linked to the
utterances and rules that generated them. Scripts, goal trees,
and a semantic network are brought to bear by general,
domain-independent conversational rules to understand and
generate coherent topic transitions and specific output
utterances.
</bodyText>
<listItem confidence="0.600632">
1. Rules, topics, and utterances
</listItem>
<bodyText confidence="0.997976641304348">
Numerous systems have been proposed to model human use
of language in conversation (speech acts [1], MICS [3],
Grosz [5]). They have attacked the problem from several
different directions. Often an attempt has been made to
develop some intersentential analog of syntax, despite the
severe problems that grammar-oriented parsers have
experienced. The program described in this paper avoids the
use of such a grammar, using instead a model of the
conversation&apos;s topics to provide the necessary connections
between utterances. It is similar to the ELI parsing system,
developed by Riesbeck and Schenk [7], in that it uses
relatively small, independent segments of code (or &amp;quot;rules&amp;quot;) to
decide how to respond to each utterance, given the context
of the utterances that have already occurred. The program
currently operates in the role of a graduate student
discussing qualifier exams, although the rules and control
structures are independent of the domain, and do not assume
any a priori topic of discussion.
The main goals of this project are:
. To develop a small number of general rules that
manipulate internal models of topics in order to
produce a coherent conversation.
. To develop a representation for these models of
topics which will enable the rules to generate
responses, control the flow of conversation, and
maintain a history of the system&apos;s actions during
the current conversation.
This research was sponsored in part by the Defense
Advanced Research Projects Agency (DOD), ARPA Order No.
3597, monitored by the Air Force Avionics Laboratory Under
Contract F33615-78-C-1551.
The views and conclusions contained in this document are
those of the author and should not be interpreted as
representing the official policies, either expressed or implied,
of the Defense Advanced Research Projects Agency or the
US Government.
. To integrate information from a semantic
network, scripts, dynamic goal trees, and the
current conversation in order to allow intelligent
action by the rules.
The rule-based approach was chosen because it appears to
work in a better and more natural way than syntactic pattern
matching in the domain of single utterances, even though a
grammatical structure can be clearly demonstrated there. If it
is awkward to use a grammar for single-sentence analysis,
why expect it to work in the larger domain of human
discourse,- where there is no obviously demonstrable
&amp;quot;syntactic&amp;quot; structure? In place of grammar productions,
rules are used which can initiate and close topics, and form
utterances based on the input, current topics, and long-term
knowledge. This set of rules does not include any domain-
specific inferences; instead, these are placed into the
semantic network when the situations in which they apply are
discussed.
It is important to realize that a &amp;quot;topic&amp;quot; in the sense used in
this paper is not the same thing as the concept of &amp;quot;focus&amp;quot;
used in the anaphora and coreference disambiguation
literature. There, the idea is to decide which part of a
sentence is being focused on (the &amp;quot;topic&amp;quot; of the sentence),
so that the system can determine which phrase will be
referred to by any future anaphoric references (such as
pronouns). In this paper, a topic is a concept, possibly
encompassing more than the sentence itself, which is
&amp;quot;brought to mind&amp;quot; when a person hears an utterance (the
&amp;quot;topic&amp;quot; of a conversation). It is used to decide which
utterances can be generated in response to the input
utterance, something that the focus of a sentence (by itself)
can not in general do. The topics need to be stored (as
opposed to possibly generating them when needed) simply
because a topic raised by an input utterance might not be
addressed until a more interesting topic has been discussed.
The data structure used to represent a topic is simply an
object whose value is a Conceptual Dependency (or CD) [8]
description of the topic, with pointers to rules., utterances,
and other topics which are causally or temporally related to it,
plus an indication of what conversational goal of the program
this topic is intended to fulfill. The types of relations
represented include: the rule (and any utterances involved)
that resulted in the generation of the topic, any utterances
generated from the topic, the topics generated before and
after this one (if any), and the rule (and utterances) that
resulted in the closing of this topic (if it has been closed).
Utterances have a similar representation: a CD expression
with pointers to the rules, topics, and other utterances to
which they are related. This interconnected set of CD
expressions is referred to as the topic-utterance graph, a
small example of which (without CDs) is illustrated in Figure
1-1. The various pointers allow the program to remember
what it has or has not done, and why. Some are used by rules
that have already been implemented. while others are
provided for rules not yet built (the current rules are
described in sections 2.2 and 3).
</bodyText>
<page confidence="0.941659">
83
</page>
<equation confidence="0.9147755">
UTTS U1 .4-=- 7— U2 U3
TOPICS --, T1 T2 T3 T4
v./
R1
R2=-__- =7= __ --
•
</equation>
<figureCaption confidence="0.936179">
Figure 1-1: A topic-utterance graph
</figureCaption>
<sectionHeader confidence="0.862491" genericHeader="method">
2. The computational model
</sectionHeader>
<bodyText confidence="0.99477238">
The system under implementation is, as the title says, a rule-
based conversation participant. Since language was
originally only spoken, and used primarily as an immediate
communication device, it is not unreasonable to assume that
the mental machinery we wish to model is designed primarily
for use in an interactive fashion, such as in dialogue. Thus, it
is more natural to model one interacting participant than to try
to model an external observer&apos;s understanding of the whole
interaction.
2.1. Control
One of the nice properties of rule-based systems is that they
tend to have simple control structures. In the conversation
participant: the rule application routine is simply an
initialization followed by a loop in which a CD expression is
input, rules are tried until one produces a reply-wait signal,
and the output CD is printed. A special token is output to
indicate that the conversation is over, causing an exit from
the loop. One can view this part of the model as an
input/output interface, connecting the data structures that
the rules access with the outside world.
Control decisions outside of the rules themselves are handled
by the agenda structure and the interest-rating routine. An
agenda is essentially a list of lists, with each of the sublists
referred to as a &amp;quot;bucker. Each bucket holds the names of
one or more rules. The actual firing of rules is not as simple
as indicated in the above paragraph, in that all of the rules in •
a bucket are tested, and allowed to fire if their test clauses are
true. After all the rules in a bucket have been tested, if any of
them have produced a reply-wait signal, the &amp;quot;best&amp;quot; utterance
is chosen for output by the interest-rating routine, and the
main loop described above continues. If none have indicated
a need to wait, the next bucket is then tried. Thus, the rules in
the first bucket are always tried and have highest priority.
Priority decreases on a bucket-by-bucket basis down to the
last bucket. In a normal agenda, the act of firing is the same
as what I am calling the reply-wait signal, but in this system
there is an additional twist. It is necessary to have a way to
produce two sentences in a row, not necessarily tightly
related to each other (such as an interjection followed by a
question). Rather than trying to guarantee that all such sets
of rules are in single buckets, the rules have been given the
ability to fire, produce an utterance, cause it to be output
immediately, and not have the agenda stopped, simply by
indicating that a reply-wait is not needed. It is also possible
for a rule to fire without producing either an utterance or a
reply-wait, as is the case for rules that simply create topics, or
to produce a list of utterances, which the interest-rater must
then look through.
The interest-rating routine determines which of the
utterances produced by the rules in a bucket (and not
immediately output) is the best, and so should be output. This
is done by comparing the proposed utterance to our model of
the goals of the speaker, the listener, and the person being
discussed. Currently only the goals of the person being
discussed are examined, but this will be extended to include
the goals of the other two. The comparison involves looking
through our model of his goal tree, giving an utterance a
higher ranking for matching a more important goal. This is
adjusted by a small amount to favor utterances which imply
reaching a goal and to disfavor those which imply failing to
reach it. Goal trees are stored in long-term memory (see next
section).
2.2. Memo ries
There are three main kinds of memory in this model: working
memory, long-term memory, and rule memory. The data
structures representing working memory include several
global variables plus the topic-utterance graph. The topic-
utterance graph has the general form of two doubly-linked
lists, one consisting of all utterances input and output (in
chronological order) and the other containing the topics (in
the order they were generated), with various pointers
indicating the relationships between individual topics and
utterances. These were detailed in section 1.
Long-term memory is represented as a semantic network [2].
Input utterances which are accepted as true, as well as their
immediate inferences, are stored here. The typical semantic
network concept has been extended somewhat to include two
types of information not usually found there: goal trees and
scripts.
Goal trees [6, 3] are stored under individual tokens or classes
(on the property GOALS) by name. They consist of several
CD concepts linked together by SUBGOAL/SUPERGOAL
links, with the top SUPERGOAL being the most important
goal, and with importance decreasing with distance below the
top of the goal tree. Goal trees represent the program&apos;s
model of a person or organization&apos;s goals. Unlike an earlier
conversation program [3], in this system they can be changed
during the course of a conversation as the program gathers
new information about the entities it already knows something
about. For example, if the program knows that graduate
students want to pass a particular test, and that Frank is a
graduate student, and it hears that Frank passed the test, it
will create an individual goal tree for Frank, and remove the
goal of passing that test. This is done by the routine which
stores CDs in the semantic network, whenever a goal is
mentioned as the second clause of an inference rule that is
being stored. If the rule is stored as true, the first clause of
the implication is made a subgoal of the mentioned goal in the
actor&apos;s goal tree. If the rule is negated, any subgoal matching
the first clause is removed from the goal tree.
</bodyText>
<page confidence="0.991784">
84
</page>
<bodyText confidence="0.99596925">
As for scripts [9], these are the model&apos;s episodic memory and
are stored as tokens in the semantic network, under the class
SCRIPT. Each one represents a detailed knowledge of some
sequence of events (and states), and can contain instances of
other scripts as events. The individual events are represented
in CD. and are generally descriptions of steps in a commonly
occuring routine, such as going to a restaurant or taking a
train trip. In the current context, the main script deals with
the various aspects of a graduate student taking a qualifier.
There are parameters to a script, called &amp;quot;roles&amp;quot; • in this case,
the student, the writers of the exam, the graders, etc. Each
role has some required preconditions. For example, any
writer must be a professor at this university. There are also
postconditions, such as the fact that if the student passes the
qual he/she has fulfilled that requirement for the Ph.D. and
will be pleased. This post-condition is an example of a
domain-dependent inference rule, which is stored in the
semantic network when a situation from the domain is
discussed.
Finally, we have the rule memory. This is just the group of
data objects whose names appear in the agenda. Unlike the
other data objects, however, rules contain Lisp code, stored
in two parts: the TEST and the ACTION. The TEST code is
executed whenever the rule is being tried, and determines
whether it fires or not. It is thus an indication of when this rule
is applicable. (The conditions under which a rule is tried were
given in the section on Control, section 2.1). The ACTION
code is executed when the rule fires, and returns either a list
of utterances (with an implied reply-wait), an utterance with
an indication that no reply wait is necessary, or NIL, the
standard Lisp symbol for &apos;&apos;nothing&amp;quot;. The rules can have side
effects, such as creating a possible topic and then returning
NIL. Although rules are connected into the topic-utterance
graph, they are not really considered part of it, since they are
a permanent part of the system, and contain Lisp code rather
than CD expressions.
</bodyText>
<sectionHeader confidence="0.707623" genericHeader="method">
3. An example explained
</sectionHeader>
<bodyText confidence="0.9987994">
A sample of what the present version of the system can do will
now be examined. It is written in MacLisp, with utterances
input and output in CD. This assumes the existence of
programs to map English to CD and CD to English, both of
which have been previously done to a degree. The agenda
currently contains six rules. The two in the highest priority
bucket stop the conversation if the other person says
&amp;quot;goodbye&amp;quot; or leaves (Rule3-3 and Rule3-4). They are there
to test the control of the system, and will have to be made
more sophisticated (i.e., they should try to keep up the
conversation if important active topics remain).
The three rules in the next bucket are the heart of the system
at its current level of development. The first two raise topics
to request missing information. The first (Rule1) asks about
missing pre-conditions for a script instance, such as when
someone who is not known to be a student takes a qualifier.
The second (Rule2) asks about incompletely specified post-
conditions, such as ,the actual project that someone must do
if they get a remedial. At this university, a remedial is a
conditional pass, where the student must complete a project
in the same area as the qual in order to complete this degree
requirement; there are four quais in the curriculum. The third
rule in this bucket (Rule4) generates questions from topics
that are open requests for information, and is illustrated in
Figure 3-1.
</bodyText>
<equation confidence="0.977763">
RULE4
TEST: (FOR-EACH TOPICS
(AND (EQUAL &apos;REQINFO (GET X
&apos;CPURPOSE))
(NULL (GET X &apos;CLOSEDBY))))
ACTION: (MAPCAN &apos;(LAMBDA (X)
(PROG (TMP)
(RETURN (COND ((SETO TMP
(QUESTIONIZE (GET.
HYPO (
EVAL X))))
(MAPCAN &apos;(LAMBDA (Y)
(COND (Y
(LIST (UTTER Y (LIST X))))))
IMP))))))
TEST-RESULT)
</equation>
<bodyText confidence="0.993006947368421">
Test: Are there any topics which are requests for information
which have not been answered?
Action: Retrieve the hypothetical part, form all &amp;quot;necessary&apos;
questions, and offer them as utterances.
Figu re 3-1: Rule4
The last bucket in the agenda simply has a rule which says &amp;quot;I
don&apos;t understand&amp;quot; in response to things that none of the
previous rules generated a response to (RuleK). This serves
as a safety net for the control structure, so it does not have to
worry about what to do if no response is generated.
Now let us look at how the program handles an actual
conversation fragment. The program always begins by asking
&amp;quot;What&apos;s new?&amp;quot;, to which (this time) it gets the reply, &amp;quot;Frank
got a remedial on his hardware qual.&amp;quot; The CD form for this is
shown in Figure 3-2 (the program currently assumes that the
person it is talking to is a student it knows named John). The
CD version is an instance of the qual script, with Frank,
hardware, and a remedial being the taker, area, and result,
respectively.
</bodyText>
<equation confidence="0.9843536">
U0002
((&lt; =? (=UAL &amp;AREA (*HARDWARE&amp;quot;) &amp;TAKER
(*FRANK&apos;) &amp;RESULT (REMEDIAL))))
(ISA (*UTTERANCE&apos;) PERSON *JOHN • PRED
UTTS)
</equation>
<figureCaption confidence="0.812695">
Figure 3-2: First input utterance
</figureCaption>
<bodyText confidence="0.994240909090909">
When the rules examine this, five topics are raised, one due to
the pre-condition that he has not passed the qual before (by
Rule1), and four due to various partially specified post-
conditions (by Rule2):
. If Frank was confident, he will be unhappy-
. If he was not confident, he will be content.
. He has to do a project. We don&apos;t know what.
. If he has completed his project, he might be able
to graduate.
The system only asks about things it does not know. In this
case, it knows that Frank is a student, so it does not ask about
</bodyText>
<page confidence="0.998549">
85
</page>
<bodyText confidence="0.8711855">
that. As an example. the topic that asks whether he is content
is illustrated in Figure 3-3.
</bodyText>
<figure confidence="0.602641380952381">
T0005
((CON ((&lt; = &gt; ($OUAL &amp;AREA
(&apos;HARDWARE&amp;quot;)
&amp;TAKER
(*FRANK&apos;)
&amp;RESULT
(&apos;REMEDIAL&apos;))))
LEADTO
((CON ((ACTOR (*FRANK&apos;) IS
(CONFIDENCE&apos; VAL (&gt;0)))
MOD
(*NEG • &apos;HYPO&apos;))
LEADTO
((ACTOR (*FRANK&apos;) IS (*HAPPINESS&apos;
VAL (0)))))
MOD
(&apos;HYPO&apos;))))
(INITIATED (U0013) SUCC 10009 CPURPOSE
REOINFO
INITIATEDBY (RULE2 U0002) ISA (*TOPIC&amp;quot;).
PRED 10004)
</figure>
<figureCaption confidence="0.994686">
Figure 3-3: A sample topic in detail
</figureCaption>
<bodyText confidence="0.999162565217391">
Along with raising these topics, the rules store the utterance
and script post-inferences in the semantic network, under all
the nodes mentioned in them. The following have been
stored under Frank by this point:
. Frank got a remedial on his hardware qual.
. If he was confident, he&apos;ll be unhappy.
. If he was not confident, he&apos;ll be content.
. Passing the hardware qual will not contribute to
his graduating.
. He has a hardware project to do.
. Finishing his hardware project will contribute to
his graduating.
While these were being stored, Frank&apos;s goal tree was altered.
This occurred because two of the post-inferences are
themselves inference rules that affect whether he will
graduate, and graduating is already assumed to be a goal of
any student. Thus when the first is stored, a new goal tree is
created for Frank (since his interests were represented before
by the Student goal tree), and the goal of passing the
hardware qual is removed. When &apos;the second is stored, the
goal of finishing the project is added below that of graduating
on Frank&apos;s tree. These goal trees are illustrated in Figures 3-4
and 3-5.
</bodyText>
<equation confidence="0.604168166666667">
((ACTOR (*STUDENT&apos;) IS (*HAPPINESS* VAL
(5)))) Subgoal
((&lt; =) ($GRAD &amp;ACTOR (*STUDENT&apos;) &amp;SCHOOL
(&amp;quot;CMU&amp;quot;)))) Subgoal
((&lt; =&gt; ($0UAL &amp;TAKER (*STUDENT&apos;) &amp;AREA
(*HARDWARE&apos;) &amp;RESULT (*PASSED&amp;quot;))))
</equation>
<figureCaption confidence="0.694953">
Figure 3-4: A student&apos;s goal tree
</figureCaption>
<equation confidence="0.832967285714285">
((ACTOR (FRANK) IS (HAPPINESS • VAL (5))))
Subgoal
((&lt; = &gt; ($GRAD &amp;ACTOR (*FRANK&amp;quot;) &amp;SCHOOL
(&amp;quot;CML.1&amp;quot;)))) Subgoal
((&lt; = &gt; ($PROJECT &amp;STUDENT (*FRANK&apos;) &amp;AREA
(&amp;quot; HARDWARE •) &amp;RESULT (&apos;COMPLETED&apos;)))
MOD (HYPO) TIME (&gt; *NOW&amp;quot;))
</equation>
<figureCaption confidence="0.957921">
Figure 3-5: Frank&apos;s new goal tree
</figureCaption>
<bodyText confidence="0.993238435897436">
At this point, six utterances are generated by Rule4. They are
given in Figure 3-6. Three are generated from the first topic,
one is generated from each of the next three topics, and none
is generated from the last topic. The interest rating routine
now compares these utterances to Frank&apos;s goals, and picks
the most interesting one. Because of the new goal tree, the
last three utterances match none of Frank&apos;s goals, and
receive zero ratings. The first one matches his third goal in a
neutral way, and receives a rating of 56 (an utterance
receives 64 points for the top goal, minus 4 for each level
below top, plus or minus one for positive/negative
implications. These numbers are, of course, arbitrary, as long
as ratings from different goals do not overlap). The second
one matches his top goal in a neutral way, and receives 64.
Finally, the third one matches his top goal in a negative way,
and receives 63. Therefore, the second question gets
uttered, and ends up with the links shown in Figure 3-7. The
other generated utterances are discarded, possibly to be
regenerated later, if their topics are still open.
((&lt; =&gt; ($PROJECT &amp;STUDENT (*FRANK&amp;quot;) &amp;AREA
(&apos;HARDWARE&apos;) &amp;BODY (&apos;r))))
What project does he have to do?
((ACTOR (*FRANK&apos;) IS (&amp;quot;HAPPINESS* VAL (0)))
MOD ell)
Is he content?
((ACTOR (*FRANK&amp;quot;) IS (*HAPPINESS&apos; VAL (-3)))
MOD (•?•))
Is he unhappy?
((&lt; =&gt; ($OUAL &amp;TAKER (&amp;quot;FRANK&amp;quot;) &amp;AREA
(&apos;HARD WARE&apos;))) MOD (•?* &amp;quot;NEG*))
Hadn&apos;t he taken it before?
((&lt; =&gt; ($0UAL &amp;TAKER (&apos;FRANKe) &amp;AREA
(*HARDWARE&apos;) &amp;RESULT (&amp;quot;CANCELLED&amp;quot;)))
MOD (•?•))
Had it been cancelled on him before?
((&lt; =&gt; ($OUAL &amp;TAKER (*FRANK&apos;) &amp;AREA
(*HARDWARE&apos;) &amp;RESULT (&amp;quot;FAILED*))) MOD
Cr))
Had he failed it before?
</bodyText>
<figureCaption confidence="0.859426">
Figure 3-6: The six possible utterances generated
</figureCaption>
<sectionHeader confidence="0.965916" genericHeader="method">
4. Other work, future work
</sectionHeader>
<bodyText confidence="0.9997855">
Two other approaches used in modelling conversation are
task-oriented and speech acts based systems. Both of these
methodologies have their merits, but neither attacks all the
same aspects of the problem that this system does. Task-
</bodyText>
<page confidence="0.973862">
86
</page>
<figure confidence="0.980501285714286">
U0013
((ACTOR (*FRANK&apos;) IS (*HAPPINESS* VAL (0)))
MOD (*?*))
(PRED U0002 ISA (*UTTERANCE.) PERSON
• ME*
INTEREST-REASON (G0006) INTEREST 64
INITIATEDBY (RULE4 T0005))
</figure>
<figureCaption confidence="0.991889">
Figure 3-7: System&apos;s response to first utterance
</figureCaption>
<bodyText confidence="0.9994086">
oriented systems [5] operate in the context of some fixed task
which both speakers are trying to accomplish. Because of
this, they can infer the topics that are likely to be discussed
from the semantic structure of the task. For example, a task-
oriented system talking about qualifiers would use the
knowledge of how to be a student in order to talk about those
things relevant to passing qualifiers (simulating a very
studious student). It would not usually ask a question like &amp;quot;Is
Frank content?&amp;quot;, because that does not matter from a
practical point of view.
Speech acts based systems (such as [1]) try to reason about
the plans that the actors in the conversation are trying to
execute, viewing each utterance as an operator on the
environment. Consequently, they are concerned mostly
about what people mean when they use indirect speech acts
(such as using &amp;quot;It&apos;s cold in here&amp;quot; to say &amp;quot;Close the window&amp;quot;)
and are not as concerned about trying to say interesting
things as this system is. Another way to look at the two kinds
of systems is that speech acts systems reason about the
actors&apos; plans and assume fixed goals, whereas this system
reasons primarily about their goals.
rules are truly independent of the subject matter, trying to
make the system work with several scripts at once (as
SAM [4] does), and improving the semantic network to handle
the well-known problems which may arise.
</bodyText>
<sectionHeader confidence="0.998077" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.86916">
Allen, J. F. and Perrault, C. R.
Analyzing Intention in Utterances.
Artificial Intelligence 15(3):143-178, December, 1980.
Brachman, R. J. •
On the Epistemological Status of Semantic Networks.
In Findler, N. V. (editor), Associative Networks:
Representation and Use of Knowledge by
Computers, chapter 1 in particular. Academic
Press, New York, 1979.
</reference>
<page confidence="0.853102">
[31
</page>
<bodyText confidence="0.9998765">
As for related work, ELI (the language analyzer mentioned in
section 1) and this system (when fully developed) could
theoretically be merged into a single conversation system,
with some rules working on mapping English into CD, and
others using the CD to decide what responses to generate. In
fact, there are situations in which one needs to make use of
both kinds of information (such as when a phrase signals a [4]
topic shift: &amp;quot;On the other hand...&amp;quot;). One of the possible
directions for future work is the incorporation and integration
of a rule-based parser into the system, along with some form
of rule-based English generation. Another related system,
MICS [3], had research goals and a set of knowledge sources
somewhat similar to this system&apos;s, but it differed primarily in
that it could not alter its goal trees during a conversation, nor
did it have explicit data structures for representing topics (the
selection of topics was built into the interpreter).
Tha main results of this research so far have been the topic-
utterance graph and dynamic goal trees. Although some way
of holding the intersentential information was obviously
needed, no precise form was postulated initially. The current
structure was invented after working with an earlier set of
rules to discover the most useful form the topics could take.
Similarly, the idea that a changing view of someone else&apos;s
goals should be used to control the course of the
conversation arose during work on producing the interest-
rating routine. The current system is, of course, by no means
a complete model of human discourse. More rules need to be
developed, and the current ones need to be refined.
In addition to implementing more rules and incorporating a
parser, possible areas for future work include replacing the
interest-rater with a second agenda (containing interest-
determining rules), changing scripts and testing whether the
</bodyText>
<figure confidence="0.895631">
(51
[6]
Carbonell, J. G.
Subjective Understanding: Computer Models of Belief
Systems.
PhD thesis, Yale University, January, 1979.
Computer Science Research Report #150.
Cullingford, R. E. •
Script Application: Computer Understanding of
Newspaper Stories.
PhD thesis, Yale University, January, 1978.
Computer Science Research Report #116.
</figure>
<bodyText confidence="0.92796652631579">
Grosz, B.J.
The Representation and use of Focus in Dialogue
Understanding.
Technical Report 151, Stanford Research Institute,
July, 1977.
Newell, A. and Simon, H. A.
Human Problem Solving.
Prentice Hail, Englewood Cliffs, N. J., 1972, chapter 8.
Riesbeck, C. and Schank, R. C.
Comprehension by Computer: Expectation Based
Analysis of Sentences in Context.
Technical Report 78, Department of Computer
Science, Yale University, 1976.
Schenk, R. C.
Conceptual Information Processing.
North-Holland, 1975, chapter 3.
Schank, R. C. and Abelson, R.
Scripts, Plans, Goals and Understanding.
Erlbaum, 1977, chapter 3.
</bodyText>
<page confidence="0.998889">
87
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999862">A Rule-based Conversation Participant</title>
<author confidence="0.999971">Robert E Frederking</author>
<affiliation confidence="0.99999">Computer Science Department, Carnegie-Mellon University</affiliation>
<address confidence="0.999881">Pittsburgh, Pennsylvania 15213</address>
<abstract confidence="0.999764475">The problem of modeling human understanding and generation of a coherent dialog is investigated by simulating a conversation participant. The rule-based system currently under development attempts to capture the intuitive concept of &amp;quot;topic&amp;quot; using data structures consisting of declarative representations of the subjects under discussion linked to the utterances and rules that generated them. Scripts, goal trees, and a semantic network are brought to bear by general, domain-independent conversational rules to understand and generate coherent topic transitions and specific output utterances. 1. Rules, topics, and utterances Numerous systems have been proposed to model human use of language in conversation (speech acts [1], MICS [3], Grosz [5]). They have attacked the problem from several different directions. Often an attempt has been made to develop some intersentential analog of syntax, despite the severe problems that grammar-oriented parsers have experienced. The program described in this paper avoids the use of such a grammar, using instead a model of the conversation&apos;s topics to provide the necessary connections between utterances. It is similar to the ELI parsing system, developed by Riesbeck and Schenk [7], in that it uses relatively small, independent segments of code (or &amp;quot;rules&amp;quot;) to decide how to respond to each utterance, given the context of the utterances that have already occurred. The program currently operates in the role of a graduate student discussing qualifier exams, although the rules and control structures are independent of the domain, and do not assume priori of discussion. The main goals of this project are: . To develop a small number of general rules that manipulate internal models of topics in order to produce a coherent conversation. . To develop a representation for these models of topics which will enable the rules to generate responses, control the flow of conversation, and maintain a history of the system&apos;s actions during the current conversation.</abstract>
<note confidence="0.8986528">This research was sponsored in part by the Defense Advanced Research Projects Agency (DOD), ARPA Order No. 3597, monitored by the Air Force Avionics Laboratory Under Contract F33615-78-C-1551. The views and conclusions contained in this document are</note>
<abstract confidence="0.996832320346321">those of the author and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the US Government. . To integrate information from a semantic network, scripts, dynamic goal trees, and the current conversation in order to allow intelligent action by the rules. The rule-based approach was chosen because it appears to work in a better and more natural way than syntactic pattern matching in the domain of single utterances, even though a grammatical structure can be clearly demonstrated there. If it is awkward to use a grammar for single-sentence analysis, why expect it to work in the larger domain of human discourse,where there is no obviously demonstrable &amp;quot;syntactic&amp;quot; structure? In place of grammar productions, rules are used which can initiate and close topics, and form utterances based on the input, current topics, and long-term knowledge. This set of rules does not include any domainspecific inferences; instead, these are placed into the semantic network when the situations in which they apply are discussed. It is important to realize that a &amp;quot;topic&amp;quot; in the sense used in this paper is not the same thing as the concept of &amp;quot;focus&amp;quot; used in the anaphora and coreference disambiguation literature. There, the idea is to decide which part of a sentence is being focused on (the &amp;quot;topic&amp;quot; of the sentence), so that the system can determine which phrase will be referred to by any future anaphoric references (such as pronouns). In this paper, a topic is a concept, possibly encompassing more than the sentence itself, which is &amp;quot;brought to mind&amp;quot; when a person hears an utterance (the &amp;quot;topic&amp;quot; of a conversation). It is used to decide which utterances can be generated in response to the input utterance, something that the focus of a sentence (by itself) can not in general do. The topics need to be stored (as opposed to possibly generating them when needed) simply because a topic raised by an input utterance might not be addressed until a more interesting topic has been discussed. The data structure used to represent a topic is simply an object whose value is a Conceptual Dependency (or CD) [8] description of the topic, with pointers to rules., utterances, and other topics which are causally or temporally related to it, plus an indication of what conversational goal of the program this topic is intended to fulfill. The types of relations represented include: the rule (and any utterances involved) that resulted in the generation of the topic, any utterances generated from the topic, the topics generated before and after this one (if any), and the rule (and utterances) that resulted in the closing of this topic (if it has been closed). Utterances have a similar representation: a CD expression with pointers to the rules, topics, and other utterances to which they are related. This interconnected set of CD expressions is referred to as the topic-utterance graph, a small example of which (without CDs) is illustrated in Figure 1-1. The various pointers allow the program to remember it has or has not done, and Some are used by rules that have already been implemented. while others are provided for rules not yet built (the current rules are described in sections 2.2 and 3). 83 U1 U2 U3 TOPICS --, T1 T2 T3 T4 R1 R2=-__- =7= __ -- • Figure 1-1: A topic-utterance graph 2. The computational model The system under implementation is, as the title says, a rulebased conversation participant. Since language was originally only spoken, and used primarily as an immediate communication device, it is not unreasonable to assume that the mental machinery we wish to model is designed primarily for use in an interactive fashion, such as in dialogue. Thus, it is more natural to model one interacting participant than to try to model an external observer&apos;s understanding of the whole interaction. 2.1. Control One of the nice properties of rule-based systems is that they tend to have simple control structures. In the conversation participant: the rule application routine is simply an initialization followed by a loop in which a CD expression is input, rules are tried until one produces a reply-wait signal, and the output CD is printed. A special token is output to indicate that the conversation is over, causing an exit from the loop. One can view this part of the model as an input/output interface, connecting the data structures that the rules access with the outside world. Control decisions outside of the rules themselves are handled by the agenda structure and the interest-rating routine. An agenda is essentially a list of lists, with each of the sublists referred to as a &amp;quot;bucker. Each bucket holds the names of one or more rules. The actual firing of rules is not as simple as indicated in the above paragraph, in that all of the rules in • a bucket are tested, and allowed to fire if their test clauses are true. After all the rules in a bucket have been tested, if any of them have produced a reply-wait signal, the &amp;quot;best&amp;quot; utterance is chosen for output by the interest-rating routine, and the main loop described above continues. If none have indicated a need to wait, the next bucket is then tried. Thus, the rules in the first bucket are always tried and have highest priority. Priority decreases on a bucket-by-bucket basis down to the last bucket. In a normal agenda, the act of firing is the same as what I am calling the reply-wait signal, but in this system there is an additional twist. It is necessary to have a way to produce two sentences in a row, not necessarily tightly related to each other (such as an interjection followed by a question). Rather than trying to guarantee that all such sets of rules are in single buckets, the rules have been given the ability to fire, produce an utterance, cause it to be output immediately, and not have the agenda stopped, simply by indicating that a reply-wait is not needed. It is also possible for a rule to fire without producing either an utterance or a reply-wait, as is the case for rules that simply create topics, or to produce a list of utterances, which the interest-rater must then look through. The interest-rating routine determines which of the utterances produced by the rules in a bucket (and not immediately output) is the best, and so should be output. This is done by comparing the proposed utterance to our model of the goals of the speaker, the listener, and the person being discussed. Currently only the goals of the person being discussed are examined, but this will be extended to include the goals of the other two. The comparison involves looking through our model of his goal tree, giving an utterance a higher ranking for matching a more important goal. This is adjusted by a small amount to favor utterances which imply reaching a goal and to disfavor those which imply failing to reach it. Goal trees are stored in long-term memory (see next section). 2.2. Memo ries There are three main kinds of memory in this model: working memory, long-term memory, and rule memory. The data structures representing working memory include several global variables plus the topic-utterance graph. The topicutterance graph has the general form of two doubly-linked lists, one consisting of all utterances input and output (in chronological order) and the other containing the topics (in the order they were generated), with various pointers indicating the relationships between individual topics and utterances. These were detailed in section 1. Long-term memory is represented as a semantic network [2]. Input utterances which are accepted as true, as well as their inferences, are stored typical semantic network concept has been extended somewhat to include two types of information not usually found there: goal trees and scripts. Goal trees [6, 3] are stored under individual tokens or classes (on the property GOALS) by name. They consist of several CD concepts linked together by SUBGOAL/SUPERGOAL links, with the top SUPERGOAL being the most important goal, and with importance decreasing with distance below the top of the goal tree. Goal trees represent the program&apos;s model of a person or organization&apos;s goals. Unlike an earlier conversation program [3], in this system they can be changed during the course of a conversation as the program gathers new information about the entities it already knows something about. For example, if the program knows that graduate students want to pass a particular test, and that Frank is a graduate student, and it hears that Frank passed the test, it will create an individual goal tree for Frank, and remove the goal of passing that test. This is done by the routine which stores CDs in the semantic network, whenever a goal is mentioned as the second clause of an inference rule that is being stored. If the rule is stored as true, the first clause of the implication is made a subgoal of the mentioned goal in the actor&apos;s goal tree. If the rule is negated, any subgoal matching the first clause is removed from the goal tree. 84 As for scripts [9], these are the model&apos;s episodic memory and are stored as tokens in the semantic network, under the class SCRIPT. Each one represents a detailed knowledge of some sequence of events (and states), and can contain instances of other scripts as events. The individual events are represented in CD. and are generally descriptions of steps in a commonly occuring routine, such as going to a restaurant or taking a train trip. In the current context, the main script deals with the various aspects of a graduate student taking a qualifier. There are parameters to a script, called &amp;quot;roles&amp;quot; • in this case, the student, the writers of the exam, the graders, etc. Each role has some required preconditions. For example, any writer must be a professor at this university. There are also postconditions, such as the fact that if the student passes the qual he/she has fulfilled that requirement for the Ph.D. and will be pleased. This post-condition is an example of a domain-dependent inference rule, which is stored in the semantic network when a situation from the domain is discussed. Finally, we have the rule memory. This is just the group of data objects whose names appear in the agenda. Unlike the other data objects, however, rules contain Lisp code, stored in two parts: the TEST and the ACTION. The TEST code is executed whenever the rule is being tried, and determines whether it fires or not. It is thus an indication of when this rule is applicable. (The conditions under which a rule is tried were given in the section on Control, section 2.1). The ACTION code is executed when the rule fires, and returns either a list of utterances (with an implied reply-wait), an utterance with an indication that no reply wait is necessary, or NIL, the standard Lisp symbol for &apos;&apos;nothing&amp;quot;. The rules can have side effects, such as creating a possible topic and then returning NIL. Although rules are connected into the topic-utterance graph, they are not really considered part of it, since they are a permanent part of the system, and contain Lisp code rather than CD expressions. 3. An example explained A sample of what the present version of the system can do will now be examined. It is written in MacLisp, with utterances input and output in CD. This assumes the existence of programs to map English to CD and CD to English, both of which have been previously done to a degree. The agenda currently contains six rules. The two in the highest priority bucket stop the conversation if the other person says &amp;quot;goodbye&amp;quot; or leaves (Rule3-3 and Rule3-4). They are there to test the control of the system, and will have to be made more sophisticated (i.e., they should try to keep up the conversation if important active topics remain). The three rules in the next bucket are the heart of the system at its current level of development. The first two raise topics to request missing information. The first (Rule1) asks about missing pre-conditions for a script instance, such as when someone who is not known to be a student takes a qualifier. The second (Rule2) asks about incompletely specified postconditions, such as ,the actual project that someone must do if they get a remedial. At this university, a remedial is a conditional pass, where the student must complete a project in the same area as the qual in order to complete this degree requirement; there are four quais in the curriculum. The third rule in this bucket (Rule4) generates questions from topics that are open requests for information, and is illustrated in Figure 3-1.</abstract>
<pubnum confidence="0.204413">RULE4</pubnum>
<title confidence="0.407198923076923">TEST: (FOR-EACH TOPICS (AND (EQUAL &apos;REQINFO (GET X &apos;CPURPOSE)) (NULL (GET X &apos;CLOSEDBY)))) ACTION: (MAPCAN &apos;(LAMBDA (X) (PROG (TMP) (RETURN (COND ((SETO TMP (QUESTIONIZE (GET. HYPO ( EVAL X)))) (MAPCAN &apos;(LAMBDA (Y) (COND (Y (LIST (UTTER Y (LIST X</title>
<abstract confidence="0.938127487804878">IMP)))))) TEST-RESULT) Test: Are there any topics which are requests for information which have not been answered? Action: Retrieve the hypothetical part, form all &amp;quot;necessary&apos; questions, and offer them as utterances. Figu re 3-1: Rule4 The last bucket in the agenda simply has a rule which says &amp;quot;I don&apos;t understand&amp;quot; in response to things that none of the previous rules generated a response to (RuleK). This serves as a safety net for the control structure, so it does not have to worry about what to do if no response is generated. Now let us look at how the program handles an actual conversation fragment. The program always begins by asking &amp;quot;What&apos;s new?&amp;quot;, to which (this time) it gets the reply, &amp;quot;Frank got a remedial on his hardware qual.&amp;quot; The CD form for this is shown in Figure 3-2 (the program currently assumes that the person it is talking to is a student it knows named John). The CD version is an instance of the qual script, with Frank, hardware, and a remedial being the taker, area, and result, respectively. U0002 ((&lt; =? (=UAL &amp;AREA (*HARDWARE&amp;quot;) &amp;TAKER (*FRANK&apos;) &amp;RESULT (REMEDIAL)))) (ISA (*UTTERANCE&apos;) PERSON *JOHN • PRED UTTS) Figure 3-2: First input utterance When the rules examine this, five topics are raised, one due to the pre-condition that he has not passed the qual before (by due to various partially specified postconditions (by Rule2): . If Frank was confident, he will be unhappy- . If he was not confident, he will be content. . He has to do a project. We don&apos;t know what. . If he has completed his project, he might be able to graduate. The system only asks about things it does not know. In this case, it knows that Frank is a student, so it does not ask about 85 that. As an example. the topic that asks whether he is content is illustrated in Figure 3-3.</abstract>
<note confidence="0.4210359">T0005 ((CON ((&lt; = &gt; ($OUAL &amp;AREA (&apos;HARDWARE&amp;quot;) &amp;TAKER (*FRANK&apos;) &amp;RESULT (&apos;REMEDIAL&apos;)))) LEADTO ((CON ((ACTOR (*FRANK&apos;) IS (CONFIDENCE&apos; VAL (&gt;0</note>
<title confidence="0.549921">MOD (*NEG • &apos;HYPO&apos;)) LEADTO ((ACTOR (*FRANK&apos;) IS (*HAPPINESS&apos;</title>
<email confidence="0.455623">MOD</email>
<note confidence="0.929113">(&apos;HYPO&apos;)))) (INITIATED (U0013) SUCC 10009 CPURPOSE REOINFO INITIATEDBY (RULE2 U0002) ISA (*TOPIC&amp;quot;). PRED 10004) Figure 3-3: A sample topic in detail</note>
<abstract confidence="0.985249304347826">Along with raising these topics, the rules store the utterance and script post-inferences in the semantic network, under all the nodes mentioned in them. The following have been stored under Frank by this point: . Frank got a remedial on his hardware qual. . If he was confident, he&apos;ll be unhappy. . If he was not confident, he&apos;ll be content. . Passing the hardware qual will not contribute to his graduating. . He has a hardware project to do. . Finishing his hardware project will contribute to his graduating. While these were being stored, Frank&apos;s goal tree was altered. This occurred because two of the post-inferences are themselves inference rules that affect whether he will graduate, and graduating is already assumed to be a goal of any student. Thus when the first is stored, a new goal tree is created for Frank (since his interests were represented before by the Student goal tree), and the goal of passing the hardware qual is removed. When &apos;the second is stored, the goal of finishing the project is added below that of graduating on Frank&apos;s tree. These goal trees are illustrated in Figures 3-4 and 3-5.</abstract>
<note confidence="0.7030976875">ACTOR (*STUDENT&apos;) IS (*HAPPINESS* VAL (5)))) Subgoal ((&lt; =) ($GRAD &amp;ACTOR (*STUDENT&apos;) &amp;SCHOOL (&amp;quot;CMU&amp;quot;)))) Subgoal ((&lt; =&gt; ($0UAL &amp;TAKER (*STUDENT&apos;) &amp;AREA (*HARDWARE&apos;) &amp;RESULT (*PASSED&amp;quot;)))) Figure 3-4: A student&apos;s goal tree ((ACTOR (FRANK) IS (HAPPINESS • VAL (5)))) Subgoal ((&lt; = &gt; ($GRAD &amp;ACTOR (*FRANK&amp;quot;) &amp;SCHOOL (&amp;quot;CML.1&amp;quot;)))) Subgoal ((&lt; = &gt; ($PROJECT &amp;STUDENT (*FRANK&apos;) &amp;AREA (&amp;quot; HARDWARE •) &amp;RESULT (&apos;COMPLETED&apos;))) MOD (HYPO) TIME (&gt; *NOW&amp;quot;)) Figure 3-5: Frank&apos;s new goal tree At this point, six utterances are generated by Rule4. They are</note>
<abstract confidence="0.753827522727273">given in Figure 3-6. Three are generated from the first topic, one is generated from each of the next three topics, and none is generated from the last topic. The interest rating routine now compares these utterances to Frank&apos;s goals, and picks the most interesting one. Because of the new goal tree, the last three utterances match none of Frank&apos;s goals, and receive zero ratings. The first one matches his third goal in a neutral way, and receives a rating of 56 (an utterance receives 64 points for the top goal, minus 4 for each level below top, plus or minus one for positive/negative implications. These numbers are, of course, arbitrary, as long as ratings from different goals do not overlap). The second one matches his top goal in a neutral way, and receives 64. Finally, the third one matches his top goal in a negative way, and receives 63. Therefore, the second question gets uttered, and ends up with the links shown in Figure 3-7. The other generated utterances are discarded, possibly to be regenerated later, if their topics are still open. ((&lt; =&gt; ($PROJECT &amp;STUDENT (*FRANK&amp;quot;) &amp;AREA &amp;BODY What project does he have to do? ((ACTOR (*FRANK&apos;) IS (&amp;quot;HAPPINESS* VAL (0))) MOD ell) Is he content? ((ACTOR (*FRANK&amp;quot;) IS (*HAPPINESS&apos; VAL (-3))) MOD (•?•)) Is he unhappy? ((&lt; =&gt; ($OUAL &amp;TAKER (&amp;quot;FRANK&amp;quot;) &amp;AREA (&apos;HARD WARE&apos;))) MOD (•?* &amp;quot;NEG*)) Hadn&apos;t he taken it before? ((&lt; =&gt; ($0UAL &amp;TAKER (&apos;FRANKe) &amp;AREA (*HARDWARE&apos;) &amp;RESULT (&amp;quot;CANCELLED&amp;quot;))) MOD (•?•)) Had it been cancelled on him before? ((&lt; =&gt; ($OUAL &amp;TAKER (*FRANK&apos;) &amp;AREA (*HARDWARE&apos;) &amp;RESULT (&amp;quot;FAILED*))) MOD Cr)) Had he failed it before? Figure 3-6: The six possible utterances generated 4. Other work, future work Two other approaches used in modelling conversation are task-oriented and speech acts based systems. Both of these methodologies have their merits, but neither attacks all the aspects of the problem that this system does. Task-</abstract>
<note confidence="0.49774">86 U0013 ((ACTOR (*FRANK&apos;) IS (*HAPPINESS* VAL (0</note>
<affiliation confidence="0.726057">MOD (*?*</affiliation>
<address confidence="0.815549">U0002 ISA PERSON</address>
<abstract confidence="0.941149413793103">ME* INTEREST-REASON (G0006) INTEREST 64 INITIATEDBY (RULE4 T0005)) Figure 3-7: System&apos;s response to first utterance oriented systems [5] operate in the context of some fixed task which both speakers are trying to accomplish. Because of this, they can infer the topics that are likely to be discussed from the semantic structure of the task. For example, a taskoriented system talking about qualifiers would use the knowledge of how to be a student in order to talk about those things relevant to passing qualifiers (simulating a very studious student). It would not usually ask a question like &amp;quot;Is Frank content?&amp;quot;, because that does not matter from a practical point of view. Speech acts based systems (such as [1]) try to reason about the plans that the actors in the conversation are trying to execute, viewing each utterance as an operator on the environment. Consequently, they are concerned mostly about what people mean when they use indirect speech acts (such as using &amp;quot;It&apos;s cold in here&amp;quot; to say &amp;quot;Close the window&amp;quot;) are not as concerned about trying to say things as this system is. Another way to look at the two kinds of systems is that speech acts systems reason about the actors&apos; plans and assume fixed goals, whereas this system reasons primarily about their goals. rules are truly independent of the subject matter, trying to make the system work with several scripts at once (as SAM [4] does), and improving the semantic network to handle the well-known problems which may arise.</abstract>
<note confidence="0.917164363636364">References Allen, J. F. and Perrault, C. R. Analyzing Intention in Utterances. Intelligence December, 1980. Brachman, R. J. • On the Epistemological Status of Semantic Networks. Findler, N. V. (editor), Networks: Representation and Use of Knowledge by 1 in particular. Academic Press, New York, 1979. As for related work, ELI (the language analyzer mentioned in</note>
<abstract confidence="0.975596696969697">section 1) and this system (when fully developed) could theoretically be merged into a single conversation system, with some rules working on mapping English into CD, and others using the CD to decide what responses to generate. In fact, there are situations in which one needs to make use of both kinds of information (such as when a phrase signals a [4] topic shift: &amp;quot;On the other hand...&amp;quot;). One of the possible directions for future work is the incorporation and integration of a rule-based parser into the system, along with some form of rule-based English generation. Another related system, MICS [3], had research goals and a set of knowledge sources somewhat similar to this system&apos;s, but it differed primarily in that it could not alter its goal trees during a conversation, nor did it have explicit data structures for representing topics (the selection of topics was built into the interpreter). Tha main results of this research so far have been the topicutterance graph and dynamic goal trees. Although some way of holding the intersentential information was obviously needed, no precise form was postulated initially. The current structure was invented after working with an earlier set of to discover the most useful form could take. Similarly, the idea that a changing view of someone else&apos;s goals should be used to control the course of the conversation arose during work on producing the interestrating routine. The current system is, of course, by no means a complete model of human discourse. More rules need to be developed, and the current ones need to be refined. In addition to implementing more rules and incorporating a parser, possible areas for future work include replacing the interest-rater with a second agenda (containing interestdetermining rules), changing scripts and testing whether the [6] Carbonell, J. G.</abstract>
<note confidence="0.848038608695652">Subjective Understanding: Computer Models of Belief Systems. PhD thesis, Yale University, January, 1979. Computer Science Research Report #150. Cullingford, R. E. • Script Application: Computer Understanding of Newspaper Stories. PhD thesis, Yale University, January, 1978. Computer Science Research Report #116. The Representation and use of Focus in Dialogue Understanding. Technical Report 151, Stanford Research Institute, July, 1977. Newell, A. and Simon, H. A. Human Problem Solving. Prentice Hail, Englewood Cliffs, N. J., 1972, chapter 8. Riesbeck, C. and Schank, R. C. Comprehension by Computer: Expectation Based Analysis of Sentences in Context. Technical Report 78, Department of Computer Science, Yale University, 1976. Schenk, R. C. Conceptual Information Processing.</note>
<keyword confidence="0.72267975">North-Holland, 1975, chapter 3. Schank, R. C. and Abelson, R. Scripts, Plans, Goals and Understanding. Erlbaum, 1977, chapter 3.</keyword>
<intro confidence="0.713912">87</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J F Allen</author>
<author>C R Perrault</author>
</authors>
<title>Analyzing Intention in Utterances.</title>
<date>1980</date>
<journal>Artificial Intelligence</journal>
<pages>15--3</pages>
<marker>Allen, Perrault, 1980</marker>
<rawString>Allen, J. F. and Perrault, C. R. Analyzing Intention in Utterances. Artificial Intelligence 15(3):143-178, December, 1980.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R J Brachman</author>
</authors>
<title>On the Epistemological Status of Semantic Networks.</title>
<marker>Brachman, </marker>
<rawString>Brachman, R. J. • On the Epistemological Status of Semantic Networks.</rawString>
</citation>
<citation valid="true">
<date>1979</date>
<booktitle>Associative Networks: Representation and Use of Knowledge by Computers, chapter 1 in particular.</booktitle>
<editor>In Findler, N. V. (editor),</editor>
<publisher>Academic Press,</publisher>
<location>New York,</location>
<marker>1979</marker>
<rawString>In Findler, N. V. (editor), Associative Networks: Representation and Use of Knowledge by Computers, chapter 1 in particular. Academic Press, New York, 1979.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>