<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007743">
<title confidence="0.959157">
Optimizing Word Alignment Combination For Phrase Table Training
</title>
<author confidence="0.857608">
Yonggang Deng and Bowen Zhou
</author>
<affiliation confidence="0.588148">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.565045">
Yorktown Heights, NY 10598, USA
</address>
<email confidence="0.983597">
{ydeng,zhou}@us.ibm.com
</email>
<sectionHeader confidence="0.993473" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999092">
Combining word alignments trained in
two translation directions has mostly re-
lied on heuristics that are not directly
motivated by intended applications. We
propose a novel method that performs
combination as an optimization process.
Our algorithm explicitly maximizes the ef-
fectiveness function with greedy search
for phrase table training or synchronized
grammar extraction. Experimental results
show that the proposed method leads to
significantly better translation quality than
existing methods. Analysis suggests that
this simple approach is able to maintain
accuracy while maximizing coverage.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966633333334">
Word alignment is the process of identifying
word-to-word links between parallel sentences. It
is a fundamental and often a necessary step before
linguistic knowledge acquisitions, such as train-
ing a phrase translation table in phrasal machine
translation (MT) system (Koehn et al., 2003), or
extracting hierarchial phrase rules or synchronized
grammars in syntax-based translation framework.
Most word alignment models distinguish trans-
lation direction in deriving word alignment matrix.
Given a parallel sentence, word alignments in two
directions are established first, and then they are
combined as knowledge source for phrase train-
ing or rule extraction. This process is also called
symmetrization. It is a common practice in most
state of the art MT systems. Widely used align-
ment models, such as IBM Model serial (Brown
et al., 1993) and HMM , all assume one-to-many
alignments. Since many-to-many links are com-
monly observed in natural language, symmetriza-
tion is able to make up for this modeling limita-
tion. On the other hand, combining two direc-
tional alignments practically can lead to improved
performance. Symmetrization can also be real-
ized during alignment model training (Liang et al.,
2006; Zens et al., 2004).
Given two sets of word alignments trained in
two translation directions, two extreme combina-
tion are intersection and union. While intersec-
tion achieves high precision with low recall, union
is the opposite. A right balance of these two ex-
treme cases would offer a good coverage with rea-
sonable accuracy. So starting from intersection,
gradually adding elements in the union by heuris-
tics is typically used. Koehn et al. (2003) grow
the set of word links by appending neighboring
points, while Och and Hey (2003) try to avoid both
horizontal and vertical neighbors. These heuristic-
based combination methods are not driven explic-
itly by the intended application of the resulting
output. Ayan (2005) exploits many advanced ma-
chine learning techniques for general word align-
ment combination problem. However, human
annotation is required for supervised training in
those techniques.
We propose a new combination method. Like
heuristics, we aim to find a balance between in-
tersection and union. But unlike heuristics, com-
bination is carried out as an optimization process
driven by an effectiveness function. We evaluate
the impact of each alignment pair w.r.t. the target
application, say phrase table training, and gradu-
ally add or remove the word link that currently
can maximize the predicted benefit measured by
the effectiveness function. More specifically, we
consider the goal of word alignment combination
is for phrase table training, and we directly moti-
vate word alignment combination as a process of
maximizing the number of phrase translations that
can be extracted within a sentence pair.
</bodyText>
<sectionHeader confidence="0.705124" genericHeader="method">
2 Combination As Optimization Process
</sectionHeader>
<bodyText confidence="0.5373995">
Given a parallel sentence (e = ei, f = fi ), a
word link is represented by a pair of indices (i, j),
</bodyText>
<page confidence="0.966911">
229
</page>
<note confidence="0.9267">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 229–232,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.976158843750001">
which means that Foreign word fj is aligned with
English word ei. The direction of word alignments
is ignored. Since the goal of word alignment com-
bination is for phrase table training, we first for-
mally define a phrase translation. Provided with
a set of static word alignments A, a phrase pair
(ei2
i1, fj2
j1 ) is considered translation of each other if
and only if there exists at least one word link be-
tween them and no cross phrase boundary links ex-
ist in A, i.e., for all (i, j) E A, i E [i1, i2] iff j E
[j1, j2]. Notice that by this definition, it does not
matter whether boundary words of the phrase pairs
should be aligned or not. Let PPn(A) denote the
set of phrase pairs that can be extracted with A
where up to n boundary words are allowed to be
not-aligned, i.e., aligned to empty word NULL. As
can be imagined, increasing n would improve re-
call of phrase table but likely to hurt precision. For
word alignment combination, we focus on the set
with high accuracy where n = 0.
Let A1, A2 denote two sets of word alignments
to be combined for the given sentence pair. For
instance, A1 could be word alignments from En-
glish to foreign while A2 the other direction. On
different setup, A1 could be Model-4 alignments,
while A2 is from HMM. In the first combination
method we presented in Algorithm 1, we start with
intersection AI. A, is the candidate link set to be
evaluated and appended to the combined set A. Its
initial value is the difference between union and
intersection. We assume that there is an effective-
ness function g(·) which quantitatively measures
the ‘goodness’ of a alignment set for the intended
application. A higher number indicates a better
alignment set. We use the function g to drive the
process. Each time, we identify the best word link
(ˆi, ˆj) in the candidate set that can maximize the
function g and append it to the current set A. This
process is repeated until the candidate set is empty
or adding any link in the set would lead to degra-
dation. Finally (line 15 to 21), we pickup word
links in the candidate set to align those uncov-
ered words. This is applied to maximize cover-
age, which is similar as the ‘final’ in (Koehn et al.,
2003). Again, we use the function g(·) to rank the
word links in A, and sequentially append them to
A depending on current word coverage.
The algorithm clearly is a greedy search pro-
cedure that maximizes the function g. Since we
plan to take the combined word alignments for
phrase translation training, a natural choice for
g is the number of phrase pairs that can be ex-
tracted with the given alignment set. We choose
g(A) = |PP0(A)|, where we only count phrase
pairs that all boundary words are aligned. The
reason of putting a tight constraint is to maintain
phrase table accuracy while improving the cover-
age. By keeping track of the span of currently
aligned words, we can have efficient implemen-
tation of the function g.
Algorithm 1 Combination of A1 and A2 as an Optimized
Expanding Process
</bodyText>
<listItem confidence="0.955595666666667">
1: AI = A1 n A2, AU = A1 U A2
2: A = AI, Ac = AU − AI
3: total = g(A)
4: while Ac =� 0 do
5: curMax = max(i,j)∈A, g(A U {(i, j)})
6: if curMax ≥ total then
7: (ˆi, ˆj) = argmax(i,j)∈A, g(A U {(i, j)})
8: A = A U {(
9: Ac = Ac − {(
</listItem>
<figure confidence="0.702776538461538">
10: total = curMax
11: else {adding any link will make it worse}
12: break
13: end if
14: end while
15: while Ac =� 0 do
16: (ˆi, ˆj) = argmax(i,j)∈A, g(A U {(i, j)})
17: if eˆi is not aligned or fˆj is not aligned then
18: A = A U {(
19: end if
20: Ac = Ac − {(
21: end while
22: return A
</figure>
<bodyText confidence="0.999758333333333">
Alternatively, the optimization can go in oppo-
site direction. We start with the union A = AU,
and gradually remove the worse word link (ˆi, ˆj) =
argmax(i,j)∈Ac g(A − {(i, j)}) that could max-
imize the effectiveness function. Similarly, this
shrinking process is repeated until either candidate
set is empty or removing any link in the candidate
set would reduce the value of function g.
Other choice of ‘goodness’ function g is pos-
sible. For instance, one could consider syntactic
constraints, or weight phrase pairs differently ac-
cording to their global co-occurrence. The basic
idea is to implement the combination as an itera-
tive customized optimization process that is driven
by the application.
</bodyText>
<sectionHeader confidence="0.998626" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.99798125">
We test the proposed new idea on Persian Farsi to
English translation. The task is to translate spoken
Farsi into English. We decode reference transcrip-
tion so recognition is not an issue. The training
</bodyText>
<figure confidence="0.9490355">
ˆ
i,
ˆj)}
ˆ
i,
ˆj)}
ˆ
i,
ˆj)}
ˆ
i,
ˆj)}
</figure>
<page confidence="0.989475">
230
</page>
<bodyText confidence="0.99995824">
data was provided by the DARPA TransTac pro-
gram. It consists of around 110K sentence pairs
with 850K English words in the military force
protection domain. We train IBM Model-4 using
GIZA++ toolkit (Och and Ney, 2003) in two trans-
lation directions and perform different word align-
ment combination. The resulting alignment set is
used to train a phrase translation table, where Farsi
phrases are limited to up to 6 words.
The quality of resulting phrase translation table
is measured by translation results. Our decoder
is a phrase-based multi-stack implementation of
the log-linear model similar to Pharaoh (Koehn et
al., 2003). Like other log-linear model based de-
coders, active features in our translation engine in-
clude translation models in two directions, lexicon
weights in two directions, language model, lexi-
calized reordering models, sentence length penalty
and other heuristics. These feature weights are
tuned on the dev set to achieve optimal transla-
tion performance evaluated by automatic metric.
The language model is a statistical 4-gram model
estimated with Modified Kneser-Ney smoothing
(Chen and Goodman, 1996) using only English
sentences in the parallel training data.
</bodyText>
<subsectionHeader confidence="0.997957">
3.1 Phrase Table Comparison
</subsectionHeader>
<bodyText confidence="0.999929960784314">
We first study the impact of different word align-
ment combination methods on phrase translation
table, and compare our approaches to heuristic
based methods. The same English to Farsi and
Farsi to English Model-4 word alignments are
used, but we try different combination methods
and analysis the final alignment set and the result-
ing phase translation table. Table 1 presents some
statistics. Each row corresponds to a particular
combination. The first two are intersection (I) and
union (U). The next two methods are heuristic (H)
in (Och and Ney, 2003) and grow-diagonal (GD)
proposed in (Koehn et al., 2003). Our proposed
methods are presented in the following two rows:
one is optimization as an expanding process (OE),
the other is optimization as an shrinking process
(OS). In the last four rows, we add ‘final’ opera-
tion (line 15 to 21 in Algorithm 1).
For each method, we calculate the output align-
ment set size as a percentage of the union (the
2nd column) and resulting phrase table (PP,,,(A))
size (in thousand) with different constrain on the
maximum number of unaligned boundary words
n = 0, 1, 2 (the next 3 columns). As we can
see, the intersection has less than half of all word
links in the pool. This implies the underlying word
alignment quality leaves much room for improve-
ments, mainly due to data sparseness. Not sur-
prisingly, when relaxing unaligned boundary word
number from 0 to 2, the phrase table size increases
more than 7 times. This is the result of very low
recall of word alignments, consequently the esti-
mated phrase table PP2(A) has very low accu-
racy. Union suffers from the opposite problem:
many incorrect word links prevent good phrase
pairs from being extracted.
The two heuristic methods and our proposed
optimization approaches achieve somewhat a bal-
ance between I and U. By comparing size of
PP0(A) (3rd column), optimization methods are
able to identify much more phrase pairs with sim-
ilar size of alignment set. This confirms that the
new method is indeed moving to the desired di-
rection of extracting as many accurate (all bound-
ary words should be aligned) phrase pairs as pos-
sible. We still notice that ratio of |PP2(A) |and
|PP0(A) |(the last column) is high. We suspect
that the ratio of this two phrase table size might
somewhat be indicative of the phrase table accu-
racy, which is hard to estimate without manual an-
notation though.
</bodyText>
<table confidence="0.998415083333334">
Method |A ||PP0 ||PP1 ||PP2 ||P P2|
|AU  ||PP0|
I 45% 424 2047 3658 8.63
U 100% 354 555 578 1.63
H 78% 538 1225 1519 2.82
GD 82% 499 1081 1484 2.97
OS 84% 592 1110 1210 2.04
OE 78% 659 1359 1615 2.45
HF 95% 427 670 697 1.63
GDF 97% 412 647 673 1.63
OSF 89% 484 752 781 1.61
OEF 89% 476 739 768 1.61
</table>
<tableCaption confidence="0.877160333333333">
Table 1: Statistics of word alignment set and the
resulting phrase table size (number of entries in
thousand (K)) with different combination methods
</tableCaption>
<subsectionHeader confidence="0.999731">
3.2 Translation Results
</subsectionHeader>
<bodyText confidence="0.999902375">
The ultimate goal of word alignment combination
is for building translation system. The quality of
resulting phrase tables is measured by automatic
translation metric. We have one dev set (1430 sen-
tences with 11483 running words), test set 1 (1390
sentences with 10334 running words) and test set
2 (417 sentences with 4239 running words). The
dev set and test set 1 are part of all available Farsi-
</bodyText>
<page confidence="0.993586">
231
</page>
<bodyText confidence="0.99965302631579">
English parallel corpus. They are holdout from
training data as tuning and testing. The test set 2
is the standard NIST offline evaluation set, where
4 references are available for each sentence. The
dev and test set 1 are much closer to the training
set than the standard test set 2. We tune all fea-
ture weights automatically (Och, 2003) to maxi-
mize the BLEU (Papineni et al., 2002) score on
the dev set.
Table 2 shows BLEU score of different com-
bination methods on all three sets. Union per-
forms much worse on the dev and test1 than inter-
section, while intersection achieved the same per-
formance on test2 as union but with more than 6
times of phrase table size. Grow-diagonal (GD)
has more than 1 bleu point on test2 than intersec-
tion but with less than half of phrase table size.
The proposed new method OE is consistently bet-
ter than both heuristic methods GD and H, with
more than 1 point on dev/teset1 and 0.7 point on
test2. Comparing the last group to the middle one,
we can see the effect of the ‘final’ operation on
all four methods. Tabel 1 shows that after apply-
ing the final operation, phrase table size is cut into
half. When evaluated with automatic translation
metric, all four methods generally perform much
worse on dev and test1 that are close to training
data, but better on NIST standard test2. We ob-
serve half BLEU point improvement for optimiza-
tion method but marginal gain for heuristic-based
approaches. This suggest that the phrase table ac-
curacy get improved with the final operation. Op-
timization method directly tries to maximize the
number of phrase pairs that can be extracted. We
observe that it (OEF) is able to find more than
14% more phrase pairs than heuristic methods and
achieve 1 BLEU point gain than the best heuristic
method (GDF).
</bodyText>
<table confidence="0.999581">
Method dev test1 test2
I 0.396 0.308 0.348
U 0.341 0.294 0.348
H 0.400 0.314 0.341
GD 0.391 0.314 0.360
OS 0.383 0.316 0.356
OE 0.410 0.329 0.367
HF 0.361 0.297 0.343
GDF 0.361 0.301 0.362
OSF 0.372 0.305 0.361
OEF 0.370 0.306 0.372
</table>
<tableCaption confidence="0.896636">
Table 2: Translation results (BLEU score) with
phrase tables trained with different word align-
ment combination methods
</tableCaption>
<sectionHeader confidence="0.99841" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999991708333333">
We presented a simple yet effective method for
word alignment symmetrization and combination
in general. The problem is formulated as an opti-
mization with greedy search driven by an effec-
tiveness function, which can be customized di-
rectly to maximum benefit for intended applica-
tions such as phrase table training or synchronized
grammar extraction in machine translation. Ex-
perimental results demonstrated consistent better
BLEU scores than the best heuristic method. The
optimization process can better maintain accuracy
while improving coverage.
The algorithm is generic and leaves much space
for variations. For instance, designing a better ef-
fectiveness function g, or considering a soft link
with some probability rather than binary 0/1 con-
nection would potentially be opportunities for fur-
ther improvement. On the other hand, the search
space of current algorithm is limited by the pool
of candidate set, it is possible to suggest new links
while driven by the target function.
Acknowledgments We thank the DARPA
TransTac program for funding and the anonymous
reviewers for their constructive suggestions.
</bodyText>
<sectionHeader confidence="0.999114" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999899666666667">
N. F. Ayan. 2005. Combining Linguistic and Machine Learn-
ing Techniques for Word Alignment Improvement. Ph.D.
thesis, University of Maryland, College Park, November.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.
1993. The mathematics of machine translation: Parameter
estimation. Computational Linguistics, 19:263–312.
S. F. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In Proc. of
ACL, pages 310–318.
P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT-NAACL, pages 48–54.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by agree-
ment. In Proc. of HLT-NAACL, pages 104–111.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational Lin-
guistics, 29(1):19–51.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL, pages 160–167.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu:
a method for automatic evaluation of machine translation.
In Proc. of ACL, pages 311–318.
R. Zens, E. Matusov, and H. Ney. 2004. Improved word
alignment using a symmetric lexicon model. In Proc. of
COLING, pages 36–42.
</reference>
<page confidence="0.99518">
232
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.984489">
<title confidence="0.999964">Optimizing Word Alignment Combination For Phrase Table Training</title>
<author confidence="0.997164">Deng Zhou</author>
<affiliation confidence="0.999981">IBM T.J. Watson Research Center</affiliation>
<address confidence="0.997252">Yorktown Heights, NY 10598, USA</address>
<abstract confidence="0.9993718125">Combining word alignments trained in two translation directions has mostly relied on heuristics that are not directly motivated by intended applications. We propose a novel method that performs combination as an optimization process. Our algorithm explicitly maximizes the effectiveness function with greedy search for phrase table training or synchronized grammar extraction. Experimental results show that the proposed method leads to significantly better translation quality than existing methods. Analysis suggests that this simple approach is able to maintain accuracy while maximizing coverage.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N F Ayan</author>
</authors>
<title>Combining Linguistic and Machine Learning Techniques for Word Alignment Improvement.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Maryland, College Park,</institution>
<contexts>
<context position="2732" citStr="Ayan (2005)" startWordPosition="411" endWordPosition="412">ombination are intersection and union. While intersection achieves high precision with low recall, union is the opposite. A right balance of these two extreme cases would offer a good coverage with reasonable accuracy. So starting from intersection, gradually adding elements in the union by heuristics is typically used. Koehn et al. (2003) grow the set of word links by appending neighboring points, while Och and Hey (2003) try to avoid both horizontal and vertical neighbors. These heuristicbased combination methods are not driven explicitly by the intended application of the resulting output. Ayan (2005) exploits many advanced machine learning techniques for general word alignment combination problem. However, human annotation is required for supervised training in those techniques. We propose a new combination method. Like heuristics, we aim to find a balance between intersection and union. But unlike heuristics, combination is carried out as an optimization process driven by an effectiveness function. We evaluate the impact of each alignment pair w.r.t. the target application, say phrase table training, and gradually add or remove the word link that currently can maximize the predicted bene</context>
</contexts>
<marker>Ayan, 2005</marker>
<rawString>N. F. Ayan. 2005. Combining Linguistic and Machine Learning Techniques for Word Alignment Improvement. Ph.D. thesis, University of Maryland, College Park, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>R Mercer</author>
</authors>
<title>The mathematics of machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--263</pages>
<contexts>
<context position="1646" citStr="Brown et al., 1993" startWordPosition="234" endWordPosition="237">ble in phrasal machine translation (MT) system (Koehn et al., 2003), or extracting hierarchial phrase rules or synchronized grammars in syntax-based translation framework. Most word alignment models distinguish translation direction in deriving word alignment matrix. Given a parallel sentence, word alignments in two directions are established first, and then they are combined as knowledge source for phrase training or rule extraction. This process is also called symmetrization. It is a common practice in most state of the art MT systems. Widely used alignment models, such as IBM Model serial (Brown et al., 1993) and HMM , all assume one-to-many alignments. Since many-to-many links are commonly observed in natural language, symmetrization is able to make up for this modeling limitation. On the other hand, combining two directional alignments practically can lead to improved performance. Symmetrization can also be realized during alignment model training (Liang et al., 2006; Zens et al., 2004). Given two sets of word alignments trained in two translation directions, two extreme combination are intersection and union. While intersection achieves high precision with low recall, union is the opposite. A r</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer. 1993. The mathematics of machine translation: Parameter estimation. Computational Linguistics, 19:263–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>310--318</pages>
<contexts>
<context position="9502" citStr="Chen and Goodman, 1996" startWordPosition="1611" endWordPosition="1614">. Our decoder is a phrase-based multi-stack implementation of the log-linear model similar to Pharaoh (Koehn et al., 2003). Like other log-linear model based decoders, active features in our translation engine include translation models in two directions, lexicon weights in two directions, language model, lexicalized reordering models, sentence length penalty and other heuristics. These feature weights are tuned on the dev set to achieve optimal translation performance evaluated by automatic metric. The language model is a statistical 4-gram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using only English sentences in the parallel training data. 3.1 Phrase Table Comparison We first study the impact of different word alignment combination methods on phrase translation table, and compare our approaches to heuristic based methods. The same English to Farsi and Farsi to English Model-4 word alignments are used, but we try different combination methods and analysis the final alignment set and the resulting phase translation table. Table 1 presents some statistics. Each row corresponds to a particular combination. The first two are intersection (I) and union (U). The next two meth</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>S. F. Chen and J. Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proc. of ACL, pages 310–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrasebased translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="1094" citStr="Koehn et al., 2003" startWordPosition="149" endWordPosition="152">s function with greedy search for phrase table training or synchronized grammar extraction. Experimental results show that the proposed method leads to significantly better translation quality than existing methods. Analysis suggests that this simple approach is able to maintain accuracy while maximizing coverage. 1 Introduction Word alignment is the process of identifying word-to-word links between parallel sentences. It is a fundamental and often a necessary step before linguistic knowledge acquisitions, such as training a phrase translation table in phrasal machine translation (MT) system (Koehn et al., 2003), or extracting hierarchial phrase rules or synchronized grammars in syntax-based translation framework. Most word alignment models distinguish translation direction in deriving word alignment matrix. Given a parallel sentence, word alignments in two directions are established first, and then they are combined as knowledge source for phrase training or rule extraction. This process is also called symmetrization. It is a common practice in most state of the art MT systems. Widely used alignment models, such as IBM Model serial (Brown et al., 1993) and HMM , all assume one-to-many alignments. Si</context>
<context position="2462" citStr="Koehn et al. (2003)" startWordPosition="366" endWordPosition="369">combining two directional alignments practically can lead to improved performance. Symmetrization can also be realized during alignment model training (Liang et al., 2006; Zens et al., 2004). Given two sets of word alignments trained in two translation directions, two extreme combination are intersection and union. While intersection achieves high precision with low recall, union is the opposite. A right balance of these two extreme cases would offer a good coverage with reasonable accuracy. So starting from intersection, gradually adding elements in the union by heuristics is typically used. Koehn et al. (2003) grow the set of word links by appending neighboring points, while Och and Hey (2003) try to avoid both horizontal and vertical neighbors. These heuristicbased combination methods are not driven explicitly by the intended application of the resulting output. Ayan (2005) exploits many advanced machine learning techniques for general word alignment combination problem. However, human annotation is required for supervised training in those techniques. We propose a new combination method. Like heuristics, we aim to find a balance between intersection and union. But unlike heuristics, combination i</context>
<context position="6091" citStr="Koehn et al., 2003" startWordPosition="1000" endWordPosition="1003">ely measures the ‘goodness’ of a alignment set for the intended application. A higher number indicates a better alignment set. We use the function g to drive the process. Each time, we identify the best word link (ˆi, ˆj) in the candidate set that can maximize the function g and append it to the current set A. This process is repeated until the candidate set is empty or adding any link in the set would lead to degradation. Finally (line 15 to 21), we pickup word links in the candidate set to align those uncovered words. This is applied to maximize coverage, which is similar as the ‘final’ in (Koehn et al., 2003). Again, we use the function g(·) to rank the word links in A, and sequentially append them to A depending on current word coverage. The algorithm clearly is a greedy search procedure that maximizes the function g. Since we plan to take the combined word alignments for phrase translation training, a natural choice for g is the number of phrase pairs that can be extracted with the given alignment set. We choose g(A) = |PP0(A)|, where we only count phrase pairs that all boundary words are aligned. The reason of putting a tight constraint is to maintain phrase table accuracy while improving the c</context>
<context position="9001" citStr="Koehn et al., 2003" startWordPosition="1538" endWordPosition="1541"> provided by the DARPA TransTac program. It consists of around 110K sentence pairs with 850K English words in the military force protection domain. We train IBM Model-4 using GIZA++ toolkit (Och and Ney, 2003) in two translation directions and perform different word alignment combination. The resulting alignment set is used to train a phrase translation table, where Farsi phrases are limited to up to 6 words. The quality of resulting phrase translation table is measured by translation results. Our decoder is a phrase-based multi-stack implementation of the log-linear model similar to Pharaoh (Koehn et al., 2003). Like other log-linear model based decoders, active features in our translation engine include translation models in two directions, lexicon weights in two directions, language model, lexicalized reordering models, sentence length penalty and other heuristics. These feature weights are tuned on the dev set to achieve optimal translation performance evaluated by automatic metric. The language model is a statistical 4-gram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using only English sentences in the parallel training data. 3.1 Phrase Table Comparison We first s</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrasebased translation. In Proc. of HLT-NAACL, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>B Taskar</author>
<author>D Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="2013" citStr="Liang et al., 2006" startWordPosition="292" endWordPosition="295">e combined as knowledge source for phrase training or rule extraction. This process is also called symmetrization. It is a common practice in most state of the art MT systems. Widely used alignment models, such as IBM Model serial (Brown et al., 1993) and HMM , all assume one-to-many alignments. Since many-to-many links are commonly observed in natural language, symmetrization is able to make up for this modeling limitation. On the other hand, combining two directional alignments practically can lead to improved performance. Symmetrization can also be realized during alignment model training (Liang et al., 2006; Zens et al., 2004). Given two sets of word alignments trained in two translation directions, two extreme combination are intersection and union. While intersection achieves high precision with low recall, union is the opposite. A right balance of these two extreme cases would offer a good coverage with reasonable accuracy. So starting from intersection, gradually adding elements in the union by heuristics is typically used. Koehn et al. (2003) grow the set of word links by appending neighboring points, while Och and Hey (2003) try to avoid both horizontal and vertical neighbors. These heuris</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>P. Liang, B. Taskar, and D. Klein. 2006. Alignment by agreement. In Proc. of HLT-NAACL, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="8591" citStr="Och and Ney, 2003" startWordPosition="1474" endWordPosition="1477">. The basic idea is to implement the combination as an iterative customized optimization process that is driven by the application. 3 Experimental Results We test the proposed new idea on Persian Farsi to English translation. The task is to translate spoken Farsi into English. We decode reference transcription so recognition is not an issue. The training ˆ i, ˆj)} ˆ i, ˆj)} ˆ i, ˆj)} ˆ i, ˆj)} 230 data was provided by the DARPA TransTac program. It consists of around 110K sentence pairs with 850K English words in the military force protection domain. We train IBM Model-4 using GIZA++ toolkit (Och and Ney, 2003) in two translation directions and perform different word alignment combination. The resulting alignment set is used to train a phrase translation table, where Farsi phrases are limited to up to 6 words. The quality of resulting phrase translation table is measured by translation results. Our decoder is a phrase-based multi-stack implementation of the log-linear model similar to Pharaoh (Koehn et al., 2003). Like other log-linear model based decoders, active features in our translation engine include translation models in two directions, lexicon weights in two directions, language model, lexic</context>
<context position="10146" citStr="Och and Ney, 2003" startWordPosition="1714" endWordPosition="1717">ces in the parallel training data. 3.1 Phrase Table Comparison We first study the impact of different word alignment combination methods on phrase translation table, and compare our approaches to heuristic based methods. The same English to Farsi and Farsi to English Model-4 word alignments are used, but we try different combination methods and analysis the final alignment set and the resulting phase translation table. Table 1 presents some statistics. Each row corresponds to a particular combination. The first two are intersection (I) and union (U). The next two methods are heuristic (H) in (Och and Ney, 2003) and grow-diagonal (GD) proposed in (Koehn et al., 2003). Our proposed methods are presented in the following two rows: one is optimization as an expanding process (OE), the other is optimization as an shrinking process (OS). In the last four rows, we add ‘final’ operation (line 15 to 21 in Algorithm 1). For each method, we calculate the output alignment set size as a percentage of the union (the 2nd column) and resulting phrase table (PP,,,(A)) size (in thousand) with different constrain on the maximum number of unaligned boundary words n = 0, 1, 2 (the next 3 columns). As we can see, the int</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="13184" citStr="Och, 2003" startWordPosition="2256" endWordPosition="2257">ured by automatic translation metric. We have one dev set (1430 sentences with 11483 running words), test set 1 (1390 sentences with 10334 running words) and test set 2 (417 sentences with 4239 running words). The dev set and test set 1 are part of all available Farsi231 English parallel corpus. They are holdout from training data as tuning and testing. The test set 2 is the standard NIST offline evaluation set, where 4 references are available for each sentence. The dev and test set 1 are much closer to the training set than the standard test set 2. We tune all feature weights automatically (Och, 2003) to maximize the BLEU (Papineni et al., 2002) score on the dev set. Table 2 shows BLEU score of different combination methods on all three sets. Union performs much worse on the dev and test1 than intersection, while intersection achieved the same performance on test2 as union but with more than 6 times of phrase table size. Grow-diagonal (GD) has more than 1 bleu point on test2 than intersection but with less than half of phrase table size. The proposed new method OE is consistently better than both heuristic methods GD and H, with more than 1 point on dev/teset1 and 0.7 point on test2. Compa</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="13229" citStr="Papineni et al., 2002" startWordPosition="2263" endWordPosition="2266">ric. We have one dev set (1430 sentences with 11483 running words), test set 1 (1390 sentences with 10334 running words) and test set 2 (417 sentences with 4239 running words). The dev set and test set 1 are part of all available Farsi231 English parallel corpus. They are holdout from training data as tuning and testing. The test set 2 is the standard NIST offline evaluation set, where 4 references are available for each sentence. The dev and test set 1 are much closer to the training set than the standard test set 2. We tune all feature weights automatically (Och, 2003) to maximize the BLEU (Papineni et al., 2002) score on the dev set. Table 2 shows BLEU score of different combination methods on all three sets. Union performs much worse on the dev and test1 than intersection, while intersection achieved the same performance on test2 as union but with more than 6 times of phrase table size. Grow-diagonal (GD) has more than 1 bleu point on test2 than intersection but with less than half of phrase table size. The proposed new method OE is consistently better than both heuristic methods GD and H, with more than 1 point on dev/teset1 and 0.7 point on test2. Comparing the last group to the middle one, we can</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. of ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>E Matusov</author>
<author>H Ney</author>
</authors>
<title>Improved word alignment using a symmetric lexicon model.</title>
<date>2004</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>36--42</pages>
<contexts>
<context position="2033" citStr="Zens et al., 2004" startWordPosition="296" endWordPosition="299">dge source for phrase training or rule extraction. This process is also called symmetrization. It is a common practice in most state of the art MT systems. Widely used alignment models, such as IBM Model serial (Brown et al., 1993) and HMM , all assume one-to-many alignments. Since many-to-many links are commonly observed in natural language, symmetrization is able to make up for this modeling limitation. On the other hand, combining two directional alignments practically can lead to improved performance. Symmetrization can also be realized during alignment model training (Liang et al., 2006; Zens et al., 2004). Given two sets of word alignments trained in two translation directions, two extreme combination are intersection and union. While intersection achieves high precision with low recall, union is the opposite. A right balance of these two extreme cases would offer a good coverage with reasonable accuracy. So starting from intersection, gradually adding elements in the union by heuristics is typically used. Koehn et al. (2003) grow the set of word links by appending neighboring points, while Och and Hey (2003) try to avoid both horizontal and vertical neighbors. These heuristicbased combination</context>
</contexts>
<marker>Zens, Matusov, Ney, 2004</marker>
<rawString>R. Zens, E. Matusov, and H. Ney. 2004. Improved word alignment using a symmetric lexicon model. In Proc. of COLING, pages 36–42.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>