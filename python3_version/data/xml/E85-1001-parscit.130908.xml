<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.476494">
NATURAL LANGUAGES AND TIE CROSSLY 111HRARCET
-Andras Kornai
Institute of Linguistics
Hungarian Academy of Sciences
Budapest, P.C.B. 19. 11-1250 Hungary
</title>
<listItem confidence="0.19792">
•
</listItem>
<sectionHeader confidence="0.938547" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.98167312">
The central claim of the paper is that NI
stringsets are regular. Three independent
arguments are offered in favor of this
position: one based on parsimony
considerations, one employing the
McCullogh-Pitts (1943) model of seisms,
and a purely linguistic one. It is
possible to derive explicit upper bounds
for the number of (live) states in
acceptors: the results show that finite
state NL parsers can be implemented on
present-day computers. The position at NI
stringsets within the regular family is
also investigated: it is proved that NLa
are counter-free, but not locally
testable.
Q. jOroduction
The question whether the grammatical
sentences of natural languages form
regular (Type 3), context free (Type 2),
context sensitive (Type I), or recursively
enumerable (7ype 0) nets has been subject
to much discussion ever since it was posed
by Chomsky in his seminal 1956 paper.
However, there seems to be little
agreement among the linguists concerned
with the &apos;geographic&apos; position of natural
languages (NLe): for instance Reich (1868)
claims NLE, to be finite- state (Type 2),
while Matthews (19/9) argues that they are
not even recursively enumerable.
Pullum and Gazdar (1982) have
demonstrated that the standard linsuigIls
arguments against the context- freeness of
natural languages are fallacious they
did not consider, the
asialiuguistic argument offered by
Matthews. In Section 1 of this paper I
will briefly outline and challenge this
argument, and in Section 2 I will argue In
favor of Reich&apos;s position. The claim that
NI.. are Type 3 ham several implications
for linguistic (meta)theory: these will be
discussed in Section 3.
The paper presupposes 10ffc
familiarity with the basic notions and
notations of format language theory: when
no specific reference is given, the reader
will find ei proof both in Salome. (1973)
and Harrison (1976)
</bodyText>
<sectionHeader confidence="0.835698" genericHeader="categories and subject descriptors">
10 BALULAk IAMMMAAAJA RA 12.1211 lansualie*
</sectionHeader>
<bodyText confidence="0.990322195121951">
Ihe-extagiaLgaal vies of natural languages,
i.e. the identification of NLe with the
set- of their grammatical strings
(sentencee) is sometimes regarded an idea
characteristic of generative linguistics.
Since it was Chossky (19E1:1E) who first
made this view explicit, this is not
vitally unjust: yet it is quite clear that
the same idea was /implicit in such of the
work of the structuralist period.&apos; In
fact, the &apos;discovery procedures&apos; developed
by the structuralists in order to arrive
at • colncise description (g ) cf
NI from a met of utterances (corpus) were
without exception based on the assumption
that native speakers of the language are
capable of judging the g ticality of
utterances presented to them. Although
these procedures are, by and large,
practical (empirical) and sechanical
(algorithmic), their presentation already
involved 46, certain t of idealization.
For instance, it is obvious that
native speakers themselves utter
ungrammatical sentences from time to time,
and it is also clear that they can
understand (parse) sentences that are not
strictly &apos;grammatical&apos;. Nevertheless,
these methods work quite well in the
actual practice of NI description, and the
structuralist methodology has often been
compared to that of chemistry, physics,
and other natural sciences.2
Matthews (1979) has canted doubts an
the fundamental assumption of these
procedures: be claims that native speakers
are in tact unatli to judge the
grammaticality of material presented to
them. The relevant part of his
argumentation is reproduced below:
-------
</bodyText>
<reference confidence="0.692897">
1) See e.g. Def 4 in Eloomfield 1926, or
Farris 1))46 eh 1.0
2) See e.g.-Carroll 1953, Levi-Strauss
1958 ch. 2
</reference>
<note confidence="0.4183665">
&amp;quot;Consider (1) and (2). if native speakers
instantiate
</note>
<tableCaption confidence="0.7181894">
(1) the canoe floated down the river
sank
(2) the editor authors the newspaper
hired liked laughed
an effective procedure in their
classification of sentences, then
presumably the classIficaton of (1) and
(2) should not depend on their position in
a list of test &apos;sentences that also
includes sentences similar to (3) and (4).
</tableCaption>
<listItem confidence="0.52265225">
(3) the man (that was) thrown down the
stairs died
(4) the editor (whom) the authors the
newspaper hired liked laughed
</listItem>
<bodyText confidence="0.765114114285714">
but in fact it does. (1) and (2) will
typically be classified as ungrammatical
if they precede sentences similar to (2)
and (4), but grasmatical if they follow
thee. Such cases are quite common.&amp;quot; (p
212)
Moreover, &amp;quot;there is considerable
empirical evidence to suggest that native
speakers employ a battery of heuristic
strategies when parsing and classifying
sentences. Their reliance on , such
strategies does not preclude their having
available to them an effective procedure
for deciding sembershis in their language;
however, in the &amp;become* of empirical
evidence for such a procedure, we are
certainly not required to postulate its
existence.&amp;quot; (p 213)
From this, Matthews concludes that
Putnam (1961) was not justified in
appealing to Church&apos;s thesis in order to
show that NLs are recursive: for 12 native
speakers have no effective procedure for
deciding membership in the set of
grammatical sentences, then there is co
guarantee that such procedure exists. But
is it really the cause that the &amp;quot;battery of.
heuristic procedures&amp;quot; employed by native
speakers falls outside the scope of
Church&apos;s thesis? Nell- confirmed natural
lawe3 are usually taken to be universally
valid -- it. is unclear why should Church&apos;s
thesis be an exception to this, and
Matthews offers no evidence to corroborate
his views on this point.
</bodyText>
<subsectionHeader confidence="0.7785065">
Putnam&apos;s original argument derives
its strength from Church&apos;s thesis. If NIA
</subsectionHeader>
<bodyText confidence="0.974279666666666">
are Type 0, then the heuristic
strategies of native speakers will be
instances of precisely that sort of
procedures that Church&apos;s thesis predicts
zol to exist: on the one hand, they are
&apos;intuitively effective&apos;, and on the otter
hand, they are not Turing computable.
The phenommon observed by Matthews,
namely that native speakers can be coaxed
into accepting (or rejecting) sentences
has little to do with the recursiveness cf
the battery et heuristics they eeplcy:
rather, it calls the extensional view of
language in question. The problem is a
sethodological one: if NLa are defined to
be sets of ago/meatiest sentences, how can
one test potential elesecte for
senbership? This problem becomes
particularly acute in borderline cases
(such as (1-4) above), and for the
linguist who wants to check the
predictions oA his grammar it matters but
little that such dubious sentences are
(statistically) infrequent.
The easiest way to act-we this
problem is to give up the aseuaction that
srassatic&amp;lity is a yes/no question:
</bodyText>
<tableCaption confidence="0.771157">
*degrees of g ticalreas* can be
introduced (see e.g. Chossky 1961) and NLs
can be treated as graded (or even fuzzy)
sets. This approach, tewever, can only be
applied in the study cf idiolects
(languages of individual speakers),
because there Is no way to arrive at a
graded set that will reflect the sue cf
indite/dusl opinions in a reasonably
faithful manner.
Suppose, for instance, that we have
</tableCaption>
<construct confidence="0.8102856">
three speakers, 2, 1, and Zip and each of
the. classifies the sentences al h, ard c
consistently (that is, if he prefers a to
b and b to c, Ahem he prefers a to c,
etc.). Now, if for speaker X a&gt;h&gt;c, for Y
</construct>
<bodyText confidence="0.94674875">
and for Z- c&gt;e)t, then the &apos;majority
;refers a to b, b to c, and c to a; in
ether words, the &apos;general opinion&apos; is
inconsistent (non- transitive). The
&amp;quot;possibility theorem&amp;quot; of Arrow (1950)
sakes it clear that the example is
typical: under very general cenditions,
there is sinply no way to aggregate graded
sets in such &amp; manner that the (partial)
orderings imposed by the individual
gradations are pre d. Therefore, the
&apos;degrees of gransaticalnesel approach rust
be relegated to the- study of idiolecte in
any case a. most linguists, however,
reject it entirely Newmeyer 1980 ch.
5.5.2, 5.7.1).
</bodyText>
<subsectionHeader confidence="0.959445">
Yes/no grammaticality Judgments, on
</subsectionHeader>
<bodyText confidence="0.989723692307692">
the- other hand..shew kably little
variation from speaker to speaker in any
given speech cossunity, and it is this
Intrasukjective legtntilitv (cf. /tkcnen
1581) that justifies the escirical study
cf Idle/acts* and-even &apos;languages&apos;. But if
Watthews is right, and native speakers are
unable to &amp;quot;classify any sentence over the
vocabulary of their language consistently
as -either grammatical-or ung tient&amp;quot; (P
211), then intrasubjective testability
31 For independent sotivaton of Churchle
thesis, see e.g. lagers 1967: ch. 1.7
</bodyText>
<page confidence="0.963566">
2
</page>
<bodyText confidence="0.988459863636364">
will be inpossible to achieve. The
question is: what makes the native speaker
inconsistent? In Matthews&apos; example, there
can be little doubt that the cause of the
inconsistency is the teet, aituallon: the
speaker&apos;s linguistic intuition is not the
same before and after reading sentescos
(3-4).
This source of inconsistency can be
eliminated fairly easily: if the sentences
are presented in a random manner
(preferably, with &amp;quot;fitter&amp;quot; sentences among
them), then no &amp;quot;cues provided by the
context of classification&amp;quot; (p 213) viii be
present. Naturally, linguistically
relevant experiments will have to control
many other factors (see e.g. Greenbaum and
Quirk 1970), but as we shall see, there is
no need to discuss these individually.
Fro, the point of intrasubjective
testability, it can be safely said that
well-designed experiments usually provide
highly consistent dates (even in the case
of borderline sentences), and the
extensional view of Nis can be maintained
on an empirical basis as well. The actual
sets designated as NI. will, at least to a
certain extent, depend on the choice of
experimental technique, but any Masa
experimental method can be thought of as
an algorithm for deciding questions of
membership witg Its Lig sl s Massa
genele.!
Since the existing experimentel
methods can be replaced by (interactive)
computer- programs, the question boils
down to this: is a Turing machine with a
human oracle more powerful than one with a
Turing machine oracle? By Church&apos;s thesiet
the answer Is negative, and as Turing
machines with recursive oracles are no
more powerful than Turing machines without
oracle (see e.g. Rogers 1967 ch. 9.4). Ms
must be recursive.
Notice, that this line of reasoning
is independent of the particular choice of
experimental technique, or what is the
same, of the precise definition of Rte.
This is a consequence of the fact that the
experimental methods used in empirical
sciences (including linguistics) hardly
merit this Esse unless they are well-
defined and &apos;mechanical&apos; to such an extent
that their algorithmazation poses no real
problems. (For instance, the procedure
outlined above does not make crucial
reference to random sequences: the
Irsndomizationl of test- sentences can be
carried out with the aid of geenooreodoe
sequences generated by Turing machine.)
This is not to say that introspective
evidence or intuition plays no role in
linguistics (or in general, in the
development of science) -- but questions
4) For the definition of oracles, see e.g.
Rogers 1967 ch. 9
concerning the position of natural
languages in the Chossky hierarchy can
hardly be meaningful unless we have some
definition of His (1.0. some experimental
sethod to test membership) tc work with.
2. Um caaulaslix al Zatural IGAIAMARA2
Finite &apos; state NI models were first
developed by Rockett (199E). Although
Cimmiskr- (1957 -ch 3.1) attempted to
demonstrate the inadequacy of such models,
several lingulsts5 advocated their use,
and the stretificational school Cf
linguistics (Lab 1966) persists in
employing a- formalism which Is, in
essence, equivalent to finite automata
(cf. Table 1 of Borgida 1983).
As Reich (1969) has pointed out,
Chomsky&apos;s demonstration is based on the
assumption that His are self-embedding Is
IS grialleary degzeg. This means, that the
sentences (1-2) and&apos; (5-6) suet be equally
grammatical: •
</bodyText>
<table confidence="0.9338875">
the boss editor authors the
newspaper hired liked hates
laughed
the cons/Vte boss editor authors
the newspaper hired liked hates
chairs agreed.,
</table>
<bodyText confidence="0.9054728">
The experiments (Miller and Isard 1964,
Marks 1968). h , do not support this
conclusion: native speakers of . English
react to (5-6) and (7-8) the same way6
the boss editor authors the
</bodyText>
<listItem confidence="0.692928">
• newspaper hired liked hates
.1aughed cursed
</listItem>
<bodyText confidence="0.99112175">
the secretary commitie boss editor
-authors the newspaper hired liked
hates chairsagreed
Since (7-8) are ungresaetical im any
grammar of English.- Chossky&apos;s original
demonstration is tar from convincing, and
the question whether Nis are Type 2 is
still open.
In fact, the only way to show that
MIA are not Lkalis is to exhibit some
infinite sequence of graematical
sentences: fortunately, the pattern
</bodyText>
<listItem confidence="0.9162131">
exespiilied in (1-6) is not necessary for
this. Coordinated constructions as in
9) Especially the ones working with
computers. See e.g. Marcus 1964, Church
19S0
63 Chomsky (1962) regards (5-6)
grammatical. (-but unacceptable) and (7-8)
ungrammatical: for the aethodalogical
isplications of this position see Greene
(19/2).
</listItem>
<page confidence="0.993391">
3
</page>
<bodyText confidence="0.987374595419848">
/ have seen Tom
T have seen Tom and Dick
I have seen Tom, Dick and Barry
can be as long as we wish: the
grammaticality of such sentences Is
independent of the number of conjuncts.
Similar (right.- and lef+-recursive)
patterns can be found in any NL, but all
of theme can be described by regular
expressions. Therefore, if grammars do not
have to account for iterated
self-eabeddings, the principle of
scientific parsimony will point to the
Bilumal language family accomodating every
possible finite NL corpus and their
regular extensions. Prom this cerspective,
the Type-3 family in more than sufficient:
since it contains every finite language
and Is closed under regular operations, it
provides a generous upper bound for the
family of NI.e.
A more direct argument can be based
an the biological make-up of the human
brain: as individual neurons can be
modelled by finite automata (McCulloch
-Pitts 1943), and • finite
three-dimensicnel array of such automata
can be substituted by one finite automaton
(see Insane 1996), /Ms must be regular.
Although finite state models of NIs
usually do not claim &amp;quot;neurological
reality&amp;quot; (see ch 3.2 of Sullivan 1980),
the above reasoning gives us an =air
pound on the complexity of finite automata
necessary to describe NLe: since the
relevant part of the brain contains no
more than 100 cells, and one cell has cca.
102-103 states, non-deterministic automata
with 10105 states will be sufficient.
Since the neurological organizatien
of the human brain is unlikely to parallel
the actual organization of the
(internalized) grammar of native speakers,
it Is not surprising that the application
of linguistic methods gives a much sharper
upner hound: as we shall see, finite
deterministic NL acceptors need not have
more than 1016 states. This estimation
can be derived from the investigation of
the syntactic monoide defined by it..
(For the definition of syntactic monnids,
see McNaughton--Papert 1968, and for a
systematic exposition, see ch 3.10 of
Eilenberg 1974.)
Elements of the syntactic sonoid
correspond to the distributioaal clagsse
of structuralist linguistics: two strings
will belong to the same class if and only
if they have the same distribution, i.e.
iff they can be substituted for each other
in any sentence of the language in
question. The distributional classes
formed by strings of length one will be
the elements of the terminal alphabet: but
It should be kept in mind that these
function as preterminals inasmuch as each
of them steads for a (not necessarily
finite) class of elesents. In a
morpheme-based approach, terminals are
called sornheme glasses: these can be set
up by the procedure outlined In ch 1! Cl
Ferris ( 19511.
In a, word-based approach, that is,
if we take words to be the ultimate
syntactic constituents, the terminals will
be called lexical (sn)cateacrles: In
either ease,&apos; the number of terminals is
clearly finite. There are nc sore than 20
lexical categories: and in any given NL
there are less than 300 morpheme classes.
Eowever, fully formed words eith different
inflexional affixes will belong to
different distributional classes, and if
we &apos;,mat to maintain the regularity of
laziest insertion, lexical entries (e.g.
verbs or verb stems) with different
nutcategorization frames will fall in
different subclasses. Traditional accounts
of lexical. (sub)categorization also allow
for overlapping classes (in cases of
tosonimy). For the sake of simplicity, I
will bike the Montana atoms of such
systems as basic: this way, elements like
&apos;rum&apos; or &apos;divorce&apos; will be neither nouns
nor verbs but will be listed under a
separate category for Inoue-verbs&amp;quot;.
But even if we take all these
lectors into account, it can be safely
meld that the number of morpheme classes
dots not exceed 103 and the rusher of
lexical subcategories does not exceed 1C4
in any given NL. In other cords, it is
possible to select for any given NL a
&apos;core vocabulary&apos; (or morpheme list) of
104 (103) elenenis in such a manner that
every word (morpheme) not appearing in the
list will be disteitutionally equivalent
ic one already on It. ,This means that the
rveber of states that can be reached in
one step from a given state of a finite
state NL acceptor cannot exceed 104 and
conversely, any given state can be reached
from at most 104 states in one step.
The states of finite automata are in
one-to-one correspondence with the classes
of right-distribution : two strings over
the terminal vocabulary will take the
(minimal.) automaton tn the same state iif
they can be substituted for each other in
every right-side environment. As a special
case, it should be mentioned that the
strings that do not appear as initial
parts of grammatical sentences will give
only one state in the automaton: these,
therefore, can be disregarded. The
remaining strings (i.e. the ones that can
te finished grammatically) have a property
7) Present-day syntacticians seem to favor
the latter approach: for discussion see
Sabine (1959), Chomsky ( 1970 ), Lieber
( 16E1).
</bodyText>
<page confidence="0.995139">
4
</page>
<bodyText confidence="0.999952428571429">
peculiar to NI.: they can always be
finished with er. most four words (or
twelve morphemes).6 This means that the
final state. of NL acceptors can be
reached from every live state in at most
tour (twelve) steps. Therefore, the number
of live states is at mcst 1016 (1036).
</bodyText>
<subsectionHeader confidence="0.528845">
CanessmsnaiLa
</subsectionHeader>
<bodyText confidence="0.978220079365079">
It should be emphasized that the above
estimation in still very generous: a
systematic study of sentence endings Is
highly unlikely to reveal sore than 103
different patterns in any given 141., and
the proper order of magnitude seems to be
104. If the automaton has to account for
the morphology of the language as well,
106-10, states will be necessary -- this
is, perhaps, outside the capabilities of
present-day computers. In any case, finite
automata can be implemented on SAI
theoretical (or actual) model of serial
computation like Turing machines, random
access machines, etc. to accept languages
in linear time.
Although native speakers understand
grammatical sentences in meel time, their
performance as NI acceptor is somewhat
hindered by the fact that the heurintuc
algorithm they use is not adopted to
ungrammatical strings: usually they spend
some (limited) time with deliberation, and
sometimes they want to hear the &apos;scut
string a second . time. But even in this
(worst) case recognition happens in linear
time, and in this respect at least, finite
automats constitute realistic models of
native speakers.
The importance of thin fact for
Linguistic matatheory should not be
underestimated: those frameworks (like
transformatonal grammar, see Rounds 1915)
that generate languages with exponential
(polynomial) recognition complexity sake
the prediction that there are problems
which can be solved both by humans and
Turing machines in a measured time, with
humans showing an exponential (polynomial)
gain over machines in the long run. For
instance, Lexical- Functional G (see
Bresnan 1983) makes the claim that humans
can solve certain NF-herd problese in
linear time (cf. Berwick 1882), and this
is not very likely. On the other hand,
those framework/a (like Generalised Phrase-
Structure C 9 see Cazdar 1992) that
generate only languages of polynomial time
complexity might have nose psychological
8) This property is ea linear version of
the Depth Hypothesis (Tneve 1961).
9) For the sake of simplicity I have
supposed that sentences in embedded
position are freely interchangeable, i.e.
that there is only one accepting state.
reality: At: leaSt there is nothing in
present-day complexity theory that
precludes the possibility of one
implementation (e.g. sulti-tape luring
sachines, org.for that matter, the brain)
gaining a small polynomial /actor over
another one • (e.g. single-tape Turing
each/nem).
</bodyText>
<listItem confidence="0.845168">
• Another advantage of Type 3 NI
modeles is that they sake the trebles ci
language acquisition solvable, at least
theoretically-. It Ls sell known that no
</listItem>
<bodyText confidence="0.977834666666667">
algorithm can ctelde whether a
context-free -grammar generates a given
context-free language: therefore, if every
(infinite) context-free language is • a
passible NL. Church&apos;s thesis will make it
impossible for the child to acquire one,
cot even in case they have access to an
oracle (say, the parents) that tells them
whether a string helongs to the language
or not. Therefore, it is sometimes
supposed that the primary linguistic data
accessible during language acquisition
contains not only strings, but the
associated tree-structures as well. But
if NLs are- regular, the problem is
solvable without -recourse tc thio rather
strange assumption:. given an upper bound
on the number of-states in the canonical
automaton generating the language, it Is
passible to reconstruct the automaton in a
finite =amber of queries (Moore 1996).
Since the number of queries is at least
1012 oven if the child has access to a
&apos;representative sample&apos; of 104 sentences
that teaches every live state in the
automaton lsee AnSluin 1981), and it is
impossible to make sore than 106 queries
is a /intim*. NIA must form a proper
subset of regular languages.
Ittfant., there is reasca to suppose
that every NL will be ns=samallas.
Specifically, a string zy4z sill belong to
scree NL-if and only if tysz is also in it.
This L obvious if y is a coordinate
conjunct as certain languages
differentiate between singular, dual,
trial, and plural, the number 4 cannot he
reduced. If y is the repeated part of some
left- or right- recursive construction
(e.g. a thaA.4..clause), five copies will he
just as grammatical as four copies were:
the converse also seems to hold. If this
characterisation al NLa is true, the
traditional mode of language description
in &apos;tactical&apos; terms is fully justified,
because very non-counting language can be
built up from the elements of the alphabet
using only catenation and Boolean
operations (licNaustton- Papert 1E11).
Ccaversely, as the traditional
ghomotaclic. -morphotactic, and syntactic
descriptions of Nle used only catenation,
union, • intersection, and sometimes
coeplesentstion lin the form of &apos;negative
conditions&apos; ).• the generative power of the
14es and .Arrsgement sodel (see Rockett
1994) dome not exceed that of counter-free
</bodyText>
<page confidence="0.98624">
5
</page>
<bodyText confidence="0.896987857142857">
automata.
It is also possible to develop a
Lowe r bound for the family of NI*: the
phenomenon of syntactic concord over
unbounded domains (which I suppose to be
present in every NL) will guarantee that
NLs cannot be locally testable. The
following demonstration is based on a
regular expression used by Pullum- Gazdar
(1982): coordination (the outermost neon.
*) has been added in order to create non-
Initial and non- final elements that have
to agree with each other.
(12) ((Which problem did your professor
say ((she + you) thought)* was
unsolvable). + (Which problems did
your professor say ((she + you)
thought)* were unsolvable))*
Suppose, indirectly, that English is
k-testable for some fixed k, and consider
the following strings:
</bodyText>
<listItem confidence="0.763246666666667">
(13) ((Which problem did your professor
say (she thought you thought)k was
unsolvable) (Which problems did
</listItem>
<bodyText confidence="0.82608353125">
your professor say (she thoght
you thought)k were unsolvable))2
( 14 ) ((Which problem did your protegee,
say (she thought you thought)1f
were unsolvable) (Which problem
did your professor say (she thoght
you thought)k was unsolvable)
(Which problems did your professor
say (she thought you thought)k was
unsolvable) (Which problems did
your prof sty (she thoght you
thought)k were unsolvable))
Apart from the order of the conjuncts, the
only difference between (13) and (14) is
that In the latter subject- predicate
number agreement is violated in the first
and the third conjuncts. Therefore. (14)
Is ungrammatical, but it has the saae
subwords of length k (and with the same
multiplicity) as the gramsatical (1Z).
This contradicts our hypothesis that
English was k-testable.
Hopefully, this special position of
NLs in the Chomsky hierarchy can be
utilised in. streamlining the (oracle)
algorithms modelling language acquisition,
because such algorithms (if actually
Implemented) would greatly simplify the
descriptive work of the linguist, and, at
least to a. certain extent, would finally
fulfill the structuralists&apos; pronise of
discovery procedure.
</bodyText>
<sectionHeader confidence="0.999008" genericHeader="acknowledgments">
ACTNOWLEDGEMENT
</sectionHeader>
<bodyText confidence="0.98830225">
/ as grateful to Gerald Gazdar for his
valuable criticisms of an earlier version
of this article. The usual disclaisers
sooly•
</bodyText>
<sectionHeader confidence="0.997574" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.884941895522388">
Angluin, D. 1881: A note on the number of
queries needed to identify regular
languages. Information 8 Control 51,
76-87
Arrow, X. L. 1950: A difficulty in the
concept of social welfare. Journal
• of Political Economy SE, 32E-346
Berwick, E. C. 19E2: Computational
complexity and Lexocal- Functional
Grammar. American Journal cf
Computational Linguistics E, S1-10E
Bloomfield, L. 1926: A set of postulates
for the science of language.
Language 2, 152-164
Borgida. k. •T. 1983: Some formal results
about stratificational g aaaaa re and
• their relevance to linguistics.
Mathematical Systems Theory 16,
29-56
Bresnan, J. 1983: The mental
• representation of grammatical
relations (ed) MIT Pres, Cambridge,
Mass.
Carroll, J. B. 1E52: The study of
language. Barverd University Press,
Cambridge, Wass.
Choesky, N. 1851: Syntactic Structures.
Moutonr The Hague Choseky, N. 1E56:
Three models for the description cf
language. I.R.E. Transactions on
Intsrsation Theory VT-2.
Cbeesky, N. 1961: Some methodological
remarks on generative grammar. Word
17, 219-239
Chomsky, N. 1963: Formal properties cf
grammars. In: Luce- Bush- Galanter
(ads) Handbook of mathematical
psychology. Wiley, New Ulric
Chossky, N. 1fle: emarks on
nomlaalization. In: Jacobs-Rosenbaum
feedings in English
Transformational G • Ginn,
Waltham, Mass.184- 221
Church. 16 1980: Cs parsing strategies and
closure. Proceedings of the 18th
Annual Meeting of the ACL 101-111
Iilenbarg, S. 1974: Automata, languages,
and machines. Academic Press, New
&amp;quot;torte
Gazdar, G. 1882: Phrase structure g
In: Jacobson-Pullum (ads): The
• Mature of Syntactic Representation.
Reldel, Dordrecht 131-1E6
Creenbaus, J.- H. Cuirk 1970: Elicitation
experiments in English: linguistic
studios In usage and attitude.
Loners*, London
Greene, J. 1E12: Psycholinguistics.
Penguin, Hersomdmworth
Barris, Z. 1946: Pros morpheme to
utterance. Language, 22, 161-1E3
Barris, Z. 1951: Methods in Structural
Linguistics. tniversity of Chicago
Press
Berrison, M. A. /878: Introduction to
Perms/ Language Theory.Addison-
Wesley, Rmadingr Wass.
</reference>
<page confidence="0.984475">
6
</page>
<reference confidence="0.998821305882353">
Hackett, C. 1954: Two sodels of
grassatics1 description. Word 1C,
210-231
Hockett, C. 1955: A manual of phonology.
International Journal of American
Linguistics, Memoir. 11
Itkonen, E. 1981: The concept of
linguistic intuition. In: Coulsans
(ed): A Festschrift for Native
Speaker. Mouton, The Hague, I21-14C
Kleene, S. C. 1956: Representation c/
events in nerve nets and finite
automata. In: Shannon - McCarthy
(ode): Automata studies. Princeton
University Preen 3-41
Lamb, S. M. 1966: Outline c/
stratificational grammar. Georgetown
University Press, Washington D.C.
Levi-Strauss, C. 1958: Anthropolcgie
structural.. Plon, Paris.
Lieber, R. 1981: On the Crganization of
the Lexicon. IULC
Marcus, S. 1564: Gr tici si autosate
finite. Editure Academie&apos;, Bucharest
Narks, L. E. 1968 Scaling of
grammaticalness of self-embedded
English sentences. Verbal Learning 8
Verbal Behavior 7, 965-461
Matthews, N.J. 1979: Are the grassatical
sentences of a language a recursive
set? Synth... 40, 209-224
McCullogh, V. S. - V. Pitts 1943: A
logical calculus of the ideas
immanent in nervous activity.
Bulletin of mathematical biophysics
5, 115-133
McNaughton, R. - S. Paport 1968: Tho
syntactic Ronald of a regular event.
In Arbit (ed): Algebraic theory el
machines, languages, and semigroups.
Academic Press, New York 291-312
McNaughton. R - S. Pspert 1611:
Counter-free automata. Seseerct
Monograph no. EF., MIT Press,
Cambridge, Mass.
Willer, G. A. - S. heard 1964: Free recall
of self-embedded English sentences.
Information 8 Control 7, 292-302
Moore, E. F. 1956: Gedanken-experisents on
sequential machines. In: Shannon -
McCarthy (eds): Automata studies.
Princeton University Press 129-152
Neveyer, F. J. 1980: Linguistic ttecry in
America. Acsdeele Preset New York
Fullum. G. and G. Gezdar 1952: Natural
languases andcotext free
lang ges.Linguisticsn and
Philosophy 4, 471-504
Putnam, B. 1961: Some issues in the thecry
of grammar. Proc. Symposia in
Applied Mathematics 1951
Reich, P.A. 1969: The finiteness cl
natural languages. Language 4E,
831-843
Robins, R.N. 1959: In defense of IP.
Trans. Fhilol. Soc. 11E-144
losers, M. 1967: The theory of recursive
functions and effective
computability. McGraw-Bill, New &apos;York
Sounds, W. 1915: A g tical
characterisaticn of exronential-tise
Languages. Proc. 16th Symcosius on
Switching Theory and Automata
135-143
Selomaa, A. 1973: Forsal Languages.
Academic Press, New York
Sullivan, W. J. 1950: Syntax and
Linguistic tics in
strstificstionel theory. In: lirth-
Moravcsik Cede) Current approaches
to syntax. Acadesic Press, New York
301-327
leave. V. H. 1911: The depth hypothesis.
Jskobson (ed): Proc. of Sysposia
in Applied Mattematice 12, 130-138
</reference>
<page confidence="0.999517">
7
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000600">
<title confidence="0.965748">NATURAL LANGUAGES AND TIE CROSSLY 111HRARCET</title>
<author confidence="0.999744">-Andras Kornai</author>
<affiliation confidence="0.998201">Institute of Linguistics Hungarian Academy of Sciences</affiliation>
<address confidence="0.988889">Budapest, P.C.B. 19. 11-1250 Hungary</address>
<email confidence="0.84307">•</email>
<abstract confidence="0.99884073105498">central claim of the paper NI stringsets are regular. Three independent arguments are offered in favor of this position: one based on parsimony considerations, one employing the McCullogh-Pitts (1943) model of seisms, and a purely linguistic one. It is possible to derive explicit upper bounds for the number of (live) states in acceptors: the results show that finite state NL parsers can be implemented on present-day computers. The position at NI stringsets within the regular family is also investigated: it is proved that NLa are counter-free, but not locally testable. jOroduction The question whether the grammatical sentences of natural languages form regular (Type 3), context free (Type 2), context sensitive (Type I), or recursively enumerable (7ype 0) nets has been subject to much discussion ever since it was posed by Chomsky in his seminal 1956 paper. However, there seems to be little agreement among the linguists concerned with the &apos;geographic&apos; position of natural instance Reich (1868) be finitestate (Type 2), while Matthews (19/9) argues that they are not even recursively enumerable. Pullum and Gazdar (1982) have demonstrated that the standard linsuigIls arguments against the contextfreeness of natural languages are fallacious they did not consider, the asialiuguistic argument offered by Matthews. In Section 1 of this paper I will briefly outline and challenge this argument, and in Section 2 I will argue In favor of Reich&apos;s position. The claim that Type 3 ham several implications for linguistic (meta)theory: these will be discussed in Section 3. paper presupposes 10ffc familiarity with the basic notions and notations of format language theory: when no specific reference is given, the reader will find ei proof both in Salome. (1973) and Harrison (1976) BALULAk IAMMMAAAJA 12.1211 vies of natural languages, i.e. the identification of NLe with the of strings (sentencee) is sometimes regarded an idea generative linguistics. it was (19E1:1E) who first view this is not vitally unjust: yet it is quite clear that the same idea was /implicit in such of the work of the structuralist period.&apos; In fact, the &apos;discovery procedures&apos; developed by the structuralists in order to arrive at • colncise description (g ) cf NI from a met of utterances (corpus) were without exception based on the assumption that native speakers of the language are capable of judging the g ticality of utterances presented to them. Although these procedures are, by and large, practical (empirical) and sechanical (algorithmic), their presentation already involved 46, certain t of idealization. For instance, it is obvious that native speakers themselves utter ungrammatical sentences from time to time, and it is also clear that they can understand (parse) sentences that are not strictly &apos;grammatical&apos;. Nevertheless, methods work quite well in actual practice of NI description, and the structuralist methodology has often been compared to that of chemistry, physics, other natural Matthews (1979) has canted doubts an the fundamental assumption of these procedures: be claims that native speakers in tact judge the grammaticality of material presented to them. The relevant part of his below: ------- 1) See e.g. Def 4 in Eloomfield 1926, or Farris 1))46 eh 1.0 2) See e.g.-Carroll 1953, Levi-Strauss 1958 ch. 2 (1) and native speakers instantiate (1) the canoe floated down the river sank (2) the editor authors the newspaper hired liked laughed an effective procedure in their classification of sentences, then presumably the classIficaton of (1) and (2) should not depend on their position in a list of test &apos;sentences that also sentences similar to (4). (3) the man (that was) thrown down the stairs died (4) the editor (whom) the authors the newspaper hired liked laughed but in fact it does. (1) and (2) will typically be classified as ungrammatical they precede sentences similar to and (4), but grasmatical if they follow thee. Such cases are quite common.&amp;quot; (p 212) Moreover, &amp;quot;there is considerable empirical evidence to suggest that native speakers employ a battery of heuristic strategies when parsing and classifying sentences. Their reliance on , such strategies does not preclude their having available to them an effective procedure for deciding sembershis in their language; however, in the &amp;become* of empirical evidence for such a procedure, we are certainly not required to postulate its existence.&amp;quot; (p 213) From this, Matthews concludes that Putnam (1961) was not justified in appealing to Church&apos;s thesis in order to that recursive: for 12 native speakers have no effective procedure for deciding membership in the set of grammatical sentences, then there is co guarantee that such procedure exists. But it really the cause that the &amp;quot;battery heuristic procedures&amp;quot; employed by native speakers falls outside the scope of Church&apos;s thesis? Nellconfirmed natural are usually taken to be universally valid -it. is unclear why should Church&apos;s thesis be an exception to this, and Matthews offers no evidence to corroborate his views on this point. Putnam&apos;s original argument derives its strength from Church&apos;s thesis. If NIA are Type 0, then the heuristic strategies of native speakers will be instances of precisely that sort of procedures that Church&apos;s thesis predicts exist: on the one hand, they are &apos;intuitively effective&apos;, and on the otter they Turing computable. The phenommon observed by Matthews, namely that native speakers can be coaxed into accepting (or rejecting) sentences has little to do with the recursiveness cf the battery et heuristics they eeplcy: it calls the extensional view language in question. The problem is a sethodological one: if NLa are defined to be sets of ago/meatiest sentences, how can one test potential elesecte for senbership? This problem becomes particularly acute in borderline cases above), and for the linguist who wants to check the predictions oA his grammar it matters but little that such dubious sentences are (statistically) infrequent. The easiest way to act-we this problem is to give up the aseuaction that srassatic&amp;lity is a yes/no question: of g ticalreas* can be (see e.g. Chossky 1961) and can be treated as graded (or even fuzzy) sets. This approach, tewever, can only be applied in the study cf idiolects (languages of individual speakers), because there Is no way to arrive at a set that will reflect the indite/dusl opinions in a reasonably faithful manner. Suppose, for instance, that we have three speakers, 2, 1, and Zip and each of the. classifies the sentences al h, ard c consistently (that is, if he prefers a to b and b to c, Ahem he prefers a to c, etc.). Now, if for speaker X a&gt;h&gt;c, for Y and for Zc&gt;e)t, then the &apos;majority ;refers a to b, b to c, and c to a; in ether words, the &apos;general opinion&apos; is inconsistent (nontransitive). The &amp;quot;possibility theorem&amp;quot; of Arrow (1950) sakes it clear that the example is typical: under very general cenditions, there is sinply no way to aggregate graded in such that the (partial) orderings imposed by the individual gradations are pre d. Therefore, the of gransaticalnesel approach to thestudy of idiolecte linguists, however, reject it entirely Newmeyer 1980 ch. 5.5.2, 5.7.1). Yes/no grammaticality Judgments, on other hand..shew kably little variation from speaker to speaker in any given speech cossunity, and it is this legtntilitv(cf. /tkcnen 1581) that justifies the escirical study cf Idle/acts* and-even &apos;languages&apos;. But if Watthews is right, and native speakers are unable to &amp;quot;classify any sentence over the of their language as -either grammatical-or ung tient&amp;quot; (P testability independent sotivaton of Churchle thesis, see e.g. lagers 1967: ch. 1.7 2 will be inpossible to achieve. The question is: what makes the native speaker inconsistent? In Matthews&apos; example, there can be little doubt that the cause of the is the teet,aituallon: speaker&apos;s linguistic intuition is not the same before and after reading sentescos (3-4). This source of inconsistency can be eliminated fairly easily: if the sentences are presented in a random manner (preferably, with &amp;quot;fitter&amp;quot; sentences among them), then no &amp;quot;cues provided by the context of classification&amp;quot; (p 213) viii be present. Naturally, linguistically relevant experiments will have to control many other factors (see e.g. Greenbaum and Quirk 1970), but as we shall see, there is no need to discuss these individually. Fro, the point of intrasubjective testability, it can be safely said that well-designed experiments usually provide highly consistent dates (even in the case of borderline sentences), and the extensional view of Nis can be maintained on an empirical basis as well. The actual sets designated as NI. will, at least to a certain extent, depend on the choice of technique, but experimental method can be thought of as an algorithm for deciding questions of witgIts Lig Massa genele.! Since the existing experimentel methods can be replaced by (interactive) computerprograms, the question boils down to this: is a Turing machine with a human oracle more powerful than one with a Turing machine oracle? By Church&apos;s thesiet the answer Is negative, and as Turing machines with recursive oracles are no more powerful than Turing machines without oracle (see e.g. Rogers 1967 ch. 9.4). Ms must be recursive. Notice, that this line of reasoning is independent of the particular choice of experimental technique, or what is the same, of the precise definition of Rte. This is a consequence of the fact that the experimental methods used in empirical sciences (including linguistics) hardly merit this Esse unless they are welldefined and &apos;mechanical&apos; to such an extent that their algorithmazation poses no real problems. (For instance, the procedure outlined above does not make crucial reference to random sequences: the Irsndomizationl of testsentences can be out with the aid of sequences generated by Turing machine.) This is not to say that introspective evidence or intuition plays no role in linguistics (or in general, in the development of science) -but questions 4) For the definition of oracles, see e.g. 9 concerning the position of natural languages in the Chossky hierarchy can hardly be meaningful unless we have some definition of His (1.0. some experimental sethod to test membership) tc work with. Um al ZaturalIGAIAMARA2 &apos; state NI models developed by Rockett (199E). Although (1957 -ch 3.1) attempted to demonstrate the inadequacy of such models, advocated their use, the stretificational school (Lab in employing aformalism which Is, in essence, equivalent to finite automata (cf. Table 1 of Borgida 1983). As Reich (1969) has pointed out, Chomsky&apos;s demonstration is based on the assumption that His are self-embedding Is grialleary degzeg. This means, that and&apos; be equally grammatical: • the boss editor authors the newspaper hired liked hates laughed the cons/Vte boss editor authors the newspaper hired liked hates The experiments (Miller and Isard 1964, Marks 1968). h , do not support this native of . English to and (7-8) the same the boss editor authors the • newspaper hired liked hates .1aughed cursed the secretary commitie boss editor -authors the newspaper hired liked hates chairsagreed are ungresaetical im any grammar of English.- Chossky&apos;s original demonstration is tar from convincing, and the question whether Nis are Type 2 is still open. In fact, the only way to show that are not is exhibit some infinite sequence of graematical sentences: fortunately, the pattern exespiilied in (1-6) is not necessary for this. Coordinated constructions as in 9) Especially the ones working with computers. See e.g. Marcus 1964, Church 19S0 (1962) regards (5-6) grammatical. (-but unacceptable) and (7-8) ungrammatical: for the aethodalogical isplications of this position see Greene (19/2). 3 / have seen Tom T have seen Tom and Dick I have seen Tom, Dick and Barry can be as long as we wish: the grammaticality of such sentences Is independent of the number of conjuncts. Similar (right.and lef+-recursive) patterns can be found in any NL, but all of theme can be described by regular expressions. Therefore, if grammars do not have to account for iterated self-eabeddings, the principle of scientific parsimony will point to the family accomodating every possible finite NL corpus and their regular extensions. Prom this cerspective, the Type-3 family in more than sufficient: since it contains every finite language and Is closed under regular operations, it provides a generous upper bound for the family of NI.e. A more direct argument can be based an the biological make-up of the human brain: as individual neurons can be modelled by finite automata (McCulloch -Pitts 1943), and • finite three-dimensicnel array of such automata can be substituted by one finite automaton (see Insane 1996), /Ms must be regular. Although finite state models of NIs usually do not claim &amp;quot;neurological reality&amp;quot; (see ch 3.2 of Sullivan 1980), the above reasoning gives us an =air poundon the complexity of finite automata to describe the relevant part of the brain contains no than cells, and one cell has cca. states, non-deterministic automata states will be sufficient. Since the neurological organizatien of the human brain is unlikely to parallel the actual organization of the (internalized) grammar of native speakers, it Is not surprising that the application of linguistic methods gives a much sharper upner hound: as we shall see, finite deterministic NL acceptors need not have than states. This estimation can be derived from the investigation of the syntactic monoide defined by it.. (For the definition of syntactic monnids, see McNaughton--Papert 1968, and for a systematic exposition, see ch 3.10 of Eilenberg 1974.) Elements of the syntactic sonoid to the clagsse of structuralist linguistics: two strings will belong to the same class if and only if they have the same distribution, i.e. iff they can be substituted for each other in any sentence of the language in question. The distributional classes formed by strings of length one will be elements of the terminalalphabet: but It should be kept in mind that these function as preterminals inasmuch as each of them steads for a (not necessarily finite) class of elesents. In a morpheme-based approach, terminals are glasses:these can be set up by the procedure outlined In ch 1! Cl Ferris ( 19511. In a, word-based approach, that is, we to be the ultimate syntactic constituents, the terminals will called (sn)cateacrles:In either ease,&apos; the number of terminals is clearly finite. There are nc sore than 20 lexical categories: and in any given NL there are less than 300 morpheme classes. Eowever, fully formed words eith different inflexional affixes will belong to different distributional classes, and if we &apos;,mat to maintain the regularity of laziest insertion, lexical entries (e.g. verbs or verb stems) with different nutcategorization frames will fall in different subclasses. Traditional accounts lexical. (sub)categorization for overlapping classes (in cases of tosonimy). For the sake of simplicity, I bike the atomsof such systems as basic: this way, elements like &apos;rum&apos; or &apos;divorce&apos; will be neither nouns nor verbs but will be listed under a separate category for Inoue-verbs&amp;quot;. But even if we take all these it can be safely meld that the number of morpheme classes not exceed and the rusher of subcategories does not exceed in any given NL. In other cords, it is possible to select for any given NL a &apos;core vocabulary&apos; (or morpheme list) of elenenis in such a manner that every word (morpheme) not appearing in the disteitutionally equivalent ic one already on It. ,This means that the rveber of states that can be reached in one step from a given state of a finite NL acceptor cannot exceed and conversely, any given state can be reached at most states in one step. The states of finite automata are in one-to-one correspondence with the classes of right-distribution : two strings over the terminal vocabulary will take the (minimal.) automaton tn the same state iif they can be substituted for each other in every right-side environment. As a special case, it should be mentioned that the strings that do not appear as initial parts of grammatical sentences will give only one state in the automaton: these, therefore, can be disregarded. The remaining strings (i.e. the ones that can te finished grammatically) have a property 7) Present-day syntacticians seem to favor the latter approach: for discussion see (1959), Chomsky ( ), Lieber ( 16E1). 4 peculiar to NI.: they can always be with er. mostfour words (or This means that the final state. of NL acceptors can be reached from every live state in at most tour (twelve) steps. Therefore, the number live states is at mcst CanessmsnaiLa It should be emphasized that the above estimation in still very generous: a systematic study of sentence endings Is unlikely to reveal sore than different patterns in any given 141., and the proper order of magnitude seems to be If the automaton has to account for the morphology of the language as well, states will be necessary -this is, perhaps, outside the capabilities of present-day computers. In any case, finite automata can be implemented on SAI theoretical (or actual) model of serial computation like Turing machines, random access machines, etc. to accept languages lineartime. Although native speakers understand grammatical sentences in meel time, their performance as NI acceptor is somewhat hindered by the fact that the heurintuc algorithm they use is not adopted to ungrammatical strings: usually they spend some (limited) time with deliberation, and sometimes they want to hear the &apos;scut string a second . time. But even in this (worst) case recognition happens in linear time, and in this respect at least, finite automats constitute realistic models of native speakers. The importance of thin fact for Linguistic matatheory should not be underestimated: those frameworks (like grammar, see Rounds that generate languages with exponential (polynomial) recognition complexity sake the prediction that there are problems which can be solved both by humans and Turing machines in a measured time, with humans showing an exponential (polynomial) gain over machines in the long run. For instance, Lexical- Functional G (see Bresnan 1983) makes the claim that humans can solve certain NF-herd problese in linear time (cf. Berwick 1882), and this is not very likely. On the other hand, framework/a (like Generalised Phrase- Structure C 9 see Cazdar 1992) that generate only languages of polynomial time complexity might have nose psychological 8) This property is ea linear version of the Depth Hypothesis (Tneve 1961). 9) For the sake of simplicity I have in embedded position are freely interchangeable, i.e. that there is only one accepting state. reality: At: leaSt there is nothing in present-day complexity theory that precludes the possibility of one implementation (e.g. sulti-tape luring that matter, the brain) gaining a small polynomial /actor over another one • (e.g. single-tape Turing each/nem). • Another advantage of Type 3 NI modeles is that they sake the trebles ci language acquisition solvable, at least theoretically-. It Ls sell known that no algorithm can ctelde whether a context-free -grammar generates a given context-free language: therefore, if every (infinite) context-free language is • a passible NL. Church&apos;s thesis will make it impossible for the child to acquire one, cot even in case they have access to an (say, that tells them whether a string helongs to the language or not. Therefore, it is sometimes supposed that the primary linguistic data accessible during language acquisition contains not only strings, but the associated tree-structures as well. But if NLs areregular, the problem is solvable without -recourse tc thio rather strange assumption:. given an upper bound on the number of-states in the canonical automaton generating the language, it Is passible to reconstruct the automaton in a finite =amber of queries (Moore 1996). Since the number of queries is at least oven if the child has access to a sample&apos; of sentences that teaches every live state in the automaton lsee AnSluin 1981), and it is to make sore than queries is a /intim*. NIA must form a proper subset of regular languages. Ittfant., there is reasca to suppose every NL will be a string sill belong to scree NL-if and only if tysz is also in it. This L obvious if y is a coordinate conjunct as certain languages differentiate between singular, dual, trial, and plural, the number 4 cannot he reduced. If y is the repeated part of some leftor rightrecursive construction a five copies will he just as grammatical as four copies were: the converse also seems to hold. If this al true, the traditional mode of language description in &apos;tactical&apos; terms is fully justified, because very non-counting language can be built up from the elements of the alphabet using only catenation and Boolean operations (licNaustton- Papert 1E11). Ccaversely, as the traditional ghomotaclic. -morphotactic, and syntactic descriptions of Nle used only catenation, union, • intersection, and sometimes coeplesentstion lin the form of &apos;negative conditions&apos; ).• the generative power of the 14es and .Arrsgement sodel (see Rockett 1994) dome not exceed that of counter-free 5 automata. It is also possible to develop a Lower boundfor the family of NI*: the phenomenon of syntactic concord over unbounded domains (which I suppose to be present in every NL) will guarantee that cannot be testable.The following demonstration is based on a regular expression used by Pullum- Gazdar (1982): coordination (the outermost neon. *) has been added in order to create non- Initial and nonfinal elements that have to agree with each other. (12) ((Which problem did your professor say ((she + you) thought)* was unsolvable). + (Which problems did professor say + thought)* were unsolvable))* Suppose, indirectly, that English is k-testable for some fixed k, and consider the following strings: (13) ((Which problem did your professor say (she thought you thought)k was (Which problems your professor say (she thoght you thought)k were unsolvable))2 14 ) problem did your protegee, say (she thought you thought)1f were unsolvable) (Which problem did your professor say (she thoght you thought)k was unsolvable) (Which problems did your professor say (she thought you thought)k was unsolvable) (Which problems did your prof sty (she thoght you thought)k were unsolvable)) Apart from the order of the conjuncts, the only difference between (13) and (14) is that In the latter subjectpredicate number agreement is violated in the first and the third conjuncts. Therefore. (14) Is ungrammatical, but it has the saae subwords of length k (and with the same multiplicity) as the gramsatical (1Z). This contradicts our hypothesis that English was k-testable. Hopefully, this special position of NLs in the Chomsky hierarchy can be utilised in. streamlining the (oracle) algorithms modelling language acquisition, because such algorithms (if actually Implemented) would greatly simplify the descriptive work of the linguist, and, at least to a. certain extent, would finally fulfill the structuralists&apos; pronise of discovery procedure. ACTNOWLEDGEMENT / as grateful to Gerald Gazdar for his valuable criticisms of an earlier version of this article. The usual disclaisers sooly• REFERENCES Angluin, D. 1881: A note on the number of queries needed to identify regular</abstract>
<note confidence="0.619642666666667">Information 8 Control 76-87 Arrow, X. L. 1950: A difficulty in the concept of social welfare. Journal of Political Economy E. C. complexity and Lexocal- Functional Grammar. American Journal cf Computational Linguistics E, S1-10E Bloomfield, L. 1926: A set of postulates for the science of language. Language 2, 152-164 Borgida. k. •T. 1983: Some formal results about stratificational g aaaaa re and • their relevance to linguistics. Mathematical Systems Theory 16, 29-56 J. mental • representation of grammatical Pres, Cambridge, Mass.</note>
<address confidence="0.761499">Carroll, J. B. 1E52: The study of</address>
<affiliation confidence="0.96754">language. Barverd University Press,</affiliation>
<address confidence="0.992969">Cambridge, Wass.</address>
<note confidence="0.644260428571429">N. Structures. The Hague Choseky, N. Three models for the description cf language. I.R.E. Transactions on Intsrsation Theory VT-2. Cbeesky, N. 1961: Some methodological remarks on generative grammar. Word 17, 219-239 Chomsky, N. 1963: Formal properties cf grammars. In: Luce- Bush- Galanter (ads) Handbook of mathematical psychology. Wiley, New Ulric 1fle: on nomlaalization. In: Jacobs-Rosenbaum</note>
<author confidence="0.466373">feedings in English</author>
<affiliation confidence="0.896518">Transformational G • Ginn,</affiliation>
<address confidence="0.964578">Waltham, Mass.184- 221</address>
<note confidence="0.832429428571429">Church. 16 1980: Cs parsing strategies and closure. Proceedings of the 18th Annual Meeting of the ACL 101-111 Iilenbarg, S. 1974: Automata, languages, and machines. Academic Press, New &amp;quot;torte Gazdar, G. 1882: Phrase structure g In: Jacobson-Pullum (ads): The • Mature of Syntactic Representation. Reldel, Dordrecht 131-1E6 Creenbaus, J.- H. Cuirk 1970: Elicitation experiments in English: linguistic studios In usage and attitude. Loners*, London Greene, J. 1E12: Psycholinguistics. Penguin, Hersomdmworth Barris, Z. 1946: Pros morpheme to utterance. Language, 22, 161-1E3 Barris, Z. 1951: Methods in Structural Linguistics. tniversity of Chicago Press</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>e g See</author>
</authors>
<journal>Def</journal>
<booktitle>in Eloomfield 1926, or Farris 1))46 eh 1.0</booktitle>
<volume>4</volume>
<marker>See, </marker>
<rawString>1) See e.g. Def 4 in Eloomfield 1926, or Farris 1))46 eh 1.0</rawString>
</citation>
<citation valid="true">
<title>2) See e.g.-Carroll 1953, Levi-Strauss</title>
<date>1958</date>
<pages>2</pages>
<marker>1958</marker>
<rawString>2) See e.g.-Carroll 1953, Levi-Strauss 1958 ch. 2</rawString>
</citation>
<citation valid="false">
<authors>
<author>D Angluin</author>
</authors>
<title>1881: A note on the number of queries needed to identify regular languages.</title>
<journal>Information 8 Control</journal>
<volume>51</volume>
<pages>76--87</pages>
<marker>Angluin, </marker>
<rawString>Angluin, D. 1881: A note on the number of queries needed to identify regular languages. Information 8 Control 51, 76-87</rawString>
</citation>
<citation valid="true">
<authors>
<author>X L Arrow</author>
</authors>
<title>A difficulty in the concept of social welfare.</title>
<date>1950</date>
<journal>Journal • of Political</journal>
<location>Economy SE, 32E-346</location>
<marker>Arrow, 1950</marker>
<rawString>Arrow, X. L. 1950: A difficulty in the concept of social welfare. Journal • of Political Economy SE, 32E-346</rawString>
</citation>
<citation valid="false">
<authors>
<author>E C Berwick</author>
</authors>
<title>19E2: Computational complexity and Lexocal- Functional Grammar.</title>
<journal>American Journal cf Computational Linguistics E,</journal>
<pages>1--10</pages>
<marker>Berwick, </marker>
<rawString>Berwick, E. C. 19E2: Computational complexity and Lexocal- Functional Grammar. American Journal cf Computational Linguistics E, S1-10E</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bloomfield</author>
</authors>
<title>A set of postulates for the science of language.</title>
<date>1926</date>
<journal>Language</journal>
<volume>2</volume>
<pages>152--164</pages>
<marker>Bloomfield, 1926</marker>
<rawString>Bloomfield, L. 1926: A set of postulates for the science of language. Language 2, 152-164</rawString>
</citation>
<citation valid="true">
<authors>
<author>k •T</author>
</authors>
<title>Some formal results about stratificational g aaaaa re and • their relevance to linguistics.</title>
<date>1983</date>
<journal>Mathematical Systems Theory</journal>
<volume>16</volume>
<pages>29--56</pages>
<marker>•T, 1983</marker>
<rawString>Borgida. k. •T. 1983: Some formal results about stratificational g aaaaa re and • their relevance to linguistics. Mathematical Systems Theory 16, 29-56</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bresnan</author>
</authors>
<title>The mental • representation of grammatical relations (ed) MIT Pres,</title>
<date>1983</date>
<location>Cambridge, Mass.</location>
<marker>Bresnan, 1983</marker>
<rawString>Bresnan, J. 1983: The mental • representation of grammatical relations (ed) MIT Pres, Cambridge, Mass.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J B Carroll</author>
</authors>
<title>1E52: The study of language.</title>
<publisher>Barverd University Press,</publisher>
<location>Cambridge, Wass.</location>
<marker>Carroll, </marker>
<rawString>Carroll, J. B. 1E52: The study of language. Barverd University Press, Cambridge, Wass.</rawString>
</citation>
<citation valid="false">
<authors>
<author>N Choesky</author>
</authors>
<title>1851: Syntactic Structures. Moutonr The Hague Choseky, N. 1E56: Three models for the description cf language.</title>
<journal>I.R.E. Transactions on Intsrsation Theory</journal>
<volume>2</volume>
<marker>Choesky, </marker>
<rawString>Choesky, N. 1851: Syntactic Structures. Moutonr The Hague Choseky, N. 1E56: Three models for the description cf language. I.R.E. Transactions on Intsrsation Theory VT-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cbeesky</author>
</authors>
<title>Some methodological remarks on generative grammar.</title>
<date>1961</date>
<journal>Word</journal>
<volume>17</volume>
<pages>219--239</pages>
<marker>Cbeesky, 1961</marker>
<rawString>Cbeesky, N. 1961: Some methodological remarks on generative grammar. Word 17, 219-239</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Formal properties cf grammars. In: Luce- Bush- Galanter (ads) Handbook of mathematical psychology.</title>
<date>1963</date>
<publisher>Wiley,</publisher>
<location>New Ulric</location>
<marker>Chomsky, 1963</marker>
<rawString>Chomsky, N. 1963: Formal properties cf grammars. In: Luce- Bush- Galanter (ads) Handbook of mathematical psychology. Wiley, New Ulric</rawString>
</citation>
<citation valid="false">
<authors>
<author>N Chossky</author>
</authors>
<title>1fle: emarks on nomlaalization. In:</title>
<booktitle>Jacobs-Rosenbaum feedings in English Transformational G • Ginn,</booktitle>
<pages>221</pages>
<location>Waltham, Mass.184-</location>
<marker>Chossky, </marker>
<rawString>Chossky, N. 1fle: emarks on nomlaalization. In: Jacobs-Rosenbaum feedings in English Transformational G • Ginn, Waltham, Mass.184- 221</rawString>
</citation>
<citation valid="true">
<authors>
<author>Church</author>
</authors>
<title>Cs parsing strategies and closure.</title>
<date>1980</date>
<booktitle>Proceedings of the 18th Annual Meeting of the ACL</booktitle>
<volume>16</volume>
<pages>101--111</pages>
<marker>Church, 1980</marker>
<rawString>Church. 16 1980: Cs parsing strategies and closure. Proceedings of the 18th Annual Meeting of the ACL 101-111</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Iilenbarg</author>
</authors>
<title>Automata, languages, and machines.</title>
<date>1974</date>
<publisher>Academic Press,</publisher>
<location>New &amp;quot;torte</location>
<marker>Iilenbarg, 1974</marker>
<rawString>Iilenbarg, S. 1974: Automata, languages, and machines. Academic Press, New &amp;quot;torte</rawString>
</citation>
<citation valid="false">
<authors>
<author>G Gazdar</author>
</authors>
<title>1882: Phrase structure g In: Jacobson-Pullum (ads): The • Mature of Syntactic Representation. Reldel,</title>
<pages>131--1</pages>
<location>Dordrecht</location>
<marker>Gazdar, </marker>
<rawString>Gazdar, G. 1882: Phrase structure g In: Jacobson-Pullum (ads): The • Mature of Syntactic Representation. Reldel, Dordrecht 131-1E6</rawString>
</citation>
<citation valid="false">
<authors>
<author>J- H Creenbaus</author>
</authors>
<title>Cuirk 1970: Elicitation experiments in English: linguistic studios In usage and attitude.</title>
<location>Loners*, London</location>
<marker>Creenbaus, </marker>
<rawString>Creenbaus, J.- H. Cuirk 1970: Elicitation experiments in English: linguistic studios In usage and attitude. Loners*, London</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Greene</author>
</authors>
<date></date>
<publisher>Psycholinguistics. Penguin, Hersomdmworth</publisher>
<marker>Greene, </marker>
<rawString>Greene, J. 1E12: Psycholinguistics. Penguin, Hersomdmworth</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Barris</author>
</authors>
<title>Pros morpheme to utterance.</title>
<date>1946</date>
<journal>Language,</journal>
<volume>22</volume>
<pages>161--1</pages>
<marker>Barris, 1946</marker>
<rawString>Barris, Z. 1946: Pros morpheme to utterance. Language, 22, 161-1E3</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Barris</author>
</authors>
<title>Methods in Structural Linguistics. tniversity of</title>
<date>1951</date>
<publisher>Chicago Press</publisher>
<marker>Barris, 1951</marker>
<rawString>Barris, Z. 1951: Methods in Structural Linguistics. tniversity of Chicago Press</rawString>
</citation>
<citation valid="false">
<authors>
<author>M A Berrison</author>
</authors>
<title>878: Introduction to Perms/ Language Theory.AddisonWesley,</title>
<location>Rmadingr Wass.</location>
<marker>Berrison, </marker>
<rawString>Berrison, M. A. /878: Introduction to Perms/ Language Theory.AddisonWesley, Rmadingr Wass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hackett</author>
</authors>
<title>Two sodels of grassatics1 description.</title>
<date>1954</date>
<journal>Word</journal>
<volume>1</volume>
<pages>210--231</pages>
<marker>Hackett, 1954</marker>
<rawString>Hackett, C. 1954: Two sodels of grassatics1 description. Word 1C, 210-231</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hockett</author>
</authors>
<title>A manual of phonology.</title>
<date>1955</date>
<journal>International Journal of American Linguistics, Memoir.</journal>
<volume>11</volume>
<marker>Hockett, 1955</marker>
<rawString>Hockett, C. 1955: A manual of phonology. International Journal of American Linguistics, Memoir. 11</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Itkonen</author>
</authors>
<title>The concept of linguistic intuition. In: Coulsans (ed): A Festschrift for Native Speaker. Mouton, The Hague,</title>
<date>1981</date>
<pages>21--14</pages>
<marker>Itkonen, 1981</marker>
<rawString>Itkonen, E. 1981: The concept of linguistic intuition. In: Coulsans (ed): A Festschrift for Native Speaker. Mouton, The Hague, I21-14C</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Kleene</author>
</authors>
<title>Representation c/ events in nerve nets and finite automata. In: Shannon - McCarthy (ode): Automata studies.</title>
<date>1956</date>
<pages>3--41</pages>
<publisher>Princeton University Preen</publisher>
<marker>Kleene, 1956</marker>
<rawString>Kleene, S. C. 1956: Representation c/ events in nerve nets and finite automata. In: Shannon - McCarthy (ode): Automata studies. Princeton University Preen 3-41</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Lamb</author>
</authors>
<title>Outline c/ stratificational grammar.</title>
<date>1966</date>
<publisher>Georgetown University Press,</publisher>
<location>Washington D.C.</location>
<marker>Lamb, 1966</marker>
<rawString>Lamb, S. M. 1966: Outline c/ stratificational grammar. Georgetown University Press, Washington D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Levi-Strauss</author>
</authors>
<title>Anthropolcgie structural..</title>
<date>1958</date>
<location>Plon, Paris.</location>
<marker>Levi-Strauss, 1958</marker>
<rawString>Levi-Strauss, C. 1958: Anthropolcgie structural.. Plon, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Lieber</author>
</authors>
<title>On the Crganization of the Lexicon.</title>
<date>1981</date>
<publisher>IULC</publisher>
<marker>Lieber, 1981</marker>
<rawString>Lieber, R. 1981: On the Crganization of the Lexicon. IULC</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Marcus</author>
</authors>
<title>1564: Gr tici si autosate finite. Editure Academie&apos;,</title>
<location>Bucharest</location>
<marker>Marcus, </marker>
<rawString>Marcus, S. 1564: Gr tici si autosate finite. Editure Academie&apos;, Bucharest</rawString>
</citation>
<citation valid="true">
<authors>
<author>L E Narks</author>
</authors>
<title>Scaling of grammaticalness of self-embedded English sentences.</title>
<date>1968</date>
<journal>Verbal Learning 8 Verbal Behavior</journal>
<volume>7</volume>
<pages>965--461</pages>
<marker>Narks, 1968</marker>
<rawString>Narks, L. E. 1968 Scaling of grammaticalness of self-embedded English sentences. Verbal Learning 8 Verbal Behavior 7, 965-461</rawString>
</citation>
<citation valid="true">
<authors>
<author>N J Matthews</author>
</authors>
<title>Are the grassatical sentences of a language a recursive set?</title>
<date>1979</date>
<journal>Synth...</journal>
<volume>40</volume>
<pages>209--224</pages>
<marker>Matthews, 1979</marker>
<rawString>Matthews, N.J. 1979: Are the grassatical sentences of a language a recursive set? Synth... 40, 209-224</rawString>
</citation>
<citation valid="true">
<authors>
<author>V S McCullogh</author>
</authors>
<title>A logical calculus of the ideas immanent in nervous activity.</title>
<date>1943</date>
<journal>Bulletin of mathematical biophysics</journal>
<volume>5</volume>
<pages>115--133</pages>
<marker>McCullogh, 1943</marker>
<rawString>McCullogh, V. S. - V. Pitts 1943: A logical calculus of the ideas immanent in nervous activity. Bulletin of mathematical biophysics 5, 115-133</rawString>
</citation>
<citation valid="false">
<authors>
<author>R McNaughton</author>
</authors>
<title>Paport 1968: Tho syntactic Ronald of a regular event. In Arbit (ed): Algebraic theory el machines, languages, and semigroups.</title>
<pages>291--312</pages>
<publisher>Academic Press,</publisher>
<location>New York</location>
<marker>McNaughton, </marker>
<rawString>McNaughton, R. - S. Paport 1968: Tho syntactic Ronald of a regular event. In Arbit (ed): Algebraic theory el machines, languages, and semigroups. Academic Press, New York 291-312</rawString>
</citation>
<citation valid="false">
<authors>
<author>R</author>
</authors>
<title>Pspert 1611: Counter-free automata. Seseerct Monograph no.</title>
<publisher>EF., MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<marker>R, </marker>
<rawString>McNaughton. R - S. Pspert 1611: Counter-free automata. Seseerct Monograph no. EF., MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="false">
<authors>
<author>G A Willer</author>
</authors>
<title>heard 1964: Free recall of self-embedded English sentences.</title>
<journal>Information</journal>
<volume>8</volume>
<pages>292--302</pages>
<marker>Willer, </marker>
<rawString>Willer, G. A. - S. heard 1964: Free recall of self-embedded English sentences. Information 8 Control 7, 292-302</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Moore</author>
</authors>
<title>Gedanken-experisents on sequential machines. In: Shannon -McCarthy (eds): Automata studies.</title>
<date>1956</date>
<pages>129--152</pages>
<publisher>Princeton University Press</publisher>
<marker>Moore, 1956</marker>
<rawString>Moore, E. F. 1956: Gedanken-experisents on sequential machines. In: Shannon -McCarthy (eds): Automata studies. Princeton University Press 129-152</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Neveyer</author>
</authors>
<title>Linguistic ttecry in America. Acsdeele Preset</title>
<date>1980</date>
<location>New York</location>
<marker>Neveyer, 1980</marker>
<rawString>Neveyer, F. J. 1980: Linguistic ttecry in America. Acsdeele Preset New York</rawString>
</citation>
<citation valid="true">
<authors>
<author>G</author>
<author>G Gezdar</author>
</authors>
<title>Natural languases andcotext free lang ges.Linguisticsn and</title>
<date>1952</date>
<journal>Philosophy</journal>
<volume>4</volume>
<pages>471--504</pages>
<marker>G, Gezdar, 1952</marker>
<rawString>Fullum. G. and G. Gezdar 1952: Natural languases andcotext free lang ges.Linguisticsn and Philosophy 4, 471-504</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Putnam</author>
</authors>
<title>Some issues in the thecry of grammar.</title>
<date>1961</date>
<booktitle>Proc. Symposia in Applied Mathematics</booktitle>
<marker>Putnam, 1961</marker>
<rawString>Putnam, B. 1961: Some issues in the thecry of grammar. Proc. Symposia in Applied Mathematics 1951</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A Reich</author>
</authors>
<title>The finiteness cl natural languages. Language 4E,</title>
<date>1969</date>
<pages>831--843</pages>
<marker>Reich, 1969</marker>
<rawString>Reich, P.A. 1969: The finiteness cl natural languages. Language 4E, 831-843</rawString>
</citation>
<citation valid="true">
<authors>
<author>R N Robins</author>
</authors>
<date>1959</date>
<booktitle>In defense of IP. Trans. Fhilol. Soc. 11E-144</booktitle>
<marker>Robins, 1959</marker>
<rawString>Robins, R.N. 1959: In defense of IP. Trans. Fhilol. Soc. 11E-144</rawString>
</citation>
<citation valid="true">
<authors>
<author>M losers</author>
</authors>
<title>The theory of recursive functions and effective computability.</title>
<date>1967</date>
<publisher>McGraw-Bill,</publisher>
<location>New &apos;York</location>
<marker>losers, 1967</marker>
<rawString>losers, M. 1967: The theory of recursive functions and effective computability. McGraw-Bill, New &apos;York</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Sounds</author>
</authors>
<title>A g tical characterisaticn of exronential-tise Languages.</title>
<date>1915</date>
<booktitle>Proc. 16th Symcosius on Switching Theory and Automata</booktitle>
<pages>135--143</pages>
<marker>Sounds, 1915</marker>
<rawString>Sounds, W. 1915: A g tical characterisaticn of exronential-tise Languages. Proc. 16th Symcosius on Switching Theory and Automata 135-143</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Selomaa</author>
</authors>
<title>Forsal Languages.</title>
<date>1973</date>
<publisher>Academic Press,</publisher>
<location>New York</location>
<marker>Selomaa, 1973</marker>
<rawString>Selomaa, A. 1973: Forsal Languages. Academic Press, New York</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J Sullivan</author>
</authors>
<title>Syntax and Linguistic tics in strstificstionel theory. In: lirthMoravcsik Cede) Current approaches to syntax.</title>
<date>1950</date>
<pages>301--327</pages>
<publisher>Acadesic Press,</publisher>
<location>New York</location>
<marker>Sullivan, 1950</marker>
<rawString>Sullivan, W. J. 1950: Syntax and Linguistic tics in strstificstionel theory. In: lirthMoravcsik Cede) Current approaches to syntax. Acadesic Press, New York 301-327</rawString>
</citation>
<citation valid="true">
<authors>
<author>V H</author>
</authors>
<title>The depth hypothesis. Jskobson (ed):</title>
<date>1911</date>
<booktitle>Proc. of Sysposia in Applied Mattematice</booktitle>
<volume>12</volume>
<pages>130--138</pages>
<marker>H, 1911</marker>
<rawString>leave. V. H. 1911: The depth hypothesis. Jskobson (ed): Proc. of Sysposia in Applied Mattematice 12, 130-138</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>