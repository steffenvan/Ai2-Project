<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000060">
<title confidence="0.672302">
Large-Margin Learning of Submodular Summarization Models
</title>
<author confidence="0.986844">
Ruben Sipos Pannaga Shivaswamy Thorsten Joachims
</author>
<affiliation confidence="0.9996005">
Dept. of Computer Science Dept. of Computer Science Dept. of Computer Science
Cornell University Cornell University Cornell University
</affiliation>
<address confidence="0.524539">
Ithaca, NY 14853 USA Ithaca, NY 14853 USA Ithaca, NY 14853 USA
</address>
<email confidence="0.996921">
rs@cs.cornell.edu pannaga@cs.cornell.edu tj@cs.cornell.edu
</email>
<sectionHeader confidence="0.994731" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999719888888889">
In this paper, we present a supervised
learning approach to training submodu-
lar scoring functions for extractive multi-
document summarization. By taking a
structured prediction approach, we pro-
vide a large-margin method that directly
optimizes a convex relaxation of the de-
sired performance measure. The learning
method applies to all submodular summa-
rization methods, and we demonstrate its
effectiveness for both pairwise as well as
coverage-based scoring functions on mul-
tiple datasets. Compared to state-of-the-
art functions that were tuned manually, our
method significantly improves performance
and enables high-fidelity models with num-
ber of parameters well beyond what could
reasonably be tuned by hand.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960423076924">
Automatic document summarization is the prob-
lem of constructing a short text describing the
main points in a (set of) document(s). Exam-
ple applications range from generating short sum-
maries of news articles, to presenting snippets for
URLs in web-search. In this paper we focus on
extractive multi-document summarization, where
the final summary is a subset of the sentences
from multiple input documents. In this way, ex-
tractive summarization avoids the hard problem
of generating well-formed natural-language sen-
tences, since only existing sentences from the in-
put documents are presented as part of the sum-
mary.
A current state-of-the-art method for document
summarization was recently proposed by Lin and
Bilmes (2010), using a submodular scoring func-
tion based on inter-sentence similarity. On the one
hand, this scoring function rewards summaries
that are similar to many sentences in the origi-
nal documents (i.e. promotes coverage). On the
other hand, it penalizes summaries that contain
sentences that are similar to each other (i.e. dis-
courages redundancy). While obtaining the exact
summary that optimizes the objective is computa-
tionally hard, they show that a greedy algorithm
is guaranteed to compute a good approximation.
However, their work does not address how to
select a good inter-sentence similarity measure,
leaving this problem as well as selecting an appro-
priate trade-off between coverage and redundancy
to manual tuning.
To overcome this problem, we propose a su-
pervised learning method that can learn both
the similarity measure as well as the cover-
age/reduncancy trade-off from training data. Fur-
thermore, our learning algorithm is not limited to
the model of Lin and Bilmes (2010), but applies to
all monotone submodular summarization models.
Due to the diminishing-returns property of mono-
tone submodular set functions and their computa-
tional tractability, this class of functions provides
a rich space for designing summarization meth-
ods. To illustrate the generality of our approach,
we also provide experiments for a coverage-based
model originally developed for diversified infor-
mation retrieval (Swaminathan et al., 2009).
In general, our method learns a parameterized
monotone submodular scoring function from su-
pervised training data, and its implementation is
available for download.1 Given a set of docu-
ments and their summaries as training examples,
</bodyText>
<footnote confidence="0.984696">
1http://www.cs.cornell.edu/˜rs/sfour/
</footnote>
<page confidence="0.940128">
224
</page>
<note confidence="0.9776765">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 224–233,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999891407407407">
we formulate the learning problem as a struc-
tured prediction problem and derive a maximum-
margin algorithm in the structural support vec-
tor machine (SVM) framework. Note that, un-
like other learning approaches, our method does
not require a heuristic decomposition of the learn-
ing task into binary classification problems (Ku-
piec et al., 1995), but directly optimizes a struc-
tured prediction. This enables our algorithm to di-
rectly optimize the desired performance measure
(e.g. ROUGE) during training. Furthermore, our
method is not limited to linear-chain dependen-
cies like (Conroy and O’leary, 2001; Shen et al.,
2007), but can learn any monotone submodular
scoring function.
This ability to easily train summarization mod-
els makes it possible to efficiently tune models
to various types of document collections. In par-
ticular, we find that our learning method can re-
liably tune models with hundreds of parameters
based on a training set of about 30 examples.
This increases the fidelity of models compared
to their hand-tuned counterparts, showing sig-
nificantly improved empirical performance. We
provide a detailed investigation into the sources
of these improvements, identifying further direc-
tions for research.
</bodyText>
<sectionHeader confidence="0.999674" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999935876712329">
Work on extractive summarization spans a large
range of approaches. Starting with unsupervised
methods, one of the widely known approaches
is Maximal Marginal Relevance (MMR) (Car-
bonell and Goldstein, 1998). It uses a greedy ap-
proach for selection and considers the trade-off
between relevance and redundancy. Later it was
extended (Goldstein et al., 2000) to support multi-
document settings by incorporating additional in-
formation available in this case. Good results can
be achieved by reformulating this as a knapsack
packing problem and solving it using dynamic
programing (McDonald, 2007). Alternatively, we
can use annotated phrases as textual units and se-
lect a subset that covers most concepts present
in the input (Filatova and Hatzivassiloglou, 2004)
(which can also be achieved by our coverage scor-
ing function if it is extended with appropriate fea-
tures).
A popular stochastic graph-based summariza-
tion method is LexRank (Erkan and Radev, 2004).
It computes sentence importance based on the
concept of eigenvector centrality in a graph of
sentence similarities. Similarly, TextRank (Mi-
halcea and Tarau, 2004) is also graph based rank-
ing system for identification of important sen-
tences in a document by using sentence similar-
ity and PageRank (Brin and Page, 1998). Sen-
tence extraction can also be implemented using
other graph based scoring approaches (Mihalcea,
2004) such as HITS (Kleinberg, 1999) and po-
sitional power functions. Graph based methods
can also be paired with clustering such as in Col-
labSum (Wan et al., 2007). This approach first
uses clustering to obtain document clusters and
then uses graph based algorithm for sentence se-
lection which includes inter and intra-document
sentence similarities. Another clustering-based
algorithm (Nomoto and Matsumoto, 2001) is a
diversity-based extension of MMR that finds di-
versity by clustering and then proceeds to reduce
redundancy by selecting a representative for each
cluster.
The manually tuned sentence pairwise model
(Lin and Bilmes, 2010; Lin and Bilmes, 2011) we
took inspiration from is based on budgeted sub-
modular optimization. A summary is produced
by maximizing an objective function that includes
coverage and redundancy terms. Coverage is de-
fined as the sum of sentence similarities between
the selected summary and the rest of the sen-
tences, while redundancy is the sum of pairwise
intra-summary sentence similarities. Another ap-
proach based on submodularity (Qazvinian et al.,
2010) relies on extracting important keyphrases
from citation sentences for a given paper and us-
ing them to build the summary.
In the supervised setting, several early methods
(Kupiec et al., 1995) made independent binary de-
cisions whether to include a particular sentence
in the summary or not. This ignores dependen-
cies between sentences and can result in high re-
dundancy. The same problem arises when using
learning-to-rank approaches such as ranking sup-
port vector machines, support vector regression
and gradient boosted decision trees to select the
most relevant sentences for the summary (Metzler
and Kanungo, 2008).
Introducing some dependencies can improve
the performance. One limited way of introduc-
ing dependencies between sentences is by using a
linear-chain HMM. The HMM is assumed to pro-
duce the summary by having a chain transitioning
</bodyText>
<page confidence="0.997232">
225
</page>
<bodyText confidence="0.999950806451613">
between summarization and non-summarization
states (Conroy and O’leary, 2001) while travers-
ing the sentences in a document. A more expres-
sive approach is using a CRF for sequence label-
ing (Shen et al., 2007) which can utilize larger and
not necessarily independent feature spaces. The
disadvantage of using linear chain models, how-
ever, is that they represent the summary as a se-
quence of sentences. Dependencies between sen-
tences that are far away from each other cannot
be modeled efficiently. In contrast to such lin-
ear chain models, our approach on submodular
scoring functions can model long-range depen-
dencies. In this way our method can use proper-
ties of the whole summary when deciding which
sentences to include in it.
More closely related to our work is that of Li
et al. (2009). They use the diversified retrieval
method proposed in Yue and Joachims (2008) for
document summarization. Moreover, they assume
that subtopic labels are available so that additional
constraints for diversity, coverage and balance can
be added to the structural SVM learning prob-
lem. In contrast, our approach does not require the
knowledge of subtopics (thus allowing us to ap-
ply it to a wider range of tasks) and avoids adding
additional constraints (simplifying the algorithm).
Furthermore, it can use different submodular ob-
jective functions, for example word coverage and
sentence pairwise models described later in this
paper.
Another closely related work also takes a max-
margin discriminative learning approach in the
structural SVM framework (Berg-Kirkpatrick et
al., 2011) or by using MIRA (Martins and Smith,
2009) to learn the parameters for summarizing
a set of documents. However, they do not con-
sider submodular functions, but instead solve an
Integer Linear Program (ILP) or an approxima-
tion thereof. The ILP encodes a compression
model where arbitrary parts of the parse trees
of sentences in the summary can be cut and re-
moved. This allows them to select parts of sen-
tences and yet preserve some gramatical struc-
ture. Their work focuses on learning a particular
compression model based on ILP inference, while
our work explores learning a general and large
class of sentence selection models using submod-
ular optimization. The third notable approach
uses SEARN (Daum´e, 2006) to learn parameters
for joint summarization and compression model,
however it uses vine-growth model and employs
search to to find the best policy which is then used
to generate a summary.
A specific subclass of submodular (but not
monotone) functions are defined by Determinan-
tal Point Processes (DPPs) (Kulesza and Taskar,
2011). While they provide an elegant probabilis-
tic interpretation of the resulting summarization
models, the lack of monotonicity means that no
efficient approximation algorithms are known for
computing the highest-scoring summary.
</bodyText>
<sectionHeader confidence="0.985429" genericHeader="method">
3 Submodular document summarization
</sectionHeader>
<bodyText confidence="0.999780777777778">
In this section, we illustrate how document sum-
marization can be addressed using submodular set
functions. The set of documents to be summa-
rized is split into a set of individual sentences
x = {s1, ..., s,,,}. The summarization method
then selects a subset y� ⊆ x of sentences that max-
imizes a given scoring function Fx : 2x → R
subject to a budget constraint (e.g. less than B
characters).
</bodyText>
<equation confidence="0.9712585">
y� = arg max Fx(y) s.t. |y |≤ B (1)
yCx
</equation>
<bodyText confidence="0.976947615384615">
In the following we restrict the admissible scoring
functions F to be submodular.
Definition 1. Given a set x, a function F : 2x →
R is submodular ifffor all u ∈ U and all sets s
and t such that s ⊆ t ⊆ x, we have,
F(s ∪ {u}) − F(s) ≥ F(t ∪ {u}) − F(t).
Intuitively, this definition says that adding u to
a subset s of t increases f at least as much as
adding it to t. Using two specific submodular
functions as examples, the following sections il-
lustrate how this diminishing returns property nat-
urally reflects the trade-off between maximizing
coverage while minimizing redundancy.
</bodyText>
<subsectionHeader confidence="0.999359">
3.1 Pairwise scoring function
</subsectionHeader>
<bodyText confidence="0.9996625">
The first submodular scoring function we con-
sider was proposed by Lin and Bilmes (2010) and
is based on a model of pairwise sentence similar-
ities. It scores a summary y using the following
function, which Lin and Bilmes (2010) show is
submodular:
</bodyText>
<equation confidence="0.969629">
Fx(y) = � σ(i, j) − λ � σ(i,j). (2)
iEx\y,jEy i,jEy:i=,4j
</equation>
<page confidence="0.996407">
226
</page>
<figureCaption confidence="0.999692666666667">
Figure 1: Illustration of the pairwise model. Not all
edges are shown for clarity purposes. Edge thickness
denotes the similarity score.
</figureCaption>
<bodyText confidence="0.999883466666667">
In the above equation, σ(i, j) &gt; 0 denotes a mea-
sure of similarity between pairs of sentences i and
j. The first term in Eq. 2 is a measure of how simi-
lar the sentences included in summary y are to the
other sentences in x. The second term penalizes
y by how similar its sentences are to each other.
λ &gt; 0 is a scalar parameter that trades off be-
tween the two terms. Maximizing Fx(y) amounts
to increasing the similarity of the summary to ex-
cluded sentences while minimizing repetitions in
the summary. An example is illustrated in Figure
1. In the simplest case, σ(i, j) may be the TFIDF
(Salton and Buckley, 1988) cosine similarity, but
we will show later how to learn sophisticated sim-
ilarity functions.
</bodyText>
<subsectionHeader confidence="0.999728">
3.2 Coverage scoring function
</subsectionHeader>
<bodyText confidence="0.999992923076923">
A second scoring function we consider was
first proposed for diversified document retrieval
(Swaminathan et al., 2009; Yue and Joachims,
2008), but it naturally applies to document sum-
marization as well (Li et al., 2009). It is based on
a notion of word coverage, where each word v has
some importance weight ω(v) &gt; 0. A summary
y covers a word if at least one of its sentences
contains the word. The score of a summary is
then simply the sum of the word weights its cov-
ers (though we could also include a concave dis-
count function that rewards covering a word mul-
tiple times (Raman et al., 2011)):
</bodyText>
<equation confidence="0.9968365">
Fx(y) = � ω(v). (3)
VEV(y)
</equation>
<bodyText confidence="0.997978">
In the above equation, V (y) denotes the union of
all words in y. This function is analogous to a
maximum coverage problem, which is known to
be submodular (Khuller et al., 1999).
</bodyText>
<figureCaption confidence="0.9989875">
Figure 2: Illustration of the coverage model. Word
border thickness represents importance.
</figureCaption>
<bodyText confidence="0.996336636363636">
An example of how a summary is scored is il-
lustrated in the Figure 2. Analogous to the defini-
tion of similarity σ(i, j) in the pairwise model, the
choice of the word importance function ω(v) is
crucial in the coverage model. A simple heuristic
is to weigh words highly that occur in many sen-
tences of x, but in few other documents (Swami-
nathan et al., 2009). However, we will show in the
following how to learn ω(v) from training data.
Algorithm 1 Greedy algorithm for finding the
best summary y� given a scoring function Fx(y).
</bodyText>
<equation confidence="0.9795775">
Parameter: r &gt; 0.
y�+— 0
A + —x
while A =� 0 do
k +— arg max
lEA Fx(y U {l}) − Fx(y)
(cl)r
if ck+EzEO cz GB and Fx(9U{k})−Fx(9)&gt;
0 then
y�+— y� U {k}
end if
A +— A\{k}
</equation>
<bodyText confidence="0.427556">
end while
</bodyText>
<subsectionHeader confidence="0.999474">
3.3 Computing a Summary
</subsectionHeader>
<bodyText confidence="0.99995875">
Computing the summary that maximizes either of
the two scoring functions from above (i.e. Eqns.
(2) and (3)) is NP-hard (McDonald, 2007). How-
ever, it is known that the greedy algorithm 1 can
achieve a 1 − 1/e approximation to the optimum
solution for any linear budget constraint (Lin and
Bilmes, 2010; Khuller et al., 1999). Even further,
this algorithm provides a 1 − 1/e approximation
for any monotone submodular scoring function.
The algorithm starts with an empty summariza-
tion. In each step, a sentence is added to the sum-
mary that results in the maximum relative increase
</bodyText>
<page confidence="0.985548">
227
</page>
<bodyText confidence="0.999952473684211">
of the objective. The increase is relative to the
amount of budget that is used by the added sen-
tence. The algorithm terminates when the budget
B is reached.
Note that the algorithm has a parameter r in
the denominator of the selection rule, which Lin
and Bilmes (2010) report to have some impact
on performance. In the algorithm, ci represents
the cost of the sentence (i.e., length). Thus, the
algorithm actually selects sentences with large
marginal unity relative to their length (trade-off
controlled by the parameter r). Selecting r to be
less than 1 gives more importance to “information
density” (i.e. sentences that have a higher ratio
of score increase per length). The 1 − e1 greedy
approximation guarantee holds despite this addi-
tional parameter (Lin and Bilmes, 2010). More
details on our choice of r and its effects are pro-
vided in the experiments section.
</bodyText>
<sectionHeader confidence="0.98112" genericHeader="method">
4 Learning algorithm
</sectionHeader>
<bodyText confidence="0.999976555555556">
In this section, we propose a supervised learning
method for training a submodular scoring func-
tion to produce desirable summaries. In particu-
lar, for the pairwise and the coverage model, we
show how to learn the similarity function σ(i, j)
and the word importance weights ω(v) respec-
tively. In particular, we parameterize σ(i, j) and
ω(v) using a linear model, allowing that each de-
pends on the full set of input sentences x:
</bodyText>
<equation confidence="0.718178">
σx(i,j) = wTφpx(i,j) ωx(v) = wTφcx(v). (4)
</equation>
<bodyText confidence="0.999946769230769">
In the above equations, w is a weight vector that
is learned, and φpx(i, j) and φcx(v) are feature vec-
tors. In the pairwise model, φpx(i, j) may include
feature like the TFIDF cosine between i and j or
the number of words from the document titles that
i and j share. In the coverage model, φcx(v) may
include features like a binary indicator of whether
v occurs in more than 10% of the sentences in x
or whether v occurs in the document title.
We propose to learn the weights following a
large-margin framework using structural SVMs
(Tsochantaridis et al., 2005). Structural SVMs
learn a discriminant function
</bodyText>
<equation confidence="0.981936">
h(x) = arg max wT*(x, y) (5)
yEY
</equation>
<bodyText confidence="0.998246714285714">
that predicts a structured output y given a (pos-
sibly also structured) input x. *(x, y) ∈ RN is
called the joint feature-map between input x and
output y. Note that both submodular scoring func-
tion in Eqns. (2) and (3) can be brought into the
form wT *(x, y) for the linear parametrization in
Eq. (6) and (7):
</bodyText>
<equation confidence="0.9996405">
*p(x, y)= E φpx(i,j) − λ E φpx(i,j), (6)
iEx\y,jEy i,jEy:i=,4j
*c(x, y)= E φcx(v). (7)
vEV (y)
</equation>
<bodyText confidence="0.9941330625">
After this transformation, it is easy to see that
computing the maximizing summary in Eq. (1)
and the structural SVM prediction rule in Eq. (5)
are equivalent.
To learn the weight vector w, structural SVMs
require training examples (x1, y1), ..., (xn, yn) of
input/output pairs. In document summarization,
however, the “correct” extractive summary is typ-
ically not known. Instead, training documents
xi are typically annotated with multiple manual
(non-extractive) summaries (denoted by Y i). To
determine a single extractive target summary yi
for training, we find the extractive summary that
(approximately) optimizes ROUGE score – or
some other loss function A(Yi, y) – with respect
to Yi.
</bodyText>
<equation confidence="0.9839125">
yi = argmin A(Y i, y) (8)
yEY
</equation>
<bodyText confidence="0.999994384615385">
We call the yi determined in this way the “target”
summary for xi. Note that yi is a greedily con-
structed approximate target summary based on its
proximity to Yi via A. Because of this, we will
learn a model that can predict approximately good
summaries yi from xi. However, we believe that
most of the score difference between manual sum-
maries and yi (as explored in the experiments sec-
tion) is due to it being an extractive summary and
not due to greedy construction.
Following the structural SVM approach, we
can now formulate the problem of learning w as
the following quadratic program (QP):
</bodyText>
<equation confidence="0.9831786">
2 kwk2 + 1 n i=1
n
ξi (9)
s.t. wT*(xi, yi) − wT*(xi, 9i) ≥
A(�yi, Yi) − ξi, ∀9i =6 yi, ∀1 ≤ i ≤ n.
</equation>
<bodyText confidence="0.994182">
The above formulation ensures that the scor-
ing function with the target summary (i.e.
wT*(xi, yi)) is larger than the scoring function
</bodyText>
<equation confidence="0.407743">
min
w,ξ&gt;0
</equation>
<page confidence="0.928902">
228
</page>
<bodyText confidence="0.533668666666667">
Algorithm 2 Cutting-plane algorithm for solving
the learning optimization problem.
Parameter: desired tolerance c &gt; 0.
</bodyText>
<equation confidence="0.921810636363636">
8i : Wi ;
repeat
for 8i do
y� arg max wT,&amp;(xi, y) + O(Yi, y)
y
if wT,&amp;(xi,yi) + c G wT,&amp;(xi, 9) +
O(Yi, y) −�i then
Wi Wi U {y}
w solve QP (9) using constraints Wi
end if
end for
</equation>
<bodyText confidence="0.996665875">
until no Wi has changed during iteration
for any other summary yi (i.e., wT,&amp;(xi, 9i)).
The objective function learns a large-margin
weight vector w while trading it off with an upper
bound on the empirical loss. The two quantities
are traded off with a parameter C &gt; 0.
Even though the QP has exponentially many
constraints in the number of sentences in the in-
put documents, it can be solved approximately
in polynomial time via a cutting plane algorithm
(Tsochantaridis et al., 2005). The steps of the
cutting-plane algorithm are shown in Algorithm
2. In each iteration of the algorithm, for each
training document xi, a summary yi which most
violates the constraint in (9) is found. This is done
by finding
</bodyText>
<equation confidence="0.560366">
y� arg max wT,&amp;(xi, y) + O(Y i, y),
yEY
</equation>
<bodyText confidence="0.979364428571428">
for which we use a variant of the greedy algorithm
in Figure 1. After a violating constraint for each
training example is added, the resulting quadratic
program is solved. These steps are repeated until
all the constraints are satisfied to a required preci-
sion c.
Finally, special care has to be taken to appro-
priately define the loss function O given the dis-
parity of Yi and yi. Therefore, we first define an
intermediate loss function
OR(Y, y) = max(0, 1 − ROUGE1F(Y, y)),
based on the ROUGE-1 F score. To ensure that
the loss function is zero for the target label as de-
fined in (8), we normalized the above loss as be-
</bodyText>
<equation confidence="0.8894885">
low:
O(Yi, 9) = max(0, OR(Yi, 9) − OR(Yi, yi)),
</equation>
<bodyText confidence="0.999803727272727">
The loss O was used in our experiments. Thus
training a structural SVM with this loss aims to
maximize the ROUGE-1 F score with the man-
ual summaries provided in the training examples,
while trading it off with margin. Note that we
could also use a different loss function (as the
method is not tied to this particular choice), if we
had a different target evaluation metric. Finally,
once a w is obtained from structural SVM train-
ing, a predicted summary for a test document x
can be obtained from (5).
</bodyText>
<sectionHeader confidence="0.999553" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999813387096774">
In this section, we empirically evaluate the ap-
proach proposed in this paper. Following Lin and
Bilmes (2010), experiments were conducted on
two different datasets (DUC ’03 and ’04). These
datasets contain document sets with four manual
summaries for each set. For each document set,
we concatenated all the articles and split them
into sentences using the tool provided with the
’03 dataset. For the supervised setting we used
10 resamplings with a random 20/5/5 (’03) and
40/5/5 (’04) train/test/validation split. We deter-
mined the best C value in (9) using the perfor-
mance on each validation set and then report aver-
age performence over the corresponding test sets.
Baseline performance (the approach of Lin and
Bilmes (2010)) was computed using all 10 test
sets as a single test set. For all experiments and
datasets, we used r = 0.3 in the greedy algorithm
as recommended in Lin and Bilmes (2010) for the
’03 dataset. We find that changing r has only a
small influence on performance.2
The construction of features for learning is or-
ganized by word groups. The most trivial group
is simply all words (basic). Considering the prop-
erties of the words themselves, we constructed
several features from properties such as capital-
ized words, non-stop words and words of cer-
tain length (cap+stop+len). We obtained another
set of features from the most frequently occur-
ing words in all the articles (minmax). We also
considered the position of a sentence (containing
</bodyText>
<footnote confidence="0.817010666666667">
2Setting r to 1 and thus eliminating the non-linearity does
lower the score (e.g. to 0.38466 for the pairwise model on
DUC ’03 compared with the results on Figure 3).
</footnote>
<page confidence="0.998558">
229
</page>
<bodyText confidence="0.999971243243243">
the word) in the article as another feature (loca-
tion). All those word groups can then be further
refined by selecting different thresholds, weight-
ing schemes (e.g. TFIDF) and forming binned
variants of these features.
For the pairwise model we use cosine similar-
ity between sentences using only words in a given
word group during computation. For the word
coverage model we create separate features for
covering words in different groups. This gives us
fairly comparable feature strength in both mod-
els. The only further addition is the use of differ-
ent word coverage levels in the coverage model.
First we consider how well does a sentence cover
a word (e.g. a sentence with five instances of the
same word might cover it better than another with
only a single instance). And secondly we look at
how important it is to cover a word (e.g. if a word
appears in a large fraction of sentences we might
want to be sure to cover it). Combining those two
criteria using different thresholds we get a set of
features for each word. Our coverage features are
motivated from the approach of Yue and Joachims
(2008). In contrast, the hand-tuned pairwise base-
line uses only TFIDF weighted cosine similarity
between sentences using all words, following the
approach in Lin and Bilmes (2010).
The resulting summaries are evaluated using
ROUGE version 1.5.5 (Lin and Hovy, 2003). We
selected the ROUGE-1 F measure because it was
used by Lin and Bilmes (2010) and because it is
one of the commonly used performance scores in
recent work. However, our learning method ap-
plies to other performance measures as well. Note
that we use the ROUGE-1 F measure both for the
loss function during learning, as well as for the
evaluation of the predicted summaries.
</bodyText>
<subsectionHeader confidence="0.6518855">
5.1 How does learning compare to manual
tuning?
</subsectionHeader>
<bodyText confidence="0.9998851">
In our first experiment, we compare our super-
vised learning approach to the hand-tuned ap-
proach. The results from this experiment are sum-
marized in Figure 3. First, supervised training
of the pairwise model (Lin and Bilmes, 2010)
resulted in a statistically significant (p &lt; 0.05
using paired t-test) increase in performance on
both datasets compared to our reimplementation
of the manually tuned pairwise model. Note that
our reimplementation of the approach of Lin and
Bilmes (2010) resulted in slightly different per-
formance numbers than those reported in Lin and
Bilmes (2010) – better on DUC ’03 and somewhat
lower on DUC ’04, if evaluated on the same selec-
tion of test examples as theirs. We conjecture that
this is due to small differences in implementation
and/or preprocessing of the dataset. Furthermore,
as authors of Lin and Bilmes (2010) note in their
paper, the ’03 and ’04 datasets behave quite dif-
ferently.
</bodyText>
<table confidence="0.988676142857143">
model dataset ROUGE-1 F (stderr)
pairwise DUC ’03 0.3929 (0.0074)
coverage 0.3784 (0.0059)
hand-tuned 0.3571 (0.0063)
pairwise DUC ’04 0.4066 (0.0061)
coverage 0.3992 (0.0054)
hand-tuned 0.3935 (0.0052)
</table>
<figureCaption confidence="0.9901646">
Figure 3: Results obtained on DUC ’03 and ’04
datasets using the supervised models. Increase in per-
formance over the hand-tuned is statistically signifi-
cant (p &lt; 0.05) for the pairwise model on the both
datasets, but only on DUC ’03 for the coverage model.
</figureCaption>
<bodyText confidence="0.983482">
Figure 3 also reports the performance for
the coverage model as trained by our algorithm.
These results can be compared against those for
the pairwise model. Since we are using features
of comparable strength in both approaches, as
well as the same greedy algorithm and structural
SVM learning method, this comparison largely
reflects the quality of models themselves. On the
’04 dataset both models achieve the same perfor-
mance while on ’03 the pairwise model performs
significantly (p &lt; 0.05) better than the coverage
model.
Overall, the pairwise model appears to perform
slightly better than the coverage model with the
datasets and features we used. Therefore, we fo-
cus on the pairwise model in the following.
</bodyText>
<subsectionHeader confidence="0.999439">
5.2 How fast does the algorithm learn?
</subsectionHeader>
<bodyText confidence="0.993556">
Hand-tuned approaches have limited flexibility.
Whenever we move to a significantly different
collection of documents we have to reinvest time
to retune it. Learning can make this adaptation
to a new collection more automatic and faster –
especially since training data has to be collected
even for manual tuning.
Figure 4 evaluates how effectively the learn-
ing algorithm can make use of a given amount of
training data. In particular, the figure shows the
</bodyText>
<page confidence="0.995652">
230
</page>
<figureCaption confidence="0.9987048">
Figure 4: Learning curve for the pairwise model on
DUC ’04 dataset showing ROUGE-1 F scores for
different numbers of learning examples (logarithmic
scale). The dashed line represents the preformance of
the hand-tuned model.
</figureCaption>
<bodyText confidence="0.999928">
learning curve for our approach. Even with very
few training examples, the learning approach al-
ready outperforms the baseline. Furthermore, at
the maximum number of training examples avail-
able to us the curve still increases. We therefore
conjecture that more data would further improve
performance.
</bodyText>
<subsectionHeader confidence="0.992343">
5.3 Where is room for improvement?
</subsectionHeader>
<bodyText confidence="0.984364558139535">
To get a rough estimate of what is actually achiev-
able in terms of the final ROUGE-1 F score, we
looked at different “upper bounds” under vari-
ous scenarios (Figure 5). First, ROUGE score
is computed by using four manual summaries
from different assessors, so that we can estimate
inter-subject disagreement. If one computes the
ROUGE score of a held-out summary against the
remaining three summaries, the resulting perfor-
mance is given in the row labeled human of Fig-
ure 5. It provides a reasonable estimate of human
performance.
Second, in extractive summarization we re-
strict summaries to sentences from the documents
themselves, which is likely to lead to a reduc-
tion in ROUGE. To estimate this drop, we use the
greedy algorithm to select the extractive summary
that maximizes ROUGE on the test documents.
The resulting performance is given in the row ex-
tractive of Figure 5. On both dataset, the drop
in performance for this (approximately3) optimal
3We compared the greedy algorithm with exhaustive
search for up to three selected sentences (more than that
would take too long). In about half the cases we got the same
solution, in other cases the soultion was on average about 1%
extractive summary is about 10 points of ROUGE.
Third, we expect some drop in performance,
since our model may not be able to fit the optimal
extractive summaries due to a lack of expressive-
ness. This can be estimated by looking at train-
ing set performance, as reported in row model fit
of Figure 5. On both datasets, we see a drop of
about 5 points of ROUGE performance. Adding
more and better features might help the model fit
the data better.
Finally, a last drop in performance may come
from overfitting. The test set ROUGE scores are
given in the row prediction of Figure 5. Note that
the drop between training and test performance
is rather small, so overfitting is not an issue and
is well controlled in our algorithm. We therefore
conclude that increasing model fidelity seems like
a promising direction for further improvements.
</bodyText>
<table confidence="0.981537555555556">
bound dataset ROUGE-1 F
human DUC ’03 0.56235
extractive 0.45497
model fit 0.40873
prediction 0.39294
human DUC ’04 0.55221
extractive 0.45199
model fit 0.40963
prediction 0.40662
</table>
<figureCaption confidence="0.9568444">
Figure 5: Upper bounds on ROUGE-1 F scores: agree-
ment between manual summaries, greedily computed
best extractive summaries, best model fit on the train
set (using the best C value) and the test scores of the
pairwise model.
</figureCaption>
<subsectionHeader confidence="0.976824">
5.4 Which features are most useful?
</subsectionHeader>
<bodyText confidence="0.9665816">
To understand which features affected the final
performance of our approach, we assessed the
strength of each set of our features. In particu-
lar, we looked at how the final test score changes
when we removed certain features groups (de-
scribed in the beginning of Section 5) as shown
in Figure 6.
The most important group of features are the
basic features (pure cosine similarity between
sentences) since removing them results in the
largest drop in performance. However, other fea-
tures play a significant role too (i.e. only the ba-
sic ones are not enough to achieve good perfor-
below optimal confirming that greedy selection works quite
well.
</bodyText>
<page confidence="0.991969">
231
</page>
<bodyText confidence="0.999815230769231">
mance). This confirms that performance can be
improved by adding richer fatures instead of us-
ing only a single similarity score as in Lin and
Bilmes (2010). Using learning for these complex
model is essential, since hand-tuning is likely to
be intractable.
The second most important group of features
considering the drop in performance (i.e. loca-
tion) looks at positions of sentences in the arti-
cles. This makes intuitive sense because the first
sentences in news articles are usually packed with
information. The other three groups do not have a
significant impact on their own.
</bodyText>
<table confidence="0.872416555555556">
removed ROUGE-1 F
group
none 0.40662
basic 0.38681
all except basic 0.39723
location 0.39782
sent+doc 0.39901
cap+stop+len 0.40273
minmax 0.40721
</table>
<figureCaption confidence="0.9941765">
Figure 6: Effects of removing different feature groups
on the DUC ’04 dataset. Bold font marks significant
difference (p &lt; 0.05) when compared to the full pari-
wise model. The most important are basic similar-
ity features including all words (similar to (Lin and
Bilmes, 2010)). The last feature group actually low-
ered the score but is included in the model because we
only found this out later on DUC ’04 dataset.
</figureCaption>
<subsectionHeader confidence="0.5224745">
5.5 How important is it to train with
multiple summaries?
</subsectionHeader>
<bodyText confidence="0.99998903030303">
While having four manual summaries may be im-
portant for computing a reliable ROUGE score
for evaluation, it is not clear whether such an ap-
proach is the most efficient use of annotator re-
sources for training. In our final experiment, we
trained our method using only a single manual
summary for each set of documents. When us-
ing only a single manual summary, we arbitrarily
took the first one out of the provided four refer-
ence summaries and used only it to compute the
target label for training (instead of using average
loss towards all four of them). Otherwise, the ex-
perimental setup was the same as in the previous
subsections, using the pairwise model.
For DUC ’04, the ROUGE-1 F score obtained
using only a single summary per document set
was 0.4010, which is slightly but not significantly
lower than the 0.4066 obtained with four sum-
maries (as shown on Figure 3). Similarly, on DUC
’03 the performance drop from 0.3929 to 0.3838
was not significant as well.
Based on those results, we conjecture that hav-
ing more documents sets with only a single man-
ual summary is more useful for training than
fewer training examples with better labels (i.e.
multiple summaries). In both cases, we spend
approximately the same amount of effort (as the
summaries are the most expensive component of
the training data), however having more training
examples helps (according to the learning curve
presented before) while spending effort on multi-
ple summaries appears to have only minor benefit
for training.
</bodyText>
<sectionHeader confidence="0.999654" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999851055555555">
This paper presented a supervised learning ap-
proach to extractive document summarization
based on structual SVMs. The learning method
applies to all submodular scoring functions, rang-
ing from pairwise-similarity models to coverage-
based approaches. The learning problem is for-
mulated into a convex quadratic program and was
then solved approximately using a cutting-plane
method. In an empirical evaluation, the structural
SVM approach significantly outperforms conven-
tional hand-tuned models on the DUC ’03 and
’04 datasets. A key advantage of the learn-
ing approach is its ability to handle large num-
bers of features, providing substantial flexibility
for building high-fidelity summarization models.
Furthermore, it shows good control of overfitting,
making it possible to train models even with only
a few training examples.
</bodyText>
<sectionHeader confidence="0.998237" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99957125">
We thank Claire Cardie and the members of the
Cornell NLP Seminar for their valuable feedback.
This research was funded in part through NSF
Awards IIS-0812091 and IIS-0905467.
</bodyText>
<sectionHeader confidence="0.999258" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99878575">
T. Berg-Kirkpatrick, D. Gillick and D. Klein. Jointly
Learning to Extract and Compress. In Proceedings
of ACL, 2011.
S. Brin and L. Page. The Anatomy of a Large-Scale
</reference>
<page confidence="0.931078">
232
</page>
<reference confidence="0.999863404040404">
Hypertextual Web Search Engine. In Proceedings of
WWW, 1998.
J. Carbonell and J. Goldstein. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In Proceedings of SI-
GIR, 1998.
J. M. Conroy and D. P. O’leary. Text summarization via
hidden markov models. In Proceedings of SIGIR,
2001.
H. Daum´e III. Practical Structured Learning Tech-
niques for Natural Language Processing. Ph.D.
Thesis, 2006.
G. Erkan and D. R. Radev. LexRank: Graph-based
Lexical Centrality as Salience in Text Summariza-
tion. In Journal of Artificial Intelligence Research,
Vol. 22, 2004, pp. 457–479.
E. Filatova and V. Hatzivassiloglou. Event-Based Ex-
tractive Summarization. In Proceedings of ACL
Workshop on Summarization, 2004.
T. Finley and T. Joachims. Training structural SVMs
when exact inference is intractable. In Proceedings
of ICML, 2008.
D. Gillick and Y. Liu. A scalable global model for sum-
marization. In Proceedings of ACL Workshop on
Integer Linear Programming for Natural Language
Processing, 2009.
J. Goldstein, V. Mittal, J. Carbonell, and M.
Kantrowitz. Multi-document summarization by sen-
tence extraction. In Proceedings of NAACL-ANLP,
2000.
S. Khuller, A. Moss and J. Naor. The budgeted maxi-
mum coverage problem. In Information Processing
Letters, Vol. 70, Issue 1, 1999, pp. 39–45.
J. M. Kleinberg. Authoritative sources in a hyperlinked
environment. In Journal of the ACM, Vol. 46, Issue
5, 1999, pp. 604-632.
A. Kulesza and B. Taskar. Learning Determinantal
Point Processes. In Proceedings of UAI, 2011.
J. Kupiec, J. Pedersen, and F. Chen. A trainable docu-
ment summarizer. In Proceedings of SIGIR, 1995.
L. Li, Ke Zhou, G. Xue, H. Zha, and Y. Yu. Enhanc-
ing Diversity, Coverage and Balance for Summa-
rization through Structure Learning. In Proceedings
of WWW, 2009.
H. Lin and J. Bilmes. 2010. Multi-document summa-
rization via budgeted maximization of submodular
functions. In Proceedings of NAACL-HLT, 2010.
H. Lin and J. Bilmes. 2011. A Class of Submodu-
lar Functions for Document Summarization. In Pro-
ceedings of ACL-HLT, 2011.
C. Y. Lin and E. Hovy. Automatic evaluation of sum-
maries using N-gram co-occurrence statistics. In
Proceedings of NAACL, 2003.
F. T. Martins and N. A. Smith. Summarization with
a joint model for sentence extraction and compres-
sion. In Proceedings of ACL Workshop on Integer
Linear Programming for Natural Language Process-
ing, 2009.
R. McDonald. 2007. A Study of Global Inference Al-
gorithms in Multi-document Summarization. In Ad-
vances in Information Retrieval, Lecture Notes in
Computer Science, 2007, pp. 557–564.
D. Metzler and T. Kanungo. Machine learned sen-
tence selection strategies for query-biased summa-
rization. In Proceedings of SIGIR, 2008.
R. Mihalcea. 2004. Graph-based ranking algorithms
for sentence extraction, applied to text summa-
rization. In Proceedings of the ACL on Interactive
poster and demonstration sessions, 2004.
R. Mihalcea and P. Tarau. Textrank: Bringing order
into texts. In Proceedings of EMNLP, 2004.
T. Nomoto and Y. Matsumoto. A new approach to un-
supervised text summarization. In Proceedings of
SIGIR, 2001.
V. Qazvinian, D. R. Radev, and A. ¨Ozg¨ur. 2010. Cita-
tion Summarization Through Keyphrase Extraction.
In Proceedings of COLING, 2010.
K. Raman, T. Joachims and P. Shivaswamy. Structured
Learning of Two-Level Dynamic Rankings. In Pro-
ceedings of CIKM, 2011.
G. Salton and C. Buckley. Term-weighting approaches
in automatic text retrieval. In Information process-
ing and management, 1988, pp. 513–523.
D. Shen, J. T. Sun, H. Li, Q. Yang, and Z. Chen.
Document summarization using conditional ran-
dom fields. In Proceedings of IJCAI, 2007.
A. Swaminathan, C. V. Mathew and D. Kirovski.
Essential Pages. In Proceedings of WI-IAT, IEEE
Computer Society, 2009.
I. Tsochantaridis, T. Hofmann, T. Joachims and Y. Al-
tun. Large margin methods for structured and inter-
dependent output variables. In Journal of Machine
Learning Research, Vol. 6, 2005, pp. 1453-1484.
X. Wan, J. Yang, and J. Xiao. Collabsum: Exploit-
ing multiple document clustering for collaborative
single document summarizations. In Proceedings of
SIGIR, 2007.
Y. Yue and T. Joachims. Predicting diverse subsets us-
ing structural svms. In Proceedings of ICML, 2008.
</reference>
<page confidence="0.998958">
233
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.975063">
<title confidence="0.999767">Large-Margin Learning of Submodular Summarization Models</title>
<author confidence="0.999863">Ruben Sipos Pannaga Shivaswamy Thorsten Joachims</author>
<affiliation confidence="0.9999095">Dept. of Computer Science Dept. of Computer Science Dept. of Computer Science Cornell University Cornell University Cornell University</affiliation>
<address confidence="0.999843">Ithaca, NY 14853 USA Ithaca, NY 14853 USA Ithaca, NY 14853</address>
<email confidence="0.999387">rs@cs.cornell.edupannaga@cs.cornell.edutj@cs.cornell.edu</email>
<abstract confidence="0.99870052631579">In this paper, we present a supervised learning approach to training submodular scoring functions for extractive multidocument summarization. By taking a structured prediction approach, we provide a large-margin method that directly optimizes a convex relaxation of the desired performance measure. The learning method applies to all submodular summarization methods, and we demonstrate its effectiveness for both pairwise as well as coverage-based scoring functions on multiple datasets. Compared to state-of-theart functions that were tuned manually, our method significantly improves performance and enables high-fidelity models with number of parameters well beyond what could reasonably be tuned by hand.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Berg-Kirkpatrick</author>
<author>D Gillick</author>
<author>D Klein</author>
</authors>
<title>Jointly Learning to Extract and Compress.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<contexts>
<context position="9858" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="1496" endWordPosition="1499">re available so that additional constraints for diversity, coverage and balance can be added to the structural SVM learning problem. In contrast, our approach does not require the knowledge of subtopics (thus allowing us to apply it to a wider range of tasks) and avoids adding additional constraints (simplifying the algorithm). Furthermore, it can use different submodular objective functions, for example word coverage and sentence pairwise models described later in this paper. Another closely related work also takes a maxmargin discriminative learning approach in the structural SVM framework (Berg-Kirkpatrick et al., 2011) or by using MIRA (Martins and Smith, 2009) to learn the parameters for summarizing a set of documents. However, they do not consider submodular functions, but instead solve an Integer Linear Program (ILP) or an approximation thereof. The ILP encodes a compression model where arbitrary parts of the parse trees of sentences in the summary can be cut and removed. This allows them to select parts of sentences and yet preserve some gramatical structure. Their work focuses on learning a particular compression model based on ILP inference, while our work explores learning a general and large class o</context>
</contexts>
<marker>Berg-Kirkpatrick, Gillick, Klein, 2011</marker>
<rawString>T. Berg-Kirkpatrick, D. Gillick and D. Klein. Jointly Learning to Extract and Compress. In Proceedings of ACL, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brin</author>
<author>L Page</author>
</authors>
<title>The Anatomy of a Large-Scale Hypertextual Web Search Engine.</title>
<date>1998</date>
<booktitle>In Proceedings of WWW,</booktitle>
<contexts>
<context position="6244" citStr="Brin and Page, 1998" startWordPosition="931" endWordPosition="934"> and select a subset that covers most concepts present in the input (Filatova and Hatzivassiloglou, 2004) (which can also be achieved by our coverage scoring function if it is extended with appropriate features). A popular stochastic graph-based summarization method is LexRank (Erkan and Radev, 2004). It computes sentence importance based on the concept of eigenvector centrality in a graph of sentence similarities. Similarly, TextRank (Mihalcea and Tarau, 2004) is also graph based ranking system for identification of important sentences in a document by using sentence similarity and PageRank (Brin and Page, 1998). Sentence extraction can also be implemented using other graph based scoring approaches (Mihalcea, 2004) such as HITS (Kleinberg, 1999) and positional power functions. Graph based methods can also be paired with clustering such as in CollabSum (Wan et al., 2007). This approach first uses clustering to obtain document clusters and then uses graph based algorithm for sentence selection which includes inter and intra-document sentence similarities. Another clustering-based algorithm (Nomoto and Matsumoto, 2001) is a diversity-based extension of MMR that finds diversity by clustering and then pro</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>S. Brin and L. Page. The Anatomy of a Large-Scale Hypertextual Web Search Engine. In Proceedings of WWW, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carbonell</author>
<author>J Goldstein</author>
</authors>
<title>The use of MMR, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<contexts>
<context position="5176" citStr="Carbonell and Goldstein, 1998" startWordPosition="762" endWordPosition="766">ions. In particular, we find that our learning method can reliably tune models with hundreds of parameters based on a training set of about 30 examples. This increases the fidelity of models compared to their hand-tuned counterparts, showing significantly improved empirical performance. We provide a detailed investigation into the sources of these improvements, identifying further directions for research. 2 Related work Work on extractive summarization spans a large range of approaches. Starting with unsupervised methods, one of the widely known approaches is Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998). It uses a greedy approach for selection and considers the trade-off between relevance and redundancy. Later it was extended (Goldstein et al., 2000) to support multidocument settings by incorporating additional information available in this case. Good results can be achieved by reformulating this as a knapsack packing problem and solving it using dynamic programing (McDonald, 2007). Alternatively, we can use annotated phrases as textual units and select a subset that covers most concepts present in the input (Filatova and Hatzivassiloglou, 2004) (which can also be achieved by our coverage sc</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>J. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proceedings of SIGIR, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Conroy</author>
<author>D P O’leary</author>
</authors>
<title>Text summarization via hidden markov models.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<marker>Conroy, O’leary, 2001</marker>
<rawString>J. M. Conroy and D. P. O’leary. Text summarization via hidden markov models. In Proceedings of SIGIR, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
</authors>
<title>Practical Structured Learning Techniques for Natural Language Processing.</title>
<date>2006</date>
<tech>Ph.D. Thesis,</tech>
<marker>Daum´e, 2006</marker>
<rawString>H. Daum´e III. Practical Structured Learning Techniques for Natural Language Processing. Ph.D. Thesis, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Erkan</author>
<author>D R Radev</author>
</authors>
<title>LexRank: Graph-based Lexical Centrality as Salience in Text Summarization.</title>
<date>2004</date>
<journal>In Journal of Artificial Intelligence Research,</journal>
<volume>22</volume>
<pages>457--479</pages>
<contexts>
<context position="5925" citStr="Erkan and Radev, 2004" startWordPosition="880" endWordPosition="883">Goldstein et al., 2000) to support multidocument settings by incorporating additional information available in this case. Good results can be achieved by reformulating this as a knapsack packing problem and solving it using dynamic programing (McDonald, 2007). Alternatively, we can use annotated phrases as textual units and select a subset that covers most concepts present in the input (Filatova and Hatzivassiloglou, 2004) (which can also be achieved by our coverage scoring function if it is extended with appropriate features). A popular stochastic graph-based summarization method is LexRank (Erkan and Radev, 2004). It computes sentence importance based on the concept of eigenvector centrality in a graph of sentence similarities. Similarly, TextRank (Mihalcea and Tarau, 2004) is also graph based ranking system for identification of important sentences in a document by using sentence similarity and PageRank (Brin and Page, 1998). Sentence extraction can also be implemented using other graph based scoring approaches (Mihalcea, 2004) such as HITS (Kleinberg, 1999) and positional power functions. Graph based methods can also be paired with clustering such as in CollabSum (Wan et al., 2007). This approach fi</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G. Erkan and D. R. Radev. LexRank: Graph-based Lexical Centrality as Salience in Text Summarization. In Journal of Artificial Intelligence Research, Vol. 22, 2004, pp. 457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Filatova</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Event-Based Extractive Summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL Workshop on Summarization,</booktitle>
<contexts>
<context position="5729" citStr="Filatova and Hatzivassiloglou, 2004" startWordPosition="848" endWordPosition="851">nown approaches is Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998). It uses a greedy approach for selection and considers the trade-off between relevance and redundancy. Later it was extended (Goldstein et al., 2000) to support multidocument settings by incorporating additional information available in this case. Good results can be achieved by reformulating this as a knapsack packing problem and solving it using dynamic programing (McDonald, 2007). Alternatively, we can use annotated phrases as textual units and select a subset that covers most concepts present in the input (Filatova and Hatzivassiloglou, 2004) (which can also be achieved by our coverage scoring function if it is extended with appropriate features). A popular stochastic graph-based summarization method is LexRank (Erkan and Radev, 2004). It computes sentence importance based on the concept of eigenvector centrality in a graph of sentence similarities. Similarly, TextRank (Mihalcea and Tarau, 2004) is also graph based ranking system for identification of important sentences in a document by using sentence similarity and PageRank (Brin and Page, 1998). Sentence extraction can also be implemented using other graph based scoring approac</context>
</contexts>
<marker>Filatova, Hatzivassiloglou, 2004</marker>
<rawString>E. Filatova and V. Hatzivassiloglou. Event-Based Extractive Summarization. In Proceedings of ACL Workshop on Summarization, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Finley</author>
<author>T Joachims</author>
</authors>
<title>Training structural SVMs when exact inference is intractable.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML,</booktitle>
<marker>Finley, Joachims, 2008</marker>
<rawString>T. Finley and T. Joachims. Training structural SVMs when exact inference is intractable. In Proceedings of ICML, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gillick</author>
<author>Y Liu</author>
</authors>
<title>A scalable global model for summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL Workshop on Integer Linear Programming for Natural Language Processing,</booktitle>
<marker>Gillick, Liu, 2009</marker>
<rawString>D. Gillick and Y. Liu. A scalable global model for summarization. In Proceedings of ACL Workshop on Integer Linear Programming for Natural Language Processing, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldstein</author>
<author>V Mittal</author>
<author>J Carbonell</author>
<author>M Kantrowitz</author>
</authors>
<title>Multi-document summarization by sentence extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL-ANLP,</booktitle>
<contexts>
<context position="5326" citStr="Goldstein et al., 2000" startWordPosition="787" endWordPosition="790"> increases the fidelity of models compared to their hand-tuned counterparts, showing significantly improved empirical performance. We provide a detailed investigation into the sources of these improvements, identifying further directions for research. 2 Related work Work on extractive summarization spans a large range of approaches. Starting with unsupervised methods, one of the widely known approaches is Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998). It uses a greedy approach for selection and considers the trade-off between relevance and redundancy. Later it was extended (Goldstein et al., 2000) to support multidocument settings by incorporating additional information available in this case. Good results can be achieved by reformulating this as a knapsack packing problem and solving it using dynamic programing (McDonald, 2007). Alternatively, we can use annotated phrases as textual units and select a subset that covers most concepts present in the input (Filatova and Hatzivassiloglou, 2004) (which can also be achieved by our coverage scoring function if it is extended with appropriate features). A popular stochastic graph-based summarization method is LexRank (Erkan and Radev, 2004).</context>
</contexts>
<marker>Goldstein, Mittal, Carbonell, Kantrowitz, 2000</marker>
<rawString>J. Goldstein, V. Mittal, J. Carbonell, and M. Kantrowitz. Multi-document summarization by sentence extraction. In Proceedings of NAACL-ANLP, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Khuller</author>
<author>A Moss</author>
<author>J Naor</author>
</authors>
<title>The budgeted maximum coverage problem.</title>
<date>1999</date>
<booktitle>In Information Processing Letters,</booktitle>
<volume>70</volume>
<pages>39--45</pages>
<contexts>
<context position="14195" citStr="Khuller et al., 1999" startWordPosition="2258" endWordPosition="2261">mmarization as well (Li et al., 2009). It is based on a notion of word coverage, where each word v has some importance weight ω(v) &gt; 0. A summary y covers a word if at least one of its sentences contains the word. The score of a summary is then simply the sum of the word weights its covers (though we could also include a concave discount function that rewards covering a word multiple times (Raman et al., 2011)): Fx(y) = � ω(v). (3) VEV(y) In the above equation, V (y) denotes the union of all words in y. This function is analogous to a maximum coverage problem, which is known to be submodular (Khuller et al., 1999). Figure 2: Illustration of the coverage model. Word border thickness represents importance. An example of how a summary is scored is illustrated in the Figure 2. Analogous to the definition of similarity σ(i, j) in the pairwise model, the choice of the word importance function ω(v) is crucial in the coverage model. A simple heuristic is to weigh words highly that occur in many sentences of x, but in few other documents (Swaminathan et al., 2009). However, we will show in the following how to learn ω(v) from training data. Algorithm 1 Greedy algorithm for finding the best summary y� given a sc</context>
</contexts>
<marker>Khuller, Moss, Naor, 1999</marker>
<rawString>S. Khuller, A. Moss and J. Naor. The budgeted maximum coverage problem. In Information Processing Letters, Vol. 70, Issue 1, 1999, pp. 39–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Kleinberg</author>
</authors>
<title>Authoritative sources in a hyperlinked environment.</title>
<date>1999</date>
<journal>In Journal of the ACM,</journal>
<volume>46</volume>
<pages>604--632</pages>
<contexts>
<context position="6380" citStr="Kleinberg, 1999" startWordPosition="953" endWordPosition="954"> coverage scoring function if it is extended with appropriate features). A popular stochastic graph-based summarization method is LexRank (Erkan and Radev, 2004). It computes sentence importance based on the concept of eigenvector centrality in a graph of sentence similarities. Similarly, TextRank (Mihalcea and Tarau, 2004) is also graph based ranking system for identification of important sentences in a document by using sentence similarity and PageRank (Brin and Page, 1998). Sentence extraction can also be implemented using other graph based scoring approaches (Mihalcea, 2004) such as HITS (Kleinberg, 1999) and positional power functions. Graph based methods can also be paired with clustering such as in CollabSum (Wan et al., 2007). This approach first uses clustering to obtain document clusters and then uses graph based algorithm for sentence selection which includes inter and intra-document sentence similarities. Another clustering-based algorithm (Nomoto and Matsumoto, 2001) is a diversity-based extension of MMR that finds diversity by clustering and then proceeds to reduce redundancy by selecting a representative for each cluster. The manually tuned sentence pairwise model (Lin and Bilmes, 2</context>
</contexts>
<marker>Kleinberg, 1999</marker>
<rawString>J. M. Kleinberg. Authoritative sources in a hyperlinked environment. In Journal of the ACM, Vol. 46, Issue 5, 1999, pp. 604-632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kulesza</author>
<author>B Taskar</author>
</authors>
<title>Learning Determinantal Point Processes.</title>
<date>2011</date>
<booktitle>In Proceedings of UAI,</booktitle>
<contexts>
<context position="10900" citStr="Kulesza and Taskar, 2011" startWordPosition="1666" endWordPosition="1669"> preserve some gramatical structure. Their work focuses on learning a particular compression model based on ILP inference, while our work explores learning a general and large class of sentence selection models using submodular optimization. The third notable approach uses SEARN (Daum´e, 2006) to learn parameters for joint summarization and compression model, however it uses vine-growth model and employs search to to find the best policy which is then used to generate a summary. A specific subclass of submodular (but not monotone) functions are defined by Determinantal Point Processes (DPPs) (Kulesza and Taskar, 2011). While they provide an elegant probabilistic interpretation of the resulting summarization models, the lack of monotonicity means that no efficient approximation algorithms are known for computing the highest-scoring summary. 3 Submodular document summarization In this section, we illustrate how document summarization can be addressed using submodular set functions. The set of documents to be summarized is split into a set of individual sentences x = {s1, ..., s,,,}. The summarization method then selects a subset y� ⊆ x of sentences that maximizes a given scoring function Fx : 2x → R subject </context>
</contexts>
<marker>Kulesza, Taskar, 2011</marker>
<rawString>A. Kulesza and B. Taskar. Learning Determinantal Point Processes. In Proceedings of UAI, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
<author>J Pedersen</author>
<author>F Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<contexts>
<context position="4079" citStr="Kupiec et al., 1995" startWordPosition="597" endWordPosition="601">ries as training examples, 1http://www.cs.cornell.edu/˜rs/sfour/ 224 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 224–233, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics we formulate the learning problem as a structured prediction problem and derive a maximummargin algorithm in the structural support vector machine (SVM) framework. Note that, unlike other learning approaches, our method does not require a heuristic decomposition of the learning task into binary classification problems (Kupiec et al., 1995), but directly optimizes a structured prediction. This enables our algorithm to directly optimize the desired performance measure (e.g. ROUGE) during training. Furthermore, our method is not limited to linear-chain dependencies like (Conroy and O’leary, 2001; Shen et al., 2007), but can learn any monotone submodular scoring function. This ability to easily train summarization models makes it possible to efficiently tune models to various types of document collections. In particular, we find that our learning method can reliably tune models with hundreds of parameters based on a training set of</context>
<context position="7630" citStr="Kupiec et al., 1995" startWordPosition="1144" endWordPosition="1147">took inspiration from is based on budgeted submodular optimization. A summary is produced by maximizing an objective function that includes coverage and redundancy terms. Coverage is defined as the sum of sentence similarities between the selected summary and the rest of the sentences, while redundancy is the sum of pairwise intra-summary sentence similarities. Another approach based on submodularity (Qazvinian et al., 2010) relies on extracting important keyphrases from citation sentences for a given paper and using them to build the summary. In the supervised setting, several early methods (Kupiec et al., 1995) made independent binary decisions whether to include a particular sentence in the summary or not. This ignores dependencies between sentences and can result in high redundancy. The same problem arises when using learning-to-rank approaches such as ranking support vector machines, support vector regression and gradient boosted decision trees to select the most relevant sentences for the summary (Metzler and Kanungo, 2008). Introducing some dependencies can improve the performance. One limited way of introducing dependencies between sentences is by using a linear-chain HMM. The HMM is assumed t</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>J. Kupiec, J. Pedersen, and F. Chen. A trainable document summarizer. In Proceedings of SIGIR, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Li</author>
<author>Ke Zhou</author>
<author>G Xue</author>
<author>H Zha</author>
<author>Y Yu</author>
</authors>
<title>Enhancing Diversity, Coverage and Balance for Summarization through Structure Learning.</title>
<date>2009</date>
<booktitle>In Proceedings of WWW,</booktitle>
<contexts>
<context position="9076" citStr="Li et al. (2009)" startWordPosition="1379" endWordPosition="1382">quence labeling (Shen et al., 2007) which can utilize larger and not necessarily independent feature spaces. The disadvantage of using linear chain models, however, is that they represent the summary as a sequence of sentences. Dependencies between sentences that are far away from each other cannot be modeled efficiently. In contrast to such linear chain models, our approach on submodular scoring functions can model long-range dependencies. In this way our method can use properties of the whole summary when deciding which sentences to include in it. More closely related to our work is that of Li et al. (2009). They use the diversified retrieval method proposed in Yue and Joachims (2008) for document summarization. Moreover, they assume that subtopic labels are available so that additional constraints for diversity, coverage and balance can be added to the structural SVM learning problem. In contrast, our approach does not require the knowledge of subtopics (thus allowing us to apply it to a wider range of tasks) and avoids adding additional constraints (simplifying the algorithm). Furthermore, it can use different submodular objective functions, for example word coverage and sentence pairwise mode</context>
<context position="13611" citStr="Li et al., 2009" startWordPosition="2144" endWordPosition="2147">ff between the two terms. Maximizing Fx(y) amounts to increasing the similarity of the summary to excluded sentences while minimizing repetitions in the summary. An example is illustrated in Figure 1. In the simplest case, σ(i, j) may be the TFIDF (Salton and Buckley, 1988) cosine similarity, but we will show later how to learn sophisticated similarity functions. 3.2 Coverage scoring function A second scoring function we consider was first proposed for diversified document retrieval (Swaminathan et al., 2009; Yue and Joachims, 2008), but it naturally applies to document summarization as well (Li et al., 2009). It is based on a notion of word coverage, where each word v has some importance weight ω(v) &gt; 0. A summary y covers a word if at least one of its sentences contains the word. The score of a summary is then simply the sum of the word weights its covers (though we could also include a concave discount function that rewards covering a word multiple times (Raman et al., 2011)): Fx(y) = � ω(v). (3) VEV(y) In the above equation, V (y) denotes the union of all words in y. This function is analogous to a maximum coverage problem, which is known to be submodular (Khuller et al., 1999). Figure 2: Illu</context>
</contexts>
<marker>Li, Zhou, Xue, Zha, Yu, 2009</marker>
<rawString>L. Li, Ke Zhou, G. Xue, H. Zha, and Y. Yu. Enhancing Diversity, Coverage and Balance for Summarization through Structure Learning. In Proceedings of WWW, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lin</author>
<author>J Bilmes</author>
</authors>
<title>Multi-document summarization via budgeted maximization of submodular functions.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<contexts>
<context position="1819" citStr="Lin and Bilmes (2010)" startWordPosition="259" endWordPosition="262">ts in a (set of) document(s). Example applications range from generating short summaries of news articles, to presenting snippets for URLs in web-search. In this paper we focus on extractive multi-document summarization, where the final summary is a subset of the sentences from multiple input documents. In this way, extractive summarization avoids the hard problem of generating well-formed natural-language sentences, since only existing sentences from the input documents are presented as part of the summary. A current state-of-the-art method for document summarization was recently proposed by Lin and Bilmes (2010), using a submodular scoring function based on inter-sentence similarity. On the one hand, this scoring function rewards summaries that are similar to many sentences in the original documents (i.e. promotes coverage). On the other hand, it penalizes summaries that contain sentences that are similar to each other (i.e. discourages redundancy). While obtaining the exact summary that optimizes the objective is computationally hard, they show that a greedy algorithm is guaranteed to compute a good approximation. However, their work does not address how to select a good inter-sentence similarity me</context>
<context position="6983" citStr="Lin and Bilmes, 2010" startWordPosition="1042" endWordPosition="1045">(Kleinberg, 1999) and positional power functions. Graph based methods can also be paired with clustering such as in CollabSum (Wan et al., 2007). This approach first uses clustering to obtain document clusters and then uses graph based algorithm for sentence selection which includes inter and intra-document sentence similarities. Another clustering-based algorithm (Nomoto and Matsumoto, 2001) is a diversity-based extension of MMR that finds diversity by clustering and then proceeds to reduce redundancy by selecting a representative for each cluster. The manually tuned sentence pairwise model (Lin and Bilmes, 2010; Lin and Bilmes, 2011) we took inspiration from is based on budgeted submodular optimization. A summary is produced by maximizing an objective function that includes coverage and redundancy terms. Coverage is defined as the sum of sentence similarities between the selected summary and the rest of the sentences, while redundancy is the sum of pairwise intra-summary sentence similarities. Another approach based on submodularity (Qazvinian et al., 2010) relies on extracting important keyphrases from citation sentences for a given paper and using them to build the summary. In the supervised setti</context>
<context position="12295" citStr="Lin and Bilmes (2010)" startWordPosition="1912" endWordPosition="1915">. Definition 1. Given a set x, a function F : 2x → R is submodular ifffor all u ∈ U and all sets s and t such that s ⊆ t ⊆ x, we have, F(s ∪ {u}) − F(s) ≥ F(t ∪ {u}) − F(t). Intuitively, this definition says that adding u to a subset s of t increases f at least as much as adding it to t. Using two specific submodular functions as examples, the following sections illustrate how this diminishing returns property naturally reflects the trade-off between maximizing coverage while minimizing redundancy. 3.1 Pairwise scoring function The first submodular scoring function we consider was proposed by Lin and Bilmes (2010) and is based on a model of pairwise sentence similarities. It scores a summary y using the following function, which Lin and Bilmes (2010) show is submodular: Fx(y) = � σ(i, j) − λ � σ(i,j). (2) iEx\y,jEy i,jEy:i=,4j 226 Figure 1: Illustration of the pairwise model. Not all edges are shown for clarity purposes. Edge thickness denotes the similarity score. In the above equation, σ(i, j) &gt; 0 denotes a measure of similarity between pairs of sentences i and j. The first term in Eq. 2 is a measure of how similar the sentences included in summary y are to the other sentences in x. The second term p</context>
<context position="15320" citStr="Lin and Bilmes, 2010" startWordPosition="2467" endWordPosition="2470">(v) from training data. Algorithm 1 Greedy algorithm for finding the best summary y� given a scoring function Fx(y). Parameter: r &gt; 0. y�+— 0 A + —x while A =� 0 do k +— arg max lEA Fx(y U {l}) − Fx(y) (cl)r if ck+EzEO cz GB and Fx(9U{k})−Fx(9)&gt; 0 then y�+— y� U {k} end if A +— A\{k} end while 3.3 Computing a Summary Computing the summary that maximizes either of the two scoring functions from above (i.e. Eqns. (2) and (3)) is NP-hard (McDonald, 2007). However, it is known that the greedy algorithm 1 can achieve a 1 − 1/e approximation to the optimum solution for any linear budget constraint (Lin and Bilmes, 2010; Khuller et al., 1999). Even further, this algorithm provides a 1 − 1/e approximation for any monotone submodular scoring function. The algorithm starts with an empty summarization. In each step, a sentence is added to the summary that results in the maximum relative increase 227 of the objective. The increase is relative to the amount of budget that is used by the added sentence. The algorithm terminates when the budget B is reached. Note that the algorithm has a parameter r in the denominator of the selection rule, which Lin and Bilmes (2010) report to have some impact on performance. In th</context>
<context position="21922" citStr="Lin and Bilmes (2010)" startWordPosition="3638" endWordPosition="3641">oss O was used in our experiments. Thus training a structural SVM with this loss aims to maximize the ROUGE-1 F score with the manual summaries provided in the training examples, while trading it off with margin. Note that we could also use a different loss function (as the method is not tied to this particular choice), if we had a different target evaluation metric. Finally, once a w is obtained from structural SVM training, a predicted summary for a test document x can be obtained from (5). 5 Experiments In this section, we empirically evaluate the approach proposed in this paper. Following Lin and Bilmes (2010), experiments were conducted on two different datasets (DUC ’03 and ’04). These datasets contain document sets with four manual summaries for each set. For each document set, we concatenated all the articles and split them into sentences using the tool provided with the ’03 dataset. For the supervised setting we used 10 resamplings with a random 20/5/5 (’03) and 40/5/5 (’04) train/test/validation split. We determined the best C value in (9) using the performance on each validation set and then report average performence over the corresponding test sets. Baseline performance (the approach of Li</context>
<context position="24727" citStr="Lin and Bilmes (2010)" startWordPosition="4113" endWordPosition="4116">g. a sentence with five instances of the same word might cover it better than another with only a single instance). And secondly we look at how important it is to cover a word (e.g. if a word appears in a large fraction of sentences we might want to be sure to cover it). Combining those two criteria using different thresholds we get a set of features for each word. Our coverage features are motivated from the approach of Yue and Joachims (2008). In contrast, the hand-tuned pairwise baseline uses only TFIDF weighted cosine similarity between sentences using all words, following the approach in Lin and Bilmes (2010). The resulting summaries are evaluated using ROUGE version 1.5.5 (Lin and Hovy, 2003). We selected the ROUGE-1 F measure because it was used by Lin and Bilmes (2010) and because it is one of the commonly used performance scores in recent work. However, our learning method applies to other performance measures as well. Note that we use the ROUGE-1 F measure both for the loss function during learning, as well as for the evaluation of the predicted summaries. 5.1 How does learning compare to manual tuning? In our first experiment, we compare our supervised learning approach to the hand-tuned app</context>
<context position="26087" citStr="Lin and Bilmes (2010)" startWordPosition="4339" endWordPosition="4342"> resulted in a statistically significant (p &lt; 0.05 using paired t-test) increase in performance on both datasets compared to our reimplementation of the manually tuned pairwise model. Note that our reimplementation of the approach of Lin and Bilmes (2010) resulted in slightly different performance numbers than those reported in Lin and Bilmes (2010) – better on DUC ’03 and somewhat lower on DUC ’04, if evaluated on the same selection of test examples as theirs. We conjecture that this is due to small differences in implementation and/or preprocessing of the dataset. Furthermore, as authors of Lin and Bilmes (2010) note in their paper, the ’03 and ’04 datasets behave quite differently. model dataset ROUGE-1 F (stderr) pairwise DUC ’03 0.3929 (0.0074) coverage 0.3784 (0.0059) hand-tuned 0.3571 (0.0063) pairwise DUC ’04 0.4066 (0.0061) coverage 0.3992 (0.0054) hand-tuned 0.3935 (0.0052) Figure 3: Results obtained on DUC ’03 and ’04 datasets using the supervised models. Increase in performance over the hand-tuned is statistically significant (p &lt; 0.05) for the pairwise model on the both datasets, but only on DUC ’03 for the coverage model. Figure 3 also reports the performance for the coverage model as tra</context>
<context position="31660" citStr="Lin and Bilmes (2010)" startWordPosition="5259" endWordPosition="5262">re changes when we removed certain features groups (described in the beginning of Section 5) as shown in Figure 6. The most important group of features are the basic features (pure cosine similarity between sentences) since removing them results in the largest drop in performance. However, other features play a significant role too (i.e. only the basic ones are not enough to achieve good perforbelow optimal confirming that greedy selection works quite well. 231 mance). This confirms that performance can be improved by adding richer fatures instead of using only a single similarity score as in Lin and Bilmes (2010). Using learning for these complex model is essential, since hand-tuning is likely to be intractable. The second most important group of features considering the drop in performance (i.e. location) looks at positions of sentences in the articles. This makes intuitive sense because the first sentences in news articles are usually packed with information. The other three groups do not have a significant impact on their own. removed ROUGE-1 F group none 0.40662 basic 0.38681 all except basic 0.39723 location 0.39782 sent+doc 0.39901 cap+stop+len 0.40273 minmax 0.40721 Figure 6: Effects of removin</context>
</contexts>
<marker>Lin, Bilmes, 2010</marker>
<rawString>H. Lin and J. Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. In Proceedings of NAACL-HLT, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lin</author>
<author>J Bilmes</author>
</authors>
<title>A Class of Submodular Functions for Document Summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT,</booktitle>
<contexts>
<context position="7006" citStr="Lin and Bilmes, 2011" startWordPosition="1046" endWordPosition="1049">positional power functions. Graph based methods can also be paired with clustering such as in CollabSum (Wan et al., 2007). This approach first uses clustering to obtain document clusters and then uses graph based algorithm for sentence selection which includes inter and intra-document sentence similarities. Another clustering-based algorithm (Nomoto and Matsumoto, 2001) is a diversity-based extension of MMR that finds diversity by clustering and then proceeds to reduce redundancy by selecting a representative for each cluster. The manually tuned sentence pairwise model (Lin and Bilmes, 2010; Lin and Bilmes, 2011) we took inspiration from is based on budgeted submodular optimization. A summary is produced by maximizing an objective function that includes coverage and redundancy terms. Coverage is defined as the sum of sentence similarities between the selected summary and the rest of the sentences, while redundancy is the sum of pairwise intra-summary sentence similarities. Another approach based on submodularity (Qazvinian et al., 2010) relies on extracting important keyphrases from citation sentences for a given paper and using them to build the summary. In the supervised setting, several early metho</context>
</contexts>
<marker>Lin, Bilmes, 2011</marker>
<rawString>H. Lin and J. Bilmes. 2011. A Class of Submodular Functions for Document Summarization. In Proceedings of ACL-HLT, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
<author>E Hovy</author>
</authors>
<title>Automatic evaluation of summaries using N-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<contexts>
<context position="24813" citStr="Lin and Hovy, 2003" startWordPosition="4126" endWordPosition="4129">th only a single instance). And secondly we look at how important it is to cover a word (e.g. if a word appears in a large fraction of sentences we might want to be sure to cover it). Combining those two criteria using different thresholds we get a set of features for each word. Our coverage features are motivated from the approach of Yue and Joachims (2008). In contrast, the hand-tuned pairwise baseline uses only TFIDF weighted cosine similarity between sentences using all words, following the approach in Lin and Bilmes (2010). The resulting summaries are evaluated using ROUGE version 1.5.5 (Lin and Hovy, 2003). We selected the ROUGE-1 F measure because it was used by Lin and Bilmes (2010) and because it is one of the commonly used performance scores in recent work. However, our learning method applies to other performance measures as well. Note that we use the ROUGE-1 F measure both for the loss function during learning, as well as for the evaluation of the predicted summaries. 5.1 How does learning compare to manual tuning? In our first experiment, we compare our supervised learning approach to the hand-tuned approach. The results from this experiment are summarized in Figure 3. First, supervised </context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>C. Y. Lin and E. Hovy. Automatic evaluation of summaries using N-gram co-occurrence statistics. In Proceedings of NAACL, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F T Martins</author>
<author>N A Smith</author>
</authors>
<title>Summarization with a joint model for sentence extraction and compression.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL Workshop on Integer Linear Programming for Natural Language Processing,</booktitle>
<contexts>
<context position="9901" citStr="Martins and Smith, 2009" startWordPosition="1504" endWordPosition="1507">iversity, coverage and balance can be added to the structural SVM learning problem. In contrast, our approach does not require the knowledge of subtopics (thus allowing us to apply it to a wider range of tasks) and avoids adding additional constraints (simplifying the algorithm). Furthermore, it can use different submodular objective functions, for example word coverage and sentence pairwise models described later in this paper. Another closely related work also takes a maxmargin discriminative learning approach in the structural SVM framework (Berg-Kirkpatrick et al., 2011) or by using MIRA (Martins and Smith, 2009) to learn the parameters for summarizing a set of documents. However, they do not consider submodular functions, but instead solve an Integer Linear Program (ILP) or an approximation thereof. The ILP encodes a compression model where arbitrary parts of the parse trees of sentences in the summary can be cut and removed. This allows them to select parts of sentences and yet preserve some gramatical structure. Their work focuses on learning a particular compression model based on ILP inference, while our work explores learning a general and large class of sentence selection models using submodula</context>
</contexts>
<marker>Martins, Smith, 2009</marker>
<rawString>F. T. Martins and N. A. Smith. Summarization with a joint model for sentence extraction and compression. In Proceedings of ACL Workshop on Integer Linear Programming for Natural Language Processing, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
</authors>
<title>A Study of Global Inference Algorithms in Multi-document Summarization.</title>
<date>2007</date>
<booktitle>In Advances in Information Retrieval, Lecture Notes in Computer Science,</booktitle>
<pages>557--564</pages>
<contexts>
<context position="5562" citStr="McDonald, 2007" startWordPosition="824" endWordPosition="825">search. 2 Related work Work on extractive summarization spans a large range of approaches. Starting with unsupervised methods, one of the widely known approaches is Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998). It uses a greedy approach for selection and considers the trade-off between relevance and redundancy. Later it was extended (Goldstein et al., 2000) to support multidocument settings by incorporating additional information available in this case. Good results can be achieved by reformulating this as a knapsack packing problem and solving it using dynamic programing (McDonald, 2007). Alternatively, we can use annotated phrases as textual units and select a subset that covers most concepts present in the input (Filatova and Hatzivassiloglou, 2004) (which can also be achieved by our coverage scoring function if it is extended with appropriate features). A popular stochastic graph-based summarization method is LexRank (Erkan and Radev, 2004). It computes sentence importance based on the concept of eigenvector centrality in a graph of sentence similarities. Similarly, TextRank (Mihalcea and Tarau, 2004) is also graph based ranking system for identification of important sente</context>
<context position="15155" citStr="McDonald, 2007" startWordPosition="2439" endWordPosition="2440">igh words highly that occur in many sentences of x, but in few other documents (Swaminathan et al., 2009). However, we will show in the following how to learn ω(v) from training data. Algorithm 1 Greedy algorithm for finding the best summary y� given a scoring function Fx(y). Parameter: r &gt; 0. y�+— 0 A + —x while A =� 0 do k +— arg max lEA Fx(y U {l}) − Fx(y) (cl)r if ck+EzEO cz GB and Fx(9U{k})−Fx(9)&gt; 0 then y�+— y� U {k} end if A +— A\{k} end while 3.3 Computing a Summary Computing the summary that maximizes either of the two scoring functions from above (i.e. Eqns. (2) and (3)) is NP-hard (McDonald, 2007). However, it is known that the greedy algorithm 1 can achieve a 1 − 1/e approximation to the optimum solution for any linear budget constraint (Lin and Bilmes, 2010; Khuller et al., 1999). Even further, this algorithm provides a 1 − 1/e approximation for any monotone submodular scoring function. The algorithm starts with an empty summarization. In each step, a sentence is added to the summary that results in the maximum relative increase 227 of the objective. The increase is relative to the amount of budget that is used by the added sentence. The algorithm terminates when the budget B is reac</context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>R. McDonald. 2007. A Study of Global Inference Algorithms in Multi-document Summarization. In Advances in Information Retrieval, Lecture Notes in Computer Science, 2007, pp. 557–564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Metzler</author>
<author>T Kanungo</author>
</authors>
<title>Machine learned sentence selection strategies for query-biased summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<contexts>
<context position="8055" citStr="Metzler and Kanungo, 2008" startWordPosition="1209" endWordPosition="1212">l., 2010) relies on extracting important keyphrases from citation sentences for a given paper and using them to build the summary. In the supervised setting, several early methods (Kupiec et al., 1995) made independent binary decisions whether to include a particular sentence in the summary or not. This ignores dependencies between sentences and can result in high redundancy. The same problem arises when using learning-to-rank approaches such as ranking support vector machines, support vector regression and gradient boosted decision trees to select the most relevant sentences for the summary (Metzler and Kanungo, 2008). Introducing some dependencies can improve the performance. One limited way of introducing dependencies between sentences is by using a linear-chain HMM. The HMM is assumed to produce the summary by having a chain transitioning 225 between summarization and non-summarization states (Conroy and O’leary, 2001) while traversing the sentences in a document. A more expressive approach is using a CRF for sequence labeling (Shen et al., 2007) which can utilize larger and not necessarily independent feature spaces. The disadvantage of using linear chain models, however, is that they represent the sum</context>
</contexts>
<marker>Metzler, Kanungo, 2008</marker>
<rawString>D. Metzler and T. Kanungo. Machine learned sentence selection strategies for query-biased summarization. In Proceedings of SIGIR, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Graph-based ranking algorithms for sentence extraction, applied to text summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL on Interactive poster and demonstration sessions,</booktitle>
<contexts>
<context position="6349" citStr="Mihalcea, 2004" startWordPosition="948" endWordPosition="949">ch can also be achieved by our coverage scoring function if it is extended with appropriate features). A popular stochastic graph-based summarization method is LexRank (Erkan and Radev, 2004). It computes sentence importance based on the concept of eigenvector centrality in a graph of sentence similarities. Similarly, TextRank (Mihalcea and Tarau, 2004) is also graph based ranking system for identification of important sentences in a document by using sentence similarity and PageRank (Brin and Page, 1998). Sentence extraction can also be implemented using other graph based scoring approaches (Mihalcea, 2004) such as HITS (Kleinberg, 1999) and positional power functions. Graph based methods can also be paired with clustering such as in CollabSum (Wan et al., 2007). This approach first uses clustering to obtain document clusters and then uses graph based algorithm for sentence selection which includes inter and intra-document sentence similarities. Another clustering-based algorithm (Nomoto and Matsumoto, 2001) is a diversity-based extension of MMR that finds diversity by clustering and then proceeds to reduce redundancy by selecting a representative for each cluster. The manually tuned sentence pa</context>
</contexts>
<marker>Mihalcea, 2004</marker>
<rawString>R. Mihalcea. 2004. Graph-based ranking algorithms for sentence extraction, applied to text summarization. In Proceedings of the ACL on Interactive poster and demonstration sessions, 2004. R. Mihalcea and P. Tarau. Textrank: Bringing order into texts. In Proceedings of EMNLP, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nomoto</author>
<author>Y Matsumoto</author>
</authors>
<title>A new approach to unsupervised text summarization.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<contexts>
<context position="6758" citStr="Nomoto and Matsumoto, 2001" startWordPosition="1007" endWordPosition="1010">entification of important sentences in a document by using sentence similarity and PageRank (Brin and Page, 1998). Sentence extraction can also be implemented using other graph based scoring approaches (Mihalcea, 2004) such as HITS (Kleinberg, 1999) and positional power functions. Graph based methods can also be paired with clustering such as in CollabSum (Wan et al., 2007). This approach first uses clustering to obtain document clusters and then uses graph based algorithm for sentence selection which includes inter and intra-document sentence similarities. Another clustering-based algorithm (Nomoto and Matsumoto, 2001) is a diversity-based extension of MMR that finds diversity by clustering and then proceeds to reduce redundancy by selecting a representative for each cluster. The manually tuned sentence pairwise model (Lin and Bilmes, 2010; Lin and Bilmes, 2011) we took inspiration from is based on budgeted submodular optimization. A summary is produced by maximizing an objective function that includes coverage and redundancy terms. Coverage is defined as the sum of sentence similarities between the selected summary and the rest of the sentences, while redundancy is the sum of pairwise intra-summary sentenc</context>
</contexts>
<marker>Nomoto, Matsumoto, 2001</marker>
<rawString>T. Nomoto and Y. Matsumoto. A new approach to unsupervised text summarization. In Proceedings of SIGIR, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Qazvinian</author>
<author>D R Radev</author>
<author>A ¨Ozg¨ur</author>
</authors>
<title>Citation Summarization Through Keyphrase Extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<marker>Qazvinian, Radev, ¨Ozg¨ur, 2010</marker>
<rawString>V. Qazvinian, D. R. Radev, and A. ¨Ozg¨ur. 2010. Citation Summarization Through Keyphrase Extraction. In Proceedings of COLING, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Raman</author>
<author>T Joachims</author>
<author>P Shivaswamy</author>
</authors>
<title>Structured Learning of Two-Level Dynamic Rankings.</title>
<date>2011</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<contexts>
<context position="13987" citStr="Raman et al., 2011" startWordPosition="2219" endWordPosition="2222">erage scoring function A second scoring function we consider was first proposed for diversified document retrieval (Swaminathan et al., 2009; Yue and Joachims, 2008), but it naturally applies to document summarization as well (Li et al., 2009). It is based on a notion of word coverage, where each word v has some importance weight ω(v) &gt; 0. A summary y covers a word if at least one of its sentences contains the word. The score of a summary is then simply the sum of the word weights its covers (though we could also include a concave discount function that rewards covering a word multiple times (Raman et al., 2011)): Fx(y) = � ω(v). (3) VEV(y) In the above equation, V (y) denotes the union of all words in y. This function is analogous to a maximum coverage problem, which is known to be submodular (Khuller et al., 1999). Figure 2: Illustration of the coverage model. Word border thickness represents importance. An example of how a summary is scored is illustrated in the Figure 2. Analogous to the definition of similarity σ(i, j) in the pairwise model, the choice of the word importance function ω(v) is crucial in the coverage model. A simple heuristic is to weigh words highly that occur in many sentences o</context>
</contexts>
<marker>Raman, Joachims, Shivaswamy, 2011</marker>
<rawString>K. Raman, T. Joachims and P. Shivaswamy. Structured Learning of Two-Level Dynamic Rankings. In Proceedings of CIKM, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Term-weighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>In Information processing and management,</booktitle>
<pages>513--523</pages>
<contexts>
<context position="13269" citStr="Salton and Buckley, 1988" startWordPosition="2091" endWordPosition="2094">ty score. In the above equation, σ(i, j) &gt; 0 denotes a measure of similarity between pairs of sentences i and j. The first term in Eq. 2 is a measure of how similar the sentences included in summary y are to the other sentences in x. The second term penalizes y by how similar its sentences are to each other. λ &gt; 0 is a scalar parameter that trades off between the two terms. Maximizing Fx(y) amounts to increasing the similarity of the summary to excluded sentences while minimizing repetitions in the summary. An example is illustrated in Figure 1. In the simplest case, σ(i, j) may be the TFIDF (Salton and Buckley, 1988) cosine similarity, but we will show later how to learn sophisticated similarity functions. 3.2 Coverage scoring function A second scoring function we consider was first proposed for diversified document retrieval (Swaminathan et al., 2009; Yue and Joachims, 2008), but it naturally applies to document summarization as well (Li et al., 2009). It is based on a notion of word coverage, where each word v has some importance weight ω(v) &gt; 0. A summary y covers a word if at least one of its sentences contains the word. The score of a summary is then simply the sum of the word weights its covers (tho</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>G. Salton and C. Buckley. Term-weighting approaches in automatic text retrieval. In Information processing and management, 1988, pp. 513–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shen</author>
<author>J T Sun</author>
<author>H Li</author>
<author>Q Yang</author>
<author>Z Chen</author>
</authors>
<title>Document summarization using conditional random fields.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<contexts>
<context position="4357" citStr="Shen et al., 2007" startWordPosition="640" endWordPosition="643">e formulate the learning problem as a structured prediction problem and derive a maximummargin algorithm in the structural support vector machine (SVM) framework. Note that, unlike other learning approaches, our method does not require a heuristic decomposition of the learning task into binary classification problems (Kupiec et al., 1995), but directly optimizes a structured prediction. This enables our algorithm to directly optimize the desired performance measure (e.g. ROUGE) during training. Furthermore, our method is not limited to linear-chain dependencies like (Conroy and O’leary, 2001; Shen et al., 2007), but can learn any monotone submodular scoring function. This ability to easily train summarization models makes it possible to efficiently tune models to various types of document collections. In particular, we find that our learning method can reliably tune models with hundreds of parameters based on a training set of about 30 examples. This increases the fidelity of models compared to their hand-tuned counterparts, showing significantly improved empirical performance. We provide a detailed investigation into the sources of these improvements, identifying further directions for research. 2 </context>
<context position="8495" citStr="Shen et al., 2007" startWordPosition="1280" endWordPosition="1283">s ranking support vector machines, support vector regression and gradient boosted decision trees to select the most relevant sentences for the summary (Metzler and Kanungo, 2008). Introducing some dependencies can improve the performance. One limited way of introducing dependencies between sentences is by using a linear-chain HMM. The HMM is assumed to produce the summary by having a chain transitioning 225 between summarization and non-summarization states (Conroy and O’leary, 2001) while traversing the sentences in a document. A more expressive approach is using a CRF for sequence labeling (Shen et al., 2007) which can utilize larger and not necessarily independent feature spaces. The disadvantage of using linear chain models, however, is that they represent the summary as a sequence of sentences. Dependencies between sentences that are far away from each other cannot be modeled efficiently. In contrast to such linear chain models, our approach on submodular scoring functions can model long-range dependencies. In this way our method can use properties of the whole summary when deciding which sentences to include in it. More closely related to our work is that of Li et al. (2009). They use the dive</context>
</contexts>
<marker>Shen, Sun, Li, Yang, Chen, 2007</marker>
<rawString>D. Shen, J. T. Sun, H. Li, Q. Yang, and Z. Chen. Document summarization using conditional random fields. In Proceedings of IJCAI, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Swaminathan</author>
<author>C V Mathew</author>
<author>D Kirovski</author>
</authors>
<title>Essential Pages.</title>
<date>2009</date>
<booktitle>In Proceedings of WI-IAT, IEEE</booktitle>
<publisher>Computer Society,</publisher>
<contexts>
<context position="3252" citStr="Swaminathan et al., 2009" startWordPosition="477" endWordPosition="480">h the similarity measure as well as the coverage/reduncancy trade-off from training data. Furthermore, our learning algorithm is not limited to the model of Lin and Bilmes (2010), but applies to all monotone submodular summarization models. Due to the diminishing-returns property of monotone submodular set functions and their computational tractability, this class of functions provides a rich space for designing summarization methods. To illustrate the generality of our approach, we also provide experiments for a coverage-based model originally developed for diversified information retrieval (Swaminathan et al., 2009). In general, our method learns a parameterized monotone submodular scoring function from supervised training data, and its implementation is available for download.1 Given a set of documents and their summaries as training examples, 1http://www.cs.cornell.edu/˜rs/sfour/ 224 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 224–233, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics we formulate the learning problem as a structured prediction problem and derive a maximummargin algorithm in the s</context>
<context position="13508" citStr="Swaminathan et al., 2009" startWordPosition="2126" endWordPosition="2129">cond term penalizes y by how similar its sentences are to each other. λ &gt; 0 is a scalar parameter that trades off between the two terms. Maximizing Fx(y) amounts to increasing the similarity of the summary to excluded sentences while minimizing repetitions in the summary. An example is illustrated in Figure 1. In the simplest case, σ(i, j) may be the TFIDF (Salton and Buckley, 1988) cosine similarity, but we will show later how to learn sophisticated similarity functions. 3.2 Coverage scoring function A second scoring function we consider was first proposed for diversified document retrieval (Swaminathan et al., 2009; Yue and Joachims, 2008), but it naturally applies to document summarization as well (Li et al., 2009). It is based on a notion of word coverage, where each word v has some importance weight ω(v) &gt; 0. A summary y covers a word if at least one of its sentences contains the word. The score of a summary is then simply the sum of the word weights its covers (though we could also include a concave discount function that rewards covering a word multiple times (Raman et al., 2011)): Fx(y) = � ω(v). (3) VEV(y) In the above equation, V (y) denotes the union of all words in y. This function is analogou</context>
</contexts>
<marker>Swaminathan, Mathew, Kirovski, 2009</marker>
<rawString>A. Swaminathan, C. V. Mathew and D. Kirovski. Essential Pages. In Proceedings of WI-IAT, IEEE Computer Society, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tsochantaridis</author>
<author>T Hofmann</author>
<author>T Joachims</author>
<author>Y Altun</author>
</authors>
<title>Large margin methods for structured and interdependent output variables.</title>
<date>2005</date>
<journal>In Journal of Machine Learning Research,</journal>
<volume>6</volume>
<pages>1453--1484</pages>
<contexts>
<context position="17526" citStr="Tsochantaridis et al., 2005" startWordPosition="2846" endWordPosition="2849"> of input sentences x: σx(i,j) = wTφpx(i,j) ωx(v) = wTφcx(v). (4) In the above equations, w is a weight vector that is learned, and φpx(i, j) and φcx(v) are feature vectors. In the pairwise model, φpx(i, j) may include feature like the TFIDF cosine between i and j or the number of words from the document titles that i and j share. In the coverage model, φcx(v) may include features like a binary indicator of whether v occurs in more than 10% of the sentences in x or whether v occurs in the document title. We propose to learn the weights following a large-margin framework using structural SVMs (Tsochantaridis et al., 2005). Structural SVMs learn a discriminant function h(x) = arg max wT*(x, y) (5) yEY that predicts a structured output y given a (possibly also structured) input x. *(x, y) ∈ RN is called the joint feature-map between input x and output y. Note that both submodular scoring function in Eqns. (2) and (3) can be brought into the form wT *(x, y) for the linear parametrization in Eq. (6) and (7): *p(x, y)= E φpx(i,j) − λ E φpx(i,j), (6) iEx\y,jEy i,jEy:i=,4j *c(x, y)= E φcx(v). (7) vEV (y) After this transformation, it is easy to see that computing the maximizing summary in Eq. (1) and the structural S</context>
<context position="20363" citStr="Tsochantaridis et al., 2005" startWordPosition="3354" endWordPosition="3357">x wT,&amp;(xi, y) + O(Yi, y) y if wT,&amp;(xi,yi) + c G wT,&amp;(xi, 9) + O(Yi, y) −�i then Wi Wi U {y} w solve QP (9) using constraints Wi end if end for until no Wi has changed during iteration for any other summary yi (i.e., wT,&amp;(xi, 9i)). The objective function learns a large-margin weight vector w while trading it off with an upper bound on the empirical loss. The two quantities are traded off with a parameter C &gt; 0. Even though the QP has exponentially many constraints in the number of sentences in the input documents, it can be solved approximately in polynomial time via a cutting plane algorithm (Tsochantaridis et al., 2005). The steps of the cutting-plane algorithm are shown in Algorithm 2. In each iteration of the algorithm, for each training document xi, a summary yi which most violates the constraint in (9) is found. This is done by finding y� arg max wT,&amp;(xi, y) + O(Y i, y), yEY for which we use a variant of the greedy algorithm in Figure 1. After a violating constraint for each training example is added, the resulting quadratic program is solved. These steps are repeated until all the constraints are satisfied to a required precision c. Finally, special care has to be taken to appropriately define the loss </context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2005</marker>
<rawString>I. Tsochantaridis, T. Hofmann, T. Joachims and Y. Altun. Large margin methods for structured and interdependent output variables. In Journal of Machine Learning Research, Vol. 6, 2005, pp. 1453-1484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
<author>J Yang</author>
<author>J Xiao</author>
</authors>
<title>Collabsum: Exploiting multiple document clustering for collaborative single document summarizations.</title>
<date>2007</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<contexts>
<context position="6507" citStr="Wan et al., 2007" startWordPosition="974" endWordPosition="977"> is LexRank (Erkan and Radev, 2004). It computes sentence importance based on the concept of eigenvector centrality in a graph of sentence similarities. Similarly, TextRank (Mihalcea and Tarau, 2004) is also graph based ranking system for identification of important sentences in a document by using sentence similarity and PageRank (Brin and Page, 1998). Sentence extraction can also be implemented using other graph based scoring approaches (Mihalcea, 2004) such as HITS (Kleinberg, 1999) and positional power functions. Graph based methods can also be paired with clustering such as in CollabSum (Wan et al., 2007). This approach first uses clustering to obtain document clusters and then uses graph based algorithm for sentence selection which includes inter and intra-document sentence similarities. Another clustering-based algorithm (Nomoto and Matsumoto, 2001) is a diversity-based extension of MMR that finds diversity by clustering and then proceeds to reduce redundancy by selecting a representative for each cluster. The manually tuned sentence pairwise model (Lin and Bilmes, 2010; Lin and Bilmes, 2011) we took inspiration from is based on budgeted submodular optimization. A summary is produced by maxi</context>
</contexts>
<marker>Wan, Yang, Xiao, 2007</marker>
<rawString>X. Wan, J. Yang, and J. Xiao. Collabsum: Exploiting multiple document clustering for collaborative single document summarizations. In Proceedings of SIGIR, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yue</author>
<author>T Joachims</author>
</authors>
<title>Predicting diverse subsets using structural svms.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML,</booktitle>
<contexts>
<context position="9155" citStr="Yue and Joachims (2008)" startWordPosition="1391" endWordPosition="1394">essarily independent feature spaces. The disadvantage of using linear chain models, however, is that they represent the summary as a sequence of sentences. Dependencies between sentences that are far away from each other cannot be modeled efficiently. In contrast to such linear chain models, our approach on submodular scoring functions can model long-range dependencies. In this way our method can use properties of the whole summary when deciding which sentences to include in it. More closely related to our work is that of Li et al. (2009). They use the diversified retrieval method proposed in Yue and Joachims (2008) for document summarization. Moreover, they assume that subtopic labels are available so that additional constraints for diversity, coverage and balance can be added to the structural SVM learning problem. In contrast, our approach does not require the knowledge of subtopics (thus allowing us to apply it to a wider range of tasks) and avoids adding additional constraints (simplifying the algorithm). Furthermore, it can use different submodular objective functions, for example word coverage and sentence pairwise models described later in this paper. Another closely related work also takes a max</context>
<context position="13533" citStr="Yue and Joachims, 2008" startWordPosition="2130" endWordPosition="2133">ow similar its sentences are to each other. λ &gt; 0 is a scalar parameter that trades off between the two terms. Maximizing Fx(y) amounts to increasing the similarity of the summary to excluded sentences while minimizing repetitions in the summary. An example is illustrated in Figure 1. In the simplest case, σ(i, j) may be the TFIDF (Salton and Buckley, 1988) cosine similarity, but we will show later how to learn sophisticated similarity functions. 3.2 Coverage scoring function A second scoring function we consider was first proposed for diversified document retrieval (Swaminathan et al., 2009; Yue and Joachims, 2008), but it naturally applies to document summarization as well (Li et al., 2009). It is based on a notion of word coverage, where each word v has some importance weight ω(v) &gt; 0. A summary y covers a word if at least one of its sentences contains the word. The score of a summary is then simply the sum of the word weights its covers (though we could also include a concave discount function that rewards covering a word multiple times (Raman et al., 2011)): Fx(y) = � ω(v). (3) VEV(y) In the above equation, V (y) denotes the union of all words in y. This function is analogous to a maximum coverage p</context>
<context position="24554" citStr="Yue and Joachims (2008)" startWordPosition="4087" endWordPosition="4090">ength in both models. The only further addition is the use of different word coverage levels in the coverage model. First we consider how well does a sentence cover a word (e.g. a sentence with five instances of the same word might cover it better than another with only a single instance). And secondly we look at how important it is to cover a word (e.g. if a word appears in a large fraction of sentences we might want to be sure to cover it). Combining those two criteria using different thresholds we get a set of features for each word. Our coverage features are motivated from the approach of Yue and Joachims (2008). In contrast, the hand-tuned pairwise baseline uses only TFIDF weighted cosine similarity between sentences using all words, following the approach in Lin and Bilmes (2010). The resulting summaries are evaluated using ROUGE version 1.5.5 (Lin and Hovy, 2003). We selected the ROUGE-1 F measure because it was used by Lin and Bilmes (2010) and because it is one of the commonly used performance scores in recent work. However, our learning method applies to other performance measures as well. Note that we use the ROUGE-1 F measure both for the loss function during learning, as well as for the eval</context>
</contexts>
<marker>Yue, Joachims, 2008</marker>
<rawString>Y. Yue and T. Joachims. Predicting diverse subsets using structural svms. In Proceedings of ICML, 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>