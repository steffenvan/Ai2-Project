<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000395">
<title confidence="0.992782">
A Direct Syntax-Driven Reordering Model for Phrase-Based Machine
Translation
</title>
<author confidence="0.982087">
Niyu Ge
</author>
<affiliation confidence="0.948842">
IBM T.J.Watson Research
</affiliation>
<address confidence="0.678335">
Yorktown Heights, NY 10598
</address>
<email confidence="0.977191">
niyuge@us.ibm.com
</email>
<sectionHeader confidence="0.997011" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99910295">
This paper presents a direct word reordering
model with novel syntax-based features for sta-
tistical machine translation. Reordering models
address the problem of reordering source lan-
guage into the word order of the target language.
IBM Models 3 through 5 have reordering com-
ponents that use surface word information but
very little context information to determine the
traversal order of the source sentence. Since the
late 1990s, phrase-based machine translation
solves much of the local reorderings by using
phrasal translations. The problem of long-
distance reordering has become a central re-
search topic in modeling distortions. We present
a syntax driven maximum entropy reordering
model that directly predicts the source traversal
order and is able to model arbitrarily long dis-
tance word movement. We show that this model
significantly improves machine translation qual-
ity.
</bodyText>
<sectionHeader confidence="0.999521" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999461166666667">
Machine translation reordering models model the
problem of the word order when translating a
source language into a target language. For exam-
ple in Spanish and Arabic, adjectives often come
after the nouns they modify whereas in English
modifying adjectives usually precede the nouns.
When translating Spanish or Arabic into English,
the position of the adjectives need to be properly
reordered to be placed before the nouns to make
fluent English.
In this paper, we present a word reordering model
that models the word reordering process in transla-
tion. The paper is organized as follows. §2 out-
lines previous approaches to reordering. §3 details
our model and its training and decoding process.
§4 discusses experiments to evaluate the model
and §5 presents machine translation results. §6 is
discussion and conclusion.
</bodyText>
<sectionHeader confidence="0.998447" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999868142857143">
The word reordering problem has been one of the
major problems in statistical machine translation
(SMT). Since exploring all possible reorderings
of a source sentence is an NP-complete problem
(Knight 1999), SMT systems limit words to be re-
ordered within a window of length k. IBM Models
3 through 5 (Brown et.al. 1993) model reorderings
based on surface word information. For example,
Model 4 attempts to assign target-language posi-
tions to source-language words by modeling d(j  |i,
l, m) where j is the target-language position, i is the
source-language position, l and m are respectively
source and target sentence lengths. These models
are not effective in modeling reorderings because
they don’t have enough context and lack structural
information.
Phrase-based SMT systems such as (Koehn et.al.
2003) move from using words as translation units
to using phrases. One of the advantages of phrase-
based SMT systems is that local reorderings are
inherent in the phrase translations. However,
phrase-based SMT systems capture reordering in-
stances and not reordering phenomena. For exam-
ple, if the Arabic phrase “the car red” and its
English translation “the red car’ is seen in the
training data, phrase-based SMT is able to produce
the correct English for the Arabic ‘the car red’.
However it will not be able to produce ‘the blue
car’ for the Arabic ‘the car blue’ if the training
data does not contain this phrase pair. Phrases do
not capture the phenomenon that Arabic adjectives
and nouns need to be reordered. Another problem
with phrase-based SMT is the problem of long-
range reorderings. Recent work on reordering has
been focusing on capturing general reordering
</bodyText>
<page confidence="0.982461">
849
</page>
<note confidence="0.752064">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 849–857,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999414">
phenomena (as opposed to instances) and on solv-
ing long-range reordering problems.
(Al-onaizan et.al. 2006) proposes 3 distor-
tion models, the inbound, outbound, and pair mod-
els. They together model the likelihood of
translating a source word at position i given that
the source word at position j has just been trans-
lated. These models perform better than n-gram
based language models but are limited in their use
of only the surface strings.
Instead of directly modeling the distance
of word movement, phrasal level reordering mod-
els model how to move phrases, also called orien-
tations. Orientations typically apply to adjacent
phrases. Two adjacent phrases can be either
placed monotonically (sometimes called straight)
or swapped (non-monotonically or inverted).
Early orientation models do not use lexical con-
tents such as (Zens et. al., 2004). More recently,
(Xiong et.al. 2006; Zens 2006; Och et. al, 2004;
Tillmann, 2004; Kumar et al., 2005, Ni et al.,
2009) all presented models that use lexical features
from the phrases to predict their orientations.
These models are very powerful in predicting local
phrase placements. More recently (Galley et.al.
2008) introduced a hierarchical orientation model
that captures some non-local phrase reorderings by
a shift reduce algorithm. Because of the heavy use
of lexical features, these models tend to suffer
from data sparseness problems. Another limitation
is that these models are restricted to reorderings
with no gaps and phrases that are adjacent.
We present a probabilistic reordering model
that models directly the source translation se-
quence and explicitly assigns probabilities to the
reorderings of the source input with no restrictions
on gap, length or adjacency. This is different from
the approaches of pre-order such as (Xia and
McCord 2004; Collins et.al. 2005; Kanthak et. al.
2005; Li et. al., 2007). Although our model can
be used to produce top N pre-ordered source, the
experiments reported here do not use the model in
the pre-order mode. Instead, the reordering model
is used to generate a reorder lattice which encodes
many reorderings and their costs (negative log
probability). This reorder lattice is independent of
the translation decoder. In principle, any decoder
can use this lattice for its reordering needs. We
have integrated the reorder lattice into a phrase-
based. The experiments reported here are from the
phrase-based decoder.
We present the reordering model based on
maximum entropy models. We then describe the
syntactic features in the context of Chinese to Eng-
lish translation.
</bodyText>
<sectionHeader confidence="0.979441" genericHeader="method">
3 Maximum Entropy Reordering Model
</sectionHeader>
<bodyText confidence="0.834257">
The model takes a source sequence of length n:
S = [s1 ,s2,... sn ]
and models its translation or visit order according
to the target language:
</bodyText>
<equation confidence="0.989442">
V = [v1, v2,... vn ]
</equation>
<bodyText confidence="0.995425666666667">
where vj is the source position for target position j.
For example, if the 2nd source word is to be trans-
lated first, then v1 = 2. We find V such that
</bodyText>
<equation confidence="0.9988965">
) (
}
n
max rl p(vj  |S,v1...vj−
j=
1
</equation>
<bodyText confidence="0.97384075">
In equation (1) {} is the set of possible visit or-
ders. We want to find a visit order V such that the
probability p(V|S) is maximized. Equation (2) is a
component-wise decomposition of (1).
Let
f= vj and h = (S, v1 ...vj−1 )
We use the maximum entropy model to estimate
equation (2):
</bodyText>
<equation confidence="0.9330106">
p(f  |h) = Z 1 exp(E AkOk (f , h)) (3) k
where Z(h) is the normalization constant
Z(h) = E exp E ( ,
AkOk f h
f k
</equation>
<bodyText confidence="0.999776">
In equation (3), Ok(f, h) are binary-valued features.
During training, instead of exploring all possible
permutations, samples are drawn given the correct
path only.
</bodyText>
<subsectionHeader confidence="0.998795">
3.1 Feature Overview
</subsectionHeader>
<bodyText confidence="0.999934285714286">
Most of our features Ok(f, h) are syntax-based.
They examine how each parse node is reordered
during translation. We also have a few non-syntax
features that inspect the surface words and part-of-
speech tags. They complement syntax features by
capturing lexical dependencies and guarding
against parsing errors. Instead of directly model-
</bodyText>
<equation confidence="0.939342166666667">
arg max p V S
( |
)
1
{v
VE
1
2
)
)(
) (
)
</equation>
<page confidence="0.7435805">
4
850
</page>
<bodyText confidence="0.999980461538461">
ing the absolute source position vj, we model the
jump from the last source position vj-1. All features
share two common components: j (for jump), and
cov (for coverage). Jumps are bucketed and
capped at 4 to prevent data sparsity. Coverage is
an integer indicating the visiting status of the
words between the jump. Coverage is 0 if none of
the words was visited prior to this step, 1 if all
were visited, and 2 if some but not all were visited.
(j, cov) are present in all features and are removed
from the descriptions below. A couple of features
use a variation of Jump and Coverage. These will
be described in the feature description.
</bodyText>
<subsectionHeader confidence="0.999672">
3.2 Parse-based Syntax Features
</subsectionHeader>
<bodyText confidence="0.999934608695652">
We use the sentence pair in Figure 1. as a work-
ing example when describing the features. Shown
in the figure are a Chinese-English parallel sen-
tence pair, the word alignments between them, and
ure 1. If a target is aligned to more than one
source, we assume the visit order is left to right.
In Figure 1, source words 2 and 8 are aligned to the
English ‘at’ and we define the visit sequence to be
8 following 2.
Chinese and English differ in the positioning of
the modifiers. In English, non-adjectival modifiers
follow the object they modify. This is most
prominent in the use of relative clauses and prepo-
sitional phrases. Chinese in contrast is a pre-
modification language where modifiers whether
adjectival, clausal or prepositional typically pre-
cede the object they modify. In Figure 1., the
Chinese prepositional phrase PP (in lightly shaded
box in the parse tree) spanning range [2,8] pre-
cedes the verb phrase VP2 at positions [9,10].
These two phrases are swapped in English as
shown by the two lightly shaded boxes in the
alignment grid. The relative clause CP (in dark
</bodyText>
<table confidence="0.616339">
Step: 1 2 3 4 5 6 7 8 9 10 11
Visit Sequence: 1 9 10 2 8 7 6 3 4 5 11
</table>
<figureCaption confidence="0.997442">
Figure 1. A Chinese-English Parallel Sentence with Chinese Parse
</figureCaption>
<bodyText confidence="0.999465777777778">
the Chinese parse tree. The parse tree is simpli-
fied. Some details such as part-of-speech tags are
omitted and denoted by triangles. The first step is
to determine the source visit sequence from the
word alignment, also shown at the bottom of Fig-
shaded box in the parse tree) in Chinese spanning
range [3,6] precedes the noun phrase NP3 at posi-
tion 7 whereas these two phrases are again
swapped in English.
</bodyText>
<page confidence="0.991243">
851
</page>
<bodyText confidence="0.999991535714286">
The phenomenon for the reordering model to
capture is that node VP1’s two children PP and
VP2 (lightly shaded) need to be swapped regard-
less of how long the PP phrase is. This is also true
for node NP2 whose two children CP and NP3
(dark shaded) need to be reversed.
Parse-based features model how to reorder the
constituents in the parse by learning how to walk
the parse nodes. For every non-unary node in the
parse we learn such features as which of its child is
visited first and for subsequent visits how to jump
from one child to another. For the treelet VP1 4
PP VP2 in Figure 1, we learn to visit the child VP2
first, then PP.
We now define the notion of ‘node visit’. When
a source word si is visited at step j, we find its path
to root from the leaf node denoted as PathToRooti.
We say all the nodes contained in PathToRooti are
being visited at that step. Parse-based features are
applied to every qualifying node in PathToRooti.
Unary extensions do not qualify and are ignored.
Since part-of-speech tags are unary branches,
parse-based features apply from the lowest-level
labels. Another condition depends on the jump
and is discussed in section §3.4. All our features
are encoded by a vector of integers and are denoted
as φ (·) in this paper. We now describe the fea-
tures.
</bodyText>
<subsectionHeader confidence="0.889152">
3.2.1 First Child Features
</subsectionHeader>
<bodyText confidence="0.998224266666667">
The first-child feature applies when a node is vis-
ited for the first time. The feature learns which of
the node’s child to visit first. This feature learns
such phenomena as translating the main verb first
under a VP or translating the main NP first under
an NP. The feature is defined as φ(currentLabel,
parentLabel, nthNode, j, cov) where
currentLabel = label of the current parse node
parentLabel = label of the parent node
nthNode = an integer indicating the nth occurrence
of the current node
In Figure 1, when source word 9 is visited at step
2, its PathToRoot is computed which is [VP2, VP1,
IP1]. The first-child feature applied to VP2 is
φ(VP2, VP1, 1, 4, 1) since
</bodyText>
<equation confidence="0.6803765">
currentLabel = VP2; parentLabel = VP1;
nthChild = 1: VP2 is the 1st VP among its parent’s
children
j = 4: actual jump from 1 is 8 and is capped.
</equation>
<bodyText confidence="0.961892285714286">
cov = 0: words in between the jump [1,9] are not
yet visited at this step.
The semantics of this feature is that when a VP
node is visited, the first VP child under it is visited
first. This feature learns to visit the first VP first
which is usually the head VP no matter where it is
positioned or how many modifiers precede it.
</bodyText>
<subsectionHeader confidence="0.661812">
3.2.2 Node Jump Features
</subsectionHeader>
<bodyText confidence="0.999848523809524">
This feature applies on all subsequent visits to the
parse node. This feature models how to jump from
one sibling to another sibling. This feature has
these components: φ(currentLabel, parentLabel,
fromLable, nodeJump,cov) where
fromLabel = the node label where the jump is from
nodeJump = node distance from that node
This feature effectively captures syntactic reorder-
ings by looking at the node jump instead of surface
distance jump. In our example, a node-jump fea-
ture for jumping from source 10 to 2 at step 4 at
VP1 level is φ(PP, VP1, VP2, -1, 2) where
currentLabel = PP where source word 2 is under
parentLabel = VP1
fromLabel = VP2 where source word 10 is under
nodeJump = -1 since the jump is from VP2 to PP
cov = 2 because in between [2,10] word 9 has been
visited and other words have not.
This feature captures the necessary information
for the ‘PP VP’ reorderings regardless of how long
the PP or VP phrase is.
</bodyText>
<subsectionHeader confidence="0.818894">
3.2.3 Jump Over Sibling Features
</subsectionHeader>
<bodyText confidence="0.999916470588235">
To make a correct jump from one sibling to the
other, siblings that are jumped over should also be
considered. For example in Chinese, while jump-
ing over a PP to cover a VP is a good jump, jump-
ing over an ADVP to cover a VP may not be
because adverbs in both Chinese and English often
precede the verb they modify. The jump-over-
sibling features help distinguish these cases. This
feature’s components are φ(currentLabel, parent-
Label, jumpOverSibling, siblingCov, j) where jum-
pOverSibling is the label of the sibling that is
jumped over and siblingCov is the coverage status
of that sibling.
This feature applies to every sibling that is
jumped over. At step 2 where the jump is from
source 1 to 9, this feature at VP1 level is φ(VP2,
VP1, PP, 0, 4) because PP is a sibling of VP2 and
</bodyText>
<page confidence="0.991441">
852
</page>
<bodyText confidence="0.9880445">
is jumped over, PP is not covered at this step, and
the jump is capped to be 4.
</bodyText>
<subsectionHeader confidence="0.691374">
3.2.4 Back Jump Sibling Features
</subsectionHeader>
<bodyText confidence="0.998967133333333">
For every forward jump of length greater than 1,
there is a backward jump to cover those words that
were skipped. In these situations we want to know
how far we can move forward before we must
jump backward. The back-jump-sibling feature
applies when the jump is backward (distance is
negative) and inspects the sibling to the right. It
generates φ(currentLabel, rightSiblingCov, j).
When jumping from 10 to 2 at step 4, this feature
is φ(PP, 1, -4) where -4 is the jump and
currentLabel = PP where source word 2 is under
rightSiblingCoverage = 1 since VP2 has been
completed visited at this time. This feature learns
to go back to PP when its right sibling (VP2) is
completed.
</bodyText>
<subsectionHeader confidence="0.496818">
3.2.5 Broken Features
</subsectionHeader>
<bodyText confidence="0.999169666666667">
Translations do not always respect the constituent
boundaries defined by the source parse tree. Con-
sider the fragment in Figure 2.
</bodyText>
<figureCaption confidence="0.999842">
Figure 2. A ‘Broken’ Tree
</figureCaption>
<bodyText confidence="0.999917857142857">
After the VV under VP2 is translated (“account
for”), a transition is made to translate the ADVP
(“approximately”) leaving VP2 partially translated.
We say that the node VP2 is broken at this step.
This type of feature has been shown to be useful
for machine translation (Marton &amp; Resnik 2008).
Here, broken features model the context under
which a node is broken by observing the feature
φ(curTag, prevTag, parentLabel, j, cov). For the
transition of source word 2 to source word 1 in
Figure 2, a broken feature applies at VP2: φ(AD,
VV, VP2, -1 ,1). This feature learns that a VP can
be broken when making a jump from a verb (VV)
to an adverb (AD).
</bodyText>
<subsectionHeader confidence="0.978736">
3.3 Non-Parse Features
</subsectionHeader>
<bodyText confidence="0.999618">
Non-parse features do not use or use less fine-
grained information from the parse tree.
</bodyText>
<subsectionHeader confidence="0.705456">
3.3.1 Barrier Features
</subsectionHeader>
<bodyText confidence="0.999865714285714">
Barrier features model the intuition that certain
words such as punctuation should not move freely.
This phenomenon has been observed and shown to
be helpful in (Xiong et. al., 2008). We call these
words barrier words. Barrier features are φ(barri-
erWord, cov, j). All punctuations are barrier
words.
</bodyText>
<subsectionHeader confidence="0.963841">
3.3.2 Number of Zero Islands Features
</subsectionHeader>
<bodyText confidence="0.999982066666667">
Although word reorderings can involve words
far apart, certain jump patterns are highly unlikely.
For example, the coverage pattern ‘1010101010’
where every other source word is translated would
be very improbable. Let the right most covered
source word be the frontier. For every jump, the
number-of-zero-islands feature computes the num-
ber of uncovered source islands to the left of the
frontier. Additionally it takes into account the
number of parse nodes in between. This feature is
defined as φ(numZeroIslands, j, num-
ParseNodesInBetween). The number of parse
nodes is the number of maximum spanning nodes
in between the jump. The jump at step 2 from
source 1 to 9 triggers this number-zero-island fea-
ture φ(1, 4, 1). The source coverage status at step 2
is 10000000100 because the first source word has
been visited and the current visit is source 9. All
words in between have not been visited. There is 1
contiguous sequence of 0’s between the first ‘1’
and the last ‘1’, hence the numZeroIslands = 1.
There is one parse node PP that spans all the
source words from 2 to 8, therefore the last argu-
ment to the feature is 1. If instead, the transition
was from source 1 to 8, then there would be 2
maximum spanning parse nodes for source [2,7]
which are nodes P and NP2. The feature would be
φ(1, 4, 2). This feature discourages scattered
jumps that leave lots of zero islands and jump over
lots of parse nodes.
</bodyText>
<subsectionHeader confidence="0.936336">
3.4 Training
</subsectionHeader>
<bodyText confidence="0.999438333333333">
Training the maximum entropy reordering model
needs word alignments and source-side parses. We
use hand alignments from LDC. The training data
</bodyText>
<page confidence="0.998061">
853
</page>
<bodyText confidence="0.968915">
statistics are shown in Table 1. We use the (Levy
and Manning 2003) parser on Chinese.
</bodyText>
<table confidence="0.866771">
Data #Sentences #Words
LDC2006E93 10,408 230,764
LDC2008E57 11,463 194,024
</table>
<tableCaption confidence="0.999492">
Table 1. Training Data
</tableCaption>
<bodyText confidence="0.912204333333333">
From the word alignments we first determine the
source visit sequence. Table 2 details how the visit
sequence is determined in various cases.
</bodyText>
<table confidence="0.9537264">
Alignment Type S-T Visit Sequence
1-1 Left to right from target
m-1 Left to right from source
1-m Left most target link
fly Attaches left
</table>
<tableCaption confidence="0.999264">
Table 2. Determining visit sequence
</tableCaption>
<bodyText confidence="0.994621142857143">
The first column shows alignment type from
source (S) to target (T). 1-1 means one source
word aligns to one target word. m-1 means many
source words align to one target and vice versa. fly
means unaligned source words.
After the source visit sequence is decided, fea-
tures are generated. Note that the height of the tree
is not uniform for all the words. To preserve the
structure and also alleviate the depth problem, we
use the lowest-level-common-ancestor approach.
For every jump, we generate features bottom up
until we reach the node that is the common ances-
tor of the origin and the destination of the jump. In
Figure 1 there is a jump from source 7 to 6 at step
7. The lowest-level-common-ancestor for source 6
and 7 is the node NP2 and features are generated
up to the level of NP2. Features on this training
data are shown in the second column in Table 5.
The MaxEnt model on this data is efficiently
trained at 15 minutes per iteration (24 sen-
tences/sec or 471 words/sec).
</bodyText>
<sectionHeader confidence="0.999959" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996125">
4.1 Reorder Evaluation
</subsectionHeader>
<bodyText confidence="0.9999889">
To evaluate how accurate the reordering model is,
we first compute its prediction accuracy. We
choose the first 100 sentences from NIST MT03 as
our test set for this evaluation. We manually word
align them to the first set of reference using LDC
annotation guidelines version 1.0 of April 2006.
An average of 73% of the training sentences con-
tain unaligned source words and over 87% of the
test sentences contain unaligned source words.
The unaligned source words are mostly function
words. Because the visit sequence of unaligned
source words are determined not by truth but by
heuristics (Table 2), they pose a problem in evalua-
tion.
We thus evaluate the model by measuring the ac-
curacy of its decision conditioned on true history.
We measure performance on the model’s top-N
choices for N = 1,2, and 3. Results are in Table 3.
The table also shows the accuracy of no reorder-
ing in the Monotone column.
</bodyText>
<table confidence="0.99098075">
Top-N Accuracy Monotone
1 80.56% 65.39%
2 90.66% -
3 93.05% -
</table>
<tableCaption confidence="0.998811">
Table 3. Reordering model performance
</tableCaption>
<figureCaption confidence="0.97922775">
Figure 3 plots accuracy vs. MaxEnt training itera-
tion. Accuracy starts low at 74.7% and reaches is
highest at iteration 8 and fluctuates around 80.5%
thereafter.
</figureCaption>
<figure confidence="0.995654416666667">
81
80
79
78
77
76
75
74
73
72
71
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
</figure>
<figureCaption confidence="0.999992">
Figure 3. Accuracy vs. MaxEnt Training Iteration
</figureCaption>
<bodyText confidence="0.915211">
We analyze 50 errors from the top-1 run. The er-
rors are categorized and shown in Table 4.
</bodyText>
<table confidence="0.9975324">
Error Category Percentage
Lexical 34%
Parse 30%
Model 20%
Reference 16%
</table>
<tableCaption confidence="0.999585">
Table 4. Error Analysis
</tableCaption>
<bodyText confidence="0.999934833333333">
‘Lexical’ errors are those that rise from lexical
choice of source words. For example, an “ADVP
VP” structure would normally be visited mono-
tonically. However, in case of Chinese phrase ‘so
do’, they should be swapped. More than a third of
the errors are of this nature. Errors in the Refer-
</bodyText>
<page confidence="0.997471">
854
</page>
<bodyText confidence="0.999019928571429">
ence category are those that are marked wrong be-
cause of the particular English reference. The pro-
posed reorderings are correct but they don’t match
the reference reorderings. Another 30% of the er-
rors are due to parsing errors. The Model errors
are due to two sources. One is the depth problem
mentioned above. Local statistics for some very
deep treelets overwhelm the global statistics and
local jumps win over the long jumps in these cases.
Another problem is the data sparseness. For ex-
ample, the model has learned to reorder the ‘PP
VP’ structure but there is not much data for ‘PP
ADVP VP’. The model fails to jump over PP into
ADVP.
</bodyText>
<subsectionHeader confidence="0.99396">
4.2 Feature Utility
</subsectionHeader>
<bodyText confidence="0.999955636363636">
We conduct ablation studies to see the utilities of
each feature. We take the best feature set which
gives the performance in Table 3 and takes away
one feature type at a time. The results are in Table
5. The first row keeps all the features. The Sub-
tract column shows performance after subtracting
each feature while keeping all the other features.
The Add column shows performance of adding the
feature. Using just first-child features gets
75.97%. Adding node-jump features moves the
accuracy to 78.40% and so on.
</bodyText>
<table confidence="0.9994258">
Features #Features Sub- Add
tract
- 80.56% -
First Child 7,559 79.87% 75.97%
Node Jump 6,334 79.52% 78.40%
JumpOver Sib. 2,403 80.52% 79.00%
BackJump 602 80.48% 79.05%
Broken 15,183 80.30% 79.13%
Barrier 158 80.26% 79.22%
NumZ Islands 200 79.52% 80.56%
</table>
<tableCaption confidence="0.999842">
Table 5. Ablation study on features
</tableCaption>
<sectionHeader confidence="0.992481" genericHeader="method">
5 Translation Experiments
</sectionHeader>
<subsectionHeader confidence="0.999016">
5.1 Reorder Lattice Generation
</subsectionHeader>
<bodyText confidence="0.99752695">
The reordering model is used to generate reorder
lattices which are used by machine translation de-
coders. Reorder lattices have been frequently used
in decoding in works such as (Zhang et. al 2007,
Kumar et.al. 2005, Hildebrand et.al. 2008), to
name just a few. The main difference here is that
our lattices encode probabilities from the reorder-
ing model and are not used to preorder the source.
The lattice contains reorderings and their cost
(negative log probability). Figure 4 shows a reor-
der lattice example. Nodes are lattice states. Arcs
store source word positions to be visited (trans-
lated) and their cost and they are delimited by
comma in the figure. Lower cost indicates better
choice. Figure 4 is much simplified for readability.
It shows only the best path (highlighted) and a few
neighboring arcs. For example, it shows source
words 1, 2, and 8 are the top 3 choices at step 1.
Position 1 is the best choice with the lowest cost of
0.302 and so on.
</bodyText>
<figureCaption confidence="0.999044">
Figure 4. A lattice example
</figureCaption>
<bodyText confidence="0.9999545">
The sentence is shown at the bottom of the figure.
The first part of the reference (true) path is indi-
cated by the alignment which is source sequence 1,
8, 9, and 2. We see that this matches the lattice’s
top-1 choice.
Lattice generation takes source sentence and
source parse as input. The lattice generation proc-
ess makes use of a beam search algorithm. Every
node in the lattice generates top-N next possible
positions and the rest is pruned away. A coverage
vector is maintained on each path to ensure each
source word is visited exactly once. A wide
beam width explores many source positions at any
step and results in a bushy lattice. This is needed
for machine translation because the parses are er-
rorful. The structures that are hard for MT to reor-
der are also hard for parsers to parse. Labels criti-
cal to reordering such as CP are among the least
accurate labels. Overall parsing accuracy is
83.63% but CP accuracy is 73.11%. We need a
wide beam to include more long jumps to compen-
sate the parsing errors.
</bodyText>
<subsectionHeader confidence="0.986594">
5.2 Machine Translation
</subsectionHeader>
<bodyText confidence="0.9965745">
We run MT experiments on NIST Chinese-English
test sets MT03-05. We compare the performance
</bodyText>
<page confidence="0.995941">
855
</page>
<bodyText confidence="0.999754785714286">
of using distance-based reordering and using maxi-
mum entropy reordering lattices. The decoder is a
log-linear phrase based decoder. Translation mod-
els are trained from HMM alignments. A
smoothed 5-gram English LM is built on the Eng-
lish Gigaword corpus and English side of the Chi-
nese-English parallel corpora. In the experiments,
lexicalized distance-based reordering allows up to
9 words to be jumped over. MT performance is
measured by BLEUr4n4 (Papineni et.al. 2001).
The test set statistics and experiment results are
show in Table 6. Decoding with MaxEnt reorder
lattices shows significant improvement for all con-
ditions.
</bodyText>
<table confidence="0.997996">
Data #Segs Lex Reord Lattice Gain
Skip-9
MT03 919 0.3005 0.3315 +3.1
MT04 1788 0.3250 0.3388 +1.38
MT05 1082 0.2957 0.3236 +2.79
</table>
<tableCaption confidence="0.910327">
Table 6. MT results
</tableCaption>
<bodyText confidence="0.999918857142857">
Figures 5 shows an example from MT output
with word alignments to the Chinese input. The
MaxEnt reordering model correctly reorders two
source modifiers at source positions 8 and 22. The
Skip9 output reorders locally whereas the MaxEnt
lattice output shows much more complex reorder-
ings.
</bodyText>
<sectionHeader confidence="0.999681" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999965315789474">
We present a direct syntax-based reordering model
that captures source structural information. The
model is capable of handling reorderings of arbi-
trary length. Long-range reorderings are essential
in translation between languages with great word
order differences such as Chinese-English and
Arabic-English. We have shown that phrase based
SMT can benefit significantly from such a reorder-
ing model.
The current model is not regularized and feature
selection by thresholding the feature counts is quite
primitive. Regularizing the model will prevent
overfitting, especially given the small training data
set. Regularization will also make the ablation
study more meaningful.
The reordering model presented here aims at
capturing structural differences between source
and target languages. It does not have enough
lexical features to deal with lexical idiosyncrasies.
</bodyText>
<figure confidence="0.565885">
ME Lattice MT Skip9 MT
</figure>
<figureCaption confidence="0.99795">
Figure 5. MT comparison
</figureCaption>
<bodyText confidence="0.999953666666667">
Our initial attempt at adding lexical pair jump fea-
tures φ(fromWord, toWord, j) has not proved use-
ful. It hurt accuracy by 3% (from 80% to 77%).
We see from Table 4 that 34% of the errors are due
to source lexical choices which indicates the weak-
ness of the current lexical features. Regularization
of the model might also make a difference with the
lexical features.
Reordering and word choice in translation are not
independent of each other. We have shown some
initial success with a separate reordering model. In
the future, we will build joint models on reordering
and translation. This approach will also address
some of the reordering problems due to source
lexical idiosyncrasies.
</bodyText>
<sectionHeader confidence="0.997277" genericHeader="acknowledgments">
7 Acknowledgement
</sectionHeader>
<bodyText confidence="0.999772875">
We would like to acknowledge the support of
DARPA under Grant HR0011-08-C-0110 for fund-
ing part of this work. The views, opinions, and/or
findings contained in this article are those of the
author and should not be interpreted as represent-
ing the official views or policies, either expressed
or implied, of the Defense Advanced Research
Projects Agency or the Department of Defense.
</bodyText>
<sectionHeader confidence="0.99485" genericHeader="references">
References
</sectionHeader>
<page confidence="0.99663">
856
</page>
<reference confidence="0.999860210526316">
A.S.Hildebrand, K.Rottmann, Mohamed Noamany, Qin
Gao, S. Hewavitharana, N. Bach and Stephan Voga.
2008. Recent Improvements in the CMU Large Scale
Chinese-English SMT System. In Proceedings of
ACL 2008 (Short Papers)
C. Wang, M. Collins, and Philipp Koehn. 2007. Chi-
nese Syntactic Reordering for Statistical Machine
Translation. In Proceedings of EMNLP 2007
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,
Minghui Li, and Yi Guan. 2007. A Probabilistic
Approach to syntax-based Reordering for Statistical
Machine Translation. In Proceedings of ACL 2007.
Christoph Tillmannn. 2004. A Block Orientation
Model for Statistical Machine Translation. In Pro-
ceedings of HLT-NAACL 2004.
David Chiang. 2005. A Hierarchical Phrase-based
Model for Statistical Machine Translation. In Pro-
ceedings of ACL 2005.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Compuntational Linguistics, Vol. 23, pp 377-
404
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for
Statistical Machine Translation. In Proceedings of
ACL 2006.
Deyi Xiong, Min Zhang, Aiti Aw, Haitao Mi, Qun Liu
and Shouxun Lin. 2008. Refinements in FTG-based
Statistical Machine Translation. In Proceedings of
ICJNLP 2008
Dongdong Zhang, Mu Li, Chi-Ho Li, and Ming Zhou.
2007. Phrase Reordering Model Integrating Syntac-
tic Knowledge for SMT. In Proceedings of EMNLP
2007
Fei Xia and Michael McCord. 2004. Improving a Sta-
tistical MT System with Automatically Learned Re-
write Patterns. In Proceedings of COLING 2004.
Franz Josef Och and Hermann Ney. 2004. The Align-
ment Template Approach to Statistical Machine
Translation. Computational Linguistics, Vol. 30(4).
pp. 417-449
Kenji Yamada and Kevin Knight 2001. A Syntax-based
Statistical Translation Model. In Proceedings of
ACL 2001
Kevin Knight. 1999. Decoding Complexity in Word
Replacement Translation Models. Computational
Linguistics, 25(4):607-615
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2001. A Method for Automatic Evaluation
for MT. In Proceedings of ACL 2001
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings of ACL 2005.
Michell Galley, Christoph D. Manning. 2008. A Simple
and Effective Hierarchical Phrase Reordering
Model. Proceedings of the EMNLP 2008
Perter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation.
Computation Linguistics, 19(2).
Philip Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In Pro-
ceedings of NLT/NAACL 2003.
Richard Zens, Hermann Ney, Taro Watanabe, and Eii-
chiro Sumita. 2004. Reordering Constraints for
Phrase-based Statistical Machine Translation. In
Proceedings of COLING 2004.
Richard Zens and Hermann Ney. 2006. Discriminative
Reordering Models for Statistical Machine Transla-
tion. In Proceedings of the Workshop on Statistical
Machine Translation, 2006.
Roger Levy and Christoph Manning. 2003. Is it harder
to parse Chinese, or the Chinese Treebank? In Pro-
ceedings of ACL 2003
Shankar Kumar and William Byrne. 2005. Local
Phrase Roerdering Models for Statistical Machine
Translation. In Proceedings of HLT/EMNLP 2005
Stephan Kanthak, David Vilar, Evgeny Matusov, Rich-
ard Zens, and Hermann Ney. 2005. Novel Reorder-
ing Approaches in Phrase-based Statistical Machine
Translation. In Proceedings of the Workshop on
Building and Using Parallel Texts 2005.
Y. Al-Onaizan . K. 2006 Distortion Models for Statisti-
cal Machine Translation. In Proceedings of ACL
2006.
Yizhao Ni, C.J.Saunders, S. Szedmak and M.Niranjan
2009 Handling phrase reorderings for machine
translation. In Proceedings of ACL2009
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007.
Improved Chunk-level Reordering for Statistical Ma-
chine Translation. In Proceedings of HLT/NAACL
2007.
Yuval Marton and Philip Resnik. 2008. Soft Syntactic
Constraints for Hierarchical Phrased-based Transla-
tion. In Proceedings of ACL 2008.
</reference>
<page confidence="0.998199">
857
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.557377">
<title confidence="0.9988335">A Direct Syntax-Driven Reordering Model for Phrase-Based Machine Translation</title>
<author confidence="0.823294">Niyu</author>
<affiliation confidence="0.8273375">IBM T.J.Watson Yorktown Heights, NY</affiliation>
<email confidence="0.998144">niyuge@us.ibm.com</email>
<abstract confidence="0.991246380952381">This paper presents a direct word reordering model with novel syntax-based features for statistical machine translation. Reordering models address the problem of reordering source language into the word order of the target language. IBM Models 3 through 5 have reordering components that use surface word information but very little context information to determine the traversal order of the source sentence. Since the late 1990s, phrase-based machine translation solves much of the local reorderings by using phrasal translations. The problem of longdistance reordering has become a central research topic in modeling distortions. We present a syntax driven maximum entropy reordering model that directly predicts the source traversal order and is able to model arbitrarily long distance word movement. We show that this model significantly improves machine translation quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Rottmann A S Hildebrand</author>
<author>Mohamed Noamany</author>
<author>Qin Gao</author>
<author>S Hewavitharana</author>
<author>N Bach</author>
<author>Stephan Voga</author>
</authors>
<date>2008</date>
<booktitle>Recent Improvements in the CMU Large Scale Chinese-English SMT System. In Proceedings of ACL</booktitle>
<note>(Short Papers)</note>
<marker>Hildebrand, Noamany, Gao, Hewavitharana, Bach, Voga, 2008</marker>
<rawString>A.S.Hildebrand, K.Rottmann, Mohamed Noamany, Qin Gao, S. Hewavitharana, N. Bach and Stephan Voga. 2008. Recent Improvements in the CMU Large Scale Chinese-English SMT System. In Proceedings of ACL 2008 (Short Papers)</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wang</author>
<author>M Collins</author>
<author>Philipp Koehn</author>
</authors>
<title>Chinese Syntactic Reordering for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>C. Wang, M. Collins, and Philipp Koehn. 2007. Chinese Syntactic Reordering for Statistical Machine Translation. In Proceedings of EMNLP 2007</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-Ho Li</author>
<author>Dongdong Zhang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Minghui Li</author>
<author>Yi Guan</author>
</authors>
<title>A Probabilistic Approach to syntax-based Reordering for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Li, Zhang, Li, Zhou, Li, Guan, 2007</marker>
<rawString>Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou, Minghui Li, and Yi Guan. 2007. A Probabilistic Approach to syntax-based Reordering for Statistical Machine Translation. In Proceedings of ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmannn</author>
</authors>
<title>A Block Orientation Model for Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<marker>Tillmannn, 2004</marker>
<rawString>Christoph Tillmannn. 2004. A Block Orientation Model for Statistical Machine Translation. In Proceedings of HLT-NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A Hierarchical Phrase-based Model for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A Hierarchical Phrase-based Model for Statistical Machine Translation. In Proceedings of ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora.</title>
<date>1997</date>
<journal>Compuntational Linguistics,</journal>
<volume>23</volume>
<pages>377--404</pages>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Compuntational Linguistics, Vol. 23, pp 377-404</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation. In Proceedings of ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Aiti Aw</author>
<author>Haitao Mi</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Refinements in FTG-based Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ICJNLP</booktitle>
<marker>Xiong, Zhang, Aw, Mi, Liu, Lin, 2008</marker>
<rawString>Deyi Xiong, Min Zhang, Aiti Aw, Haitao Mi, Qun Liu and Shouxun Lin. 2008. Refinements in FTG-based Statistical Machine Translation. In Proceedings of ICJNLP 2008</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dongdong Zhang</author>
<author>Mu Li</author>
<author>Chi-Ho Li</author>
<author>Ming Zhou</author>
</authors>
<title>Phrase Reordering Model Integrating Syntactic Knowledge for SMT.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<marker>Zhang, Li, Li, Zhou, 2007</marker>
<rawString>Dongdong Zhang, Mu Li, Chi-Ho Li, and Ming Zhou. 2007. Phrase Reordering Model Integrating Syntactic Knowledge for SMT. In Proceedings of EMNLP 2007</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a Statistical MT System with Automatically Learned Rewrite Patterns.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="5578" citStr="Xia and McCord 2004" startWordPosition="864" endWordPosition="867">al orientation model that captures some non-local phrase reorderings by a shift reduce algorithm. Because of the heavy use of lexical features, these models tend to suffer from data sparseness problems. Another limitation is that these models are restricted to reorderings with no gaps and phrases that are adjacent. We present a probabilistic reordering model that models directly the source translation sequence and explicitly assigns probabilities to the reorderings of the source input with no restrictions on gap, length or adjacency. This is different from the approaches of pre-order such as (Xia and McCord 2004; Collins et.al. 2005; Kanthak et. al. 2005; Li et. al., 2007). Although our model can be used to produce top N pre-ordered source, the experiments reported here do not use the model in the pre-order mode. Instead, the reordering model is used to generate a reorder lattice which encodes many reorderings and their costs (negative log probability). This reorder lattice is independent of the translation decoder. In principle, any decoder can use this lattice for its reordering needs. We have integrated the reorder lattice into a phrasebased. The experiments reported here are from the phrase-based</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a Statistical MT System with Automatically Learned Rewrite Patterns. In Proceedings of COLING 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The Alignment Template Approach to Statistical Machine Translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<pages>417--449</pages>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The Alignment Template Approach to Statistical Machine Translation. Computational Linguistics, Vol. 30(4). pp. 417-449</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A Syntax-based Statistical Translation Model.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight 2001. A Syntax-based Statistical Translation Model. In Proceedings of ACL 2001</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>Decoding Complexity in Word Replacement Translation Models. Computational Linguistics,</title>
<date>1999</date>
<pages>25--4</pages>
<contexts>
<context position="2103" citStr="Knight 1999" startWordPosition="318" endWordPosition="319"> fluent English. In this paper, we present a word reordering model that models the word reordering process in translation. The paper is organized as follows. §2 outlines previous approaches to reordering. §3 details our model and its training and decoding process. §4 discusses experiments to evaluate the model and §5 presents machine translation results. §6 is discussion and conclusion. 2 Previous Work The word reordering problem has been one of the major problems in statistical machine translation (SMT). Since exploring all possible reorderings of a source sentence is an NP-complete problem (Knight 1999), SMT systems limit words to be reordered within a window of length k. IBM Models 3 through 5 (Brown et.al. 1993) model reorderings based on surface word information. For example, Model 4 attempts to assign target-language positions to source-language words by modeling d(j |i, l, m) where j is the target-language position, i is the source-language position, l and m are respectively source and target sentence lengths. These models are not effective in modeling reorderings because they don’t have enough context and lack structural information. Phrase-based SMT systems such as (Koehn et.al. 2003)</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>Kevin Knight. 1999. Decoding Complexity in Word Replacement Translation Models. Computational Linguistics, 25(4):607-615</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Weijing Zhu</author>
</authors>
<title>A Method for Automatic Evaluation for MT.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu. 2001. A Method for Automatic Evaluation for MT. In Proceedings of ACL 2001</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kucerova</author>
</authors>
<title>Clause Restructuring for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause Restructuring for Statistical Machine Translation. In Proceedings of ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michell Galley</author>
<author>Christoph D Manning</author>
</authors>
<title>A Simple and Effective Hierarchical Phrase Reordering Model.</title>
<date>2008</date>
<booktitle>Proceedings of the EMNLP</booktitle>
<marker>Galley, Manning, 2008</marker>
<rawString>Michell Galley, Christoph D. Manning. 2008. A Simple and Effective Hierarchical Phrase Reordering Model. Proceedings of the EMNLP 2008</rawString>
</citation>
<citation valid="true">
<authors>
<author>Perter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<date>1993</date>
<journal>The Mathematics of Statistical Machine Translation. Computation Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Perter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation. Computation Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NLT/NAACL</booktitle>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philip Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-based Translation. In Proceedings of NLT/NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Reordering Constraints for Phrase-based Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING</booktitle>
<marker>Zens, Ney, Watanabe, Sumita, 2004</marker>
<rawString>Richard Zens, Hermann Ney, Taro Watanabe, and Eiichiro Sumita. 2004. Reordering Constraints for Phrase-based Statistical Machine Translation. In Proceedings of COLING 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative Reordering Models for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<marker>Zens, Ney, 2006</marker>
<rawString>Richard Zens and Hermann Ney. 2006. Discriminative Reordering Models for Statistical Machine Translation. In Proceedings of the Workshop on Statistical Machine Translation, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Christoph Manning</author>
</authors>
<title>Is it harder to parse Chinese, or the Chinese Treebank?</title>
<date>2003</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="17967" citStr="Levy and Manning 2003" startWordPosition="3083" endWordPosition="3086">parse node PP that spans all the source words from 2 to 8, therefore the last argument to the feature is 1. If instead, the transition was from source 1 to 8, then there would be 2 maximum spanning parse nodes for source [2,7] which are nodes P and NP2. The feature would be φ(1, 4, 2). This feature discourages scattered jumps that leave lots of zero islands and jump over lots of parse nodes. 3.4 Training Training the maximum entropy reordering model needs word alignments and source-side parses. We use hand alignments from LDC. The training data 853 statistics are shown in Table 1. We use the (Levy and Manning 2003) parser on Chinese. Data #Sentences #Words LDC2006E93 10,408 230,764 LDC2008E57 11,463 194,024 Table 1. Training Data From the word alignments we first determine the source visit sequence. Table 2 details how the visit sequence is determined in various cases. Alignment Type S-T Visit Sequence 1-1 Left to right from target m-1 Left to right from source 1-m Left most target link fly Attaches left Table 2. Determining visit sequence The first column shows alignment type from source (S) to target (T). 1-1 means one source word aligns to one target word. m-1 means many source words align to one tar</context>
</contexts>
<marker>Levy, Manning, 2003</marker>
<rawString>Roger Levy and Christoph Manning. 2003. Is it harder to parse Chinese, or the Chinese Treebank? In Proceedings of ACL 2003</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Local Phrase Roerdering Models for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP</booktitle>
<marker>Kumar, Byrne, 2005</marker>
<rawString>Shankar Kumar and William Byrne. 2005. Local Phrase Roerdering Models for Statistical Machine Translation. In Proceedings of HLT/EMNLP 2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Kanthak</author>
<author>David Vilar</author>
<author>Evgeny Matusov</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Novel Reordering Approaches in Phrase-based Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Building and Using Parallel Texts</booktitle>
<marker>Kanthak, Vilar, Matusov, Zens, Ney, 2005</marker>
<rawString>Stephan Kanthak, David Vilar, Evgeny Matusov, Richard Zens, and Hermann Ney. 2005. Novel Reordering Approaches in Phrase-based Statistical Machine Translation. In Proceedings of the Workshop on Building and Using Parallel Texts 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
</authors>
<title>Distortion Models for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Al-Onaizan, 2006</marker>
<rawString>Y. Al-Onaizan . K. 2006 Distortion Models for Statistical Machine Translation. In Proceedings of ACL 2006.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yizhao Ni</author>
<author>S C J Saunders</author>
</authors>
<title>Szedmak and M.Niranjan 2009 Handling phrase reorderings for machine translation.</title>
<booktitle>In Proceedings of ACL2009</booktitle>
<marker>Ni, Saunders, </marker>
<rawString>Yizhao Ni, C.J.Saunders, S. Szedmak and M.Niranjan 2009 Handling phrase reorderings for machine translation. In Proceedings of ACL2009</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuqi Zhang</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Chunk-level Reordering for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT/NAACL</booktitle>
<marker>Zhang, Zens, Ney, 2007</marker>
<rawString>Yuqi Zhang, Richard Zens, and Hermann Ney. 2007. Improved Chunk-level Reordering for Statistical Machine Translation. In Proceedings of HLT/NAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft Syntactic Constraints for Hierarchical Phrased-based Translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="15494" citStr="Marton &amp; Resnik 2008" startWordPosition="2657" endWordPosition="2660">blingCoverage = 1 since VP2 has been completed visited at this time. This feature learns to go back to PP when its right sibling (VP2) is completed. 3.2.5 Broken Features Translations do not always respect the constituent boundaries defined by the source parse tree. Consider the fragment in Figure 2. Figure 2. A ‘Broken’ Tree After the VV under VP2 is translated (“account for”), a transition is made to translate the ADVP (“approximately”) leaving VP2 partially translated. We say that the node VP2 is broken at this step. This type of feature has been shown to be useful for machine translation (Marton &amp; Resnik 2008). Here, broken features model the context under which a node is broken by observing the feature φ(curTag, prevTag, parentLabel, j, cov). For the transition of source word 2 to source word 1 in Figure 2, a broken feature applies at VP2: φ(AD, VV, VP2, -1 ,1). This feature learns that a VP can be broken when making a jump from a verb (VV) to an adverb (AD). 3.3 Non-Parse Features Non-parse features do not use or use less finegrained information from the parse tree. 3.3.1 Barrier Features Barrier features model the intuition that certain words such as punctuation should not move freely. This phen</context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft Syntactic Constraints for Hierarchical Phrased-based Translation. In Proceedings of ACL 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>