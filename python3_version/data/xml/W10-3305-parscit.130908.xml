<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9723025">
Learning Semantic Network Patterns for Hypernymy
Extraction
</title>
<author confidence="0.845355">
Tim vor der Br¨uck
</author>
<affiliation confidence="0.614091">
Intelligent Information and Communication Systems (IICS)
FernUniversit¨at in Hagen
</affiliation>
<email confidence="0.564001">
tim.vorderbrueck@fernuni-hagen.de
</email>
<sectionHeader confidence="0.96676" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99962435">
Current approaches of hypernymy ac-
quisition are mostly based on syntactic
or surface representations and extract
hypernymy relations between surface
word forms and not word readings.
In this paper we present a purely se-
mantic approach for hypernymy ex-
traction based on semantic networks
(SNs). This approach employs a set
of patterns
suB0(a1, a2) ← premise where the
premise part of a pattern is given by a
SN. Furthermore this paper describes
how the patterns can be derived by
relational statistical learning following
the Minimum Description Length prin-
ciple (MDL). The evaluation demon-
strates the usefulness of the learned
patterns and also of the entire hyper-
nymy extraction system.
</bodyText>
<sectionHeader confidence="0.995602" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999927137931035">
A concept is a hypernym of another concept
if the first concept denotes a superset of the
second. For instance, the class of animals is a
superset of the class of dogs. Thus, animal is
a hypernym of its hyponym dog and a hyper-
nymy relation holds between animal and dog.
A large collection of hypernymy (supertype)
relations is needed for a multitude of tasks
in natural language processing. Hypernyms
are required for deriving inferences in ques-
tion answering systems, they can be employed
to identify similar words for information re-
trieval or they can be useful to avoid word-
repetition in natural language generation sys-
tems. To build a taxonomy manually requires
a large amount of work. Thus, automatic ap-
proaches for their construction are preferable.
In this work we introduce a semantically ori-
ented approach where the hypernyms are ex-
tracted using a set of patterns which are nei-
ther syntactic nor surface-oriented but instead
purely semantic and are based on a SN for-
malism. The patterns are applied on a set
of SNs which are automatically derived from
the German Wikipediai by a deep syntactico-
semantic analysis. Furthermore, these pat-
terns are automatically created by a machine
learning approach based on the MDL princi-
ple.
</bodyText>
<sectionHeader confidence="0.998229" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.991741576923077">
Patterns for hypernymy extraction were first
introduced by Hearst (Hearst, 1992), the so-
called Hearst patterns. An example of such a
pattern is:
NPhypo {,NPhypo}*{,} and other NPhyper.
These patterns are applied on arbitrary
texts and the instantiated variables NPhypo
and NPhyper are then extracted as a concrete
hypernymy relation.
Apart from the handcrafted patterns there
was also some work to determine patterns
automatically from texts (Snow and others,
2005). For that, Snow et al. collected sen-
tences in a given text corpus with known hy-
pernym noun pairs. These sentences are then
parsed by a dependency parser. Afterwards,
the path in the dependency tree is extracted
which connects the corresponding nouns with
each other. To account for certain key words
indicating a hypernymy relation like such (see
first Hearst pattern) they added the links to
the word on either side of the two nouns (if not
yet contained) to the path too. Frequently oc-
&apos;Note that for better readability the examples are
translated from German into English throughout this
paper.
</bodyText>
<page confidence="0.98816">
38
</page>
<note confidence="0.8411885">
Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 38–47,
Beijing, August 2010
</note>
<bodyText confidence="0.99993936">
curring paths are then learned as patterns for
indicating a hypernymy relation.
An alternative approach for learning pat-
terns which is based on a surface instead of
a syntactic representation was proposed by
Morin et al. (Morin and Jaquemin, 2004).
They investigate sentences containing pairs of
known hypernyms and hyponyms as well. All
these sentences are converted into so-called
“lexico-syntactic expressions” where all NPs
and lists of NPs are replaced by special sym-
bols, e.g.: NP find in NP such as LIST. A
similarity measure between two such expres-
sions is defined as the sum of the maximal
length of common substrings for the maxi-
mum text windows before, between and after
the hyponym/hypernym pair. All sentences
are then clustered according to this similarity
measure. The representative pattern (called
candidate pattern) of each cluster is defined to
be the expression with the lowest mean square
error (deviation) to all other expressions in
the same similarity cluster. The patterns to
be used for hyponymy detection are the can-
didate patterns of all clusters found.
</bodyText>
<sectionHeader confidence="0.997374" genericHeader="method">
3 MultiNet
</sectionHeader>
<bodyText confidence="0.973071095238095">
MultiNet is an SN formalism (Helbig, 2006).
In contrast to SNs like WordNet (Fellbaum,
1998) or GermaNet (Hamp and Feldweg,
1997), which contain lexical relations between
synsets, MultiNet is designed to comprehen-
sively represent the semantics of natural lan-
guage expressions. An SN in the MultiNet
formalism is given as a set of vertices and
arcs where the vertices represent the concepts
(word readings) and the arcs the relations (or
functions) between the concepts. A vertex can
be lexicalized if it is directly associated to a
lexical entry or non-lexicalized. An example
SN is shown in Fig. 1. Note that each vertex
of the SN is assigned both a unique ID (e.g.,
c2) and a label which is the associated lexical
entry for lexicalized vertices and anon for non-
lexicalized vertices. Thus, two SNs differing
only by the IDs of the non-lexicalized vertices
are considered equivalent. Important Multi-
Net relations/functions are (Helbig, 2006):
</bodyText>
<listItem confidence="0.994271631578948">
• AGT: Conceptual role: Agent
• ATTR: Specification of an attribute
• VAL: Relation between a specific at-
tribute and its value
• PROP: Relation between object and prop-
erty
• *ITMS: Function enumerating a set
• PRED: Predicative concept characterizing
a plurality
• OBJ: Neutral object
• SUB0: Relation of conceptual subordi-
nation (hyponymy) and hyperrelation to
SUBR, SUBS, and SUB
• SUBS: Relation of conceptual subordina-
tion (for situations)
• SUBR: Relation of conceptual subordina-
tion (for relations)
• SUB: Relation of conceptual subordina-
tion other than SUBS and SUBR
</listItem>
<bodyText confidence="0.997071846153846">
MultiNet is supported by a semantic lexicon
(Hartrumpf and others, 2003) which defines,
in addition to traditional grammatical entries
like gender and number, semantic information
consisting of one or more ontological sorts and
several semantic features for each lexicon en-
try. The ontological sorts (more than 40) form
a taxonomy. In contrast to other taxonomies,
ontological sorts are not necessarily lexical-
ized, i.e., they need not denote lexical entries.
The following list shows a small selection of
ontological sorts which are inherited from ob-
ject:
</bodyText>
<listItem confidence="0.999988">
• Concrete objects: e.g., milk, honey
– Discrete objects: e.g., chair
– Substances: e.g.,, milk, honey
• Abstract objects: e.g., race, robbery
</listItem>
<bodyText confidence="0.7968188">
Semantic features denote certain semantic
properties for objects. Such a property can
either be present, not present or underspeci-
fied. A selection of several semantic features
is given below:
ANIMAL, ANIMATE, ARTIF (artificial), HUMAN,
SPATIAL, THCONC (theoretical concept)
Example for the concept bottle.1.12: dis-
crete object; ANIMAL -, ANIMATE -, ARTIF +,
HUMAN -, SPATIAL +, THCONC -, .. .
</bodyText>
<footnote confidence="0.872723">
2the suffix .1.1 denotes the reading numbered .1.1
of the word bottle.
</footnote>
<page confidence="0.996304">
39
</page>
<figure confidence="0.756476">
c9
denote.1.1
</figure>
<figureCaption confidence="0.919392">
Figure 1: Matching a pattern to an SN. Bold lines indicate matched arcs, the dashed line the
inferred arc.
</figureCaption>
<figure confidence="0.6931682">
c7
a1=skyscraper.1.1
c3
SUBS
c5
c8
very.1.1
c1
c6
c4
present.0
c10
a2=house.1.1
tall.1.1
c2
</figure>
<bodyText confidence="0.9960908">
The SNs as described here are automati-
cally constructed from (German) texts by the
deep linguistic parser WOCADI3(Hartrumpf,
2002) whose parsing process is based on a
word class functional analysis.
</bodyText>
<sectionHeader confidence="0.912114" genericHeader="method">
4 Application of Deep Patterns
</sectionHeader>
<bodyText confidence="0.999596588235294">
The extraction of hyponyms as described here
is based on a set of patterns. Each pattern
consists of a conclusion part suB0(a1 , a2) and
a premise part in form of an SN where both a1
and a2 have to show up. The patterns are ap-
plied by a pattern matcher (or automated the-
orem prover if axioms are used) which matches
the premise with an SN. The variable bindings
for a1 and a2 are given by the matched con-
cepts of the SN. An example pattern which
matches to the sentence: A skyscraper de-
notes a very tall building. is D4 (see Ta-
ble 1). The pattern matching process is il-
lustrated in Fig.1. The resulting instantiated
conclusion which is stored in the knowledge
base is suB0(skyscraper.1.1, house.1.1). Ad-
vantages by using the MultiNet SN formalism
</bodyText>
<footnote confidence="0.9031595">
3WOCADI is the abbreviation for word class
disambiguation.
</footnote>
<bodyText confidence="0.999823791666667">
for hypernym (and instance-of relation) acqui-
sition consists of: learning relations between
word readings instead of words, the possibil-
ity to apply logical axioms and background
knowledge, and that person names are already
parsed.
An example sentence from the Wikipedia
corpus where a hypernymy relation was suc-
cessfully extracted by our deep approach and
which illustrates the usefulness of this ap-
proach is: In any case, not all incidents
from the Bermuda Triangle or from other
world areas are fully explained. From this sen-
tence, a hypernymy pair cannot be extracted
by the Hearst pattern X or other Y. The ap-
plication of this pattern fails due to the word
from which cannot be matched. To extract
this relation by means of shallow patterns an
additional pattern would have to be intro-
duced. This could also be the case if syntactic
patterns were used instead since the coordina-
tion of Bermuda Triangle and world areas is
not represented in the syntactic constituency
tree but only on a semantic level4.
</bodyText>
<footnote confidence="0.6154415">
4Note that some dependency parsers normalize
some syntactic variations too.
</footnote>
<page confidence="0.997294">
40
</page>
<sectionHeader confidence="0.781543666666667" genericHeader="method">
5 Graph Substructure Learning By
Following the Minimum
Description Length Principle
</sectionHeader>
<bodyText confidence="0.998797465116279">
In this section, we describe how the patterns
can be learned by a supervised machine learn-
ing approach following the Minimum Descrip-
tion Length principle. This principle states
that the best hypothesis for a given data set
is that one which minimizes the description
of the data (Rissanen, 1989), i.e., compresses
the data the most. Basically we follow the
substructure learning approach of Cook and
Holder (Cook and Holder, 1994).
According to this approach, the description
length to minimize is the number of bits re-
quired to encode a certain graph which is com-
pressed by means of a substructure. If a lot
of graph vertices can be matched with the
substructure vertices, this description length
will be quite small. For our learning scenario
we investigate collection of SNs containing a
known hypernymy relationship. A pattern
(given by a substructure in the premise) which
compresses this set quite well is expected to be
useful for extracting hypernyms.
Let us first determine the number of bits to
encode the entire graph or SN. A graph can be
represented by its adjacency matrix and a set
of vertex and arc labels. Since an adjacency
matrix consists only of ones and zeros, it is
well suitable for a binary encoding. For the
encoding process, we do not regard the label
names directly but instead their number as-
suming an ordering exists on the label names
(e.g., alphabetical).
c1 c2 c3 c4 c5 c6 c7 c8 c9 c10
c1 0 0 0 0 0 0 0 0 0 0
c2 0 0 0 0 0 0 0 0 0 0
c3 0 0 0 0 0 0 0 0 0 0
c4 1 1 0 0 0 0 0 0 0 0
c5 0 0 1 0 0 0 0 0 0 0
c6 0 0 0 0 0 0 1 0 0 0
c7 0 0 0 0 0 0 0 0 0 0
c8 0 0 0 0 1 1 0 0 1 1
c9 0 0 0 0 0 0 0 0 0 0
c10 0 0 0 0 0 0 0 0 0 0
</bodyText>
<figureCaption confidence="0.992163">
Figure 2: Adjacency matrix of the SN.
</figureCaption>
<bodyText confidence="0.99980936">
To encode all labels the number of labels
and a list of all label numbers have to be spec-
ified, e.g., 3,1,2,1 for 3 vertices with two dif-
ferent label numbers5 (1,2). The first number
encoding (3) starts at position 0 in the bit
string, the second (1) at position 2 = Flog2 3],
the third one at position 2+Flog2 2], etc. Since
the graph actually need not to be encoded in
this way but only the length of the encoding
is important, non-integer numbers of bits are
accepted for simplicity too. If there are a total
of lu different labels, then each encoded label
number requires log2(lu) bits. The total num-
ber of bits to encode the vertex labels are then
given by:
vbits = log2(v) + v log2(lu) in which v denotes
the total number of vertices6.
In the next step, the adjacency matrix is en-
coded where each row is processed separately.
A straightforward approach for encoding one
row would be to use v number of bits, one for
every column. However, the number of zeros
are generally much larger than the number of
ones which means that a better compression
of the data is possible by exploiting this fact.
</bodyText>
<equation confidence="0.6617855">
Consider the case that a certain matrix row
( v )
contains exactly m ones. There are
m
</equation>
<bodyText confidence="0.953157470588235">
possibilities to distribute the ones to the indi-
vidual cells. All possible permutations could
be specified in a list. In this case it is only
necessary to specify the position in this list to
uniquely describe one row. Let b = maxi ki.
Then the number of ones in one row can be
)
v
ki
bits are required to encode the distribution
of ones in one row. Additionally, log2(b + 1)
bits are needed to encode b which is only nec-
essary once for the matrix. Let us consider
the adjacency matrix given in Fig. 2 of the
SN shown in Fig. 1 with 10 rows and columns
where each row contains at most four ones.
To encode the row c4, containing two ones, re-
</bodyText>
<footnote confidence="0.9535556">
5The commas are only included for better readabil-
ity and are actually not encoded.
6The approach of Cook and Holder is a bit inex-
act here. To be precise, the number of bits needed to
encode v and b would have to be known a priori.
</footnote>
<equation confidence="0.824325">
(encoded using log2(b + 1) bits. log2
41
� 10 �
quires log2(4) + log2 =7.49 bits which
2
</equation>
<bodyText confidence="0.999896">
is smaller than 10 bits which were necessary
for the naive approach. The total length rbits
of the encoding is given by:
</bodyText>
<equation confidence="0.9996785">
rbits = log2(b + 1) +
~ (1)
� log2 ki
v
]
=(v + 1)log2(b + 1)+
~ v�
log2 ki
</equation>
<bodyText confidence="0.983046954545455">
Finally, the arcs need to be encoded. Let
e(i, j) be the number of arcs between vertex
i and j in the graph and m := maxi,je(i, j).
log2(m) bits are required to encode the num-
ber of arcs between both vertices and log2(le)
bits are needed for the arc label (out of a set
of le elements). Then the entire number of
bits is given by (e is the number of arcs in the
graph):
(2)
where K is the number of ones in the adja-
cency matrix.
The total description length of the graph is
then given by: vbits + rbits + ebits.
Now let us investigate how the description
length of the compressed graph is determined.
In the original algorithm the substructure is
replaced in the graph by a single vertex. The
description length of the graph compressed by
the substructure is then given by the descrip-
tion length of the substructure added by the
description length of the modified graph.
</bodyText>
<table confidence="0.985046454545454">
c1 c2 c3 c4 c5 c6 c7 c8 c9 c10
c1 0 0 0 0 0 0 0 0 0 0
c2 0 0 0 0 0 0 0 0 0 0
c3 0 0 0 0 0 0 0 0 0 0
c4 1 1 0 0 0 0 0 0 0 0
c5 0 0 x 0 0 0 0 0 0 0
c6 0 0 0 0 0 0 x 0 0 0
c7 0 0 0 0 0 0 0 0 0 0
c8 0 0 0 0 x x 0 0 x x
c9 0 0 0 0 0 0 0 0 0 0
c10 0 0 0 0 0 0 0 0 0 0
</table>
<figureCaption confidence="0.807889">
Figure 3: Adjacency matrix of the compressed
SN. Vertices whose connections can be com-
pletely inferred from the pattern are removed.
</figureCaption>
<bodyText confidence="0.928292">
In our method there are two major differ-
ences from the graph learning approach of
Cook and Holder.
</bodyText>
<listItem confidence="0.961208">
• Not a single graph is compressed but a
set of graphs.
• For the approach of Cook and Holder, it
is unknown which vertex of the substruc-
ture a graph node is actually connected
with. Thus, the description is not com-
plete and the original graph could not be
reconstructed using the substructure and
the compressed graph. To make the de-
scription complete we specify the bind-
ings of the substructure vertices to the
graph vertices.
</listItem>
<bodyText confidence="0.999961875">
The generalization of the Cook and Holder-
algorithm to a set of graphs is quite straight
forward. The total description length of a set
of compressed graphs is given by the descrip-
tion length of the substructure (here pattern)
added to the sum of the description lengths of
each SN compressed by this pattern.
Additional bits are needed to encode the
vertex bindings (assuming the pattern premise
is contained in the SN). First the number
of bindings bin ([1, vp], vp: number of non-
lexicalized vertices appearing in a pattern) has
to be specified which requires log2(vp) bits.
The number of bits needed to encode a single
binding is given by log2(vp) + log2(v) (vertex
indices: [0, vp−1] to [0, v−1]). Thus, the total
</bodyText>
<equation confidence="0.993728058823529">
ebits =log2(m) + Xv Xv [A[i, j]log2(m)+
i=1 j=1
e(i,j) log2(le)]
=log2(m) + e log2(le)+
A[i,j] log2(m)
=e(log2(le)) + (K + 1)log2(m)
Xv
i=1
Xv
j=1
Xv [log2(b + 1)+
i=1
Xv
i=1
42
(3)
log2(vp)
</equation>
<bodyText confidence="0.999275474576271">
Note that not all bindings need to be en-
coded. The number of required binding en-
codings can be determined as follows. First
all bindings for all non-lexicalized pattern ver-
tices are determined. Then all cells from the
adjacency matrix of the SN which contain a
one and are also contained in the adjacency
matrix of the pattern, if this binding is ap-
plied to the non-lexicalized pattern vertices,
are set to zero. Vertices which contain only ze-
ros in the adjacency matrix on both columns
and rows are removed from the adjacency ma-
trix/graph. The arcs from and to this ver-
tex can be completely inferred by the pattern
which means that all vertices this vertex is
connected with are also contained in the pat-
tern. Since SNs differing only by the IDs of
their non-lexicalized vertices are considered
identical, no binding has to be specified for
such a vertex. Additionally, the modified ad-
jacency matrix is the result of the compres-
sion by the pattern, i.e., vbits, rbits, and ebits
are determined from the modified adjacency
matrix/graph if the pattern was successfully
matched to the SN.
Let us consider our example pattern D4
(Table 1). The following bindings are deter-
mined: a1: c3 (a1); a: c8; c: c6; b: c5; a2: c7
(a2)
The bindings for a1 and a2 need not to be
remembered since all hyponym vertices are re-
named to a1 and the hypernym vertices to
a2 in order to learn generic patterns for arbi-
trary hypernyms/hyponyms. The cells of the
adjacency matrix which are associated to the
arcs: SCAR(c8, c5), SuB(c5, a1), OBJ(c8, c6),
SuBS(c8, c9), TEMP(c8, c10) are set to zero
(marked by a cross in Fig. 3) since these arcs
are also represented in the pattern using the
bindings stated above. The rows and columns
of c3, c5, c7, and c9 of the modified graph
adjacency matrix only contain zeros. Thus,
these rows can be removed from the adja-
cency matrix and the associated concepts can
be eliminated from the vertex set of the SN.
The findings of the optimal patterns is done
compositionally employing a beam search ap-
proach. First this approach starts with pat-
terns containing only a single arc. These
patterns are then extended by adding one
arc after another preferring patterns lead-
ing to small description lengths of the com-
pressed SNs. Note that only pattern premises
are allowed which are fully connected, e.g.,
SuB(a, c) n SuB(e, f) is no acceptable premise.
Two lists are used during the search,
local besti for guiding the search process and
global best for storing the best global results
found so far:
</bodyText>
<listItem confidence="0.98251125">
• local besti: The k best patterns of
length i
• global best: The k best patterns of any
length
</listItem>
<bodyText confidence="0.99178372">
The list local besti is determined by extend-
ing all elements from local besti−1 by one
arc and only keeping the k arcs leading to
the smallest description length. The list
global best is updated after each change of
the list local besti. This process is iterated
as long as the total description length can be
further reduced, i.e., DL(local besti+1[0]) &lt;
DL(local besti[0]), where DL : Pattern → R
denotes the description length of a pattern and
[0] accesses the first element of a list.
The list global best contains as the result of
this approach the k patterns with the smallest
overall compressed description length7. Note
however that it is often not recommended
to use all elements of global best since this
list contains oftentimes patterns where the
premise part is a subgraph (can be inferred
by) another premise pattern part contained in
this list and their combination would actually
not reduce the description length. Thus, in
addition to the original approach of Cook and
Holder, a dependency resolution is done.
The following iterative approach is pro-
posed to cancel out such dependent patterns:
</bodyText>
<listItem confidence="0.6757725">
1. Start with the first entry of the global list:
depend best := {global best[0]}
</listItem>
<footnote confidence="0.8442115">
7compressed description length: short for descrip-
tion length of the SNs compressed by the pattern
</footnote>
<figure confidence="0.2307275">
number of required bits is given by
binbits =bin(log2(vp) + log2(v))+
</figure>
<page confidence="0.938515">
43
</page>
<figure confidence="0.208384">
ID Definition Matching Expression
</figure>
<equation confidence="0.994750434782609">
SUB0(a1, a2) ←
SUB(g, a2) ∧ ATTCH(g, f)∧
D1 SUBR(e, sub.0) ∧ TEMP(e, present.0)∧
ARG2(e, f) ∧ ARG1(e, d)∧
SUB(d, a1)
SUB0(a1, a2) ←
SUB(f, a2) ∧ EQU(g, f)∧
D2 SUBR(e, equ.0) ∧ TEMP(e, present.0)∧
ARG2(e, f) ∧ ARG1(e, d)∧
SUB(d, a1)
SUB0(a1, a2) ←
PRED(g, a2) ∧ ATTCH(g, f)∧
D3 SUBR(e, pred.0) ∧ ARG2(e, f)∧
TEMP(e, present.0) ∧ ARG1(e, d)∧
PRED(d, a1)
SUB0(a1, a2) ←
SUB(f, a2) ∧ SUBS(e, denote.1.1 )∧
TEMP(e, present.0) ∧ OBJ(e, f)∧
SCAR(e, d) ∧ SUB(d, a1)
SUB0(a1, a2) ←
D5 PROP(f, other.1.1) ∧ PRED(f, a2)∧
foll&apos;ffMS(d, f) ∧ PRED(d, a1)
D4
</equation>
<bodyText confidence="0.665618083333333">
An applehypo is a type
of fruithyper.
Psycho-linguisticshypo is a sciencehyper
of the human ability to speak.
Hepialidaehypo are a kind of insectshyper.
literal translation from: Die
Wurxelbohrer sind eine Familie
der Schmetterlinge.
A skyscraperhypo
denotes a very tall buildinghyper.
duckshypo and other
animalshyper
</bodyText>
<table confidence="0.792766285714286">
SUB0(a1, a2) ←
Ds the instrumenthyper cellohypo
SUB(d, a2) ∧ SUB(d, a1)
D7 SUB0(a1, a2) ← SUB(f, a2)∧ The Morton numberhypo is a
TEMP(e, present.0) ∧ SUBR(e, sub.0)∧ dimensionless indicatorhyper.
SUB(d, a1) ∧ ARG2(e, f)∧
ARG1(e, d)
</table>
<tableCaption confidence="0.988517">
Table 1: A selection of automatically learned patterns.
</tableCaption>
<listItem confidence="0.996136583333333">
2. Set index:=1
3. Calculate the combined (compressed)
description length of depend best and
{global best[index]}
4. If the combined description length
is reduced add global best[index] to
depend best, otherwise leave depend best
unchanged
5. If counter ≥ length(global best) then re-
turn depend best
6. index := index + 1
7. Go back to step 3
</listItem>
<sectionHeader confidence="0.709793" genericHeader="method">
6 System Architecture
</sectionHeader>
<bodyText confidence="0.9968475">
In this section, we give an overview over our
hypernymy extraction system. The following
procedure is employed to identify hypernymy
relations in Wikipedia (see Fig. 4):
</bodyText>
<listItem confidence="0.5583938">
1. At first, all sentences of Wikipedia are
analyzed by the deep analyzer WOCADI
(Hartrumpf, 2002). As a result of the
parsing process, a token list, a syntactic
dependency tree, and an SN is created.
</listItem>
<figureCaption confidence="0.992569">
Figure 4: Activity diagram of the hypernym
extraction process.
</figureCaption>
<figure confidence="0.997758875">
HaGenLex
Text
WOCADI
Analysis
Deep Pattern
Application
Deep patterns
Shallow patterns
Shallow Pattern
Application
Tokens SN
Validation
(Filter)
Validation
(Score)
KB
</figure>
<page confidence="0.993612">
44
</page>
<listItem confidence="0.964112470588235">
2. Shallow patterns based on regular expres-
sions are applied to the token lists, and
deep patterns (learned and hand-crafted)
are applied to the SNs to generate pro-
posals for hypernymy relations.
3. A validation tool using ontological sorts
and semantic features checks whether the
proposals are technically admissible at all
to reduce the amount of data to be stored
in the knowledge base KB.
4. If the validation is successful, the hyper-
nymy hypothesis is integrated into KB.
Steps 2–4 are repeated until all sentences
are processed.
5. Each hypernymy hypothesis in KB is as-
signed a confidence score estimating its
reliability.
</listItem>
<sectionHeader confidence="0.742463" genericHeader="method">
7 Validation Features
</sectionHeader>
<bodyText confidence="0.999863546875">
The knowledge acquisition carried out is fol-
lowed by a two-step validation. In the first
step, we check the ontological sorts and se-
mantic features of relational arguments for
subsumption. For instance, a discrete con-
cept (ontological sort: d) denoting a human
being (semantic feature: human +) can only
be hypernym of an other object, if this object
is both discrete and a human being as well.
Only relational candidates for which semantic
features and ontological sorts can be shown
to be compatible are stored in the knowledge
base.
In a second step, each relational candidate
in the knowledge base is assigned a quality
score. This is done by means of a support
vector machine (SVM) on several features.
The SVM determines the classification (hy-
pernymy or non-hypernymy) and a probabil-
ity value for each hypernymy hypothesis. If
the classification is ’hypernymy’, the score is
defined by this probability value, otherwise as
one minus this value.
Correctness Rate: The feature Correctness
Rate takes into account that the assumed hy-
pernym alone is already a strong indication
for the correctness or incorrectness of the in-
vestigated relation. The same holds for the
assumed hyponym as well. For instance, re-
lation hypotheses with hypernym liquid and
town are usually correct. However, this is
not the case for abstract concepts. Moreover,
movie names are often extracted incompletely
since they can consist of several tokens. Thus,
this indicator determines how often a concept
pair is classified correctly if a certain concept
shows up in the first (hyponym) or second (hy-
pernym) position.
Frequency: The feature frequency regards
the quotient of the occurrences of the hy-
ponym in other extracted relations in hy-
ponym position and the hypernym in hyper-
nym position.
This feature is based on two assumption.
First, we assume that general terms normally
occur more frequently in large text corpora
than very specific ones (Joho and Sanderson,
2007). Second, we assume that usually a hy-
pernym has more hyponyms than vice-versa.
Context: Generally, the hyponym can ap-
pear in the same textual context as its hyper-
nym. The textual context can be described as
a set of other concepts (or words for shallow
approaches) which occur in the neighborhood
of the investigated hyponym/hypernym can-
didate pair investigated on a large text cor-
pus. Instead of the textual context we re-
gard the semantic context. More specifically,
the distributions of all concepts are regarded
which are connected with the assumed hyper-
nym/hyponym concept by the MultiNet-PROP
(property) relation. The formula to estimate
the similarity was basically taken from (Cimi-
ano and others, 2005).
</bodyText>
<table confidence="0.999621714285714">
ID Precision First Sent. # Matches
D1 0.275 0.323 5 484
D2 0.183 0.230 35 497
D3 0.514 0.780 937
D4 0.536 0.706 1581
D5 0.592 - 3 461
D6 0.171 0.167 37 655
</table>
<tableCaption confidence="0.987319">
Table 2: Precision of hypernymy hypotheses
extracted by patterns without usage of the val-
idation component (D7 not yet evaluated).
</tableCaption>
<bodyText confidence="0.897522">
See (vor der Br¨uck, 2010) for a more de-
</bodyText>
<page confidence="0.997237">
45
</page>
<table confidence="0.99917">
Score &gt;0.95 &gt;0.90 &gt;0.85 &gt;0.80 &gt;0.75 &gt;0.70 &gt;0.65 &gt;0.60 &gt;0.55
Precision 1.0000 0.8723 0.8649 0.8248 0.8203 0.7049 0.6781 0.5741 0.5703
</table>
<tableCaption confidence="0.999881">
Table 3: Precision of the extracted hypernymy relations for different confidence score intervals.
</tableCaption>
<bodyText confidence="0.641687">
tailed description of the validation features.
</bodyText>
<sectionHeader confidence="0.991671" genericHeader="evaluation">
8 Evaluation
</sectionHeader>
<bodyText confidence="0.999927469879519">
We applied the pattern learning process on
a collection of 600 SN, derived by WOCADI
from Wikipedia, which contain hyponymically
related concepts. Table 1 contains some of the
extracted patterns including a typical expres-
sion to which this pattern could be matched.
The predicate follf(a, b) used in this table
specifies that argument a precedes argument
b in the argument list of function f. Patterns
D1-D4 and D7 contain concept definitions
where the defined concept is, in many cases,
the hyponym of the defining concept. In pat-
tern D1 and D7 the defining concept is directly
identified by the parser as hypernym of the de-
fined concept (sUBR(e, sub.0)). In pattern D2
the defining concept is recognized as equiva-
lent to the defined concept (sUBR(e, equ.0)).
However, in most of the cases the defining
concept consists of a meaning molecule, i.e.,
a complex concept where some inner concept
is modified by an additional expression (often
a property or an additional subclause). If this
expression is dropped which is done by the
pattern D2 the remaining concept becomes a
hypernym of the defined concept. Pattern D5
is a well-known Hearst pattern. Pattern D6
is used to match to appositions. However, for
that the representation of appositions in the
SN, as provided by the parser, could be im-
proved since the order of the two concepts in
a sentence is not clear by regarding only the
SN, i.e., from the expression the instrument
cello both sUB0(instrument.1.1, cello.1.1)
and sUB0(cello.1.1, instrument.1.1) could be
extracted. The incorrect relation hypoth-
esis has to be filtered out (hopefully)
by the validation component. A bet-
ter representation would be by employ-
ing the TUPL*(c1, ... , cn) predicate which
combines several concepts with regard to
their order. So the example expression
should better be represented by sUB(d, e) n
TUPL*(e, instrument.1.1, cello.1.1).
Precision values for the hyponymy relation
hypotheses extracted by the learned patterns,
which are applied on a subset of the German
Wikipedia, are given in Table 2. The first
precision value specifies the overall precision,
the second the precision if only hypernymy hy-
potheses are considered which were extracted
from first sentences of Wikipedia articles. The
precision is usually increased considerably if
only such sentences are regarded. Note that
this precision value was not given for pattern
D5 which usually cannot be matched to such
sentences. The last number specifies the to-
tal amount of sentences a pattern could be
matched to.
Furthermore, besides the pattern extraction
process, the entire hypernymy acquisition sys-
tem was validated, too. In total 391153 dif-
ferent hypernymy hypotheses were extracted
employing 22 deep and 19 shallow patterns.
149 900 of the relations were only determined
by the deep but not by the shallow patterns
which shows that the recall can be consider-
ably increased by using deep patterns in addi-
tion. But also precision profits from the usage
of deep patterns. The average precision of all
relations extracted by both shallow and deep
patterns is 0.514 that is considerably higher
than the average precision for the relations
only extracted by shallow patterns (0.243).
The correctness of an extracted relation hy-
pothesis is given for several confidence score
intervals in Table 3. There are 89 944 con-
cept pairs with a score above 0.7, 3 558 of
them were annotated with the information
of whether the hypernymy relation actually
holds.
Note that recall is very difficult to specify
since for doing this the number of hypernymy
relations which are theoretically extractable
</bodyText>
<page confidence="0.997192">
46
</page>
<bodyText confidence="0.9999018">
from a text corpus has to be known where
different annotators can have very dissenting
opinions about this number. Thus, we just
gave the number of relation hypotheses ex-
ceeding a certain score. However the precision
obtained by our system is quite competitive
to other approaches for hypernymy extrac-
tion like the one of Erik Tjong and Kim Sang
which extracts hypernyms in Dutch (Tjong
and Sang, 2007) (Precision: 0.48).
</bodyText>
<sectionHeader confidence="0.906969" genericHeader="conclusions">
9 Conclusion and Outlook
</sectionHeader>
<bodyText confidence="0.999991611111111">
We showed a method to automatically derive
patterns for hypernymy extraction in form of
SNs by following the MDL principle. A list
of such patterns together with precision and
number of matches were given to show the
usefulness of the applied approach. The pat-
terns were applied on the Wikipedia corpus
to extract hypernymy hypotheses. These hy-
potheses were validated using several features.
Depending on the score, an arbitrary high pre-
cision can be reached. Currently, we deter-
mine confidence values for the precision values
of the pattern example. Further future work
includes the application of our learning algo-
rithm to larger text corpora in order to find
additional patterns. Also an investigation of
how this method can be used for other types
of semantic relations is of interest.
</bodyText>
<sectionHeader confidence="0.99179" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998857142857143">
We want to thank all of our department
which contributed to this work, especially
Sven Hartrumpf and Alexander Pilz-Lansley
for proofreading this paper. This work was
in part funded by the DFG project Semantis-
che Duplikatserkennung mithilfe von Textual
Entailment (HE 2847/11-1).
</bodyText>
<sectionHeader confidence="0.998438" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999575109090909">
Cimiano, P. et al. 2005. Learning taxonomic re-
lations from heterogeneous sources of evidence.
In Buitelaar, P. et al., editors, Ontology Learn-
ing from Text: Methods, evaluation and applica-
tions, pages 59–73. IOS Press, Amsterdam, The
Netherlands.
Cook, D. and L. Holder. 1994. Substructure dis-
covery using minimum description length and
background knowledge. Journal of Artificial In-
telligence Research, 1:231–255.
Fellbaum, C., editor. 1998. WordNet An Elec-
tronic Lexical Database. MIT Press, Cam-
bridge, Massachusetts.
Hamp, B. and H. Feldweg. 1997. Germanet - a
lexical-semantic net for german. In Proc. of the
ACL workshop of Automatic Information Ex-
traction and Building of Lexical Semantic Re-
sources for NLP Applications, Madrid, Spain.
Hartrumpf, S. et al. 2003. The semantically based
computer lexicon HaGenLex – Structure and
technological environment. Traitement automa-
tique des langues, 44(2):81–105.
Hartrumpf, S. 2002. Hybrid Disambiguation in
Natural Language Analysis. Ph.D. thesis, Fern-
Universit¨at in Hagen, Fachbereich Informatik,
Hagen, Germany.
Hearst, M. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. of
COLING, Nantes, France.
Helbig, H. 2006. Knowledge Representation and
the Semantics of Natural Language. Springer,
Berlin, Germany.
Joho, H. and M. Sanderson. 2007. Document fre-
quency and term specificity. In Proc. of RIAO,
Pittsburgh, Pennsylvania.
Morin, E. and C. Jaquemin. 2004. Automatic
acquisition and expansion of hypernym links.
Computers and the Humanities, 38(4):363–396.
Rissanen, J. 1989. Stochastic Complexity in
Statistical Inquiry. World Scientific Publishing
Company, Hackensack, New Jersey.
Snow, R. et al. 2005. Learning syntactic patterns
for automatic hypernym discovery. In Advances
in Neural Information Processing Systems 17,
pages 1297–1304. MIT Press, Cambridge, Mas-
sachusetts.
Tjong, E. and K. Sang. 2007. Extracting hy-
pernym pairs from the web. In Proceedings
of the 45 Annual Meeting of the ACL on In-
teractive Poster and Demonstration Sessions,
Prague, Czech Republic.
vor der Br¨uck, T. 2010. Hypernymy extraction
using a semantic network representation. Inter-
national Journal of Computational Linguistics
and Applications (IJCLA), 1(1).
</reference>
<page confidence="0.999487">
47
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.452418">
<title confidence="0.830898">Learning Semantic Network Patterns for Hypernymy Extraction Tim vor der Intelligent Information and Communication Systems</title>
<author confidence="0.89642">FernUniversit¨at in</author>
<email confidence="0.966899">tim.vorderbrueck@fernuni-hagen.de</email>
<abstract confidence="0.999221333333333">Current approaches of hypernymy acquisition are mostly based on syntactic or surface representations and extract hypernymy relations between surface word forms and not word readings. In this paper we present a purely semantic approach for hypernymy extraction based on semantic networks (SNs). This approach employs a set of patterns the premise part of a pattern is given by a SN. Furthermore this paper describes how the patterns can be derived by relational statistical learning following the Minimum Description Length principle (MDL). The evaluation demonstrates the usefulness of the learned patterns and also of the entire hypernymy extraction system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Cimiano</author>
</authors>
<title>Learning taxonomic relations from heterogeneous sources of evidence.</title>
<date>2005</date>
<booktitle>Ontology Learning from Text: Methods, evaluation and applications,</booktitle>
<pages>59--73</pages>
<editor>In Buitelaar, P. et al., editors,</editor>
<publisher>IOS Press,</publisher>
<location>Amsterdam, The Netherlands.</location>
<marker>Cimiano, 2005</marker>
<rawString>Cimiano, P. et al. 2005. Learning taxonomic relations from heterogeneous sources of evidence. In Buitelaar, P. et al., editors, Ontology Learning from Text: Methods, evaluation and applications, pages 59–73. IOS Press, Amsterdam, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cook</author>
<author>L Holder</author>
</authors>
<title>Substructure discovery using minimum description length and background knowledge.</title>
<date>1994</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>1--231</pages>
<contexts>
<context position="9971" citStr="Cook and Holder, 1994" startWordPosition="1606" endWordPosition="1609">t only on a semantic level4. 4Note that some dependency parsers normalize some syntactic variations too. 40 5 Graph Substructure Learning By Following the Minimum Description Length Principle In this section, we describe how the patterns can be learned by a supervised machine learning approach following the Minimum Description Length principle. This principle states that the best hypothesis for a given data set is that one which minimizes the description of the data (Rissanen, 1989), i.e., compresses the data the most. Basically we follow the substructure learning approach of Cook and Holder (Cook and Holder, 1994). According to this approach, the description length to minimize is the number of bits required to encode a certain graph which is compressed by means of a substructure. If a lot of graph vertices can be matched with the substructure vertices, this description length will be quite small. For our learning scenario we investigate collection of SNs containing a known hypernymy relationship. A pattern (given by a substructure in the premise) which compresses this set quite well is expected to be useful for extracting hypernyms. Let us first determine the number of bits to encode the entire graph o</context>
</contexts>
<marker>Cook, Holder, 1994</marker>
<rawString>Cook, D. and L. Holder. 1994. Substructure discovery using minimum description length and background knowledge. Journal of Artificial Intelligence Research, 1:231–255.</rawString>
</citation>
<citation valid="true">
<title>WordNet An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Fellbaum, C., editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>1998</marker>
<rawString>Fellbaum, C., editor. 1998. WordNet An Electronic Lexical Database. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Hamp</author>
<author>H Feldweg</author>
</authors>
<title>Germanet - a lexical-semantic net for german.</title>
<date>1997</date>
<booktitle>In Proc. of the ACL workshop of Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="4564" citStr="Hamp and Feldweg, 1997" startWordPosition="725" endWordPosition="728">length of common substrings for the maximum text windows before, between and after the hyponym/hypernym pair. All sentences are then clustered according to this similarity measure. The representative pattern (called candidate pattern) of each cluster is defined to be the expression with the lowest mean square error (deviation) to all other expressions in the same similarity cluster. The patterns to be used for hyponymy detection are the candidate patterns of all clusters found. 3 MultiNet MultiNet is an SN formalism (Helbig, 2006). In contrast to SNs like WordNet (Fellbaum, 1998) or GermaNet (Hamp and Feldweg, 1997), which contain lexical relations between synsets, MultiNet is designed to comprehensively represent the semantics of natural language expressions. An SN in the MultiNet formalism is given as a set of vertices and arcs where the vertices represent the concepts (word readings) and the arcs the relations (or functions) between the concepts. A vertex can be lexicalized if it is directly associated to a lexical entry or non-lexicalized. An example SN is shown in Fig. 1. Note that each vertex of the SN is assigned both a unique ID (e.g., c2) and a label which is the associated lexical entry for lex</context>
</contexts>
<marker>Hamp, Feldweg, 1997</marker>
<rawString>Hamp, B. and H. Feldweg. 1997. Germanet - a lexical-semantic net for german. In Proc. of the ACL workshop of Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hartrumpf</author>
</authors>
<title>The semantically based computer lexicon</title>
<date>2003</date>
<booktitle>HaGenLex – Structure and technological environment. Traitement automatique des langues,</booktitle>
<pages>44--2</pages>
<marker>Hartrumpf, 2003</marker>
<rawString>Hartrumpf, S. et al. 2003. The semantically based computer lexicon HaGenLex – Structure and technological environment. Traitement automatique des langues, 44(2):81–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hartrumpf</author>
</authors>
<title>Hybrid Disambiguation in Natural Language Analysis.</title>
<date>2002</date>
<booktitle>Ph.D. thesis, FernUniversit¨at in Hagen, Fachbereich Informatik,</booktitle>
<location>Hagen, Germany.</location>
<contexts>
<context position="7463" citStr="Hartrumpf, 2002" startWordPosition="1186" endWordPosition="1187">IMAL, ANIMATE, ARTIF (artificial), HUMAN, SPATIAL, THCONC (theoretical concept) Example for the concept bottle.1.12: discrete object; ANIMAL -, ANIMATE -, ARTIF +, HUMAN -, SPATIAL +, THCONC -, .. . 2the suffix .1.1 denotes the reading numbered .1.1 of the word bottle. 39 c9 denote.1.1 Figure 1: Matching a pattern to an SN. Bold lines indicate matched arcs, the dashed line the inferred arc. c7 a1=skyscraper.1.1 c3 SUBS c5 c8 very.1.1 c1 c6 c4 present.0 c10 a2=house.1.1 tall.1.1 c2 The SNs as described here are automatically constructed from (German) texts by the deep linguistic parser WOCADI3(Hartrumpf, 2002) whose parsing process is based on a word class functional analysis. 4 Application of Deep Patterns The extraction of hyponyms as described here is based on a set of patterns. Each pattern consists of a conclusion part suB0(a1 , a2) and a premise part in form of an SN where both a1 and a2 have to show up. The patterns are applied by a pattern matcher (or automated theorem prover if axioms are used) which matches the premise with an SN. The variable bindings for a1 and a2 are given by the matched concepts of the SN. An example pattern which matches to the sentence: A skyscraper denotes a very t</context>
<context position="22063" citStr="Hartrumpf, 2002" startWordPosition="3896" endWordPosition="3897">. Calculate the combined (compressed) description length of depend best and {global best[index]} 4. If the combined description length is reduced add global best[index] to depend best, otherwise leave depend best unchanged 5. If counter ≥ length(global best) then return depend best 6. index := index + 1 7. Go back to step 3 6 System Architecture In this section, we give an overview over our hypernymy extraction system. The following procedure is employed to identify hypernymy relations in Wikipedia (see Fig. 4): 1. At first, all sentences of Wikipedia are analyzed by the deep analyzer WOCADI (Hartrumpf, 2002). As a result of the parsing process, a token list, a syntactic dependency tree, and an SN is created. Figure 4: Activity diagram of the hypernym extraction process. HaGenLex Text WOCADI Analysis Deep Pattern Application Deep patterns Shallow patterns Shallow Pattern Application Tokens SN Validation (Filter) Validation (Score) KB 44 2. Shallow patterns based on regular expressions are applied to the token lists, and deep patterns (learned and hand-crafted) are applied to the SNs to generate proposals for hypernymy relations. 3. A validation tool using ontological sorts and semantic features ch</context>
</contexts>
<marker>Hartrumpf, 2002</marker>
<rawString>Hartrumpf, S. 2002. Hybrid Disambiguation in Natural Language Analysis. Ph.D. thesis, FernUniversit¨at in Hagen, Fachbereich Informatik, Hagen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proc. of COLING,</booktitle>
<location>Nantes, France.</location>
<contexts>
<context position="2238" citStr="Hearst, 1992" startWordPosition="353" endWordPosition="354">for their construction are preferable. In this work we introduce a semantically oriented approach where the hypernyms are extracted using a set of patterns which are neither syntactic nor surface-oriented but instead purely semantic and are based on a SN formalism. The patterns are applied on a set of SNs which are automatically derived from the German Wikipediai by a deep syntacticosemantic analysis. Furthermore, these patterns are automatically created by a machine learning approach based on the MDL principle. 2 Related Work Patterns for hypernymy extraction were first introduced by Hearst (Hearst, 1992), the socalled Hearst patterns. An example of such a pattern is: NPhypo {,NPhypo}*{,} and other NPhyper. These patterns are applied on arbitrary texts and the instantiated variables NPhypo and NPhyper are then extracted as a concrete hypernymy relation. Apart from the handcrafted patterns there was also some work to determine patterns automatically from texts (Snow and others, 2005). For that, Snow et al. collected sentences in a given text corpus with known hypernym noun pairs. These sentences are then parsed by a dependency parser. Afterwards, the path in the dependency tree is extracted whi</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Hearst, M. 1992. Automatic acquisition of hyponyms from large text corpora. In Proc. of COLING, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Helbig</author>
</authors>
<title>Knowledge Representation and the Semantics of Natural Language.</title>
<date>2006</date>
<publisher>Springer,</publisher>
<location>Berlin, Germany.</location>
<contexts>
<context position="4477" citStr="Helbig, 2006" startWordPosition="713" endWordPosition="714">ty measure between two such expressions is defined as the sum of the maximal length of common substrings for the maximum text windows before, between and after the hyponym/hypernym pair. All sentences are then clustered according to this similarity measure. The representative pattern (called candidate pattern) of each cluster is defined to be the expression with the lowest mean square error (deviation) to all other expressions in the same similarity cluster. The patterns to be used for hyponymy detection are the candidate patterns of all clusters found. 3 MultiNet MultiNet is an SN formalism (Helbig, 2006). In contrast to SNs like WordNet (Fellbaum, 1998) or GermaNet (Hamp and Feldweg, 1997), which contain lexical relations between synsets, MultiNet is designed to comprehensively represent the semantics of natural language expressions. An SN in the MultiNet formalism is given as a set of vertices and arcs where the vertices represent the concepts (word readings) and the arcs the relations (or functions) between the concepts. A vertex can be lexicalized if it is directly associated to a lexical entry or non-lexicalized. An example SN is shown in Fig. 1. Note that each vertex of the SN is assigne</context>
</contexts>
<marker>Helbig, 2006</marker>
<rawString>Helbig, H. 2006. Knowledge Representation and the Semantics of Natural Language. Springer, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Joho</author>
<author>M Sanderson</author>
</authors>
<title>Document frequency and term specificity.</title>
<date>2007</date>
<booktitle>In Proc. of RIAO,</booktitle>
<location>Pittsburgh, Pennsylvania.</location>
<contexts>
<context position="25001" citStr="Joho and Sanderson, 2007" startWordPosition="4366" endWordPosition="4369">cepts. Moreover, movie names are often extracted incompletely since they can consist of several tokens. Thus, this indicator determines how often a concept pair is classified correctly if a certain concept shows up in the first (hyponym) or second (hypernym) position. Frequency: The feature frequency regards the quotient of the occurrences of the hyponym in other extracted relations in hyponym position and the hypernym in hypernym position. This feature is based on two assumption. First, we assume that general terms normally occur more frequently in large text corpora than very specific ones (Joho and Sanderson, 2007). Second, we assume that usually a hypernym has more hyponyms than vice-versa. Context: Generally, the hyponym can appear in the same textual context as its hypernym. The textual context can be described as a set of other concepts (or words for shallow approaches) which occur in the neighborhood of the investigated hyponym/hypernym candidate pair investigated on a large text corpus. Instead of the textual context we regard the semantic context. More specifically, the distributions of all concepts are regarded which are connected with the assumed hypernym/hyponym concept by the MultiNet-PROP (p</context>
</contexts>
<marker>Joho, Sanderson, 2007</marker>
<rawString>Joho, H. and M. Sanderson. 2007. Document frequency and term specificity. In Proc. of RIAO, Pittsburgh, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Morin</author>
<author>C Jaquemin</author>
</authors>
<title>Automatic acquisition and expansion of hypernym links.</title>
<date>2004</date>
<journal>Computers and the Humanities,</journal>
<volume>38</volume>
<issue>4</issue>
<contexts>
<context position="3589" citStr="Morin and Jaquemin, 2004" startWordPosition="569" endWordPosition="572">h (see first Hearst pattern) they added the links to the word on either side of the two nouns (if not yet contained) to the path too. Frequently oc&apos;Note that for better readability the examples are translated from German into English throughout this paper. 38 Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 38–47, Beijing, August 2010 curring paths are then learned as patterns for indicating a hypernymy relation. An alternative approach for learning patterns which is based on a surface instead of a syntactic representation was proposed by Morin et al. (Morin and Jaquemin, 2004). They investigate sentences containing pairs of known hypernyms and hyponyms as well. All these sentences are converted into so-called “lexico-syntactic expressions” where all NPs and lists of NPs are replaced by special symbols, e.g.: NP find in NP such as LIST. A similarity measure between two such expressions is defined as the sum of the maximal length of common substrings for the maximum text windows before, between and after the hyponym/hypernym pair. All sentences are then clustered according to this similarity measure. The representative pattern (called candidate pattern) of each clust</context>
</contexts>
<marker>Morin, Jaquemin, 2004</marker>
<rawString>Morin, E. and C. Jaquemin. 2004. Automatic acquisition and expansion of hypernym links. Computers and the Humanities, 38(4):363–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rissanen</author>
</authors>
<title>Stochastic Complexity in Statistical Inquiry.</title>
<date>1989</date>
<publisher>World Scientific Publishing Company,</publisher>
<location>Hackensack, New Jersey.</location>
<contexts>
<context position="9836" citStr="Rissanen, 1989" startWordPosition="1587" endWordPosition="1588">used instead since the coordination of Bermuda Triangle and world areas is not represented in the syntactic constituency tree but only on a semantic level4. 4Note that some dependency parsers normalize some syntactic variations too. 40 5 Graph Substructure Learning By Following the Minimum Description Length Principle In this section, we describe how the patterns can be learned by a supervised machine learning approach following the Minimum Description Length principle. This principle states that the best hypothesis for a given data set is that one which minimizes the description of the data (Rissanen, 1989), i.e., compresses the data the most. Basically we follow the substructure learning approach of Cook and Holder (Cook and Holder, 1994). According to this approach, the description length to minimize is the number of bits required to encode a certain graph which is compressed by means of a substructure. If a lot of graph vertices can be matched with the substructure vertices, this description length will be quite small. For our learning scenario we investigate collection of SNs containing a known hypernymy relationship. A pattern (given by a substructure in the premise) which compresses this s</context>
</contexts>
<marker>Rissanen, 1989</marker>
<rawString>Rissanen, J. 1989. Stochastic Complexity in Statistical Inquiry. World Scientific Publishing Company, Hackensack, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery.</title>
<date>2005</date>
<booktitle>In Advances in Neural Information Processing Systems 17,</booktitle>
<pages>1297--1304</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>Snow, 2005</marker>
<rawString>Snow, R. et al. 2005. Learning syntactic patterns for automatic hypernym discovery. In Advances in Neural Information Processing Systems 17, pages 1297–1304. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Tjong</author>
<author>K Sang</author>
</authors>
<title>Extracting hypernym pairs from the web.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45 Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="30331" citStr="Tjong and Sang, 2007" startWordPosition="5235" endWordPosition="5238">notated with the information of whether the hypernymy relation actually holds. Note that recall is very difficult to specify since for doing this the number of hypernymy relations which are theoretically extractable 46 from a text corpus has to be known where different annotators can have very dissenting opinions about this number. Thus, we just gave the number of relation hypotheses exceeding a certain score. However the precision obtained by our system is quite competitive to other approaches for hypernymy extraction like the one of Erik Tjong and Kim Sang which extracts hypernyms in Dutch (Tjong and Sang, 2007) (Precision: 0.48). 9 Conclusion and Outlook We showed a method to automatically derive patterns for hypernymy extraction in form of SNs by following the MDL principle. A list of such patterns together with precision and number of matches were given to show the usefulness of the applied approach. The patterns were applied on the Wikipedia corpus to extract hypernymy hypotheses. These hypotheses were validated using several features. Depending on the score, an arbitrary high precision can be reached. Currently, we determine confidence values for the precision values of the pattern example. Furt</context>
</contexts>
<marker>Tjong, Sang, 2007</marker>
<rawString>Tjong, E. and K. Sang. 2007. Extracting hypernym pairs from the web. In Proceedings of the 45 Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>vor der Br¨uck</author>
<author>T</author>
</authors>
<title>Hypernymy extraction using a semantic network representation.</title>
<date>2010</date>
<journal>International Journal of Computational Linguistics and Applications (IJCLA),</journal>
<volume>1</volume>
<issue>1</issue>
<marker>der Br¨uck, T, 2010</marker>
<rawString>vor der Br¨uck, T. 2010. Hypernymy extraction using a semantic network representation. International Journal of Computational Linguistics and Applications (IJCLA), 1(1).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>