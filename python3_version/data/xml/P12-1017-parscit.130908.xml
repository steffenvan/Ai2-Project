<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.9969875">
Deciphering Foreign Language by Combining Language Models and
Context Vectors
</title>
<author confidence="0.980558">
Malte Nuhn and Arne Mauser* and Hermann Ney
</author>
<affiliation confidence="0.9802965">
Human Language Technology and Pattern Recognition Group
RWTH Aachen University, Germany
</affiliation>
<email confidence="0.986545">
&lt;surname&gt;@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.993941" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.993751294117647">
In this paper we show how to train statis-
tical machine translation systems on real-
life tasks using only non-parallel monolingual
data from two languages. We present a mod-
ification of the method shown in (Ravi and
Knight, 2011) that is scalable to vocabulary
sizes of several thousand words. On the task
shown in (Ravi and Knight, 2011) we obtain
better results with only 5% of the computa-
tional effort when running our method with
an n-gram language model. The efficiency
improvement of our method allows us to run
experiments with vocabulary sizes of around
5,000 words, such as a non-parallel version of
the VERBMOBIL corpus. We also report re-
sults using data from the monolingual French
and English GIGAWORD corpora.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.983265055555556">
It has long been a vision of science fiction writers
and scientists to be able to universally communi-
cate in all languages. In these visions, even previ-
ously unknown languages can be learned automati-
cally from analyzing foreign language input.
In this work, we attempt to learn statistical trans-
lation models from only monolingual data in the
source and target language. The reasoning behind
this idea is that the elements of languages share sta-
tistical similarities that can be automatically identi-
fied and matched with other languages.
This work is a big step towards large-scale and
large-vocabulary unsupervised training of statistical
translation models. Previous approaches have faced
constraints in vocabulary or data size. We show how
*Author now at Google Inc., amauser@google.com.
to scale unsupervised training to real-life transla-
tion tasks and how large-scale experiments can be
done. Monolingual data is more readily available,
if not abundant compared to true parallel or even
just translated data. Learning from only monolin-
gual data in real-life translation tasks could improve
especially low resource language pairs where few or
no parallel texts are available.
In addition to that, this approach offers the op-
portunity to decipher new or unknown languages
and derive translations based solely on the available
monolingual data. While we do tackle the full unsu-
pervised learning task for MT, we make some very
basic assumptions about the languages we are deal-
ing with:
1. We have large amounts of data available in
source and target language. This is not a very
strong assumption as books and text on the in-
ternet are readily available for almost all lan-
guages.
</bodyText>
<listItem confidence="0.984103909090909">
2. We can divide the given text in tokens and
sentence-like units. This implies that we know
enough about the language to tokenize and
sentence-split a given text. Again, for the vast
majority of languages, this is not a strong re-
striction.
3. The writing system is one-dimensional left-to-
right. It has been shown (Lin and Knight, 2006)
that the writing direction can be determined
separately and therefore this assumption does
not pose a real restriction.
</listItem>
<bodyText confidence="0.973048">
Previous approaches to unsupervised training for
SMT prove feasible only for vocabulary sizes up to
around 500 words (Ravi and Knight, 2011) and data
</bodyText>
<page confidence="0.98019">
156
</page>
<note confidence="0.9857815">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156–164,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999870933333334">
sets of roughly 15,000 sentences containing only
about 4 tokens per sentence on average. Real data
as it occurs in texts such as web pages or news texts
does not meet any of these characteristics.
In this work, we will develop, describe, and
evaluate methods for large vocabulary unsupervised
learning of machine translation models suitable for
real-world tasks. The remainder of this paper is
structured as follows: In Section 2, we will review
the related work and describe how our approach ex-
tends existing work. Section 3 describes the model
and training criterion used in this work. The im-
plementation and the training of this model is then
described in Section 5 and experimentally evaluated
in Section 6.
</bodyText>
<sectionHeader confidence="0.999812" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999834545454546">
Unsupervised training of statistical translations sys-
tems without parallel data and related problems have
been addressed before. In this section, we will re-
view previous approaches and highlight similarities
and differences to our work. Several steps have been
made in this area, such as (Knight and Yamada,
1999), (Ravi and Knight, 2008), or (Snyder et al.,
2010), to name just a few. The main difference of
our work is, that it allows for much larger vocab-
ulary sizes and more data to be used than previous
work while at the same time not being dependent on
seed lexica and/or any other knowledge of the lan-
guages.
Close to the methods described in this work,
Ravi and Knight (2011) treat training and transla-
tion without parallel data as a deciphering prob-
lem. Their best performing approach uses an EM-
Algorithm to train a generative word based trans-
lation model. They perform experiments on a
Spanish/English task with vocabulary sizes of about
500 words and achieve a performance of around
20 BLEU compared to 70 BLEU obtained by a sys-
tem that was trained on parallel data. Our work uses
the same training criterion and is based on the same
generative story. However, we use a new training
procedure whose critical parts have constant time
and memory complexity with respect to the vocab-
ulary size so that our methods can scale to much
larger vocabulary sizes while also being faster.
In a different approach, Koehn and Knight (2002)
induce a bilingual lexicon from only non-parallel
data. To achieve this they use a seed lexicon which
they systematically extend by using orthographic as
well as distributional features such as context, and
frequency. They perform their experiments on non-
parallel German-English news texts, and test their
mappings against a bilingual lexicon. We use a
greedy method similar to (Koehn and Knight, 2002)
for extending a given lexicon, and we implicitly also
use the frequency as a feature. However, we perform
fully unsupervised training and do not start with a
seed lexicon or use linguistic features.
Similarly, Haghighi et al. (2008) induce a one-
to-one translation lexicon only from non-parallel
monolingual data. Also starting with a seed lexi-
con, they use a generative model based on canoni-
cal correlation analysis to systematically extend the
lexicon using context as well as spelling features.
They evaluate their method on a variety of tasks,
ranging from inherently parallel data (EUROPARL)
to unrelated corpora (100k sentences of the GIGA-
WORD corpus). They report F-measure scores of the
induced entries between 30 to 70. As mentioned
above, our work neither uses a seed lexicon nor or-
thographic features.
</bodyText>
<sectionHeader confidence="0.99882" genericHeader="method">
3 Translation Model
</sectionHeader>
<bodyText confidence="0.990753611111111">
In this section, we describe the statistical training
criterion and the translation model that is trained us-
ing monolingual data. In addition to the mathemat-
ical formulation of the model we describe approxi-
mations used.
Throughout this work, we denote the source lan-
guage words as f and target language words as e.
The source vocabulary is Vf and we write the size
of this vocabulary as |Vf|. The same notation holds
for the target vocabulary with Ve and |Ve|.
As training criterion for the translation model’s
parameters 0, Ravi and Knight (2011) suggest
arg max {n E P(e) · pe(f|e) } (1)
e f e
We would like to obtain 0 from Equation 1 using
the EM Algorithm (Dempster et al., 1977). This
becomes increasingly difficult with more complex
translation models. Therefore, we use a simplified
</bodyText>
<page confidence="0.995587">
157
</page>
<bodyText confidence="0.73987675">
translation model that still contains all basic phe-
nomena of a generic translation process. We formu-
late the translation process with the same generative
story presented in (Ravi and Knight, 2011):
</bodyText>
<listItem confidence="0.999127909090909">
1. Stochastically generate the target sentence ac-
cording to an n-gram language model.
2. Insert NULL tokens between any two adjacent
positions of the target string with uniform prob-
ability.
3. For each target token ei (including NULL)
choose a foreign translation fi (including
NULL) with probability Pθ(fi|ei).
4. Locally reorder any two adjacent foreign words
fi−1, fi with probability P(SWAP) = 0.1.
5. Remove the remaining NULL tokens.
</listItem>
<bodyText confidence="0.999937">
In practice, however, it is not feasible to deal with
the full parameter table Pθ(fi|ei) which models the
lexicon. Instead we only allow translation models
where for each source word f the number of words
e&apos; with P(f|e&apos;) =6 0 is below some fixed value. We
will refer to this value as the maximum number of
candidates of the translation model and denote it
with NC. Note that for a given e this does not nec-
essarily restrict the number of entries P(f&apos;|e) =6 0.
Also note that with a fixed value of NC, time and
memory complexity of the EM step is O(1) with re-
spect to |Ve |and |Vf|.
In the following we divide the problem of maxi-
mizing Equation 1 into two parts:
</bodyText>
<listItem confidence="0.874453">
1. Determining a set of active lexicon entries.
2. Choosing the translation probabilities for the
given set of active lexicon entries.
</listItem>
<bodyText confidence="0.999948333333333">
The second task can be achieved by running the
EM algorithm on the restricted translation model.
We deal with the first task in the following section.
</bodyText>
<sectionHeader confidence="0.997923" genericHeader="method">
4 Monolingual Context Similarity
</sectionHeader>
<bodyText confidence="0.995830222222222">
As described in Section 3 we need some mecha-
nism to iteratively choose an active set of translation
candidates. Based on the assumption that some of
the active candidates and their respective probabili-
ties are already correct, we induce new active candi-
dates. In the context of information retrieval, Salton
et al. (1975) introduce a document space where each
document identified by one or more index terms is
represented by a high dimensional vector of term
weights. Given two vectors v1 and v2 of two doc-
uments it is then possible to calculate a similarity
coefficient between those given documents (which
is usually denoted as s(v1, v2)). Similar to this we
represent source and target words in a high dimen-
sional vector space of target word weights which we
call context vectors and use a similarity coefficient
to find possible translation pairs. We first initialize
these context vectors using the following procedure:
</bodyText>
<listItem confidence="0.9960314">
1. Using only the monolingual data for the target
language, prepare the context vectors vei with
entries vei,ej:
(a) Initialize all vei,ej = 0
(b) For each target sentence E:
</listItem>
<bodyText confidence="0.6736625">
For each word ei in E:
For each word ej =6 ei in E:
</bodyText>
<equation confidence="0.88124275">
vei,ej = vei,ej + 1.
(c) Normalize each vector vei such that
E (vei,ej)2 ! 1 holds.
ej
</equation>
<bodyText confidence="0.7769405">
Using the notation ei = (ej : vei,ej,... ) these
vectors might for example look like
work = (early : 0.2, late : 0.1,... )
time = (early: 0.2, late : 0.2,... ).
2. Prepare context vectors vfi,ej for the source
language using only the monolingual data for
the source language and the translation model’s
current parameter estimate 0:
</bodyText>
<listItem confidence="0.9989674">
(a) Initialize all vfi,ej = 0
(b) Let Eθ(F) denote the most probable
translation of the foreign sentence F ob-
tained by using the current estimate 0.
(c) For each source sentence F:
</listItem>
<equation confidence="0.629493625">
For each word fi in F:
For each word ej =6 Eθ(fi)1 in
Eθ(F):
vfi,ej = vfi,ej + 1
(d) Normalize each vector vfi such that
E
ej (vfi,ej)2 ! 1 holds.
1denoting that ej is not the translation of fi in Eθ(F)
</equation>
<page confidence="0.982791">
158
</page>
<bodyText confidence="0.9877425">
Adapting the notation described above, these
vectors might for example look like
</bodyText>
<equation confidence="0.681661">
Arbeit = (early : 0.25, late : 0.05,... )
Zeit = (early : 0.15, late : 0.25,... )
</equation>
<bodyText confidence="0.999983">
Once we have set up the context vectors ve and
vf, we can retrieve translation candidates for some
source word f by finding those words e&apos; that maxi-
mize the similarity coefficient s(ve0, vf), as well as
candidates for a given target word e by finding those
words f&apos; that maximize s(ve, vf0). In our implemen-
tation we use the Euclidean distance
</bodyText>
<equation confidence="0.772388">
d(ve, vf) = ||ve − vf||2. (2)
</equation>
<bodyText confidence="0.999800230769231">
as distance measure.2 The normalization of context
vectors described above is motivated by the fact that
the context vectors should be invariant with respect
to the absolute number of occurrences of words.3
Instead of just finding the best candidates for a
given word, we are interested in an assignment that
involves all source and target words, minimizing the
sum of distances between the assigned words. In
case of a one-to-one mapping the problem of assign-
ing translation candidates such that the sum of dis-
tances is minimal can be solved optimally in poly-
nomial time using the hungarian algorithm (Kuhn,
1955). In our case we are dealing with a many-
to-many assignment that needs to satisfy the max-
imum number of candidates constraints. For this,
we solve the problem in a greedy fashion by simply
choosing the best pairs (e, f) first. As soon as a tar-
get word e or source word f has reached the limit
of maximum candidates, we skip all further candi-
dates for that word e (or f respectively). This step
involves calculating and sorting all |Ve ||Vf |dis-
tances which can be done in time O(V 2 log(V )),
with V = max(|Ve|, |Vf|). A simplified example of
this procedure is depicted in Figure 1. The example
already shows that the assignment obtained by this
algorithm is in general not optimal.
</bodyText>
<footnote confidence="0.9230135">
2We then obtain pairs (e, f) that minimize d.
3This gives the same similarity ordering as using un-
normalized vectors with the cosine similarity measure
ve·vf which can be interpreted as measuring the cosine
||ve||2·||vf ||2
of the angle between the vectors, see (Manning et al., 2008).
Still it is noteworthy that this procedure is not equivalent to the
tf-IDF context vectors described in (Salton et al., 1975).
</footnote>
<figureCaption confidence="0.997595666666667">
Figure 1: Hypothetical example for a greedy one-to-one
assignment of translation candidates. The optimal assign-
ment would contain (time,Zeit) and (work,Arbeit).
</figureCaption>
<sectionHeader confidence="0.885022" genericHeader="method">
5 Training Algorithm and Implementation
</sectionHeader>
<bodyText confidence="0.999795454545454">
Given the model presented in Section 3 and the
methods illustrated in Section 4, we now describe
how to train this model.
As described in Section 4, the overall procedure
is divided into two alternating steps: After initializa-
tion we first perform EM training of the translation
model for 20-30 iterations using a 2-gram or 3-gram
language model in the target language. With the ob-
tained best translations we induce new translation
candidates using context similarity. This procedure
is depicted in Figure 2.
</bodyText>
<subsectionHeader confidence="0.893632">
5.1 Initialization
</subsectionHeader>
<bodyText confidence="0.999957833333333">
Let NC be the maximum number of candidates per
source word we allow, Ve and Vf be the target/source
vocabulary and r(e) and r(f) the frequency rank of
a source/target word. Each word f E Vf with fre-
quency rank r(f) is assigned to all words e E Ve
with frequency rank
</bodyText>
<equation confidence="0.99287">
r(e) E [ start(f) , end(f) ] (3)
</equation>
<bodyText confidence="0.559221">
where
</bodyText>
<equation confidence="0.99761075">
start(f) = max(0 , min (JVeJ − Nc , L JVeJ JVfJ r(f) − NcJ ))
2
(4)
end(f) =min (start(f) + Nc, JVeJ) . (5)
</equation>
<bodyText confidence="0.99997325">
This defines a diagonal beam4 when visualizing
the lexicon entries in a matrix where both source
and target words are sorted by their frequency rank.
However, note that the result of sorting by frequency
</bodyText>
<footnote confidence="0.9813235">
4The diagonal has some artifacts for the highest and lowest
frequency ranks. See, for example, left side of Figure 2.
</footnote>
<figure confidence="0.9813118125">
y
Arbeit (f)
time (e)
x
work (e)
Zeit (f)
159
Initialization
target words
source words E source words C source words
EM Iterations
target words
Context Vectors
target words
EM Iterations
. . .
</figure>
<figureCaption confidence="0.939504">
Figure 2: Visualization of the training procedure. The big rectangles represent word lexica in different stages of the
training procedure. The small rectangles represent word pairs (e, f) for which e is a translation candidate of f, while
dots represent word pairs (e, f) for which this is not the case. Source and target words are sorted by frequency so that
the most frequent source words appear on the very left, and the most frequent target words appear at the very bottom.
</figureCaption>
<bodyText confidence="0.9995121">
and thus the frequency ranks are not unique when
there are words with the same frequency. In this
case, we initially obtain some not further specified
frequency ordering, which is then kept throughout
the procedure.
This initialization proves useful as we show by
taking an IBM1 lexicon P(f|e) extracted on the
parallel VERBMOBIL corpus (Wahlster, 2000): For
each word e we calculate the weighted rank differ-
ence
</bodyText>
<equation confidence="0.9794425">
�Ar�vg(e) = P(f|e) · |(r(e) − r(f) |(6)
f
</equation>
<bodyText confidence="0.999914428571429">
and count how many of those weighted rank dif-
ferences are smaller than a given value 2c . Here
we see that for about 1% of the words the weighted
rank difference lies within NC = 50, and even about
3% for NC = 150 respectively. This shows that the
initialization provides a first solid guess of possible
translations.
</bodyText>
<subsectionHeader confidence="0.993359">
5.2 EM Algorithm
</subsectionHeader>
<bodyText confidence="0.999990777777778">
The generative story described in Section 3 is im-
plemented as a cascade of a permutation, insertion,
lexicon, deletion and language model finite state
transducers using OpenFST (Allauzen et al., 2007).
Our FST representation of the LM makes use of
failure transitions as described in (Allauzen et al.,
2003). We use the forward-backward algorithm on
the composed transducers to efficiently train the lex-
icon model using the EM algorithm.
</bodyText>
<subsectionHeader confidence="0.996331">
5.3 Context Vector Step
</subsectionHeader>
<bodyText confidence="0.999919533333333">
Given the trained parameters 0 from the previous run
of the EM algorithm we set the context vectors ve
and vf up as described in Section 4. We then calcu-
late and sort all |Ve|·|Vf |distances which proves fea-
sible in a few CPU hours even for vocabulary sizes
of more than 50,000 words. This is achieved with
the GNU SORT tool, which uses external sorting for
sorting large amounts of data.
To set up the new lexicon we keep the b2c c
best translations for each source word with respect
to P(e|f), which we obtained in the previous EM
run. Experiments showed that it is helpful to also
limit the number of candidates per target words. We
therefore prune the resulting lexicon using P(f|e)
to a maximum of b�� 2 c candidates per target word
</bodyText>
<equation confidence="0.523402">
C
</equation>
<bodyText confidence="0.999893615384616">
afterwards. Then we fill the lexicon with new can-
didates using the previously sorted list of candidate
pairs such that the final lexicon has at most NC
candidates per source word and at most Nc can-
didates per target word. We set Nc to some value
Nc &gt; NC. All experiments in this work were run
with Nc = 300. Values of Nc ≈ NC seem to pro-
duce poorer results. Not limiting the number of can-
didates per target word at all also typically results in
weaker performance. After the lexicon is filled with
candidates, we initialize the probabilities to be uni-
form. With this new lexicon the process is iterated
starting with the EM training.
</bodyText>
<sectionHeader confidence="0.998512" genericHeader="method">
6 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999945857142857">
We evaluate our method on three different corpora.
At first we apply our method to non-parallel Span-
ish/English data that is based on the OPUS corpus
(Tiedemann, 2009) and that was also used in (Ravi
and Knight, 2011). We show that our method per-
forms better by 1.6 BLEU than the best performing
method described in (Ravi and Knight, 2011) while
</bodyText>
<page confidence="0.991947">
160
</page>
<table confidence="0.9998299">
Name Lang. Sent. Words Voc.
Spanish 13,181 39,185 562
OPUS
English 19,770 61,835 411
German 27,861 282,831 5,964
VERBMOBIL
English 27,862 294,902 3,723
French 100,000 1,725,993 68,259
GIGAWORD
English 100,000 1,788,025 64,621
</table>
<tableCaption confidence="0.999916">
Table 1: Statistics of the corpora used in this paper.
</tableCaption>
<bodyText confidence="0.999883">
being approximately 15 to 20 times faster than their
n-gram based approach.
After that we apply our method to a non-parallel
version of the German/English VERBMOBIL corpus,
which has a vocabulary size of 6,000 words on the
German side, and 3,500 words on the target side and
which thereby is approximately one order of magni-
tude larger than the previous OPUS experiment.
We finally run our system on a subset of the non-
parallel French/English GIGAWORD corpus, which
has a vocabulary size of 60,000 words for both
French and English. We show first interesting re-
sults on such a big task.
In case of the OPUS and VERBMOBIL corpus,
we evaluate the results using BLEU (Papineni et al.,
2002) and TER (Snover et al., 2006) to reference
translations. We report all scores in percent. For
BLEU higher values are better, for TER lower val-
ues are better. We also compare the results on these
corpora to a system trained on parallel data.
In case of the GIGAWORD corpus we show lexi-
con entries obtained during training.
</bodyText>
<subsectionHeader confidence="0.8036725">
6.1 OPUS Subtitle Corpus
6.1.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.99131675">
We apply our method to the corpus described in
Table 6. This exact corpus was also used in (Ravi
and Knight, 2011). The best performing methods
in (Ravi and Knight, 2011) use the full 411 x 579
lexicon model and apply standard EM training. Us-
ing a 2-gram LM they obtain 15.3 BLEU and with
a whole segment LM, they achieve 19.3 BLEU. In
comparison to this baseline we run our algorithm
with NC = 50 candidates per source word for both,
a 2-gram and a 3-gram LM. We use 30 EM iterations
between each context vector step. For both cases we
run 7 EM+Context cycles.
</bodyText>
<sectionHeader confidence="0.877635" genericHeader="method">
6.1.2 Results
</sectionHeader>
<bodyText confidence="0.998919785714286">
Figure 3 and Figure 4 show the evolution of BLEU
and TER scores for applying our method using a 2-
gram and a 3-gram LM.
In case of the 2-gram LM (Figure 3) the transla-
tion quality increases until it reaches a plateau after
5 EM+Context cycles. In case of the 3-gram LM
(Figure 4) the statement only holds with respect to
TER. It is notable that during the first iterations TER
only improves very little until a large chunk of the
language unravels after the third iteration. This be-
havior may be caused by the fact that the corpus only
provides a relatively small amount of context infor-
mation for each word, since sentence lengths are 3-4
words on average.
</bodyText>
<figure confidence="0.386396">
Iteration
</figure>
<figureCaption confidence="0.98806925">
Figure 3: Results on the OPUS corpus with a 2-gram LM,
NC = 50, and 30 EM iterations between each context
vector step. The dashed line shows the best result using a
2-gram LM in (Ravi and Knight, 2011).
</figureCaption>
<bodyText confidence="0.999543666666666">
Table 2 summarizes these results and compares
them with (Ravi and Knight, 2011). Our 3-gram
based method performs by 1.6 BLEU better than
their best system which is a statistically significant
improvement at 95% confidence level. Furthermore,
Table 2 compares the CPU time needed for training.
Our 3-gram based method is 15-20 times faster than
running the EM based training procedure presented
in (Ravi and Knight, 2011) with a 3-gram LM5.
</bodyText>
<note confidence="0.5003665">
5(Ravi and Knight, 2011) only report results using a 2-gram
LM and a whole-segment LM.
</note>
<figure confidence="0.98027105">
80
Full EM best (BLEU)
78 TER
76
74
72
70
68
66
0 1 2 3 4 5 6 7 8
BLEU 16
14
12
10
8
BLEU
TER
161
0 1 2 3 4 5 6 7 8
Iteration
</figure>
<figureCaption confidence="0.958859">
Figure 4: Results on the OPUS corpus with a 3-gram LM,
Nc = 50, and 30 EM iterations between each context
vector step. The dashed line shows the best result using a
whole-segment LM in (Ravi and Knight, 2011)
</figureCaption>
<table confidence="0.999911846153846">
Method CPU BLEU TER
EM, 2-gram LM
411 cand. p. source word ≈850h6 15.3 −
(Ravi and Knight, 2011)
EM, Whole-segment LM
411 cand. p. source word −7 19.3 −
(Ravi and Knight, 2011)
EM+Context, 2-gram LM
50 cand. p. source word 50h8 15.2 66.6
(this work)
EM+Context, 3-gram LM
50 cand. p. source word 200h8 20.9 64.5
(this work)
</table>
<tableCaption confidence="0.999771">
Table 2: Results obtained on the OPUS corpus.
</tableCaption>
<bodyText confidence="0.867201333333333">
To summarize: Our method is significantly faster
than n-gram LM based approaches and obtains bet-
ter results than any previously published method.
</bodyText>
<footnote confidence="0.996671818181818">
6Estimated by running full EM using the 2-gram LM using
our implementation for 90 Iterations yielding 15.2 BLEU.
7≈4,000h when running full EM using a 3-gram LM, using
our implementation. Estimated by running only the first itera-
tion and by assuming that the final result will be obtained after
90 iterations. However, (Ravi and Knight, 2011) report results
using a whole segment LM, assigning P(e) &gt; 0 only to se-
quences seen in training. This seems to work for the given task
but we believe that it can not be a general replacement for higher
order n-gram LMs.
8Estimated by running our method for 5 × 30 iterations.
</footnote>
<subsectionHeader confidence="0.9895985">
6.2 VERBMOBIL Corpus
6.2.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999976588235294">
The VERBMOBIL corpus is a German/English
corpus dealing with short sentences for making ap-
pointments. We prepared a non-parallel subset of
the original VERBMOBIL (Wahlster, 2000) by split-
ting the corpus into two parts and then selecting only
the German side from the first half, and the English
side from the second half such that the target side
is not the translation of the source side. The source
and target vocabularies of the resulting non-parallel
corpus are both more than 9 times bigger compared
to the OPUS vocabularies. Also the total amount of
word tokens is more than 5 times larger compared
to the OPUS corpus. Table 6 shows the statistics of
this corpus. We run our method for 5 EM+Context
cycles (30 EM iterations each) using a 2-gram LM.
After that we run another five EM+Context cycles
using a 3-gram LM.
</bodyText>
<sectionHeader confidence="0.7585" genericHeader="method">
6.2.2 Results
</sectionHeader>
<bodyText confidence="0.994718666666667">
Our results on the VERBMOBIL corpus are sum-
marized in Table 3. Even on this more complex
task our method achieves encouraging results: The
</bodyText>
<subsectionHeader confidence="0.493421">
Method BLEU TER
</subsectionHeader>
<table confidence="0.9779615">
5 × 30 Iterations EM+Context 11.7 67.4
50 cand. p. source word, 2-gram LM
+ 5 × 30 Iterations EM+Context 15.5 63.2
50 cand. p. source word, 3-gram LM
</table>
<tableCaption confidence="0.999918">
Table 3: Results obtained on the VERBMOBIL corpus.
</tableCaption>
<bodyText confidence="0.998743818181818">
translation quality increases from iteration to itera-
tion until the algorithm finally reaches 11.7 BLEU
using only the 2-gram LM. Running further five
cycles using a 3-gram LM achieves a final perfor-
mance of 15.5 BLEU. Och (2002) reports results of
48.2 BLEU for a single-word based translation sys-
tem and 56.1 BLEU using the alignment template
approach, both trained on parallel data. However, it
should be noted that our experiment only uses 50%
of the original VERBMOBIL training data to simulate
a truly non-parallel setup.
</bodyText>
<figure confidence="0.996365294117647">
Full EM best (BLEU)
BLEU
TER
BLEU 24
22
20
18
16
14
12
10
8
72 TER
70
68
66
64
</figure>
<page confidence="0.976586">
162
</page>
<table confidence="0.876858285714286">
Iter. e p(f1|e) f1 p(f2|e) f2 p(f3|e) f3 p(f4|e) f4 p(f5|e) f5
the 0.43 la 0.31 l’ 0.11 une 0.04 le 0.04 les
several 0.57 plusieurs 0.21 les 0.09 des 0.03 nombreuses 0.02 deux
where 0.63 o`u 0.17 mais 0.06 indique 0.04 pr´ecise 0.02 appelle
see 0.49 ´eviter 0.09 effet 0.09 voir 0.05 envisager 0.04 dire
January 0.25 octobre 0.22 mars 0.09 juillet 0.07 aoˆut 0.07 janvier
− Germany 0.24 Italie 0.12 Espagne 0.06 Japon 0.05 retour 0.05 Suisse
</table>
<tableCaption confidence="0.681487">
Table 4: Lexicon entries obtained by running our method on the non-parallel GIGAWORD corpus. The first column
shows in which iteration the algorithm found the first correct translations f (compared to a parallely trained lexicon)
among the top 5 candidates
</tableCaption>
<sectionHeader confidence="0.715127" genericHeader="method">
6.3 GIGAWORD
6.3.1 Experimental Setup
</sectionHeader>
<bodyText confidence="0.99995745">
This setup is based on a subset of the monolingual
GIGAWORD corpus. We selected 100,000 French
sentences from the news agency AFP and 100,000
sentences from the news agency Xinhua. To have a
more reliable set of training instances, we selected
only sentences with more than 7 tokens. Note that
these corpora form true non-parallel data which, be-
sides the length filtering, were not specifically pre-
selected or pre-processed. More details on these
non-parallel corpora are summarized in Table 6. The
vocabularies have a size of approximately 60,000
words which is more than 100 times larger than the
vocabularies of the OPUS corpus. Also it incor-
porates more than 25 times as many tokens as the
OPUS corpus.
After initialization, we run our method with
NC = 150 candidates per source word for 20 EM
iterations using a 2-gram LM. After the first context
vector step with NC = 50 we run another 4 x 20
iterations with NC = 50 with a 2-gram LM.
</bodyText>
<sectionHeader confidence="0.942554" genericHeader="evaluation">
6.3.2 Results
</sectionHeader>
<bodyText confidence="0.999885222222222">
Table 4 shows example lexicon entries we ob-
tained. Note that we obtained these results by us-
ing purely non-parallel data, and that we neither
used a seed lexicon, nor orthographic features to as-
sign e.g. numbers or proper names: All results are
obtained using 2-gram statistics and the context of
words only. We find the results encouraging and
think that they show the potential of large-scale un-
supervised techniques for MT in the future.
</bodyText>
<sectionHeader confidence="0.999046" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999981695652174">
We presented a method for learning statistical ma-
chine translation models from non-parallel data. The
key to our method lies in limiting the translation
model to a limited set of translation candidates and
then using the EM algorithm to learn the probabil-
ities. Based on the translations obtained with this
model we obtain new translation candidates using
a context vector approach. This method increased
the training speed by a factor of 10-20 compared
to methods known in literature and also resulted
in a 1.6 BLEU point increase compared to previ-
ous approaches. Due to this efficiency improvement
we were able to tackle larger tasks, such as a non-
parallel version of the VERBMOBIL corpus having
a nearly 10 times larger vocabulary. We also had a
look at first results of our method on an even larger
Task, incorporating a vocabulary of 60,000 words.
We have shown that, using a limited set of trans-
lation candidates, we can significantly reduce the
computational complexity of the learning task. This
work serves as a big step towards large-scale unsu-
pervised training for statistical machine translation
systems.
</bodyText>
<sectionHeader confidence="0.996512" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999977666666667">
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation. The authors would like to thank Su-
jith Ravi and Kevin Knight for providing us with the
OPUS subtitle corpus and David Rybach for kindly
sharing his knowledge about the OpenFST library.
</bodyText>
<page confidence="0.998669">
163
</page>
<sectionHeader confidence="0.995889" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999952719101123">
Cyril Allauzen, Mehryar Mohri, and Brian Roark.
2003. Generalized algorithms for constructing sta-
tistical language models. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 40–47. Association for
Computational Linguistics.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. Openfst: A
general and efficient weighted finite-state transducer
library. In Jan Holub and Jan Zd´arek, editors, CIAA,
volume 4783 of Lecture Notes in Computer Science,
pages 11–23. Springer.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society, B, 39.
Aria Haghighi, Percy Liang, T Berg-Kirkpatrick, and
Dan Klein. 2008. Learning Bilingual Lexicons from
Monolingual Corpora. In Proceedings of ACL08 HLT,
pages 771–779. Association for Computational Lin-
guistics.
Kevin Knight and Kenji Yamada. 1999. A computa-
tional approach to deciphering unknown scripts. In
ACL Workshop on Unsupervised Learning in Natural
Language Processing, number 1, pages 37–44. Cite-
seer.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In Pro-
ceedings of the ACL02 workshop on Unsupervised lex-
ical acquisition, number July, pages 9–16. Association
for Computational Linguistics.
Harold W. Kuhn. 1955. The Hungarian method for the
assignment problem. Naval Research Logistic Quar-
terly, 2:83–97.
Shou-de Lin and Kevin Knight. 2006. Discovering
the linear writing order of a two-dimensional ancient
hieroglyphic script. Artificial Intelligence, 170:409–
421, April.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schuetze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, 1 edition, July.
Franz J. Och. 2002. Statistical Machine Translation:
From Single-Word Models to Alignment Templates.
Ph.D. thesis, RWTH Aachen University, Aachen, Ger-
many, October.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ’02, pages 311–318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Sujith Ravi and Kevin Knight. 2008. Attacking decipher-
ment problems optimally with low-order n-gram mod-
els. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
’08, pages 812–819, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 12–21,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
Gerard M. Salton, Andrew K. C. Wong, and Chang S.
Yang. 1975. A vector space model for automatic in-
dexing. Commun. ACM, 18(11):613–620, November.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Americas,
pages 223–231, Cambridge, Massachusetts, USA, Au-
gust.
Benjamin Snyder, Regina Barzilay, and Kevin Knight.
2010. A statistical model for lost language decipher-
ment. In 48th Annual Meeting of the Association for
Computational Linguistics, number July, pages 1048–
1057.
J¨org Tiedemann. 2009. News from OPUS - A collec-
tion of multilingual parallel corpora with tools and in-
terfaces. In N. Nicolov, K. Bontcheva, G. Angelova,
and R. Mitkov, editors, Recent Advances in Natural
Language Processing, volume V, pages 237–248. John
Benjamins, Amsterdam/Philadelphia, Borovets, Bul-
garia.
Wolfgang Wahlster, editor. 2000. Verbmobil: Foun-
dations of speech-to-speech translations. Springer-
Verlag, Berlin.
</reference>
<page confidence="0.998522">
164
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.520780">
<title confidence="0.9632">Deciphering Foreign Language by Combining Language Models Context Vectors Nuhn and Human Language Technology and Pattern Recognition</title>
<affiliation confidence="0.711741">RWTH Aachen University,</affiliation>
<email confidence="0.994668"><surname>@cs.rwth-aachen.de</email>
<abstract confidence="0.999441">In this paper we show how to train statistical machine translation systems on reallife tasks using only non-parallel monolingual data from two languages. We present a modification of the method shown in (Ravi and Knight, 2011) that is scalable to vocabulary sizes of several thousand words. On the task shown in (Ravi and Knight, 2011) we obtain better results with only 5% of the computational effort when running our method with language model. The efficiency improvement of our method allows us to run experiments with vocabulary sizes of around 5,000 words, such as a non-parallel version of We also report results using data from the monolingual French</abstract>
<intro confidence="0.865266">English</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Mehryar Mohri</author>
<author>Brian Roark</author>
</authors>
<title>Generalized algorithms for constructing statistical language models.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>40--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16721" citStr="Allauzen et al., 2003" startWordPosition="2823" endWordPosition="2826">f those weighted rank differences are smaller than a given value 2c . Here we see that for about 1% of the words the weighted rank difference lies within NC = 50, and even about 3% for NC = 150 respectively. This shows that the initialization provides a first solid guess of possible translations. 5.2 EM Algorithm The generative story described in Section 3 is implemented as a cascade of a permutation, insertion, lexicon, deletion and language model finite state transducers using OpenFST (Allauzen et al., 2007). Our FST representation of the LM makes use of failure transitions as described in (Allauzen et al., 2003). We use the forward-backward algorithm on the composed transducers to efficiently train the lexicon model using the EM algorithm. 5.3 Context Vector Step Given the trained parameters 0 from the previous run of the EM algorithm we set the context vectors ve and vf up as described in Section 4. We then calculate and sort all |Ve|·|Vf |distances which proves feasible in a few CPU hours even for vocabulary sizes of more than 50,000 words. This is achieved with the GNU SORT tool, which uses external sorting for sorting large amounts of data. To set up the new lexicon we keep the b2c c best transla</context>
</contexts>
<marker>Allauzen, Mohri, Roark, 2003</marker>
<rawString>Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003. Generalized algorithms for constructing statistical language models. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 40–47. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
</authors>
<title>Wojciech Skut, and Mehryar Mohri.</title>
<date>2007</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>4783</volume>
<pages>11--23</pages>
<editor>In Jan Holub and Jan Zd´arek, editors, CIAA,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="16614" citStr="Allauzen et al., 2007" startWordPosition="2805" endWordPosition="2808"> e we calculate the weighted rank difference �Ar�vg(e) = P(f|e) · |(r(e) − r(f) |(6) f and count how many of those weighted rank differences are smaller than a given value 2c . Here we see that for about 1% of the words the weighted rank difference lies within NC = 50, and even about 3% for NC = 150 respectively. This shows that the initialization provides a first solid guess of possible translations. 5.2 EM Algorithm The generative story described in Section 3 is implemented as a cascade of a permutation, insertion, lexicon, deletion and language model finite state transducers using OpenFST (Allauzen et al., 2007). Our FST representation of the LM makes use of failure transitions as described in (Allauzen et al., 2003). We use the forward-backward algorithm on the composed transducers to efficiently train the lexicon model using the EM algorithm. 5.3 Context Vector Step Given the trained parameters 0 from the previous run of the EM algorithm we set the context vectors ve and vf up as described in Section 4. We then calculate and sort all |Ve|·|Vf |distances which proves feasible in a few CPU hours even for vocabulary sizes of more than 50,000 words. This is achieved with the GNU SORT tool, which uses e</context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, 2007</marker>
<rawString>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. Openfst: A general and efficient weighted finite-state transducer library. In Jan Holub and Jan Zd´arek, editors, CIAA, volume 4783 of Lecture Notes in Computer Science, pages 11–23. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur P Dempster</author>
<author>Nan M Laird</author>
<author>Donald B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, B,</journal>
<volume>39</volume>
<contexts>
<context position="7552" citStr="Dempster et al., 1977" startWordPosition="1231" endWordPosition="1234">n and the translation model that is trained using monolingual data. In addition to the mathematical formulation of the model we describe approximations used. Throughout this work, we denote the source language words as f and target language words as e. The source vocabulary is Vf and we write the size of this vocabulary as |Vf|. The same notation holds for the target vocabulary with Ve and |Ve|. As training criterion for the translation model’s parameters 0, Ravi and Knight (2011) suggest arg max {n E P(e) · pe(f|e) } (1) e f e We would like to obtain 0 from Equation 1 using the EM Algorithm (Dempster et al., 1977). This becomes increasingly difficult with more complex translation models. Therefore, we use a simplified 157 translation model that still contains all basic phenomena of a generic translation process. We formulate the translation process with the same generative story presented in (Ravi and Knight, 2011): 1. Stochastically generate the target sentence according to an n-gram language model. 2. Insert NULL tokens between any two adjacent positions of the target string with uniform probability. 3. For each target token ei (including NULL) choose a foreign translation fi (including NULL) with pr</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, B, 39.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>T Berg-Kirkpatrick</author>
</authors>
<location>and</location>
<marker>Haghighi, Liang, Berg-Kirkpatrick, </marker>
<rawString>Aria Haghighi, Percy Liang, T Berg-Kirkpatrick, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
</authors>
<title>Learning Bilingual Lexicons from Monolingual Corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL08 HLT,</booktitle>
<pages>771--779</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Klein, 2008</marker>
<rawString>Dan Klein. 2008. Learning Bilingual Lexicons from Monolingual Corpora. In Proceedings of ACL08 HLT, pages 771–779. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Kenji Yamada</author>
</authors>
<title>A computational approach to deciphering unknown scripts.</title>
<date>1999</date>
<booktitle>In ACL Workshop on Unsupervised Learning in Natural Language Processing,</booktitle>
<volume>1</volume>
<pages>37--44</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="4507" citStr="Knight and Yamada, 1999" startWordPosition="715" endWordPosition="718"> In Section 2, we will review the related work and describe how our approach extends existing work. Section 3 describes the model and training criterion used in this work. The implementation and the training of this model is then described in Section 5 and experimentally evaluated in Section 6. 2 Related Work Unsupervised training of statistical translations systems without parallel data and related problems have been addressed before. In this section, we will review previous approaches and highlight similarities and differences to our work. Several steps have been made in this area, such as (Knight and Yamada, 1999), (Ravi and Knight, 2008), or (Snyder et al., 2010), to name just a few. The main difference of our work is, that it allows for much larger vocabulary sizes and more data to be used than previous work while at the same time not being dependent on seed lexica and/or any other knowledge of the languages. Close to the methods described in this work, Ravi and Knight (2011) treat training and translation without parallel data as a deciphering problem. Their best performing approach uses an EMAlgorithm to train a generative word based translation model. They perform experiments on a Spanish/English </context>
</contexts>
<marker>Knight, Yamada, 1999</marker>
<rawString>Kevin Knight and Kenji Yamada. 1999. A computational approach to deciphering unknown scripts. In ACL Workshop on Unsupervised Learning in Natural Language Processing, number 1, pages 37–44. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Learning a translation lexicon from monolingual corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL02 workshop on Unsupervised lexical acquisition, number July,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5632" citStr="Koehn and Knight (2002)" startWordPosition="913" endWordPosition="916">to train a generative word based translation model. They perform experiments on a Spanish/English task with vocabulary sizes of about 500 words and achieve a performance of around 20 BLEU compared to 70 BLEU obtained by a system that was trained on parallel data. Our work uses the same training criterion and is based on the same generative story. However, we use a new training procedure whose critical parts have constant time and memory complexity with respect to the vocabulary size so that our methods can scale to much larger vocabulary sizes while also being faster. In a different approach, Koehn and Knight (2002) induce a bilingual lexicon from only non-parallel data. To achieve this they use a seed lexicon which they systematically extend by using orthographic as well as distributional features such as context, and frequency. They perform their experiments on nonparallel German-English news texts, and test their mappings against a bilingual lexicon. We use a greedy method similar to (Koehn and Knight, 2002) for extending a given lexicon, and we implicitly also use the frequency as a feature. However, we perform fully unsupervised training and do not start with a seed lexicon or use linguistic feature</context>
</contexts>
<marker>Koehn, Knight, 2002</marker>
<rawString>Philipp Koehn and Kevin Knight. 2002. Learning a translation lexicon from monolingual corpora. In Proceedings of the ACL02 workshop on Unsupervised lexical acquisition, number July, pages 9–16. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold W Kuhn</author>
</authors>
<title>The Hungarian method for the assignment problem.</title>
<date>1955</date>
<journal>Naval Research Logistic Quarterly,</journal>
<pages>2--83</pages>
<contexts>
<context position="12393" citStr="Kuhn, 1955" startWordPosition="2075" endWordPosition="2076">distance measure.2 The normalization of context vectors described above is motivated by the fact that the context vectors should be invariant with respect to the absolute number of occurrences of words.3 Instead of just finding the best candidates for a given word, we are interested in an assignment that involves all source and target words, minimizing the sum of distances between the assigned words. In case of a one-to-one mapping the problem of assigning translation candidates such that the sum of distances is minimal can be solved optimally in polynomial time using the hungarian algorithm (Kuhn, 1955). In our case we are dealing with a manyto-many assignment that needs to satisfy the maximum number of candidates constraints. For this, we solve the problem in a greedy fashion by simply choosing the best pairs (e, f) first. As soon as a target word e or source word f has reached the limit of maximum candidates, we skip all further candidates for that word e (or f respectively). This step involves calculating and sorting all |Ve ||Vf |distances which can be done in time O(V 2 log(V )), with V = max(|Ve|, |Vf|). A simplified example of this procedure is depicted in Figure 1. The example alread</context>
</contexts>
<marker>Kuhn, 1955</marker>
<rawString>Harold W. Kuhn. 1955. The Hungarian method for the assignment problem. Naval Research Logistic Quarterly, 2:83–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shou-de Lin</author>
<author>Kevin Knight</author>
</authors>
<title>Discovering the linear writing order of a two-dimensional ancient hieroglyphic script.</title>
<date>2006</date>
<journal>Artificial Intelligence,</journal>
<volume>170</volume>
<pages>421</pages>
<marker>Shou-de Lin, Knight, 2006</marker>
<rawString>Shou-de Lin and Kevin Knight. 2006. Discovering the linear writing order of a two-dimensional ancient hieroglyphic script. Artificial Intelligence, 170:409– 421, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Schuetze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<volume>1</volume>
<pages>edition,</pages>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="13359" citStr="Manning et al., 2008" startWordPosition="2246" endWordPosition="2249"> for that word e (or f respectively). This step involves calculating and sorting all |Ve ||Vf |distances which can be done in time O(V 2 log(V )), with V = max(|Ve|, |Vf|). A simplified example of this procedure is depicted in Figure 1. The example already shows that the assignment obtained by this algorithm is in general not optimal. 2We then obtain pairs (e, f) that minimize d. 3This gives the same similarity ordering as using unnormalized vectors with the cosine similarity measure ve·vf which can be interpreted as measuring the cosine ||ve||2·||vf ||2 of the angle between the vectors, see (Manning et al., 2008). Still it is noteworthy that this procedure is not equivalent to the tf-IDF context vectors described in (Salton et al., 1975). Figure 1: Hypothetical example for a greedy one-to-one assignment of translation candidates. The optimal assignment would contain (time,Zeit) and (work,Arbeit). 5 Training Algorithm and Implementation Given the model presented in Section 3 and the methods illustrated in Section 4, we now describe how to train this model. As described in Section 4, the overall procedure is divided into two alternating steps: After initialization we first perform EM training of the tra</context>
</contexts>
<marker>Manning, Raghavan, Schuetze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schuetze. 2008. Introduction to Information Retrieval. Cambridge University Press, 1 edition, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Statistical Machine Translation: From Single-Word Models to Alignment Templates.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>RWTH Aachen University,</institution>
<location>Aachen, Germany,</location>
<contexts>
<context position="24880" citStr="Och (2002)" startWordPosition="4278" endWordPosition="4279">3-gram LM. 6.2.2 Results Our results on the VERBMOBIL corpus are summarized in Table 3. Even on this more complex task our method achieves encouraging results: The Method BLEU TER 5 × 30 Iterations EM+Context 11.7 67.4 50 cand. p. source word, 2-gram LM + 5 × 30 Iterations EM+Context 15.5 63.2 50 cand. p. source word, 3-gram LM Table 3: Results obtained on the VERBMOBIL corpus. translation quality increases from iteration to iteration until the algorithm finally reaches 11.7 BLEU using only the 2-gram LM. Running further five cycles using a 3-gram LM achieves a final performance of 15.5 BLEU. Och (2002) reports results of 48.2 BLEU for a single-word based translation system and 56.1 BLEU using the alignment template approach, both trained on parallel data. However, it should be noted that our experiment only uses 50% of the original VERBMOBIL training data to simulate a truly non-parallel setup. Full EM best (BLEU) BLEU TER BLEU 24 22 20 18 16 14 12 10 8 72 TER 70 68 66 64 162 Iter. e p(f1|e) f1 p(f2|e) f2 p(f3|e) f3 p(f4|e) f4 p(f5|e) f5 the 0.43 la 0.31 l’ 0.11 une 0.04 le 0.04 les several 0.57 plusieurs 0.21 les 0.09 des 0.03 nombreuses 0.02 deux where 0.63 o`u 0.17 mais 0.06 indique 0.04</context>
</contexts>
<marker>Och, 2002</marker>
<rawString>Franz J. Och. 2002. Statistical Machine Translation: From Single-Word Models to Alignment Templates. Ph.D. thesis, RWTH Aachen University, Aachen, Germany, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="19594" citStr="Papineni et al., 2002" startWordPosition="3327" endWordPosition="3330"> approach. After that we apply our method to a non-parallel version of the German/English VERBMOBIL corpus, which has a vocabulary size of 6,000 words on the German side, and 3,500 words on the target side and which thereby is approximately one order of magnitude larger than the previous OPUS experiment. We finally run our system on a subset of the nonparallel French/English GIGAWORD corpus, which has a vocabulary size of 60,000 words for both French and English. We show first interesting results on such a big task. In case of the OPUS and VERBMOBIL corpus, we evaluate the results using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) to reference translations. We report all scores in percent. For BLEU higher values are better, for TER lower values are better. We also compare the results on these corpora to a system trained on parallel data. In case of the GIGAWORD corpus we show lexicon entries obtained during training. 6.1 OPUS Subtitle Corpus 6.1.1 Experimental Setup We apply our method to the corpus described in Table 6. This exact corpus was also used in (Ravi and Knight, 2011). The best performing methods in (Ravi and Knight, 2011) use the full 411 x 579 lexicon model and apply standard </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Attacking decipherment problems optimally with low-order n-gram models.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>812--819</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4532" citStr="Ravi and Knight, 2008" startWordPosition="719" endWordPosition="722">ew the related work and describe how our approach extends existing work. Section 3 describes the model and training criterion used in this work. The implementation and the training of this model is then described in Section 5 and experimentally evaluated in Section 6. 2 Related Work Unsupervised training of statistical translations systems without parallel data and related problems have been addressed before. In this section, we will review previous approaches and highlight similarities and differences to our work. Several steps have been made in this area, such as (Knight and Yamada, 1999), (Ravi and Knight, 2008), or (Snyder et al., 2010), to name just a few. The main difference of our work is, that it allows for much larger vocabulary sizes and more data to be used than previous work while at the same time not being dependent on seed lexica and/or any other knowledge of the languages. Close to the methods described in this work, Ravi and Knight (2011) treat training and translation without parallel data as a deciphering problem. Their best performing approach uses an EMAlgorithm to train a generative word based translation model. They perform experiments on a Spanish/English task with vocabulary size</context>
</contexts>
<marker>Ravi, Knight, 2008</marker>
<rawString>Sujith Ravi and Kevin Knight. 2008. Attacking decipherment problems optimally with low-order n-gram models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 812–819, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Deciphering foreign language.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>12--21</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="3259" citStr="Ravi and Knight, 2011" startWordPosition="517" endWordPosition="520">or almost all languages. 2. We can divide the given text in tokens and sentence-like units. This implies that we know enough about the language to tokenize and sentence-split a given text. Again, for the vast majority of languages, this is not a strong restriction. 3. The writing system is one-dimensional left-toright. It has been shown (Lin and Knight, 2006) that the writing direction can be determined separately and therefore this assumption does not pose a real restriction. Previous approaches to unsupervised training for SMT prove feasible only for vocabulary sizes up to around 500 words (Ravi and Knight, 2011) and data 156 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156–164, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics sets of roughly 15,000 sentences containing only about 4 tokens per sentence on average. Real data as it occurs in texts such as web pages or news texts does not meet any of these characteristics. In this work, we will develop, describe, and evaluate methods for large vocabulary unsupervised learning of machine translation models suitable for real-world tasks. The remainder of this paper i</context>
<context position="4878" citStr="Ravi and Knight (2011)" startWordPosition="785" endWordPosition="788">out parallel data and related problems have been addressed before. In this section, we will review previous approaches and highlight similarities and differences to our work. Several steps have been made in this area, such as (Knight and Yamada, 1999), (Ravi and Knight, 2008), or (Snyder et al., 2010), to name just a few. The main difference of our work is, that it allows for much larger vocabulary sizes and more data to be used than previous work while at the same time not being dependent on seed lexica and/or any other knowledge of the languages. Close to the methods described in this work, Ravi and Knight (2011) treat training and translation without parallel data as a deciphering problem. Their best performing approach uses an EMAlgorithm to train a generative word based translation model. They perform experiments on a Spanish/English task with vocabulary sizes of about 500 words and achieve a performance of around 20 BLEU compared to 70 BLEU obtained by a system that was trained on parallel data. Our work uses the same training criterion and is based on the same generative story. However, we use a new training procedure whose critical parts have constant time and memory complexity with respect to t</context>
<context position="7415" citStr="Ravi and Knight (2011)" startWordPosition="1201" endWordPosition="1204">neither uses a seed lexicon nor orthographic features. 3 Translation Model In this section, we describe the statistical training criterion and the translation model that is trained using monolingual data. In addition to the mathematical formulation of the model we describe approximations used. Throughout this work, we denote the source language words as f and target language words as e. The source vocabulary is Vf and we write the size of this vocabulary as |Vf|. The same notation holds for the target vocabulary with Ve and |Ve|. As training criterion for the translation model’s parameters 0, Ravi and Knight (2011) suggest arg max {n E P(e) · pe(f|e) } (1) e f e We would like to obtain 0 from Equation 1 using the EM Algorithm (Dempster et al., 1977). This becomes increasingly difficult with more complex translation models. Therefore, we use a simplified 157 translation model that still contains all basic phenomena of a generic translation process. We formulate the translation process with the same generative story presented in (Ravi and Knight, 2011): 1. Stochastically generate the target sentence according to an n-gram language model. 2. Insert NULL tokens between any two adjacent positions of the targ</context>
<context position="18493" citStr="Ravi and Knight, 2011" startWordPosition="3142" endWordPosition="3145">All experiments in this work were run with Nc = 300. Values of Nc ≈ NC seem to produce poorer results. Not limiting the number of candidates per target word at all also typically results in weaker performance. After the lexicon is filled with candidates, we initialize the probabilities to be uniform. With this new lexicon the process is iterated starting with the EM training. 6 Experimental Evaluation We evaluate our method on three different corpora. At first we apply our method to non-parallel Spanish/English data that is based on the OPUS corpus (Tiedemann, 2009) and that was also used in (Ravi and Knight, 2011). We show that our method performs better by 1.6 BLEU than the best performing method described in (Ravi and Knight, 2011) while 160 Name Lang. Sent. Words Voc. Spanish 13,181 39,185 562 OPUS English 19,770 61,835 411 German 27,861 282,831 5,964 VERBMOBIL English 27,862 294,902 3,723 French 100,000 1,725,993 68,259 GIGAWORD English 100,000 1,788,025 64,621 Table 1: Statistics of the corpora used in this paper. being approximately 15 to 20 times faster than their n-gram based approach. After that we apply our method to a non-parallel version of the German/English VERBMOBIL corpus, which has a v</context>
<context position="20081" citStr="Ravi and Knight, 2011" startWordPosition="3413" endWordPosition="3416">esting results on such a big task. In case of the OPUS and VERBMOBIL corpus, we evaluate the results using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) to reference translations. We report all scores in percent. For BLEU higher values are better, for TER lower values are better. We also compare the results on these corpora to a system trained on parallel data. In case of the GIGAWORD corpus we show lexicon entries obtained during training. 6.1 OPUS Subtitle Corpus 6.1.1 Experimental Setup We apply our method to the corpus described in Table 6. This exact corpus was also used in (Ravi and Knight, 2011). The best performing methods in (Ravi and Knight, 2011) use the full 411 x 579 lexicon model and apply standard EM training. Using a 2-gram LM they obtain 15.3 BLEU and with a whole segment LM, they achieve 19.3 BLEU. In comparison to this baseline we run our algorithm with NC = 50 candidates per source word for both, a 2-gram and a 3-gram LM. We use 30 EM iterations between each context vector step. For both cases we run 7 EM+Context cycles. 6.1.2 Results Figure 3 and Figure 4 show the evolution of BLEU and TER scores for applying our method using a 2- gram and a 3-gram LM. In case of the 2-</context>
<context position="21413" citStr="Ravi and Knight, 2011" startWordPosition="3660" endWordPosition="3663">case of the 3-gram LM (Figure 4) the statement only holds with respect to TER. It is notable that during the first iterations TER only improves very little until a large chunk of the language unravels after the third iteration. This behavior may be caused by the fact that the corpus only provides a relatively small amount of context information for each word, since sentence lengths are 3-4 words on average. Iteration Figure 3: Results on the OPUS corpus with a 2-gram LM, NC = 50, and 30 EM iterations between each context vector step. The dashed line shows the best result using a 2-gram LM in (Ravi and Knight, 2011). Table 2 summarizes these results and compares them with (Ravi and Knight, 2011). Our 3-gram based method performs by 1.6 BLEU better than their best system which is a statistically significant improvement at 95% confidence level. Furthermore, Table 2 compares the CPU time needed for training. Our 3-gram based method is 15-20 times faster than running the EM based training procedure presented in (Ravi and Knight, 2011) with a 3-gram LM5. 5(Ravi and Knight, 2011) only report results using a 2-gram LM and a whole-segment LM. 80 Full EM best (BLEU) 78 TER 76 74 72 70 68 66 0 1 2 3 4 5 6 7 8 BLEU</context>
<context position="23136" citStr="Ravi and Knight, 2011" startWordPosition="3972" endWordPosition="3975">6 (this work) EM+Context, 3-gram LM 50 cand. p. source word 200h8 20.9 64.5 (this work) Table 2: Results obtained on the OPUS corpus. To summarize: Our method is significantly faster than n-gram LM based approaches and obtains better results than any previously published method. 6Estimated by running full EM using the 2-gram LM using our implementation for 90 Iterations yielding 15.2 BLEU. 7≈4,000h when running full EM using a 3-gram LM, using our implementation. Estimated by running only the first iteration and by assuming that the final result will be obtained after 90 iterations. However, (Ravi and Knight, 2011) report results using a whole segment LM, assigning P(e) &gt; 0 only to sequences seen in training. This seems to work for the given task but we believe that it can not be a general replacement for higher order n-gram LMs. 8Estimated by running our method for 5 × 30 iterations. 6.2 VERBMOBIL Corpus 6.2.1 Experimental Setup The VERBMOBIL corpus is a German/English corpus dealing with short sentences for making appointments. We prepared a non-parallel subset of the original VERBMOBIL (Wahlster, 2000) by splitting the corpus into two parts and then selecting only the German side from the first half,</context>
</contexts>
<marker>Ravi, Knight, 2011</marker>
<rawString>Sujith Ravi and Kevin Knight. 2011. Deciphering foreign language. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 12–21, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard M Salton</author>
<author>Andrew K C Wong</author>
<author>Chang S Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Commun. ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<contexts>
<context position="9603" citStr="Salton et al. (1975)" startWordPosition="1576" endWordPosition="1579"> of active lexicon entries. 2. Choosing the translation probabilities for the given set of active lexicon entries. The second task can be achieved by running the EM algorithm on the restricted translation model. We deal with the first task in the following section. 4 Monolingual Context Similarity As described in Section 3 we need some mechanism to iteratively choose an active set of translation candidates. Based on the assumption that some of the active candidates and their respective probabilities are already correct, we induce new active candidates. In the context of information retrieval, Salton et al. (1975) introduce a document space where each document identified by one or more index terms is represented by a high dimensional vector of term weights. Given two vectors v1 and v2 of two documents it is then possible to calculate a similarity coefficient between those given documents (which is usually denoted as s(v1, v2)). Similar to this we represent source and target words in a high dimensional vector space of target word weights which we call context vectors and use a similarity coefficient to find possible translation pairs. We first initialize these context vectors using the following procedu</context>
<context position="13486" citStr="Salton et al., 1975" startWordPosition="2267" endWordPosition="2270">me O(V 2 log(V )), with V = max(|Ve|, |Vf|). A simplified example of this procedure is depicted in Figure 1. The example already shows that the assignment obtained by this algorithm is in general not optimal. 2We then obtain pairs (e, f) that minimize d. 3This gives the same similarity ordering as using unnormalized vectors with the cosine similarity measure ve·vf which can be interpreted as measuring the cosine ||ve||2·||vf ||2 of the angle between the vectors, see (Manning et al., 2008). Still it is noteworthy that this procedure is not equivalent to the tf-IDF context vectors described in (Salton et al., 1975). Figure 1: Hypothetical example for a greedy one-to-one assignment of translation candidates. The optimal assignment would contain (time,Zeit) and (work,Arbeit). 5 Training Algorithm and Implementation Given the model presented in Section 3 and the methods illustrated in Section 4, we now describe how to train this model. As described in Section 4, the overall procedure is divided into two alternating steps: After initialization we first perform EM training of the translation model for 20-30 iterations using a 2-gram or 3-gram language model in the target language. With the obtained best tran</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>Gerard M. Salton, Andrew K. C. Wong, and Chang S. Yang. 1975. A vector space model for automatic indexing. Commun. ACM, 18(11):613–620, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<location>Cambridge, Massachusetts, USA,</location>
<contexts>
<context position="19624" citStr="Snover et al., 2006" startWordPosition="3333" endWordPosition="3336">ur method to a non-parallel version of the German/English VERBMOBIL corpus, which has a vocabulary size of 6,000 words on the German side, and 3,500 words on the target side and which thereby is approximately one order of magnitude larger than the previous OPUS experiment. We finally run our system on a subset of the nonparallel French/English GIGAWORD corpus, which has a vocabulary size of 60,000 words for both French and English. We show first interesting results on such a big task. In case of the OPUS and VERBMOBIL corpus, we evaluate the results using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) to reference translations. We report all scores in percent. For BLEU higher values are better, for TER lower values are better. We also compare the results on these corpora to a system trained on parallel data. In case of the GIGAWORD corpus we show lexicon entries obtained during training. 6.1 OPUS Subtitle Corpus 6.1.1 Experimental Setup We apply our method to the corpus described in Table 6. This exact corpus was also used in (Ravi and Knight, 2011). The best performing methods in (Ravi and Knight, 2011) use the full 411 x 579 lexicon model and apply standard EM training. Using a 2-gram LM</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 223–231, Cambridge, Massachusetts, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
<author>Kevin Knight</author>
</authors>
<title>A statistical model for lost language decipherment.</title>
<date>2010</date>
<booktitle>In 48th Annual Meeting of the Association for Computational Linguistics, number July,</booktitle>
<pages>1048--1057</pages>
<contexts>
<context position="4558" citStr="Snyder et al., 2010" startWordPosition="724" endWordPosition="727">ribe how our approach extends existing work. Section 3 describes the model and training criterion used in this work. The implementation and the training of this model is then described in Section 5 and experimentally evaluated in Section 6. 2 Related Work Unsupervised training of statistical translations systems without parallel data and related problems have been addressed before. In this section, we will review previous approaches and highlight similarities and differences to our work. Several steps have been made in this area, such as (Knight and Yamada, 1999), (Ravi and Knight, 2008), or (Snyder et al., 2010), to name just a few. The main difference of our work is, that it allows for much larger vocabulary sizes and more data to be used than previous work while at the same time not being dependent on seed lexica and/or any other knowledge of the languages. Close to the methods described in this work, Ravi and Knight (2011) treat training and translation without parallel data as a deciphering problem. Their best performing approach uses an EMAlgorithm to train a generative word based translation model. They perform experiments on a Spanish/English task with vocabulary sizes of about 500 words and a</context>
</contexts>
<marker>Snyder, Barzilay, Knight, 2010</marker>
<rawString>Benjamin Snyder, Regina Barzilay, and Kevin Knight. 2010. A statistical model for lost language decipherment. In 48th Annual Meeting of the Association for Computational Linguistics, number July, pages 1048– 1057.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>News from OPUS - A collection of multilingual parallel corpora with tools and interfaces.</title>
<date>2009</date>
<booktitle>Recent Advances in Natural Language Processing,</booktitle>
<volume>volume V,</volume>
<pages>237--248</pages>
<editor>In N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors,</editor>
<location>Amsterdam/Philadelphia, Borovets, Bulgaria.</location>
<contexts>
<context position="18443" citStr="Tiedemann, 2009" startWordPosition="3134" endWordPosition="3135">rget word. We set Nc to some value Nc &gt; NC. All experiments in this work were run with Nc = 300. Values of Nc ≈ NC seem to produce poorer results. Not limiting the number of candidates per target word at all also typically results in weaker performance. After the lexicon is filled with candidates, we initialize the probabilities to be uniform. With this new lexicon the process is iterated starting with the EM training. 6 Experimental Evaluation We evaluate our method on three different corpora. At first we apply our method to non-parallel Spanish/English data that is based on the OPUS corpus (Tiedemann, 2009) and that was also used in (Ravi and Knight, 2011). We show that our method performs better by 1.6 BLEU than the best performing method described in (Ravi and Knight, 2011) while 160 Name Lang. Sent. Words Voc. Spanish 13,181 39,185 562 OPUS English 19,770 61,835 411 German 27,861 282,831 5,964 VERBMOBIL English 27,862 294,902 3,723 French 100,000 1,725,993 68,259 GIGAWORD English 100,000 1,788,025 64,621 Table 1: Statistics of the corpora used in this paper. being approximately 15 to 20 times faster than their n-gram based approach. After that we apply our method to a non-parallel version of </context>
</contexts>
<marker>Tiedemann, 2009</marker>
<rawString>J¨org Tiedemann. 2009. News from OPUS - A collection of multilingual parallel corpora with tools and interfaces. In N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors, Recent Advances in Natural Language Processing, volume V, pages 237–248. John Benjamins, Amsterdam/Philadelphia, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<title>Verbmobil: Foundations of speech-to-speech translations.</title>
<date>2000</date>
<editor>Wolfgang Wahlster, editor.</editor>
<publisher>SpringerVerlag,</publisher>
<location>Berlin.</location>
<marker>2000</marker>
<rawString>Wolfgang Wahlster, editor. 2000. Verbmobil: Foundations of speech-to-speech translations. SpringerVerlag, Berlin.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>