<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000186">
<title confidence="0.988486">
Zero subject detection for Polish
</title>
<author confidence="0.994273">
Mateusz Kope´c
</author>
<affiliation confidence="0.99848">
Institute of Computer Science, Polish Academy of Sciences,
</affiliation>
<address confidence="0.91139">
Jana Kazimierza 5, 01-248 Warsaw, Poland
</address>
<email confidence="0.994835">
m.kopec@ipipan.waw.pl
</email>
<sectionHeader confidence="0.99379" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999893142857143">
This article reports on the first machine
learning experiments on detection of null
subjects in Polish. It emphasizes the role
of zero subject detection as the part of
mention detection – the initial step of end-
to-end coreference resolution. Anaphora
resolution is not studied in this article.
</bodyText>
<sectionHeader confidence="0.999263" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999677125">
Zero subject detection is an important issue for
anaphora and coreference resolution for the null-
subject languages, including all Balto-Slavic lan-
guages and most Romance languages. Their dis-
tinctive feature is the possibility for an indepen-
dent clause to lack an explicit subject. Person,
number, and/or gender agreement with the refer-
ent is indicated by the morphology of the verb:
</bodyText>
<listItem confidence="0.6990895">
(1) Maria wróciła ju˙z z Francji. ØSp˛edziła tam
miesi ˛ac.
</listItem>
<bodyText confidence="0.933476882352941">
“Maria came back from France. ØHadsingular:feminine spent
a month there.”
The recently created Polish Coreference Cor-
pus1 (PCC) (Ogrodniczuk et al., 2013) contains
zero subject annotation. A markable representing
the null subject is the verbal form following the
position where the argument would have been ex-
pected. As tested on the development part of the
corpus (described in detail later), omitting a per-
sonal pronoun is a frequent issue in the Polish lan-
guage – about 30% of verbs do not have explicit
subjects. Russo et al. (2012) reports similar fig-
ures for Italian (30.42%) and Spanish (41.17%).
Moreover, these null subjects are often part of
large coreference clusters – the average size of a
non-singleton coreference cluster in the develop-
ment subcorpus was 3.56 mentions. At the same
</bodyText>
<footnote confidence="0.763811">
1Publicly available at http://zil.ipipan.waw.
pl/PolishCoreferenceCorpus.
</footnote>
<bodyText confidence="0.9889182">
time, the non-singleton coreference cluster con-
taining at least one zero subject had on average
5.89 mentions.
A mention detection module heavily influences
the final coreference resolution score of an end-
to-end coreference resolution system. In Ogrod-
niczuk and Kope´c (2011a) the system working on
gold mentions achieved 82.90% F1 BLANC (Re-
casens and Hovy, 2011), whereas on system men-
tions the result dropped to 38.13% (the zero sub-
ject detection module was not implemented).
The aim of this paper is to find a method of au-
tomatic zero subject detection to improve the ac-
curacy of mention detection as the initial step of
coreference resolution.
</bodyText>
<sectionHeader confidence="0.999747" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999902833333333">
We present some of the most recent articles about
machine learning zero subject detection.
Rello et al. (2012b) describes a Brazilian Por-
tuguese corpus with 5665 finite verbs total, out
of which 77% have an explicit subject, 21% a
zero pronoun and 2% are impersonal construc-
tions. They extract various verb, clause and neigh-
boring token features for each verb occurrence and
classify it into one of these 3 classes, achieving
83.04% accuracy of a decision tree learning classi-
fier, better than the baseline result of the Palavras
parser. A very similar study is conducted also
for Spanish (Rello et al., 2012a), with the best
result of the lazy learning classifier K∗ (Cleary
and Trigg, 1995) of 87.6% accuracy, outperform-
ing the baseline of Connexor parser.
Chinese zero pronoun detection and resolution
is presented by Zhao and Ng (2007). Features for
zero pronoun identification consider mainly the
gold standard parse tree structure. Their training
corpus contained only 343 zero pronouns, as com-
pared to 10098 verbs with explicit subjects – for
Chinese, the phenomenon is much less frequent
than for Polish or Spanish. Therefore they weigh
</bodyText>
<page confidence="0.974383">
221
</page>
<note confidence="0.6869805">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 221–225,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999943954545455">
positive and negative examples to get the balance
between precision and recall – the best result of
50.9% F1 measure for positive to negative exam-
ple weight ratio of 8:1 is reported.
A study for the Romanian language (Mihaila et
al., 2011) describes a corpus consisting of 2741
sentences and 997 zero pronouns. Class imbalance
is solved by training machine learning algorithms
on all positive examples (zero pronouns) and the
same number of negative examples (sampled from
the corpus). Features used consider morphosyn-
tactic information about the verb, precedence of
the reflective pronoun “se” and the number of
verbs in the sentence. Their best ensemble clas-
sifier scored 74.5% accuracy.
Only a few studies (for example (Broda et al.,
2012; Ogrodniczuk and Kope´c, 2011b; Kope´c and
Ogrodniczuk, 2012)) consider the problem of rule-
based or machine learning coreference resolution
for the Polish language, however these attempts
leave zero subject detection as a non-trivial task
for further study.
</bodyText>
<sectionHeader confidence="0.975912" genericHeader="method">
3 Problem statement
</sectionHeader>
<bodyText confidence="0.999039">
Table 1 presents part of speech definitions as-
sumed in this article, based on the book about the
National Corpus of Polish (Przepiórkowski et al.,
2012). Coarse-grained POS indicates whether a
word with a given part of speech may be a subject
(Noun) or a verb (Verb) in a sentence. The last four
columns present which morphosyntactic informa-
tion is available for each part of speech. There are
few differences in this definition with respect to
the original approach in the book:
</bodyText>
<listItem confidence="0.999058111111111">
• We treat numerals, gerunds and pronouns as
Nouns – because they are frequently sub-
jects of the sentence and have the same
morphosyntactic information as “standard”
nouns.
• We do not consider siebie (“self”, tradition-
ally treated as pronoun) as a Noun, as it can-
not be a subject.
• Tags: impt, imps, inf, pcon, pant, pact, ppas,
</listItem>
<bodyText confidence="0.5472745">
pred, which are traditionally considered verb
tags, are not treated by us as Verbs, because
they cannot have a subject.
With such a definition of parts of speech, our
task may be stated as follows: given a clause with
a Verb, decide whether the clause contains a Noun
</bodyText>
<table confidence="0.9989138125">
Coarse- POS Tag Number Case Gender Person
-grained
POS
Noun subst + + +
Depreciative form depr + + +
Main numeral num + + +
Noun Collective numeral num+ + +
col
Gerund ger + + +
Personal pronoun – 1st, 2nd person ppron12 + + + +
Personal pronoun – 3rd person ppron3 + + + +
Non-past form fin + +
Future by´c bedzie + +
Verb Agglutinate by´c aglt + +
L-participle praet + +
winien-like verb winien + +
</table>
<tableCaption confidence="0.999764">
Table 1: Parts of speech
</tableCaption>
<bodyText confidence="0.999960875">
which is the Verb’s explicit subject. From now on
in this paper, the words “noun” and “verb” have
the meaning of Noun and Verb, respectively. In
this study, we do not try to handle the cases of
subjects not being nouns, as judging from our ob-
servations, it is very infrequent. We do take into
account in our solution the cases of the subject not
in the nominative case, as in the example:
</bodyText>
<listItem confidence="0.7849345">
(2) Pieni˛edzynoun:genitive nie starczy dla wszys-
tkich.
</listItem>
<bodyText confidence="0.988050913043478">
“There wouldn’t be enough money for everyone.”
It is worth noting that Polish is a free-word-
order language, therefore there are many possible
places for the subject to appear, with respect to the
position of the verb.
As the corpus has only automatic morphosyn-
tactic information available (provided by the PAN-
TERA tagger (Aceda´nski, 2010)), not corrected by
the coreference annotators, the only verbs consid-
ered in this study are the ones found by the tag-
ger. If such a verb was marked as a mention by
the coreference annotator (verb mention in table
2), it is a positive example for our machine learn-
ing study, otherwise a negative one. Sentence and
clause segmentation in the corpus was also auto-
matic. We are aware that the corpus used for the
study was not perfectly suited for the task – verbs
with a zero subject are not marked there explicitly,
but can only be found based on automatic tagging.
However the tagging error of detecting verbs is re-
ported as not higher than 0.04% (for the fin tag,
see (Aceda´nski, 2010) for details), so we consider
the resource sufficiently correct.
</bodyText>
<sectionHeader confidence="0.985275" genericHeader="method">
4 Development and evaluation data
</sectionHeader>
<bodyText confidence="0.96608925">
Each text of the Polish Coreference Corpus is a
250-350 word sample, consisting of full, subse-
quent paragraphs extracted from a larger text. Text
genres balance correspond to the National Corpus
</bodyText>
<page confidence="0.988277">
222
</page>
<table confidence="0.93103075">
Corpus # texts # sentences # tokens # verbs # mentions # verb mentions
Development 390 6481 110379 10801 37250 3104
Evaluation 389 6737 110474 11000 37167 3106
Total 779 13218 220853 21801 74417 6210
</table>
<tableCaption confidence="0.999183">
Table 2: Zero subject study data statistics
</tableCaption>
<bodyText confidence="0.999834142857143">
of Polish (Przepiórkowski et al., 2012). At the
time this study started, 779 out of 1773 texts (ran-
domly chosen) of the Polish Coreference Corpus
were already manually annotated. Annotated texts
were randomly split into two equal-sized subcor-
pora for development and evaluation. Their de-
tailed statistics are presented in Table 2.
</bodyText>
<subsectionHeader confidence="0.819533">
4.1 Inter-annotator agreement
</subsectionHeader>
<bodyText confidence="0.9999764">
210 texts of the Polish Coreference Corpus were
annotated independently by two annotators. This
part was analyzed for the inter-annotator agree-
ment of deciding if a verb has a zero subject or
not. In the data there were 5879 verbs total,
for which observed agreement yielded 92.57%.
Agreement expected by chance (assuming a per
annotator chance annotation probability distribu-
tion) equalled 57.52%, therefore chance-corrected
Cohen’s n for the task equalled 82.51%.
</bodyText>
<subsectionHeader confidence="0.960969">
4.2 Results of full dependency parsing
</subsectionHeader>
<bodyText confidence="0.971249066666667">
The first Polish dependency parser was recently
developed and described by Wróblewska (2012).
The author reports 71% LAS2 and 75.2% UAS3
performance of this parser. This parser was used
to detect null subjects – every verb lacking the
dependency relation of the subject type (subj)
was marked as missing the subject. This base-
line method achieved accuracy of 67.23%, preci-
sion of 46.53%, recall of 90.47% and F1 equal to
61.45%. These results are worse than a simple ma-
jority baseline classifier, therefore current state-of-
the-art Polish dependency parsing is not a satisfac-
tory solution to the task stated in this article.
represents the window around the verb, with fol-
lowing values: the clause containing the verb, the
sentence containing the verb, windows of 1 to 5
tokens before the verb, windows of 1 to 5 tokens
after the verb, windows of 1 to 5 tokens both be-
fore and after the verb. Variable c1 represents
compatibility of noun and verb, with values be-
ing any nonempty subset of the set of following
conditions: case of the noun equal to nominative
(NOM), number agreement with the verb (NUM),
person or gender agreement (POG), depending on
which was available to check, see Table 1. Vari-
able c2 is similar to c1, with the following values:
{NOM}, {POG}, {NOM, POG}.
Feature Type
Verb features
number of the verb – to help with cases of plural verbs having two nominal
or more singular nouns as subject
tag of the verb – as it may happen, that some parts of speech behave boolean
differently
is the verb on the pseudo-verbs list extracted from (´Swidzi´nski, boolean
1994) – i.e. may not require a subject
Neighboring token features
tag of the next token nominal
tag of the previous token nominal
is the previous tag equal to praet – a redundant feature to the pre- boolean
vious one, but it should help with the cases like:
... byłapraetmaglt:pri ... &amp;quot;... (I) was ... &amp;quot;
when we split a word into a L-participle and agglutinate. Annota-
tion guidelines were to only mark the agglutinate as a mention,
when the verb does not have an explicit subject
does one of the previous two tokens have the pred tag – should al- boolean
low detecting examples similar to:
Mo˙znapred si˛e byłopraet tego spodziewa´c.
&amp;quot;... It could have been expected.... &amp;quot;
Trzebapred byłopraet my´sle´c wcze´sniej.
&amp;quot;(One) should have thought before.&amp;quot;
when było (&amp;quot;have&amp;quot;) cannot have subject, as it is part of an imper-
sonal construction
is the next tag inf – similar role to the previous feature, as in: boolean
Wtedy nale˙zyfin poprosi´cinf. &amp;quot;(One) should then askfor it.&amp;quot;
when nale˙zy (&amp;quot;one should&amp;quot;) cannot have a subject
is the previous token a comma boolean
Length features
number of tokens in the sentence (following the hypothesis, that the numerical
shorter the sentence/clause, the less likely for the subject to appear)
number of tokens in the clause with the verb numerical
</bodyText>
<subsectionHeader confidence="0.96566">
Subject candidate existence features
</subsectionHeader>
<bodyText confidence="0.963851">
existence of a noun not preceded by jak/jako (&amp;quot;as&amp;quot;) in window w boolean
fulfilling conditions from set c1
existence of at least two nouns not preceded by jak/jako (&amp;quot;as&amp;quot;) in boolean
window w both fulfilling conditions from set c2
</bodyText>
<tableCaption confidence="0.987036">
Table 3: Features
</tableCaption>
<sectionHeader confidence="0.998539" genericHeader="method">
5 Features
</sectionHeader>
<bodyText confidence="0.999943571428571">
Based on a number of experiments on the develop-
ment corpus, we chose a number of features pre-
sented in table 3.
Subject candidate existence features from the
bottom of the table 3 use variables: c1, c2 and w.
Separate feature was generated for each combi-
nation of these three variables. The variable w
</bodyText>
<footnote confidence="0.994307">
2Labeled attachment score – the percentage of tokens that
are assigned a correct head and a correct dependency type.
3Unlabeled attachment score – the percentage of tokens
that are assigned a correct head.
</footnote>
<sectionHeader confidence="0.99801" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.9999094">
Presented features were used to train a machine
learning algorithm. We chose the JRip imple-
mentation of RIPPER (Cohen, 1995) from WEKA
(Hall et al., 2009) for the possibility to interpret the
rules, which is outside of the scope of this paper.
</bodyText>
<subsectionHeader confidence="0.986044">
6.1 Accuracy on the development corpus
</subsectionHeader>
<bodyText confidence="0.999331">
A baseline model which always predicts that a
verb has an explicit subject achieves 71.13% ac-
</bodyText>
<page confidence="0.997399">
223
</page>
<table confidence="0.7677708">
True values
null subject explicit subject
Accuracy on evaluation corpus
null subject 2093 815
explicit subject 1013 7079
</table>
<tableCaption confidence="0.998491">
Table 4: Confusion matrix
</tableCaption>
<bodyText confidence="0.999943153846154">
curacy on the development data. The upper bound
of the ITA (as stated earlier) is around 92.57% ac-
curacy.
We used 10-fold cross-validation which was re-
peated 10 times with different random seeds for
training and train/test splits. The average from the
total of 100 trials (each cross-validation split sep-
arately) was equal to 82.74%, with standard devi-
ation of 1.27%. As the Shapiro-Wilk (1965) test
for normality for this data gives p-value of 0.38, it
may be assumed that it follows the normal distri-
bution. In that case, the 95% confidence interval
for the accuracy is equal to [82.49%, 82.99%].
</bodyText>
<subsectionHeader confidence="0.999461">
6.2 Accuracy on the evaluation corpus
</subsectionHeader>
<bodyText confidence="0.9999918">
The evaluation corpus was used only for two ex-
periments presented below: to calculate accuracy
and learning curve of the developed solution.
We used the model learnt on the development
corpus and tested it on the evaluation corpus,
achieving 83.38% accuracy. A majority classifier
would achieve 71.76% accuracy on this corpus.
The confusion matrix is depicted in Table 4. For
finding the null subjects, recall of 67.39% and pre-
cision of 71.97% gives F1 measure of 69.60%.
</bodyText>
<subsectionHeader confidence="0.999892">
6.3 Learning curve
</subsectionHeader>
<bodyText confidence="0.999922157894737">
To test how the number of training examples in-
fluences the quality of the trained classifier, we
used subsets of the development corpus of various
sizes as training sets. The test set was the same
in all cases (the evaluation corpus). Proportions
of the examples used ranged from 5% to 100%
of the development corpus, each proportion was
tested 10 times to provide an estimation of vari-
ance. For example, to evaluate the efficiency of
the classifier trained on 5% of the training exam-
ples, we randomly sampled 5% of the examples,
trained the classifier and tested it on the full evalu-
ation corpus. Then we repeated it another 9 times,
randomly choosing a different 5% portion of the
examples for training.
Again the Shapiro-Wilk test was taken to assess
the normality of results for each proportion, out
of 19 proportions tested (the proportion of 1 was
of course not tested for normality), only 3 had p-
</bodyText>
<subsectionHeader confidence="0.257911">
Proportion of training set used
</subsectionHeader>
<figureCaption confidence="0.999618">
Figure 1: Learning curve
</figureCaption>
<bodyText confidence="0.999885363636364">
value less than 0.1, therefore we assumed that the
data is distributed approximately normally. The
95% confidence intervals of the classifiers trained
on a given proportion of the development corpus
are shown in the Figure 1. The algorithm clearly
benefits from having more training examples. We
observe that the curve is generally of the desired
shape, yet it flattens when approaching the full
training set used. It may suggest that the devel-
oped solution would not be able to significantly
exceed 84%, even given more training examples.
</bodyText>
<sectionHeader confidence="0.955821" genericHeader="conclusions">
7 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999987176470588">
This article presented an efficient zero subject de-
tection module for Polish. We highlighted some
difficult examples to take into account and pro-
posed a solution for the Polish language.
The achieved accuracy of 83.38% significantly
exceeds the baseline of majority tagging, equal to
71.76%, but there is still room for improvement,
as the upper bound of 92.57% was computed. The
achieved result for the task of null subject detec-
tion looks promising for the application in mention
detection for coreference resolution.
The invented solution needs to be incorporated
in a complete coreference resolver for Polish and
evaluated for the extent to which using such an ad-
vanced separate classifier for zero subject detec-
tion improves the mention detection and, further-
more, end-to-end coreference resolution accuracy.
</bodyText>
<sectionHeader confidence="0.996514" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999301125">
The work reported here was cofounded by the
Computer-based methods for coreference resolu-
tion in Polish texts project financed by the Pol-
ish National Science Centre (contract number
6505/B/T02/2011/40) and by the European Union
from resources of the European Social Fund.
Project PO KL „Information technologies: Re-
search and their interdisciplinary applications”.
</bodyText>
<figure confidence="0.934023333333333">
0.2 0.4 0.6 0.8 1.0
0.79 0.82
Predictions
</figure>
<page confidence="0.99614">
224
</page>
<sectionHeader confidence="0.989983" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999922685393259">
Szymon Aceda´nski. 2010. A Morphosyntactic Brill
Tagger for Inflectional Languages. In Hrafn Lofts-
son, Eiríkur Rögnvaldsson, and Sigrún Helgadóttir,
editors, Advances in Natural Language Processing,
volume 6233 of Lecture Notes in Computer Science,
pages 3–14. Springer.
Bartosz Broda, Łukasz Burdka, and Marek Maziarz.
2012. IKAR: An Improved Kit for Anaphora Res-
olution for Polish. In COLING (Demos), pages 25–
32.
John G. Cleary and Leonard E. Trigg. 1995. K*:
An instance-based learner using an entropic distance
measure. In In Proceedings of the 12th International
Conference on Machine Learning, pages 108–114.
Morgan Kaufmann.
William W. Cohen. 1995. Fast effective rule induc-
tion. In In Proceedings of the Twelfth International
Conference on Machine Learning, pages 115–123.
Morgan Kaufmann.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10–18, November.
Mateusz Kope´c and Maciej Ogrodniczuk. 2012. Cre-
ating a Coreference Resolution System for Pol-
ish. In Proceedings of the Eighth International
Conference on Language Resources and Evalua-
tion, LREC 2012, pages 192–195, Istanbul, Turkey.
ELRA.
Claudiu Mihaila, Iustina Ilisei, and Diana Inkpen.
2011. Zero Pronominal Anaphora Resolution for the
Romanian Language. Research Journal on Com-
puter Science and Computer Engineering with Ap-
plications” POLIBITS, 42.
Maciej Ogrodniczuk and Mateusz Kope´c. 2011a. End-
to-end coreference resolution baseline system for
Polish. In Zygmunt Vetulani, editor, Proceedings of
the 5th Language &amp; Technology Conference: Hu-
man Language Technologies as a Challenge for
Computer Science and Linguistics, pages 167–171,
Pozna´n, Poland.
Maciej Ogrodniczuk and Mateusz Kope´c. 2011b.
Rule-based coreference resolution module for Pol-
ish. In Proceedings of the 8th Discourse Anaphora
and Anaphor Resolution Colloquium (DAARC
2011), pages 191–200, Faro, Portugal.
Maciej Ogrodniczuk, Katarzyna Głowi´nska, Mateusz
Kope´c, Agata Savary, and Magdalena Zawisławska.
2013. Polish coreference corpus. pages 494–498.
Adam Przepiórkowski, Mirosław Ba´nko, Rafał L.
Górski, and Barbara Lewandowska-Tomaszczyk,
editors. 2012. Narodowy Korpus J˛ezyka Polskiego
[Eng.: National Corpus of Polish]. Wydawnictwo
Naukowe PWN, Warsaw.
Marta Recasens and E. Hovy. 2011. BLANC: Imple-
menting the Rand index for coreference evaluation.
pages 485–510.
Luz Rello, Ricardo Baeza-Yates, and Ruslan Mitkov.
2012a. Elliphant: Improved Automatic Detection
of Zero Subjects and Impersonal Constructions in
Spanish. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 706–715, Avignon,
France, April. Association for Computational Lin-
guistics.
Luz Rello, Gabriela Ferraro, and Iria Gayo. 2012b.
A First Approach to the Automatic Detection of
Zero Subjects and Impersonal Constructions in Por-
tuguese. Procesamiento del Lenguaje Natural,
49:163–170.
Lorenza Russo, Sharid Loáiciga, and Asheesh Gulati.
2012. Improving machine translation of null sub-
jects in Italian and Spanish. In Proceedings of
the Student Research Workshop at the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 81–89, Avignon,
France, April. Association for Computational Lin-
guistics.
S. S. Shapiro and M. B. Wilk. 1965. An analysis
of variance test for normality (complete samples).
Biometrika, 52(3/4):591–611, Dec.
Marek ´Swidzi´nski. 1994. Syntactic dictionary of pol-
ish verbs.
Alina Wróblewska. 2012. Polish dependency bank.
Linguistic Issues in Language Technology, 7(1).
Shanheng Zhao and Hwee Tou Ng. 2007. Identifica-
tion and Resolution of Chinese Zero Pronouns: A
Machine Learning Approach. In EMNLP-CoNLL,
pages 541–550. ACL.
</reference>
<page confidence="0.998821">
225
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.806609">
<title confidence="0.883288666666667">Zero subject detection for Polish Mateusz Institute of Computer Science, Polish Academy of</title>
<author confidence="0.910296">Jana Kazimierza</author>
<email confidence="0.997499">m.kopec@ipipan.waw.pl</email>
<abstract confidence="0.998577375">This article reports on the first machine learning experiments on detection of null subjects in Polish. It emphasizes the role of zero subject detection as the part of mention detection – the initial step of endto-end coreference resolution. Anaphora resolution is not studied in this article.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Szymon Aceda´nski</author>
</authors>
<title>A Morphosyntactic Brill Tagger for Inflectional Languages.</title>
<date>2010</date>
<booktitle>In Hrafn Loftsson, Eiríkur Rögnvaldsson, and Sigrún Helgadóttir, editors, Advances in Natural Language Processing,</booktitle>
<volume>6233</volume>
<pages>3--14</pages>
<publisher>Springer.</publisher>
<marker>Aceda´nski, 2010</marker>
<rawString>Szymon Aceda´nski. 2010. A Morphosyntactic Brill Tagger for Inflectional Languages. In Hrafn Loftsson, Eiríkur Rögnvaldsson, and Sigrún Helgadóttir, editors, Advances in Natural Language Processing, volume 6233 of Lecture Notes in Computer Science, pages 3–14. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bartosz Broda</author>
<author>Łukasz Burdka</author>
<author>Marek Maziarz</author>
</authors>
<title>IKAR: An Improved Kit for Anaphora Resolution for Polish.</title>
<date>2012</date>
<booktitle>In COLING (Demos),</booktitle>
<pages>25--32</pages>
<contexts>
<context position="4567" citStr="Broda et al., 2012" startWordPosition="715" endWordPosition="718">sitive to negative example weight ratio of 8:1 is reported. A study for the Romanian language (Mihaila et al., 2011) describes a corpus consisting of 2741 sentences and 997 zero pronouns. Class imbalance is solved by training machine learning algorithms on all positive examples (zero pronouns) and the same number of negative examples (sampled from the corpus). Features used consider morphosyntactic information about the verb, precedence of the reflective pronoun “se” and the number of verbs in the sentence. Their best ensemble classifier scored 74.5% accuracy. Only a few studies (for example (Broda et al., 2012; Ogrodniczuk and Kope´c, 2011b; Kope´c and Ogrodniczuk, 2012)) consider the problem of rulebased or machine learning coreference resolution for the Polish language, however these attempts leave zero subject detection as a non-trivial task for further study. 3 Problem statement Table 1 presents part of speech definitions assumed in this article, based on the book about the National Corpus of Polish (Przepiórkowski et al., 2012). Coarse-grained POS indicates whether a word with a given part of speech may be a subject (Noun) or a verb (Verb) in a sentence. The last four columns present which mor</context>
</contexts>
<marker>Broda, Burdka, Maziarz, 2012</marker>
<rawString>Bartosz Broda, Łukasz Burdka, and Marek Maziarz. 2012. IKAR: An Improved Kit for Anaphora Resolution for Polish. In COLING (Demos), pages 25– 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John G Cleary</author>
<author>Leonard E Trigg</author>
</authors>
<title>K*: An instance-based learner using an entropic distance measure. In</title>
<date>1995</date>
<booktitle>In Proceedings of the 12th International Conference on Machine Learning,</booktitle>
<pages>108--114</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="3158" citStr="Cleary and Trigg, 1995" startWordPosition="497" endWordPosition="500">ro subject detection. Rello et al. (2012b) describes a Brazilian Portuguese corpus with 5665 finite verbs total, out of which 77% have an explicit subject, 21% a zero pronoun and 2% are impersonal constructions. They extract various verb, clause and neighboring token features for each verb occurrence and classify it into one of these 3 classes, achieving 83.04% accuracy of a decision tree learning classifier, better than the baseline result of the Palavras parser. A very similar study is conducted also for Spanish (Rello et al., 2012a), with the best result of the lazy learning classifier K∗ (Cleary and Trigg, 1995) of 87.6% accuracy, outperforming the baseline of Connexor parser. Chinese zero pronoun detection and resolution is presented by Zhao and Ng (2007). Features for zero pronoun identification consider mainly the gold standard parse tree structure. Their training corpus contained only 343 zero pronouns, as compared to 10098 verbs with explicit subjects – for Chinese, the phenomenon is much less frequent than for Polish or Spanish. Therefore they weigh 221 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 221–225, Gothenburg, Sweden,</context>
</contexts>
<marker>Cleary, Trigg, 1995</marker>
<rawString>John G. Cleary and Leonard E. Trigg. 1995. K*: An instance-based learner using an entropic distance measure. In In Proceedings of the 12th International Conference on Machine Learning, pages 108–114. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
</authors>
<title>Fast effective rule induction. In</title>
<date>1995</date>
<booktitle>In Proceedings of the Twelfth International Conference on Machine Learning,</booktitle>
<pages>115--123</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="13027" citStr="Cohen, 1995" startWordPosition="2159" endWordPosition="2160">on the development corpus, we chose a number of features presented in table 3. Subject candidate existence features from the bottom of the table 3 use variables: c1, c2 and w. Separate feature was generated for each combination of these three variables. The variable w 2Labeled attachment score – the percentage of tokens that are assigned a correct head and a correct dependency type. 3Unlabeled attachment score – the percentage of tokens that are assigned a correct head. 6 Evaluation Presented features were used to train a machine learning algorithm. We chose the JRip implementation of RIPPER (Cohen, 1995) from WEKA (Hall et al., 2009) for the possibility to interpret the rules, which is outside of the scope of this paper. 6.1 Accuracy on the development corpus A baseline model which always predicts that a verb has an explicit subject achieves 71.13% ac223 True values null subject explicit subject Accuracy on evaluation corpus null subject 2093 815 explicit subject 1013 7079 Table 4: Confusion matrix curacy on the development data. The upper bound of the ITA (as stated earlier) is around 92.57% accuracy. We used 10-fold cross-validation which was repeated 10 times with different random seeds fo</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>William W. Cohen. 1995. Fast effective rule induction. In In Proceedings of the Twelfth International Conference on Machine Learning, pages 115–123. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="13057" citStr="Hall et al., 2009" startWordPosition="2163" endWordPosition="2166">s, we chose a number of features presented in table 3. Subject candidate existence features from the bottom of the table 3 use variables: c1, c2 and w. Separate feature was generated for each combination of these three variables. The variable w 2Labeled attachment score – the percentage of tokens that are assigned a correct head and a correct dependency type. 3Unlabeled attachment score – the percentage of tokens that are assigned a correct head. 6 Evaluation Presented features were used to train a machine learning algorithm. We chose the JRip implementation of RIPPER (Cohen, 1995) from WEKA (Hall et al., 2009) for the possibility to interpret the rules, which is outside of the scope of this paper. 6.1 Accuracy on the development corpus A baseline model which always predicts that a verb has an explicit subject achieves 71.13% ac223 True values null subject explicit subject Accuracy on evaluation corpus null subject 2093 815 explicit subject 1013 7079 Table 4: Confusion matrix curacy on the development data. The upper bound of the ITA (as stated earlier) is around 92.57% accuracy. We used 10-fold cross-validation which was repeated 10 times with different random seeds for training and train/test spli</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: an update. SIGKDD Explor. Newsl., 11(1):10–18, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mateusz Kope´c</author>
<author>Maciej Ogrodniczuk</author>
</authors>
<title>Creating a Coreference Resolution System for Polish.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eighth International Conference on Language Resources and Evaluation, LREC 2012,</booktitle>
<pages>192--195</pages>
<location>Istanbul, Turkey. ELRA.</location>
<marker>Kope´c, Ogrodniczuk, 2012</marker>
<rawString>Mateusz Kope´c and Maciej Ogrodniczuk. 2012. Creating a Coreference Resolution System for Polish. In Proceedings of the Eighth International Conference on Language Resources and Evaluation, LREC 2012, pages 192–195, Istanbul, Turkey. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudiu Mihaila</author>
<author>Iustina Ilisei</author>
<author>Diana Inkpen</author>
</authors>
<title>Zero Pronominal Anaphora Resolution for the Romanian Language.</title>
<date>2011</date>
<journal>Research Journal on Computer Science and Computer Engineering with Applications” POLIBITS,</journal>
<volume>42</volume>
<contexts>
<context position="4065" citStr="Mihaila et al., 2011" startWordPosition="637" endWordPosition="640"> zero pronouns, as compared to 10098 verbs with explicit subjects – for Chinese, the phenomenon is much less frequent than for Polish or Spanish. Therefore they weigh 221 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 221–225, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics positive and negative examples to get the balance between precision and recall – the best result of 50.9% F1 measure for positive to negative example weight ratio of 8:1 is reported. A study for the Romanian language (Mihaila et al., 2011) describes a corpus consisting of 2741 sentences and 997 zero pronouns. Class imbalance is solved by training machine learning algorithms on all positive examples (zero pronouns) and the same number of negative examples (sampled from the corpus). Features used consider morphosyntactic information about the verb, precedence of the reflective pronoun “se” and the number of verbs in the sentence. Their best ensemble classifier scored 74.5% accuracy. Only a few studies (for example (Broda et al., 2012; Ogrodniczuk and Kope´c, 2011b; Kope´c and Ogrodniczuk, 2012)) consider the problem of rulebased </context>
</contexts>
<marker>Mihaila, Ilisei, Inkpen, 2011</marker>
<rawString>Claudiu Mihaila, Iustina Ilisei, and Diana Inkpen. 2011. Zero Pronominal Anaphora Resolution for the Romanian Language. Research Journal on Computer Science and Computer Engineering with Applications” POLIBITS, 42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maciej Ogrodniczuk</author>
<author>Mateusz Kope´c</author>
</authors>
<title>Endto-end coreference resolution baseline system for Polish.</title>
<date>2011</date>
<booktitle>Proceedings of the 5th Language &amp; Technology Conference: Human Language Technologies as a Challenge for Computer Science and Linguistics,</booktitle>
<pages>167--171</pages>
<editor>In Zygmunt Vetulani, editor,</editor>
<location>Pozna´n, Poland.</location>
<marker>Ogrodniczuk, Kope´c, 2011</marker>
<rawString>Maciej Ogrodniczuk and Mateusz Kope´c. 2011a. Endto-end coreference resolution baseline system for Polish. In Zygmunt Vetulani, editor, Proceedings of the 5th Language &amp; Technology Conference: Human Language Technologies as a Challenge for Computer Science and Linguistics, pages 167–171, Pozna´n, Poland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maciej Ogrodniczuk</author>
<author>Mateusz Kope´c</author>
</authors>
<title>Rule-based coreference resolution module for Polish.</title>
<date>2011</date>
<booktitle>In Proceedings of the 8th Discourse Anaphora and Anaphor Resolution Colloquium (DAARC 2011),</booktitle>
<pages>191--200</pages>
<location>Faro, Portugal.</location>
<marker>Ogrodniczuk, Kope´c, 2011</marker>
<rawString>Maciej Ogrodniczuk and Mateusz Kope´c. 2011b. Rule-based coreference resolution module for Polish. In Proceedings of the 8th Discourse Anaphora and Anaphor Resolution Colloquium (DAARC 2011), pages 191–200, Faro, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maciej Ogrodniczuk</author>
<author>Katarzyna Głowi´nska</author>
<author>Mateusz Kope´c</author>
<author>Agata Savary</author>
<author>Magdalena Zawisławska</author>
</authors>
<title>Polish coreference corpus.</title>
<date>2013</date>
<pages>494--498</pages>
<marker>Ogrodniczuk, Głowi´nska, Kope´c, Savary, Zawisławska, 2013</marker>
<rawString>Maciej Ogrodniczuk, Katarzyna Głowi´nska, Mateusz Kope´c, Agata Savary, and Magdalena Zawisławska. 2013. Polish coreference corpus. pages 494–498.</rawString>
</citation>
<citation valid="true">
<date>2012</date>
<booktitle>Narodowy Korpus J˛ezyka Polskiego [Eng.: National Corpus of Polish]. Wydawnictwo Naukowe PWN,</booktitle>
<editor>Adam Przepiórkowski, Mirosław Ba´nko, Rafał L. Górski, and Barbara Lewandowska-Tomaszczyk, editors.</editor>
<location>Warsaw.</location>
<contexts>
<context position="1468" citStr="(2012)" startWordPosition="229" endWordPosition="229">the verb: (1) Maria wróciła ju˙z z Francji. ØSp˛edziła tam miesi ˛ac. “Maria came back from France. ØHadsingular:feminine spent a month there.” The recently created Polish Coreference Corpus1 (PCC) (Ogrodniczuk et al., 2013) contains zero subject annotation. A markable representing the null subject is the verbal form following the position where the argument would have been expected. As tested on the development part of the corpus (described in detail later), omitting a personal pronoun is a frequent issue in the Polish language – about 30% of verbs do not have explicit subjects. Russo et al. (2012) reports similar figures for Italian (30.42%) and Spanish (41.17%). Moreover, these null subjects are often part of large coreference clusters – the average size of a non-singleton coreference cluster in the development subcorpus was 3.56 mentions. At the same 1Publicly available at http://zil.ipipan.waw. pl/PolishCoreferenceCorpus. time, the non-singleton coreference cluster containing at least one zero subject had on average 5.89 mentions. A mention detection module heavily influences the final coreference resolution score of an endto-end coreference resolution system. In Ogrodniczuk and Kop</context>
<context position="9321" citStr="(2012)" startWordPosition="1529" endWordPosition="1529">greement 210 texts of the Polish Coreference Corpus were annotated independently by two annotators. This part was analyzed for the inter-annotator agreement of deciding if a verb has a zero subject or not. In the data there were 5879 verbs total, for which observed agreement yielded 92.57%. Agreement expected by chance (assuming a per annotator chance annotation probability distribution) equalled 57.52%, therefore chance-corrected Cohen’s n for the task equalled 82.51%. 4.2 Results of full dependency parsing The first Polish dependency parser was recently developed and described by Wróblewska (2012). The author reports 71% LAS2 and 75.2% UAS3 performance of this parser. This parser was used to detect null subjects – every verb lacking the dependency relation of the subject type (subj) was marked as missing the subject. This baseline method achieved accuracy of 67.23%, precision of 46.53%, recall of 90.47% and F1 equal to 61.45%. These results are worse than a simple majority baseline classifier, therefore current state-ofthe-art Polish dependency parsing is not a satisfactory solution to the task stated in this article. represents the window around the verb, with following values: the cl</context>
</contexts>
<marker>2012</marker>
<rawString>Adam Przepiórkowski, Mirosław Ba´nko, Rafał L. Górski, and Barbara Lewandowska-Tomaszczyk, editors. 2012. Narodowy Korpus J˛ezyka Polskiego [Eng.: National Corpus of Polish]. Wydawnictwo Naukowe PWN, Warsaw.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>E Hovy</author>
</authors>
<title>BLANC: Implementing the Rand index for coreference evaluation.</title>
<date>2011</date>
<pages>485--510</pages>
<contexts>
<context position="2166" citStr="Recasens and Hovy, 2011" startWordPosition="327" endWordPosition="331">er, these null subjects are often part of large coreference clusters – the average size of a non-singleton coreference cluster in the development subcorpus was 3.56 mentions. At the same 1Publicly available at http://zil.ipipan.waw. pl/PolishCoreferenceCorpus. time, the non-singleton coreference cluster containing at least one zero subject had on average 5.89 mentions. A mention detection module heavily influences the final coreference resolution score of an endto-end coreference resolution system. In Ogrodniczuk and Kope´c (2011a) the system working on gold mentions achieved 82.90% F1 BLANC (Recasens and Hovy, 2011), whereas on system mentions the result dropped to 38.13% (the zero subject detection module was not implemented). The aim of this paper is to find a method of automatic zero subject detection to improve the accuracy of mention detection as the initial step of coreference resolution. 2 Related Work We present some of the most recent articles about machine learning zero subject detection. Rello et al. (2012b) describes a Brazilian Portuguese corpus with 5665 finite verbs total, out of which 77% have an explicit subject, 21% a zero pronoun and 2% are impersonal constructions. They extract variou</context>
</contexts>
<marker>Recasens, Hovy, 2011</marker>
<rawString>Marta Recasens and E. Hovy. 2011. BLANC: Implementing the Rand index for coreference evaluation. pages 485–510.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luz Rello</author>
<author>Ricardo Baeza-Yates</author>
<author>Ruslan Mitkov</author>
</authors>
<title>Elliphant: Improved Automatic Detection of Zero Subjects and Impersonal Constructions in Spanish.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>706--715</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Avignon, France,</location>
<contexts>
<context position="2575" citStr="Rello et al. (2012" startWordPosition="399" endWordPosition="402">es the final coreference resolution score of an endto-end coreference resolution system. In Ogrodniczuk and Kope´c (2011a) the system working on gold mentions achieved 82.90% F1 BLANC (Recasens and Hovy, 2011), whereas on system mentions the result dropped to 38.13% (the zero subject detection module was not implemented). The aim of this paper is to find a method of automatic zero subject detection to improve the accuracy of mention detection as the initial step of coreference resolution. 2 Related Work We present some of the most recent articles about machine learning zero subject detection. Rello et al. (2012b) describes a Brazilian Portuguese corpus with 5665 finite verbs total, out of which 77% have an explicit subject, 21% a zero pronoun and 2% are impersonal constructions. They extract various verb, clause and neighboring token features for each verb occurrence and classify it into one of these 3 classes, achieving 83.04% accuracy of a decision tree learning classifier, better than the baseline result of the Palavras parser. A very similar study is conducted also for Spanish (Rello et al., 2012a), with the best result of the lazy learning classifier K∗ (Cleary and Trigg, 1995) of 87.6% accurac</context>
</contexts>
<marker>Rello, Baeza-Yates, Mitkov, 2012</marker>
<rawString>Luz Rello, Ricardo Baeza-Yates, and Ruslan Mitkov. 2012a. Elliphant: Improved Automatic Detection of Zero Subjects and Impersonal Constructions in Spanish. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 706–715, Avignon, France, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luz Rello</author>
<author>Gabriela Ferraro</author>
<author>Iria Gayo</author>
</authors>
<title>A First Approach to the Automatic Detection</title>
<date>2012</date>
<booktitle>of Zero Subjects and Impersonal Constructions in Portuguese. Procesamiento del Lenguaje Natural,</booktitle>
<pages>49--163</pages>
<contexts>
<context position="2575" citStr="Rello et al. (2012" startWordPosition="399" endWordPosition="402">es the final coreference resolution score of an endto-end coreference resolution system. In Ogrodniczuk and Kope´c (2011a) the system working on gold mentions achieved 82.90% F1 BLANC (Recasens and Hovy, 2011), whereas on system mentions the result dropped to 38.13% (the zero subject detection module was not implemented). The aim of this paper is to find a method of automatic zero subject detection to improve the accuracy of mention detection as the initial step of coreference resolution. 2 Related Work We present some of the most recent articles about machine learning zero subject detection. Rello et al. (2012b) describes a Brazilian Portuguese corpus with 5665 finite verbs total, out of which 77% have an explicit subject, 21% a zero pronoun and 2% are impersonal constructions. They extract various verb, clause and neighboring token features for each verb occurrence and classify it into one of these 3 classes, achieving 83.04% accuracy of a decision tree learning classifier, better than the baseline result of the Palavras parser. A very similar study is conducted also for Spanish (Rello et al., 2012a), with the best result of the lazy learning classifier K∗ (Cleary and Trigg, 1995) of 87.6% accurac</context>
</contexts>
<marker>Rello, Ferraro, Gayo, 2012</marker>
<rawString>Luz Rello, Gabriela Ferraro, and Iria Gayo. 2012b. A First Approach to the Automatic Detection of Zero Subjects and Impersonal Constructions in Portuguese. Procesamiento del Lenguaje Natural, 49:163–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lorenza Russo</author>
<author>Sharid Loáiciga</author>
<author>Asheesh Gulati</author>
</authors>
<title>Improving machine translation of null subjects in Italian and Spanish.</title>
<date>2012</date>
<booktitle>In Proceedings of the Student Research Workshop at the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>81--89</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Avignon, France,</location>
<contexts>
<context position="1468" citStr="Russo et al. (2012)" startWordPosition="226" endWordPosition="229">orphology of the verb: (1) Maria wróciła ju˙z z Francji. ØSp˛edziła tam miesi ˛ac. “Maria came back from France. ØHadsingular:feminine spent a month there.” The recently created Polish Coreference Corpus1 (PCC) (Ogrodniczuk et al., 2013) contains zero subject annotation. A markable representing the null subject is the verbal form following the position where the argument would have been expected. As tested on the development part of the corpus (described in detail later), omitting a personal pronoun is a frequent issue in the Polish language – about 30% of verbs do not have explicit subjects. Russo et al. (2012) reports similar figures for Italian (30.42%) and Spanish (41.17%). Moreover, these null subjects are often part of large coreference clusters – the average size of a non-singleton coreference cluster in the development subcorpus was 3.56 mentions. At the same 1Publicly available at http://zil.ipipan.waw. pl/PolishCoreferenceCorpus. time, the non-singleton coreference cluster containing at least one zero subject had on average 5.89 mentions. A mention detection module heavily influences the final coreference resolution score of an endto-end coreference resolution system. In Ogrodniczuk and Kop</context>
</contexts>
<marker>Russo, Loáiciga, Gulati, 2012</marker>
<rawString>Lorenza Russo, Sharid Loáiciga, and Asheesh Gulati. 2012. Improving machine translation of null subjects in Italian and Spanish. In Proceedings of the Student Research Workshop at the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 81–89, Avignon, France, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Shapiro</author>
<author>M B Wilk</author>
</authors>
<title>An analysis of variance test for normality (complete samples).</title>
<date>1965</date>
<journal>Biometrika,</journal>
<pages>52--3</pages>
<marker>Shapiro, Wilk, 1965</marker>
<rawString>S. S. Shapiro and M. B. Wilk. 1965. An analysis of variance test for normality (complete samples). Biometrika, 52(3/4):591–611, Dec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marek ´Swidzi´nski</author>
</authors>
<title>Syntactic dictionary of polish verbs.</title>
<date>1994</date>
<marker>´Swidzi´nski, 1994</marker>
<rawString>Marek ´Swidzi´nski. 1994. Syntactic dictionary of polish verbs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alina Wróblewska</author>
</authors>
<title>Polish dependency bank.</title>
<date>2012</date>
<journal>Linguistic Issues in Language Technology,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="9321" citStr="Wróblewska (2012)" startWordPosition="1528" endWordPosition="1529">annotator agreement 210 texts of the Polish Coreference Corpus were annotated independently by two annotators. This part was analyzed for the inter-annotator agreement of deciding if a verb has a zero subject or not. In the data there were 5879 verbs total, for which observed agreement yielded 92.57%. Agreement expected by chance (assuming a per annotator chance annotation probability distribution) equalled 57.52%, therefore chance-corrected Cohen’s n for the task equalled 82.51%. 4.2 Results of full dependency parsing The first Polish dependency parser was recently developed and described by Wróblewska (2012). The author reports 71% LAS2 and 75.2% UAS3 performance of this parser. This parser was used to detect null subjects – every verb lacking the dependency relation of the subject type (subj) was marked as missing the subject. This baseline method achieved accuracy of 67.23%, precision of 46.53%, recall of 90.47% and F1 equal to 61.45%. These results are worse than a simple majority baseline classifier, therefore current state-ofthe-art Polish dependency parsing is not a satisfactory solution to the task stated in this article. represents the window around the verb, with following values: the cl</context>
</contexts>
<marker>Wróblewska, 2012</marker>
<rawString>Alina Wróblewska. 2012. Polish dependency bank. Linguistic Issues in Language Technology, 7(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shanheng Zhao</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Identification and Resolution of Chinese Zero Pronouns: A Machine Learning Approach.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>541--550</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="3305" citStr="Zhao and Ng (2007)" startWordPosition="520" endWordPosition="523">ct, 21% a zero pronoun and 2% are impersonal constructions. They extract various verb, clause and neighboring token features for each verb occurrence and classify it into one of these 3 classes, achieving 83.04% accuracy of a decision tree learning classifier, better than the baseline result of the Palavras parser. A very similar study is conducted also for Spanish (Rello et al., 2012a), with the best result of the lazy learning classifier K∗ (Cleary and Trigg, 1995) of 87.6% accuracy, outperforming the baseline of Connexor parser. Chinese zero pronoun detection and resolution is presented by Zhao and Ng (2007). Features for zero pronoun identification consider mainly the gold standard parse tree structure. Their training corpus contained only 343 zero pronouns, as compared to 10098 verbs with explicit subjects – for Chinese, the phenomenon is much less frequent than for Polish or Spanish. Therefore they weigh 221 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 221–225, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics positive and negative examples to get the balance between precision and recall </context>
</contexts>
<marker>Zhao, Ng, 2007</marker>
<rawString>Shanheng Zhao and Hwee Tou Ng. 2007. Identification and Resolution of Chinese Zero Pronouns: A Machine Learning Approach. In EMNLP-CoNLL, pages 541–550. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>