<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001620">
<title confidence="0.807024">
Dependency-Based Statistical Machine Translation
</title>
<note confidence="0.887303666666667">
Heidi J. Fox
Brown Laboratory for Linguistic Information Processing
Brown University, Box 1910, Providence, RI 02912
</note>
<email confidence="0.998168">
hjf@cs.brown.edu
</email>
<sectionHeader confidence="0.993872" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999491222222222">
We present a Czech-English statistical
machine translation system which per-
forms tree-to-tree translation of depen-
dency structures. The only bilingual re-
source required is a sentence-aligned par-
allel corpus. All other resources are
monolingual. We also refer to an evalua-
tion method and plan to compare our sys-
tem’s output with a benchmark system.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999787181818182">
The goal of statistical machine translation (SMT) is
to develop mathematical models of the translation
process whose parameters can be automatically esti-
mated from a parallel corpus. Given a string of for-
eign words F, we seek to find the English string E
which is a “correct” translation of the foreign string.
The first work on SMT done at IBM (Brown et al.,
1990; Brown et al., 1992; Brown et al., 1993; Berger
et al., 1994), used a noisy-channel model, resulting
in what Brown et al. (1993) call “the Fundamental
Equation of Machine Translation”:
</bodyText>
<equation confidence="0.9695055">
E�= argmax
E P(E)P(F  |E) (1)
</equation>
<bodyText confidence="0.994338125">
In this equation we see that the translation prob-
lem is factored into two subproblems. P(E) is the
language model and P(F  |E) is the translation
model. The work described here focuses on devel-
oping improvements to the translation model.
While the IBM work was groundbreaking, it was
also deficient in several ways. Their model trans-
lates words in isolation, and the component which
</bodyText>
<page confidence="0.98304">
91
</page>
<bodyText confidence="0.996904222222222">
accounts for word order differences between lan-
guages is based on linear position in the sentence.
Conspicuously absent is all but the most elementary
use of syntactic information. Several researchers
have subsequently formulated models which incor-
porate the intuition that syntactically close con-
stituents tend to stay close across languages. Below
are descriptions of some of these different methods
of integrating syntax.
</bodyText>
<listItem confidence="0.980972458333333">
• Stochastic Inversion Transduction Grammars
(Wu and Wong, 1998): This formalism uses a
grammar for English and from it derives a pos-
sible grammar for the foreign language. This
derivation includes adding productions where
the order of the RHS is reversed from the or-
dering of the English.
• Syntax-based Statistical Translation (Yamada
and Knight, 2001): This model extends the
above by allowing all possible permutations of
the RHS of the English rules.
• Statistical Phrase-based Translation (Koehn
et al., 2003): Here “phrase-based” means
“subsequence-based”, as there is no guarantee
that the phrases learned by the model will have
any relation to what we would think of as syn-
tactic phrases.
• Dependency-based Translation (ˇCmejrek et al.,
2003): This model assumes a dependency
parser for the foreign language. The syntactic
structure and labels are preserved during trans-
lation. Transfer is purely lexical. A generator
builds an English sentence out of the structure,
labels, and translated words.
</listItem>
<subsubsectionHeader confidence="0.325467">
Proceedings of the ACL Student Research Workshop, pages 91–96,
</subsubsectionHeader>
<page confidence="0.710723">
Ann Arbor, Michigan, June 2005. c�2005 Association for Computational Linguistics
</page>
<figure confidence="0.998747666666667">
SPLIT
EN2
EN1
NNPS NN
japan automobile dealers association
EN1
EN3
NNP NNP NNPS NN
japan automobile dealers association
CZ3
CZ2
CZ1
EN2
NNP NNP
...
...
...
...
</figure>
<sectionHeader confidence="0.846294" genericHeader="introduction">
2 System Overview
</sectionHeader>
<bodyText confidence="0.97518747368421">
The basic framework of our system is quite similar
to that of ˇCmejrek et al. (2003) (we reuse many of
their ancillary modules). The difference is in how
we use the dependency structures. ˇCmejrek et al.
only translate the lexical items. The dependency
structure and any features on the nodes are preserved
and all other processing is left to the generator. In
addition to lexical translation, our system models
structural changes and changes to feature values, for
although dependency structures are fairly well pre-
served across languages (Fox, 2002), there are cer-
tainly many instances where the structure must be
modified.
While the entire translation system is too large to
discuss in detail here, I will provide brief descrip-
tions of ancillary components. References are pro-
vided, where available, for those who want more in-
formation.
N N A N
</bodyText>
<subsectionHeader confidence="0.993697">
2.1 Corpus Preparation
</subsectionHeader>
<bodyText confidence="0.985719407407407">
Our parallel Czech-English corpus is comprised of
Wall Street Journal articles from 1989. The English
data is from the University of Pennsylvania Tree-
bank (Marcus et al., 1993; Marcus et al., 1994).
The Czech translations of these articles are provided
as part of the Prague Dependency Treebank (PDT)
(B¨ohmov´a et al., 2001). In order to learn the pa-
rameters for our model, we must first create aligned
dependency structures for the sentence pairs in our
corpus. This process begins with the building of de-
pendency structures.
Since Czech is a highly inflected language, mor-
phological tagging is extremely helpful for down-
stream processing. We generate the tags using
the system described in (Hajiˇc and Hladk´a, 1998).
The tagged sentences are parsed by the Charniak
parser, this time trained on Czech data from the PDT.
The resulting phrase structures are converted to tec-
togrammatical dependency structures via the proce-
dure documented in (B¨ohmov´a, 2001). Under this
formalism, function words are deleted and any in-
formation contained in them is preserved in features
attached to the remaining nodes. Finally, functors
(such as agent or patient) are automatically assigned
to nodes in the tree (ˇZabokrtsk´y et al., 2002).
On the English side, the process is simpler. We
... asociace obchodnik japonsk´y automobil ...
</bodyText>
<figureCaption confidence="0.998863">
Figure 1: Left SPLIT Example
</figureCaption>
<bodyText confidence="0.999914166666667">
parse with the Charniak parser (Charniak, 2000)
and convert the resulting phrase-structure trees to a
function-argument formalism, which, like the tec-
togrammatic formalism, removes function words.
This conversion is accomplished via deterministic
application of approximately 20 rules.
</bodyText>
<subsectionHeader confidence="0.999567">
2.2 Aligning the Dependency Structures
</subsectionHeader>
<bodyText confidence="0.999964142857143">
We now generate the alignments between the pairs
of dependency structures we have created. We be-
gin by producing word alignments with a model very
similar to that of IBM Model 4 (Brown et al., 1993).
We keep fifty possible alignments and require that
each word has at least two possible alignments. We
then align phrases based on the alignments of the
words in each phrase span. If there is no satisfac-
tory alignment, then we allow for structural muta-
tions. The probabilities for these mutations are re-
fined via another round of alignment. The structural
mutations allowed are described below. Examples
are shown in phrase-structure format rather than de-
pendency format for ease of explanation.
</bodyText>
<page confidence="0.795126">
92
</page>
<figure confidence="0.998657727272727">
EN4
EN3
EN1
... bear stearns ...
NNP NNP
EN1
EN2
77 NN WDT VBD NNP
BUD ... fiscal year which began august ...
N N
N
... spoleˇcnost bear stearns ...
ERASE EN4 ERASE
EN3
77 NN WDT VBD NNP
... fiscal year which began august ...
CZ2
CZ1
EN2
CZ2
CZ1
A N P V N
</figure>
<figureCaption confidence="0.999363">
Figure 2: BUD Example
</figureCaption>
<listItem confidence="0.964354363636364">
• KEEP: No change. This is the default.
• SPLIT: One English phrase aligns with two
Czech phrases and splitting the English phrase
results in a better alignment. There are three
types of split (left, right, middle) whose proba-
bilities are also estimated. In the original struc-
ture of Figure 1, English node EN1 would align
with Czech nodes CZ1 and CZ2. Splitting the
English by adding child node EN3 results in a
better alignment.
• BUD: This adds a unary level in the English
</listItem>
<bodyText confidence="0.682306">
tree in the case when one English node aligns
to two Czech nodes, but one of the Czech nodes
is the parent of the other. In Figure 2, the Czech
has one extra word “spoleˇcnost” (“company”)
compared with the English. English node EN1
would normally align to both CZ1 and CZ2.
Adding a unary node EN2 to the English results
in a better alignment.
</bodyText>
<listItem confidence="0.998394125">
• ERASE: There is no corresponding Czech node
for the English one. In Figure 3, the English has
two nodes, EN1 and EN2, which have no corre-
sponding Czech nodes. Erasing them brings the
Czech and English structures into alignment.
• PHRASE-TO-WORD: An entire English
phrase aligns with one Czech word. This
operates similarly to ERASE.
</listItem>
<figure confidence="0.674067">
... fisk´alnirok kter´y zaˇr´ı srpen ...
</figure>
<figureCaption confidence="0.998843">
Figure 3: ERASE Example
</figureCaption>
<sectionHeader confidence="0.935969" genericHeader="method">
3 Translation Model
</sectionHeader>
<bodyText confidence="0.9999352">
Given E, the parse of the English string, our trans-
lation model can be formalized as P(F  |E). Let
E1 ... En be the nodes in the English parse, F be
a parse of the Czech string, and F1 ... F be the
nodes in the Czech parse. Then,
</bodyText>
<equation confidence="0.895351">
P(F  |E) = � P (F1 . . . F� |E1 ... En) (2)
FforF
</equation>
<bodyText confidence="0.999969833333333">
We initially make several strong independence as-
sumptions which we hope to eventually weaken.
The first is that each Czech parse node is generated
independently of every other one. Further, we spec-
ify that each English parse node generates exactly
one (possibly NULL) Czech parse node.
</bodyText>
<equation confidence="0.983982">
P(F  |E) = � P(Fi  |E1 ... En) = n P(Fi  |Ei)
FiEF i=1
(3)
</equation>
<bodyText confidence="0.9219585">
An English parse node Ei contains the following
information:
</bodyText>
<listItem confidence="0.99856925">
• An English word: ei
• A part of speech: tz
• A vector of n features (e.g. negation or tense):
C �z[1],...,�z[n] &gt;
</listItem>
<page confidence="0.803285">
93
</page>
<listItem confidence="0.987957">
• A list of dependent nodes
</listItem>
<bodyText confidence="0.999845948717949">
In order to produce a Czech parse node .F&apos;i, we
must generate the following:
Lemma fi: We generate the Czech lemma fi de-
pendent only on the English word ei.
Part of Speech tfi : We generate Czech part of
speech tfi dependent on the part of speech of
the Czech parent tfpar(i) and the corresponding
English part of speech tei.
Features &lt; 0fi [1], � � � , 0fi [n] &gt;: There are several
features (see Table 1) associated with each
parse node. Of these, all except IND are typi-
cal morphological and analytical features. IND
(indicator) is a loosely-specified feature com-
prised of functors, where assigned, and other
words or small phrases (often prepositions)
which are attached to the node and indicate
something about the node’s function in the sen-
tence. (e.g. an IND of “at” could indicate a
locative function). We generate each Czech
feature 0fi [j] dependent only on its correspond-
ing English feature 0e i [j].
Head Position hi: When an English word is
aligned to the head of a Czech phrase, the
English word is typically also the head of its
respective phrase. But, this is not always the
case, so we model the probability that the En-
glish head will be aligned to either the Czech
head or to one of its children. To simplify,
we set the probability for each particular child
being the head to be uniform in the number
of children. The head position is generated
independent of the rest of the sentence.
Structural Mutation mi: Dependency structures
are fairly well preserved across languages, but
there are cases when the structures need to be
modified. Section 2.2 contains descriptions
of the different structural changes which
we model. The mutation type is generated
independent of the rest of the sentence.
</bodyText>
<table confidence="0.999213333333333">
Feature Description
NEG Negation
STY Style (e.g. statement, question)
QUO Is node part of a quoted expression?
MD Modal verb associated with node
TEN Tense (past, present, future)
MOOD Mood (infinitive, perfect, progressive)
CONJ Is node part of a conjoined expression?
IND Indicator
</table>
<tableCaption confidence="0.996908">
Table 1: Features
</tableCaption>
<subsectionHeader confidence="0.999695">
3.1 Model with Independence Assumptions
</subsectionHeader>
<bodyText confidence="0.999733">
With all of the independence assumptions described
above(, the translation model becomes:
</bodyText>
<equation confidence="0.997685">
P(.F&apos;i  |£i) = P(fi  |ei)P(tfi  |tei , tfpar(i))
n
xP(hi)P(mi) H P(0f i[j]  |0ei [j]) (4)
j=1
</equation>
<sectionHeader confidence="0.987844" genericHeader="method">
4 Training
</sectionHeader>
<bodyText confidence="0.999934571428571">
The Czech and English data are preprocessed (see
Section 2.1) and the resulting dependency structures
are aligned (see Section 2.2). We estimate the model
parameters from this aligned data by maximum like-
lihood estimation. In addition, we gather the inverse
probabilities P(E  |F) for use in the figure of merit
which guides the decoder’s search.
</bodyText>
<sectionHeader confidence="0.995934" genericHeader="method">
5 Decoding
</sectionHeader>
<bodyText confidence="0.963818">
Given a Czech sentence to translate, we first pro-
cess it as described in Section 2.1. The resulting de-
pendency structure is the input to the decoder. The
decoder itself is a best-first decoder whose priority
queue holds partially-constructed English nodes.
For our figure of merit to guide the search, we use
the probability P(E  |F). We normalize this us-
ing the perplexity (2H) to compensate for the differ-
ent number of possible values for the features 0[j].
Given two different features whose values have the
same probability, the figure of merit for the feature
with the greater uncertainty will be boosted. This
prevents features with few possible values from mo-
nopolizing the search at the expense of the other fea-
tures. Thus, for feature 0ei [j], the figure of merit is
FOM = P(0e i[j]  |0fi [j]) x 2H(�i fj]|φz fj]) (5)
</bodyText>
<page confidence="0.996063">
94
</page>
<bodyText confidence="0.999532857142857">
Since our goal is to build a forest of partial trans-
lations, we translate each Czech dependency node
independently of the others. (As more conditioning
factors are added in the future, we will instead trans-
late small subtrees rather than single nodes.) Each
translated node £i is constructed incrementally in the
following order:
</bodyText>
<listItem confidence="0.99936075">
1. Choose the head position hi
2. Generate the part of speech tei
3. For j = 1..n, generate Oei [j]
4. Choose a structural mutation mi
</listItem>
<bodyText confidence="0.999813166666667">
English nodes continue to be generated until ei-
ther the queue or some other stopping condition
is reached (e.g. having a certain number of possi-
ble translations for each Czech node). After stop-
ping, we are left with a forest of English dependency
nodes or subtrees.
</bodyText>
<sectionHeader confidence="0.99643" genericHeader="method">
6 Language Model
</sectionHeader>
<bodyText confidence="0.999966666666667">
We use a syntax-based language model which was
originally developed for use in speech recognition
(Charniak, 2001) and later adapted to work with a
syntax-based machine translation system (Charniak
et al., 2001). This language model requires a for-
est of partial phrase structures as input. Therefore,
the format of the output of the decoder must be
changed. This is the inverse transformation of the
one performed during corpus preparation. We ac-
complish this with a statistical tree transformation
model whose parameters are estimated during the
corpus preparation phase.
</bodyText>
<sectionHeader confidence="0.992694" genericHeader="evaluation">
7 Evaluation
</sectionHeader>
<bodyText confidence="0.999604590909091">
We propose to evaluate system performance with
version 0.9 of the NIST automated scorer (NIST,
2002), which is a modification of the BLEU sys-
tem (Papineni et al., 2001). BLEU calculates a score
based on a weighted sum of the counts of matching
n-grams, along with a penalty for a significant dif-
ference in length between the system output and the
reference translation closest in length. Experiments
have shown a high degree of correlation between
BLEU score and the translation quality judgments
of humans. The most interesting difference in the
NIST scorer is that it weights n-grams based on a
notion of informativeness. Details of the scorer can
be found in their paper.
For our experiments, we propose to use the data
from the PDT, which has already been segmented
into training, held out (or development test), and
evaluation sets. As a baseline, we will run the
GIZA++ implementation of IBM’s Model 4 trans-
lation algorithm under the same training conditions
as our own system (Al-Onaizan et al., 1999; Och and
Ney, 2000; Germann et al., 2001).
</bodyText>
<sectionHeader confidence="0.99614" genericHeader="discussions">
8 Future Work
</sectionHeader>
<bodyText confidence="0.999990842105263">
Our first priority is to complete the final pieces so
that we have an end-to-end system to experiment
with. Once we are able to evaluate our system out-
put, our first priority will be to analyze the system
errors and adjust the model accordingly. We recog-
nize that our independence assumptions are gener-
ally too strong, and improving them is a hight pri-
ority. Adding more conditioning factors should im-
prove the quality of the decoder output as well as re-
ducing the amount of probability mass lost on struc-
tures which are not well formed. With this will come
sparse data issues, so it will also be important for us
to incorporate smoothing into the model.
There are many interesting subproblems which
deserve attention and we hope to examine at least a
couple of these in the near future. Among these are
discontinuous constituents, head switching, phrasal
translation, English word stemming, and improved
modeling of structural changes.
</bodyText>
<sectionHeader confidence="0.997574" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.954596">
This work was supported in part by NSF grant
IGERT-9870676. We would like to thank Jan Hajiˇc,
Martin ˇCmejrek, Jan Cuˇrin for all of their assistance.
</bodyText>
<sectionHeader confidence="0.998829" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.927624142857143">
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-
Josef Och, David Purdy, Noah A. Smith, and
David Yarowsky. 1999. Statistical machine
translation: Final report, JHU workshop 1999.
www.clsp.jhu.edu/ws99/projects/mt/final report/mt-
final-report.ps.
</reference>
<page confidence="0.992642">
95
</page>
<reference confidence="0.99941949">
Adam L. Berger, Peter F. Brown, Stephen A. Della Pietra,
Vincent J. Della Pietra, John R. Gillett, John D. Laf-
ferty, Robert L. Mercer, Harry Printz, and Luboˇs Ureˇs.
1994. The Candide system for machine translation. In
Proceedings of the ARPA Human Language Technol-
ogy Workshop.
Alena B¨ohmov´a, Jan Hajiˇc, Eva Hajiˇcov´a, and Barbora
Hladk´a. 2001. The Prague Dependency Treebank:
Three-level annotation scenario. In Anne Abeill´e, ed-
itor, Treebanks: Building and Using Syntactically An-
notated Corpora. Kluwer Academic Publishers.
Alena B¨ohmov´a. 2001. Automatic procedures in tec-
togrammatical tagging. The Prague Bulletin of Math-
ematical Linguistics, 76.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Computa-
tional Linguistics, 16(2):79–85.
Peter F. Brown, Stephen A. Della Petra, Vincent J.
Della Pietra, John D. Lafferty, and Robert L. Mer-
cer. 1992. Analysis, statistical transfer, and synthesis
in machine translation. In Proceedings of the Fourth
International Conference on Theoretical and Method-
ological Issues in Machine Translation, pages 83–100.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The math-
ematics of machine translation: Parameter estimation.
Computational Linguistics, 19(2):263–311, June.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2001. Syntax-based language models for statistical
machine translation. In Proceedings of the 39th An-
nual Meeting of the Association for Computational
Linguistics, Toulouse, France, July.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of the 39th Annual
Meeting of the Association for Computational Linguis-
tics, pages 116–123, Toulouse, France, July.
Martin ˇCmejrek, Jan Cuˇr´ın, and Jiˇr´ı Havelka. 2003.
Czech-English Dependency-based Machine Transla-
tion. In EACL 2003 Proceedings of the Conference,
pages 83–90, April 12–17, 2003.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2002), July.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proceed-
ings of the 39th Annual Meeting of the Association for
Computational Linguistics.
Jan Hajiˇc and Barbora Hladk´a. 1998. Tagging Inflec-
tive Languages: Prediction of Morphological Cate-
gories for a Rich, Structured Tagset. In Proceedings of
COLING-ACL Conference, pages 483–490, Montreal,
Canada.
Philip Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the Human Language Technology and North Ameri-
can Association for Computational Linguistics Con-
ference, Edmonton, Canada, May.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 13(2):313–330, June.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure. In
Proceedings of the ARPA Human Language Technol-
ogy Workshop, pages 114–119.
NIST. 2002. Automatic evaluation of machine trans-
lation quality using n-gram co-occurrence statistics.
www.nist.gov/speech/tests/mt/doc/ngram-study.pdf.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 440–447.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: A method for automatic evalu-
ation of machine translation. Technical report, IBM.
Dekai Wu and Hongsing Wong. 1998. Machine trans-
lation with a stochastic grammatical channel. In Pro-
ceedings ofthe 36th Annual Meeting ofthe Association
for Computational Linguistics, pages 1408–1414.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Meeting of the Association for Compu-
tational Linguistics.
Zdenˇek ˇZabokrtsk´y, Petr Sgall, and Saˇso Dˇzeroski. 2002.
Machine learning approach to automatic functor as-
signment in the Prague Dependency Treebank. In Pro-
ceedings of LREC 2002 (Third International Confer-
ence on Language Resources and Evaluation), vol-
ume V, pages 1513–1520, Las Palmas de Gran Ca-
naria, Spain.
</reference>
<page confidence="0.998393">
96
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.780150">
<title confidence="0.999682">Dependency-Based Statistical Machine Translation</title>
<author confidence="0.999999">Heidi J Fox</author>
<affiliation confidence="0.997268">Brown Laboratory for Linguistic Information Processing</affiliation>
<address confidence="0.800351">Brown University, Box 1910, Providence, RI 02912</address>
<email confidence="0.999446">hjf@cs.brown.edu</email>
<abstract confidence="0.9978044">We present a Czech-English statistical machine translation system which performs tree-to-tree translation of dependency structures. The only bilingual resource required is a sentence-aligned parallel corpus. All other resources are monolingual. We also refer to an evaluation method and plan to compare our system’s output with a benchmark system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John Lafferty</author>
<author>Dan Melamed</author>
<author>FranzJosef Och</author>
<author>David Purdy</author>
<author>Noah A Smith</author>
</authors>
<location>and</location>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, </marker>
<rawString>Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, FranzJosef Och, David Purdy, Noah A. Smith, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Statistical machine translation: Final report, JHU workshop 1999. www.clsp.jhu.edu/ws99/projects/mt/final report/mtfinal-report.ps.</title>
<date>1999</date>
<marker>Yarowsky, 1999</marker>
<rawString>David Yarowsky. 1999. Statistical machine translation: Final report, JHU workshop 1999. www.clsp.jhu.edu/ws99/projects/mt/final report/mtfinal-report.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>John R Gillett</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Harry Printz</author>
<author>Luboˇs Ureˇs</author>
</authors>
<title>The Candide system for machine translation.</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA Human Language Technology Workshop.</booktitle>
<marker>Berger, Brown, Pietra, Pietra, Gillett, Lafferty, Mercer, Printz, Ureˇs, 1994</marker>
<rawString>Adam L. Berger, Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, John R. Gillett, John D. Lafferty, Robert L. Mercer, Harry Printz, and Luboˇs Ureˇs. 1994. The Candide system for machine translation. In Proceedings of the ARPA Human Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alena B¨ohmov´a</author>
<author>Jan Hajiˇc</author>
<author>Eva Hajiˇcov´a</author>
<author>Barbora Hladk´a</author>
</authors>
<title>The Prague Dependency Treebank: Three-level annotation scenario.</title>
<date>2001</date>
<editor>In Anne Abeill´e, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>B¨ohmov´a, Hajiˇc, Hajiˇcov´a, Hladk´a, 2001</marker>
<rawString>Alena B¨ohmov´a, Jan Hajiˇc, Eva Hajiˇcov´a, and Barbora Hladk´a. 2001. The Prague Dependency Treebank: Three-level annotation scenario. In Anne Abeill´e, editor, Treebanks: Building and Using Syntactically Annotated Corpora. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alena B¨ohmov´a</author>
</authors>
<title>Automatic procedures in tectogrammatical tagging.</title>
<date>2001</date>
<journal>The Prague Bulletin of Mathematical Linguistics,</journal>
<volume>76</volume>
<marker>B¨ohmov´a, 2001</marker>
<rawString>Alena B¨ohmov´a. 2001. Automatic procedures in tectogrammatical tagging. The Prague Bulletin of Mathematical Linguistics, 76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Fredrick Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Paul S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="919" citStr="Brown et al., 1990" startWordPosition="138" endWordPosition="141">dependency structures. The only bilingual resource required is a sentence-aligned parallel corpus. All other resources are monolingual. We also refer to an evaluation method and plan to compare our system’s output with a benchmark system. 1 Introduction The goal of statistical machine translation (SMT) is to develop mathematical models of the translation process whose parameters can be automatically estimated from a parallel corpus. Given a string of foreign words F, we seek to find the English string E which is a “correct” translation of the foreign string. The first work on SMT done at IBM (Brown et al., 1990; Brown et al., 1992; Brown et al., 1993; Berger et al., 1994), used a noisy-channel model, resulting in what Brown et al. (1993) call “the Fundamental Equation of Machine Translation”: E�= argmax E P(E)P(F |E) (1) In this equation we see that the translation problem is factored into two subproblems. P(E) is the language model and P(F |E) is the translation model. The work described here focuses on developing improvements to the translation model. While the IBM work was groundbreaking, it was also deficient in several ways. Their model translates words in isolation, and the component which 91 </context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Petra</author>
<author>Vincent J Della Pietra</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
</authors>
<title>Analysis, statistical transfer, and synthesis in machine translation.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>83--100</pages>
<contexts>
<context position="939" citStr="Brown et al., 1992" startWordPosition="142" endWordPosition="145">s. The only bilingual resource required is a sentence-aligned parallel corpus. All other resources are monolingual. We also refer to an evaluation method and plan to compare our system’s output with a benchmark system. 1 Introduction The goal of statistical machine translation (SMT) is to develop mathematical models of the translation process whose parameters can be automatically estimated from a parallel corpus. Given a string of foreign words F, we seek to find the English string E which is a “correct” translation of the foreign string. The first work on SMT done at IBM (Brown et al., 1990; Brown et al., 1992; Brown et al., 1993; Berger et al., 1994), used a noisy-channel model, resulting in what Brown et al. (1993) call “the Fundamental Equation of Machine Translation”: E�= argmax E P(E)P(F |E) (1) In this equation we see that the translation problem is factored into two subproblems. P(E) is the language model and P(F |E) is the translation model. The work described here focuses on developing improvements to the translation model. While the IBM work was groundbreaking, it was also deficient in several ways. Their model translates words in isolation, and the component which 91 accounts for word or</context>
</contexts>
<marker>Brown, Petra, Pietra, Lafferty, Mercer, 1992</marker>
<rawString>Peter F. Brown, Stephen A. Della Petra, Vincent J. Della Pietra, John D. Lafferty, and Robert L. Mercer. 1992. Analysis, statistical transfer, and synthesis in machine translation. In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation, pages 83–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="959" citStr="Brown et al., 1993" startWordPosition="146" endWordPosition="149">l resource required is a sentence-aligned parallel corpus. All other resources are monolingual. We also refer to an evaluation method and plan to compare our system’s output with a benchmark system. 1 Introduction The goal of statistical machine translation (SMT) is to develop mathematical models of the translation process whose parameters can be automatically estimated from a parallel corpus. Given a string of foreign words F, we seek to find the English string E which is a “correct” translation of the foreign string. The first work on SMT done at IBM (Brown et al., 1990; Brown et al., 1992; Brown et al., 1993; Berger et al., 1994), used a noisy-channel model, resulting in what Brown et al. (1993) call “the Fundamental Equation of Machine Translation”: E�= argmax E P(E)P(F |E) (1) In this equation we see that the translation problem is factored into two subproblems. P(E) is the language model and P(F |E) is the translation model. The work described here focuses on developing improvements to the translation model. While the IBM work was groundbreaking, it was also deficient in several ways. Their model translates words in isolation, and the component which 91 accounts for word order differences betw</context>
<context position="6020" citStr="Brown et al., 1993" startWordPosition="944" endWordPosition="947">cess is simpler. We ... asociace obchodnik japonsk´y automobil ... Figure 1: Left SPLIT Example parse with the Charniak parser (Charniak, 2000) and convert the resulting phrase-structure trees to a function-argument formalism, which, like the tectogrammatic formalism, removes function words. This conversion is accomplished via deterministic application of approximately 20 rules. 2.2 Aligning the Dependency Structures We now generate the alignments between the pairs of dependency structures we have created. We begin by producing word alignments with a model very similar to that of IBM Model 4 (Brown et al., 1993). We keep fifty possible alignments and require that each word has at least two possible alignments. We then align phrases based on the alignments of the words in each phrase span. If there is no satisfactory alignment, then we allow for structural mutations. The probabilities for these mutations are refined via another round of alignment. The structural mutations allowed are described below. Examples are shown in phrase-structure format rather than dependency format for ease of explanation. 92 EN4 EN3 EN1 ... bear stearns ... NNP NNP EN1 EN2 77 NN WDT VBD NNP BUD ... fiscal year which began a</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Kevin Knight</author>
<author>Kenji Yamada</author>
</authors>
<title>Syntax-based language models for statistical machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Toulouse, France,</location>
<contexts>
<context position="13258" citStr="Charniak et al., 2001" startWordPosition="2213" endWordPosition="2216">se the head position hi 2. Generate the part of speech tei 3. For j = 1..n, generate Oei [j] 4. Choose a structural mutation mi English nodes continue to be generated until either the queue or some other stopping condition is reached (e.g. having a certain number of possible translations for each Czech node). After stopping, we are left with a forest of English dependency nodes or subtrees. 6 Language Model We use a syntax-based language model which was originally developed for use in speech recognition (Charniak, 2001) and later adapted to work with a syntax-based machine translation system (Charniak et al., 2001). This language model requires a forest of partial phrase structures as input. Therefore, the format of the output of the decoder must be changed. This is the inverse transformation of the one performed during corpus preparation. We accomplish this with a statistical tree transformation model whose parameters are estimated during the corpus preparation phase. 7 Evaluation We propose to evaluate system performance with version 0.9 of the NIST automated scorer (NIST, 2002), which is a modification of the BLEU system (Papineni et al., 2001). BLEU calculates a score based on a weighted sum of the </context>
</contexts>
<marker>Charniak, Knight, Yamada, 2001</marker>
<rawString>Eugene Charniak, Kevin Knight, and Kenji Yamada. 2001. Syntax-based language models for statistical machine translation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5544" citStr="Charniak, 2000" startWordPosition="875" endWordPosition="876">e trained on Czech data from the PDT. The resulting phrase structures are converted to tectogrammatical dependency structures via the procedure documented in (B¨ohmov´a, 2001). Under this formalism, function words are deleted and any information contained in them is preserved in features attached to the remaining nodes. Finally, functors (such as agent or patient) are automatically assigned to nodes in the tree (ˇZabokrtsk´y et al., 2002). On the English side, the process is simpler. We ... asociace obchodnik japonsk´y automobil ... Figure 1: Left SPLIT Example parse with the Charniak parser (Charniak, 2000) and convert the resulting phrase-structure trees to a function-argument formalism, which, like the tectogrammatic formalism, removes function words. This conversion is accomplished via deterministic application of approximately 20 rules. 2.2 Aligning the Dependency Structures We now generate the alignments between the pairs of dependency structures we have created. We begin by producing word alignments with a model very similar to that of IBM Model 4 (Brown et al., 1993). We keep fifty possible alignments and require that each word has at least two possible alignments. We then align phrases b</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>116--123</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="13161" citStr="Charniak, 2001" startWordPosition="2200" endWordPosition="2201">des.) Each translated node £i is constructed incrementally in the following order: 1. Choose the head position hi 2. Generate the part of speech tei 3. For j = 1..n, generate Oei [j] 4. Choose a structural mutation mi English nodes continue to be generated until either the queue or some other stopping condition is reached (e.g. having a certain number of possible translations for each Czech node). After stopping, we are left with a forest of English dependency nodes or subtrees. 6 Language Model We use a syntax-based language model which was originally developed for use in speech recognition (Charniak, 2001) and later adapted to work with a syntax-based machine translation system (Charniak et al., 2001). This language model requires a forest of partial phrase structures as input. Therefore, the format of the output of the decoder must be changed. This is the inverse transformation of the one performed during corpus preparation. We accomplish this with a statistical tree transformation model whose parameters are estimated during the corpus preparation phase. 7 Evaluation We propose to evaluate system performance with version 0.9 of the NIST automated scorer (NIST, 2002), which is a modification of</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>Eugene Charniak. 2001. Immediate-head parsing for language models. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, pages 116–123, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin ˇCmejrek</author>
<author>Jan Cuˇr´ın</author>
<author>Jiˇr´ı Havelka</author>
</authors>
<title>Czech-English Dependency-based Machine Translation. In</title>
<date>2003</date>
<booktitle>EACL 2003 Proceedings of the Conference,</booktitle>
<pages>83--90</pages>
<marker>ˇCmejrek, Cuˇr´ın, Havelka, 2003</marker>
<rawString>Martin ˇCmejrek, Jan Cuˇr´ın, and Jiˇr´ı Havelka. 2003. Czech-English Dependency-based Machine Translation. In EACL 2003 Proceedings of the Conference, pages 83–90, April 12–17, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="3824" citStr="Fox, 2002" startWordPosition="599" endWordPosition="600"> CZ3 CZ2 CZ1 EN2 NNP NNP ... ... ... ... 2 System Overview The basic framework of our system is quite similar to that of ˇCmejrek et al. (2003) (we reuse many of their ancillary modules). The difference is in how we use the dependency structures. ˇCmejrek et al. only translate the lexical items. The dependency structure and any features on the nodes are preserved and all other processing is left to the generator. In addition to lexical translation, our system models structural changes and changes to feature values, for although dependency structures are fairly well preserved across languages (Fox, 2002), there are certainly many instances where the structure must be modified. While the entire translation system is too large to discuss in detail here, I will provide brief descriptions of ancillary components. References are provided, where available, for those who want more information. N N A N 2.1 Corpus Preparation Our parallel Czech-English corpus is comprised of Wall Street Journal articles from 1989. The English data is from the University of Pennsylvania Treebank (Marcus et al., 1993; Marcus et al., 1994). The Czech translations of these articles are provided as part of the Prague Depen</context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Heidi Fox. 2002. Phrasal cohesion and statistical machine translation. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Kenji Yamada</author>
</authors>
<title>Fast decoding and optimal decoding for machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="14682" citStr="Germann et al., 2001" startWordPosition="2448" endWordPosition="2451">correlation between BLEU score and the translation quality judgments of humans. The most interesting difference in the NIST scorer is that it weights n-grams based on a notion of informativeness. Details of the scorer can be found in their paper. For our experiments, we propose to use the data from the PDT, which has already been segmented into training, held out (or development test), and evaluation sets. As a baseline, we will run the GIZA++ implementation of IBM’s Model 4 translation algorithm under the same training conditions as our own system (Al-Onaizan et al., 1999; Och and Ney, 2000; Germann et al., 2001). 8 Future Work Our first priority is to complete the final pieces so that we have an end-to-end system to experiment with. Once we are able to evaluate our system output, our first priority will be to analyze the system errors and adjust the model accordingly. We recognize that our independence assumptions are generally too strong, and improving them is a hight priority. Adding more conditioning factors should improve the quality of the decoder output as well as reducing the amount of probability mass lost on structures which are not well formed. With this will come sparse data issues, so it </context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2001</marker>
<rawString>Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2001. Fast decoding and optimal decoding for machine translation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Barbora Hladk´a</author>
</authors>
<title>Tagging Inflective Languages: Prediction of Morphological Categories for a Rich, Structured Tagset.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL Conference,</booktitle>
<pages>483--490</pages>
<location>Montreal, Canada.</location>
<marker>Hajiˇc, Hladk´a, 1998</marker>
<rawString>Jan Hajiˇc and Barbora Hladk´a. 1998. Tagging Inflective Languages: Prediction of Morphological Categories for a Rich, Structured Tagset. In Proceedings of COLING-ACL Conference, pages 483–490, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference,</booktitle>
<location>Edmonton, Canada,</location>
<contexts>
<context position="2459" citStr="Koehn et al., 2003" startWordPosition="382" endWordPosition="385">lose across languages. Below are descriptions of some of these different methods of integrating syntax. • Stochastic Inversion Transduction Grammars (Wu and Wong, 1998): This formalism uses a grammar for English and from it derives a possible grammar for the foreign language. This derivation includes adding productions where the order of the RHS is reversed from the ordering of the English. • Syntax-based Statistical Translation (Yamada and Knight, 2001): This model extends the above by allowing all possible permutations of the RHS of the English rules. • Statistical Phrase-based Translation (Koehn et al., 2003): Here “phrase-based” means “subsequence-based”, as there is no guarantee that the phrases learned by the model will have any relation to what we would think of as syntactic phrases. • Dependency-based Translation (ˇCmejrek et al., 2003): This model assumes a dependency parser for the foreign language. The syntactic structure and labels are preserved during translation. Transfer is purely lexical. A generator builds an English sentence out of the structure, labels, and translated words. Proceedings of the ACL Student Research Workshop, pages 91–96, Ann Arbor, Michigan, June 2005. c�2005 Associ</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philip Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference, Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="4319" citStr="Marcus et al., 1993" startWordPosition="679" endWordPosition="682">ges and changes to feature values, for although dependency structures are fairly well preserved across languages (Fox, 2002), there are certainly many instances where the structure must be modified. While the entire translation system is too large to discuss in detail here, I will provide brief descriptions of ancillary components. References are provided, where available, for those who want more information. N N A N 2.1 Corpus Preparation Our parallel Czech-English corpus is comprised of Wall Street Journal articles from 1989. The English data is from the University of Pennsylvania Treebank (Marcus et al., 1993; Marcus et al., 1994). The Czech translations of these articles are provided as part of the Prague Dependency Treebank (PDT) (B¨ohmov´a et al., 2001). In order to learn the parameters for our model, we must first create aligned dependency structures for the sentence pairs in our corpus. This process begins with the building of dependency structures. Since Czech is a highly inflected language, morphological tagging is extremely helpful for downstream processing. We generate the tags using the system described in (Hajiˇc and Hladk´a, 1998). The tagged sentences are parsed by the Charniak parser</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 13(2):313–330, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert MacIntyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA Human Language Technology Workshop,</booktitle>
<pages>114--119</pages>
<contexts>
<context position="4341" citStr="Marcus et al., 1994" startWordPosition="683" endWordPosition="686">ature values, for although dependency structures are fairly well preserved across languages (Fox, 2002), there are certainly many instances where the structure must be modified. While the entire translation system is too large to discuss in detail here, I will provide brief descriptions of ancillary components. References are provided, where available, for those who want more information. N N A N 2.1 Corpus Preparation Our parallel Czech-English corpus is comprised of Wall Street Journal articles from 1989. The English data is from the University of Pennsylvania Treebank (Marcus et al., 1993; Marcus et al., 1994). The Czech translations of these articles are provided as part of the Prague Dependency Treebank (PDT) (B¨ohmov´a et al., 2001). In order to learn the parameters for our model, we must first create aligned dependency structures for the sentence pairs in our corpus. This process begins with the building of dependency structures. Since Czech is a highly inflected language, morphological tagging is extremely helpful for downstream processing. We generate the tags using the system described in (Hajiˇc and Hladk´a, 1998). The tagged sentences are parsed by the Charniak parser, this time trained on</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure. In Proceedings of the ARPA Human Language Technology Workshop, pages 114–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<note>www.nist.gov/speech/tests/mt/doc/ngram-study.pdf.</note>
<contexts>
<context position="13733" citStr="NIST, 2002" startWordPosition="2289" endWordPosition="2290"> in speech recognition (Charniak, 2001) and later adapted to work with a syntax-based machine translation system (Charniak et al., 2001). This language model requires a forest of partial phrase structures as input. Therefore, the format of the output of the decoder must be changed. This is the inverse transformation of the one performed during corpus preparation. We accomplish this with a statistical tree transformation model whose parameters are estimated during the corpus preparation phase. 7 Evaluation We propose to evaluate system performance with version 0.9 of the NIST automated scorer (NIST, 2002), which is a modification of the BLEU system (Papineni et al., 2001). BLEU calculates a score based on a weighted sum of the counts of matching n-grams, along with a penalty for a significant difference in length between the system output and the reference translation closest in length. Experiments have shown a high degree of correlation between BLEU score and the translation quality judgments of humans. The most interesting difference in the NIST scorer is that it weights n-grams based on a notion of informativeness. Details of the scorer can be found in their paper. For our experiments, we p</context>
</contexts>
<marker>NIST, 2002</marker>
<rawString>NIST. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. www.nist.gov/speech/tests/mt/doc/ngram-study.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="14659" citStr="Och and Ney, 2000" startWordPosition="2444" endWordPosition="2447">n a high degree of correlation between BLEU score and the translation quality judgments of humans. The most interesting difference in the NIST scorer is that it weights n-grams based on a notion of informativeness. Details of the scorer can be found in their paper. For our experiments, we propose to use the data from the PDT, which has already been segmented into training, held out (or development test), and evaluation sets. As a baseline, we will run the GIZA++ implementation of IBM’s Model 4 translation algorithm under the same training conditions as our own system (Al-Onaizan et al., 1999; Och and Ney, 2000; Germann et al., 2001). 8 Future Work Our first priority is to complete the final pieces so that we have an end-to-end system to experiment with. Once we are able to evaluate our system output, our first priority will be to analyze the system errors and adjust the model accordingly. We recognize that our independence assumptions are generally too strong, and improving them is a hight priority. Adding more conditioning factors should improve the quality of the decoder output as well as reducing the amount of probability mass lost on structures which are not well formed. With this will come spa</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 440–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2001</date>
<tech>Technical report, IBM.</tech>
<contexts>
<context position="13801" citStr="Papineni et al., 2001" startWordPosition="2300" endWordPosition="2303"> to work with a syntax-based machine translation system (Charniak et al., 2001). This language model requires a forest of partial phrase structures as input. Therefore, the format of the output of the decoder must be changed. This is the inverse transformation of the one performed during corpus preparation. We accomplish this with a statistical tree transformation model whose parameters are estimated during the corpus preparation phase. 7 Evaluation We propose to evaluate system performance with version 0.9 of the NIST automated scorer (NIST, 2002), which is a modification of the BLEU system (Papineni et al., 2001). BLEU calculates a score based on a weighted sum of the counts of matching n-grams, along with a penalty for a significant difference in length between the system output and the reference translation closest in length. Experiments have shown a high degree of correlation between BLEU score and the translation quality judgments of humans. The most interesting difference in the NIST scorer is that it weights n-grams based on a notion of informativeness. Details of the scorer can be found in their paper. For our experiments, we propose to use the data from the PDT, which has already been segmente</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. Bleu: A method for automatic evaluation of machine translation. Technical report, IBM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Hongsing Wong</author>
</authors>
<title>Machine translation with a stochastic grammatical channel.</title>
<date>1998</date>
<booktitle>In Proceedings ofthe 36th Annual Meeting ofthe Association for Computational Linguistics,</booktitle>
<pages>1408--1414</pages>
<contexts>
<context position="2008" citStr="Wu and Wong, 1998" startWordPosition="310" endWordPosition="313">was groundbreaking, it was also deficient in several ways. Their model translates words in isolation, and the component which 91 accounts for word order differences between languages is based on linear position in the sentence. Conspicuously absent is all but the most elementary use of syntactic information. Several researchers have subsequently formulated models which incorporate the intuition that syntactically close constituents tend to stay close across languages. Below are descriptions of some of these different methods of integrating syntax. • Stochastic Inversion Transduction Grammars (Wu and Wong, 1998): This formalism uses a grammar for English and from it derives a possible grammar for the foreign language. This derivation includes adding productions where the order of the RHS is reversed from the ordering of the English. • Syntax-based Statistical Translation (Yamada and Knight, 2001): This model extends the above by allowing all possible permutations of the RHS of the English rules. • Statistical Phrase-based Translation (Koehn et al., 2003): Here “phrase-based” means “subsequence-based”, as there is no guarantee that the phrases learned by the model will have any relation to what we wou</context>
</contexts>
<marker>Wu, Wong, 1998</marker>
<rawString>Dekai Wu and Hongsing Wong. 1998. Machine translation with a stochastic grammatical channel. In Proceedings ofthe 36th Annual Meeting ofthe Association for Computational Linguistics, pages 1408–1414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2298" citStr="Yamada and Knight, 2001" startWordPosition="357" endWordPosition="360">of syntactic information. Several researchers have subsequently formulated models which incorporate the intuition that syntactically close constituents tend to stay close across languages. Below are descriptions of some of these different methods of integrating syntax. • Stochastic Inversion Transduction Grammars (Wu and Wong, 1998): This formalism uses a grammar for English and from it derives a possible grammar for the foreign language. This derivation includes adding productions where the order of the RHS is reversed from the ordering of the English. • Syntax-based Statistical Translation (Yamada and Knight, 2001): This model extends the above by allowing all possible permutations of the RHS of the English rules. • Statistical Phrase-based Translation (Koehn et al., 2003): Here “phrase-based” means “subsequence-based”, as there is no guarantee that the phrases learned by the model will have any relation to what we would think of as syntactic phrases. • Dependency-based Translation (ˇCmejrek et al., 2003): This model assumes a dependency parser for the foreign language. The syntactic structure and labels are preserved during translation. Transfer is purely lexical. A generator builds an English sentence</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zdenˇek ˇZabokrtsk´y</author>
<author>Petr Sgall</author>
<author>Saˇso Dˇzeroski</author>
</authors>
<title>Machine learning approach to automatic functor assignment in the Prague Dependency Treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of LREC 2002 (Third International Conference on Language Resources and Evaluation),</booktitle>
<volume>volume V,</volume>
<pages>1513--1520</pages>
<marker>ˇZabokrtsk´y, Sgall, Dˇzeroski, 2002</marker>
<rawString>Zdenˇek ˇZabokrtsk´y, Petr Sgall, and Saˇso Dˇzeroski. 2002. Machine learning approach to automatic functor assignment in the Prague Dependency Treebank. In Proceedings of LREC 2002 (Third International Conference on Language Resources and Evaluation), volume V, pages 1513–1520, Las Palmas de Gran Canaria, Spain.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>