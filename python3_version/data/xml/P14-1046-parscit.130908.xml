<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.9976915">
Interpretable Semantic Vectors from a Joint Model of Brain- and Text-
Based Meaning
</title>
<author confidence="0.991342">
Alona Fyshe&apos;, Partha P. Talukdar&apos;, Brian Murphy&apos;, Tom M. Mitchell&apos;
</author>
<affiliation confidence="0.997174">
&apos;Machine Learning Department, Carnegie Mellon University
&apos;School of Electronics, Electrical Engineering and Computer Science
Queen’s University Belfast
</affiliation>
<email confidence="0.992776">
[afyshe,partha.talukdar,tom.mitchell]@cs.cmu.edu
brian.murphy@qub.ac.uk
</email>
<sectionHeader confidence="0.994666" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999929409090909">
Vector space models (VSMs) represent
word meanings as points in a high dimen-
sional space. VSMs are typically created
using a large text corpora, and so repre-
sent word semantics as observed in text.
We present a new algorithm (JNNSE) that
can incorporate a measure of semantics
not previously used to create VSMs: brain
activation data recorded while people read
words. The resulting model takes advan-
tage of the complementary strengths and
weaknesses of corpus and brain activation
data to give a more complete representa-
tion of semantics. Evaluations show that
the model 1) matches a behavioral mea-
sure of semantics more closely, 2) can
be used to predict corpus data for unseen
words and 3) has predictive power that
generalizes across brain imaging technolo-
gies and across subjects. We believe that
the model is thus a more faithful represen-
tation of mental vocabularies.
</bodyText>
<sectionHeader confidence="0.998869" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999815381818182">
Vector Space Models (VSMs) represent lexical
meaning by assigning each word a point in high di-
mensional space. Beyond their use in NLP appli-
cations, they are of interest to cognitive scientists
as an objective and data-driven method to discover
word meanings (Landauer and Dumais, 1997).
Typically, VSMs are created by collecting word
usage statistics from large amounts of text data and
applying some dimensionality reduction technique
like Singular Value Decomposition (SVD). The
basic assumption is that semantics drives a per-
son’s language production behavior, and as a result
co-occurrence patterns in written text indirectly
encode word meaning. The raw co-occurrence
statistics are unwieldy, but in the compressed
VSM the distance between any two words is con-
ceived to represent their mutual semantic similar-
ity (Sahlgren, 2006; Turney and Pantel, 2010), as
perceived and judged by speakers. This space then
reflects the “semantic ground truth” of shared lex-
ical meanings in a language community’s vocab-
ulary. However corpus-based VSMs have been
criticized as being noisy or incomplete representa-
tions of meaning (Glenberg and Robertson, 2000).
For example, multiple word senses collide in the
same vector, and noise from mis-parsed sentences
or spam documents can interfere with the final se-
mantic representation.
When a person is reading or writing, the se-
mantic content of each word will be necessarily
activated in the mind, and so in patterns of ac-
tivity over individual neurons. In principle then,
brain activity could replace corpus data as input
to a VSM, and contemporary imaging techniques
allow us to attempt this. Functional Magnetic Res-
onance Imaging (fMRI) and Magnetoencephalog-
raphy (MEG) are two brain activation recording
technologies that measure neuronal activation in
aggregate, and have been shown to have a pre-
dictive relationship with models of word mean-
ing (Mitchell et al., 2008; Palatucci et al., 2009;
Sudre et al., 2012; Murphy et al., 2012b).1
If brain activation data encodes semantics, we
theorized that including brain data in a model of
semantics could result in a model more consistent
with semantic ground truth. However, the inclu-
sion of brain data will only improve a text-based
model if brain data contains semantic information
not readily available in the corpus. In addition,
if a semantic test involves another subject’s brain
activation data, performance can improve only if
the additional semantic information is consistent
across brains. Of course, brains differ in shape,
size and in connectivity, so additional information
encoded in one brain might not translate to an-
</bodyText>
<footnote confidence="0.966217">
1For more details on fMRI and MEG, see Section 4.2
</footnote>
<page confidence="0.95699">
489
</page>
<note confidence="0.8325535">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 489–499,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999845738095238">
other. Furthermore, different brain imaging tech-
nologies measure very different correlates of neu-
ronal activity. Due to these differences, it is possi-
ble that one subject’s brain activation data cannot
improve a model’s performance on another sub-
ject’s brain data, or for brain data collected using
a different recording technology. Indeed, inter-
subject models of brain activation is an open re-
search area (Conroy et al., 2013), as is learning the
relationship between recording technologies (En-
gell et al., 2012; Hall et al., 2013). Brain data
can also be corrupted by many types of noise (e.g.
recording room interference, movement artifacts),
another possible hindrance to the use of brain data
in VSMs.
VSMs are interesting from both engineering
and scientific standpoints. In this work we fo-
cus on the scientific question: Can the inclusion
of brain data improve semantic representations
learned from corpus data? What can we learn from
such a model? From an engineering perspective,
brain activation data will likely never replace text
data. Brain activation recordings are both expen-
sive and time consuming to collect, whereas tex-
tual data is vast and much of it is free to download.
However, from a scientific perspective, combining
text and brain data could lead to more consistent
semantic models, in turn leading to a better un-
derstanding of semantics and semantic modeling
generally.
In this paper, we leverage both kinds of data to
build a hybrid VSM using a new matrix factor-
ization method (JNNSE). Our hypothesis is that
the noise of brain and corpus derived statistics
will be largely orthogonal, and so the two data
sources will have complementary strengths as in-
put to VSMs. If this hypothesis is correct, we
should find that the resulting VSM is more suc-
cessful in modeling word semantics as encoded in
human judgements, as well as separate corpus and
brain data that was not used in the derivation of the
model. We will show that our method:
</bodyText>
<listItem confidence="0.963711583333333">
1. creates a VSM that is more correlated to an
independent measure of word semantics.
2. produces word vectors that are more pre-
dictable from the brain activity of different
people, even when brain data is collected
with a different recording technology.
3. predicts corpus representations of withheld
words more accurately than a model that does
not combine data sources.
4. directly maps semantic concepts onto the
brain by jointly learning neural representa-
tions.
</listItem>
<bodyText confidence="0.999957">
Together, these results suggest that corpus and
brain activation data measure semantics in com-
patible and complimentary ways. Our results
are evidence that a joint model of brain- and
text-based semantics may be closer to seman-
tic ground truth than text-only models. Our
findings also indicate that there is additional se-
mantic information available in brain activation
data that is not present in corpus data, and that
there are elements of semantics currently lack-
ing in text-based VSMs. We have made avail-
able the top performing VSMs created with brain
and text data (http://www.cs.cmu.edu/
˜afyshe/papers/acl2014/).
In the following sections we will review NNSE,
and our extension, JNNSE. We will describe the
data used and the experiments to support our posi-
tion that brain data is a valuable source of semantic
information that compliments text data.
</bodyText>
<sectionHeader confidence="0.933837" genericHeader="method">
2 Non-Negative Sparse Embedding
</sectionHeader>
<bodyText confidence="0.995903">
Non-Negative Sparse Embedding (NNSE) (Mur-
phy et al., 2012a) is an algorithm that produces
a latent representation using matrix factorization.
Standard NNSE begins with a matrix X E Rw×c
made of c corpus statistics for w words. NNSE
solves the following objective function:
</bodyText>
<equation confidence="0.9992084">
� � �
Xi,: − Ai,: x D 2 + λ� A 1
(1)
subject to: Di,:DTi,: &lt; 1, b 1 &lt; i &lt; E (2)
Ai,j &gt; 0, 1 &lt; i &lt; w, 1 &lt; j &lt; E (3)
</equation>
<bodyText confidence="0.999978428571428">
The solution will find a matrix A E Rw×` that is
sparse, non-negative, and represents word seman-
tics in an E-dimensional latent space. D E R`×c
gives the encoding of corpus statistics in the la-
tent space. Together, they factor the original cor-
pus statistics matrix X in a way that minimizes
the reconstruction error. The L1 constraint encour-
ages sparsity in A; λ is a hyperparameter. Equa-
tion 2 constrains D to eliminate solutions where
A is made arbitrarily small by making D arbi-
trarily large. Equation 3 ensures that A is non-
negative. We may increase E to give more dimen-
sional space to represent word semantics, or de-
crease E for more compact representations.
</bodyText>
<equation confidence="0.71242025">
argmin
A,D
Xw
i=1
</equation>
<page confidence="0.981518">
490
</page>
<bodyText confidence="0.999948222222222">
The sparse and non-negative representation in
A produces a more interpretable semantic space,
where interpretability is quantified with a behav-
ioral task (Chang et al., 2009; Murphy et al.,
2012a). To illustrate the interpretability of NNSE,
we describe a word by selecting the word’s top
scoring dimensions, and selecting the top scoring
words in those dimensions. For example, the word
chair has the following top scoring dimensions:
</bodyText>
<listItem confidence="0.999116666666667">
1. chairs, seating, couches;
2. mattress, futon, mattresses;
3. supervisor, coordinator, advisor.
</listItem>
<bodyText confidence="0.999780583333333">
These dimensions cover two of the distinct mean-
ings of the word chair (furniture and person of
power).
NNSE’s sparsity constraint dictates that each
word can have a non-zero score in only a few di-
mensions, which aligns well to previous feature
elicitation experiments in psychology. In feature
elicitation, participants are asked to name the char-
acteristics (features) of an object. The number of
characteristics named is usually small (McRae et
al., 2005), which supports the requirement of spar-
sity in the learned latent space.
</bodyText>
<sectionHeader confidence="0.975396" genericHeader="method">
3 Joint Non-Negative Sparse Embedding
</sectionHeader>
<bodyText confidence="0.99407704">
We extend NNSEs to incorporate an additional
source of data for a subset of the words in X,
and call the approach Joint Non-Negative Sparse
Embeddings (JNNSEs). The JNNSE algorithm
is general enough to incorporate any new infor-
mation about the a word w, but for this study
we will focus on brain activation recordings of
a human subject reading single words. We
will incorporate either fMRI or MEG data, and
call the resulting models JNNSE(fMRI+Text) and
JNNSE(MEG+Text) and refer to them generally
as JNNSE(Brain+Text). For clarity, from here
on, we will refer to NNSE as NNSE(Text), or
NNSE(Brain) depending on the single source of
input data used.
Let us order the rows of the corpus data X so
that the first 1... w0 rows have both corpus statis-
tics and brain activation recordings. Each brain
activation recording is a row in the brain data ma-
trix Y E Rw/×v where v is the number of features
derived from the recording. For MEG recordings,
v =sensors x time points= 306 x 150. For fMRI
v = grey-matter voxels =� 20, 000 depending on
the brain anatomy of each individual subject. The
new objective function is:
</bodyText>
<equation confidence="0.9728748">
I IXi,: − Ai,: x D(c)112+
� �
�Yi,: − Ai,: x D(b)� 2 + λ� A 1
(4)
subject to: D(c)
i,: D(c)T &lt; 1, b 1 &lt; i &lt; ` (5)
( ( — — —
D(:b)D(b)T: &lt;1,b 1 &lt; i &lt; ` (6)
Ai,j &gt; 0, 1 &lt; i &lt; w, 1 &lt; j &lt; `
(7)
</equation>
<bodyText confidence="0.982648611111111">
We have introduced an additional constraint on the
rows 1... w0, requiring that some of the learned
representations in A also reconstruct the brain ac-
tivation recordings (Y ) through representations in
D(b) E R`×v. Let us use A0 to refer to the brain-
constrained rows of A. Words that are close in
“brain space” must have similar representations in
A0, which can further percolate to affect the rep-
resentations of other words in A via closeness in
“corpus space”.
With A or D fixed, the objective function for
NNSE(Text) and JNNSE(Brain+Text) is convex.
However, we are solving for A and D, so the prob-
lem is non-convex. To solve for this objective, we
use the online algorithm of Section 3 from Mairal
et al. (Mairal et al., 2010). This algorithm is
guaranteed to converge, and in practice we found
that JNNSE(Brain+Text) converged as quickly as
NNSE(Text) for the same `. We used the SPAMS
package2 to solve, and set λ = 0.025. This al-
gorithm was a very easy extension to NNSE(Text)
and required very little additional tuning.
We also consider learning shared representa-
tions in the case where data X and Y contain the
effects of known disjoint features. For example,
when a person reads a word, the recorded brain
activation data Y will contain the physiological
response to viewing the stimulus, which is unre-
lated to the semantics of the word. These sig-
nals can be attributed to, for example, the num-
ber of letters in the word and the number of white
pixels on the screen (Sudre et al., 2012). To ac-
count for such effects in the data, we augment
A0 with a set of n fixed, manually defined fea-
tures (e.g. word length) to create A0percept E
Rw×(`+n). D(b) E R(`+n)×v is used with A0percept,
</bodyText>
<footnote confidence="0.769867">
2SPAMS Package: http://spams-devel.gforge.inria.fr/
</footnote>
<equation confidence="0.959867">
argmin
A,D(c),D(b)
Xw
i=1
w/
X
i=1
</equation>
<page confidence="0.956776">
491
</page>
<bodyText confidence="0.9999805">
to reconstruct the brain data Y . More gener-
ally, one could instead allocate a certain num-
ber of latent features specific to X or Y, both of
which could be learned, as explored in some re-
lated work (Gupta et al., 2013). We use 11 per-
ceptual features that characterize the non-semantic
features of the word stimulus (for a list, see sup-
plementary material at http://www.cs.cmu.
edu/-afyshe/papers/acl2014/).
The JNNSE algorithm is advantageous in that
it can handle partially paired data. That is, the
algorithm does not require that every row in X
also have a row in Y . Fully paired data is a re-
quirement of many other approaches (White et al.,
2012; Jia and Darrell, 2010). Our approach al-
lows us to leverage the semantic information in
corpus data even for words without brain activa-
tion recordings.
JNNSE(Brain+Text) does not require brain data
to be mapped to a common average brain, which
is often the case when one wants to generalize be-
tween human subjects. Such mappings can blur
and distort data, making it less useful for subse-
quent prediction steps. We avoid these mappings,
and instead use the fact that similar words elicit
similar brain activation within a subject. In the
JNNSE algorithm, it is this closeness in “brain
space” that guides the creation of the latent space
A. Leveraging intra-subject distance measures
to study inter-subject encodings has been studied
previously (Kriegeskorte et al., 2008a; Raizada
and Connolly, 2012), and has even been used
across species (humans and primates) (Kriegesko-
rte et al., 2008b).
Though we restrict ourselves to using one sub-
ject per JNNSE(Brain+Text) model, the JNNSE
algorithm could easily be extended to include
data from multiple brain imaging experiments by
adding a new squared loss term for additional
brain data.
</bodyText>
<subsectionHeader confidence="0.869259">
3.1 Related Work
</subsectionHeader>
<bodyText confidence="0.999974117647059">
Perhaps the most well known related approach
to joining data sources is Canonical Correlation
Analysis (CCA) (Hotelling, 1936), which has been
applied to brain activation data in the past (Rus-
tandi et al., 2009). CCA seeks two linear trans-
formations that maximally correlate two data sets
in the transformed form. CCA requires that the
data sources be paired (all rows in the corpus data
must have a corresponding brain data), as corre-
lation between points is integral to the objective.
To apply CCA to our data we would need to dis-
card the vast majority of our corpus data, and use
only the 60 rows of X with corresponding rows
in Y. While CCA holds the input data fixed and
maximally correlates the transformed form, we
hold the transformed form fixed and seek a solu-
tion that maximally correlates the reconstruction
(AD(c) or A&apos;D(b)) with the data (X and Y respec-
tively). This shift in error compensation is what
allows our data to be only partially paired. While
a Bayesian formulation of CCA can handle miss-
ing data, our model has missing data for &gt; 97% of
the full w × (v + c) brain and corpus data matrix.
To our knowledge, this extreme amount of missing
data has not been explored with Bayesian CCA.
One could also use a topic model style formula-
tion to represent this semantic representation task.
Supervised topic models (Blei and McAuliffe,
2007) use a latent topic to generate two observed
outputs: words in a document and a categorical la-
bel for the document. The same idea could be ap-
plied here: the latent semantic representation gen-
erates the observed brain activity and corpus statis-
tics. Generative and discriminative models both
have their own strengths and weaknesses, gener-
ative models being particularly strong when data
sources are limited (Ng and Jordan, 2002). Our
task is an interesting blend of data-limited and
data-rich problem scenarios.
In the past, various pieces of additional informa-
tion have been incorporated into semantic models.
For example, models with behavioral data (Sil-
berer and Lapata, 2012) and models with visual
information (Bruni et al., 2011; Silberer et al.,
2013) have both shown to improve semantic rep-
resentations. Other works have correlated VSMs
built with text or images with brain activation
data (Murphy et al., 2012b; Anderson et al., 2013).
To our knowledge, this work is the first to integrate
brain activation data into the construction of the
VSM.
</bodyText>
<sectionHeader confidence="0.999598" genericHeader="method">
4 Data
</sectionHeader>
<subsectionHeader confidence="0.999737">
4.1 Corpus Data
</subsectionHeader>
<bodyText confidence="0.999759333333333">
The corpus statistics used here are the download-
able vectors from Fyshe et al. (2013)3. They
are compiled from a 16 billion word subset of
ClueWeb09 (Callan and Hoy, 2009) and contain
two types of corpus features: dependency and doc-
ument features, found to be complimentary for
</bodyText>
<footnote confidence="0.9981065">
3http://www.cs.cmu.edu/-afyshe/papers/
conll2013/
</footnote>
<page confidence="0.995508">
492
</page>
<bodyText confidence="0.999744833333333">
most tasks. Dependency statistics were derived
by dependency parsing the corpus and compil-
ing counts for all dependencies incident on the
word. Document statistics are word-document
co-occurrence counts. Count thresholding was
applied to reduce noise, and positive pointwise-
mutual-information (PPMI) (Church and Hanks,
1990) was applied to the counts. SVD was ap-
plied to the document and dependency statistics
and the top 1000 dimensions of each type were
retained. We selected the rows corresponding to
noun-tagged words (approx. 17000 words).
</bodyText>
<subsectionHeader confidence="0.995849">
4.2 Brain Activation Data
</subsectionHeader>
<bodyText confidence="0.999990333333333">
We have MEG and fMRI data at our disposal.
MEG measures the magnetic field caused by many
thousands of neurons firing together, and has good
time resolution (1000 Hz) but poor spatial reso-
lution. fMRI measures the change in blood oxy-
genation that results from differential neural ac-
tivity, and has good spatial resolution but poor
time resolution (0.5-1 Hz). We have fMRI data
and MEG data for 18 subjects (9 in each imaging
modality) viewing 60 concrete nouns (Mitchell et
al., 2008; Sudre et al., 2012). The 60 words span
12 word categories (animals, buildings, tools, in-
sects, body parts, furniture, building parts, uten-
sils, vehicles, objects, clothing, food). Each of the
60 words was presented with a line drawing, so
word ambiguity is not an issue. For both record-
ing modalities, all trials for a particular word were
averaged together to create one training instance
per word, with 60 training instances in all for each
subject and imaging modality. More preprocess-
ing details appear in the supplementary material.
</bodyText>
<sectionHeader confidence="0.996485" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999773">
Here we explore several variations of JNNSE and
NNSE formulations. For a comparison of the
models used, see Table 1.
</bodyText>
<subsectionHeader confidence="0.997947">
5.1 Correlation to Behavioral Data
</subsectionHeader>
<bodyText confidence="0.999986959183674">
To test if our joint model of Brain+Text is closer
to semantic ground truth we compared the latent
representation A learned via JNNSE(Brain+Text)
or NNSE(Text) to an independent behavioral mea-
sure of semantics. We collected behavioral data
for the 60 nouns in the form of answers to 218
semantic questions. Answers were gathered with
Mechanical Turk. The full list of questions ap-
pear in the supplementary material. Some exam-
ple questions are:“Is it alive?”, and “Can it bend?”.
Mechanical Turk users were asked to respond to
each question for each word on a scale of 1-5. At
least 3 respondents answered each question and
the median score was used. This gives us a se-
mantic representation of each of the 60 words in
a 218-dimensional behavioral space. Because we
required answers to each of the questions for all
words, we do not have the problems of sparsity
that exist for feature production norms from other
studies (McRae et al., 2005). In addition, our an-
swers are ratings, rather than binary yes/no an-
swers.
For a given value of E we solve the NNSE(Text)
and JNNSE(Brain+Text) objective function as de-
tailed in Equation 1 and 4 respectively. We com-
pared JNNSE(Brain+Text) and NNSE(Text) mod-
els by measuring the correlation of all pairwise
distances in JNNSE(Brain+Text) and NNSE(Text)
space to the pairwise distances in the 218-
dimensional semantic space. Distances were
calculated using normalized Euclidean distance
(equivalent in rank-ordering to cosine distance,
but more suitable for sparse vectors). Figure 1
shows the results of this correlation test. The er-
ror bars for the JNNSE(Brain+Text) models rep-
resent a 95% confidence interval calculated using
the standard error of the mean (SEM) over the 9
person-specific JNNSE(Brain+Text) models. Be-
cause there is only one NNSE(Text) model for
each dimension setting, no SEM can be calculated,
but it suffices to show that the NNSE(Text) corre-
lation does not fall into the 95% confidence inter-
val of the JNNSE(Brain+Text) models. The SVD
matrix for the original corpus data has correlation
0.4279 to the behavioral data, also below the 95%
confidence interval for all JNNSE models. The re-
sults show that a model that incorporates brain ac-
tivation data is more faithful to a behavioral mea-
sure of semantics.
</bodyText>
<subsectionHeader confidence="0.997843">
5.2 Word Prediction from Brain Activation
</subsectionHeader>
<bodyText confidence="0.996859333333333">
We now show that the JNNSE(Brain+Text) vec-
tors are more consistent with independent sam-
ples of brain activity collected from different sub-
jects, even when recorded using different record-
ing technologies. As previously mentioned, be-
cause there is a large degree of variation between
brains and because MEG and fMRI measure very
different correlates of neuronal activity, this type
of generalization has proven to be very challeng-
ing and is an open research question in the neuro-
science community.
The output A of the JNNSE(Brain+Text) or
</bodyText>
<page confidence="0.999447">
493
</page>
<tableCaption confidence="0.997558">
Table 1: A Comparison of the models explored in this paper, and the data upon which they operate.
</tableCaption>
<table confidence="0.953634">
Model Name Section(s) Text Data Brain Data Withheld Data
NNSE(Text) 2, 5 ✓ x -
NNSE(Brain) 2, 5.2.1, 5.3 x ✓ -
JNNSE(Brain+Text) 3, 5 ✓ ✓ -
JNNSE(Brain+Text): Dropout task 5.2.2 ✓ ✓ subset of brain data
JNNSE(Brain+Text): Predict corpus 5.3 ✓ ✓ subset of text data
Correlation of Semantic Question Distances to JNNSE(fMRI)
Number of Latent Dimensions
</table>
<figureCaption confidence="0.9598875">
Figure 1: Correlation of JNNSE(Brain+Text) and
NNSE(Text) models with the distances in a se-
mantic space constructed from behavioral data.
Error bars indicate SEM.
</figureCaption>
<bodyText confidence="0.983751703125">
NNSE(Text) algorithm can be used as a VSM,
which we use for the task of word prediction from
fMRI or MEG recordings. A JNNSE(Brain+Text)
created with a particular human subject’s data is
never used in the prediction framework with that
same subject. For example, if we use fMRI data
from subject 1 to create a JNNSE(fMRI+Text), we
will test it with the remaining 8 fMRI subjects, but
all 9 MEG subjects (fMRI and MEG subjects are
disjoint).
Let us call the VSM learned with
JNNSE(Brain+Text) or NNSE(Text) the se-
mantic vectors. We can train a weight matrix W
that predicts the semantic vector a of a word from
that word’s brain activation vector x: a = Wx.
W can be learned with a variety of methods, we
will use L2 regularized regression. One can also
train regressors that predict the brain activation
data from the semantic vector: x = Wa, but we
have found this to give lower predictive accuracy.
Note that we must re-train our weight matrix W
for each subject (instead of re-using D(b) from
Equation 4) because testing always occurs on a
different subject, and the brain activation data is
not inter-subject aligned.
We train ` independent L2 regularized regres-
sors to predict the `-dimensional vectors a =
{a1 ... atI. The predictions are concatenated
to produce a predicted semantic vector: aˆ =
{ˆa1, . . ., ˆat}. We assess word prediction perfor-
mance by testing if the model can differentiate be-
tween two unseen words, a task named 2 vs. 2 pre-
diction (Mitchell et al., 2008; Sudre et al., 2012).
We choose the assignment of the two held out se-
mantic vectors (a(1), a(2)) to predicted semantic
vectors (ˆa(1), ˆa(2)) that minimizes the sum of the
two normalized Euclidean distances. 2 vs. 2 ac-
curacy is the percentage of tests where the correct
assignment is chosen.
The 60 nouns fall into 12 word categories.
Words in the same word category (e.g. screw-
driver and hammer) are closer in semantic space
than words in different word categories, which
makes some 2 vs. 2 tests more difficult than oth-
ers. We choose 150 random pairs of words (with
each word represented equally) to estimate the dif-
ficulty of a typical word pair, without having to
20) word pairs. The same 150 random
test all (
pairs are used for all subjects and all VSMs. Ex-
pected chance performance on the 2 vs. 2 test is
50%.
Results for testing on fMRI data in the
2 vs. 2 framework appear in Figure 2.
JNNSE(fMRI+Text) data performed on aver-
age 6% better than the best NNSE(Text), and
exceeding even the original SVD corpus represen-
tations while maintaining interpretability. These
results generalize across brain activity recording
types; JNNSE(MEG+Text) performs as well as
JNNSE(fMRI+Text) when tested on fMRI data.
The results are consistent when testing on MEG
data: JNNSE(MEG+Text) or JNNSE(fMRI+Text)
outperforms NNSE(Text) (see Figure 3).
</bodyText>
<figure confidence="0.99843675">
250 500 1000
Correlation
0.48
0.46
0.44
0.42
0.5
0.4
JNNSE(fMRI+Text)
JNNSE(MEG+Text)
NNSE(Text)
SVD(Text)
</figure>
<page confidence="0.715172">
494
</page>
<figure confidence="0.8251205">
2 vs. 2 Acc. for JNNSE and NNSE, tested on fMRI data
Number of Latent Dimensions
</figure>
<figureCaption confidence="0.83472475">
Figure 2: Average 2 vs. 2 accuracy for
NNSE(Text) and JNNSE(Brain+Text), tested on
fMRI data. Models created with one subject’s
fMRI data were not used to compute 2 vs. 2 ac-
</figureCaption>
<bodyText confidence="0.991372125">
curacy for that same subject.
NNSE(Text) performance decreases as the
number of latent dimension increases. This im-
plies that without the regularizing effect of brain
activation data, the extra NNSE(Text) dimensions
are being used to overfit to the corpus data, or
possibly to fit semantic properties not detectable
with current brain imaging technologies. How-
ever, when brain activation data is included, in-
creasing the number of latent dimensions strictly
increases performance for JNNSE(fMRI+Text).
JNNSE(MEG+Text) has peak performance with
500 latent dimensions, with ∼ 1% decrease in
performance at 1000 latent dimensions. In previ-
ous work, the ability to decode words from brain
activation data was found to improve with added
latent dimensions (Murphy et al., 2012a). Our
results may differ because our words are POS
tagged, and we included only nouns for the final
NNSE(Text) model. We found that with the orig-
inal A = 0.05 setting from Murphy et al. (Mur-
phy et al., 2012a) produced vectors that were too
sparse; four of the 60 test words had all-zero vec-
tors (JNNSE(Brain+Text) models did have any all-
zero vectors). To improve the NNSE(Text) vectors
for a fair comparison, we reduced A = 0.025, un-
der which NNSE(Text) did not produce any all-
zero vectors for the 60 words.
Our results show that brain activation data con-
tributes additional information, which leads to an
increase in performance for the task of word pre-
diction from brain activation data. This suggests
</bodyText>
<figure confidence="0.456632">
2 vs. 2 Acc. for JNNSE and NNSE, tested on MEG data
Number of Latent Dimensions
</figure>
<figureCaption confidence="0.5858175">
Figure 3: Average 2 vs. 2 accuracy for
NNSE(Text) and JNNSE(Brain+Text), tested on
MEG data. Models created with one subject’s
MEG data were not used to compute 2 vs. 2 ac-
</figureCaption>
<bodyText confidence="0.976351714285714">
curacy for that same subject.
that corpus-only models may not capture all rel-
evant semantic information. This conflicts with
previous studies which found that semantic vec-
tors culled from corpus statistics contain all of the
semantic information required to predict brain ac-
tivation (Bullinaria and Levy, 2013).
</bodyText>
<subsubsectionHeader confidence="0.522665">
5.2.1 Prediction from a Brain-only Model
</subsubsectionHeader>
<bodyText confidence="0.999902428571429">
How much predictive power does the corpus data
provide to this word prediction task? To test
this, we calculated the 2 vs. 2 accuracy for a
NNSE(Brain) model trained on brain activation
data only. We train NNSE(Brain) with one sub-
ject’s data and use the resulting vectors to calculate
2 vs. 2 accuracy for the remaining subjects. We
have brain data for only 60 words, so using E ≥ 60
latent dimensions leads to an under-constrained
system and a degenerate solution wherein only one
latent dimension is active for any word (and where
the brain data can be perfectly reconstructed). The
degenerate solution makes it impossible to gen-
eralize across words and leads to performance at
chance levels. An NNSE(MEG) trained on MEG
data gave maximum 2 vs. 2 accuracy of 67% when
E = 20. The reduced performance may be due to
the limited training data and the low SNR of the
data, but could also be attributed to the lack of cor-
pus information, which provides another piece of
semantic information.
</bodyText>
<figure confidence="0.996172444444445">
250 500 1000
2 vs. 2 Accuracy
74
72
70
68
66
64
JNNSE(fMRI+Text)
JNNSE(MEG+Text)
NNSE(Text)
SVD(Text)
250 500 1000
2 vs. 2 Accuracy
82
80
78
76
74
72
70
68
66
JNNSE(fMRI+Text)
JNNSE(MEG+Text)
NNSE(Text)
SVD(Text)
</figure>
<page confidence="0.993767">
495
</page>
<subsectionHeader confidence="0.678833">
5.2.2 Effect on Rows Without Brain Data
</subsectionHeader>
<bodyText confidence="0.999984206896552">
It is possible that some JNNSE(Brain+Text) di-
mensions are being used exclusively to fit brain
activation data, and not the semantics represented
in both brain and corpus data. If a particular
dimension j is solely used for brain data, the
sparsity constraint will favor solutions that sets
A(Zj) = 0 for i &gt; w&apos; (no brain data constraint),
and A(Zj) &gt; 0 for some 0 &lt; i &lt; w&apos; (brain data
constrained). We found that there were no such
dimensions in the JNNSE(Brain+Text). In fact for
the E = 1000 JNNSE(Brain+Text), all latent di-
mensions had greater than ∼ 25% non-zero en-
tries, which implies that all dimensions are being
shared between the two data inputs (corpus and
brain activation), and are used to reconstruct both.
To test that the brain activation data is truly in-
fluencing rows of A not constrained by brain acti-
vation data, we performed a dropout test. We split
the original 60 words into two 30 word groups (as
evenly as possible across word categories). We
trained JNNSE(fMRI+Text) with 30 words, and
tested word prediction with the remaining 8 sub-
jects and the other 30 words. Thus, the training
and testing word sets are disjoint. Because of the
reduced size of the training data, we did see a drop
in performance, but JNNSE(fMRI+Text) vectors
still gave word prediction performance 7% higher
than NNSE(Text) vectors. Full results appear in
the supplementary material.
</bodyText>
<subsectionHeader confidence="0.999479">
5.3 Predicting Corpus Data
</subsectionHeader>
<bodyText confidence="0.989861576923077">
Here we ask: can an accurate latent representa-
tion of a word be constructed using only brain
activation data? This task simulates the scenario
where there is no reliable corpus representation of
a word, but brain data is available. This scenario
may occur for seldom-used words that fall below
the thresholds used for the compilation of corpus
statistics. It could also be useful for acronym to-
kens (lol, omg) found in social media contexts
where the meaning of the token is actually a full
sentence.
We trained a JNNSE(fMRI+Text) with brain
data for all 60 words, but withhold the corpus data
for 30 of the 60 words (as evenly distributed as
possible amongst the 12 word categories). The
brain activation data for the 30 withheld words
will allow us to create latent representations in
A for withheld words. Simultaneously, we will
learn a mapping from the latent representation to
the corpus data (D(&apos;)). This task cannot be per-
Table 2: Mean rank accuracy over 30 words
using corpus representations predicted by a
JNNSE(MEG+Text) model trained with some
rows of the corpus data withheld. Significance
is calculated using Fisher’s method to combine p-
values for each of the subject-dependent models.
</bodyText>
<table confidence="0.902564">
Latent Dim size Rank Accuracy p-value
250 65.30 &lt; 10−19
500 67.37 &lt; 10−24
1000 63.47 &lt; 10−15
</table>
<bodyText confidence="0.99738228125">
formed with a NNSE(Text) model because one
cannot learn a latent representation of a word with-
out data of some kind. This further emphasizes the
impact of brain imaging data, which will allow us
to generalize to previously unseen words in corpus
space.
We use the latent representations in A for each
of the words without corpus data and the mapping
to corpus space D(&apos;) to predict the withheld cor-
pus data in X. We then rank the withheld rows of
X by their distance to the predicted row of X and
calculate the mean rank accuracy of the held out
words. Results in Table 2 show that we can recre-
ate the withheld corpus data using brain activation
data. Peak mean rank accuracy (67.37) is attained
at E = 500 latent dimensions. This result shows
that neural semantic representations can create a
latent representation that is faithful to unseen cor-
pus statistics, providing further evidence that the
two data sources share a strong common element.
How much power is the remaining corpus data
supplying in scenarios where we withhold cor-
pus data? To answer this question, we trained an
NNSE(Brain) model on 30 words of brain activa-
tion, and then trained a regressor to predict cor-
pus data from those latent brain-only representa-
tions. We use the trained regressor to predict the
corpus data for the remaining 30 words. Peak per-
formance is attained at E = 10 latent dimensions,
giving mean rank accuracy of 62.37, significantly
worse than the model that includes both corpus
and brain activation data (67.37).
</bodyText>
<subsectionHeader confidence="0.996369">
5.4 Mapping Semantics onto the Brain
</subsectionHeader>
<bodyText confidence="0.999935833333333">
Because our method incorporates brain data into
an interpretable semantic model, we can directly
map semantic concepts onto the brain. To do
this, we examined the mappings from the latent
space to the brain space via D(b). We found that
the most interpretable mappings come from mod-
</bodyText>
<page confidence="0.995456">
496
</page>
<figure confidence="0.321438">
Fusiform
(a) D(b) matrix, subject P3, dimension with top words bath-
</figure>
<reference confidence="0.350846666666667">
room, balcony, kitchen. MNI coordinates z=-12 (left) and z=-18
(right). Fusiform is associated with shelter words.
(b) D(b) matrix; subject P1; dimension with top words ankle,
elbow, knee. MNI coordinates z=60 (left) and z=54 (right). Pre-
and post-central areas are activated for body part words.
(c) D(b) matrix; subject P1; dimension with top scoring words
buffet, brunch, lunch. MNI coordinates z=30 (left) and z=24
(right). Pars opercularis is believed to be part of the gustatory
cortex, which responds to food related words.
</reference>
<figureCaption confidence="0.727608">
Figure 4: The mappings (D(b)) from latent se-
</figureCaption>
<bodyText confidence="0.987617590909091">
mantic space (A) to brain space (Y ) for fMRI and
words from three semantic categories. Shown are
representations of the fMRI slices such that the
back of the head is at the top of the image, the
front of the head is at the bottom.
els where the perceptual features had been scaled
down (divided by a constant factor), which en-
courages more of the data to be explained by
the semantic features in A. Figure 4 shows the
mappings (D(b)) for dimensions related to shel-
ter, food and body parts. The red areas align
with areas of the brain previously known to be
activated by the corresponding concepts (Mitchell
et al., 2008; Just et al., 2010). Our model
has learned these mappings in an unsupervised
setting by relating semantic knowledge gleaned
from word usage to patterns of activation in the
brain. This illustrates how the interpretability of
JNNSE can allow one to explore semantics in
the human brain. The mappings for one subject
are available for download (http://www.cs.
cmu.edu/˜afyshe/papers/acl2014/).
</bodyText>
<sectionHeader confidence="0.991453" genericHeader="discussions">
6 Future Work and Conclusion
</sectionHeader>
<bodyText confidence="0.999988461538462">
We are interested in pursuing many future projects
inspired by the success of this model. We would
like to extend the JNNSE algorithm to incorporate
data from multiple subjects, multiple modalities
and multiple experiments with non-overlapping
words. Including behavioral data and image data
is another possibility.
We have explored a model of semantics that in-
corporates text and brain activation data. Though
the number of words for which we have brain acti-
vation data is comparatively small, we have shown
that including even this small amount of data has
a positive impact on the learned latent representa-
tions, including for words without brain data. We
have provided evidence that the latent representa-
tions are closer to the neural representation of se-
mantics, and possibly, closer to semantic ground
truth. Our results reveal that there are aspects of
semantics not currently represented in text-based
VSMs, indicating that there may be room for im-
provement in either the data or algorithms used to
create VSMs. Our findings also indicate that using
the brain as a semantic test can separate models
that capture this additional semantic information
from those that do not. Thus, the brain is an im-
portant source of both training and testing data.
</bodyText>
<sectionHeader confidence="0.99756" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999532666666667">
This work was supported in part by NIH un-
der award 5R01HD075328-02, by DARPA under
award FA8750-13-2-0005, and by a fellowship to
Alona Fyshe from the Multimodal Neuroimag-
ing Training Program funded by NIH awards
T90DA022761 and R90DA023420.
</bodyText>
<sectionHeader confidence="0.998461" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9984231">
Andrew J Anderson, Elia Bruni, Ulisse Bordignon,
Massimo Poesio, and Marco Baroni. 2013. Of
words , eyes and brains : Correlating image-based
distributional semantic models with neural represen-
tations of concepts. In Proceedings of the Confer-
ence on Empirical Methods on Natural Language
Processing.
David M Blei and Jon D. McAuliffe. 2007. Supervised
topic models. In Advances in Neural Information
Processing Systems, pages 1–22.
</reference>
<figure confidence="0.967249">
Postcentral
Precentral
Pars
Opercularis
</figure>
<page confidence="0.982812">
497
</page>
<reference confidence="0.997092899082569">
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. In
Proceedings of the EMNLP 2011 Geometrical Mod-
els for Natural Language Semantics (GEMS).
John A Bullinaria and Joseph P Levy. 2013. Limiting
factors for mapping corpus-based semantic repre-
sentations to brain activity. PloS one, 8(3):e57191,
January.
Jamie Callan and Mark Hoy. 2009. The ClueWeb09
Dataset.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David M Blei. 2009. Reading
Tea Leaves : How Humans Interpret Topic Models.
In Advances in Neural Information Processing Sys-
tems, pages 1–9.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22–29.
Bryan R Conroy, Benjamin D Singer, J Swaroop Gun-
tupalli, Peter J Ramadge, and James V Haxby. 2013.
Inter-subject alignment of human cortical anatomy
using functional connectivity. NeuroImage, 81:400–
11, November.
Andrew D Engell, Scott Huettel, and Gregory Mc-
Carthy. 2012. The fMRI BOLD signal tracks elec-
trophysiological spectral perturbations, not event-
related potentials. NeuroImage, 59(3):2600–6,
February.
Alona Fyshe, Partha Talukdar, Brian Murphy, and Tom
Mitchell. 2013. Documents and Dependencies : an
Exploration of Vector Space Models for Semantic
Composition. In Computational Natural Language
Learning, Sofia, Bulgaria.
Arthur M Glenberg and David a Robertson. 2000.
Symbol Grounding and Meaning: A Compari-
son of High-Dimensional and Embodied Theories
of Meaning. Journal of Memory and Language,
43(3):379–401, October.
Sunil Kumar Gupta, Dinh Phung, Brett Adams, and
Svetha Venkatesh. 2013. Regularized nonnegative
shared subspace learning. Data Mining and Knowl-
edge Discovery, 26(1):57–97.
Emma L Hall, Siˆan E Robson, Peter G Morris, and
Matthew J Brookes. 2013. The relationship be-
tween MEG and fMRI. NeuroImage, November.
Harold Hotelling. 1936. Relations between two sets of
variates. Biometrika, 28(3/4):321–377.
Yangqing Jia and Trevor Darrell. 2010. Factorized La-
tent Spaces with Structured Sparsity. In Advances in
Neural Information Processing Systems, volume 23.
Marcel Adam Just, Vladimir L Cherkassky, Sandesh
Aryal, and Tom M Mitchell. 2010. A neuroseman-
tic theory of concrete noun representation based on
the underlying brain codes. PloS one, 5(1):e8622,
January.
Nikolaus Kriegeskorte, Marieke Mur, and Peter Ban-
dettini. 2008a. Representational similarity analysis
- connecting the branches of systems neuroscience.
Frontiers in systems neuroscience, 2(November):4,
January.
Nikolaus Kriegeskorte, Marieke Mur, Douglas A Ruff,
Roozbeh Kiani, Jerzy Bodurka, Hossein Esteky,
Keiji Tanaka, and Peter A Bandettin. 2008b. Match-
ing Categorical Object Representations in Inferior
Temporal Cortex of Man and Monkey. Neuron,
60(6):1126–1141.
TK Landauer and ST Dumais. 1997. A solution to
Plato’s problem: The latent semantic analysis the-
ory of acquisition, induction, and representation of
knowledge. Psychological review, 1(2):211–240.
Julien Mairal, Francis Bach, J Ponce, and Guillermo
Sapiro. 2010. Online learning for matrix factor-
ization and sparse coding. The Journal of Machine
Learning Research, 11:19–60.
Ken McRae, George S Cree, Mark S Seidenberg, and
Chris McNorgan. 2005. Semantic feature produc-
tion norms for a large set of living and nonliving
things. Behavior research methods, 37(4):547–59,
November.
Tom M Mitchell, Svetlana V Shinkareva, Andrew Carl-
son, Kai-Min Chang, Vicente L Malave, Robert A
Mason, and Marcel Adam Just. 2008. Pre-
dicting human brain activity associated with the
meanings of nouns. Science (New York, N.Y.),
320(5880):1191–5, May.
Brian Murphy, Partha Talukdar, and Tom Mitchell.
2012a. Learning Effective and Interpretable Se-
mantic Models using Non-Negative Sparse Embed-
ding. In Proceedings of Conference on Computa-
tional Linguistics (COLING).
Brian Murphy, Partha Talukdar, and Tom Mitchell.
2012b. Selecting Corpus-Semantic Models for Neu-
rolinguistic Decoding. In First Joint Conference
on Lexical and Computational Semantics (*SEM),
pages 114–123, Montreal, Quebec, Canada.
Andrew Y. Ng and Michael I. Jordan. 2002. On dis-
criminative vs. generative classifiers: A compari-
son of logistic regression and naive bayes. In Ad-
vances in neural information processing systems,
volume 14.
Mark Palatucci, Geoffrey Hinton, Dean Pomerleau,
and Tom M Mitchell. 2009. Zero-Shot Learning
with Semantic Output Codes. Advances in Neural
Information Processing Systems, 22:1410–1418.
Rajeev D S Raizada and Andrew C Connolly. 2012.
What Makes Different People’s Representations
Alike : Neural Similarity Space Solves the Problem
of Across-subject fMRI Decoding. Journal of Cog-
nitive Neuroscience, 24(4):868–877.
</reference>
<page confidence="0.985178">
498
</page>
<reference confidence="0.99791834375">
Indrayana Rustandi, Marcel Adam Just, and Tom M
Mitchell. 2009. Integrating Multiple-Study
Multiple-Subject fMRI Datasets Using Canonical
Correlation Analysis. In MICCAI 2009 Workshop:
Statistical modeling and detection issues in intra-
and inter-subject functional MRI data analysis.
Magnus Sahlgren. 2006. The Word-Space Model Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words. Doctor
of philosophy, Stockholm University.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1423–1433.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of Semantic Representation with Vi-
sual Attributes. In Association for Computational
Linguistics 2013, Sofia, Bulgaria.
Gustavo Sudre, Dean Pomerleau, Mark Palatucci, Leila
Wehbe, Alona Fyshe, Riitta Salmelin, and Tom
Mitchell. 2012. Tracking Neural Coding of Per-
ceptual and Semantic Features of Concrete Nouns.
NeuroImage, 62(1):463–451, May.
Peter D Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning : Vector Space Models of Se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.
Martha White, Yaoliang Yu, Xinhua Zhang, and Dale
Schuurmans. 2012. Convex multi-view subspace
learning. In Advances in Neural Information Pro-
cessing Systems, pages 1–14.
</reference>
<page confidence="0.99918">
499
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.595421">
<title confidence="0.9982015">Semantic Vectors from a Joint Model of Brainand Based Meaning</title>
<author confidence="0.989795">Partha P Brian Tom M</author>
<affiliation confidence="0.858749">Learning Department, Carnegie Mellon of Electronics, Electrical Engineering and Computer Queen’s University</affiliation>
<email confidence="0.994311">brian.murphy@qub.ac.uk</email>
<abstract confidence="0.999001260869565">Vector space models (VSMs) represent word meanings as points in a high dimensional space. VSMs are typically created using a large text corpora, and so represent word semantics as observed in text. We present a new algorithm (JNNSE) that can incorporate a measure of semantics not previously used to create VSMs: brain activation data recorded while people read words. The resulting model takes advantage of the complementary strengths and weaknesses of corpus and brain activation data to give a more complete representation of semantics. Evaluations show that the model 1) matches a behavioral measure of semantics more closely, 2) can be used to predict corpus data for unseen words and 3) has predictive power that generalizes across brain imaging technologies and across subjects. We believe that the model is thus a more faithful representation of mental vocabularies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>balcony room</author>
<author>kitchen</author>
</authors>
<title>MNI coordinates z=-12 (left) and z=-18 (right). Fusiform is associated with shelter words.</title>
<marker>room, kitchen, </marker>
<rawString>room, balcony, kitchen. MNI coordinates z=-12 (left) and z=-18 (right). Fusiform is associated with shelter words.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D</author>
</authors>
<title>matrix; subject P1; dimension with top words ankle, elbow, knee. MNI coordinates z=60 (left) and z=54 (right). Preand post-central areas are activated for body part words. (c) D(b) matrix; subject P1; dimension with top scoring words buffet, brunch, lunch. MNI coordinates z=30 (left) and z=24 (right). Pars opercularis is believed to be part of the gustatory cortex, which responds to food related words.</title>
<marker>D, </marker>
<rawString>(b) D(b) matrix; subject P1; dimension with top words ankle, elbow, knee. MNI coordinates z=60 (left) and z=54 (right). Preand post-central areas are activated for body part words. (c) D(b) matrix; subject P1; dimension with top scoring words buffet, brunch, lunch. MNI coordinates z=30 (left) and z=24 (right). Pars opercularis is believed to be part of the gustatory cortex, which responds to food related words.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew J Anderson</author>
<author>Elia Bruni</author>
<author>Ulisse Bordignon</author>
<author>Massimo Poesio</author>
<author>Marco Baroni</author>
</authors>
<title>Of words , eyes and brains : Correlating image-based distributional semantic models with neural representations of concepts.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods on Natural Language Processing.</booktitle>
<contexts>
<context position="16827" citStr="Anderson et al., 2013" startWordPosition="2795" endWordPosition="2798">weaknesses, generative models being particularly strong when data sources are limited (Ng and Jordan, 2002). Our task is an interesting blend of data-limited and data-rich problem scenarios. In the past, various pieces of additional information have been incorporated into semantic models. For example, models with behavioral data (Silberer and Lapata, 2012) and models with visual information (Bruni et al., 2011; Silberer et al., 2013) have both shown to improve semantic representations. Other works have correlated VSMs built with text or images with brain activation data (Murphy et al., 2012b; Anderson et al., 2013). To our knowledge, this work is the first to integrate brain activation data into the construction of the VSM. 4 Data 4.1 Corpus Data The corpus statistics used here are the downloadable vectors from Fyshe et al. (2013)3. They are compiled from a 16 billion word subset of ClueWeb09 (Callan and Hoy, 2009) and contain two types of corpus features: dependency and document features, found to be complimentary for 3http://www.cs.cmu.edu/-afyshe/papers/ conll2013/ 492 most tasks. Dependency statistics were derived by dependency parsing the corpus and compiling counts for all dependencies incident on</context>
</contexts>
<marker>Anderson, Bruni, Bordignon, Poesio, Baroni, 2013</marker>
<rawString>Andrew J Anderson, Elia Bruni, Ulisse Bordignon, Massimo Poesio, and Marco Baroni. 2013. Of words , eyes and brains : Correlating image-based distributional semantic models with neural representations of concepts. In Proceedings of the Conference on Empirical Methods on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Jon D McAuliffe</author>
</authors>
<title>Supervised topic models.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>1--22</pages>
<contexts>
<context position="15885" citStr="Blei and McAuliffe, 2007" startWordPosition="2645" endWordPosition="2648">ransformed form fixed and seek a solution that maximally correlates the reconstruction (AD(c) or A&apos;D(b)) with the data (X and Y respectively). This shift in error compensation is what allows our data to be only partially paired. While a Bayesian formulation of CCA can handle missing data, our model has missing data for &gt; 97% of the full w × (v + c) brain and corpus data matrix. To our knowledge, this extreme amount of missing data has not been explored with Bayesian CCA. One could also use a topic model style formulation to represent this semantic representation task. Supervised topic models (Blei and McAuliffe, 2007) use a latent topic to generate two observed outputs: words in a document and a categorical label for the document. The same idea could be applied here: the latent semantic representation generates the observed brain activity and corpus statistics. Generative and discriminative models both have their own strengths and weaknesses, generative models being particularly strong when data sources are limited (Ng and Jordan, 2002). Our task is an interesting blend of data-limited and data-rich problem scenarios. In the past, various pieces of additional information have been incorporated into semanti</context>
</contexts>
<marker>Blei, McAuliffe, 2007</marker>
<rawString>David M Blei and Jon D. McAuliffe. 2007. Supervised topic models. In Advances in Neural Information Processing Systems, pages 1–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Giang Binh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Distributional semantics from text and images.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP</booktitle>
<contexts>
<context position="16618" citStr="Bruni et al., 2011" startWordPosition="2761" endWordPosition="2764"> The same idea could be applied here: the latent semantic representation generates the observed brain activity and corpus statistics. Generative and discriminative models both have their own strengths and weaknesses, generative models being particularly strong when data sources are limited (Ng and Jordan, 2002). Our task is an interesting blend of data-limited and data-rich problem scenarios. In the past, various pieces of additional information have been incorporated into semantic models. For example, models with behavioral data (Silberer and Lapata, 2012) and models with visual information (Bruni et al., 2011; Silberer et al., 2013) have both shown to improve semantic representations. Other works have correlated VSMs built with text or images with brain activation data (Murphy et al., 2012b; Anderson et al., 2013). To our knowledge, this work is the first to integrate brain activation data into the construction of the VSM. 4 Data 4.1 Corpus Data The corpus statistics used here are the downloadable vectors from Fyshe et al. (2013)3. They are compiled from a 16 billion word subset of ClueWeb09 (Callan and Hoy, 2009) and contain two types of corpus features: dependency and document features, found to</context>
</contexts>
<marker>Bruni, Tran, Baroni, 2011</marker>
<rawString>Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011. Distributional semantics from text and images. In Proceedings of the EMNLP 2011 Geometrical Models for Natural Language Semantics (GEMS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bullinaria</author>
<author>Joseph P Levy</author>
</authors>
<title>Limiting factors for mapping corpus-based semantic representations to brain activity.</title>
<date>2013</date>
<journal>PloS one,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="27752" citStr="Bullinaria and Levy, 2013" startWordPosition="4598" endWordPosition="4601">ask of word prediction from brain activation data. This suggests 2 vs. 2 Acc. for JNNSE and NNSE, tested on MEG data Number of Latent Dimensions Figure 3: Average 2 vs. 2 accuracy for NNSE(Text) and JNNSE(Brain+Text), tested on MEG data. Models created with one subject’s MEG data were not used to compute 2 vs. 2 accuracy for that same subject. that corpus-only models may not capture all relevant semantic information. This conflicts with previous studies which found that semantic vectors culled from corpus statistics contain all of the semantic information required to predict brain activation (Bullinaria and Levy, 2013). 5.2.1 Prediction from a Brain-only Model How much predictive power does the corpus data provide to this word prediction task? To test this, we calculated the 2 vs. 2 accuracy for a NNSE(Brain) model trained on brain activation data only. We train NNSE(Brain) with one subject’s data and use the resulting vectors to calculate 2 vs. 2 accuracy for the remaining subjects. We have brain data for only 60 words, so using E ≥ 60 latent dimensions leads to an under-constrained system and a degenerate solution wherein only one latent dimension is active for any word (and where the brain data can be pe</context>
</contexts>
<marker>Bullinaria, Levy, 2013</marker>
<rawString>John A Bullinaria and Joseph P Levy. 2013. Limiting factors for mapping corpus-based semantic representations to brain activity. PloS one, 8(3):e57191, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jamie Callan</author>
<author>Mark Hoy</author>
</authors>
<date>2009</date>
<booktitle>The ClueWeb09 Dataset.</booktitle>
<contexts>
<context position="17133" citStr="Callan and Hoy, 2009" startWordPosition="2849" endWordPosition="2852">ls with behavioral data (Silberer and Lapata, 2012) and models with visual information (Bruni et al., 2011; Silberer et al., 2013) have both shown to improve semantic representations. Other works have correlated VSMs built with text or images with brain activation data (Murphy et al., 2012b; Anderson et al., 2013). To our knowledge, this work is the first to integrate brain activation data into the construction of the VSM. 4 Data 4.1 Corpus Data The corpus statistics used here are the downloadable vectors from Fyshe et al. (2013)3. They are compiled from a 16 billion word subset of ClueWeb09 (Callan and Hoy, 2009) and contain two types of corpus features: dependency and document features, found to be complimentary for 3http://www.cs.cmu.edu/-afyshe/papers/ conll2013/ 492 most tasks. Dependency statistics were derived by dependency parsing the corpus and compiling counts for all dependencies incident on the word. Document statistics are word-document co-occurrence counts. Count thresholding was applied to reduce noise, and positive pointwisemutual-information (PPMI) (Church and Hanks, 1990) was applied to the counts. SVD was applied to the document and dependency statistics and the top 1000 dimensions o</context>
</contexts>
<marker>Callan, Hoy, 2009</marker>
<rawString>Jamie Callan and Mark Hoy. 2009. The ClueWeb09 Dataset.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>Jordan Boyd-Graber</author>
<author>Sean Gerrish</author>
<author>Chong Wang</author>
<author>David M Blei</author>
</authors>
<title>Reading Tea Leaves : How Humans Interpret Topic Models.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="8710" citStr="Chang et al., 2009" startWordPosition="1398" endWordPosition="1401">orpus statistics matrix X in a way that minimizes the reconstruction error. The L1 constraint encourages sparsity in A; λ is a hyperparameter. Equation 2 constrains D to eliminate solutions where A is made arbitrarily small by making D arbitrarily large. Equation 3 ensures that A is nonnegative. We may increase E to give more dimensional space to represent word semantics, or decrease E for more compact representations. argmin A,D Xw i=1 490 The sparse and non-negative representation in A produces a more interpretable semantic space, where interpretability is quantified with a behavioral task (Chang et al., 2009; Murphy et al., 2012a). To illustrate the interpretability of NNSE, we describe a word by selecting the word’s top scoring dimensions, and selecting the top scoring words in those dimensions. For example, the word chair has the following top scoring dimensions: 1. chairs, seating, couches; 2. mattress, futon, mattresses; 3. supervisor, coordinator, advisor. These dimensions cover two of the distinct meanings of the word chair (furniture and person of power). NNSE’s sparsity constraint dictates that each word can have a non-zero score in only a few dimensions, which aligns well to previous fea</context>
</contexts>
<marker>Chang, Boyd-Graber, Gerrish, Wang, Blei, 2009</marker>
<rawString>Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish, Chong Wang, and David M Blei. 2009. Reading Tea Leaves : How Humans Interpret Topic Models. In Advances in Neural Information Processing Systems, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<booktitle>Computational linguistics,</booktitle>
<pages>16--1</pages>
<contexts>
<context position="17618" citStr="Church and Hanks, 1990" startWordPosition="2913" endWordPosition="2916">re the downloadable vectors from Fyshe et al. (2013)3. They are compiled from a 16 billion word subset of ClueWeb09 (Callan and Hoy, 2009) and contain two types of corpus features: dependency and document features, found to be complimentary for 3http://www.cs.cmu.edu/-afyshe/papers/ conll2013/ 492 most tasks. Dependency statistics were derived by dependency parsing the corpus and compiling counts for all dependencies incident on the word. Document statistics are word-document co-occurrence counts. Count thresholding was applied to reduce noise, and positive pointwisemutual-information (PPMI) (Church and Hanks, 1990) was applied to the counts. SVD was applied to the document and dependency statistics and the top 1000 dimensions of each type were retained. We selected the rows corresponding to noun-tagged words (approx. 17000 words). 4.2 Brain Activation Data We have MEG and fMRI data at our disposal. MEG measures the magnetic field caused by many thousands of neurons firing together, and has good time resolution (1000 Hz) but poor spatial resolution. fMRI measures the change in blood oxygenation that results from differential neural activity, and has good spatial resolution but poor time resolution (0.5-1</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryan R Conroy</author>
<author>Benjamin D Singer</author>
<author>J Swaroop Guntupalli</author>
<author>Peter J Ramadge</author>
<author>James V Haxby</author>
</authors>
<title>Inter-subject alignment of human cortical anatomy using functional connectivity.</title>
<date>2013</date>
<journal>NeuroImage,</journal>
<volume>81</volume>
<pages>11</pages>
<contexts>
<context position="4572" citStr="Conroy et al., 2013" startWordPosition="697" endWordPosition="700">ceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 489–499, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics other. Furthermore, different brain imaging technologies measure very different correlates of neuronal activity. Due to these differences, it is possible that one subject’s brain activation data cannot improve a model’s performance on another subject’s brain data, or for brain data collected using a different recording technology. Indeed, intersubject models of brain activation is an open research area (Conroy et al., 2013), as is learning the relationship between recording technologies (Engell et al., 2012; Hall et al., 2013). Brain data can also be corrupted by many types of noise (e.g. recording room interference, movement artifacts), another possible hindrance to the use of brain data in VSMs. VSMs are interesting from both engineering and scientific standpoints. In this work we focus on the scientific question: Can the inclusion of brain data improve semantic representations learned from corpus data? What can we learn from such a model? From an engineering perspective, brain activation data will likely neve</context>
</contexts>
<marker>Conroy, Singer, Guntupalli, Ramadge, Haxby, 2013</marker>
<rawString>Bryan R Conroy, Benjamin D Singer, J Swaroop Guntupalli, Peter J Ramadge, and James V Haxby. 2013. Inter-subject alignment of human cortical anatomy using functional connectivity. NeuroImage, 81:400– 11, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew D Engell</author>
<author>Scott Huettel</author>
<author>Gregory McCarthy</author>
</authors>
<title>The fMRI BOLD signal tracks electrophysiological spectral perturbations, not eventrelated potentials.</title>
<date>2012</date>
<journal>NeuroImage,</journal>
<volume>59</volume>
<issue>3</issue>
<contexts>
<context position="4657" citStr="Engell et al., 2012" startWordPosition="709" endWordPosition="713">pages 489–499, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics other. Furthermore, different brain imaging technologies measure very different correlates of neuronal activity. Due to these differences, it is possible that one subject’s brain activation data cannot improve a model’s performance on another subject’s brain data, or for brain data collected using a different recording technology. Indeed, intersubject models of brain activation is an open research area (Conroy et al., 2013), as is learning the relationship between recording technologies (Engell et al., 2012; Hall et al., 2013). Brain data can also be corrupted by many types of noise (e.g. recording room interference, movement artifacts), another possible hindrance to the use of brain data in VSMs. VSMs are interesting from both engineering and scientific standpoints. In this work we focus on the scientific question: Can the inclusion of brain data improve semantic representations learned from corpus data? What can we learn from such a model? From an engineering perspective, brain activation data will likely never replace text data. Brain activation recordings are both expensive and time consumin</context>
</contexts>
<marker>Engell, Huettel, McCarthy, 2012</marker>
<rawString>Andrew D Engell, Scott Huettel, and Gregory McCarthy. 2012. The fMRI BOLD signal tracks electrophysiological spectral perturbations, not eventrelated potentials. NeuroImage, 59(3):2600–6, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alona Fyshe</author>
<author>Partha Talukdar</author>
<author>Brian Murphy</author>
<author>Tom Mitchell</author>
</authors>
<title>Documents and Dependencies : an Exploration of Vector Space Models for Semantic Composition.</title>
<date>2013</date>
<booktitle>In Computational Natural Language Learning,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="17047" citStr="Fyshe et al. (2013)" startWordPosition="2834" endWordPosition="2837">dditional information have been incorporated into semantic models. For example, models with behavioral data (Silberer and Lapata, 2012) and models with visual information (Bruni et al., 2011; Silberer et al., 2013) have both shown to improve semantic representations. Other works have correlated VSMs built with text or images with brain activation data (Murphy et al., 2012b; Anderson et al., 2013). To our knowledge, this work is the first to integrate brain activation data into the construction of the VSM. 4 Data 4.1 Corpus Data The corpus statistics used here are the downloadable vectors from Fyshe et al. (2013)3. They are compiled from a 16 billion word subset of ClueWeb09 (Callan and Hoy, 2009) and contain two types of corpus features: dependency and document features, found to be complimentary for 3http://www.cs.cmu.edu/-afyshe/papers/ conll2013/ 492 most tasks. Dependency statistics were derived by dependency parsing the corpus and compiling counts for all dependencies incident on the word. Document statistics are word-document co-occurrence counts. Count thresholding was applied to reduce noise, and positive pointwisemutual-information (PPMI) (Church and Hanks, 1990) was applied to the counts. S</context>
</contexts>
<marker>Fyshe, Talukdar, Murphy, Mitchell, 2013</marker>
<rawString>Alona Fyshe, Partha Talukdar, Brian Murphy, and Tom Mitchell. 2013. Documents and Dependencies : an Exploration of Vector Space Models for Semantic Composition. In Computational Natural Language Learning, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur M Glenberg</author>
<author>David a Robertson</author>
</authors>
<title>Symbol Grounding and Meaning: A Comparison of High-Dimensional and Embodied Theories of Meaning.</title>
<date>2000</date>
<journal>Journal of Memory and Language,</journal>
<volume>43</volume>
<issue>3</issue>
<contexts>
<context position="2422" citStr="Glenberg and Robertson, 2000" startWordPosition="360" endWordPosition="363">es a person’s language production behavior, and as a result co-occurrence patterns in written text indirectly encode word meaning. The raw co-occurrence statistics are unwieldy, but in the compressed VSM the distance between any two words is conceived to represent their mutual semantic similarity (Sahlgren, 2006; Turney and Pantel, 2010), as perceived and judged by speakers. This space then reflects the “semantic ground truth” of shared lexical meanings in a language community’s vocabulary. However corpus-based VSMs have been criticized as being noisy or incomplete representations of meaning (Glenberg and Robertson, 2000). For example, multiple word senses collide in the same vector, and noise from mis-parsed sentences or spam documents can interfere with the final semantic representation. When a person is reading or writing, the semantic content of each word will be necessarily activated in the mind, and so in patterns of activity over individual neurons. In principle then, brain activity could replace corpus data as input to a VSM, and contemporary imaging techniques allow us to attempt this. Functional Magnetic Resonance Imaging (fMRI) and Magnetoencephalography (MEG) are two brain activation recording tech</context>
</contexts>
<marker>Glenberg, Robertson, 2000</marker>
<rawString>Arthur M Glenberg and David a Robertson. 2000. Symbol Grounding and Meaning: A Comparison of High-Dimensional and Embodied Theories of Meaning. Journal of Memory and Language, 43(3):379–401, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunil Kumar Gupta</author>
<author>Dinh Phung</author>
<author>Brett Adams</author>
<author>Svetha Venkatesh</author>
</authors>
<title>Regularized nonnegative shared subspace learning.</title>
<date>2013</date>
<journal>Data Mining and Knowledge Discovery,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="12945" citStr="Gupta et al., 2013" startWordPosition="2151" endWordPosition="2154"> for example, the number of letters in the word and the number of white pixels on the screen (Sudre et al., 2012). To account for such effects in the data, we augment A0 with a set of n fixed, manually defined features (e.g. word length) to create A0percept E Rw×(`+n). D(b) E R(`+n)×v is used with A0percept, 2SPAMS Package: http://spams-devel.gforge.inria.fr/ argmin A,D(c),D(b) Xw i=1 w/ X i=1 491 to reconstruct the brain data Y . More generally, one could instead allocate a certain number of latent features specific to X or Y, both of which could be learned, as explored in some related work (Gupta et al., 2013). We use 11 perceptual features that characterize the non-semantic features of the word stimulus (for a list, see supplementary material at http://www.cs.cmu. edu/-afyshe/papers/acl2014/). The JNNSE algorithm is advantageous in that it can handle partially paired data. That is, the algorithm does not require that every row in X also have a row in Y . Fully paired data is a requirement of many other approaches (White et al., 2012; Jia and Darrell, 2010). Our approach allows us to leverage the semantic information in corpus data even for words without brain activation recordings. JNNSE(Brain+Tex</context>
</contexts>
<marker>Gupta, Phung, Adams, Venkatesh, 2013</marker>
<rawString>Sunil Kumar Gupta, Dinh Phung, Brett Adams, and Svetha Venkatesh. 2013. Regularized nonnegative shared subspace learning. Data Mining and Knowledge Discovery, 26(1):57–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emma L Hall</author>
<author>Siˆan E Robson</author>
<author>Peter G Morris</author>
<author>Matthew J Brookes</author>
</authors>
<date>2013</date>
<booktitle>The relationship between MEG and fMRI. NeuroImage,</booktitle>
<contexts>
<context position="4677" citStr="Hall et al., 2013" startWordPosition="714" endWordPosition="717">ore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics other. Furthermore, different brain imaging technologies measure very different correlates of neuronal activity. Due to these differences, it is possible that one subject’s brain activation data cannot improve a model’s performance on another subject’s brain data, or for brain data collected using a different recording technology. Indeed, intersubject models of brain activation is an open research area (Conroy et al., 2013), as is learning the relationship between recording technologies (Engell et al., 2012; Hall et al., 2013). Brain data can also be corrupted by many types of noise (e.g. recording room interference, movement artifacts), another possible hindrance to the use of brain data in VSMs. VSMs are interesting from both engineering and scientific standpoints. In this work we focus on the scientific question: Can the inclusion of brain data improve semantic representations learned from corpus data? What can we learn from such a model? From an engineering perspective, brain activation data will likely never replace text data. Brain activation recordings are both expensive and time consuming to collect, wherea</context>
</contexts>
<marker>Hall, Robson, Morris, Brookes, 2013</marker>
<rawString>Emma L Hall, Siˆan E Robson, Peter G Morris, and Matthew J Brookes. 2013. The relationship between MEG and fMRI. NeuroImage, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Hotelling</author>
</authors>
<title>Relations between two sets of variates.</title>
<date>1936</date>
<journal>Biometrika,</journal>
<pages>28--3</pages>
<contexts>
<context position="14654" citStr="Hotelling, 1936" startWordPosition="2427" endWordPosition="2428">ject distance measures to study inter-subject encodings has been studied previously (Kriegeskorte et al., 2008a; Raizada and Connolly, 2012), and has even been used across species (humans and primates) (Kriegeskorte et al., 2008b). Though we restrict ourselves to using one subject per JNNSE(Brain+Text) model, the JNNSE algorithm could easily be extended to include data from multiple brain imaging experiments by adding a new squared loss term for additional brain data. 3.1 Related Work Perhaps the most well known related approach to joining data sources is Canonical Correlation Analysis (CCA) (Hotelling, 1936), which has been applied to brain activation data in the past (Rustandi et al., 2009). CCA seeks two linear transformations that maximally correlate two data sets in the transformed form. CCA requires that the data sources be paired (all rows in the corpus data must have a corresponding brain data), as correlation between points is integral to the objective. To apply CCA to our data we would need to discard the vast majority of our corpus data, and use only the 60 rows of X with corresponding rows in Y. While CCA holds the input data fixed and maximally correlates the transformed form, we hold</context>
</contexts>
<marker>Hotelling, 1936</marker>
<rawString>Harold Hotelling. 1936. Relations between two sets of variates. Biometrika, 28(3/4):321–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yangqing Jia</author>
<author>Trevor Darrell</author>
</authors>
<title>Factorized Latent Spaces with Structured Sparsity.</title>
<date>2010</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<volume>23</volume>
<contexts>
<context position="13401" citStr="Jia and Darrell, 2010" startWordPosition="2227" endWordPosition="2230">e could instead allocate a certain number of latent features specific to X or Y, both of which could be learned, as explored in some related work (Gupta et al., 2013). We use 11 perceptual features that characterize the non-semantic features of the word stimulus (for a list, see supplementary material at http://www.cs.cmu. edu/-afyshe/papers/acl2014/). The JNNSE algorithm is advantageous in that it can handle partially paired data. That is, the algorithm does not require that every row in X also have a row in Y . Fully paired data is a requirement of many other approaches (White et al., 2012; Jia and Darrell, 2010). Our approach allows us to leverage the semantic information in corpus data even for words without brain activation recordings. JNNSE(Brain+Text) does not require brain data to be mapped to a common average brain, which is often the case when one wants to generalize between human subjects. Such mappings can blur and distort data, making it less useful for subsequent prediction steps. We avoid these mappings, and instead use the fact that similar words elicit similar brain activation within a subject. In the JNNSE algorithm, it is this closeness in “brain space” that guides the creation of the</context>
</contexts>
<marker>Jia, Darrell, 2010</marker>
<rawString>Yangqing Jia and Trevor Darrell. 2010. Factorized Latent Spaces with Structured Sparsity. In Advances in Neural Information Processing Systems, volume 23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcel Adam Just</author>
<author>Vladimir L Cherkassky</author>
<author>Sandesh Aryal</author>
<author>Tom M Mitchell</author>
</authors>
<title>A neurosemantic theory of concrete noun representation based on the underlying brain codes.</title>
<date>2010</date>
<journal>PloS one,</journal>
<volume>5</volume>
<issue>1</issue>
<marker>Just, Cherkassky, Aryal, Mitchell, 2010</marker>
<rawString>Marcel Adam Just, Vladimir L Cherkassky, Sandesh Aryal, and Tom M Mitchell. 2010. A neurosemantic theory of concrete noun representation based on the underlying brain codes. PloS one, 5(1):e8622, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikolaus Kriegeskorte</author>
<author>Marieke Mur</author>
<author>Peter Bandettini</author>
</authors>
<title>Representational similarity analysis - connecting the branches of systems neuroscience. Frontiers in systems neuroscience,</title>
<date>2008</date>
<pages>2--4</pages>
<contexts>
<context position="14148" citStr="Kriegeskorte et al., 2008" startWordPosition="2347" endWordPosition="2350">dings. JNNSE(Brain+Text) does not require brain data to be mapped to a common average brain, which is often the case when one wants to generalize between human subjects. Such mappings can blur and distort data, making it less useful for subsequent prediction steps. We avoid these mappings, and instead use the fact that similar words elicit similar brain activation within a subject. In the JNNSE algorithm, it is this closeness in “brain space” that guides the creation of the latent space A. Leveraging intra-subject distance measures to study inter-subject encodings has been studied previously (Kriegeskorte et al., 2008a; Raizada and Connolly, 2012), and has even been used across species (humans and primates) (Kriegeskorte et al., 2008b). Though we restrict ourselves to using one subject per JNNSE(Brain+Text) model, the JNNSE algorithm could easily be extended to include data from multiple brain imaging experiments by adding a new squared loss term for additional brain data. 3.1 Related Work Perhaps the most well known related approach to joining data sources is Canonical Correlation Analysis (CCA) (Hotelling, 1936), which has been applied to brain activation data in the past (Rustandi et al., 2009). CCA see</context>
</contexts>
<marker>Kriegeskorte, Mur, Bandettini, 2008</marker>
<rawString>Nikolaus Kriegeskorte, Marieke Mur, and Peter Bandettini. 2008a. Representational similarity analysis - connecting the branches of systems neuroscience. Frontiers in systems neuroscience, 2(November):4, January.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Nikolaus Kriegeskorte</author>
</authors>
<title>Marieke Mur, Douglas A Ruff, Roozbeh Kiani, Jerzy Bodurka, Hossein Esteky, Keiji Tanaka, and Peter A Bandettin.</title>
<booktitle>2008b. Matching Categorical Object Representations in Inferior Temporal Cortex of Man and Monkey. Neuron,</booktitle>
<pages>60--6</pages>
<marker>Kriegeskorte, </marker>
<rawString>Nikolaus Kriegeskorte, Marieke Mur, Douglas A Ruff, Roozbeh Kiani, Jerzy Bodurka, Hossein Esteky, Keiji Tanaka, and Peter A Bandettin. 2008b. Matching Categorical Object Representations in Inferior Temporal Cortex of Man and Monkey. Neuron, 60(6):1126–1141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TK Landauer</author>
<author>ST Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological review,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="1558" citStr="Landauer and Dumais, 1997" startWordPosition="231" endWordPosition="234">s show that the model 1) matches a behavioral measure of semantics more closely, 2) can be used to predict corpus data for unseen words and 3) has predictive power that generalizes across brain imaging technologies and across subjects. We believe that the model is thus a more faithful representation of mental vocabularies. 1 Introduction Vector Space Models (VSMs) represent lexical meaning by assigning each word a point in high dimensional space. Beyond their use in NLP applications, they are of interest to cognitive scientists as an objective and data-driven method to discover word meanings (Landauer and Dumais, 1997). Typically, VSMs are created by collecting word usage statistics from large amounts of text data and applying some dimensionality reduction technique like Singular Value Decomposition (SVD). The basic assumption is that semantics drives a person’s language production behavior, and as a result co-occurrence patterns in written text indirectly encode word meaning. The raw co-occurrence statistics are unwieldy, but in the compressed VSM the distance between any two words is conceived to represent their mutual semantic similarity (Sahlgren, 2006; Turney and Pantel, 2010), as perceived and judged </context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>TK Landauer and ST Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review, 1(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julien Mairal</author>
<author>Francis Bach</author>
<author>J Ponce</author>
<author>Guillermo Sapiro</author>
</authors>
<title>Online learning for matrix factorization and sparse coding.</title>
<date>2010</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>11--19</pages>
<contexts>
<context position="11673" citStr="Mairal et al., 2010" startWordPosition="1925" endWordPosition="1928">tions in A also reconstruct the brain activation recordings (Y ) through representations in D(b) E R`×v. Let us use A0 to refer to the brainconstrained rows of A. Words that are close in “brain space” must have similar representations in A0, which can further percolate to affect the representations of other words in A via closeness in “corpus space”. With A or D fixed, the objective function for NNSE(Text) and JNNSE(Brain+Text) is convex. However, we are solving for A and D, so the problem is non-convex. To solve for this objective, we use the online algorithm of Section 3 from Mairal et al. (Mairal et al., 2010). This algorithm is guaranteed to converge, and in practice we found that JNNSE(Brain+Text) converged as quickly as NNSE(Text) for the same `. We used the SPAMS package2 to solve, and set λ = 0.025. This algorithm was a very easy extension to NNSE(Text) and required very little additional tuning. We also consider learning shared representations in the case where data X and Y contain the effects of known disjoint features. For example, when a person reads a word, the recorded brain activation data Y will contain the physiological response to viewing the stimulus, which is unrelated to the seman</context>
</contexts>
<marker>Mairal, Bach, Ponce, Sapiro, 2010</marker>
<rawString>Julien Mairal, Francis Bach, J Ponce, and Guillermo Sapiro. 2010. Online learning for matrix factorization and sparse coding. The Journal of Machine Learning Research, 11:19–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken McRae</author>
<author>George S Cree</author>
<author>Mark S Seidenberg</author>
<author>Chris McNorgan</author>
</authors>
<title>Semantic feature production norms for a large set of living and nonliving things. Behavior research methods,</title>
<date>2005</date>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="9527" citStr="McRae et al., 2005" startWordPosition="1524" endWordPosition="1527">xample, the word chair has the following top scoring dimensions: 1. chairs, seating, couches; 2. mattress, futon, mattresses; 3. supervisor, coordinator, advisor. These dimensions cover two of the distinct meanings of the word chair (furniture and person of power). NNSE’s sparsity constraint dictates that each word can have a non-zero score in only a few dimensions, which aligns well to previous feature elicitation experiments in psychology. In feature elicitation, participants are asked to name the characteristics (features) of an object. The number of characteristics named is usually small (McRae et al., 2005), which supports the requirement of sparsity in the learned latent space. 3 Joint Non-Negative Sparse Embedding We extend NNSEs to incorporate an additional source of data for a subset of the words in X, and call the approach Joint Non-Negative Sparse Embeddings (JNNSEs). The JNNSE algorithm is general enough to incorporate any new information about the a word w, but for this study we will focus on brain activation recordings of a human subject reading single words. We will incorporate either fMRI or MEG data, and call the resulting models JNNSE(fMRI+Text) and JNNSE(MEG+Text) and refer to them</context>
<context position="20003" citStr="McRae et al., 2005" startWordPosition="3311" endWordPosition="3314">athered with Mechanical Turk. The full list of questions appear in the supplementary material. Some example questions are:“Is it alive?”, and “Can it bend?”. Mechanical Turk users were asked to respond to each question for each word on a scale of 1-5. At least 3 respondents answered each question and the median score was used. This gives us a semantic representation of each of the 60 words in a 218-dimensional behavioral space. Because we required answers to each of the questions for all words, we do not have the problems of sparsity that exist for feature production norms from other studies (McRae et al., 2005). In addition, our answers are ratings, rather than binary yes/no answers. For a given value of E we solve the NNSE(Text) and JNNSE(Brain+Text) objective function as detailed in Equation 1 and 4 respectively. We compared JNNSE(Brain+Text) and NNSE(Text) models by measuring the correlation of all pairwise distances in JNNSE(Brain+Text) and NNSE(Text) space to the pairwise distances in the 218- dimensional semantic space. Distances were calculated using normalized Euclidean distance (equivalent in rank-ordering to cosine distance, but more suitable for sparse vectors). Figure 1 shows the results</context>
</contexts>
<marker>McRae, Cree, Seidenberg, McNorgan, 2005</marker>
<rawString>Ken McRae, George S Cree, Mark S Seidenberg, and Chris McNorgan. 2005. Semantic feature production norms for a large set of living and nonliving things. Behavior research methods, 37(4):547–59, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom M Mitchell</author>
<author>Svetlana V Shinkareva</author>
<author>Andrew Carlson</author>
<author>Kai-Min Chang</author>
<author>Vicente L Malave</author>
<author>Robert A Mason</author>
<author>Marcel Adam Just</author>
</authors>
<title>Predicting human brain activity associated with the meanings of nouns.</title>
<date>2008</date>
<publisher>Science</publisher>
<location>(New York, N.Y.), 320(5880):1191–5,</location>
<contexts>
<context position="3182" citStr="Mitchell et al., 2008" startWordPosition="482" endWordPosition="485">e final semantic representation. When a person is reading or writing, the semantic content of each word will be necessarily activated in the mind, and so in patterns of activity over individual neurons. In principle then, brain activity could replace corpus data as input to a VSM, and contemporary imaging techniques allow us to attempt this. Functional Magnetic Resonance Imaging (fMRI) and Magnetoencephalography (MEG) are two brain activation recording technologies that measure neuronal activation in aggregate, and have been shown to have a predictive relationship with models of word meaning (Mitchell et al., 2008; Palatucci et al., 2009; Sudre et al., 2012; Murphy et al., 2012b).1 If brain activation data encodes semantics, we theorized that including brain data in a model of semantics could result in a model more consistent with semantic ground truth. However, the inclusion of brain data will only improve a text-based model if brain data contains semantic information not readily available in the corpus. In addition, if a semantic test involves another subject’s brain activation data, performance can improve only if the additional semantic information is consistent across brains. Of course, brains dif</context>
<context position="18348" citStr="Mitchell et al., 2008" startWordPosition="3036" endWordPosition="3039">ions of each type were retained. We selected the rows corresponding to noun-tagged words (approx. 17000 words). 4.2 Brain Activation Data We have MEG and fMRI data at our disposal. MEG measures the magnetic field caused by many thousands of neurons firing together, and has good time resolution (1000 Hz) but poor spatial resolution. fMRI measures the change in blood oxygenation that results from differential neural activity, and has good spatial resolution but poor time resolution (0.5-1 Hz). We have fMRI data and MEG data for 18 subjects (9 in each imaging modality) viewing 60 concrete nouns (Mitchell et al., 2008; Sudre et al., 2012). The 60 words span 12 word categories (animals, buildings, tools, insects, body parts, furniture, building parts, utensils, vehicles, objects, clothing, food). Each of the 60 words was presented with a line drawing, so word ambiguity is not an issue. For both recording modalities, all trials for a particular word were averaged together to create one training instance per word, with 60 training instances in all for each subject and imaging modality. More preprocessing details appear in the supplementary material. 5 Experimental Results Here we explore several variations of</context>
<context position="24008" citStr="Mitchell et al., 2008" startWordPosition="3978" endWordPosition="3981">this to give lower predictive accuracy. Note that we must re-train our weight matrix W for each subject (instead of re-using D(b) from Equation 4) because testing always occurs on a different subject, and the brain activation data is not inter-subject aligned. We train ` independent L2 regularized regressors to predict the `-dimensional vectors a = {a1 ... atI. The predictions are concatenated to produce a predicted semantic vector: aˆ = {ˆa1, . . ., ˆat}. We assess word prediction performance by testing if the model can differentiate between two unseen words, a task named 2 vs. 2 prediction (Mitchell et al., 2008; Sudre et al., 2012). We choose the assignment of the two held out semantic vectors (a(1), a(2)) to predicted semantic vectors (ˆa(1), ˆa(2)) that minimizes the sum of the two normalized Euclidean distances. 2 vs. 2 accuracy is the percentage of tests where the correct assignment is chosen. The 60 nouns fall into 12 word categories. Words in the same word category (e.g. screwdriver and hammer) are closer in semantic space than words in different word categories, which makes some 2 vs. 2 tests more difficult than others. We choose 150 random pairs of words (with each word represented equally) </context>
</contexts>
<marker>Mitchell, Shinkareva, Carlson, Chang, Malave, Mason, Just, 2008</marker>
<rawString>Tom M Mitchell, Svetlana V Shinkareva, Andrew Carlson, Kai-Min Chang, Vicente L Malave, Robert A Mason, and Marcel Adam Just. 2008. Predicting human brain activity associated with the meanings of nouns. Science (New York, N.Y.), 320(5880):1191–5, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Murphy</author>
<author>Partha Talukdar</author>
<author>Tom Mitchell</author>
</authors>
<title>Learning Effective and Interpretable Semantic Models using Non-Negative Sparse Embedding.</title>
<date>2012</date>
<booktitle>In Proceedings of Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="3247" citStr="Murphy et al., 2012" startWordPosition="494" endWordPosition="497">g, the semantic content of each word will be necessarily activated in the mind, and so in patterns of activity over individual neurons. In principle then, brain activity could replace corpus data as input to a VSM, and contemporary imaging techniques allow us to attempt this. Functional Magnetic Resonance Imaging (fMRI) and Magnetoencephalography (MEG) are two brain activation recording technologies that measure neuronal activation in aggregate, and have been shown to have a predictive relationship with models of word meaning (Mitchell et al., 2008; Palatucci et al., 2009; Sudre et al., 2012; Murphy et al., 2012b).1 If brain activation data encodes semantics, we theorized that including brain data in a model of semantics could result in a model more consistent with semantic ground truth. However, the inclusion of brain data will only improve a text-based model if brain data contains semantic information not readily available in the corpus. In addition, if a semantic test involves another subject’s brain activation data, performance can improve only if the additional semantic information is consistent across brains. Of course, brains differ in shape, size and in connectivity, so additional information</context>
<context position="7520" citStr="Murphy et al., 2012" startWordPosition="1175" endWordPosition="1179"> information available in brain activation data that is not present in corpus data, and that there are elements of semantics currently lacking in text-based VSMs. We have made available the top performing VSMs created with brain and text data (http://www.cs.cmu.edu/ ˜afyshe/papers/acl2014/). In the following sections we will review NNSE, and our extension, JNNSE. We will describe the data used and the experiments to support our position that brain data is a valuable source of semantic information that compliments text data. 2 Non-Negative Sparse Embedding Non-Negative Sparse Embedding (NNSE) (Murphy et al., 2012a) is an algorithm that produces a latent representation using matrix factorization. Standard NNSE begins with a matrix X E Rw×c made of c corpus statistics for w words. NNSE solves the following objective function: � � � Xi,: − Ai,: x D 2 + λ� A 1 (1) subject to: Di,:DTi,: &lt; 1, b 1 &lt; i &lt; E (2) Ai,j &gt; 0, 1 &lt; i &lt; w, 1 &lt; j &lt; E (3) The solution will find a matrix A E Rw×` that is sparse, non-negative, and represents word semantics in an E-dimensional latent space. D E R`×c gives the encoding of corpus statistics in the latent space. Together, they factor the original corpus statistics matrix X in</context>
<context position="16802" citStr="Murphy et al., 2012" startWordPosition="2791" endWordPosition="2794">eir own strengths and weaknesses, generative models being particularly strong when data sources are limited (Ng and Jordan, 2002). Our task is an interesting blend of data-limited and data-rich problem scenarios. In the past, various pieces of additional information have been incorporated into semantic models. For example, models with behavioral data (Silberer and Lapata, 2012) and models with visual information (Bruni et al., 2011; Silberer et al., 2013) have both shown to improve semantic representations. Other works have correlated VSMs built with text or images with brain activation data (Murphy et al., 2012b; Anderson et al., 2013). To our knowledge, this work is the first to integrate brain activation data into the construction of the VSM. 4 Data 4.1 Corpus Data The corpus statistics used here are the downloadable vectors from Fyshe et al. (2013)3. They are compiled from a 16 billion word subset of ClueWeb09 (Callan and Hoy, 2009) and contain two types of corpus features: dependency and document features, found to be complimentary for 3http://www.cs.cmu.edu/-afyshe/papers/ conll2013/ 492 most tasks. Dependency statistics were derived by dependency parsing the corpus and compiling counts for all</context>
<context position="26485" citStr="Murphy et al., 2012" startWordPosition="4380" endWordPosition="4383">ffect of brain activation data, the extra NNSE(Text) dimensions are being used to overfit to the corpus data, or possibly to fit semantic properties not detectable with current brain imaging technologies. However, when brain activation data is included, increasing the number of latent dimensions strictly increases performance for JNNSE(fMRI+Text). JNNSE(MEG+Text) has peak performance with 500 latent dimensions, with ∼ 1% decrease in performance at 1000 latent dimensions. In previous work, the ability to decode words from brain activation data was found to improve with added latent dimensions (Murphy et al., 2012a). Our results may differ because our words are POS tagged, and we included only nouns for the final NNSE(Text) model. We found that with the original A = 0.05 setting from Murphy et al. (Murphy et al., 2012a) produced vectors that were too sparse; four of the 60 test words had all-zero vectors (JNNSE(Brain+Text) models did have any allzero vectors). To improve the NNSE(Text) vectors for a fair comparison, we reduced A = 0.025, under which NNSE(Text) did not produce any allzero vectors for the 60 words. Our results show that brain activation data contributes additional information, which lead</context>
</contexts>
<marker>Murphy, Talukdar, Mitchell, 2012</marker>
<rawString>Brian Murphy, Partha Talukdar, and Tom Mitchell. 2012a. Learning Effective and Interpretable Semantic Models using Non-Negative Sparse Embedding. In Proceedings of Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Murphy</author>
<author>Partha Talukdar</author>
<author>Tom Mitchell</author>
</authors>
<title>Selecting Corpus-Semantic Models for Neurolinguistic Decoding.</title>
<date>2012</date>
<booktitle>In First Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>114--123</pages>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="3247" citStr="Murphy et al., 2012" startWordPosition="494" endWordPosition="497">g, the semantic content of each word will be necessarily activated in the mind, and so in patterns of activity over individual neurons. In principle then, brain activity could replace corpus data as input to a VSM, and contemporary imaging techniques allow us to attempt this. Functional Magnetic Resonance Imaging (fMRI) and Magnetoencephalography (MEG) are two brain activation recording technologies that measure neuronal activation in aggregate, and have been shown to have a predictive relationship with models of word meaning (Mitchell et al., 2008; Palatucci et al., 2009; Sudre et al., 2012; Murphy et al., 2012b).1 If brain activation data encodes semantics, we theorized that including brain data in a model of semantics could result in a model more consistent with semantic ground truth. However, the inclusion of brain data will only improve a text-based model if brain data contains semantic information not readily available in the corpus. In addition, if a semantic test involves another subject’s brain activation data, performance can improve only if the additional semantic information is consistent across brains. Of course, brains differ in shape, size and in connectivity, so additional information</context>
<context position="7520" citStr="Murphy et al., 2012" startWordPosition="1175" endWordPosition="1179"> information available in brain activation data that is not present in corpus data, and that there are elements of semantics currently lacking in text-based VSMs. We have made available the top performing VSMs created with brain and text data (http://www.cs.cmu.edu/ ˜afyshe/papers/acl2014/). In the following sections we will review NNSE, and our extension, JNNSE. We will describe the data used and the experiments to support our position that brain data is a valuable source of semantic information that compliments text data. 2 Non-Negative Sparse Embedding Non-Negative Sparse Embedding (NNSE) (Murphy et al., 2012a) is an algorithm that produces a latent representation using matrix factorization. Standard NNSE begins with a matrix X E Rw×c made of c corpus statistics for w words. NNSE solves the following objective function: � � � Xi,: − Ai,: x D 2 + λ� A 1 (1) subject to: Di,:DTi,: &lt; 1, b 1 &lt; i &lt; E (2) Ai,j &gt; 0, 1 &lt; i &lt; w, 1 &lt; j &lt; E (3) The solution will find a matrix A E Rw×` that is sparse, non-negative, and represents word semantics in an E-dimensional latent space. D E R`×c gives the encoding of corpus statistics in the latent space. Together, they factor the original corpus statistics matrix X in</context>
<context position="16802" citStr="Murphy et al., 2012" startWordPosition="2791" endWordPosition="2794">eir own strengths and weaknesses, generative models being particularly strong when data sources are limited (Ng and Jordan, 2002). Our task is an interesting blend of data-limited and data-rich problem scenarios. In the past, various pieces of additional information have been incorporated into semantic models. For example, models with behavioral data (Silberer and Lapata, 2012) and models with visual information (Bruni et al., 2011; Silberer et al., 2013) have both shown to improve semantic representations. Other works have correlated VSMs built with text or images with brain activation data (Murphy et al., 2012b; Anderson et al., 2013). To our knowledge, this work is the first to integrate brain activation data into the construction of the VSM. 4 Data 4.1 Corpus Data The corpus statistics used here are the downloadable vectors from Fyshe et al. (2013)3. They are compiled from a 16 billion word subset of ClueWeb09 (Callan and Hoy, 2009) and contain two types of corpus features: dependency and document features, found to be complimentary for 3http://www.cs.cmu.edu/-afyshe/papers/ conll2013/ 492 most tasks. Dependency statistics were derived by dependency parsing the corpus and compiling counts for all</context>
<context position="26485" citStr="Murphy et al., 2012" startWordPosition="4380" endWordPosition="4383">ffect of brain activation data, the extra NNSE(Text) dimensions are being used to overfit to the corpus data, or possibly to fit semantic properties not detectable with current brain imaging technologies. However, when brain activation data is included, increasing the number of latent dimensions strictly increases performance for JNNSE(fMRI+Text). JNNSE(MEG+Text) has peak performance with 500 latent dimensions, with ∼ 1% decrease in performance at 1000 latent dimensions. In previous work, the ability to decode words from brain activation data was found to improve with added latent dimensions (Murphy et al., 2012a). Our results may differ because our words are POS tagged, and we included only nouns for the final NNSE(Text) model. We found that with the original A = 0.05 setting from Murphy et al. (Murphy et al., 2012a) produced vectors that were too sparse; four of the 60 test words had all-zero vectors (JNNSE(Brain+Text) models did have any allzero vectors). To improve the NNSE(Text) vectors for a fair comparison, we reduced A = 0.025, under which NNSE(Text) did not produce any allzero vectors for the 60 words. Our results show that brain activation data contributes additional information, which lead</context>
</contexts>
<marker>Murphy, Talukdar, Mitchell, 2012</marker>
<rawString>Brian Murphy, Partha Talukdar, and Tom Mitchell. 2012b. Selecting Corpus-Semantic Models for Neurolinguistic Decoding. In First Joint Conference on Lexical and Computational Semantics (*SEM), pages 114–123, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes.</title>
<date>2002</date>
<booktitle>In Advances in neural information processing systems,</booktitle>
<volume>14</volume>
<contexts>
<context position="16312" citStr="Ng and Jordan, 2002" startWordPosition="2714" endWordPosition="2717">ta has not been explored with Bayesian CCA. One could also use a topic model style formulation to represent this semantic representation task. Supervised topic models (Blei and McAuliffe, 2007) use a latent topic to generate two observed outputs: words in a document and a categorical label for the document. The same idea could be applied here: the latent semantic representation generates the observed brain activity and corpus statistics. Generative and discriminative models both have their own strengths and weaknesses, generative models being particularly strong when data sources are limited (Ng and Jordan, 2002). Our task is an interesting blend of data-limited and data-rich problem scenarios. In the past, various pieces of additional information have been incorporated into semantic models. For example, models with behavioral data (Silberer and Lapata, 2012) and models with visual information (Bruni et al., 2011; Silberer et al., 2013) have both shown to improve semantic representations. Other works have correlated VSMs built with text or images with brain activation data (Murphy et al., 2012b; Anderson et al., 2013). To our knowledge, this work is the first to integrate brain activation data into th</context>
</contexts>
<marker>Ng, Jordan, 2002</marker>
<rawString>Andrew Y. Ng and Michael I. Jordan. 2002. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. In Advances in neural information processing systems, volume 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Palatucci</author>
<author>Geoffrey Hinton</author>
<author>Dean Pomerleau</author>
<author>Tom M Mitchell</author>
</authors>
<date>2009</date>
<booktitle>Zero-Shot Learning with Semantic Output Codes. Advances in Neural Information Processing Systems,</booktitle>
<pages>22--1410</pages>
<contexts>
<context position="3206" citStr="Palatucci et al., 2009" startWordPosition="486" endWordPosition="489">entation. When a person is reading or writing, the semantic content of each word will be necessarily activated in the mind, and so in patterns of activity over individual neurons. In principle then, brain activity could replace corpus data as input to a VSM, and contemporary imaging techniques allow us to attempt this. Functional Magnetic Resonance Imaging (fMRI) and Magnetoencephalography (MEG) are two brain activation recording technologies that measure neuronal activation in aggregate, and have been shown to have a predictive relationship with models of word meaning (Mitchell et al., 2008; Palatucci et al., 2009; Sudre et al., 2012; Murphy et al., 2012b).1 If brain activation data encodes semantics, we theorized that including brain data in a model of semantics could result in a model more consistent with semantic ground truth. However, the inclusion of brain data will only improve a text-based model if brain data contains semantic information not readily available in the corpus. In addition, if a semantic test involves another subject’s brain activation data, performance can improve only if the additional semantic information is consistent across brains. Of course, brains differ in shape, size and i</context>
</contexts>
<marker>Palatucci, Hinton, Pomerleau, Mitchell, 2009</marker>
<rawString>Mark Palatucci, Geoffrey Hinton, Dean Pomerleau, and Tom M Mitchell. 2009. Zero-Shot Learning with Semantic Output Codes. Advances in Neural Information Processing Systems, 22:1410–1418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajeev D S Raizada</author>
<author>Andrew C Connolly</author>
</authors>
<title>What Makes Different People’s Representations Alike : Neural Similarity Space Solves the Problem of Across-subject fMRI Decoding.</title>
<date>2012</date>
<journal>Journal of Cognitive Neuroscience,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="14178" citStr="Raizada and Connolly, 2012" startWordPosition="2351" endWordPosition="2354">s not require brain data to be mapped to a common average brain, which is often the case when one wants to generalize between human subjects. Such mappings can blur and distort data, making it less useful for subsequent prediction steps. We avoid these mappings, and instead use the fact that similar words elicit similar brain activation within a subject. In the JNNSE algorithm, it is this closeness in “brain space” that guides the creation of the latent space A. Leveraging intra-subject distance measures to study inter-subject encodings has been studied previously (Kriegeskorte et al., 2008a; Raizada and Connolly, 2012), and has even been used across species (humans and primates) (Kriegeskorte et al., 2008b). Though we restrict ourselves to using one subject per JNNSE(Brain+Text) model, the JNNSE algorithm could easily be extended to include data from multiple brain imaging experiments by adding a new squared loss term for additional brain data. 3.1 Related Work Perhaps the most well known related approach to joining data sources is Canonical Correlation Analysis (CCA) (Hotelling, 1936), which has been applied to brain activation data in the past (Rustandi et al., 2009). CCA seeks two linear transformations </context>
</contexts>
<marker>Raizada, Connolly, 2012</marker>
<rawString>Rajeev D S Raizada and Andrew C Connolly. 2012. What Makes Different People’s Representations Alike : Neural Similarity Space Solves the Problem of Across-subject fMRI Decoding. Journal of Cognitive Neuroscience, 24(4):868–877.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Indrayana Rustandi</author>
<author>Marcel Adam Just</author>
<author>Tom M Mitchell</author>
</authors>
<title>Integrating Multiple-Study Multiple-Subject fMRI Datasets Using Canonical Correlation Analysis.</title>
<date>2009</date>
<booktitle>In MICCAI</booktitle>
<contexts>
<context position="14739" citStr="Rustandi et al., 2009" startWordPosition="2440" endWordPosition="2444">usly (Kriegeskorte et al., 2008a; Raizada and Connolly, 2012), and has even been used across species (humans and primates) (Kriegeskorte et al., 2008b). Though we restrict ourselves to using one subject per JNNSE(Brain+Text) model, the JNNSE algorithm could easily be extended to include data from multiple brain imaging experiments by adding a new squared loss term for additional brain data. 3.1 Related Work Perhaps the most well known related approach to joining data sources is Canonical Correlation Analysis (CCA) (Hotelling, 1936), which has been applied to brain activation data in the past (Rustandi et al., 2009). CCA seeks two linear transformations that maximally correlate two data sets in the transformed form. CCA requires that the data sources be paired (all rows in the corpus data must have a corresponding brain data), as correlation between points is integral to the objective. To apply CCA to our data we would need to discard the vast majority of our corpus data, and use only the 60 rows of X with corresponding rows in Y. While CCA holds the input data fixed and maximally correlates the transformed form, we hold the transformed form fixed and seek a solution that maximally correlates the reconst</context>
</contexts>
<marker>Rustandi, Just, Mitchell, 2009</marker>
<rawString>Indrayana Rustandi, Marcel Adam Just, and Tom M Mitchell. 2009. Integrating Multiple-Study Multiple-Subject fMRI Datasets Using Canonical Correlation Analysis. In MICCAI 2009 Workshop: Statistical modeling and detection issues in intraand inter-subject functional MRI data analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The Word-Space Model Using distributional analysis to represent syntagmatic and paradigmatic relations between words. Doctor of philosophy,</title>
<date>2006</date>
<institution>Stockholm University.</institution>
<contexts>
<context position="2106" citStr="Sahlgren, 2006" startWordPosition="314" endWordPosition="315">riven method to discover word meanings (Landauer and Dumais, 1997). Typically, VSMs are created by collecting word usage statistics from large amounts of text data and applying some dimensionality reduction technique like Singular Value Decomposition (SVD). The basic assumption is that semantics drives a person’s language production behavior, and as a result co-occurrence patterns in written text indirectly encode word meaning. The raw co-occurrence statistics are unwieldy, but in the compressed VSM the distance between any two words is conceived to represent their mutual semantic similarity (Sahlgren, 2006; Turney and Pantel, 2010), as perceived and judged by speakers. This space then reflects the “semantic ground truth” of shared lexical meanings in a language community’s vocabulary. However corpus-based VSMs have been criticized as being noisy or incomplete representations of meaning (Glenberg and Robertson, 2000). For example, multiple word senses collide in the same vector, and noise from mis-parsed sentences or spam documents can interfere with the final semantic representation. When a person is reading or writing, the semantic content of each word will be necessarily activated in the mind</context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Magnus Sahlgren. 2006. The Word-Space Model Using distributional analysis to represent syntagmatic and paradigmatic relations between words. Doctor of philosophy, Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Mirella Lapata</author>
</authors>
<title>Grounded models of semantic representation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1423--1433</pages>
<contexts>
<context position="16563" citStr="Silberer and Lapata, 2012" startWordPosition="2751" endWordPosition="2755">: words in a document and a categorical label for the document. The same idea could be applied here: the latent semantic representation generates the observed brain activity and corpus statistics. Generative and discriminative models both have their own strengths and weaknesses, generative models being particularly strong when data sources are limited (Ng and Jordan, 2002). Our task is an interesting blend of data-limited and data-rich problem scenarios. In the past, various pieces of additional information have been incorporated into semantic models. For example, models with behavioral data (Silberer and Lapata, 2012) and models with visual information (Bruni et al., 2011; Silberer et al., 2013) have both shown to improve semantic representations. Other works have correlated VSMs built with text or images with brain activation data (Murphy et al., 2012b; Anderson et al., 2013). To our knowledge, this work is the first to integrate brain activation data into the construction of the VSM. 4 Data 4.1 Corpus Data The corpus statistics used here are the downloadable vectors from Fyshe et al. (2013)3. They are compiled from a 16 billion word subset of ClueWeb09 (Callan and Hoy, 2009) and contain two types of corp</context>
</contexts>
<marker>Silberer, Lapata, 2012</marker>
<rawString>Carina Silberer and Mirella Lapata. 2012. Grounded models of semantic representation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1423–1433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Vittorio Ferrari</author>
<author>Mirella Lapata</author>
</authors>
<title>Models of Semantic Representation with Visual Attributes.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics 2013,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="16642" citStr="Silberer et al., 2013" startWordPosition="2765" endWordPosition="2768"> be applied here: the latent semantic representation generates the observed brain activity and corpus statistics. Generative and discriminative models both have their own strengths and weaknesses, generative models being particularly strong when data sources are limited (Ng and Jordan, 2002). Our task is an interesting blend of data-limited and data-rich problem scenarios. In the past, various pieces of additional information have been incorporated into semantic models. For example, models with behavioral data (Silberer and Lapata, 2012) and models with visual information (Bruni et al., 2011; Silberer et al., 2013) have both shown to improve semantic representations. Other works have correlated VSMs built with text or images with brain activation data (Murphy et al., 2012b; Anderson et al., 2013). To our knowledge, this work is the first to integrate brain activation data into the construction of the VSM. 4 Data 4.1 Corpus Data The corpus statistics used here are the downloadable vectors from Fyshe et al. (2013)3. They are compiled from a 16 billion word subset of ClueWeb09 (Callan and Hoy, 2009) and contain two types of corpus features: dependency and document features, found to be complimentary for 3h</context>
</contexts>
<marker>Silberer, Ferrari, Lapata, 2013</marker>
<rawString>Carina Silberer, Vittorio Ferrari, and Mirella Lapata. 2013. Models of Semantic Representation with Visual Attributes. In Association for Computational Linguistics 2013, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gustavo Sudre</author>
<author>Dean Pomerleau</author>
<author>Mark Palatucci</author>
<author>Leila Wehbe</author>
<author>Alona Fyshe</author>
<author>Riitta Salmelin</author>
<author>Tom Mitchell</author>
</authors>
<title>Tracking Neural Coding of Perceptual and Semantic Features of Concrete Nouns.</title>
<date>2012</date>
<journal>NeuroImage,</journal>
<volume>62</volume>
<issue>1</issue>
<contexts>
<context position="3226" citStr="Sudre et al., 2012" startWordPosition="490" endWordPosition="493">is reading or writing, the semantic content of each word will be necessarily activated in the mind, and so in patterns of activity over individual neurons. In principle then, brain activity could replace corpus data as input to a VSM, and contemporary imaging techniques allow us to attempt this. Functional Magnetic Resonance Imaging (fMRI) and Magnetoencephalography (MEG) are two brain activation recording technologies that measure neuronal activation in aggregate, and have been shown to have a predictive relationship with models of word meaning (Mitchell et al., 2008; Palatucci et al., 2009; Sudre et al., 2012; Murphy et al., 2012b).1 If brain activation data encodes semantics, we theorized that including brain data in a model of semantics could result in a model more consistent with semantic ground truth. However, the inclusion of brain data will only improve a text-based model if brain data contains semantic information not readily available in the corpus. In addition, if a semantic test involves another subject’s brain activation data, performance can improve only if the additional semantic information is consistent across brains. Of course, brains differ in shape, size and in connectivity, so a</context>
<context position="12439" citStr="Sudre et al., 2012" startWordPosition="2060" endWordPosition="2063">sed the SPAMS package2 to solve, and set λ = 0.025. This algorithm was a very easy extension to NNSE(Text) and required very little additional tuning. We also consider learning shared representations in the case where data X and Y contain the effects of known disjoint features. For example, when a person reads a word, the recorded brain activation data Y will contain the physiological response to viewing the stimulus, which is unrelated to the semantics of the word. These signals can be attributed to, for example, the number of letters in the word and the number of white pixels on the screen (Sudre et al., 2012). To account for such effects in the data, we augment A0 with a set of n fixed, manually defined features (e.g. word length) to create A0percept E Rw×(`+n). D(b) E R(`+n)×v is used with A0percept, 2SPAMS Package: http://spams-devel.gforge.inria.fr/ argmin A,D(c),D(b) Xw i=1 w/ X i=1 491 to reconstruct the brain data Y . More generally, one could instead allocate a certain number of latent features specific to X or Y, both of which could be learned, as explored in some related work (Gupta et al., 2013). We use 11 perceptual features that characterize the non-semantic features of the word stimul</context>
<context position="18369" citStr="Sudre et al., 2012" startWordPosition="3040" endWordPosition="3043">retained. We selected the rows corresponding to noun-tagged words (approx. 17000 words). 4.2 Brain Activation Data We have MEG and fMRI data at our disposal. MEG measures the magnetic field caused by many thousands of neurons firing together, and has good time resolution (1000 Hz) but poor spatial resolution. fMRI measures the change in blood oxygenation that results from differential neural activity, and has good spatial resolution but poor time resolution (0.5-1 Hz). We have fMRI data and MEG data for 18 subjects (9 in each imaging modality) viewing 60 concrete nouns (Mitchell et al., 2008; Sudre et al., 2012). The 60 words span 12 word categories (animals, buildings, tools, insects, body parts, furniture, building parts, utensils, vehicles, objects, clothing, food). Each of the 60 words was presented with a line drawing, so word ambiguity is not an issue. For both recording modalities, all trials for a particular word were averaged together to create one training instance per word, with 60 training instances in all for each subject and imaging modality. More preprocessing details appear in the supplementary material. 5 Experimental Results Here we explore several variations of JNNSE and NNSE formu</context>
<context position="24029" citStr="Sudre et al., 2012" startWordPosition="3982" endWordPosition="3985">ictive accuracy. Note that we must re-train our weight matrix W for each subject (instead of re-using D(b) from Equation 4) because testing always occurs on a different subject, and the brain activation data is not inter-subject aligned. We train ` independent L2 regularized regressors to predict the `-dimensional vectors a = {a1 ... atI. The predictions are concatenated to produce a predicted semantic vector: aˆ = {ˆa1, . . ., ˆat}. We assess word prediction performance by testing if the model can differentiate between two unseen words, a task named 2 vs. 2 prediction (Mitchell et al., 2008; Sudre et al., 2012). We choose the assignment of the two held out semantic vectors (a(1), a(2)) to predicted semantic vectors (ˆa(1), ˆa(2)) that minimizes the sum of the two normalized Euclidean distances. 2 vs. 2 accuracy is the percentage of tests where the correct assignment is chosen. The 60 nouns fall into 12 word categories. Words in the same word category (e.g. screwdriver and hammer) are closer in semantic space than words in different word categories, which makes some 2 vs. 2 tests more difficult than others. We choose 150 random pairs of words (with each word represented equally) to estimate the diffi</context>
</contexts>
<marker>Sudre, Pomerleau, Palatucci, Wehbe, Fyshe, Salmelin, Mitchell, 2012</marker>
<rawString>Gustavo Sudre, Dean Pomerleau, Mark Palatucci, Leila Wehbe, Alona Fyshe, Riitta Salmelin, and Tom Mitchell. 2012. Tracking Neural Coding of Perceptual and Semantic Features of Concrete Nouns. NeuroImage, 62(1):463–451, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From Frequency to Meaning : Vector Space Models of Semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="2132" citStr="Turney and Pantel, 2010" startWordPosition="316" endWordPosition="319">discover word meanings (Landauer and Dumais, 1997). Typically, VSMs are created by collecting word usage statistics from large amounts of text data and applying some dimensionality reduction technique like Singular Value Decomposition (SVD). The basic assumption is that semantics drives a person’s language production behavior, and as a result co-occurrence patterns in written text indirectly encode word meaning. The raw co-occurrence statistics are unwieldy, but in the compressed VSM the distance between any two words is conceived to represent their mutual semantic similarity (Sahlgren, 2006; Turney and Pantel, 2010), as perceived and judged by speakers. This space then reflects the “semantic ground truth” of shared lexical meanings in a language community’s vocabulary. However corpus-based VSMs have been criticized as being noisy or incomplete representations of meaning (Glenberg and Robertson, 2000). For example, multiple word senses collide in the same vector, and noise from mis-parsed sentences or spam documents can interfere with the final semantic representation. When a person is reading or writing, the semantic content of each word will be necessarily activated in the mind, and so in patterns of ac</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D Turney and Patrick Pantel. 2010. From Frequency to Meaning : Vector Space Models of Semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha White</author>
<author>Yaoliang Yu</author>
<author>Xinhua Zhang</author>
<author>Dale Schuurmans</author>
</authors>
<title>Convex multi-view subspace learning.</title>
<date>2012</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>1--14</pages>
<contexts>
<context position="13377" citStr="White et al., 2012" startWordPosition="2223" endWordPosition="2226">. More generally, one could instead allocate a certain number of latent features specific to X or Y, both of which could be learned, as explored in some related work (Gupta et al., 2013). We use 11 perceptual features that characterize the non-semantic features of the word stimulus (for a list, see supplementary material at http://www.cs.cmu. edu/-afyshe/papers/acl2014/). The JNNSE algorithm is advantageous in that it can handle partially paired data. That is, the algorithm does not require that every row in X also have a row in Y . Fully paired data is a requirement of many other approaches (White et al., 2012; Jia and Darrell, 2010). Our approach allows us to leverage the semantic information in corpus data even for words without brain activation recordings. JNNSE(Brain+Text) does not require brain data to be mapped to a common average brain, which is often the case when one wants to generalize between human subjects. Such mappings can blur and distort data, making it less useful for subsequent prediction steps. We avoid these mappings, and instead use the fact that similar words elicit similar brain activation within a subject. In the JNNSE algorithm, it is this closeness in “brain space” that gu</context>
</contexts>
<marker>White, Yu, Zhang, Schuurmans, 2012</marker>
<rawString>Martha White, Yaoliang Yu, Xinhua Zhang, and Dale Schuurmans. 2012. Convex multi-view subspace learning. In Advances in Neural Information Processing Systems, pages 1–14.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>