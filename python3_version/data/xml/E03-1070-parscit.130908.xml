<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.9966795">
QUALIFIER: Question Answering by Lexical Fabric
and External Resources
</title>
<author confidence="0.999149">
Hui Yang
</author>
<affiliation confidence="0.874231">
Department of Computer Science
National University of Singapore
3 Science Drive 2, Singapore 117543
</affiliation>
<email confidence="0.995338">
yangh@comp.nus.edu.sg
</email>
<author confidence="0.995577">
Tat-Seng Chua
</author>
<affiliation confidence="0.874080333333333">
Department of Computer Science
National University of Singapore
3 Science Drive 2, Singapore 117543
</affiliation>
<email confidence="0.995484">
chuats@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.99559" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999964882352941">
One of the major challenges in TREC-
style question-answering (QA) is to over-
come the mismatch in the lexical repre-
sentations in the query space and
document space. This is particularly se-
vere in QA as exact answers, rather than
documents, are required in response to
questions. Most current approaches over-
come the mismatch problem by employ-
ing either data redundancy strategy
through the use of Web or linguistic re-
sources. This paper investigates the inte-
gration of lexical relations and Web
knowledge to tackle this problem. The re-
sults obtained on TREC11 QA corpus in-
dicate that our approach is both feasible
and effective.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994249173913044">
Open domain Question Answering (QA) is an
information retrieval paradigm that is attracting
increasing attention from the information re-
trieval (IR), information extraction (IE), and
natural language processing (NLP) communities
(AAAI Spring Symposium Series 2002, ACL-
EACL 2002). A QA system retrieves concise
answers to open-domain natural language ques-
tions, where a large text collection (termed the
QA corpus) is used as the source for these an-
swers. Contrary to traditional IR tasks, it is not
acceptable for a QA system to retrieve a full
document, or a paragraph, in response to a ques-
tion. Contrary to traditional IE tasks, no pre-
specified domain restrictions are placed on the
questions, which may be of any type and in any
topic. Modern QA systems must therefore com-
bine the strengths of traditional IR and NLP/IE to
provide an apposite way to answering questions.
The QA task in the TREC conference series
(Voorhees 2002) has motivated much of the re-
cent works focusing on fact-based, short-answer
questions. Examples of such questions include:
&amp;quot;Who is Tom Cruise married to?&amp;quot; or &amp;quot;How many
chromosomes does a human zygote have?&amp;quot;. For
the most recent TREC-11 conference, the task
consists of 500 questions posed over a QA corpus
containing more than one million newspaper arti-
cles. Instead of previous years&apos; 50-byte or 250-
byte text fragments, exact answers are expected
from the QA corpus with supports of documen-
tary evidences.
One of the major challenges in TREC-style
QA is to overcome the mismatch in the lexical
representations between the query space and
document space. This mismatch, also known as
the QA gap, is caused by the differences in the
set of terms used in the question formulation and
answer strings in the corpus. Given a source,
such as the QA corpus, that contains only a rela-
tively small number of answers to a query, we are
faced with the difficulty to map the questions to
answers by way of uncovering the complex lexi-
cal, syntactic, or semantic relationships between
the question and the answer strings.
Recent redundancy-based approaches (Brill et
</bodyText>
<note confidence="0.7257065">
al 2002, Clarke et al 2002, Kwok et al 2001,
Radev et al 2001) proposed the use of data, in-
</note>
<page confidence="0.998739">
363
</page>
<bodyText confidence="0.999825742857143">
stead of methods, to do most of the work to
bridge the QA gap. These methods suggest that
the greater the answer redundancy in the source
data collection, the more likely that we can find
an answer that occurs in a simple relation to the
question. With the availability of rich linguistic
resources, we can also minimize the need to per-
form complex linguistic processing. However,
this does not mean that NLP is now out of the
picture. For some question/answer pairs, deep
reasoning is still needed to relate the two. Many
QA research groups have used a variety of lin-
guistic resources — part-of-speech tagging, syn-
tactic parsing, semantic relations, named entity
extraction, WordNet, on-line dictionaries, query
logs and ontologies, etc (Harabagiu et al 2002,
Hovy et al 2002).
This paper investigates the integration of both
linguistic knowledge and external resources for
TREC-style question answering. In particular, we
describe a high performance question answering
system called QUALIFIER (QUestion Answer-
ing by Lexical Fabric and External Re-
sources) and analyze its effectiveness using the
TREC-11 benchmark. Our results show that
combining lexical information and external re-
sources with a custom text search produces an
effective question-answering system.
The rest of the paper is organized as follows.
Section 2 presents related work. Sections 3 and 4
respectively discuss the design and architecture
of the system. Section 5 elaborates on the use of
external resources for QA, while Section 6 details
the experimental results. Section 7 concludes the
paper with discussions for future work.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999814233333333">
The idea of using the external resources for ques-
tion answering is an emerging topic of interest
among the computational linguistic communities.
The TREC-10 QA track demonstrated that the
use of the Web redundancy could be exploited at
different levels in the process of finding answers
to natural language questions. Several studies
(Brill et al 2002,Clarke et al 2002, Kwok et al
2001) suggested that the application of Web
search can improve the precision of a QA system
by 25-30%. A common feature of these ap-
proaches is to use the Web to introduce data re-
dundancy for a more reliable answer extraction
from local text collections. Radev et al [20] pro-
posed a probabilistic algorithm that learns the
best query paraphrase of a question searching the
Web.
Many groups (Buchholz 2002, Chen et al
2002, Harabagiu et al 2002, Hovy et al 2002.)
working on question answering also employ a
variety of linguistic resources, such as the part-
of-speech tagging, syntactic parsing, semantic
relations, named entity extraction, dictionaries,
WordNet, etc. Moldovan and Rus (2001) pro-
posed the use of logic form transformation of
WordNet for QA. Lin (2002) gave a detailed
comparison of the Web-based and linguistic-
based approaches to QA, and concluded that
combining both approaches could lead to better
performance on answering definition questions.
</bodyText>
<sectionHeader confidence="0.99916" genericHeader="method">
3 Design Consideration
</sectionHeader>
<bodyText confidence="0.999780148148148">
To effectively perform open domain QA, two
fundamental problems must be solved. The first
is to bridge the gap between the query and
document spaces. Most recent QA systems
adopt the following general pipelined approach
to: (a) classify the question according to the type
of its answer; (b) employ IR technology, with the
question as a query, to retrieve a small portion of
the document collection; and (c) analyze the re-
turned documents to detect entities of the appro-
priate type. In step (b), the traditional IR systems
assume that there is close lexical similarity be-
tween the queries and the corresponding docu-
ments. In practice, however, there is often very
little overlap between the terms used in a ques-
tion and those appearing in its answer. For exam-
ple, the best response to the question &amp;quot;Where &apos;s a
good place to get dinner?&amp;quot; might be &amp;quot;McDon-
ald&apos;s&amp;quot; and &amp;quot;Jade Crystal Kitchen has nice
Shanghai Tang Bao&amp;quot;, which have no tokens in
common with the query. Usually, the QA gap
reveals itself at four different levels, namely, the
lexical, syntactic, semantic and discourse levels.
As a result, the traditional bag-of-words retrieval
techniques might be less effective at matching
questions to exact answers than matching key-
words to documents.
</bodyText>
<page confidence="0.999013">
364
</page>
<figureCaption confidence="0.986489">
Reduce the #01 expanded content words
Figure 1: System Overflew of QUALIFIER
</figureCaption>
<figure confidence="0.99946">
Candidate
sentences
Question Analysis
Question
Clasoification
Question Parsing
Original
Content
Words
Question
Answer
Extraction
Sentence Ranking
Document
Retrieval
Answer
.4_
Using External
Knowled e Resources
Wob
Word Net
Relevant Expanded
TREC doc Content
Words
</figure>
<bodyText confidence="0.999122777777778">
The second fundamental problem is to exploit
the associations among QA event elements.
The world consists of two basic types of things:
entities and events. From their definitions in
WordNet, an entity is anything having existence
(living or nonliving) and an event is something
that happens at a given place and time. This tax-
onomy is also applicable to QA task, i.e., the
questions can be considered as enquiries about
either entities or events. Usually, the entity ques-
tions expect the entity properties or the entities
themselves as the answers, such as the definition
questions. More generally, questions often show
great interests in several aspects of events,
namely Location, Time, Subject, Object, Quantity
and Description. Table 1 shows the correspon-
dences of the most common WH-question classes
and the QA event elements.
</bodyText>
<table confidence="0.999615142857143">
WH-Question QA Event Elements
Who Subject, Object
Where Location
When Time
What Subject, Object, Description
Which Subject, Object,
How Quantity, Description
</table>
<tableCaption confidence="0.8423865">
Table 1: Correspondence of WH-Questions &amp; Event
Elements
</tableCaption>
<bodyText confidence="0.998256833333333">
Our major observation is that a QA event
shows great cohesive affinity to all its elements
and the elements are likely to be closely coupled
by this event. Although some elements may ap-
pear in different places of the text collection or
may even be absent, there must be innate associa-
tions among these elements if they belong to the
same event. Hence, even if we only know a por-
tion of the elements (e.g. Time, Subject, Object),
we can use this information to narrow the search
process to find the rest of elements (e.g. Loca-
tion, etc). However, it is difficult to find correct
unknown element(s) because of insufficient and
inexact known elements.
To tackle these two problems effectively, we
explore the use of external resources to extract
terms that are highly correlated with the query,
and use these terms to expand the query. Instead
of treating the web and linguistic resources sepa-
rately, we explore an innovative approach to fuse
the lexical and semantic knowledge to support
effective QA. Our focus is to link the questions
and the answers together by discovering a portion
or all of the elements for certain QA events. We
explore the use of world knowledge (the Web
and WordNet glosses) to find more known ele-
ments and exploit the lexical knowledge (Word-
Net synsets and morphemics) to find their exact
forms. We would like to call our approach Event-
based QA.
</bodyText>
<sectionHeader confidence="0.976918" genericHeader="method">
4 System Architecture
</sectionHeader>
<bodyText confidence="0.999972125">
Our system, named QUALIFIER, adopts the by
now more or less standard QA system architec-
ture as shown in Figure 1. It includes modules to
perform question analysis, query formulation by
using external resources, document retrieval,
candidate sentence selection and exact answer
extraction.
During question analysis, QUALIFIER identi-
fies detailed question classes, answer types, and
pertinent content query terms or phrases to facili-
tate the seeking of exact answers. It uses a rule-
based question classifier to perform the syntactic-
semantic analysis of the questions and determines
the question types in a two-level question taxon-
omy. The first level in the question taxonomy
corresponds to the more general named entities
</bodyText>
<page confidence="0.995208">
365
</page>
<bodyText confidence="0.996982425">
like Human, Location, Time, Number, Object,
Description and Others. The second level con-
tains question classes that correspond to fine-
grained named entities to facilitate accurate an-
swer extraction. Examples of second level classes
for, say Location, are Country, City, State, River,
Mountain etc. The taxonomy is similar to that
used in Li &amp; Roth 2002. Our rule-based approach
could achieve an accuracy of over 98% on
TREC-11 questions.
At the stage in query formulation,
QUALIFIER uses the knowledge of both the
Web and WordNet to expand the original query.
This is done by first using the original query to
search the web for top N,„ documents and extract-
ing additional web terms that co-occur frequently
in the local context of the query terms. It then
uses WordNet to find other terms in the retrieved
web documents that are lexically related to the
expanded query terms.
Given the expanded query, QUALIFIER em-
ploys the MG system (Witten et al 1999) to
search for top N ranking documents in the QA
corpus. Next, it selects candidate answer sen-
tences from the top returned documents. These
sentences are ranked based on certain criteria to
maximize the answer recall and precision (Yang
&amp; Chua 2003). NLP analysis is performed on
these candidate sentences to extract part-of-
speech tags, base Noun Phrases, Named Entities,
etc.
Finally, QUALIFIER performs answer selec-
tion by matching the expected answer type to the
NLP results. Named entity in the candidate sen-
tence is returned as the final answer if it fits the
expected answer type and is within a short dis-
tance to the original query.
The following section describes the details of
the query formulation and answer selection using
external recourses.
</bodyText>
<sectionHeader confidence="0.919807" genericHeader="method">
5 The Use of External Knowledge
</sectionHeader>
<bodyText confidence="0.9994875">
For the short, factual questions in TREC, the que-
ries are either too brief or do not fully cover the
</bodyText>
<equation confidence="0.586747666666667">
terms used in the corpus. Given a query, =
(o)
q2 ...qk ] usually with k&lt;=4, the prob-
</equation>
<bodyText confidence="0.788774047619048">
lem for retrieving all the documents relevant to
is that the query does not contain most of the
terms used in the document space to represent
the same concept. For example, given the ques-
tion: &amp;quot;What is the name of the volcano that de-
stroyed the ancient city of Pompeii?&amp;quot;, two of the
passages containing possible answer in the QA
corpus are:
a. 79 - Mount Vesuvius erupts and buries
Italian cities of Pompeii and Herculaneum.
b. In A.D. 79, long-dormant Mount Vesu-
vius erupted, burying the Roman cities of Pom-
peii and Herculaneum in volcanic ash.
As can be seen, there are very few common
content words between the question and the pas-
sages. Thus we resort to using general open re-
sources to overcome this problem. The external
general resources that can be readily used include
the Web, WordNet, Knowledge bases, and query
logs. In our system, we focus on the amalgama-
tion of the Web and WordNet.
</bodyText>
<subsectionHeader confidence="0.994782">
5.1 Using the Web
</subsectionHeader>
<bodyText confidence="0.999823083333334">
The Web is the most rapidly growing and com-
plete knowledge resource in the world now. The
terms in the relevant documents retrieved from
the Web are likely to be similar or even the same
as those in the QA corpus since they both contain
information about the facts of nature or the fac-
tual events in the history. Data redundancy of the
web documents plays an important role to effec-
tively retrieve the information for a certain entity
or an element of an event.
Aiming to solve the question-answer chasm at
the semantic and discourse levels, QUALIFIER
uses the Web as an additional resource to get
more knowledge of the entities and events. It
uses on the original content words in q&amp;quot; to re-
trieve the top N„, documents in the Web using
Google and then extracts the terms in those
documents that are highly correlated with the
original query terms. That is, for Vqi&amp;quot; Ea it
extracts the list of nearby non-trivial words, wi,
that are in the same sentence as q,&amp;quot; or within p
words away from q()• The system further ranks
all terms wik Elm, by computing their probabilities
of correlation with %&amp;quot; as:
</bodyText>
<equation confidence="0.939103666666667">
Pr ob(wik ) = ds(wik e))
(1)
ds(wik v e))
</equation>
<page confidence="0.988156">
366
</page>
<bodyText confidence="0.995471222222222">
where ds(wikile) gives the number of in-
stances that wk and q/°) appear together; and
ds(w,kVe ) gives the number of instances that
either wfic or qi(p) appears. Finally, QUALIFIER
merges all wi to form C for
For the above Pompeii example, the top 10
terms extracted from the Web are: &amp;quot;vesuvius 79
ad roman eruption herculaneum buried active
Italian&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.99987">
5.2 Using WordNet
</subsectionHeader>
<bodyText confidence="0.999927769230769">
The Web is useful at bridging the semantic and
discourse gaps by providing the words that occur
frequently with the original query terms in the
local context. It however, lacks information on
lexical relationships among these terms. In con-
trast to the Web, WordNet focuses on the lexical
knowledge fabric by unearthing the &amp;quot;synony-
mous&amp;quot; terms. Thus to overcome the QA gap at
the lexical and syntactic levels, QUALIFIER
looks up WordNet to fmd words that are lexically
related to the original content words. For the
aforementioned Pompeii example, we find the
followings by searching the glosses and synsets.
</bodyText>
<listItem confidence="0.372501333333333">
a. Ancient
-Gloss: &amp;quot;belonging to times long past especially
of the historical period before the fall of the
Western Roman Empire&amp;quot;
-Synset: {age-old, antique}
b. Volcano
</listItem>
<bodyText confidence="0.86319975">
-Gloss: &amp;quot;a fissure in the earth&apos;s crust (or in the
surface of some other planet) through which mol-
ten lava and gases erupt&amp;quot;
-Synset: {vent, crater}
c. Destroy
-Gloss: &amp;quot;destroy completely; damage irreparably&amp;quot;
-Synset: {ruin}
Obviously, the glosses and synsets of the terms
in g&amp;quot; contain useful terms that relate to potential
answer candidates in the QA corpus. Here we use
WordNet to extract the gloss words Gq and synset
words Sq for (0)
</bodyText>
<subsectionHeader confidence="0.998524">
5.3 Integration of External Resources
</subsectionHeader>
<bodyText confidence="0.933665157894737">
To link questions and answers at all the four lev-
els of gaps, i.e., the lexical, syntactic, semantic
and discourse levels, we need to combine the ex-
ternal knowledge sources. One approach is to
expand the query by adding the top k words in
C , and those in Gq and Sq. However, if we sim-
ply append all the terms, the resulting expanded
query will likely to be too broad and contain too
many terms out of context. Our experiments indi-
cate that in many cases, adding additional terms
from WordNet, i.e. those from Gq and Sq, adds
more noise than information to the query. In gen-
eral, we need to restrict the glosses and syno-
nyms to only those terms found in the web
documents, to ensure that they are in the right
context. We solve this problem by using Gq and
S to increase terms found in C as follow:
—q —q
Given wk E Cq:
</bodyText>
<listItem confidence="0.975027">
• if wk E Gq, increase wk by a;
• if wke Sq, increase wk by 13;
</listItem>
<bodyText confidence="0.9606958">
where 0 &lt; &lt; a &lt; 1.
The final weight for each term is normalized
and the top m terms above a certain cut-off
threshold cs are selected for expanding the origi-
nal query as:
</bodyText>
<equation confidence="0.998777">
(1) (o)
g = g + {top m terms E Cq with weights} (2)
</equation>
<bodyText confidence="0.990374">
where m=20 initially in our experiments.
For the Pompeii example, the final expanded
</bodyText>
<equation confidence="0.601323">
(1) i „
</equation>
<bodyText confidence="0.8271772">
query g s: volcano destroyed ancient city
Pompeii vesuvius eruption 79 ad roman hercula-
neum&amp;quot;. The expanded query contains many over-
lapping terms or concepts with the passages
containing the answers.
</bodyText>
<table confidence="0.998883666666667">
QA Event Element Query Term
Subject Volcano, vesuvius
Object Pompeii
Location roman
Time 79 ad
Description Destroyed, eruption, herculaneum
</table>
<tableCaption confidence="0.99843">
Table 2: Term Classification for Pompeii Example
</tableCaption>
<bodyText confidence="0.984142545454545">
If we classify the terms in the newly formu-
lated query (see Table 2), they are actually corre-
sponding to one or more of the QA event
elements we discussed in Section 3. One promis-
ing advantage of our approach is that we are able
to answer any factual questions about the ele-
ments in this QA event other than just &amp;quot;What is
the name of the volcano that destroyed the an-
cient city of Pompeii?&amp;quot;. For instance, we can eas-
ily handle questions like &amp;quot;When was the ancient
city of Pompeii destroyed?&amp;quot; and &amp;quot;Which two
</bodyText>
<page confidence="0.996381">
367
</page>
<bodyText confidence="0.998727333333333">
Roman cities were destroyed by Mount Vesu-
vius?&amp;quot; etc. with the same set of knowledge. Cur-
rently, we are exploring the use of Semantic
Perceptron Net (Liu &amp; Chua 2001) to derive se-
mantic word groups in order to form a more
structured utilization of external knowledge.
</bodyText>
<subsectionHeader confidence="0.9122045">
5.4 Document Retrieval &amp; Answer Se-
lection
</subsectionHeader>
<bodyText confidence="0.999773795454546">
Given q(1), QUALIFIER makes use of the MG
tool to retrieve up to N (N=50) relevant docu-
ments from the QA corpus. We choose Boolean
retrieval because of the short length of the que-
ries, and to avoid returning too many irrelevant
documents when using the similarity based re-
trieval. If q(1) does not return sufficient number of
relevant documents, the extra terms added is re-
duced and the Boolean search is repeated. There-
fore, we successively relax the constraint to
ensure precision.
QUALIFIER next performs sentence boundary
detection on the retrieved documents. It selects
the top k sentences by evaluating the similarity
between each of the sentences with the query in
terms of basic query terms, noun phrases, answer
target, etc.
Finally, it performs the tagging of fine-grained
named entity for the top K sentences. From these
sentences, it extracts the string that matches the
question classes (answer target) as the answer.
Once an answer is found in the top ith sentence,
the system will stop the search for the rest of (K-
O sentences. Sometimes, there may be more than
one matching strings in a single sentence. We
will choose the string, which is nearest to the
original query terms.
For some questions, the system cannot find
any answer and so we reduce the number of extra
terms (m&lt;20 in Equation 2) added to g&amp;quot; by p
(p=1). This is to ensure that the Boolean retrieval
process can retrieve more documents from the
QA corpus. It repeats the document/sentence re-
trieval and answer extraction process for up to L
such iterations (L=5). If it still cannot find an ex-
act answer at the end of 5 iterations, a NIL an-
swer is returned. We call this method successive
constraint relaxation. This strategy helps to in-
crease recall while preserving precision.
As an alternative to the successive constraint
relaxation using Boolean retrieval, similarity-
based search may be used to improve recall pos-
sibly at the expense of precision. We will inves-
tigate some of these issues in the next Section.
</bodyText>
<sectionHeader confidence="0.997933" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9997985">
We use all the 500 questions of TREC-11 QA
track as our test set. The performance of
QUALIFIER without the use of WordNet and
web is considered as the baseline.
</bodyText>
<subsectionHeader confidence="0.999516">
6.1 Effects of Web Search Strategies
</subsectionHeader>
<bodyText confidence="0.990911416666667">
We first study the effects of employing different
strategies to search the web on the QA perform-
ance. For Web search, we adopt Google as the
search engine and examine only snippets returned
by Google instead of looking at full web pages.
We study the performance of QUALIFIER by
varying the number of top ranked web pages re-
turned N, and the cut-off threshold a (see Equa-
tion 2) for selecting the terms in Cq to be added to
(0). The variations are:
a) The number of top ranked web pages re-
turned (Nw): 10, 25, 50, 75 and 100.
b) The cut-off thresholds (a): 0.1, 0.2, 0.3, 0.4,
and 0.5.
Table 3 summarizes the effects of these varia-
tions on the performance of TREC-11 questions.
Due to space constraint, Table 3 only shows the
precision score, P, which is the ratio of correct
answers returned by QUALIFIER. From the re-
sults, we can see that the best result is obtained
when we consider the top 75 ranked web pages,
and a term weight cut-off threshold of 0.2. The
finding is consistent with the results reported in
(Lin 2002) for the definition type questions.
</bodyText>
<table confidence="0.999696">
10 25 50 75 100
0.1 0.492 0.492 0.494 0.500 0.504
0.2 0.536 0.536 0.538 0.548 0.544
0.3 0.506 0.506 0.512 0.512 0.512
0.4 0.426 0.426 0.430 0.432 0.428
0.5 0.398 0.398 0.412 0.418 0.412
</table>
<tableCaption confidence="0.999839">
Table 3: The Precision Score of 25 Web Runs
</tableCaption>
<subsectionHeader confidence="0.999995">
6.2 Using External Resources
</subsectionHeader>
<bodyText confidence="0.999936">
To investigate the performance of combining
lexical knowledge such as WordNet and external
resource like the Web, we conduct several ex-
</bodyText>
<page confidence="0.996773">
368
</page>
<bodyText confidence="0.873108">
periments to test different uses or these re-
sources:
</bodyText>
<listItem confidence="0.9684635">
• Baseline: We perform QA without using the
external resources.
• WordNet: Here we perform QA by using
different types of lexical knowledge obtained
from WordNet. We use either the glosses G q, or
synset Sq or both. In these tests, we simply add
all related terms found in Gq or Sq into d ).
• Web: Here we add up to top m context words
from Cq into d l ) based on Equation (2).
• Web + WordNet: Here we combine both
Web and WordNet knowledge, but do not con-
strain the new terms from WordNet. This is to
test the effects of adding some WordNet terms
out of context.
• Web + WordNet with constraint as defined in
Section 5.3.
</listItem>
<bodyText confidence="0.999373428571429">
In these test, we examine the top 75 web snip-
pets returned by Google with a cut-off threshold
a of 0.2. Also, we use the answer patterns and the
evaluation script provided by NIST to score all
runs automatically. For each run, we compute P,
the precision, and CWS, the confidence-weighted
score. Table 4 summarizes the results of the tests.
</bodyText>
<table confidence="0.999461125">
Method P CWS
Baseline 0.438 0.440
Baseline + WordNet Gloss 0.442 0.448
Baseline + WordNet Synset 0.438 0.446
Baseline + WordNet (Gloss,Synset) 0.442 0.446
Baseline + Web 0.548 0.578
Baseline + Web + WordNet 0.552 0.588
Baseline + Web + WordNet + constraint 0388 0.610
</table>
<tableCaption confidence="0.999573">
Table 4: Different Query Formulation Methods
</tableCaption>
<bodyText confidence="0.856397428571429">
From Table 4, we can draw the following ob-
servations.
• The use of lexical knowledge from WordNet
without constraint does not seem to be effective
for QA, as compared to baseline. This is because
it tends to add too many terms out of context into
(1)
</bodyText>
<listItem confidence="0.9684015">
•
• Web-based query formulation improves the
</listItem>
<bodyText confidence="0.601595">
baseline performance by 25.1% in Precision and
31.5% in CWS. This confirms the results of
many studies that using Web to extract highly
correlated terms generally improves the QA per-
formance.
</bodyText>
<listItem confidence="0.807072142857143">
• The use of WordNet resource without con-
straint in conjunction with Web again does not
help QA performance.
• The best performance (P: 0.588, CWS:
0.610) is achieved when combining the Web and
WordNet with constraint as outlined in Section
5.3.
</listItem>
<subsectionHeader confidence="0.990429">
6.3 Boolean Search vs. Similarity Search
</subsectionHeader>
<bodyText confidence="0.999956913043478">
In all the above experiments, we employ succes-
sive constraint relaxation technique to perform
up to 5 iterations of Boolean search on the QA
corpus as outlined in Section 5.4. The intuition
here is that similarity-based search tends to return
too many irrelevant QA documents, thus de-
grades the overall precision of QA. Our observa-
tion of the Boolean-based approach is that we
tend to return too many NIL answers prema-
turely. In order to test our intuition and to maxi-
mize the chances of finding exact answers, we
conduct a series of tests by employing a combi-
nation of Boolean search and/or similarity-based
search.
The results are presented in Table 5. As can be
seen, the best result is obtained when performing
up to 5 successive relaxation iterations of Boo-
lean search followed by a similarity-based
search. This is the most thorough search process
we have conducted with the aim of finding an
exact answer if possible and only returning a NIL
answer as the last resort. It works well as our an-
swer selection process is quite strict.
</bodyText>
<table confidence="0.9996035">
Search Method P CWS
Boolean 0.386 0.426
Boolean+5iterations 0.580 0.610
Similarity 0.266 0.240
Boolean+Similarity 0.450 0.466
Boolean+5iterations+Similarity g.602 0.632
</table>
<tableCaption confidence="0.999662">
Table 5: Results of Boolean vs Similarity Search
</tableCaption>
<sectionHeader confidence="0.986095" genericHeader="conclusions">
7 Conclusion and Future Directions
</sectionHeader>
<bodyText confidence="0.999981916666667">
We have presented the QUALIFIER question
answering system. QUALIFIER employs a novel
approach to QA based on the intuition that there
exists implicit knowledge that connects an an-
swer to a question, and that this knowledge can
be used to discover the information about a QA
entity or different aspects of a QA event. Lexical
fabric like WordNet and external recourse like
the Web are integrated to find the linkage be-
tween questions and answers.
Our results obtained on the TREC-11 QA cor-
pus correlate well with the human assessment of
</bodyText>
<page confidence="0.996959">
369
</page>
<bodyText confidence="0.999922529411765">
answers&apos; correctness and demonstrate that our
approach is feasible and effective for open do-
main question answering.
We are currently refining our approach in sev-
eral directions. First, we are improving our query
formulation by considering a combination of lo-
cal context, global context and lexical term corre-
lations. Second, we are working towards
template-based approach on answer selection that
incorporates some of the current ideas on ques-
tion profiling and answer proofing, etc. Third, we
will explore the structured use of external re-
sources using the semantic perceptron net ap-
proach (Liu &amp; Chua 2001). Our long-term
research plan includes Interactive QA, and the
handling of more difficult analysis and opinion
type questions.
</bodyText>
<sectionHeader confidence="0.999672" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999819075">
AAAI Spring Symposium Series. 2002. Mining
Answers from Text and Knowledge Bases.
ACL-EACL. 2002. Workshop on Open-domain
Question Answering.
E. Brill, J. Lin M. Banko, S. Dumais, and A. Ng.
2002. Data-intensive question answering. Text RE-
trieval Conference (TREC 2001)
E. Brill, S. Dumais and M. Banko. 2002. An analysis
of the AskiVISR question-answering system. In
Proceedings of 2002 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2002).
S. Buchholz. 2002. Using grammatical relations, an-
swer frequencies and the World Wide Web for
TREC question Answering. In Proceedings of the
Tenth Text Retrieval Conference (TREC 2001).
J. Chen, A. R. Diekema, M. D. Taffet, N. McCracken,
N. E. Ozgencil, 0. Yilmazel, E. D. Liddy. 2002.
Question answering: CNLP at the TREC-10 ques-
tion answering track. In Proceedings of the Tenth
Text Retrieval Conference (TREC 2001).
C. Clarke, G. Cormack and T. Lynam. 2002. Web
reinforced question answering. In Proceedings of
the Tenth Text REtrieval Conference (TREC 2001).
C. Clarke, G. Cormack and T. Lynam. 2001. Exploit-
ing redundancy in question answering. In Proceed-
ings of the 24th Annual International ACM SIGIR
Conference on Research and Development in In-
formation Retrieval (SIGIR&apos;2001), 358-365.
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,
M. Surdeanu, R. Bunescu, R. Girju, V. Rus and P.
Morarescu. 2001. FALCON: Boosting knowledge
for question answering. In Proceedings of the Ninth
Text Retrieval Conference (TREC-9), 479-488.
E. Hovy, U. Hermjakob and C. Lin. 2002. The use of
external knowledge in factoid QA. In Proceedings
of the Tenth Text REtrieval Conference (TREC
2001).
C. Kwok, 0. Etzioni and D. Weld. 2001. Scaling
question answering to the Web. In Proceedings of
the 10th World Wide Web Conference (WWW&apos;10),
150-161.
X. Li and D. Roth. 2002. Learning Question Classifi-
ers. In Proceedings of the 19th International Con-
ference on Computational Linguistics, 2002
C.Y. Lin. 2002. The Effectiveness of Dictionmy and
Web-Based Answer Reranking. In Proceedings of
the 19th International Conference on Computa-
tional Linguistics (COLING 2002).
J. Liu and T. S. Chua. 2001 Building semantic percep-
tron net for topic spotting. In Proceedings of 37th
Meeting of Association of Computational Linguis-
tics (ACL 2001),370-377.
D. I. Moldovan and Vasile Rus. 2001.Logic Form
Transformation of WordNet and its Applicability to
Question Answering. In Proceedings of the ACL
2001 Conference, July 2001.
M. A. Pasca and S. M. Harabagiu. 2001. High per-
formance question/answering. In Proceedings of
the 24th Annuallnternational ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval (SIGIR&apos;2001), 366-374.
J. Prager, E. Brown, A. Coden and D. Radev. 2000.
Question answering by predictive annotation. In
Proceedings of the 23rd Annual International ACM
SIGIR Conference on Research and Development
in Information Retrieval (SIGIR&apos;2000), 184-191.
D. R. Radev, Weiguo Fan, Hong Qi, Harris Wu, and
Amardeep Grewal. 2002. Probabilistic question
answering from the web. In The Eleventh Interna-
tional World Wide Web Conference,2002.
E.M.Voorhees. 2002. Overview of the TREC 2001
Question Answering Track. In Proceedings of the
Tenth Text REtrieval Conference (TREC 2001)
I. Witten, A. Moffat, and T. Bell. 1999. Managing
Gigabytes. Morgan Kaufmann.
H. Yang and T. S. Chua. 2003. The Integration of
Lexical Knowledge and External Resources for
Question Answering. In Proceedings of the Tenth
Text REtrieval Conference (TREC 2002)
</reference>
<page confidence="0.99826">
370
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.249048">
<title confidence="0.998509">QUALIFIER: Question Answering by Lexical Fabric and External Resources</title>
<author confidence="0.999927">Hui Yang</author>
<affiliation confidence="0.999733">Computer Science National University of Singapore</affiliation>
<address confidence="0.487872">3 Science Drive 2, Singapore 117543</address>
<email confidence="0.929122">yangh@comp.nus.edu.sg</email>
<author confidence="0.967042">Tat-Seng Chua</author>
<affiliation confidence="0.9997385">Computer Science National University of Singapore</affiliation>
<address confidence="0.572631">3 Science Drive 2, Singapore 117543</address>
<email confidence="0.961401">chuats@comp.nus.edu.sg</email>
<abstract confidence="0.998744055555556">One of the major challenges in TRECstyle question-answering (QA) is to overcome the mismatch in the lexical representations in the query space and document space. This is particularly severe in QA as exact answers, rather than documents, are required in response to questions. Most current approaches overcome the mismatch problem by employing either data redundancy strategy through the use of Web or linguistic resources. This paper investigates the integration of lexical relations and Web knowledge to tackle this problem. The results obtained on TREC11 QA corpus indicate that our approach is both feasible and effective.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>AAAI Spring Symposium Series.</title>
<date>2002</date>
<contexts>
<context position="5938" citStr="(2002)" startWordPosition="959" endWordPosition="959">o introduce data redundancy for a more reliable answer extraction from local text collections. Radev et al [20] proposed a probabilistic algorithm that learns the best query paraphrase of a question searching the Web. Many groups (Buchholz 2002, Chen et al 2002, Harabagiu et al 2002, Hovy et al 2002.) working on question answering also employ a variety of linguistic resources, such as the partof-speech tagging, syntactic parsing, semantic relations, named entity extraction, dictionaries, WordNet, etc. Moldovan and Rus (2001) proposed the use of logic form transformation of WordNet for QA. Lin (2002) gave a detailed comparison of the Web-based and linguisticbased approaches to QA, and concluded that combining both approaches could lead to better performance on answering definition questions. 3 Design Consideration To effectively perform open domain QA, two fundamental problems must be solved. The first is to bridge the gap between the query and document spaces. Most recent QA systems adopt the following general pipelined approach to: (a) classify the question according to the type of its answer; (b) employ IR technology, with the question as a query, to retrieve a small portion of the doc</context>
</contexts>
<marker>2002</marker>
<rawString>AAAI Spring Symposium Series. 2002. Mining Answers from Text and Knowledge Bases.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ACL-EACL</author>
</authors>
<date>2002</date>
<booktitle>Workshop on Open-domain Question Answering.</booktitle>
<marker>ACL-EACL, 2002</marker>
<rawString>ACL-EACL. 2002. Workshop on Open-domain Question Answering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>J Lin M Banko</author>
<author>S Dumais</author>
<author>A Ng</author>
</authors>
<date>2002</date>
<booktitle>Data-intensive question answering. Text REtrieval Conference (TREC</booktitle>
<contexts>
<context position="3085" citStr="Brill et al 2002" startWordPosition="493" endWordPosition="496">tyle QA is to overcome the mismatch in the lexical representations between the query space and document space. This mismatch, also known as the QA gap, is caused by the differences in the set of terms used in the question formulation and answer strings in the corpus. Given a source, such as the QA corpus, that contains only a relatively small number of answers to a query, we are faced with the difficulty to map the questions to answers by way of uncovering the complex lexical, syntactic, or semantic relationships between the question and the answer strings. Recent redundancy-based approaches (Brill et al 2002, Clarke et al 2002, Kwok et al 2001, Radev et al 2001) proposed the use of data, in363 stead of methods, to do most of the work to bridge the QA gap. These methods suggest that the greater the answer redundancy in the source data collection, the more likely that we can find an answer that occurs in a simple relation to the question. With the availability of rich linguistic resources, we can also minimize the need to perform complex linguistic processing. However, this does not mean that NLP is now out of the picture. For some question/answer pairs, deep reasoning is still needed to relate the</context>
<context position="5142" citStr="Brill et al 2002" startWordPosition="823" endWordPosition="826">s 3 and 4 respectively discuss the design and architecture of the system. Section 5 elaborates on the use of external resources for QA, while Section 6 details the experimental results. Section 7 concludes the paper with discussions for future work. 2 Related Work The idea of using the external resources for question answering is an emerging topic of interest among the computational linguistic communities. The TREC-10 QA track demonstrated that the use of the Web redundancy could be exploited at different levels in the process of finding answers to natural language questions. Several studies (Brill et al 2002,Clarke et al 2002, Kwok et al 2001) suggested that the application of Web search can improve the precision of a QA system by 25-30%. A common feature of these approaches is to use the Web to introduce data redundancy for a more reliable answer extraction from local text collections. Radev et al [20] proposed a probabilistic algorithm that learns the best query paraphrase of a question searching the Web. Many groups (Buchholz 2002, Chen et al 2002, Harabagiu et al 2002, Hovy et al 2002.) working on question answering also employ a variety of linguistic resources, such as the partof-speech tagg</context>
</contexts>
<marker>Brill, Banko, Dumais, Ng, 2002</marker>
<rawString>E. Brill, J. Lin M. Banko, S. Dumais, and A. Ng. 2002. Data-intensive question answering. Text REtrieval Conference (TREC 2001)</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>S Dumais</author>
<author>M Banko</author>
</authors>
<title>An analysis of the AskiVISR question-answering system.</title>
<date>2002</date>
<booktitle>In Proceedings of 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="3085" citStr="Brill et al 2002" startWordPosition="493" endWordPosition="496">tyle QA is to overcome the mismatch in the lexical representations between the query space and document space. This mismatch, also known as the QA gap, is caused by the differences in the set of terms used in the question formulation and answer strings in the corpus. Given a source, such as the QA corpus, that contains only a relatively small number of answers to a query, we are faced with the difficulty to map the questions to answers by way of uncovering the complex lexical, syntactic, or semantic relationships between the question and the answer strings. Recent redundancy-based approaches (Brill et al 2002, Clarke et al 2002, Kwok et al 2001, Radev et al 2001) proposed the use of data, in363 stead of methods, to do most of the work to bridge the QA gap. These methods suggest that the greater the answer redundancy in the source data collection, the more likely that we can find an answer that occurs in a simple relation to the question. With the availability of rich linguistic resources, we can also minimize the need to perform complex linguistic processing. However, this does not mean that NLP is now out of the picture. For some question/answer pairs, deep reasoning is still needed to relate the</context>
<context position="5142" citStr="Brill et al 2002" startWordPosition="823" endWordPosition="826">s 3 and 4 respectively discuss the design and architecture of the system. Section 5 elaborates on the use of external resources for QA, while Section 6 details the experimental results. Section 7 concludes the paper with discussions for future work. 2 Related Work The idea of using the external resources for question answering is an emerging topic of interest among the computational linguistic communities. The TREC-10 QA track demonstrated that the use of the Web redundancy could be exploited at different levels in the process of finding answers to natural language questions. Several studies (Brill et al 2002,Clarke et al 2002, Kwok et al 2001) suggested that the application of Web search can improve the precision of a QA system by 25-30%. A common feature of these approaches is to use the Web to introduce data redundancy for a more reliable answer extraction from local text collections. Radev et al [20] proposed a probabilistic algorithm that learns the best query paraphrase of a question searching the Web. Many groups (Buchholz 2002, Chen et al 2002, Harabagiu et al 2002, Hovy et al 2002.) working on question answering also employ a variety of linguistic resources, such as the partof-speech tagg</context>
</contexts>
<marker>Brill, Dumais, Banko, 2002</marker>
<rawString>E. Brill, S. Dumais and M. Banko. 2002. An analysis of the AskiVISR question-answering system. In Proceedings of 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
</authors>
<title>Using grammatical relations, answer frequencies and the World Wide Web for TREC question Answering.</title>
<date>2002</date>
<booktitle>In Proceedings of the Tenth Text Retrieval Conference (TREC</booktitle>
<contexts>
<context position="5576" citStr="Buchholz 2002" startWordPosition="901" endWordPosition="902">trated that the use of the Web redundancy could be exploited at different levels in the process of finding answers to natural language questions. Several studies (Brill et al 2002,Clarke et al 2002, Kwok et al 2001) suggested that the application of Web search can improve the precision of a QA system by 25-30%. A common feature of these approaches is to use the Web to introduce data redundancy for a more reliable answer extraction from local text collections. Radev et al [20] proposed a probabilistic algorithm that learns the best query paraphrase of a question searching the Web. Many groups (Buchholz 2002, Chen et al 2002, Harabagiu et al 2002, Hovy et al 2002.) working on question answering also employ a variety of linguistic resources, such as the partof-speech tagging, syntactic parsing, semantic relations, named entity extraction, dictionaries, WordNet, etc. Moldovan and Rus (2001) proposed the use of logic form transformation of WordNet for QA. Lin (2002) gave a detailed comparison of the Web-based and linguisticbased approaches to QA, and concluded that combining both approaches could lead to better performance on answering definition questions. 3 Design Consideration To effectively perf</context>
</contexts>
<marker>Buchholz, 2002</marker>
<rawString>S. Buchholz. 2002. Using grammatical relations, answer frequencies and the World Wide Web for TREC question Answering. In Proceedings of the Tenth Text Retrieval Conference (TREC 2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chen</author>
<author>A R Diekema</author>
<author>M D Taffet</author>
<author>N McCracken</author>
<author>N E Ozgencil</author>
</authors>
<title>Question answering: CNLP at the TREC-10 question answering track.</title>
<date>2002</date>
<booktitle>In Proceedings of the Tenth Text Retrieval Conference (TREC</booktitle>
<contexts>
<context position="5593" citStr="Chen et al 2002" startWordPosition="903" endWordPosition="906"> use of the Web redundancy could be exploited at different levels in the process of finding answers to natural language questions. Several studies (Brill et al 2002,Clarke et al 2002, Kwok et al 2001) suggested that the application of Web search can improve the precision of a QA system by 25-30%. A common feature of these approaches is to use the Web to introduce data redundancy for a more reliable answer extraction from local text collections. Radev et al [20] proposed a probabilistic algorithm that learns the best query paraphrase of a question searching the Web. Many groups (Buchholz 2002, Chen et al 2002, Harabagiu et al 2002, Hovy et al 2002.) working on question answering also employ a variety of linguistic resources, such as the partof-speech tagging, syntactic parsing, semantic relations, named entity extraction, dictionaries, WordNet, etc. Moldovan and Rus (2001) proposed the use of logic form transformation of WordNet for QA. Lin (2002) gave a detailed comparison of the Web-based and linguisticbased approaches to QA, and concluded that combining both approaches could lead to better performance on answering definition questions. 3 Design Consideration To effectively perform open domain Q</context>
</contexts>
<marker>Chen, Diekema, Taffet, McCracken, Ozgencil, 2002</marker>
<rawString>J. Chen, A. R. Diekema, M. D. Taffet, N. McCracken, N. E. Ozgencil, 0. Yilmazel, E. D. Liddy. 2002. Question answering: CNLP at the TREC-10 question answering track. In Proceedings of the Tenth Text Retrieval Conference (TREC 2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Clarke</author>
<author>G Cormack</author>
<author>T Lynam</author>
</authors>
<title>Web reinforced question answering.</title>
<date>2002</date>
<booktitle>In Proceedings of the Tenth Text REtrieval Conference (TREC</booktitle>
<contexts>
<context position="3104" citStr="Clarke et al 2002" startWordPosition="497" endWordPosition="500">come the mismatch in the lexical representations between the query space and document space. This mismatch, also known as the QA gap, is caused by the differences in the set of terms used in the question formulation and answer strings in the corpus. Given a source, such as the QA corpus, that contains only a relatively small number of answers to a query, we are faced with the difficulty to map the questions to answers by way of uncovering the complex lexical, syntactic, or semantic relationships between the question and the answer strings. Recent redundancy-based approaches (Brill et al 2002, Clarke et al 2002, Kwok et al 2001, Radev et al 2001) proposed the use of data, in363 stead of methods, to do most of the work to bridge the QA gap. These methods suggest that the greater the answer redundancy in the source data collection, the more likely that we can find an answer that occurs in a simple relation to the question. With the availability of rich linguistic resources, we can also minimize the need to perform complex linguistic processing. However, this does not mean that NLP is now out of the picture. For some question/answer pairs, deep reasoning is still needed to relate the two. Many QA resea</context>
<context position="5160" citStr="Clarke et al 2002" startWordPosition="826" endWordPosition="829">ively discuss the design and architecture of the system. Section 5 elaborates on the use of external resources for QA, while Section 6 details the experimental results. Section 7 concludes the paper with discussions for future work. 2 Related Work The idea of using the external resources for question answering is an emerging topic of interest among the computational linguistic communities. The TREC-10 QA track demonstrated that the use of the Web redundancy could be exploited at different levels in the process of finding answers to natural language questions. Several studies (Brill et al 2002,Clarke et al 2002, Kwok et al 2001) suggested that the application of Web search can improve the precision of a QA system by 25-30%. A common feature of these approaches is to use the Web to introduce data redundancy for a more reliable answer extraction from local text collections. Radev et al [20] proposed a probabilistic algorithm that learns the best query paraphrase of a question searching the Web. Many groups (Buchholz 2002, Chen et al 2002, Harabagiu et al 2002, Hovy et al 2002.) working on question answering also employ a variety of linguistic resources, such as the partof-speech tagging, syntactic par</context>
</contexts>
<marker>Clarke, Cormack, Lynam, 2002</marker>
<rawString>C. Clarke, G. Cormack and T. Lynam. 2002. Web reinforced question answering. In Proceedings of the Tenth Text REtrieval Conference (TREC 2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Clarke</author>
<author>G Cormack</author>
<author>T Lynam</author>
</authors>
<title>Exploiting redundancy in question answering.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;2001),</booktitle>
<pages>358--365</pages>
<marker>Clarke, Cormack, Lynam, 2001</marker>
<rawString>C. Clarke, G. Cormack and T. Lynam. 2001. Exploiting redundancy in question answering. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;2001), 358-365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>D Moldovan</author>
<author>M Pasca</author>
<author>R Mihalcea</author>
<author>M Surdeanu</author>
<author>R Bunescu</author>
<author>R Girju</author>
<author>V Rus</author>
<author>P Morarescu</author>
</authors>
<title>FALCON: Boosting knowledge for question answering.</title>
<date>2001</date>
<booktitle>In Proceedings of the Ninth Text Retrieval Conference (TREC-9),</booktitle>
<pages>479--488</pages>
<marker>Harabagiu, Moldovan, Pasca, Mihalcea, Surdeanu, Bunescu, Girju, Rus, Morarescu, 2001</marker>
<rawString>S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea, M. Surdeanu, R. Bunescu, R. Girju, V. Rus and P. Morarescu. 2001. FALCON: Boosting knowledge for question answering. In Proceedings of the Ninth Text Retrieval Conference (TREC-9), 479-488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>U Hermjakob</author>
<author>C Lin</author>
</authors>
<title>The use of external knowledge in factoid QA.</title>
<date>2002</date>
<booktitle>In Proceedings of the Tenth Text REtrieval Conference (TREC</booktitle>
<contexts>
<context position="3950" citStr="Hovy et al 2002" startWordPosition="642" endWordPosition="645">re likely that we can find an answer that occurs in a simple relation to the question. With the availability of rich linguistic resources, we can also minimize the need to perform complex linguistic processing. However, this does not mean that NLP is now out of the picture. For some question/answer pairs, deep reasoning is still needed to relate the two. Many QA research groups have used a variety of linguistic resources — part-of-speech tagging, syntactic parsing, semantic relations, named entity extraction, WordNet, on-line dictionaries, query logs and ontologies, etc (Harabagiu et al 2002, Hovy et al 2002). This paper investigates the integration of both linguistic knowledge and external resources for TREC-style question answering. In particular, we describe a high performance question answering system called QUALIFIER (QUestion Answering by Lexical Fabric and External Resources) and analyze its effectiveness using the TREC-11 benchmark. Our results show that combining lexical information and external resources with a custom text search produces an effective question-answering system. The rest of the paper is organized as follows. Section 2 presents related work. Sections 3 and 4 respectively d</context>
<context position="5632" citStr="Hovy et al 2002" startWordPosition="911" endWordPosition="914">loited at different levels in the process of finding answers to natural language questions. Several studies (Brill et al 2002,Clarke et al 2002, Kwok et al 2001) suggested that the application of Web search can improve the precision of a QA system by 25-30%. A common feature of these approaches is to use the Web to introduce data redundancy for a more reliable answer extraction from local text collections. Radev et al [20] proposed a probabilistic algorithm that learns the best query paraphrase of a question searching the Web. Many groups (Buchholz 2002, Chen et al 2002, Harabagiu et al 2002, Hovy et al 2002.) working on question answering also employ a variety of linguistic resources, such as the partof-speech tagging, syntactic parsing, semantic relations, named entity extraction, dictionaries, WordNet, etc. Moldovan and Rus (2001) proposed the use of logic form transformation of WordNet for QA. Lin (2002) gave a detailed comparison of the Web-based and linguisticbased approaches to QA, and concluded that combining both approaches could lead to better performance on answering definition questions. 3 Design Consideration To effectively perform open domain QA, two fundamental problems must be sol</context>
</contexts>
<marker>Hovy, Hermjakob, Lin, 2002</marker>
<rawString>E. Hovy, U. Hermjakob and C. Lin. 2002. The use of external knowledge in factoid QA. In Proceedings of the Tenth Text REtrieval Conference (TREC 2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Etzioni</author>
<author>D Weld</author>
</authors>
<title>Scaling question answering to the Web.</title>
<date>2001</date>
<booktitle>In Proceedings of the 10th World Wide Web Conference (WWW&apos;10),</booktitle>
<pages>150--161</pages>
<marker>Etzioni, Weld, 2001</marker>
<rawString>C. Kwok, 0. Etzioni and D. Weld. 2001. Scaling question answering to the Web. In Proceedings of the 10th World Wide Web Conference (WWW&apos;10), 150-161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>D Roth</author>
</authors>
<title>Learning Question Classifiers.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics,</booktitle>
<contexts>
<context position="11268" citStr="Li &amp; Roth 2002" startWordPosition="1811" endWordPosition="1814">s. It uses a rulebased question classifier to perform the syntacticsemantic analysis of the questions and determines the question types in a two-level question taxonomy. The first level in the question taxonomy corresponds to the more general named entities 365 like Human, Location, Time, Number, Object, Description and Others. The second level contains question classes that correspond to finegrained named entities to facilitate accurate answer extraction. Examples of second level classes for, say Location, are Country, City, State, River, Mountain etc. The taxonomy is similar to that used in Li &amp; Roth 2002. Our rule-based approach could achieve an accuracy of over 98% on TREC-11 questions. At the stage in query formulation, QUALIFIER uses the knowledge of both the Web and WordNet to expand the original query. This is done by first using the original query to search the web for top N,„ documents and extracting additional web terms that co-occur frequently in the local context of the query terms. It then uses WordNet to find other terms in the retrieved web documents that are lexically related to the expanded query terms. Given the expanded query, QUALIFIER employs the MG system (Witten et al 199</context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>X. Li and D. Roth. 2002. Learning Question Classifiers. In Proceedings of the 19th International Conference on Computational Linguistics, 2002</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
</authors>
<title>The Effectiveness of Dictionmy and Web-Based Answer Reranking.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context position="5938" citStr="Lin (2002)" startWordPosition="958" endWordPosition="959">eb to introduce data redundancy for a more reliable answer extraction from local text collections. Radev et al [20] proposed a probabilistic algorithm that learns the best query paraphrase of a question searching the Web. Many groups (Buchholz 2002, Chen et al 2002, Harabagiu et al 2002, Hovy et al 2002.) working on question answering also employ a variety of linguistic resources, such as the partof-speech tagging, syntactic parsing, semantic relations, named entity extraction, dictionaries, WordNet, etc. Moldovan and Rus (2001) proposed the use of logic form transformation of WordNet for QA. Lin (2002) gave a detailed comparison of the Web-based and linguisticbased approaches to QA, and concluded that combining both approaches could lead to better performance on answering definition questions. 3 Design Consideration To effectively perform open domain QA, two fundamental problems must be solved. The first is to bridge the gap between the query and document spaces. Most recent QA systems adopt the following general pipelined approach to: (a) classify the question according to the type of its answer; (b) employ IR technology, with the question as a query, to retrieve a small portion of the doc</context>
<context position="22162" citStr="Lin 2002" startWordPosition="3742" endWordPosition="3743">added to (0). The variations are: a) The number of top ranked web pages returned (Nw): 10, 25, 50, 75 and 100. b) The cut-off thresholds (a): 0.1, 0.2, 0.3, 0.4, and 0.5. Table 3 summarizes the effects of these variations on the performance of TREC-11 questions. Due to space constraint, Table 3 only shows the precision score, P, which is the ratio of correct answers returned by QUALIFIER. From the results, we can see that the best result is obtained when we consider the top 75 ranked web pages, and a term weight cut-off threshold of 0.2. The finding is consistent with the results reported in (Lin 2002) for the definition type questions. 10 25 50 75 100 0.1 0.492 0.492 0.494 0.500 0.504 0.2 0.536 0.536 0.538 0.548 0.544 0.3 0.506 0.506 0.512 0.512 0.512 0.4 0.426 0.426 0.430 0.432 0.428 0.5 0.398 0.398 0.412 0.418 0.412 Table 3: The Precision Score of 25 Web Runs 6.2 Using External Resources To investigate the performance of combining lexical knowledge such as WordNet and external resource like the Web, we conduct several ex368 periments to test different uses or these resources: • Baseline: We perform QA without using the external resources. • WordNet: Here we perform QA by using different </context>
</contexts>
<marker>Lin, 2002</marker>
<rawString>C.Y. Lin. 2002. The Effectiveness of Dictionmy and Web-Based Answer Reranking. In Proceedings of the 19th International Conference on Computational Linguistics (COLING 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Liu</author>
<author>T S Chua</author>
</authors>
<title>Building semantic perceptron net for topic spotting.</title>
<date>2001</date>
<booktitle>In Proceedings of 37th Meeting of Association of Computational Linguistics (ACL</booktitle>
<pages>2001--370</pages>
<contexts>
<context position="18787" citStr="Liu &amp; Chua 2001" startWordPosition="3148" endWordPosition="3151">Table 2), they are actually corresponding to one or more of the QA event elements we discussed in Section 3. One promising advantage of our approach is that we are able to answer any factual questions about the elements in this QA event other than just &amp;quot;What is the name of the volcano that destroyed the ancient city of Pompeii?&amp;quot;. For instance, we can easily handle questions like &amp;quot;When was the ancient city of Pompeii destroyed?&amp;quot; and &amp;quot;Which two 367 Roman cities were destroyed by Mount Vesuvius?&amp;quot; etc. with the same set of knowledge. Currently, we are exploring the use of Semantic Perceptron Net (Liu &amp; Chua 2001) to derive semantic word groups in order to form a more structured utilization of external knowledge. 5.4 Document Retrieval &amp; Answer Selection Given q(1), QUALIFIER makes use of the MG tool to retrieve up to N (N=50) relevant documents from the QA corpus. We choose Boolean retrieval because of the short length of the queries, and to avoid returning too many irrelevant documents when using the similarity based retrieval. If q(1) does not return sufficient number of relevant documents, the extra terms added is reduced and the Boolean search is repeated. Therefore, we successively relax the cons</context>
</contexts>
<marker>Liu, Chua, 2001</marker>
<rawString>J. Liu and T. S. Chua. 2001 Building semantic perceptron net for topic spotting. In Proceedings of 37th Meeting of Association of Computational Linguistics (ACL 2001),370-377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D I</author>
</authors>
<title>Moldovan and Vasile Rus. 2001.Logic Form Transformation of WordNet and its Applicability to Question Answering.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACL 2001 Conference,</booktitle>
<marker>I, 2001</marker>
<rawString>D. I. Moldovan and Vasile Rus. 2001.Logic Form Transformation of WordNet and its Applicability to Question Answering. In Proceedings of the ACL 2001 Conference, July 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Pasca</author>
<author>S M Harabagiu</author>
</authors>
<title>High performance question/answering.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th Annuallnternational ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;2001),</booktitle>
<pages>366--374</pages>
<marker>Pasca, Harabagiu, 2001</marker>
<rawString>M. A. Pasca and S. M. Harabagiu. 2001. High performance question/answering. In Proceedings of the 24th Annuallnternational ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;2001), 366-374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
<author>E Brown</author>
<author>A Coden</author>
<author>D Radev</author>
</authors>
<title>Question answering by predictive annotation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;2000),</booktitle>
<pages>184--191</pages>
<marker>Prager, Brown, Coden, Radev, 2000</marker>
<rawString>J. Prager, E. Brown, A. Coden and D. Radev. 2000. Question answering by predictive annotation. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;2000), 184-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>Weiguo Fan</author>
<author>Hong Qi</author>
<author>Harris Wu</author>
<author>Amardeep Grewal</author>
</authors>
<title>Probabilistic question answering from the web.</title>
<date>2002</date>
<booktitle>In The Eleventh International World Wide Web Conference,2002.</booktitle>
<marker>Radev, Fan, Qi, Wu, Grewal, 2002</marker>
<rawString>D. R. Radev, Weiguo Fan, Hong Qi, Harris Wu, and Amardeep Grewal. 2002. Probabilistic question answering from the web. In The Eleventh International World Wide Web Conference,2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2002</date>
<booktitle>In Proceedings of the Tenth Text REtrieval Conference (TREC</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="1922" citStr="Voorhees 2002" startWordPosition="301" endWordPosition="302">to open-domain natural language questions, where a large text collection (termed the QA corpus) is used as the source for these answers. Contrary to traditional IR tasks, it is not acceptable for a QA system to retrieve a full document, or a paragraph, in response to a question. Contrary to traditional IE tasks, no prespecified domain restrictions are placed on the questions, which may be of any type and in any topic. Modern QA systems must therefore combine the strengths of traditional IR and NLP/IE to provide an apposite way to answering questions. The QA task in the TREC conference series (Voorhees 2002) has motivated much of the recent works focusing on fact-based, short-answer questions. Examples of such questions include: &amp;quot;Who is Tom Cruise married to?&amp;quot; or &amp;quot;How many chromosomes does a human zygote have?&amp;quot;. For the most recent TREC-11 conference, the task consists of 500 questions posed over a QA corpus containing more than one million newspaper articles. Instead of previous years&apos; 50-byte or 250- byte text fragments, exact answers are expected from the QA corpus with supports of documentary evidences. One of the major challenges in TREC-style QA is to overcome the mismatch in the lexical re</context>
</contexts>
<marker>Voorhees, 2002</marker>
<rawString>E.M.Voorhees. 2002. Overview of the TREC 2001 Question Answering Track. In Proceedings of the Tenth Text REtrieval Conference (TREC 2001) I. Witten, A. Moffat, and T. Bell. 1999. Managing Gigabytes. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yang</author>
<author>T S Chua</author>
</authors>
<title>The Integration of Lexical Knowledge and External Resources for Question Answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the Tenth Text REtrieval Conference (TREC</booktitle>
<contexts>
<context position="12119" citStr="Yang &amp; Chua 2003" startWordPosition="1957" endWordPosition="1960">g the original query to search the web for top N,„ documents and extracting additional web terms that co-occur frequently in the local context of the query terms. It then uses WordNet to find other terms in the retrieved web documents that are lexically related to the expanded query terms. Given the expanded query, QUALIFIER employs the MG system (Witten et al 1999) to search for top N ranking documents in the QA corpus. Next, it selects candidate answer sentences from the top returned documents. These sentences are ranked based on certain criteria to maximize the answer recall and precision (Yang &amp; Chua 2003). NLP analysis is performed on these candidate sentences to extract part-ofspeech tags, base Noun Phrases, Named Entities, etc. Finally, QUALIFIER performs answer selection by matching the expected answer type to the NLP results. Named entity in the candidate sentence is returned as the final answer if it fits the expected answer type and is within a short distance to the original query. The following section describes the details of the query formulation and answer selection using external recourses. 5 The Use of External Knowledge For the short, factual questions in TREC, the queries are eit</context>
</contexts>
<marker>Yang, Chua, 2003</marker>
<rawString>H. Yang and T. S. Chua. 2003. The Integration of Lexical Knowledge and External Resources for Question Answering. In Proceedings of the Tenth Text REtrieval Conference (TREC 2002)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>