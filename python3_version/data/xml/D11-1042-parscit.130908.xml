<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000022">
<title confidence="0.970366">
Corroborating Text Evaluation Results with Heterogeneous Measures
</title>
<author confidence="0.596414">
Enrique Amig´o † Julio Gonzalo † Jes´us Gim´enez ‡ Felisa Verdejo†
† UNED, Madrid
</author>
<email confidence="0.958807">
{enrique,julio,felisa}@lsi.uned.es
</email>
<address confidence="0.42693">
‡ UPC, Barcelona
</address>
<email confidence="0.998054">
{jgimenez}@lsi.upc.edu
</email>
<sectionHeader confidence="0.998593" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999900904761905">
Automatically produced texts (e.g. transla-
tions or summaries) are usually evaluated with
n-gram based measures such as BLEU or
ROUGE, while the wide set of more sophisti-
cated measures that have been proposed in the
last years remains largely ignored for practical
purposes. In this paper we first present an in-
depth analysis of the state of the art in order
to clarify this issue. After this, we formalize
and verify empirically a set of properties that
every text evaluation measure based on simi-
larity to human-produced references satisfies.
These properties imply that corroborating sys-
tem improvements with additional measures
always increases the overall reliability of the
evaluation process. In addition, the greater the
heterogeneity of the measures (which is mea-
surable) the higher their combined reliability.
These results support the use of heterogeneous
measures in order to consolidate text evalua-
tion results.
</bodyText>
<sectionHeader confidence="0.999467" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999547511111112">
The automatic evaluation of textual outputs is a
core issue in many Natural Language Processing
(NLP) tasks such as Natural Language Generation,
Machine Translation (MT) and Automatic Sum-
marization (AS). State-of-the-art automatic evalu-
ation methods all operate by rewarding similari-
ties between automatically-produced candidate out-
puts and manually-produced reference solutions, so-
called human references or models.
Over the last decade, a wide variety of measures,
based on different quality assumptions, have been
proposed. Recent work suggests exploiting exter-
nal knowledge sources and/or deep linguistic an-
notation, and measure combination (see Section 2).
However, original measures based on lexical match-
ing, such as BLEU (Papineni et al., 2001a) and
ROUGE (Lin, 2004) are still preferred as de facto
standards in MT and AS, respectively. There are,
in our opinion, two main reasons behind this fact.
First, the use of a common measure certainly allows
researchers to carry out objective comparisons be-
tween their work and other published results. Sec-
ond, the advantages of novel measures are not easy
to demonstrate in terms of correlation with human
judgements.
Our goal is not to answer which is the most re-
liable metric or to propose yet another novel mea-
sure. Rather than this, we first analyze in depth the
state of the art, concluding that it is not easy to de-
termine the reliability of a measure. In absence of a
clear proof of the advantages of novel measures, sys-
tem developers naturally tend to prefer well-known
standard measures. Second, we formalize and check
empirically two intrinsic properties that any evalua-
tion measure based on similarity to human-produced
references satisfies. Assuming that a measure satis-
fies a set of basic formal constraints, these properties
imply that corroborating a system comparison with
additional measures always increases the overall re-
liability of the evaluation process, even when the
added measures have a low correlation with human
judgements. In most papers, evaluation results are
corroborated with similar n-gram based measures
(eg. BLEU and ROUGE). However, according to
our second property, the greater the heterogeneity of
</bodyText>
<page confidence="0.983171">
455
</page>
<note confidence="0.957949">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 455–466,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.974644326797386">
the measures (which is measurable) the higher their 2007; Owczarzak et al., 2007a; Owczarzak et al.,
reliability. The practical implication is that, corrob- 2007b; Owczarzak et al., 2008; Chan and Ng,
orating evaluation results with measures based on 2008; Kahn et al., 2009), CCG parsing (Mehay and
higher linguistic levels increases the heterogeneity, Brew, 2007), syntactic constituents (Liu and Gildea,
and therefore, the reliability of evaluation results. 2005; Gim´enez and M`arquez, 2007), named entities
2 State of the Art (Reeder et al., 2001; Gim´enez and M`arquez, 2007),
2.1 Individual measures semantic roles (Gim´enez and M`arquez, 2007), dis-
Among NLP disciplines, MT probably has the course representations (Gim´enez, 2008), and textual
widest set of automatic evaluation measures. The entailment features (Pad´o et al., 2009). In general,
dominant approach to automatic MT evaluation is, when a higher linguistic level is incorporated, lin-
today, based on lexical metrics (also called n-gram guistic features at lower levels are preserved.
based metrics). These metrics work by rewarding The proposals for summarization evaluation are
lexical similarity between candidate translations and less numerous. Some proposals for AS tasks are
a set of manually-produced reference translations. based on syntactic units (Tratz and Hovy, 2008), de-
Lexical metrics can be classified according to how pendency triples (Owczarzak, 2009) or convolution
they compute similarity. Some are based on edit dis- kernels (Hirao et al., 2005) which reported some re-
tance, e.g., WER (Nießen et al., 2000), PER (Till- liability improvement over ROUGE in terms of cor-
mann et al., 1997), and TER (Snover et al., 2006). relation with human judgements.
Other metrics are based on computing lexical preci- In general, however, it is not easy to determine
sion, e.g., BLEU (Papineni et al., 2001b) and NIST clearly the contribution of deeper linguistic knowl-
(Doddington, 2002), lexical recall, e.g., ROUGE edge in those proposals. In the case of MT, im-
(Lin and Och, 2004a) and CDER (Leusch et al., provements versus BLEU have been reported (Liu
2006), or a balance between the two, e.g., GTM and Gildea, 2005; Kahn et al., 2009), but not over
(Melamed et al., 2003; Turian et al., 2003b), ME- a more elaborated metric such as METEOR (Mehay
TEOR (Banerjee and Lavie, 2005), BLANC (Lita et and Brew, 2007; Chan and Ng, 2008). Besides, con-
al., 2005), SIA (Liu and Gildea, 2006), MAXSIM troversial results on their performance at sentence vs
(Chan and Ng, 2008), and Ol (Gim´enez, 2008). system level have been reported in shared evaluation
The lexical measure BLEU has been criticized in tasks (Callison-Burch et al., 2008; Callison-Burch et
many ways. Some drawbacks of BLEU are the lack al., 2009; Callison-Burch et al., 2010).
of interpretability (Turian et al., 2003a), the fact that 2.2 Combined measures
it is not necessary to increase BLEU to improve sys- Several researchers have suggested integrating het-
tems (Callison-burch and Osborne, 2006), the over- erogeneous measures. Some of them optimize the
scoring of statistical MT systems (Le and Przybocki, measure combination function according to the met-
2005), the low reliability over rich morphology lan- ric’s ability to emulate the behavior of human as-
guages (Homola et al., 2009), or even the fact that a sessors (i.e., correlation with human assessments).
poor system translation of a book can obtain higher For instance, using linear combinations (Pad´o et al.,
BLEU results than a manually produced translation 2009; Liu and Gildea, 2007; Gim´enez and M`arquez,
(Culy and Riehemann, 2003). 2008), Decision Trees (Akiba et al., 2001; Quirk,
The reaction to these criticisms has been focused 2004), regression based algorithms (Paul et al.,
on the development of more sophisticated measures 2007; Albrecht and Hwa, 2007a; Albrecht and Hwa,
in which candidate and reference translations are 2007b) or a variety of supervised machine learn-
automatically annotated and compared at different ing algorithms(Quirk et al., 2005; Corston-Oliver et
linguistic levels. Some of the features employed al., 2001; Kulesza and Shieber, 2004; Gamon et al.,
include parts of speech (Popovic and Ney, 2007; 2005; Amig´o et al., 2005).
Gim´enez and M`arquez, 2007), syntactic dependen- Some of these works report evidence on the con-
cies (Liu and Gildea, 2005; Gim´enez and M`arquez, tribution of combining heterogeneous measures. For
456 instance, Albrecht and Hwa included syntax-based
measures together with lexical measures, outper- (Lin and Och, 2004a). Banerjee and Lavie (2005)
forming other combination schemes (Albrecht and argued that the reliability of metrics at the document
Hwa, 2007a; Albrecht and Hwa, 2007b). Liu and level can be due to averaging effects but might not
Gildea, after examining the contribution of each be robust across sentence translations. In order to
component metric, found that “metrics showing dif- address this issue, they computed the translation-by-
ferent properties of a sentence are more likely to translation correlation with human assessments (i.e.,
make a good combined metric”(Liu and Gildea, correlation at the sentence level).
2007). Akiba et al., which combined multiple edit- However, correlation with human judgements is
distance features based on lexical, morphosyntac- not enough to determine the reliability of measures.
tic and lexical semantic information, observed that First, correlation at sentence level (unlike correla-
their approach improved single editing distance for tion at system level) tends to be low and difficult to
several data sets (Akiba et al., 2001). More evi- interpret. Second, correlation at system and segment
dence was provided by Corston and Oliver. They levels can produce contradictory results. In (Amig´o
showed that results on the task of discriminating be- et al., 2009) it is observed that higher linguistic lev-
tween manual and automatic translations improve els in measures increases the correlation with human
when combining linguistic and n-gram based fea- judgements at the system level at the cost of corre-
tures. In addition, they showed that this mixed com- lation at the segment level. As far as we know, a
bination improved over the combination of linguistic clear explanation for these phenomena has not been
or n-gram based measures alone (Corston-Oliver et provided yet.
al., 2001). (Pad´o et al., 2009) reported a reliability Third, a high correlation at system level does
improvement by including measures based on tex- not ensure a high reliability. Culy and Rieheman
tual entailment in the set. In (Gim´enez and M`arquez, observed that, although BLEU can achieve a high
2008), a simple arithmetic mean of scores for com- correlation at system level in some test suites, it
bining measures at different linguistic levels was ap- over-scores a poor automatic translation of “Tom
plied with remarkable results in recent shared evalu- Sawyer” against a human produced translation (Culy
ation tasks (Callison-Burch et al., 2010). and Riehemann, 2003). This meta-evaluation crite-
2.3 Meta-evaluation criteria rion based on the ability to discern between man-
Meta-evaluation methods have been gradually intro- ual and automatic translations have been referred to
duced together with evaluation measures. For in- as human likeness (Amig´o et al., 2006), in contrast
stance, Papineni et al. (2001b) evaluated the reliabil- to correlation with human judgements which is re-
ity of the BLEU metric according to its ability to em- ferred to as human acceptability. Examples of meta-
ulate human assessors, as measured in terms of Pear- measures based on this criterion are ORANGE (Lin
son correlation with human assessments of adequacy and Och, 2004b) and KING (Amig´o et al., 2005).
and fluency at the document level. The measure In addition, many of the approaches to metric com-
NIST (Doddington, 2002) was meta-evaluated also bination described in Section 2.2 take human like-
in terms of correlation with human assessments, but ness as the optimization criterion (Corston-Oliver
over different document sources and for a varying et al., 2001; Kulesza and Shieber, 2004; Gamon et
number of references and segment sizes. Melamed al., 2005). The main advantage of meta-evaluation
et al. (2003) argued, at the time of introducing the based on human likeness is that, since human as-
GTM metric, that Pearson correlation coefficients sessments are not required, metrics can be evaluated
can be affected by scale properties. They suggested over larger test beds. However, the meta-evaluation
using the non-parametric Spearman correlation co- in terms of human likeness is difficult to interpret.
efficients instead. Lin and Och meta-evaluated 2.4 The use of evaluation measures
ROUGE over both Pearson and Spearman correla- In general, the state of the art includes a wide set
tion over a wide set of metrics, including NIST, of results that show the drawbacks of n-gram based
WER, PER, and variants of ROUGE, BLEU and measures as BLEU, and a wide set of proposals for
GTM. They obtained similar results in both cases new single and combined measures which are meta-
457
evaluated in terms of human acceptability (i.e., their ments implies growing x. Formally, if x ranges from
ability to emulate human judges, typically measured 0 to 1:
in terms of correlation with human judgements) or f(s) = f(g) ++ x(s, g) = 1
human-likeness (i.e., their ability to discern between (f(s) = f(s&apos;) U {e E f(g) \ f(s&apos;)}) -+ x(s, g) &gt; x(s&apos;, g)
automatic and human translations) (Amig´o et al., (f(s) = f(s&apos;) − {e E f(s&apos;) \ f(g)}) -+ x(s, g) &gt; x(s&apos;, g)
2006). However, the original measures BLEU and For instance, a random function and the reversal
ROUGE are still preferred. of a similarity funtion (f&apos;(s) = 1
We believe that one of the reasons is the lack of f(s)) do not satisfy
an in-depth study on to what extent providing ad- these constraints. While the F measure over Pre-
ditional evaluation results with other metrics con- cision and Recall satisfies these constraints1, pre-
tributes to the reliability of such results. The state of cision and recall in isolation do not satisfy all of
the art suggests that the use of heterogeneous mea- them: maximum recall can be achieved without re-
sures can improve the evaluation reliability. How- sembling the goldstandard text decomposition; and
ever, as far as we know, there is no comprehen- maximum precision can be achieved with only a few
sive analysis on the contribution of novel measures overlapped elements.
when corroborating evaluation results with addi- BLEU (Papineni et al., 2001a) computes the n-
tional measures. gram precision while the metric ROUGE (Lin and
3 Similarity Based Evaluation Measures Och, 2004a) computes the n-gram recall. How-
In general, automatic evaluation measures applied ever, in general, both metrics satisfy all the con-
in tasks like MT or AS are similarity measures be- straints, given that BLEU includes a brevity penalty
tween system outputs and human references. These and ROUGE penalizes or limits the system output
measures are related with precision, recall or overlap length. The measure METEOR creates an align-
over specific types of linguistic units. For instance, ment between the two strings (Banerjee and Lavie,
ROUGE measures n-gram recall. Other measures 2005). This overlap-based measure satisfies also the
that work at higher linguistic levels apply precision, previous constraints. Measures based on edit dis-
recall or overlap of linguistic components such as tance over n-grams (Tillmann et al., 1997; Nießen
dependency relations, grammatical categories, se- et al., 2000) or other linguistic units (Akiba et al.,
mantic roles, etc. 2001; Popovic and Ney, 2007) match also our def-
In order to delimit our hypothesis, let us first de- inition of similarity measure. The editing distance
fine what is a similarity measure in this context. Un- is minimum when the two compared text are equal.
fortunately, as far as we know, there is no formal The more the evaluated text contains elements from
concept covering the properties of current evaluation the gold-standard the more the editing distance is re-
similarity measures. A close concept is that of “met- duced (higher similarity). The word ordering can be
ric” or “distance function”. But, actually, measures also expressed in terms of a decomposition function.
such as ROUGE or BLEU are not proper “metrics”, A similar reasoning applies to every relevant mea-
because they do not satisfy the symmetry and the tri- sure in the state-of-the art.
angle inequality properties. Therefore, we need a 4 Data Sets and Measures
new definition. 4.1 Data sets
Being Q the universe of system outputs s and In this paper, we provide empirical results for
gold-standards g, we assume that a similarity mea- MT and AS. For MT, we use the data sets from
sure, in our context, is a function x : Q&apos; −-+ R such the Arabic-to-English (AE) and Chinese-to-English
that there exists a decomposition function f : Q −-+ (CE) NIST MT Evaluation campaigns in 2004 and
{e1..en} (e.g., words or other linguistic units or
relationships) satisfying the following constraints:
(i) maximum similarity is achieved only when then
the decomposition of the system output resembles
exactly the gold-standard decomposition; and (ii)
growing overlap or removing non overlapped ele-
458
1There is an exception. In an extreme case, when recall is
zero, removing non overlapped elements does not modify the F
measure.
AE2004 CE2004 AE2005 CE2005
#human-references 5 5 5 4
#systems 5 10 7 10
#system-outputs-assessed 5 10 6 5
#system-outputs 1,353 1,788 1,056 1,082
#outputs-assessed per-system 347 447 266 272
</bodyText>
<tableCaption confidence="0.977237">
Table 1: Description of the test beds from 2004 and 2005 NIST MT evaluation campaigns used in the experiments
throughout the paper.
</tableCaption>
<table confidence="0.999243333333333">
DUC 2005 DUC 2006
#human-references 3-4 3-4
#systems 32 35
#system-outputs-assessed 32 35
#system-outputs 50 50
#outputs-assessed per-system 50 50
</table>
<tableCaption confidence="0.9683605">
Table 2: Description of the test beds from 2005 and 2006 DUC evaluation campaigns used in the experiments through-
out the paper.
</tableCaption>
<bodyText confidence="0.9997260625">
20052. Both include two translations exercises: for
the 2005 campaign we contacted each participant
individually and asked for permission to use their
data3. In our experiments, we take the sum of ad-
equacy and fluency, both in a 1-5 scale, as a global
measure of quality (LDC, 2005). Thus, human as-
sessments are in a 2-10 scale. For AS, we have used
the AS test suites developed in the DUC 2005 and
DUC 2006 evaluation campaigns4. This AS task
was to generate a question focused summary of 250
words from a set of 25-50 documents to a complex
question. Summaries were evaluated according to
several criteria. Here, we will consider the respon-
siveness judgements, in which the quality score was
an integer between 1 and 5. See Tables 1 and 2 for a
brief quantitative description of these test beds.
</bodyText>
<footnote confidence="0.965198">
2http://www.nist.gov/speech/tests/mt
3We are grateful to a number of groups and companies who
</footnote>
<affiliation confidence="0.700682">
responded positively: University of Southern California Infor-
mation Sciences Institute (ISI), University of Maryland (UMD),
Johns Hopkins University &amp; University of Cambridge (JHU-
CU), IBM, University of Edinburgh, University of Aachen
(RWTH), National Research Council of Canada (NRC), Chi-
nese Academy of Sciences Institute of Computing Technology
(ICT), Instituto Trentino di Cultura - Centro per la Ricerca Sci-
entifica e Tecnologica(ITC-IRST), MITRE.
</affiliation>
<footnote confidence="0.914253">
4http://duc.nist.gov/
</footnote>
<subsectionHeader confidence="0.979535">
4.2 Measures
</subsectionHeader>
<bodyText confidence="0.999364346153846">
As for evaluation measures, for MT we have used a
rich set of 64 measures provided within the ASIYA
Toolkit (Gim´enez and M`arquez, 2010)5. This in-
cludes measures operating at different linguistic lev-
els: lexical, syntactic, and semantic. At the lexical
level this set includes variants of 8 measures em-
ployed in the state of the art: BLEU, NIST, GTM,
METEOR, ROUGE, WER, PER and TER. In addi-
tion, we have included a basic measure Ol that com-
putes the lexical overlap without considering word
ordering. All these measures have similar granular-
ity. They use n-grams of a varying length as the ba-
sic unit with additional information provided by lin-
guistic tools. The underlying similarity criteria in-
clude precision, recall, overlap, or edit rate, and the
decomposition functions include words, dependency
tree nodes (DP HWC, DP-Or, etc.), constituency
parsing (CP-STM), discourse roles (DR-Or), seman-
tic roles (SR-Or), named entities, etc. Further details
on the measure set may be found in the ASIYA tech-
nical manual (Gim´enez and M`arquez, 2010).
According to our computations, our measures
cover high and low correlations at both levels. Cor-
relation at system level spans between 0.63 and 0.95.
Correlations at sentence level ranges from 0.18 up to
0.54. We will discriminate between two subsets of
</bodyText>
<footnote confidence="0.960454">
5http://www.lsi.upc.edu/˜nlp/Asiya
</footnote>
<page confidence="0.994104">
459
</page>
<bodyText confidence="0.98728452173913">
measures. The first one includes those that decom- we can ensure a system improvement when all mea-
pose the text into words, n-grams, stems or lexical sures corroborate the result. Then the additive relia-
semantic tags. This set includes BLEU, ROUGE, bility property can be stated as:
NIST, GTM, PER and WER families. We will re- R(X U {x}) &gt; R(X)
fer to them as “lexical” measures. The second set We could think of violating this property by
are those that consider deeper linguistic levels such adding, for instance, a measure consisting of a ran-
as parts of speech, syntactic dependencies, syntactic dom function (x&apos;(s) = rand(0..1)) or a reversal of
constituents, etc. We will refer to them as “linguis- the original measure (x&apos;(s) = 1/x(s)). These kind
tic” measures. of measures, however, would not satisfy the con-
In the case of automatic summarization (AS), we straints defined in Section 3.
have employed the standard variants of ROUGE This property is based on the idea that similar-
(Lin, 2004). These 7 measures are ROUGE-{1..4}, ity with human references according to any aspect
ROUGE-SU, ROUGE-L and ROUGE-W. In addi- should not imply statistically a quality decrease. Al-
tion we have included the reversed precision version though our test suites includes measures with low
for each variant and the F measure of both. Notice correlation at segment and system level, we can con-
that the original ROUGE measures are oriented to firm empirically that all of them satisfy this property.
recall. In total, we have 21 measures for the sum- We have developed the following experiment:
marization task. All of them are based on n-gram taking all possible measure pairs in the test suites,
overlap. we have compared their reliability as a set versus the
5 Additive reliability maximal reliability of any of them (by computing
As discussed in Section 2, a number of recent pub- the difference R(X) − max(R(x1), R(x2)). Figure
lications address the problem of measure combi- 1 shows the obtained distribution of this difference
nation with successful results, specially when het- for our MT and AS test suites. Remarkably, in al-
erogeneous measures are combined. The following most every case this difference is positive.
property clarifies this issue and justifies the use of This result has a key implication: Corroborating
heterogeneous measures when corroborating evalu- evaluation results with a new measure, even when
ation results. It asserts that the reliability of system it has lower correlation with human judgements, in-
improvements always increases when the evaluation creases the reliability of results. Therefore, if the
result is corroborated by an additional similarity correlation with judgements is not determinant, the
measure, regardless of the correlation achieved by question is now what factor determines the contri-
the additional measure in isolation. bution of the new measures. According to the fol-
For the sake of clarity, in the rest of the paper, lowing property, this factor is the heterogeneity of
we will denote the similarity x(s, g) between sys- measures.
tem output s and human reference g by x(s). The 6 Heterogeneity
quality of a system output s will be referred to as This property states that the reliability of any mea-
Q(s). Let us define the reliability R(X) of a mea- sure combination is lower bounded by the hetero-
sure set as the probability of a real improvement (as geneity of the measure set. In other words, a single
measured by human judges) when a score improve- measure can be more or less reliable, but a system
ment is observed simultaneously for all measures in improvement according to all measures in an het-
the set X. : erogeneous set is reliable.
R(X) _— P(Q(s) &gt; Q(s&apos;)Jx(s) &gt; x(s&apos;) bx E X) Let us define the heterogeneity H(X) of a set of
According to this definition, we may not be able measures X as, given two system outputs s and s&apos;
to predict the quality of any system output (i.e. a such that g =� s =� s&apos; =� g (g is the reference
translation) with a highly reliable measure set, but text), the probability that there exist two measures
460 that contradict each other. That is:
H(X) _— P(Elx, x&apos; E X.x(s) &gt; x(s&apos;) n x&apos;(s) &lt; x&apos;(s&apos;))
</bodyText>
<figureCaption confidence="0.999918">
Figure 1: Additive reliability for metric pairs.
Figure 2: Heterogeneity vs. reliability in MT test suites.
</figureCaption>
<bodyText confidence="0.898788666666667">
Thus, given a set X of measures, the property
states that there exists a strict growing function F
such that:
</bodyText>
<equation confidence="0.387507">
R(X) ≥ F(H(X)) and H(X) = 1 → R(X) = 1
</equation>
<bodyText confidence="0.983541260869565">
In other words, the more the similarity measures
tend to contradict each other, the more a unanimous
improvement over all similarity measures is reliable.
Clearly, the harder it is that measures agree, the more
meaningful it is when they do.
The first part is derived from the Additive Re-
liability property. Intuitively, any individual mea-
sure has zero heterogeneity. Increasing the hetero-
geneity implies joining measures or measure sets
progressively. According to the Additive Reliabil-
ity property, this joining implies a reliability in-
crease. Therefore, the higher the heterogeneity, the
higher the minimum Reliability achieved by the cor-
responding measure sets.
The second part is derived from the Heterogeneity
definition. If H(X) = 1 then, for any distinct pair
of outputs that differ from the reference, there exist
at least two measures in the set contradicting each
other. That is, H(X) = 1 implies that:
∀s =6 s&apos; =6 g(∃x, x&apos; ∈ X.x(s) &gt; x(s&apos;) ∧ x&apos;(s) &lt; x&apos;(s&apos;))
Therefore, if one output improves the other ac-
cording to all measures, then the output must be
equal than the reference.
</bodyText>
<equation confidence="0.978325">
¬(∃x, x0 ∈ X.x(s) &gt; x(s0) ∧ x0(s) &lt; x0(s0)) →
¬(g =6s =6s0 =6g) → g = s ∨ g = s0
</equation>
<bodyText confidence="0.986867333333333">
According to the first constraint of similarity mea-
sures, a text that is equal to the reference achieves
the maximum score:
</bodyText>
<equation confidence="0.981514">
g = s → f(g) = g(s) → ∀x.x(s) ≥ x(s0)
</equation>
<bodyText confidence="0.925467">
Finally, if we assume that the reference (human pro-
duced texts) has a maximum quality, then it will
have equal or higher quality than the other output.
</bodyText>
<equation confidence="0.97789">
g = s → Q(s) ≥ Q(s0)
</equation>
<bodyText confidence="0.561553">
Therefore, the reliability of the measure set is maxi-
mal. In summary, if H(X) = 1 then:
</bodyText>
<equation confidence="0.9990935">
R(X) = P(Q(s) ≥ Q(s0)|x(s) ≥ x(s0) ∀x ∈ X) =
= P(Q(s) ≥ Q(s0)|s = g) = 1
</equation>
<bodyText confidence="0.994696727272727">
Figures 2 and 3 show the relationship between the
heterogeneity of randomly selected measure sets and
their reliability for the MT and summarization test
suites. As the figures show, the higher the hetero-
geneity, the higher the reliability of the measure set.
The results in AS are less pronounced due to the re-
dundancy in ROUGE measure.
Notice that the heterogeneity property does not
necessarily imply a high correlation between reli-
ability and heterogeneity. For instance, an ideal
single measure would have zero heterogeneity and
</bodyText>
<page confidence="0.998897">
461
</page>
<figureCaption confidence="0.9988635">
Figure 3: Heterogeneity vs. reliability in summarization
test suites.
</figureCaption>
<bodyText confidence="0.975225838709677">
achieve maximum reliability, appearing in the top
left area. The property rather brings us to the fol-
lowing situation: let us suppose that we have a set
of single measures available which achieve a certain
range of reliability. We can improve our system ac-
cording to any of these measures. Without human
assessments, we do not know what is the most re-
liable measure. But if we combine them, increas-
ing the heterogeneity, the minimal reliability of the
selected measures will be higher. This implies that
combining heterogeneous measures (e.g. at high lin-
guistic levels) that do not achieve high correlation
in isolation, is better than corroborating results with
any individual measure alone, such as ROUGE and
BLEU, which is the common practice in the state of
the art.
The main drawback of this property is that in-
creasing the heterogeneity implies a sensitivity re-
duction. For instance, if H(X) = 0.9, then only
for 10% of output pairs in the corpus there exists
an improvement according to all measures. In other
words, unanimous evaluation results from heteroge-
neous measures are reliable but harder to achieve for
the system developer. The next section investigates
on this issue.
Finally, Figure 4 shows that linguistic measures
increase the heterogeneity of measure sets. We have
generated sets of metrics of size 1 to 10 made up
by lexical or lexical and linguistic metrics. As the
figure shows, in the second case, the measure sets
achieve a higher heterogeneity.
</bodyText>
<figureCaption confidence="0.9940615">
Figure 4: Heterogeneity of lexical measures vs. lexical
and linguistic measures.
</figureCaption>
<sectionHeader confidence="0.777007" genericHeader="introduction">
7 Score thresholds vs. Additive Reliability
</sectionHeader>
<bodyText confidence="0.9982065">
According to the previous properties, corroborating
evaluation results with several measures increases
the reliability of evaluation results at the cost of sen-
sitivity. On the other hand, increasing the score
threshold of a single measure should have a similar
effect. Which is then the best methodology to im-
prove reliability? In this section we provide exper-
imental evidence on the relationship between both
ways of increasing reliability: we have found that,
corroborating evaluation results over single texts
with additional measures is more reliable than re-
quiring higher score differences according to any in-
dividual measure in the set. More specifically, we
have found that the reliability of a measure set is
higher than the reliability of each of the individual
measures at a similar level of sensitivity.
Formally, we define the sensitivity S(X) of a met-
ric set X as the probability of finding a score im-
provement within text pairs with a real (i.e. human
assessed) quality improvement:
</bodyText>
<equation confidence="0.935638">
S(X) = P(x(s) &gt; x(s&apos;)bx E X|Q(s) &gt; Q(s&apos;))
</equation>
<bodyText confidence="0.917371666666667">
Being Rth(x) and Sth(x) the reliability and sen-
sitivity of a single measure x for a certain increase
score threshold th:
</bodyText>
<page confidence="0.999099">
462
</page>
<figureCaption confidence="0.97312175">
Figure 5: Heterogeneity vs. reliability Gain for MT test
suites.
Figure 6: Heterogeneity vs. reliability Gain for MT test
suites.
</figureCaption>
<equation confidence="0.999902">
Rth(x) = P(Q(s) &gt; Q(s&apos;)|x(s) − x(s&apos;) &gt; th)
Sth(x) = P(x(s) − x(s&apos;) &gt; th|Q(s) &gt; Q(s&apos;))
</equation>
<bodyText confidence="0.997198">
The property that we want to check is that, at the
same sensitivity level, combining measures is more
reliable than increasing the score threshold of single
measures:
</bodyText>
<equation confidence="0.870578333333333">
S(X) = Sth(x).x E X −+ R(X) &gt; Rth(x)
Note that if we had a perfect measure xp such that
R(xp) = S(xp) = 1, then combining this measure
</equation>
<bodyText confidence="0.988708333333334">
with a low reliability measure xl would produce a
lower sensitivity, but the maximal reliability would
be preserved.
In order to confirm empirically this property, we
have developed the following experiment: (i) We
compute the reliability and sensitivity of randomly
chosen measure sets over single text pairs. We have
generated sets of 2,3,5,10,20 and 40 measures. In
the case of summarization corpora we have com-
bined up to 20 measures. In addition, we com-
pute also the heterogeneity H(X) of each measure
set; (ii) Experimenting with different values for the
threshold th, we compute the reliability of single
measures for all potential sensitivity levels; (iii) For
each measure set, we compare the reliability of the
measure set versus the reliability of single measures
at the same sensitivity level. We will refer to this as
the Reliability Gain:
</bodyText>
<equation confidence="0.5111515">
Reliability Gain =
R(X) − max{Rth(x)/x E X n Sth(x) = S(X)}
</equation>
<bodyText confidence="0.996927952380952">
If there are several reliability values with the same
sensitivity for a given single measures, we choose
the highest reliability value for the single measure.
Figures 5 and 6 illustrate the results for the MT
and AS corpora. The horizontal axis represents the
Heterogeneity of measure sets, while the vertical
axis represents the reliability gain. Remarkably, the
reliability gain is positive for all cases in our test
suites. The maximum reliability gain is 0.34 in the
case of MT and 0.08 for AS (note that summariza-
tion measures are more redundant in our corpora).
In both test suites, the largest information gains are
obtained with highly heterogeneous measure sets.
In summary, given comparable measures in terms
of reliability, corroborating evaluation results with
several measures is more effective than optimizing
systems according to the best measure in the set.
This empirical property provides an additional ev-
idence in favour of the use of heterogeneous mea-
sures and, in particular, of the use of linguistic mea-
sures in combination with standard lexical measures.
</bodyText>
<sectionHeader confidence="0.998435" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.940023">
In this paper, we have analyzed the state of the art in
order to clarify why novel text evaluation measures
</bodyText>
<page confidence="0.9994">
463
</page>
<bodyText confidence="0.99994535483871">
are not exploited by the community. Our first con-
clusion is that it is not easy to determine the reliabil-
ity of measures, which is highly corpus-dependent
and often contradictory when comparing correlation
with human judgements at segment vs. system lev-
els.
In order to tackle this issue, we have studied a
number of properties that suggest the convenience of
using heterogeneous measures to corroborate eval-
uation results. According to these properties, we
can ensure that, even when if we can not determine
the reliability of individual measures, corroborating
a system improvement with additional measures al-
ways increases the reliability of the results. In ad-
dition, the more heterogeneous the measures em-
ployed (which is measurable), the higher the relia-
bility of the results. But perhaps the most impor-
tant practical finding is that the reliability at similar
sensitivity levels by corroborating evaluation results
with several measures is always higher than improv-
ing systems according to any of the combined mea-
sures in isolation.
These properties point to the practical advantages
of considering linguistic knowledge (beyond lexi-
cal information) in measures, even if they do not
achieve a high correlation with human judgements.
Our experiments show that linguistic knowledge in-
creases the heterogeneity of measure sets, which
in turn increases the reliability of evaluation results
when corroborating system comparisons with sev-
eral measures.
</bodyText>
<sectionHeader confidence="0.976233" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9488015">
This work has been partially funded by the Spanish
Government (Holopedia, TIN2010-21128-C02 and
OpenMT-2, TIN2009-14675-C03) and the Euro-
pean Community’s Seventh Framework Programme
(FP7/2007-2013) under grant agreement number
247762 (FAUST project, FP7-ICT-2009-4-247762).
</bodyText>
<sectionHeader confidence="0.995292" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.985471166666667">
Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita.
2001. Using Multiple Edit Distances to Automatically
Rank Machine Translation Output. In Proceedings of
Machine Translation Summit VIII, pages 15–20.
Joshua Albrecht and Rebecca Hwa. 2007a. A Re-
examination of Machine Learning Approaches for
Sentence-Level MT Evaluation. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 880–887.
Joshua Albrecht and Rebecca Hwa. 2007b. Regression
for Sentence-Level MT Evaluation with Pseudo Refer-
ences. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 296–303.
Enrique Amig´o, Julio Gonzalo, Anselmo Pe nas, and Fe-
lisa Verdejo. 2005. QARLA: a Framework for the
Evaluation of Automatic Summarization. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 280–289.
Enrique Amig´o, Jes´us Gim´enez, Julio Gonzalo, and Llu´ıs
M`arquez. 2006. MT Evaluation: Human-Like vs. Hu-
man Acceptable. In Proceedings of the Joint 21st In-
ternational Conference on Computational Linguistics
and the 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL), pages 17–
24.
Enrique Amig´o, Jes´us Gim´enez, Julio Gonzalo, and Fe-
lisa Verdejo. 2009. The contribution of linguis-
tic features to automatic machine translation evalua-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 1 - Volume 1, ACL ’09,
pages 306–314, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization.
Chris Callison-burch and Miles Osborne. 2006. Re-
evaluating the role of bleu in machine translation re-
search. In In EACL, pages 249–256.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 70–106.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009 work-
shop on statistical machine translation. In Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17–53. Revised August 2010.
</reference>
<page confidence="0.999017">
464
</page>
<reference confidence="0.853313897196262">
Yee Seng Chan and Hwee Tou Ng. 2008. MAXSIM:
A maximum similarity metric for machine translation
evaluation. In Proceedings of ACL-08: HLT, pages
55–62.
Simon Corston-Oliver, Michael Gamon, and Chris
Brockett. 2001. A Machine Learning Approach to the
Automatic Evaluation of Machine Translation. In Pro-
ceedings of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 140–147.
Christopher Culy and Susanne Z. Riehemann. 2003. The
Limits of N-gram Translation Evaluation Metrics. In
Proceedings of MT-SUMMIT IX, pages 1–8.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2nd Inter-
national Conference on Human Language Technology,
pages 138–145.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-Level MT evaluation without refer-
ence translations: beyond language modeling. In Pro-
ceedings of EAMT, pages 103–111.
Jes´us Gim´enez and Llu´ıs M`arquez. 2007. Linguistic
Features for Automatic Evaluation of Heterogeneous
MT Systems. In Proceedings of the ACL Workshop on
Statistical Machine Translation, pages 256–264.
Jes´us Gim´enez and Llu´ıs M`arquez. 2008. Hetero-
geneous Automatic MT Evaluation Through Non-
Parametric Metric Combinations. In Proceedings of
the Third International Joint Conference on Natural
Language Processing (IJCNLP), pages 319–326.
Jes´us Gim´enez and Llu´ıs M`arquez. 2010. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathemat-
ical Linguistics, 1(94):77–86.
Jes´us Gim´enez. 2008. Empirical Machine Transla-
tion and its Evaluation. Ph.D. thesis, Universitat
Polit`ecnica de Catalunya.
Tsutomu Hirao, Manabu Okumura, and Hideki Isozaki.
2005. Kernel-based approach for automatic evaluation
of natural language generation technologies: Applica-
tion to automatic summarization. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 145–152, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
Petr Homola, Vladislav Kuboˇn, and Pavel Pecina. 2009.
A simple automatic mt evaluation metric. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, StatMT ’09, pages 33–36, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Jeremy G. Kahn, Matthew Snover, and Mari Ostendorf.
2009. Expected Dependency Pair Match: Predicting
translation quality with expected syntactic structure.
Machine Translation.
Alex Kulesza and Stuart M. Shieber. 2004. A learning
approach to improving sentence-level MT evaluation.
In Proceedings of the 10th International Conference
on Theoretical and Methodological Issues in Machine
Translation (TMI), pages 75–84.
LDC. 2005. Linguistic Data Annotation Spec-
ification: Assessment of Adequacy and Flu-
ency in Translations. Revision 1.5. Tech-
nical report, Linguistic Data Consortium.
http://www.ldc.upenn.edu/Projects/
TIDES/Translation/TransAssess04.pdf.
Audrey Le and Mark Przybocki. 2005. NIST 2005 ma-
chine translation evaluation official results. In Official
release of automatic evaluation scores for all submis-
sions, August.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006.
CDER: Efficient MT Evaluation Using Block Move-
ments. In Proceedings of 11th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics (EACL), pages 241–248.
Chin-Yew Lin and Franz Josef Och. 2004a. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statics. In Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics (ACL).
Chin-Yew Lin and Franz Josef Och. 2004b. ORANGE: a
Method for Evaluating Automatic Evaluation Metrics
for Machine Translation. In Proceedings of the 20th
International Conference on Computational Linguis-
tics (COLING).
Chin-Yew Lin. 2004. Rouge: A Package for Auto-
matic Evaluation of Summaries. In Marie-Francine
Moens and Stan Szpakowicz, editors, Text Summariza-
tion Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
Lucian Vlad Lita, Monica Rogati, and Alon Lavie. 2005.
BLANC: Learning Evaluation Metrics for MT. In
Proceedings of the Joint Conference on Human Lan-
guage Technology and Empirical Methods in Natural
Language Processing (HLT-EMNLP), pages 740–747.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings ofACL Workshop on Intrinsic and Extrinsic Eval-
uation Measures for MT and/or Summarization, pages
25–32.
Ding Liu and Daniel Gildea. 2006. Stochastic Iter-
ative Alignment for Machine Translation Evaluation.
In Proceedings of the Joint 21st International Confer-
ence on Computational Linguistics and the 44th An-
nual Meeting of the Association for Computational
Linguistics (COLING-ACL), pages 539–546.
</reference>
<page confidence="0.998064">
465
</page>
<reference confidence="0.999698269230769">
Ding Liu and Daniel Gildea. 2007. Source-Language
Features and Maximum Correlation Training for Ma-
chine Translation Evaluation. In Proceedings of the
2007 Meeting of the North American Chapter of the
Association for Computational Linguistics (NAACL),
pages 41–48.
Dennis Mehay and Chris Brew. 2007. BLEUATRE:
Flattening Syntactic Dependencies for MT Evaluation.
In Proceedings of the 11th Conference on Theoreti-
cal and Methodological Issues in Machine Translation
(TMI).
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Translation. In
Proceedings of the Joint Conference on Human Lan-
guage Technology and the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL).
Sonja Nießen, Franz Josef Och, Gregor Leusch, and Her-
mann Ney. 2000. An Evaluation Tool for Machine
Translation: Fast Evaluation for MT Research. In Pro-
ceedings of the 2nd International Conference on Lan-
guage Resources and Evaluation (LREC).
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007a. Dependency-Based Automatic Evalua-
tion for Machine Translation. In Proceedings of SSST,
NAACL-HLT/AMTA Workshop on Syntax and Struc-
ture in Statistical Translation, pages 80–87.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2007b. Labelled Dependencies in Machine Transla-
tion Evaluation. In Proceedings of the ACL Workshop
on Statistical Machine Translation, pages 104–111.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2008. Evaluating machine translation with lfg depen-
dencies. Machine Translation, 21(2):95–119.
Karolina Owczarzak. 2009. Depeval(summ):
dependency-based evaluation for automatic sum-
maries. In ACL-IJCNLP ’09: Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1, pages
190–198, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Sebastian Pad´o, Michael Galley, Dan Jurafsky, and
Christopher D. Manning. 2009. Robust machine
translation evaluation with entailment features. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 297–305.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001a.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311–318, Philadelphia, jul.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001b. Bleu: a method for automatic evalu-
ation of machine translation, RC22176. Technical re-
port, IBM T.J. Watson Research Center.
Michael Paul, Andrew Finch, and Eiichiro Sumita. 2007.
Reducing Human Assessments of Machine Transla-
tion Quality to Binary Classifiers. In Proceedings of
the 11th Conference on Theoretical and Methodologi-
cal Issues in Machine Translation (TMI).
Maja Popovic and Hermann Ney. 2007. Word Error
Rates: Decomposition over POS classes and Applica-
tions for Error Analysis. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
48–55, Prague, Czech Republic, June. Association for
Computational Linguistics.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency Treelet Translation: Syntactically Informed
Phrasal SMT. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 271–279.
Chris Quirk. 2004. Training a Sentence-Level Machine
Translation Confidence Metric. In Proceedings of the
4th International Conference on Language Resources
and Evaluation (LREC), pages 825–828.
Florence Reeder, Keith Miller, Jennifer Doyon, and John
White. 2001. The Naming of Things and the Confu-
sion of Tongues: an MT Metric. In Proceedings of
the Workshop on MT Evaluation ”Who did what to
whom?” at Machine Translation Summit VIII, pages
55–59.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Americas
(AMTA), pages 223–231.
Christoph Tillmann, Stefan Vogel, Hermann Ney, A. Zu-
biaga, and H. Sawaf. 1997. Accelerated DP based
Search for Statistical Translation. In Proceedings of
European Conference on Speech Communication and
Technology.
Stephen Tratz and Eduard Hovy. 2008. Summarization
evaluation using transformed basic elements. In In
Proceedings of TAC-08. Gaithersburg, Maryland.
Joseph Turian, Luke Shen, and I. Dan Melamed. 2003a.
Evaluation of machine translation and its evaluation.
In In Proceedings of MT Summit IX, pages 386–393.
Joseph P. Turian, Luke Shen, and I. Dan Melamed.
2003b. Evaluation of Machine Translation and its
Evaluation. In Proceedings of MT SUMMIT IX.
</reference>
<page confidence="0.999597">
466
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.819217">
<title confidence="0.999854">Corroborating Text Evaluation Results with Heterogeneous Measures</title>
<author confidence="0.997828">Amig´o Gonzalo Gim´enez</author>
<affiliation confidence="0.914335">Madrid</affiliation>
<address confidence="0.909343">Barcelona</address>
<abstract confidence="0.999039136363637">Automatically produced texts (e.g. translations or summaries) are usually evaluated with based measures such as BLEU or ROUGE, while the wide set of more sophisticated measures that have been proposed in the last years remains largely ignored for practical purposes. In this paper we first present an indepth analysis of the state of the art in order to clarify this issue. After this, we formalize and verify empirically a set of properties that every text evaluation measure based on similarity to human-produced references satisfies. These properties imply that corroborating system improvements with additional measures always increases the overall reliability of the evaluation process. In addition, the greater the the measures (which is measurable) the higher their combined reliability. These results support the use of heterogeneous measures in order to consolidate text evaluation results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yasuhiro Akiba</author>
<author>Kenji Imamura</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Using Multiple Edit Distances to Automatically Rank Machine Translation Output.</title>
<date>2001</date>
<booktitle>In Proceedings of Machine Translation Summit VIII,</booktitle>
<pages>15--20</pages>
<contexts>
<context position="7240" citStr="Akiba et al., 2001" startWordPosition="1107" endWordPosition="1110">res. Some of them optimize the scoring of statistical MT systems (Le and Przybocki, measure combination function according to the met2005), the low reliability over rich morphology lan- ric’s ability to emulate the behavior of human asguages (Homola et al., 2009), or even the fact that a sessors (i.e., correlation with human assessments). poor system translation of a book can obtain higher For instance, using linear combinations (Pad´o et al., BLEU results than a manually produced translation 2009; Liu and Gildea, 2007; Gim´enez and M`arquez, (Culy and Riehemann, 2003). 2008), Decision Trees (Akiba et al., 2001; Quirk, The reaction to these criticisms has been focused 2004), regression based algorithms (Paul et al., on the development of more sophisticated measures 2007; Albrecht and Hwa, 2007a; Albrecht and Hwa, in which candidate and reference translations are 2007b) or a variety of supervised machine learnautomatically annotated and compared at different ing algorithms(Quirk et al., 2005; Corston-Oliver et linguistic levels. Some of the features employed al., 2001; Kulesza and Shieber, 2004; Gamon et al., include parts of speech (Popovic and Ney, 2007; 2005; Amig´o et al., 2005). Gim´enez and M`a</context>
<context position="9212" citStr="Akiba et al., 2001" startWordPosition="1401" endWordPosition="1404">s of a sentence are more likely to translation correlation with human assessments (i.e., make a good combined metric”(Liu and Gildea, correlation at the sentence level). 2007). Akiba et al., which combined multiple edit- However, correlation with human judgements is distance features based on lexical, morphosyntac- not enough to determine the reliability of measures. tic and lexical semantic information, observed that First, correlation at sentence level (unlike correlatheir approach improved single editing distance for tion at system level) tends to be low and difficult to several data sets (Akiba et al., 2001). More evi- interpret. Second, correlation at system and segment dence was provided by Corston and Oliver. They levels can produce contradictory results. In (Amig´o showed that results on the task of discriminating be- et al., 2009) it is observed that higher linguistic levtween manual and automatic translations improve els in measures increases the correlation with human when combining linguistic and n-gram based fea- judgements at the system level at the cost of corretures. In addition, they showed that this mixed com- lation at the segment level. As far as we know, a bination improved over </context>
</contexts>
<marker>Akiba, Imamura, Sumita, 2001</marker>
<rawString>Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita. 2001. Using Multiple Edit Distances to Automatically Rank Machine Translation Output. In Proceedings of Machine Translation Summit VIII, pages 15–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Albrecht</author>
<author>Rebecca Hwa</author>
</authors>
<title>A Reexamination of Machine Learning Approaches for Sentence-Level MT Evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>880--887</pages>
<contexts>
<context position="7426" citStr="Albrecht and Hwa, 2007" startWordPosition="1135" endWordPosition="1138">an- ric’s ability to emulate the behavior of human asguages (Homola et al., 2009), or even the fact that a sessors (i.e., correlation with human assessments). poor system translation of a book can obtain higher For instance, using linear combinations (Pad´o et al., BLEU results than a manually produced translation 2009; Liu and Gildea, 2007; Gim´enez and M`arquez, (Culy and Riehemann, 2003). 2008), Decision Trees (Akiba et al., 2001; Quirk, The reaction to these criticisms has been focused 2004), regression based algorithms (Paul et al., on the development of more sophisticated measures 2007; Albrecht and Hwa, 2007a; Albrecht and Hwa, in which candidate and reference translations are 2007b) or a variety of supervised machine learnautomatically annotated and compared at different ing algorithms(Quirk et al., 2005; Corston-Oliver et linguistic levels. Some of the features employed al., 2001; Kulesza and Shieber, 2004; Gamon et al., include parts of speech (Popovic and Ney, 2007; 2005; Amig´o et al., 2005). Gim´enez and M`arquez, 2007), syntactic dependen- Some of these works report evidence on the concies (Liu and Gildea, 2005; Gim´enez and M`arquez, tribution of combining heterogeneous measures. For 456 </context>
</contexts>
<marker>Albrecht, Hwa, 2007</marker>
<rawString>Joshua Albrecht and Rebecca Hwa. 2007a. A Reexamination of Machine Learning Approaches for Sentence-Level MT Evaluation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), pages 880–887.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Albrecht</author>
<author>Rebecca Hwa</author>
</authors>
<title>Regression for Sentence-Level MT Evaluation with Pseudo References.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>296--303</pages>
<contexts>
<context position="7426" citStr="Albrecht and Hwa, 2007" startWordPosition="1135" endWordPosition="1138">an- ric’s ability to emulate the behavior of human asguages (Homola et al., 2009), or even the fact that a sessors (i.e., correlation with human assessments). poor system translation of a book can obtain higher For instance, using linear combinations (Pad´o et al., BLEU results than a manually produced translation 2009; Liu and Gildea, 2007; Gim´enez and M`arquez, (Culy and Riehemann, 2003). 2008), Decision Trees (Akiba et al., 2001; Quirk, The reaction to these criticisms has been focused 2004), regression based algorithms (Paul et al., on the development of more sophisticated measures 2007; Albrecht and Hwa, 2007a; Albrecht and Hwa, in which candidate and reference translations are 2007b) or a variety of supervised machine learnautomatically annotated and compared at different ing algorithms(Quirk et al., 2005; Corston-Oliver et linguistic levels. Some of the features employed al., 2001; Kulesza and Shieber, 2004; Gamon et al., include parts of speech (Popovic and Ney, 2007; 2005; Amig´o et al., 2005). Gim´enez and M`arquez, 2007), syntactic dependen- Some of these works report evidence on the concies (Liu and Gildea, 2005; Gim´enez and M`arquez, tribution of combining heterogeneous measures. For 456 </context>
</contexts>
<marker>Albrecht, Hwa, 2007</marker>
<rawString>Joshua Albrecht and Rebecca Hwa. 2007b. Regression for Sentence-Level MT Evaluation with Pseudo References. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), pages 296–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Amig´o</author>
<author>Julio Gonzalo</author>
</authors>
<title>Anselmo Pe nas, and Felisa Verdejo.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>280--289</pages>
<marker>Amig´o, Gonzalo, 2005</marker>
<rawString>Enrique Amig´o, Julio Gonzalo, Anselmo Pe nas, and Felisa Verdejo. 2005. QARLA: a Framework for the Evaluation of Automatic Summarization. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 280–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Amig´o</author>
<author>Jes´us Gim´enez</author>
<author>Julio Gonzalo</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>MT Evaluation: Human-Like vs. Human Acceptable.</title>
<date>2006</date>
<booktitle>In Proceedings of the Joint 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL),</booktitle>
<pages>17--24</pages>
<marker>Amig´o, Gim´enez, Gonzalo, M`arquez, 2006</marker>
<rawString>Enrique Amig´o, Jes´us Gim´enez, Julio Gonzalo, and Llu´ıs M`arquez. 2006. MT Evaluation: Human-Like vs. Human Acceptable. In Proceedings of the Joint 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL), pages 17– 24.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Enrique Amig´o</author>
<author>Jes´us Gim´enez</author>
<author>Julio Gonzalo</author>
<author>Felisa Verdejo</author>
</authors>
<title>The contribution of linguistic features to automatic machine translation evaluation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL ’09,</booktitle>
<pages>306--314</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Amig´o, Gim´enez, Gonzalo, Verdejo, 2009</marker>
<rawString>Enrique Amig´o, Jes´us Gim´enez, Julio Gonzalo, and Felisa Verdejo. 2009. The contribution of linguistic features to automatic machine translation evaluation. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL ’09, pages 306–314, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</booktitle>
<contexts>
<context position="5916" citStr="Banerjee and Lavie, 2005" startWordPosition="899" endWordPosition="902">gements. Other metrics are based on computing lexical preci- In general, however, it is not easy to determine sion, e.g., BLEU (Papineni et al., 2001b) and NIST clearly the contribution of deeper linguistic knowl(Doddington, 2002), lexical recall, e.g., ROUGE edge in those proposals. In the case of MT, im(Lin and Och, 2004a) and CDER (Leusch et al., provements versus BLEU have been reported (Liu 2006), or a balance between the two, e.g., GTM and Gildea, 2005; Kahn et al., 2009), but not over (Melamed et al., 2003; Turian et al., 2003b), ME- a more elaborated metric such as METEOR (Mehay TEOR (Banerjee and Lavie, 2005), BLANC (Lita et and Brew, 2007; Chan and Ng, 2008). Besides, conal., 2005), SIA (Liu and Gildea, 2006), MAXSIM troversial results on their performance at sentence vs (Chan and Ng, 2008), and Ol (Gim´enez, 2008). system level have been reported in shared evaluation The lexical measure BLEU has been criticized in tasks (Callison-Burch et al., 2008; Callison-Burch et many ways. Some drawbacks of BLEU are the lack al., 2009; Callison-Burch et al., 2010). of interpretability (Turian et al., 2003a), the fact that 2.2 Combined measures it is not necessary to increase BLEU to improve sys- Several res</context>
<context position="8171" citStr="Banerjee and Lavie (2005)" startWordPosition="1245" endWordPosition="1248">tomatically annotated and compared at different ing algorithms(Quirk et al., 2005; Corston-Oliver et linguistic levels. Some of the features employed al., 2001; Kulesza and Shieber, 2004; Gamon et al., include parts of speech (Popovic and Ney, 2007; 2005; Amig´o et al., 2005). Gim´enez and M`arquez, 2007), syntactic dependen- Some of these works report evidence on the concies (Liu and Gildea, 2005; Gim´enez and M`arquez, tribution of combining heterogeneous measures. For 456 instance, Albrecht and Hwa included syntax-based measures together with lexical measures, outper- (Lin and Och, 2004a). Banerjee and Lavie (2005) forming other combination schemes (Albrecht and argued that the reliability of metrics at the document Hwa, 2007a; Albrecht and Hwa, 2007b). Liu and level can be due to averaging effects but might not Gildea, after examining the contribution of each be robust across sentence translations. In order to component metric, found that “metrics showing dif- address this issue, they computed the translation-byferent properties of a sentence are more likely to translation correlation with human assessments (i.e., make a good combined metric”(Liu and Gildea, correlation at the sentence level). 2007). A</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-burch</author>
<author>Miles Osborne</author>
</authors>
<title>Reevaluating the role of bleu in machine translation research. In</title>
<date>2006</date>
<booktitle>In EACL,</booktitle>
<pages>249--256</pages>
<contexts>
<context position="6594" citStr="Callison-burch and Osborne, 2006" startWordPosition="1006" endWordPosition="1009"> 2008). Besides, conal., 2005), SIA (Liu and Gildea, 2006), MAXSIM troversial results on their performance at sentence vs (Chan and Ng, 2008), and Ol (Gim´enez, 2008). system level have been reported in shared evaluation The lexical measure BLEU has been criticized in tasks (Callison-Burch et al., 2008; Callison-Burch et many ways. Some drawbacks of BLEU are the lack al., 2009; Callison-Burch et al., 2010). of interpretability (Turian et al., 2003a), the fact that 2.2 Combined measures it is not necessary to increase BLEU to improve sys- Several researchers have suggested integrating hettems (Callison-burch and Osborne, 2006), the over- erogeneous measures. Some of them optimize the scoring of statistical MT systems (Le and Przybocki, measure combination function according to the met2005), the low reliability over rich morphology lan- ric’s ability to emulate the behavior of human asguages (Homola et al., 2009), or even the fact that a sessors (i.e., correlation with human assessments). poor system translation of a book can obtain higher For instance, using linear combinations (Pad´o et al., BLEU results than a manually produced translation 2009; Liu and Gildea, 2007; Gim´enez and M`arquez, (Culy and Riehemann, 20</context>
</contexts>
<marker>Callison-burch, Osborne, 2006</marker>
<rawString>Chris Callison-burch and Miles Osborne. 2006. Reevaluating the role of bleu in machine translation research. In In EACL, pages 249–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Further meta-evaluation of machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>70--106</pages>
<contexts>
<context position="6264" citStr="Callison-Burch et al., 2008" startWordPosition="956" endWordPosition="959"> al., provements versus BLEU have been reported (Liu 2006), or a balance between the two, e.g., GTM and Gildea, 2005; Kahn et al., 2009), but not over (Melamed et al., 2003; Turian et al., 2003b), ME- a more elaborated metric such as METEOR (Mehay TEOR (Banerjee and Lavie, 2005), BLANC (Lita et and Brew, 2007; Chan and Ng, 2008). Besides, conal., 2005), SIA (Liu and Gildea, 2006), MAXSIM troversial results on their performance at sentence vs (Chan and Ng, 2008), and Ol (Gim´enez, 2008). system level have been reported in shared evaluation The lexical measure BLEU has been criticized in tasks (Callison-Burch et al., 2008; Callison-Burch et many ways. Some drawbacks of BLEU are the lack al., 2009; Callison-Burch et al., 2010). of interpretability (Turian et al., 2003a), the fact that 2.2 Combined measures it is not necessary to increase BLEU to improve sys- Several researchers have suggested integrating hettems (Callison-burch and Osborne, 2006), the over- erogeneous measures. Some of them optimize the scoring of statistical MT systems (Le and Przybocki, measure combination function according to the met2005), the low reliability over rich morphology lan- ric’s ability to emulate the behavior of human asguages </context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2008</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2008. Further meta-evaluation of machine translation. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 70–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Findings of the 2009 workshop on statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation.</booktitle>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of the 2009 workshop on statistical machine translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Kay Peterson</author>
<author>Mark Przybocki</author>
<author>Omar Zaidan</author>
</authors>
<title>Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>17--53</pages>
<location>Revised</location>
<contexts>
<context position="6370" citStr="Callison-Burch et al., 2010" startWordPosition="973" endWordPosition="976">ildea, 2005; Kahn et al., 2009), but not over (Melamed et al., 2003; Turian et al., 2003b), ME- a more elaborated metric such as METEOR (Mehay TEOR (Banerjee and Lavie, 2005), BLANC (Lita et and Brew, 2007; Chan and Ng, 2008). Besides, conal., 2005), SIA (Liu and Gildea, 2006), MAXSIM troversial results on their performance at sentence vs (Chan and Ng, 2008), and Ol (Gim´enez, 2008). system level have been reported in shared evaluation The lexical measure BLEU has been criticized in tasks (Callison-Burch et al., 2008; Callison-Burch et many ways. Some drawbacks of BLEU are the lack al., 2009; Callison-Burch et al., 2010). of interpretability (Turian et al., 2003a), the fact that 2.2 Combined measures it is not necessary to increase BLEU to improve sys- Several researchers have suggested integrating hettems (Callison-burch and Osborne, 2006), the over- erogeneous measures. Some of them optimize the scoring of statistical MT systems (Le and Przybocki, measure combination function according to the met2005), the low reliability over rich morphology lan- ric’s ability to emulate the behavior of human asguages (Homola et al., 2009), or even the fact that a sessors (i.e., correlation with human assessments). poor sy</context>
<context position="10613" citStr="Callison-Burch et al., 2010" startWordPosition="1622" endWordPosition="1625">009) reported a reliability Third, a high correlation at system level does improvement by including measures based on tex- not ensure a high reliability. Culy and Rieheman tual entailment in the set. In (Gim´enez and M`arquez, observed that, although BLEU can achieve a high 2008), a simple arithmetic mean of scores for com- correlation at system level in some test suites, it bining measures at different linguistic levels was ap- over-scores a poor automatic translation of “Tom plied with remarkable results in recent shared evalu- Sawyer” against a human produced translation (Culy ation tasks (Callison-Burch et al., 2010). and Riehemann, 2003). This meta-evaluation crite2.3 Meta-evaluation criteria rion based on the ability to discern between manMeta-evaluation methods have been gradually intro- ual and automatic translations have been referred to duced together with evaluation measures. For in- as human likeness (Amig´o et al., 2006), in contrast stance, Papineni et al. (2001b) evaluated the reliabil- to correlation with human judgements which is reity of the BLEU metric according to its ability to em- ferred to as human acceptability. Examples of metaulate human assessors, as measured in terms of Pear- measu</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Peterson, Przybocki, Zaidan, 2010</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Peterson, Mark Przybocki, and Omar Zaidan. 2010. Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 17–53. Revised August 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>MAXSIM: A maximum similarity metric for machine translation evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>55--62</pages>
<contexts>
<context position="5967" citStr="Chan and Ng, 2008" startWordPosition="909" endWordPosition="912">i- In general, however, it is not easy to determine sion, e.g., BLEU (Papineni et al., 2001b) and NIST clearly the contribution of deeper linguistic knowl(Doddington, 2002), lexical recall, e.g., ROUGE edge in those proposals. In the case of MT, im(Lin and Och, 2004a) and CDER (Leusch et al., provements versus BLEU have been reported (Liu 2006), or a balance between the two, e.g., GTM and Gildea, 2005; Kahn et al., 2009), but not over (Melamed et al., 2003; Turian et al., 2003b), ME- a more elaborated metric such as METEOR (Mehay TEOR (Banerjee and Lavie, 2005), BLANC (Lita et and Brew, 2007; Chan and Ng, 2008). Besides, conal., 2005), SIA (Liu and Gildea, 2006), MAXSIM troversial results on their performance at sentence vs (Chan and Ng, 2008), and Ol (Gim´enez, 2008). system level have been reported in shared evaluation The lexical measure BLEU has been criticized in tasks (Callison-Burch et al., 2008; Callison-Burch et many ways. Some drawbacks of BLEU are the lack al., 2009; Callison-Burch et al., 2010). of interpretability (Turian et al., 2003a), the fact that 2.2 Combined measures it is not necessary to increase BLEU to improve sys- Several researchers have suggested integrating hettems (Callis</context>
</contexts>
<marker>Chan, Ng, 2008</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2008. MAXSIM: A maximum similarity metric for machine translation evaluation. In Proceedings of ACL-08: HLT, pages 55–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Corston-Oliver</author>
<author>Michael Gamon</author>
<author>Chris Brockett</author>
</authors>
<title>A Machine Learning Approach to the Automatic Evaluation of Machine Translation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>140--147</pages>
<marker>Corston-Oliver, Gamon, Brockett, 2001</marker>
<rawString>Simon Corston-Oliver, Michael Gamon, and Chris Brockett. 2001. A Machine Learning Approach to the Automatic Evaluation of Machine Translation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL), pages 140–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Culy</author>
<author>Susanne Z Riehemann</author>
</authors>
<title>The Limits of N-gram Translation Evaluation Metrics.</title>
<date>2003</date>
<booktitle>In Proceedings of MT-SUMMIT IX,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="7197" citStr="Culy and Riehemann, 2003" startWordPosition="1100" endWordPosition="1103">rch and Osborne, 2006), the over- erogeneous measures. Some of them optimize the scoring of statistical MT systems (Le and Przybocki, measure combination function according to the met2005), the low reliability over rich morphology lan- ric’s ability to emulate the behavior of human asguages (Homola et al., 2009), or even the fact that a sessors (i.e., correlation with human assessments). poor system translation of a book can obtain higher For instance, using linear combinations (Pad´o et al., BLEU results than a manually produced translation 2009; Liu and Gildea, 2007; Gim´enez and M`arquez, (Culy and Riehemann, 2003). 2008), Decision Trees (Akiba et al., 2001; Quirk, The reaction to these criticisms has been focused 2004), regression based algorithms (Paul et al., on the development of more sophisticated measures 2007; Albrecht and Hwa, 2007a; Albrecht and Hwa, in which candidate and reference translations are 2007b) or a variety of supervised machine learnautomatically annotated and compared at different ing algorithms(Quirk et al., 2005; Corston-Oliver et linguistic levels. Some of the features employed al., 2001; Kulesza and Shieber, 2004; Gamon et al., include parts of speech (Popovic and Ney, 2007; 2</context>
</contexts>
<marker>Culy, Riehemann, 2003</marker>
<rawString>Christopher Culy and Susanne Z. Riehemann. 2003. The Limits of N-gram Translation Evaluation Metrics. In Proceedings of MT-SUMMIT IX, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using N-gram CoOccurrence Statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd International Conference on Human Language Technology,</booktitle>
<pages>138--145</pages>
<contexts>
<context position="5521" citStr="Doddington, 2002" startWordPosition="829" endWordPosition="831">2008), deLexical metrics can be classified according to how pendency triples (Owczarzak, 2009) or convolution they compute similarity. Some are based on edit dis- kernels (Hirao et al., 2005) which reported some retance, e.g., WER (Nießen et al., 2000), PER (Till- liability improvement over ROUGE in terms of cormann et al., 1997), and TER (Snover et al., 2006). relation with human judgements. Other metrics are based on computing lexical preci- In general, however, it is not easy to determine sion, e.g., BLEU (Papineni et al., 2001b) and NIST clearly the contribution of deeper linguistic knowl(Doddington, 2002), lexical recall, e.g., ROUGE edge in those proposals. In the case of MT, im(Lin and Och, 2004a) and CDER (Leusch et al., provements versus BLEU have been reported (Liu 2006), or a balance between the two, e.g., GTM and Gildea, 2005; Kahn et al., 2009), but not over (Melamed et al., 2003; Turian et al., 2003b), ME- a more elaborated metric such as METEOR (Mehay TEOR (Banerjee and Lavie, 2005), BLANC (Lita et and Brew, 2007; Chan and Ng, 2008). Besides, conal., 2005), SIA (Liu and Gildea, 2006), MAXSIM troversial results on their performance at sentence vs (Chan and Ng, 2008), and Ol (Gim´enez,</context>
<context position="11475" citStr="Doddington, 2002" startWordPosition="1760" endWordPosition="1761"> with evaluation measures. For in- as human likeness (Amig´o et al., 2006), in contrast stance, Papineni et al. (2001b) evaluated the reliabil- to correlation with human judgements which is reity of the BLEU metric according to its ability to em- ferred to as human acceptability. Examples of metaulate human assessors, as measured in terms of Pear- measures based on this criterion are ORANGE (Lin son correlation with human assessments of adequacy and Och, 2004b) and KING (Amig´o et al., 2005). and fluency at the document level. The measure In addition, many of the approaches to metric comNIST (Doddington, 2002) was meta-evaluated also bination described in Section 2.2 take human likein terms of correlation with human assessments, but ness as the optimization criterion (Corston-Oliver over different document sources and for a varying et al., 2001; Kulesza and Shieber, 2004; Gamon et number of references and segment sizes. Melamed al., 2005). The main advantage of meta-evaluation et al. (2003) argued, at the time of introducing the based on human likeness is that, since human asGTM metric, that Pearson correlation coefficients sessments are not required, metrics can be evaluated can be affected by sca</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic Evaluation of Machine Translation Quality Using N-gram CoOccurrence Statistics. In Proceedings of the 2nd International Conference on Human Language Technology, pages 138–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Anthony Aue</author>
<author>Martine Smets</author>
</authors>
<title>Sentence-Level MT evaluation without reference translations: beyond language modeling.</title>
<date>2005</date>
<booktitle>In Proceedings of EAMT,</booktitle>
<pages>103--111</pages>
<marker>Gamon, Aue, Smets, 2005</marker>
<rawString>Michael Gamon, Anthony Aue, and Martine Smets. 2005. Sentence-Level MT evaluation without reference translations: beyond language modeling. In Proceedings of EAMT, pages 103–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Linguistic Features for Automatic Evaluation of Heterogeneous MT Systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL Workshop on Statistical Machine Translation,</booktitle>
<pages>256--264</pages>
<marker>Gim´enez, M`arquez, 2007</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2007. Linguistic Features for Automatic Evaluation of Heterogeneous MT Systems. In Proceedings of the ACL Workshop on Statistical Machine Translation, pages 256–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Heterogeneous Automatic MT Evaluation Through NonParametric Metric Combinations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third International Joint Conference on Natural Language Processing (IJCNLP),</booktitle>
<pages>319--326</pages>
<marker>Gim´enez, M`arquez, 2008</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2008. Heterogeneous Automatic MT Evaluation Through NonParametric Metric Combinations. In Proceedings of the Third International Joint Conference on Natural Language Processing (IJCNLP), pages 319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Asiya: An Open Toolkit for Automatic Machine Translation (Meta-)Evaluation.</title>
<date>2010</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<volume>1</volume>
<issue>94</issue>
<marker>Gim´enez, M`arquez, 2010</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2010. Asiya: An Open Toolkit for Automatic Machine Translation (Meta-)Evaluation. The Prague Bulletin of Mathematical Linguistics, 1(94):77–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
</authors>
<title>Empirical Machine Translation and its Evaluation.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Universitat Polit`ecnica de Catalunya.</institution>
<marker>Gim´enez, 2008</marker>
<rawString>Jes´us Gim´enez. 2008. Empirical Machine Translation and its Evaluation. Ph.D. thesis, Universitat Polit`ecnica de Catalunya.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tsutomu Hirao</author>
<author>Manabu Okumura</author>
<author>Hideki Isozaki</author>
</authors>
<title>Kernel-based approach for automatic evaluation of natural language generation technologies: Application to automatic summarization.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>145--152</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="5095" citStr="Hirao et al., 2005" startWordPosition="758" endWordPosition="761">r linguistic level is incorporated, lintoday, based on lexical metrics (also called n-gram guistic features at lower levels are preserved. based metrics). These metrics work by rewarding The proposals for summarization evaluation are lexical similarity between candidate translations and less numerous. Some proposals for AS tasks are a set of manually-produced reference translations. based on syntactic units (Tratz and Hovy, 2008), deLexical metrics can be classified according to how pendency triples (Owczarzak, 2009) or convolution they compute similarity. Some are based on edit dis- kernels (Hirao et al., 2005) which reported some retance, e.g., WER (Nießen et al., 2000), PER (Till- liability improvement over ROUGE in terms of cormann et al., 1997), and TER (Snover et al., 2006). relation with human judgements. Other metrics are based on computing lexical preci- In general, however, it is not easy to determine sion, e.g., BLEU (Papineni et al., 2001b) and NIST clearly the contribution of deeper linguistic knowl(Doddington, 2002), lexical recall, e.g., ROUGE edge in those proposals. In the case of MT, im(Lin and Och, 2004a) and CDER (Leusch et al., provements versus BLEU have been reported (Liu 2006)</context>
</contexts>
<marker>Hirao, Okumura, Isozaki, 2005</marker>
<rawString>Tsutomu Hirao, Manabu Okumura, and Hideki Isozaki. 2005. Kernel-based approach for automatic evaluation of natural language generation technologies: Application to automatic summarization. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 145–152, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Homola</author>
<author>Vladislav Kuboˇn</author>
<author>Pavel Pecina</author>
</authors>
<title>A simple automatic mt evaluation metric.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT ’09,</booktitle>
<pages>33--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Homola, Kuboˇn, Pecina, 2009</marker>
<rawString>Petr Homola, Vladislav Kuboˇn, and Pavel Pecina. 2009. A simple automatic mt evaluation metric. In Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT ’09, pages 33–36, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeremy G Kahn</author>
<author>Matthew Snover</author>
<author>Mari Ostendorf</author>
</authors>
<title>Expected Dependency Pair Match: Predicting translation quality with expected syntactic structure. Machine Translation.</title>
<date>2009</date>
<contexts>
<context position="3835" citStr="Kahn et al., 2009" startWordPosition="576" endWordPosition="579">rroborated with similar n-gram based measures (eg. BLEU and ROUGE). However, according to our second property, the greater the heterogeneity of 455 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 455–466, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics the measures (which is measurable) the higher their 2007; Owczarzak et al., 2007a; Owczarzak et al., reliability. The practical implication is that, corrob- 2007b; Owczarzak et al., 2008; Chan and Ng, orating evaluation results with measures based on 2008; Kahn et al., 2009), CCG parsing (Mehay and higher linguistic levels increases the heterogeneity, Brew, 2007), syntactic constituents (Liu and Gildea, and therefore, the reliability of evaluation results. 2005; Gim´enez and M`arquez, 2007), named entities 2 State of the Art (Reeder et al., 2001; Gim´enez and M`arquez, 2007), 2.1 Individual measures semantic roles (Gim´enez and M`arquez, 2007), disAmong NLP disciplines, MT probably has the course representations (Gim´enez, 2008), and textual widest set of automatic evaluation measures. The entailment features (Pad´o et al., 2009). In general, dominant approach to</context>
<context position="5773" citStr="Kahn et al., 2009" startWordPosition="874" endWordPosition="877">), PER (Till- liability improvement over ROUGE in terms of cormann et al., 1997), and TER (Snover et al., 2006). relation with human judgements. Other metrics are based on computing lexical preci- In general, however, it is not easy to determine sion, e.g., BLEU (Papineni et al., 2001b) and NIST clearly the contribution of deeper linguistic knowl(Doddington, 2002), lexical recall, e.g., ROUGE edge in those proposals. In the case of MT, im(Lin and Och, 2004a) and CDER (Leusch et al., provements versus BLEU have been reported (Liu 2006), or a balance between the two, e.g., GTM and Gildea, 2005; Kahn et al., 2009), but not over (Melamed et al., 2003; Turian et al., 2003b), ME- a more elaborated metric such as METEOR (Mehay TEOR (Banerjee and Lavie, 2005), BLANC (Lita et and Brew, 2007; Chan and Ng, 2008). Besides, conal., 2005), SIA (Liu and Gildea, 2006), MAXSIM troversial results on their performance at sentence vs (Chan and Ng, 2008), and Ol (Gim´enez, 2008). system level have been reported in shared evaluation The lexical measure BLEU has been criticized in tasks (Callison-Burch et al., 2008; Callison-Burch et many ways. Some drawbacks of BLEU are the lack al., 2009; Callison-Burch et al., 2010). o</context>
</contexts>
<marker>Kahn, Snover, Ostendorf, 2009</marker>
<rawString>Jeremy G. Kahn, Matthew Snover, and Mari Ostendorf. 2009. Expected Dependency Pair Match: Predicting translation quality with expected syntactic structure. Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Kulesza</author>
<author>Stuart M Shieber</author>
</authors>
<title>A learning approach to improving sentence-level MT evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI),</booktitle>
<pages>75--84</pages>
<contexts>
<context position="7732" citStr="Kulesza and Shieber, 2004" startWordPosition="1179" endWordPosition="1182"> translation 2009; Liu and Gildea, 2007; Gim´enez and M`arquez, (Culy and Riehemann, 2003). 2008), Decision Trees (Akiba et al., 2001; Quirk, The reaction to these criticisms has been focused 2004), regression based algorithms (Paul et al., on the development of more sophisticated measures 2007; Albrecht and Hwa, 2007a; Albrecht and Hwa, in which candidate and reference translations are 2007b) or a variety of supervised machine learnautomatically annotated and compared at different ing algorithms(Quirk et al., 2005; Corston-Oliver et linguistic levels. Some of the features employed al., 2001; Kulesza and Shieber, 2004; Gamon et al., include parts of speech (Popovic and Ney, 2007; 2005; Amig´o et al., 2005). Gim´enez and M`arquez, 2007), syntactic dependen- Some of these works report evidence on the concies (Liu and Gildea, 2005; Gim´enez and M`arquez, tribution of combining heterogeneous measures. For 456 instance, Albrecht and Hwa included syntax-based measures together with lexical measures, outper- (Lin and Och, 2004a). Banerjee and Lavie (2005) forming other combination schemes (Albrecht and argued that the reliability of metrics at the document Hwa, 2007a; Albrecht and Hwa, 2007b). Liu and level can b</context>
<context position="11741" citStr="Kulesza and Shieber, 2004" startWordPosition="1798" endWordPosition="1801">s human acceptability. Examples of metaulate human assessors, as measured in terms of Pear- measures based on this criterion are ORANGE (Lin son correlation with human assessments of adequacy and Och, 2004b) and KING (Amig´o et al., 2005). and fluency at the document level. The measure In addition, many of the approaches to metric comNIST (Doddington, 2002) was meta-evaluated also bination described in Section 2.2 take human likein terms of correlation with human assessments, but ness as the optimization criterion (Corston-Oliver over different document sources and for a varying et al., 2001; Kulesza and Shieber, 2004; Gamon et number of references and segment sizes. Melamed al., 2005). The main advantage of meta-evaluation et al. (2003) argued, at the time of introducing the based on human likeness is that, since human asGTM metric, that Pearson correlation coefficients sessments are not required, metrics can be evaluated can be affected by scale properties. They suggested over larger test beds. However, the meta-evaluation using the non-parametric Spearman correlation co- in terms of human likeness is difficult to interpret. efficients instead. Lin and Och meta-evaluated 2.4 The use of evaluation measure</context>
</contexts>
<marker>Kulesza, Shieber, 2004</marker>
<rawString>Alex Kulesza and Stuart M. Shieber. 2004. A learning approach to improving sentence-level MT evaluation. In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI), pages 75–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LDC</author>
</authors>
<title>Linguistic Data Annotation Specification: Assessment of Adequacy and Fluency in Translations. Revision 1.5. Technical report, Linguistic Data Consortium.</title>
<date>2005</date>
<note>http://www.ldc.upenn.edu/Projects/ TIDES/Translation/TransAssess04.pdf.</note>
<contexts>
<context position="17947" citStr="LDC, 2005" startWordPosition="2815" endWordPosition="2816">paigns used in the experiments throughout the paper. DUC 2005 DUC 2006 #human-references 3-4 3-4 #systems 32 35 #system-outputs-assessed 32 35 #system-outputs 50 50 #outputs-assessed per-system 50 50 Table 2: Description of the test beds from 2005 and 2006 DUC evaluation campaigns used in the experiments throughout the paper. 20052. Both include two translations exercises: for the 2005 campaign we contacted each participant individually and asked for permission to use their data3. In our experiments, we take the sum of adequacy and fluency, both in a 1-5 scale, as a global measure of quality (LDC, 2005). Thus, human assessments are in a 2-10 scale. For AS, we have used the AS test suites developed in the DUC 2005 and DUC 2006 evaluation campaigns4. This AS task was to generate a question focused summary of 250 words from a set of 25-50 documents to a complex question. Summaries were evaluated according to several criteria. Here, we will consider the responsiveness judgements, in which the quality score was an integer between 1 and 5. See Tables 1 and 2 for a brief quantitative description of these test beds. 2http://www.nist.gov/speech/tests/mt 3We are grateful to a number of groups and comp</context>
</contexts>
<marker>LDC, 2005</marker>
<rawString>LDC. 2005. Linguistic Data Annotation Specification: Assessment of Adequacy and Fluency in Translations. Revision 1.5. Technical report, Linguistic Data Consortium. http://www.ldc.upenn.edu/Projects/ TIDES/Translation/TransAssess04.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Audrey Le and Mark Przybocki</author>
</authors>
<title>machine translation evaluation official results. In Official release of automatic evaluation scores for all submissions,</title>
<date>2005</date>
<marker>Przybocki, 2005</marker>
<rawString>Audrey Le and Mark Przybocki. 2005. NIST 2005 machine translation evaluation official results. In Official release of automatic evaluation scores for all submissions, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Leusch</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>CDER: Efficient MT Evaluation Using Block Movements.</title>
<date>2006</date>
<booktitle>In Proceedings of 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>241--248</pages>
<marker>Leusch, Ueffing, Ney, 2006</marker>
<rawString>Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006. CDER: Efficient MT Evaluation Using Block Movements. In Proceedings of 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 241–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statics.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="5615" citStr="Lin and Och, 2004" startWordPosition="845" endWordPosition="849"> or convolution they compute similarity. Some are based on edit dis- kernels (Hirao et al., 2005) which reported some retance, e.g., WER (Nießen et al., 2000), PER (Till- liability improvement over ROUGE in terms of cormann et al., 1997), and TER (Snover et al., 2006). relation with human judgements. Other metrics are based on computing lexical preci- In general, however, it is not easy to determine sion, e.g., BLEU (Papineni et al., 2001b) and NIST clearly the contribution of deeper linguistic knowl(Doddington, 2002), lexical recall, e.g., ROUGE edge in those proposals. In the case of MT, im(Lin and Och, 2004a) and CDER (Leusch et al., provements versus BLEU have been reported (Liu 2006), or a balance between the two, e.g., GTM and Gildea, 2005; Kahn et al., 2009), but not over (Melamed et al., 2003; Turian et al., 2003b), ME- a more elaborated metric such as METEOR (Mehay TEOR (Banerjee and Lavie, 2005), BLANC (Lita et and Brew, 2007; Chan and Ng, 2008). Besides, conal., 2005), SIA (Liu and Gildea, 2006), MAXSIM troversial results on their performance at sentence vs (Chan and Ng, 2008), and Ol (Gim´enez, 2008). system level have been reported in shared evaluation The lexical measure BLEU has been</context>
<context position="8142" citStr="Lin and Och, 2004" startWordPosition="1241" endWordPosition="1244">vised machine learnautomatically annotated and compared at different ing algorithms(Quirk et al., 2005; Corston-Oliver et linguistic levels. Some of the features employed al., 2001; Kulesza and Shieber, 2004; Gamon et al., include parts of speech (Popovic and Ney, 2007; 2005; Amig´o et al., 2005). Gim´enez and M`arquez, 2007), syntactic dependen- Some of these works report evidence on the concies (Liu and Gildea, 2005; Gim´enez and M`arquez, tribution of combining heterogeneous measures. For 456 instance, Albrecht and Hwa included syntax-based measures together with lexical measures, outper- (Lin and Och, 2004a). Banerjee and Lavie (2005) forming other combination schemes (Albrecht and argued that the reliability of metrics at the document Hwa, 2007a; Albrecht and Hwa, 2007b). Liu and level can be due to averaging effects but might not Gildea, after examining the contribution of each be robust across sentence translations. In order to component metric, found that “metrics showing dif- address this issue, they computed the translation-byferent properties of a sentence are more likely to translation correlation with human assessments (i.e., make a good combined metric”(Liu and Gildea, correlation at </context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004a. Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statics. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>ORANGE: a Method for Evaluating Automatic Evaluation Metrics for Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="5615" citStr="Lin and Och, 2004" startWordPosition="845" endWordPosition="849"> or convolution they compute similarity. Some are based on edit dis- kernels (Hirao et al., 2005) which reported some retance, e.g., WER (Nießen et al., 2000), PER (Till- liability improvement over ROUGE in terms of cormann et al., 1997), and TER (Snover et al., 2006). relation with human judgements. Other metrics are based on computing lexical preci- In general, however, it is not easy to determine sion, e.g., BLEU (Papineni et al., 2001b) and NIST clearly the contribution of deeper linguistic knowl(Doddington, 2002), lexical recall, e.g., ROUGE edge in those proposals. In the case of MT, im(Lin and Och, 2004a) and CDER (Leusch et al., provements versus BLEU have been reported (Liu 2006), or a balance between the two, e.g., GTM and Gildea, 2005; Kahn et al., 2009), but not over (Melamed et al., 2003; Turian et al., 2003b), ME- a more elaborated metric such as METEOR (Mehay TEOR (Banerjee and Lavie, 2005), BLANC (Lita et and Brew, 2007; Chan and Ng, 2008). Besides, conal., 2005), SIA (Liu and Gildea, 2006), MAXSIM troversial results on their performance at sentence vs (Chan and Ng, 2008), and Ol (Gim´enez, 2008). system level have been reported in shared evaluation The lexical measure BLEU has been</context>
<context position="8142" citStr="Lin and Och, 2004" startWordPosition="1241" endWordPosition="1244">vised machine learnautomatically annotated and compared at different ing algorithms(Quirk et al., 2005; Corston-Oliver et linguistic levels. Some of the features employed al., 2001; Kulesza and Shieber, 2004; Gamon et al., include parts of speech (Popovic and Ney, 2007; 2005; Amig´o et al., 2005). Gim´enez and M`arquez, 2007), syntactic dependen- Some of these works report evidence on the concies (Liu and Gildea, 2005; Gim´enez and M`arquez, tribution of combining heterogeneous measures. For 456 instance, Albrecht and Hwa included syntax-based measures together with lexical measures, outper- (Lin and Och, 2004a). Banerjee and Lavie (2005) forming other combination schemes (Albrecht and argued that the reliability of metrics at the document Hwa, 2007a; Albrecht and Hwa, 2007b). Liu and level can be due to averaging effects but might not Gildea, after examining the contribution of each be robust across sentence translations. In order to component metric, found that “metrics showing dif- address this issue, they computed the translation-byferent properties of a sentence are more likely to translation correlation with human assessments (i.e., make a good combined metric”(Liu and Gildea, correlation at </context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004b. ORANGE: a Method for Evaluating Automatic Evaluation Metrics for Machine Translation. In Proceedings of the 20th International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>In Marie-Francine Moens and Stan Szpakowicz, editors, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,</booktitle>
<pages>74--81</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="1946" citStr="Lin, 2004" startWordPosition="281" endWordPosition="282">(MT) and Automatic Summarization (AS). State-of-the-art automatic evaluation methods all operate by rewarding similarities between automatically-produced candidate outputs and manually-produced reference solutions, socalled human references or models. Over the last decade, a wide variety of measures, based on different quality assumptions, have been proposed. Recent work suggests exploiting external knowledge sources and/or deep linguistic annotation, and measure combination (see Section 2). However, original measures based on lexical matching, such as BLEU (Papineni et al., 2001a) and ROUGE (Lin, 2004) are still preferred as de facto standards in MT and AS, respectively. There are, in our opinion, two main reasons behind this fact. First, the use of a common measure certainly allows researchers to carry out objective comparisons between their work and other published results. Second, the advantages of novel measures are not easy to demonstrate in terms of correlation with human judgements. Our goal is not to answer which is the most reliable metric or to propose yet another novel measure. Rather than this, we first analyze in depth the state of the art, concluding that it is not easy to det</context>
<context position="21384" citStr="Lin, 2004" startWordPosition="3365" endWordPosition="3367">nd set We could think of violating this property by are those that consider deeper linguistic levels such adding, for instance, a measure consisting of a ranas parts of speech, syntactic dependencies, syntactic dom function (x&apos;(s) = rand(0..1)) or a reversal of constituents, etc. We will refer to them as “linguis- the original measure (x&apos;(s) = 1/x(s)). These kind tic” measures. of measures, however, would not satisfy the conIn the case of automatic summarization (AS), we straints defined in Section 3. have employed the standard variants of ROUGE This property is based on the idea that similar(Lin, 2004). These 7 measures are ROUGE-{1..4}, ity with human references according to any aspect ROUGE-SU, ROUGE-L and ROUGE-W. In addi- should not imply statistically a quality decrease. Altion we have included the reversed precision version though our test suites includes measures with low for each variant and the F measure of both. Notice correlation at segment and system level, we can conthat the original ROUGE measures are oriented to firm empirically that all of them satisfy this property. recall. In total, we have 21 measures for the sum- We have developed the following experiment: marization tas</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A Package for Automatic Evaluation of Summaries. In Marie-Francine Moens and Stan Szpakowicz, editors, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucian Vlad Lita</author>
<author>Monica Rogati</author>
<author>Alon Lavie</author>
</authors>
<title>BLANC: Learning Evaluation Metrics for MT.</title>
<date>2005</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT-EMNLP),</booktitle>
<pages>740--747</pages>
<marker>Lita, Rogati, Lavie, 2005</marker>
<rawString>Lucian Vlad Lita, Monica Rogati, and Alon Lavie. 2005. BLANC: Learning Evaluation Metrics for MT. In Proceedings of the Joint Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT-EMNLP), pages 740–747.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic Features for Evaluation of Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="7946" citStr="Liu and Gildea, 2005" startWordPosition="1215" endWordPosition="1218">orithms (Paul et al., on the development of more sophisticated measures 2007; Albrecht and Hwa, 2007a; Albrecht and Hwa, in which candidate and reference translations are 2007b) or a variety of supervised machine learnautomatically annotated and compared at different ing algorithms(Quirk et al., 2005; Corston-Oliver et linguistic levels. Some of the features employed al., 2001; Kulesza and Shieber, 2004; Gamon et al., include parts of speech (Popovic and Ney, 2007; 2005; Amig´o et al., 2005). Gim´enez and M`arquez, 2007), syntactic dependen- Some of these works report evidence on the concies (Liu and Gildea, 2005; Gim´enez and M`arquez, tribution of combining heterogeneous measures. For 456 instance, Albrecht and Hwa included syntax-based measures together with lexical measures, outper- (Lin and Och, 2004a). Banerjee and Lavie (2005) forming other combination schemes (Albrecht and argued that the reliability of metrics at the document Hwa, 2007a; Albrecht and Hwa, 2007b). Liu and level can be due to averaging effects but might not Gildea, after examining the contribution of each be robust across sentence translations. In order to component metric, found that “metrics showing dif- address this issue, t</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. 2005. Syntactic Features for Evaluation of Machine Translation. In Proceedings ofACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Stochastic Iterative Alignment for Machine Translation Evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Joint 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL),</booktitle>
<pages>539--546</pages>
<contexts>
<context position="6019" citStr="Liu and Gildea, 2006" startWordPosition="918" endWordPosition="921">ne sion, e.g., BLEU (Papineni et al., 2001b) and NIST clearly the contribution of deeper linguistic knowl(Doddington, 2002), lexical recall, e.g., ROUGE edge in those proposals. In the case of MT, im(Lin and Och, 2004a) and CDER (Leusch et al., provements versus BLEU have been reported (Liu 2006), or a balance between the two, e.g., GTM and Gildea, 2005; Kahn et al., 2009), but not over (Melamed et al., 2003; Turian et al., 2003b), ME- a more elaborated metric such as METEOR (Mehay TEOR (Banerjee and Lavie, 2005), BLANC (Lita et and Brew, 2007; Chan and Ng, 2008). Besides, conal., 2005), SIA (Liu and Gildea, 2006), MAXSIM troversial results on their performance at sentence vs (Chan and Ng, 2008), and Ol (Gim´enez, 2008). system level have been reported in shared evaluation The lexical measure BLEU has been criticized in tasks (Callison-Burch et al., 2008; Callison-Burch et many ways. Some drawbacks of BLEU are the lack al., 2009; Callison-Burch et al., 2010). of interpretability (Turian et al., 2003a), the fact that 2.2 Combined measures it is not necessary to increase BLEU to improve sys- Several researchers have suggested integrating hettems (Callison-burch and Osborne, 2006), the over- erogeneous me</context>
</contexts>
<marker>Liu, Gildea, 2006</marker>
<rawString>Ding Liu and Daniel Gildea. 2006. Stochastic Iterative Alignment for Machine Translation Evaluation. In Proceedings of the Joint 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL), pages 539–546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Source-Language Features and Maximum Correlation Training for Machine Translation Evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<pages>41--48</pages>
<contexts>
<context position="7146" citStr="Liu and Gildea, 2007" startWordPosition="1093" endWordPosition="1096">ave suggested integrating hettems (Callison-burch and Osborne, 2006), the over- erogeneous measures. Some of them optimize the scoring of statistical MT systems (Le and Przybocki, measure combination function according to the met2005), the low reliability over rich morphology lan- ric’s ability to emulate the behavior of human asguages (Homola et al., 2009), or even the fact that a sessors (i.e., correlation with human assessments). poor system translation of a book can obtain higher For instance, using linear combinations (Pad´o et al., BLEU results than a manually produced translation 2009; Liu and Gildea, 2007; Gim´enez and M`arquez, (Culy and Riehemann, 2003). 2008), Decision Trees (Akiba et al., 2001; Quirk, The reaction to these criticisms has been focused 2004), regression based algorithms (Paul et al., on the development of more sophisticated measures 2007; Albrecht and Hwa, 2007a; Albrecht and Hwa, in which candidate and reference translations are 2007b) or a variety of supervised machine learnautomatically annotated and compared at different ing algorithms(Quirk et al., 2005; Corston-Oliver et linguistic levels. Some of the features employed al., 2001; Kulesza and Shieber, 2004; Gamon et al.</context>
</contexts>
<marker>Liu, Gildea, 2007</marker>
<rawString>Ding Liu and Daniel Gildea. 2007. Source-Language Features and Maximum Correlation Training for Machine Translation Evaluation. In Proceedings of the 2007 Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Mehay</author>
<author>Chris Brew</author>
</authors>
<title>BLEUATRE: Flattening Syntactic Dependencies for MT Evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI).</booktitle>
<marker>Mehay, Brew, 2007</marker>
<rawString>Dennis Mehay and Chris Brew. 2007. BLEUATRE: Flattening Syntactic Dependencies for MT Evaluation. In Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
<author>Ryan Green</author>
<author>Joseph P Turian</author>
</authors>
<title>Precision and Recall of Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technology and the North American Chapter of the Association for Computational Linguistics (HLTNAACL).</booktitle>
<contexts>
<context position="5809" citStr="Melamed et al., 2003" startWordPosition="881" endWordPosition="884">t over ROUGE in terms of cormann et al., 1997), and TER (Snover et al., 2006). relation with human judgements. Other metrics are based on computing lexical preci- In general, however, it is not easy to determine sion, e.g., BLEU (Papineni et al., 2001b) and NIST clearly the contribution of deeper linguistic knowl(Doddington, 2002), lexical recall, e.g., ROUGE edge in those proposals. In the case of MT, im(Lin and Och, 2004a) and CDER (Leusch et al., provements versus BLEU have been reported (Liu 2006), or a balance between the two, e.g., GTM and Gildea, 2005; Kahn et al., 2009), but not over (Melamed et al., 2003; Turian et al., 2003b), ME- a more elaborated metric such as METEOR (Mehay TEOR (Banerjee and Lavie, 2005), BLANC (Lita et and Brew, 2007; Chan and Ng, 2008). Besides, conal., 2005), SIA (Liu and Gildea, 2006), MAXSIM troversial results on their performance at sentence vs (Chan and Ng, 2008), and Ol (Gim´enez, 2008). system level have been reported in shared evaluation The lexical measure BLEU has been criticized in tasks (Callison-Burch et al., 2008; Callison-Burch et many ways. Some drawbacks of BLEU are the lack al., 2009; Callison-Burch et al., 2010). of interpretability (Turian et al., 2</context>
</contexts>
<marker>Melamed, Green, Turian, 2003</marker>
<rawString>I. Dan Melamed, Ryan Green, and Joseph P. Turian. 2003. Precision and Recall of Machine Translation. In Proceedings of the Joint Conference on Human Language Technology and the North American Chapter of the Association for Computational Linguistics (HLTNAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Franz Josef Och</author>
<author>Gregor Leusch</author>
<author>Hermann Ney</author>
</authors>
<title>An Evaluation Tool for Machine Translation: Fast Evaluation for MT Research.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="5156" citStr="Nießen et al., 2000" startWordPosition="769" endWordPosition="772">al metrics (also called n-gram guistic features at lower levels are preserved. based metrics). These metrics work by rewarding The proposals for summarization evaluation are lexical similarity between candidate translations and less numerous. Some proposals for AS tasks are a set of manually-produced reference translations. based on syntactic units (Tratz and Hovy, 2008), deLexical metrics can be classified according to how pendency triples (Owczarzak, 2009) or convolution they compute similarity. Some are based on edit dis- kernels (Hirao et al., 2005) which reported some retance, e.g., WER (Nießen et al., 2000), PER (Till- liability improvement over ROUGE in terms of cormann et al., 1997), and TER (Snover et al., 2006). relation with human judgements. Other metrics are based on computing lexical preci- In general, however, it is not easy to determine sion, e.g., BLEU (Papineni et al., 2001b) and NIST clearly the contribution of deeper linguistic knowl(Doddington, 2002), lexical recall, e.g., ROUGE edge in those proposals. In the case of MT, im(Lin and Och, 2004a) and CDER (Leusch et al., provements versus BLEU have been reported (Liu 2006), or a balance between the two, e.g., GTM and Gildea, 2005; K</context>
</contexts>
<marker>Nießen, Och, Leusch, Ney, 2000</marker>
<rawString>Sonja Nießen, Franz Josef Och, Gregor Leusch, and Hermann Ney. 2000. An Evaluation Tool for Machine Translation: Fast Evaluation for MT Research. In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Dependency-Based Automatic Evaluation for Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of SSST, NAACL-HLT/AMTA Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>80--87</pages>
<marker>Owczarzak, van Genabith, Way, 2007</marker>
<rawString>Karolina Owczarzak, Josef van Genabith, and Andy Way. 2007a. Dependency-Based Automatic Evaluation for Machine Translation. In Proceedings of SSST, NAACL-HLT/AMTA Workshop on Syntax and Structure in Statistical Translation, pages 80–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Labelled Dependencies in Machine Translation Evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL Workshop on Statistical Machine Translation,</booktitle>
<pages>104--111</pages>
<marker>Owczarzak, van Genabith, Way, 2007</marker>
<rawString>Karolina Owczarzak, Josef van Genabith, and Andy Way. 2007b. Labelled Dependencies in Machine Translation Evaluation. In Proceedings of the ACL Workshop on Statistical Machine Translation, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Evaluating machine translation with lfg dependencies.</title>
<date>2008</date>
<journal>Machine Translation,</journal>
<volume>21</volume>
<issue>2</issue>
<marker>Owczarzak, van Genabith, Way, 2008</marker>
<rawString>Karolina Owczarzak, Josef van Genabith, and Andy Way. 2008. Evaluating machine translation with lfg dependencies. Machine Translation, 21(2):95–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
</authors>
<title>Depeval(summ): dependency-based evaluation for automatic summaries. In</title>
<date>2009</date>
<booktitle>ACL-IJCNLP ’09: Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP:</booktitle>
<volume>1</volume>
<pages>190--198</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4998" citStr="Owczarzak, 2009" startWordPosition="744" endWordPosition="745">Pad´o et al., 2009). In general, dominant approach to automatic MT evaluation is, when a higher linguistic level is incorporated, lintoday, based on lexical metrics (also called n-gram guistic features at lower levels are preserved. based metrics). These metrics work by rewarding The proposals for summarization evaluation are lexical similarity between candidate translations and less numerous. Some proposals for AS tasks are a set of manually-produced reference translations. based on syntactic units (Tratz and Hovy, 2008), deLexical metrics can be classified according to how pendency triples (Owczarzak, 2009) or convolution they compute similarity. Some are based on edit dis- kernels (Hirao et al., 2005) which reported some retance, e.g., WER (Nießen et al., 2000), PER (Till- liability improvement over ROUGE in terms of cormann et al., 1997), and TER (Snover et al., 2006). relation with human judgements. Other metrics are based on computing lexical preci- In general, however, it is not easy to determine sion, e.g., BLEU (Papineni et al., 2001b) and NIST clearly the contribution of deeper linguistic knowl(Doddington, 2002), lexical recall, e.g., ROUGE edge in those proposals. In the case of MT, im(</context>
</contexts>
<marker>Owczarzak, 2009</marker>
<rawString>Karolina Owczarzak. 2009. Depeval(summ): dependency-based evaluation for automatic summaries. In ACL-IJCNLP ’09: Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1, pages 190–198, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Michael Galley</author>
<author>Dan Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Robust machine translation evaluation with entailment features.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>297--305</pages>
<marker>Pad´o, Galley, Jurafsky, Manning, 2009</marker>
<rawString>Sebastian Pad´o, Michael Galley, Dan Jurafsky, and Christopher D. Manning. 2009. Robust machine translation evaluation with entailment features. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 297–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<location>Philadelphia,</location>
<contexts>
<context position="1922" citStr="Papineni et al., 2001" startWordPosition="275" endWordPosition="278">ge Generation, Machine Translation (MT) and Automatic Summarization (AS). State-of-the-art automatic evaluation methods all operate by rewarding similarities between automatically-produced candidate outputs and manually-produced reference solutions, socalled human references or models. Over the last decade, a wide variety of measures, based on different quality assumptions, have been proposed. Recent work suggests exploiting external knowledge sources and/or deep linguistic annotation, and measure combination (see Section 2). However, original measures based on lexical matching, such as BLEU (Papineni et al., 2001a) and ROUGE (Lin, 2004) are still preferred as de facto standards in MT and AS, respectively. There are, in our opinion, two main reasons behind this fact. First, the use of a common measure certainly allows researchers to carry out objective comparisons between their work and other published results. Second, the advantages of novel measures are not easy to demonstrate in terms of correlation with human judgements. Our goal is not to answer which is the most reliable metric or to propose yet another novel measure. Rather than this, we first analyze in depth the state of the art, concluding th</context>
<context position="5440" citStr="Papineni et al., 2001" startWordPosition="817" endWordPosition="820"> manually-produced reference translations. based on syntactic units (Tratz and Hovy, 2008), deLexical metrics can be classified according to how pendency triples (Owczarzak, 2009) or convolution they compute similarity. Some are based on edit dis- kernels (Hirao et al., 2005) which reported some retance, e.g., WER (Nießen et al., 2000), PER (Till- liability improvement over ROUGE in terms of cormann et al., 1997), and TER (Snover et al., 2006). relation with human judgements. Other metrics are based on computing lexical preci- In general, however, it is not easy to determine sion, e.g., BLEU (Papineni et al., 2001b) and NIST clearly the contribution of deeper linguistic knowl(Doddington, 2002), lexical recall, e.g., ROUGE edge in those proposals. In the case of MT, im(Lin and Och, 2004a) and CDER (Leusch et al., provements versus BLEU have been reported (Liu 2006), or a balance between the two, e.g., GTM and Gildea, 2005; Kahn et al., 2009), but not over (Melamed et al., 2003; Turian et al., 2003b), ME- a more elaborated metric such as METEOR (Mehay TEOR (Banerjee and Lavie, 2005), BLANC (Lita et and Brew, 2007; Chan and Ng, 2008). Besides, conal., 2005), SIA (Liu and Gildea, 2006), MAXSIM troversial r</context>
<context position="10975" citStr="Papineni et al. (2001" startWordPosition="1675" endWordPosition="1678">ites, it bining measures at different linguistic levels was ap- over-scores a poor automatic translation of “Tom plied with remarkable results in recent shared evalu- Sawyer” against a human produced translation (Culy ation tasks (Callison-Burch et al., 2010). and Riehemann, 2003). This meta-evaluation crite2.3 Meta-evaluation criteria rion based on the ability to discern between manMeta-evaluation methods have been gradually intro- ual and automatic translations have been referred to duced together with evaluation measures. For in- as human likeness (Amig´o et al., 2006), in contrast stance, Papineni et al. (2001b) evaluated the reliabil- to correlation with human judgements which is reity of the BLEU metric according to its ability to em- ferred to as human acceptability. Examples of metaulate human assessors, as measured in terms of Pear- measures based on this criterion are ORANGE (Lin son correlation with human assessments of adequacy and Och, 2004b) and KING (Amig´o et al., 2005). and fluency at the document level. The measure In addition, many of the approaches to metric comNIST (Doddington, 2002) was meta-evaluated also bination described in Section 2.2 take human likein terms of correlation wi</context>
<context position="14186" citStr="Papineni et al., 2001" startWordPosition="2209" endWordPosition="2212">etrics con- cision and Recall satisfies these constraints1, pretributes to the reliability of such results. The state of cision and recall in isolation do not satisfy all of the art suggests that the use of heterogeneous mea- them: maximum recall can be achieved without resures can improve the evaluation reliability. How- sembling the goldstandard text decomposition; and ever, as far as we know, there is no comprehen- maximum precision can be achieved with only a few sive analysis on the contribution of novel measures overlapped elements. when corroborating evaluation results with addi- BLEU (Papineni et al., 2001a) computes the ntional measures. gram precision while the metric ROUGE (Lin and 3 Similarity Based Evaluation Measures Och, 2004a) computes the n-gram recall. HowIn general, automatic evaluation measures applied ever, in general, both metrics satisfy all the conin tasks like MT or AS are similarity measures be- straints, given that BLEU includes a brevity penalty tween system outputs and human references. These and ROUGE penalizes or limits the system output measures are related with precision, recall or overlap length. The measure METEOR creates an alignover specific types of linguistic unit</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001a. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311–318, Philadelphia, jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation,</title>
<date>2001</date>
<tech>RC22176. Technical report, IBM</tech>
<institution>T.J. Watson Research Center.</institution>
<contexts>
<context position="1922" citStr="Papineni et al., 2001" startWordPosition="275" endWordPosition="278">ge Generation, Machine Translation (MT) and Automatic Summarization (AS). State-of-the-art automatic evaluation methods all operate by rewarding similarities between automatically-produced candidate outputs and manually-produced reference solutions, socalled human references or models. Over the last decade, a wide variety of measures, based on different quality assumptions, have been proposed. Recent work suggests exploiting external knowledge sources and/or deep linguistic annotation, and measure combination (see Section 2). However, original measures based on lexical matching, such as BLEU (Papineni et al., 2001a) and ROUGE (Lin, 2004) are still preferred as de facto standards in MT and AS, respectively. There are, in our opinion, two main reasons behind this fact. First, the use of a common measure certainly allows researchers to carry out objective comparisons between their work and other published results. Second, the advantages of novel measures are not easy to demonstrate in terms of correlation with human judgements. Our goal is not to answer which is the most reliable metric or to propose yet another novel measure. Rather than this, we first analyze in depth the state of the art, concluding th</context>
<context position="5440" citStr="Papineni et al., 2001" startWordPosition="817" endWordPosition="820"> manually-produced reference translations. based on syntactic units (Tratz and Hovy, 2008), deLexical metrics can be classified according to how pendency triples (Owczarzak, 2009) or convolution they compute similarity. Some are based on edit dis- kernels (Hirao et al., 2005) which reported some retance, e.g., WER (Nießen et al., 2000), PER (Till- liability improvement over ROUGE in terms of cormann et al., 1997), and TER (Snover et al., 2006). relation with human judgements. Other metrics are based on computing lexical preci- In general, however, it is not easy to determine sion, e.g., BLEU (Papineni et al., 2001b) and NIST clearly the contribution of deeper linguistic knowl(Doddington, 2002), lexical recall, e.g., ROUGE edge in those proposals. In the case of MT, im(Lin and Och, 2004a) and CDER (Leusch et al., provements versus BLEU have been reported (Liu 2006), or a balance between the two, e.g., GTM and Gildea, 2005; Kahn et al., 2009), but not over (Melamed et al., 2003; Turian et al., 2003b), ME- a more elaborated metric such as METEOR (Mehay TEOR (Banerjee and Lavie, 2005), BLANC (Lita et and Brew, 2007; Chan and Ng, 2008). Besides, conal., 2005), SIA (Liu and Gildea, 2006), MAXSIM troversial r</context>
<context position="10975" citStr="Papineni et al. (2001" startWordPosition="1675" endWordPosition="1678">ites, it bining measures at different linguistic levels was ap- over-scores a poor automatic translation of “Tom plied with remarkable results in recent shared evalu- Sawyer” against a human produced translation (Culy ation tasks (Callison-Burch et al., 2010). and Riehemann, 2003). This meta-evaluation crite2.3 Meta-evaluation criteria rion based on the ability to discern between manMeta-evaluation methods have been gradually intro- ual and automatic translations have been referred to duced together with evaluation measures. For in- as human likeness (Amig´o et al., 2006), in contrast stance, Papineni et al. (2001b) evaluated the reliabil- to correlation with human judgements which is reity of the BLEU metric according to its ability to em- ferred to as human acceptability. Examples of metaulate human assessors, as measured in terms of Pear- measures based on this criterion are ORANGE (Lin son correlation with human assessments of adequacy and Och, 2004b) and KING (Amig´o et al., 2005). and fluency at the document level. The measure In addition, many of the approaches to metric comNIST (Doddington, 2002) was meta-evaluated also bination described in Section 2.2 take human likein terms of correlation wi</context>
<context position="14186" citStr="Papineni et al., 2001" startWordPosition="2209" endWordPosition="2212">etrics con- cision and Recall satisfies these constraints1, pretributes to the reliability of such results. The state of cision and recall in isolation do not satisfy all of the art suggests that the use of heterogeneous mea- them: maximum recall can be achieved without resures can improve the evaluation reliability. How- sembling the goldstandard text decomposition; and ever, as far as we know, there is no comprehen- maximum precision can be achieved with only a few sive analysis on the contribution of novel measures overlapped elements. when corroborating evaluation results with addi- BLEU (Papineni et al., 2001a) computes the ntional measures. gram precision while the metric ROUGE (Lin and 3 Similarity Based Evaluation Measures Och, 2004a) computes the n-gram recall. HowIn general, automatic evaluation measures applied ever, in general, both metrics satisfy all the conin tasks like MT or AS are similarity measures be- straints, given that BLEU includes a brevity penalty tween system outputs and human references. These and ROUGE penalizes or limits the system output measures are related with precision, recall or overlap length. The measure METEOR creates an alignover specific types of linguistic unit</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001b. Bleu: a method for automatic evaluation of machine translation, RC22176. Technical report, IBM T.J. Watson Research Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
<author>Andrew Finch</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Reducing Human Assessments of Machine Translation Quality to Binary Classifiers.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI).</booktitle>
<marker>Paul, Finch, Sumita, 2007</marker>
<rawString>Michael Paul, Andrew Finch, and Eiichiro Sumita. 2007. Reducing Human Assessments of Machine Translation Quality to Binary Classifiers. In Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovic</author>
<author>Hermann Ney</author>
</authors>
<title>Word Error Rates: Decomposition over POS classes and Applications for Error Analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>48--55</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7794" citStr="Popovic and Ney, 2007" startWordPosition="1190" endWordPosition="1193">uly and Riehemann, 2003). 2008), Decision Trees (Akiba et al., 2001; Quirk, The reaction to these criticisms has been focused 2004), regression based algorithms (Paul et al., on the development of more sophisticated measures 2007; Albrecht and Hwa, 2007a; Albrecht and Hwa, in which candidate and reference translations are 2007b) or a variety of supervised machine learnautomatically annotated and compared at different ing algorithms(Quirk et al., 2005; Corston-Oliver et linguistic levels. Some of the features employed al., 2001; Kulesza and Shieber, 2004; Gamon et al., include parts of speech (Popovic and Ney, 2007; 2005; Amig´o et al., 2005). Gim´enez and M`arquez, 2007), syntactic dependen- Some of these works report evidence on the concies (Liu and Gildea, 2005; Gim´enez and M`arquez, tribution of combining heterogeneous measures. For 456 instance, Albrecht and Hwa included syntax-based measures together with lexical measures, outper- (Lin and Och, 2004a). Banerjee and Lavie (2005) forming other combination schemes (Albrecht and argued that the reliability of metrics at the document Hwa, 2007a; Albrecht and Hwa, 2007b). Liu and level can be due to averaging effects but might not Gildea, after examini</context>
<context position="15307" citStr="Popovic and Ney, 2007" startWordPosition="2380" endWordPosition="2383">, recall or overlap length. The measure METEOR creates an alignover specific types of linguistic units. For instance, ment between the two strings (Banerjee and Lavie, ROUGE measures n-gram recall. Other measures 2005). This overlap-based measure satisfies also the that work at higher linguistic levels apply precision, previous constraints. Measures based on edit disrecall or overlap of linguistic components such as tance over n-grams (Tillmann et al., 1997; Nießen dependency relations, grammatical categories, se- et al., 2000) or other linguistic units (Akiba et al., mantic roles, etc. 2001; Popovic and Ney, 2007) match also our defIn order to delimit our hypothesis, let us first de- inition of similarity measure. The editing distance fine what is a similarity measure in this context. Un- is minimum when the two compared text are equal. fortunately, as far as we know, there is no formal The more the evaluated text contains elements from concept covering the properties of current evaluation the gold-standard the more the editing distance is resimilarity measures. A close concept is that of “met- duced (higher similarity). The word ordering can be ric” or “distance function”. But, actually, measures also</context>
</contexts>
<marker>Popovic, Ney, 2007</marker>
<rawString>Maja Popovic and Hermann Ney. 2007. Word Error Rates: Decomposition over POS classes and Applications for Error Analysis. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 48–55, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency Treelet Translation: Syntactically Informed Phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>271--279</pages>
<contexts>
<context position="7627" citStr="Quirk et al., 2005" startWordPosition="1164" endWordPosition="1167">igher For instance, using linear combinations (Pad´o et al., BLEU results than a manually produced translation 2009; Liu and Gildea, 2007; Gim´enez and M`arquez, (Culy and Riehemann, 2003). 2008), Decision Trees (Akiba et al., 2001; Quirk, The reaction to these criticisms has been focused 2004), regression based algorithms (Paul et al., on the development of more sophisticated measures 2007; Albrecht and Hwa, 2007a; Albrecht and Hwa, in which candidate and reference translations are 2007b) or a variety of supervised machine learnautomatically annotated and compared at different ing algorithms(Quirk et al., 2005; Corston-Oliver et linguistic levels. Some of the features employed al., 2001; Kulesza and Shieber, 2004; Gamon et al., include parts of speech (Popovic and Ney, 2007; 2005; Amig´o et al., 2005). Gim´enez and M`arquez, 2007), syntactic dependen- Some of these works report evidence on the concies (Liu and Gildea, 2005; Gim´enez and M`arquez, tribution of combining heterogeneous measures. For 456 instance, Albrecht and Hwa included syntax-based measures together with lexical measures, outper- (Lin and Och, 2004a). Banerjee and Lavie (2005) forming other combination schemes (Albrecht and argued </context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency Treelet Translation: Syntactically Informed Phrasal SMT. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 271–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
</authors>
<title>Training a Sentence-Level Machine Translation Confidence Metric.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>825--828</pages>
<marker>Quirk, 2004</marker>
<rawString>Chris Quirk. 2004. Training a Sentence-Level Machine Translation Confidence Metric. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC), pages 825–828.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florence Reeder</author>
<author>Keith Miller</author>
<author>Jennifer Doyon</author>
<author>John White</author>
</authors>
<title>The Naming of Things and the Confusion of Tongues: an MT Metric.</title>
<date>2001</date>
<booktitle>In Proceedings of the Workshop on MT Evaluation ”Who did what to whom?” at Machine Translation Summit VIII,</booktitle>
<pages>55--59</pages>
<contexts>
<context position="4111" citStr="Reeder et al., 2001" startWordPosition="616" endWordPosition="619"> July 27–31, 2011. c�2011 Association for Computational Linguistics the measures (which is measurable) the higher their 2007; Owczarzak et al., 2007a; Owczarzak et al., reliability. The practical implication is that, corrob- 2007b; Owczarzak et al., 2008; Chan and Ng, orating evaluation results with measures based on 2008; Kahn et al., 2009), CCG parsing (Mehay and higher linguistic levels increases the heterogeneity, Brew, 2007), syntactic constituents (Liu and Gildea, and therefore, the reliability of evaluation results. 2005; Gim´enez and M`arquez, 2007), named entities 2 State of the Art (Reeder et al., 2001; Gim´enez and M`arquez, 2007), 2.1 Individual measures semantic roles (Gim´enez and M`arquez, 2007), disAmong NLP disciplines, MT probably has the course representations (Gim´enez, 2008), and textual widest set of automatic evaluation measures. The entailment features (Pad´o et al., 2009). In general, dominant approach to automatic MT evaluation is, when a higher linguistic level is incorporated, lintoday, based on lexical metrics (also called n-gram guistic features at lower levels are preserved. based metrics). These metrics work by rewarding The proposals for summarization evaluation are l</context>
</contexts>
<marker>Reeder, Miller, Doyon, White, 2001</marker>
<rawString>Florence Reeder, Keith Miller, Jennifer Doyon, and John White. 2001. The Naming of Things and the Confusion of Tongues: an MT Metric. In Proceedings of the Workshop on MT Evaluation ”Who did what to whom?” at Machine Translation Summit VIII, pages 55–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<pages>223--231</pages>
<contexts>
<context position="5266" citStr="Snover et al., 2006" startWordPosition="789" endWordPosition="792">ork by rewarding The proposals for summarization evaluation are lexical similarity between candidate translations and less numerous. Some proposals for AS tasks are a set of manually-produced reference translations. based on syntactic units (Tratz and Hovy, 2008), deLexical metrics can be classified according to how pendency triples (Owczarzak, 2009) or convolution they compute similarity. Some are based on edit dis- kernels (Hirao et al., 2005) which reported some retance, e.g., WER (Nießen et al., 2000), PER (Till- liability improvement over ROUGE in terms of cormann et al., 1997), and TER (Snover et al., 2006). relation with human judgements. Other metrics are based on computing lexical preci- In general, however, it is not easy to determine sion, e.g., BLEU (Papineni et al., 2001b) and NIST clearly the contribution of deeper linguistic knowl(Doddington, 2002), lexical recall, e.g., ROUGE edge in those proposals. In the case of MT, im(Lin and Och, 2004a) and CDER (Leusch et al., provements versus BLEU have been reported (Liu 2006), or a balance between the two, e.g., GTM and Gildea, 2005; Kahn et al., 2009), but not over (Melamed et al., 2003; Turian et al., 2003b), ME- a more elaborated metric suc</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA), pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Stefan Vogel</author>
<author>Hermann Ney</author>
<author>A Zubiaga</author>
<author>H Sawaf</author>
</authors>
<title>Accelerated DP based Search for Statistical Translation.</title>
<date>1997</date>
<booktitle>In Proceedings of European Conference on Speech Communication and Technology.</booktitle>
<contexts>
<context position="15146" citStr="Tillmann et al., 1997" startWordPosition="2356" endWordPosition="2359"> includes a brevity penalty tween system outputs and human references. These and ROUGE penalizes or limits the system output measures are related with precision, recall or overlap length. The measure METEOR creates an alignover specific types of linguistic units. For instance, ment between the two strings (Banerjee and Lavie, ROUGE measures n-gram recall. Other measures 2005). This overlap-based measure satisfies also the that work at higher linguistic levels apply precision, previous constraints. Measures based on edit disrecall or overlap of linguistic components such as tance over n-grams (Tillmann et al., 1997; Nießen dependency relations, grammatical categories, se- et al., 2000) or other linguistic units (Akiba et al., mantic roles, etc. 2001; Popovic and Ney, 2007) match also our defIn order to delimit our hypothesis, let us first de- inition of similarity measure. The editing distance fine what is a similarity measure in this context. Un- is minimum when the two compared text are equal. fortunately, as far as we know, there is no formal The more the evaluated text contains elements from concept covering the properties of current evaluation the gold-standard the more the editing distance is resi</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Zubiaga, Sawaf, 1997</marker>
<rawString>Christoph Tillmann, Stefan Vogel, Hermann Ney, A. Zubiaga, and H. Sawaf. 1997. Accelerated DP based Search for Statistical Translation. In Proceedings of European Conference on Speech Communication and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Tratz</author>
<author>Eduard Hovy</author>
</authors>
<title>Summarization evaluation using transformed basic elements. In</title>
<date>2008</date>
<booktitle>In Proceedings of TAC-08.</booktitle>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="4909" citStr="Tratz and Hovy, 2008" startWordPosition="729" endWordPosition="732">nez, 2008), and textual widest set of automatic evaluation measures. The entailment features (Pad´o et al., 2009). In general, dominant approach to automatic MT evaluation is, when a higher linguistic level is incorporated, lintoday, based on lexical metrics (also called n-gram guistic features at lower levels are preserved. based metrics). These metrics work by rewarding The proposals for summarization evaluation are lexical similarity between candidate translations and less numerous. Some proposals for AS tasks are a set of manually-produced reference translations. based on syntactic units (Tratz and Hovy, 2008), deLexical metrics can be classified according to how pendency triples (Owczarzak, 2009) or convolution they compute similarity. Some are based on edit dis- kernels (Hirao et al., 2005) which reported some retance, e.g., WER (Nießen et al., 2000), PER (Till- liability improvement over ROUGE in terms of cormann et al., 1997), and TER (Snover et al., 2006). relation with human judgements. Other metrics are based on computing lexical preci- In general, however, it is not easy to determine sion, e.g., BLEU (Papineni et al., 2001b) and NIST clearly the contribution of deeper linguistic knowl(Doddi</context>
</contexts>
<marker>Tratz, Hovy, 2008</marker>
<rawString>Stephen Tratz and Eduard Hovy. 2008. Summarization evaluation using transformed basic elements. In In Proceedings of TAC-08. Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Luke Shen</author>
<author>I Dan Melamed</author>
</authors>
<title>Evaluation of machine translation and its evaluation. In</title>
<date>2003</date>
<booktitle>In Proceedings of MT Summit IX,</booktitle>
<pages>386--393</pages>
<contexts>
<context position="5830" citStr="Turian et al., 2003" startWordPosition="885" endWordPosition="888">of cormann et al., 1997), and TER (Snover et al., 2006). relation with human judgements. Other metrics are based on computing lexical preci- In general, however, it is not easy to determine sion, e.g., BLEU (Papineni et al., 2001b) and NIST clearly the contribution of deeper linguistic knowl(Doddington, 2002), lexical recall, e.g., ROUGE edge in those proposals. In the case of MT, im(Lin and Och, 2004a) and CDER (Leusch et al., provements versus BLEU have been reported (Liu 2006), or a balance between the two, e.g., GTM and Gildea, 2005; Kahn et al., 2009), but not over (Melamed et al., 2003; Turian et al., 2003b), ME- a more elaborated metric such as METEOR (Mehay TEOR (Banerjee and Lavie, 2005), BLANC (Lita et and Brew, 2007; Chan and Ng, 2008). Besides, conal., 2005), SIA (Liu and Gildea, 2006), MAXSIM troversial results on their performance at sentence vs (Chan and Ng, 2008), and Ol (Gim´enez, 2008). system level have been reported in shared evaluation The lexical measure BLEU has been criticized in tasks (Callison-Burch et al., 2008; Callison-Burch et many ways. Some drawbacks of BLEU are the lack al., 2009; Callison-Burch et al., 2010). of interpretability (Turian et al., 2003a), the fact that </context>
</contexts>
<marker>Turian, Shen, Melamed, 2003</marker>
<rawString>Joseph Turian, Luke Shen, and I. Dan Melamed. 2003a. Evaluation of machine translation and its evaluation. In In Proceedings of MT Summit IX, pages 386–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph P Turian</author>
<author>Luke Shen</author>
<author>I Dan Melamed</author>
</authors>
<title>Evaluation of Machine Translation and its Evaluation.</title>
<date>2003</date>
<booktitle>In Proceedings of MT SUMMIT IX.</booktitle>
<contexts>
<context position="5830" citStr="Turian et al., 2003" startWordPosition="885" endWordPosition="888">of cormann et al., 1997), and TER (Snover et al., 2006). relation with human judgements. Other metrics are based on computing lexical preci- In general, however, it is not easy to determine sion, e.g., BLEU (Papineni et al., 2001b) and NIST clearly the contribution of deeper linguistic knowl(Doddington, 2002), lexical recall, e.g., ROUGE edge in those proposals. In the case of MT, im(Lin and Och, 2004a) and CDER (Leusch et al., provements versus BLEU have been reported (Liu 2006), or a balance between the two, e.g., GTM and Gildea, 2005; Kahn et al., 2009), but not over (Melamed et al., 2003; Turian et al., 2003b), ME- a more elaborated metric such as METEOR (Mehay TEOR (Banerjee and Lavie, 2005), BLANC (Lita et and Brew, 2007; Chan and Ng, 2008). Besides, conal., 2005), SIA (Liu and Gildea, 2006), MAXSIM troversial results on their performance at sentence vs (Chan and Ng, 2008), and Ol (Gim´enez, 2008). system level have been reported in shared evaluation The lexical measure BLEU has been criticized in tasks (Callison-Burch et al., 2008; Callison-Burch et many ways. Some drawbacks of BLEU are the lack al., 2009; Callison-Burch et al., 2010). of interpretability (Turian et al., 2003a), the fact that </context>
</contexts>
<marker>Turian, Shen, Melamed, 2003</marker>
<rawString>Joseph P. Turian, Luke Shen, and I. Dan Melamed. 2003b. Evaluation of Machine Translation and its Evaluation. In Proceedings of MT SUMMIT IX.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>