<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000966">
<title confidence="0.989608">
An Overview of Microsoft Web N-gram Corpus and Applications
</title>
<author confidence="0.839146">
Kuansan Wang Christopher Thrasher Evelyne Viegas
Xiaolong Li Bo-june (Paul) Hsu
</author>
<affiliation confidence="0.96998">
Microsoft Research
</affiliation>
<address confidence="0.9347995">
One Microsoft Way
Redmond, WA, 98052, USA
</address>
<email confidence="0.997004">
webngram@microsoft.com
</email>
<sectionHeader confidence="0.995613" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996571">
This document describes the properties and
some applications of the Microsoft Web N-
gram corpus. The corpus is designed to have
the following characteristics. First, in contrast
to static data distribution of previous corpus
releases, this N-gram corpus is made publicly
available as an XML Web Service so that it
can be updated as deemed necessary by the
user community to include new words and
phrases constantly being added to the Web.
Secondly, the corpus makes available various
sections of a Web document, specifically, the
body, title, and anchor text, as separates mod-
els as text contents in these sections are found
to possess significantly different statistical
properties and therefore are treated as distinct
languages from the language modeling point
of view. The usages of the corpus are demon-
strated here in two NLP tasks: phrase segmen-
tation and word breaking.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999893979591837">
Since Banko and Brill’s pioneering work almost a
decade ago (Banko and Brill 2001), it has been
widely observed that the effectiveness of statistical
natural language processing (NLP) techniques is
highly susceptible to the data size used to develop
them. As empirical studies have repeatedly shown
that simple algorithms can often outperform their
more complicated counterparts in wide varieties of
NLP applications with large datasets, many have
come to believe that it is the size of data, not the
sophistication of the algorithms that ultimately
play the central role in modern NLP (Norvig,
2008). Towards this end, there have been consider-
able efforts in the NLP community to gather ever
larger datasets, culminating the release of the Eng-
lish Giga-word corpus (Graff and Cieri, 2003) and
the 1 Tera-word Google N-gram (Thorsten and
Franz, 2006) created from arguably the largest text
source available, the World Wide Web.
Recent research, however, suggests that studies
on the document body alone may no longer be suf-
ficient in understanding the language usages in our
daily lives. A document, for example, is typically
associated with multiple text streams. In addition
to the document body that contains the bulk of the
contents, there are also the title and the file-
name/URL the authors choose to name the docu-
ment. On the web, a document is often linked with
anchor text or short messages from social network
applications that other authors use to summarize
the document, and from the search logs we learn
the text queries formulated by the general public to
specify the document. A large scale studies reveal
that these text streams have significantly different
properties and lead to varying degrees of perfor-
mance in many NLP applications (Wang et al,
2010, Huang et al, 2010). Consequently from the
statistical modeling point of view, these streams
are better regarded as composed in distinctive lan-
guages and treated as such.
This observation motivates the creation of Mi-
crosoft Web N-gram corpus in which the materials
from the body, title and anchor text are made
available separately. Another notable feature of the
corpus is that Microsoft Web N-gram is available
as a cross-platform XML Web service1 that can be
freely and readily accessible by users through the
Internet anytime and anywhere. The service archi-
tecture also makes it straightforward to perform on
</bodyText>
<footnote confidence="0.97307">
1 Please visit http://research.microsoft.com/web-ngram for
more information.
</footnote>
<page confidence="0.986013">
45
</page>
<subsubsectionHeader confidence="0.34886">
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 45–48,
</subsubsectionHeader>
<bodyText confidence="0.8056485">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
demand updates of the corpus with the new con-
tents that can facilitate the research on the dynam-
ics of the Web.2
</bodyText>
<sectionHeader confidence="0.973681" genericHeader="method">
2 General Model Information
</sectionHeader>
<bodyText confidence="0.999939090909091">
Like the Google N-gram, Microsoft Web N-gram
corpus is based on the web documents indexed by
a commercial web search engine in the EN-US
market, which, in this case, is the Bing service
from Microsoft. The URLs in this market visited
by Bing are at the order of hundreds of billion,
though the spam and other low quality web pages
are actively excluded using Bing’s proprietary al-
gorithms. The various streams of the web docu-
ments are then downloaded, parsed and tokenized
by Bing, in which process the text is lowercased
with the punctuation marks removed. However, no
stemming, spelling corrections or inflections are
performed.
Unlike the Google N-gram release which con-
tains raw N-gram counts, Microsoft Web N-gram
provides open-vocabulary, smoothed back-off N-
gram models for the three text streams using the
CALM algorithm (Wang and Li, 2009) that dy-
namically adapts the N-gram models as web doc-
uments are crawled. The design of CALM ensures
that new N-grams are incorporated into the models
as soon as they are encountered in the crawling and
become statistically significant. The models are
therefore kept up-to-date with the web contents.
CALM is also designed to make sure that dupli-
cated contents will not have outsized impacts in
biasing the N-gram statistics. This property is use-
ful as Bing’s crawler visits URLs in parallel and on
the web many URLs are pointing to the same con-
tents. Currently, the maximum order of the N-gram
available is 5, and the numbers of N-grams are
shown in Table 1.
</bodyText>
<tableCaption confidence="0.999443">
Table 1: Numbers of N-grams for various streams
</tableCaption>
<table confidence="0.9991455">
Body Title Anchor
1-gram 1.2B 60M 150M
2-gram 11.7B 464M 1.1B
3-gram 60.1B 1.4B 3.2B
4-gram 148.5B 2.3B 5.1B
5-gram 237B 3.8B 8.9B
</table>
<footnote confidence="0.527774">
2 The WSDL for the web service is located at http://web-
ngram.research.microsoft.com/Lookup.svc/mex?wsdl.
</footnote>
<bodyText confidence="0.999654625">
CALM algorithm adapts the model from a seed
model based on the June 30, 2009 snapshot of the
Web with the algorithm described and imple-
mented in the MSRLM toolkit (Nguyen et al,
2007). The numbers of tokens in the body, title,
and anchor text in the snapshot are of the order of
1.4 trillion, 12.5 billion, and 357 billion, respec-
tively.
</bodyText>
<sectionHeader confidence="0.922837" genericHeader="method">
3 Search Query Segmentation
</sectionHeader>
<bodyText confidence="0.9999484">
In this demonstration, we implement a straightfor-
ward algorithm that generates hypotheses of the
segment boundaries at all possible placements in a
query and rank their likelihoods using the N-gram
service. In other words, a query of T terms will
have 2T-1 segmentation hypotheses. Using the fam-
ous query “mike siwek lawyer mi” described in
(Levy, 2010) as an example, the likelihoods and
the segmented queries for the top 5 hypotheses are
shown in Figure 1.
</bodyText>
<figure confidence="0.59397">
Body:
</figure>
<figureCaption confidence="0.9922505">
Figure 1: Top 5 segmentation hypotheses under
body, title, and anchor language models.
</figureCaption>
<bodyText confidence="0.99936825">
As can be seen, the distinctive styles of the lan-
guages used to compose the body, title, and the
anchor text contribute to their respective models
producing different outcomes on the segmentation
</bodyText>
<figure confidence="0.6081225">
Anchor:
Title:
</figure>
<page confidence="0.997069">
46
</page>
<bodyText confidence="0.9991212">
task, many of which research issues have been ex-
plored in (Huang et al, 2010). It is hopeful that the
release of Microsoft Web N-gram service can ena-
ble the community in general to accelerate the re-
search on this and related areas.
</bodyText>
<sectionHeader confidence="0.938306" genericHeader="method">
4 Word Breaking Demonstration
</sectionHeader>
<bodyText confidence="0.983563631578947">
Word breaking is a challenging NLP task, yet the
effectiveness of employing large amount of data to
tackle word breaking problems has been demon-
strated in (Norvig, 2008). To demonstrate the ap-
plicability of the web N-gram service for the work
breaking problem, we implement the rudimentary
algorithm described in (Norvig, 2008) and extend
it to use body N-gram for ranking the hypotheses.
In essence, the word breaking task can be regarded
as a segmentation task at the character level where
the segment boundaries are delimitated by white
spaces. By using a larger N-gram model, the demo
can successfully tackle the challenging word
breaking examples as mentioned in (Norvig, 2008).
Figure 2 shows the top 5 hypotheses of the simple
algorithm. We note that the word breaking algo-
rithm can fail to insert desired spaces into strings
that are URL fragments and occurred in the docu-
ment body frequently enough.
</bodyText>
<figureCaption confidence="0.7821725">
Figure 2: Norvig&apos;s word breaking examples (Norvig,
2008) re-examined with Microsoft Web N-gram
</figureCaption>
<page confidence="0.998261">
47
</page>
<bodyText confidence="0.999646875">
Two surprising side effects of creating the N-
gram models from the web in general are worth
noting. First, as more and more documents contain
multi-lingual contents, the Microsoft Web N-gram
corpus inevitably include languages other than EN-
US, the intended language. Figure 3 shows exam-
ples in German, French and Chinese (Romanized)
each.
</bodyText>
<figureCaption confidence="0.9712915">
Figure 3: Word breaking examples for foreign lan-
guages: German (top), French and Romanized Chi-
</figureCaption>
<bodyText confidence="0.937917714285714">
nese
Secondly, since the web documents contain many
abbreviations that are popular in short messaging,
the consequent N-gram model lends the simple
word breaking algorithm to cope with the common
short hands surprisingly well. An example that de-
codes the short hand for “wait for you” is shown in
</bodyText>
<figureCaption confidence="0.701314666666667">
Figure 4.
Figure 4: A word breaking example on SMS-style
message.
</figureCaption>
<sectionHeader confidence="0.995983" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999588146341463">
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1. Linguistic Data Consortium, ISBN: 1-
58563-397-6, Philadelphia.
Michel Banko and Eric Brill. 2001. Mitigating the pauc-
ity-of-data problem: exploring the effect of training
corpus size on classifier performance for natural lan-
guage processing. Proc. 1st Internal Conference on
human language technology research, 1-5, San Di-
ego, CA.
David Graff and Christopher Cieri. 2003. English Gi-
gaword. Linguistic Data Consortium, ISBN: 1-
58563-260-0, Philadelphia.
Jian Huang, Jianfeng Gao, Jiangbo Miao, Xiaolong Li,
Kuansan Wang, and Fritz Behr. 2010. Exploring web
scale language models for search query processing.
In Proc. 19th International World Wide Web Confe-
rence (WWW-2010), Raleigh, NC.
Steven Levy, 2010. How Google’s algorithm rules the
web. Wired Magazine, February.
Patrick Nguyen, Jianfeng Gao, and Milind Mahajan.
2007. MSRLM: a scalable language modeling tool-
kit. Microsoft Research Technical Report MSR-TR-
2007-144.
Peter Norvig. 2008. Statistical learning as the ultimate
agile development tool. ACM 17th Conference on In-
formation and Knowledge Management Industry
Event (CIKM-2008), Napa Valley, CA.
Kuansan Wang, Jianfeng Gao, and Xiaolong Li. 2010.
The multi-style language usages on the Web and
their implications on information retrieval. In sub-
mission.
Kuansan Wang, Xiaolong Li and Jianfeng Gao, 2010.
Multi-style language model for web scale informa-
tion retrieval. In Proc. ACM 33rd Conference on Re-
search and Development in Information Retrieval
(SIGIR-2010), Geneva, Switzerland.
Kuansan Wang and Xiaolong Li, 2009. Efficacy of a
constantly adaptive language modeling technique for
web scale application. In Proc. IEEE International
Conference on Acoustics, Speech, and Signal
Processing (ICASSP-2009), Taipei, Taiwan.
</reference>
<page confidence="0.999353">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.432398">
<title confidence="0.999781">An Overview of Microsoft Web N-gram Corpus and Applications</title>
<author confidence="0.824888">Kuansan Wang Christopher Thrasher Evelyne Viegas Xiaolong Li Bo-june</author>
<affiliation confidence="0.945391">Microsoft</affiliation>
<address confidence="0.8899945">One Microsoft Redmond, WA, 98052,</address>
<email confidence="0.999815">webngram@microsoft.com</email>
<abstract confidence="0.994883904761905">This document describes the properties and some applications of the Microsoft Web Ngram corpus. The corpus is designed to have the following characteristics. First, in contrast to static data distribution of previous corpus releases, this N-gram corpus is made publicly available as an XML Web Service so that it can be updated as deemed necessary by the user community to include new words and phrases constantly being added to the Web. Secondly, the corpus makes available various sections of a Web document, specifically, the body, title, and anchor text, as separates models as text contents in these sections are found to possess significantly different statistical properties and therefore are treated as distinct languages from the language modeling point of view. The usages of the corpus are demonstrated here in two NLP tasks: phrase segmentation and word breaking.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<date>2006</date>
<booktitle>Web 1T 5-gram Version 1. Linguistic Data Consortium, ISBN:</booktitle>
<pages>1--58563</pages>
<location>Philadelphia.</location>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1. Linguistic Data Consortium, ISBN: 1-58563-397-6, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Banko</author>
<author>Eric Brill</author>
</authors>
<title>Mitigating the paucity-of-data problem: exploring the effect of training corpus size on classifier performance for natural language processing.</title>
<date>2001</date>
<booktitle>Proc. 1st Internal Conference on human language technology research,</booktitle>
<pages>1--5</pages>
<location>San Diego, CA.</location>
<contexts>
<context position="1206" citStr="Banko and Brill 2001" startWordPosition="184" endWordPosition="187">he user community to include new words and phrases constantly being added to the Web. Secondly, the corpus makes available various sections of a Web document, specifically, the body, title, and anchor text, as separates models as text contents in these sections are found to possess significantly different statistical properties and therefore are treated as distinct languages from the language modeling point of view. The usages of the corpus are demonstrated here in two NLP tasks: phrase segmentation and word breaking. 1 Introduction Since Banko and Brill’s pioneering work almost a decade ago (Banko and Brill 2001), it has been widely observed that the effectiveness of statistical natural language processing (NLP) techniques is highly susceptible to the data size used to develop them. As empirical studies have repeatedly shown that simple algorithms can often outperform their more complicated counterparts in wide varieties of NLP applications with large datasets, many have come to believe that it is the size of data, not the sophistication of the algorithms that ultimately play the central role in modern NLP (Norvig, 2008). Towards this end, there have been considerable efforts in the NLP community to g</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Michel Banko and Eric Brill. 2001. Mitigating the paucity-of-data problem: exploring the effect of training corpus size on classifier performance for natural language processing. Proc. 1st Internal Conference on human language technology research, 1-5, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Christopher Cieri</author>
</authors>
<date>2003</date>
<booktitle>English Gigaword. Linguistic Data Consortium, ISBN:</booktitle>
<pages>1--58563</pages>
<location>Philadelphia.</location>
<contexts>
<context position="1913" citStr="Graff and Cieri, 2003" startWordPosition="295" endWordPosition="298">processing (NLP) techniques is highly susceptible to the data size used to develop them. As empirical studies have repeatedly shown that simple algorithms can often outperform their more complicated counterparts in wide varieties of NLP applications with large datasets, many have come to believe that it is the size of data, not the sophistication of the algorithms that ultimately play the central role in modern NLP (Norvig, 2008). Towards this end, there have been considerable efforts in the NLP community to gather ever larger datasets, culminating the release of the English Giga-word corpus (Graff and Cieri, 2003) and the 1 Tera-word Google N-gram (Thorsten and Franz, 2006) created from arguably the largest text source available, the World Wide Web. Recent research, however, suggests that studies on the document body alone may no longer be sufficient in understanding the language usages in our daily lives. A document, for example, is typically associated with multiple text streams. In addition to the document body that contains the bulk of the contents, there are also the title and the filename/URL the authors choose to name the document. On the web, a document is often linked with anchor text or short</context>
</contexts>
<marker>Graff, Cieri, 2003</marker>
<rawString>David Graff and Christopher Cieri. 2003. English Gigaword. Linguistic Data Consortium, ISBN: 1-58563-260-0, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian Huang</author>
<author>Jianfeng Gao</author>
<author>Jiangbo Miao</author>
<author>Xiaolong Li</author>
<author>Kuansan Wang</author>
<author>Fritz Behr</author>
</authors>
<title>Exploring web scale language models for search query processing.</title>
<date>2010</date>
<booktitle>In Proc. 19th International World Wide Web Conference (WWW-2010),</booktitle>
<location>Raleigh, NC.</location>
<contexts>
<context position="2913" citStr="Huang et al, 2010" startWordPosition="462" endWordPosition="465">ition to the document body that contains the bulk of the contents, there are also the title and the filename/URL the authors choose to name the document. On the web, a document is often linked with anchor text or short messages from social network applications that other authors use to summarize the document, and from the search logs we learn the text queries formulated by the general public to specify the document. A large scale studies reveal that these text streams have significantly different properties and lead to varying degrees of performance in many NLP applications (Wang et al, 2010, Huang et al, 2010). Consequently from the statistical modeling point of view, these streams are better regarded as composed in distinctive languages and treated as such. This observation motivates the creation of Microsoft Web N-gram corpus in which the materials from the body, title and anchor text are made available separately. Another notable feature of the corpus is that Microsoft Web N-gram is available as a cross-platform XML Web service1 that can be freely and readily accessible by users through the Internet anytime and anywhere. The service architecture also makes it straightforward to perform on 1 Plea</context>
<context position="6883" citStr="Huang et al, 2010" startWordPosition="1113" endWordPosition="1116">query of T terms will have 2T-1 segmentation hypotheses. Using the famous query “mike siwek lawyer mi” described in (Levy, 2010) as an example, the likelihoods and the segmented queries for the top 5 hypotheses are shown in Figure 1. Body: Figure 1: Top 5 segmentation hypotheses under body, title, and anchor language models. As can be seen, the distinctive styles of the languages used to compose the body, title, and the anchor text contribute to their respective models producing different outcomes on the segmentation Anchor: Title: 46 task, many of which research issues have been explored in (Huang et al, 2010). It is hopeful that the release of Microsoft Web N-gram service can enable the community in general to accelerate the research on this and related areas. 4 Word Breaking Demonstration Word breaking is a challenging NLP task, yet the effectiveness of employing large amount of data to tackle word breaking problems has been demonstrated in (Norvig, 2008). To demonstrate the applicability of the web N-gram service for the work breaking problem, we implement the rudimentary algorithm described in (Norvig, 2008) and extend it to use body N-gram for ranking the hypotheses. In essence, the word break</context>
</contexts>
<marker>Huang, Gao, Miao, Li, Wang, Behr, 2010</marker>
<rawString>Jian Huang, Jianfeng Gao, Jiangbo Miao, Xiaolong Li, Kuansan Wang, and Fritz Behr. 2010. Exploring web scale language models for search query processing. In Proc. 19th International World Wide Web Conference (WWW-2010), Raleigh, NC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Levy</author>
</authors>
<title>How Google’s algorithm rules the web. Wired Magazine,</title>
<date>2010</date>
<contexts>
<context position="6393" citStr="Levy, 2010" startWordPosition="1032" endWordPosition="1033">gorithm described and implemented in the MSRLM toolkit (Nguyen et al, 2007). The numbers of tokens in the body, title, and anchor text in the snapshot are of the order of 1.4 trillion, 12.5 billion, and 357 billion, respectively. 3 Search Query Segmentation In this demonstration, we implement a straightforward algorithm that generates hypotheses of the segment boundaries at all possible placements in a query and rank their likelihoods using the N-gram service. In other words, a query of T terms will have 2T-1 segmentation hypotheses. Using the famous query “mike siwek lawyer mi” described in (Levy, 2010) as an example, the likelihoods and the segmented queries for the top 5 hypotheses are shown in Figure 1. Body: Figure 1: Top 5 segmentation hypotheses under body, title, and anchor language models. As can be seen, the distinctive styles of the languages used to compose the body, title, and the anchor text contribute to their respective models producing different outcomes on the segmentation Anchor: Title: 46 task, many of which research issues have been explored in (Huang et al, 2010). It is hopeful that the release of Microsoft Web N-gram service can enable the community in general to accele</context>
</contexts>
<marker>Levy, 2010</marker>
<rawString>Steven Levy, 2010. How Google’s algorithm rules the web. Wired Magazine, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Nguyen</author>
<author>Jianfeng Gao</author>
<author>Milind Mahajan</author>
</authors>
<title>MSRLM: a scalable language modeling toolkit. Microsoft Research</title>
<date>2007</date>
<tech>Technical Report MSR-TR2007-144.</tech>
<contexts>
<context position="5857" citStr="Nguyen et al, 2007" startWordPosition="941" endWordPosition="944">any URLs are pointing to the same contents. Currently, the maximum order of the N-gram available is 5, and the numbers of N-grams are shown in Table 1. Table 1: Numbers of N-grams for various streams Body Title Anchor 1-gram 1.2B 60M 150M 2-gram 11.7B 464M 1.1B 3-gram 60.1B 1.4B 3.2B 4-gram 148.5B 2.3B 5.1B 5-gram 237B 3.8B 8.9B 2 The WSDL for the web service is located at http://webngram.research.microsoft.com/Lookup.svc/mex?wsdl. CALM algorithm adapts the model from a seed model based on the June 30, 2009 snapshot of the Web with the algorithm described and implemented in the MSRLM toolkit (Nguyen et al, 2007). The numbers of tokens in the body, title, and anchor text in the snapshot are of the order of 1.4 trillion, 12.5 billion, and 357 billion, respectively. 3 Search Query Segmentation In this demonstration, we implement a straightforward algorithm that generates hypotheses of the segment boundaries at all possible placements in a query and rank their likelihoods using the N-gram service. In other words, a query of T terms will have 2T-1 segmentation hypotheses. Using the famous query “mike siwek lawyer mi” described in (Levy, 2010) as an example, the likelihoods and the segmented queries for th</context>
</contexts>
<marker>Nguyen, Gao, Mahajan, 2007</marker>
<rawString>Patrick Nguyen, Jianfeng Gao, and Milind Mahajan. 2007. MSRLM: a scalable language modeling toolkit. Microsoft Research Technical Report MSR-TR2007-144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Norvig</author>
</authors>
<title>Statistical learning as the ultimate agile development tool.</title>
<date>2008</date>
<booktitle>ACM 17th Conference on Information and Knowledge Management Industry Event (CIKM-2008),</booktitle>
<location>Napa Valley, CA.</location>
<contexts>
<context position="1724" citStr="Norvig, 2008" startWordPosition="266" endWordPosition="267">Introduction Since Banko and Brill’s pioneering work almost a decade ago (Banko and Brill 2001), it has been widely observed that the effectiveness of statistical natural language processing (NLP) techniques is highly susceptible to the data size used to develop them. As empirical studies have repeatedly shown that simple algorithms can often outperform their more complicated counterparts in wide varieties of NLP applications with large datasets, many have come to believe that it is the size of data, not the sophistication of the algorithms that ultimately play the central role in modern NLP (Norvig, 2008). Towards this end, there have been considerable efforts in the NLP community to gather ever larger datasets, culminating the release of the English Giga-word corpus (Graff and Cieri, 2003) and the 1 Tera-word Google N-gram (Thorsten and Franz, 2006) created from arguably the largest text source available, the World Wide Web. Recent research, however, suggests that studies on the document body alone may no longer be sufficient in understanding the language usages in our daily lives. A document, for example, is typically associated with multiple text streams. In addition to the document body th</context>
<context position="7237" citStr="Norvig, 2008" startWordPosition="1175" endWordPosition="1176">styles of the languages used to compose the body, title, and the anchor text contribute to their respective models producing different outcomes on the segmentation Anchor: Title: 46 task, many of which research issues have been explored in (Huang et al, 2010). It is hopeful that the release of Microsoft Web N-gram service can enable the community in general to accelerate the research on this and related areas. 4 Word Breaking Demonstration Word breaking is a challenging NLP task, yet the effectiveness of employing large amount of data to tackle word breaking problems has been demonstrated in (Norvig, 2008). To demonstrate the applicability of the web N-gram service for the work breaking problem, we implement the rudimentary algorithm described in (Norvig, 2008) and extend it to use body N-gram for ranking the hypotheses. In essence, the word breaking task can be regarded as a segmentation task at the character level where the segment boundaries are delimitated by white spaces. By using a larger N-gram model, the demo can successfully tackle the challenging word breaking examples as mentioned in (Norvig, 2008). Figure 2 shows the top 5 hypotheses of the simple algorithm. We note that the word br</context>
</contexts>
<marker>Norvig, 2008</marker>
<rawString>Peter Norvig. 2008. Statistical learning as the ultimate agile development tool. ACM 17th Conference on Information and Knowledge Management Industry Event (CIKM-2008), Napa Valley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuansan Wang</author>
<author>Jianfeng Gao</author>
<author>Xiaolong Li</author>
</authors>
<title>The multi-style language usages on the Web and their implications on information retrieval.</title>
<date>2010</date>
<booktitle>In submission.</booktitle>
<contexts>
<context position="2893" citStr="Wang et al, 2010" startWordPosition="458" endWordPosition="461">xt streams. In addition to the document body that contains the bulk of the contents, there are also the title and the filename/URL the authors choose to name the document. On the web, a document is often linked with anchor text or short messages from social network applications that other authors use to summarize the document, and from the search logs we learn the text queries formulated by the general public to specify the document. A large scale studies reveal that these text streams have significantly different properties and lead to varying degrees of performance in many NLP applications (Wang et al, 2010, Huang et al, 2010). Consequently from the statistical modeling point of view, these streams are better regarded as composed in distinctive languages and treated as such. This observation motivates the creation of Microsoft Web N-gram corpus in which the materials from the body, title and anchor text are made available separately. Another notable feature of the corpus is that Microsoft Web N-gram is available as a cross-platform XML Web service1 that can be freely and readily accessible by users through the Internet anytime and anywhere. The service architecture also makes it straightforward </context>
</contexts>
<marker>Wang, Gao, Li, 2010</marker>
<rawString>Kuansan Wang, Jianfeng Gao, and Xiaolong Li. 2010. The multi-style language usages on the Web and their implications on information retrieval. In submission.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuansan Wang</author>
<author>Xiaolong Li</author>
<author>Jianfeng Gao</author>
</authors>
<title>Multi-style language model for web scale information retrieval.</title>
<date>2010</date>
<booktitle>In Proc. ACM 33rd Conference on Research and Development in Information Retrieval (SIGIR-2010),</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="2893" citStr="Wang et al, 2010" startWordPosition="458" endWordPosition="461">xt streams. In addition to the document body that contains the bulk of the contents, there are also the title and the filename/URL the authors choose to name the document. On the web, a document is often linked with anchor text or short messages from social network applications that other authors use to summarize the document, and from the search logs we learn the text queries formulated by the general public to specify the document. A large scale studies reveal that these text streams have significantly different properties and lead to varying degrees of performance in many NLP applications (Wang et al, 2010, Huang et al, 2010). Consequently from the statistical modeling point of view, these streams are better regarded as composed in distinctive languages and treated as such. This observation motivates the creation of Microsoft Web N-gram corpus in which the materials from the body, title and anchor text are made available separately. Another notable feature of the corpus is that Microsoft Web N-gram is available as a cross-platform XML Web service1 that can be freely and readily accessible by users through the Internet anytime and anywhere. The service architecture also makes it straightforward </context>
</contexts>
<marker>Wang, Li, Gao, 2010</marker>
<rawString>Kuansan Wang, Xiaolong Li and Jianfeng Gao, 2010. Multi-style language model for web scale information retrieval. In Proc. ACM 33rd Conference on Research and Development in Information Retrieval (SIGIR-2010), Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuansan Wang</author>
<author>Xiaolong Li</author>
</authors>
<title>Efficacy of a constantly adaptive language modeling technique for web scale application.</title>
<date>2009</date>
<booktitle>In Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP-2009),</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="4731" citStr="Wang and Li, 2009" startWordPosition="749" endWordPosition="752">ited by Bing are at the order of hundreds of billion, though the spam and other low quality web pages are actively excluded using Bing’s proprietary algorithms. The various streams of the web documents are then downloaded, parsed and tokenized by Bing, in which process the text is lowercased with the punctuation marks removed. However, no stemming, spelling corrections or inflections are performed. Unlike the Google N-gram release which contains raw N-gram counts, Microsoft Web N-gram provides open-vocabulary, smoothed back-off Ngram models for the three text streams using the CALM algorithm (Wang and Li, 2009) that dynamically adapts the N-gram models as web documents are crawled. The design of CALM ensures that new N-grams are incorporated into the models as soon as they are encountered in the crawling and become statistically significant. The models are therefore kept up-to-date with the web contents. CALM is also designed to make sure that duplicated contents will not have outsized impacts in biasing the N-gram statistics. This property is useful as Bing’s crawler visits URLs in parallel and on the web many URLs are pointing to the same contents. Currently, the maximum order of the N-gram availa</context>
</contexts>
<marker>Wang, Li, 2009</marker>
<rawString>Kuansan Wang and Xiaolong Li, 2009. Efficacy of a constantly adaptive language modeling technique for web scale application. In Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP-2009), Taipei, Taiwan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>