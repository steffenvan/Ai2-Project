<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001785">
<title confidence="0.999023">
Generalizing Semantic Role Annotations
Across Syntactically Similar Verbs
</title>
<author confidence="0.996065">
Andrew S. Gordon Reid Swanson
</author>
<affiliation confidence="0.9984745">
Institute for Creative Technologies Institute for Creative Technologies
University of Southern California University of Southern California
</affiliation>
<address confidence="0.58471">
Marina del Rey, CA 90292 USA Marina del Rey, CA 90292 USA
</address>
<email confidence="0.998348">
gordon@ict.usc.edu swansonr@ict.usc.edu
</email>
<sectionHeader confidence="0.995626" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999757">
Large corpora of parsed sentences with
semantic role labels (e.g. PropBank) pro-
vide training data for use in the creation
of high-performance automatic semantic
role labeling systems. Despite the size of
these corpora, individual verbs (or role-
sets) often have only a handful of in-
stances in these corpora, and only a
fraction of English verbs have even a sin-
gle annotation. In this paper, we describe
an approach for dealing with this sparse
data problem, enabling accurate semantic
role labeling for novel verbs (rolesets)
with only a single training example. Our
approach involves the identification of
syntactically similar verbs found in Prop-
Bank, the alignment of arguments in their
corresponding rolesets, and the use of
their corresponding annotations in Prop-
Bank as surrogate training data.
</bodyText>
<sectionHeader confidence="0.989852" genericHeader="method">
1 Generalizing Semantic Role Annotations
</sectionHeader>
<bodyText confidence="0.998150883720931">
A recent release of the PropBank (Palmer et al.,
2005) corpus of semantic role annotations of Tree-
bank parses contained 112,917 labeled instances of
4,250 rolesets corresponding to 3,257 verbs, as
illustrated by this example for the verb buy.
[arg0 Chuck] [buy.01 bought] [arg1 a car] [arg2 from
Jerry] [arg3 for $1000].
Annotations similar to these have been used to cre-
ate automated semantic role labeling systems
(Pradhan et al., 2005; Moschitti et al., 2006) for
use in natural language processing applications that
require only shallow semantic parsing. As with all
machine-learning approaches, the performance of
these systems is heavily dependent on the avail-
ability of adequate amounts of training data. How-
ever, the number of annotated instances in
PropBank varies greatly from verb to verb; there
are 617 annotations for the want roleset, only 7 for
desire, and 0 for any sense of the verb yearn. Do
we need to keep annotating larger and larger cor-
pora in order to generate accurate semantic label-
ing systems for verbs like yearn?
A better approach may be to generalize the data
that exists already to handle novel verbs. It is rea-
sonable to suppose that there must be a number of
verbs within the PropBank corpus that behave
nearly exactly like yearn in the way that they relate
to their constituent arguments. Rather than annotat-
ing new sentences that contain the verb yearn, we
could simply find these similar verbs and use their
annotations as surrogate training data.
This paper describes an approach to generalizing
semantic role annotations across different verbs,
involving two distinct steps. The first step is to
order all of the verbs with semantic role annota-
tions according to their syntactic similarity to the
target verb, followed by the second step of aligning
argument labels between different rolesets. To
evaluate this approach we developed a simple
automated semantic role labeling algorithm based
on the frequency of parse-tree paths, and then
compared its performance when using real and sur-
rogate training data from PropBank.
</bodyText>
<page confidence="0.96937">
192
</page>
<note confidence="0.9258">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 192–199,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.855728" genericHeader="method">
2 Parse Tree Paths
</sectionHeader>
<bodyText confidence="0.970664192307692">
A key concept in understanding our approach to
both automated semantic role annotation and gen-
eralization is the notion of a parse tree path. Parse
tree paths were used for semantic role labeling by
Gildea and Jurafsky (2002) as descriptive features
of the syntactic relationship between predicates
and their arguments in the parse tree of a sentence.
Predicates are typically assumed to be specific tar-
get words (verbs), and arguments are assumed to
be spans of words in the sentence that are domi-
nated by nodes in the parse tree. A parse tree path
can be described as a sequence of transitions up
from the target word then down to the node that
dominates the argument span (e.g. Figure 1).
Figure 1: An example parse tree path from the
predicate ate to the argument NP He, represented
as TVBTVPTSINP
Parse tree paths are particularly interesting for
automated semantic role labeling because they
generalize well across syntactically similar sen-
tences. For example, the parse tree path in Figure 1
would still correctly identify the “eater” argument
in the given sentence if the personal pronoun “he”
were swapped with a markedly different noun
phrase, e.g. “the attendees of the annual holiday
breakfast.”
</bodyText>
<sectionHeader confidence="0.975385" genericHeader="method">
3 A Simple Semantic Role Labeler
</sectionHeader>
<bodyText confidence="0.999047285714286">
To explore issues surrounding the generalization of
semantic role annotations across verbs, we began
by authoring a simple automated semantic role la-
beling algorithm that assigns labels according to
the frequency of the parse tree paths seen in train-
ing data. To construct a labeler for a specific role-
set, training data consisting of parsed sentences
with role-labeled parse tree constituents are ana-
lyzed to identify all of the parse tree paths between
predicates and arguments, which are then tabulated
and sorted by frequency. For example, Table 1 lists
the 10 most frequent pairs of arguments and parse
tree paths for the want.01 roleset in a recent release
of PropBank.
</bodyText>
<table confidence="0.997995636363636">
Count Argument Parse tree path
189 ARG0 TVBPTVPTSINP
159 ARG1 TVBPTVPIS
125 ARG0 TVBZTVPTSINP
110 ARG1 TVBZTVPIS
102 ARG0 TVBTVPTVPTSINP
98 ARG1 TVBTVPIS
96 ARG0 TVBDTVPTSINP
79 ARGM TVBTVPTVPIRB
76 ARG1 TVBDTVPIS
43 ARG1 TVBPTVPINP
</table>
<tableCaption confidence="0.983611">
Table 1. Top 10 most frequent parse tree paths for
</tableCaption>
<bodyText confidence="0.990858685714286">
arguments of the PropBank want.01 roleset, based
on 617 annotations
To automatically assign role labels to an unla-
beled parse tree, each entry in the table is consid-
ered in order of highest frequency. Beginning from
the target word in the sentence (e.g. wants) a check
is made to determine if the entry includes a possi-
ble parse tree path in the parse tree of the sentence.
If so, then the constituent is assigned the role label
of the entry, and all subsequent entries in the table
that have the same argument label or lead to sub-
constituents of the labeled node are invalidated.
Only subsequent entries that assign core arguments
of the roleset (e.g. ARG0, ARG1) are invalidated,
allowing for multiple assignments of non-core la-
bels (e.g. ARGM) to a test sentence. In cases
where the path leads to more than one node in a
sentence, the leftmost path is selected. This process
then continues down the list of valid table entries,
assigning additional labels to unlabeled parse tree
constituents, until the end of the table is reached.
This approach also offers a simple means of
dealing with multiple-constituent arguments,
which occasionally appear in PropBank data. In
these cases, the data is listed as unique entries in
the frequency table, where each of the parse tree
paths to the multiple constituents are listed as a set.
The labeling algorithm will assign the argument of
the entry only if all parse tree paths in the set are
present in the sentence.
The expected performance of this approach to
semantic role labeling was evaluated using the
PropBank data using a leave-one-out cross-
validation experimental design. Precision and re-
call scores were calculated for each of the 3,086
</bodyText>
<page confidence="0.988393">
193
</page>
<bodyText confidence="0.993167279069767">
rolesets with at least two annotations. Figure 2 Next we executed a four-step analysis process
graphs the average precision, recall, and F-score for each of the 3,257 verbs in the PropBank cor-
for rolesets according to the number of training pus. In the first step, we identified each of the sen-
examples of the roleset in the PropBank corpus. tences in the prepared GigaWord corpus that
An additional curve in Figure 2 plots the percent- contained any inflection of the given verb. To
age of these PropBank rolesets that have the given automatically identify all verb inflections, we util-
amount of training data or more. For example, F- ized the English DELA electronic dictionary
scores above 0.7 are first reached with 62 training (Courtois, 2004), which contained all but 21 of the
examples, but only 8% of PropBank rolesets have PropBank verbs (for which we provided the inflec-
this much training data available. tions ourselves), with old-English verb inflections
removed. We extracted GigaWord sentences con-
taining these inflections by using the GNU grep
program and a template regular expression for each
inflection list. The results of these searches were
collected in 3,257 files (one for each verb). The
largest of these files was for inflections of the verb
say (15.9 million sentences), and the smallest was
for the verb namedrop (4 sentences).
The second step was to automatically generate
syntactic parse trees for the GigaWord sentences
found for each verb. It was our original intention to
parse all of the found sentences, but we found that
the slow speed of contemporary syntactic parsers
made this impractical. Instead, we focused our ef-
forts on the first 100 sentences found for each of
the 3,257 verbs with 100 or fewer tokens: a total of
324,461 sentences (average of 99.6 per verb). For
this task we utilized the August 2005 release of the
Charniak parser with the default speed/accuracy
settings (Charniak, 2000), which required roughly
360 hours of processor time on a 2.5 GHz
PowerPC G5.
The third step was to characterize the syntactic
context of the verbs based on where they appeared
within the parse trees. For this purpose, we utilized
parse tree paths as a means of converting tree
structures into a flat, feature-vector representation.
For each sentence, we identified all possible parse
tree paths that begin from the verb inflection and
terminate at a constituent that does not include the
verb inflection. For example, the syntactic context
of the verb in Figure 1 can be described by the fol-
lowing five parse tree paths:
</bodyText>
<listItem confidence="0.7145658">
1. TVBTVPTSINP
2. TVBTVPTSINPIPRP
3. TVBTVPINP
4. TVBTVPINPIDT
5. TVBTVPINPINN
</listItem>
<figureCaption confidence="0.858305">
Possible parse tree paths were identified for
every parsed sentence for a given verb, and the
frequencies of each unique path were tabulated
Figure 2. Performance of our semantic role label-
ing approach on PropBank rolesets
4 Identifying Syntactically Similar Verbs
A key part of generalizing semantic role annota-
tions is to calculate the syntactic similarity be-
tween verbs. The expectation here is that verbs that
</figureCaption>
<bodyText confidence="0.98019679710145">
appear in syntactically similar contexts are going
to behave similarly in the way that they relate to
their arguments. In this section we describe a fully
automated approach to calculating the syntactic
similarity between verbs.
Our approach is strictly empirical; the similarity
of verbs is determined by examining the syntactic
contexts in which they appear in a large text cor-
pus. Our approach is analogous to previous work
in extracting collocations from large text corpora
using syntactic information (Lin, 1998). In our
work, we utilized the GigaWord corpus of English
newswire text (Linguistic Data Consortium, 2003),
consisting of nearly 12 gigabytes of textual data.
To prepare this corpus for analysis, we extracted
the body text from each of the 4.1 million entries
in the corpus and applied a maximum-entropy al-
gorithm to identify sentence boundaries (Reynar
and Ratnaparkhi, 1997).
194
into a feature vector representation. Parse tree
paths where the first node was not a Treebank part-
of-speech tag for a verb were discarded, effectively
filtering the non-verb homonyms of the set of in-
flections. The resulting feature vectors were nor-
malized by dividing the values of each feature by
the number of verb instances used to generate the
parse tree paths; the value of each feature indicates
the proportion of observed inflections in which the
parse tree path is possible. As a representative ex-
ample, 95 verb forms of abandon were found in
the first 100 GigaWord sentences containing any
inflection of this verb. For this verb, 4,472 possible
parse tree paths were tabulated into 3,145 unique
features, 2501 of which occurred only once.
The fourth step was to compute the distance be-
tween a given verb and each of the 3,257 feature
vector representations describing the syntactic con-
text of PropBank verbs. We computed and com-
pared the performance of a wide variety of possible
vector-based distance metrics, including Euclidean,
Manhattan, and Chi-square (with un-normalized
frequency counts), but found that the ubiquitous
cosine measure was least sensitive to variations in
sample size between verbs. To facilitate a com-
parative performance evaluation (section 6), pair-
wise cosine distance measures were calculated
between each pair of PropBank verbs and sorted
into individual files, producing 3,257 lists of 3,257
verbs ordered by similarity.
Table 2 lists the 25 most syntactically similar
pairs of verbs among all PropBank verbs. There
are a number of notable observations in this list.
First is the extremely high similarity between bind
and bound. This is partly due to the fact that they
share an inflection (bound is the irregular past
tense form of bind), so the first 100 instances of
GigaWord sentences for each verb overlap signifi-
cantly, resulting in overlapping feature vector rep-
resentations. Although this problem appears to be
restricted to this one pair of verbs, it could be
avoided in the future by using the part-of-speech
tag in the parse tree to help distinguish between
verb lemmas.
A second observation of Table 2 is that several
verbs appear multiple times in this list, yielding
sets of verbs that all have high syntactic similarity.
Three of these sets account for 19 of the verbs in
this list:
</bodyText>
<listItem confidence="0.93707725">
1. plunge, tumble, dive, jump, fall, fell, dip
2. assail, chide, lambaste
3. buffet, embroil, lock, superimpose, whip-
saw, pluck, whisk, mar, ensconce
</listItem>
<bodyText confidence="0.99911919047619">
The appearance of these sets suggests that our
method of computing syntactic similarity could be
used to identify distinct clusters of verbs that be-
have in very similar ways. In future work, it would
be particularly interesting to compare empirically-
derived verb clusters to verb classes derived from
theoretical considerations (Levin, 1993), and to the
automated verb classification techniques that use
these classes (Joanis and Stevenson, 2003).
A third observation of Table 2 is that the verb
pairs with the highest syntactic similarity are often
synonyms, e.g. the cluster of assail, chide, and
lambaste. As a striking example, the 14 most syn-
tactically similar verbs to believe (in order) are
think, guess, hope, feel, wonder, theorize, fear,
reckon, contend, suppose, understand, know,
doubt, and suggest – all mental action verbs. This
observation further supports the distributional hy-
pothesis of word similarity and corresponding
technologies for identifying synonyms by similar-
ity of lexical-syntactic context (Lin, 1998).
</bodyText>
<table confidence="0.994182461538461">
Verb pairs (instances) Cosine
bind (83) bound (95) 0.950
plunge (94) tumble (87) 0.888
dive (36) plunge (94) 0.867
dive (36) tumble (87) 0.866
jump (79) tumble (87) 0.865
fall (84) fell (102) 0.859
intersperse (99) perch (81) 0.859
assail (100) chide (98) 0.859
dip (81) fell (102) 0.858
buffet (72) embroil (100) 0.856
embroil (100) lock (73) 0.856
embroil (100) superimpose (100) 0.856
fell (102) jump (79) 0.855
fell (102) tumble (87) 0.855
embroil (100) whipsaw (63) 0.850
pluck (100) whisk (99) 0.849
acquit (100) hospitalize (99) 0.849
disincline (70) obligate (94) 0.848
jump (79) plunge (94) 0.848
dive (36) jump (79) 0.847
assail (100) lambaste (100) 0.847
festoon (98) strew (100) 0.846
mar (78) whipsaw (63) 0.846
pluck (100) whipsaw (63) 0.846
ensconce (101) whipsaw (63) 0.845
</table>
<tableCaption confidence="0.987648">
Table 2. Top 25 most syntactically similar pairs of
</tableCaption>
<bodyText confidence="0.8055">
the 3257 verbs in PropBank. Each verb is listed
with the number of inflection instances used to
calculate the cosine measurement.
</bodyText>
<page confidence="0.99815">
195
</page>
<sectionHeader confidence="0.985737" genericHeader="method">
5 Aligning Arguments Across Rolesets
</sectionHeader>
<bodyText confidence="0.999928336734694">
The second key aspect of our approach to general-
izing annotations is to make mappings between the
argument roles of the novel target verb and the
roles used for a given roleset in the PropBank cor-
pus. For example, if we’d like to apply the training
data for a roleset of the verb desire in PropBank to
a novel roleset for the verb yearn, we need to know
that the desirer corresponds to the yearner, the de-
sired to the yearned-for, etc. In this section, we
describe an approach to argument alignment that
involves the application of the semantic role label-
ing approach described in section 3 to a single
training example for the target verb.
To simplify the process of aligning argument la-
bels across rolesets, we make a number of assump-
tions. First, we only consider cases where two
rolesets have exactly the same number of argu-
ments. The version of the PropBank corpus that we
used in this research contained 4250 rolesets, each
with 6 or fewer roles (typically two or three). Ac-
cordingly, when attempting to apply PropBank
data to a novel roleset with a given argument count
(e.g. two), we only consider the subset of Prop-
Bank data that labels rolesets with exactly the same
count.
Second, our approach requires at least one fully-
annotated training example for the target roleset. A
fully-annotated sentence is one that contains a la-
beled constituent in its parse tree for each role in
the roleset. As an illustration, the example sentence
in section 1 (for the roleset buy.01) would not be
considered a fully-annotated training example, as
only four of the five arguments of the PropBank
buy.01 roleset are present in the sentence (it is
missing a benefactor, as in “Chuck bought his
mother a car from Jerry for $1000”).
In both of these simplifying requirements, we
ignore role labels that may be assigned to a sen-
tence but that are not defined as part of the roleset,
specifically the ARGM labels used in PropBank to
label standard proposition modifiers (e.g. location,
time, manner).
Our approach begins with a list of verbs ordered
by their calculated syntactic similarity to the target
verb, as described in section 4 of this paper. We
subsequently apply two steps that transform this
list into an ordered set of rolesets that can be
aligned with the roles used in one or more fully-
annotated training examples of the target verb. In
describing these two steps, we use instigate as an
example target verb. Instigate already appears in
the PropBank corpus as a two-argument roleset,
but it has only a single training example:
[arg0 The Mahatma, or &amp;quot;great souled one,&amp;quot;]
[instigate.01 instigated] [arg1 several campaigns of
passive resistance against the British
government in India].
The syntactic similarity of instigate to all Prop-
Bank verbs was calculated in the manner described
in the previous section. This resulting list of 3,180
entries begins with the following fourteen verbs:
orchestrate, misrepresent, summarize, wreak, rub,
chase, refuse, embezzle, harass, spew, thrash, un-
earth, snub, and erect.
The first step is to replace each of the verbs in
the ordered list with corresponding rolesets from
PropBank that have the same number of roles as
the target verb. As an example, our target roleset
for the verb instigate has two arguments, so each
verb in the ordered list is replaced with the set of
corresponding rolesets that also have two argu-
ments, or removed if no two-argument rolesets
exist for the verb in the PropBank corpus. The or-
dered list of verbs for instigate is transformed into
an ordered list of 2,115 rolesets with two argu-
ments, beginning with the following five entries:
orchestrate.01, chase.01, unearth.01, snub.01, and
erect.01.
The second step is to identify the alignments be-
tween the arguments of the target roleset and each
of the rolesets in the ordered list. Beginning with
the first roleset on the list (e.g. orchestrate.01), we
build a semantic role labeler (as described in sec-
tion 3) using its available training annotations from
the PropPank corpus. We then apply this labeler to
the single, fully-annotated example sentence for
the target verb, treating it as if it were a test exam-
ple of the same roleset. We then check to see if any
of the core (numbered) role labels overlap with the
annotations that are provided. In cases where an
annotated constituent of the target test sentence is
assigned a label from the source roleset, then the
roleset mappings are noted along with the entry in
the ordered list. If no mappings are found, the role-
set is removed from the ordered list.
For example, the roleset for orchestrate.01 con-
tains two arguments (ARG0 and ARG1) that corre-
spond to the “conductor, manager” and the “things
</bodyText>
<page confidence="0.998028">
196
</page>
<bodyText confidence="0.9999144">
being coordinated or managed”. This roleset is
used for only three sentence annotations in the
PropBank corpus. Using these annotations as train-
ing data, we build a semantic role labeler for this
roleset and apply it to the annotated sentence for
instigate.01, treating it as if it were a test sentence
for the roleset orchestrate.01. The labeler assigns
the orchestrate.01 label 4RG1 to the same con-
stituent labeled 4RG1 in the test sentence, but fails
to assign a label to the other argument constituent
in the test sentence. Therefore, a single mapping is
recorded in the ordered list of rolesets, namely that
4RG1 of orchestrate.01 can be mapped to 4RG1
of instigate.01.
After all of the rolesets are considered, we are
left with a filtered list of rolesets with their argu-
ment mappings, ordered by their syntactic similar-
ity to the target verb. For the roleset instigate.01,
this list consists of 789 entries, beginning with the
following 5 mappings.
</bodyText>
<listItem confidence="0.6916924">
1. orchestrate.01, 1:1
2. chase.01, 0:0, 1:1
3. unearth.01, 0:0, 1:1
4. snub.01, 1:1
5. erect.01, 0:0, 1:1
</listItem>
<bodyText confidence="0.9999754">
Given this list, arbitrary amounts of PropBank
annotations can be used as surrogate training data
for the instigate.01 roleset, beginning at the top of
the list. To utilize surrogate training data in our
semantic role labeling approach (Section 3), we
combine parse tree path information for a selected
portion of surrogate training data into a single list
sorted by frequency, and apply these files to test
sentences as normal.
Although we use an existing PropBank roleset
(instigate.01) as an example in this section, this
approach will work for any novel roleset where
one fully-annotated training example is available.
For example, arbitrary amounts of surrogate Prop-
Bank data can be found for the novel verb yearn by
1) searching for sentences with the verb yearn in
the GigaWord corpus, 2) calculating the syntactic
similarity between yearn and all PropBank verbs
as described in Section 4, 3) aligning the argu-
ments in a single fully-annotated example of yearn
with ProbBank rolesets with the same number of
arguments using the method described in this sec-
tion, and 4) selecting arbitrary amounts of Prop-
Bank annotations to use as surrogate training data,
starting from the top of the resulting list.
</bodyText>
<sectionHeader confidence="0.99424" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999994367346939">
We conducted a large-scale evaluation to deter-
mine the performance of our semantic role labeling
algorithm when using variable amounts of surro-
gate training data, and compared these results to
the performance that could be obtained using vari-
ous amounts of real training data (as described in
section 3). Our hypothesis was that learning-curves
for surrogate-trained labelers would be somewhat
less steep, but that the availability of large-amounts
of surrogate training data would more than make
up for the gap.
To test this hypothesis, we conducted an evalua-
tion using the PropBank corpus as our testing data
as well as our source for surrogate training data. As
described in section 5, our approach requires the
availability of at least one fully-annotated sentence
for a given roleset. Only 28.5% of the PropBank
annotations assign labels for each of the numbered
arguments in their given roleset, and only 2,858 of
the 4,250 rolesets used in PropBank annotations
(66.5%) have at least one fully-annotated sentence.
Of these, 2,807 rolesets were for verbs that ap-
peared at least once in our analysis of the Giga-
Word corpus (Section 4). Accordingly, we
evaluated our approach using the annotations for
this set of 2,807 rolesets as test data. For each of
these rolesets, various amounts of surrogate train-
ing data were gathered from all 4,250 rolesets rep-
resented in PropBank, leaving out the data for
whichever roleset was being tested.
For each of the target 2,807 rolesets, we gener-
ated a list of semantic role mappings ordered by
syntactic similarity, using the methods described in
sections 4 and 5. In aligning arguments, only a sin-
gle training example from the target roleset was
used, namely the first annotation within the Prop-
Bank corpus where all of the rolesets arguments
were assigned. Our approach failed to identify any
argument mappings for 41 of the target rolesets,
leaving them without any surrogate training data to
utilize. Of the remaining 2,766 rolesets, the num-
ber of mapped rolesets for a given target ranged
from 1,041 to 1 (mean = 608, stdev = 297).
For each of the 2,766 target rolesets with aligna-
ble roles, we gathered increasingly larger amounts
of surrogate training data by descending the or-
dered list of mappings translating the PropBank
data for each entry according to its argument map-
pings. Then each of these incrementally larger sets
</bodyText>
<page confidence="0.995931">
197
</page>
<bodyText confidence="0.998415038461538">
of training data was then used to build a semantic
role labeler as described in section 3. The perform-
ance of each of the resulting labelers was then
evaluated by applying it to all of the test data
available for target roleset in PropBank, using the
same scoring methods described in section 3. The
performance scores for each labeler were recorded
along with the total number of surrogate training
examples used to build the labeler.
Figure 3 presents the performance result of our
semantic role labeling approach using various
amounts of surrogate training data. Along with
precision, recall, and F-score data, Figure 3 also
graphs the percentage of PropBank rolesets for
which a given amount of training data had been
identified using our approach, of the 2,858 rolesets
with at least one fully-annotated training example.
For instance, with 120 surrogate annotations our
system achieves an F-score above 0.5, and we
identified this much surrogate training data for
96% of PropBank rolesets with at least one fully-
annotated sentence. This represents 64% of all
PropBank rolesets that are used for annotation.
Beyond 120 surrogate training examples, F-scores
remain around 0.6 before slowly declining after
around 700 examples.
</bodyText>
<figureCaption confidence="0.6646765">
Figure 3. Performance of our semantic role label-
ing approach on PropBank rolesets using various
amounts of surrogate training data
Several interesting comparisons can be made be-
tween the results presented in Figure 3 and those in
Figure 2, where actual PropBank training data is
used instead of surrogate training data. First, the
precision obtained with surrogate training data is
roughly 10% lower than with real data. Second, the
recall performance of surrogate data performs
</figureCaption>
<bodyText confidence="0.999951666666666">
similar to real data at first, but is consistently 10%
lower than with real data after the first 50 training
examples. Accordingly, F-scores for surrogate
training data are 10% lower overall.
Even though the performance obtained using
surrogate training data is less than with actual data,
there is abundant amounts of it available for most
PropBank rolesets. Comparing the “% of rolesets”
plots in Figures 2 and 3, the real value of surrogate
training data is apparent. Figure 2 suggests that
over 20 real training examples are needed to
achieve F-scores that are consistently above 0.5,
but that less than 20% of PropBank rolesets have
this much data available. In contrast, 64% of all
PropBank rolesets can achieve this F-score per-
formance with the use of surrogate training data.
This percentage increases to 96% if every Prop-
Bank roleset is given at least one fully annotated
sentence, where all of its numbered arguments are
assigned to constituents.
In addition to supplementing the real training
data available for existing PropBank rolesets, these
results predict the labeling performance that can be
obtained by applying this technique to a novel
roleset with one fully-annotated training example,
e.g. for the verb yearn. Using the first 120 surro-
gate training examples and our simple semantic
role labeling approach, we would expect F-scores
that are above 0.5, and that using the first 700
would yield F-scores around 0.6.
</bodyText>
<sectionHeader confidence="0.999521" genericHeader="conclusions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999893222222222">
The overall performance of our semantic role la-
beling approach is not competitive with leading
contemporary systems, which typically employ
support vector machine learning algorithms with
syntactic features (Pradhan et al., 2005) or syntac-
tic tree kernels (Moschitti et al., 2006). However,
our work highlights a number of characteristics of
the semantic role labeling task that will be helpful
in improving performance in future systems. Parse
tree paths features can be used to achieve high pre-
cision in semantic role labeling, but much of this
precision may be specific to individual verbs. By
generalizing parse tree path features only across
syntactically similar verbs, we have shown that the
drop in precision can be limited to roughly 10%.
The approach that we describe in this paper is
not dependent on the use of PropBank rolesets; any
large corpus of semantic role annotations could be
</bodyText>
<page confidence="0.995835">
198
</page>
<bodyText confidence="0.999993272727273">
generalized in this manner. In particular, our ap-
proach would be applicable to corpora with frame-
specific role labels, e.g. FrameNet (Baker et al.,
1998). Likewise, our approach to generalizing
parse tree path feature across syntactically similar
verbs may improve the performance of automated
semantic role labeling systems based on FrameNet
data. Our work suggests that feature generalization
based on verb-similarity may compliment ap-
proaches to generalization based on role-similarity
(Gildea and Jurafsky, 2002; Baldewein et al.,
2004).
There are a number of improvements that could
be made to the approach described in this paper.
Enhancements to the simple semantic role labeling
algorithm would improve the alignment of argu-
ments across rolesets, which would help align role-
sets with greater syntactic similarity, as well as
improve the performance obtained using the surro-
gate training data in assigning semantic roles.
This research raises many questions about the
relationship between syntactic context and verb
semantics. An important area for future research
will be to explore the correlation between our dis-
tance metric for syntactic similarity and various
quantitative measures of semantic similarity
(Pedersen, et al., 2004). Particularly interesting
would be to explore whether different senses of a
given verb exhibited markedly different profiles of
syntactic context. A strong syntactic/semantic cor-
relation would suggest that further gains in the use
of surrogate annotation data could be gained if syn-
tactic similarity was computed between rolesets
rather than their verbs. However, this would first
require accurate word-sense disambiguation both
for the test sentences as well as for the parsed cor-
pora used to calculate parse tree path frequencies.
Alternatively, parse tree path profiles associated
with rolesets may be useful for word sense disam-
biguation, where the probability of a sense is com-
puted as the likelihood that an ambiguous verb&apos;s
parse tree paths are sampled from the distributions
associated with each verb sense. These topics will
be the focus of our future work in this area.
</bodyText>
<sectionHeader confidence="0.998843" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999970833333333">
The project or effort depicted was or is sponsored
by the U.S. Army Research, Development, and
Engineering Command (RDECOM), and that the
content or information does not necessarily reflect
the position or the policy of the Government, and
no official endorsement should be inferred.
</bodyText>
<sectionHeader confidence="0.996467" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999205">
Baker, C., Fillmore, C., and Lowe, J. 1998. The Ber-
keley FrameNet Project, In Proceedings of COLING-
ACL, Montreal.
Baldewein, U., Erk, K., Pado, S., and Prescher, D. 2004.
Semantic role labeling with similarity-based gener-
alization using EM-based clustering. Proceedings of
Senseval-3, Barcelona.
Charniak, E. 2000. A maximum-entropy-inspired
parser, Proceedings NAACL-ANLP, Seattle.
Courtois, B. 2004. Dictionnaires électroniques DELAF
anglais et français. In C. Leclère, E. Laporte, M. Piot
and M. Silberztein (eds.) Syntax, Lexis and Lexicon-
Grammar: Papers in Honour of Maurice Gross. Am-
sterdam: John Benjamins.
Gildea, D. and Jurafsky, D. 2002. Automatic Labeling
of Semantic Roles. Computational Linguistics 28:3,
245-288.
Joanis, E. and Stevenson, S. 2003. A general feature
space for automatic verb classification. Proceedings
EACL, Budapest.
Levin, B. 1993. English Verb Classes and Alterna-tions:
A Preliminary Investigation. Chicago, IL: University
of Chicago Press.
Lin, D. 1998. Automatic Retrieval and Clustering of
Similar Words. COLING-ACL, Montreal.
Linguistic Data Consortium. 2003. English Gigaword.
Catalog number LDC2003T05. Available from LDC
at http://www.ldc.upenn.edu.
Moschitti, A., Pighin, D. and Basili, R. 2006. Semantic
Role Labeling via Tree Kernel joint inference. Pro-
ceedings of CoNLL, New York.
Palmer, M., Gildea, D., and Kingsbury, P. 2005. The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics 31(1):71-106.
Pedersen, T., Patwardhan, S. and Michelizzi, J. 2004.
WordNet::Similarity - Measuring the Relatedness of
Concepts. Proceedings NAACL-04, Boston, MA.
Pradhan, S., Ward, W., Hacioglu, K., Martin, J., and
Jurafsky, D. 2005. Semantic role labeling using dif-
ferent syntactic views. Proceedings ACL-2005, Ann
Arbor, MI.
Reynar, J. and Ratnaparkhi, A. 1997. A Maximum En-
tropy Approach to Identifying Sentence Boundaries.
Proceedings of ANLP, Washington, D.C.
</reference>
<page confidence="0.998914">
199
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.990122">Generalizing Semantic Role Annotations Across Syntactically Similar Verbs</title>
<author confidence="0.999978">Andrew S Gordon Reid Swanson</author>
<affiliation confidence="0.999489">Institute for Creative Technologies Institute for Creative Technologies University of Southern California University of Southern California</affiliation>
<address confidence="0.995406">Marina del Rey, CA 90292 USA Marina del Rey, CA 90292 USA</address>
<email confidence="0.999742">gordon@ict.usc.eduswansonr@ict.usc.edu</email>
<abstract confidence="0.998079753846154">Large corpora of parsed sentences with semantic role labels (e.g. PropBank) provide training data for use in the creation of high-performance automatic semantic role labeling systems. Despite the size of these corpora, individual verbs (or rolesets) often have only a handful of instances in these corpora, and only a fraction of English verbs have even a single annotation. In this paper, we describe an approach for dealing with this sparse data problem, enabling accurate semantic role labeling for novel verbs (rolesets) with only a single training example. Our approach involves the identification of syntactically similar verbs found in Prop- Bank, the alignment of arguments in their corresponding rolesets, and the use of their corresponding annotations in Prop- Bank as surrogate training data. 1 Generalizing Semantic Role Annotations A recent release of the PropBank (Palmer et al., 2005) corpus of semantic role annotations of Treebank parses contained 112,917 labeled instances of 4,250 rolesets corresponding to 3,257 verbs, as by this example for the verb Chuck]bought]a car] from for $1000]. Annotations similar to these have been used to create automated semantic role labeling systems (Pradhan et al., 2005; Moschitti et al., 2006) for use in natural language processing applications that require only shallow semantic parsing. As with all machine-learning approaches, the performance of these systems is heavily dependent on the availability of adequate amounts of training data. However, the number of annotated instances in PropBank varies greatly from verb to verb; there 617 annotations for the only 7 for and 0 for any sense of the verb Do we need to keep annotating larger and larger corpora in order to generate accurate semantic labelsystems for verbs like A better approach may be to generalize the data that exists already to handle novel verbs. It is reasonable to suppose that there must be a number of verbs within the PropBank corpus that behave exactly like the way that they relate to their constituent arguments. Rather than annotatnew sentences that contain the verb we could simply find these similar verbs and use their annotations as surrogate training data. This paper describes an approach to generalizing semantic role annotations across different verbs, involving two distinct steps. The first step is to order all of the verbs with semantic role annotations according to their syntactic similarity to the target verb, followed by the second step of aligning argument labels between different rolesets. To evaluate this approach we developed a simple automated semantic role labeling algorithm based on the frequency of parse-tree paths, and then compared its performance when using real and surrogate training data from PropBank.</abstract>
<note confidence="0.729169333333333">192 of the 45th Annual Meeting of the Association of Computational pages 192–199, Czech Republic, June 2007. Association for Computational Linguistics</note>
<title confidence="0.29575">2 Parse Tree Paths</title>
<abstract confidence="0.970350748148149">A key concept in understanding our approach to both automated semantic role annotation and genis the notion of a tree Parse tree paths were used for semantic role labeling by Gildea and Jurafsky (2002) as descriptive features of the syntactic relationship between predicates and their arguments in the parse tree of a sentence. Predicates are typically assumed to be specific target words (verbs), and arguments are assumed to be spans of words in the sentence that are dominated by nodes in the parse tree. A parse tree path can be described as a sequence of transitions up from the target word then down to the node that dominates the argument span (e.g. Figure 1). Figure 1: An example parse tree path from the the argument NP represented Parse tree paths are particularly interesting for automated semantic role labeling because they generalize well across syntactically similar sentences. For example, the parse tree path in Figure 1 would still correctly identify the “eater” argument in the given sentence if the personal pronoun “he” were swapped with a markedly different noun phrase, e.g. “the attendees of the annual holiday breakfast.” 3 A Simple Semantic Role Labeler To explore issues surrounding the generalization of semantic role annotations across verbs, we began by authoring a simple automated semantic role labeling algorithm that assigns labels according to the frequency of the parse tree paths seen in training data. To construct a labeler for a specific roleset, training data consisting of parsed sentences with role-labeled parse tree constituents are analyzed to identify all of the parse tree paths between predicates and arguments, which are then tabulated and sorted by frequency. For example, Table 1 lists the 10 most frequent pairs of arguments and parse paths for the in a recent release of PropBank. Count Argument Parse tree path 189 ARG0 159 ARG1 125 ARG0 110 ARG1 102 ARG0 98 ARG1 96 ARG0 79 ARGM 76 ARG1 43 ARG1 Table 1. Top 10 most frequent parse tree paths for arguments of the PropBank want.01 roleset, based on 617 annotations To automatically assign role labels to an unlabeled parse tree, each entry in the table is considered in order of highest frequency. Beginning from target word in the sentence (e.g. a check is made to determine if the entry includes a possible parse tree path in the parse tree of the sentence. If so, then the constituent is assigned the role label of the entry, and all subsequent entries in the table that have the same argument label or lead to subconstituents of the labeled node are invalidated. Only subsequent entries that assign core arguments of the roleset (e.g. ARG0, ARG1) are invalidated, allowing for multiple assignments of non-core labels (e.g. ARGM) to a test sentence. In cases where the path leads to more than one node in a sentence, the leftmost path is selected. This process then continues down the list of valid table entries, assigning additional labels to unlabeled parse tree constituents, until the end of the table is reached. This approach also offers a simple means of dealing with multiple-constituent arguments, which occasionally appear in PropBank data. In these cases, the data is listed as unique entries in the frequency table, where each of the parse tree paths to the multiple constituents are listed as a set. The labeling algorithm will assign the argument of the entry only if all parse tree paths in the set are present in the sentence. The expected performance of this approach to semantic role labeling was evaluated using the PropBank data using a leave-one-out crossvalidation experimental design. Precision and recall scores were calculated for each of the 3,086 193 rolesets with at least two annotations. Figure 2 graphs the average precision, recall, and F-score for rolesets according to the number of training examples of the roleset in the PropBank corpus. An additional curve in Figure 2 plots the percent-age of these PropBank rolesets that have the given amount of training data or more. For example, F-scores above 0.7 are first reached with 62 training examples, but only 8% of PropBank rolesets have this much training data available. Next we executed a four-step analysis process for each of the 3,257 verbs in the PropBank cor-pus. In the first step, we identified each of the sen-tences in the prepared GigaWord corpus that contained any inflection of the given verb. To automatically identify all verb inflections, we util-ized the English DELA electronic dictionary (Courtois, 2004), which contained all but 21 of the PropBank verbs (for which we provided the inflec-tions ourselves), with old-English verb inflections removed. We extracted GigaWord sentences conthese inflections by using the GNU program and a template regular expression for each inflection list. The results of these searches were collected in 3,257 files (one for each verb). The largest of these files was for inflections of the verb million sentences), and the smallest was the verb sentences). The second step was to automatically generate syntactic parse trees for the GigaWord sentences found for each verb. It was our original intention to parse all of the found sentences, but we found that the slow speed of contemporary syntactic parsers made this impractical. Instead, we focused our ef-forts on the first 100 sentences found for each of the 3,257 verbs with 100 or fewer tokens: a total of 324,461 sentences (average of 99.6 per verb). For this task we utilized the August 2005 release of the Charniak parser with the default speed/accuracy settings (Charniak, 2000), which required roughly 360 hours of processor time on a 2.5 GHz PowerPC G5. The third step was to characterize the syntactic context of the verbs based on where they appeared within the parse trees. For this purpose, we utilized parse tree paths as a means of converting tree structures into a flat, feature-vector representation. each sentence, we identified all tree paths that begin from the verb inflection and terminate at a constituent that does not include the verb inflection. For example, the syntactic context of the verb in Figure 1 can be described by the fol-lowing five parse tree paths: 1. 2. 3. 4. 5. Possible parse tree paths were identified for every parsed sentence for a given verb, and the frequencies of each unique path were tabulated Figure 2. Performance of our semantic role label-ing approach on PropBank rolesets 4 Identifying Syntactically Similar Verbs A key part of generalizing semantic role annota-tions is to calculate the syntactic similarity be-tween verbs. The expectation here is that verbs that appear in syntactically similar contexts are going to behave similarly in the way that they relate to their arguments. In this section we describe a fully automated approach to calculating the syntactic similarity between verbs. Our approach is strictly empirical; the similarity of verbs is determined by examining the syntactic contexts in which they appear in a large text cor-pus. Our approach is analogous to previous work in extracting collocations from large text corpora using syntactic information (Lin, 1998). In our work, we utilized the GigaWord corpus of English newswire text (Linguistic Data Consortium, 2003), consisting of nearly 12 gigabytes of textual data. To prepare this corpus for analysis, we extracted the body text from each of the 4.1 million entries in the corpus and applied a maximum-entropy al-gorithm to identify sentence boundaries (Reynar and Ratnaparkhi, 1997). 194 into a feature vector representation. Parse tree paths where the first node was not a Treebank partof-speech tag for a verb were discarded, effectively filtering the non-verb homonyms of the set of inflections. The resulting feature vectors were normalized by dividing the values of each feature by the number of verb instances used to generate the parse tree paths; the value of each feature indicates the proportion of observed inflections in which the parse tree path is possible. As a representative ex- 95 verb forms of found in the first 100 GigaWord sentences containing any inflection of this verb. For this verb, 4,472 possible parse tree paths were tabulated into 3,145 unique features, 2501 of which occurred only once. The fourth step was to compute the distance between a given verb and each of the 3,257 feature vector representations describing the syntactic context of PropBank verbs. We computed and compared the performance of a wide variety of possible vector-based distance metrics, including Euclidean, Manhattan, and Chi-square (with un-normalized frequency counts), but found that the ubiquitous cosine measure was least sensitive to variations in sample size between verbs. To facilitate a comparative performance evaluation (section 6), pairwise cosine distance measures were calculated between each pair of PropBank verbs and sorted into individual files, producing 3,257 lists of 3,257 verbs ordered by similarity. Table 2 lists the 25 most syntactically similar pairs of verbs among all PropBank verbs. There are a number of notable observations in this list. is the extremely high similarity between This is partly due to the fact that they an inflection the irregular past form of so the first 100 instances of GigaWord sentences for each verb overlap significantly, resulting in overlapping feature vector representations. Although this problem appears to be restricted to this one pair of verbs, it could be avoided in the future by using the part-of-speech tag in the parse tree to help distinguish between verb lemmas. A second observation of Table 2 is that several verbs appear multiple times in this list, yielding sets of verbs that all have high syntactic similarity. Three of these sets account for 19 of the verbs in this list: 1. plunge, tumble, dive, jump, fall, fell, dip 2. assail, chide, lambaste 3. buffet, embroil, lock, superimpose, whipsaw, pluck, whisk, mar, ensconce The appearance of these sets suggests that our method of computing syntactic similarity could be used to identify distinct clusters of verbs that behave in very similar ways. In future work, it would be particularly interesting to compare empiricallyderived verb clusters to verb classes derived from theoretical considerations (Levin, 1993), and to the automated verb classification techniques that use these classes (Joanis and Stevenson, 2003). A third observation of Table 2 is that the verb pairs with the highest syntactic similarity are often e.g. the cluster of and As a striking example, the 14 most synsimilar verbs to order) are and mental action verbs. This observation further supports the distributional hypothesis of word similarity and corresponding technologies for identifying synonyms by similarity of lexical-syntactic context (Lin, 1998). Verb pairs (instances) Cosine bind (83) bound (95) 0.950 plunge (94) tumble (87) 0.888 dive (36) plunge (94) 0.867 dive (36) tumble (87) 0.866 jump (79) tumble (87) 0.865 fall (84) fell (102) 0.859 intersperse (99) perch (81) 0.859 assail (100) chide (98) 0.859 dip (81) fell (102) 0.858 buffet (72) embroil (100) 0.856 embroil (100) lock (73) 0.856 embroil (100) superimpose (100) 0.856 fell (102) jump (79) 0.855 fell (102) tumble (87) 0.855 embroil (100) whipsaw (63) 0.850 pluck (100) whisk (99) 0.849 acquit (100) hospitalize (99) 0.849 disincline (70) obligate (94) 0.848 jump (79) plunge (94) 0.848 dive (36) jump (79) 0.847 assail (100) lambaste (100) 0.847 festoon (98) strew (100) 0.846 mar (78) whipsaw (63) 0.846 pluck (100) whipsaw (63) 0.846 ensconce (101) whipsaw (63) 0.845 Table 2. Top 25 most syntactically similar pairs of the 3257 verbs in PropBank. Each verb is listed with the number of inflection instances used to calculate the cosine measurement. 195 5 Aligning Arguments Across Rolesets The second key aspect of our approach to generalizing annotations is to make mappings between the argument roles of the novel target verb and the roles used for a given roleset in the PropBank corpus. For example, if we’d like to apply the training for a roleset of the verb PropBank to novel roleset for the verb we need to know the to the the dethe etc. In this section, we describe an approach to argument alignment that involves the application of the semantic role labeling approach described in section 3 to a single training example for the target verb. To simplify the process of aligning argument labels across rolesets, we make a number of assumptions. First, we only consider cases where two rolesets have exactly the same number of arguments. The version of the PropBank corpus that we used in this research contained 4250 rolesets, each with 6 or fewer roles (typically two or three). Accordingly, when attempting to apply PropBank data to a novel roleset with a given argument count (e.g. two), we only consider the subset of Prop- Bank data that labels rolesets with exactly the same count. Second, our approach requires at least one fullyannotated training example for the target roleset. A fully-annotated sentence is one that contains a labeled constituent in its parse tree for each role in the roleset. As an illustration, the example sentence section 1 (for the roleset would not be considered a fully-annotated training example, as only four of the five arguments of the PropBank are present in the sentence (it is a as in “Chuck bought car from Jerry for $1000”). In both of these simplifying requirements, we ignore role labels that may be assigned to a sentence but that are not defined as part of the roleset, the used in PropBank to label standard proposition modifiers (e.g. location, time, manner). Our approach begins with a list of verbs ordered by their calculated syntactic similarity to the target verb, as described in section 4 of this paper. We subsequently apply two steps that transform this list into an ordered set of rolesets that can be aligned with the roles used in one or more fullyannotated training examples of the target verb. In these two steps, we use an example target verb. Instigate already appears in the PropBank corpus as a two-argument roleset, but it has only a single training example: Mahatma, or &amp;quot;great souled one,&amp;quot;] instigated]several campaigns of passive resistance against the British government in India]. syntactic similarity of all Prop- Bank verbs was calculated in the manner described in the previous section. This resulting list of 3,180 entries begins with the following fourteen verbs: unand The first step is to replace each of the verbs in the ordered list with corresponding rolesets from PropBank that have the same number of roles as the target verb. As an example, our target roleset the verb two arguments, so each verb in the ordered list is replaced with the set of corresponding rolesets that also have two arguments, or removed if no two-argument rolesets exist for the verb in the PropBank corpus. The orlist of verbs for transformed into an ordered list of 2,115 rolesets with two arguments, beginning with the following five entries: and The second step is to identify the alignments between the arguments of the target roleset and each of the rolesets in the ordered list. Beginning with first roleset on the list (e.g. we build a semantic role labeler (as described in section 3) using its available training annotations from the PropPank corpus. We then apply this labeler to the single, fully-annotated example sentence for the target verb, treating it as if it were a test example of the same roleset. We then check to see if any the core (numbered) role labels the annotations that are provided. In cases where an annotated constituent of the target test sentence is assigned a label from the source roleset, then the roleset mappings are noted along with the entry in the ordered list. If no mappings are found, the roleset is removed from the ordered list. example, the roleset for contwo arguments that correspond to the “conductor, manager” and the “things 196 being coordinated or managed”. This roleset is used for only three sentence annotations in the PropBank corpus. Using these annotations as training data, we build a semantic role labeler for this roleset and apply it to the annotated sentence for treating it as if it were a test sentence the roleset The labeler assigns the same conlabeled the test sentence, but fails to assign a label to the other argument constituent in the test sentence. Therefore, a single mapping is recorded in the ordered list of rolesets, namely that be mapped to After all of the rolesets are considered, we are left with a filtered list of rolesets with their argument mappings, ordered by their syntactic similarity to the target verb. For the roleset instigate.01, this list consists of 789 entries, beginning with the following 5 mappings. 1. 1:1 2. 0:0, 1:1 3. 0:0, 1:1 4. 1:1 5. 0:0, 1:1 Given this list, arbitrary amounts of PropBank annotations can be used as surrogate training data the beginning at the top of the list. To utilize surrogate training data in our semantic role labeling approach (Section 3), we combine parse tree path information for a selected portion of surrogate training data into a single list sorted by frequency, and apply these files to test sentences as normal. Although we use an existing PropBank roleset as an example in this section, this approach will work for any novel roleset where one fully-annotated training example is available. For example, arbitrary amounts of surrogate Propdata can be found for the novel verb searching for sentences with the verb the GigaWord corpus, 2) calculating the syntactic between all PropBank verbs as described in Section 4, 3) aligning the arguin a single fully-annotated example of with ProbBank rolesets with the same number of arguments using the method described in this section, and 4) selecting arbitrary amounts of Prop- Bank annotations to use as surrogate training data, starting from the top of the resulting list. 6 Evaluation We conducted a large-scale evaluation to determine the performance of our semantic role labeling algorithm when using variable amounts of surrogate training data, and compared these results to the performance that could be obtained using various amounts of real training data (as described in section 3). Our hypothesis was that learning-curves for surrogate-trained labelers would be somewhat less steep, but that the availability of large-amounts of surrogate training data would more than make up for the gap. To test this hypothesis, we conducted an evaluation using the PropBank corpus as our testing data as well as our source for surrogate training data. As described in section 5, our approach requires the availability of at least one fully-annotated sentence for a given roleset. Only 28.5% of the PropBank annotations assign labels for each of the numbered arguments in their given roleset, and only 2,858 of the 4,250 rolesets used in PropBank annotations (66.5%) have at least one fully-annotated sentence. Of these, 2,807 rolesets were for verbs that appeared at least once in our analysis of the Giga- Word corpus (Section 4). Accordingly, we evaluated our approach using the annotations for this set of 2,807 rolesets as test data. For each of these rolesets, various amounts of surrogate training data were gathered from all 4,250 rolesets represented in PropBank, leaving out the data for whichever roleset was being tested. For each of the target 2,807 rolesets, we generated a list of semantic role mappings ordered by syntactic similarity, using the methods described in sections 4 and 5. In aligning arguments, only a single training example from the target roleset was used, namely the first annotation within the Prop- Bank corpus where all of the rolesets arguments were assigned. Our approach failed to identify any argument mappings for 41 of the target rolesets, leaving them without any surrogate training data to utilize. Of the remaining 2,766 rolesets, the number of mapped rolesets for a given target ranged from 1,041 to 1 (mean = 608, stdev = 297). For each of the 2,766 target rolesets with alignable roles, we gathered increasingly larger amounts of surrogate training data by descending the ordered list of mappings translating the PropBank data for each entry according to its argument mappings. Then each of these incrementally larger sets 197 of training data was then used to build a semantic role labeler as described in section 3. The performance of each of the resulting labelers was then evaluated by applying it to all of the test data available for target roleset in PropBank, using the same scoring methods described in section 3. The performance scores for each labeler were recorded along with the total number of surrogate training examples used to build the labeler. Figure 3 presents the performance result of our semantic role labeling approach using various amounts of surrogate training data. Along with precision, recall, and F-score data, Figure 3 also graphs the percentage of PropBank rolesets for which a given amount of training data had been identified using our approach, of the 2,858 rolesets with at least one fully-annotated training example. For instance, with 120 surrogate annotations our system achieves an F-score above 0.5, and we identified this much surrogate training data for 96% of PropBank rolesets with at least one fullyannotated sentence. This represents 64% of all PropBank rolesets that are used for annotation. Beyond 120 surrogate training examples, F-scores remain around 0.6 before slowly declining after around 700 examples. Figure 3. Performance of our semantic role labeling approach on PropBank rolesets using various amounts of surrogate training data Several interesting comparisons can be made between the results presented in Figure 3 and those in Figure 2, where actual PropBank training data is used instead of surrogate training data. First, the precision obtained with surrogate training data is roughly 10% lower than with real data. Second, the recall performance of surrogate data performs similar to real data at first, but is consistently 10% lower than with real data after the first 50 training examples. Accordingly, F-scores for surrogate training data are 10% lower overall. Even though the performance obtained using surrogate training data is less than with actual data, there is abundant amounts of it available for most PropBank rolesets. Comparing the “% of rolesets” plots in Figures 2 and 3, the real value of surrogate training data is apparent. Figure 2 suggests that over 20 real training examples are needed to achieve F-scores that are consistently above 0.5, but that less than 20% of PropBank rolesets have this much data available. In contrast, 64% of all PropBank rolesets can achieve this F-score performance with the use of surrogate training data. This percentage increases to 96% if every Prop- Bank roleset is given at least one fully annotated sentence, where all of its numbered arguments are assigned to constituents. In addition to supplementing the real training data available for existing PropBank rolesets, these results predict the labeling performance that can be obtained by applying this technique to a novel roleset with one fully-annotated training example, for the verb Using the first 120 surrogate training examples and our simple semantic role labeling approach, we would expect F-scores that are above 0.5, and that using the first 700 would yield F-scores around 0.6. 7 Discussion The overall performance of our semantic role labeling approach is not competitive with leading contemporary systems, which typically employ support vector machine learning algorithms with syntactic features (Pradhan et al., 2005) or syntactic tree kernels (Moschitti et al., 2006). However, our work highlights a number of characteristics of the semantic role labeling task that will be helpful in improving performance in future systems. Parse tree paths features can be used to achieve high precision in semantic role labeling, but much of this precision may be specific to individual verbs. By generalizing parse tree path features only across syntactically similar verbs, we have shown that the drop in precision can be limited to roughly 10%. The approach that we describe in this paper is not dependent on the use of PropBank rolesets; any large corpus of semantic role annotations could be 198 generalized in this manner. In particular, our approach would be applicable to corpora with framespecific role labels, e.g. FrameNet (Baker et al., 1998). Likewise, our approach to generalizing parse tree path feature across syntactically similar verbs may improve the performance of automated semantic role labeling systems based on FrameNet data. Our work suggests that feature generalization based on verb-similarity may compliment approaches to generalization based on role-similarity (Gildea and Jurafsky, 2002; Baldewein et al., 2004). There are a number of improvements that could be made to the approach described in this paper. Enhancements to the simple semantic role labeling algorithm would improve the alignment of arguments across rolesets, which would help align rolesets with greater syntactic similarity, as well as improve the performance obtained using the surrogate training data in assigning semantic roles. This research raises many questions about the relationship between syntactic context and verb semantics. An important area for future research will be to explore the correlation between our distance metric for syntactic similarity and various quantitative measures of semantic similarity (Pedersen, et al., 2004). Particularly interesting would be to explore whether different senses of a given verb exhibited markedly different profiles of syntactic context. A strong syntactic/semantic correlation would suggest that further gains in the use of surrogate annotation data could be gained if syntactic similarity was computed between rolesets rather than their verbs. However, this would first require accurate word-sense disambiguation both for the test sentences as well as for the parsed corpora used to calculate parse tree path frequencies. Alternatively, parse tree path profiles associated with rolesets may be useful for word sense disambiguation, where the probability of a sense is computed as the likelihood that an ambiguous verb&apos;s parse tree paths are sampled from the distributions associated with each verb sense. These topics will be the focus of our future work in this area. Acknowledgments The project or effort depicted was or is sponsored by the U.S. Army Research, Development, and Engineering Command (RDECOM), and that the content or information does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred.</abstract>
<note confidence="0.63485055">References Baker, C., Fillmore, C., and Lowe, J. 1998. The Berkeley FrameNet Project, In Proceedings of COLING- ACL, Montreal. Baldewein, U., Erk, K., Pado, S., and Prescher, D. 2004. Semantic role labeling with similarity-based generalization using EM-based clustering. Proceedings of Senseval-3, Barcelona. Charniak, E. 2000. A maximum-entropy-inspired parser, Proceedings NAACL-ANLP, Seattle. Courtois, B. 2004. Dictionnaires électroniques DELAF anglais et français. In C. Leclère, E. Laporte, M. Piot and M. Silberztein (eds.) Syntax, Lexis and Lexicon- Grammar: Papers in Honour of Maurice Gross. Amsterdam: John Benjamins. Gildea, D. and Jurafsky, D. 2002. Automatic Labeling of Semantic Roles. Computational Linguistics 28:3, 245-288. Joanis, E. and Stevenson, S. 2003. A general feature space for automatic verb classification. Proceedings</note>
<address confidence="0.702755">EACL, Budapest. Levin, B. 1993. English Verb Classes and Alterna-tions:</address>
<affiliation confidence="0.693086">A Preliminary Investigation. Chicago, IL: University of Chicago Press.</affiliation>
<address confidence="0.729921">Lin, D. 1998. Automatic Retrieval and Clustering of</address>
<note confidence="0.958445666666667">Similar Words. COLING-ACL, Montreal. Linguistic Data Consortium. 2003. English Gigaword. Catalog number LDC2003T05. Available from LDC at http://www.ldc.upenn.edu. Moschitti, A., Pighin, D. and Basili, R. 2006. Semantic Role Labeling via Tree Kernel joint inference. Proceedings of CoNLL, New York. Palmer, M., Gildea, D., and Kingsbury, P. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics 31(1):71-106. Pedersen, T., Patwardhan, S. and Michelizzi, J. 2004. WordNet::Similarity - Measuring the Relatedness of Concepts. Proceedings NAACL-04, Boston, MA. Pradhan, S., Ward, W., Hacioglu, K., Martin, J., and Jurafsky, D. 2005. Semantic role labeling using different syntactic views. Proceedings ACL-2005, Ann Arbor, MI. Reynar, J. and Ratnaparkhi, A. 1997. A Maximum Entropy Approach to Identifying Sentence Boundaries. Proceedings of ANLP, Washington, D.C. 199</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Baker</author>
<author>C Fillmore</author>
<author>J Lowe</author>
</authors>
<title>The Berkeley FrameNet Project,</title>
<date>1998</date>
<booktitle>In Proceedings of COLINGACL,</booktitle>
<location>Montreal.</location>
<contexts>
<context position="29397" citStr="Baker et al., 1998" startWordPosition="4786" endWordPosition="4789">ms. Parse tree paths features can be used to achieve high precision in semantic role labeling, but much of this precision may be specific to individual verbs. By generalizing parse tree path features only across syntactically similar verbs, we have shown that the drop in precision can be limited to roughly 10%. The approach that we describe in this paper is not dependent on the use of PropBank rolesets; any large corpus of semantic role annotations could be 198 generalized in this manner. In particular, our approach would be applicable to corpora with framespecific role labels, e.g. FrameNet (Baker et al., 1998). Likewise, our approach to generalizing parse tree path feature across syntactically similar verbs may improve the performance of automated semantic role labeling systems based on FrameNet data. Our work suggests that feature generalization based on verb-similarity may compliment approaches to generalization based on role-similarity (Gildea and Jurafsky, 2002; Baldewein et al., 2004). There are a number of improvements that could be made to the approach described in this paper. Enhancements to the simple semantic role labeling algorithm would improve the alignment of arguments across rolesets</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Baker, C., Fillmore, C., and Lowe, J. 1998. The Berkeley FrameNet Project, In Proceedings of COLINGACL, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Baldewein</author>
<author>K Erk</author>
<author>S Pado</author>
<author>D Prescher</author>
</authors>
<title>Semantic role labeling with similarity-based generalization using EM-based clustering.</title>
<date>2004</date>
<booktitle>Proceedings of Senseval-3,</booktitle>
<location>Barcelona.</location>
<contexts>
<context position="29784" citStr="Baldewein et al., 2004" startWordPosition="4839" endWordPosition="4842">e of PropBank rolesets; any large corpus of semantic role annotations could be 198 generalized in this manner. In particular, our approach would be applicable to corpora with framespecific role labels, e.g. FrameNet (Baker et al., 1998). Likewise, our approach to generalizing parse tree path feature across syntactically similar verbs may improve the performance of automated semantic role labeling systems based on FrameNet data. Our work suggests that feature generalization based on verb-similarity may compliment approaches to generalization based on role-similarity (Gildea and Jurafsky, 2002; Baldewein et al., 2004). There are a number of improvements that could be made to the approach described in this paper. Enhancements to the simple semantic role labeling algorithm would improve the alignment of arguments across rolesets, which would help align rolesets with greater syntactic similarity, as well as improve the performance obtained using the surrogate training data in assigning semantic roles. This research raises many questions about the relationship between syntactic context and verb semantics. An important area for future research will be to explore the correlation between our distance metric for s</context>
</contexts>
<marker>Baldewein, Erk, Pado, Prescher, 2004</marker>
<rawString>Baldewein, U., Erk, K., Pado, S., and Prescher, D. 2004. Semantic role labeling with similarity-based generalization using EM-based clustering. Proceedings of Senseval-3, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser,</title>
<date>2000</date>
<booktitle>Proceedings NAACL-ANLP,</booktitle>
<location>Seattle.</location>
<contexts>
<context position="9292" citStr="Charniak, 2000" startWordPosition="1504" endWordPosition="1505">or the verb namedrop (4 sentences). The second step was to automatically generate syntactic parse trees for the GigaWord sentences found for each verb. It was our original intention to parse all of the found sentences, but we found that the slow speed of contemporary syntactic parsers made this impractical. Instead, we focused our efforts on the first 100 sentences found for each of the 3,257 verbs with 100 or fewer tokens: a total of 324,461 sentences (average of 99.6 per verb). For this task we utilized the August 2005 release of the Charniak parser with the default speed/accuracy settings (Charniak, 2000), which required roughly 360 hours of processor time on a 2.5 GHz PowerPC G5. The third step was to characterize the syntactic context of the verbs based on where they appeared within the parse trees. For this purpose, we utilized parse tree paths as a means of converting tree structures into a flat, feature-vector representation. For each sentence, we identified all possible parse tree paths that begin from the verb inflection and terminate at a constituent that does not include the verb inflection. For example, the syntactic context of the verb in Figure 1 can be described by the following f</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Charniak, E. 2000. A maximum-entropy-inspired parser, Proceedings NAACL-ANLP, Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Courtois</author>
</authors>
<title>Dictionnaires électroniques DELAF anglais et français. In</title>
<date>2004</date>
<booktitle>Syntax, Lexis and LexiconGrammar: Papers in Honour of Maurice Gross.</booktitle>
<editor>C. Leclère, E. Laporte, M. Piot and M. Silberztein (eds.)</editor>
<location>Amsterdam: John Benjamins.</location>
<contexts>
<context position="8106" citStr="Courtois, 2004" startWordPosition="1310" endWordPosition="1311">and F-score for each of the 3,257 verbs in the PropBank corfor rolesets according to the number of training pus. In the first step, we identified each of the senexamples of the roleset in the PropBank corpus. tences in the prepared GigaWord corpus that An additional curve in Figure 2 plots the percent- contained any inflection of the given verb. To age of these PropBank rolesets that have the given automatically identify all verb inflections, we utilamount of training data or more. For example, F- ized the English DELA electronic dictionary scores above 0.7 are first reached with 62 training (Courtois, 2004), which contained all but 21 of the examples, but only 8% of PropBank rolesets have PropBank verbs (for which we provided the inflecthis much training data available. tions ourselves), with old-English verb inflections removed. We extracted GigaWord sentences containing these inflections by using the GNU grep program and a template regular expression for each inflection list. The results of these searches were collected in 3,257 files (one for each verb). The largest of these files was for inflections of the verb say (15.9 million sentences), and the smallest was for the verb namedrop (4 sente</context>
</contexts>
<marker>Courtois, 2004</marker>
<rawString>Courtois, B. 2004. Dictionnaires électroniques DELAF anglais et français. In C. Leclère, E. Laporte, M. Piot and M. Silberztein (eds.) Syntax, Lexis and LexiconGrammar: Papers in Honour of Maurice Gross. Amsterdam: John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic Labeling of Semantic Roles.</title>
<date>2002</date>
<journal>Computational Linguistics</journal>
<volume>28</volume>
<pages>245--288</pages>
<contexts>
<context position="3686" citStr="Gildea and Jurafsky (2002)" startWordPosition="573" endWordPosition="576">d a simple automated semantic role labeling algorithm based on the frequency of parse-tree paths, and then compared its performance when using real and surrogate training data from PropBank. 192 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 192–199, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics 2 Parse Tree Paths A key concept in understanding our approach to both automated semantic role annotation and generalization is the notion of a parse tree path. Parse tree paths were used for semantic role labeling by Gildea and Jurafsky (2002) as descriptive features of the syntactic relationship between predicates and their arguments in the parse tree of a sentence. Predicates are typically assumed to be specific target words (verbs), and arguments are assumed to be spans of words in the sentence that are dominated by nodes in the parse tree. A parse tree path can be described as a sequence of transitions up from the target word then down to the node that dominates the argument span (e.g. Figure 1). Figure 1: An example parse tree path from the predicate ate to the argument NP He, represented as TVBTVPTSINP Parse tree paths are pa</context>
<context position="29759" citStr="Gildea and Jurafsky, 2002" startWordPosition="4835" endWordPosition="4838"> is not dependent on the use of PropBank rolesets; any large corpus of semantic role annotations could be 198 generalized in this manner. In particular, our approach would be applicable to corpora with framespecific role labels, e.g. FrameNet (Baker et al., 1998). Likewise, our approach to generalizing parse tree path feature across syntactically similar verbs may improve the performance of automated semantic role labeling systems based on FrameNet data. Our work suggests that feature generalization based on verb-similarity may compliment approaches to generalization based on role-similarity (Gildea and Jurafsky, 2002; Baldewein et al., 2004). There are a number of improvements that could be made to the approach described in this paper. Enhancements to the simple semantic role labeling algorithm would improve the alignment of arguments across rolesets, which would help align rolesets with greater syntactic similarity, as well as improve the performance obtained using the surrogate training data in assigning semantic roles. This research raises many questions about the relationship between syntactic context and verb semantics. An important area for future research will be to explore the correlation between </context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Gildea, D. and Jurafsky, D. 2002. Automatic Labeling of Semantic Roles. Computational Linguistics 28:3, 245-288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Joanis</author>
<author>S Stevenson</author>
</authors>
<title>A general feature space for automatic verb classification.</title>
<date>2003</date>
<booktitle>Proceedings EACL,</booktitle>
<location>Budapest.</location>
<contexts>
<context position="14240" citStr="Joanis and Stevenson, 2003" startWordPosition="2296" endWordPosition="2299"> 19 of the verbs in this list: 1. plunge, tumble, dive, jump, fall, fell, dip 2. assail, chide, lambaste 3. buffet, embroil, lock, superimpose, whipsaw, pluck, whisk, mar, ensconce The appearance of these sets suggests that our method of computing syntactic similarity could be used to identify distinct clusters of verbs that behave in very similar ways. In future work, it would be particularly interesting to compare empiricallyderived verb clusters to verb classes derived from theoretical considerations (Levin, 1993), and to the automated verb classification techniques that use these classes (Joanis and Stevenson, 2003). A third observation of Table 2 is that the verb pairs with the highest syntactic similarity are often synonyms, e.g. the cluster of assail, chide, and lambaste. As a striking example, the 14 most syntactically similar verbs to believe (in order) are think, guess, hope, feel, wonder, theorize, fear, reckon, contend, suppose, understand, know, doubt, and suggest – all mental action verbs. This observation further supports the distributional hypothesis of word similarity and corresponding technologies for identifying synonyms by similarity of lexical-syntactic context (Lin, 1998). Verb pairs (i</context>
</contexts>
<marker>Joanis, Stevenson, 2003</marker>
<rawString>Joanis, E. and Stevenson, S. 2003. A general feature space for automatic verb classification. Proceedings EACL, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>English Verb Classes and Alterna-tions: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press.</publisher>
<location>Chicago, IL:</location>
<contexts>
<context position="14135" citStr="Levin, 1993" startWordPosition="2283" endWordPosition="2284">ing sets of verbs that all have high syntactic similarity. Three of these sets account for 19 of the verbs in this list: 1. plunge, tumble, dive, jump, fall, fell, dip 2. assail, chide, lambaste 3. buffet, embroil, lock, superimpose, whipsaw, pluck, whisk, mar, ensconce The appearance of these sets suggests that our method of computing syntactic similarity could be used to identify distinct clusters of verbs that behave in very similar ways. In future work, it would be particularly interesting to compare empiricallyderived verb clusters to verb classes derived from theoretical considerations (Levin, 1993), and to the automated verb classification techniques that use these classes (Joanis and Stevenson, 2003). A third observation of Table 2 is that the verb pairs with the highest syntactic similarity are often synonyms, e.g. the cluster of assail, chide, and lambaste. As a striking example, the 14 most syntactically similar verbs to believe (in order) are think, guess, hope, feel, wonder, theorize, fear, reckon, contend, suppose, understand, know, doubt, and suggest – all mental action verbs. This observation further supports the distributional hypothesis of word similarity and corresponding te</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Levin, B. 1993. English Verb Classes and Alterna-tions: A Preliminary Investigation. Chicago, IL: University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<location>COLING-ACL, Montreal.</location>
<contexts>
<context position="10924" citStr="Lin, 1998" startWordPosition="1765" endWordPosition="1766">to calculate the syntactic similarity between verbs. The expectation here is that verbs that appear in syntactically similar contexts are going to behave similarly in the way that they relate to their arguments. In this section we describe a fully automated approach to calculating the syntactic similarity between verbs. Our approach is strictly empirical; the similarity of verbs is determined by examining the syntactic contexts in which they appear in a large text corpus. Our approach is analogous to previous work in extracting collocations from large text corpora using syntactic information (Lin, 1998). In our work, we utilized the GigaWord corpus of English newswire text (Linguistic Data Consortium, 2003), consisting of nearly 12 gigabytes of textual data. To prepare this corpus for analysis, we extracted the body text from each of the 4.1 million entries in the corpus and applied a maximum-entropy algorithm to identify sentence boundaries (Reynar and Ratnaparkhi, 1997). 194 into a feature vector representation. Parse tree paths where the first node was not a Treebank partof-speech tag for a verb were discarded, effectively filtering the non-verb homonyms of the set of inflections. The res</context>
<context position="14825" citStr="Lin, 1998" startWordPosition="2386" endWordPosition="2387">anis and Stevenson, 2003). A third observation of Table 2 is that the verb pairs with the highest syntactic similarity are often synonyms, e.g. the cluster of assail, chide, and lambaste. As a striking example, the 14 most syntactically similar verbs to believe (in order) are think, guess, hope, feel, wonder, theorize, fear, reckon, contend, suppose, understand, know, doubt, and suggest – all mental action verbs. This observation further supports the distributional hypothesis of word similarity and corresponding technologies for identifying synonyms by similarity of lexical-syntactic context (Lin, 1998). Verb pairs (instances) Cosine bind (83) bound (95) 0.950 plunge (94) tumble (87) 0.888 dive (36) plunge (94) 0.867 dive (36) tumble (87) 0.866 jump (79) tumble (87) 0.865 fall (84) fell (102) 0.859 intersperse (99) perch (81) 0.859 assail (100) chide (98) 0.859 dip (81) fell (102) 0.858 buffet (72) embroil (100) 0.856 embroil (100) lock (73) 0.856 embroil (100) superimpose (100) 0.856 fell (102) jump (79) 0.855 fell (102) tumble (87) 0.855 embroil (100) whipsaw (63) 0.850 pluck (100) whisk (99) 0.849 acquit (100) hospitalize (99) 0.849 disincline (70) obligate (94) 0.848 jump (79) plunge (94</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. 1998. Automatic Retrieval and Clustering of Similar Words. COLING-ACL, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linguistic Data Consortium</author>
</authors>
<title>English Gigaword. Catalog number LDC2003T05. Available from LDC at http://www.ldc.upenn.edu.</title>
<date>2003</date>
<contexts>
<context position="11030" citStr="Consortium, 2003" startWordPosition="1781" endWordPosition="1782"> in syntactically similar contexts are going to behave similarly in the way that they relate to their arguments. In this section we describe a fully automated approach to calculating the syntactic similarity between verbs. Our approach is strictly empirical; the similarity of verbs is determined by examining the syntactic contexts in which they appear in a large text corpus. Our approach is analogous to previous work in extracting collocations from large text corpora using syntactic information (Lin, 1998). In our work, we utilized the GigaWord corpus of English newswire text (Linguistic Data Consortium, 2003), consisting of nearly 12 gigabytes of textual data. To prepare this corpus for analysis, we extracted the body text from each of the 4.1 million entries in the corpus and applied a maximum-entropy algorithm to identify sentence boundaries (Reynar and Ratnaparkhi, 1997). 194 into a feature vector representation. Parse tree paths where the first node was not a Treebank partof-speech tag for a verb were discarded, effectively filtering the non-verb homonyms of the set of inflections. The resulting feature vectors were normalized by dividing the values of each feature by the number of verb instan</context>
</contexts>
<marker>Consortium, 2003</marker>
<rawString>Linguistic Data Consortium. 2003. English Gigaword. Catalog number LDC2003T05. Available from LDC at http://www.ldc.upenn.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>D Pighin</author>
<author>R Basili</author>
</authors>
<title>Semantic Role Labeling via Tree Kernel joint inference.</title>
<date>2006</date>
<booktitle>Proceedings of CoNLL,</booktitle>
<location>New York.</location>
<contexts>
<context position="1654" citStr="Moschitti et al., 2006" startWordPosition="246" endWordPosition="249">in their corresponding rolesets, and the use of their corresponding annotations in PropBank as surrogate training data. 1 Generalizing Semantic Role Annotations A recent release of the PropBank (Palmer et al., 2005) corpus of semantic role annotations of Treebank parses contained 112,917 labeled instances of 4,250 rolesets corresponding to 3,257 verbs, as illustrated by this example for the verb buy. [arg0 Chuck] [buy.01 bought] [arg1 a car] [arg2 from Jerry] [arg3 for $1000]. Annotations similar to these have been used to create automated semantic role labeling systems (Pradhan et al., 2005; Moschitti et al., 2006) for use in natural language processing applications that require only shallow semantic parsing. As with all machine-learning approaches, the performance of these systems is heavily dependent on the availability of adequate amounts of training data. However, the number of annotated instances in PropBank varies greatly from verb to verb; there are 617 annotations for the want roleset, only 7 for desire, and 0 for any sense of the verb yearn. Do we need to keep annotating larger and larger corpora in order to generate accurate semantic labeling systems for verbs like yearn? A better approach may</context>
<context position="28623" citStr="Moschitti et al., 2006" startWordPosition="4658" endWordPosition="4661"> be obtained by applying this technique to a novel roleset with one fully-annotated training example, e.g. for the verb yearn. Using the first 120 surrogate training examples and our simple semantic role labeling approach, we would expect F-scores that are above 0.5, and that using the first 700 would yield F-scores around 0.6. 7 Discussion The overall performance of our semantic role labeling approach is not competitive with leading contemporary systems, which typically employ support vector machine learning algorithms with syntactic features (Pradhan et al., 2005) or syntactic tree kernels (Moschitti et al., 2006). However, our work highlights a number of characteristics of the semantic role labeling task that will be helpful in improving performance in future systems. Parse tree paths features can be used to achieve high precision in semantic role labeling, but much of this precision may be specific to individual verbs. By generalizing parse tree path features only across syntactically similar verbs, we have shown that the drop in precision can be limited to roughly 10%. The approach that we describe in this paper is not dependent on the use of PropBank rolesets; any large corpus of semantic role anno</context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2006</marker>
<rawString>Moschitti, A., Pighin, D. and Basili, R. 2006. Semantic Role Labeling via Tree Kernel joint inference. Proceedings of CoNLL, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The Proposition Bank: An Annotated Corpus of Semantic Roles.</title>
<date>2005</date>
<journal>Computational Linguistics</journal>
<pages>31--1</pages>
<contexts>
<context position="1246" citStr="Palmer et al., 2005" startWordPosition="181" endWordPosition="184"> of instances in these corpora, and only a fraction of English verbs have even a single annotation. In this paper, we describe an approach for dealing with this sparse data problem, enabling accurate semantic role labeling for novel verbs (rolesets) with only a single training example. Our approach involves the identification of syntactically similar verbs found in PropBank, the alignment of arguments in their corresponding rolesets, and the use of their corresponding annotations in PropBank as surrogate training data. 1 Generalizing Semantic Role Annotations A recent release of the PropBank (Palmer et al., 2005) corpus of semantic role annotations of Treebank parses contained 112,917 labeled instances of 4,250 rolesets corresponding to 3,257 verbs, as illustrated by this example for the verb buy. [arg0 Chuck] [buy.01 bought] [arg1 a car] [arg2 from Jerry] [arg3 for $1000]. Annotations similar to these have been used to create automated semantic role labeling systems (Pradhan et al., 2005; Moschitti et al., 2006) for use in natural language processing applications that require only shallow semantic parsing. As with all machine-learning approaches, the performance of these systems is heavily dependent </context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Palmer, M., Gildea, D., and Kingsbury, P. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics 31(1):71-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>S Patwardhan</author>
<author>J Michelizzi</author>
</authors>
<title>WordNet::Similarity - Measuring the Relatedness of Concepts.</title>
<date>2004</date>
<booktitle>Proceedings NAACL-04,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="30485" citStr="Pedersen, et al., 2004" startWordPosition="4945" endWordPosition="4948">bed in this paper. Enhancements to the simple semantic role labeling algorithm would improve the alignment of arguments across rolesets, which would help align rolesets with greater syntactic similarity, as well as improve the performance obtained using the surrogate training data in assigning semantic roles. This research raises many questions about the relationship between syntactic context and verb semantics. An important area for future research will be to explore the correlation between our distance metric for syntactic similarity and various quantitative measures of semantic similarity (Pedersen, et al., 2004). Particularly interesting would be to explore whether different senses of a given verb exhibited markedly different profiles of syntactic context. A strong syntactic/semantic correlation would suggest that further gains in the use of surrogate annotation data could be gained if syntactic similarity was computed between rolesets rather than their verbs. However, this would first require accurate word-sense disambiguation both for the test sentences as well as for the parsed corpora used to calculate parse tree path frequencies. Alternatively, parse tree path profiles associated with rolesets m</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Pedersen, T., Patwardhan, S. and Michelizzi, J. 2004. WordNet::Similarity - Measuring the Relatedness of Concepts. Proceedings NAACL-04, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pradhan</author>
<author>W Ward</author>
<author>K Hacioglu</author>
<author>J Martin</author>
<author>D Jurafsky</author>
</authors>
<title>Semantic role labeling using different syntactic views.</title>
<date>2005</date>
<booktitle>Proceedings ACL-2005,</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="1629" citStr="Pradhan et al., 2005" startWordPosition="242" endWordPosition="245">lignment of arguments in their corresponding rolesets, and the use of their corresponding annotations in PropBank as surrogate training data. 1 Generalizing Semantic Role Annotations A recent release of the PropBank (Palmer et al., 2005) corpus of semantic role annotations of Treebank parses contained 112,917 labeled instances of 4,250 rolesets corresponding to 3,257 verbs, as illustrated by this example for the verb buy. [arg0 Chuck] [buy.01 bought] [arg1 a car] [arg2 from Jerry] [arg3 for $1000]. Annotations similar to these have been used to create automated semantic role labeling systems (Pradhan et al., 2005; Moschitti et al., 2006) for use in natural language processing applications that require only shallow semantic parsing. As with all machine-learning approaches, the performance of these systems is heavily dependent on the availability of adequate amounts of training data. However, the number of annotated instances in PropBank varies greatly from verb to verb; there are 617 annotations for the want roleset, only 7 for desire, and 0 for any sense of the verb yearn. Do we need to keep annotating larger and larger corpora in order to generate accurate semantic labeling systems for verbs like yea</context>
<context position="28572" citStr="Pradhan et al., 2005" startWordPosition="4649" endWordPosition="4652">results predict the labeling performance that can be obtained by applying this technique to a novel roleset with one fully-annotated training example, e.g. for the verb yearn. Using the first 120 surrogate training examples and our simple semantic role labeling approach, we would expect F-scores that are above 0.5, and that using the first 700 would yield F-scores around 0.6. 7 Discussion The overall performance of our semantic role labeling approach is not competitive with leading contemporary systems, which typically employ support vector machine learning algorithms with syntactic features (Pradhan et al., 2005) or syntactic tree kernels (Moschitti et al., 2006). However, our work highlights a number of characteristics of the semantic role labeling task that will be helpful in improving performance in future systems. Parse tree paths features can be used to achieve high precision in semantic role labeling, but much of this precision may be specific to individual verbs. By generalizing parse tree path features only across syntactically similar verbs, we have shown that the drop in precision can be limited to roughly 10%. The approach that we describe in this paper is not dependent on the use of PropBa</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2005</marker>
<rawString>Pradhan, S., Ward, W., Hacioglu, K., Martin, J., and Jurafsky, D. 2005. Semantic role labeling using different syntactic views. Proceedings ACL-2005, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Reynar</author>
<author>A Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Approach to Identifying Sentence Boundaries.</title>
<date>1997</date>
<booktitle>Proceedings of ANLP,</booktitle>
<location>Washington, D.C.</location>
<contexts>
<context position="11300" citStr="Reynar and Ratnaparkhi, 1997" startWordPosition="1822" endWordPosition="1825"> the similarity of verbs is determined by examining the syntactic contexts in which they appear in a large text corpus. Our approach is analogous to previous work in extracting collocations from large text corpora using syntactic information (Lin, 1998). In our work, we utilized the GigaWord corpus of English newswire text (Linguistic Data Consortium, 2003), consisting of nearly 12 gigabytes of textual data. To prepare this corpus for analysis, we extracted the body text from each of the 4.1 million entries in the corpus and applied a maximum-entropy algorithm to identify sentence boundaries (Reynar and Ratnaparkhi, 1997). 194 into a feature vector representation. Parse tree paths where the first node was not a Treebank partof-speech tag for a verb were discarded, effectively filtering the non-verb homonyms of the set of inflections. The resulting feature vectors were normalized by dividing the values of each feature by the number of verb instances used to generate the parse tree paths; the value of each feature indicates the proportion of observed inflections in which the parse tree path is possible. As a representative example, 95 verb forms of abandon were found in the first 100 GigaWord sentences containin</context>
</contexts>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>Reynar, J. and Ratnaparkhi, A. 1997. A Maximum Entropy Approach to Identifying Sentence Boundaries. Proceedings of ANLP, Washington, D.C.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>