<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.891064">
Minibatch and Parallelization for Online Large Margin Structured Learning
</title>
<author confidence="0.998111">
Kai Zhao&apos; Liang Huang2,&apos;
</author>
<affiliation confidence="0.9776035">
&apos;Computer Science Program, Graduate Center 2Computer Science Dept, Queens College
City University of New York City University of New York
</affiliation>
<email confidence="0.99899">
kzhao@gc.cuny.edu huang@cs.qc.cuny.edu
</email>
<sectionHeader confidence="0.995046" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9966926">
Online learning algorithms such as perceptron
and MIRA have become popular for many
NLP tasks thanks to their simpler architec-
ture and faster convergence over batch learn-
ing methods. However, while batch learning
such as CRF is easily parallelizable, online
learning is much harder to parallelize: previ-
ous efforts often witness a decrease in the con-
verged accuracy, and the speedup is typically
very small (-3) even with many (10+) pro-
cessors. We instead present a much simpler
architecture based on “mini-batches”, which
is trivially parallelizable. We show that, un-
like previous methods, minibatch learning (in
serial mode) actually improves the converged
accuracy for both perceptron and MIRA learn-
ing, and when combined with simple paral-
lelization, minibatch leads to very significant
speedups (up to 9x on 12 processors) on state-
of-the-art parsing and tagging systems.
</bodyText>
<sectionHeader confidence="0.99878" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999842576923077">
Online structured learning algorithms such as the
structured perceptron (Collins, 2002) and k-best
MIRA (McDonald et al., 2005) have become more
and more popular for many NLP tasks such as de-
pendency parsing and part-of-speech tagging. This
is because, compared to their batch learning counter-
parts, online learning methods offer faster conver-
gence rates and better scalability to large datasets,
while using much less memory and a much simpler
architecture which only needs 1-best or k-best de-
coding. However, online learning for NLP typically
involves expensive inference on each example for 10
or more passes over millions of examples, which of-
ten makes training too slow in practice; for example
systems such as the popular (2nd-order) MST parser
(McDonald and Pereira, 2006) usually require the
order of days to train on the Treebank on a com-
modity machine (McDonald et al., 2010).
There are mainly two ways to address this scala-
bility problem. On one hand, researchers have been
developing modified learning algorithms that allow
inexact search (Collins and Roark, 2004; Huang et
al., 2012). However, the learner still needs to loop
over the whole training data (on the order of mil-
lions of sentences) many times. For example the
best-performing method in Huang et al. (2012) still
requires 5-6 hours to train a very fast parser.
On the other hand, with the increasing popularity
of multicore and cluster computers, there is a grow-
ing interest in speeding up training via paralleliza-
tion. While batch learning such as CRF (Lafferty
et al., 2001) is often trivially parallelizable (Chu et
al., 2007) since each update is a batch-aggregate of
the update from each (independent) example, online
learning is much harder to parallelize due to the de-
pendency between examples, i.e., the update on the
first example should in principle influence the de-
coding of all remaining examples. Thus if we de-
code and update the first and the 1000th examples
in parallel, we lose their interactions which is one
of the reasons for online learners’ fast convergence.
This explains why previous work such as the itera-
tive parameter mixing (IPM) method of McDonald
et al. (2010) witnesses a decrease in the accuracies
of parallelly-learned models, and the speedup is typ-
ically very small (about 3 in their experiments) even
with 10+ processors.
We instead explore the idea of “minibatch” for on-
line large-margin structured learning such as percep-
tron and MIRA. We argue that minibatch is advan-
tageous in both serial and parallel settings.
First, for minibatch perceptron in the serial set-
</bodyText>
<page confidence="0.972589">
370
</page>
<note confidence="0.470803">
Proceedings of NAACL-HLT 2013, pages 370–379,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.99985646875">
ting, our intuition is that, although decoding is done
independently within one minibatch, updates are
done by averaging update vectors in batch, provid-
ing a “mixing effect” similar to “averaged parame-
ters” of Collins (2002) which is also found in IPM
(McDonald et al., 2010), and online EM (Liang and
Klein, 2009).
Secondly, minibatch MIRA in the serial setting
has an advantage that, different from previous meth-
ods such as SGD which simply sum up the up-
dates from all examples in a minibatch, a minibatch
MIRA update tries to simultaneously satisfy an ag-
gregated set of constraints that are collected from
multiple examples in the minibatch. Thus each mini-
batch MIRA update involves an optimization over
many more constraints than in pure online MIRA,
which could potentially lead to a better margin. In
other words we can view MIRA as an online version
or stepwise approximation of SVM, and minibatch
MIRA can be seen as a better approximation as well
as a middleground between pure MIRA and SVM.1
More interestingly, the minibatch architecture is
trivially parallelizable since the examples within
each minibatch could be decoded in parallel on mul-
tiple processors (while the update is still done in se-
rial). This is known as “synchronous minibatch”
and has been explored by many researchers (Gim-
pel et al., 2010; Finkel et al., 2008), but all previ-
ous works focus on probabilistic models along with
SGD or EM learning methods while our work is the
first effort on large-margin methods.
We make the following contributions:
</bodyText>
<listItem confidence="0.954614090909091">
• Theoretically, we present a serial minibatch
framework (Section 3) for online large-margin
learning and prove the convergence theorems
for minibatch perceptron and minibatch MIRA.
• Empirically, we show that serial minibatch
could speed up convergence and improve the
converged accuracy for both MIRA and percep-
tron on state-of-the-art dependency parsing and
part-of-speech tagging systems.
• In addition, when combined with simple (syn-
chronous) parallelization, minibatch MIRA
</listItem>
<footnote confidence="0.994105">
1This is similar to Pegasos (Shalev-Shwartz et al., 2007) that
applies subgradient descent over a minibatch. Pegasos becomes
pure online when the minibatch size is 1.
</footnote>
<construct confidence="0.515175">
Algorithm 1 Generic Online Learning.
</construct>
<bodyText confidence="0.8672865">
Input: data D = {(x(t), y(t))}t1 and feature map Φ
Output: weight vector w
</bodyText>
<listItem confidence="0.959193333333333">
1: repeat
2: for each example (x, y) in D do
3: C — FINDCONSTRAINTS(x, y, w) &gt; decoding
4: if C =� 0 then UPDATE(w, C)
5: until converged
leads to very significant speedups (up to 9x on
12 processors) that are much higher than that of
IPM (McDonald et al., 2010) on state-of-the-art
parsing and tagging systems.
</listItem>
<sectionHeader confidence="0.913436" genericHeader="method">
2 Online Learning: Perceptron and MIRA
</sectionHeader>
<bodyText confidence="0.9990544">
We first present a unified framework for online
large-margin learning, where perceptron and MIRA
are two special cases. Shown in Algorithm 1, the
online learner considers each input example (x, y)
sequentially and performs two steps:
</bodyText>
<listItem confidence="0.993148">
1. find the set C of violating constraints, and
2. update the weight vector w according to C.
</listItem>
<bodyText confidence="0.989484434782609">
Here a triple (x, y, z) is said to be a “violating con-
straint” with respect to model w if the incorrect la-
bel z scores higher than (or equal to) the correct
label y in w, i.e., w · 0Φ((x, y, z)) G 0, where
0Φ((x, y, z)) is a short-hand notation for the up-
date vector Φ(x, y) — Φ(x, z) and Φ is the feature
map (see Huang et al. (2012) for details). The sub-
routines FINDCONSTRAINTS and UPDATE are anal-
ogous to “APIs”, to be specified by specific instances
of this online learning framework. For example, the
structured perceptron algorithm of Collins (2002)
is implemented in Algorithm 2 where FINDCON-
STRAINTS returns a singleton constraint if the 1-best
decoding result z (the highest scoring label accord-
ing to the current model) is different from the true
label y. Note that in the UPDATE function, C is al-
ways a singleton constraint for the perceptron, but
we make it more general (as a set) to handle the
batch update in the minibatch version in Section 3.
On the other hand, Algorith 3 presents the k-best
MIRA Algorithm of McDonald et al. (2005) which
generalizes multiclass MIRA (Crammer and Singer,
2003) for structured prediction. The decoder now
</bodyText>
<page confidence="0.996697">
371
</page>
<bodyText confidence="0.48183">
Algorithm 2 Perceptron (Collins, 2002).
</bodyText>
<listItem confidence="0.938421666666667">
1: function FINDCONSTRAINTS(x, y, w)
2: z ← argmaxsEY(x) w · Φ(x, s) . decoding
3: if z =6 y then return {hx, y, zi}
4: else return ∅
5: procedure UPDATE(w, C)
6: w ← w + |c |EcEC ΔΦ(c) . (batch) update
</listItem>
<bodyText confidence="0.981992">
Algorithm 3 k-best MIRA (McDonald et al., 2005).
finds the k-best solutions Z first, and returns a set
of violating constraints in Z, The update in MIRA
is more interesting: it searches for the new model
w&apos; with minimum change from the current model
w so that w&apos; corrects each violating constraint by
a margin at least as large as the loss `(y, z) of the
incorrect label z.
Although not mentioned in the pseudocode, we
also employ “averaged parameters” (Collins, 2002)
for both perceptron and MIRA in all experiments.
</bodyText>
<sectionHeader confidence="0.98533" genericHeader="method">
3 Serial Minibatch
</sectionHeader>
<bodyText confidence="0.999796384615385">
The idea of serial minibatch learning is extremely
simple: divide the data into dn/me minibatches
of size m, and do batch updates after decoding
each minibatch (see Algorithm 4). The FIND-
CONSTRAINTS and UPDATE subroutines remain un-
changed for both perceptron and MIRA, although
it is important to note that a perceptron batch up-
date uses the average of update vectors, not the sum,
which simplifies the proof. This architecture is of-
ten called “synchronous minibatch” in the literature
(Gimpel et al., 2010; Liang and Klein, 2009; Finkel
et al., 2008). It could be viewed as a middleground
between pure online learning and batch learning.
</bodyText>
<subsectionHeader confidence="0.999982">
3.1 Convergence of Minibatch Perceptron
</subsectionHeader>
<bodyText confidence="0.998441">
We denote C(D) to be the set of all possible violat-
ing constraints in data D (cf. Huang et al. (2012)):
</bodyText>
<equation confidence="0.765309">
C(D) = {hx, y, zi  |(x, y) ∈ D, z ∈ Y(x) − {y}}.
</equation>
<bodyText confidence="0.926392333333333">
Algorithm 4 Serial Minibatch Online Learning.
Input: data D, feature map Φ, and minibatch size m
Output: weight vector w
</bodyText>
<listItem confidence="0.988872666666667">
1: Split D into dn/me minibatches D1 ... Dfn/ml
2: repeat
3: for i ← 1... dn/me do . for each minibatch
4: C ← ∪(x y)ED,,FINDCONSTRAINTS(x, y, w)
5: if C =6 ∅ then UPDATE(w, C) . batch update
6: until converged
</listItem>
<bodyText confidence="0.778867533333333">
A training set D is separable by feature map Φ
with margin δ &gt; 0 if there exists a unit oracle vec-
tor u with kuk = 1 such that u · ΔΦ(hx, y, zi) ≥ δ,
for all hx, y, zi ∈ C(D). Furthermore, let radius
R ≥ kΔΦ(hx, y, zi)k for all hx, y, zi ∈ C(D).
Theorem 1. For a separable dataset D with margin
δ and radius R, the minibatch perceptron algorithm
(Algorithms 4 and 2) will terminate after t minibatch
updates where t ≤ R2/δ2.
Proof. Let wt be the weight vector before the tth
update; w0 = 0. Suppose the tth update happens
on the constraint set Ct = {c1, c2,... , ca} where
a = |Ct|, and each ci = hxi, yi, zii. We convert
them to the set of update vectors vi = ΔΦ(ci) =
ΔΦ(hxi, yi, zii) for all i. We know that:
</bodyText>
<listItem confidence="0.984663">
1. u · vi ≥ δ (margin on unit oracle vector)
2. wt · vi ≤ 0 (violation: zi dominates yi)
3. kvik2 ≤ R2 (radius)
Now the update looks like
</listItem>
<equation confidence="0.9404625">
wt+1 = wt + 1 1 E
EC ΔΦ(c) = wt + i vi.
|Ct |cECt a
(1)
</equation>
<bodyText confidence="0.885624">
We will bound kwt+1k from two directions:
</bodyText>
<listItem confidence="0.619527">
1. Dot product both sides of the update equa-
tion (1) with the unit oracle vector u, we have
</listItem>
<figure confidence="0.588400642857143">
u · wt+1 = u · wt + 1 Ei u · vi
a
1≥ u · wt + a Ei δ (margin)
= u · wt + δ
≥ tδ (by induction)
1: function FINDCONSTRAINTS(x, y, w)
2: Z ← k-bestzEY(x)w · Φ(x, z)
3: Z ← {z ∈ Z  |z =6 y,w · ΔΦ(hx,y,zi) ≤ 0}
4: return {(hx, y, zi, `(y, z))  |z ∈ Z}
5: procedure UPDATE(w, C)
6: w ← argmin
w&apos;:V(c,`)EC, w&apos;·A4P(c)&gt;`
kw/ − wk2
(Ei = a)
</figure>
<page confidence="0.967301">
372
</page>
<listItem confidence="0.8743876">
Since for any two vectors a and b we have
kakkbk ≥ a·b, thus kukkwt+1k ≥ u·wt+1 ≥
tδ. As u is a unit vector, we have kwt+1k ≥ tδ.
2. On the other hand, take the norm of both sides
of Eq. (1):
</listItem>
<equation confidence="0.999327461538462">
kwt+1k2 = kwt + 1P i vik2
a
1
=kwtk2 +kP avik2 + 2 wt · Pi vi
i a
≤kwtk2 + kPi avik2 + 0 (violation)
1
kwt k2 + Pi a I I vi k2 (Jensen’s)
1
≤kwtk2 + P (radius)
i aR2
=kwtk2 + R2 (Pi = a)
≤tR2 (by induction)
</equation>
<bodyText confidence="0.681536">
Combining the two bounds, we have
</bodyText>
<equation confidence="0.976435">
t2δ2 ≤ kwt+1k2 ≤ tR2
</equation>
<bodyText confidence="0.9195768">
thus the number of minibatch updates t ≤ R2/δ2.
Note that this bound is identical to that of pure
online perceptron (Collins, 2002, Theorem 1) and is
irrelevant to minibatch size m. The use of Jensen’s
inequality is inspired by McDonald et al. (2010).
</bodyText>
<subsectionHeader confidence="0.999986">
3.2 Convergence of Minibatch MIRA
</subsectionHeader>
<bodyText confidence="0.9986485">
We also give a proof of convergence for MIRA with
relaxation.2 We present the optimization problem in
the UPDATE function of Algorithm 3 as a quadratic
program (QP) with slack variable ξ:
</bodyText>
<equation confidence="0.995214333333333">
wt+1 ← argmin
Wt+1 kwt+1 − wtk2 + ξ
s.t. wt+1 · vi ≥ `i −ξ, for all(ci, `i) ∈ Ct
</equation>
<bodyText confidence="0.9942615">
where vi = 04)(ci) is the update vector for con-
straint ci. Consider the Lagrangian:
</bodyText>
<equation confidence="0.852724666666667">
L =kwt+1 − wtk2 + ξ + X |Ct |ηi(`i − w&apos; · vi − ξ)
i=1
ηi ≥ 0, for 1 ≤ i ≤ |Ct|.
</equation>
<bodyText confidence="0.924871">
2Actually this relaxation is not necessary for the conver-
gence proof. We employ it here solely to make the proof shorter.
It is not used in the experiments either.
Set the partial derivatives to 0 with respect to w&apos; and
ξ we have:
</bodyText>
<equation confidence="0.999659666666667">
w&apos; = w + Pi ηivi (2)
P
i ηi = 1 (3)
</equation>
<bodyText confidence="0.9854136">
This result suggests that the weight change can al-
ways be represnted by a linear combination of the
update vectors (i.e. normal vectors of the constraint
hyperplanes), with the linear coefficencies sum to 1.
Theorem 2 (convergence of minibatch MIRA). For
a separable dataset D with margin δ and radius R,
the minibatch MIRA algorithm (Algorithm 4 and 3)
will make t updates where t ≤ R2/δ2.
Proof. 1. Dot product both sides of Equation 2
with unit oracle vector u:
</bodyText>
<equation confidence="0.9582875">
u · wt+1 = u · wt + Pi ηiu · vi
≥u · wt + Pi ηiδ (margin)
=u · wt + δ (Eq. 3)
=tδ (by induction)
</equation>
<listItem confidence="0.810948">
2. On the other hand
</listItem>
<equation confidence="0.99964075">
kwt+1k2 = kwt + Pi ηivik2
=kwtk2 + kPi ηivik2 + 2 wt · Pi ηivi
≤kwtk2 + kPi ηivik2 + 0 (violation)
≤kwtk2 + Pi ηiv2 (Jensen’s)
i
≤kwtk2 + Pi ηiR2 (radius)
=kwtk2 + R2 (Eq. 3)
≤tR2 (by induction)
</equation>
<bodyText confidence="0.557719">
From the two bounds we have:
</bodyText>
<equation confidence="0.967229">
t2δ2 ≤ kwt+1k2 ≤ tR2
</equation>
<bodyText confidence="0.9992125">
thus within at most t ≤ R2/δ2 minibatch up-
dates MIRA will converge.
</bodyText>
<sectionHeader confidence="0.987529" genericHeader="method">
4 Parallelized Minibatch
</sectionHeader>
<bodyText confidence="0.999987333333333">
The key insight into parallelization is that the calcu-
lation of constraints (i.e. decoding) for each exam-
ple within a minibatch is completely independent of
</bodyText>
<page confidence="0.992944">
373
</page>
<figure confidence="0.999657591836734">
⊕
update
update
update
update
2
3
4
1
update
update
update
update
5
8
6
7
update
update
update
update
10
12
11
9
update
update
update
update
13
14
15
16
3
5
7
1
4
6
8
2
⊕
update
9
11
13
15
10
12
14
16
⊕
update
12
3
9
1
14
15
4
6
update
update
⊕
⊕
13
16
5
8
10
11
2
7
update
update
15
16
2
1
update
update
10
3
4
9
update
update
13
14
5
6
update
update
12
11
8
7
(a) IPM (b) unbalanced (c) balanced (d) asynchronous
</figure>
<figureCaption confidence="0.997371428571429">
Figure 1: Comparison of various methods for parallelizing online learning (number of processors p = 4). (a) iterative
parameter mixing (McDonald et al., 2010). (b) unbalanced minibatch parallelization (minibatch size m = 8). (c)
minibatch parallelization after load-balancing (within each minibatch). (d) asynchronous minibatch parallelization
(Gimpel et al., 2010) (not implemented here). Each numbered box denotes the decoding of one example, and �
denotes an aggregate operation, i.e., the merging of constraints after each minibatch or the mixing of weights after
each iteration in IPM. Each gray shaded box denotes time wasted due to synchronization in (a)-(c) or blocking in (d).
Note that in (d) at most one update can happen concurrently, making it substantially harder to implement than (a)-(c).
</figureCaption>
<bodyText confidence="0.927204638297872">
other examples in the same batch. Thus we can eas-
ily distribute decoding for different examples in the
same minibatch to different processors.
Shown in Algorithm 5, for each minibatch Di, we
split Di into groups of equal size, and assign each
group to a processor to decode. After all processors
finish, we collect all constraints and do an update
based on the union of all constraints. Figure 1 (b) il-
lustrates minibatch parallelization, with comparison
to iterative parameter mixing (IPM) of McDonald et
al. (2010) (see Figure 1 (a)).
This synchronous parallelization framework
should provide significant speedups over the serial
mode. However, in each minibatch, inevitably,
some processors will end up waiting for others to
finish, especially when the lengths of sentences vary
substantially (see the shaded area in Figure 1 (b)).
To alleviate this problem, we propose “per-
minibatch load-balancing”, which rearranges the
sentences within each minibatch based on their
lengths (which correlate with their decoding times)
so that the total workload on each processor is bal-
anced (Figure 1c). It is important to note that this
shuffling does not affect learning at all thanks to the
independence of each example within a minibatch.
Basically, we put the shortest and longest sentences
into the first thread, the second shortest and second
longest into the second thread, etc. Although this is
not necessary optimal scheduling, it works well in
practice. As long as decoding time is linear in the
length of sentence (as in incremental parsing or tag-
ging), we expect a much smaller variance in process-
ing time on each processor in one minibatch, which
is confirmed in the experiments (see Figure 8).3
3In IPM, however, the waiting time is negligible, since the
workload on each processor is almost balanced, analogous to
a huge minibatch (Fig. 1a). Furthermore, shuffling does affect
learning here since each thread in IPM is a pure online learner.
So our IPM implementation does not use load-balancing.
Algorithm 5 Parallized Minibatch Online Learning.
Input: D, (b, minibatch size m, and # of processors p
Output: weight vector w
Split D into rn/m] minibatches D1 ... Dfes,/ml
Split each Di into m/p groups Di,1 ... Di,m/p
repeat
for i +— 1... rn/m] do &gt; for each minibatch
for j +— 1... m/p in parallel do
</bodyText>
<figure confidence="0.755437">
Cj +— U(y,y)ED;�� FINDCONSTRAINTS(x, y, w)
C +— UjCj &gt; in serial
if C =� 0 then UPDATE(w, C) &gt; in serial
until converged
</figure>
<page confidence="0.996387">
374
</page>
<sectionHeader confidence="0.998193" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999929909090909">
We conduct experiments on two typical structured
prediction problems: incremental dependency pars-
ing and part-of-speech tagging; both are done on
state-of-the-art baseline. We also compare our
parallelized minibatch algorithm with the iterative
parameter mixing (IPM) method of McDonald et
al. (2010). We perform our experiments on a
commodity 64-bit Dell Precision T7600 worksta-
tion with two 3.1GHz 8-core CPUs (16 processors
in total) and 64GB RAM. We use Python 2.7’s
multiprocessing module in all experiments.4
</bodyText>
<subsectionHeader confidence="0.89283">
5.1 Dependency Parsing with MIRA
</subsectionHeader>
<bodyText confidence="0.9999783">
We base our experiments on our dynamic program-
ming incremental dependency parser (Huang and
Sagae, 2010).5 Following Huang et al. (2012), we
use max-violation update and beam size b = 8. We
evaluate on the standard Penn Treebank (PTB) us-
ing the standard split: Sections 02-21 for training,
and Section 22 as the held-out set (which is indeed
the test-set in this setting, following McDonald et
al. (2010) and Gimpel et al. (2010)). We then ex-
tend it to employ 1-best MIRA learning. As stated
in Section 2, MIRA separates the gold label y from
the incorrect label z with a margin at least as large
as the loss e(y, z). Here in incremental dependency
parsing we define the loss function between a gold
tree y and an incorrect partial tree z as the number
of incorrect edges in z, plus the number of correct
edges in y which are already ruled out by z. This
MIRA extension results in slightly higher accuracy
of 92.36, which we will use as the pure online learn-
ing baseline in the comparisons below.
</bodyText>
<subsubsectionHeader confidence="0.693833">
5.1.1 Serial Minibatch
</subsubsectionHeader>
<bodyText confidence="0.9982962">
We first run minibatch in the serial mode with
varying minibatch size of 4, 16, 24, 32, and 48 (see
Figure 2). We can make the following observations.
First, except for the largest minibatch size of 48,
minibatch learning generally improves the accuracy
</bodyText>
<footnote confidence="0.887069">
4We turn off garbage-collection in worker processes oth-
erwise their running times will be highly unbalanced. We also
admit that Python is not the best choice for parallelization, e.g.,
asychronous minibatch (Gimpel et al., 2010) requires “shared
memory” not found in the current Python (see also Sec. 6).
5Available at http://acl.cs.qc.edu/. The version
with minibatch parallelization will be available there soon.
</footnote>
<figure confidence="0.9006775">
0 1 2 3 4 5 6 7 8
wall-clock time (hours)
</figure>
<figureCaption confidence="0.982129333333333">
Figure 2: Minibatch with various minibatch sizes (m =
4,16, 24, 32, 48) for parsing with MIRA, compared to
pure MIRA (m = 1). All curves are on a single CPU.
</figureCaption>
<bodyText confidence="0.999876066666667">
of the converged model, which is explained by our
intuition that optimization with a larger constraint
set could improve the margin. In particular, m = 16
achieves the highest accuracy of 92.53, which is a
0.27 improvement over the baseline.
Secondly, minibatch learning can reach high lev-
els of accuracy faster than the baseline can. For ex-
ample, minibatch of size 4 can reach 92.35 in 3.5
hours, and minibatch of size 24 in 3.7 hours, while
the pure online baseline needs 6.9 hours. In other
words, just minibatch alone in serial mode can al-
ready speed up learning. This is also explained by
the intuition of better optimization above, and con-
tributes significantly to the final speedup of paral-
lelized minibatch.
Lastly, larger minibatch sizes slow down the con-
vergence, with m = 4 converging the fastest and
m = 48 the slowest. This can be explained by the
trade-off between the relative strengths from online
learning and batch update: with larger batch sizes,
we lose the dependencies between examples within
the same minibatch.
Although larger minibatches slow down conver-
gence, they actually offer better potential for paral-
lelization since the number of processors p has to be
smaller than minibatch size m (in fact, p should di-
vide m). For example, m = 24 can work with 2, 3,
4, 6, 8, or 12 processors while m = 4 can only work
with 2 or 4 and the speed up of 12 processors could
easily make up for the slightly slower convergence
</bodyText>
<figure confidence="0.968453705882353">
m=1
m=4
m=16
m=24
m=32
m=48
accuracy on held-out 92.5
92.25
92
91.75
91.5
91.25
91
90.75
375
0 1 2 3 4 5 6 7 8
wall-clock time (hours)
</figure>
<figureCaption confidence="0.9960426">
Figure 3: Parallelized minibatch is much faster than iter-
ative parameter mixing. Top: minibatch of size 24 using
4 and 12 processors offers significant speedups over the
serial minibatch and pure online baselines. Bottom: IPM
with the same processors offers very small speedups.
</figureCaption>
<bodyText confidence="0.92870975">
rate. So there seems to be a “sweetspot” of mini-
batch sizes, similar to the tipping point observed in
McDonald et al. (2010) when adding more proces-
sors starts to hurt convergence.
</bodyText>
<subsubsectionHeader confidence="0.976577">
5.1.2 Parallelized Minibatch vs. IPM
</subsubsectionHeader>
<bodyText confidence="0.984513181818182">
In the following experiments we use minibatch
size of m = 24 and run it in parallel mode on vari-
ous numbers of processors (p = 2 — 12). Figure 3
(top) shows that 4 and 12 processors lead to very
significant speedups over the serial minibatch and
pure online baselines. For example, it takes the 12
processors only 0.66 hours to reach an accuracy of
92.35, which takes the pure online MIRA 6.9 hours,
amounting to an impressive speedup of 10.5.
We compare our minibatch parallelization with
the iterative parameter mixing (IPM) of McDonald
et al. (2010). Figure 3 (bottom) shows that IPM not
only offers much smaller speedups, but also con-
verges lower, and this drop in accuracy worsens with
more processors.
Figure 4 gives a detailed analysis of speedups.
Here we perform both extrinsic and intrinsic com-
parisons. In the former, we care about the time to
reach a given accuracy; in this plot we use 92.27
which is the converged accuracy of IPM on 12 pro-
cessors. We choose it since it is the lowest accu-
number of processors
</bodyText>
<figureCaption confidence="0.546451166666667">
Figure 4: Speedups of minibatch parallelization vs. IPM
on 1 to 12 processors (parsing with MIRA). Extrinsic
comparisons use “the time to reach an accuracy of 92.27”
for speed calculations, 92.27 being the converged accu-
racy of IPM using 12 processors. Intrinsic comparisons
use average time per iteration regardless of accuracy.
</figureCaption>
<bodyText confidence="0.999977607142857">
racy among all converged models; choosing a higher
accuracy would reveal even larger speedups for our
methods. This figure shows that our method offers
superlinear speedups with small number of proces-
sors (1 to 6), and almost linear speedups with large
number of processors (8 and 12). Note that even
p = 1 offers a speedup of 1.5 thanks to serial mini-
batch’s faster convergence; in other words, within
the 9 fold speed-up at p = 12, parallelization con-
tributes about 6 and minibatch about 1.5. By con-
trast, IPM only offers an almost constant speedup of
around 3, which is consistent with the findings of
McDonald et al. (2010) (both of their experiments
show a speedup of around 3).
We also try to understand where the speedup
comes from. For that purpose we study intrinsic
speedup, which is about the speed regardless of ac-
curacy (see Figure 4). For our minibatch method,
intrinsic speedup is the average time per iteration
of a parallel run over the serial minibatch base-
line. This answers the questions such as “how CPU-
efficient is our parallelization” or “how much CPU
time is wasted”. We can see that with small num-
ber of processors (2 to 4), the efficiency, defined as
Sp/p where Sp is the intrinsic speedup for p pro-
cessors, is almost 100% (ideal linear speedup), but
with more processors it decreases to around 50%
with p = 12, meaning about half of CPU time is
</bodyText>
<figure confidence="0.990926825581395">
baseline
m=24,p=1
m=24,p=4
m=24,p=12
0 1 2 3 4 5 6 7 8
92.4
92.2
92
91.8
91.6
91.4
92.4
92.2
91.8
91.6
91.4
92
baseline
IPM,p=4
IPM,p=12
2 4 6 8 10 12
12
11
10
4
9
8
7
6
5
3
2
1
minibatch(extrinsic)
minibatch(intrinsic)
IPM(extrinsic)
IPM(intrinsic)
accuracy
accuracy
speedups
12
11
10
9
8
7
6
5
4
3
2
1
376
m=1
m=16
m=24
m=48
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8
wall-clock time (hours)
96.95
96.85
96.9
96.8
97
baseline
m=24,p=1
m=24,p=4
m=24,p=12
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8
accuracy
96.95
96.85
96.9
96.8
97
baseline
IPM,p=4
IPM,p=12
accuracy
accuracy on held-out
97.05
97
96.95
96.9
96.85
96.8
</figure>
<figureCaption confidence="0.749347">
Figure 5: Minibatch learning for tagging with perceptron
(m = 16, 24, 32) compared with baseline (m = 1) for
tagging with perceptron. All curves are on single CPU.
</figureCaption>
<bodyText confidence="0.99983025">
wasted. This wasting is due to two sources: first, the
load-balancing problem worsens with more proces-
sors, and secondly, the update procedure still runs in
serial mode with p − 1 processors sleeping.
</bodyText>
<subsectionHeader confidence="0.999494">
5.2 Part-of-Speech Tagging with Perceptron
</subsectionHeader>
<bodyText confidence="0.999988444444445">
Part-of-speech tagging is usually considered as a
simpler task compared to dependency parsing. Here
we show that using minibatch can also bring better
accuracies and speedups for part-of-speech tagging.
We implement a part-of-speech tagger with aver-
aged perceptron. Following the standard splitting of
Penn Treebank (Collins, 2002), we use Sections 00-
18 for training and Sections 19-21 as held-out. Our
implementation provides an accuracy of 96.98 with
beam size 8.
First we run the tagger on a single processor with
minibatch sizes 8, 16, 24, and 32. As in Figure 5, we
observe similar convergence acceleration and higher
accuracies with minibatch. In particular, minibatch
of size m = 16 provides the highest accuracy of
97.04, giving an improvement of 0.06. This im-
provement is smaller than what we observe in MIRA
learning for dependency parsing experiments, which
can be partly explained by the fast convergence of
the tagger, and that perceptron does not involve op-
timization in the updates.
Then we choose minibatch of size 24 to investi-
gate the parallelization performance. As Figure 6
(top) shows, with 12 processors our method takes
only 0.10 hours to converge to an accuracy of 97.00,
compared to the baseline of 96.98 with 0.45 hours.
We also compare our method with IPM as in Fig-
</bodyText>
<figure confidence="0.9544625">
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8
wall-clock time (hours)
</figure>
<figureCaption confidence="0.988933">
Figure 6: Parallelized minibatch is faster than iterative
</figureCaption>
<bodyText confidence="0.904652466666667">
parameter mixing (on tagging with perceptron). Top:
minibatch of size 24 using 4 and 12 processors offers
significant speedups over the baselines. Bottom: IPM
with the same 4 and 12 processors offers slightly smaller
speedups. Note that IPM with 4 processors converges
lower than other parallelization curves.
ure 6 (bottom). Again, our method converges faster
and better than IPM, but this time the differences are
much smaller than those in parsing.
Figure 7 uses 96.97 as a criteria to evaluate the
extrinsic speedups given by our method and IPM.
Again we choose this number because it is the lowest
accuracy all learners can reach. As the figure sug-
gests, although our method does not have a higher
pure parallelization speedup (intrinsic speedup), it
still outperforms IPM.
We are interested in the reason why tagging ben-
efits less from minibatch and parallelization com-
pared to parsing. Further investigation reveals that
in tagging the working load of different processors
are more unbalanced than in parsing. Figure 8 shows
that, when p is small, waiting time is negligible, but
when p = 12, tagging wastes about 40% of CPU
cycles and parser about 30%. By contrast, there
is almost no waiting time in IPM and the intrinsic
speedup for IPM is almost linear. The communica-
tion overhead is not included in this figure, but by
comparing it to the speedups (Figures 4 and 7), we
conclude that the communication overhead is about
10% for both parsing and tagging at p = 12.
</bodyText>
<page confidence="0.981324">
377
</page>
<figure confidence="0.996608166666667">
12
11
10
9
8
7
6
5
4
3
2
1
</figure>
<figureCaption confidence="0.999919">
Figure 7: Speedups of minibatch parallelization and IPM
</figureCaption>
<bodyText confidence="0.610468625">
on 1 to 12 processors (tagging with perceptron). Extrin-
sic speedup uses “the time to reach an accuracy of 96.97”
as the criterion to measure speed. Intrinsic speedup mea-
sures the pure parallelization speedup. IPM has an al-
most linear intrinsic speedup but a near constant extrinsic
speedup of about 3 to 4.
2 4 6 8 10 12
number of processors
</bodyText>
<figureCaption confidence="0.896457">
Figure 8: Percentage of time wasted due to synchroniza-
</figureCaption>
<bodyText confidence="0.968825142857143">
tion (waiting for other processors to finish) (minibatch
m = 24), which corresponds to the gray blocks in Fig-
ure 1 (b-c). The number of sentences assigned to each
processor decreases with more processors, which wors-
ens the unbalance. Our load-balancing strategy (Figure 1
(c)) alleviates this problem effectively. The communica-
tion overhead and update time are not included.
</bodyText>
<sectionHeader confidence="0.999839" genericHeader="method">
6 Related Work and Discussions
</sectionHeader>
<bodyText confidence="0.999746809523809">
Besides synchronous minibatch and iterative param-
eter mixing (IPM) discussed above, there is another
method of asychronous minibatch parallelization
(Zinkevich et al., 2009; Gimpel et al., 2010; Chiang,
2012), as in Figure 1. The key advantage of asyn-
chronous over synchronous minibatch is that the for-
mer allows processors to remain near-constant use,
while the latter wastes a significant amount of time
when some processors finish earlier than others in a
minibatch, as found in our experiments. Gimpel et
al. (2010) show significant speedups of asychronous
parallelization over synchronous minibatch on SGD
and EM methods, and Chiang (2012) finds asyn-
chronous parallelization to be much faster than IPM
on MIRA for machine translation. However, asyn-
chronous is significantly more complicated to imple-
ment, which involves locking when one processor
makes an update (see Fig. 1 (d)), and (in languages
like Python) message-passing to other processors af-
ter update. Whether this added complexity is worth-
while on large-margin learning is an open question.
</bodyText>
<sectionHeader confidence="0.998416" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999998478260869">
We have presented a simple minibatch paralleliza-
tion paradigm to speed up large-margin structured
learning algorithms such as (averaged) perceptron
and MIRA. Minibatch has an advantage in both se-
rial and parallel settings, and our experiments con-
firmed that a minibatch size of around 16 or 24 leads
to a significant speedups over the pure online base-
line, and when combined with parallelization, leads
to almost linear speedups for MIRA, and very signif-
icant speedups for perceptron. These speedups are
significantly higher than those of iterative parame-
ter mixing of McDonald et al. (2010) which were
almost constant (3-4) in both our and their own ex-
periments regardless of the number of processors.
One of the limitations of this work is that although
decoding is done in parallel, update is still done in
serial and in MIRA the quadratic optimization step
(Hildreth algorithm (Hildreth, 1957)) scales super-
linearly with the number of constraints. This pre-
vents us from using very large minibatches. For
future work, we would like to explore parallelized
quadratic optimization and larger minibatch sizes,
and eventually apply it to machine translation.
</bodyText>
<sectionHeader confidence="0.977381" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999818833333333">
We thank Ryan McDonald, Yoav Goldberg, and Hal
Daum´e, III for helpful discussions, and the anony-
mous reviewers for suggestions. This work was
partially supported by DARPA FA8750-13-2-0041
“Deep Exploration and Filtering of Text” (DEFT)
Program and by Queens College for equipment.
</bodyText>
<figure confidence="0.997205967741936">
% of waiting time
40
60
50
30
20
10
0
parser(balanced)
tagger(balanced)
parser(unbalanced)
tagger(unbalanced)
number of processors
speedup ratio
2 4 6 8 10 12
12
11
10
4
9
8
7
6
5
3
2
1
minibatch(extrinsic)
minibatch(intrinsic)
IPM(extrinsic)
IPM(intrinsic)
</figure>
<page confidence="0.989007">
378
</page>
<sectionHeader confidence="0.992686" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999747056603773">
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. J. Machine
Learning Research (JMLR), 13:1159–1187.
C.-T. Chu, S.-K. Kim, Y.-A. Lin, Y.-Y. Yu, G. Bradski,
A. Ng, and K. Olukotun. 2007. Map-reduce for ma-
chine learning on multicore. In Advances in Neural
Information Processing Systems 19.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951–991, March.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL.
Kevin Gimpel, Dipanjan Das, and Noah Smith. 2010.
Distributed asynchronous online learning for natural
language processing. In Proceedings of CoNLL.
Clifford Hildreth. 1957. A quadratic programming pro-
cedure. Naval Research Logistics Quarterly, 4(1):79–
85.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of ACL 2010.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Proceed-
ings of NAACL.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML.
Percy Liang and Dan Klein. 2009. Online em for unsu-
pervised models. In Proceedings of NAACL.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd ACL.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Proceedings of NAACL, June.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.
2007. Pegasos: Primal estimated sub-gradient solver
for svm. In Proceedings of ICML.
M. Zinkevich, A. J. Smola, and J. Langford. 2009. Slow
learners are fast. In Advances in Neural Information
Processing Systems 22.
</reference>
<page confidence="0.999168">
379
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.417399">
<title confidence="0.984053">Minibatch and Parallelization for Online Large Margin Structured Learning</title>
<affiliation confidence="0.512658">Science Program, Graduate Center Science Dept, Queens College City University of New York City University of New York</affiliation>
<email confidence="0.999615">kzhao@gc.cuny.eduhuang@cs.qc.cuny.edu</email>
<abstract confidence="0.999688904761905">Online learning algorithms such as perceptron and MIRA have become popular for many NLP tasks thanks to their simpler architecture and faster convergence over batch learning methods. However, while batch learning such as CRF is easily parallelizable, online learning is much harder to parallelize: previous efforts often witness a decrease in the converged accuracy, and the speedup is typically small even with many (10+) processors. We instead present a much simpler architecture based on “mini-batches”, which is trivially parallelizable. We show that, unlike previous methods, minibatch learning (in mode) actually converged accuracy for both perceptron and MIRA learning, and when combined with simple parallelization, minibatch leads to very significant speedups (up to 9x on 12 processors) on stateof-the-art parsing and tagging systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hope and fear for discriminative training of statistical translation models.</title>
<date>2012</date>
<journal>J. Machine Learning Research (JMLR),</journal>
<pages>13--1159</pages>
<contexts>
<context position="29749" citStr="Chiang, 2012" startWordPosition="5224" endWordPosition="5225">ion (waiting for other processors to finish) (minibatch m = 24), which corresponds to the gray blocks in Figure 1 (b-c). The number of sentences assigned to each processor decreases with more processors, which worsens the unbalance. Our load-balancing strategy (Figure 1 (c)) alleviates this problem effectively. The communication overhead and update time are not included. 6 Related Work and Discussions Besides synchronous minibatch and iterative parameter mixing (IPM) discussed above, there is another method of asychronous minibatch parallelization (Zinkevich et al., 2009; Gimpel et al., 2010; Chiang, 2012), as in Figure 1. The key advantage of asynchronous over synchronous minibatch is that the former allows processors to remain near-constant use, while the latter wastes a significant amount of time when some processors finish earlier than others in a minibatch, as found in our experiments. Gimpel et al. (2010) show significant speedups of asychronous parallelization over synchronous minibatch on SGD and EM methods, and Chiang (2012) finds asynchronous parallelization to be much faster than IPM on MIRA for machine translation. However, asynchronous is significantly more complicated to implement</context>
</contexts>
<marker>Chiang, 2012</marker>
<rawString>David Chiang. 2012. Hope and fear for discriminative training of statistical translation models. J. Machine Learning Research (JMLR), 13:1159–1187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-T Chu</author>
<author>S-K Kim</author>
<author>Y-A Lin</author>
<author>Y-Y Yu</author>
<author>G Bradski</author>
<author>A Ng</author>
<author>K Olukotun</author>
</authors>
<title>Map-reduce for machine learning on multicore.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems 19.</booktitle>
<contexts>
<context position="2779" citStr="Chu et al., 2007" startWordPosition="431" endWordPosition="434">veloping modified learning algorithms that allow inexact search (Collins and Roark, 2004; Huang et al., 2012). However, the learner still needs to loop over the whole training data (on the order of millions of sentences) many times. For example the best-performing method in Huang et al. (2012) still requires 5-6 hours to train a very fast parser. On the other hand, with the increasing popularity of multicore and cluster computers, there is a growing interest in speeding up training via parallelization. While batch learning such as CRF (Lafferty et al., 2001) is often trivially parallelizable (Chu et al., 2007) since each update is a batch-aggregate of the update from each (independent) example, online learning is much harder to parallelize due to the dependency between examples, i.e., the update on the first example should in principle influence the decoding of all remaining examples. Thus if we decode and update the first and the 1000th examples in parallel, we lose their interactions which is one of the reasons for online learners’ fast convergence. This explains why previous work such as the iterative parameter mixing (IPM) method of McDonald et al. (2010) witnesses a decrease in the accuracies </context>
</contexts>
<marker>Chu, Kim, Lin, Yu, Bradski, Ng, Olukotun, 2007</marker>
<rawString>C.-T. Chu, S.-K. Kim, Y.-A. Lin, Y.-Y. Yu, G. Bradski, A. Ng, and K. Olukotun. 2007. Map-reduce for machine learning on multicore. In Advances in Neural Information Processing Systems 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2250" citStr="Collins and Roark, 2004" startWordPosition="341" endWordPosition="344">cture which only needs 1-best or k-best decoding. However, online learning for NLP typically involves expensive inference on each example for 10 or more passes over millions of examples, which often makes training too slow in practice; for example systems such as the popular (2nd-order) MST parser (McDonald and Pereira, 2006) usually require the order of days to train on the Treebank on a commodity machine (McDonald et al., 2010). There are mainly two ways to address this scalability problem. On one hand, researchers have been developing modified learning algorithms that allow inexact search (Collins and Roark, 2004; Huang et al., 2012). However, the learner still needs to loop over the whole training data (on the order of millions of sentences) many times. For example the best-performing method in Huang et al. (2012) still requires 5-6 hours to train a very fast parser. On the other hand, with the increasing popularity of multicore and cluster computers, there is a growing interest in speeding up training via parallelization. While batch learning such as CRF (Lafferty et al., 2001) is often trivially parallelizable (Chu et al., 2007) since each update is a batch-aggregate of the update from each (indepe</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1262" citStr="Collins, 2002" startWordPosition="183" endWordPosition="184">cy, and the speedup is typically very small (-3) even with many (10+) processors. We instead present a much simpler architecture based on “mini-batches”, which is trivially parallelizable. We show that, unlike previous methods, minibatch learning (in serial mode) actually improves the converged accuracy for both perceptron and MIRA learning, and when combined with simple parallelization, minibatch leads to very significant speedups (up to 9x on 12 processors) on stateof-the-art parsing and tagging systems. 1 Introduction Online structured learning algorithms such as the structured perceptron (Collins, 2002) and k-best MIRA (McDonald et al., 2005) have become more and more popular for many NLP tasks such as dependency parsing and part-of-speech tagging. This is because, compared to their batch learning counterparts, online learning methods offer faster convergence rates and better scalability to large datasets, while using much less memory and a much simpler architecture which only needs 1-best or k-best decoding. However, online learning for NLP typically involves expensive inference on each example for 10 or more passes over millions of examples, which often makes training too slow in practice;</context>
<context position="4106" citStr="Collins (2002)" startWordPosition="645" endWordPosition="646">+ processors. We instead explore the idea of “minibatch” for online large-margin structured learning such as perceptron and MIRA. We argue that minibatch is advantageous in both serial and parallel settings. First, for minibatch perceptron in the serial set370 Proceedings of NAACL-HLT 2013, pages 370–379, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics ting, our intuition is that, although decoding is done independently within one minibatch, updates are done by averaging update vectors in batch, providing a “mixing effect” similar to “averaged parameters” of Collins (2002) which is also found in IPM (McDonald et al., 2010), and online EM (Liang and Klein, 2009). Secondly, minibatch MIRA in the serial setting has an advantage that, different from previous methods such as SGD which simply sum up the updates from all examples in a minibatch, a minibatch MIRA update tries to simultaneously satisfy an aggregated set of constraints that are collected from multiple examples in the minibatch. Thus each minibatch MIRA update involves an optimization over many more constraints than in pure online MIRA, which could potentially lead to a better margin. In other words we ca</context>
<context position="7405" citStr="Collins (2002)" startWordPosition="1202" endWordPosition="1203">raints, and 2. update the weight vector w according to C. Here a triple (x, y, z) is said to be a “violating constraint” with respect to model w if the incorrect label z scores higher than (or equal to) the correct label y in w, i.e., w · 0Φ((x, y, z)) G 0, where 0Φ((x, y, z)) is a short-hand notation for the update vector Φ(x, y) — Φ(x, z) and Φ is the feature map (see Huang et al. (2012) for details). The subroutines FINDCONSTRAINTS and UPDATE are analogous to “APIs”, to be specified by specific instances of this online learning framework. For example, the structured perceptron algorithm of Collins (2002) is implemented in Algorithm 2 where FINDCONSTRAINTS returns a singleton constraint if the 1-best decoding result z (the highest scoring label according to the current model) is different from the true label y. Note that in the UPDATE function, C is always a singleton constraint for the perceptron, but we make it more general (as a set) to handle the batch update in the minibatch version in Section 3. On the other hand, Algorith 3 presents the k-best MIRA Algorithm of McDonald et al. (2005) which generalizes multiclass MIRA (Crammer and Singer, 2003) for structured prediction. The decoder now </context>
<context position="8720" citStr="Collins, 2002" startWordPosition="1436" endWordPosition="1437">w · Φ(x, s) . decoding 3: if z =6 y then return {hx, y, zi} 4: else return ∅ 5: procedure UPDATE(w, C) 6: w ← w + |c |EcEC ΔΦ(c) . (batch) update Algorithm 3 k-best MIRA (McDonald et al., 2005). finds the k-best solutions Z first, and returns a set of violating constraints in Z, The update in MIRA is more interesting: it searches for the new model w&apos; with minimum change from the current model w so that w&apos; corrects each violating constraint by a margin at least as large as the loss `(y, z) of the incorrect label z. Although not mentioned in the pseudocode, we also employ “averaged parameters” (Collins, 2002) for both perceptron and MIRA in all experiments. 3 Serial Minibatch The idea of serial minibatch learning is extremely simple: divide the data into dn/me minibatches of size m, and do batch updates after decoding each minibatch (see Algorithm 4). The FINDCONSTRAINTS and UPDATE subroutines remain unchanged for both perceptron and MIRA, although it is important to note that a perceptron batch update uses the average of update vectors, not the sum, which simplifies the proof. This architecture is often called “synchronous minibatch” in the literature (Gimpel et al., 2010; Liang and Klein, 2009; </context>
<context position="11910" citStr="Collins, 2002" startWordPosition="2099" endWordPosition="2100">(c)&gt;` kw/ − wk2 (Ei = a) 372 Since for any two vectors a and b we have kakkbk ≥ a·b, thus kukkwt+1k ≥ u·wt+1 ≥ tδ. As u is a unit vector, we have kwt+1k ≥ tδ. 2. On the other hand, take the norm of both sides of Eq. (1): kwt+1k2 = kwt + 1P i vik2 a 1 =kwtk2 +kP avik2 + 2 wt · Pi vi i a ≤kwtk2 + kPi avik2 + 0 (violation) 1 kwt k2 + Pi a I I vi k2 (Jensen’s) 1 ≤kwtk2 + P (radius) i aR2 =kwtk2 + R2 (Pi = a) ≤tR2 (by induction) Combining the two bounds, we have t2δ2 ≤ kwt+1k2 ≤ tR2 thus the number of minibatch updates t ≤ R2/δ2. Note that this bound is identical to that of pure online perceptron (Collins, 2002, Theorem 1) and is irrelevant to minibatch size m. The use of Jensen’s inequality is inspired by McDonald et al. (2010). 3.2 Convergence of Minibatch MIRA We also give a proof of convergence for MIRA with relaxation.2 We present the optimization problem in the UPDATE function of Algorithm 3 as a quadratic program (QP) with slack variable ξ: wt+1 ← argmin Wt+1 kwt+1 − wtk2 + ξ s.t. wt+1 · vi ≥ `i −ξ, for all(ci, `i) ∈ Ct where vi = 04)(ci) is the update vector for constraint ci. Consider the Lagrangian: L =kwt+1 − wtk2 + ξ + X |Ct |ηi(`i − w&apos; · vi − ξ) i=1 ηi ≥ 0, for 1 ≤ i ≤ |Ct|. 2Actually t</context>
<context position="26095" citStr="Collins, 2002" startWordPosition="4604" endWordPosition="4605">ceptron. All curves are on single CPU. wasted. This wasting is due to two sources: first, the load-balancing problem worsens with more processors, and secondly, the update procedure still runs in serial mode with p − 1 processors sleeping. 5.2 Part-of-Speech Tagging with Perceptron Part-of-speech tagging is usually considered as a simpler task compared to dependency parsing. Here we show that using minibatch can also bring better accuracies and speedups for part-of-speech tagging. We implement a part-of-speech tagger with averaged perceptron. Following the standard splitting of Penn Treebank (Collins, 2002), we use Sections 00- 18 for training and Sections 19-21 as held-out. Our implementation provides an accuracy of 96.98 with beam size 8. First we run the tagger on a single processor with minibatch sizes 8, 16, 24, and 32. As in Figure 5, we observe similar convergence acceleration and higher accuracies with minibatch. In particular, minibatch of size m = 16 provides the highest accuracy of 97.04, giving an improvement of 0.06. This improvement is smaller than what we observe in MIRA learning for dependency parsing experiments, which can be partly explained by the fast convergence of the tagge</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>3</volume>
<contexts>
<context position="7961" citStr="Crammer and Singer, 2003" startWordPosition="1296" endWordPosition="1299">k. For example, the structured perceptron algorithm of Collins (2002) is implemented in Algorithm 2 where FINDCONSTRAINTS returns a singleton constraint if the 1-best decoding result z (the highest scoring label according to the current model) is different from the true label y. Note that in the UPDATE function, C is always a singleton constraint for the perceptron, but we make it more general (as a set) to handle the batch update in the minibatch version in Section 3. On the other hand, Algorith 3 presents the k-best MIRA Algorithm of McDonald et al. (2005) which generalizes multiclass MIRA (Crammer and Singer, 2003) for structured prediction. The decoder now 371 Algorithm 2 Perceptron (Collins, 2002). 1: function FINDCONSTRAINTS(x, y, w) 2: z ← argmaxsEY(x) w · Φ(x, s) . decoding 3: if z =6 y then return {hx, y, zi} 4: else return ∅ 5: procedure UPDATE(w, C) 6: w ← w + |c |EcEC ΔΦ(c) . (batch) update Algorithm 3 k-best MIRA (McDonald et al., 2005). finds the k-best solutions Z first, and returns a set of violating constraints in Z, The update in MIRA is more interesting: it searches for the new model w&apos; with minimum change from the current model w so that w&apos; corrects each violating constraint by a margin</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. J. Mach. Learn. Res., 3:951–991, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Alex Kleeman</author>
<author>Christopher D Manning</author>
</authors>
<title>Efficient, feature-based, conditional random field parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5221" citStr="Finkel et al., 2008" startWordPosition="831" endWordPosition="834">nstraints than in pure online MIRA, which could potentially lead to a better margin. In other words we can view MIRA as an online version or stepwise approximation of SVM, and minibatch MIRA can be seen as a better approximation as well as a middleground between pure MIRA and SVM.1 More interestingly, the minibatch architecture is trivially parallelizable since the examples within each minibatch could be decoded in parallel on multiple processors (while the update is still done in serial). This is known as “synchronous minibatch” and has been explored by many researchers (Gimpel et al., 2010; Finkel et al., 2008), but all previous works focus on probabilistic models along with SGD or EM learning methods while our work is the first effort on large-margin methods. We make the following contributions: • Theoretically, we present a serial minibatch framework (Section 3) for online large-margin learning and prove the convergence theorems for minibatch perceptron and minibatch MIRA. • Empirically, we show that serial minibatch could speed up convergence and improve the converged accuracy for both MIRA and perceptron on state-of-the-art dependency parsing and part-of-speech tagging systems. • In addition, wh</context>
<context position="9340" citStr="Finkel et al., 2008" startWordPosition="1536" endWordPosition="1539"> for both perceptron and MIRA in all experiments. 3 Serial Minibatch The idea of serial minibatch learning is extremely simple: divide the data into dn/me minibatches of size m, and do batch updates after decoding each minibatch (see Algorithm 4). The FINDCONSTRAINTS and UPDATE subroutines remain unchanged for both perceptron and MIRA, although it is important to note that a perceptron batch update uses the average of update vectors, not the sum, which simplifies the proof. This architecture is often called “synchronous minibatch” in the literature (Gimpel et al., 2010; Liang and Klein, 2009; Finkel et al., 2008). It could be viewed as a middleground between pure online learning and batch learning. 3.1 Convergence of Minibatch Perceptron We denote C(D) to be the set of all possible violating constraints in data D (cf. Huang et al. (2012)): C(D) = {hx, y, zi |(x, y) ∈ D, z ∈ Y(x) − {y}}. Algorithm 4 Serial Minibatch Online Learning. Input: data D, feature map Φ, and minibatch size m Output: weight vector w 1: Split D into dn/me minibatches D1 ... Dfn/ml 2: repeat 3: for i ← 1... dn/me do . for each minibatch 4: C ← ∪(x y)ED,,FINDCONSTRAINTS(x, y, w) 5: if C =6 ∅ then UPDATE(w, C) . batch update 6: unti</context>
</contexts>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning. 2008. Efficient, feature-based, conditional random field parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Dipanjan Das</author>
<author>Noah Smith</author>
</authors>
<title>Distributed asynchronous online learning for natural language processing.</title>
<date>2010</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="5199" citStr="Gimpel et al., 2010" startWordPosition="826" endWordPosition="830">ion over many more constraints than in pure online MIRA, which could potentially lead to a better margin. In other words we can view MIRA as an online version or stepwise approximation of SVM, and minibatch MIRA can be seen as a better approximation as well as a middleground between pure MIRA and SVM.1 More interestingly, the minibatch architecture is trivially parallelizable since the examples within each minibatch could be decoded in parallel on multiple processors (while the update is still done in serial). This is known as “synchronous minibatch” and has been explored by many researchers (Gimpel et al., 2010; Finkel et al., 2008), but all previous works focus on probabilistic models along with SGD or EM learning methods while our work is the first effort on large-margin methods. We make the following contributions: • Theoretically, we present a serial minibatch framework (Section 3) for online large-margin learning and prove the convergence theorems for minibatch perceptron and minibatch MIRA. • Empirically, we show that serial minibatch could speed up convergence and improve the converged accuracy for both MIRA and perceptron on state-of-the-art dependency parsing and part-of-speech tagging syst</context>
<context position="9295" citStr="Gimpel et al., 2010" startWordPosition="1528" endWordPosition="1531">employ “averaged parameters” (Collins, 2002) for both perceptron and MIRA in all experiments. 3 Serial Minibatch The idea of serial minibatch learning is extremely simple: divide the data into dn/me minibatches of size m, and do batch updates after decoding each minibatch (see Algorithm 4). The FINDCONSTRAINTS and UPDATE subroutines remain unchanged for both perceptron and MIRA, although it is important to note that a perceptron batch update uses the average of update vectors, not the sum, which simplifies the proof. This architecture is often called “synchronous minibatch” in the literature (Gimpel et al., 2010; Liang and Klein, 2009; Finkel et al., 2008). It could be viewed as a middleground between pure online learning and batch learning. 3.1 Convergence of Minibatch Perceptron We denote C(D) to be the set of all possible violating constraints in data D (cf. Huang et al. (2012)): C(D) = {hx, y, zi |(x, y) ∈ D, z ∈ Y(x) − {y}}. Algorithm 4 Serial Minibatch Online Learning. Input: data D, feature map Φ, and minibatch size m Output: weight vector w 1: Split D into dn/me minibatches D1 ... Dfn/ml 2: repeat 3: for i ← 1... dn/me do . for each minibatch 4: C ← ∪(x y)ED,,FINDCONSTRAINTS(x, y, w) 5: if C </context>
<context position="14628" citStr="Gimpel et al., 2010" startWordPosition="2637" endWordPosition="2640"> 4 6 8 2 ⊕ update 9 11 13 15 10 12 14 16 ⊕ update 12 3 9 1 14 15 4 6 update update ⊕ ⊕ 13 16 5 8 10 11 2 7 update update 15 16 2 1 update update 10 3 4 9 update update 13 14 5 6 update update 12 11 8 7 (a) IPM (b) unbalanced (c) balanced (d) asynchronous Figure 1: Comparison of various methods for parallelizing online learning (number of processors p = 4). (a) iterative parameter mixing (McDonald et al., 2010). (b) unbalanced minibatch parallelization (minibatch size m = 8). (c) minibatch parallelization after load-balancing (within each minibatch). (d) asynchronous minibatch parallelization (Gimpel et al., 2010) (not implemented here). Each numbered box denotes the decoding of one example, and � denotes an aggregate operation, i.e., the merging of constraints after each minibatch or the mixing of weights after each iteration in IPM. Each gray shaded box denotes time wasted due to synchronization in (a)-(c) or blocking in (d). Note that in (d) at most one update can happen concurrently, making it substantially harder to implement than (a)-(c). other examples in the same batch. Thus we can easily distribute decoding for different examples in the same minibatch to different processors. Shown in Algorith</context>
<context position="18492" citStr="Gimpel et al. (2010)" startWordPosition="3268" endWordPosition="3271">recision T7600 workstation with two 3.1GHz 8-core CPUs (16 processors in total) and 64GB RAM. We use Python 2.7’s multiprocessing module in all experiments.4 5.1 Dependency Parsing with MIRA We base our experiments on our dynamic programming incremental dependency parser (Huang and Sagae, 2010).5 Following Huang et al. (2012), we use max-violation update and beam size b = 8. We evaluate on the standard Penn Treebank (PTB) using the standard split: Sections 02-21 for training, and Section 22 as the held-out set (which is indeed the test-set in this setting, following McDonald et al. (2010) and Gimpel et al. (2010)). We then extend it to employ 1-best MIRA learning. As stated in Section 2, MIRA separates the gold label y from the incorrect label z with a margin at least as large as the loss e(y, z). Here in incremental dependency parsing we define the loss function between a gold tree y and an incorrect partial tree z as the number of incorrect edges in z, plus the number of correct edges in y which are already ruled out by z. This MIRA extension results in slightly higher accuracy of 92.36, which we will use as the pure online learning baseline in the comparisons below. 5.1.1 Serial Minibatch We first </context>
<context position="29734" citStr="Gimpel et al., 2010" startWordPosition="5220" endWordPosition="5223">d due to synchronization (waiting for other processors to finish) (minibatch m = 24), which corresponds to the gray blocks in Figure 1 (b-c). The number of sentences assigned to each processor decreases with more processors, which worsens the unbalance. Our load-balancing strategy (Figure 1 (c)) alleviates this problem effectively. The communication overhead and update time are not included. 6 Related Work and Discussions Besides synchronous minibatch and iterative parameter mixing (IPM) discussed above, there is another method of asychronous minibatch parallelization (Zinkevich et al., 2009; Gimpel et al., 2010; Chiang, 2012), as in Figure 1. The key advantage of asynchronous over synchronous minibatch is that the former allows processors to remain near-constant use, while the latter wastes a significant amount of time when some processors finish earlier than others in a minibatch, as found in our experiments. Gimpel et al. (2010) show significant speedups of asychronous parallelization over synchronous minibatch on SGD and EM methods, and Chiang (2012) finds asynchronous parallelization to be much faster than IPM on MIRA for machine translation. However, asynchronous is significantly more complicat</context>
</contexts>
<marker>Gimpel, Das, Smith, 2010</marker>
<rawString>Kevin Gimpel, Dipanjan Das, and Noah Smith. 2010. Distributed asynchronous online learning for natural language processing. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Clifford Hildreth</author>
</authors>
<title>A quadratic programming procedure.</title>
<date>1957</date>
<journal>Naval Research Logistics Quarterly,</journal>
<volume>4</volume>
<issue>1</issue>
<pages>85</pages>
<contexts>
<context position="31525" citStr="Hildreth, 1957" startWordPosition="5507" endWordPosition="5508">nd 16 or 24 leads to a significant speedups over the pure online baseline, and when combined with parallelization, leads to almost linear speedups for MIRA, and very significant speedups for perceptron. These speedups are significantly higher than those of iterative parameter mixing of McDonald et al. (2010) which were almost constant (3-4) in both our and their own experiments regardless of the number of processors. One of the limitations of this work is that although decoding is done in parallel, update is still done in serial and in MIRA the quadratic optimization step (Hildreth algorithm (Hildreth, 1957)) scales superlinearly with the number of constraints. This prevents us from using very large minibatches. For future work, we would like to explore parallelized quadratic optimization and larger minibatch sizes, and eventually apply it to machine translation. Acknowledgement We thank Ryan McDonald, Yoav Goldberg, and Hal Daum´e, III for helpful discussions, and the anonymous reviewers for suggestions. This work was partially supported by DARPA FA8750-13-2-0041 “Deep Exploration and Filtering of Text” (DEFT) Program and by Queens College for equipment. % of waiting time 40 60 50 30 20 10 0 par</context>
</contexts>
<marker>Hildreth, 1957</marker>
<rawString>Clifford Hildreth. 1957. A quadratic programming procedure. Naval Research Logistics Quarterly, 4(1):79– 85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="18167" citStr="Huang and Sagae, 2010" startWordPosition="3211" endWordPosition="3214">typical structured prediction problems: incremental dependency parsing and part-of-speech tagging; both are done on state-of-the-art baseline. We also compare our parallelized minibatch algorithm with the iterative parameter mixing (IPM) method of McDonald et al. (2010). We perform our experiments on a commodity 64-bit Dell Precision T7600 workstation with two 3.1GHz 8-core CPUs (16 processors in total) and 64GB RAM. We use Python 2.7’s multiprocessing module in all experiments.4 5.1 Dependency Parsing with MIRA We base our experiments on our dynamic programming incremental dependency parser (Huang and Sagae, 2010).5 Following Huang et al. (2012), we use max-violation update and beam size b = 8. We evaluate on the standard Penn Treebank (PTB) using the standard split: Sections 02-21 for training, and Section 22 as the held-out set (which is indeed the test-set in this setting, following McDonald et al. (2010) and Gimpel et al. (2010)). We then extend it to employ 1-best MIRA learning. As stated in Section 2, MIRA separates the gold label y from the incorrect label z with a margin at least as large as the loss e(y, z). Here in incremental dependency parsing we define the loss function between a gold tree</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of ACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Suphan Fayong</author>
<author>Yang Guo</author>
</authors>
<title>Structured perceptron with inexact search.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="2271" citStr="Huang et al., 2012" startWordPosition="345" endWordPosition="348">best or k-best decoding. However, online learning for NLP typically involves expensive inference on each example for 10 or more passes over millions of examples, which often makes training too slow in practice; for example systems such as the popular (2nd-order) MST parser (McDonald and Pereira, 2006) usually require the order of days to train on the Treebank on a commodity machine (McDonald et al., 2010). There are mainly two ways to address this scalability problem. On one hand, researchers have been developing modified learning algorithms that allow inexact search (Collins and Roark, 2004; Huang et al., 2012). However, the learner still needs to loop over the whole training data (on the order of millions of sentences) many times. For example the best-performing method in Huang et al. (2012) still requires 5-6 hours to train a very fast parser. On the other hand, with the increasing popularity of multicore and cluster computers, there is a growing interest in speeding up training via parallelization. While batch learning such as CRF (Lafferty et al., 2001) is often trivially parallelizable (Chu et al., 2007) since each update is a batch-aggregate of the update from each (independent) example, onlin</context>
<context position="7183" citStr="Huang et al. (2012)" startWordPosition="1167" endWordPosition="1170">line large-margin learning, where perceptron and MIRA are two special cases. Shown in Algorithm 1, the online learner considers each input example (x, y) sequentially and performs two steps: 1. find the set C of violating constraints, and 2. update the weight vector w according to C. Here a triple (x, y, z) is said to be a “violating constraint” with respect to model w if the incorrect label z scores higher than (or equal to) the correct label y in w, i.e., w · 0Φ((x, y, z)) G 0, where 0Φ((x, y, z)) is a short-hand notation for the update vector Φ(x, y) — Φ(x, z) and Φ is the feature map (see Huang et al. (2012) for details). The subroutines FINDCONSTRAINTS and UPDATE are analogous to “APIs”, to be specified by specific instances of this online learning framework. For example, the structured perceptron algorithm of Collins (2002) is implemented in Algorithm 2 where FINDCONSTRAINTS returns a singleton constraint if the 1-best decoding result z (the highest scoring label according to the current model) is different from the true label y. Note that in the UPDATE function, C is always a singleton constraint for the perceptron, but we make it more general (as a set) to handle the batch update in the minib</context>
<context position="9569" citStr="Huang et al. (2012)" startWordPosition="1576" endWordPosition="1579">see Algorithm 4). The FINDCONSTRAINTS and UPDATE subroutines remain unchanged for both perceptron and MIRA, although it is important to note that a perceptron batch update uses the average of update vectors, not the sum, which simplifies the proof. This architecture is often called “synchronous minibatch” in the literature (Gimpel et al., 2010; Liang and Klein, 2009; Finkel et al., 2008). It could be viewed as a middleground between pure online learning and batch learning. 3.1 Convergence of Minibatch Perceptron We denote C(D) to be the set of all possible violating constraints in data D (cf. Huang et al. (2012)): C(D) = {hx, y, zi |(x, y) ∈ D, z ∈ Y(x) − {y}}. Algorithm 4 Serial Minibatch Online Learning. Input: data D, feature map Φ, and minibatch size m Output: weight vector w 1: Split D into dn/me minibatches D1 ... Dfn/ml 2: repeat 3: for i ← 1... dn/me do . for each minibatch 4: C ← ∪(x y)ED,,FINDCONSTRAINTS(x, y, w) 5: if C =6 ∅ then UPDATE(w, C) . batch update 6: until converged A training set D is separable by feature map Φ with margin δ &gt; 0 if there exists a unit oracle vector u with kuk = 1 such that u · ΔΦ(hx, y, zi) ≥ δ, for all hx, y, zi ∈ C(D). Furthermore, let radius R ≥ kΔΦ(hx, y, zi</context>
<context position="18199" citStr="Huang et al. (2012)" startWordPosition="3216" endWordPosition="3219">ems: incremental dependency parsing and part-of-speech tagging; both are done on state-of-the-art baseline. We also compare our parallelized minibatch algorithm with the iterative parameter mixing (IPM) method of McDonald et al. (2010). We perform our experiments on a commodity 64-bit Dell Precision T7600 workstation with two 3.1GHz 8-core CPUs (16 processors in total) and 64GB RAM. We use Python 2.7’s multiprocessing module in all experiments.4 5.1 Dependency Parsing with MIRA We base our experiments on our dynamic programming incremental dependency parser (Huang and Sagae, 2010).5 Following Huang et al. (2012), we use max-violation update and beam size b = 8. We evaluate on the standard Penn Treebank (PTB) using the standard split: Sections 02-21 for training, and Section 22 as the held-out set (which is indeed the test-set in this setting, following McDonald et al. (2010) and Gimpel et al. (2010)). We then extend it to employ 1-best MIRA learning. As stated in Section 2, MIRA separates the gold label y from the incorrect label z with a margin at least as large as the loss e(y, z). Here in incremental dependency parsing we define the loss function between a gold tree y and an incorrect partial tree</context>
</contexts>
<marker>Huang, Fayong, Guo, 2012</marker>
<rawString>Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="2726" citStr="Lafferty et al., 2001" startWordPosition="423" endWordPosition="426">scalability problem. On one hand, researchers have been developing modified learning algorithms that allow inexact search (Collins and Roark, 2004; Huang et al., 2012). However, the learner still needs to loop over the whole training data (on the order of millions of sentences) many times. For example the best-performing method in Huang et al. (2012) still requires 5-6 hours to train a very fast parser. On the other hand, with the increasing popularity of multicore and cluster computers, there is a growing interest in speeding up training via parallelization. While batch learning such as CRF (Lafferty et al., 2001) is often trivially parallelizable (Chu et al., 2007) since each update is a batch-aggregate of the update from each (independent) example, online learning is much harder to parallelize due to the dependency between examples, i.e., the update on the first example should in principle influence the decoding of all remaining examples. Thus if we decode and update the first and the 1000th examples in parallel, we lose their interactions which is one of the reasons for online learners’ fast convergence. This explains why previous work such as the iterative parameter mixing (IPM) method of McDonald </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>Online em for unsupervised models.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="4196" citStr="Liang and Klein, 2009" startWordPosition="660" endWordPosition="663">tructured learning such as perceptron and MIRA. We argue that minibatch is advantageous in both serial and parallel settings. First, for minibatch perceptron in the serial set370 Proceedings of NAACL-HLT 2013, pages 370–379, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics ting, our intuition is that, although decoding is done independently within one minibatch, updates are done by averaging update vectors in batch, providing a “mixing effect” similar to “averaged parameters” of Collins (2002) which is also found in IPM (McDonald et al., 2010), and online EM (Liang and Klein, 2009). Secondly, minibatch MIRA in the serial setting has an advantage that, different from previous methods such as SGD which simply sum up the updates from all examples in a minibatch, a minibatch MIRA update tries to simultaneously satisfy an aggregated set of constraints that are collected from multiple examples in the minibatch. Thus each minibatch MIRA update involves an optimization over many more constraints than in pure online MIRA, which could potentially lead to a better margin. In other words we can view MIRA as an online version or stepwise approximation of SVM, and minibatch MIRA can </context>
<context position="9318" citStr="Liang and Klein, 2009" startWordPosition="1532" endWordPosition="1535">meters” (Collins, 2002) for both perceptron and MIRA in all experiments. 3 Serial Minibatch The idea of serial minibatch learning is extremely simple: divide the data into dn/me minibatches of size m, and do batch updates after decoding each minibatch (see Algorithm 4). The FINDCONSTRAINTS and UPDATE subroutines remain unchanged for both perceptron and MIRA, although it is important to note that a perceptron batch update uses the average of update vectors, not the sum, which simplifies the proof. This architecture is often called “synchronous minibatch” in the literature (Gimpel et al., 2010; Liang and Klein, 2009; Finkel et al., 2008). It could be viewed as a middleground between pure online learning and batch learning. 3.1 Convergence of Minibatch Perceptron We denote C(D) to be the set of all possible violating constraints in data D (cf. Huang et al. (2012)): C(D) = {hx, y, zi |(x, y) ∈ D, z ∈ Y(x) − {y}}. Algorithm 4 Serial Minibatch Online Learning. Input: data D, feature map Φ, and minibatch size m Output: weight vector w 1: Split D into dn/me minibatches D1 ... Dfn/ml 2: repeat 3: for i ← 1... dn/me do . for each minibatch 4: C ← ∪(x y)ED,,FINDCONSTRAINTS(x, y, w) 5: if C =6 ∅ then UPDATE(w, C) </context>
</contexts>
<marker>Liang, Klein, 2009</marker>
<rawString>Percy Liang and Dan Klein. 2009. Online em for unsupervised models. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="1954" citStr="McDonald and Pereira, 2006" startWordPosition="292" endWordPosition="295">e popular for many NLP tasks such as dependency parsing and part-of-speech tagging. This is because, compared to their batch learning counterparts, online learning methods offer faster convergence rates and better scalability to large datasets, while using much less memory and a much simpler architecture which only needs 1-best or k-best decoding. However, online learning for NLP typically involves expensive inference on each example for 10 or more passes over millions of examples, which often makes training too slow in practice; for example systems such as the popular (2nd-order) MST parser (McDonald and Pereira, 2006) usually require the order of days to train on the Treebank on a commodity machine (McDonald et al., 2010). There are mainly two ways to address this scalability problem. On one hand, researchers have been developing modified learning algorithms that allow inexact search (Collins and Roark, 2004; Huang et al., 2012). However, the learner still needs to loop over the whole training data (on the order of millions of sentences) many times. For example the best-performing method in Huang et al. (2012) still requires 5-6 hours to train a very fast parser. On the other hand, with the increasing popu</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL.</booktitle>
<contexts>
<context position="1302" citStr="McDonald et al., 2005" startWordPosition="188" endWordPosition="191"> very small (-3) even with many (10+) processors. We instead present a much simpler architecture based on “mini-batches”, which is trivially parallelizable. We show that, unlike previous methods, minibatch learning (in serial mode) actually improves the converged accuracy for both perceptron and MIRA learning, and when combined with simple parallelization, minibatch leads to very significant speedups (up to 9x on 12 processors) on stateof-the-art parsing and tagging systems. 1 Introduction Online structured learning algorithms such as the structured perceptron (Collins, 2002) and k-best MIRA (McDonald et al., 2005) have become more and more popular for many NLP tasks such as dependency parsing and part-of-speech tagging. This is because, compared to their batch learning counterparts, online learning methods offer faster convergence rates and better scalability to large datasets, while using much less memory and a much simpler architecture which only needs 1-best or k-best decoding. However, online learning for NLP typically involves expensive inference on each example for 10 or more passes over millions of examples, which often makes training too slow in practice; for example systems such as the popular</context>
<context position="7900" citStr="McDonald et al. (2005)" startWordPosition="1288" endWordPosition="1291">ied by specific instances of this online learning framework. For example, the structured perceptron algorithm of Collins (2002) is implemented in Algorithm 2 where FINDCONSTRAINTS returns a singleton constraint if the 1-best decoding result z (the highest scoring label according to the current model) is different from the true label y. Note that in the UPDATE function, C is always a singleton constraint for the perceptron, but we make it more general (as a set) to handle the batch update in the minibatch version in Section 3. On the other hand, Algorith 3 presents the k-best MIRA Algorithm of McDonald et al. (2005) which generalizes multiclass MIRA (Crammer and Singer, 2003) for structured prediction. The decoder now 371 Algorithm 2 Perceptron (Collins, 2002). 1: function FINDCONSTRAINTS(x, y, w) 2: z ← argmaxsEY(x) w · Φ(x, s) . decoding 3: if z =6 y then return {hx, y, zi} 4: else return ∅ 5: procedure UPDATE(w, C) 6: w ← w + |c |EcEC ΔΦ(c) . (batch) update Algorithm 3 k-best MIRA (McDonald et al., 2005). finds the k-best solutions Z first, and returns a set of violating constraints in Z, The update in MIRA is more interesting: it searches for the new model w&apos; with minimum change from the current mode</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Keith Hall</author>
<author>Gideon Mann</author>
</authors>
<title>Distributed training strategies for the structured perceptron.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<contexts>
<context position="2060" citStr="McDonald et al., 2010" startWordPosition="312" endWordPosition="315">o their batch learning counterparts, online learning methods offer faster convergence rates and better scalability to large datasets, while using much less memory and a much simpler architecture which only needs 1-best or k-best decoding. However, online learning for NLP typically involves expensive inference on each example for 10 or more passes over millions of examples, which often makes training too slow in practice; for example systems such as the popular (2nd-order) MST parser (McDonald and Pereira, 2006) usually require the order of days to train on the Treebank on a commodity machine (McDonald et al., 2010). There are mainly two ways to address this scalability problem. On one hand, researchers have been developing modified learning algorithms that allow inexact search (Collins and Roark, 2004; Huang et al., 2012). However, the learner still needs to loop over the whole training data (on the order of millions of sentences) many times. For example the best-performing method in Huang et al. (2012) still requires 5-6 hours to train a very fast parser. On the other hand, with the increasing popularity of multicore and cluster computers, there is a growing interest in speeding up training via paralle</context>
<context position="3339" citStr="McDonald et al. (2010)" startWordPosition="525" endWordPosition="528">l., 2001) is often trivially parallelizable (Chu et al., 2007) since each update is a batch-aggregate of the update from each (independent) example, online learning is much harder to parallelize due to the dependency between examples, i.e., the update on the first example should in principle influence the decoding of all remaining examples. Thus if we decode and update the first and the 1000th examples in parallel, we lose their interactions which is one of the reasons for online learners’ fast convergence. This explains why previous work such as the iterative parameter mixing (IPM) method of McDonald et al. (2010) witnesses a decrease in the accuracies of parallelly-learned models, and the speedup is typically very small (about 3 in their experiments) even with 10+ processors. We instead explore the idea of “minibatch” for online large-margin structured learning such as perceptron and MIRA. We argue that minibatch is advantageous in both serial and parallel settings. First, for minibatch perceptron in the serial set370 Proceedings of NAACL-HLT 2013, pages 370–379, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics ting, our intuition is that, although decoding is done in</context>
<context position="6432" citStr="McDonald et al., 2010" startWordPosition="1027" endWordPosition="1030">ddition, when combined with simple (synchronous) parallelization, minibatch MIRA 1This is similar to Pegasos (Shalev-Shwartz et al., 2007) that applies subgradient descent over a minibatch. Pegasos becomes pure online when the minibatch size is 1. Algorithm 1 Generic Online Learning. Input: data D = {(x(t), y(t))}t1 and feature map Φ Output: weight vector w 1: repeat 2: for each example (x, y) in D do 3: C — FINDCONSTRAINTS(x, y, w) &gt; decoding 4: if C =� 0 then UPDATE(w, C) 5: until converged leads to very significant speedups (up to 9x on 12 processors) that are much higher than that of IPM (McDonald et al., 2010) on state-of-the-art parsing and tagging systems. 2 Online Learning: Perceptron and MIRA We first present a unified framework for online large-margin learning, where perceptron and MIRA are two special cases. Shown in Algorithm 1, the online learner considers each input example (x, y) sequentially and performs two steps: 1. find the set C of violating constraints, and 2. update the weight vector w according to C. Here a triple (x, y, z) is said to be a “violating constraint” with respect to model w if the incorrect label z scores higher than (or equal to) the correct label y in w, i.e., w · 0Φ</context>
<context position="12030" citStr="McDonald et al. (2010)" startWordPosition="2118" endWordPosition="2121">tδ. As u is a unit vector, we have kwt+1k ≥ tδ. 2. On the other hand, take the norm of both sides of Eq. (1): kwt+1k2 = kwt + 1P i vik2 a 1 =kwtk2 +kP avik2 + 2 wt · Pi vi i a ≤kwtk2 + kPi avik2 + 0 (violation) 1 kwt k2 + Pi a I I vi k2 (Jensen’s) 1 ≤kwtk2 + P (radius) i aR2 =kwtk2 + R2 (Pi = a) ≤tR2 (by induction) Combining the two bounds, we have t2δ2 ≤ kwt+1k2 ≤ tR2 thus the number of minibatch updates t ≤ R2/δ2. Note that this bound is identical to that of pure online perceptron (Collins, 2002, Theorem 1) and is irrelevant to minibatch size m. The use of Jensen’s inequality is inspired by McDonald et al. (2010). 3.2 Convergence of Minibatch MIRA We also give a proof of convergence for MIRA with relaxation.2 We present the optimization problem in the UPDATE function of Algorithm 3 as a quadratic program (QP) with slack variable ξ: wt+1 ← argmin Wt+1 kwt+1 − wtk2 + ξ s.t. wt+1 · vi ≥ `i −ξ, for all(ci, `i) ∈ Ct where vi = 04)(ci) is the update vector for constraint ci. Consider the Lagrangian: L =kwt+1 − wtk2 + ξ + X |Ct |ηi(`i − w&apos; · vi − ξ) i=1 ηi ≥ 0, for 1 ≤ i ≤ |Ct|. 2Actually this relaxation is not necessary for the convergence proof. We employ it here solely to make the proof shorter. It is not</context>
<context position="14421" citStr="McDonald et al., 2010" startWordPosition="2612" endWordPosition="2615">hin a minibatch is completely independent of 373 ⊕ update update update update 2 3 4 1 update update update update 5 8 6 7 update update update update 10 12 11 9 update update update update 13 14 15 16 3 5 7 1 4 6 8 2 ⊕ update 9 11 13 15 10 12 14 16 ⊕ update 12 3 9 1 14 15 4 6 update update ⊕ ⊕ 13 16 5 8 10 11 2 7 update update 15 16 2 1 update update 10 3 4 9 update update 13 14 5 6 update update 12 11 8 7 (a) IPM (b) unbalanced (c) balanced (d) asynchronous Figure 1: Comparison of various methods for parallelizing online learning (number of processors p = 4). (a) iterative parameter mixing (McDonald et al., 2010). (b) unbalanced minibatch parallelization (minibatch size m = 8). (c) minibatch parallelization after load-balancing (within each minibatch). (d) asynchronous minibatch parallelization (Gimpel et al., 2010) (not implemented here). Each numbered box denotes the decoding of one example, and � denotes an aggregate operation, i.e., the merging of constraints after each minibatch or the mixing of weights after each iteration in IPM. Each gray shaded box denotes time wasted due to synchronization in (a)-(c) or blocking in (d). Note that in (d) at most one update can happen concurrently, making it s</context>
<context position="17815" citStr="McDonald et al. (2010)" startWordPosition="3156" endWordPosition="3159">tor w Split D into rn/m] minibatches D1 ... Dfes,/ml Split each Di into m/p groups Di,1 ... Di,m/p repeat for i +— 1... rn/m] do &gt; for each minibatch for j +— 1... m/p in parallel do Cj +— U(y,y)ED;�� FINDCONSTRAINTS(x, y, w) C +— UjCj &gt; in serial if C =� 0 then UPDATE(w, C) &gt; in serial until converged 374 5 Experiments We conduct experiments on two typical structured prediction problems: incremental dependency parsing and part-of-speech tagging; both are done on state-of-the-art baseline. We also compare our parallelized minibatch algorithm with the iterative parameter mixing (IPM) method of McDonald et al. (2010). We perform our experiments on a commodity 64-bit Dell Precision T7600 workstation with two 3.1GHz 8-core CPUs (16 processors in total) and 64GB RAM. We use Python 2.7’s multiprocessing module in all experiments.4 5.1 Dependency Parsing with MIRA We base our experiments on our dynamic programming incremental dependency parser (Huang and Sagae, 2010).5 Following Huang et al. (2012), we use max-violation update and beam size b = 8. We evaluate on the standard Penn Treebank (PTB) using the standard split: Sections 02-21 for training, and Section 22 as the held-out set (which is indeed the test-s</context>
<context position="21931" citStr="McDonald et al. (2010)" startWordPosition="3875" endWordPosition="3878"> speed up of 12 processors could easily make up for the slightly slower convergence m=1 m=4 m=16 m=24 m=32 m=48 accuracy on held-out 92.5 92.25 92 91.75 91.5 91.25 91 90.75 375 0 1 2 3 4 5 6 7 8 wall-clock time (hours) Figure 3: Parallelized minibatch is much faster than iterative parameter mixing. Top: minibatch of size 24 using 4 and 12 processors offers significant speedups over the serial minibatch and pure online baselines. Bottom: IPM with the same processors offers very small speedups. rate. So there seems to be a “sweetspot” of minibatch sizes, similar to the tipping point observed in McDonald et al. (2010) when adding more processors starts to hurt convergence. 5.1.2 Parallelized Minibatch vs. IPM In the following experiments we use minibatch size of m = 24 and run it in parallel mode on various numbers of processors (p = 2 — 12). Figure 3 (top) shows that 4 and 12 processors lead to very significant speedups over the serial minibatch and pure online baselines. For example, it takes the 12 processors only 0.66 hours to reach an accuracy of 92.35, which takes the pure online MIRA 6.9 hours, amounting to an impressive speedup of 10.5. We compare our minibatch parallelization with the iterative pa</context>
<context position="24005" citStr="McDonald et al. (2010)" startWordPosition="4233" endWordPosition="4236">among all converged models; choosing a higher accuracy would reveal even larger speedups for our methods. This figure shows that our method offers superlinear speedups with small number of processors (1 to 6), and almost linear speedups with large number of processors (8 and 12). Note that even p = 1 offers a speedup of 1.5 thanks to serial minibatch’s faster convergence; in other words, within the 9 fold speed-up at p = 12, parallelization contributes about 6 and minibatch about 1.5. By contrast, IPM only offers an almost constant speedup of around 3, which is consistent with the findings of McDonald et al. (2010) (both of their experiments show a speedup of around 3). We also try to understand where the speedup comes from. For that purpose we study intrinsic speedup, which is about the speed regardless of accuracy (see Figure 4). For our minibatch method, intrinsic speedup is the average time per iteration of a parallel run over the serial minibatch baseline. This answers the questions such as “how CPUefficient is our parallelization” or “how much CPU time is wasted”. We can see that with small number of processors (2 to 4), the efficiency, defined as Sp/p where Sp is the intrinsic speedup for p proce</context>
<context position="31219" citStr="McDonald et al. (2010)" startWordPosition="5454" endWordPosition="5457"> 7 Conclusions and Future Work We have presented a simple minibatch parallelization paradigm to speed up large-margin structured learning algorithms such as (averaged) perceptron and MIRA. Minibatch has an advantage in both serial and parallel settings, and our experiments confirmed that a minibatch size of around 16 or 24 leads to a significant speedups over the pure online baseline, and when combined with parallelization, leads to almost linear speedups for MIRA, and very significant speedups for perceptron. These speedups are significantly higher than those of iterative parameter mixing of McDonald et al. (2010) which were almost constant (3-4) in both our and their own experiments regardless of the number of processors. One of the limitations of this work is that although decoding is done in parallel, update is still done in serial and in MIRA the quadratic optimization step (Hildreth algorithm (Hildreth, 1957)) scales superlinearly with the number of constraints. This prevents us from using very large minibatches. For future work, we would like to explore parallelized quadratic optimization and larger minibatch sizes, and eventually apply it to machine translation. Acknowledgement We thank Ryan McD</context>
</contexts>
<marker>McDonald, Hall, Mann, 2010</marker>
<rawString>Ryan McDonald, Keith Hall, and Gideon Mann. 2010. Distributed training strategies for the structured perceptron. In Proceedings of NAACL, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
<author>Nathan Srebro</author>
</authors>
<title>Pegasos: Primal estimated sub-gradient solver for svm.</title>
<date>2007</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="5948" citStr="Shalev-Shwartz et al., 2007" startWordPosition="938" endWordPosition="941">our work is the first effort on large-margin methods. We make the following contributions: • Theoretically, we present a serial minibatch framework (Section 3) for online large-margin learning and prove the convergence theorems for minibatch perceptron and minibatch MIRA. • Empirically, we show that serial minibatch could speed up convergence and improve the converged accuracy for both MIRA and perceptron on state-of-the-art dependency parsing and part-of-speech tagging systems. • In addition, when combined with simple (synchronous) parallelization, minibatch MIRA 1This is similar to Pegasos (Shalev-Shwartz et al., 2007) that applies subgradient descent over a minibatch. Pegasos becomes pure online when the minibatch size is 1. Algorithm 1 Generic Online Learning. Input: data D = {(x(t), y(t))}t1 and feature map Φ Output: weight vector w 1: repeat 2: for each example (x, y) in D do 3: C — FINDCONSTRAINTS(x, y, w) &gt; decoding 4: if C =� 0 then UPDATE(w, C) 5: until converged leads to very significant speedups (up to 9x on 12 processors) that are much higher than that of IPM (McDonald et al., 2010) on state-of-the-art parsing and tagging systems. 2 Online Learning: Perceptron and MIRA We first present a unified </context>
</contexts>
<marker>Shalev-Shwartz, Singer, Srebro, 2007</marker>
<rawString>Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. 2007. Pegasos: Primal estimated sub-gradient solver for svm. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zinkevich</author>
<author>A J Smola</author>
<author>J Langford</author>
</authors>
<title>Slow learners are fast.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 22.</booktitle>
<contexts>
<context position="29713" citStr="Zinkevich et al., 2009" startWordPosition="5216" endWordPosition="5219">Percentage of time wasted due to synchronization (waiting for other processors to finish) (minibatch m = 24), which corresponds to the gray blocks in Figure 1 (b-c). The number of sentences assigned to each processor decreases with more processors, which worsens the unbalance. Our load-balancing strategy (Figure 1 (c)) alleviates this problem effectively. The communication overhead and update time are not included. 6 Related Work and Discussions Besides synchronous minibatch and iterative parameter mixing (IPM) discussed above, there is another method of asychronous minibatch parallelization (Zinkevich et al., 2009; Gimpel et al., 2010; Chiang, 2012), as in Figure 1. The key advantage of asynchronous over synchronous minibatch is that the former allows processors to remain near-constant use, while the latter wastes a significant amount of time when some processors finish earlier than others in a minibatch, as found in our experiments. Gimpel et al. (2010) show significant speedups of asychronous parallelization over synchronous minibatch on SGD and EM methods, and Chiang (2012) finds asynchronous parallelization to be much faster than IPM on MIRA for machine translation. However, asynchronous is signifi</context>
</contexts>
<marker>Zinkevich, Smola, Langford, 2009</marker>
<rawString>M. Zinkevich, A. J. Smola, and J. Langford. 2009. Slow learners are fast. In Advances in Neural Information Processing Systems 22.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>