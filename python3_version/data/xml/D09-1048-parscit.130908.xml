<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.998906">
Projecting Parameters for Multilingual Word Sense Disambiguation
</title>
<author confidence="0.993827">
Mitesh M. Khapra Sapan Shah Piyush Kedia Pushpak Bhattacharyya
</author>
<affiliation confidence="0.9997135">
Department of Computer Science and Engineering
Indian Institute of Technology, Bombay
</affiliation>
<address confidence="0.4975115">
Powai, Mumbai – 400076,
Maharashtra, India.
</address>
<email confidence="0.995385">
{miteshk,sapan,charasi,pb}@cse.iitb.ac.in
</email>
<sectionHeader confidence="0.997348" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999824068965517">
We report in this paper a way of doing Word
Sense Disambiguation (WSD) that has its ori-
gin in multilingual MT and that is cognizant
of the fact that parallel corpora, wordnets and
sense annotated corpora are scarce re-
sources. With respect to these resources, lan-
guages show different levels of readiness;
however a more resource fortunate language
can help a less resource fortunate language.
Our WSD method can be applied to a lan-
guage even when no sense tagged corpora for
that language is available. This is achieved by
projecting wordnet and corpus parameters
from another language to the language in
question. The approach is centered around a
novel synset based multilingual dictionary and
the empirical observation that within a domain
the distribution of senses remains more or less
invariant across languages. The effectiveness
of our approach is verified by doing parameter
projection and then running two different
WSD algorithms. The accuracy values of ap-
proximately 75% (F1-score) for three lan-
guages in two different domains establish the
fact that within a domain it is possible to cir-
cumvent the problem of scarcity of resources
by projecting parameters like sense distribu-
tions, corpus-co-occurrences, conceptual dis-
tance, etc. from one language to another.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999970020833334">
Currently efforts are on in India to build large scale
Machine Translation and Cross Lingual Search
systems in consortia mode. These efforts are large,
in the sense that 10-11 institutes and 6-7 languages
spanning the length and breadth of the country are
involved. The approach taken for translation is
transfer based which needs to tackle the problem of
word sense disambiguation (WSD) (Sergei et. al.,
2003). Since 90s machine learning based ap-
proaches to WSD using sense marked corpora have
gained ground (Eneko Agirre &amp; Philip Edmonds,
2007). However, the creation of sense marked cor-
pora has always remained a costly proposition.
Statistical MT has obviated the need for elaborate
resources for WSD, because WSD in SMT hap-
pens implicitly through parallel corpora (Brown et.
al., 1993). But parallel corpora too are a very cost-
ly resource.
The above situation brings out the challenges
involved in Indian language MT and CLIR. Lack
of resources coupled with the multiplicity of Indian
languages severely affects the performance of sev-
eral NLP tasks. In the light of this, we focus on the
problem of developing methodologies that reuse
resources. The idea is to do the annotation work
for one language and find ways of using them for
another language.
Our work on WSD takes place in a multilingual
setting involving Hindi (national language of India;
500 million speaker base), Marathi (20 million
speaker base), Bengali (185 million speaker base)
and Tamil (74 million speaker base). The wordnet
of Hindi and sense marked corpora of Hindi are
used for all these languages. Our methodology
rests on a novel multilingual dictionary organiza-
tion and on the idea of “parameter projection” from
Hindi to the other languages. Also the domains of
interest are tourism and health.
The roadmap of the paper is as follows. Section
2 describes related work. In section 3 we introduce
the parameters essential for domain-specific WSD.
Section 4 builds the case for parameter projection.
Section 5 introduces the Multilingual Dictionary
Framework which plays a key role in parameter
projection. Section 6 is the core of the work, where
we present parameter projection from one language
to another. Section 7 describes two WSD algo-
rithms which combine various parameters for do-
</bodyText>
<page confidence="0.987643">
459
</page>
<note confidence="0.996672">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 459–467,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.977974">
main-specific WSD. Experiments and results are
presented in sections 8 and 9. Section 10 concludes
the paper.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999209305555556">
Knowledge based approaches to WSD such as
Lesk&apos;s algorithm (Michael Lesk, 1986), Walker&apos;s
algorithm (Walker D. &amp; Amsler R., 1986), concep-
tual density (Agirre Eneko &amp; German Rigau, 1996)
and random walk algorithm (Mihalcea Rada, 2005)
essentially do Machine Readable Dictionary loo-
kup. However, these are fundamentally overlap
based algorithms which suffer from overlap sparsi-
ty, dictionary definitions being generally small in
length.
Supervised learning algorithms for WSD are
mostly word specific classifiers, e.g., WSD using
SVM (Lee et. al., 2004), Exemplar based WSD
(Ng Hwee T. &amp; Hian B. Lee, 1996) and decision
list based algorithm (Yarowsky, 1994). The re-
quirement of a large training corpus renders these
algorithms unsuitable for resource scarce languag-
es.
Semi-supervised and unsupervised algorithms
do not need large amount of annotated corpora, but
are again word specific classifiers, e.g., semi-
supervised decision list algorithm (Yarowsky,
1995) and Hyperlex (Véronis Jean, 2004)). Hybrid
approaches like WSD using Structural Semantic
Interconnections (Roberto Navigli &amp; Paolo Velar-
di, 2005) use combinations of more than one
knowledge sources (wordnet as well as a small
amount of tagged corpora). This allows them to
capture important information encoded in wordnet
(Fellbaum, 1998) as well as draw syntactic genera-
lizations from minimally tagged corpora.
At this point we state that no single existing so-
lution to WSD completely meets our requirements
of multilinguality, high domain accuracy and
good performance in the face of not-so-large
annotated corpora.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="method">
3 Parameters for WSD
</sectionHeader>
<bodyText confidence="0.998960666666667">
We discuss a number of parameters that play a
crucial role in WSD. To appreciate this, consider
the following example:
The river flows through this region to meet the sea.
The word sea is ambiguous and has three senses as
given in the Princeton Wordnet (PWN):
</bodyText>
<listItem confidence="0.978504333333333">
S1: (n) sea (a division of an ocean or a large body
of salt water partially enclosed by land)
S2: (n) ocean, sea (anything apparently limitless in
quantity or volume)
S3: (n) sea (turbulent water with swells of consi-
derable size) &amp;quot;heavy seas&amp;quot;
</listItem>
<bodyText confidence="0.999584173913043">
Our first parameter is obtained from Domain
specific sense distributions. In the above example,
the first sense is more frequent in the tourism do-
main (verified from manually sense marked tour-
ism corpora). Domain specific sense distribution
information should be harnessed in the WSD task.
The second parameter arises from the domin-
ance of senses in the domain. Senses are ex-
pressed by synsets, and we define a dominant
sense as follows:
A synset node in the wordnet hypernymy
hierarchy is called Dominant if the syn-
sets in the sub-tree below the synset are
frequently occurring in the domain cor-
pora.
A few dominant senses in the Tourism domain are
{place, country, city, area}, {body of water}, {flo-
ra, fauna}, {mode of transport} and {fine arts}. In
disambiguating a word, that sense which belongs
to the sub-tree of a domain-specific dominant
sense should be given a higher score than other
senses. The value of this parameter (0) is decided
as follows:
</bodyText>
<equation confidence="0.69813">
0 = 1; if the candidate synset is a dominant synset
0 = 0.5; if the candidate synset belongs to the sub-
tree of a dominant synset
0 = 0.001; if the candidate synset is neither a do-
</equation>
<bodyText confidence="0.9978727">
minant synset nor belongs to the sub-tree of a do-
minant synset.
Our third parameter comes from Corpus co-
occurrence. Co-occurring monosemous words as
well as already disambiguated words in the con-
text help in disambiguation. For example, the word
river appearing in the context of sea is a mono-
semous word. The frequency of co-occurrence of
river with the “water body” sense of sea is high in
the tourism domain. Corpus co-occurrence is cal-
</bodyText>
<page confidence="0.998062">
460
</page>
<bodyText confidence="0.99649435">
culated by considering the senses which occur in a
window of 10 words around a sense.
Our fourth parameter is based on the semantic
distance between any pair of synsets in terms of
the shortest path length between two synsets in the
wordnet graph. An edge in the shortest path can be
any semantic relation from the wordnet relation
repository (e.g., hypernymy, hyponymy, meronymy,
holonymy, troponymy etc.).
For nouns we do something additional over and
above the semantic distance. We take advantage of
the deeper hierarchy of noun senses in the wordnet
structure. This gives rise to our fifth and final pa-
rameter which arises out of the conceptual dis-
tance between a pair of senses. Conceptual
distance between two synsets S1 and S2 is calcu-
lated using Equation (1), motivated by Agirre Ene-
ko &amp; German Rigau (1996).
Length of the path between (S1,
S2) in terms of hypernymy hie-
</bodyText>
<equation confidence="0.948411">
rarchy (1)
</equation>
<bodyText confidence="0.964707125">
Height of the lowest common
ancestor of S1 and S2 in the word-
net hierarchy
The conceptual distance is proportional to the
path length between the synsets, as it should be.
The distance is also inversely proportional to the
height of the common ancestor of two sense nodes,
because as the common ancestor becomes more
and more general the conceptual relatedness tends
to get vacuous (e.g., two nodes being related
through entity which is the common ancestor of
EVERYTHING, does not really say anything
about the relatedness).
To summarize, our various parameters used for
domain-specific WSD are:
Wordnet-dependent parameters
</bodyText>
<listItem confidence="0.999665">
• belongingness-to-dominant-concept
• conceptual-distance
• semantic-distance
Corpus-dependent parameters
• sense distributions
• corpus co-occurrence.
</listItem>
<bodyText confidence="0.9858355">
In section 7 we show how these parameters are
used to come up with a scoring function for WSD.
</bodyText>
<sectionHeader confidence="0.944878" genericHeader="method">
4 Building a case for Parameter Projec-
tion
</sectionHeader>
<bodyText confidence="0.999919222222222">
Wordnet-dependent parameters depend on the
graph based structure of Wordnet whereas the
Corpus-dependent parameters depend on various
statistics learnt from a sense marked corpora. Both
the tasks of (a) constructing a wordnet from scratch
and (b) collecting sense marked corpora for mul-
tiple languages are tedious and expensive. An im-
portant question being addressed in this paper is:
whether the effort required in constructing seman-
tic graphs for multiple wordnets and collecting
sense marked corpora can be avoided? Our find-
ings seem to suggest that by projecting relations
from the wordnet of a language and by projecting
corpus statistics from the sense marked corpora of
the language we can achieve this end. Before we
proceed to discuss the way to realize parameter
projection, we present a novel dictionary which
facilitates this task.
</bodyText>
<sectionHeader confidence="0.986344" genericHeader="method">
5 Synset based multilingual dictionary
</sectionHeader>
<bodyText confidence="0.9999077">
Parameter projection as described in section 4 rests
on a novel and effective method of storage and use
of dictionary in a multilingual setting proposed by
Mohanty et. al. (2008). For the purpose of current
discussion, we will call this multilingual dictionary
framework MultiDict. One important departure
from traditional dictionary is that synsets are
linked, and after that the words inside the syn-
sets are linked. The basic mapping is thus be-
tween synsets and thereafter between the words.
</bodyText>
<table confidence="0.903683571428571">
Concepts L1 L2 (Hindi) L3 (Mara-
(Eng- thi)
lish)
04321: a {male {लड़का ladkaa, {मुलगाmulgaa ,
youthful child, बालक,baalak पोरगाporgaa ,
male per- boy} बच्चा पोरpor }
son bachchaa}
</table>
<tableCaption confidence="0.995085">
Table 1: Multilingual Dictionary Framework
</tableCaption>
<bodyText confidence="0.998196636363636">
Table 1 shows the structure of MultiDict, with one
example row standing for the concept of boy. The
first column is the pivot describing a concept with
a unique ID. The subsequent columns show the
words expressing the concept in respective lan-
guages (in the example table above, English, Hindi
and Marathi). Thus to express the concept „04321:
a youthful male person‟, there are two lexical ele-
ments in English, which constitute a synset. Cor-
respondingly, the Hindi and Marathi synsets
contain 3 words each.
</bodyText>
<figure confidence="0.734385">
Concep-
tual
Distance
(S1, S2)
</figure>
<page confidence="0.99451">
461
</page>
<bodyText confidence="0.999952923076923">
It may be noted that the central language whose
synsets the synsets of other languages link to is
Hindi. This way of linking synsets- more popularly
known as the expansion approach- has several ad-
vantages as discussed in (Mohanty et. al., 2008).
One advantage germane to the point of this paper
is that the synsets in a particular column automati-
cally inherit the various semantic relations of the
Hindi wordnet (Dipak Narayan et. al., 2000),
which saves the effort involved in reconstructing
these relations for multiple languages.
After the synsets are linked, cross linkages are
set up manually from the words of a synset to the
words of a linked synset of the central language.
The average number of such links per synset per
language pair is approximately 3. These cross-
linkages actually solve the problem of lexical
choice in translating from text of one language to
another.
Thus for the Marathi word मुलगा {mulagaa} de-
noting “a youthful male person”, the correct lexi-
cal substitute from the corresponding Hindi synset
is लड़का {ladakaa} (Figure 1). One might argue that
any word within the synset could serve the purpose
of translation. However, the exact lexical substitu-
tion has to respect native speaker acceptability.
</bodyText>
<figure confidence="0.334387">
Marathi Synset Hindi Synset English Synset
</figure>
<figureCaption confidence="0.9418755">
Figure 1: Cross linked synset members for the
concept: a youthful male person
</figureCaption>
<bodyText confidence="0.9999495">
We put these cross linkages to another use, as
described later.
Since it is the MultiDict which is at the heart of
parameter projection, we would like to summarize
the main points of this section. (1) By linking with
the synsets of Hindi, the cost of building wordnets
of other languages is partly reduced (semantic rela-
tions are inherited). The wordnet parameters of
Hindi wordnet now become projectable to other
languages. (2) By using the cross linked words in
the synsets, corpus parameters become projectable
(vide next section).
</bodyText>
<sectionHeader confidence="0.983167" genericHeader="method">
6 Parameter projection using MultDict
</sectionHeader>
<subsectionHeader confidence="0.990226">
6.1 P(Sense|Word) parameter
</subsectionHeader>
<bodyText confidence="0.9862166">
Suppose a word (say, W) in language L1 (say, Ma-
rathi) has k senses. For each of these k senses we
are interested in finding the parameter P(Si|W)-
which is the probability of sense Si given the word
W expressed as:
</bodyText>
<equation confidence="0.774422">
P (Si l W) = j #(Sj , W)
</equation>
<bodyText confidence="0.9917148">
where `#‟ indicates `count-of‟. Consider the exam-
ple of two senses of the Marathi word सागर
{saagar}, viz., sea and abundance and the corres-
ponding cross-linked words in Hindi (Figure 2 be-
low):
</bodyText>
<figure confidence="0.968546333333333">
Marathi Hindi
saagar (sea)
{abundance}
</figure>
<figureCaption confidence="0.996429333333333">
Figure 2: Two senses of the Marathi word सागर
(saagar), viz., {water body} and {abundance}, and
the corresponding cross-linked words in Hindi1.
</figureCaption>
<bodyText confidence="0.9307785">
The probability P({water body}|saagar) for Mara-
thi is
</bodyText>
<equation confidence="0.633928">
#({water body}, saagar)
#({water body}, saagar) + #({abundance},saagar)
</equation>
<bodyText confidence="0.9998995">
We propose that this can be approximated by the
counts from Hindi sense marked corpora by replac-
ing saagar with the cross linked Hindi words sa-
mudra and saagar, as per Figure 2:
</bodyText>
<footnote confidence="0.830941">
#({water body}, samudra)
#({water body}, samudra) + #({abundance},saagar)
1 Sense_8231 shows the same word saagar for both Marathi
and Hindi. This is not uncommon, since Marathi and Hindi are
sister languages.
</footnote>
<figure confidence="0.999054266666667">
मुलगा
/MW1
mulagaa,
पोरगा
/MW2
poragaa,
पोर /MW3
pora
लड़का
/HW1
ladakaa,
बालक
/HW2
baalak,
बच्चा /HW3
bachcha,
छोरा /HW4
choraa
male-child
/HW1,
boy
/HW2
saagar (sea)
{water body}
samudra (sea)
{water body}
Sense_2650
#(Si , W)
Sense_8231 saagar (sea)
{abundance}
</figure>
<page confidence="0.998467">
462
</page>
<bodyText confidence="0.9989355">
Thus, the following formula is used for calculat-
ing the sense distributions of Marathi words using
the sense marked Hindi corpus from the same do-
main:
</bodyText>
<equation confidence="0.393894">
#(Si , cross_linked_hindi_word)
j #(Sj , cross_linked_hindi_word)
</equation>
<bodyText confidence="0.999384384615385">
Note that we are not interested in the exact sense
distribution of the words, but only in the relative
sense distribution.
To prove that the projected relative distribution
is faithful to the actual relative distribution of
senses, we obtained the sense distribution statistics
of a set of Marathi words from a sense tagged Ma-
rathi corpus (we call the sense marked corpora of a
language its self corpora). These sense distribu-
tion statistics were compared with the statistics for
these same words obtained by projecting from a
sense tagged Hindi corpus using Equation (2). The
results are summarized in Table 2.
</bodyText>
<table confidence="0.999613">
Sr. Marathi Synset P(S|word) P(S|word) as
No Word as learnt projected
from from sense
sense tagged
tagged Hindi cor-
Marathi pus
corpus
1 ककिं मत { worth } 0.684 0.714
(kimat)
{ price } 0.315 0.285
2 रस्ता { roadway } 0.164 0.209
(rasta)
{road, 0.835 0.770
route}
3 ठिकाण { land site, 0.962 0.878
(thikan) place}
{ home } 0.037 0.12
4 सागर {water 1.00 1.00
(saagar) body}
{abun- 0 0
dance}
</table>
<tableCaption confidence="0.995461">
Table 2: Comparison of the sense distributions of
</tableCaption>
<bodyText confidence="0.990493962962963">
some Marathi words learnt from Marathi sense
tagged corpus with those projected from Hindi
sense tagged corpus.
The fourth row of Table 2 shows that whenever
सागर (saagar) (sea) appears in the Marathi tourism
corpus there is a 100% chance that it will appear in
the “water body” sense and 0% chance that it will
appear in the sense of “abundance”. Column 5
shows that the same probability values are ob-
tained using projections from Hindi tourism cor-
pus. Taking another example, the third row shows
that whenever ठिकाण (thikaan) (place, home) ap-
pears in the Marathi tourism corpus there is a much
higher chance of it appearing in the sense of
“place” (96.2%) then in the sense of “home”
(3.7%). Column 5 shows that the relative proba-
bilities of the two senses remain the same even
when using projections from Hindi tourism corpus
(i.e. by using the corresponding cross-linked words
in Hindi). To quantify these observations, we cal-
culated the average KL divergence and Spearman‟s
correlation co-efficient between the two distribu-
tions. The KL divergence is 0.766 and Spearman‟s
correlation co-efficient is 0.299. Both these values
indicate that there is a high degree of similarity
between the distributions learnt using projection
and those learnt from the self corpus.
</bodyText>
<subsectionHeader confidence="0.998894">
6.2 Co-occurrence parameter
</subsectionHeader>
<bodyText confidence="0.998772444444444">
Similarly, within a domain, the statistics of co-
occurrence of senses remain the same across lan-
guages. For example, the co-occurrence of the Ma-
rathi synsets {आकाव (akash) (sky), अिंबर (ambar)
(sky)} and {मेघ (megh) (cloud), अभ्र (abhra)
(cloud)} in the Marathi corpus remains more or
less same as (or proportional to) the co-occurrence
between the corresponding Hindi synsets in the
Hindi corpus.
</bodyText>
<table confidence="0.997798380952381">
Sr. No Synset Co- P(co- P(co-
occurring occurrence) occurrence)
Synset as learnt as learnt
from sense from sense
tagged tagged
Marathi Hindi
corpus corpus
1 {रोप, रोपटे} {झाड, ऴृक्ष, 0.125 0.125
{small bush} तरुऴर, द्रुम,
तरू, पादप}
{tree}
2 {मेघ, अभ्र} {आकाव, 0.167 0.154
{cloud} आभाळ,
अिंबर}
{sky}
3 {क्षेत्र, इऱाका, {यात्रा, 0.0019 0.0017
इऱाका, सफ़र}
भ{travel}
ूखंड}
{geographical
area}
</table>
<tableCaption confidence="0.9543025">
Table 3: Comparison of the corpus co-occurrence
statistics learnt from Marathi and Hindi Tourism
</tableCaption>
<equation confidence="0.693422333333333">
corpus.
P(Si1W) =
(2)
</equation>
<page confidence="0.997993">
463
</page>
<bodyText confidence="0.999563875">
Table 3 shows a few examples depicting similarity
between co-occurrence statistics learnt from Mara-
thi tourism corpus and Hindi tourism corpus. Note
that we are talking about co-occurrence of synsets
and not words. For example, the second row shows
that the probability of co-occurrence of the synsets
{cloud} and {sky} is almost same in the Marathi
and Hindi corpus.
</bodyText>
<sectionHeader confidence="0.972379" genericHeader="method">
7 Our algorithms for WSD
</sectionHeader>
<bodyText confidence="0.999878">
We describe two algorithms to establish the use-
fulness of the idea of parameter projection. The
first algorithm- called iterative WSD (IWSD-) is
greedy, and the second based on PageRank algo-
rithm is exhaustive. Both use scoring functions that
make use of the parameters detailed in the previous
sections.
</bodyText>
<subsectionHeader confidence="0.868152">
7.1 Iterative WSD (IWSD)
</subsectionHeader>
<bodyText confidence="0.9984079">
We have been motivated by the Energy expression
in Hopfield network (Hopfield, 1982) in formulat-
ing a scoring function for ranking the senses. Hop-
field Network is a fully connected bidirectional
symmetric network of bi-polar (0/1 or +1/-1) neu-
rons. We consider the asynchronous Hopfield
Network. At any instant, a randomly chosen neu-
ron (a) examines the weighted sum of the input, (b)
compares this value with a threshold and (c) gets to
the state of 1 or 0, depending on whether the input
is greater than or less than or equal to the thre-
shold. The assembly of 0/1 states of individual
neurons defines a state of the whole network. Each
state has associated with it an energy, E, given by
the following expression
the global macroscopic property of energy of the
network. This fact has been the primary insight for
equation (4) which was proposed to score the most
appropriate synset in the given context. The cor-
respondences are as follows:
</bodyText>
<table confidence="0.972402375">
Neuron 4 Synset
Self-activation 4 Corpus Sense Distribu-
tion
Weight of connec- Weight as a function of
tion between two 4 corpus co-occurrence
neurons and Wordnet distance
measures between syn-
sets
</table>
<equation confidence="0.843664222222222">
S* = argmax ei*Vi+ IWij * Vi* Vj 4
i j c J
where,
J = Set of disambiguated Words
Bi = BelongingnessToDominantConcept (Si)
Vi = P(Si  |word)
Wij = CorpusCooccurences (Si, Sj )
* 1/WNConceptualDistance (Si, Sj )
* 1/WNSemanticGraphDistance(Si,Sj)
</equation>
<bodyText confidence="0.999737363636364">
The component ei * Vi of the energy due to the self
activation of a neuron can be compared to the cor-
pus specific sense of a word in a domain. The other
component wij * Vi * Vj coming from the interaction
of activations can be compared to the score of a
sense due to its interaction in the form of corpus
co-occurrence, conceptual distance, and wordnet-
based semantic distance with the senses of other
words in the sentence. The first component thus
captures the rather static corpus sense, whereas the
second expression brings in the sentential context.
</bodyText>
<equation confidence="0.951313666666667">
N N
E = −eiVi + I I Wij Vi Vj (3)
j &gt;i
</equation>
<bodyText confidence="0.9998254">
where, N is the total number of neurons in the net-
work, Vi and Vj are the activations of neurons i and
j respectively and Wij is the weight of the connec-
tion between neurons i and j. Energy is a funda-
mental property of Hopfield networks, providing
the necessary machinery for discussing conver-
gence, stability and such other considerations.
The energy expression as given above cleanly
separates the influence of self-activations of neu-
rons and that of interactions amongst neurons to
</bodyText>
<listItem confidence="0.939797857142857">
Algorithm 1: performIterativeWSD(sentence)
1. Tag all monosemous words in the sentence.
2. Iteratively disambiguate the remaining words in the
sentence in increasing order of their degree of polyse-
my.
3. At each stage select that sense for a word which max-
imizes the score given by Equation (4)
</listItem>
<sectionHeader confidence="0.785247" genericHeader="method">
Algorithm1: Iterative WSD
</sectionHeader>
<bodyText confidence="0.9997686">
IWSD is clearly a greedy algorithm. It bases its
decisions on already disambiguated words, and
ignores words with higher degree of polysemy. For
example, while disambiguating bisemous words,
the algorithm uses only the monosemous words.
</bodyText>
<equation confidence="0.786745">
i=1
</equation>
<page confidence="0.998101">
464
</page>
<subsectionHeader confidence="0.990092">
7.2 Modified PageRank algorithm
</subsectionHeader>
<bodyText confidence="0.959295333333333">
Rada Mihalcea (2005) proposed the idea of using
PageRank algorithm to find the best combination
of senses in a sense graph. The nodes in a sense
graph correspond to the senses of all the words in a
sentence and the edges depict the strength of inte-
raction between senses. The score of each node in
the graph is then calculated using the following
recursive formula:
Wij
</bodyText>
<subsectionHeader confidence="0.911567">
SjEIn (Si SkEOut (Si Wjk
</subsectionHeader>
<bodyText confidence="0.983276">
Instead of calculating Wij based on the overlap
between the definition of senses Si and S as pro-
posed by Rada Mihalcea (2005), we calculate the
edge weights using the following formula:
</bodyText>
<equation confidence="0.967915166666667">
Wij = CorpusCooccurences (Si, Sj )
* 1/WNConceptualDistance(Si,Sj)
* 1/WNSemanticGrapDistance(Si,Sj)
* P(Si  |wordi)
* P(Sj  |wordj)
d = damping factor (typically 0.85)
</equation>
<bodyText confidence="0.999881142857143">
This formula helps capture the edge weights in
terms of the corpus bias as well as the interaction
between the senses in the corpus and wordnet. It
should be noted that this algorithm is not greedy.
Unlike IWSD, this algorithm allows all the senses
of all words to play a role in the disambiguation
process.
</bodyText>
<sectionHeader confidence="0.985237" genericHeader="method">
8 Experimental Setup:
</sectionHeader>
<bodyText confidence="0.9995145">
We tested our algorithm on tourism corpora in 3
languages (viz., Marathi, Bengali and Tamil) and
health corpora in 1 language (Marathi) using pro-
jections from Hindi. The corpora for both the do-
mains were manually sense tagged. A 4-fold cross
validation was done for all the languages in both
the domains. The size of the corpus for each lan-
guage is described in Table 4.
</bodyText>
<table confidence="0.995778625">
Language # of polysemous words
(tokens)
Tourism Health
Domain Domain
Hindi 50890 29631
Marathi 32694 8540
Bengali 9435 -
Tamil 17868 -
</table>
<tableCaption confidence="0.994065">
Table 4: Size of manually sense tagged corpora for
different languages.
Table 5 shows the number of synsets in MultiDict
</tableCaption>
<table confidence="0.909488857142857">
for each language.
Language # of synsets in
MultiDict
Hindi 29833
Marathi 16600
Bengali 10732
Tamil 5727
</table>
<tableCaption confidence="0.987135">
Table 5: Number of synsets for each language
</tableCaption>
<table confidence="0.995861133333333">
score(Si =
1 − d)+ d *
* Score(Sj
Algorithm Language
Marathi Bengali
P % R % F % P % R % F %
IWSD (training on self corpora; no parameter pro- 81.29 80.42 80.85 81.62 78.75 79.94
jection)
IWSD (training on Hindi and reusing parameters 73.45 70.33 71.86 79.83 79.65 79.79
for another language)
PageRank (training on self corpora; no parameter 79.61 79.61 79.61 76.41 76.41 76.41
projection)
PageRank (training on Hindi and reusing parame- 71.11 71.11 71.11 75.05 75.05 75.05
ters for another language)
Wordnet Baseline 58.07 58.07 58.07 52.25 52.25 52.25
</table>
<tableCaption confidence="0.995097">
Table 6: Precision, Recall and F-scores of IWSD, PageRank and Wordnet Baseline. Values are re-
ported with and without parameter projection.
</tableCaption>
<page confidence="0.999478">
465
</page>
<sectionHeader confidence="0.998784" genericHeader="method">
9 Results and Discussions
</sectionHeader>
<bodyText confidence="0.999986192307692">
Table 6 shows the results of disambiguation (preci-
sion, recall and F-score). We give values for two
algorithms in the tourism domain: IWSD and Pa-
geRank. In each case figures are given for both
with and without parameter projection. The word-
net baseline figures too are presented for the sake
of grounding the results.
Note the lines of numbers in bold, and compare
them with the numbers in the preceding line. This
shows the fall in accuracy value when one tries the
parameter projection approach in place of self cor-
pora. For example, consider the F-score as given
by IWSD for Marathi. It degrades from about 81%
to 72% in using parameter projection in place of
self corpora. Still, the value is much more than the
baseline, viz., the wordnet first sense (a typically
reported baseline).
Coming to PageRank for Marathi, the fall in ac-
curacy is about 8%. Appendix A shows the corres-
ponding figure for Tamil with IWSD as 10%.
Appendix B reports the fall to be 11% for a differ-
ent domain- Health- for Marathi (using IWSD).
In all these cases, even after degradation the per-
formance is far above the wordnet baseline. This
shows that one could trade accuracy with the cost
of creating sense annotated corpora.
</bodyText>
<sectionHeader confidence="0.98883" genericHeader="method">
10 Conclusion and Future Work:
</sectionHeader>
<bodyText confidence="0.99837596">
Based on our study for 3 languages and 2 domains,
we conclude the following:
(i) Domain specific sense distributions- if
obtainable- can be exploited to advantage.
(ii) Since sense distributions remain same across
languages, it is possible to create a disambiguation
engine that will work even in the absence of sense
tagged corpus for some resource deprived
language, provided (a) there are aligned and cross
linked sense dictionaries for the language in
question and another resource rich language, (b)
the domain in which disambiguation needs to be
performed for the resource deprived language is
the same as the domain for which sense tagged
corpora is available for the resource rich language.
(iii) Provided the accuracy reduction is not drastic,
it may make sense to trade high accuracy for the
effort in collecting sense marked corpora.
It would be interesting to test our algorithm on
other domains and other languages to conclusively
establish the effectiveness of parameter projection
for multilingual WSD.
It would also be interesting to analyze the con-
tribution of corpus and wordnet parameters inde-
pendently.
</bodyText>
<sectionHeader confidence="0.998603" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.99226925">
Agirre Eneko &amp; German Rigau. 1996. Word sense dis-
ambiguation using conceptual density. In Proceed-
ings of the 16th International Conference on
Computational Linguistics (COLING), Copenhagen,
Denmark.
Dipak Narayan, Debasri Chakrabarti, Prabhakar Pande
and P. Bhattacharyya. 2002. An Experience in Build-
ing the Indo WordNet - a WordNet for Hindi. First
International Conference on Global WordNet, My-
sore, India.
Eneko Agirre &amp; Philip Edmonds. 2007. Word Sense
Disambiguation Algorithms and Applications. Sprin-
ger Publications.
Fellbaum, C. 1998. WordNet: An Electronic Lexical
Database. The MIT Press.
Hindi Wordnet.
http://www.cfilt.iitb.ac.in/wordnet/webhwn/
J. J. Hopfield. April 1982. &amp;quot;Neural networks and physi-
cal systems with emergent collective computational
abilities&amp;quot;, Proceedings of the National Academy of
Sciences of the USA, vol. 79 no. 8 pp. 2554-2558.
Lee Yoong K., Hwee T. Ng &amp; Tee K. Chia. 2004. Su-
pervised word sense disambiguation with support
vector machines and multiple knowledge sources.
Proceedings of Senseval-3: Third International
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text, Barcelona, Spain, 137-140.
Lin Dekang. 1997. Using syntactic dependency as local
context to resolve word sense ambiguity. In Proceed-
ings of the 35th Annual Meeting of the Association
for Computational Linguistics (ACL), Madrid, 64-71.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a
pine cone from an ice cream cone. In Proceedings of
the 5th annual international conference on Systems
documentation, Toronto, Ontario, Canada.
Mihalcea Rada. 2005. Large vocabulary unsupervised
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
the Joint Human Language Technology and Empiri-
</reference>
<page confidence="0.995336">
466
</page>
<reference confidence="0.999855934782608">
cal Methods in Natural Language Processing Confe-
rence (HLT/EMNLP), Vancouver, Canada, 411-418.
Ng Hwee T. &amp; Hian B. Lee. 1996. Integrating multiple
knowledge sources to disambiguate word senses: An
exemplar-based approach. In Proceedings of the 34th
Annual Meeting of the Association for Computation-
al Linguistics (ACL), Santa Cruz, U.S.A., 40-47.
Peter F. Brown and Vincent J.Della Pietra and Stephen
A. Della Pietra and Robert. L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics Vol
19, 263-311.
Rajat Mohanty, Pushpak Bhattacharyya, Prabhakar
Pande, Shraddha Kalele, Mitesh Khapra and Aditya
Sharma. 2008. Synset Based Multilingual Dictionary:
Insights, Applications and Challenges. Global Word-
net Conference, Szeged, Hungary, January 22-25.
Resnik Philip. 1997. Selectional preference and sense
disambiguation. In Proceedings of ACL Workshop
on Tagging Text with Lexical Semantics, Why, What
and How? Washington, U.S.A., 52-57.
Roberto Navigli, Paolo Velardi. 2005. Structural Se-
mantic Interconnections: A Knowledge-Based Ap-
proach to Word Sense Disambiguation. IEEE
Transactions On Pattern Analysis and Machine Intel-
ligence.
Sergei Nirenburg, Harold Somers, and Yorick Wilks.
2003. Readings in Machine Translation. Cambridge,
MA: MIT Press.
Véronis Jean. 2004. HyperLex: Lexical cartography for
information retrieval. Computer Speech &amp; Language,
18(3):223-252.
Walker D. and Amsler R. 1986. The Use of Machine
Readable Dictionaries in Sublanguage Analysis. In
Analyzing Language in Restricted Domains, Grish-
man and Kittredge (eds), LEA Press, pp. 69-83.
Yarowsky David. 1994. Decision lists for lexical ambi-
guity resolution: Application to accent restoration in
Spanish and French. In Proceedings of the 32nd An-
nual Meeting of the association for Computational
Linguistics (ACL), Las Cruces, U.S.A., 88-95.
Yarowsky David. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics (ACL),
Cambridge, MA, 189-196.
</reference>
<sectionHeader confidence="0.7003815" genericHeader="method">
Appendix A: Results for Tamil (Tourism
Domain)
</sectionHeader>
<table confidence="0.991816">
Algorithm P % R % F %
IWSD (training on 89.50 88.18 88.83
Tamil)
IWSD (training on 84.60 73.79 78.82
Hindi and reusing for
Tamil)
Wordnet Baseline 65.62 65.62 65.62
</table>
<tableCaption confidence="0.9815785">
Table 7: Tamil Tourism corpus using parameters
projected from Hindi
</tableCaption>
<sectionHeader confidence="0.87521" genericHeader="method">
Appendix B: Results for Marathi (Health
Domain)
</sectionHeader>
<table confidence="0.648315142857143">
Algorithm P % R % F %
Words
IWSD (training on Mara- 84.28 81.25 82.74
thi)
IWSD (training on Hindi 75.96 67.75 71.62
and reusing for Marathi)
Wordnet Baseline 60.32 60.32 60.32
</table>
<tableCaption confidence="0.957175">
Table 8: Marathi Health corpus parameters pro-
jected from Hindi
</tableCaption>
<page confidence="0.998673">
467
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.705525">
<title confidence="0.999963">Projecting Parameters for Multilingual Word Sense Disambiguation</title>
<author confidence="0.999767">Mitesh M Khapra Sapan Shah Piyush Kedia Pushpak Bhattacharyya</author>
<affiliation confidence="0.999984">Department of Computer Science and Indian Institute of Technology,</affiliation>
<address confidence="0.8444475">Mumbai Maharashtra,</address>
<email confidence="0.992967">miteshk@cse.iitb.ac.in</email>
<email confidence="0.992967">sapan@cse.iitb.ac.in</email>
<email confidence="0.992967">charasi@cse.iitb.ac.in</email>
<email confidence="0.992967">pb@cse.iitb.ac.in</email>
<abstract confidence="0.9990156">We report in this paper a way of doing Word Sense Disambiguation (WSD) that has its origin in multilingual MT and that is cognizant the fact that corpora, wordnets sense annotated corpora are scarce rerespect to these resources, languages show different levels of readiness; however a more resource fortunate language help a less resource fortunate Our WSD method can be applied to a language even when no sense tagged corpora for that language is available. This is achieved by projecting wordnet and corpus parameters from another language to the language in approach is centered around a novel synset based multilingual dictionary and the empirical observation that within a domain the distribution of senses remains more or less invariant across languages. The effectiveness of our approach is verified by doing parameter projection and then running two different WSD algorithms. The accuracy values of approximately 75% (F1-score) for three languages in two different domains establish the fact that within a domain it is possible to circumvent the problem of scarcity of resources by projecting parameters like sense distributions, corpus-co-occurrences, conceptual disone language to another.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Agirre Eneko</author>
<author>German Rigau</author>
</authors>
<title>Word sense disambiguation using conceptual density.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING),</booktitle>
<location>Copenhagen, Denmark.</location>
<marker>Eneko, Rigau, 1996</marker>
<rawString>Agirre Eneko &amp; German Rigau. 1996. Word sense disambiguation using conceptual density. In Proceedings of the 16th International Conference on Computational Linguistics (COLING), Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipak Narayan</author>
<author>Debasri Chakrabarti</author>
<author>Prabhakar Pande</author>
<author>P Bhattacharyya</author>
</authors>
<title>An Experience in Building the Indo WordNet - a WordNet for Hindi.</title>
<date>2002</date>
<booktitle>First International Conference on Global WordNet, Mysore,</booktitle>
<marker>Narayan, Chakrabarti, Pande, Bhattacharyya, 2002</marker>
<rawString>Dipak Narayan, Debasri Chakrabarti, Prabhakar Pande and P. Bhattacharyya. 2002. An Experience in Building the Indo WordNet - a WordNet for Hindi. First International Conference on Global WordNet, Mysore, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Philip Edmonds</author>
</authors>
<title>Word Sense Disambiguation Algorithms and Applications.</title>
<date>2007</date>
<publisher>Springer Publications.</publisher>
<marker>Agirre, Edmonds, 2007</marker>
<rawString>Eneko Agirre &amp; Philip Edmonds. 2007. Word Sense Disambiguation Algorithms and Applications. Springer Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="5447" citStr="Fellbaum, 1998" startWordPosition="841" endWordPosition="842">training corpus renders these algorithms unsuitable for resource scarce languages. Semi-supervised and unsupervised algorithms do not need large amount of annotated corpora, but are again word specific classifiers, e.g., semisupervised decision list algorithm (Yarowsky, 1995) and Hyperlex (Véronis Jean, 2004)). Hybrid approaches like WSD using Structural Semantic Interconnections (Roberto Navigli &amp; Paolo Velardi, 2005) use combinations of more than one knowledge sources (wordnet as well as a small amount of tagged corpora). This allows them to capture important information encoded in wordnet (Fellbaum, 1998) as well as draw syntactic generalizations from minimally tagged corpora. At this point we state that no single existing solution to WSD completely meets our requirements of multilinguality, high domain accuracy and good performance in the face of not-so-large annotated corpora. 3 Parameters for WSD We discuss a number of parameters that play a crucial role in WSD. To appreciate this, consider the following example: The river flows through this region to meet the sea. The word sea is ambiguous and has three senses as given in the Princeton Wordnet (PWN): S1: (n) sea (a division of an ocean or </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, C. 1998. WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hindi Wordnet</author>
</authors>
<note>http://www.cfilt.iitb.ac.in/wordnet/webhwn/</note>
<marker>Wordnet, </marker>
<rawString>Hindi Wordnet. http://www.cfilt.iitb.ac.in/wordnet/webhwn/</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Hopfield</author>
</authors>
<title>Neural networks and physical systems with emergent collective computational abilities&amp;quot;,</title>
<date>1982</date>
<booktitle>Proceedings of the National Academy of Sciences of the USA,</booktitle>
<volume>79</volume>
<pages>2554--2558</pages>
<contexts>
<context position="19409" citStr="Hopfield, 1982" startWordPosition="3135" endWordPosition="3136">synsets and not words. For example, the second row shows that the probability of co-occurrence of the synsets {cloud} and {sky} is almost same in the Marathi and Hindi corpus. 7 Our algorithms for WSD We describe two algorithms to establish the usefulness of the idea of parameter projection. The first algorithm- called iterative WSD (IWSD-) is greedy, and the second based on PageRank algorithm is exhaustive. Both use scoring functions that make use of the parameters detailed in the previous sections. 7.1 Iterative WSD (IWSD) We have been motivated by the Energy expression in Hopfield network (Hopfield, 1982) in formulating a scoring function for ranking the senses. Hopfield Network is a fully connected bidirectional symmetric network of bi-polar (0/1 or +1/-1) neurons. We consider the asynchronous Hopfield Network. At any instant, a randomly chosen neuron (a) examines the weighted sum of the input, (b) compares this value with a threshold and (c) gets to the state of 1 or 0, depending on whether the input is greater than or less than or equal to the threshold. The assembly of 0/1 states of individual neurons defines a state of the whole network. Each state has associated with it an energy, E, giv</context>
</contexts>
<marker>Hopfield, 1982</marker>
<rawString>J. J. Hopfield. April 1982. &amp;quot;Neural networks and physical systems with emergent collective computational abilities&amp;quot;, Proceedings of the National Academy of Sciences of the USA, vol. 79 no. 8 pp. 2554-2558.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee Yoong K</author>
<author>Hwee T Ng</author>
<author>Tee K Chia</author>
</authors>
<title>Supervised word sense disambiguation with support vector machines and multiple knowledge sources.</title>
<date>2004</date>
<booktitle>Proceedings of Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>137--140</pages>
<location>Barcelona,</location>
<marker>K, Ng, Chia, 2004</marker>
<rawString>Lee Yoong K., Hwee T. Ng &amp; Tee K. Chia. 2004. Supervised word sense disambiguation with support vector machines and multiple knowledge sources. Proceedings of Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, 137-140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Dekang</author>
</authors>
<title>Using syntactic dependency as local context to resolve word sense ambiguity.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>64--71</pages>
<location>Madrid,</location>
<marker>Dekang, 1997</marker>
<rawString>Lin Dekang. 1997. Using syntactic dependency as local context to resolve word sense ambiguity. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL), Madrid, 64-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the 5th annual international conference on Systems documentation,</booktitle>
<location>Toronto, Ontario, Canada.</location>
<contexts>
<context position="4227" citStr="Lesk, 1986" startWordPosition="661" endWordPosition="662"> Dictionary Framework which plays a key role in parameter projection. Section 6 is the core of the work, where we present parameter projection from one language to another. Section 7 describes two WSD algorithms which combine various parameters for do459 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 459–467, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP main-specific WSD. Experiments and results are presented in sections 8 and 9. Section 10 concludes the paper. 2 Related work Knowledge based approaches to WSD such as Lesk&apos;s algorithm (Michael Lesk, 1986), Walker&apos;s algorithm (Walker D. &amp; Amsler R., 1986), conceptual density (Agirre Eneko &amp; German Rigau, 1996) and random walk algorithm (Mihalcea Rada, 2005) essentially do Machine Readable Dictionary lookup. However, these are fundamentally overlap based algorithms which suffer from overlap sparsity, dictionary definitions being generally small in length. Supervised learning algorithms for WSD are mostly word specific classifiers, e.g., WSD using SVM (Lee et. al., 2004), Exemplar based WSD (Ng Hwee T. &amp; Hian B. Lee, 1996) and decision list based algorithm (Yarowsky, 1994). The requirement of a l</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In Proceedings of the 5th annual international conference on Systems documentation, Toronto, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihalcea Rada</author>
</authors>
<title>Large vocabulary unsupervised word sense disambiguation with graph-based algorithms for sequence data labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the Joint Human Language Technology and Empirical Methods in Natural Language Processing Conference (HLT/EMNLP),</booktitle>
<pages>411--418</pages>
<location>Vancouver, Canada,</location>
<contexts>
<context position="4381" citStr="Rada, 2005" startWordPosition="685" endWordPosition="686">nguage to another. Section 7 describes two WSD algorithms which combine various parameters for do459 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 459–467, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP main-specific WSD. Experiments and results are presented in sections 8 and 9. Section 10 concludes the paper. 2 Related work Knowledge based approaches to WSD such as Lesk&apos;s algorithm (Michael Lesk, 1986), Walker&apos;s algorithm (Walker D. &amp; Amsler R., 1986), conceptual density (Agirre Eneko &amp; German Rigau, 1996) and random walk algorithm (Mihalcea Rada, 2005) essentially do Machine Readable Dictionary lookup. However, these are fundamentally overlap based algorithms which suffer from overlap sparsity, dictionary definitions being generally small in length. Supervised learning algorithms for WSD are mostly word specific classifiers, e.g., WSD using SVM (Lee et. al., 2004), Exemplar based WSD (Ng Hwee T. &amp; Hian B. Lee, 1996) and decision list based algorithm (Yarowsky, 1994). The requirement of a large training corpus renders these algorithms unsuitable for resource scarce languages. Semi-supervised and unsupervised algorithms do not need large amou</context>
</contexts>
<marker>Rada, 2005</marker>
<rawString>Mihalcea Rada. 2005. Large vocabulary unsupervised word sense disambiguation with graph-based algorithms for sequence data labeling. In Proceedings of the Joint Human Language Technology and Empirical Methods in Natural Language Processing Conference (HLT/EMNLP), Vancouver, Canada, 411-418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ng Hwee T</author>
<author>Hian B Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word senses: An exemplar-based approach.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>40--47</pages>
<location>Santa Cruz, U.S.A.,</location>
<marker>T, Lee, 1996</marker>
<rawString>Ng Hwee T. &amp; Hian B. Lee. 1996. Integrating multiple knowledge sources to disambiguate word senses: An exemplar-based approach. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL), Santa Cruz, U.S.A., 40-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mercer</author>
</authors>
<date>1993</date>
<journal>The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics Vol</journal>
<volume>19</volume>
<pages>263--311</pages>
<marker>Mercer, 1993</marker>
<rawString>Peter F. Brown and Vincent J.Della Pietra and Stephen A. Della Pietra and Robert. L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics Vol 19, 263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajat Mohanty</author>
</authors>
<title>Pushpak Bhattacharyya, Prabhakar Pande, Shraddha Kalele, Mitesh Khapra and Aditya Sharma.</title>
<date>2008</date>
<booktitle>Synset Based Multilingual Dictionary: Insights, Applications and Challenges. Global Wordnet Conference,</booktitle>
<location>Szeged, Hungary,</location>
<marker>Mohanty, 2008</marker>
<rawString>Rajat Mohanty, Pushpak Bhattacharyya, Prabhakar Pande, Shraddha Kalele, Mitesh Khapra and Aditya Sharma. 2008. Synset Based Multilingual Dictionary: Insights, Applications and Challenges. Global Wordnet Conference, Szeged, Hungary, January 22-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Resnik Philip</author>
</authors>
<title>Selectional preference and sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL Workshop on Tagging Text with Lexical Semantics, Why, What and How?</booktitle>
<pages>52--57</pages>
<location>Washington, U.S.A.,</location>
<marker>Philip, 1997</marker>
<rawString>Resnik Philip. 1997. Selectional preference and sense disambiguation. In Proceedings of ACL Workshop on Tagging Text with Lexical Semantics, Why, What and How? Washington, U.S.A., 52-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paolo Velardi</author>
</authors>
<title>Structural Semantic Interconnections: A Knowledge-Based Approach to Word Sense Disambiguation.</title>
<date>2005</date>
<journal>IEEE Transactions On Pattern Analysis and Machine Intelligence.</journal>
<marker>Navigli, Velardi, 2005</marker>
<rawString>Roberto Navigli, Paolo Velardi. 2005. Structural Semantic Interconnections: A Knowledge-Based Approach to Word Sense Disambiguation. IEEE Transactions On Pattern Analysis and Machine Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergei Nirenburg</author>
<author>Harold Somers</author>
<author>Yorick Wilks</author>
</authors>
<date>2003</date>
<booktitle>Readings in Machine Translation.</booktitle>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<marker>Nirenburg, Somers, Wilks, 2003</marker>
<rawString>Sergei Nirenburg, Harold Somers, and Yorick Wilks. 2003. Readings in Machine Translation. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Véronis Jean</author>
</authors>
<title>HyperLex: Lexical cartography for information retrieval.</title>
<date>2004</date>
<journal>Computer Speech &amp; Language,</journal>
<pages>18--3</pages>
<contexts>
<context position="5142" citStr="Jean, 2004" startWordPosition="796" endWordPosition="797">ionary definitions being generally small in length. Supervised learning algorithms for WSD are mostly word specific classifiers, e.g., WSD using SVM (Lee et. al., 2004), Exemplar based WSD (Ng Hwee T. &amp; Hian B. Lee, 1996) and decision list based algorithm (Yarowsky, 1994). The requirement of a large training corpus renders these algorithms unsuitable for resource scarce languages. Semi-supervised and unsupervised algorithms do not need large amount of annotated corpora, but are again word specific classifiers, e.g., semisupervised decision list algorithm (Yarowsky, 1995) and Hyperlex (Véronis Jean, 2004)). Hybrid approaches like WSD using Structural Semantic Interconnections (Roberto Navigli &amp; Paolo Velardi, 2005) use combinations of more than one knowledge sources (wordnet as well as a small amount of tagged corpora). This allows them to capture important information encoded in wordnet (Fellbaum, 1998) as well as draw syntactic generalizations from minimally tagged corpora. At this point we state that no single existing solution to WSD completely meets our requirements of multilinguality, high domain accuracy and good performance in the face of not-so-large annotated corpora. 3 Parameters fo</context>
</contexts>
<marker>Jean, 2004</marker>
<rawString>Véronis Jean. 2004. HyperLex: Lexical cartography for information retrieval. Computer Speech &amp; Language, 18(3):223-252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Walker</author>
<author>R Amsler</author>
</authors>
<title>The Use of Machine Readable Dictionaries in Sublanguage Analysis.</title>
<date>1986</date>
<booktitle>In Analyzing Language in Restricted Domains, Grishman and Kittredge (eds),</booktitle>
<pages>69--83</pages>
<publisher>LEA Press,</publisher>
<marker>Walker, Amsler, 1986</marker>
<rawString>Walker D. and Amsler R. 1986. The Use of Machine Readable Dictionaries in Sublanguage Analysis. In Analyzing Language in Restricted Domains, Grishman and Kittredge (eds), LEA Press, pp. 69-83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yarowsky David</author>
</authors>
<title>Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the association for Computational Linguistics (ACL), Las</booktitle>
<pages>88--95</pages>
<location>Cruces, U.S.A.,</location>
<marker>David, 1994</marker>
<rawString>Yarowsky David. 1994. Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French. In Proceedings of the 32nd Annual Meeting of the association for Computational Linguistics (ACL), Las Cruces, U.S.A., 88-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yarowsky David</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>189--196</pages>
<location>Cambridge, MA,</location>
<marker>David, 1995</marker>
<rawString>Yarowsky David. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL), Cambridge, MA, 189-196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>