<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9844595">
Summarize What You Are Interested In:
An Optimization Framework for Interactive Personalized Summarization
</title>
<author confidence="0.995909">
Rui Yan
</author>
<affiliation confidence="0.839979">
Department of Computer
Science and Technology,
Peking University,
Beijing 100871, China
</affiliation>
<email confidence="0.938698">
r.yan@pku.edu.cn
</email>
<author confidence="0.798543">
Jian-Yun Nie
</author>
<affiliation confidence="0.693575">
D´epartement d´informatique
</affiliation>
<address confidence="0.662731333333333">
et de recherche op´erationnelle,
Universit´e de Montr´eal,
Montr´eal, H3C 3J7 Qu´ebec, Canada
</address>
<email confidence="0.955428">
nie@iro.umontreal.ca
</email>
<author confidence="0.994224">
Xiaoming Li
</author>
<affiliation confidence="0.8422915">
Department of Computer
Science and Technology,
Peking University,
Beijing 100871, China
</affiliation>
<email confidence="0.974108">
lxm@pku.edu.cn
</email>
<sectionHeader confidence="0.994593" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999739035714286">
Most traditional summarization methods treat
their outputs as static and plain texts, which
fail to capture user interests during summa-
rization because the generated summaries are
the same for different users. However, users
have individual preferences on a particular
source document collection and obviously a
universal summary for all users might not al-
ways be satisfactory. Hence we investigate
an important and challenging problem in sum-
mary generation, i.e., Interactive Personalized
Summarization (IPS), which generates sum-
maries in an interactive and personalized man-
ner. Given the source documents, IPS captures
user interests by enabling interactive clicks
and incorporates personalization by model-
ing captured reader preference. We develop
experimental systems to compare 5 rival al-
gorithms on 4 instinctively different datasets
which amount to 5197 documents. Evalua-
tion results in ROUGE metrics indicate the
comparable performance between IPS and the
best competing system but IPS produces sum-
maries with much more user satisfaction ac-
cording to evaluator ratings. Besides, low
ROUGE consistency among these user pre-
ferred summaries indicates the existence of
personalization.
</bodyText>
<sectionHeader confidence="0.99813" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.983089125">
In the era of information explosion, people need new
information to update their knowledge whilst infor-
mation on Web is updating extremely fast. Multi-
document summarization has been proposed to ad-
dress such dilemma by producing a summary de-
livering the majority of information content from a
document set, and hence is a necessity.
Traditional summarization methods play an im-
portant role with the exponential document growth
on the Web. However, for the readers, the impact of
human interests has seldom been considered. Tra-
ditional summarization utilizes the same methodol-
ogy to generate the same summary no matter who is
reading. However, users may have bias on what they
prefer to read due to their potential interests: they
need personalization. Therefore, traditional summa-
rization methods are to some extent insufficient.
Topic biased summarization tries for personaliza-
tion by pre-defining human interests as several gen-
eral categories, such as health or science. Readers
are required to select their possible interests before
summary generation so that the chosen topic has
priority during summarization. Unfortunately, such
topic biased summarization is not sufficient for two
reasons: (1) interests cannot usually be accurately
pre-defined by ambiguous topic categories and (2)
user interests cannot always be foreknown. Often
users do not really know what general ideas or detail
information they are interested in until they read the
summaries. Therefore, more flexible interactions
are required to establish personalization.
Due to all the insufficiencies of existed sum-
marization approaches, we introduce a new multi-
document summarization task of Interactive Person-
alized Summarization (IPS) and a novel solution for
the task. Taking a document collection as input, the
system outputs a summary aligned both with source
corpus and with user personalization, which is cap-
tured by flexible human−system interactions. We
1342
</bodyText>
<note confidence="0.9931785">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1342–1351,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.99916778125">
build an experimental system on 4 real datasets to use algorithms similar to PageRank and HITS to
verify the effectiveness of our methods compared compute sentence importance. Wan et al. improve
with 4 rivals. The contribution of IPS is manifold the graph-ranking algorithm by differentiating intra-
by addressing following challenges: document and inter-document links between sen-
• The 1st challenge for IPS is to integrate user tences (2007b) and incorporate cluster information
interests into traditional summary components. We in the graph model to evaluate sentences (2008).
measure the utilities of these components and com- To date, topics (or themes, clusters) in documents
bine them. We formulate the task into a balanced have been discovered and used for sentence selec-
optimization framework via iterative substitution to tion for topic biased summarization (Wan and Yang,
generate summaries with maximum overall utilities. 2008; Gong and Liu, 2001). Wan et al. have
• The 2nd challenge is to capture user inter- proposed a manifold-ranking method to make uni-
ests through interaction. We develop an interactive form use of sentence-to-sentence and sentence-to-
mechanism of “click” and “examine” between read- topic relationships to generate topic biased sum-
ers and summaries and address sparse data by “click maries (2007a). Leuski et al. in (2003) pre-define
smoothing” under the scenario of few user clicks. several topic concepts, assuming users will foresee
We start by reviewing previous works. In Section their interested topics and then generate the topic
3 we provide IPS overview, describe user interac- biased summary. However, such assumption is not
tion and optimize component combination with per- quite reasonable because user interests may not be
sonalization. We conduct empirical evaluation and forecasted, or pre-defined accurately as we have ex-
demonstrate the experimental system in Section 4. plained in last section.
Finally we draw conclusions in Section 5. The above algorithms are usually traditional ex-
2 Related Work tensions of generic summarizers. They do not in-
Multi-Document Summarization (MDS) has drawn volve interactive mechanisms to capture reader in-
much attention in recent years and gained emphasis terests, nor do they utilize user preference for per-
in conferences such as ACL, EMNLP and SIGIR, sonalization in summarization. Wan et al. in (2008)
etc. General MDS can either be extractive or ab- have proposed a summarization biased to neighbor-
stractive. The former assigns salient scores to se- ing reading context through anchor texts. How-
mantic units (e.g. sentences, paragraphs) of the doc- ever, such scenario does not apply to contexts with-
uments indicating their importance and then extracts out human-edited anchor texts like Wikipedia they
top ranked ones, while the latter demands informa- have used. Our approach can naturally and simulta-
tion fusion(e.g. sentence compression and reformu- neously take into account traditional summary ele-
lation). Here we focus on extractive summarization. ments and user interests and combine both in opti-
Centroid-based method is one of the most popular mization under a wider practical scenario.
extractive summarization method. MEAD (Radev 3 Interactive Personalized Summarization
et al., 2004) and NeATS (Lin and Hovy, 2002) are Personalization based on user preference can be
such implementations, using position and term fre- captured via various alternative ways, such as eye-
quency, etc. MMR (Goldstein et al., 1999) algorithm tracking or mouse-tracking instruments used in (Guo
is used to remove redundancy. Most recently, the and Agichtein, 2010). In this study, we utilize inter-
graph-based ranking methods have been proposed to active user clicks/examinations for personalization.
rank sentences or passages based on the “votes” or Unlike traditional summarization, IPS supports
“recommendations” between each other. The graph- human−system interaction by clicking into the sum-
based methods first construct a graph representing mary sentences and examining source contexts. The
the sentence relationships at different granularities implicit feedback of user clicks indicates what they
and then evaluate the saliency score of the sentences are interested in and the system collects preference
based on the graph. TextRank (Mihalcea and Tarau, information to update summaries if readers wish to.
2005) and LexPageRank (Erkan and Radev, 2004) We obtain an associated tuple &lt;q, c&gt; between a
1343
clicked sentence q and the examined contexts c.
As q has close semantic coherence with neigh-
boring contexts due to consistency in human natural
language, we consider a window of sentences cen-
tered at the clicked sentence q as c, which is a bag of
sentences. The window size k is a parameter to set.
However, click data is often sparse: users are not
likely to click more than 1/10 of total summary sen-
tences within a single generation. We amplify these
tiny hints of user interest by click smoothing.
We change the flat summary structure into a hi-
erarchical organization by extracting important se-
mantic units (denoted as u) and establishing link-
age between them. If the clicked sentence q con-
tains u, we diffuse the click impact to the correlated
units, which makes a single click perform as multi-
ple clicks and the sparse data is smoothed.
</bodyText>
<subsectionHeader confidence="0.950843">
Problem Formulation
</subsectionHeader>
<bodyText confidence="0.997936285714285">
Input: Given the sentence collection D decom-
posed by documents, D = {s1, s2,... , s|D|} and
the clicked sentence record Q = {q1, q2,... }, we
generate summaries in sentences. A user click is
associated with a tuple &lt;q, (u), c&gt; where the exis-
tence of u depends on whether q contains u. The
collection of semantic units is denoted as M =
{u1, u2,... , u|M|}.
Output: A summary S as a set of sentences
{s1, s2, ... , s|S|} and S ⊂ D according to the pre-
specified compression rate φ (0 &lt; φ &lt; 1).
After the overview and formulation of IPS prob-
lem, we move on to the major components of User
Interaction and Personalized Summarization.
</bodyText>
<subsectionHeader confidence="0.997087">
3.1 User Interaction
</subsectionHeader>
<bodyText confidence="0.99738705">
Hypertexify Summaries. We hypertexify the sum-
mary structure by establishing linkage between se-
mantic units. There are several possible formats for
semantic units, such as words or n-grams, etc. As
single words are proved to be not illustrative of se-
mantic meanings (Zhao et al., 2011) and n-grams are
rigid in length, we choose to extract semantic units
at a phrase granularity. Among all phrases from
source texts, some are of higher importance to at-
tract user interests, such as hot concepts or popu-
lar event names. We utilize the toolkit provided by
(Zhao et al., 2011) based on graph proximity LDA
(Blei et al., 2003) to extract key phrases and their
corresponding topic. A topic T is represented by
{(u1, π(u1, T)), (u2, π(u2, T)),... } where π(u, T)
is the probability of u belonging to topic T. We in-
vert the topic-unit representation in Table 1, where
each u is represented as a topic vector. The corre-
lation corr(.) between ui, uj is measured by cosine
similarity sim(.) on topic distribution vector ~ui, ~uj.
</bodyText>
<equation confidence="0.802466">
corr(ui, uj) = simtopic(~ui, ~uj) (1)
</equation>
<tableCaption confidence="0.696265">
Table 1: Inverted representation of topic-unit vector.
</tableCaption>
<equation confidence="0.7564234">
~u1 π(u1, T1) π(u1, T2) ... π(u1, Tn)
~u2 π(u2, T1) π(u2, T2) ... π(u2, Tn)
. .. ..... ... .
.. .
~u|M |π(u|M|, T1) π(u|M|, T2) ... π(u|M|, Tn)
</equation>
<bodyText confidence="0.9998188">
When the summary is hypertexified by established
linkage, users click into the generated summary to
examine what they are interested in. A single click
on one sentence become multiple clicks via click
smoothing when the indicative function I(u|q) = 1.
</bodyText>
<equation confidence="0.93015175">
�
1 q contains u;
I(u|q) = (2)
0 otherwise.
</equation>
<bodyText confidence="0.6484576">
The click smoothing brings pseudo clicks
asso-
and contexts
The entire user feed-
back texts A from q can be writt
</bodyText>
<equation confidence="0.8099818">
q0
u0
c0.
en as:
corr(u0, u)(u0+γ·c0)+γ·c (3)
</equation>
<bodyText confidence="0.999909166666667">
where γ is the weight tradeoff between u and asso-
ciated contexts c. If I(u|q) = 0, only the examined
context c is feedbacked for user preference; other-
wise, correlative contexts with u are taken into con-
sideration, which is a process of impact diffusion.
ciated with
</bodyText>
<subsectionHeader confidence="0.998792">
3.2 Personalized Summarization
</subsectionHeader>
<bodyText confidence="0.918737153846154">
Traditional summarization involves two essential re-
quirements: (1) coverage: the summary should
keep alignment with the source collection, which is
proved to be significant (Li et al., 2009). (2) di-
versity: according to
principle (Goldstein et
al., 1999) and its applications (Wan et al., 2007b;
Wan and Yang, 2008), a good summary should be
concise and contain as few redundan
MMR
t sentences as
possible, i.e., two sentences providing similar infor-
mation should not both present. According to our
</bodyText>
<equation confidence="0.9135528">
A(q) = I(u|q)
j
� |M|
=1
1344
</equation>
<bodyText confidence="0.998983266666667">
investigation, we observe that a well generated sum-
mary should properly consider a key component of
(3) user interests, which captures user preference to
summarize what they are interested in.
All above requirements involve a measurement
of similarity between two word distributions Θ1
and Θ2. Cosine, Kullback-Leibler divergence DKL
and Jensen Shannon divergence DJS are all able
to measure the similarity, but (Louis and Nenkova,
2009) indicate the superiority of DJS in summa-
rization task. We also introduce a pair of decreas-
ing/increasing logistic functions, L1(x) = 1/(1 +
ex) and L2(x) = ex/(1 + ex), to map the diver-
gence into interval [0,1]. V is the vocabulary set
and tf denotes the term frequency for word w.
</bodyText>
<equation confidence="0.9959585">
1
DJS(Θ1||Θ2) = 2[DKL(Θ1||Θ2)+DKL(Θ2||Θ1)]
</equation>
<bodyText confidence="0.60495875">
where
p(w|Θ1)logp(w|Θ1)
p(w|Θ2)
where
</bodyText>
<equation confidence="0.9981935">
tf(w, Θ)
p(w|Θ) = w/ tf(w�, Θ).
</equation>
<bodyText confidence="0.969809111111111">
Modeling Interest for User Utility. Given a gener-
ated summary S, users tend to scrutinize texts rele-
vant to their interests. Texts related to user implicit
feedback are collected as A = �Q�
i=1 A(qi). Intu-
itively, the smaller distance between the word distri-
bution of final summary (ΘS) and the word distri-
bution of user preference (ΘA), the higher utility of
user interests Uuser(S) will be, i.e.,
</bodyText>
<equation confidence="0.998047">
Uuser(S) = L1(DJS(ΘS||ΘA)). (4)
</equation>
<bodyText confidence="0.9999">
We model the utility of traditional summarization
Utrad(S) using a linear interpolation controlled by
parameter δ between utility from coverage Uc(S)
and utility Ud(S) from diversity:
</bodyText>
<equation confidence="0.996038">
Utrad(S) = Uc(S) + δ · Ud(S). (5)
</equation>
<bodyText confidence="0.969607714285714">
Coverage Utility. The summary should share a
closer word distribution with the source collection
(Allan et al., 2001; Li et al., 2009). A good summary
focuses on minimizing the loss of main information
from the whole collection D. Utility from coverage
Uc(S) is defined as follows and for coverage utility,
smaller divergence is desired.
</bodyText>
<equation confidence="0.998653">
Uc(S) = L1(DJS(ΘS||ΘD)). (6)
</equation>
<bodyText confidence="0.999483142857143">
Diversity Utility. Diversity measures the novelty
degree of any sentence s compared with all other
sentences within S, i.e., the distances between all
other sentences and itself. Diversity utility Ud(S) is
an average novelty score for all sentences in S. For
diversity utility, larger distance is desired, and hence
we use the increasing function L2 as follows:
</bodyText>
<equation confidence="0.864786">
L2(DJS(Θs||Θ(S−s))). (7)
</equation>
<subsectionHeader confidence="0.981206">
3.3 Balanced Optimization Framework
</subsectionHeader>
<bodyText confidence="0.999331">
A well generated summary S should be sufficiently
aligned with the original source corpus, and also
be optimized given the user interests. The utility
of an individual summary U(S) is evaluated by the
weighted combination of these components, con-
trolled by parameter λ for balanced weights.
</bodyText>
<equation confidence="0.998228">
U(S) = Utrad(S) + λ · Uuser(S) (8)
</equation>
<bodyText confidence="0.999806">
Given the sentence set D and the compression rate
φ, there are φ·|D |out of |D |possibilities to generate
S. The IPS task is to predict the optimized sentence
subset of S* from the space of all combinations. The
objective function is as follows:
</bodyText>
<equation confidence="0.903577">
S* = argmax U(S). (9)
S
</equation>
<bodyText confidence="0.99978875">
As U(S) is measured based on preferred interests
from user interaction within a generation in our sys-
tem, we extract S iteratively to approximate S*, i.e,
maximize U(S) based on the user feedbacks from
the interaction sessions. Each session is an iteration.
We use a similar framework as we have proposed in
(Yan et al., 2011).
During every session, the top ranked sentences are
strong candidates for the summary to generate and
the rank methodology is based on the metrics U(.).
The algorithm tends to highly rank sentences which
are with both coverage utility and interest utility, and
are diversified in balance: we rank each sentence s
according to U(s) under such metrics.
Consider S(n−1) generated in the (n-1)-th session
which consists of top φ|D |ranked sentences, as well
</bodyText>
<equation confidence="0.975825571428571">
�
DKL(Θ1||Θ2) =
kEV
1 �
Ud(S) = |S|
sES
1345
</equation>
<bodyText confidence="0.961105625">
as the top 0|D |ranked sentences in the n-th iteration
(denoted by O(n)), they have an intersection set of
Z(n) = Sn−1∩On. There is a substitutable sentence
set X(n) = S(n−1) − Z(n) and a new candidate sen-
tence set Y(n) = O(n) − Z(n). We substitute x(n)
sentences with y(n), where x(n) ⊆ X(n) and y(n)
⊆ Y(n). During every iteration, our goal is to find a
substitutive pair &lt;x, y&gt; for S:
</bodyText>
<equation confidence="0.933309">
&lt;x,y&gt; : X × Y → R.
</equation>
<bodyText confidence="0.9993105">
To measure the performance of such a substitu-
tion, a discriminant utility gain function AUx,y
</bodyText>
<equation confidence="0.974068">
AU(n)
x(n),y(n) = U(S(n)) − U(S(n−1))
= U((S(n−1) − x(n)) ∪ y(n)) − U(S(n−1))
(10)
</equation>
<bodyText confidence="0.981458285714286">
is employed to quantify the penalty. Therefore, we
predict the substitutive pair by maximizing the gain
function AUx,y over the state set R, with a size of
EYk=0 AkXCkY, where &lt;x, y&gt;∈ R. Finally the ob-
jective function of Equation (9) changes into maxi-
mization of utility gain by substitute xˆ with yˆ during
each iteration:
</bodyText>
<equation confidence="0.8802395">
&lt; ˆx, yˆ &gt;= argmax AUx,y. (11)
x⊆X,y⊆Y
</equation>
<bodyText confidence="0.999970428571428">
Note that the objectives of interest utility opti-
mization and traditional utility optimization are not
always the same because the word distributions in
these texts are usually different. The substitutive
pair &lt;x, y&gt; may perform well based on the user
preference component while not on the traditional
summary part and vice versa. There is a tradeoff
between both user optimization and traditional opti-
mization and hence we need to balance them by λ.
The objective Equation (11) is actually to maxi-
mize AU(S) from all possible substitutive pairs be-
tween two iteration sessions to generate S. The al-
gorithm is shown in Algorithm 1. The threshold E is
set at 0.001 in this study.
</bodyText>
<sectionHeader confidence="0.997593" genericHeader="method">
4 Experiments and Evaluation
</sectionHeader>
<subsectionHeader confidence="0.844211">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.998938333333333">
IPS can be tested on any document set but a tiny
corpus to summarize may not cover abundant effec-
tive interests to attract user clicks indicating their
</bodyText>
<listItem confidence="0.949780173913043">
Algorithm 1 Regenerative Optimization
1: Input: D, E, 0
2: for all s ∈ D do
3: calculate Utrad(s)
4: end for
5: S ← top 0|D |ranked sentences
6: while new generation=TRUE do
7: collect clicks and update utility from U0 to U
8: if |U(S) − U0(S) |&gt; E then
9: for all s ∈ D do
10: calculate U(s)
11: end for
12: O ← top 0|D |ranked sentences by U(s)
13: Z ← S ∩ O
14: X ← S − Z, Y ← O − Z
15: for all &lt;x, y&gt; pair where x ⊆ X, y ⊆ Y
do
16: AUx,y = U((S − x) ∪ y) − U(S)
17: end for
18: &lt; ˆx, yˆ &gt;= argmax AUx,y
19: S ← (S − ˆx) ∪ yˆ
20: end if
21: end while
</listItem>
<bodyText confidence="0.999687642857143">
preference. Besides, the scenario of small corpus is
not quite practical for the exponential growing web.
Therefore, we test IPS on large real world datasets.
We build 4 news story sets which consist of docu-
ments and reference summaries to evaluate our pro-
posed framework empirically. We downloaded 5197
news articles from 10 selected sources. As shown in
Table 2, three of the sources are in UK, one of them
is in China and the rest are in US. We choose them
because many of these websites provide handcrafted
summaries for their special reports, which serve as
reference summaries. These events belong to differ-
ent categories of Rule of Interpretation (ROI) (Ku-
maran and Allan, 2004). Statistics are in Table 3.
</bodyText>
<subsectionHeader confidence="0.912815">
4.2 Experimental System Setups
</subsectionHeader>
<listItem confidence="0.985721">
• Preprocessing. Given a collection of documents,
we first decompose them into sentences. Stop-words
are removed and words stemming is performed.
Then the word distributions can be calculated.
• User Interface Design. Users are required to
specify the overall compression rate 0 and the sys-
tem extracts 0|D |sentences according to user utility
</listItem>
<page confidence="0.607877">
1346
</page>
<figureCaption confidence="0.577350166666667">
Figure 1: A demonstration system for Interactive Personalized Summarization when compression rate φ is specified
(e.g. 5%). For convenience of browsing, we number the selected sentences (see in part 3). Extracted semantic units,
such as “drilling mud”, are in bold and underlined format (see in part 1). When the user clicks a sentence (part 4), the
clicked sentence ID is kept in the click record (part 2). Mis-clicked records revocation can be operated by clicking
the deletion icon “X” (see in part 3). Once a sentence is clicked, user can track the sentence into the popup source
document to examine the contexts. The selected sentences are highlighted in the source documents (see in part 5).
</figureCaption>
<tableCaption confidence="0.993185">
Table 2: News sources of 4 datasets
</tableCaption>
<note confidence="0.906733333333333">
News Sources Nation News Sources Nation
BBC UK Fox News US
Xinhua China MSNBC US
CNN US Guardian UK
ABC US New York Times US
Reuters UK Washington Post US
</note>
<tableCaption confidence="0.997793">
Table 3: Detailed basic information of 4 datasets.
</tableCaption>
<table confidence="0.82687">
News Subjects #size #docs #RS Avg.L
1.Influenza A 115026 2557 5 83
2.BP Oil Spill 63021 1468 6 76
3.Haiti Earthquake 12073 247 2 32
4.Jackson Death 37819 925 3 64
#size: total sentence counts; #RS: the number of reference summaries;
Avg.L: average length of reference summary measured in sentences.
</table>
<bodyText confidence="0.999790625">
and traditional utility. User utility is obtained from
interaction. The system keeps the clicked sentence
records and calculates the user feedback by Equa-
tion (3) during every session. Consider sometimes
users click into the summary due to confusion or
mis-operations, but not their real interests. The sys-
tem supports click records revocation. More details
of the user interface is demonstrated in Figure 1.
</bodyText>
<subsectionHeader confidence="0.995875">
4.3 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9997085">
We include both subjective evaluation from 3 evalu-
ators based on their personalized interests and pref-
erence, and the objective evaluation based on the
widely used ROUGE metrics (Lin and Hovy, 2003).
</bodyText>
<subsectionHeader confidence="0.560205">
Evaluator Judgments
</subsectionHeader>
<bodyText confidence="0.999951">
Evaluators are requested to express an opinion
over all summaries based on the sentences which
they deem to be important for the news. In general
a summary can be rated in a 5-point scale, where
“1” for “terrible”, “2” for “bad”, “3” for “normal”,
“4” for “good” and “5” for “excellent”. Evaluators
are allowed to judge at any scores between 1 and 5,
e.g. a score of “3.3” is adopted when the evaluator
feels difficult to decide whether “3” or “4” is more
</bodyText>
<page confidence="0.508077">
1347
</page>
<bodyText confidence="0.801271">
appropriate but with preference towards “3”.
</bodyText>
<sectionHeader confidence="0.645912" genericHeader="method">
ROUGE Evaluation
</sectionHeader>
<bodyText confidence="0.999904363636364">
The DUC usually officially employs ROUGE
measures for summarization evaluation, which mea-
sures summarization quality by counting overlap-
ping units such as the N-gram, word sequences, and
word pairs between the candidate summary and the
reference summary. We use ROUGE-N as follows:
where N stands for the length of the N-gram and N-
gramERefSum denotes the N-grams in the reference
summaries while N-gramECandSum denotes the N-
grams in the candidate summaries. Countmatch(N-
gram) is the maximum number of N-gram in the
candidate summary and in the set of reference sum-
maries. Count(N-gram) is the number of N-grams in
the reference summaries or candidate summary.
According to (Lin and Hovy, 2003), among all
sub-metrics in ROUGE, ROUGE-N (N=1, 2) is rela-
tively simple and works well. In this paper, we eval-
uate our experiments using all methods provided by
the ROUGE package (version 1.55) and only report
ROUGE-1, since the conclusions drawn from differ-
ent methods are quite similar. Intuitively, the higher
the ROUGE scores, the similar two summaries are.
</bodyText>
<subsectionHeader confidence="0.971521">
4.4 Algorithms for Comparison
</subsectionHeader>
<bodyText confidence="0.99977046875">
We implement the following widely used multi-
document summarization algorithms as the baseline
systems, which are all designed for traditional sum-
marization without user interaction. For fairness we
conduct the same preprocessing for all algorithms.
Random: The method selects sentences ran-
domly for each document collection.
Centroid: The method applies MEAD algorithm
(Radev et al., 2004) to extract sentences according to
the following parameters: centroid value, positional
value, and first-sentence overlap.
GMDS: The Graph-based MDS proposed by
(Wan and Yang, 2008) first constructs a sentence
connectivity graph based on cosine similarity and
then selects important sentences based on the con-
cept of eigenvector centrality.
IPSini: The initial generated summary from IPS
merely models coverage and diversity utility, which
is similar to the previous work described in (Allan et
al., 2001) with different goals and frameworks.
IPS: Our proposed algorithms with personaliza-
tion component to capture interest by user feed-
backs. IPS generates summaries via iterative sen-
tence substitutions within user interactive sessions.
RefSum: As we have used multiple reference
summaries from websites, we not only provide
ROUGE evaluations of the competing systems but
also of the reference summaries against each other,
which provides a good indicator of not only the
upper bound ROUGE score that any system could
achieve, but also human inconsistency among refer-
ence summaries, indicating personalization.
</bodyText>
<subsectionHeader confidence="0.994656">
4.5 Overall Performance Comparison
</subsectionHeader>
<bodyText confidence="0.999727666666667">
We take the average ROUGE-1 performance and hu-
man ratings on all sets. The overall results are shown
in Figure 2 and details are listed in Tables 4∼6.
</bodyText>
<figureCaption confidence="0.986657">
Figure 2: Overall performance on 6 datasets.
</figureCaption>
<bodyText confidence="0.963863">
From the results, we have following observations:
</bodyText>
<listItem confidence="0.898557071428572">
• Random has the worst performance as expected,
both in ROUGE-1 scores and human judgements.
• The ROUGE-1 and human ratings of Centroid
and GMDS are better than those of Random. This is
mainly because the Centroid based algorithm takes
into account positional value and first-sentence over-
lap, which facilitates main aspects summarization
and PageRank-based GMDS ranks the sentence us-
ing eigenvector centrality which implicitly accounts
for information subsumption among all sentences.
• In general, the GMDS system slightly outper-
forms Centroid system in ROUGE-1, but the human
judgements of GMDS and Centroid are of no signifi-
cant difference. This is probably due to the difficulty
</listItem>
<figure confidence="0.7470369">
E E
SE{RefSum} N-gramES
E
SE{RefSum}
Countmatch(N-gram)
Count(N-gram)
ROUGE-N =
E
N-gramES
1348
</figure>
<tableCaption confidence="0.8001775">
Table 4: Overall performance comparison on Influenza A.
ROI∗ category: Science.
</tableCaption>
<table confidence="0.999946285714286">
Systems R-1 95%-conf. H-1 H-2 H-3
RefSum 0.491 0.44958 3.5 3.0 3.9
Random 0.257 0.75694 1.2 1.0 1.0
Centroid 0.331 0.45073 2.5 3.0 3.5
GMDS 0.364 0.33269 3.0 2.7 3.5
IPSini 0.302 0.21213 2.0 2.5 2.5
IPS 0.337 0.46757 4.8 4.5 4.5
</table>
<tableCaption confidence="0.923359">
Table 5: Overall performance comparison on BP Oil
Leak. ROI category: Accidents.
</tableCaption>
<table confidence="0.999937285714286">
Systems R-1 95%-conf. H-1 H-2 H-3
RefSum 0.517 0.48618 4.0 3.3 3.9
Random 0.262 0.64406 1.5 1.0 1.5
Centroid 0.369 0.34743 3.2 3.0 3.5
GMDS 0.389 0.43877 3.5 3.0 3.9
IPSini 0.327 0.53722 3.0 2.5 3.0
IPS 0.372 0.35681 4.8 4.5 4.5
</table>
<tableCaption confidence="0.9420585">
Table 6: Overall performance comparison on Haiti Earth-
quake. ROI category: Disasters.
</tableCaption>
<table confidence="0.999934142857143">
Systems R-1 95%-conf. H-1 H-2 H-3
RefSum 0.528 0.30450 3.8 4.0 4.0
Random 0.266 0.75694 1.5 1.5 1.8
Centroid 0.362 0.43045 3.6 3.0 4.0
GMDS 0.380 0.33694 3.9 3.5 4.0
IPSini 0.331 0.34120 2.8 2.5 3.0
IPS 0.391 0.40069 5.0 4.7 5.0
</table>
<tableCaption confidence="0.8622375">
Table 7: Overall performance comparison on Michael
Jackson Death. ROI category: Legal Cases.
</tableCaption>
<table confidence="0.999891857142857">
Systems R-1 95%-conf. H-1 H-2 H-3
RefSum 0.482 0.47052 3.5 3.5 4.0
Random 0.232 0.52426 1.2 1.0 1.5
Centroid 0.320 0.21045 3.0 2.5 2.7
GMDS 0.341 0.30070 3.5 3.3 3.9
IPSini 0.287 0.48526 2.5 2.0 2.2
IPS 0.324 0.36897 5.0 4.5 4.8
</table>
<tableCaption confidence="0.752275">
∗ROI: news categorization defined by Linguistic Data Consortium.
Available at http://www.ldc.upenn.edu/projects/tdt4/annotation
</tableCaption>
<bodyText confidence="0.62402">
of human judgements on comparable summaries.
• The results of ROUGE-1 and ratings for IPSini
are better than Random but worse than Centroid and
GMDS. The reason in this case may be that IPSini
does not capture sufficient attributes: coverage and
diversity are merely fundamental requirements.
</bodyText>
<listItem confidence="0.9584335">
• Traditional summarization considers sentence
selection based on corpus only, and hence neglects
</listItem>
<tableCaption confidence="0.985063">
Table 8: Ratings consistency between evaluators: mean
± standard deviation over the 4 datasets.
</tableCaption>
<table confidence="0.999103333333333">
RefSum Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.35±0.09 0.30±0.33
Evaluator 2 0.50±0.14
Random Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.23±0.04 0.20±0.02
Evaluator 2 0.33±0.06
Centroid Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.45±0.03 0.50±0.12
Evaluator 2 0.55±0.11
GMDS Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.35±0.02 0.35±0.03
Evaluator 2 0.70±0.03
IPSini Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.45±0.01 0.25±0.04
Evaluator 2 0.30±0.06
IPS Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.35±0.01 0.18±0.02
Evaluator 2 0.28±0.04
</table>
<bodyText confidence="0.9943211875">
user interests. Many sentences are extracted due to
arbitrary assumption of reader preference, which re-
sults in a low user satisfaction. Human judgements
under our proposed IPS framework greatly outper-
form baselines, indicating that the appropriate use
of human interests for summarization are beneficial.
The ROUGE-1 performance for IPS is not as ideal
as that of GMDS. This situation may result from the
divergence between user interests and general infor-
mation provided by mass media propaganda, which
again motivates the need for personalization.
Although the high disparities between different
human evaluators have been observed in (Gong and
Liu, 2001), we still examine the consistency among
3 evaluators and their preferred summaries to prove
the motivation of personalization in our work.
</bodyText>
<subsectionHeader confidence="0.994139">
4.6 Consistency Analysis for Personalization
</subsectionHeader>
<bodyText confidence="0.999950857142857">
The low ROUGE-1 scores of RefSum indicate the
inconsistency among reference summaries. We con-
duct personalization analysis from two perspectives:
(1) human rating consistency and (2) content consis-
tency among human supervised summaries.
We calculate the mean and variance of rating vari-
ations among evaluator judgements, listed in Table
</bodyText>
<page confidence="0.875171">
1349
</page>
<tableCaption confidence="0.9951905">
Table 9: Content consistency among evaluators super-
vised summaries.
</tableCaption>
<bodyText confidence="0.946329892857143">
Evaluator 1 Evaluator 2 Evaluator 3
Evaluator 1 0.273 0.398
Evaluator 2 0.289 0.257
Evaluator 3 0.407 0.235
RefSum 0.365 0.302 0.394
8. We see that for Random the average rating vari-
ation is 0.25, for IPS is 0.27, for IPSini is 0.33, for
RefSum is 0.38, for GMDS is 0.47 and for Centroid
is the highest, 0.50. Such phenomenon indicates
for poor generated summaries, such as Random or
IPSini, humans have consensus, but for normal sum-
maries without personalized interests, they are likely
to have disparities, surprisingly, even for RefSum.
General summaries provided by mass media satisfy
part of audiences, but obviously not all of them.
The high rating consistency of IPS indicates peo-
ple tend to favor summaries generated according to
their interests. We next examine content consistency
of these summaries with high rating consistency.
As shown in Table 9, although highly scored,
these human supervised summaries still have low
content consistency (especially Evaluator 2). The
low content consistency between RefSum and su-
pervised summaries shows reader have individual
personalization. Note that the inconsistency among
evaluators is larger than that between RefSum and
supervised summaries, indicating interests take a
high proportion in evaluator supervised summaries.
</bodyText>
<subsectionHeader confidence="0.980154">
4.7 Parameter Settings
</subsectionHeader>
<bodyText confidence="0.999477">
S controls coverage/diversity tradeoff. We tune S on
IPSini and apply the optimal S directly in IPS. Ac-
cording to the statistics in (Yan et al., 2010), the se-
mantic coherent context is about 7 sentences. There-
fore, we empirically choose k=3 for the examined
context window. The number of topics is set at
n=50. We assign an equal weight (&apos;y = 1) to seman-
tic units and examined contexts according to analog-
ical research of summarization from implicit feed-
backs via clickthrough data (Sun et al., 2005).
A is the key parameter in IPS approach, control-
ling the weight of user utility during the process of
interactive personalized summarization.
Through Figure 3, we see that when A is small
</bodyText>
<figureCaption confidence="0.999509">
Figure 3: A v.s. human ratings and ROUGE scores.
</figureCaption>
<bodyText confidence="0.999837315789474">
(A E [0.01, 0.1]), both human judgements and
ROUGE evaluation scores have little difference.
When A E [0.1, 1], ROUGE scores increase signifi-
cantly but human satisfaction shows little response.
A E [1,10] brings large user utility enhancement be-
cause user may find what they are interested in but
ROUGE scores start to decay. When A E [10, 100],
ROUGE scores drop much because the emphasized
user interests may guide the generated summaries
divergent away from the original corpus.
In Figure 4 we examine how A attracts user clicks
and regeneration counts until satisfaction. As the re-
sult indicates, both counts increase as A increases.
When A is small (from 0.01 to 0.1), readers find
no more interesting aspects through clicks and re-
generations and stop due to the bad user experience.
As A increases, the system mines more relevant sen-
tences according to personalized interests and hence
attracts user clicks and intention to regenerate.
</bodyText>
<figureCaption confidence="0.999336">
Figure 4: A v.s. click counts and regeneration counts.
</figureCaption>
<bodyText confidence="0.922854418181818">
1350
5 Conclusion In Proceedings of the 27th annual international ACM
We present an important and novel summariza- SIGIR’04, pages 297–304.
tion problem, Interactive Personalized Summariza- Anton Leuski, Chin-Yew Lin, and Eduard Hovy. 2003.
tion (IPS), which generates summaries based on ineats: interactive multi-document summarization. In
human−system interaction for “interests” and per- Proceedings of ACL’03, pages 125–128.
sonalization. We formally formulate IPS as a combi- Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha,
nation of user utility and traditional summary utility, and Yong Yu. 2009. Enhancing diversity, cover-
such as coverage and diversity. We implement a sys- age and balance for summarization through structure
tem under such framework for experiments on real learning. In Proceedings of WWW’09, pages 71–80.
web datasets to compare all approaches. Through Chin-Yew Lin and Eduard Hovy. 2002. From single
our experiments we notice that user personalization to multi-document summarization: a prototype system
of interests plays an important role in summary gen- and its evaluation. In Proceedings of ACL’02, pages
eration, which largely increase human ratings due to 457–464.
user satisfaction. Besides, our experiments indicate Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
the inconsistency between user preferred summaries ation of summaries using n-gram co-occurrence statis-
and reference summaries measured by ROUGE, and tics. In Proceedings of NAACL’03, pages 71–78.
hence prove the effectiveness of personalization. Annie Louis and Ani Nenkova. 2009. Automatically
Acknowledgments evaluating content selection in summarization without
This work was partially supported by HGJ 2010 human models. In EMNLP’09, pages 306–314.
Grant 2011ZX01042-001-001 and NSFC with Grant R. Mihalcea and P. Tarau. 2005. A language indepen-
No.61073082, 60933004. Rui Yan was supported by dent algorithm for single and multiple document sum-
the MediaTek Fellowship. marization. In Proceedings of IJCNLP, volume 5.
References D.R. Radev, H. Jing, and M. Sty. 2004. Centroid-based
James Allan, Rahul Gupta, and Vikas Khandelwal. 2001. summarization of multiple documents. Information
Temporal summaries of new topics. In Proceedings of Processing and Management, 40(6):919–938.
the 24th annual international SIGIR’01, pages 10–18. Jian-Tao Sun, Dou Shen, Hua-Jun Zeng, Qiang Yang,
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent Yuchang Lu, and Zheng Chen. 2005. Web-page sum-
dirichlet allocation. The Journal of Machine Learning marization using clickthrough data. In Proceedings of
Research, 3:993–1022. SIGIR’05, pages 194–201.
G. Erkan and D.R. Radev. 2004. Lexpagerank: Prestige Stephen Wan and C´ecile Paris. 2008. In-browser sum-
in multi-document text summarization. In Proceed- marisation: generating elaborative summaries biased
ings of EMNLP’04, volume 4. towards the reading context. In ACL-HLT’08, pages
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and 129–132.
Jaime Carbonell. 1999. Summarizing text documents: Xiaojun Wan and Jianwu Yang. 2008. Multi-document
sentence selection and evaluation metrics. In Proceed- summarization using cluster-based link analysis. In
ings of SIGIR’99, pages 121–128. Proceedings of SIGIR’08, pages 299–306.
Yihong Gong and Xin Liu. 2001. Generic text sum- X. Wan, J. Yang, and J. Xiao. 2007a. Manifold-ranking
marization using relevance measure and latent seman- based topic-focused multi-document summarization.
tic analysis. In Proceedings of the 24th international In Proceedings of IJCAI, volume 7, pages 2903–2908.
ACM SIGIR conference, SIGIR ’01, pages 19–25. X. Wan, J. Yang, and J. Xiao. 2007b. Single document
Q. Guo and E. Agichtein. 2010. Ready to buy or just summarization with document expansion. In Proceed-
browsing?: detecting web searcher goals from inter- ings of the 22nd AAAI’07, pages 931–936.
action data. In Proceeding of the 33rd international Rui Yan, Yu Li, Yan Zhang, and Xiaoming Li. 2010.
ACM SIGIR conference, SIGIR’10, pages 130–137. Event recognition from news webpages through latent
Giridhar Kumaran and James Allan. 2004. Text clas- ingredients extraction. In AIRS’10, pages 490–501.
sification and named entities for new event detection. Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
1351 Xiaoming Li, and Yan Zhang. 2011. Evolution-
ary timeline summarization: a balanced optimization
framework via iterative substitution. In Proceedings
of the 34th annual international ACM SIGIR’11.
Xin Zhao, Jing Jiang, Jing He, Yang Song, Palakorn
Achanauparp, Ee-Peng Lim, and Xiaoming Li. 2011.
Topical Keyphrase Extraction from Twitter. In Pro-
ceedings of ACL-HLT’11.
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.059901">
<title confidence="0.9991435">Summarize What You Are Interested In: An Optimization Framework for Interactive Personalized Summarization</title>
<author confidence="0.996857">Rui</author>
<affiliation confidence="0.883841">Department of Science and Peking</affiliation>
<address confidence="0.991016">Beijing 100871,</address>
<email confidence="0.912951">r.yan@pku.edu.cn</email>
<author confidence="0.493506">Jian-Yun</author>
<affiliation confidence="0.837541333333333">D´epartement et de recherche Universit´e de</affiliation>
<address confidence="0.9313">Montr´eal, H3C 3J7 Qu´ebec,</address>
<email confidence="0.731256">nie@iro.umontreal.caXiaoming</email>
<affiliation confidence="0.877751666666667">Department of Science and Peking</affiliation>
<address confidence="0.992792">Beijing 100871,</address>
<email confidence="0.971612">lxm@pku.edu.cn</email>
<abstract confidence="0.996657517241379">Most traditional summarization methods treat their outputs as static and plain texts, which fail to capture user interests during summarization because the generated summaries are the same for different users. However, users have individual preferences on a particular source document collection and obviously a universal summary for all users might not always be satisfactory. Hence we investigate an important and challenging problem in summary generation, i.e., Interactive Personalized Summarization (IPS), which generates summaries in an interactive and personalized manner. Given the source documents, IPS captures user interests by enabling interactive clicks and incorporates personalization by modeling captured reader preference. We develop experimental systems to compare 5 rival algorithms on 4 instinctively different datasets which amount to 5197 documents. Evaluation results in ROUGE metrics indicate the comparable performance between IPS and the best competing system but IPS produces summaries with much more user satisfaction according to evaluator ratings. Besides, low ROUGE consistency among these user preferred summaries indicates the existence of personalization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>