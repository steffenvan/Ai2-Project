<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.074987">
<title confidence="0.958887">
Detection of Agreement and Disagreement in Broadcast Conversations
</title>
<author confidence="0.803166">
Wang&apos;Sibel Yaman2y*Kristin
</author>
<note confidence="0.676131333333333">
Wen Precoda&apos; Colleen Richey&apos; Geoffrey Raymond&apos;
&apos;SRI International, 333 Ravenswood Avenue, Menlo Park, CA 94025, USA
2IBM T. J. Watson Research Center P.O.Box 218, Yorktown Heights, NY 10598, USA
</note>
<affiliation confidence="0.936155">
&apos;University of California, Santa Barbara, CA, USA
</affiliation>
<email confidence="0.992713">
{wwang,precoda,colleen}@speech.sri.com, syaman@us.ibm.com, graymond@soc.ucsb.edu
</email>
<sectionHeader confidence="0.995647" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998891863636364">
We present Conditional Random Fields
based approaches for detecting agree-
ment/disagreement between speakers in
English broadcast conversation shows. We
develop annotation approaches for a variety
of linguistic phenomena. Various lexical,
structural, durational, and prosodic features
are explored. We compare the performance
when using features extracted from au-
tomatically generated annotations against
that when using human annotations. We
investigate the efficacy of adding prosodic
features on top of lexical, structural, and
durational features. Since the training data
is highly imbalanced, we explore two sam-
pling approaches, random downsampling
and ensemble downsampling. Overall, our
approach achieves 79.2% (precision), 50.5%
(recall), 61.7% (F1) for agreement detection
and 69.2% (precision), 46.9% (recall), and
55.9% (F1) for disagreement detection, on the
English broadcast conversation data.
</bodyText>
<sectionHeader confidence="0.999128" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999597818181818">
In this work, we present models for detecting
agreement/disagreement (denoted (dis)agreement)
between speakers in English broadcast conversation
shows. The Broadcast Conversation (BC) genre dif-
fers from the Broadcast News (BN) genre in that
it is more interactive and spontaneous, referring to
free speech in news-style TV and radio programs
and consisting of talk shows, interviews, call-in
programs, live reports, and round-tables. Previous
This work was performed while the author was at ICSI.
work on detecting (dis)agreements has been focused
on meeting data. (Hillard et al., 2003), (Galley
et al., 2004), (Hahn et al., 2006) used spurt-level
agreement annotations from the ICSI meeting cor-
pus (Janin et al., 2003). (Hillard et al., 2003) ex-
plored unsupervised machine learning approaches
and on manual transcripts, they achieved an over-
all 3-way agreement/disagreement classification ac-
curacy as 82% with keyword features. (Galley et
al., 2004) explored Bayesian Networks for the de-
tection of (dis)agreements. They used adjacency
pair information to determine the structure of their
conditional Markov model and outperformed the re-
sults of (Hillard et al., 2003) by improving the 3-
way classification accuracy into 86.9%. (Hahn et al.,
2006) explored semi-supervised learning algorithms
and reached a competitive performance of 86.7%
3-way classification accuracy on manual transcrip-
tions with only lexical features. (Germesin and Wil-
son, 2009) investigated supervised machine learn-
ing techniques and yields competitive results on the
annotated data from the AMI meeting corpus (Mc-
Cowan et al., 2005).
Our work differs from these previous studies in
two major categories. One is that a different def-
inition of (dis)agreement was used. In the cur-
rent work, a (dis)agreement occurs when a respond-
ing speaker agrees with, accepts, or disagrees with
or rejects, a statement or proposition by a first
speaker. Second, we explored (dis)agreement de-
tection in broadcast conversation. Due to the dif-
ference in publicity and intimacy/collegiality be-
tween speakers in broadcast conversations vs. meet-
ings, (dis)agreement may have different character-
</bodyText>
<page confidence="0.986583">
374
</page>
<note confidence="0.583707">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 374–378,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999891166666667">
istics. Different from the unsupervised approaches
in (Hillard et al., 2003) and semi-supervised ap-
proaches in (Hahn et al., 2006), we conducted su-
pervised training. Also, different from (Hillard et
al., 2003) and (Galley et al., 2004), our classifica-
tion was carried out on the utterance level, instead
of on the spurt-level. Galley et al. extended Hillard
et al.’s work by adding features from previous spurts
and features from the general dialog context to in-
fer the class of the current spurt, on top of fea-
tures from the current spurt (local features) used by
Hillard et al. Galley et al. used adjacency pairs to
describe the interaction between speakers and the re-
lations between consecutive spurts. In this prelim-
inary study on broadcast conversation, we directly
modeled (dis)agreement detection without using ad-
jacency pairs. Still, within the conditional random
fields (CRF) framework, we explored features from
preceding and following utterances to consider con-
text in the discourse structure. We explored a wide
variety of features, including lexical, structural, du-
rational, and prosodic features. To our knowledge,
this is the first work to systematically investigate
detection of agreement/disagreement for broadcast
conversation data. The remainder of the paper is or-
ganized as follows. Section 2 presents our data and
automatic annotation modules. Section 3 describes
various features and the CRF model we explored.
Experimental results and discussion appear in Sec-
tion 4, as well as conclusions and future directions.
</bodyText>
<sectionHeader confidence="0.800502" genericHeader="introduction">
2 Data and Automatic Annotation
</sectionHeader>
<bodyText confidence="0.99995426984127">
In this work, we selected English broadcast con-
versation data from the DARPA GALE pro-
gram collected data (GALE Phase 1 Release
4, LDC2006E91; GALE Phase 4 Release 2,
LDC2009E15). Human transcriptions and manual
speaker turn labels are used in this study. Also,
since the (dis)agreement detection output will be
used to analyze social roles and relations of an inter-
acting group, we first manually marked soundbites
and then excluded soundbites during annotation and
modeling. We recruited annotators to provide man-
ual annotations of speaker roles and (dis)agreement
to use for the supervised training of models. We de-
fined a set of speaker roles as follows. Host/chair
is a person associated with running the discussions
or calling the meeting. Reporting participant is a
person reporting from the field, from a subcommit-
tee, etc. Commentator participant/Topic participant
is a person providing commentary on some subject,
or person who is the subject of the conversation and
plays a role, e.g., as a newsmaker. Audience par-
ticipant is an ordinary person who may call in, ask
questions at a microphone at e.g. a large presenta-
tion, or be interviewed because of their presence at a
news event. Other is any speaker who does not fit in
one of the above categories, such as a voice talent,
an announcer doing show openings or commercial
breaks, or a translator.
Agreements and disagreements are com-
posed of different combinations of initiating
utterances and responses. We reformulated the
(dis)agreement detection task as the sequence
tagging of 11 (dis)agreement-related labels for
identifying whether a given utterance is initiating
a (dis)agreement opportunity, is a (dis)agreement
response to such an opportunity, or is neither of
these, in the show. For example, a Negative tag
question followed by a negation response forms an
agreement, that is, A: [Negative tag] This is not
black and white, is it? B: [Agreeing Response]
No, it isn’t. The data sparsity problem is serious.
Among all 27,071 utterances, only 2,589 utterances
are involved in (dis)agreement as initiating or
response utterances, about 10% only among all
data, while 24,482 utterances are not involved.
These annotators also labeled shows with a va-
riety of linguistic phenomena (denoted language
use constituents, LUC), including discourse mark-
ers, disfluencies, person addresses and person men-
tions, prefaces, extreme case formulations, and dia-
log act tags (DAT). We categorized dialog acts into
statement, question, backchannel, and incomplete.
We classified disfluencies (DF) into filled pauses
(e.g., uh, um), repetitions, corrections, and false
starts. Person address (PA) terms are terms that a
speaker uses to address another person. Person men-
tions (PM) are references to non-participants in the
conversation. Discourse markers (DM) are words
or phrases that are related to the structure of the
discourse and express a relation between two utter-
ances, for example, I mean, you know. Prefaces
(PR) are sentence-initial lexical tokens serving func-
tions close to discourse markers (e.g., Well, I think
</bodyText>
<page confidence="0.965771">
375
</page>
<bodyText confidence="0.99729758">
that...). Extreme case formulations (ECF) are lexi- calculated the minimum, maximum, range, mean,
cal patterns emphasizing extremeness (e.g., This is standard deviation, skewness and kurtosis values. A
the best book I have ever read). In the end, we man- decision tree model was used to compute posteriors
ually annotated 49 English shows. We preprocessed from prosodic features and we used cumulative bin-
English manual transcripts by removing transcriber ning of posteriors as final features , similar to (Liu et
annotation markers and noise, removing punctuation al., 2006).
and case information, and conducting text normal- As illustrated in Section 2, we reformulated the
ization. We also built automatic rule-based and sta- (dis)agreement detection task as a sequence tagging
tistical annotation tools for these LUCs. problem. We used the Mallet package (McCallum,
3 Features and Model 2002) to implement the linear chain CRF model for
We explored lexical, structural, durational, and sequence tagging. A CRF is an undirected graph-
prosodic features for (dis)agreement detection. We ical model that defines a global log-linear distribu-
included a set of “lexical” features, including n- tion of the state (or label) sequenceEconditioned
grams extracted from all of that speaker’s utter- on an observation sequence, in our case including
ances, denoted ngram features. Other lexical fea- the sequence of sentencesSand the corresponding
tures include the presence of negation and acquies- sequence of features for this sequence of sentences
cence, yes/no equivalents, positive and negative tag F. The model is optimized globally over the en-
questions, and other features distinguishing differ- tire sequence. The CRF model is trained to maxi-
ent types of initiating utterances and responses. We mize the conditional log-likelihood of a given train-
also included various lexical features extracted from ing setP(EjS,F). During testing, the most likely
LUC annotations, denoted LUC features. These ad- sequenceEis found using the Viterbi algorithm.
ditional features include features related to the pres- One of the motivations of choosing conditional ran-
ence of prefaces, the counts of types and tokens dom fields was to avoid the label-bias problem found
of discourse markers, extreme case formulations, in hidden Markov models. Compared to Maxi-
disfluencies, person addressing events, and person mum Entropy modeling, the CRF model is opti-
mentions, and the normalized values of these counts mized globally over the entire sequence, whereas the
by sentence length. We also include a set of features ME model makes a decision at each point individu-
related to the DAT of the current utterance and pre- ally without considering the context event informa-
ceding and following utterances. tion.
We developed a set of “structural” and “dura- 4 Experiments
tional” features, inspired by conversation analysis, All (dis)agreement detection results are based on n-
to quantitatively represent the different participation fold cross-validation. In this procedure, we held
and interaction patterns of speakers in a show. We out one show as the test set, randomly held out an-
extracted features related to pausing and overlaps other show as the dev set, trained models on the
between consecutive turns, the absolute and relative rest of the data, and tested the model on the held-
duration of consecutive turns, and so on. out show. We iterated through all shows and com-
We used a set of prosodic features including puted the overall accuracy. Table 1 shows the re-
pause, duration, and the speech rate of a speaker. We sults of (dis)agreement detection using all features
also used pitch and energy of the voice. Prosodic except prosodic features. We compared two condi-
features were computed on words and phonetic tions: (1) features extracted completely from the au-
alignment of manual transcripts. Features are com- tomatic LUC annotations and automatically detected
puted for the beginning and ending words of an ut- speaker roles, and (2) features from manual speaker
terance. For the duration features, we used the aver- role labels and manual LUC annotations when man-
age and maximum vowel duration from forced align- ual annotations are available. Table 1 showed that
ment, both unnormalized and normalized for vowel running a fully automatic system to generate auto-
identity and phone context. For pitch and energy, we matic annotations and automatic speaker roles pro-
376
duced comparable performance to the system using
features from manual annotations whenever avail-
able.
</bodyText>
<tableCaption confidence="0.961513166666667">
Table 1: Precision (%), recall (%), and F1 (%) of
(dis)agreement detection using features extracted from
manual speaker role labels and manual LUC annota-
tions when available, denoted Manual Annotation, and
automatic LUC annotations and automatically detected
speaker roles, denoted Automatic Annotation.
</tableCaption>
<table confidence="0.999527875">
Agreement
P R F1
Manual Annotation 81.5 43.2 56.5
Automatic Annotation 79.5 44.6 57.1
Disagreement
P R F1
Manual Annotation 70.1 38.5 49.7
Automatic Annotation 64.3 36.6 46.6
</table>
<bodyText confidence="0.999783857142857">
We then focused on the condition of using fea-
tures from manual annotations when available and
added prosodic features as described in Section 3.
The results are shown in Table 2. Adding prosodic
features produced a 0.7% absolute gain on F1 on
agreement detection, and 1.5% absolute gain on F1
on disagreement detection.
</bodyText>
<tableCaption confidence="0.978728333333333">
Table 2: Precision (%), recall (%), and F1 (%) of
(dis)agreement detection using manual annotations with-
out and with prosodic features.
</tableCaption>
<table confidence="0.99605725">
Agreement
P R F1
w/o prosodic 81.5 43.2 56.5
with prosodic 81.8 44.0 57.2
Disagreement
P R F1
w/o prosodic 70.1 38.5 49.7
with prosodic 70.8 40.1 51.2
</table>
<bodyText confidence="0.999039054054054">
Note that only about 10% utterances among all
data are involved in (dis)agreement. This indicates
a highly imbalanced data set as one class is more
heavily represented than the other/others. We sus-
pected that this high imbalance has played a ma-
jor role in the high precision and low recall results
we obtained so far. Various approaches have been
studied to handle imbalanced data for classifications,
trying to balance the class distribution in the train-
ing set by either oversampling the minority class or
downsampling the majority class. In this prelimi-
nary study of sampling approaches for handling im-
balanced data for CRF training, we investigated two
approaches, random downsampling and ensemble
downsampling. Random downsampling randomly
downsamples the majority class to equate the num-
ber of minority and majority class samples. Ensem-
ble downsampling is a refinement of random down-
sampling which doesn’t discard any majority class
samples. Instead, we partitioned the majority class
samples into N subspaces with each subspace con-
taining the same number of samples as the minority
class. Then we train N CRF models, each based
on the minority class samples and one disjoint parti-
tion from the N subspaces. During testing, the pos-
terior probability for one utterance is averaged over
the N CRF models. The results from these two sam-
pling approaches as well as the baseline are shown
in Table 3. Both sampling approaches achieved sig-
nificant improvement over the baseline, i.e., train-
ing on the original data set, and ensemble downsam-
pling produced better performance than downsam-
pling. We noticed that both sampling approaches
degraded slightly in precision but improved signif-
icantly in recall, resulting in 4.5% absolute gain on
F1 for agreement detection and 4.7% absolute gain
on F1 for disagreement detection.
</bodyText>
<tableCaption confidence="0.99783975">
Table 3: Precision (%), recall (%), and F1 (%) of
(dis)agreement detection without sampling, with random
downsampling and ensemble downsampling. Manual an-
notations and prosodic features are used.
</tableCaption>
<table confidence="0.9984767">
Agreement
P R F1
Baseline 81.8 44.0 57.2
Random downsampling 78.5 48.7 60.1
Ensemble downsampling 79.2 50.5 61.7
Disagreement
P R F1
Baseline 70.8 40.1 51.2
Random downsampling 67.3 44.8 53.8
Ensemble downsampling 69.2 46.9 55.9
</table>
<bodyText confidence="0.9449845">
In conclusion, this paper presents our work on
detection of agreements and disagreements in En-
</bodyText>
<page confidence="0.994159">
377
</page>
<bodyText confidence="0.999405315789474">
glish broadcast conversation data. We explored a
variety of features, including lexical, structural, du-
rational, and prosodic features. We experimented
these features using a linear-chain conditional ran-
dom fields model and conducted supervised train-
ing. We observed significant improvement from
adding prosodic features and employing two sam-
pling approaches, random downsampling and en-
semble downsampling. Overall, we achieved 79.2%
(precision), 50.5% (recall), 61.7% (F1) for agree-
ment detection and 69.2% (precision), 46.9% (re-
call), and 55.9% (F1) for disagreement detection, on
English broadcast conversation data. In future work,
we plan to continue adding and refining features, ex-
plore dependencies between features and contextual
cues with respect to agreements and disagreements,
and investigate the efficacy of other machine learn-
ing approaches such as Bayesian networks and Sup-
port Vector Machines.
</bodyText>
<sectionHeader confidence="0.997447" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999971230769231">
The authors thank Gokhan Tur and Dilek Hakkani-
T¨ur for valuable insights and suggestions. This
work has been supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via
Army Research Laboratory (ARL) contract num-
ber W911NF-09-C-0089. The U.S. Government is
authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copy-
right annotation thereon. The views and conclusions
contained herein are those of the authors and should
not be interpreted as necessarily representing the of-
ficial policies or endorsements, either expressed or
implied, of IARPA, ARL, or the U.S. Government.
</bodyText>
<sectionHeader confidence="0.99928" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999666027777778">
M. Galley, K. McKeown, J. Hirschberg, and E. Shriberg.
2004. Identifying agreement and disagreement in con-
versational speech: Use ofbayesian networks to model
pragmatic dependencies. In Proceedings ofACL.
S. Germesin and T. Wilson. 2009. Agreement detection
in multiparty conversation. In Proceedings ofInterna-
tional Conference on Multimodal Interfaces.
S. Hahn, R. Ladner, and M. Ostendorf. 2006. Agree-
ment/disagreement classification: Exploiting unla-
beled data using constraint classifiers. In Proceedings
ofHLT/NAACL.
D. Hillard, M. Ostendorf, and E. Shriberg. 2003. De-
tection of agreement vs. disagreement in meetings:
Training with unlabeled data. In Proceedings of
HLT/NAACL.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke,
and C. Wooters. 2003. The ICSI Meeting Corpus. In
Proc. ICASSP, Hong Kong, April.
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Dustin
Hillard, Mari Ostendorf, and Mary Harper. 2006.
Enriching speech recognition with automatic detec-
tion of sentence boundaries and disfluencies. IEEE
Transactions on Audio, Speech, and Language Pro-
cessing, 14(5):1526–1540, September. Special Issue
on Progress in Rich Transcription.
Andrew McCallum. 2002. Mallet: A machine learning
for language toolkit. http://mallet.cs.umass.edu.
I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bour-
ban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec,
V. Karaiskos, M. Kronenthal, G. Lathoud, M. Lincoln,
A. Lisowska, W. Post, D. Reidsma, and P. Wellner.
2005. The AMI meeting corpus. In Proceedings of
Measuring Behavior 2005, the 5th International Con-
ference on Methods and Techniques in Behavioral Re-
search.
</reference>
<page confidence="0.998245">
378
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.819280">
<title confidence="0.901794">Detection of Agreement and Disagreement in Broadcast Conversations</title>
<address confidence="0.983783">International, 333 Ravenswood Avenue, Menlo Park, CA 94025, T. J. Watson Research Center P.O.Box 218, Yorktown Heights, NY 10598, of California, Santa Barbara, CA, USA</address>
<email confidence="0.99992">syaman@us.ibm.com,graymond@soc.ucsb.edu</email>
<abstract confidence="0.996906043478261">We present Conditional Random Fields based approaches for detecting agreement/disagreement between speakers in English broadcast conversation shows. We develop annotation approaches for a variety of linguistic phenomena. Various lexical, structural, durational, and prosodic features are explored. We compare the performance when using features extracted from automatically generated annotations against that when using human annotations. We investigate the efficacy of adding prosodic features on top of lexical, structural, and durational features. Since the training data is highly imbalanced, we explore two sampling approaches, random downsampling and ensemble downsampling. Overall, our approach achieves 79.2% (precision), 50.5% (recall), 61.7% (F1) for agreement detection and 69.2% (precision), 46.9% (recall), and 55.9% (F1) for disagreement detection, on the English broadcast conversation data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
<author>J Hirschberg</author>
<author>E Shriberg</author>
</authors>
<title>Identifying agreement and disagreement in conversational speech: Use ofbayesian networks to model pragmatic dependencies.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1961" citStr="Galley et al., 2004" startWordPosition="258" endWordPosition="261">roduction In this work, we present models for detecting agreement/disagreement (denoted (dis)agreement) between speakers in English broadcast conversation shows. The Broadcast Conversation (BC) genre differs from the Broadcast News (BN) genre in that it is more interactive and spontaneous, referring to free speech in news-style TV and radio programs and consisting of talk shows, interviews, call-in programs, live reports, and round-tables. Previous This work was performed while the author was at ICSI. work on detecting (dis)agreements has been focused on meeting data. (Hillard et al., 2003), (Galley et al., 2004), (Hahn et al., 2006) used spurt-level agreement annotations from the ICSI meeting corpus (Janin et al., 2003). (Hillard et al., 2003) explored unsupervised machine learning approaches and on manual transcripts, they achieved an overall 3-way agreement/disagreement classification accuracy as 82% with keyword features. (Galley et al., 2004) explored Bayesian Networks for the detection of (dis)agreements. They used adjacency pair information to determine the structure of their conditional Markov model and outperformed the results of (Hillard et al., 2003) by improving the 3- way classification a</context>
<context position="3938" citStr="Galley et al., 2004" startWordPosition="551" endWordPosition="554">ast conversation. Due to the difference in publicity and intimacy/collegiality between speakers in broadcast conversations vs. meetings, (dis)agreement may have different character374 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 374–378, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics istics. Different from the unsupervised approaches in (Hillard et al., 2003) and semi-supervised approaches in (Hahn et al., 2006), we conducted supervised training. Also, different from (Hillard et al., 2003) and (Galley et al., 2004), our classification was carried out on the utterance level, instead of on the spurt-level. Galley et al. extended Hillard et al.’s work by adding features from previous spurts and features from the general dialog context to infer the class of the current spurt, on top of features from the current spurt (local features) used by Hillard et al. Galley et al. used adjacency pairs to describe the interaction between speakers and the relations between consecutive spurts. In this preliminary study on broadcast conversation, we directly modeled (dis)agreement detection without using adjacency pairs. </context>
</contexts>
<marker>Galley, McKeown, Hirschberg, Shriberg, 2004</marker>
<rawString>M. Galley, K. McKeown, J. Hirschberg, and E. Shriberg. 2004. Identifying agreement and disagreement in conversational speech: Use ofbayesian networks to model pragmatic dependencies. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Germesin</author>
<author>T Wilson</author>
</authors>
<title>Agreement detection in multiparty conversation.</title>
<date>2009</date>
<booktitle>In Proceedings ofInternational Conference on Multimodal Interfaces.</booktitle>
<contexts>
<context position="2803" citStr="Germesin and Wilson, 2009" startWordPosition="381" endWordPosition="385"> achieved an overall 3-way agreement/disagreement classification accuracy as 82% with keyword features. (Galley et al., 2004) explored Bayesian Networks for the detection of (dis)agreements. They used adjacency pair information to determine the structure of their conditional Markov model and outperformed the results of (Hillard et al., 2003) by improving the 3- way classification accuracy into 86.9%. (Hahn et al., 2006) explored semi-supervised learning algorithms and reached a competitive performance of 86.7% 3-way classification accuracy on manual transcriptions with only lexical features. (Germesin and Wilson, 2009) investigated supervised machine learning techniques and yields competitive results on the annotated data from the AMI meeting corpus (McCowan et al., 2005). Our work differs from these previous studies in two major categories. One is that a different definition of (dis)agreement was used. In the current work, a (dis)agreement occurs when a responding speaker agrees with, accepts, or disagrees with or rejects, a statement or proposition by a first speaker. Second, we explored (dis)agreement detection in broadcast conversation. Due to the difference in publicity and intimacy/collegiality betwee</context>
</contexts>
<marker>Germesin, Wilson, 2009</marker>
<rawString>S. Germesin and T. Wilson. 2009. Agreement detection in multiparty conversation. In Proceedings ofInternational Conference on Multimodal Interfaces.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hahn</author>
<author>R Ladner</author>
<author>M Ostendorf</author>
</authors>
<title>Agreement/disagreement classification: Exploiting unlabeled data using constraint classifiers.</title>
<date>2006</date>
<booktitle>In Proceedings ofHLT/NAACL.</booktitle>
<contexts>
<context position="1982" citStr="Hahn et al., 2006" startWordPosition="262" endWordPosition="265"> we present models for detecting agreement/disagreement (denoted (dis)agreement) between speakers in English broadcast conversation shows. The Broadcast Conversation (BC) genre differs from the Broadcast News (BN) genre in that it is more interactive and spontaneous, referring to free speech in news-style TV and radio programs and consisting of talk shows, interviews, call-in programs, live reports, and round-tables. Previous This work was performed while the author was at ICSI. work on detecting (dis)agreements has been focused on meeting data. (Hillard et al., 2003), (Galley et al., 2004), (Hahn et al., 2006) used spurt-level agreement annotations from the ICSI meeting corpus (Janin et al., 2003). (Hillard et al., 2003) explored unsupervised machine learning approaches and on manual transcripts, they achieved an overall 3-way agreement/disagreement classification accuracy as 82% with keyword features. (Galley et al., 2004) explored Bayesian Networks for the detection of (dis)agreements. They used adjacency pair information to determine the structure of their conditional Markov model and outperformed the results of (Hillard et al., 2003) by improving the 3- way classification accuracy into 86.9%. (</context>
<context position="3833" citStr="Hahn et al., 2006" startWordPosition="534" endWordPosition="537">, a statement or proposition by a first speaker. Second, we explored (dis)agreement detection in broadcast conversation. Due to the difference in publicity and intimacy/collegiality between speakers in broadcast conversations vs. meetings, (dis)agreement may have different character374 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 374–378, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics istics. Different from the unsupervised approaches in (Hillard et al., 2003) and semi-supervised approaches in (Hahn et al., 2006), we conducted supervised training. Also, different from (Hillard et al., 2003) and (Galley et al., 2004), our classification was carried out on the utterance level, instead of on the spurt-level. Galley et al. extended Hillard et al.’s work by adding features from previous spurts and features from the general dialog context to infer the class of the current spurt, on top of features from the current spurt (local features) used by Hillard et al. Galley et al. used adjacency pairs to describe the interaction between speakers and the relations between consecutive spurts. In this preliminary stud</context>
</contexts>
<marker>Hahn, Ladner, Ostendorf, 2006</marker>
<rawString>S. Hahn, R. Ladner, and M. Ostendorf. 2006. Agreement/disagreement classification: Exploiting unlabeled data using constraint classifiers. In Proceedings ofHLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hillard</author>
<author>M Ostendorf</author>
<author>E Shriberg</author>
</authors>
<title>Detection of agreement vs. disagreement in meetings: Training with unlabeled data.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="1938" citStr="Hillard et al., 2003" startWordPosition="254" endWordPosition="257">conversation data. 1 Introduction In this work, we present models for detecting agreement/disagreement (denoted (dis)agreement) between speakers in English broadcast conversation shows. The Broadcast Conversation (BC) genre differs from the Broadcast News (BN) genre in that it is more interactive and spontaneous, referring to free speech in news-style TV and radio programs and consisting of talk shows, interviews, call-in programs, live reports, and round-tables. Previous This work was performed while the author was at ICSI. work on detecting (dis)agreements has been focused on meeting data. (Hillard et al., 2003), (Galley et al., 2004), (Hahn et al., 2006) used spurt-level agreement annotations from the ICSI meeting corpus (Janin et al., 2003). (Hillard et al., 2003) explored unsupervised machine learning approaches and on manual transcripts, they achieved an overall 3-way agreement/disagreement classification accuracy as 82% with keyword features. (Galley et al., 2004) explored Bayesian Networks for the detection of (dis)agreements. They used adjacency pair information to determine the structure of their conditional Markov model and outperformed the results of (Hillard et al., 2003) by improving the </context>
<context position="3779" citStr="Hillard et al., 2003" startWordPosition="525" endWordPosition="528">peaker agrees with, accepts, or disagrees with or rejects, a statement or proposition by a first speaker. Second, we explored (dis)agreement detection in broadcast conversation. Due to the difference in publicity and intimacy/collegiality between speakers in broadcast conversations vs. meetings, (dis)agreement may have different character374 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 374–378, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics istics. Different from the unsupervised approaches in (Hillard et al., 2003) and semi-supervised approaches in (Hahn et al., 2006), we conducted supervised training. Also, different from (Hillard et al., 2003) and (Galley et al., 2004), our classification was carried out on the utterance level, instead of on the spurt-level. Galley et al. extended Hillard et al.’s work by adding features from previous spurts and features from the general dialog context to infer the class of the current spurt, on top of features from the current spurt (local features) used by Hillard et al. Galley et al. used adjacency pairs to describe the interaction between speakers and the relation</context>
</contexts>
<marker>Hillard, Ostendorf, Shriberg, 2003</marker>
<rawString>D. Hillard, M. Ostendorf, and E. Shriberg. 2003. Detection of agreement vs. disagreement in meetings: Training with unlabeled data. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Janin</author>
<author>D Baron</author>
<author>J Edwards</author>
<author>D Ellis</author>
<author>D Gelbart</author>
<author>N Morgan</author>
<author>B Peskin</author>
<author>T Pfau</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>C Wooters</author>
</authors>
<title>The ICSI Meeting Corpus.</title>
<date>2003</date>
<booktitle>In Proc. ICASSP,</booktitle>
<location>Hong Kong,</location>
<contexts>
<context position="2071" citStr="Janin et al., 2003" startWordPosition="276" endWordPosition="279"> speakers in English broadcast conversation shows. The Broadcast Conversation (BC) genre differs from the Broadcast News (BN) genre in that it is more interactive and spontaneous, referring to free speech in news-style TV and radio programs and consisting of talk shows, interviews, call-in programs, live reports, and round-tables. Previous This work was performed while the author was at ICSI. work on detecting (dis)agreements has been focused on meeting data. (Hillard et al., 2003), (Galley et al., 2004), (Hahn et al., 2006) used spurt-level agreement annotations from the ICSI meeting corpus (Janin et al., 2003). (Hillard et al., 2003) explored unsupervised machine learning approaches and on manual transcripts, they achieved an overall 3-way agreement/disagreement classification accuracy as 82% with keyword features. (Galley et al., 2004) explored Bayesian Networks for the detection of (dis)agreements. They used adjacency pair information to determine the structure of their conditional Markov model and outperformed the results of (Hillard et al., 2003) by improving the 3- way classification accuracy into 86.9%. (Hahn et al., 2006) explored semi-supervised learning algorithms and reached a competitive</context>
</contexts>
<marker>Janin, Baron, Edwards, Ellis, Gelbart, Morgan, Peskin, Pfau, Shriberg, Stolcke, Wooters, 2003</marker>
<rawString>A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters. 2003. The ICSI Meeting Corpus. In Proc. ICASSP, Hong Kong, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
<author>Dustin Hillard</author>
<author>Mari Ostendorf</author>
<author>Mary Harper</author>
</authors>
<title>Enriching speech recognition with automatic detection of sentence boundaries and disfluencies.</title>
<date>2006</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<booktitle>Special Issue on Progress in Rich Transcription.</booktitle>
<volume>14</volume>
<issue>5</issue>
<marker>Liu, Shriberg, Stolcke, Hillard, Ostendorf, Harper, 2006</marker>
<rawString>Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Dustin Hillard, Mari Ostendorf, and Mary Harper. 2006. Enriching speech recognition with automatic detection of sentence boundaries and disfluencies. IEEE Transactions on Audio, Speech, and Language Processing, 14(5):1526–1540, September. Special Issue on Progress in Rich Transcription.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<marker>McCallum, 2002</marker>
<rawString>Andrew McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I McCowan</author>
<author>J Carletta</author>
<author>W Kraaij</author>
<author>S Ashby</author>
<author>S Bourban</author>
<author>M Flynn</author>
<author>M Guillemot</author>
<author>T Hain</author>
<author>J Kadlec</author>
<author>V Karaiskos</author>
<author>M Kronenthal</author>
<author>G Lathoud</author>
<author>M Lincoln</author>
<author>A Lisowska</author>
<author>W Post</author>
<author>D Reidsma</author>
<author>P Wellner</author>
</authors>
<title>The AMI meeting corpus.</title>
<date>2005</date>
<booktitle>In Proceedings of Measuring Behavior 2005, the 5th International Conference on Methods and Techniques in Behavioral Research.</booktitle>
<contexts>
<context position="2959" citStr="McCowan et al., 2005" startWordPosition="405" endWordPosition="409">detection of (dis)agreements. They used adjacency pair information to determine the structure of their conditional Markov model and outperformed the results of (Hillard et al., 2003) by improving the 3- way classification accuracy into 86.9%. (Hahn et al., 2006) explored semi-supervised learning algorithms and reached a competitive performance of 86.7% 3-way classification accuracy on manual transcriptions with only lexical features. (Germesin and Wilson, 2009) investigated supervised machine learning techniques and yields competitive results on the annotated data from the AMI meeting corpus (McCowan et al., 2005). Our work differs from these previous studies in two major categories. One is that a different definition of (dis)agreement was used. In the current work, a (dis)agreement occurs when a responding speaker agrees with, accepts, or disagrees with or rejects, a statement or proposition by a first speaker. Second, we explored (dis)agreement detection in broadcast conversation. Due to the difference in publicity and intimacy/collegiality between speakers in broadcast conversations vs. meetings, (dis)agreement may have different character374 Proceedings of the 49th Annual Meeting of the Association</context>
</contexts>
<marker>McCowan, Carletta, Kraaij, Ashby, Bourban, Flynn, Guillemot, Hain, Kadlec, Karaiskos, Kronenthal, Lathoud, Lincoln, Lisowska, Post, Reidsma, Wellner, 2005</marker>
<rawString>I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska, W. Post, D. Reidsma, and P. Wellner. 2005. The AMI meeting corpus. In Proceedings of Measuring Behavior 2005, the 5th International Conference on Methods and Techniques in Behavioral Research.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>