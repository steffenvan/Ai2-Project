<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002343">
<title confidence="0.673237">
Named Entity Recognition in Biomedical Texts using an HMM Model
</title>
<author confidence="0.991469">
Shaojun Zhao
</author>
<affiliation confidence="0.998725">
Department of Computing Science
University of Alberta
</affiliation>
<address confidence="0.583241">
Edmonton, Canada, T6G 2H8
</address>
<email confidence="0.973995">
shaojun@cs.ualberta.ca
</email>
<sectionHeader confidence="0.991508" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999813">
Although there exists a huge number of
biomedical texts online, there is a lack of tools
good enough to help people get information or
knowledge from them. Named entity
Recognition (NER) becomes very important
for further processing like information
retrieval, information extraction and
knowledge discovery. We introduce a Hidden
Markov Model (HMM) for NER, with a word
similarity-based smoothing. Our experiment
shows that the word similarity-based
smoothing can improve the performance by
using huge unlabeled data. While many
systems have laboriously hand-coded rules for
all kinds of word features, we show that word
similarity is a potential method to
automatically get word formation, prefix,
suffix and abbreviation information
automatically from biomedical texts, as well
as useful word distribution information.
</bodyText>
<sectionHeader confidence="0.998693" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999878434782608">
In the Message Understanding Conference
(MUC), Named entity Recognition aims to classify
proper nouns, dates, time, measures and locations,
etc. Many researchers adapt their systems from
MUC to the biomedical domain, such as (Fukuda
et al 1998), (Proux et al 1998), (Nobata et al 2000),
(Collier et al 2000), (Gaizauskas et al 2000),
(Kazama et al 2002), (Takeuchi et al 2002), (Lee
et al 2003) and (Zhou et al 2004). As opposed to
rule-based systems, machine learning-based
systems could train their models on labeled data.
But due to the irregular forms of biomedical texts,
people still need to carefully choose word features
for their systems. This work requires domain
specific knowledge. How to get the domain
knowledge automatically is a question that has not
been fully investigated. Our system is built on an
HMM model with the words themselves as the
features. Huge unlabeled corpus is gathered from
MEDLINE. Word similarity information is
computed from the corpus and we use a word
similarity-based smoothing to overcome the data
sparseness.
</bodyText>
<sectionHeader confidence="0.98577" genericHeader="method">
2 Data Preparation
</sectionHeader>
<subsectionHeader confidence="0.611594">
2.1 Labeled Data
</subsectionHeader>
<bodyText confidence="0.999958727272727">
Our labeled data is from GENIA 3.02 (Ohta et al
2002), which contains 2,000 abstracts (360K
words). It has been annotated with semantic
information such as DNA, protein annotations.
These are useful for training models. It contains
Part of Speech (POS) information as well.
Although POS is not considered very useful for
NER in newspaper articles, it can dramatically
improve the performance of NER in biomedical
texts (Zhou et al 2004). Our model is trained from
this labeled data.
</bodyText>
<subsectionHeader confidence="0.999369">
2.2 Unlabeled Data
</subsectionHeader>
<bodyText confidence="0.999937">
We downloaded 17G XML abstract data from
MEDLINE, which contains 1,381,132 abstracts.
Compared to the labeled data, we have far more
unlabeled data, and the amount of available
unlabeled data increases every day. We used this
unlabeled data for computing word similarity. We
extracted 66,303,526 proximity relationships from
the unlabeled data.
</bodyText>
<sectionHeader confidence="0.996197" genericHeader="method">
3 Distributional Word Similarity
</sectionHeader>
<bodyText confidence="0.9999493125">
“Words that tend to appear in the same contexts
tend to have similar meanings.” (Harris 1968). For
example, the words corruption and abuse are
similar because both of them can be subjects of
verbs like arouse, become, betray, cause, continue,
cost, exist, force, go on, grow, have, increase, lead
to, and persist, etc, and both of them can modify
nouns like accusation, act, allegation, appearance,
and case, etc.
Many methods have been proposed to compute
distributional similarity between words, e.g.,
(Hindle, 1990), (Pereira et al. 1993), (Grefenstette
1994) and (Lin 1998). Almost all of the methods
represent a word by a feature vector where each
feature corresponds to a type of context in which
the word appeared.
</bodyText>
<page confidence="0.996609">
84
</page>
<subsectionHeader confidence="0.994294">
3.1 Proximity-based Similarity
</subsectionHeader>
<bodyText confidence="0.99971347368421">
It is natural to use dependency relationship
(Mel&apos;čuk, 1987) as features, but a parser has to be
available. Since biomedical text is highly irregular,
and is very different from text like newspaper, a
parser developed for the newspaper domain may
not perform very well on biomedical text. Since
most dependency relationships involve words that
are situated close to one another, the dependency
relationships can often be approximated by co-
occurrence relationships within a small window
(Turney 2001); (Terra and Clarke 2003). We
define the features of the word w to be the first
non-stop word on either side of w and the
intervening stop words (which can be defined as
the top-k most frequent words in the corpus). For
example, for a sentence “He got a job from this
company.” (Considering a, from and this to be stop
words.), the features of job provided by this
sentence are shown in Table 1.
</bodyText>
<table confidence="0.990144714285714">
Features Frequency
(left, got) 0.50
(left, a) 0.50
(right ,from) 0.33
(right, this) 0.33
(right, company) 0.33
... ...
</table>
<tableCaption confidence="0.960146">
Table 1: Features for word “job”
</tableCaption>
<subsectionHeader confidence="0.999649">
3.2 Computing Word Similarity
</subsectionHeader>
<bodyText confidence="0.9998368">
Once the contexts of a word are represented as a
feature vector, the similarity between two words
can be computed using their context vectors. We
use (u1, u2 ... un) and (v1, v2 ... vn) to denote the
feature vectors for the words u and v respectively,
where n is the number of feature types extracted
from a corpus. We use fi to denote the ith feature.
The point-wise mutual information (PMI)
between a feature fi and a word u measures the
strength association between them. It is defined as:
</bodyText>
<equation confidence="0.9944909">
 P f u
, 
( )
, log  ( ) ( )
pmi ( )
i
i = P f P u
f u
 ×
i 
</equation>
<bodyText confidence="0.999901166666667">
where P(fi,u) is the probability of fi co-occurring
with u; P(fi) is the probability of fi co-occurring
with any word; and P(u) is the probability of any
feature co-occurring with u.
The similarity between word u and v is defined
as the Cosine of PMI:
</bodyText>
<equation confidence="0.97994975">
n
∑ pmi f u pmi f v
( ) ( )
, × ,
i
( ) = 1 i i
u v
, =
</equation>
<bodyText confidence="0.9997183125">
Different similarity measures of distributional
similarity can affect the quality of the result to s
statistically significant degree. (Zhao and Lin 2004)
shows that the Cosine of PMI is a significantly
better similarity measure than several other
commonly used similarity measures.
Similar words are computed for each word in the
unlabeled data. Only a subset of the similarity
information is useful, because the similarity of
words outside of the training data and test data
vocabulary is not used. We only take into account
the similar words that occur in the training data
more than 10 times and only those word pairs
which have point-wise mutual information greater
than a threshold (0.04). Table 2 shows the
computing result for “IL-0”1:
</bodyText>
<figure confidence="0.949158636363636">
Similar Words Similarity
interleukin-0 0.510891
IL-00 0.486665
IFN-gamma 0.44945
TNF-alpha 0.44702
GM-CSF 0.438226
TNF 0.37703
IL-0beta 0.365072
interferon-gamma 0.350704
IL0 0.336974
... ...
</figure>
<tableCaption confidence="0.923182">
Table 2: Similar words for “IL-0”
</tableCaption>
<bodyText confidence="0.9906795">
Table 2 also shows that the similar words can
capture word formation (IL-00, IL-0beta, and IL0
etc) and abbreviation (interleukin-0) information.
A complete list of these word pairs and their
similarity is available online 2 . The rule-based
system may not able to capture words like IL-0ra,
IL-0Ralpha, which are in the similar word list of
IL-0, and it is very likely that they belong to the
same semantic category. Many different kinds of
expressions for numbers (like 0, 00-00, 00.00, -00,
0/0, five, six, 0-, iii, IV etc) are grouped together
automatically.
</bodyText>
<sectionHeader confidence="0.992523" genericHeader="method">
4 HMM Model and Smoothing Schema
</sectionHeader>
<bodyText confidence="0.999995083333333">
We follow the HMM model introduced in (Zhou
et al 2004). The structure of an HMM model
contains States and observations. In our model,
each state is represented by a semantic tag, or a
POS tag if the semantic tag is not available; each
observation contains a word sequence. The main
computing difficulty in (Zhou et al 2004) is the
probability of a tag given a word sequence:
formula (1). We use formula (2) to estimate
formula (1). If the bigram is unseen in the training
data, we use formula (3). If the unigram is also
unseen, we use the unknown information which is
</bodyText>
<footnote confidence="0.8852865">
1 We changed any single digit to 0.
2 http://www.cs.ualberta.ca/~shaojun/biolist.txt
</footnote>
<equation confidence="0.968136363636363">
word
sim
n
∑pmi
i=1
( ) ∑ ( )
2 n 2
f u
, × pmi f v
,
i i = 1 i
</equation>
<page confidence="0.985017">
85
</page>
<bodyText confidence="0.989103">
gathered from the low frequency words in the
training data.
</bodyText>
<equation confidence="0.990897428571429">
P tag wordsequen ce
(  |)
t
P tag word word
(  |, )
t t t+ 1
P(tagt  |wordt) (3)
</equation>
<bodyText confidence="0.9521965">
We find that about 26% of the bigrams (wordt,
wordt+1) in the testing data is unseen, so the
smoothing is critical.
In order to compute formula (1), we can use the
back-off (Katz 1987); (Bikel et al 1999) approach.
Baseline1 and Baseline2 in our system use
different back-off schema.
The following formula is introduced in (Lee
1999) for word similarity-based smoothing:
sim w w P tag w
</bodyText>
<equation confidence="0.999348333333333">
( , ) (  |)
′ ′
t t t t
</equation>
<bodyText confidence="0.999899818181818">
where S(w) is a set of candidate similar words and
sim(w,w’) is the similarity between word w and w’.
Word similarity-based smoothing approach is used
in our system to make advantage of the huge
unlabeled corpus. In order to plug the word
similarity-based smoothing into our HMM model,
we made several extensions to formula (4).
For each word w, we define p as the distribution
of w’s tags, which are annotated in the training
data. We use the KL-Divergence to compute the
distance between two distributions:
</bodyText>
<equation confidence="0.996074125">
= ∑ ( ( ) )
KLD ( ) ( ) ( )
p x
p p 1
1  ||2 p x
1 log p x
2
x
</equation>
<bodyText confidence="0.9999705">
We define the similarity between the tag
distributions of word w and w’ as:
</bodyText>
<equation confidence="0.995343">
1
1+ KLD(P(tag w) P(tag w ))
     |′
</equation>
<bodyText confidence="0.9995">
The harmonic average of word similarity and tag
distribution similarity is defined as the similarity of
word w and w’ used in our system.
</bodyText>
<equation confidence="0.9109115">
2sim w w sim w w
( ) ( )
, ′ × , ′
word tag
s w w
( )
, ′ =
simword (w, w′ )+ simtag ( w, w′ )
</equation>
<bodyText confidence="0.888202">
So, we get formula (5) and (6). Formula (5) is
for bigram smoothing and formula (6) is for
unigram smoothing.
</bodyText>
<equation confidence="0.979231428571429">
∑s w w P tag w w
( ) (
, ′  |, ′ )
t + 1 t + 1 t t t + 1
w S w
′ ∈ ( )
t + 1 t + 1
s w w
( )
, ′
t+1 t +1
w t ′ ∈
+1
P(tagt  |wt )
</equation>
<bodyText confidence="0.917437">
Because it is natural to back-off from bigram to
unigram, in our system, a back-off smoothing
approach is combined with the word similarity-
based smoothing. We follow these procedures to
compute formula (1).
</bodyText>
<listItem confidence="0.992382428571428">
1. Check the frequency of the bigram (wt, wt+1).
If the frequency is high (&gt;10), use formula
(2). Stop.
2. Check the frequency of the unigram (wt). If
the frequency of the unigram is high (&gt;30),
use formula (3). Stop.
3. Try formula (5) for bigram smoothing, and
check the frequency summary of the similar
words involved in the smoothing. If the
summary is high (&gt;10), use formula (5).
Stop.
4. Try formula (6) for unigram smoothing, and
check the frequency summary for this case.
If the summary is high (&gt;30), use formula
(6). Stop.
5. If the bigram is not unseen, use formula (2).
Stop.
6. If the unigram is not unseen, use formula (3).
Stop.
7. Use low frequency (&lt;5) word information in
the training data and formula (3).
</listItem>
<bodyText confidence="0.9622065">
Our Baseline1 uses step 5, 6 and 7; Baseline2
uses step 1, 2, 5, 6 and 7.
</bodyText>
<sectionHeader confidence="0.937825" genericHeader="evaluation">
5 Experiment Result
</sectionHeader>
<bodyText confidence="0.595007">
The experiment results are shown in Table 3:
</bodyText>
<table confidence="0.99938425">
Methods R P F-score
Baseline1 64.77% 59.87% 62.22%
Baseline2 66.99% 61.25% 63.99%
Our system 69.41% 62.98% 66.04%
</table>
<tableCaption confidence="0.999741">
Table 3: Performance comparison
</tableCaption>
<bodyText confidence="0.999485333333333">
The Baseline2 outperforms Baseline1 because it
prevents from using low frequency unigrams, and
our system outperforms Baseline1 and Baseline2
because it prevents from using low frequency
bigrams and unigrams. Our system benefits from
huge unlabeled corpus.
</bodyText>
<sectionHeader confidence="0.995939" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999971666666667">
We trained an HMM model on labelled data to
recognize named entities in biomedical texts. Word
similarity information was computed from huge
unlabeled data. A word similarity-based smoothing
method was integrated into the system, and
improved the overall performance. We would like
to see if it could also be plugged into other existing
systems, and hopefully also improve their
performance.
We also argue that the automatically acquired
similar words are rich with word features, such as
word formation, prefix, suffix, abbreviation,
expression variation and clustering information.
We will further investigate the usefulness of them
in the future.
</bodyText>
<figure confidence="0.993762162162162">
)
w S
t ′∈
( )
w t
=
)
P(tag
 |w
t t
(4)
∑
sim(wt, w′t
( )
w t
w′t+1∈ S
∑
simtag (w, w′) =
P tag w w
(  |, )
t t t+1
=
s(wt,w′t)P(tagt  |w′t)
sw w
( )
, ′
t t
=
∑
w S w
( )
t ′ ∈
+ 1 t + 1
∑
w′t+1∈ S(wt+1)
∑
S(wt+1)
</figure>
<page confidence="0.977515">
86
</page>
<sectionHeader confidence="0.981954" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.872512142857143">
Thanks to Dekang Lin and other members in the
Natural Language Processing Group at the
University of Alberta for helpful discussion, the
anonymous reviewers for their insightful
comments. This material is based upon work
supported by the Alberta Ingenuity Centre for
Machine Learning (AICML).
</bodyText>
<sectionHeader confidence="0.998389" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999276484210527">
Bikel, D., Schwartz, R., Weischedel, R. 1999. An
Algorithm that Learns What&apos;s in a Name. In Proc.
of Machine Learning (Special Issue on NLP).
Collier, N., Nobata, C., Tsujii, J. 2000.
Extracting the names of genes and gene products
with a hidden Markov model. In Proc. of
COLING 2000, pages 201-207.
Fukuda, K., Tsunoda, T., Tamura, A., Takagi, T.
1998. “Toward Information extraction:
Identifying protein names from biological
papers”, in Proc. of the Pacific Symposium on
Biocomputing 98 (PSB 98), Hawaii
Gaizauskas, R., Demetriou, G., Humphreys, K.
2000. Term Recognition and Classification in
Biological Science Journal Articles. In Proc. of
the Computational Terminology for Medical and
Biological Applications Workshop of the 2nd
International Conference on NLP, pages 37-44.
Grefenstette, G. 1994. Explorations in Automatic
Thesaurus Discovery, Kluwer Academic
Publishers, Boston
Harris, Z.S. 1968. Mathematical Structures of
Language. New York: Wiley.
Hindle, D. 1990. Noun Classification from
Predicate-Argument Structures. In Proceedings
of ACL-90. pp. 268-275. Pittsburgh,
Pennsylvania
Katz, S.M. 1987. Estimation of Component of a
Speech Recognizer. IEEE Transactions on
Acoustics. Speech and Signal Processing. 35.
400-401.
Kazama, J., Makino, T., Ohta, Y., Tsujii, J. 2002.
Tuning Support Vector Machines for Biomedical
Named Entity Recognition. In Proc. of the
Workshop on Natural Language Processing in
the Biomedical Domain (at ACL’2002), pages 1-
8
Lee, K.J., Hwang, Y.S., Rim H.C. 2003. Two
phase biomedical NE Recognition based on
SVMs. In Proceedings of the ACL-03 Workshop
on Natural Language Processing in Biomedicine.
pp.33-40. Sapporo, Japan
Lee, L. 1999. Measures of distributional similarity.
In Proc. of the 37th Annual Meeting of the
Association for Computational Linguistics, 1999,
pp. 25-32.
Lin, D. 1998. Automatic Retrieval and Clustering
of Similar Words. In Proceedings of COLING-
ACL98. Montreal, Canada.
Mel&apos;čuk, I. A., 1987. Dependency Syntax: theory
and practice. State University of New York
Press. Albany, NY.
Nobata, C., Collier, N., Tsujii, J. 2000.
Comparison between Tagged Corpora for the
Named Entity Task. In the Proceedings of ACL
2000 Workshop on Comparing Corpora. Hong
Kong, China. pp. 20-27
Ohta, T., Tateisi, Y., Kim, J., Mima, H., Tsujii, J.
2002. The GENIA corpus: An annotated
research abstract corpus in molecular biology
domain. In Proc. of HLT 2002.
Pereira, F., Tishby, N., Lee, L. 1993.
Distributional Clustering of English Words. In
Proceedings of ACL-93. pp. 183-190. Columbus,
Ohio.
Proux, D., Rechenmann, F., Julliard, L., Pillet, V.,
Jacq, B. 1998. Detecting Gene Symbols and
Names in Biological Texts: A First Step toward
Pertinent Information Extraction. In Proc. of
Genome Inform Ser Workshop Genome Inform,
pages 72-80.
Takeuchi, K., Collier, N. 2002. Use of Support
Vector Machines in Extended Named Entity
Recognition. In Proc. of the Sixth Conference on
Natural Language Learning (CONLL 2002),
pages 119-125.
Terra, E. L., Clarke, C. 2003. Frequency Estimates
for Statistical Word Similarity Measures. In the
Proceedings of the 2003 Human Language
Technology Conference, pp.244-251. Edmonton,
Canada, May
Turney, P.D. 2001. Mining the Web for synonyms:
PMI-IR versus LSA on TOEFL, Proceedings of
the Twelfth European Conference on Machine
Learning (ECML-2001), Freiburg, Germany, pp.
491-502.
Zhou, G., Zhang, J., Su, J., Shen, D., Tan, C. 2004.
Recognizing names in biomedical texts: a
machine learning approach. Bioinformatics
Advance Access.
Zhao, S., Lin, D. 2004. A Nearest-Neighbor
Method for Resolving PP-Attachment Ambiguity.
In Proceedings of the First International Joint
Conference on Natural Language Processing,
2004. Sanya, China.
</reference>
<page confidence="0.999476">
87
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.937440">
<title confidence="0.99801">Named Entity Recognition in Biomedical Texts using an HMM Model</title>
<author confidence="0.95816">Shaojun</author>
<affiliation confidence="0.9997575">Department of Computing University of</affiliation>
<address confidence="0.990732">Edmonton, Canada, T6G</address>
<email confidence="0.99505">shaojun@cs.ualberta.ca</email>
<abstract confidence="0.999685714285714">Although there exists a huge number of biomedical texts online, there is a lack of tools good enough to help people get information or knowledge from them. Named entity Recognition (NER) becomes very important for further processing like information retrieval, information extraction and knowledge discovery. We introduce a Hidden Markov Model (HMM) for NER, with a word similarity-based smoothing. Our experiment shows that the word similarity-based smoothing can improve the performance by using huge unlabeled data. While many systems have laboriously hand-coded rules for all kinds of word features, we show that word similarity is a potential method to automatically get word formation, prefix, suffix and abbreviation information automatically from biomedical texts, as well as useful word distribution information.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Bikel</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>An Algorithm that Learns What&apos;s in a Name.</title>
<date>1999</date>
<booktitle>In Proc. of Machine Learning (Special Issue</booktitle>
<pages>201--207</pages>
<contexts>
<context position="8326" citStr="Bikel et al 1999" startWordPosition="1405" endWordPosition="1408">is unseen in the training data, we use formula (3). If the unigram is also unseen, we use the unknown information which is 1 We changed any single digit to 0. 2 http://www.cs.ualberta.ca/~shaojun/biolist.txt word sim n ∑pmi i=1 ( ) ∑ ( ) 2 n 2 f u , × pmi f v , i i = 1 i 85 gathered from the low frequency words in the training data. P tag wordsequen ce ( |) t P tag word word ( |, ) t t t+ 1 P(tagt |wordt) (3) We find that about 26% of the bigrams (wordt, wordt+1) in the testing data is unseen, so the smoothing is critical. In order to compute formula (1), we can use the back-off (Katz 1987); (Bikel et al 1999) approach. Baseline1 and Baseline2 in our system use different back-off schema. The following formula is introduced in (Lee 1999) for word similarity-based smoothing: sim w w P tag w ( , ) ( |) ′ ′ t t t t where S(w) is a set of candidate similar words and sim(w,w’) is the similarity between word w and w’. Word similarity-based smoothing approach is used in our system to make advantage of the huge unlabeled corpus. In order to plug the word similarity-based smoothing into our HMM model, we made several extensions to formula (4). For each word w, we define p as the distribution of w’s tags, whi</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>Bikel, D., Schwartz, R., Weischedel, R. 1999. An Algorithm that Learns What&apos;s in a Name. In Proc. of Machine Learning (Special Issue on NLP). Collier, N., Nobata, C., Tsujii, J. 2000. Extracting the names of genes and gene products with a hidden Markov model. In Proc. of COLING 2000, pages 201-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Fukuda</author>
<author>T Tsunoda</author>
<author>A Tamura</author>
<author>T Takagi</author>
</authors>
<title>Toward Information extraction: Identifying protein names from biological papers”,</title>
<date>1998</date>
<booktitle>in Proc. of the Pacific Symposium on Biocomputing 98 (PSB 98),</booktitle>
<location>Hawaii</location>
<contexts>
<context position="1270" citStr="Fukuda et al 1998" startWordPosition="179" endWordPosition="182">n improve the performance by using huge unlabeled data. While many systems have laboriously hand-coded rules for all kinds of word features, we show that word similarity is a potential method to automatically get word formation, prefix, suffix and abbreviation information automatically from biomedical texts, as well as useful word distribution information. 1 Introduction In the Message Understanding Conference (MUC), Named entity Recognition aims to classify proper nouns, dates, time, measures and locations, etc. Many researchers adapt their systems from MUC to the biomedical domain, such as (Fukuda et al 1998), (Proux et al 1998), (Nobata et al 2000), (Collier et al 2000), (Gaizauskas et al 2000), (Kazama et al 2002), (Takeuchi et al 2002), (Lee et al 2003) and (Zhou et al 2004). As opposed to rule-based systems, machine learning-based systems could train their models on labeled data. But due to the irregular forms of biomedical texts, people still need to carefully choose word features for their systems. This work requires domain specific knowledge. How to get the domain knowledge automatically is a question that has not been fully investigated. Our system is built on an HMM model with the words t</context>
</contexts>
<marker>Fukuda, Tsunoda, Tamura, Takagi, 1998</marker>
<rawString>Fukuda, K., Tsunoda, T., Tamura, A., Takagi, T. 1998. “Toward Information extraction: Identifying protein names from biological papers”, in Proc. of the Pacific Symposium on Biocomputing 98 (PSB 98), Hawaii</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Gaizauskas</author>
<author>G Demetriou</author>
<author>K Humphreys</author>
</authors>
<title>Term Recognition and Classification in Biological Science Journal Articles.</title>
<date>2000</date>
<booktitle>In Proc. of the Computational Terminology for Medical and Biological Applications Workshop of the 2nd International Conference on NLP,</booktitle>
<pages>37--44</pages>
<contexts>
<context position="1358" citStr="Gaizauskas et al 2000" startWordPosition="195" endWordPosition="198">riously hand-coded rules for all kinds of word features, we show that word similarity is a potential method to automatically get word formation, prefix, suffix and abbreviation information automatically from biomedical texts, as well as useful word distribution information. 1 Introduction In the Message Understanding Conference (MUC), Named entity Recognition aims to classify proper nouns, dates, time, measures and locations, etc. Many researchers adapt their systems from MUC to the biomedical domain, such as (Fukuda et al 1998), (Proux et al 1998), (Nobata et al 2000), (Collier et al 2000), (Gaizauskas et al 2000), (Kazama et al 2002), (Takeuchi et al 2002), (Lee et al 2003) and (Zhou et al 2004). As opposed to rule-based systems, machine learning-based systems could train their models on labeled data. But due to the irregular forms of biomedical texts, people still need to carefully choose word features for their systems. This work requires domain specific knowledge. How to get the domain knowledge automatically is a question that has not been fully investigated. Our system is built on an HMM model with the words themselves as the features. Huge unlabeled corpus is gathered from MEDLINE. Word similari</context>
</contexts>
<marker>Gaizauskas, Demetriou, Humphreys, 2000</marker>
<rawString>Gaizauskas, R., Demetriou, G., Humphreys, K. 2000. Term Recognition and Classification in Biological Science Journal Articles. In Proc. of the Computational Terminology for Medical and Biological Applications Workshop of the 2nd International Conference on NLP, pages 37-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery,</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston</location>
<contexts>
<context position="3557" citStr="Grefenstette 1994" startWordPosition="544" endWordPosition="545">om the unlabeled data. 3 Distributional Word Similarity “Words that tend to appear in the same contexts tend to have similar meanings.” (Harris 1968). For example, the words corruption and abuse are similar because both of them can be subjects of verbs like arouse, become, betray, cause, continue, cost, exist, force, go on, grow, have, increase, lead to, and persist, etc, and both of them can modify nouns like accusation, act, allegation, appearance, and case, etc. Many methods have been proposed to compute distributional similarity between words, e.g., (Hindle, 1990), (Pereira et al. 1993), (Grefenstette 1994) and (Lin 1998). Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared. 84 3.1 Proximity-based Similarity It is natural to use dependency relationship (Mel&apos;čuk, 1987) as features, but a parser has to be available. Since biomedical text is highly irregular, and is very different from text like newspaper, a parser developed for the newspaper domain may not perform very well on biomedical text. Since most dependency relationships involve words that are situated close to one another, the dependency relationship</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Grefenstette, G. 1994. Explorations in Automatic Thesaurus Discovery, Kluwer Academic Publishers, Boston</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z S Harris</author>
</authors>
<date>1968</date>
<booktitle>Mathematical Structures of Language.</booktitle>
<publisher>Wiley.</publisher>
<location>New York:</location>
<contexts>
<context position="3088" citStr="Harris 1968" startWordPosition="472" endWordPosition="473">ically improve the performance of NER in biomedical texts (Zhou et al 2004). Our model is trained from this labeled data. 2.2 Unlabeled Data We downloaded 17G XML abstract data from MEDLINE, which contains 1,381,132 abstracts. Compared to the labeled data, we have far more unlabeled data, and the amount of available unlabeled data increases every day. We used this unlabeled data for computing word similarity. We extracted 66,303,526 proximity relationships from the unlabeled data. 3 Distributional Word Similarity “Words that tend to appear in the same contexts tend to have similar meanings.” (Harris 1968). For example, the words corruption and abuse are similar because both of them can be subjects of verbs like arouse, become, betray, cause, continue, cost, exist, force, go on, grow, have, increase, lead to, and persist, etc, and both of them can modify nouns like accusation, act, allegation, appearance, and case, etc. Many methods have been proposed to compute distributional similarity between words, e.g., (Hindle, 1990), (Pereira et al. 1993), (Grefenstette 1994) and (Lin 1998). Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Harris, Z.S. 1968. Mathematical Structures of Language. New York: Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun Classification from Predicate-Argument Structures.</title>
<date>1990</date>
<booktitle>In Proceedings of ACL-90.</booktitle>
<pages>268--275</pages>
<location>Pittsburgh, Pennsylvania</location>
<contexts>
<context position="3513" citStr="Hindle, 1990" startWordPosition="538" endWordPosition="539">d 66,303,526 proximity relationships from the unlabeled data. 3 Distributional Word Similarity “Words that tend to appear in the same contexts tend to have similar meanings.” (Harris 1968). For example, the words corruption and abuse are similar because both of them can be subjects of verbs like arouse, become, betray, cause, continue, cost, exist, force, go on, grow, have, increase, lead to, and persist, etc, and both of them can modify nouns like accusation, act, allegation, appearance, and case, etc. Many methods have been proposed to compute distributional similarity between words, e.g., (Hindle, 1990), (Pereira et al. 1993), (Grefenstette 1994) and (Lin 1998). Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared. 84 3.1 Proximity-based Similarity It is natural to use dependency relationship (Mel&apos;čuk, 1987) as features, but a parser has to be available. Since biomedical text is highly irregular, and is very different from text like newspaper, a parser developed for the newspaper domain may not perform very well on biomedical text. Since most dependency relationships involve words that are situated close</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>Hindle, D. 1990. Noun Classification from Predicate-Argument Structures. In Proceedings of ACL-90. pp. 268-275. Pittsburgh, Pennsylvania</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Katz</author>
</authors>
<title>Estimation of Component of a Speech Recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics. Speech and Signal Processing.</journal>
<volume>35</volume>
<pages>400--401</pages>
<contexts>
<context position="8306" citStr="Katz 1987" startWordPosition="1403" endWordPosition="1404">f the bigram is unseen in the training data, we use formula (3). If the unigram is also unseen, we use the unknown information which is 1 We changed any single digit to 0. 2 http://www.cs.ualberta.ca/~shaojun/biolist.txt word sim n ∑pmi i=1 ( ) ∑ ( ) 2 n 2 f u , × pmi f v , i i = 1 i 85 gathered from the low frequency words in the training data. P tag wordsequen ce ( |) t P tag word word ( |, ) t t t+ 1 P(tagt |wordt) (3) We find that about 26% of the bigrams (wordt, wordt+1) in the testing data is unseen, so the smoothing is critical. In order to compute formula (1), we can use the back-off (Katz 1987); (Bikel et al 1999) approach. Baseline1 and Baseline2 in our system use different back-off schema. The following formula is introduced in (Lee 1999) for word similarity-based smoothing: sim w w P tag w ( , ) ( |) ′ ′ t t t t where S(w) is a set of candidate similar words and sim(w,w’) is the similarity between word w and w’. Word similarity-based smoothing approach is used in our system to make advantage of the huge unlabeled corpus. In order to plug the word similarity-based smoothing into our HMM model, we made several extensions to formula (4). For each word w, we define p as the distribut</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Katz, S.M. 1987. Estimation of Component of a Speech Recognizer. IEEE Transactions on Acoustics. Speech and Signal Processing. 35. 400-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kazama</author>
<author>T Makino</author>
<author>Y Ohta</author>
<author>J Tsujii</author>
</authors>
<title>Tuning Support Vector Machines for Biomedical Named Entity Recognition.</title>
<date>2002</date>
<booktitle>In Proc. of the Workshop on Natural Language Processing in the Biomedical Domain (at ACL’2002),</booktitle>
<pages>1--8</pages>
<contexts>
<context position="1379" citStr="Kazama et al 2002" startWordPosition="199" endWordPosition="202">for all kinds of word features, we show that word similarity is a potential method to automatically get word formation, prefix, suffix and abbreviation information automatically from biomedical texts, as well as useful word distribution information. 1 Introduction In the Message Understanding Conference (MUC), Named entity Recognition aims to classify proper nouns, dates, time, measures and locations, etc. Many researchers adapt their systems from MUC to the biomedical domain, such as (Fukuda et al 1998), (Proux et al 1998), (Nobata et al 2000), (Collier et al 2000), (Gaizauskas et al 2000), (Kazama et al 2002), (Takeuchi et al 2002), (Lee et al 2003) and (Zhou et al 2004). As opposed to rule-based systems, machine learning-based systems could train their models on labeled data. But due to the irregular forms of biomedical texts, people still need to carefully choose word features for their systems. This work requires domain specific knowledge. How to get the domain knowledge automatically is a question that has not been fully investigated. Our system is built on an HMM model with the words themselves as the features. Huge unlabeled corpus is gathered from MEDLINE. Word similarity information is com</context>
</contexts>
<marker>Kazama, Makino, Ohta, Tsujii, 2002</marker>
<rawString>Kazama, J., Makino, T., Ohta, Y., Tsujii, J. 2002. Tuning Support Vector Machines for Biomedical Named Entity Recognition. In Proc. of the Workshop on Natural Language Processing in the Biomedical Domain (at ACL’2002), pages 1-8</rawString>
</citation>
<citation valid="true">
<authors>
<author>K J Lee</author>
<author>Y S Hwang</author>
<author>H C Rim</author>
</authors>
<title>Two phase biomedical NE Recognition based on SVMs.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL-03 Workshop on Natural Language Processing in Biomedicine.</booktitle>
<pages>33--40</pages>
<location>Sapporo, Japan</location>
<contexts>
<context position="1420" citStr="Lee et al 2003" startWordPosition="207" endWordPosition="210"> word similarity is a potential method to automatically get word formation, prefix, suffix and abbreviation information automatically from biomedical texts, as well as useful word distribution information. 1 Introduction In the Message Understanding Conference (MUC), Named entity Recognition aims to classify proper nouns, dates, time, measures and locations, etc. Many researchers adapt their systems from MUC to the biomedical domain, such as (Fukuda et al 1998), (Proux et al 1998), (Nobata et al 2000), (Collier et al 2000), (Gaizauskas et al 2000), (Kazama et al 2002), (Takeuchi et al 2002), (Lee et al 2003) and (Zhou et al 2004). As opposed to rule-based systems, machine learning-based systems could train their models on labeled data. But due to the irregular forms of biomedical texts, people still need to carefully choose word features for their systems. This work requires domain specific knowledge. How to get the domain knowledge automatically is a question that has not been fully investigated. Our system is built on an HMM model with the words themselves as the features. Huge unlabeled corpus is gathered from MEDLINE. Word similarity information is computed from the corpus and we use a word s</context>
</contexts>
<marker>Lee, Hwang, Rim, 2003</marker>
<rawString>Lee, K.J., Hwang, Y.S., Rim H.C. 2003. Two phase biomedical NE Recognition based on SVMs. In Proceedings of the ACL-03 Workshop on Natural Language Processing in Biomedicine. pp.33-40. Sapporo, Japan</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In Proc. of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="8455" citStr="Lee 1999" startWordPosition="1426" endWordPosition="1427">ny single digit to 0. 2 http://www.cs.ualberta.ca/~shaojun/biolist.txt word sim n ∑pmi i=1 ( ) ∑ ( ) 2 n 2 f u , × pmi f v , i i = 1 i 85 gathered from the low frequency words in the training data. P tag wordsequen ce ( |) t P tag word word ( |, ) t t t+ 1 P(tagt |wordt) (3) We find that about 26% of the bigrams (wordt, wordt+1) in the testing data is unseen, so the smoothing is critical. In order to compute formula (1), we can use the back-off (Katz 1987); (Bikel et al 1999) approach. Baseline1 and Baseline2 in our system use different back-off schema. The following formula is introduced in (Lee 1999) for word similarity-based smoothing: sim w w P tag w ( , ) ( |) ′ ′ t t t t where S(w) is a set of candidate similar words and sim(w,w’) is the similarity between word w and w’. Word similarity-based smoothing approach is used in our system to make advantage of the huge unlabeled corpus. In order to plug the word similarity-based smoothing into our HMM model, we made several extensions to formula (4). For each word w, we define p as the distribution of w’s tags, which are annotated in the training data. We use the KL-Divergence to compute the distance between two distributions: = ∑ ( ( ) ) KL</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lee, L. 1999. Measures of distributional similarity. In Proc. of the 37th Annual Meeting of the Association for Computational Linguistics, 1999, pp. 25-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLINGACL98.</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="3572" citStr="Lin 1998" startWordPosition="547" endWordPosition="548"> Distributional Word Similarity “Words that tend to appear in the same contexts tend to have similar meanings.” (Harris 1968). For example, the words corruption and abuse are similar because both of them can be subjects of verbs like arouse, become, betray, cause, continue, cost, exist, force, go on, grow, have, increase, lead to, and persist, etc, and both of them can modify nouns like accusation, act, allegation, appearance, and case, etc. Many methods have been proposed to compute distributional similarity between words, e.g., (Hindle, 1990), (Pereira et al. 1993), (Grefenstette 1994) and (Lin 1998). Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared. 84 3.1 Proximity-based Similarity It is natural to use dependency relationship (Mel&apos;čuk, 1987) as features, but a parser has to be available. Since biomedical text is highly irregular, and is very different from text like newspaper, a parser developed for the newspaper domain may not perform very well on biomedical text. Since most dependency relationships involve words that are situated close to one another, the dependency relationships can often be </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. 1998. Automatic Retrieval and Clustering of Similar Words. In Proceedings of COLINGACL98. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Mel&apos;čuk</author>
</authors>
<title>Dependency Syntax: theory and practice.</title>
<date>1987</date>
<institution>State University of New York Press.</institution>
<location>Albany, NY.</location>
<contexts>
<context position="3811" citStr="Mel&apos;čuk, 1987" startWordPosition="586" endWordPosition="587">arouse, become, betray, cause, continue, cost, exist, force, go on, grow, have, increase, lead to, and persist, etc, and both of them can modify nouns like accusation, act, allegation, appearance, and case, etc. Many methods have been proposed to compute distributional similarity between words, e.g., (Hindle, 1990), (Pereira et al. 1993), (Grefenstette 1994) and (Lin 1998). Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared. 84 3.1 Proximity-based Similarity It is natural to use dependency relationship (Mel&apos;čuk, 1987) as features, but a parser has to be available. Since biomedical text is highly irregular, and is very different from text like newspaper, a parser developed for the newspaper domain may not perform very well on biomedical text. Since most dependency relationships involve words that are situated close to one another, the dependency relationships can often be approximated by cooccurrence relationships within a small window (Turney 2001); (Terra and Clarke 2003). We define the features of the word w to be the first non-stop word on either side of w and the intervening stop words (which can be de</context>
</contexts>
<marker>Mel&apos;čuk, 1987</marker>
<rawString>Mel&apos;čuk, I. A., 1987. Dependency Syntax: theory and practice. State University of New York Press. Albany, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Nobata</author>
<author>N Collier</author>
<author>J Tsujii</author>
</authors>
<title>Comparison between Tagged Corpora for the Named Entity Task.</title>
<date>2000</date>
<booktitle>In the Proceedings of ACL 2000 Workshop on Comparing Corpora. Hong Kong,</booktitle>
<pages>20--27</pages>
<contexts>
<context position="1311" citStr="Nobata et al 2000" startWordPosition="187" endWordPosition="190">nlabeled data. While many systems have laboriously hand-coded rules for all kinds of word features, we show that word similarity is a potential method to automatically get word formation, prefix, suffix and abbreviation information automatically from biomedical texts, as well as useful word distribution information. 1 Introduction In the Message Understanding Conference (MUC), Named entity Recognition aims to classify proper nouns, dates, time, measures and locations, etc. Many researchers adapt their systems from MUC to the biomedical domain, such as (Fukuda et al 1998), (Proux et al 1998), (Nobata et al 2000), (Collier et al 2000), (Gaizauskas et al 2000), (Kazama et al 2002), (Takeuchi et al 2002), (Lee et al 2003) and (Zhou et al 2004). As opposed to rule-based systems, machine learning-based systems could train their models on labeled data. But due to the irregular forms of biomedical texts, people still need to carefully choose word features for their systems. This work requires domain specific knowledge. How to get the domain knowledge automatically is a question that has not been fully investigated. Our system is built on an HMM model with the words themselves as the features. Huge unlabeled</context>
</contexts>
<marker>Nobata, Collier, Tsujii, 2000</marker>
<rawString>Nobata, C., Collier, N., Tsujii, J. 2000. Comparison between Tagged Corpora for the Named Entity Task. In the Proceedings of ACL 2000 Workshop on Comparing Corpora. Hong Kong, China. pp. 20-27</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Ohta</author>
<author>Y Tateisi</author>
<author>J Kim</author>
<author>H Mima</author>
<author>J Tsujii</author>
</authors>
<title>The GENIA corpus: An annotated research abstract corpus in molecular biology domain.</title>
<date>2002</date>
<booktitle>In Proc. of HLT</booktitle>
<contexts>
<context position="2168" citStr="Ohta et al 2002" startWordPosition="328" endWordPosition="331">ut due to the irregular forms of biomedical texts, people still need to carefully choose word features for their systems. This work requires domain specific knowledge. How to get the domain knowledge automatically is a question that has not been fully investigated. Our system is built on an HMM model with the words themselves as the features. Huge unlabeled corpus is gathered from MEDLINE. Word similarity information is computed from the corpus and we use a word similarity-based smoothing to overcome the data sparseness. 2 Data Preparation 2.1 Labeled Data Our labeled data is from GENIA 3.02 (Ohta et al 2002), which contains 2,000 abstracts (360K words). It has been annotated with semantic information such as DNA, protein annotations. These are useful for training models. It contains Part of Speech (POS) information as well. Although POS is not considered very useful for NER in newspaper articles, it can dramatically improve the performance of NER in biomedical texts (Zhou et al 2004). Our model is trained from this labeled data. 2.2 Unlabeled Data We downloaded 17G XML abstract data from MEDLINE, which contains 1,381,132 abstracts. Compared to the labeled data, we have far more unlabeled data, an</context>
</contexts>
<marker>Ohta, Tateisi, Kim, Mima, Tsujii, 2002</marker>
<rawString>Ohta, T., Tateisi, Y., Kim, J., Mima, H., Tsujii, J. 2002. The GENIA corpus: An annotated research abstract corpus in molecular biology domain. In Proc. of HLT 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>N Tishby</author>
<author>L Lee</author>
</authors>
<title>Distributional Clustering of English Words.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL-93.</booktitle>
<pages>183--190</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="3536" citStr="Pereira et al. 1993" startWordPosition="540" endWordPosition="543">ximity relationships from the unlabeled data. 3 Distributional Word Similarity “Words that tend to appear in the same contexts tend to have similar meanings.” (Harris 1968). For example, the words corruption and abuse are similar because both of them can be subjects of verbs like arouse, become, betray, cause, continue, cost, exist, force, go on, grow, have, increase, lead to, and persist, etc, and both of them can modify nouns like accusation, act, allegation, appearance, and case, etc. Many methods have been proposed to compute distributional similarity between words, e.g., (Hindle, 1990), (Pereira et al. 1993), (Grefenstette 1994) and (Lin 1998). Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared. 84 3.1 Proximity-based Similarity It is natural to use dependency relationship (Mel&apos;čuk, 1987) as features, but a parser has to be available. Since biomedical text is highly irregular, and is very different from text like newspaper, a parser developed for the newspaper domain may not perform very well on biomedical text. Since most dependency relationships involve words that are situated close to one another, the de</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Pereira, F., Tishby, N., Lee, L. 1993. Distributional Clustering of English Words. In Proceedings of ACL-93. pp. 183-190. Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Proux</author>
<author>F Rechenmann</author>
<author>L Julliard</author>
<author>V Pillet</author>
<author>B Jacq</author>
</authors>
<title>Detecting Gene Symbols and Names in Biological Texts: A First Step toward Pertinent Information Extraction.</title>
<date>1998</date>
<booktitle>In Proc. of Genome Inform Ser Workshop Genome Inform,</booktitle>
<pages>72--80</pages>
<contexts>
<context position="1290" citStr="Proux et al 1998" startWordPosition="183" endWordPosition="186">ance by using huge unlabeled data. While many systems have laboriously hand-coded rules for all kinds of word features, we show that word similarity is a potential method to automatically get word formation, prefix, suffix and abbreviation information automatically from biomedical texts, as well as useful word distribution information. 1 Introduction In the Message Understanding Conference (MUC), Named entity Recognition aims to classify proper nouns, dates, time, measures and locations, etc. Many researchers adapt their systems from MUC to the biomedical domain, such as (Fukuda et al 1998), (Proux et al 1998), (Nobata et al 2000), (Collier et al 2000), (Gaizauskas et al 2000), (Kazama et al 2002), (Takeuchi et al 2002), (Lee et al 2003) and (Zhou et al 2004). As opposed to rule-based systems, machine learning-based systems could train their models on labeled data. But due to the irregular forms of biomedical texts, people still need to carefully choose word features for their systems. This work requires domain specific knowledge. How to get the domain knowledge automatically is a question that has not been fully investigated. Our system is built on an HMM model with the words themselves as the fea</context>
</contexts>
<marker>Proux, Rechenmann, Julliard, Pillet, Jacq, 1998</marker>
<rawString>Proux, D., Rechenmann, F., Julliard, L., Pillet, V., Jacq, B. 1998. Detecting Gene Symbols and Names in Biological Texts: A First Step toward Pertinent Information Extraction. In Proc. of Genome Inform Ser Workshop Genome Inform, pages 72-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Takeuchi</author>
<author>N Collier</author>
</authors>
<title>Use of Support Vector Machines in Extended Named Entity Recognition.</title>
<date>2002</date>
<booktitle>In Proc. of the Sixth Conference on Natural Language Learning (CONLL</booktitle>
<pages>119--125</pages>
<marker>Takeuchi, Collier, 2002</marker>
<rawString>Takeuchi, K., Collier, N. 2002. Use of Support Vector Machines in Extended Named Entity Recognition. In Proc. of the Sixth Conference on Natural Language Learning (CONLL 2002), pages 119-125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E L Terra</author>
<author>C Clarke</author>
</authors>
<title>Frequency Estimates for Statistical Word Similarity Measures.</title>
<date>2003</date>
<booktitle>In the Proceedings of the 2003 Human Language Technology Conference,</booktitle>
<pages>244--251</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="4275" citStr="Terra and Clarke 2003" startWordPosition="656" endWordPosition="659">re corresponds to a type of context in which the word appeared. 84 3.1 Proximity-based Similarity It is natural to use dependency relationship (Mel&apos;čuk, 1987) as features, but a parser has to be available. Since biomedical text is highly irregular, and is very different from text like newspaper, a parser developed for the newspaper domain may not perform very well on biomedical text. Since most dependency relationships involve words that are situated close to one another, the dependency relationships can often be approximated by cooccurrence relationships within a small window (Turney 2001); (Terra and Clarke 2003). We define the features of the word w to be the first non-stop word on either side of w and the intervening stop words (which can be defined as the top-k most frequent words in the corpus). For example, for a sentence “He got a job from this company.” (Considering a, from and this to be stop words.), the features of job provided by this sentence are shown in Table 1. Features Frequency (left, got) 0.50 (left, a) 0.50 (right ,from) 0.33 (right, this) 0.33 (right, company) 0.33 ... ... Table 1: Features for word “job” 3.2 Computing Word Similarity Once the contexts of a word are represented as </context>
</contexts>
<marker>Terra, Clarke, 2003</marker>
<rawString>Terra, E. L., Clarke, C. 2003. Frequency Estimates for Statistical Word Similarity Measures. In the Proceedings of the 2003 Human Language Technology Conference, pp.244-251. Edmonton, Canada, May</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Mining the Web for synonyms: PMI-IR versus LSA on TOEFL,</title>
<date>2001</date>
<booktitle>Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001),</booktitle>
<pages>491--502</pages>
<location>Freiburg, Germany,</location>
<contexts>
<context position="4250" citStr="Turney 2001" startWordPosition="654" endWordPosition="655">here each feature corresponds to a type of context in which the word appeared. 84 3.1 Proximity-based Similarity It is natural to use dependency relationship (Mel&apos;čuk, 1987) as features, but a parser has to be available. Since biomedical text is highly irregular, and is very different from text like newspaper, a parser developed for the newspaper domain may not perform very well on biomedical text. Since most dependency relationships involve words that are situated close to one another, the dependency relationships can often be approximated by cooccurrence relationships within a small window (Turney 2001); (Terra and Clarke 2003). We define the features of the word w to be the first non-stop word on either side of w and the intervening stop words (which can be defined as the top-k most frequent words in the corpus). For example, for a sentence “He got a job from this company.” (Considering a, from and this to be stop words.), the features of job provided by this sentence are shown in Table 1. Features Frequency (left, got) 0.50 (left, a) 0.50 (right ,from) 0.33 (right, this) 0.33 (right, company) 0.33 ... ... Table 1: Features for word “job” 3.2 Computing Word Similarity Once the contexts of a</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Turney, P.D. 2001. Mining the Web for synonyms: PMI-IR versus LSA on TOEFL, Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001), Freiburg, Germany, pp. 491-502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zhou</author>
<author>J Zhang</author>
<author>J Su</author>
<author>D Shen</author>
<author>C Tan</author>
</authors>
<title>Recognizing names in biomedical texts: a machine learning approach. Bioinformatics Advance Access.</title>
<date>2004</date>
<contexts>
<context position="1442" citStr="Zhou et al 2004" startWordPosition="212" endWordPosition="215"> potential method to automatically get word formation, prefix, suffix and abbreviation information automatically from biomedical texts, as well as useful word distribution information. 1 Introduction In the Message Understanding Conference (MUC), Named entity Recognition aims to classify proper nouns, dates, time, measures and locations, etc. Many researchers adapt their systems from MUC to the biomedical domain, such as (Fukuda et al 1998), (Proux et al 1998), (Nobata et al 2000), (Collier et al 2000), (Gaizauskas et al 2000), (Kazama et al 2002), (Takeuchi et al 2002), (Lee et al 2003) and (Zhou et al 2004). As opposed to rule-based systems, machine learning-based systems could train their models on labeled data. But due to the irregular forms of biomedical texts, people still need to carefully choose word features for their systems. This work requires domain specific knowledge. How to get the domain knowledge automatically is a question that has not been fully investigated. Our system is built on an HMM model with the words themselves as the features. Huge unlabeled corpus is gathered from MEDLINE. Word similarity information is computed from the corpus and we use a word similarity-based smooth</context>
<context position="7317" citStr="Zhou et al 2004" startWordPosition="1206" endWordPosition="1209"> words can capture word formation (IL-00, IL-0beta, and IL0 etc) and abbreviation (interleukin-0) information. A complete list of these word pairs and their similarity is available online 2 . The rule-based system may not able to capture words like IL-0ra, IL-0Ralpha, which are in the similar word list of IL-0, and it is very likely that they belong to the same semantic category. Many different kinds of expressions for numbers (like 0, 00-00, 00.00, -00, 0/0, five, six, 0-, iii, IV etc) are grouped together automatically. 4 HMM Model and Smoothing Schema We follow the HMM model introduced in (Zhou et al 2004). The structure of an HMM model contains States and observations. In our model, each state is represented by a semantic tag, or a POS tag if the semantic tag is not available; each observation contains a word sequence. The main computing difficulty in (Zhou et al 2004) is the probability of a tag given a word sequence: formula (1). We use formula (2) to estimate formula (1). If the bigram is unseen in the training data, we use formula (3). If the unigram is also unseen, we use the unknown information which is 1 We changed any single digit to 0. 2 http://www.cs.ualberta.ca/~shaojun/biolist.txt </context>
</contexts>
<marker>Zhou, Zhang, Su, Shen, Tan, 2004</marker>
<rawString>Zhou, G., Zhang, J., Su, J., Shen, D., Tan, C. 2004. Recognizing names in biomedical texts: a machine learning approach. Bioinformatics Advance Access.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Zhao</author>
<author>D Lin</author>
</authors>
<title>A Nearest-Neighbor Method for Resolving PP-Attachment Ambiguity.</title>
<date>2004</date>
<booktitle>In Proceedings of the First International Joint Conference on Natural Language Processing,</booktitle>
<location>Sanya, China.</location>
<contexts>
<context position="5850" citStr="Zhao and Lin 2004" startWordPosition="970" endWordPosition="973">eature fi and a word u measures the strength association between them. It is defined as:  P f u ,  ( ) , log  ( ) ( ) pmi ( ) i i = P f P u f u  × i  where P(fi,u) is the probability of fi co-occurring with u; P(fi) is the probability of fi co-occurring with any word; and P(u) is the probability of any feature co-occurring with u. The similarity between word u and v is defined as the Cosine of PMI: n ∑ pmi f u pmi f v ( ) ( ) , × , i ( ) = 1 i i u v , = Different similarity measures of distributional similarity can affect the quality of the result to s statistically significant degree. (Zhao and Lin 2004) shows that the Cosine of PMI is a significantly better similarity measure than several other commonly used similarity measures. Similar words are computed for each word in the unlabeled data. Only a subset of the similarity information is useful, because the similarity of words outside of the training data and test data vocabulary is not used. We only take into account the similar words that occur in the training data more than 10 times and only those word pairs which have point-wise mutual information greater than a threshold (0.04). Table 2 shows the computing result for “IL-0”1: Similar Wo</context>
</contexts>
<marker>Zhao, Lin, 2004</marker>
<rawString>Zhao, S., Lin, D. 2004. A Nearest-Neighbor Method for Resolving PP-Attachment Ambiguity. In Proceedings of the First International Joint Conference on Natural Language Processing, 2004. Sanya, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>