<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008443">
<title confidence="0.995002">
Expectations of Word Sense in Parallel Corpora
</title>
<author confidence="0.981626">
Xuchen Yao, Benjamin Van Durme and Chris Callison-Burch
</author>
<affiliation confidence="0.9346795">
Center for Language and Speech Processing, and HLTCOE
Johns Hopkins University
</affiliation>
<sectionHeader confidence="0.977242" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999641611111111">
Given a parallel corpus, if two distinct words
in language A, a1 and a2, are aligned to the
same word b1 in language B, then this might
signal that b1 is polysemous, or it might sig-
nal a1 and a2 are synonyms. Both assump-
tions with successful work have been put for-
ward in the literature. We investigate these
assumptions, along with other questions of
word sense, by looking at sampled parallel
sentences containing tokens of the same type
in English, asking how often they mean the
same thing when they are: 1. aligned to the
same foreign type; and 2. aligned to different
foreign types. Results for French-English and
Chinese-English parallel corpora show simi-
lar behavior: Synonymy is only very weakly
the more prevalent scenario, where both cases
regularly occur.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99766425">
Parallel corpora have been used for both paraphrase
induction and word sense disambiguation (WSD).
Usually one of the following two assumptions is
made for these tasks:
</bodyText>
<listItem confidence="0.902635571428571">
1. Polysemy If two different words in language
A are aligned to the same word in language B,
then the word in language B is polysemous.
2. Synonymy If two different words in language
A are aligned to the same word in language B,
then the two words in A are synonyms, and thus
is not evidence of polysemy in B.
</listItem>
<bodyText confidence="0.987540275">
Despite the alternate nature of these assumptions,
both have associated articles in which a researcher
claimed success. Under the polysemy assumption,
Gale et al. (1992) used French translations as En-
glish sense indicators in the task of WSD. For in-
stance, for the English word duty, the French transla-
tion droit was taken to signal its tax sense and devoir
to signal its obligation sense. These French words
were used as labels for different English senses.
Similarly, in a cross-lingual WSD setting,1 Lefever
et al. (2011) treated each English-foreign alignment
as a so-called ParaSense, using it as a proxy for hu-
man labeled training data.
Under the synonymy assumption, Diab and
Resnik (2002) did word sense tagging by grouping
together all English words that are translated into
the same French word and by further enforcing that
the majority sense for these English words was pro-
jected as the sense for the French word. Bannard and
Callison-Burch (2005) applied the idea that French
phrases aligned to the same English phrase are para-
phrases in a system that induces paraphrases by piv-
oting through aligned foreign phrases.
Based on this, and other successful prior work,
it seems neither of the assumptions must hold
universally. Therefore we investigate how often
we might expect one or the other to dominate:
we sample polysemous words from wide-domain
{French,Chinese}-English corpora, and use Ama-
zon’s Mechanical Turk (MTurk) to annotate word
sense on the English side. We calculate empirical
probabilities based on counting over the competing
polysemous and synonymous scenario labels.
A key factor deciding the validity of our conclu-
sion is the reliability of the annotations derived via
MTurk. Thus our first step is to evaluate the abil-
ity of Turkers to perform WSD. After verifying this
1E.g., given a sentence “... more power, more duty ...”, the
task asks to give a French translation of duty, which should be
devior, after first recognizing the underlying obligation sense.
</bodyText>
<page confidence="0.903694666666667">
621
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 621–625,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999913666666667">
as a reasonable process for acquiring large amounts
of WSD labeled data, we go on to frame the experi-
mental design, giving final results in Sec. 4.
</bodyText>
<sectionHeader confidence="0.941092" genericHeader="method">
2 Turker Reliability
</sectionHeader>
<bodyText confidence="0.999579764705882">
While Amazon’s Mechanical Turk (MTurk) has
been been considered in the past for constructing
lexical semantic resources (e.g., (Snow et al., 2008;
Akkaya et al., 2010; Parent and Eskenazi, 2010;
Rumshisky, 2011)), word sense annotation is sensi-
tive to subjectivity and usually achieves low agree-
ment rate even among experts. Thus we first asked
Turkers to re-annotate a sample of existing gold-
standard data. With an eye towards costs saving, we
also considered how many Turkers would be needed
per item to produce results of sufficient quality.
Turkers were presented sentences from the test
portion of the word sense induction task of
SemEval-2007 (Agirre and Soroa, 2007), covering
2,559 instances of 35 nouns, expert-annotated with
OntoNotes (Hovy et al., 2006) senses. Two versions
of the task were designed:
</bodyText>
<listItem confidence="0.962712333333333">
1. compare: given the same word in different
sentences, tell whether their meaning is THE
SAME, ALMOST THE SAME, UNLIKELY THE
SAME or DIFFERENT, where the results were
collapsed post-hoc into a binary same/different
categorization;
2. sense map: map the meaning of a given word
in a sentential context to its proper OntoNotes
definition.
</listItem>
<bodyText confidence="0.995925625">
For both tasks, 2, 599 examples were presented.
We measure inter-coder agreement using Krip-
pendorff’s Alpha (Krippendorff, 2004; Artstein and
Poesio, 2008), where a &gt; 0.8 is considered to be
reliable and 0.667 &lt; a &lt; 0.8 allows for tenta-
tive conclusions. Two points emerge from Table 1:
there were greater agreement rates for sense map
than compare, and 3 Turkers were sufficient.
</bodyText>
<sectionHeader confidence="0.995501" genericHeader="method">
3 Experiment Design
</sectionHeader>
<bodyText confidence="0.718745">
Data Selection We used two parallel corpora: the
French-English 109 corpus (Callison-Burch et al.,
2009) and the GALE Chinese-English corpus.
</bodyText>
<table confidence="0.9545138">
a-Turker a-maj. maj.-agr.
compares 0.47 0.66 0.87
compare3 0.44 0.52 0.83
sense maps 0.79 0.93 0.95
sense map3 0.75 0.87 0.91
</table>
<tableCaption confidence="0.749628666666667">
Table 1: MTurk result on testing Turker reliability. Krip-
pendorff’s Alpha is used to measure agreement. a-
Turker: how Turkers agree among themselves, a-maj.:
</tableCaption>
<bodyText confidence="0.989711896551724">
how the majority agrees with true value, maj.-agr.: agree-
ment between the majority vote and true value. a-maj.
indicates the confidence level about the maj.-agr. value.
Subscripts denote either 5 Turkers, or 3 randomly se-
lected of the 5.
For each corpus we selected 50 words, w, at ran-
dom from OntoNotes,2 constrained such that w: had
more than one sense; had a frequency &gt; 1, 000; and
was not a top 10% most frequent words.
Next we sampled 100 instances (aligned English-
foreign sentence pairs) for each word based on the
following constraints: the aligned foreign word, f,
had a frequency &gt; 20 in the foreign corpus; f had a
non-trivial alignment probability.3 We sampled pro-
portionally to the distribution of the aligned foreign
words, ensuring that at least 5 instances from each
foreign translation are sampled.4
For each corpus, this results in 100 instances for
each of 50 words, totaling 5,000 instances. We used
3 Turkers per instance for sense annotation, under
the sense map task. We note that the set of 50
randomly selected English words from the Chinese-
English corpus were entirely distinct from the 50 se-
lected words from the French-English corpus.
Probability Estimation Suppose e1 and e2 are
two tokens of the same English word type e. s(e1)
is a function that returns the sense of e1, a(e1) is
a function that returns the aligned word of e1. Let
c() be our count function, where: c(e, f) returns the
</bodyText>
<footnote confidence="0.9267743">
2OntoNotes was used as the sense inventory over alterna-
tives, owing to its coarse-grained sense definitions.
3Defined as f having index i &lt; k when foreign words are
ranked by most probable given e, where k is the minimum value
such that Ek� p(fz  |e) &gt; 0.8. E.g., if we have decreasing
probabilities p(droit I duty) = 0.6, p(devoir I duty) =
0.25, p(le I duty) = 0.03, ... then only consider droit and
devoir. This ruled out many noisy alignments.
4Thus, the instances of droit compared to that of devoir
would be 0.6/0.25.
</footnote>
<page confidence="0.99388">
622
</page>
<bodyText confidence="0.999752923076923">
number of times English word e is aligned to foreign
word f; c(es, f) returns the number of times En-
glish word e has sense s (tagged by Turkers), when
aligned to foreign word f; c(e) is the total number
of tokens of English word e; and c(es) is the number
of tokens of e with sense s.
We estimate from labeled data the probability of
three scenarios, with scenario 1 as our primary con-
cern: when two English words of the same poly-
semous type are aligned to different foreign word
types, what is the chance that they have the same
sense? Given the tokens e1 and e2, we calculate P1
as follows:
</bodyText>
<equation confidence="0.999469666666667">
P1e = P(s(e1) = s(e2)  |a(e1) =� a(e2))
Ps c2(es) − Ps,f c2(es, f)
c2(e) − P f c2(e, f)
</equation>
<bodyText confidence="0.972793625">
P1 says that given two words of the same type
(e1 and e2) that are not aligned to the same foreign
word type (a(e1) =� a(e2)), what is the probabil-
ity that they have the same sense (s(e1) = s(e2)).
We approach this estimation combinatorially. For
instance, the number of ways to choose two words
of the same type is � 2 � ^ 1 2c2(e) when c(e) is
�(e)
large.
A large value of P1 would be in support of Syn-
onymy, as the two foreign aligned words of distinct
type would have the same meaning.
Scenario 2 asks: given two English words of
the same polysemous type and aligned to the same
words (a(e1) = a(e2)), what is the probability that
they have the same sense (s(e1) = s(e2))?
</bodyText>
<equation confidence="0.994222666666667">
Pte = P(s(e1) = s(e2)  |a(e1) = a(e2))
Ps,f c2(es, f)
Pf c2(e,f)
</equation>
<bodyText confidence="0.99873625">
Finally, what is the probability of two tokens of
the same polysemous type agreeing when alignment
information is not known (e.g., without a parallel
corpus)?
</bodyText>
<equation confidence="0.326928">
Ps c2(es)
</equation>
<bodyText confidence="0.999829">
All the above equations are given per English word
type e. In later sections we report the average values
over multiple word types and their counts.
</bodyText>
<sectionHeader confidence="0.999891" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999924760869565">
Turker Experiments To minimize errors from
Turkers, for every HIT we inserted one control
sentence taken from the example sentences of
OntoNotes. Turker results with either extremely low
finishing time (&lt;10s), or average accuracy on con-
trol sentences lower than accuracy by chance, were
rejected. On average Turkers took 185 seconds to
map 10 sentences in a HIT to their OntoNotes def-
inition, receiving $0.10 per HIT. The total time for
annotating 5000 sentences was 22 hours.
Turkers had no knowledge about alignments: we
hid the aligned French/Chinese sentences from them
and these sentences were later processed to compute
P1/2/3 values. Two foreign tokens aligned with the
same source type correspond to two senses of the
same type. To give an estimate of alignment errors,
we manually examined 1/10 of all 5000 sampled
Chinese-English alignments at random and found
only 3 of them were wrong: all due to that English
content words were aligned to common Chinese
function words. This error rate is much lower than
that typically reported by alignment tools. The main
reason is explained in footnote 3: foreign words with
trivial alignment probability were removed before
calculating P1/2/3 values. Thus we believe the align-
ment was reliable.
Probability Estimation Table 2 gives the dis-
tribution of senses and word types in the sam-
pled words. Take the second numeric column of
French-English as an example: out of 50 words ran-
domly sampled, 9 have 2 distinct sense definitions
in OntoNotes. However, 17 of 50 unique word types
had exactly 2 distinct senses annotated, out of the
100 examples of a given word type: 17 words had
2 distinct senses observed. Of the 9 words with 2
official senses, on average 1.9 of those senses were
observed.
Table 3 and Figures 1 and 2 shows the result for
P1, P2 and P3 using the {French,Chinese}-English
corpora, calculated based on the majority vote of
three Turkers. High P2 values suggests that for two
tokens of the same type, aligning to the same for-
eign type is a reasonable indicator of having the
same meaning. When working with open domain
corpora, without foreign alignments, the probabil-
ity of two English words of the same type having
</bodyText>
<equation confidence="0.958855333333333">
^
^
P3e = P(s(e1) = s(e2)) ^ c2(e)
</equation>
<page confidence="0.998369">
623
</page>
<table confidence="0.9991952">
French-English Chinese-English
#senses in OntoNotes 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 18
#types in OntoNotes 0 9 7 6 8 6 2 8 4 0 10 6 11 3 8 6 4 1 1
#types observed 2 17 9 4 7 7 4 0 0 3 19 9 12 5 2 0 0 0 0
avg #senses observed 0 1.9 2.1 3.2 3.8 4.7 6.5 4.9 5.8 0 1.9 2.2 2.9 2.7 4.4 3.8 3.8 3.0 5.0
</table>
<tableCaption confidence="0.905223">
Table 2: Statistics for words sampled from parallel corpora. Average #senses observed over all words: 2.6 (French-
English), and 2.4 (Chinese-English). The sampled word keep has 18 senses in OntoNotes, with 5 observed.
</tableCaption>
<table confidence="0.999764">
P1 P2 P3 Alpha
French-English 51.2% 66.7% 59.2% 0.70
Chinese-English 59.6% 78.7% 66.7% 0.68
</table>
<tableCaption confidence="0.9922375">
Table 3: Expectations of word sense in parallel corpora.
Alpha measures how Turkers agreed with themselves.
</tableCaption>
<bodyText confidence="0.999807866666667">
identical meaning is estimated here to be roughly
59-67% (59.2% (French), 66.7% (Chinese)). This
accords with results from WSD evaluations, where
the first-sense heuristic is roughly 75-80% accu-
rate (e.g., 80.9% in SemEval’07 (Brody and Lap-
ata, 2009)). Minor algebra translates this into an ex-
pected P3 value in a range from 56% − 62.5%, up to
64% − 68%, which captures our estimates.5
Finally for our motivating scenario: values for P1
are barely higher than 50%, suggesting that Syn-
onymy more regularly holds, but not conclusively.
We expect in narrower domains, where words have
less number of senses, this is more noticeable. As
suggested by Fig.s 1 and 2, less polysemous words
tend to have higher P values.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999985833333333">
Curious as to the distinct threads of prior work based
on alternate assumptions of word sense and parallel
corpora, we derived empirical expectations on the
shared meaning of tokens of the same type appear-
ing in the same corpus. Our results suggest neither
the assumption of Polysemy nor Synonymy holds
significantly more often than the other, at least for
individual words (as opposed to phrases) and for the
open domain corpora used here. Further, we provide
an independent data point that supports earlier find-
ings as to the expected accuracy of the first sense
heuristic in word sense disambiguation.
</bodyText>
<footnote confidence="0.900803">
5Assuming worst case: no two tokens that are not the first
sense ever match, and best case: any two tokens not the first
sense always match, then assuming first-sense accuracy of 0.8
gives a range on P3 of: (0.82, 0.82 + 0.22) = (0.64, 0.68).
</footnote>
<figureCaption confidence="0.999958">
Figure 1: French-English values, by number of senses.
Figure 2: Chinese-English values, by number of senses.
</figureCaption>
<figure confidence="0.999546612903226">
3 4 5 6 7
Num.Senses.Observed
1 2
Probability
0.8
0.6
0.4
0.2
0.0
1.0
●
●
●
●
Type
P1 P2
●
P3
3 4 5 6 7
Num.Senses.Observed
1 2
Probability
0.8
0.4
0.2
1.0
.6
●
●
Type
P1 P2 P3
</figure>
<page confidence="0.993499">
624
</page>
<sectionHeader confidence="0.995313" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999907146341463">
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007
Task 02: Evaluating Word Sense Induction And Dis-
crimination Systems. In Proc. SemEval ’07.
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon mechanical turk
for subjectivity word sense disambiguation. In Proc.
NAACL Workshop on CSLDAMT.
Ron Artstein and Massimo Poesio. 2008. Inter-Coder
Agreement for Computational Linguistics. Computa-
tional Linguistics, 34(4).
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proc. ACL.
Samuel Brody and Mirella Lapata. 2009. Bayesian Word
Sense Induction. In Proc. EACL.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings Of The 2009
Workshop On Statistical Machine Translation. In
Proc. StatMT.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proc. ACL.
W.A. Gale, K.W. Church, and D. Yarowsky. 1992. Using
bilingual materials to develop word sense disambigua-
tion methods. In Proc. TMI.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
the 90% solution. In Proc. NAACL-Short.
Klaus H. Krippendorff. 2004. Content Analysis: An In-
troduction to Its Methodology.
Els Lefever, V´eronique Hoste, and Martine De Cock.
2011. Parasense or how to use parallel corpora for
word sense disambiguation. In Proc. ACL.
Gabriel Parent and Maxine Eskenazi. 2010. Clustering
dictionary definitions using amazon mechanical turk.
In Proc. NAACL Workshop on CSLDAMT.
Anna Rumshisky. 2011. Crowdsourcing word sense def-
inition. In Proc. LAW V.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast—but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proc. EMNLP.
</reference>
<page confidence="0.998799">
625
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.945699">
<title confidence="0.985883">Expectations of Word Sense in Parallel Corpora</title>
<author confidence="0.993712">Benjamin Van_Durme Yao</author>
<affiliation confidence="0.9926575">Center for Language and Speech Processing, and Johns Hopkins University</affiliation>
<abstract confidence="0.998620631578947">Given a parallel corpus, if two distinct words language A, are aligned to the word language B, then this might that polysemous, might sigsynonyms. Both assumptions with successful work have been put forward in the literature. We investigate these assumptions, along with other questions of word sense, by looking at sampled parallel sentences containing tokens of the same type in English, asking how often they mean the same thing when they are: 1. aligned to the type; and 2. aligned to foreign types. Results for French-English and Chinese-English parallel corpora show similar behavior: Synonymy is only very weakly the more prevalent scenario, where both cases regularly occur.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Semeval-2007 Task 02: Evaluating Word Sense Induction And Discrimination Systems.</title>
<date>2007</date>
<booktitle>In Proc. SemEval ’07.</booktitle>
<contexts>
<context position="4516" citStr="Agirre and Soroa, 2007" startWordPosition="731" endWordPosition="734">en considered in the past for constructing lexical semantic resources (e.g., (Snow et al., 2008; Akkaya et al., 2010; Parent and Eskenazi, 2010; Rumshisky, 2011)), word sense annotation is sensitive to subjectivity and usually achieves low agreement rate even among experts. Thus we first asked Turkers to re-annotate a sample of existing goldstandard data. With an eye towards costs saving, we also considered how many Turkers would be needed per item to produce results of sufficient quality. Turkers were presented sentences from the test portion of the word sense induction task of SemEval-2007 (Agirre and Soroa, 2007), covering 2,559 instances of 35 nouns, expert-annotated with OntoNotes (Hovy et al., 2006) senses. Two versions of the task were designed: 1. compare: given the same word in different sentences, tell whether their meaning is THE SAME, ALMOST THE SAME, UNLIKELY THE SAME or DIFFERENT, where the results were collapsed post-hoc into a binary same/different categorization; 2. sense map: map the meaning of a given word in a sentential context to its proper OntoNotes definition. For both tasks, 2, 599 examples were presented. We measure inter-coder agreement using Krippendorff’s Alpha (Krippendorff,</context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 Task 02: Evaluating Word Sense Induction And Discrimination Systems. In Proc. SemEval ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cem Akkaya</author>
<author>Alexander Conrad</author>
<author>Janyce Wiebe</author>
<author>Rada Mihalcea</author>
</authors>
<title>Amazon mechanical turk for subjectivity word sense disambiguation.</title>
<date>2010</date>
<booktitle>In Proc. NAACL Workshop on CSLDAMT.</booktitle>
<contexts>
<context position="4009" citStr="Akkaya et al., 2010" startWordPosition="650" endWordPosition="653">fter first recognizing the underlying obligation sense. 621 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 621–625, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics as a reasonable process for acquiring large amounts of WSD labeled data, we go on to frame the experimental design, giving final results in Sec. 4. 2 Turker Reliability While Amazon’s Mechanical Turk (MTurk) has been been considered in the past for constructing lexical semantic resources (e.g., (Snow et al., 2008; Akkaya et al., 2010; Parent and Eskenazi, 2010; Rumshisky, 2011)), word sense annotation is sensitive to subjectivity and usually achieves low agreement rate even among experts. Thus we first asked Turkers to re-annotate a sample of existing goldstandard data. With an eye towards costs saving, we also considered how many Turkers would be needed per item to produce results of sufficient quality. Turkers were presented sentences from the test portion of the word sense induction task of SemEval-2007 (Agirre and Soroa, 2007), covering 2,559 instances of 35 nouns, expert-annotated with OntoNotes (Hovy et al., 2006) s</context>
</contexts>
<marker>Akkaya, Conrad, Wiebe, Mihalcea, 2010</marker>
<rawString>Cem Akkaya, Alexander Conrad, Janyce Wiebe, and Rada Mihalcea. 2010. Amazon mechanical turk for subjectivity word sense disambiguation. In Proc. NAACL Workshop on CSLDAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-Coder Agreement for Computational Linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="5149" citStr="Artstein and Poesio, 2008" startWordPosition="828" endWordPosition="831">ing 2,559 instances of 35 nouns, expert-annotated with OntoNotes (Hovy et al., 2006) senses. Two versions of the task were designed: 1. compare: given the same word in different sentences, tell whether their meaning is THE SAME, ALMOST THE SAME, UNLIKELY THE SAME or DIFFERENT, where the results were collapsed post-hoc into a binary same/different categorization; 2. sense map: map the meaning of a given word in a sentential context to its proper OntoNotes definition. For both tasks, 2, 599 examples were presented. We measure inter-coder agreement using Krippendorff’s Alpha (Krippendorff, 2004; Artstein and Poesio, 2008), where a &gt; 0.8 is considered to be reliable and 0.667 &lt; a &lt; 0.8 allows for tentative conclusions. Two points emerge from Table 1: there were greater agreement rates for sense map than compare, and 3 Turkers were sufficient. 3 Experiment Design Data Selection We used two parallel corpora: the French-English 109 corpus (Callison-Burch et al., 2009) and the GALE Chinese-English corpus. a-Turker a-maj. maj.-agr. compares 0.47 0.66 0.87 compare3 0.44 0.52 0.83 sense maps 0.79 0.93 0.95 sense map3 0.75 0.87 0.91 Table 1: MTurk result on testing Turker reliability. Krippendorff’s Alpha is used to me</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-Coder Agreement for Computational Linguistics. Computational Linguistics, 34(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="2412" citStr="Bannard and Callison-Burch (2005)" startWordPosition="400" endWordPosition="403">signal its tax sense and devoir to signal its obligation sense. These French words were used as labels for different English senses. Similarly, in a cross-lingual WSD setting,1 Lefever et al. (2011) treated each English-foreign alignment as a so-called ParaSense, using it as a proxy for human labeled training data. Under the synonymy assumption, Diab and Resnik (2002) did word sense tagging by grouping together all English words that are translated into the same French word and by further enforcing that the majority sense for these English words was projected as the sense for the French word. Bannard and Callison-Burch (2005) applied the idea that French phrases aligned to the same English phrase are paraphrases in a system that induces paraphrases by pivoting through aligned foreign phrases. Based on this, and other successful prior work, it seems neither of the assumptions must hold universally. Therefore we investigate how often we might expect one or the other to dominate: we sample polysemous words from wide-domain {French,Chinese}-English corpora, and use Amazon’s Mechanical Turk (MTurk) to annotate word sense on the English side. We calculate empirical probabilities based on counting over the competing poly</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Mirella Lapata</author>
</authors>
<title>Bayesian Word Sense Induction. In</title>
<date>2009</date>
<booktitle>Proc. EACL.</booktitle>
<contexts>
<context position="12704" citStr="Brody and Lapata, 2009" startWordPosition="2175" endWordPosition="2179">arallel corpora. Average #senses observed over all words: 2.6 (FrenchEnglish), and 2.4 (Chinese-English). The sampled word keep has 18 senses in OntoNotes, with 5 observed. P1 P2 P3 Alpha French-English 51.2% 66.7% 59.2% 0.70 Chinese-English 59.6% 78.7% 66.7% 0.68 Table 3: Expectations of word sense in parallel corpora. Alpha measures how Turkers agreed with themselves. identical meaning is estimated here to be roughly 59-67% (59.2% (French), 66.7% (Chinese)). This accords with results from WSD evaluations, where the first-sense heuristic is roughly 75-80% accurate (e.g., 80.9% in SemEval’07 (Brody and Lapata, 2009)). Minor algebra translates this into an expected P3 value in a range from 56% − 62.5%, up to 64% − 68%, which captures our estimates.5 Finally for our motivating scenario: values for P1 are barely higher than 50%, suggesting that Synonymy more regularly holds, but not conclusively. We expect in narrower domains, where words have less number of senses, this is more noticeable. As suggested by Fig.s 1 and 2, less polysemous words tend to have higher P values. 5 Conclusion Curious as to the distinct threads of prior work based on alternate assumptions of word sense and parallel corpora, we deriv</context>
</contexts>
<marker>Brody, Lapata, 2009</marker>
<rawString>Samuel Brody and Mirella Lapata. 2009. Bayesian Word Sense Induction. In Proc. EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Workshop On Statistical Machine Translation. In</title>
<date>2009</date>
<booktitle>Findings Of The</booktitle>
<contexts>
<context position="5498" citStr="Callison-Burch et al., 2009" startWordPosition="887" endWordPosition="890">t categorization; 2. sense map: map the meaning of a given word in a sentential context to its proper OntoNotes definition. For both tasks, 2, 599 examples were presented. We measure inter-coder agreement using Krippendorff’s Alpha (Krippendorff, 2004; Artstein and Poesio, 2008), where a &gt; 0.8 is considered to be reliable and 0.667 &lt; a &lt; 0.8 allows for tentative conclusions. Two points emerge from Table 1: there were greater agreement rates for sense map than compare, and 3 Turkers were sufficient. 3 Experiment Design Data Selection We used two parallel corpora: the French-English 109 corpus (Callison-Burch et al., 2009) and the GALE Chinese-English corpus. a-Turker a-maj. maj.-agr. compares 0.47 0.66 0.87 compare3 0.44 0.52 0.83 sense maps 0.79 0.93 0.95 sense map3 0.75 0.87 0.91 Table 1: MTurk result on testing Turker reliability. Krippendorff’s Alpha is used to measure agreement. aTurker: how Turkers agree among themselves, a-maj.: how the majority agrees with true value, maj.-agr.: agreement between the majority vote and true value. a-maj. indicates the confidence level about the maj.-agr. value. Subscripts denote either 5 Turkers, or 3 randomly selected of the 5. For each corpus we selected 50 words, w, </context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings Of The 2009 Workshop On Statistical Machine Translation. In Proc. StatMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Philip Resnik</author>
</authors>
<title>An unsupervised method for word sense tagging using parallel corpora.</title>
<date>2002</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="2149" citStr="Diab and Resnik (2002)" startWordPosition="356" endWordPosition="359">ticles in which a researcher claimed success. Under the polysemy assumption, Gale et al. (1992) used French translations as English sense indicators in the task of WSD. For instance, for the English word duty, the French translation droit was taken to signal its tax sense and devoir to signal its obligation sense. These French words were used as labels for different English senses. Similarly, in a cross-lingual WSD setting,1 Lefever et al. (2011) treated each English-foreign alignment as a so-called ParaSense, using it as a proxy for human labeled training data. Under the synonymy assumption, Diab and Resnik (2002) did word sense tagging by grouping together all English words that are translated into the same French word and by further enforcing that the majority sense for these English words was projected as the sense for the French word. Bannard and Callison-Burch (2005) applied the idea that French phrases aligned to the same English phrase are paraphrases in a system that induces paraphrases by pivoting through aligned foreign phrases. Based on this, and other successful prior work, it seems neither of the assumptions must hold universally. Therefore we investigate how often we might expect one or t</context>
</contexts>
<marker>Diab, Resnik, 2002</marker>
<rawString>Mona Diab and Philip Resnik. 2002. An unsupervised method for word sense tagging using parallel corpora. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
<author>D Yarowsky</author>
</authors>
<title>Using bilingual materials to develop word sense disambiguation methods.</title>
<date>1992</date>
<booktitle>In Proc. TMI.</booktitle>
<contexts>
<context position="1622" citStr="Gale et al. (1992)" startWordPosition="268" endWordPosition="271">d for both paraphrase induction and word sense disambiguation (WSD). Usually one of the following two assumptions is made for these tasks: 1. Polysemy If two different words in language A are aligned to the same word in language B, then the word in language B is polysemous. 2. Synonymy If two different words in language A are aligned to the same word in language B, then the two words in A are synonyms, and thus is not evidence of polysemy in B. Despite the alternate nature of these assumptions, both have associated articles in which a researcher claimed success. Under the polysemy assumption, Gale et al. (1992) used French translations as English sense indicators in the task of WSD. For instance, for the English word duty, the French translation droit was taken to signal its tax sense and devoir to signal its obligation sense. These French words were used as labels for different English senses. Similarly, in a cross-lingual WSD setting,1 Lefever et al. (2011) treated each English-foreign alignment as a so-called ParaSense, using it as a proxy for human labeled training data. Under the synonymy assumption, Diab and Resnik (2002) did word sense tagging by grouping together all English words that are t</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>W.A. Gale, K.W. Church, and D. Yarowsky. 1992. Using bilingual materials to develop word sense disambiguation methods. In Proc. TMI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>OntoNotes: the 90% solution.</title>
<date>2006</date>
<booktitle>In Proc. NAACL-Short.</booktitle>
<contexts>
<context position="4607" citStr="Hovy et al., 2006" startWordPosition="744" endWordPosition="747"> Akkaya et al., 2010; Parent and Eskenazi, 2010; Rumshisky, 2011)), word sense annotation is sensitive to subjectivity and usually achieves low agreement rate even among experts. Thus we first asked Turkers to re-annotate a sample of existing goldstandard data. With an eye towards costs saving, we also considered how many Turkers would be needed per item to produce results of sufficient quality. Turkers were presented sentences from the test portion of the word sense induction task of SemEval-2007 (Agirre and Soroa, 2007), covering 2,559 instances of 35 nouns, expert-annotated with OntoNotes (Hovy et al., 2006) senses. Two versions of the task were designed: 1. compare: given the same word in different sentences, tell whether their meaning is THE SAME, ALMOST THE SAME, UNLIKELY THE SAME or DIFFERENT, where the results were collapsed post-hoc into a binary same/different categorization; 2. sense map: map the meaning of a given word in a sentential context to its proper OntoNotes definition. For both tasks, 2, 599 examples were presented. We measure inter-coder agreement using Krippendorff’s Alpha (Krippendorff, 2004; Artstein and Poesio, 2008), where a &gt; 0.8 is considered to be reliable and 0.667 &lt; a</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: the 90% solution. In Proc. NAACL-Short.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus H Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology.</title>
<date>2004</date>
<contexts>
<context position="5121" citStr="Krippendorff, 2004" startWordPosition="826" endWordPosition="827"> Soroa, 2007), covering 2,559 instances of 35 nouns, expert-annotated with OntoNotes (Hovy et al., 2006) senses. Two versions of the task were designed: 1. compare: given the same word in different sentences, tell whether their meaning is THE SAME, ALMOST THE SAME, UNLIKELY THE SAME or DIFFERENT, where the results were collapsed post-hoc into a binary same/different categorization; 2. sense map: map the meaning of a given word in a sentential context to its proper OntoNotes definition. For both tasks, 2, 599 examples were presented. We measure inter-coder agreement using Krippendorff’s Alpha (Krippendorff, 2004; Artstein and Poesio, 2008), where a &gt; 0.8 is considered to be reliable and 0.667 &lt; a &lt; 0.8 allows for tentative conclusions. Two points emerge from Table 1: there were greater agreement rates for sense map than compare, and 3 Turkers were sufficient. 3 Experiment Design Data Selection We used two parallel corpora: the French-English 109 corpus (Callison-Burch et al., 2009) and the GALE Chinese-English corpus. a-Turker a-maj. maj.-agr. compares 0.47 0.66 0.87 compare3 0.44 0.52 0.83 sense maps 0.79 0.93 0.95 sense map3 0.75 0.87 0.91 Table 1: MTurk result on testing Turker reliability. Krippe</context>
</contexts>
<marker>Krippendorff, 2004</marker>
<rawString>Klaus H. Krippendorff. 2004. Content Analysis: An Introduction to Its Methodology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Els Lefever</author>
<author>V´eronique Hoste</author>
<author>Martine De Cock</author>
</authors>
<title>Parasense or how to use parallel corpora for word sense disambiguation.</title>
<date>2011</date>
<booktitle>In Proc. ACL.</booktitle>
<marker>Lefever, Hoste, De Cock, 2011</marker>
<rawString>Els Lefever, V´eronique Hoste, and Martine De Cock. 2011. Parasense or how to use parallel corpora for word sense disambiguation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Parent</author>
<author>Maxine Eskenazi</author>
</authors>
<title>Clustering dictionary definitions using amazon mechanical turk.</title>
<date>2010</date>
<booktitle>In Proc. NAACL Workshop on CSLDAMT.</booktitle>
<contexts>
<context position="4036" citStr="Parent and Eskenazi, 2010" startWordPosition="654" endWordPosition="657">g the underlying obligation sense. 621 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 621–625, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics as a reasonable process for acquiring large amounts of WSD labeled data, we go on to frame the experimental design, giving final results in Sec. 4. 2 Turker Reliability While Amazon’s Mechanical Turk (MTurk) has been been considered in the past for constructing lexical semantic resources (e.g., (Snow et al., 2008; Akkaya et al., 2010; Parent and Eskenazi, 2010; Rumshisky, 2011)), word sense annotation is sensitive to subjectivity and usually achieves low agreement rate even among experts. Thus we first asked Turkers to re-annotate a sample of existing goldstandard data. With an eye towards costs saving, we also considered how many Turkers would be needed per item to produce results of sufficient quality. Turkers were presented sentences from the test portion of the word sense induction task of SemEval-2007 (Agirre and Soroa, 2007), covering 2,559 instances of 35 nouns, expert-annotated with OntoNotes (Hovy et al., 2006) senses. Two versions of the </context>
</contexts>
<marker>Parent, Eskenazi, 2010</marker>
<rawString>Gabriel Parent and Maxine Eskenazi. 2010. Clustering dictionary definitions using amazon mechanical turk. In Proc. NAACL Workshop on CSLDAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Rumshisky</author>
</authors>
<title>Crowdsourcing word sense definition.</title>
<date>2011</date>
<booktitle>In Proc. LAW V.</booktitle>
<contexts>
<context position="4054" citStr="Rumshisky, 2011" startWordPosition="658" endWordPosition="659"> sense. 621 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 621–625, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics as a reasonable process for acquiring large amounts of WSD labeled data, we go on to frame the experimental design, giving final results in Sec. 4. 2 Turker Reliability While Amazon’s Mechanical Turk (MTurk) has been been considered in the past for constructing lexical semantic resources (e.g., (Snow et al., 2008; Akkaya et al., 2010; Parent and Eskenazi, 2010; Rumshisky, 2011)), word sense annotation is sensitive to subjectivity and usually achieves low agreement rate even among experts. Thus we first asked Turkers to re-annotate a sample of existing goldstandard data. With an eye towards costs saving, we also considered how many Turkers would be needed per item to produce results of sufficient quality. Turkers were presented sentences from the test portion of the word sense induction task of SemEval-2007 (Agirre and Soroa, 2007), covering 2,559 instances of 35 nouns, expert-annotated with OntoNotes (Hovy et al., 2006) senses. Two versions of the task were designed</context>
</contexts>
<marker>Rumshisky, 2011</marker>
<rawString>Anna Rumshisky. 2011. Crowdsourcing word sense definition. In Proc. LAW V.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP.</booktitle>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks. In Proc. EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>