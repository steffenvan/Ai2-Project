<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.994538">
Evaluating Text Segmentation using Boundary Edit Distance
</title>
<author confidence="0.994333">
Chris Fournier
</author>
<affiliation confidence="0.996702">
University of Ottawa
</affiliation>
<address confidence="0.548422">
Ottawa, ON, Canada
</address>
<email confidence="0.967164">
cfour037@eecs.uottawa.ca
</email>
<sectionHeader confidence="0.983409" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999987">
This work proposes a new segmentation
evaluation metric, named boundary simi-
larity (B), an inter-coder agreement coef-
ficient adaptation, and a confusion-matrix
for segmentation that are all based upon an
adaptation of the boundary edit distance in
Fournier and Inkpen (2012). Existing seg-
mentation metrics such as Pk, WindowD-
iff, and Segmentation Similarity (S) are
all able to award partial credit for near
misses between boundaries, but are biased
towards segmentations containing few or
tightly clustered boundaries. Despite S’s
improvements, its normalization also pro-
duces cosmetically high values that over-
estimate agreement &amp; performance, lead-
ing this work to propose a solution.
</bodyText>
<sectionHeader confidence="0.992538" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999882">
Text segmentation is the task of splitting text into
segments by placing boundaries within it. Seg-
mentation is performed for a variety of purposes
and is often a pre-processing step in a larger task.
E.g., text can be topically segmented to aid video
and audio retrieval (Franz et al., 2007), question
answering (Oh et al., 2007), subjectivity analysis
(Stoyanov and Cardie, 2008), and even summa-
rization (Haghighi and Vanderwende, 2009).
A variety of segmentation granularities, or
atomic units, exist, including segmentations at the
morpheme (e.g., Sirts and Alum¨ae 2012), word
(e.g., Chang et al. 2008), sentence (e.g., Rey-
nar and Ratnaparkhi 1997), and paragraph (e.g.,
Hearst 1997) levels. Between each atomic unit lies
the potential to place a boundary. Segmentations
can also represent the structure of text as being
organized linearly (e.g., Hearst 1997), hierarchi-
cally (e.g., Eisenstein 2009), etc. Theoretically,
segmentations could also contain varying bound-
ary types, e.g., two boundary types could differen-
tiate between act and scene breaks in a play.
Because of its value to natural language pro-
cessing, various text segmentation tasks have
been automated such as topical segmentation—
for which a variety of automatic segmenters exist
(e.g., Hearst 1997, Malioutov and Barzilay 2006,
Eisenstein and Barzilay 2008, and Kazantseva and
Szpakowicz 2011). This work addresses how to
best select an automatic segmenter and which seg-
mentation metrics are most appropriate to do so.
To select an automatic segmenter for a particu-
lar task, a variety of segmentation evaluation met-
rics have been proposed, including Pk (Beefer-
man and Berger, 1999, pp. 198–200), WindowDiff
(WD; Pevzner and Hearst 2002, p. 10), and most
recently Segmentation Similarity (S; Fournier and
Inkpen 2012, p. 154–156). Each of these met-
rics have a variety of flaws: Pk and WindowD-
iff both under-penalize errors at the beginning of
segmentations (Lamprier et al., 2007) and have a
bias towards favouring segmentations with few or
tightly-clustered boundaries (Niekrasz and Moore,
2010), while S produces overly optimistic values
due to its normalization (shown later).
To overcome the flaws of existing text segmen-
tation metrics, this work proposes a new series of
metrics derived from an adaptation of boundary
edit distance (Fournier and Inkpen, 2012, p. 154–
156). This new metric is named boundary similar-
ity (B). A confusion matrix to interpret segmenta-
tion as a classification problem is also proposed,
allowing for the computation of information re-
trieval (IR) metrics such as precision and recall.1
In this work: §2 reviews existing segmentation
metrics; §3 proposes an adaptation of boundary
edit distance, a new normalization of it, a new
confusion matrix for segmentation, and an inter-
</bodyText>
<note confidence="0.894108333333334">
1An implementation of boundary edit distance, bound-
ary similarity, B-precision, and B-recall, etc. is provided at
http://nlp.chrisfournier.ca/
1702
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1702–1712,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.9990324">
coder agreement coefficient adaptation; §4 com-
pares existing segmentation metrics to those pro-
posed herein; §5 evaluates S and B based inter-
coder agreement; and §6 compares B, S, and WD
while evaluating automatic segmenters.
</bodyText>
<sectionHeader confidence="0.999685" genericHeader="introduction">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.998287">
2.1 Segmentation Evaluation
</subsectionHeader>
<bodyText confidence="0.999862777777778">
Many early studies evaluated automatic seg-
menters using information retrieval (IR) metrics
such as precision, recall, etc. These metrics looked
at segmentation as a binary classification prob-
lem and were very harsh in their comparisons—no
credit was awarded for nearly missing a boundary.
Near misses occur frequently in segmentation—
although manual coders often agree upon the bulk
of where segment lie, they frequently disagree
upon the exact position of boundaries (Artstein
and Poesio, 2008, p. 40). To attempt to overcome
this issue, both Passonneau and Litman (1993) and
Hearst (1993) conflated multiple manual segmen-
tations into one that contained only those bound-
aries which the majority of coders agreed upon. IR
metrics were then used to compare automatic seg-
menters to this majority solution. Such a major-
ity solution is unsuitable, however, because it does
not contain actual subtopic breaks, but instead the
conflation of a collection of potentially disagree-
ing solutions. Additionally, the definition of what
constitutes a majority is subjective (e.g., Passon-
neau and Litman (1993, p. 150), Litman and Pas-
sonneau (1995), Hearst (1993, p. 6) each used 4/7,
3/7, and &gt; 50%, respectively).
To address the issue of awarding partial
credit for an automatic segmenter nearly missing
a boundary—without conflating segmentations,
Beeferman and Berger (1999, pp. 198–200) pro-
posed a new metric named Pk. Pevzner and Hearst
(2002, pp. 3–4) explain Pk well: a window of size
k—where k is half of the mean manual segmen-
tation length—is slid across both automatic and
manual segmentations. A penalty is awarded if
the window’s edges are found to be in differing
or the same segments within the manual segmen-
tation and the automatic segmentation disagrees.
Pk is the sum of these penalties over all windows.
Measuring the proportion of windows in error al-
lows Pk to penalize a fully missed boundary by
k windows, whereas a nearly missed boundary is
penalized by the distance that it is offset.
Pk was not without issue, however. Pevzner
and Hearst (2002, pp. 5–10) identified that Pk:
i) penalizes false negatives (FNs)2 more than false
positives (FPs); ii) does not penalize full misses
within k units of a reference boundary; iii) penal-
ize near misses too harshly in some situations; and
iv) is sensitive to internal segment size variance.
To solve Pk’s issues, Pevzner and Hearst (2002,
pp. 10) proposed a modification referred to as
WindowDiff (WD). Its major difference is in how
it decides to penalized windows: within a window,
if the number of boundaries in the manual segmen-
tation (MZj) differs from the number of bound-
aries in the automatic segmentation (AZj), then a
penalty is given. The ratio of penalties over win-
dows then represents the degree of error between
the segmentations, as in Equation 1. This change
better allowed WD to: i) penalize FPs and FNs
more equally;3 ii) Not skip full misses; iii) Less
harshly penalize near misses; and iv) Reduce its
sensitivity to internal segment size variance.
</bodyText>
<equation confidence="0.9910044">
N−k
1
Y-
WD(M, A) = N − k
i=1,j=i+k
</equation>
<bodyText confidence="0.911048461538462">
WD did not, however, solve all of the issues
related to window-based segmentation compari-
son. WD, and inherently Pk: i) Penalize er-
rors less at the beginning and end of segmenta-
tions (Lamprier et al., 2007); ii) Are biased to-
wards favouring automatic segmentations with ei-
ther few or tightly-clustered boundaries (Niekrasz
and Moore, 2010); iii) Calculate window size k
inconsistently;4 iv) Are not symmetric5 (meaning
that they cannot be used to produce a pairwise
mean of multiple manual segmentations6).
Segmentation Similarity (S; Fournier and
Inkpen 2012, pp. 154–156) took a different ap-
proach to comparing segmentations. Instead of us-
ing windows, the work proposes a new restricted
edit distance called boundary edit distance which
differentiates between full and near misses. S then
2I.e., a boundary present in the manual but not the auto-
matic segmentation, and the reverse for a false positive.
3Georgescul et al. (2006, p. 48) noted that WD interprets
a near miss as a FP probabilistically more than as a FN.
4k must be an integer, but half of a mean may be a frac-
tion, thus rounding must be used, but no rounding method
is specified. It is also not specified whether k should be set
once during a study or recalculated for each comparison—
this work assumes the latter.
</bodyText>
<footnote confidence="0.8518972">
5Window size is calculated only upon the manual segmen-
tation, meaning that one must be a manual and other an auto-
matic segmentation.
6This also means that WD and Pk cannot be adapted to
compute inter-coder agreement coefficients.
</footnote>
<equation confidence="0.950829">
(|Mij − Aij |&gt; 0) (1)
1703
</equation>
<bodyText confidence="0.999478571428572">
normalizes the counts of full and near misses iden-
tified by boundary edit distance, as shown in Equa-
tion 2, where sa and sb are the segmentations, nt
is the maximum distance that boundaries may span
to be considered a near miss, edits(sa, sb, nt) is the
edit distance, and pb(D) is the number of potential
boundaries in a document D (pb(D) = |D |− 1).
</bodyText>
<equation confidence="0.9946295">
S(sa, sb, nt) = 1 − |edits(sa, sb, nt) |(2)
pb(D)
</equation>
<bodyText confidence="0.999564392857143">
Boundary edit distance models full misses as
the addition/deletion of a boundary, and near
misses as n-wise transpositions. An n-wise trans-
position is the act of swapping the position of
a boundary with an empty position such that it
matches a boundary in the segmentation compared
against (up to a spanning distance of nt). S also
scales the severity of a near miss by the distance
over which it is transposed, allowing it to scale
the penalty of a near misses much like WD. S is
also symmetric, allowing it to be used in pairwise
means and inter-coder agreement coefficients.
The usage of an edit distance that supported
transpositions to compare segmentations was an
advancement over window-based methods, but
boundary edit distance and its normalization S are
not without problems, specifically: i) This edit dis-
tance uses string reversals (ABCD =⇒ DCBA)
to perform transpositions, making it cumbersome
to analyse individual pairs of boundaries between
segmentations; ii) S is sensitive to variations in the
total size of a segmentation, leading it to favour
very sparse segmentations with few boundaries;
iii) S produces cosmetically high values, making
it difficult to interpret and causing over-estimation
of inter-coder agreement. In this work, these defi-
ciencies are demonstrated and a new set of metrics
are proposed as replacements.
</bodyText>
<subsectionHeader confidence="0.97481">
2.2 Inter-Coder Agreement
</subsectionHeader>
<bodyText confidence="0.99975225">
Inter-coder agreement coefficients are used to
measure whether a group of human judges (i.e.
coders) agree with each other greater than chance.
Such coefficients are used to determine the relia-
bility and replicability of the coding scheme and
instructions used to collect manual codings (Car-
letta, 1996). Although direct interpretation of such
coefficients is difficult, they are an invaluable tool
when comparing segmentation data that has been
collected with differing labels and when estimat-
ing the replicability of a study. A variety of inter-
coder agreement coefficients exist, but this work
focuses upon a selection of those discussed by Art-
stein and Poesio (2008), specifically: Scott’s π
(Scott, 1955) Fleiss’ multi-π (π*, Fleiss 1971)7,
Cohen’s κ (Cohen, 1960), and multi-κ (κ*, Davies
and Fleiss 1982). Their general forms are shown in
Equation 3, where Aa represents actual agreement,
and Ae expected (i.e., chance) agreement between
coders.
</bodyText>
<equation confidence="0.956401666666667">
Aa − Ae
κ, π, κ*, and π* = (3)
1 − Ae
</equation>
<bodyText confidence="0.999893333333333">
When calculating agreement between manual
segmenters, boundaries are considered labels and
their positions the decisions. Unfortunately, be-
cause of the frequency of near misses that oc-
cur in segmentation, using such labels and de-
cisions causes inter-coder agreement coefficients
to drastically underestimate actual agreement—
much like how automatic segmenter performance
is underestimated when segmentation is treated
as a binary classification problem. Hearst (1997,
pp. 53–54) attempted to adapt π* to award par-
tial credit for near misses by using the percentage
agreement metric of Gale et al. (1992, p. 254) to
compute actual agreement—which conflates mul-
tiple manual segmentations together according to
whether a majority of coders agree upon a bound-
ary or not. Unfortunately, such a method of com-
puting agreement grossly inflates results, and “the
statistic itself guarantees at least 50% agreement
by only pairing off coders against the majority
opinion” (Isard and Carletta, 1995, p. 63).
Fournier and Inkpen (2012, pp. 154–156) pro-
posed using pairwise mean S for actual agree-
ment to allow inter-coder agreement coefficients
to award partial credit for near misses. Unfor-
tunately, because S produces cosmetically high
values, it also causes inter-coder agreement coef-
ficients to drastically overestimates actual agree-
ment. This work demonstrates this deficiency and
proposes and evaluates a solution.
</bodyText>
<sectionHeader confidence="0.969962" genericHeader="method">
3 A New Proposal for Edit-Based Text
</sectionHeader>
<subsectionHeader confidence="0.522828">
Segmentation Evaluation
</subsectionHeader>
<bodyText confidence="0.9999035">
In this section, a new boundary edit distance based
segmentation metric and confusion matrix is pro-
posed to solve the deficiencies of S for both seg-
mentation comparison and inter-coder agreement.
</bodyText>
<footnote confidence="0.593564">
7Sometimes referred to as K (Siegel and Castellan, 1988).
</footnote>
<page confidence="0.815361">
1704
</page>
<subsectionHeader confidence="0.998021">
3.1 Boundary Edit Distance
</subsectionHeader>
<bodyText confidence="0.95468125">
In this section, Boundary Edit Distance (BED; as
proposed in Fournier and Inkpen 2012, pp. 154–
156) is introduced in more detail, and a few termi-
nological and conceptual changes are made.
</bodyText>
<listItem confidence="0.971707">
Boundary Edit Distance uses three main edit op-
erations to model segmentation differences:
• Additions/deletions (AD; referred to origi-
nally as substitutions) for full misses;
• Substitutions (S; not shown for brevity) for
confusing one boundary type with another;
• n-wise transpositions (T) for near misses.
</listItem>
<bodyText confidence="0.989020384615385">
These edit operations are symmetric and oper-
ate upon the set of boundaries that occur at each
potential boundary position in a pair of segmenta-
tions. An example of how these edit operations are
applied$ is shown in Figure 1, where a near miss
(T), a matching pair of boundaries (M), and two
full misses (ADs) are shown with the maximum
distance that a transposition can span (nt) set to 2
potential boundaries (i.e., only adjacent positions
can be transposed).
these decisions. Imagine that
is a manual seg-
mentation, and s2 is an automatic
hy-
pothesis. The transposition is a partially correct
decision, or boundary pair. The match is a correct
boundary pair. The additions/deletions, however,
could be one of two erroneous decisions: to not
place an expected boundary (FN), or to place a su-
perfluous boundary (FP).9
This work proposes assigning a correctness
score for each boundary pair/decision (shown in
Table 1) and then using the mean of this score as
a normalization of boundary edit distance. This
interpretation intuitively relates boundary edit dis-
tan
</bodyText>
<equation confidence="0.804523">
s1
segmenter’s
</equation>
<bodyText confidence="0.892392666666667">
ce to coder judgements, making it ideal for
dit for near misses.
d comparing segmentations.
calculating actual agreement in inter-coder agree-
ment coefficients an
similarity can be defined as shown in Equation 4—
</bodyText>
<equation confidence="0.9934975">
nt)
+ wt
nt) +
ord(Se, Tb)
+
+
+
(4)
</equation>
<bodyText confidence="0.992868">
This form, one minus a penalty function, was
chosen so that it was easier to compare against
other penalty functions considered (not shown
here for brevity). This normalization was also cho-
sen because itis equivalent to mean boundary pair
correctness and so that it ranges in value from 0 to
1. In the worst case, a segmentation comparison
will result in no matches, no near misses, no sub-
stitutions, and X full misses, i.e.,
</bodyText>
<equation confidence="0.973843">
= X and all
other terms in Equation 4 are zero, meaning that:
B = 1
</equation>
<bodyText confidence="0.9981">
In the best case, a segmentation comparison will
result in X matches, no near misses, no substitu-
tions, and no full misses, i.e.,
= X and all
other terms in Equation 4 are zero, meaning that:
For all other scenarios, varying numbers of
matches, near misses, substitutions and full misses
will result in values of B between 0 and 1.
Equation 4 takes two segmentations (in any or-
der), and the maximum transposition spanning
distance (nt). This distance represents the great-
est offset between boundary positions that could
be considered a near miss and can
</bodyText>
<equation confidence="0.979167923076923">
B(s1,s2,
=1− |Ae|
span(Te,
ws
|Ae|
|Te|
|Se|
|BM|
|Ae|
X + 0 + 0
−X + 0 + 0 + 0= 1 − X/X = 1 − 1 =
|BM|
0 + 0 + 0B = 1 − 0 + 0 + 0 + X= 1 − 0/X = 1 − 0 =
</equation>
<figure confidence="0.922422866666667">
be used to scale
s1
s2
2
3
T
4
3
M
4
6
AD
AD
4
2
</figure>
<figureCaption confidence="0.99993">
Figure 1: Boundary edit operations
</figureCaption>
<bodyText confidence="0.870250888888889">
In Figure 1, the location of the errors is clearly
shown. Importantly, however, pairs of boundaries
between the segmentations can be seen that rep-
resent the decisions made, and the correctness of
$A complete explanation of Boundary Edit Distance is de-
tailed in Fournier (2013, Section 4.1.2).
9Also note that the ADs are close together, and if nt &gt; 2,
then they would be considered a T, and not two ADs—this is
one way to award partial cre
</bodyText>
<figure confidence="0.983160875">
Pair Corre
ctness
Match 1
Addition/deletion 0
Transposition 1
nt)
Substitution 1
Tb)
</figure>
<tableCaption confidence="0.958716">
Table 1: Correctness of boundary
</tableCaption>
<bodyText confidence="0.499645666666667">
−wtspan(Te,
−wsord(Se,
pair
</bodyText>
<subsectionHeader confidence="0.99747">
3.2 Boundary Similarity
</subsectionHeader>
<bodyText confidence="0.9811048125">
The new boundary edit distance normalization
proposed herein is referred to as boundary similar-
ity (B). Assuming that boundary edit distance pro-
duces sets of edit operations where Ae is the set of
additions/deletions, Te the set of n-wise transpo-
sitions, Se the set of substitutions, and BM the set
of matching boundary pairs, boundary similarity
one minus the incorrectness of each boundary pair
over the total number of boundary pairs.
1705
the severity of a near miss. A variety of scaling
functions could be used, and this work arbitrarily
chooses a simple fraction to represent each trans-
position’s severity in terms of its distance from its
paired boundary over nt plus a constant wt (0 by
default), as shown in Equation 5.
</bodyText>
<equation confidence="0.96246">
wt span(Te, nt) = X wt + abs(Te[j][1] − Te[j][2])
j=1
|Te |� �
nt
</equation>
<bodyText confidence="0.999948375">
If multiple boundary types are used, then sub-
stitution edit operations would occur when one
boundary type was confused with another. As-
signing each boundary type tb ∈ Tb a number on
an ordinal scale, substitutions can be weighted by
their distance on this scale over the maximum dis-
tance plus a constant ws (0 by default), as shown
in Equation 6.
</bodyText>
<equation confidence="0.998660666666667">
ws ord(Se,Tb) =
j=1
E(wsabs(Se/[j][1] − Se//[j][2])1 + max(Tb) − min(Tb) /J
</equation>
<bodyText confidence="0.999805">
These scaling functions allow for edit penalties
to range from 0 to ws/t plus some linear distance.
</bodyText>
<subsectionHeader confidence="0.993808">
3.3 A Confusion Matrix for Segmentation
</subsectionHeader>
<bodyText confidence="0.999938444444444">
The mean correctness of each pair (i.e., B) gives an
indication of just how similar one segmentation is
to another, but what if one wants to identify some
specific attributes of the performance of an auto-
matic segmenter? Is the segmenter confusing one
boundary type with another, or is it very precise
but has poor recall? The answers to these ques-
tions can be obtained by looking at text segmenta-
tion as a multi-class classification problem.
This work proposes using a task’s set of bound-
ary types (Tb) and the lack of a boundary (0)
to represent the set of segmentation classes in
a boundary classification problem. Using these
classes, a confusion matrix (defined in Equation 7)
can be created which sums boundary pair correct-
ness so that information-retrieval metrics can be
calculated that award partial credit to near misses
by scaling edits operations.
</bodyText>
<equation confidence="0.9992465">
|BM,a |+ ws ord(Sa,p
e , Tb)
+wt span(Ta,p
e , nt) if a = p
ws ord(Sa,p
e , Tb) (7)
+wt span(Ta,p
e , nt) ifa =6 p
|Ae,a |if p = 0
|Ae,p |ifa = 0
</equation>
<bodyText confidence="0.99955825">
An example confusion matrix is shown in Fig-
ure 2 from which IR metrics such as precision, re-
call, and Fa-measure can be computed (referred to
as B-precision, B-recall, etc.).
</bodyText>
<figure confidence="0.6326325">
Actual
B Non-B
B CM(1, 1) CM(0, 1)
Non-B CM(1, 0) CM(0, 0)
</figure>
<figureCaption confidence="0.994631">
Figure 2: Example confusion matrix (Tb = {1})
</figureCaption>
<subsectionHeader confidence="0.850421">
3.4 B-Based Inter-coder Agreement
</subsectionHeader>
<bodyText confidence="0.999988083333333">
Fournier and Inkpen (2012, p. 156–157) adapted
four inter-coder agreement formulations provided
by Artstein and Poesio (2008) to use S to award
partial credit for near misses, but because S pro-
duces cosmetically high agreement values they
grossly overestimate agreement. To solve this
issue, this work instead proposes using micro-
average B (i.e., mean boundary pair correctness
over all documents and codings compared) to
solve this issue (demonstrated in §5) because it
does not over-estimate actual agreement (demon-
strated in §4 and 5).
</bodyText>
<sectionHeader confidence="0.951103" genericHeader="method">
4 Discussion of Segmentation Metrics
</sectionHeader>
<bodyText confidence="0.999623">
Before analysing how each metric compares to
each other upon a large data set, it would be useful
to investigate how they act on a smaller scale. To
that end, this section discusses how each metric in-
terprets a set of hypothetical segmentations of an
excerpt of a poem by Coleridge (1816, pp. 55–58)
titled Kubla Khan (shown in Figure 3)—chosen ar-
bitrarily for its brevity (and beauty). These seg-
mentations are topical and at the line-level.
</bodyText>
<listItem confidence="0.965605909090909">
1. In Xanadu did Kubla Khan
2. A stately pleasure-dome decree:
3. Where Alph, the sacred river, ran
4. Through caverns measureless to man
5. Down to a sunless sea.
6. So twice five miles of fertile ground
7. With walls and towers were girdled round:
8. And here were gardens bright with sinuous rills,
9. Where blossomed many an incense-bearing tree;
10. And here were forests ancient as the hills,
11. Enfolding sunny spots of greenery.
</listItem>
<figureCaption confidence="0.784171">
Figure 3: Excerpt from the poem Kubla Khan (Co-
</figureCaption>
<bodyText confidence="0.9617648">
leridge, 1816, pp. 55–58) with line numbers
Topical segmentations of this poem are difficult
to produce because there is still some structural
form (i.e., punctuation) which may dictate where
a boundary lies, but the imagery, places, times, and
subjects of the poem appear to twist and wind like
a vision in a dream. Thus, placing a topical bound-
ary in this text is a highly subjective task. One
hypothetical topical segmentation of the excerpt is
shown in Figure 4. In this section, a variety of
</bodyText>
<figure confidence="0.868081307692308">
⎧
⎨⎪⎪⎪⎪⎪⎪⎪⎪ ⎪
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
CM(a, p) =
Predicted
1706
contrived automatic segmentations are compared
to this manual segmentation to illustrate how each
metric reacts to different mistakes.
Lines Description
1–2 Kubla Khan and his decree
3–5 Waterways
6–11 Fertile ground and greenery
</figure>
<figureCaption confidence="0.999785">
Figure 4: A hypothetical manual segmentation
</figureCaption>
<bodyText confidence="0.999430888888889">
Assuming that Figure 4 represents an accept-
able manual segmentation (m), how would each
metric react to an automatic segmentation (a) that
combines the segments 1–2 and 3–5 together?
This would represent a full miss, or a false neg-
ative, as shown in Figure 5. S interprets these seg-
mentations as being quite similar, yet, the auto-
matic segmentation is missing a boundary. B and
1−WD,10 in this case, better reflect this error.
</bodyText>
<figureCaption confidence="0.990256">
Figure 5: False negative
</figureCaption>
<bodyText confidence="0.999961333333333">
How would each metric react to an automatic
segmentation that is very close to placing the
boundaries correctly, but makes the slight mis-
take of thinking that the segment on waterways
(3–5) ends a bit too early? This would repre-
sent a near miss, as shown in Figure 6. S and
1−WD incorrectly interpret this error as being
equivalent to the previous false negative—a trou-
bling result. Segmentation comparison metrics
should be able to discern between the full and a
near miss shown in these two figures, and an au-
tomatic segmenter that nearly misses a boundary
should be awarded a better score than one which
fully misses a boundary—B recognizes this and
awards the near miss a higher score.
</bodyText>
<figureCaption confidence="0.955978">
Figure 6: Near miss
</figureCaption>
<bodyText confidence="0.9995031">
How would each metric react to an automatic
segmentation that adds an additional boundary be-
tween line 8 and 9? This would not be ideal
because such a boundary falls in the middle of
a cohesive description of a garden, representing
10WD is reported as 1−WD because WD is normally a
penalty metric where a value of 0 is ideal, unlike S and B. Ad-
ditionally, k = 2 for all examples in this section because WD
computes k from the manual segmentation m, which does not
change in these examples.
a full miss, or false positive, as in Figure 7. S
and 1−WD incorrectly interpret this error as be-
ing equivalent to the previous two errors—an even
more troubling result. In this case, there are two
matching boundaries and a pair that do not match,
which is arguably preferable to the full miss and
one match in Figure 5, but not to the match and
near miss in Figure 6. B recognizes this, and
awards a higher score to this automatic segmenter
than that in Figure 5, but below Figure 6.
</bodyText>
<figureCaption confidence="0.997541">
Figure 7: False positive
</figureCaption>
<bodyText confidence="0.999274857142857">
How would each metric react to an automatic
segmentation that compensates for its lack of pre-
cision by spuriously adding boundaries in clusters
around where it thinks that segments should begin
or end? This is shown in Figure 8. This kind of
behaviour is finally penalized differently by S and
1−WD (unlike the other errors shown in this sec-
tion), but it only barely results in a dip in their val-
ues. B also penalizes this behaviour, but does so
much more harshly—in B’s interpretation, this is
as egregious as committing a false negative (e.g.,
Figure 5)—an arguably correct interpretation, if
the evaluation desires to maximize similarity with
a manual segmentation.
</bodyText>
<equation confidence="0.944791">
S B 1−WD
0.8 0.5 0.66 6
k = 2
</equation>
<figureCaption confidence="0.99312">
Figure 8: Cluster of false positives
</figureCaption>
<bodyText confidence="0.999975571428572">
These short demonstrations of how S, B, and
1−WD interpret error should lead one to con-
clude that: i) WD can penalize near misses to
the same degree as full misses—overly harshly;
ii) Both S and WD are not very discriminating
when small segments are analysed; and iii) B is
the only one of the three metrics that is able to
often discriminate between these situations. B, if
used to rank these automatic segmenters, would
rank them from best to worst performing as: the
near miss, false positive, and then a tie between
the false negative and cluster of false positives—a
reasonable ranking in the context of an evaluation
seeking similarity with a manual segmentation.
</bodyText>
<sectionHeader confidence="0.922242" genericHeader="method">
5 Segmentation Agreement
</sectionHeader>
<bodyText confidence="0.9502325">
Having a bit more confidence in B compared to S
and WD on a small scale from the previous sec-
</bodyText>
<figure confidence="0.998984318181818">
S B 1−WD
0.9 0.5 0.777
k = 2
m
a
S B 1−WD
0.9 0.75 0.777
k = 2
m
a
S B 1−WD
0.9 0.66 6 0.777
k = 2
m
a
m
a
1707
1.0
0.8
S B
2
3
4
5
6
7
8
9
10
0.8
1.0
2
3
4
5
6
7
8
9
10
0.90
0.85
0.80
0.75
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
P(miss) while P(near) = 0.0921
0.6
0.4
0.2
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
P(miss) while P(near) = 0.0921
0.6
0.4
0.2
2 3 4 5 6 7 8 9 10
Coders (quantity)
0.0
1.00
0.95
(a) S-based 7r* showing increasing full
misses with constant near misses
(b) B-based 7r* showing increasing full
misses with constant near misses
(c) S and B based 7r* with fully random
segmentations
</figure>
<figureCaption confidence="0.980558">
Figure 9: Artificial data sets illustrating how 7r adapted to use either S or B reacts to increasing full
misses and random segmentations and varying numbers of coders
</figureCaption>
<bodyText confidence="0.999953153846154">
tion, it makes sense to analyse some larger data
sets. Two such data sets are The Stargazer data
set collected by Hearst (1997) and The Moonstone
data set collected by Kazantseva and Szpakowicz
(2012). Both are linear topical segmentations at
the paragraph level with only one boundary type,
but that is where their similarities end.
The Stargazer text is a science magazine article
titled “Stargazers look for life” (Baker, 1990) seg-
mented by 7 coders and was one of twelve articles
chosen for its length (between 1,800 and 2,500
words) and for having little structural demarca-
tion. “The Moonstone” is a 19th century romance
novel by Collins (1868) segmented by 4–6 coders
per chapter; of its 23 chapters, 2 were coded in a
pilot study and another 20 were coded individually
by 27 undergraduate English students in 5 groups.
For the Stargazer data set, using S-based 7r*,
an inter-coder agreement coefficient of 0.7562 is
obtained—a reasonable level by content analysis
standards. Unfortunately, this value is highly in-
flated, and B-based 7r* gives a much more conser-
vative coefficient at 0.4405. For the Moonstone
data set, the agreement coefficients for each group
of 4–6 coders using S-based 7r* is again over-
inflated at 0.91, 0.92, 0.90, 0.94, 0.83. B-based
7r* instead reports that the coefficients should be
0.20, 0.18, 0.40, 0.38, 0.23.
Which of these coefficients should be trusted?
Is agreement in these data sets high or low? To
help answer that, this work looks at how the coders
in the data sets behaved. If the segmenters in the
Moonstone data set truly agreed with each other,
then they should have all behaved similarly. One
measure of coder behaviour is the frequency that
they placed boundaries (normalized by their op-
portunity to place boundaries, i.e. the sum of the
potential boundaries in the chapters that each seg-
mented). This normalized frequency is shown per
</bodyText>
<figure confidence="0.995488714285714">
0.25
Boundaries per potential boundary 0.20
0.15
0.10
0.05
0.00
Coder
</figure>
<figureCaption confidence="0.8221435">
Figure 11: Normalized boundaries placed by each
coder in the Moonstone data set (with mean±SD)
</figureCaption>
<bodyText confidence="0.99991316">
coder in Figure 11 for The Moonstone data set,
along with bars indicating the mean and one stan-
dard deviation above and below. As can be seen,
the coders fluctuated wildly in the frequency with
which they placed boundaries—some (e.g., coder
7) to degrees exceeding 2 standard deviations. The
Moonstone data set as a whole does not exhibit
coders who behaved similarly, supporting the as-
sertion by B-based 7r* that these coders do not
agree well (though pockets of agreement exist).
How can it be demonstrated that S-based
agreement over-estimates agreement, and B-based
agreement does not? One way to demonstrate this
is through simulation. By estimating parameters
from the large Moonstone data set such as the dis-
tribution of internal segment sizes produced by all
coders, a random segmentation of the novel with
similar characteristics can be created. From this
single random segmentation, other segmentation
can be created with a probability of either placing
an offset boundary (i.e., a near miss) or placing
an extra/omitting a boundary (i.e., a full miss)—
a pseudo-coding. Manipulating these probabilities
and keeping the probability of a near miss at a con-
stant natural level should produce a slowly declin-
</bodyText>
<figure confidence="0.997372051724138">
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
1708
0.94
0.92
0.90
0.88
0.86
0.84
0.82
= 0 = 0 = 0 = 0 = 0
Random Human BayesSeg APS MinOu�
0.75
0.70
0.65
0.60
0.55
0.50
0.45
0.40
0.35
0.80
n = 0 = 0 = 0 = 0 = 0
n = 1057 n = 41 n = 64 n = 7 n = 71
0.6
0.5
0.4
0.3
0.2
Random Human BayesSeg APS MinCut
Random Human BayesSeg APS MmOut
a� �a e u� seg e
(a) S (b) B (c) 1−WD
</figure>
<figureCaption confidence="0.997656">
Figure 10: Mean performance of 5 segmenters using varying metrics with 95% confidence intervals
</figureCaption>
<bodyText confidence="0.999944346153846">
ing amount of agreement which is unaffected by
the number of pseudo-coders. This is not appar-
ent, however, for S-based 7r* in Figure 9a; as the
probability of a full miss increases, agreement ap-
pears to rise and varies depending upon the num-
ber of pseudo-coders. B-based 7r* however shows
declining agreement and little to no variation de-
pending upon the number of pseudo-coders, as
shown in Figure 9b.
If instead of creating pseudo-coders from a ran-
dom segmentation a series of random segmenta-
tions with the same parameters were generated, a
properly functioning inter-coder agreement coef-
ficient should report some agreement (due to the
similar parameters used to create the segmenta-
tions) but it should be quite low. Figure 9c shows
this, and that S-based 7r* drastically over-estimates
what should be very low agreement whereas B-
based 7r* properly reports low agreement.
From these demonstrations, it is evident that
S-based inter-coder agreement coefficients dras-
tically over-estimate agreement, as does S itself
in pairwise mean form. B-based coefficients,
however, properly discriminate between levels of
agreement regardless of the number of coders and
do not over-estimate.
</bodyText>
<sectionHeader confidence="0.639209" genericHeader="method">
6 Evaluation of Automatic Segmenters
</sectionHeader>
<bodyText confidence="0.999970673469388">
Having looked at how S, WD, and B perform at
a small scale in §4 and on larger data set in §5,
this section demonstrates the use of these met-
rics to evaluate some automatic segmenters. Three
automatic segmenters were trained—or had their
parameters estimated upon—The Moonstone data
set, including MinCut; (Malioutov and Barzilay,
2006), BayesSeg; (Eisenstein and Barzilay, 2008),
and APS (Kazantseva and Szpakowicz, 2011).
To put this evaluation into context, an upper and
lower bound were also created comprised of a ran-
dom coder from the manual data (Human) and a
random segmenter (Random), respectively. These
automatic segmenters, and the upper and lower
bounds, were created, trained, and run by another
researcher (Anna Kazantseva) with their labels re-
moved during the development of the metrics de-
tailed herein (to improve the impartiality of these
analyses). An ideal segmentation evaluation met-
ric should, in theory, place the three automatic seg-
menters between the upper and lower bounds in
terms of performance if the metrics, and the seg-
menters, function properly.
The mean performance of the upper and lower
bounds upon the test set of the Moonstone data
set using S, B, and WD are shown in Figure 10a–
10c along with 95% confidence intervals. Despite
the difference in the scale of their values, both S
and WD performed almost identically, placing the
three automatic segmenters between the upper and
lower bounds as expected. For S, statistically sig-
nificant differences11 (α = 0.05) were found be-
tween all segmenters except between APS–human
and MinCut–BayesSeg, and WD could only find
significant differences between the automatic seg-
menters and the upper and lower bounds. B, how-
ever, shows a marked deviation, and places Min-
Cut and APS statistically significantly below the
random baseline with only BayesSeg between the
upper and lower bounds—to a significant degree.
Why would pairwise mean B act in such an
unexpected manner? The answer lies in a fur-
ther analysis using the confusion matrix proposed
earlier to calculate B-precision and B-recall (as
shown in Table 2). From the values in Table 2,
all three automatic segmenters appear to have B-
precision above the baseline and below the upper
bound, but the B-recall of both APS and MinCut
is below that of the random baseline (illustrated
</bodyText>
<note confidence="0.82798725">
11Using Kruskal-Wallis rank sum multiple comparison
tests (Siegel and Castellan, 1988, pp. 213-214) for S and
WD and the Wilcoxon-Nemenyi-McDonald-Thompson test
(Hollander and Wolfe, 1999, p. 295) for B.
</note>
<table confidence="0.978210571428571">
1709
B n B-P B-R B-F1 TP FP FN TN
Random 0.2640 f 0.0129 1057 0.3991 0.4673 0.4306 279.0 420 318 4236.0
Human 0.5285 f 0.0164 841 0.6854 0.7439 0.7135 444.5 204 153 4451.5
BayesSeg 0.3745 f 0.0146 964 0.5247 0.6224 0.5694 361.0 327 219 4346.0
APS 0.2873 f 0.0163 738 0.6773 0.3403 0.4530 212.0 101 411 4529.0
MinCut 0.2468 f 0.0141 871 0.4788 0.3496 0.4041 215.0 234 400 4404.0
</table>
<tableCaption confidence="0.7973365">
Table 2: Mean performance of 5 segmenters using micro-average B, B-precision (B-P), B-recall (B-R),
and B-Fβ-measure (B-Fl) along with the associated confusion matrix values for 5 segmenters
</tableCaption>
<figureCaption confidence="0.5760725">
Figure 12: Mean B-precision versus B-recall of 5
automatic segmenters
</figureCaption>
<bodyText confidence="0.999986076923077">
in Figure 12). These automatic segmenters were
developed and performance tuned using WD, thus
it would be expected that they would perform as
they did according to WD, but the evaluation using
B highlights WD’s bias towards sparse segmenta-
tions (i.e., those with low B-recall)—a failing that
S also appears to share. Mean B shows an un-
biased ranking of these automatic segmenters in
terms of the upper and lower bounds. B, then,
should be preferred over S and WD for an un-
biased segmentation evaluation that assumes that
similarity to a human solution is the best measure
of performance for a task.
</bodyText>
<sectionHeader confidence="0.998261" genericHeader="method">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999983666666666">
In this work, a new segmentation evaluation met-
ric, referred to as boundary similarity (B) is
proposed as an unbiased metric, along with a
boundary-edit-distance-based (BED-based) con-
fusion matrix to compute predictably biased IR
metrics such as precision and recall. Additionally,
a method of adapting inter-coder agreement coef-
ficients to award partial credit for near misses is
proposed that uses B as opposed to S for actual
agreement so as to not over-estimate agreement.
B overcomes the cosmetically high values of S
and, the bias towards segmentations with few or
tightly-clustered boundaries of WD–manifesting
in this work as a bias towards precision over recall
for both WD and S. When such precision is desir-
able, however, B-precision can be computed from
a BED-based confusion matrix, along with other
IR metrics. WD and Pk should not be preferred
because their biases do not occur consistently in
all scenarios, whereas BED-based IR metrics offer
expected biases built upon a consistent, edit-based,
interpretation of segmentation error.
B also allows for an intuitive comparison of
boundary pairs between segmentations, as op-
posed to the window counts of WD or the sim-
plistic edit count normalization of S. When an un-
biased segmentation evaluation metric is desired,
this work recommends the usage of B and the use
of an upper and lower bound to provide context.
Otherwise, if the evaluation of a segmentation task
requires some biased measure, the predictable bias
of IR metrics computed from a BED-based con-
fusion matrix is recommended. For all evalua-
tions, however, a justification for the biased/un-
biased metrics used should be given, and more
than one metric should be reported so as to allow
a reader to ascertain for themselves whether a par-
ticular automatic segmenter’s bias in some manner
is cause for concern or not.
</bodyText>
<sectionHeader confidence="0.99748" genericHeader="discussions">
8 Future Work
</sectionHeader>
<bodyText confidence="0.9999545">
Future work includes adapting this work to anal-
yse hierarchical segmentations and using it to at-
tempt to explain the low inter-coder agreement co-
efficients reported in topical segmentation tasks.
</bodyText>
<sectionHeader confidence="0.990992" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999355166666667">
I would like to thank Anna Kazantseva for her in-
valuable feedback and data. Additionally, I would
like to thank my thesis committee members—Stan
Szpakowicz, James Green, and Xiaodan Zhu—for
their feedback along with my supervisor Diana
Inkpen and colleague Martin Scaiano.
</bodyText>
<figure confidence="0.994057733333333">
.0 0.2 0.4 0.6 0.8 1.0
B − recall
1.0
0.8
0.2
0.00
Random
Human
BayesSeg
APS
MinCut
B − precision
0.6
0.4
1710
</figure>
<sectionHeader confidence="0.862175" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997493968085107">
Artstein, Ron and Massimo Poesio. 2008. Inter-
coder agreement for computational linguistics.
Computational Linguistics 34(4):555–596.
Baker, David. 1990. Stargazers look for life. South
Magazine 117:76–77.
Beeferman, Doug and Adam Berger. 1999. Sta-
tistical models for text segmentation. Machine
Learning 34:177–210.
Carletta, Jean. 1996. Assessing Agreement on
Classification Tasks: The Kappa Statistic. Com-
putational Linguistics 22(2):249–254.
Chang, Pi-Chuan, Michel Galley, and Christo-
pher D. Manning. 2008. Optimizing Chinese
word segmentation for machine translation per-
formance. In Proceedings of the Third Work-
shop on Statistical Machine Translation. Asso-
ciation for Computational Linguistics, Strouds-
burg, PA, USA, pages 224–232.
Cohen, Jacob. 1960. A Coefficient of Agreement
for Nominal Scales. Educational and Psycho-
logical Measurement 20:37–46.
Coleridge, Samuel Taylor. 1816. Christabel,
Kubla Khan, and the Pains of Sleep. John Mur-
ray.
Collins, Wilkie. 1868. The Moonstone. Tinsley
Brothers.
Davies, Mark and Joseph L. Fleiss. 1982. Measur-
ing agreement for multinomial data. Biometrics
38:1047–1051.
Eisenstein, Jacob. 2009. Hierarchical text seg-
mentation from multi-scale lexical cohesion.
In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North
American Chapter of the Association for Com-
putational Linguistics. Association for Com-
putational Linguistics, Stroudsburg, PA, USA,
pages 353–361.
Eisenstein, Jacob and Regina Barzilay. 2008.
Bayesian unsupervised topic segmentation. In
Proceedings of the 2008 Conference on Em-
pirical Methods in Natural Language Process-
ing. Association for Computational Linguistics,
Morristown, NJ, USA, pages 334–343.
Fleiss, Joseph L. 1971. Measuring nominal scale
agreement among many raters. Psychological
Bulletin 76:378–382.
Fournier, Chris and Diana Inkpen. 2012. Segmen-
tation Similarity and Agreement. In Proceed-
ings of Human Language Technologies: The
2012 Annual Conference of the North American
Chapter of the Association for Computational
Linguistics. Association for Computational Lin-
guistics, Stroudsburg, PA, USA, pages 152–
161.
Fournier, Christopher. 2013. Evaluating Text Seg-
mentation. Master’s thesis, University of Ot-
tawa.
Franz, Martin, J. Scott McCarley, and Jian-Ming
Xu. 2007. User-oriented text segmentation eval-
uation measure. In Proceedings of the 30th An-
nual International ACM SIGIR Conference on
Research and Development in Information Re-
trieval. Association for Computing Machinery,
Stroudsburg, PA, USA, pages 701–702.
Gale, William, Kenneth Ward Church, and David
Yarowsky. 1992. Estimating upper and lower
bounds on the performance of word-sense dis-
ambiguation programs. In Proceedings of
the 30th Annual Meeting of the Association
for Computational Linguistics. Association for
Computational Linguistics, Stroudsburg, PA,
USA, pages 249–256.
Georgescul, Maria, Alexander Clark, and Susan
Armstrong. 2006. An analysis of quantita-
tive aspects in the evaluation of thematic seg-
mentation algorithms. In Proceedings of the
7th SIGdial Workshop on Discourse and Dia-
logue. Association for Computational Linguis-
tics, Stroudsburg, PA, USA, pages 144–151.
Haghighi, Aria and Lucy Vanderwende. 2009.
Exploring content models for multi-document
summarization. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the As-
sociation for Computational Linguistics. Asso-
ciation for Computational Linguistics, Strouds-
burg, PA, USA, NAACL ’09, pages 362–370.
Hearst, Marti A. 1993. TextTiling: A Quantitative
Approach to Discourse. Technical report, Uni-
versity of California at Berkeley, Berkeley, CA,
USA.
Hearst, Marti A. 1997. TextTiling: Segmenting
Text into Multi-paragraph Subtopic Passages.
Computational Linguistics 23:33–64.
Hollander, Myles and Douglas A. Wolfe. 1999.
1711
Nonparametric Statistical Methods. John Wi-
ley &amp; Sons, 2nd edition.
Isard, Amy and Jean Carletta. 1995. Replicability
of transaction and action coding in the map task
corpus. In AAAI Spring Symposium: Empirical
Methods in Discourse Interpretation and Gen-
eration. pages 60–66.
Kazantseva, Anna and Stan Szpakowicz. 2011.
Linear Text Segmentation Using Affinity Prop-
agation. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language
Processing. Association for Computational Lin-
guistics, Edinburgh, Scotland, UK., pages 284–
293.
Kazantseva, Anna and Stan Szpakowicz. 2012.
Topical Segmentation: a Study of Human Per-
formance. In Proceedings of Human Language
Technologies: The 2012 Annual Conference
of the North American Chapter of the Asso-
ciation for Computational Linguistics. Associ-
ation for Computational Linguistics, Strouds-
burg, PA, USA, pages 211–220.
Lamprier, Sylvain, Tassadit Amghar, Bernard
Levrat, and Frederic Saubion. 2007. On eval-
uation methodologies for text segmentation al-
gorithms. In Proceedings of the 19th IEEE In-
ternational Conference on Tools with Artificial
Intelligence. IEEE Computer Society, Washing-
ton, DC, USA, volume 2, pages 19–26.
Litman, Diane J. and Rebecca J. Passonneau.
1995. Combining multiple knowledge sources
for discourse segmentation. In Proceedings
of the 33rd Annual Meeting of the Association
for Computational Linguistics. Association for
Computational Linguistics, Stroudsburg, PA,
USA, pages 108–115.
Malioutov, Igor and Regina Barzilay. 2006. Min-
imum cut model for spoken lecture segmen-
tation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguis-
tics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics. Associ-
ation for Computational Linguistics, Strouds-
burg, PA, USA, pages 25–32.
Niekrasz, John and Johanna D. Moore. 2010. Un-
biased discourse segmentation evaluation. In
Proceedings of the IEEE Spoken Language
Technology Workshop, SLT 2010. IEEE 2010,
pages 43–48.
Oh, Hyo-Jung, Sung Hyon Myaeng, and Myung-
Gil Jang. 2007. Semantic passage segmentation
based on sentence topics for question answer-
ing. Information Sciences 177(18):3696–3717.
Passonneau, Rebecca J. and Diane J. Litman.
1993. Intention-based segmentation: human
reliability and correlation with linguistic cues.
In Proceedings of the 31st Annual Meeting
of the Association for Computational Linguis-
tics. Association for Computational Linguistics,
Stroudsburg, PA, USA, pages 148–155.
Pevzner, Lev and Marti A. Hearst. 2002. A cri-
tique and improvement of an evaluation metric
for text segmentation. Computational Linguis-
tics 28:19–36.
Reynar, Jeffrey C. and Adwait Ratnaparkhi. 1997.
A maximum entropy approach to identifying
sentence boundaries. In Proceedings of the
5th Conference on Applied Natural Language
Processing. Association for Computational Lin-
guistics, Stroudsburg, PA, USA, pages 16–19.
Scott, William A. 1955. Reliability of content
analysis: The case of nominal scale coding.
Public Opinion Quarterly 19:321–325.
Siegel, Sidney and N. J. Castellan. 1988. Non-
parametric Statistics for the Behavioral Sci-
ences, McGraw-Hill, New York, USA, chapter
9.8. 2nd edition.
Sirts, Kairit and Tanel Alum¨ae. 2012. A Hierar-
chical Dirichlet Process Model for Joint Part-of-
Speech and Morphology Induction. In Proceed-
ings of Human Language Technologies: The
2012 Annual Conference of the North American
Chapter of the Association for Computational
Linguistics. Association for Computational Lin-
guistics, Stroudsburg, PA, USA, pages 407–
416.
Stoyanov, Veselin and Claire Cardie. 2008. Topic
identification for fine-grained opinion analysis.
In Proceedings of the 22nd International Con-
ference on Computational Linguistics. Associ-
ation for Computational Linguistics, Strouds-
burg, PA, USA, pages 817–824.
</reference>
<page confidence="0.825042">
1712
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.702089">
<title confidence="0.999683">Evaluating Text Segmentation using Boundary Edit Distance</title>
<author confidence="0.97809">Chris</author>
<affiliation confidence="0.998739">University of</affiliation>
<address confidence="0.749249">Ottawa, ON,</address>
<email confidence="0.953858">cfour037@eecs.uottawa.ca</email>
<abstract confidence="0.999445722222222">This work proposes a new segmentation metric, named simian inter-coder agreement coefficient adaptation, and a confusion-matrix for segmentation that are all based upon an adaptation of the boundary edit distance in Fournier and Inkpen (2012). Existing segmetrics such as WindowDiff, and Segmentation Similarity (S) are all able to award partial credit for near misses between boundaries, but are biased towards segmentations containing few or tightly clustered boundaries. Despite S’s improvements, its normalization also produces cosmetically high values that overestimate agreement &amp; performance, leading this work to propose a solution.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Intercoder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="4728" citStr="Artstein and Poesio, 2008" startWordPosition="707" endWordPosition="710">ercoder agreement; and §6 compares B, S, and WD while evaluating automatic segmenters. 2 Related Work 2.1 Segmentation Evaluation Many early studies evaluated automatic segmenters using information retrieval (IR) metrics such as precision, recall, etc. These metrics looked at segmentation as a binary classification problem and were very harsh in their comparisons—no credit was awarded for nearly missing a boundary. Near misses occur frequently in segmentation— although manual coders often agree upon the bulk of where segment lie, they frequently disagree upon the exact position of boundaries (Artstein and Poesio, 2008, p. 40). To attempt to overcome this issue, both Passonneau and Litman (1993) and Hearst (1993) conflated multiple manual segmentations into one that contained only those boundaries which the majority of coders agreed upon. IR metrics were then used to compare automatic segmenters to this majority solution. Such a majority solution is unsuitable, however, because it does not contain actual subtopic breaks, but instead the conflation of a collection of potentially disagreeing solutions. Additionally, the definition of what constitutes a majority is subjective (e.g., Passonneau and Litman (1993</context>
<context position="11266" citStr="Artstein and Poesio (2008)" startWordPosition="1791" endWordPosition="1795"> to measure whether a group of human judges (i.e. coders) agree with each other greater than chance. Such coefficients are used to determine the reliability and replicability of the coding scheme and instructions used to collect manual codings (Carletta, 1996). Although direct interpretation of such coefficients is difficult, they are an invaluable tool when comparing segmentation data that has been collected with differing labels and when estimating the replicability of a study. A variety of intercoder agreement coefficients exist, but this work focuses upon a selection of those discussed by Artstein and Poesio (2008), specifically: Scott’s π (Scott, 1955) Fleiss’ multi-π (π*, Fleiss 1971)7, Cohen’s κ (Cohen, 1960), and multi-κ (κ*, Davies and Fleiss 1982). Their general forms are shown in Equation 3, where Aa represents actual agreement, and Ae expected (i.e., chance) agreement between coders. Aa − Ae κ, π, κ*, and π* = (3) 1 − Ae When calculating agreement between manual segmenters, boundaries are considered labels and their positions the decisions. Unfortunately, because of the frequency of near misses that occur in segmentation, using such labels and decisions causes inter-coder agreement coefficients </context>
<context position="19867" citStr="Artstein and Poesio (2008)" startWordPosition="3264" endWordPosition="3267">near misses by scaling edits operations. |BM,a |+ ws ord(Sa,p e , Tb) +wt span(Ta,p e , nt) if a = p ws ord(Sa,p e , Tb) (7) +wt span(Ta,p e , nt) ifa =6 p |Ae,a |if p = 0 |Ae,p |ifa = 0 An example confusion matrix is shown in Figure 2 from which IR metrics such as precision, recall, and Fa-measure can be computed (referred to as B-precision, B-recall, etc.). Actual B Non-B B CM(1, 1) CM(0, 1) Non-B CM(1, 0) CM(0, 0) Figure 2: Example confusion matrix (Tb = {1}) 3.4 B-Based Inter-coder Agreement Fournier and Inkpen (2012, p. 156–157) adapted four inter-coder agreement formulations provided by Artstein and Poesio (2008) to use S to award partial credit for near misses, but because S produces cosmetically high agreement values they grossly overestimate agreement. To solve this issue, this work instead proposes using microaverage B (i.e., mean boundary pair correctness over all documents and codings compared) to solve this issue (demonstrated in §5) because it does not over-estimate actual agreement (demonstrated in §4 and 5). 4 Discussion of Segmentation Metrics Before analysing how each metric compares to each other upon a large data set, it would be useful to investigate how they act on a smaller scale. To </context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Artstein, Ron and Massimo Poesio. 2008. Intercoder agreement for computational linguistics. Computational Linguistics 34(4):555–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Baker</author>
</authors>
<title>Stargazers look for life.</title>
<date>1990</date>
<journal>South Magazine</journal>
<pages>117--76</pages>
<contexts>
<context position="26950" citStr="Baker, 1990" startWordPosition="4528" endWordPosition="4529">segmentations Figure 9: Artificial data sets illustrating how 7r adapted to use either S or B reacts to increasing full misses and random segmentations and varying numbers of coders tion, it makes sense to analyse some larger data sets. Two such data sets are The Stargazer data set collected by Hearst (1997) and The Moonstone data set collected by Kazantseva and Szpakowicz (2012). Both are linear topical segmentations at the paragraph level with only one boundary type, but that is where their similarities end. The Stargazer text is a science magazine article titled “Stargazers look for life” (Baker, 1990) segmented by 7 coders and was one of twelve articles chosen for its length (between 1,800 and 2,500 words) and for having little structural demarcation. “The Moonstone” is a 19th century romance novel by Collins (1868) segmented by 4–6 coders per chapter; of its 23 chapters, 2 were coded in a pilot study and another 20 were coded individually by 27 undergraduate English students in 5 groups. For the Stargazer data set, using S-based 7r*, an inter-coder agreement coefficient of 0.7562 is obtained—a reasonable level by content analysis standards. Unfortunately, this value is highly inflated, an</context>
</contexts>
<marker>Baker, 1990</marker>
<rawString>Baker, David. 1990. Stargazers look for life. South Magazine 117:76–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<journal>Machine Learning</journal>
<pages>34--177</pages>
<contexts>
<context position="2502" citStr="Beeferman and Berger, 1999" startWordPosition="372" endWordPosition="376">between act and scene breaks in a play. Because of its value to natural language processing, various text segmentation tasks have been automated such as topical segmentation— for which a variety of automatic segmenters exist (e.g., Hearst 1997, Malioutov and Barzilay 2006, Eisenstein and Barzilay 2008, and Kazantseva and Szpakowicz 2011). This work addresses how to best select an automatic segmenter and which segmentation metrics are most appropriate to do so. To select an automatic segmenter for a particular task, a variety of segmentation evaluation metrics have been proposed, including Pk (Beeferman and Berger, 1999, pp. 198–200), WindowDiff (WD; Pevzner and Hearst 2002, p. 10), and most recently Segmentation Similarity (S; Fournier and Inkpen 2012, p. 154–156). Each of these metrics have a variety of flaws: Pk and WindowDiff both under-penalize errors at the beginning of segmentations (Lamprier et al., 2007) and have a bias towards favouring segmentations with few or tightly-clustered boundaries (Niekrasz and Moore, 2010), while S produces overly optimistic values due to its normalization (shown later). To overcome the flaws of existing text segmentation metrics, this work proposes a new series of metri</context>
<context position="5596" citStr="Beeferman and Berger (1999" startWordPosition="842" endWordPosition="845">hen used to compare automatic segmenters to this majority solution. Such a majority solution is unsuitable, however, because it does not contain actual subtopic breaks, but instead the conflation of a collection of potentially disagreeing solutions. Additionally, the definition of what constitutes a majority is subjective (e.g., Passonneau and Litman (1993, p. 150), Litman and Passonneau (1995), Hearst (1993, p. 6) each used 4/7, 3/7, and &gt; 50%, respectively). To address the issue of awarding partial credit for an automatic segmenter nearly missing a boundary—without conflating segmentations, Beeferman and Berger (1999, pp. 198–200) proposed a new metric named Pk. Pevzner and Hearst (2002, pp. 3–4) explain Pk well: a window of size k—where k is half of the mean manual segmentation length—is slid across both automatic and manual segmentations. A penalty is awarded if the window’s edges are found to be in differing or the same segments within the manual segmentation and the automatic segmentation disagrees. Pk is the sum of these penalties over all windows. Measuring the proportion of windows in error allows Pk to penalize a fully missed boundary by k windows, whereas a nearly missed boundary is penalized by </context>
</contexts>
<marker>Beeferman, Berger, 1999</marker>
<rawString>Beeferman, Doug and Adam Berger. 1999. Statistical models for text segmentation. Machine Learning 34:177–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing Agreement on Classification Tasks: The Kappa Statistic.</title>
<date>1996</date>
<journal>Computational Linguistics</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="10900" citStr="Carletta, 1996" startWordPosition="1736" endWordPosition="1738">vour very sparse segmentations with few boundaries; iii) S produces cosmetically high values, making it difficult to interpret and causing over-estimation of inter-coder agreement. In this work, these deficiencies are demonstrated and a new set of metrics are proposed as replacements. 2.2 Inter-Coder Agreement Inter-coder agreement coefficients are used to measure whether a group of human judges (i.e. coders) agree with each other greater than chance. Such coefficients are used to determine the reliability and replicability of the coding scheme and instructions used to collect manual codings (Carletta, 1996). Although direct interpretation of such coefficients is difficult, they are an invaluable tool when comparing segmentation data that has been collected with differing labels and when estimating the replicability of a study. A variety of intercoder agreement coefficients exist, but this work focuses upon a selection of those discussed by Artstein and Poesio (2008), specifically: Scott’s π (Scott, 1955) Fleiss’ multi-π (π*, Fleiss 1971)7, Cohen’s κ (Cohen, 1960), and multi-κ (κ*, Davies and Fleiss 1982). Their general forms are shown in Equation 3, where Aa represents actual agreement, and Ae e</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Carletta, Jean. 1996. Assessing Agreement on Classification Tasks: The Kappa Statistic. Computational Linguistics 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation. Association for Computational Linguistics,</booktitle>
<pages>224--232</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="1455" citStr="Chang et al. 2008" startWordPosition="212" endWordPosition="215">ntroduction Text segmentation is the task of splitting text into segments by placing boundaries within it. Segmentation is performed for a variety of purposes and is often a pre-processing step in a larger task. E.g., text can be topically segmented to aid video and audio retrieval (Franz et al., 2007), question answering (Oh et al., 2007), subjectivity analysis (Stoyanov and Cardie, 2008), and even summarization (Haghighi and Vanderwende, 2009). A variety of segmentation granularities, or atomic units, exist, including segmentations at the morpheme (e.g., Sirts and Alum¨ae 2012), word (e.g., Chang et al. 2008), sentence (e.g., Reynar and Ratnaparkhi 1997), and paragraph (e.g., Hearst 1997) levels. Between each atomic unit lies the potential to place a boundary. Segmentations can also represent the structure of text as being organized linearly (e.g., Hearst 1997), hierarchically (e.g., Eisenstein 2009), etc. Theoretically, segmentations could also contain varying boundary types, e.g., two boundary types could differentiate between act and scene breaks in a play. Because of its value to natural language processing, various text segmentation tasks have been automated such as topical segmentation— for </context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>Chang, Pi-Chuan, Michel Galley, and Christopher D. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proceedings of the Third Workshop on Statistical Machine Translation. Association for Computational Linguistics, Stroudsburg, PA, USA, pages 224–232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A Coefficient of Agreement for Nominal Scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement</booktitle>
<pages>20--37</pages>
<contexts>
<context position="11365" citStr="Cohen, 1960" startWordPosition="1808" endWordPosition="1809">ients are used to determine the reliability and replicability of the coding scheme and instructions used to collect manual codings (Carletta, 1996). Although direct interpretation of such coefficients is difficult, they are an invaluable tool when comparing segmentation data that has been collected with differing labels and when estimating the replicability of a study. A variety of intercoder agreement coefficients exist, but this work focuses upon a selection of those discussed by Artstein and Poesio (2008), specifically: Scott’s π (Scott, 1955) Fleiss’ multi-π (π*, Fleiss 1971)7, Cohen’s κ (Cohen, 1960), and multi-κ (κ*, Davies and Fleiss 1982). Their general forms are shown in Equation 3, where Aa represents actual agreement, and Ae expected (i.e., chance) agreement between coders. Aa − Ae κ, π, κ*, and π* = (3) 1 − Ae When calculating agreement between manual segmenters, boundaries are considered labels and their positions the decisions. Unfortunately, because of the frequency of near misses that occur in segmentation, using such labels and decisions causes inter-coder agreement coefficients to drastically underestimate actual agreement— much like how automatic segmenter performance is und</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Cohen, Jacob. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement 20:37–46.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Christabel</author>
</authors>
<title>Kubla Khan, and the Pains of Sleep.</title>
<publisher>John Murray.</publisher>
<marker>Christabel, </marker>
<rawString>Coleridge, Samuel Taylor. 1816. Christabel, Kubla Khan, and the Pains of Sleep. John Murray.</rawString>
</citation>
<citation valid="false">
<title>The Moonstone. Tinsley Brothers.</title>
<marker></marker>
<rawString>Collins, Wilkie. 1868. The Moonstone. Tinsley Brothers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Davies</author>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring agreement for multinomial data.</title>
<date>1982</date>
<journal>Biometrics</journal>
<pages>38--1047</pages>
<contexts>
<context position="11407" citStr="Davies and Fleiss 1982" startWordPosition="1813" endWordPosition="1816"> reliability and replicability of the coding scheme and instructions used to collect manual codings (Carletta, 1996). Although direct interpretation of such coefficients is difficult, they are an invaluable tool when comparing segmentation data that has been collected with differing labels and when estimating the replicability of a study. A variety of intercoder agreement coefficients exist, but this work focuses upon a selection of those discussed by Artstein and Poesio (2008), specifically: Scott’s π (Scott, 1955) Fleiss’ multi-π (π*, Fleiss 1971)7, Cohen’s κ (Cohen, 1960), and multi-κ (κ*, Davies and Fleiss 1982). Their general forms are shown in Equation 3, where Aa represents actual agreement, and Ae expected (i.e., chance) agreement between coders. Aa − Ae κ, π, κ*, and π* = (3) 1 − Ae When calculating agreement between manual segmenters, boundaries are considered labels and their positions the decisions. Unfortunately, because of the frequency of near misses that occur in segmentation, using such labels and decisions causes inter-coder agreement coefficients to drastically underestimate actual agreement— much like how automatic segmenter performance is underestimated when segmentation is treated a</context>
</contexts>
<marker>Davies, Fleiss, 1982</marker>
<rawString>Davies, Mark and Joseph L. Fleiss. 1982. Measuring agreement for multinomial data. Biometrics 38:1047–1051.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
</authors>
<title>Hierarchical text segmentation from multi-scale lexical cohesion.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>353--361</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="1752" citStr="Eisenstein 2009" startWordPosition="258" endWordPosition="259">007), question answering (Oh et al., 2007), subjectivity analysis (Stoyanov and Cardie, 2008), and even summarization (Haghighi and Vanderwende, 2009). A variety of segmentation granularities, or atomic units, exist, including segmentations at the morpheme (e.g., Sirts and Alum¨ae 2012), word (e.g., Chang et al. 2008), sentence (e.g., Reynar and Ratnaparkhi 1997), and paragraph (e.g., Hearst 1997) levels. Between each atomic unit lies the potential to place a boundary. Segmentations can also represent the structure of text as being organized linearly (e.g., Hearst 1997), hierarchically (e.g., Eisenstein 2009), etc. Theoretically, segmentations could also contain varying boundary types, e.g., two boundary types could differentiate between act and scene breaks in a play. Because of its value to natural language processing, various text segmentation tasks have been automated such as topical segmentation— for which a variety of automatic segmenters exist (e.g., Hearst 1997, Malioutov and Barzilay 2006, Eisenstein and Barzilay 2008, and Kazantseva and Szpakowicz 2011). This work addresses how to best select an automatic segmenter and which segmentation metrics are most appropriate to do so. To select a</context>
</contexts>
<marker>Eisenstein, 2009</marker>
<rawString>Eisenstein, Jacob. 2009. Hierarchical text segmentation from multi-scale lexical cohesion. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, Stroudsburg, PA, USA, pages 353–361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Bayesian unsupervised topic segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,</booktitle>
<pages>334--343</pages>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="2178" citStr="Eisenstein and Barzilay 2008" startWordPosition="320" endWordPosition="323">en each atomic unit lies the potential to place a boundary. Segmentations can also represent the structure of text as being organized linearly (e.g., Hearst 1997), hierarchically (e.g., Eisenstein 2009), etc. Theoretically, segmentations could also contain varying boundary types, e.g., two boundary types could differentiate between act and scene breaks in a play. Because of its value to natural language processing, various text segmentation tasks have been automated such as topical segmentation— for which a variety of automatic segmenters exist (e.g., Hearst 1997, Malioutov and Barzilay 2006, Eisenstein and Barzilay 2008, and Kazantseva and Szpakowicz 2011). This work addresses how to best select an automatic segmenter and which segmentation metrics are most appropriate to do so. To select an automatic segmenter for a particular task, a variety of segmentation evaluation metrics have been proposed, including Pk (Beeferman and Berger, 1999, pp. 198–200), WindowDiff (WD; Pevzner and Hearst 2002, p. 10), and most recently Segmentation Similarity (S; Fournier and Inkpen 2012, p. 154–156). Each of these metrics have a variety of flaws: Pk and WindowDiff both under-penalize errors at the beginning of segmentations </context>
<context position="31876" citStr="Eisenstein and Barzilay, 2008" startWordPosition="5367" endWordPosition="5370">ically over-estimate agreement, as does S itself in pairwise mean form. B-based coefficients, however, properly discriminate between levels of agreement regardless of the number of coders and do not over-estimate. 6 Evaluation of Automatic Segmenters Having looked at how S, WD, and B perform at a small scale in §4 and on larger data set in §5, this section demonstrates the use of these metrics to evaluate some automatic segmenters. Three automatic segmenters were trained—or had their parameters estimated upon—The Moonstone data set, including MinCut; (Malioutov and Barzilay, 2006), BayesSeg; (Eisenstein and Barzilay, 2008), and APS (Kazantseva and Szpakowicz, 2011). To put this evaluation into context, an upper and lower bound were also created comprised of a random coder from the manual data (Human) and a random segmenter (Random), respectively. These automatic segmenters, and the upper and lower bounds, were created, trained, and run by another researcher (Anna Kazantseva) with their labels removed during the development of the metrics detailed herein (to improve the impartiality of these analyses). An ideal segmentation evaluation metric should, in theory, place the three automatic segmenters between the upp</context>
</contexts>
<marker>Eisenstein, Barzilay, 2008</marker>
<rawString>Eisenstein, Jacob and Regina Barzilay. 2008. Bayesian unsupervised topic segmentation. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Morristown, NJ, USA, pages 334–343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological Bulletin</journal>
<pages>76--378</pages>
<contexts>
<context position="11339" citStr="Fleiss 1971" startWordPosition="1804" endWordPosition="1805"> than chance. Such coefficients are used to determine the reliability and replicability of the coding scheme and instructions used to collect manual codings (Carletta, 1996). Although direct interpretation of such coefficients is difficult, they are an invaluable tool when comparing segmentation data that has been collected with differing labels and when estimating the replicability of a study. A variety of intercoder agreement coefficients exist, but this work focuses upon a selection of those discussed by Artstein and Poesio (2008), specifically: Scott’s π (Scott, 1955) Fleiss’ multi-π (π*, Fleiss 1971)7, Cohen’s κ (Cohen, 1960), and multi-κ (κ*, Davies and Fleiss 1982). Their general forms are shown in Equation 3, where Aa represents actual agreement, and Ae expected (i.e., chance) agreement between coders. Aa − Ae κ, π, κ*, and π* = (3) 1 − Ae When calculating agreement between manual segmenters, boundaries are considered labels and their positions the decisions. Unfortunately, because of the frequency of near misses that occur in segmentation, using such labels and decisions causes inter-coder agreement coefficients to drastically underestimate actual agreement— much like how automatic se</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Fleiss, Joseph L. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin 76:378–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Fournier</author>
<author>Diana Inkpen</author>
</authors>
<title>Segmentation Similarity and Agreement.</title>
<date>2012</date>
<booktitle>In Proceedings of Human Language Technologies: The 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>152--161</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="2637" citStr="Fournier and Inkpen 2012" startWordPosition="393" endWordPosition="396">tomated such as topical segmentation— for which a variety of automatic segmenters exist (e.g., Hearst 1997, Malioutov and Barzilay 2006, Eisenstein and Barzilay 2008, and Kazantseva and Szpakowicz 2011). This work addresses how to best select an automatic segmenter and which segmentation metrics are most appropriate to do so. To select an automatic segmenter for a particular task, a variety of segmentation evaluation metrics have been proposed, including Pk (Beeferman and Berger, 1999, pp. 198–200), WindowDiff (WD; Pevzner and Hearst 2002, p. 10), and most recently Segmentation Similarity (S; Fournier and Inkpen 2012, p. 154–156). Each of these metrics have a variety of flaws: Pk and WindowDiff both under-penalize errors at the beginning of segmentations (Lamprier et al., 2007) and have a bias towards favouring segmentations with few or tightly-clustered boundaries (Niekrasz and Moore, 2010), while S produces overly optimistic values due to its normalization (shown later). To overcome the flaws of existing text segmentation metrics, this work proposes a new series of metrics derived from an adaptation of boundary edit distance (Fournier and Inkpen, 2012, p. 154– 156). This new metric is named boundary sim</context>
<context position="7845" citStr="Fournier and Inkpen 2012" startWordPosition="1221" endWordPosition="1224">nternal segment size variance. N−k 1 YWD(M, A) = N − k i=1,j=i+k WD did not, however, solve all of the issues related to window-based segmentation comparison. WD, and inherently Pk: i) Penalize errors less at the beginning and end of segmentations (Lamprier et al., 2007); ii) Are biased towards favouring automatic segmentations with either few or tightly-clustered boundaries (Niekrasz and Moore, 2010); iii) Calculate window size k inconsistently;4 iv) Are not symmetric5 (meaning that they cannot be used to produce a pairwise mean of multiple manual segmentations6). Segmentation Similarity (S; Fournier and Inkpen 2012, pp. 154–156) took a different approach to comparing segmentations. Instead of using windows, the work proposes a new restricted edit distance called boundary edit distance which differentiates between full and near misses. S then 2I.e., a boundary present in the manual but not the automatic segmentation, and the reverse for a false positive. 3Georgescul et al. (2006, p. 48) noted that WD interprets a near miss as a FP probabilistically more than as a FN. 4k must be an integer, but half of a mean may be a fraction, thus rounding must be used, but no rounding method is specified. It is also no</context>
<context position="12610" citStr="Fournier and Inkpen (2012" startWordPosition="2000" endWordPosition="2003">mentation is treated as a binary classification problem. Hearst (1997, pp. 53–54) attempted to adapt π* to award partial credit for near misses by using the percentage agreement metric of Gale et al. (1992, p. 254) to compute actual agreement—which conflates multiple manual segmentations together according to whether a majority of coders agree upon a boundary or not. Unfortunately, such a method of computing agreement grossly inflates results, and “the statistic itself guarantees at least 50% agreement by only pairing off coders against the majority opinion” (Isard and Carletta, 1995, p. 63). Fournier and Inkpen (2012, pp. 154–156) proposed using pairwise mean S for actual agreement to allow inter-coder agreement coefficients to award partial credit for near misses. Unfortunately, because S produces cosmetically high values, it also causes inter-coder agreement coefficients to drastically overestimates actual agreement. This work demonstrates this deficiency and proposes and evaluates a solution. 3 A New Proposal for Edit-Based Text Segmentation Evaluation In this section, a new boundary edit distance based segmentation metric and confusion matrix is proposed to solve the deficiencies of S for both segment</context>
<context position="19767" citStr="Fournier and Inkpen (2012" startWordPosition="3251" endWordPosition="3254">r correctness so that information-retrieval metrics can be calculated that award partial credit to near misses by scaling edits operations. |BM,a |+ ws ord(Sa,p e , Tb) +wt span(Ta,p e , nt) if a = p ws ord(Sa,p e , Tb) (7) +wt span(Ta,p e , nt) ifa =6 p |Ae,a |if p = 0 |Ae,p |ifa = 0 An example confusion matrix is shown in Figure 2 from which IR metrics such as precision, recall, and Fa-measure can be computed (referred to as B-precision, B-recall, etc.). Actual B Non-B B CM(1, 1) CM(0, 1) Non-B CM(1, 0) CM(0, 0) Figure 2: Example confusion matrix (Tb = {1}) 3.4 B-Based Inter-coder Agreement Fournier and Inkpen (2012, p. 156–157) adapted four inter-coder agreement formulations provided by Artstein and Poesio (2008) to use S to award partial credit for near misses, but because S produces cosmetically high agreement values they grossly overestimate agreement. To solve this issue, this work instead proposes using microaverage B (i.e., mean boundary pair correctness over all documents and codings compared) to solve this issue (demonstrated in §5) because it does not over-estimate actual agreement (demonstrated in §4 and 5). 4 Discussion of Segmentation Metrics Before analysing how each metric compares to each</context>
</contexts>
<marker>Fournier, Inkpen, 2012</marker>
<rawString>Fournier, Chris and Diana Inkpen. 2012. Segmentation Similarity and Agreement. In Proceedings of Human Language Technologies: The 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, Stroudsburg, PA, USA, pages 152– 161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Fournier</author>
</authors>
<title>Evaluating Text Segmentation. Master’s thesis,</title>
<date>2013</date>
<institution>University of Ottawa.</institution>
<contexts>
<context position="16728" citStr="Fournier (2013" startWordPosition="2719" endWordPosition="2720">presents the greatest offset between boundary positions that could be considered a near miss and can B(s1,s2, =1− |Ae| span(Te, ws |Ae| |Te| |Se| |BM| |Ae| X + 0 + 0 −X + 0 + 0 + 0= 1 − X/X = 1 − 1 = |BM| 0 + 0 + 0B = 1 − 0 + 0 + 0 + X= 1 − 0/X = 1 − 0 = be used to scale s1 s2 2 3 T 4 3 M 4 6 AD AD 4 2 Figure 1: Boundary edit operations In Figure 1, the location of the errors is clearly shown. Importantly, however, pairs of boundaries between the segmentations can be seen that represent the decisions made, and the correctness of $A complete explanation of Boundary Edit Distance is detailed in Fournier (2013, Section 4.1.2). 9Also note that the ADs are close together, and if nt &gt; 2, then they would be considered a T, and not two ADs—this is one way to award partial cre Pair Corre ctness Match 1 Addition/deletion 0 Transposition 1 nt) Substitution 1 Tb) Table 1: Correctness of boundary −wtspan(Te, −wsord(Se, pair 3.2 Boundary Similarity The new boundary edit distance normalization proposed herein is referred to as boundary similarity (B). Assuming that boundary edit distance produces sets of edit operations where Ae is the set of additions/deletions, Te the set of n-wise transpositions, Se the set</context>
</contexts>
<marker>Fournier, 2013</marker>
<rawString>Fournier, Christopher. 2013. Evaluating Text Segmentation. Master’s thesis, University of Ottawa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Franz</author>
<author>J Scott McCarley</author>
<author>Jian-Ming Xu</author>
</authors>
<title>User-oriented text segmentation evaluation measure.</title>
<date>2007</date>
<booktitle>In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. Association for Computing Machinery,</booktitle>
<pages>701--702</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="1140" citStr="Franz et al., 2007" startWordPosition="167" endWordPosition="170"> to award partial credit for near misses between boundaries, but are biased towards segmentations containing few or tightly clustered boundaries. Despite S’s improvements, its normalization also produces cosmetically high values that overestimate agreement &amp; performance, leading this work to propose a solution. 1 Introduction Text segmentation is the task of splitting text into segments by placing boundaries within it. Segmentation is performed for a variety of purposes and is often a pre-processing step in a larger task. E.g., text can be topically segmented to aid video and audio retrieval (Franz et al., 2007), question answering (Oh et al., 2007), subjectivity analysis (Stoyanov and Cardie, 2008), and even summarization (Haghighi and Vanderwende, 2009). A variety of segmentation granularities, or atomic units, exist, including segmentations at the morpheme (e.g., Sirts and Alum¨ae 2012), word (e.g., Chang et al. 2008), sentence (e.g., Reynar and Ratnaparkhi 1997), and paragraph (e.g., Hearst 1997) levels. Between each atomic unit lies the potential to place a boundary. Segmentations can also represent the structure of text as being organized linearly (e.g., Hearst 1997), hierarchically (e.g., Eise</context>
</contexts>
<marker>Franz, McCarley, Xu, 2007</marker>
<rawString>Franz, Martin, J. Scott McCarley, and Jian-Ming Xu. 2007. User-oriented text segmentation evaluation measure. In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. Association for Computing Machinery, Stroudsburg, PA, USA, pages 701–702.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth Ward Church</author>
<author>David Yarowsky</author>
</authors>
<title>Estimating upper and lower bounds on the performance of word-sense disambiguation programs.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>249--256</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="12190" citStr="Gale et al. (1992" startWordPosition="1935" endWordPosition="1938"> = (3) 1 − Ae When calculating agreement between manual segmenters, boundaries are considered labels and their positions the decisions. Unfortunately, because of the frequency of near misses that occur in segmentation, using such labels and decisions causes inter-coder agreement coefficients to drastically underestimate actual agreement— much like how automatic segmenter performance is underestimated when segmentation is treated as a binary classification problem. Hearst (1997, pp. 53–54) attempted to adapt π* to award partial credit for near misses by using the percentage agreement metric of Gale et al. (1992, p. 254) to compute actual agreement—which conflates multiple manual segmentations together according to whether a majority of coders agree upon a boundary or not. Unfortunately, such a method of computing agreement grossly inflates results, and “the statistic itself guarantees at least 50% agreement by only pairing off coders against the majority opinion” (Isard and Carletta, 1995, p. 63). Fournier and Inkpen (2012, pp. 154–156) proposed using pairwise mean S for actual agreement to allow inter-coder agreement coefficients to award partial credit for near misses. Unfortunately, because S pro</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>Gale, William, Kenneth Ward Church, and David Yarowsky. 1992. Estimating upper and lower bounds on the performance of word-sense disambiguation programs. In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Stroudsburg, PA, USA, pages 249–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Georgescul</author>
<author>Alexander Clark</author>
<author>Susan Armstrong</author>
</authors>
<title>An analysis of quantitative aspects in the evaluation of thematic segmentation algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue. Association for Computational Linguistics,</booktitle>
<pages>144--151</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="8215" citStr="Georgescul et al. (2006" startWordPosition="1281" endWordPosition="1284">daries (Niekrasz and Moore, 2010); iii) Calculate window size k inconsistently;4 iv) Are not symmetric5 (meaning that they cannot be used to produce a pairwise mean of multiple manual segmentations6). Segmentation Similarity (S; Fournier and Inkpen 2012, pp. 154–156) took a different approach to comparing segmentations. Instead of using windows, the work proposes a new restricted edit distance called boundary edit distance which differentiates between full and near misses. S then 2I.e., a boundary present in the manual but not the automatic segmentation, and the reverse for a false positive. 3Georgescul et al. (2006, p. 48) noted that WD interprets a near miss as a FP probabilistically more than as a FN. 4k must be an integer, but half of a mean may be a fraction, thus rounding must be used, but no rounding method is specified. It is also not specified whether k should be set once during a study or recalculated for each comparison— this work assumes the latter. 5Window size is calculated only upon the manual segmentation, meaning that one must be a manual and other an automatic segmentation. 6This also means that WD and Pk cannot be adapted to compute inter-coder agreement coefficients. (|Mij − Aij |&gt; 0)</context>
</contexts>
<marker>Georgescul, Clark, Armstrong, 2006</marker>
<rawString>Georgescul, Maria, Alexander Clark, and Susan Armstrong. 2006. An analysis of quantitative aspects in the evaluation of thematic segmentation algorithms. In Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue. Association for Computational Linguistics, Stroudsburg, PA, USA, pages 144–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<journal>NAACL</journal>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<volume>09</volume>
<pages>362--370</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="1286" citStr="Haghighi and Vanderwende, 2009" startWordPosition="187" endWordPosition="190">boundaries. Despite S’s improvements, its normalization also produces cosmetically high values that overestimate agreement &amp; performance, leading this work to propose a solution. 1 Introduction Text segmentation is the task of splitting text into segments by placing boundaries within it. Segmentation is performed for a variety of purposes and is often a pre-processing step in a larger task. E.g., text can be topically segmented to aid video and audio retrieval (Franz et al., 2007), question answering (Oh et al., 2007), subjectivity analysis (Stoyanov and Cardie, 2008), and even summarization (Haghighi and Vanderwende, 2009). A variety of segmentation granularities, or atomic units, exist, including segmentations at the morpheme (e.g., Sirts and Alum¨ae 2012), word (e.g., Chang et al. 2008), sentence (e.g., Reynar and Ratnaparkhi 1997), and paragraph (e.g., Hearst 1997) levels. Between each atomic unit lies the potential to place a boundary. Segmentations can also represent the structure of text as being organized linearly (e.g., Hearst 1997), hierarchically (e.g., Eisenstein 2009), etc. Theoretically, segmentations could also contain varying boundary types, e.g., two boundary types could differentiate between ac</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>Haghighi, Aria and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, Stroudsburg, PA, USA, NAACL ’09, pages 362–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>TextTiling: A Quantitative Approach to Discourse.</title>
<date>1993</date>
<tech>Technical report,</tech>
<institution>University of California at Berkeley,</institution>
<location>Berkeley, CA, USA.</location>
<contexts>
<context position="4824" citStr="Hearst (1993)" startWordPosition="725" endWordPosition="726">gmentation Evaluation Many early studies evaluated automatic segmenters using information retrieval (IR) metrics such as precision, recall, etc. These metrics looked at segmentation as a binary classification problem and were very harsh in their comparisons—no credit was awarded for nearly missing a boundary. Near misses occur frequently in segmentation— although manual coders often agree upon the bulk of where segment lie, they frequently disagree upon the exact position of boundaries (Artstein and Poesio, 2008, p. 40). To attempt to overcome this issue, both Passonneau and Litman (1993) and Hearst (1993) conflated multiple manual segmentations into one that contained only those boundaries which the majority of coders agreed upon. IR metrics were then used to compare automatic segmenters to this majority solution. Such a majority solution is unsuitable, however, because it does not contain actual subtopic breaks, but instead the conflation of a collection of potentially disagreeing solutions. Additionally, the definition of what constitutes a majority is subjective (e.g., Passonneau and Litman (1993, p. 150), Litman and Passonneau (1995), Hearst (1993, p. 6) each used 4/7, 3/7, and &gt; 50%, resp</context>
</contexts>
<marker>Hearst, 1993</marker>
<rawString>Hearst, Marti A. 1993. TextTiling: A Quantitative Approach to Discourse. Technical report, University of California at Berkeley, Berkeley, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>TextTiling: Segmenting Text into Multi-paragraph Subtopic Passages.</title>
<date>1997</date>
<journal>Computational Linguistics</journal>
<pages>23--33</pages>
<contexts>
<context position="1536" citStr="Hearst 1997" startWordPosition="226" endWordPosition="227">undaries within it. Segmentation is performed for a variety of purposes and is often a pre-processing step in a larger task. E.g., text can be topically segmented to aid video and audio retrieval (Franz et al., 2007), question answering (Oh et al., 2007), subjectivity analysis (Stoyanov and Cardie, 2008), and even summarization (Haghighi and Vanderwende, 2009). A variety of segmentation granularities, or atomic units, exist, including segmentations at the morpheme (e.g., Sirts and Alum¨ae 2012), word (e.g., Chang et al. 2008), sentence (e.g., Reynar and Ratnaparkhi 1997), and paragraph (e.g., Hearst 1997) levels. Between each atomic unit lies the potential to place a boundary. Segmentations can also represent the structure of text as being organized linearly (e.g., Hearst 1997), hierarchically (e.g., Eisenstein 2009), etc. Theoretically, segmentations could also contain varying boundary types, e.g., two boundary types could differentiate between act and scene breaks in a play. Because of its value to natural language processing, various text segmentation tasks have been automated such as topical segmentation— for which a variety of automatic segmenters exist (e.g., Hearst 1997, Malioutov and B</context>
<context position="12054" citStr="Hearst (1997" startWordPosition="1912" endWordPosition="1913">Equation 3, where Aa represents actual agreement, and Ae expected (i.e., chance) agreement between coders. Aa − Ae κ, π, κ*, and π* = (3) 1 − Ae When calculating agreement between manual segmenters, boundaries are considered labels and their positions the decisions. Unfortunately, because of the frequency of near misses that occur in segmentation, using such labels and decisions causes inter-coder agreement coefficients to drastically underestimate actual agreement— much like how automatic segmenter performance is underestimated when segmentation is treated as a binary classification problem. Hearst (1997, pp. 53–54) attempted to adapt π* to award partial credit for near misses by using the percentage agreement metric of Gale et al. (1992, p. 254) to compute actual agreement—which conflates multiple manual segmentations together according to whether a majority of coders agree upon a boundary or not. Unfortunately, such a method of computing agreement grossly inflates results, and “the statistic itself guarantees at least 50% agreement by only pairing off coders against the majority opinion” (Isard and Carletta, 1995, p. 63). Fournier and Inkpen (2012, pp. 154–156) proposed using pairwise mean </context>
<context position="26647" citStr="Hearst (1997)" startWordPosition="4481" endWordPosition="4482">0.3 0.4 0.5 0.6 0.7 0.8 P(miss) while P(near) = 0.0921 0.6 0.4 0.2 2 3 4 5 6 7 8 9 10 Coders (quantity) 0.0 1.00 0.95 (a) S-based 7r* showing increasing full misses with constant near misses (b) B-based 7r* showing increasing full misses with constant near misses (c) S and B based 7r* with fully random segmentations Figure 9: Artificial data sets illustrating how 7r adapted to use either S or B reacts to increasing full misses and random segmentations and varying numbers of coders tion, it makes sense to analyse some larger data sets. Two such data sets are The Stargazer data set collected by Hearst (1997) and The Moonstone data set collected by Kazantseva and Szpakowicz (2012). Both are linear topical segmentations at the paragraph level with only one boundary type, but that is where their similarities end. The Stargazer text is a science magazine article titled “Stargazers look for life” (Baker, 1990) segmented by 7 coders and was one of twelve articles chosen for its length (between 1,800 and 2,500 words) and for having little structural demarcation. “The Moonstone” is a 19th century romance novel by Collins (1868) segmented by 4–6 coders per chapter; of its 23 chapters, 2 were coded in a pi</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Hearst, Marti A. 1997. TextTiling: Segmenting Text into Multi-paragraph Subtopic Passages. Computational Linguistics 23:33–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myles Hollander</author>
<author>Douglas A Wolfe</author>
</authors>
<date>1999</date>
<pages>1711</pages>
<contexts>
<context position="33997" citStr="Hollander and Wolfe, 1999" startWordPosition="5709" endWordPosition="5712">icant degree. Why would pairwise mean B act in such an unexpected manner? The answer lies in a further analysis using the confusion matrix proposed earlier to calculate B-precision and B-recall (as shown in Table 2). From the values in Table 2, all three automatic segmenters appear to have Bprecision above the baseline and below the upper bound, but the B-recall of both APS and MinCut is below that of the random baseline (illustrated 11Using Kruskal-Wallis rank sum multiple comparison tests (Siegel and Castellan, 1988, pp. 213-214) for S and WD and the Wilcoxon-Nemenyi-McDonald-Thompson test (Hollander and Wolfe, 1999, p. 295) for B. 1709 B n B-P B-R B-F1 TP FP FN TN Random 0.2640 f 0.0129 1057 0.3991 0.4673 0.4306 279.0 420 318 4236.0 Human 0.5285 f 0.0164 841 0.6854 0.7439 0.7135 444.5 204 153 4451.5 BayesSeg 0.3745 f 0.0146 964 0.5247 0.6224 0.5694 361.0 327 219 4346.0 APS 0.2873 f 0.0163 738 0.6773 0.3403 0.4530 212.0 101 411 4529.0 MinCut 0.2468 f 0.0141 871 0.4788 0.3496 0.4041 215.0 234 400 4404.0 Table 2: Mean performance of 5 segmenters using micro-average B, B-precision (B-P), B-recall (B-R), and B-Fβ-measure (B-Fl) along with the associated confusion matrix values for 5 segmenters Figure 12: Mea</context>
</contexts>
<marker>Hollander, Wolfe, 1999</marker>
<rawString>Hollander, Myles and Douglas A. Wolfe. 1999. 1711</rawString>
</citation>
<citation valid="false">
<title>Nonparametric Statistical Methods.</title>
<publisher>John Wiley &amp; Sons,</publisher>
<note>2nd edition.</note>
<marker></marker>
<rawString>Nonparametric Statistical Methods. John Wiley &amp; Sons, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amy Isard</author>
<author>Jean Carletta</author>
</authors>
<title>Replicability of transaction and action coding in the map task corpus.</title>
<date>1995</date>
<booktitle>In AAAI Spring Symposium: Empirical Methods in Discourse Interpretation and Generation.</booktitle>
<pages>60--66</pages>
<contexts>
<context position="12575" citStr="Isard and Carletta, 1995" startWordPosition="1994" endWordPosition="1997">ormance is underestimated when segmentation is treated as a binary classification problem. Hearst (1997, pp. 53–54) attempted to adapt π* to award partial credit for near misses by using the percentage agreement metric of Gale et al. (1992, p. 254) to compute actual agreement—which conflates multiple manual segmentations together according to whether a majority of coders agree upon a boundary or not. Unfortunately, such a method of computing agreement grossly inflates results, and “the statistic itself guarantees at least 50% agreement by only pairing off coders against the majority opinion” (Isard and Carletta, 1995, p. 63). Fournier and Inkpen (2012, pp. 154–156) proposed using pairwise mean S for actual agreement to allow inter-coder agreement coefficients to award partial credit for near misses. Unfortunately, because S produces cosmetically high values, it also causes inter-coder agreement coefficients to drastically overestimates actual agreement. This work demonstrates this deficiency and proposes and evaluates a solution. 3 A New Proposal for Edit-Based Text Segmentation Evaluation In this section, a new boundary edit distance based segmentation metric and confusion matrix is proposed to solve the</context>
</contexts>
<marker>Isard, Carletta, 1995</marker>
<rawString>Isard, Amy and Jean Carletta. 1995. Replicability of transaction and action coding in the map task corpus. In AAAI Spring Symposium: Empirical Methods in Discourse Interpretation and Generation. pages 60–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Kazantseva</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Linear Text Segmentation Using Affinity Propagation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,</booktitle>
<pages>284--293</pages>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="2215" citStr="Kazantseva and Szpakowicz 2011" startWordPosition="325" endWordPosition="328">tial to place a boundary. Segmentations can also represent the structure of text as being organized linearly (e.g., Hearst 1997), hierarchically (e.g., Eisenstein 2009), etc. Theoretically, segmentations could also contain varying boundary types, e.g., two boundary types could differentiate between act and scene breaks in a play. Because of its value to natural language processing, various text segmentation tasks have been automated such as topical segmentation— for which a variety of automatic segmenters exist (e.g., Hearst 1997, Malioutov and Barzilay 2006, Eisenstein and Barzilay 2008, and Kazantseva and Szpakowicz 2011). This work addresses how to best select an automatic segmenter and which segmentation metrics are most appropriate to do so. To select an automatic segmenter for a particular task, a variety of segmentation evaluation metrics have been proposed, including Pk (Beeferman and Berger, 1999, pp. 198–200), WindowDiff (WD; Pevzner and Hearst 2002, p. 10), and most recently Segmentation Similarity (S; Fournier and Inkpen 2012, p. 154–156). Each of these metrics have a variety of flaws: Pk and WindowDiff both under-penalize errors at the beginning of segmentations (Lamprier et al., 2007) and have a bi</context>
<context position="31919" citStr="Kazantseva and Szpakowicz, 2011" startWordPosition="5373" endWordPosition="5376"> itself in pairwise mean form. B-based coefficients, however, properly discriminate between levels of agreement regardless of the number of coders and do not over-estimate. 6 Evaluation of Automatic Segmenters Having looked at how S, WD, and B perform at a small scale in §4 and on larger data set in §5, this section demonstrates the use of these metrics to evaluate some automatic segmenters. Three automatic segmenters were trained—or had their parameters estimated upon—The Moonstone data set, including MinCut; (Malioutov and Barzilay, 2006), BayesSeg; (Eisenstein and Barzilay, 2008), and APS (Kazantseva and Szpakowicz, 2011). To put this evaluation into context, an upper and lower bound were also created comprised of a random coder from the manual data (Human) and a random segmenter (Random), respectively. These automatic segmenters, and the upper and lower bounds, were created, trained, and run by another researcher (Anna Kazantseva) with their labels removed during the development of the metrics detailed herein (to improve the impartiality of these analyses). An ideal segmentation evaluation metric should, in theory, place the three automatic segmenters between the upper and lower bounds in terms of performance</context>
</contexts>
<marker>Kazantseva, Szpakowicz, 2011</marker>
<rawString>Kazantseva, Anna and Stan Szpakowicz. 2011. Linear Text Segmentation Using Affinity Propagation. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Edinburgh, Scotland, UK., pages 284– 293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Kazantseva</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Topical Segmentation: a Study of Human Performance.</title>
<date>2012</date>
<booktitle>In Proceedings of Human Language Technologies: The 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>211--220</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="26720" citStr="Kazantseva and Szpakowicz (2012)" startWordPosition="4490" endWordPosition="4493"> 0.6 0.4 0.2 2 3 4 5 6 7 8 9 10 Coders (quantity) 0.0 1.00 0.95 (a) S-based 7r* showing increasing full misses with constant near misses (b) B-based 7r* showing increasing full misses with constant near misses (c) S and B based 7r* with fully random segmentations Figure 9: Artificial data sets illustrating how 7r adapted to use either S or B reacts to increasing full misses and random segmentations and varying numbers of coders tion, it makes sense to analyse some larger data sets. Two such data sets are The Stargazer data set collected by Hearst (1997) and The Moonstone data set collected by Kazantseva and Szpakowicz (2012). Both are linear topical segmentations at the paragraph level with only one boundary type, but that is where their similarities end. The Stargazer text is a science magazine article titled “Stargazers look for life” (Baker, 1990) segmented by 7 coders and was one of twelve articles chosen for its length (between 1,800 and 2,500 words) and for having little structural demarcation. “The Moonstone” is a 19th century romance novel by Collins (1868) segmented by 4–6 coders per chapter; of its 23 chapters, 2 were coded in a pilot study and another 20 were coded individually by 27 undergraduate Engl</context>
</contexts>
<marker>Kazantseva, Szpakowicz, 2012</marker>
<rawString>Kazantseva, Anna and Stan Szpakowicz. 2012. Topical Segmentation: a Study of Human Performance. In Proceedings of Human Language Technologies: The 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, Stroudsburg, PA, USA, pages 211–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Lamprier</author>
<author>Tassadit Amghar</author>
<author>Bernard Levrat</author>
<author>Frederic Saubion</author>
</authors>
<title>On evaluation methodologies for text segmentation algorithms.</title>
<date>2007</date>
<booktitle>In Proceedings of the 19th IEEE International Conference on Tools with Artificial Intelligence. IEEE Computer Society,</booktitle>
<volume>2</volume>
<pages>pages</pages>
<location>Washington, DC, USA,</location>
<contexts>
<context position="2801" citStr="Lamprier et al., 2007" startWordPosition="421" endWordPosition="424"> and Kazantseva and Szpakowicz 2011). This work addresses how to best select an automatic segmenter and which segmentation metrics are most appropriate to do so. To select an automatic segmenter for a particular task, a variety of segmentation evaluation metrics have been proposed, including Pk (Beeferman and Berger, 1999, pp. 198–200), WindowDiff (WD; Pevzner and Hearst 2002, p. 10), and most recently Segmentation Similarity (S; Fournier and Inkpen 2012, p. 154–156). Each of these metrics have a variety of flaws: Pk and WindowDiff both under-penalize errors at the beginning of segmentations (Lamprier et al., 2007) and have a bias towards favouring segmentations with few or tightly-clustered boundaries (Niekrasz and Moore, 2010), while S produces overly optimistic values due to its normalization (shown later). To overcome the flaws of existing text segmentation metrics, this work proposes a new series of metrics derived from an adaptation of boundary edit distance (Fournier and Inkpen, 2012, p. 154– 156). This new metric is named boundary similarity (B). A confusion matrix to interpret segmentation as a classification problem is also proposed, allowing for the computation of information retrieval (IR) m</context>
<context position="7492" citStr="Lamprier et al., 2007" startWordPosition="1170" endWordPosition="1173"> the automatic segmentation (AZj), then a penalty is given. The ratio of penalties over windows then represents the degree of error between the segmentations, as in Equation 1. This change better allowed WD to: i) penalize FPs and FNs more equally;3 ii) Not skip full misses; iii) Less harshly penalize near misses; and iv) Reduce its sensitivity to internal segment size variance. N−k 1 YWD(M, A) = N − k i=1,j=i+k WD did not, however, solve all of the issues related to window-based segmentation comparison. WD, and inherently Pk: i) Penalize errors less at the beginning and end of segmentations (Lamprier et al., 2007); ii) Are biased towards favouring automatic segmentations with either few or tightly-clustered boundaries (Niekrasz and Moore, 2010); iii) Calculate window size k inconsistently;4 iv) Are not symmetric5 (meaning that they cannot be used to produce a pairwise mean of multiple manual segmentations6). Segmentation Similarity (S; Fournier and Inkpen 2012, pp. 154–156) took a different approach to comparing segmentations. Instead of using windows, the work proposes a new restricted edit distance called boundary edit distance which differentiates between full and near misses. S then 2I.e., a bounda</context>
</contexts>
<marker>Lamprier, Amghar, Levrat, Saubion, 2007</marker>
<rawString>Lamprier, Sylvain, Tassadit Amghar, Bernard Levrat, and Frederic Saubion. 2007. On evaluation methodologies for text segmentation algorithms. In Proceedings of the 19th IEEE International Conference on Tools with Artificial Intelligence. IEEE Computer Society, Washington, DC, USA, volume 2, pages 19–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
<author>Rebecca J Passonneau</author>
</authors>
<title>Combining multiple knowledge sources for discourse segmentation.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>108--115</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="5367" citStr="Litman and Passonneau (1995)" startWordPosition="807" endWordPosition="811">o attempt to overcome this issue, both Passonneau and Litman (1993) and Hearst (1993) conflated multiple manual segmentations into one that contained only those boundaries which the majority of coders agreed upon. IR metrics were then used to compare automatic segmenters to this majority solution. Such a majority solution is unsuitable, however, because it does not contain actual subtopic breaks, but instead the conflation of a collection of potentially disagreeing solutions. Additionally, the definition of what constitutes a majority is subjective (e.g., Passonneau and Litman (1993, p. 150), Litman and Passonneau (1995), Hearst (1993, p. 6) each used 4/7, 3/7, and &gt; 50%, respectively). To address the issue of awarding partial credit for an automatic segmenter nearly missing a boundary—without conflating segmentations, Beeferman and Berger (1999, pp. 198–200) proposed a new metric named Pk. Pevzner and Hearst (2002, pp. 3–4) explain Pk well: a window of size k—where k is half of the mean manual segmentation length—is slid across both automatic and manual segmentations. A penalty is awarded if the window’s edges are found to be in differing or the same segments within the manual segmentation and the automatic </context>
</contexts>
<marker>Litman, Passonneau, 1995</marker>
<rawString>Litman, Diane J. and Rebecca J. Passonneau. 1995. Combining multiple knowledge sources for discourse segmentation. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Stroudsburg, PA, USA, pages 108–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Malioutov</author>
<author>Regina Barzilay</author>
</authors>
<title>Minimum cut model for spoken lecture segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>25--32</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="2148" citStr="Malioutov and Barzilay 2006" startWordPosition="316" endWordPosition="319">., Hearst 1997) levels. Between each atomic unit lies the potential to place a boundary. Segmentations can also represent the structure of text as being organized linearly (e.g., Hearst 1997), hierarchically (e.g., Eisenstein 2009), etc. Theoretically, segmentations could also contain varying boundary types, e.g., two boundary types could differentiate between act and scene breaks in a play. Because of its value to natural language processing, various text segmentation tasks have been automated such as topical segmentation— for which a variety of automatic segmenters exist (e.g., Hearst 1997, Malioutov and Barzilay 2006, Eisenstein and Barzilay 2008, and Kazantseva and Szpakowicz 2011). This work addresses how to best select an automatic segmenter and which segmentation metrics are most appropriate to do so. To select an automatic segmenter for a particular task, a variety of segmentation evaluation metrics have been proposed, including Pk (Beeferman and Berger, 1999, pp. 198–200), WindowDiff (WD; Pevzner and Hearst 2002, p. 10), and most recently Segmentation Similarity (S; Fournier and Inkpen 2012, p. 154–156). Each of these metrics have a variety of flaws: Pk and WindowDiff both under-penalize errors at t</context>
<context position="31833" citStr="Malioutov and Barzilay, 2006" startWordPosition="5362" endWordPosition="5365">d inter-coder agreement coefficients drastically over-estimate agreement, as does S itself in pairwise mean form. B-based coefficients, however, properly discriminate between levels of agreement regardless of the number of coders and do not over-estimate. 6 Evaluation of Automatic Segmenters Having looked at how S, WD, and B perform at a small scale in §4 and on larger data set in §5, this section demonstrates the use of these metrics to evaluate some automatic segmenters. Three automatic segmenters were trained—or had their parameters estimated upon—The Moonstone data set, including MinCut; (Malioutov and Barzilay, 2006), BayesSeg; (Eisenstein and Barzilay, 2008), and APS (Kazantseva and Szpakowicz, 2011). To put this evaluation into context, an upper and lower bound were also created comprised of a random coder from the manual data (Human) and a random segmenter (Random), respectively. These automatic segmenters, and the upper and lower bounds, were created, trained, and run by another researcher (Anna Kazantseva) with their labels removed during the development of the metrics detailed herein (to improve the impartiality of these analyses). An ideal segmentation evaluation metric should, in theory, place the</context>
</contexts>
<marker>Malioutov, Barzilay, 2006</marker>
<rawString>Malioutov, Igor and Regina Barzilay. 2006. Minimum cut model for spoken lecture segmentation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Stroudsburg, PA, USA, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Niekrasz</author>
<author>Johanna D Moore</author>
</authors>
<title>Unbiased discourse segmentation evaluation.</title>
<date>2010</date>
<booktitle>In Proceedings of the IEEE Spoken Language Technology Workshop, SLT 2010. IEEE</booktitle>
<pages>43--48</pages>
<contexts>
<context position="2917" citStr="Niekrasz and Moore, 2010" startWordPosition="437" endWordPosition="440">mentation metrics are most appropriate to do so. To select an automatic segmenter for a particular task, a variety of segmentation evaluation metrics have been proposed, including Pk (Beeferman and Berger, 1999, pp. 198–200), WindowDiff (WD; Pevzner and Hearst 2002, p. 10), and most recently Segmentation Similarity (S; Fournier and Inkpen 2012, p. 154–156). Each of these metrics have a variety of flaws: Pk and WindowDiff both under-penalize errors at the beginning of segmentations (Lamprier et al., 2007) and have a bias towards favouring segmentations with few or tightly-clustered boundaries (Niekrasz and Moore, 2010), while S produces overly optimistic values due to its normalization (shown later). To overcome the flaws of existing text segmentation metrics, this work proposes a new series of metrics derived from an adaptation of boundary edit distance (Fournier and Inkpen, 2012, p. 154– 156). This new metric is named boundary similarity (B). A confusion matrix to interpret segmentation as a classification problem is also proposed, allowing for the computation of information retrieval (IR) metrics such as precision and recall.1 In this work: §2 reviews existing segmentation metrics; §3 proposes an adaptat</context>
<context position="7625" citStr="Niekrasz and Moore, 2010" startWordPosition="1189" endWordPosition="1192">r between the segmentations, as in Equation 1. This change better allowed WD to: i) penalize FPs and FNs more equally;3 ii) Not skip full misses; iii) Less harshly penalize near misses; and iv) Reduce its sensitivity to internal segment size variance. N−k 1 YWD(M, A) = N − k i=1,j=i+k WD did not, however, solve all of the issues related to window-based segmentation comparison. WD, and inherently Pk: i) Penalize errors less at the beginning and end of segmentations (Lamprier et al., 2007); ii) Are biased towards favouring automatic segmentations with either few or tightly-clustered boundaries (Niekrasz and Moore, 2010); iii) Calculate window size k inconsistently;4 iv) Are not symmetric5 (meaning that they cannot be used to produce a pairwise mean of multiple manual segmentations6). Segmentation Similarity (S; Fournier and Inkpen 2012, pp. 154–156) took a different approach to comparing segmentations. Instead of using windows, the work proposes a new restricted edit distance called boundary edit distance which differentiates between full and near misses. S then 2I.e., a boundary present in the manual but not the automatic segmentation, and the reverse for a false positive. 3Georgescul et al. (2006, p. 48) n</context>
</contexts>
<marker>Niekrasz, Moore, 2010</marker>
<rawString>Niekrasz, John and Johanna D. Moore. 2010. Unbiased discourse segmentation evaluation. In Proceedings of the IEEE Spoken Language Technology Workshop, SLT 2010. IEEE 2010, pages 43–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hyo-Jung Oh</author>
<author>Sung Hyon Myaeng</author>
<author>MyungGil Jang</author>
</authors>
<title>Semantic passage segmentation based on sentence topics for question answering.</title>
<date>2007</date>
<journal>Information Sciences</journal>
<volume>177</volume>
<issue>18</issue>
<contexts>
<context position="1178" citStr="Oh et al., 2007" startWordPosition="173" endWordPosition="176">between boundaries, but are biased towards segmentations containing few or tightly clustered boundaries. Despite S’s improvements, its normalization also produces cosmetically high values that overestimate agreement &amp; performance, leading this work to propose a solution. 1 Introduction Text segmentation is the task of splitting text into segments by placing boundaries within it. Segmentation is performed for a variety of purposes and is often a pre-processing step in a larger task. E.g., text can be topically segmented to aid video and audio retrieval (Franz et al., 2007), question answering (Oh et al., 2007), subjectivity analysis (Stoyanov and Cardie, 2008), and even summarization (Haghighi and Vanderwende, 2009). A variety of segmentation granularities, or atomic units, exist, including segmentations at the morpheme (e.g., Sirts and Alum¨ae 2012), word (e.g., Chang et al. 2008), sentence (e.g., Reynar and Ratnaparkhi 1997), and paragraph (e.g., Hearst 1997) levels. Between each atomic unit lies the potential to place a boundary. Segmentations can also represent the structure of text as being organized linearly (e.g., Hearst 1997), hierarchically (e.g., Eisenstein 2009), etc. Theoretically, segm</context>
</contexts>
<marker>Oh, Myaeng, Jang, 2007</marker>
<rawString>Oh, Hyo-Jung, Sung Hyon Myaeng, and MyungGil Jang. 2007. Semantic passage segmentation based on sentence topics for question answering. Information Sciences 177(18):3696–3717.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Diane J Litman</author>
</authors>
<title>Intention-based segmentation: human reliability and correlation with linguistic cues.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>148--155</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="4806" citStr="Passonneau and Litman (1993)" startWordPosition="720" endWordPosition="723">segmenters. 2 Related Work 2.1 Segmentation Evaluation Many early studies evaluated automatic segmenters using information retrieval (IR) metrics such as precision, recall, etc. These metrics looked at segmentation as a binary classification problem and were very harsh in their comparisons—no credit was awarded for nearly missing a boundary. Near misses occur frequently in segmentation— although manual coders often agree upon the bulk of where segment lie, they frequently disagree upon the exact position of boundaries (Artstein and Poesio, 2008, p. 40). To attempt to overcome this issue, both Passonneau and Litman (1993) and Hearst (1993) conflated multiple manual segmentations into one that contained only those boundaries which the majority of coders agreed upon. IR metrics were then used to compare automatic segmenters to this majority solution. Such a majority solution is unsuitable, however, because it does not contain actual subtopic breaks, but instead the conflation of a collection of potentially disagreeing solutions. Additionally, the definition of what constitutes a majority is subjective (e.g., Passonneau and Litman (1993, p. 150), Litman and Passonneau (1995), Hearst (1993, p. 6) each used 4/7, 3/</context>
</contexts>
<marker>Passonneau, Litman, 1993</marker>
<rawString>Passonneau, Rebecca J. and Diane J. Litman. 1993. Intention-based segmentation: human reliability and correlation with linguistic cues. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Stroudsburg, PA, USA, pages 148–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Pevzner</author>
<author>Marti A Hearst</author>
</authors>
<title>A critique and improvement of an evaluation metric for text segmentation.</title>
<date>2002</date>
<journal>Computational Linguistics</journal>
<pages>28--19</pages>
<contexts>
<context position="2557" citStr="Pevzner and Hearst 2002" startWordPosition="381" endWordPosition="384">ue to natural language processing, various text segmentation tasks have been automated such as topical segmentation— for which a variety of automatic segmenters exist (e.g., Hearst 1997, Malioutov and Barzilay 2006, Eisenstein and Barzilay 2008, and Kazantseva and Szpakowicz 2011). This work addresses how to best select an automatic segmenter and which segmentation metrics are most appropriate to do so. To select an automatic segmenter for a particular task, a variety of segmentation evaluation metrics have been proposed, including Pk (Beeferman and Berger, 1999, pp. 198–200), WindowDiff (WD; Pevzner and Hearst 2002, p. 10), and most recently Segmentation Similarity (S; Fournier and Inkpen 2012, p. 154–156). Each of these metrics have a variety of flaws: Pk and WindowDiff both under-penalize errors at the beginning of segmentations (Lamprier et al., 2007) and have a bias towards favouring segmentations with few or tightly-clustered boundaries (Niekrasz and Moore, 2010), while S produces overly optimistic values due to its normalization (shown later). To overcome the flaws of existing text segmentation metrics, this work proposes a new series of metrics derived from an adaptation of boundary edit distance</context>
<context position="5667" citStr="Pevzner and Hearst (2002" startWordPosition="855" endWordPosition="858">a majority solution is unsuitable, however, because it does not contain actual subtopic breaks, but instead the conflation of a collection of potentially disagreeing solutions. Additionally, the definition of what constitutes a majority is subjective (e.g., Passonneau and Litman (1993, p. 150), Litman and Passonneau (1995), Hearst (1993, p. 6) each used 4/7, 3/7, and &gt; 50%, respectively). To address the issue of awarding partial credit for an automatic segmenter nearly missing a boundary—without conflating segmentations, Beeferman and Berger (1999, pp. 198–200) proposed a new metric named Pk. Pevzner and Hearst (2002, pp. 3–4) explain Pk well: a window of size k—where k is half of the mean manual segmentation length—is slid across both automatic and manual segmentations. A penalty is awarded if the window’s edges are found to be in differing or the same segments within the manual segmentation and the automatic segmentation disagrees. Pk is the sum of these penalties over all windows. Measuring the proportion of windows in error allows Pk to penalize a fully missed boundary by k windows, whereas a nearly missed boundary is penalized by the distance that it is offset. Pk was not without issue, however. Pevz</context>
</contexts>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>Pevzner, Lev and Marti A. Hearst. 2002. A critique and improvement of an evaluation metric for text segmentation. Computational Linguistics 28:19–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy approach to identifying sentence boundaries.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing. Association for Computational Linguistics,</booktitle>
<pages>16--19</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="1501" citStr="Reynar and Ratnaparkhi 1997" startWordPosition="218" endWordPosition="222"> task of splitting text into segments by placing boundaries within it. Segmentation is performed for a variety of purposes and is often a pre-processing step in a larger task. E.g., text can be topically segmented to aid video and audio retrieval (Franz et al., 2007), question answering (Oh et al., 2007), subjectivity analysis (Stoyanov and Cardie, 2008), and even summarization (Haghighi and Vanderwende, 2009). A variety of segmentation granularities, or atomic units, exist, including segmentations at the morpheme (e.g., Sirts and Alum¨ae 2012), word (e.g., Chang et al. 2008), sentence (e.g., Reynar and Ratnaparkhi 1997), and paragraph (e.g., Hearst 1997) levels. Between each atomic unit lies the potential to place a boundary. Segmentations can also represent the structure of text as being organized linearly (e.g., Hearst 1997), hierarchically (e.g., Eisenstein 2009), etc. Theoretically, segmentations could also contain varying boundary types, e.g., two boundary types could differentiate between act and scene breaks in a play. Because of its value to natural language processing, various text segmentation tasks have been automated such as topical segmentation— for which a variety of automatic segmenters exist </context>
</contexts>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>Reynar, Jeffrey C. and Adwait Ratnaparkhi. 1997. A maximum entropy approach to identifying sentence boundaries. In Proceedings of the 5th Conference on Applied Natural Language Processing. Association for Computational Linguistics, Stroudsburg, PA, USA, pages 16–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Scott</author>
</authors>
<title>Reliability of content analysis: The case of nominal scale coding. Public Opinion Quarterly 19:321–325.</title>
<date>1955</date>
<contexts>
<context position="11305" citStr="Scott, 1955" startWordPosition="1799" endWordPosition="1800">ers) agree with each other greater than chance. Such coefficients are used to determine the reliability and replicability of the coding scheme and instructions used to collect manual codings (Carletta, 1996). Although direct interpretation of such coefficients is difficult, they are an invaluable tool when comparing segmentation data that has been collected with differing labels and when estimating the replicability of a study. A variety of intercoder agreement coefficients exist, but this work focuses upon a selection of those discussed by Artstein and Poesio (2008), specifically: Scott’s π (Scott, 1955) Fleiss’ multi-π (π*, Fleiss 1971)7, Cohen’s κ (Cohen, 1960), and multi-κ (κ*, Davies and Fleiss 1982). Their general forms are shown in Equation 3, where Aa represents actual agreement, and Ae expected (i.e., chance) agreement between coders. Aa − Ae κ, π, κ*, and π* = (3) 1 − Ae When calculating agreement between manual segmenters, boundaries are considered labels and their positions the decisions. Unfortunately, because of the frequency of near misses that occur in segmentation, using such labels and decisions causes inter-coder agreement coefficients to drastically underestimate actual agr</context>
</contexts>
<marker>Scott, 1955</marker>
<rawString>Scott, William A. 1955. Reliability of content analysis: The case of nominal scale coding. Public Opinion Quarterly 19:321–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sidney Siegel</author>
<author>N J Castellan</author>
</authors>
<title>Nonparametric Statistics for the Behavioral Sciences,</title>
<date>1988</date>
<location>McGraw-Hill, New York, USA,</location>
<note>chapter 9.8. 2nd edition.</note>
<contexts>
<context position="13310" citStr="Siegel and Castellan, 1988" startWordPosition="2105" endWordPosition="2108">ow inter-coder agreement coefficients to award partial credit for near misses. Unfortunately, because S produces cosmetically high values, it also causes inter-coder agreement coefficients to drastically overestimates actual agreement. This work demonstrates this deficiency and proposes and evaluates a solution. 3 A New Proposal for Edit-Based Text Segmentation Evaluation In this section, a new boundary edit distance based segmentation metric and confusion matrix is proposed to solve the deficiencies of S for both segmentation comparison and inter-coder agreement. 7Sometimes referred to as K (Siegel and Castellan, 1988). 1704 3.1 Boundary Edit Distance In this section, Boundary Edit Distance (BED; as proposed in Fournier and Inkpen 2012, pp. 154– 156) is introduced in more detail, and a few terminological and conceptual changes are made. Boundary Edit Distance uses three main edit operations to model segmentation differences: • Additions/deletions (AD; referred to originally as substitutions) for full misses; • Substitutions (S; not shown for brevity) for confusing one boundary type with another; • n-wise transpositions (T) for near misses. These edit operations are symmetric and operate upon the set of boun</context>
<context position="33895" citStr="Siegel and Castellan, 1988" startWordPosition="5695" endWordPosition="5698">gnificantly below the random baseline with only BayesSeg between the upper and lower bounds—to a significant degree. Why would pairwise mean B act in such an unexpected manner? The answer lies in a further analysis using the confusion matrix proposed earlier to calculate B-precision and B-recall (as shown in Table 2). From the values in Table 2, all three automatic segmenters appear to have Bprecision above the baseline and below the upper bound, but the B-recall of both APS and MinCut is below that of the random baseline (illustrated 11Using Kruskal-Wallis rank sum multiple comparison tests (Siegel and Castellan, 1988, pp. 213-214) for S and WD and the Wilcoxon-Nemenyi-McDonald-Thompson test (Hollander and Wolfe, 1999, p. 295) for B. 1709 B n B-P B-R B-F1 TP FP FN TN Random 0.2640 f 0.0129 1057 0.3991 0.4673 0.4306 279.0 420 318 4236.0 Human 0.5285 f 0.0164 841 0.6854 0.7439 0.7135 444.5 204 153 4451.5 BayesSeg 0.3745 f 0.0146 964 0.5247 0.6224 0.5694 361.0 327 219 4346.0 APS 0.2873 f 0.0163 738 0.6773 0.3403 0.4530 212.0 101 411 4529.0 MinCut 0.2468 f 0.0141 871 0.4788 0.3496 0.4041 215.0 234 400 4404.0 Table 2: Mean performance of 5 segmenters using micro-average B, B-precision (B-P), B-recall (B-R), and</context>
</contexts>
<marker>Siegel, Castellan, 1988</marker>
<rawString>Siegel, Sidney and N. J. Castellan. 1988. Nonparametric Statistics for the Behavioral Sciences, McGraw-Hill, New York, USA, chapter 9.8. 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kairit Sirts</author>
<author>Tanel Alum¨ae</author>
</authors>
<title>A Hierarchical Dirichlet Process Model for Joint Part-ofSpeech and Morphology Induction.</title>
<date>2012</date>
<booktitle>In Proceedings of Human Language Technologies: The 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>407--416</pages>
<location>Stroudsburg, PA, USA,</location>
<marker>Sirts, Alum¨ae, 2012</marker>
<rawString>Sirts, Kairit and Tanel Alum¨ae. 2012. A Hierarchical Dirichlet Process Model for Joint Part-ofSpeech and Morphology Induction. In Proceedings of Human Language Technologies: The 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, Stroudsburg, PA, USA, pages 407– 416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Claire Cardie</author>
</authors>
<title>Topic identification for fine-grained opinion analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>817--824</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="1229" citStr="Stoyanov and Cardie, 2008" startWordPosition="179" endWordPosition="182">s segmentations containing few or tightly clustered boundaries. Despite S’s improvements, its normalization also produces cosmetically high values that overestimate agreement &amp; performance, leading this work to propose a solution. 1 Introduction Text segmentation is the task of splitting text into segments by placing boundaries within it. Segmentation is performed for a variety of purposes and is often a pre-processing step in a larger task. E.g., text can be topically segmented to aid video and audio retrieval (Franz et al., 2007), question answering (Oh et al., 2007), subjectivity analysis (Stoyanov and Cardie, 2008), and even summarization (Haghighi and Vanderwende, 2009). A variety of segmentation granularities, or atomic units, exist, including segmentations at the morpheme (e.g., Sirts and Alum¨ae 2012), word (e.g., Chang et al. 2008), sentence (e.g., Reynar and Ratnaparkhi 1997), and paragraph (e.g., Hearst 1997) levels. Between each atomic unit lies the potential to place a boundary. Segmentations can also represent the structure of text as being organized linearly (e.g., Hearst 1997), hierarchically (e.g., Eisenstein 2009), etc. Theoretically, segmentations could also contain varying boundary types</context>
</contexts>
<marker>Stoyanov, Cardie, 2008</marker>
<rawString>Stoyanov, Veselin and Claire Cardie. 2008. Topic identification for fine-grained opinion analysis. In Proceedings of the 22nd International Conference on Computational Linguistics. Association for Computational Linguistics, Stroudsburg, PA, USA, pages 817–824.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>