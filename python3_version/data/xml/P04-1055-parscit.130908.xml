<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009266">
<title confidence="0.998523">
Classifying Semantic Relations
in Bioscience Texts
</title>
<author confidence="0.72793">
Barbara Rosario
</author>
<note confidence="0.593463">
SIMS
</note>
<address confidence="0.9085095">
UC Berkeley
Berkeley, CA 94720
</address>
<email confidence="0.998654">
rosario@sims.berkeley.edu
</email>
<note confidence="0.944113">
Marti A. Hearst
SIMS
</note>
<address confidence="0.888348">
UC Berkeley
Berkeley, CA 94720
</address>
<email confidence="0.998196">
hearst@sims.berkeley.edu
</email>
<sectionHeader confidence="0.995616" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999880866666667">
A crucial step toward the goal of au-
tomatic extraction of propositional in-
formation from natural language text is
the identification of semantic relations
between constituents in sentences. We
examine the problem of distinguishing
among seven relation types that can oc-
cur between the entities “treatment” and
“disease” in bioscience text, and the
problem of identifying such entities. We
compare five generative graphical mod-
els and a neural network, using lexical,
syntactic, and semantic features, finding
that the latter help achieve high classifi-
cation accuracy.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999513">
The biosciences literature is rich, complex and
continually growing. The National Library of
Medicine’s MEDLINE database1 contains bibli-
ographic citations and abstracts from more than
4,600 biomedical journals, and an estimated half a
million new articles are added every year. Much
of the important, late-breaking bioscience infor-
mation is found only in textual form, and so meth-
ods are needed to automatically extract semantic
entities and the relations between them from this
text. For example, in the following sentences, hep-
atitis and its variants, which are DISEASES, are
found in different semantic relationships with var-
ious TREATMENTs:
</bodyText>
<footnote confidence="0.953146">
1http://www.nlm.nih.gov/pubs/factsheets/medline.html
</footnote>
<listItem confidence="0.9865595">
(1) Effect of interferon on hepatitis B
(2) A two-dose combined hepatitis A and B vac-
cine would facilitate immunization programs
(3) These results suggest that con A-induced hep-
atitis was ameliorated by pretreatment with
TJ-135.
</listItem>
<bodyText confidence="0.999891785714286">
In (1) there is an unspecified effect of the treat-
ment interferon on hepatitis B. In (2) the vaccine
prevents hepatitis A and B while in (3) hepatitis
is cured by the treatment TJ-135.
We refer to this problem as Relation Classifi-
cation. A related task is Role Extraction (also
called, in the literature, “information extraction”
or “named entity recognition”), defined as: given
a sentence such as “The fluoroquinolones for uri-
nary tract infections: a review”, extract all and
only the strings of text that correspond to the roles
TREATMENT (fluoroquinolones) and DISEASE
(urinary tract infections). To make inferences
about the facts in the text we need a system that
accomplishes both these tasks: the extraction of
the semantic roles and the recognition of the rela-
tionship that holds between them.
In this paper we compare five generative graph-
ical models and a discriminative model (a multi-
layer neural network) on these tasks. Recogniz-
ing subtle differences among relations is a diffi-
cult task; nevertheless the results achieved by our
models are quite promising: when the roles are not
given, the neural network achieves 79.6% accu-
racy and the best graphical model achieves 74.9%.
When the roles are given, the neural net reaches
96.9% accuracy while the best graphical model
gets 91.6% accuracy. Part of the reason for the
</bodyText>
<table confidence="0.997512259259259">
Relationship Definition and Example
Cure TREAT cures DIS
810 (648, 162) Intravenous immune globulin for
recurrent spontaneous abortion
Only DIS TREAT not mentioned
616 (492, 124) Social ties and susceptibility to the
common cold
Only TREAT DIS not mentioned
166 (132, 34) Flucticasone propionate is safe in
recommended doses
Prevent TREAT prevents the DIS
63 (50, 13) Statins for prevention of stroke
Vague Very unclear relationship
36 (28, 8) Phenylbutazone and leukemia
Side Effect DIS is a result of a TREAT
29 (24, 5) Malignant mesodermal mixed tu-
mor of the uterus following irradi-
ation
NO Cure TREAT does not cure DIS
4 (3, 1) Evidence for double resistance to
permethrin and malathion in head
lice
Total relevant: 1724 (1377, 347)
Irrelevant TREAT and DIS not present
1771 (1416, 355) Patients were followed up for 6
months
Total: 3495 (2793, 702)
</table>
<tableCaption confidence="0.99921">
Table 1: Candidate semantic relationships be-
</tableCaption>
<bodyText confidence="0.986417666666667">
tween treatments and diseases. In parentheses are
shown the numbers of sentences used for training
and testing, respectively.
success of the algorithms is the use of a large
domain-specific lexical hierarchy for generaliza-
tion across classes of nouns.
In the remainder of this paper we discuss related
work, describe the annotated dataset, describe the
models, present and discuss the results of running
the models on the relation classification and en-
tity extraction tasks and analyze the relative im-
portance of the features used.
</bodyText>
<sectionHeader confidence="0.999647" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999971962025317">
While there is much work on role extraction, very
little work has been done for relationship recogni-
tion. Moreover, many papers that claim to be do-
ing relationship recognition in reality address the
task of role extraction: (usually two) entities are
extracted and the relationship is implied by the co-
occurrence of these entities or by the presence of
some linguistic expression. These linguistic pat-
terns could in principle distinguish between differ-
ent relations, but instead are usually used to iden-
tify examples of one relation. In the related work
for statistical models there has been, to the best of
our knowledge, no attempt to distinguish between
different relations that can occur between the same
semantic entities.
In Agichtein and Gravano (2000) the goal is to
extract pairs such as (Microsoft, Redmond), where
Redmond is the location of the organization Mi-
crosoft. Their technique generates and evaluates
lexical patterns that are indicative of the relation.
Only the relation location of is tackled and the en-
tities are assumed given.
In Zelenko et al. (2002), the task is to ex-
tract the relationships person-affiliation and
organization-location. The classification (done
with Support Vector Machine and Voted Percep-
tron algorithms) is between positive and negative
sentences, where the positive sentences contain
the two entities.
In the bioscience NLP literature there are
also efforts to extract entities and relations. In
Ray and Craven (2001), Hidden Markov Models
are applied to MEDLINE text to extract the enti-
ties PROTEINS and LOCATIONS in the relation-
ship subcellular-location and the entities GENE
and DISORDER in the relationship disorder-
association. The authors acknowledge that the
task of extracting relations is different from the
task of extracting entities. Nevertheless, they con-
sider positive examples to be all the sentences
that simply contain the entities, rather than an-
alyzing which relations hold between these enti-
ties. In Craven (1999), the problem tackled is re-
lationship extraction from MEDLINE for the re-
lation subcellular-location. The authors treat it
as a text classification problem and propose and
compare two classifiers: a Naive Bayes classi-
fier and a relational learning algorithm. This
is a two-way classification, and again there is
no mention of whether the co-occurrence of the
entities actually represents the target relation.
Pustejovsky et al. (2002) use a rule-based system
to extract entities in the inhibit-relation. Their ex-
periments use sentences that contain verbal and
nominal forms of the stem inhibit. Thus the ac-
tual task performed is the extraction of entities
that are connected by some form of the stem in-
hibit, which by requiring occurrence of this word
explicitly, is not the same as finding all sen-
tences that talk about inhibiting actions. Similarly,
Rindflesch et al. (1999) identify noun phrases sur-
rounding forms of the stem bind which signify
entities that can enter into molecular binding re-
lationships. In Srinivasan and Rindflesch (2002)
MeSH term co-occurrences within MEDLINE ar-
ticles are used to attempt to infer relationships be-
tween different concepts, including diseases and
drugs.
In the bioscience domain the work on relation
classification is primary done through hand-built
rules. Feldman et al. (2002) use hand-built rules
that make use of syntactic and lexical features
and semantic constraints to find relations between
genes, proteins, drugs and diseases. The GENIES
system (Friedman et al., 2001) uses a hand-built
semantic grammar along with hand-derived syn-
tactic and semantic constraints, and recognizes
a wide range of relationships between biological
molecules.
</bodyText>
<sectionHeader confidence="0.968971" genericHeader="method">
3 Data and Features
</sectionHeader>
<bodyText confidence="0.99975895">
For our experiments, the text was obtained from
MEDLINE 20012. An annotator with biology ex-
pertise considered the titles and abstracts sepa-
rately and labeled the sentences (both roles and
relations) based solely on the content of the indi-
vidual sentences. Seven possible types of relation-
ships between TREATMENT and DISEASE were
identified. Table 1 shows, for each relation, its def-
inition, one example sentence and the number of
sentences found containing it.
We used a large domain-specific lexical hi-
erarchy (MeSH, Medical Subject Headings3) to
map words into semantic categories. There are
about 19,000 unique terms in MeSH and 15 main
sub-hierarchies, each corresponding to a major
branch of medical ontology; e.g., tree A corre-
sponds to Anatomy, tree C to Disease, and so on.
As an example, the word migraine maps to the
term C10.228, that is, C (a disease), C10 (Ner-
vous System Diseases), C10.228 (Central Ner-
</bodyText>
<footnote confidence="0.83443675">
2We used the first 100 titles and the first 40 abstracts from
each of the 59 files medline01n*.xml in Medline 2001; the
labeled data is available at biotext.berkeley.edu
3http://www.nlm.nih.gov/mesh/meshhome.html
</footnote>
<bodyText confidence="0.9978165">
vous System Diseases). When there are multi-
ple MeSH terms for one word, we simply choose
the first one. These semantic features are shown
to be very useful for our tasks (see Section 4.3).
Rosario et al. (2002) demonstrate the usefulness
of MeSH for the classification of the semantic re-
lationships between nouns in noun compounds.
The results reported in this paper were obtained
with the following features: the word itself, its part
of speech from the Brill tagger (Brill, 1995), the
phrase constituent the word belongs to, obtained
by flattening the output of a parser (Collins, 1996),
and the word’s MeSH ID (if available). In addi-
tion, we identified the sub-hierarchies of MeSH
that tend to correspond to treatments and diseases,
and convert these into a tri-valued attribute indi-
cating one of: disease, treatment or neither. Fi-
nally, we included orthographic features such as
‘is the word a number’, ‘only part of the word is a
number’, ‘first letter is capitalized’, ‘all letters are
capitalized’. In Section 4.3 we analyze the impact
of these features.
</bodyText>
<sectionHeader confidence="0.966704" genericHeader="evaluation">
4 Models and Results
</sectionHeader>
<bodyText confidence="0.9999918">
This section describes the models and their perfor-
mance on both entity extraction and relation clas-
sification. Generative models learn the prior prob-
ability of the class and the probability of the fea-
tures given the class; they are the natural choice
in cases with hidden variables (partially observed
or missing data). Since labeled data is expensive
to collect, these models may be useful when no
labels are available. However, in this paper we
test the generative models on fully observed data
and show that, although not as accurate as the dis-
criminative model, their performance is promising
enough to encourage their use for the case of par-
tially observed data.
Discriminative models learn the probability of
the class given the features. When we have fully
observed data and we just need to learn the map-
ping from features to classes (classification), a dis-
criminative approach may be more appropriate,
as shown in Ng and Jordan (2002), but has other
shortcomings as discussed below.
For the evaluation of the role extraction task, we
calculate the usual metrics of precision, recall and
F-measure. Precision is a measure of how many of
the roles extracted by the system are correct and
recall is the measure of how many of the true roles
were extracted by the system. The F-measure is
a weighted combination of precision and recall4.
Our role evaluation is very strict: every token is as-
sessed and we do not assign partial credit for con-
stituents for which only some of the words are cor-
rectly labeled. We report results for two cases: (i)
considering only the relevant sentences and (ii) in-
cluding also irrelevant sentences. For the relation
classification task, we report results in terms of
classification accuracy, choosing one out of seven
choices for (i) and one out of eight choices for (ii).
(Most papers report the results for only the rele-
vant sentences, while some papers assign credit to
their algorithms if their system extracts only one
instance of a given relation from the collection. By
contrast, in our experiments we expect the system
to extract all instances of every relation type.) For
both tasks, 75% of the data were used for training
and the rest for testing.
</bodyText>
<subsectionHeader confidence="0.746214">
4.1 Generative Models
</subsectionHeader>
<bodyText confidence="0.99968245">
In Figure 1 we show two static and three dynamic
models. The nodes labeled “Role” represent the
entities (in this case the choices are DISEASE,
TREATMENT and NULL) and the node labeled
“Relation” represents the relationship present in
the sentence. We assume here that there is a single
relation for each sentence between the entities5.
The children of the role nodes are the words and
their features, thus there are as many role states as
there are words in the sentence; for the static mod-
els, this is depicted by the box (or “plate”) which
is the standard graphical model notation for repli-
cation. For each state, the features are those
mentioned in Section 3.
The simpler static models S1 and S2 do not
assume an ordering in the role sequence. The
dynamic models were inspired by prior work on
HMM-like graphical models for role extraction
(Bikel et al., 1999; Freitag and McCallum, 2000;
Ray and Craven, 2001). These models consist of a
</bodyText>
<footnote confidence="0.8486165">
4In this paper, precision and recall are given equal weight,
that is, F-measure = .
5We found 75 sentences which contain more than one re-
lationship, often with multiple entities or the same entities
taking part in several interconnected relationships; we did not
include these in the study.
</footnote>
<figure confidence="0.720721333333333">
dynamic model (D1)
dynamic model (D2)
dynamic model (D3)
</figure>
<figureCaption confidence="0.999751">
Figure 1: Models for role and relation extraction.
</figureCaption>
<bodyText confidence="0.999353923076923">
Markov sequence of states (usually corresponding
to semantic roles) where each state generates one
or multiple observations. Model D1 in Figure 1 is
typical of these models, but we have augmented it
with the Relation node.
The task is to recover the sequence of Role
states, given the observed features. These mod-
els assume that there is an ordering in the seman-
tic roles that can be captured with the Markov as-
sumption and that the role generates the observa-
tions (the words, for example). All our models
make the additional assumption that there is a re-
lation that generates the role sequence; thus, these
</bodyText>
<figure confidence="0.991208583333333">
static model (S1) static model (S2)
f�1�
fab . . .❑ fa fa
Rao
fab . . .❑ fa fa
Relatio
ono
R&amp;quot;o
f�2� . . .�f�n�
Rao
ffb
fat . . .❑ fa
Relatio
ono
Rao
To✚
fm
fam . . .❑ fa
Relatio
ono
Rao
To✛
fib fam . . .❑ fa fib
Rao Rao
fab . . .❑ fa fib
RelatiE.
ono
fab . . .❑ fa
Rao
f1b fam . . .❑ fa f1b
Rao Rao
fab . . .❑ fa f1b
Relatio
ono
fab . . .❑ fa
Rao
</figure>
<table confidence="0.99278">
Sentences Static Dynamic
S1 S2 D1 D2 D3
No Smoothing
Only rel. 0.67 0.68 0.71 0.52 0.55
Rel. + irrel. 0.61 0.62 0.66 0.35 0.37
Absolute discounting
Only rel. 0.67 0.68 0.72 0.73 0.73
Rel. + irrel. 0.60 0.62 0.67 0.71 0.69
</table>
<tableCaption confidence="0.9364765">
Table 2: F-measures for the models of Figure 1 for
role extraction.
</tableCaption>
<bodyText confidence="0.997776878378378">
models have the appealing property that they can
simultaneously perform role extraction and rela-
tionship recognition, given the sequence of obser-
vations. In S1 and D1 the observations are inde-
pendent from the relation (given the roles). In S2
and D2, the observations are dependent on both
the relation and the role (or in other words, the re-
lation generates not only the sequence of roles but
also the observations). D2 encodes the fact that
even when the roles are given, the observations de-
pend on the relation. For example, sentences con-
taining the word prevent are more likely to repre-
sent a “prevent” kind of relationship. Finally, in
D3 only one observation per state is dependent on
both the relation and the role, the motivation being
that some observations (such as the words) depend
on the relation while others might not (like for ex-
ample, the parts of speech). In the experiments
reported here, the observations which have edges
from both the role and the relation nodes are the
words. (We ran an experiment in which this obser-
vation node was the MeSH term, obtaining similar
results.)
Model D1 defines the following joint probabil-
ity distribution over relations, roles, words and
word features, assuming the leftmost Role node is
, and is the number of words in the sen-
tence:
Model D1 is similar to the model
in Thompson et al. (2003) for the extraction
of roles, using a different domain. Structurally,
the differences are (i) Thompson et al. (2003) has
only one observation node per role and (ii) it has
an additional node “on top”, with an edge to the
relation node, to represent a predicator “trigger
word” which is always observed; the predicator
words are taken from a fixed list and one must be
present in order for a sentence to be analyzed.
The joint probability distributions for D2
and D3 are similar to Equation (1) where
we substitute the term
with for D2 and
for D3.
The parameters and
of Equation (1) are constrained to be equal.
The parameters were estimated using maximum
likelihood on the training set; we also imple-
mented a simple absolute discounting smoothing
method (Zhai and Lafferty, 2001) that improves
the results for both tasks.
Table 2 shows the results (F-measures) for the
problem of finding the most likely sequence of
roles given the features observed. In this case, the
relation is hidden and we marginalize over it6. We
experimented with different values for the smooth-
ing factor ranging from a minimum of 0.0000005
to a maximum of 10; the results shown fix the
smoothing factor at its minimum value. We found
that for the dynamic models, for a wide range
of smoothing factors, we achieved almost identi-
cal results; nevertheless, in future work, we plan
to implement cross-validation to find the optimal
smoothing factor. By contrast, the static models
were more sensitive to the value of the smoothing
factor.
Using maximum likelihood with no smoothing,
model D1 performs better than D2 and D3. This
was expected, since the parameters for models D2
and D3 are more sparse than D1. However, when
smoothing is applied, the three dynamic models
achieve similar results. Although the additional
edges in models D2 and D3 did not help much
for the task of role extraction, they did help for
relation classification, discussed next. Model D2
</bodyText>
<footnote confidence="0.98389725">
6To perform inference for the dynamic model, we
used the junction tree algorithm. We used Kevin Mur-
phy’s BNT package, found at http://www.ai.mit.edu/ mur-
phyk/Bayes/bnintro.html.
</footnote>
<equation confidence="0.732272">
(1)
</equation>
<bodyText confidence="0.999948303571428">
achieves the best F-measures: 0.73 for “only rele-
vant” and 0.71 for “rel. + irrel.”.
It is difficult to compare results with the related
work since the data, the semantic roles and the
evaluation are different; in Ray and Craven (2001)
however, the role extraction task is quite similar to
ours and the text is also from MEDLINE. They re-
port approximately an F-measure of 32% for the
extraction of the entities PROTEINS and LOCA-
TIONS, and an F-measure of 50% for GENE and
DISORDER.
The second target task is to find the most likely
relation, i.e., to classify a sentence into one of the
possible relations. Two types of experiments were
conducted. In the first, the true roles are hidden
and we classify the relations given only the ob-
servable features, marginalizing over the hidden
roles. In the second, the roles are given and only
the relations need to be inferred. Table 3 reports
the results for both conditions, both with absolute
discounting smoothing and without.
Again model D1 outperforms the other dy-
namic models when no smoothing is applied; with
smoothing and when the true roles are hidden, D2
achieves the best classification accuracies. When
the roles are given D1 is the best model; D1 does
well in the cases when both roles are not present.
By contrast, D2 does better than D1 when the pres-
ence of specific words strongly determines the out-
come (e.g., the presence “prevention” or “prevent”
helps identify the Prevent relation).
The percentage improvements of D2 and D3
versus D1 are, respectively, 10% and 6.5% for re-
lation classification and 1.4% for role extraction
(in the “only relevant”, “only features” case). This
suggests that there is a dependency between the
observations and the relation that is captured by
the additional edges in D2 and D3, but that this
dependency is more helpful in relation classifica-
tion than in role extraction.
For relation classification the static models per-
form worse than for role extraction; the decreases
in performance from D1 to S1 and from D2 to S2
are, respectively (in the “only relevant”, “only fea-
tures” case), 7.4% and 7.3% for role extraction and
27.1% and 44% for relation classification. This
suggests the importance of modeling the sequence
of roles for relation classification.
To provide an idea of where the errors occur,
Table 4 shows the confusion matrix for model D2
for the most realistic and difficult case of “rel + ir-
rel.”, “only features”. This indicates that the algo-
rithm performs poorly primarily for the cases for
which there is little training data, with the excep-
tion of the ONLY DISEASE case, which is often
mistaken for CURE.
</bodyText>
<subsectionHeader confidence="0.974972">
4.2 Neural Network
</subsectionHeader>
<bodyText confidence="0.999752052631579">
To compare the results of the generative models of
the previous section with a discriminative method,
we use a neural network, using the Matlab pack-
age to train a feed-forward network with conjugate
gradient descent.
The features are the same as those used for the
models in Section 4.1, but are represented with in-
dicator variables. That is, for each feature we cal-
culated the number of possible values and then
represented an observation of the feature as a se-
quence of binary values in which one value is set
to and the remaining values are set to .
The input layer of the NN is the concatenation
of this representation for all features. The net-
work has one hidden layer, with a hyperbolic tan-
gent function. The output layer uses a logistic sig-
moid function. The number of units of the output
layer is fixed to be the number of relations (seven
or eight) for the relation classification task and
the number of roles (three) for the role extraction
task. The network was trained for several choices
of numbers of hidden units; we chose the best-
performing networks based on training set error.
We then tested these networks on held-out testing
data.
The results for the neural network are reported
in Table 3 in the column labeled NN. These re-
sults are quite strong, achieving 79.6% accuracy
in the relation classification task when the entities
are hidden and 96.9% when the entities are given,
outperforming the graphical models. Two possible
reasons for this are: as already mentioned, the dis-
criminative approach may be the most appropriate
for fully labeled data; or the graphical models we
proposed may not be the right ones, i.e., the inde-
pendence assumptions they make may misrepre-
sent underlying dependencies.
It must be pointed out that the neural network
</bodyText>
<table confidence="0.99947125">
Sentences Input B Static Dynamic NN
S1 S2 D1 D2 D3
No Smoothing
Only rel. only feat. 46.7 51.9 50.4 65.4 58.2 61.4 79.8
roles given 51.3 52.9 66.6 43.8 49.3 92.5
Rel. + irrel. only feat. 50.6 51.2 50.2 68.9 58.7 61.4 79.6
roles given 55.7 54.4 82.3 55.2 58.8 96.6
Absolute discounting
Only rel. only feat. 46.7 51.9 50.4 66.0 72.6 70.3
roles given 51.9 53.6 83.0 76.6 76.6
Rel. + irrel. only feat. 50.6 51.1 50.2 68.9 74.9 74.6
roles given 56.1 54.8 91.6 82.0 82.3
</table>
<tableCaption confidence="0.998403">
Table 3: Accuracies of relationship classification for the models in Figure 1 and for the neural network
</tableCaption>
<bodyText confidence="0.970881363636364">
(NN). For absolute discounting, the smoothing factor was fixed at the minimum value. B is the baseline
of always choosing the most frequent relation. The best results are indicated in boldface.
is much slower than the graphical models, and re-
quires a great deal of memory; we were not able to
run the neural network package on our machines
for the role extraction task, when the feature vec-
tors are very large. The graphical models can
perform both tasks simultaneously; the percent-
age decrease in relation classification of model D2
with respect to the NN is of 8.9% for “only rele-
vant” and 5.8% for “relevant + irrelevant”.
</bodyText>
<subsectionHeader confidence="0.904716">
4.3 Features
</subsectionHeader>
<bodyText confidence="0.999989767441861">
In order to analyze the relative importance of the
different features, we performed both tasks using
the dynamic model D1 of Figure 1, leaving out
single features and sets of features (grouping all of
the features related to the MeSH hierarchy, mean-
ing both the classification of words into MeSH
IDs and the domain knowledge as defined in Sec-
tion 3). The results reported here were found with
maximum likelihood (no smoothing) and are for
the “relevant only” case; results for “relevant + ir-
relevant” were similar.
For the role extraction task, the most impor-
tant feature was the word: not using it, the
GM achieved only 0.65 F-measure (a decrease of
9.7% from 0.72 F-measure using all the features).
Leaving out the features related to MeSH the F-
measure obtained was 0.69% (a 4.1% decrease)
and the next most important feature was the part-
of-speech (0.70 F-measure not using this feature).
For all the other features, the F-measure ranged
between 0.71 and 0.73.
For the task of relation classification, the
MeSH-based features seem to be the most im-
portant. Leaving out the word again lead to the
biggest decrease in the classification accuracy for
a single feature but not so dramatically as in the
role extraction task (62.2% accuracy, for a de-
crease of 4% from the original value), but leaving
out all the MeSH features caused the accuracy to
decrease the most (a decrease of 13.2% for 56.2%
accuracy). For both tasks, the impact of the do-
main knowledge alone was negligible.
As described in Section 3, words can be mapped
to different levels of the MeSH hierarchy. Cur-
rently, we use the “second” level, so that, for ex-
ample, surgery is mapped to G02.403 (when the
whole MeSH ID is G02.403.810.762). This is
somewhat arbitrary (and mainly chosen with the
sparsity issue in mind), but in light of the impor-
tance of the MeSH features it may be worthwhile
investigating the issue of finding the optimal level
of description. (This can be seen as another form
of smoothing.)
</bodyText>
<sectionHeader confidence="0.999634" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999209">
We have addressed the problem of distinguishing
between several different relations that can hold
between two semantic entities, a difficult and im-
portant task in natural language understanding.
We have presented five graphical models and a
neural network for the tasks of semantic relation
classification and role extraction from bioscience
text. The methods proposed yield quite promis-
ing results. We also discussed the strengths and
weaknesses of the discriminative and generative
</bodyText>
<table confidence="0.999888272727273">
Prediction Num. Sent. Relation
(Train, Test) accuracy
Truth Vague OD NC Cure Prev. OT SE Irr.
Vague 0 3 0 4 0 0 0 1 28, 8 0
Only DIS (OD) 2 69 0 27 1 1 0 24 492, 124 55.6
No Cure (NC) 0 0 0 1 0 0 0 0 3, 1 0
Cure 2 5 0 150 1 1 0 3 648, 162 92.6
Prevent 0 1 0 2 5 0 0 5 50, 13 38.5
Only TREAT (OT) 0 0 0 16 0 6 1 11 132, 34 17.6
Side effect (SE) 0 0 0 3 1 0 0 1 24, 5 20
Irrelevant 1 32 1 16 2 7 0 296 1416, 355 83.4
</table>
<tableCaption confidence="0.993712">
Table 4: Confusion matrix for the dynamic model D2 for “rel + irrel.”, “only features”. In column “Num.
</tableCaption>
<bodyText confidence="0.990991117647059">
Sent.” the numbers of sentences used for training and testing and in the last column the classification
accuracies for each relation. The total accuracy for this case is 74.9%.
approaches and the use of a lexical hierarchy.
Because there is no existing gold-standard for
this problem, we have developed the relation def-
initions of Table 1; this however may not be an
exhaustive list. In the future we plan to assess ad-
ditional relation types. It is unclear at this time if
this approach will work on other types of text; the
technical nature of bioscience text may lend itself
well to this type of analysis.
Acknowledgements We thank Kaichi Sung for
her work on the relation labeling and Chris Man-
ning for helpful suggestions. This research was
supported by a grant from the ARDA AQUAINT
program, NSF DBI-0317510, and a gift from
Genentech.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999805849056604">
E. Agichtein and L. Gravano. 2000. Snowball: Ex-
tracting relations from large plain-text collections.
Proceedings ofDL ’00.
D. Bikel, R. Schwartz, and R. Weischedel. 1999. An
algorithm that learns what’s in a name. Machine
Learning, 34(1-3):211–231.
E. Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543–565.
M. Collins. 1996. A new statistical parser based on
bigram lexical dependencies. Proc. ofACL ’96.
M. Craven. 1999. Learning to extract relations from
Medline. AAAI-99 Workshop on Machine Learning
for Information Extraction.
R. Feldman, Y. Regev, M. Finkelstein-Landau,
E. Hurvitz, and B. Kogan. 2002. Mining biomed-
ical literature using information extraction. Current
Drug Discovery, Oct.
D. Freitag and A. McCallum. 2000. Information ex-
traction with HMM structures learned by stochastic
optimization. AAAI/IAAI, pages 584–589.
C. Friedman, P. Kra, H. Yu, M. Krauthammer, and
A. Rzhetzky. 2001. Genies: a natural-language pro-
cessing system for the extraction of molecular path-
ways from journal articles. Bioinformatics, 17(1).
A. Ng and M. Jordan. 2002. On discriminative vs.
generative classifiers: A comparison of logistic re-
gression and Naive Bayes. NIPS 14.
J. Pustejovsky, J. Castano, and J. Zhang. 2002. Robust
relational parsing over biomedical literature: Ex-
tracting inhibit relations. PSB 2002.
S. Ray and M. Craven. 2001. Representing sentence
structure in Hidden Markov Models for information
extraction. Proceedings ofIJCAI-2001.
T. Rindflesch, L. Hunter, and L. Aronson. 1999. Min-
ing molecular binding terminology from biomedical
text. Proceedings of the AMIA Symposium.
B. Rosario, M. Hearst, and C. Fillmore. 2002. The
descent of hierarchy, and selection in relational se-
mantics. Proceedings ofACL-02.
P. Srinivasan and T. Rindflesch. 2002. Exploring text
mining from Medline. Proceedings of the AMIA
Symposium.
C. Thompson, R. Levy, and C. Manning. 2003. A gen-
erative model for semantic role labeling. Proceed-
ings of EMCL ’03.
D. Zelenko, C. Aone, and A. Richardella. 2002. Ker-
nel methods for relation extraction. Proceedings of
EMNLP 2002.
C. Zhai and J. Lafferty. 2001. A study of smoothing
methods for language models applied to ad hoc in-
formation retrieval. In Proceedings of SIGIR ’01.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.814297">
<title confidence="0.9982905">Classifying Semantic Relations in Bioscience Texts</title>
<author confidence="0.999412">Barbara Rosario</author>
<affiliation confidence="0.967798">SIMS UC Berkeley</affiliation>
<address confidence="0.999738">Berkeley, CA 94720</address>
<email confidence="0.996567">rosario@sims.berkeley.edu</email>
<author confidence="0.99996">Marti A Hearst</author>
<affiliation confidence="0.964866">SIMS UC Berkeley</affiliation>
<address confidence="0.999708">Berkeley, CA 94720</address>
<email confidence="0.998832">hearst@sims.berkeley.edu</email>
<abstract confidence="0.9964704375">A crucial step toward the goal of automatic extraction of propositional information from natural language text is the identification of semantic relations between constituents in sentences. We examine the problem of distinguishing among seven relation types that can occur between the entities “treatment” and “disease” in bioscience text, and the problem of identifying such entities. We compare five generative graphical models and a neural network, using lexical, syntactic, and semantic features, finding that the latter help achieve high classification accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>L Gravano</author>
</authors>
<title>Snowball: Extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>Proceedings ofDL ’00.</booktitle>
<contexts>
<context position="5261" citStr="Agichtein and Gravano (2000)" startWordPosition="817" endWordPosition="820">t claim to be doing relationship recognition in reality address the task of role extraction: (usually two) entities are extracted and the relationship is implied by the cooccurrence of these entities or by the presence of some linguistic expression. These linguistic patterns could in principle distinguish between different relations, but instead are usually used to identify examples of one relation. In the related work for statistical models there has been, to the best of our knowledge, no attempt to distinguish between different relations that can occur between the same semantic entities. In Agichtein and Gravano (2000) the goal is to extract pairs such as (Microsoft, Redmond), where Redmond is the location of the organization Microsoft. Their technique generates and evaluates lexical patterns that are indicative of the relation. Only the relation location of is tackled and the entities are assumed given. In Zelenko et al. (2002), the task is to extract the relationships person-affiliation and organization-location. The classification (done with Support Vector Machine and Voted Perceptron algorithms) is between positive and negative sentences, where the positive sentences contain the two entities. In the bio</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>E. Agichtein and L. Gravano. 2000. Snowball: Extracting relations from large plain-text collections. Proceedings ofDL ’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bikel</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>An algorithm that learns what’s in a name.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="13480" citStr="Bikel et al., 1999" startWordPosition="2151" endWordPosition="2154">ce. We assume here that there is a single relation for each sentence between the entities5. The children of the role nodes are the words and their features, thus there are as many role states as there are words in the sentence; for the static models, this is depicted by the box (or “plate”) which is the standard graphical model notation for replication. For each state, the features are those mentioned in Section 3. The simpler static models S1 and S2 do not assume an ordering in the role sequence. The dynamic models were inspired by prior work on HMM-like graphical models for role extraction (Bikel et al., 1999; Freitag and McCallum, 2000; Ray and Craven, 2001). These models consist of a 4In this paper, precision and recall are given equal weight, that is, F-measure = . 5We found 75 sentences which contain more than one relationship, often with multiple entities or the same entities taking part in several interconnected relationships; we did not include these in the study. dynamic model (D1) dynamic model (D2) dynamic model (D3) Figure 1: Models for role and relation extraction. Markov sequence of states (usually corresponding to semantic roles) where each state generates one or multiple observation</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>D. Bikel, R. Schwartz, and R. Weischedel. 1999. An algorithm that learns what’s in a name. Machine Learning, 34(1-3):211–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="9800" citStr="Brill, 1995" startWordPosition="1530" endWordPosition="1531"> medline01n*.xml in Medline 2001; the labeled data is available at biotext.berkeley.edu 3http://www.nlm.nih.gov/mesh/meshhome.html vous System Diseases). When there are multiple MeSH terms for one word, we simply choose the first one. These semantic features are shown to be very useful for our tasks (see Section 4.3). Rosario et al. (2002) demonstrate the usefulness of MeSH for the classification of the semantic relationships between nouns in noun compounds. The results reported in this paper were obtained with the following features: the word itself, its part of speech from the Brill tagger (Brill, 1995), the phrase constituent the word belongs to, obtained by flattening the output of a parser (Collins, 1996), and the word’s MeSH ID (if available). In addition, we identified the sub-hierarchies of MeSH that tend to correspond to treatments and diseases, and convert these into a tri-valued attribute indicating one of: disease, treatment or neither. Finally, we included orthographic features such as ‘is the word a number’, ‘only part of the word is a number’, ‘first letter is capitalized’, ‘all letters are capitalized’. In Section 4.3 we analyze the impact of these features. 4 Models and Result</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>E. Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21(4):543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>Proc. ofACL ’96.</booktitle>
<contexts>
<context position="9907" citStr="Collins, 1996" startWordPosition="1547" endWordPosition="1548">ih.gov/mesh/meshhome.html vous System Diseases). When there are multiple MeSH terms for one word, we simply choose the first one. These semantic features are shown to be very useful for our tasks (see Section 4.3). Rosario et al. (2002) demonstrate the usefulness of MeSH for the classification of the semantic relationships between nouns in noun compounds. The results reported in this paper were obtained with the following features: the word itself, its part of speech from the Brill tagger (Brill, 1995), the phrase constituent the word belongs to, obtained by flattening the output of a parser (Collins, 1996), and the word’s MeSH ID (if available). In addition, we identified the sub-hierarchies of MeSH that tend to correspond to treatments and diseases, and convert these into a tri-valued attribute indicating one of: disease, treatment or neither. Finally, we included orthographic features such as ‘is the word a number’, ‘only part of the word is a number’, ‘first letter is capitalized’, ‘all letters are capitalized’. In Section 4.3 we analyze the impact of these features. 4 Models and Results This section describes the models and their performance on both entity extraction and relation classifica</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>M. Collins. 1996. A new statistical parser based on bigram lexical dependencies. Proc. ofACL ’96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Craven</author>
</authors>
<title>Learning to extract relations from Medline.</title>
<date>1999</date>
<booktitle>AAAI-99 Workshop on Machine Learning for Information Extraction.</booktitle>
<contexts>
<context position="6481" citStr="Craven (1999)" startWordPosition="1006" endWordPosition="1007"> NLP literature there are also efforts to extract entities and relations. In Ray and Craven (2001), Hidden Markov Models are applied to MEDLINE text to extract the entities PROTEINS and LOCATIONS in the relationship subcellular-location and the entities GENE and DISORDER in the relationship disorderassociation. The authors acknowledge that the task of extracting relations is different from the task of extracting entities. Nevertheless, they consider positive examples to be all the sentences that simply contain the entities, rather than analyzing which relations hold between these entities. In Craven (1999), the problem tackled is relationship extraction from MEDLINE for the relation subcellular-location. The authors treat it as a text classification problem and propose and compare two classifiers: a Naive Bayes classifier and a relational learning algorithm. This is a two-way classification, and again there is no mention of whether the co-occurrence of the entities actually represents the target relation. Pustejovsky et al. (2002) use a rule-based system to extract entities in the inhibit-relation. Their experiments use sentences that contain verbal and nominal forms of the stem inhibit. Thus t</context>
</contexts>
<marker>Craven, 1999</marker>
<rawString>M. Craven. 1999. Learning to extract relations from Medline. AAAI-99 Workshop on Machine Learning for Information Extraction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Feldman</author>
<author>Y Regev</author>
<author>M Finkelstein-Landau</author>
<author>E Hurvitz</author>
<author>B Kogan</author>
</authors>
<title>Mining biomedical literature using information extraction. Current Drug Discovery,</title>
<date>2002</date>
<contexts>
<context position="7800" citStr="Feldman et al. (2002)" startWordPosition="1211" endWordPosition="1214"> inhibit, which by requiring occurrence of this word explicitly, is not the same as finding all sentences that talk about inhibiting actions. Similarly, Rindflesch et al. (1999) identify noun phrases surrounding forms of the stem bind which signify entities that can enter into molecular binding relationships. In Srinivasan and Rindflesch (2002) MeSH term co-occurrences within MEDLINE articles are used to attempt to infer relationships between different concepts, including diseases and drugs. In the bioscience domain the work on relation classification is primary done through hand-built rules. Feldman et al. (2002) use hand-built rules that make use of syntactic and lexical features and semantic constraints to find relations between genes, proteins, drugs and diseases. The GENIES system (Friedman et al., 2001) uses a hand-built semantic grammar along with hand-derived syntactic and semantic constraints, and recognizes a wide range of relationships between biological molecules. 3 Data and Features For our experiments, the text was obtained from MEDLINE 20012. An annotator with biology expertise considered the titles and abstracts separately and labeled the sentences (both roles and relations) based solel</context>
</contexts>
<marker>Feldman, Regev, Finkelstein-Landau, Hurvitz, Kogan, 2002</marker>
<rawString>R. Feldman, Y. Regev, M. Finkelstein-Landau, E. Hurvitz, and B. Kogan. 2002. Mining biomedical literature using information extraction. Current Drug Discovery, Oct.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Freitag</author>
<author>A McCallum</author>
</authors>
<title>Information extraction with HMM structures learned by stochastic optimization.</title>
<date>2000</date>
<pages>584--589</pages>
<publisher>AAAI/IAAI,</publisher>
<contexts>
<context position="13508" citStr="Freitag and McCallum, 2000" startWordPosition="2155" endWordPosition="2158">hat there is a single relation for each sentence between the entities5. The children of the role nodes are the words and their features, thus there are as many role states as there are words in the sentence; for the static models, this is depicted by the box (or “plate”) which is the standard graphical model notation for replication. For each state, the features are those mentioned in Section 3. The simpler static models S1 and S2 do not assume an ordering in the role sequence. The dynamic models were inspired by prior work on HMM-like graphical models for role extraction (Bikel et al., 1999; Freitag and McCallum, 2000; Ray and Craven, 2001). These models consist of a 4In this paper, precision and recall are given equal weight, that is, F-measure = . 5We found 75 sentences which contain more than one relationship, often with multiple entities or the same entities taking part in several interconnected relationships; we did not include these in the study. dynamic model (D1) dynamic model (D2) dynamic model (D3) Figure 1: Models for role and relation extraction. Markov sequence of states (usually corresponding to semantic roles) where each state generates one or multiple observations. Model D1 in Figure 1 is t</context>
</contexts>
<marker>Freitag, McCallum, 2000</marker>
<rawString>D. Freitag and A. McCallum. 2000. Information extraction with HMM structures learned by stochastic optimization. AAAI/IAAI, pages 584–589.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Friedman</author>
<author>P Kra</author>
<author>H Yu</author>
<author>M Krauthammer</author>
<author>A Rzhetzky</author>
</authors>
<title>Genies: a natural-language processing system for the extraction of molecular pathways from journal articles.</title>
<date>2001</date>
<journal>Bioinformatics,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="7999" citStr="Friedman et al., 2001" startWordPosition="1241" endWordPosition="1244">s surrounding forms of the stem bind which signify entities that can enter into molecular binding relationships. In Srinivasan and Rindflesch (2002) MeSH term co-occurrences within MEDLINE articles are used to attempt to infer relationships between different concepts, including diseases and drugs. In the bioscience domain the work on relation classification is primary done through hand-built rules. Feldman et al. (2002) use hand-built rules that make use of syntactic and lexical features and semantic constraints to find relations between genes, proteins, drugs and diseases. The GENIES system (Friedman et al., 2001) uses a hand-built semantic grammar along with hand-derived syntactic and semantic constraints, and recognizes a wide range of relationships between biological molecules. 3 Data and Features For our experiments, the text was obtained from MEDLINE 20012. An annotator with biology expertise considered the titles and abstracts separately and labeled the sentences (both roles and relations) based solely on the content of the individual sentences. Seven possible types of relationships between TREATMENT and DISEASE were identified. Table 1 shows, for each relation, its definition, one example senten</context>
</contexts>
<marker>Friedman, Kra, Yu, Krauthammer, Rzhetzky, 2001</marker>
<rawString>C. Friedman, P. Kra, H. Yu, M. Krauthammer, and A. Rzhetzky. 2001. Genies: a natural-language processing system for the extraction of molecular pathways from journal articles. Bioinformatics, 17(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>On discriminative vs. generative classifiers: A comparison of logistic regression and Naive Bayes.</title>
<date>2002</date>
<journal>NIPS</journal>
<volume>14</volume>
<contexts>
<context position="11344" citStr="Ng and Jordan (2002)" startWordPosition="1784" endWordPosition="1787">nce labeled data is expensive to collect, these models may be useful when no labels are available. However, in this paper we test the generative models on fully observed data and show that, although not as accurate as the discriminative model, their performance is promising enough to encourage their use for the case of partially observed data. Discriminative models learn the probability of the class given the features. When we have fully observed data and we just need to learn the mapping from features to classes (classification), a discriminative approach may be more appropriate, as shown in Ng and Jordan (2002), but has other shortcomings as discussed below. For the evaluation of the role extraction task, we calculate the usual metrics of precision, recall and F-measure. Precision is a measure of how many of the roles extracted by the system are correct and recall is the measure of how many of the true roles were extracted by the system. The F-measure is a weighted combination of precision and recall4. Our role evaluation is very strict: every token is assessed and we do not assign partial credit for constituents for which only some of the words are correctly labeled. We report results for two cases</context>
</contexts>
<marker>Ng, Jordan, 2002</marker>
<rawString>A. Ng and M. Jordan. 2002. On discriminative vs. generative classifiers: A comparison of logistic regression and Naive Bayes. NIPS 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>J Castano</author>
<author>J Zhang</author>
</authors>
<title>Robust relational parsing over biomedical literature: Extracting inhibit relations. PSB</title>
<date>2002</date>
<contexts>
<context position="6914" citStr="Pustejovsky et al. (2002)" startWordPosition="1070" endWordPosition="1073">ies. Nevertheless, they consider positive examples to be all the sentences that simply contain the entities, rather than analyzing which relations hold between these entities. In Craven (1999), the problem tackled is relationship extraction from MEDLINE for the relation subcellular-location. The authors treat it as a text classification problem and propose and compare two classifiers: a Naive Bayes classifier and a relational learning algorithm. This is a two-way classification, and again there is no mention of whether the co-occurrence of the entities actually represents the target relation. Pustejovsky et al. (2002) use a rule-based system to extract entities in the inhibit-relation. Their experiments use sentences that contain verbal and nominal forms of the stem inhibit. Thus the actual task performed is the extraction of entities that are connected by some form of the stem inhibit, which by requiring occurrence of this word explicitly, is not the same as finding all sentences that talk about inhibiting actions. Similarly, Rindflesch et al. (1999) identify noun phrases surrounding forms of the stem bind which signify entities that can enter into molecular binding relationships. In Srinivasan and Rindfl</context>
</contexts>
<marker>Pustejovsky, Castano, Zhang, 2002</marker>
<rawString>J. Pustejovsky, J. Castano, and J. Zhang. 2002. Robust relational parsing over biomedical literature: Extracting inhibit relations. PSB 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ray</author>
<author>M Craven</author>
</authors>
<title>Representing sentence structure in Hidden Markov Models for information extraction.</title>
<date>2001</date>
<booktitle>Proceedings ofIJCAI-2001.</booktitle>
<contexts>
<context position="5966" citStr="Ray and Craven (2001)" startWordPosition="925" endWordPosition="928">ation of the organization Microsoft. Their technique generates and evaluates lexical patterns that are indicative of the relation. Only the relation location of is tackled and the entities are assumed given. In Zelenko et al. (2002), the task is to extract the relationships person-affiliation and organization-location. The classification (done with Support Vector Machine and Voted Perceptron algorithms) is between positive and negative sentences, where the positive sentences contain the two entities. In the bioscience NLP literature there are also efforts to extract entities and relations. In Ray and Craven (2001), Hidden Markov Models are applied to MEDLINE text to extract the entities PROTEINS and LOCATIONS in the relationship subcellular-location and the entities GENE and DISORDER in the relationship disorderassociation. The authors acknowledge that the task of extracting relations is different from the task of extracting entities. Nevertheless, they consider positive examples to be all the sentences that simply contain the entities, rather than analyzing which relations hold between these entities. In Craven (1999), the problem tackled is relationship extraction from MEDLINE for the relation subcel</context>
<context position="13531" citStr="Ray and Craven, 2001" startWordPosition="2159" endWordPosition="2162">on for each sentence between the entities5. The children of the role nodes are the words and their features, thus there are as many role states as there are words in the sentence; for the static models, this is depicted by the box (or “plate”) which is the standard graphical model notation for replication. For each state, the features are those mentioned in Section 3. The simpler static models S1 and S2 do not assume an ordering in the role sequence. The dynamic models were inspired by prior work on HMM-like graphical models for role extraction (Bikel et al., 1999; Freitag and McCallum, 2000; Ray and Craven, 2001). These models consist of a 4In this paper, precision and recall are given equal weight, that is, F-measure = . 5We found 75 sentences which contain more than one relationship, often with multiple entities or the same entities taking part in several interconnected relationships; we did not include these in the study. dynamic model (D1) dynamic model (D2) dynamic model (D3) Figure 1: Models for role and relation extraction. Markov sequence of states (usually corresponding to semantic roles) where each state generates one or multiple observations. Model D1 in Figure 1 is typical of these models,</context>
<context position="18905" citStr="Ray and Craven (2001)" startWordPosition="3108" endWordPosition="3111">ic models achieve similar results. Although the additional edges in models D2 and D3 did not help much for the task of role extraction, they did help for relation classification, discussed next. Model D2 6To perform inference for the dynamic model, we used the junction tree algorithm. We used Kevin Murphy’s BNT package, found at http://www.ai.mit.edu/ murphyk/Bayes/bnintro.html. (1) achieves the best F-measures: 0.73 for “only relevant” and 0.71 for “rel. + irrel.”. It is difficult to compare results with the related work since the data, the semantic roles and the evaluation are different; in Ray and Craven (2001) however, the role extraction task is quite similar to ours and the text is also from MEDLINE. They report approximately an F-measure of 32% for the extraction of the entities PROTEINS and LOCATIONS, and an F-measure of 50% for GENE and DISORDER. The second target task is to find the most likely relation, i.e., to classify a sentence into one of the possible relations. Two types of experiments were conducted. In the first, the true roles are hidden and we classify the relations given only the observable features, marginalizing over the hidden roles. In the second, the roles are given and only </context>
</contexts>
<marker>Ray, Craven, 2001</marker>
<rawString>S. Ray and M. Craven. 2001. Representing sentence structure in Hidden Markov Models for information extraction. Proceedings ofIJCAI-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Rindflesch</author>
<author>L Hunter</author>
<author>L Aronson</author>
</authors>
<title>Mining molecular binding terminology from biomedical text.</title>
<date>1999</date>
<booktitle>Proceedings of the AMIA Symposium.</booktitle>
<contexts>
<context position="7356" citStr="Rindflesch et al. (1999)" startWordPosition="1144" endWordPosition="1147">rithm. This is a two-way classification, and again there is no mention of whether the co-occurrence of the entities actually represents the target relation. Pustejovsky et al. (2002) use a rule-based system to extract entities in the inhibit-relation. Their experiments use sentences that contain verbal and nominal forms of the stem inhibit. Thus the actual task performed is the extraction of entities that are connected by some form of the stem inhibit, which by requiring occurrence of this word explicitly, is not the same as finding all sentences that talk about inhibiting actions. Similarly, Rindflesch et al. (1999) identify noun phrases surrounding forms of the stem bind which signify entities that can enter into molecular binding relationships. In Srinivasan and Rindflesch (2002) MeSH term co-occurrences within MEDLINE articles are used to attempt to infer relationships between different concepts, including diseases and drugs. In the bioscience domain the work on relation classification is primary done through hand-built rules. Feldman et al. (2002) use hand-built rules that make use of syntactic and lexical features and semantic constraints to find relations between genes, proteins, drugs and diseases</context>
</contexts>
<marker>Rindflesch, Hunter, Aronson, 1999</marker>
<rawString>T. Rindflesch, L. Hunter, and L. Aronson. 1999. Mining molecular binding terminology from biomedical text. Proceedings of the AMIA Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Rosario</author>
<author>M Hearst</author>
<author>C Fillmore</author>
</authors>
<title>The descent of hierarchy, and selection in relational semantics.</title>
<date>2002</date>
<booktitle>Proceedings ofACL-02.</booktitle>
<contexts>
<context position="9529" citStr="Rosario et al. (2002)" startWordPosition="1485" endWordPosition="1488">., tree A corresponds to Anatomy, tree C to Disease, and so on. As an example, the word migraine maps to the term C10.228, that is, C (a disease), C10 (Nervous System Diseases), C10.228 (Central Ner2We used the first 100 titles and the first 40 abstracts from each of the 59 files medline01n*.xml in Medline 2001; the labeled data is available at biotext.berkeley.edu 3http://www.nlm.nih.gov/mesh/meshhome.html vous System Diseases). When there are multiple MeSH terms for one word, we simply choose the first one. These semantic features are shown to be very useful for our tasks (see Section 4.3). Rosario et al. (2002) demonstrate the usefulness of MeSH for the classification of the semantic relationships between nouns in noun compounds. The results reported in this paper were obtained with the following features: the word itself, its part of speech from the Brill tagger (Brill, 1995), the phrase constituent the word belongs to, obtained by flattening the output of a parser (Collins, 1996), and the word’s MeSH ID (if available). In addition, we identified the sub-hierarchies of MeSH that tend to correspond to treatments and diseases, and convert these into a tri-valued attribute indicating one of: disease, </context>
</contexts>
<marker>Rosario, Hearst, Fillmore, 2002</marker>
<rawString>B. Rosario, M. Hearst, and C. Fillmore. 2002. The descent of hierarchy, and selection in relational semantics. Proceedings ofACL-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Srinivasan</author>
<author>T Rindflesch</author>
</authors>
<title>Exploring text mining from Medline.</title>
<date>2002</date>
<booktitle>Proceedings of the AMIA Symposium.</booktitle>
<contexts>
<context position="7525" citStr="Srinivasan and Rindflesch (2002)" startWordPosition="1170" endWordPosition="1173">ejovsky et al. (2002) use a rule-based system to extract entities in the inhibit-relation. Their experiments use sentences that contain verbal and nominal forms of the stem inhibit. Thus the actual task performed is the extraction of entities that are connected by some form of the stem inhibit, which by requiring occurrence of this word explicitly, is not the same as finding all sentences that talk about inhibiting actions. Similarly, Rindflesch et al. (1999) identify noun phrases surrounding forms of the stem bind which signify entities that can enter into molecular binding relationships. In Srinivasan and Rindflesch (2002) MeSH term co-occurrences within MEDLINE articles are used to attempt to infer relationships between different concepts, including diseases and drugs. In the bioscience domain the work on relation classification is primary done through hand-built rules. Feldman et al. (2002) use hand-built rules that make use of syntactic and lexical features and semantic constraints to find relations between genes, proteins, drugs and diseases. The GENIES system (Friedman et al., 2001) uses a hand-built semantic grammar along with hand-derived syntactic and semantic constraints, and recognizes a wide range of</context>
</contexts>
<marker>Srinivasan, Rindflesch, 2002</marker>
<rawString>P. Srinivasan and T. Rindflesch. 2002. Exploring text mining from Medline. Proceedings of the AMIA Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Thompson</author>
<author>R Levy</author>
<author>C Manning</author>
</authors>
<title>A generative model for semantic role labeling.</title>
<date>2003</date>
<booktitle>Proceedings of EMCL ’03.</booktitle>
<contexts>
<context position="16551" citStr="Thompson et al. (2003)" startWordPosition="2715" endWordPosition="2718"> motivation being that some observations (such as the words) depend on the relation while others might not (like for example, the parts of speech). In the experiments reported here, the observations which have edges from both the role and the relation nodes are the words. (We ran an experiment in which this observation node was the MeSH term, obtaining similar results.) Model D1 defines the following joint probability distribution over relations, roles, words and word features, assuming the leftmost Role node is , and is the number of words in the sentence: Model D1 is similar to the model in Thompson et al. (2003) for the extraction of roles, using a different domain. Structurally, the differences are (i) Thompson et al. (2003) has only one observation node per role and (ii) it has an additional node “on top”, with an edge to the relation node, to represent a predicator “trigger word” which is always observed; the predicator words are taken from a fixed list and one must be present in order for a sentence to be analyzed. The joint probability distributions for D2 and D3 are similar to Equation (1) where we substitute the term with for D2 and for D3. The parameters and of Equation (1) are constrained to</context>
</contexts>
<marker>Thompson, Levy, Manning, 2003</marker>
<rawString>C. Thompson, R. Levy, and C. Manning. 2003. A generative model for semantic role labeling. Proceedings of EMCL ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zelenko</author>
<author>C Aone</author>
<author>A Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2002</date>
<booktitle>Proceedings of EMNLP</booktitle>
<contexts>
<context position="5577" citStr="Zelenko et al. (2002)" startWordPosition="869" endWordPosition="872">t relations, but instead are usually used to identify examples of one relation. In the related work for statistical models there has been, to the best of our knowledge, no attempt to distinguish between different relations that can occur between the same semantic entities. In Agichtein and Gravano (2000) the goal is to extract pairs such as (Microsoft, Redmond), where Redmond is the location of the organization Microsoft. Their technique generates and evaluates lexical patterns that are indicative of the relation. Only the relation location of is tackled and the entities are assumed given. In Zelenko et al. (2002), the task is to extract the relationships person-affiliation and organization-location. The classification (done with Support Vector Machine and Voted Perceptron algorithms) is between positive and negative sentences, where the positive sentences contain the two entities. In the bioscience NLP literature there are also efforts to extract entities and relations. In Ray and Craven (2001), Hidden Markov Models are applied to MEDLINE text to extract the entities PROTEINS and LOCATIONS in the relationship subcellular-location and the entities GENE and DISORDER in the relationship disorderassociati</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2002</marker>
<rawString>D. Zelenko, C. Aone, and A. Richardella. 2002. Kernel methods for relation extraction. Proceedings of EMNLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Zhai</author>
<author>J Lafferty</author>
</authors>
<title>A study of smoothing methods for language models applied to ad hoc information retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR ’01.</booktitle>
<contexts>
<context position="17330" citStr="Zhai and Lafferty, 2001" startWordPosition="2848" endWordPosition="2851"> and (ii) it has an additional node “on top”, with an edge to the relation node, to represent a predicator “trigger word” which is always observed; the predicator words are taken from a fixed list and one must be present in order for a sentence to be analyzed. The joint probability distributions for D2 and D3 are similar to Equation (1) where we substitute the term with for D2 and for D3. The parameters and of Equation (1) are constrained to be equal. The parameters were estimated using maximum likelihood on the training set; we also implemented a simple absolute discounting smoothing method (Zhai and Lafferty, 2001) that improves the results for both tasks. Table 2 shows the results (F-measures) for the problem of finding the most likely sequence of roles given the features observed. In this case, the relation is hidden and we marginalize over it6. We experimented with different values for the smoothing factor ranging from a minimum of 0.0000005 to a maximum of 10; the results shown fix the smoothing factor at its minimum value. We found that for the dynamic models, for a wide range of smoothing factors, we achieved almost identical results; nevertheless, in future work, we plan to implement cross-valida</context>
</contexts>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>C. Zhai and J. Lafferty. 2001. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of SIGIR ’01.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>