<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000058">
<title confidence="0.993071">
Mining Entity Types from Query Logs via User Intent Modeling
</title>
<author confidence="0.975692">
Patrick Pantel
</author>
<affiliation confidence="0.922024">
Microsoft Research
</affiliation>
<address confidence="0.9596845">
One Microsoft Way
Redmond, WA 98052, USA
</address>
<email confidence="0.998366">
ppantel@microsoft.com
</email>
<author confidence="0.993866">
Thomas Lin
</author>
<affiliation confidence="0.9947125">
Computer Science &amp; Engineering
University of Washington
</affiliation>
<address confidence="0.784975">
Seattle, WA 98195, USA
</address>
<email confidence="0.998578">
tlin@cs.washington.edu
</email>
<author confidence="0.932527">
Michael Gamon
</author>
<affiliation confidence="0.884962">
Microsoft Research
</affiliation>
<address confidence="0.957014">
One Microsoft Way
Redmond, WA 98052, USA
</address>
<email confidence="0.998733">
mgamon@microsoft.com
</email>
<sectionHeader confidence="0.995639" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999739777777778">
We predict entity type distributions in Web
search queries via probabilistic inference in
graphical models that capture how entity-
bearing queries are generated. We jointly
model the interplay between latent user in-
tents that govern queries and unobserved en-
tity types, leveraging observed signals from
query formulations and document clicks. We
apply the models to resolve entity types in new
queries and to assign prior type distributions
over an existing knowledge base. Our mod-
els are efficiently trained using maximum like-
lihood estimation over millions of real-world
Web search queries. We show that modeling
user intent significantly improves entity type
resolution for head queries over the state of the
art, on several metrics, without degradation in
tail query performance.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999920309523809">
Commercial search engines are providing ever-
richer experiences around entities. Querying for a
dish on Google yields recipe filters such as cook
time, calories, and ingredients. Querying for a
movie on Yahoo triggers user ratings, cast, tweets
and showtimes. Bing further allows the movie to
be directly added to the user’s Netflix queue. En-
tity repositories such as Freebase, IMDB, Facebook
Pages, Factual, Pricegrabber, and Wikipedia are in-
creasingly leveraged to enable such experiences.
There are, however, inherent problems in the en-
tity repositories: (a) coverage: although coverage of
head entity types is often reliable, the tail can be
sparse; (b) noise: created by spammers, extraction
errors or errors in crowdsourced content; (c) am-
biguity: multiple types or entity identifiers are of-
ten associated with the same surface string; and (d)
over-expression: many entities have types that are
never used in the context of Web search.
There is an opportunity to automatically tailor
knowledge repositories to the Web search scenario.
Desirable capabilities of such a system include: (a)
determining the prior type distribution in Web search
for each entity in the repository; (b) assigning a type
distribution to new entities; (c) inferring the correct
sense of an entity in a particular query context; and
(d) adapting to a search engine and time period.
In this paper, we build such a system by lever-
aging Web search usage logs with large numbers of
user sessions seeking or transacting on entities. We
cast the task as performing probabilistic inference
in a graphical model that captures how queries are
generated, and then apply the model to contextually
recognize entity types in new queries. We motivate
and design several generative models based on the
theory that search users’ (unobserved) intents gov-
ern the types of entities, the query formulations, and
the ultimate clicks on Web documents. We show that
jointly modeling user intent and entity type signifi-
cantly outperforms the current state of the art on the
task of entity type resolution in queries. The major
contributions of our research are:
</bodyText>
<listItem confidence="0.938061333333333">
• We introduce the idea that latent user intents
can be an important factor in modeling type dis-
tributions over entities in Web search.
• We propose generative models and inference
procedures using signals from query context,
click, entity, entity type, and user intent.
</listItem>
<page confidence="0.994016">
563
</page>
<note confidence="0.9624795">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 563–571,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<listItem confidence="0.919451">
• We propose an efficient learning technique and
a robust implementation of our models, using
real-world query data, and a realistic large set
of entity types.
• We empirically show that our models outper-
</listItem>
<bodyText confidence="0.8090735">
form the state of the art and that modeling latent
intent contributes significantly to these results.
</bodyText>
<sectionHeader confidence="0.999827" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999574">
2.1 Finding Semantic Classes
</subsectionHeader>
<bodyText confidence="0.999469866666667">
A closely related problem is that of finding the se-
mantic classes of entities. Automatic techniques for
finding semantic classes include unsupervised clus-
tering (Sch¨utze, 1998; Pantel and Lin, 2002), hy-
ponym patterns (Hearst, 1992; Pantel et al., 2004;
Kozareva et al., 2008), extraction patterns (Etzioni
et al., 2005), hidden Markov models (Ritter et al.,
2009), classification (Rahman and Ng, 2010) and
many others. These techniques typically lever-
age large corpora, while projects such as WordNet
(Miller et al., 1990) and Freebase (Bollacker et al.,
2008) have employed editors to manually enumerate
words and entities with their semantic classes.
The aforementioned methods do not use query
logs or explicitly determine the relative probabilities
of different entity senses. A method might learn that
there is independently a high chance of eBay being a
website and an employer, but does not specify which
usage is more common. This is especially problem-
atic, for example, if one wishes to leverage Freebase
but only needs the most commonly used senses (e.g.,
Al Gore is a US Vice President), rather than
all possible obscure senses (Freebase contains 30+
senses, including ones such as Impersonated
Celebrity and Quotation Subject). In
scenarios such as this, our proposed method can in-
crease the usability of systems that find semantic
classes. We also expand upon text corpora meth-
ods in that the type priors can adapt to Web search
signals.
</bodyText>
<subsectionHeader confidence="0.999342">
2.2 Query Log Mining
</subsectionHeader>
<bodyText confidence="0.99941565">
Query logs have traditionally been mined to improve
search (Baeza-Yates et al., 2004; Zhang and Nas-
raoui, 2006), but they can also be used in place of
(or in addition to) text corpora for learning seman-
tic classes. Query logs can contain billions of en-
tries, they provide an independent signal from text
corpora, their timestamps allow the learning of type
priors at specific points in time, and they can contain
information such as clickthroughs that are not found
in text corpora. Sekine and Suzuki (2007) used fre-
quency features on context words in query logs to
learn semantic classes of entities. Pas¸ca (2007) used
extraction techniques to mine instances of semantic
classes from query logs. R¨ud et al. (2011) found
that cross-domain generalizations learned from Web
search results are applicable to NLP tasks such as
NER. Alfonseca et al. (2010) mined query logs to
find attributes of entity instances. However, these
projects did not learn relative probabilities of differ-
ent senses.
</bodyText>
<subsectionHeader confidence="0.99819">
2.3 User Intents in Search
</subsectionHeader>
<bodyText confidence="0.999985">
Learning from query logs also allows us to lever-
age the concept of user intents. When users sub-
mit search queries, they often have specific intents in
mind. Broder (2002) introduced 3 top level intents:
Informational (e.g., wanting to learn), Navigational
(wanting to visit a site), and Transactional (e.g.,
wanting to buy/sell). Rose and Levinson (2004) fur-
ther divided these into finer-grained subcategories,
and Yin and Shah (2010) built hierarchical tax-
onomies of search intents. Jansen et al. (2007), Hu
et al. (2009), and Radlinski et al. (2010) examined
how to infer the intent of queries. We are not aware
of any other work that has leveraged user intents to
learn type distributions.
</bodyText>
<subsectionHeader confidence="0.994342">
2.4 Topic Modeling on Query Logs
</subsectionHeader>
<bodyText confidence="0.999689666666667">
The closest work to ours is Guo et al.’s (2009) re-
search on Named Entity Recognition in Queries.
Given an entity-bearing query, they attempt to iden-
tify the entity and determine the type posteriors. Our
work significantly scales up the type posteriors com-
ponent of their work. While they only have four
potential types (Movie, Game, Book, Music) for
each entity, we employ over 70 popular types, allow-
ing much greater coverage of real entities and their
types. Because they only had four types, they were
able to hand label their training data. In contrast,
our system self-labels training examples by search-
ing query logs for high-likelihood entities, and must
handle any errors introduced by this process. Our
models also expand upon theirs by jointly modeling
</bodyText>
<page confidence="0.996602">
564
</page>
<bodyText confidence="0.999473875">
entity type with latent user intents, and by incorpo-
rating click signals.
Other projects have also demonstrated the util-
ity of topic modeling on query logs. Carman et
al. (2010) modeled users and clicked documents to
personalize search results and Gao et al. (2011) ap-
plied topic models to query logs in order to improve
document ranking for search.
</bodyText>
<sectionHeader confidence="0.996982" genericHeader="method">
3 Joint Model of Types and User Intents
</sectionHeader>
<bodyText confidence="0.999978">
We turn our attention now to the task of mining the
type distributions of entities and of resolving the
type of an entity in a particular query context. Our
approach is to probabilistically describe how entity-
bearing queries are generated in Web search. We
theorize that search queries are governed by a latent
user intent, which in turn influences the entity types,
the choice of query words, and the clicked hosts. We
develop inference procedures to infer the prior type
distributions of entities in Web search as well as to
resolve the type of an entity in a query, by maximiz-
ing the probability of observing a large collection of
real-world queries and their clicked hosts.
We represent a query q by a triple {n1, e, n2},
where e represents the entity mentioned in the query,
n1 and n2 are respectively the pre- and post-entity
contexts (possibly empty), referred to as refiners.
Details on how we obtain our corpus are presented
in Section 4.2.
</bodyText>
<subsectionHeader confidence="0.926284">
3.1 Intent-based Model (IM)
</subsectionHeader>
<bodyText confidence="0.999962375">
In this section we describe our main model, IM, il-
lustrated in Figure 1. We derive a learning algorithm
for the model in Section 3.2 and an inference proce-
dure in Section 3.3.
Recall our discussion of intents from Section 2.3.
The unobserved semantic type of an entity e in a
query is strongly correlated with the unobserved
user intent. For example, if a user queries for
“song”, then she is likely looking to ‘listen to it’,
‘download it’, ‘buy it’, or ‘find lyrics’ for it. Our
model incorporates this user intent as a latent vari-
able.
The choice of the query refiner words, n1 and n2,
is also clearly influenced by the user intent. For
example, refiners such as “lyrics” and “words” are
more likely to be used in queries where the intent is
</bodyText>
<construct confidence="0.901473777777778">
For each query/click pair {q, c}
type t ∼ Multinomial(T)
intent i ∼ Multinomial(Bt)
entity e ∼ Multinomial(ot)
switch s1 ∼ Bernoulli(ui)
switch s2 ∼ Bernoulli(ui)
if (s1) l-context n1 ∼ Multinomial(oi)
if (s2) r-context n2 ∼ Multinomial(oi)
click c ∼ Multinomial(wi)
</construct>
<tableCaption confidence="0.9917585">
Table 1: Model IM: Generative process for entity-
bearing queries.
</tableCaption>
<bodyText confidence="0.999831270270271">
to ‘find lyrics’ than in queries where the intent is to
‘listen’. The same is true for clicked hosts: clicks on
“lyrics.com” and “songlyrics.com” are more likely
to occur when the intent is to ‘find lyrics’, whereas
clicks on “pandora.com” and “last.fm” are more
likely for a ‘listen’ intent.
Model IM leverages each of these signals: latent
intent, query refiners, and clicked hosts. It generates
entity-bearing queries by first generating an entity
type, from which the user intent and entity is gen-
erated. In turn, the user intent is then used to gen-
erate the query refiners and the clicked host. In our
data analysis, we observed that over 90% of entity-
bearing queries did not contain any refiner words n1
and n2. In order to distribute more probability mass
to non-empty context words, we explicitly represent
the empty context using a switch variable that deter-
mines whether a context will be empty.
The generative process for IM is described in Ta-
ble 1. Consider the query “ymca lyrics”. Our model
first generates the type song, then given the type
it generates the entity “ymca” and the intent ‘find
lyrics’. The intent is then used to generate the pre-
and post-context words ∅ and “lyrics”, respectively,
and a click on a host such as “lyrics.com”.
For mathematical convenience, we assume that
the user intent is generated independently of the
entity itself. Without this assumption, we would
require learning a parameter for each intent-type-
entity configuration, exploding the number of pa-
rameters. Instead, we choose to include these depen-
dencies at the time of inference, as described later.
Recall that q = {n1, e, n2} and let s = {s1, s2},
where s1 = 1 if n1 is not empty and s2 = 1 if n2 is
not empty, 0 otherwise. The joint probability of the
model is the product of the conditional distributions,
as given by:
</bodyText>
<page confidence="0.981568">
565
</page>
<figure confidence="0.995169836065574">
f
t
E
T
e
n
y
t
2
Guo’09
Q
tt
E
s
T
s
e
y
tt
n 2
Q
Model M0
ff
T
tt
E
s
T
s
e
n 2
y
tt
c
Q
Model MS
ff
w
T
T
ss
y
T
K
e
s
tt
tt
n
2
i
i
c
Q
Model IM
w
q
f
T
K
K
</figure>
<figureCaption confidence="0.875530333333333">
Figure 1: Graphical models for generating entity-bearing queries. Guo&apos;09 represents the current state of the art (Guo
et al., 2009). Models M0 and M1 add an empty context switch and click information, respectively. Model IM further
constrains the query by the latent user intent.
</figureCaption>
<equation confidence="0.9981504">
P(t, i, q, c  |τ, Θ, Ψ, σ, Φ, Ω) =
P(t  |τ)P(i  |t, Θ)P(e  |t, Ψ)P(c  |i, Ω)
2
Y P(nj  |i, Φ)I[sj=1]P(sj|i, σ)
j=1
</equation>
<bodyText confidence="0.941988125">
We now define each of the terms in the joint dis-
tribution. Let T be the number of entity types. The
probability of generating a type t is governed by a
multinomial with probability vector τ:
τI[j=ˆt] , s.t.
j
where I is an indicator function set to 1 if its condi-
tion holds, and 0 otherwise.
Let K be the number of latent user intents that
govern our query log, where K is fixed in advance.
Then, the probability of intents i is defined as a
multinomial distribution with probability vector θt
such that Θ = [θ1, θ2, ..., θT] captures the matrix of
parameters across all T types:
ΘI[j=ˆi]
ˆt,j , s.t. dt
Let E be the number of known entities. The prob-
ability of generating an entity e is similarly governed
by a parameter Ψ across all T types:
ΨI[j=ˆe]
ˆt,j , s.t. dt
The probability of generating an empty or non-
empty context s given intent i is given by a Bernoulli
with parameter σi:
</bodyText>
<equation confidence="0.9885695">
P(s  |i= ˆi) = σI[s=1]
i 1 − σˆi
</equation>
<bodyText confidence="0.949857333333333">
Let V be the shared vocabulary size of all query
refiner words n1 and n2. Given an intent, i, the
probability of generating a refiner n is given by a
multinomial distribution with probability vector φi
such that Φ = [φ1, φ2, ..., φK] represents parame-
ters across intents:
</bodyText>
<equation confidence="0.987064">
P(n=ˆn  |i=ˆi) =
</equation>
<bodyText confidence="0.915026333333333">
Finally, we assume there are H possible click val-
ues, corresponding to H Web hosts. A click on a
host is similarly determined by an intent i and is gov-
erned by parameter Ω across all K intents:
ΩI[h=ˆc]
ˆi,h , s.t. di
</bodyText>
<subsectionHeader confidence="0.998213">
3.2 Learning
</subsectionHeader>
<bodyText confidence="0.997814833333333">
Given a query corpus Q consisting of N inde-
pendently and identically distributed queries qj =
{nj1, ej, nj21 and their corresponding clicked hosts
cj, we estimate the parameters τ, Θ, Ψ, σ, Φ, and
Ω by maximizing the (log) probability of observing
Q. The log P (Q) can be written as:
</bodyText>
<equation confidence="0.643922">
Pj(t, i  |q, c) log Pj(q, c, t, i)
</equation>
<bodyText confidence="0.999791555555556">
In the above equation, Pj(t, i  |q, c) is the poste-
rior distribution over types and user intents for the
jth query. We use the Expectation-Maximization
(EM) algorithm to estimate the parameters. The
parameter updates are obtained by computing the
derivative of log P(Q) with respect to each parame-
ter, and setting the resultant to 0.
The update for τ is given by the average of the
posterior distributions over the types:
</bodyText>
<equation confidence="0.987282324324324">
T
P(t=ˆt) = Y
j=1
T
X
j=1
τj = 1
P(i=ˆi  |t=ˆt) = YK
j=1
K
X Θt,j = 1
j=1
E
P(e=ˆe  |t=ˆt) = Y
j=1
E
X Ψt,j = 1
j=1
V ΦI[v=ˆn] , s.t. di V Φi,v = 1
Y i, v X
v=1 v=1
H
P(c=ˆc  |i=ˆi) = Y
h=1
XH Ωi,h = 1
h=1
N
X
j=1
logP(Q) =
X
t,i
566
PN Pi Pj(t=ˆt, i  |q, c)
j=1
PN Pt,i Pj(t, i  |q, c)
j=1
</equation>
<bodyText confidence="0.97896475">
For a fixed type t, the update for θt is given by
the weighted average of the latent intents, where the
weights are the posterior distributions over the types,
for each query:
</bodyText>
<equation confidence="0.993684">
PNj=1 Pj(t=ˆt, i=ˆi  |q, c)
Θˆt,ˆi = N
Pj=1 Pi Pj (t=ˆt, i  |q, c)
</equation>
<bodyText confidence="0.989512">
Similarly, we can update Ψ, the parameters that
govern the distribution over entities for each type:
</bodyText>
<equation confidence="0.996047">
PN Pi Pj(t=ˆt, i  |q, c)I[ej=ˆe]
j=1
Ψˆt,ˆe = PN Pi Pj(t=ˆt, i  |q, c)
j=1
</equation>
<bodyText confidence="0.99995075">
Now, for a fixed user intent i, the update for
ωi is given by the weighted average of the clicked
hosts, where the weights are the posterior distribu-
tions over the intents, for each query:
</bodyText>
<equation confidence="0.9983945">
PN P t P j(t, i=ˆi  |q, c)I[cj=ˆc]
j=1
Ωˆi,ˆc =N ˆ
Pj=1 Pt Pj (t, i=i  |q, c)
</equation>
<bodyText confidence="0.999392666666667">
Similarly, we can update Φ and σ, the parameters
that govern the distribution over query refiners and
empty contexts for each intent, as:
</bodyText>
<table confidence="0.549358166666667">
�ˆi,ˆn� El 1 Et Pj (t,i=ˆi|q,c) hI[nj1 j1=1]+I[nj2 2=1]i
E� 1 Et Pj (t,i=ˆi|q,c) hI[sj1=1]+I[sj2=1]i
and
Pj=1 Pt Pj (t, i=ˆi  |q, c) hI[s1=1] + I[s2=1]i
2PN Pt Pj(t, i=ˆi  |q, c)
j=1
</table>
<subsectionHeader confidence="0.994652">
3.3 Decoding
</subsectionHeader>
<bodyText confidence="0.999985538461539">
Given a query/click pair {q, c}, and the learned IM
model, we can apply Bayes’ rule to find the poste-
rior distribution, P(t, i  |q, c), over the types and
intents, as it is proportional to P(t, i, q, c). We com-
pute this quantity exactly by evaluating the joint for
each combination of t and i, and the observed values
of q and c.
It is important to note that at runtime when a new
query is issued, we have to resolve the entity in the
absence of any observed click. However, we do have
access to historical click probabilities, P(c  |q).
We use this information to compute P(t  |q) by
marginalizing over i as follows:
</bodyText>
<equation confidence="0.997231333333333">
H
P(t  |q) = X X P(t, i  |q, cj)P(cj  |q) (1)
i j=1
</equation>
<subsectionHeader confidence="0.822457">
3.4 Comparative Models
</subsectionHeader>
<bodyText confidence="0.992621272727273">
Figure 1 also illustrates the current state-of-the-art
model Guo&apos;09 (Guo et al., 2009), described in Sec-
tion 2.4, which utilizes only query refinement words
to infer entity type distributions. Two extensions to
this model that we further study in this paper are also
shown: Model M0 adds the empty context switch
parameter and Model M1 further adds click infor-
mation. In the interest of space, we omit the update
equations for these models, however they are triv-
ial to adapt from the derivations of Model IM pre-
sented in Sections 3.1 and 3.2.
</bodyText>
<sectionHeader confidence="0.502578" genericHeader="method">
3.5 Discussion
</sectionHeader>
<bodyText confidence="0.999786961538462">
Full Bayesian Treatment: In the above mod-
els, we learn point estimates for the parameters
(τ, Θ, Ψ, σ, Φ, Ω).One can take a Bayesian ap-
proach and treat these parameters as variables (for
instance, with Dirichlet and Beta prior distribu-
tions), and perform Bayesian inference. However,
exact inference will become intractable and we
would need to resort to methods such as variational
inference or sampling. We found this extension un-
necessary, as we had a sufficient amount of training
data to estimate all parameters reliably. In addition,
our approach enabled us to learn (and perform infer-
ence in) the model with large amounts of data with
reasonable computing time.
Fitting to an existing Knowledge Base: Al-
though in general our model decodes type distribu-
tions for arbitrary entities, in many practical cases
it is beneficial to constrain the types to those ad-
missible in a fixed knowledge base (such as Free-
base). As an example, if the entity is “ymca”,
admissible types may include song, place, and
educational institution. When resolving
types, during inference, one can restrict the search
space to only these admissible types. A desirable
side effect of this strategy is that only valid ambigu-
ities are captured in the posterior distribution.
</bodyText>
<equation confidence="0.995641">
τˆt =
σˆi =
</equation>
<page confidence="0.997534">
567
</page>
<sectionHeader confidence="0.995853" genericHeader="method">
4 Evaluation Methodology
</sectionHeader>
<bodyText confidence="0.998929666666667">
We refer to QL as a set of English Web search
queries issued to a commercial search engine over
a period of several months.
</bodyText>
<subsectionHeader confidence="0.9854">
4.1 Entity Inventory
</subsectionHeader>
<bodyText confidence="0.999980222222222">
Although our models generalize to any entity reposi-
tory, we experiment in this paper with entities cover-
ing a wide range of web search queries, coming from
73 types in Freebase. We arrived at these types by
grepping for all entities in Freebase within QL, fol-
lowing the procedure described in Section 4.2, and
then choosing the top most frequent types such that
50% of the queries are covered by an entity of one
of these types1.
</bodyText>
<subsectionHeader confidence="0.992672">
4.2 Training Data Construction
</subsectionHeader>
<bodyText confidence="0.996338306122449">
In order to learn type distributions by jointly mod-
eling user intents and a large number of types, we
require a large set of training examples containing
tagged entities and their potential types. Unlike in
Guo et al. (2009), we need a method to automatically
label QL to produce these training cases since man-
ual annotation is impossible for the range of entities
and types that we consider. Reliably recognizing en-
tities in queries is not a solved problem. However,
for training we do not require high coverage of en-
tities in QL, so high precision on a sizeable set of
query instances can be a proper proxy.
To this end, we collect candidate entities in
QL via simple string matching on Freebase entity
strings within our preselected 73 types. To achieve
high precision from this initial (high-recall, low-
precision) candidate set we use a number of heuris-
tics to only retain highly likely entities. The heuris-
tics include retaining only matches on entities that
appear capitalized more than 50% in their occur-
rences in Wikipedia. Also, a standalone score fil-
ter (Jain and Pennacchiotti, 2011) of 0.9 is used,
which is based on the ratio of string occurrence as
1In this process, we omitted any non-core Freebase type
(e.g., /user/* and /base/*), types used for representation
(e.g., /common/* and /type/*), and too general types (e.g.,
/people/person and /location/location) identi-
fied by if a type contains multiple other prominent subtypes.
Finally, we conflated seven of the types that overlapped with
each other into four types (such as /book/written work
and /book/book).
an exact match in queries to how often it occurs as a
partial match.
The resulting queries are further filtered by keep-
ing only those where the pre- and post-entity con-
texts (n1 and n2) were empty or a single word (ac-
counting for a very large fraction of the queries). We
also eliminate entries with clicked hosts that have
been clicked fewer than 100 times over the entire
QL. Finally, for training we filter out any query with
an entity that has more than two potential types2.
This step is performed to reduce recognition er-
rors by limiting the number of potential ambiguous
matches. We experimented with various thresholds
on allowable types and settled on the value two.
The resulting training data consists of several mil-
lion queries, 73 different entity types, and approx-
imately 135K different entities, 100K different re-
finer words, and 40K clicked hosts.
</bodyText>
<subsectionHeader confidence="0.999911">
4.3 Test Set Annotation
</subsectionHeader>
<bodyText confidence="0.9993392">
We sampled two datasets, HEAD and TAIL, each
consisting of 500 queries containing an entity be-
longing to one of the 73 types in our inventory, from
a frequency-weighted random sample and a uniform
random sample of QL, respectively.
We conducted a user study to establish a gold
standard of the correct entity types in each query.
A total of seven different independent and paid pro-
fessional annotators participated in the study. For
each query in our test sets, we displayed the query,
associated clicked host, and entity to the annotator,
along with a list of permissible types from our type
inventory. The annotator is tasked with identifying
all applicable types from that list, or marking the test
case as faulty because of an error in entity identifi-
cation, bad click host (e.g. dead link) or bad query
(e.g. non-English). This resulted in 2,092 test cases
({query, entity, type}-tuples). Each test case was
annotated by two annotators. Inter-annotator agree-
ment as measured by Fleiss’ r. was 0.445 (0.498
on HEAD and 0.386 on TAIL), considered moderate
agreement.
From HEAD and TAIL, we eliminated three cat-
egories of queries that did not offer any interesting
type disambiguation opportunities:
</bodyText>
<listItem confidence="0.91179">
• queries that contained entities with only one
</listItem>
<footnote confidence="0.903283">
2For testing we did not omit any entity or type.
</footnote>
<page confidence="0.982666">
568
</page>
<table confidence="0.999274714285714">
nDCG HEAD Prec@1
MAP MAPW
BFB 0.71 0.60 0.45 0.30
Guo&apos;09 0.791 0.711 0.621 0.511
M0 0.791 0.721 0.651 0.521
M1 0.83$ 0.76$ 0.72$ 0.61$
IM 0.87$ 0.82$ 0.77$ 0.73$
TAIL
nDCG MAP MAPW Prec@1
0.73 0.64 0.49 0.35
0.801 0.731 0.661 0.521
0.821 0.751 0.671 0.571
0.811 0.741 0.671 0.551
0.801 0.721 0.661 0.521
</table>
<tableCaption confidence="0.987752333333333">
Table 2: Model analysis on HEAD and TAIL. t indicates statistical significance over BFB, and t over both BFB and
Guo&apos;09. Bold indicates statistical significance over all non-bold models in the column. Significance is measured
using the Student’s t-test at 95% confidence.
</tableCaption>
<bodyText confidence="0.853355">
potential type from our inventory;
</bodyText>
<listItem confidence="0.93309875">
• queries where the annotators rated all potential
types as good; and
• queries where judges rated none of the potential
types as good
</listItem>
<bodyText confidence="0.922685333333333">
The final test sets consist of 105 head queries with
359 judged entity types and 98 tail queries with 343
judged entity types.
</bodyText>
<subsectionHeader confidence="0.996775">
4.4 Metrics
</subsectionHeader>
<bodyText confidence="0.999954">
Our task is a ranking task and therefore the classic
IR metrics nDCG (normalized discounted cumula-
tive gain) and MAP (mean average precision) are
applicable (Manning et al., 2008).
Both nDCG and MAP are sensitive to the rank
position, but not the score (probability of a type) as-
sociated with each rank, S(r). We therefore also
evaluate a weighted mean average precision score
MAPW, which replaces the precision component
of MAP, P(r), for the rth ranked type by:
</bodyText>
<equation confidence="0.895613">
P(r) - Er�=1 I(ˆr)S(ˆr) (2)
- �T=1 S(ˆr)
</equation>
<bodyText confidence="0.9999455">
where I(r) indicates if the type at rank r is judged
correct.
Our fourth metric is Prec@1, i.e. the precision of
only the top-ranked type of each query. This is espe-
cially suitable for applications where a single sense
must be determined.
</bodyText>
<subsectionHeader confidence="0.998862">
4.5 Model Settings
</subsectionHeader>
<bodyText confidence="0.999991181818182">
We trained all models in Figure 1 using the training
data from Section 4.2 over 100 EM iterations, with
two folds per model. For Model IM, we varied the
number of user intents (K) in intervals from 100 to
400 (see Figure 3), under the assumption that multi-
ple intents would exist per entity type.
We compare our results against two baselines.
The first baseline is an assignment of Freebase types
according to their frequency in our query set BFB,
and the second is Model Guo&apos;09 (Guo et al., 2009)
illustrated in Figure 1.
</bodyText>
<sectionHeader confidence="0.994559" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.998875357142857">
Table 2 lists the performance of each model on the
HEAD and TAIL sets over each metric defined in
Section 4.4. On head queries, the addition of the
empty context parameter Q and click signal Ω to-
gether (Model M1) significantly outperforms both
the baseline and the state-of-the-art model Guo&apos;09.
Further modeling the user intent in Model IM re-
sults in significantly better performance over all
models and across all metrics. Model IM shows
its biggest gains in the first position of its ranking as
evidenced by the Prec@1 metric.
We observe a different behavior on tail queries
where all models significantly outperform the base-
line BFB, but are not significantly different from
each other. In short, the strength of our proposed
model is in improving performance on the head at
no noticeable cost in the tail.
We separately tested the effect of adding the
empty context parameter Q. Figure 2 illustrates the
result on the HEAD data. Across all metrics, Q im-
proved performance over all models3. The more
expressive models benefitted more than the less ex-
pressive ones.
Table 2 reports results for Model IM using K =
200 user intents. This was determined by varying
K and selecting the top-performing value. Figure 3
illustrates the performance of Model IM with dif-
ferent values of K on the HEAD.
</bodyText>
<footnote confidence="0.9206325">
3Note that model M0 is just the addition of the Q parameter
over Guo&apos;09.
</footnote>
<page confidence="0.994315">
569
</page>
<figureCaption confidence="0.99912225">
Figure 2: The switch parameter u improves performance
of every model and metric.
Figure 3: Model performance vs. the number of latent
intents (K).
</figureCaption>
<bodyText confidence="0.999944464285715">
Our models can also assign a prior type distribu-
tion to each entity by further marginalizing Eq. 1
over query contexts n1 and n2. We measured the
quality of our learned type priors using the subset
of queries in our HEAD test set that consisted of
only an entity without any refiners. The results for
Model IM were: nDCG = 0.86, MAP = 0.80,
MAPW = 0.75, and Prec@1 = 0.70. All met-
rics are statistically significantly better than BFB,
Guo&apos;09 and M0, with 95% confidence. Compared
to Model M1, Model IM is statistically signifi-
cantly better on Prec@1 and not significantly dif-
ferent on the other metrics.
Discussion and Error Analysis: Contrary to
our results, we had expected improvements for
both HEAD and TAIL. Inspection of the TAIL
queries revealed that entities were greatly skewed
towards people (e.g., actor, author, and
politician). Analysis of the latent user in-
tent parameter O in Model IM showed that most
people types had most of their probability mass
assigned to the same three generic and common in-
tents for people types: ‘see pictures of’, ‘find bio-
graphical information about’, and ‘see video of’. In
other words, latent intents in Model IM are over-
expressive and they do not help in differentiating
people types.
The largest class of errors came from queries
bearing an entity with semantically very similar
types where our highest ranked type was not judged
correct by the annotators. For example, for the
query “philippine daily inquirer” our system ranked
newspaper ahead of periodical but a judge
rejected the former and approved the latter. For
“ikea catalogue”, our system ranked magazine
ahead of periodical, but again a judge rejected
magazine in favor of periodical.
An interesting success case in the TAIL is high-
lighted by two queries involving the entity “ymca”,
which in our data can either be a song, place,
or educational institution. Our system
learns the following priors: 0.63, 0.29, and 0.08,
respectively. For the query “jamestown ymca ny”,
IM correctly classified “ymca” as a place and for
the query “ymca palomar” it correctly classified it
as an educational institution. We further
issued the query “ymca lyrics” and the type song
was then highest ranked.
Our method is generalizable to any entity collec-
tion. Since our evaluation focused on the Freebase
collection, it remains an open question how noise
level, coverage, and breadth in a collection will af-
fect our model performance. Finally, although we
do not formally evaluate it, it is clear that training
our model on different time spans of queries should
lead to type distributions adapted to that time period.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999844875">
Jointly modeling the interplay between the under-
lying user intents and entity types in web search
queries shows significant improvements over the
current state of the art on the task of resolving entity
types in head queries. At the same time, no degrada-
tion in tail queries is observed. Our proposed models
can be efficiently trained using an EM algorithm and
can be further used to assign prior type distributions
to entities in an existing knowledge base and to in-
sert new entities into it.
Although this paper leverages latent intents in
search queries, it stops short of understanding the
nature of the intents. It remains an open problem
to characterize and enumerate intents and to iden-
tify the types of queries that benefit most from intent
models.
</bodyText>
<figure confidence="0.996734090909091">
Relative gain of switch vs. no switch
0.16
0.14
0.12
0.08
0.06
0.04
0.02
0.1
0
M0 M1 IM
Effect of Empty Switch Parameter () on HEAD
No switch
nDCG
MAP
MAPW
Prec@1
0.95
0.85
0.75
0.65
0.9
0.8
0.7
0.6
1
100 150 200 300 400
K
Model IM - Varying K (latent intents)
nDCG
MAP
MAPW
Prec@1
</figure>
<page confidence="0.979653">
570
</page>
<sectionHeader confidence="0.994095" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999888867924528">
Enrique Alfonseca, Marius Pasca, and Enrique Robledo-
Arnuncio. 2010. Acquisition of instance attributes
via labeled and related instances. In Proceedings of
SIGIR-10, pages 58–65, New York, NY, USA.
Ricardo Baeza-Yates, Carlos Hurtado, and Marcelo Men-
doza. 2004. Query recommendation using query logs
in search engines. In EDBT Workshops, Lecture Notes
in Computer Science, pages 588–596. Springer.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collabo-
ratively created graph database for structuring human
knowledge. In Proceedings of SIGMOD ’08, pages
1247–1250, New York, NY, USA.
Andrei Broder. 2002. A taxonomy of web search. SIGIR
Forum, 36:3–10.
Mark James Carman, Fabio Crestani, Morgan Harvey,
and Mark Baillie. 2010. Towards query log based per-
sonalization using topic models. In CIKM’10, pages
1849–1852.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: An
experimental study. volume 165, pages 91–134.
Jianfeng Gao, Kristina Toutanova, and Wen-tau Yih.
2011. Clickthrough-based latent semantic models for
web search. In Proceedings of SIGIR ’11, pages 675–
684, New York, NY, USA. ACM.
Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009.
Named entity recognition in query. In Proceedings
of SIGIR-09, pages 267–274, New York, NY, USA.
ACM.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th International Conference on Computational
Linguistics, pages 539–545.
Jian Hu, Gang Wang, Frederick H. Lochovsky, Jian tao
Sun, and Zheng Chen. 2009. Understanding user’s
query intent with wikipedia. In WWW, pages 471–480.
Alpa Jain and Marco Pennacchiotti. 2011. Domain-
independent entity extraction from web search query
logs. In Proceedings of WWW ’11, pages 63–64, New
York, NY, USA. ACM.
Bernard J. Jansen, Danielle L. Booth, and Amanda Spink.
2007. Determining the user intent of web search en-
gine queries. In Proceedings of WWW ’07, pages
1149–1150, New York, NY, USA. ACM.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of ACL.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Sch¨utze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1990.
Wordnet: An on-line lexical database. volume 3,
pages 235–244.
Marius Pas¸ca. 2007. Weakly-supervised discovery of
named entities using web search queries. In Proceed-
ings of the sixteenth ACM conference on Conference
on information and knowledge management, CIKM
’07, pages 683–690, New York, NY, USA. ACM.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In SIGKDD, pages 613–619, Ed-
monton, Canada.
Patrick Pantel, Deepak Ravichandran, and Eduard Hovy.
2004. Towards terascale knowledge acquisition. In
COLING, pages 771–777.
Filip Radlinski, Martin Szummer, and Nick Craswell.
2010. Inferring query intent from reformulations and
clicks. In Proceedings of the 19th international con-
ference on World wide web, WWW ’10, pages 1171–
1172, New York, NY, USA. ACM.
Altaf Rahman and Vincent Ng. 2010. Inducing fine-
grained semantic classes via hierarchical and collec-
tive classification. In Proceedings of COLING, pages
931–939.
Alan Ritter, Stephen Soderland, and Oren Etzioni. 2009.
What is this, anyway: Automatic hypernym discov-
ery. In Proceedings of AAAI-09 Spring Symposium on
Learning by Reading and Learning to Read, pages 88–
93.
Daniel E. Rose and Danny Levinson. 2004. Under-
standing user goals in web search. In Proceedings of
the 13th international conference on World Wide Web,
WWW ’04, pages 13–19, New York, NY, USA. ACM.
Stefan R¨ud, Massimiliano Ciaramita, Jens M¨uller, and
Hinrich Sch¨utze. 2011. Piggyback: Using search
engines for robust cross-domain named entity recog-
nition. In Proceedings of ACL ’11, pages 965–975,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
Hinrich Sch¨utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24:97–123,
March.
Satoshi Sekine and Hisami Suzuki. 2007. Acquiring on-
tological knowledge from query logs. In Proceedings
of the 16th international conference on World Wide
Web, WWW ’07, pages 1223–1224, New York, NY,
USA. ACM.
Xiaoxin Yin and Sarthak Shah. 2010. Building taxon-
omy of web search intents for name entity queries. In
WWW, pages 1001–1010.
Z. Zhang and O. Nasraoui. 2006. Mining search en-
gine query logs for query recommendation. In WWW,
pages 1039–1040.
</reference>
<page confidence="0.997843">
571
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.539759">
<title confidence="0.99999">Mining Entity Types from Query Logs via User Intent Modeling</title>
<author confidence="0.998588">Patrick Pantel</author>
<affiliation confidence="0.999401">Microsoft Research</affiliation>
<address confidence="0.880875">One Microsoft Redmond, WA 98052,</address>
<email confidence="0.999353">ppantel@microsoft.com</email>
<author confidence="0.969389">Thomas</author>
<affiliation confidence="0.9998125">Computer Science &amp; University of</affiliation>
<address confidence="0.998663">Seattle, WA 98195,</address>
<email confidence="0.999788">tlin@cs.washington.edu</email>
<author confidence="0.99996">Michael Gamon</author>
<affiliation confidence="0.999796">Microsoft Research</affiliation>
<address confidence="0.879728">One Microsoft Redmond, WA 98052,</address>
<email confidence="0.999735">mgamon@microsoft.com</email>
<abstract confidence="0.998090631578947">We predict entity type distributions in Web search queries via probabilistic inference in graphical models that capture how entitybearing queries are generated. We jointly model the interplay between latent user intents that govern queries and unobserved entity types, leveraging observed signals from query formulations and document clicks. We apply the models to resolve entity types in new queries and to assign prior type distributions over an existing knowledge base. Our models are efficiently trained using maximum likelihood estimation over millions of real-world Web search queries. We show that modeling user intent significantly improves entity type resolution for head queries over the state of the art, on several metrics, without degradation in tail query performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Enrique Alfonseca</author>
<author>Marius Pasca</author>
<author>Enrique RobledoArnuncio</author>
</authors>
<title>Acquisition of instance attributes via labeled and related instances.</title>
<date>2010</date>
<booktitle>In Proceedings of SIGIR-10,</booktitle>
<pages>58--65</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="6436" citStr="Alfonseca et al. (2010)" startWordPosition="1007" endWordPosition="1010">contain billions of entries, they provide an independent signal from text corpora, their timestamps allow the learning of type priors at specific points in time, and they can contain information such as clickthroughs that are not found in text corpora. Sekine and Suzuki (2007) used frequency features on context words in query logs to learn semantic classes of entities. Pas¸ca (2007) used extraction techniques to mine instances of semantic classes from query logs. R¨ud et al. (2011) found that cross-domain generalizations learned from Web search results are applicable to NLP tasks such as NER. Alfonseca et al. (2010) mined query logs to find attributes of entity instances. However, these projects did not learn relative probabilities of different senses. 2.3 User Intents in Search Learning from query logs also allows us to leverage the concept of user intents. When users submit search queries, they often have specific intents in mind. Broder (2002) introduced 3 top level intents: Informational (e.g., wanting to learn), Navigational (wanting to visit a site), and Transactional (e.g., wanting to buy/sell). Rose and Levinson (2004) further divided these into finer-grained subcategories, and Yin and Shah (2010</context>
</contexts>
<marker>Alfonseca, Pasca, RobledoArnuncio, 2010</marker>
<rawString>Enrique Alfonseca, Marius Pasca, and Enrique RobledoArnuncio. 2010. Acquisition of instance attributes via labeled and related instances. In Proceedings of SIGIR-10, pages 58–65, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ricardo Baeza-Yates</author>
<author>Carlos Hurtado</author>
<author>Marcelo Mendoza</author>
</authors>
<title>Query recommendation using query logs in search engines.</title>
<date>2004</date>
<booktitle>In EDBT Workshops, Lecture Notes in Computer Science,</booktitle>
<pages>588--596</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="5667" citStr="Baeza-Yates et al., 2004" startWordPosition="879" endWordPosition="882">. This is especially problematic, for example, if one wishes to leverage Freebase but only needs the most commonly used senses (e.g., Al Gore is a US Vice President), rather than all possible obscure senses (Freebase contains 30+ senses, including ones such as Impersonated Celebrity and Quotation Subject). In scenarios such as this, our proposed method can increase the usability of systems that find semantic classes. We also expand upon text corpora methods in that the type priors can adapt to Web search signals. 2.2 Query Log Mining Query logs have traditionally been mined to improve search (Baeza-Yates et al., 2004; Zhang and Nasraoui, 2006), but they can also be used in place of (or in addition to) text corpora for learning semantic classes. Query logs can contain billions of entries, they provide an independent signal from text corpora, their timestamps allow the learning of type priors at specific points in time, and they can contain information such as clickthroughs that are not found in text corpora. Sekine and Suzuki (2007) used frequency features on context words in query logs to learn semantic classes of entities. Pas¸ca (2007) used extraction techniques to mine instances of semantic classes fro</context>
</contexts>
<marker>Baeza-Yates, Hurtado, Mendoza, 2004</marker>
<rawString>Ricardo Baeza-Yates, Carlos Hurtado, and Marcelo Mendoza. 2004. Query recommendation using query logs in search engines. In EDBT Workshops, Lecture Notes in Computer Science, pages 588–596. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGMOD ’08,</booktitle>
<pages>1247--1250</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="4671" citStr="Bollacker et al., 2008" startWordPosition="717" endWordPosition="720">to these results. 2 Related Work 2.1 Finding Semantic Classes A closely related problem is that of finding the semantic classes of entities. Automatic techniques for finding semantic classes include unsupervised clustering (Sch¨utze, 1998; Pantel and Lin, 2002), hyponym patterns (Hearst, 1992; Pantel et al., 2004; Kozareva et al., 2008), extraction patterns (Etzioni et al., 2005), hidden Markov models (Ritter et al., 2009), classification (Rahman and Ng, 2010) and many others. These techniques typically leverage large corpora, while projects such as WordNet (Miller et al., 1990) and Freebase (Bollacker et al., 2008) have employed editors to manually enumerate words and entities with their semantic classes. The aforementioned methods do not use query logs or explicitly determine the relative probabilities of different entity senses. A method might learn that there is independently a high chance of eBay being a website and an employer, but does not specify which usage is more common. This is especially problematic, for example, if one wishes to leverage Freebase but only needs the most commonly used senses (e.g., Al Gore is a US Vice President), rather than all possible obscure senses (Freebase contains 30</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of SIGMOD ’08, pages 1247–1250, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Broder</author>
</authors>
<title>A taxonomy of web search.</title>
<date>2002</date>
<journal>SIGIR Forum,</journal>
<pages>36--3</pages>
<contexts>
<context position="6773" citStr="Broder (2002)" startWordPosition="1065" endWordPosition="1066">tic classes of entities. Pas¸ca (2007) used extraction techniques to mine instances of semantic classes from query logs. R¨ud et al. (2011) found that cross-domain generalizations learned from Web search results are applicable to NLP tasks such as NER. Alfonseca et al. (2010) mined query logs to find attributes of entity instances. However, these projects did not learn relative probabilities of different senses. 2.3 User Intents in Search Learning from query logs also allows us to leverage the concept of user intents. When users submit search queries, they often have specific intents in mind. Broder (2002) introduced 3 top level intents: Informational (e.g., wanting to learn), Navigational (wanting to visit a site), and Transactional (e.g., wanting to buy/sell). Rose and Levinson (2004) further divided these into finer-grained subcategories, and Yin and Shah (2010) built hierarchical taxonomies of search intents. Jansen et al. (2007), Hu et al. (2009), and Radlinski et al. (2010) examined how to infer the intent of queries. We are not aware of any other work that has leveraged user intents to learn type distributions. 2.4 Topic Modeling on Query Logs The closest work to ours is Guo et al.’s (20</context>
</contexts>
<marker>Broder, 2002</marker>
<rawString>Andrei Broder. 2002. A taxonomy of web search. SIGIR Forum, 36:3–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark James Carman</author>
<author>Fabio Crestani</author>
<author>Morgan Harvey</author>
<author>Mark Baillie</author>
</authors>
<title>Towards query log based personalization using topic models.</title>
<date>2010</date>
<booktitle>In CIKM’10,</booktitle>
<pages>1849--1852</pages>
<contexts>
<context position="8273" citStr="Carman et al. (2010)" startWordPosition="1309" endWordPosition="1312">e, Game, Book, Music) for each entity, we employ over 70 popular types, allowing much greater coverage of real entities and their types. Because they only had four types, they were able to hand label their training data. In contrast, our system self-labels training examples by searching query logs for high-likelihood entities, and must handle any errors introduced by this process. Our models also expand upon theirs by jointly modeling 564 entity type with latent user intents, and by incorporating click signals. Other projects have also demonstrated the utility of topic modeling on query logs. Carman et al. (2010) modeled users and clicked documents to personalize search results and Gao et al. (2011) applied topic models to query logs in order to improve document ranking for search. 3 Joint Model of Types and User Intents We turn our attention now to the task of mining the type distributions of entities and of resolving the type of an entity in a particular query context. Our approach is to probabilistically describe how entitybearing queries are generated in Web search. We theorize that search queries are governed by a latent user intent, which in turn influences the entity types, the choice of query </context>
</contexts>
<marker>Carman, Crestani, Harvey, Baillie, 2010</marker>
<rawString>Mark James Carman, Fabio Crestani, Morgan Harvey, and Mark Baillie. 2010. Towards query log based personalization using topic models. In CIKM’10, pages 1849–1852.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>AnaMaria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: An experimental study.</title>
<date>2005</date>
<volume>165</volume>
<pages>91--134</pages>
<contexts>
<context position="4430" citStr="Etzioni et al., 2005" startWordPosition="680" endWordPosition="683">a robust implementation of our models, using real-world query data, and a realistic large set of entity types. • We empirically show that our models outperform the state of the art and that modeling latent intent contributes significantly to these results. 2 Related Work 2.1 Finding Semantic Classes A closely related problem is that of finding the semantic classes of entities. Automatic techniques for finding semantic classes include unsupervised clustering (Sch¨utze, 1998; Pantel and Lin, 2002), hyponym patterns (Hearst, 1992; Pantel et al., 2004; Kozareva et al., 2008), extraction patterns (Etzioni et al., 2005), hidden Markov models (Ritter et al., 2009), classification (Rahman and Ng, 2010) and many others. These techniques typically leverage large corpora, while projects such as WordNet (Miller et al., 1990) and Freebase (Bollacker et al., 2008) have employed editors to manually enumerate words and entities with their semantic classes. The aforementioned methods do not use query logs or explicitly determine the relative probabilities of different entity senses. A method might learn that there is independently a high chance of eBay being a website and an employer, but does not specify which usage i</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, AnaMaria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the web: An experimental study. volume 165, pages 91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Kristina Toutanova</author>
<author>Wen-tau Yih</author>
</authors>
<title>Clickthrough-based latent semantic models for web search.</title>
<date>2011</date>
<booktitle>In Proceedings of SIGIR ’11,</booktitle>
<pages>675--684</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="8361" citStr="Gao et al. (2011)" startWordPosition="1323" endWordPosition="1326">er coverage of real entities and their types. Because they only had four types, they were able to hand label their training data. In contrast, our system self-labels training examples by searching query logs for high-likelihood entities, and must handle any errors introduced by this process. Our models also expand upon theirs by jointly modeling 564 entity type with latent user intents, and by incorporating click signals. Other projects have also demonstrated the utility of topic modeling on query logs. Carman et al. (2010) modeled users and clicked documents to personalize search results and Gao et al. (2011) applied topic models to query logs in order to improve document ranking for search. 3 Joint Model of Types and User Intents We turn our attention now to the task of mining the type distributions of entities and of resolving the type of an entity in a particular query context. Our approach is to probabilistically describe how entitybearing queries are generated in Web search. We theorize that search queries are governed by a latent user intent, which in turn influences the entity types, the choice of query words, and the clicked hosts. We develop inference procedures to infer the prior type di</context>
</contexts>
<marker>Gao, Toutanova, Yih, 2011</marker>
<rawString>Jianfeng Gao, Kristina Toutanova, and Wen-tau Yih. 2011. Clickthrough-based latent semantic models for web search. In Proceedings of SIGIR ’11, pages 675– 684, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiafeng Guo</author>
<author>Gu Xu</author>
<author>Xueqi Cheng</author>
<author>Hang Li</author>
</authors>
<title>Named entity recognition in query.</title>
<date>2009</date>
<booktitle>In Proceedings of SIGIR-09,</booktitle>
<pages>267--274</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="12663" citStr="Guo et al., 2009" startWordPosition="2104" endWordPosition="2107">s. Instead, we choose to include these dependencies at the time of inference, as described later. Recall that q = {n1, e, n2} and let s = {s1, s2}, where s1 = 1 if n1 is not empty and s2 = 1 if n2 is not empty, 0 otherwise. The joint probability of the model is the product of the conditional distributions, as given by: 565 f t E T e n y t 2 Guo’09 Q tt E s T s e y tt n 2 Q Model M0 ff T tt E s T s e n 2 y tt c Q Model MS ff w T T ss y T K e s tt tt n 2 i i c Q Model IM w q f T K K Figure 1: Graphical models for generating entity-bearing queries. Guo&apos;09 represents the current state of the art (Guo et al., 2009). Models M0 and M1 add an empty context switch and click information, respectively. Model IM further constrains the query by the latent user intent. P(t, i, q, c |τ, Θ, Ψ, σ, Φ, Ω) = P(t |τ)P(i |t, Θ)P(e |t, Ψ)P(c |i, Ω) 2 Y P(nj |i, Φ)I[sj=1]P(sj|i, σ) j=1 We now define each of the terms in the joint distribution. Let T be the number of entity types. The probability of generating a type t is governed by a multinomial with probability vector τ: τI[j=ˆt] , s.t. j where I is an indicator function set to 1 if its condition holds, and 0 otherwise. Let K be the number of latent user intents that go</context>
<context position="17169" citStr="Guo et al., 2009" startWordPosition="2982" endWordPosition="2985">tents, as it is proportional to P(t, i, q, c). We compute this quantity exactly by evaluating the joint for each combination of t and i, and the observed values of q and c. It is important to note that at runtime when a new query is issued, we have to resolve the entity in the absence of any observed click. However, we do have access to historical click probabilities, P(c |q). We use this information to compute P(t |q) by marginalizing over i as follows: H P(t |q) = X X P(t, i |q, cj)P(cj |q) (1) i j=1 3.4 Comparative Models Figure 1 also illustrates the current state-of-the-art model Guo&apos;09 (Guo et al., 2009), described in Section 2.4, which utilizes only query refinement words to infer entity type distributions. Two extensions to this model that we further study in this paper are also shown: Model M0 adds the empty context switch parameter and Model M1 further adds click information. In the interest of space, we omit the update equations for these models, however they are trivial to adapt from the derivations of Model IM presented in Sections 3.1 and 3.2. 3.5 Discussion Full Bayesian Treatment: In the above models, we learn point estimates for the parameters (τ, Θ, Ψ, σ, Φ, Ω).One can take a Baye</context>
<context position="19764" citStr="Guo et al. (2009)" startWordPosition="3424" endWordPosition="3427">nt in this paper with entities covering a wide range of web search queries, coming from 73 types in Freebase. We arrived at these types by grepping for all entities in Freebase within QL, following the procedure described in Section 4.2, and then choosing the top most frequent types such that 50% of the queries are covered by an entity of one of these types1. 4.2 Training Data Construction In order to learn type distributions by jointly modeling user intents and a large number of types, we require a large set of training examples containing tagged entities and their potential types. Unlike in Guo et al. (2009), we need a method to automatically label QL to produce these training cases since manual annotation is impossible for the range of entities and types that we consider. Reliably recognizing entities in queries is not a solved problem. However, for training we do not require high coverage of entities in QL, so high precision on a sizeable set of query instances can be a proper proxy. To this end, we collect candidate entities in QL via simple string matching on Freebase entity strings within our preselected 73 types. To achieve high precision from this initial (high-recall, lowprecision) candid</context>
<context position="25455" citStr="Guo et al., 2009" startWordPosition="4386" endWordPosition="4389">of each query. This is especially suitable for applications where a single sense must be determined. 4.5 Model Settings We trained all models in Figure 1 using the training data from Section 4.2 over 100 EM iterations, with two folds per model. For Model IM, we varied the number of user intents (K) in intervals from 100 to 400 (see Figure 3), under the assumption that multiple intents would exist per entity type. We compare our results against two baselines. The first baseline is an assignment of Freebase types according to their frequency in our query set BFB, and the second is Model Guo&apos;09 (Guo et al., 2009) illustrated in Figure 1. 5 Experimental Results Table 2 lists the performance of each model on the HEAD and TAIL sets over each metric defined in Section 4.4. On head queries, the addition of the empty context parameter Q and click signal Ω together (Model M1) significantly outperforms both the baseline and the state-of-the-art model Guo&apos;09. Further modeling the user intent in Model IM results in significantly better performance over all models and across all metrics. Model IM shows its biggest gains in the first position of its ranking as evidenced by the Prec@1 metric. We observe a differen</context>
</contexts>
<marker>Guo, Xu, Cheng, Li, 2009</marker>
<rawString>Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009. Named entity recognition in query. In Proceedings of SIGIR-09, pages 267–274, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="4341" citStr="Hearst, 1992" startWordPosition="668" endWordPosition="669"> for Computational Linguistics • We propose an efficient learning technique and a robust implementation of our models, using real-world query data, and a realistic large set of entity types. • We empirically show that our models outperform the state of the art and that modeling latent intent contributes significantly to these results. 2 Related Work 2.1 Finding Semantic Classes A closely related problem is that of finding the semantic classes of entities. Automatic techniques for finding semantic classes include unsupervised clustering (Sch¨utze, 1998; Pantel and Lin, 2002), hyponym patterns (Hearst, 1992; Pantel et al., 2004; Kozareva et al., 2008), extraction patterns (Etzioni et al., 2005), hidden Markov models (Ritter et al., 2009), classification (Rahman and Ng, 2010) and many others. These techniques typically leverage large corpora, while projects such as WordNet (Miller et al., 1990) and Freebase (Bollacker et al., 2008) have employed editors to manually enumerate words and entities with their semantic classes. The aforementioned methods do not use query logs or explicitly determine the relative probabilities of different entity senses. A method might learn that there is independently </context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th International Conference on Computational Linguistics, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian Hu</author>
<author>Gang Wang</author>
<author>Frederick H Lochovsky</author>
<author>Jian tao Sun</author>
<author>Zheng Chen</author>
</authors>
<title>Understanding user’s query intent with wikipedia. In</title>
<date>2009</date>
<booktitle>WWW,</booktitle>
<pages>471--480</pages>
<contexts>
<context position="7125" citStr="Hu et al. (2009)" startWordPosition="1116" endWordPosition="1119">projects did not learn relative probabilities of different senses. 2.3 User Intents in Search Learning from query logs also allows us to leverage the concept of user intents. When users submit search queries, they often have specific intents in mind. Broder (2002) introduced 3 top level intents: Informational (e.g., wanting to learn), Navigational (wanting to visit a site), and Transactional (e.g., wanting to buy/sell). Rose and Levinson (2004) further divided these into finer-grained subcategories, and Yin and Shah (2010) built hierarchical taxonomies of search intents. Jansen et al. (2007), Hu et al. (2009), and Radlinski et al. (2010) examined how to infer the intent of queries. We are not aware of any other work that has leveraged user intents to learn type distributions. 2.4 Topic Modeling on Query Logs The closest work to ours is Guo et al.’s (2009) research on Named Entity Recognition in Queries. Given an entity-bearing query, they attempt to identify the entity and determine the type posteriors. Our work significantly scales up the type posteriors component of their work. While they only have four potential types (Movie, Game, Book, Music) for each entity, we employ over 70 popular types, </context>
</contexts>
<marker>Hu, Wang, Lochovsky, Sun, Chen, 2009</marker>
<rawString>Jian Hu, Gang Wang, Frederick H. Lochovsky, Jian tao Sun, and Zheng Chen. 2009. Understanding user’s query intent with wikipedia. In WWW, pages 471–480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alpa Jain</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Domainindependent entity extraction from web search query logs.</title>
<date>2011</date>
<booktitle>In Proceedings of WWW ’11,</booktitle>
<pages>63--64</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="20634" citStr="Jain and Pennacchiotti, 2011" startWordPosition="3571" endWordPosition="3574">r, for training we do not require high coverage of entities in QL, so high precision on a sizeable set of query instances can be a proper proxy. To this end, we collect candidate entities in QL via simple string matching on Freebase entity strings within our preselected 73 types. To achieve high precision from this initial (high-recall, lowprecision) candidate set we use a number of heuristics to only retain highly likely entities. The heuristics include retaining only matches on entities that appear capitalized more than 50% in their occurrences in Wikipedia. Also, a standalone score filter (Jain and Pennacchiotti, 2011) of 0.9 is used, which is based on the ratio of string occurrence as 1In this process, we omitted any non-core Freebase type (e.g., /user/* and /base/*), types used for representation (e.g., /common/* and /type/*), and too general types (e.g., /people/person and /location/location) identified by if a type contains multiple other prominent subtypes. Finally, we conflated seven of the types that overlapped with each other into four types (such as /book/written work and /book/book). an exact match in queries to how often it occurs as a partial match. The resulting queries are further filtered by </context>
</contexts>
<marker>Jain, Pennacchiotti, 2011</marker>
<rawString>Alpa Jain and Marco Pennacchiotti. 2011. Domainindependent entity extraction from web search query logs. In Proceedings of WWW ’11, pages 63–64, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard J Jansen</author>
<author>Danielle L Booth</author>
<author>Amanda Spink</author>
</authors>
<title>Determining the user intent of web search engine queries.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW ’07,</booktitle>
<pages>1149--1150</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7107" citStr="Jansen et al. (2007)" startWordPosition="1112" endWordPosition="1115">ances. However, these projects did not learn relative probabilities of different senses. 2.3 User Intents in Search Learning from query logs also allows us to leverage the concept of user intents. When users submit search queries, they often have specific intents in mind. Broder (2002) introduced 3 top level intents: Informational (e.g., wanting to learn), Navigational (wanting to visit a site), and Transactional (e.g., wanting to buy/sell). Rose and Levinson (2004) further divided these into finer-grained subcategories, and Yin and Shah (2010) built hierarchical taxonomies of search intents. Jansen et al. (2007), Hu et al. (2009), and Radlinski et al. (2010) examined how to infer the intent of queries. We are not aware of any other work that has leveraged user intents to learn type distributions. 2.4 Topic Modeling on Query Logs The closest work to ours is Guo et al.’s (2009) research on Named Entity Recognition in Queries. Given an entity-bearing query, they attempt to identify the entity and determine the type posteriors. Our work significantly scales up the type posteriors component of their work. While they only have four potential types (Movie, Game, Book, Music) for each entity, we employ over </context>
</contexts>
<marker>Jansen, Booth, Spink, 2007</marker>
<rawString>Bernard J. Jansen, Danielle L. Booth, and Amanda Spink. 2007. Determining the user intent of web search engine queries. In Proceedings of WWW ’07, pages 1149–1150, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
<author>Eduard Hovy</author>
</authors>
<title>Semantic class learning from the web with hyponym pattern linkage graphs.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="4386" citStr="Kozareva et al., 2008" startWordPosition="674" endWordPosition="677"> propose an efficient learning technique and a robust implementation of our models, using real-world query data, and a realistic large set of entity types. • We empirically show that our models outperform the state of the art and that modeling latent intent contributes significantly to these results. 2 Related Work 2.1 Finding Semantic Classes A closely related problem is that of finding the semantic classes of entities. Automatic techniques for finding semantic classes include unsupervised clustering (Sch¨utze, 1998; Pantel and Lin, 2002), hyponym patterns (Hearst, 1992; Pantel et al., 2004; Kozareva et al., 2008), extraction patterns (Etzioni et al., 2005), hidden Markov models (Ritter et al., 2009), classification (Rahman and Ng, 2010) and many others. These techniques typically leverage large corpora, while projects such as WordNet (Miller et al., 1990) and Freebase (Bollacker et al., 2008) have employed editors to manually enumerate words and entities with their semantic classes. The aforementioned methods do not use query logs or explicitly determine the relative probabilities of different entity senses. A method might learn that there is independently a high chance of eBay being a website and an </context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008. Semantic class learning from the web with hyponym pattern linkage graphs. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine Miller</author>
</authors>
<title>Wordnet: An on-line lexical database.</title>
<date>1990</date>
<volume>3</volume>
<pages>235--244</pages>
<contexts>
<context position="4633" citStr="Miller et al., 1990" startWordPosition="711" endWordPosition="714">t intent contributes significantly to these results. 2 Related Work 2.1 Finding Semantic Classes A closely related problem is that of finding the semantic classes of entities. Automatic techniques for finding semantic classes include unsupervised clustering (Sch¨utze, 1998; Pantel and Lin, 2002), hyponym patterns (Hearst, 1992; Pantel et al., 2004; Kozareva et al., 2008), extraction patterns (Etzioni et al., 2005), hidden Markov models (Ritter et al., 2009), classification (Rahman and Ng, 2010) and many others. These techniques typically leverage large corpora, while projects such as WordNet (Miller et al., 1990) and Freebase (Bollacker et al., 2008) have employed editors to manually enumerate words and entities with their semantic classes. The aforementioned methods do not use query logs or explicitly determine the relative probabilities of different entity senses. A method might learn that there is independently a high chance of eBay being a website and an employer, but does not specify which usage is more common. This is especially problematic, for example, if one wishes to leverage Freebase but only needs the most commonly used senses (e.g., Al Gore is a US Vice President), rather than all possibl</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller. 1990. Wordnet: An on-line lexical database. volume 3, pages 235–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pas¸ca</author>
</authors>
<title>Weakly-supervised discovery of named entities using web search queries.</title>
<date>2007</date>
<booktitle>In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, CIKM ’07,</booktitle>
<pages>683--690</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Pas¸ca, 2007</marker>
<rawString>Marius Pas¸ca. 2007. Weakly-supervised discovery of named entities using web search queries. In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, CIKM ’07, pages 683–690, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In SIGKDD,</booktitle>
<pages>613--619</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="4309" citStr="Pantel and Lin, 2002" startWordPosition="661" endWordPosition="664">Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics • We propose an efficient learning technique and a robust implementation of our models, using real-world query data, and a realistic large set of entity types. • We empirically show that our models outperform the state of the art and that modeling latent intent contributes significantly to these results. 2 Related Work 2.1 Finding Semantic Classes A closely related problem is that of finding the semantic classes of entities. Automatic techniques for finding semantic classes include unsupervised clustering (Sch¨utze, 1998; Pantel and Lin, 2002), hyponym patterns (Hearst, 1992; Pantel et al., 2004; Kozareva et al., 2008), extraction patterns (Etzioni et al., 2005), hidden Markov models (Ritter et al., 2009), classification (Rahman and Ng, 2010) and many others. These techniques typically leverage large corpora, while projects such as WordNet (Miller et al., 1990) and Freebase (Bollacker et al., 2008) have employed editors to manually enumerate words and entities with their semantic classes. The aforementioned methods do not use query logs or explicitly determine the relative probabilities of different entity senses. A method might le</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel and Dekang Lin. 2002. Discovering word senses from text. In SIGKDD, pages 613–619, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Towards terascale knowledge acquisition.</title>
<date>2004</date>
<booktitle>In COLING,</booktitle>
<pages>771--777</pages>
<contexts>
<context position="4362" citStr="Pantel et al., 2004" startWordPosition="670" endWordPosition="673">onal Linguistics • We propose an efficient learning technique and a robust implementation of our models, using real-world query data, and a realistic large set of entity types. • We empirically show that our models outperform the state of the art and that modeling latent intent contributes significantly to these results. 2 Related Work 2.1 Finding Semantic Classes A closely related problem is that of finding the semantic classes of entities. Automatic techniques for finding semantic classes include unsupervised clustering (Sch¨utze, 1998; Pantel and Lin, 2002), hyponym patterns (Hearst, 1992; Pantel et al., 2004; Kozareva et al., 2008), extraction patterns (Etzioni et al., 2005), hidden Markov models (Ritter et al., 2009), classification (Rahman and Ng, 2010) and many others. These techniques typically leverage large corpora, while projects such as WordNet (Miller et al., 1990) and Freebase (Bollacker et al., 2008) have employed editors to manually enumerate words and entities with their semantic classes. The aforementioned methods do not use query logs or explicitly determine the relative probabilities of different entity senses. A method might learn that there is independently a high chance of eBay</context>
</contexts>
<marker>Pantel, Ravichandran, Hovy, 2004</marker>
<rawString>Patrick Pantel, Deepak Ravichandran, and Eduard Hovy. 2004. Towards terascale knowledge acquisition. In COLING, pages 771–777.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Filip Radlinski</author>
<author>Martin Szummer</author>
<author>Nick Craswell</author>
</authors>
<title>Inferring query intent from reformulations and clicks.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th international conference on World wide web, WWW ’10,</booktitle>
<pages>1171--1172</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7154" citStr="Radlinski et al. (2010)" startWordPosition="1121" endWordPosition="1124"> relative probabilities of different senses. 2.3 User Intents in Search Learning from query logs also allows us to leverage the concept of user intents. When users submit search queries, they often have specific intents in mind. Broder (2002) introduced 3 top level intents: Informational (e.g., wanting to learn), Navigational (wanting to visit a site), and Transactional (e.g., wanting to buy/sell). Rose and Levinson (2004) further divided these into finer-grained subcategories, and Yin and Shah (2010) built hierarchical taxonomies of search intents. Jansen et al. (2007), Hu et al. (2009), and Radlinski et al. (2010) examined how to infer the intent of queries. We are not aware of any other work that has leveraged user intents to learn type distributions. 2.4 Topic Modeling on Query Logs The closest work to ours is Guo et al.’s (2009) research on Named Entity Recognition in Queries. Given an entity-bearing query, they attempt to identify the entity and determine the type posteriors. Our work significantly scales up the type posteriors component of their work. While they only have four potential types (Movie, Game, Book, Music) for each entity, we employ over 70 popular types, allowing much greater coverag</context>
</contexts>
<marker>Radlinski, Szummer, Craswell, 2010</marker>
<rawString>Filip Radlinski, Martin Szummer, and Nick Craswell. 2010. Inferring query intent from reformulations and clicks. In Proceedings of the 19th international conference on World wide web, WWW ’10, pages 1171– 1172, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Inducing finegrained semantic classes via hierarchical and collective classification.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>931--939</pages>
<contexts>
<context position="4512" citStr="Rahman and Ng, 2010" startWordPosition="692" endWordPosition="695"> large set of entity types. • We empirically show that our models outperform the state of the art and that modeling latent intent contributes significantly to these results. 2 Related Work 2.1 Finding Semantic Classes A closely related problem is that of finding the semantic classes of entities. Automatic techniques for finding semantic classes include unsupervised clustering (Sch¨utze, 1998; Pantel and Lin, 2002), hyponym patterns (Hearst, 1992; Pantel et al., 2004; Kozareva et al., 2008), extraction patterns (Etzioni et al., 2005), hidden Markov models (Ritter et al., 2009), classification (Rahman and Ng, 2010) and many others. These techniques typically leverage large corpora, while projects such as WordNet (Miller et al., 1990) and Freebase (Bollacker et al., 2008) have employed editors to manually enumerate words and entities with their semantic classes. The aforementioned methods do not use query logs or explicitly determine the relative probabilities of different entity senses. A method might learn that there is independently a high chance of eBay being a website and an employer, but does not specify which usage is more common. This is especially problematic, for example, if one wishes to lever</context>
</contexts>
<marker>Rahman, Ng, 2010</marker>
<rawString>Altaf Rahman and Vincent Ng. 2010. Inducing finegrained semantic classes via hierarchical and collective classification. In Proceedings of COLING, pages 931–939.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>What is this, anyway: Automatic hypernym discovery.</title>
<date>2009</date>
<booktitle>In Proceedings of AAAI-09 Spring Symposium on Learning by Reading and Learning to Read,</booktitle>
<pages>88--93</pages>
<contexts>
<context position="4474" citStr="Ritter et al., 2009" startWordPosition="687" endWordPosition="690">real-world query data, and a realistic large set of entity types. • We empirically show that our models outperform the state of the art and that modeling latent intent contributes significantly to these results. 2 Related Work 2.1 Finding Semantic Classes A closely related problem is that of finding the semantic classes of entities. Automatic techniques for finding semantic classes include unsupervised clustering (Sch¨utze, 1998; Pantel and Lin, 2002), hyponym patterns (Hearst, 1992; Pantel et al., 2004; Kozareva et al., 2008), extraction patterns (Etzioni et al., 2005), hidden Markov models (Ritter et al., 2009), classification (Rahman and Ng, 2010) and many others. These techniques typically leverage large corpora, while projects such as WordNet (Miller et al., 1990) and Freebase (Bollacker et al., 2008) have employed editors to manually enumerate words and entities with their semantic classes. The aforementioned methods do not use query logs or explicitly determine the relative probabilities of different entity senses. A method might learn that there is independently a high chance of eBay being a website and an employer, but does not specify which usage is more common. This is especially problemati</context>
</contexts>
<marker>Ritter, Soderland, Etzioni, 2009</marker>
<rawString>Alan Ritter, Stephen Soderland, and Oren Etzioni. 2009. What is this, anyway: Automatic hypernym discovery. In Proceedings of AAAI-09 Spring Symposium on Learning by Reading and Learning to Read, pages 88– 93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel E Rose</author>
<author>Danny Levinson</author>
</authors>
<title>Understanding user goals in web search.</title>
<date>2004</date>
<booktitle>In Proceedings of the 13th international conference on World Wide Web, WWW ’04,</booktitle>
<pages>13--19</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6957" citStr="Rose and Levinson (2004)" startWordPosition="1089" endWordPosition="1092">ations learned from Web search results are applicable to NLP tasks such as NER. Alfonseca et al. (2010) mined query logs to find attributes of entity instances. However, these projects did not learn relative probabilities of different senses. 2.3 User Intents in Search Learning from query logs also allows us to leverage the concept of user intents. When users submit search queries, they often have specific intents in mind. Broder (2002) introduced 3 top level intents: Informational (e.g., wanting to learn), Navigational (wanting to visit a site), and Transactional (e.g., wanting to buy/sell). Rose and Levinson (2004) further divided these into finer-grained subcategories, and Yin and Shah (2010) built hierarchical taxonomies of search intents. Jansen et al. (2007), Hu et al. (2009), and Radlinski et al. (2010) examined how to infer the intent of queries. We are not aware of any other work that has leveraged user intents to learn type distributions. 2.4 Topic Modeling on Query Logs The closest work to ours is Guo et al.’s (2009) research on Named Entity Recognition in Queries. Given an entity-bearing query, they attempt to identify the entity and determine the type posteriors. Our work significantly scales</context>
</contexts>
<marker>Rose, Levinson, 2004</marker>
<rawString>Daniel E. Rose and Danny Levinson. 2004. Understanding user goals in web search. In Proceedings of the 13th international conference on World Wide Web, WWW ’04, pages 13–19, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan R¨ud</author>
<author>Massimiliano Ciaramita</author>
<author>Jens M¨uller</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Piggyback: Using search engines for robust cross-domain named entity recognition.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL ’11,</booktitle>
<pages>965--975</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<marker>R¨ud, Ciaramita, M¨uller, Sch¨utze, 2011</marker>
<rawString>Stefan R¨ud, Massimiliano Ciaramita, Jens M¨uller, and Hinrich Sch¨utze. 2011. Piggyback: Using search engines for robust cross-domain named entity recognition. In Proceedings of ACL ’11, pages 965–975, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--97</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24:97–123, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Hisami Suzuki</author>
</authors>
<title>Acquiring ontological knowledge from query logs.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th international conference on World Wide Web, WWW ’07,</booktitle>
<pages>1223--1224</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6090" citStr="Sekine and Suzuki (2007)" startWordPosition="952" endWordPosition="955"> also expand upon text corpora methods in that the type priors can adapt to Web search signals. 2.2 Query Log Mining Query logs have traditionally been mined to improve search (Baeza-Yates et al., 2004; Zhang and Nasraoui, 2006), but they can also be used in place of (or in addition to) text corpora for learning semantic classes. Query logs can contain billions of entries, they provide an independent signal from text corpora, their timestamps allow the learning of type priors at specific points in time, and they can contain information such as clickthroughs that are not found in text corpora. Sekine and Suzuki (2007) used frequency features on context words in query logs to learn semantic classes of entities. Pas¸ca (2007) used extraction techniques to mine instances of semantic classes from query logs. R¨ud et al. (2011) found that cross-domain generalizations learned from Web search results are applicable to NLP tasks such as NER. Alfonseca et al. (2010) mined query logs to find attributes of entity instances. However, these projects did not learn relative probabilities of different senses. 2.3 User Intents in Search Learning from query logs also allows us to leverage the concept of user intents. When u</context>
</contexts>
<marker>Sekine, Suzuki, 2007</marker>
<rawString>Satoshi Sekine and Hisami Suzuki. 2007. Acquiring ontological knowledge from query logs. In Proceedings of the 16th international conference on World Wide Web, WWW ’07, pages 1223–1224, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoxin Yin</author>
<author>Sarthak Shah</author>
</authors>
<title>Building taxonomy of web search intents for name entity queries.</title>
<date>2010</date>
<booktitle>In WWW,</booktitle>
<pages>1001--1010</pages>
<contexts>
<context position="7037" citStr="Yin and Shah (2010)" startWordPosition="1101" endWordPosition="1104">seca et al. (2010) mined query logs to find attributes of entity instances. However, these projects did not learn relative probabilities of different senses. 2.3 User Intents in Search Learning from query logs also allows us to leverage the concept of user intents. When users submit search queries, they often have specific intents in mind. Broder (2002) introduced 3 top level intents: Informational (e.g., wanting to learn), Navigational (wanting to visit a site), and Transactional (e.g., wanting to buy/sell). Rose and Levinson (2004) further divided these into finer-grained subcategories, and Yin and Shah (2010) built hierarchical taxonomies of search intents. Jansen et al. (2007), Hu et al. (2009), and Radlinski et al. (2010) examined how to infer the intent of queries. We are not aware of any other work that has leveraged user intents to learn type distributions. 2.4 Topic Modeling on Query Logs The closest work to ours is Guo et al.’s (2009) research on Named Entity Recognition in Queries. Given an entity-bearing query, they attempt to identify the entity and determine the type posteriors. Our work significantly scales up the type posteriors component of their work. While they only have four poten</context>
</contexts>
<marker>Yin, Shah, 2010</marker>
<rawString>Xiaoxin Yin and Sarthak Shah. 2010. Building taxonomy of web search intents for name entity queries. In WWW, pages 1001–1010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhang</author>
<author>O Nasraoui</author>
</authors>
<title>Mining search engine query logs for query recommendation. In</title>
<date>2006</date>
<booktitle>WWW,</booktitle>
<pages>1039--1040</pages>
<contexts>
<context position="5694" citStr="Zhang and Nasraoui, 2006" startWordPosition="883" endWordPosition="887">ematic, for example, if one wishes to leverage Freebase but only needs the most commonly used senses (e.g., Al Gore is a US Vice President), rather than all possible obscure senses (Freebase contains 30+ senses, including ones such as Impersonated Celebrity and Quotation Subject). In scenarios such as this, our proposed method can increase the usability of systems that find semantic classes. We also expand upon text corpora methods in that the type priors can adapt to Web search signals. 2.2 Query Log Mining Query logs have traditionally been mined to improve search (Baeza-Yates et al., 2004; Zhang and Nasraoui, 2006), but they can also be used in place of (or in addition to) text corpora for learning semantic classes. Query logs can contain billions of entries, they provide an independent signal from text corpora, their timestamps allow the learning of type priors at specific points in time, and they can contain information such as clickthroughs that are not found in text corpora. Sekine and Suzuki (2007) used frequency features on context words in query logs to learn semantic classes of entities. Pas¸ca (2007) used extraction techniques to mine instances of semantic classes from query logs. R¨ud et al. (</context>
</contexts>
<marker>Zhang, Nasraoui, 2006</marker>
<rawString>Z. Zhang and O. Nasraoui. 2006. Mining search engine query logs for query recommendation. In WWW, pages 1039–1040.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>