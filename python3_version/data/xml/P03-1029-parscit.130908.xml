<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001621">
<title confidence="0.999565">
An Improved Extraction Pattern Representation Model
for Automatic IE Pattern Acquisition
</title>
<author confidence="0.998758">
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman
</author>
<affiliation confidence="0.999866">
Department of Computer Science
</affiliation>
<address confidence="0.867061">
New York University
715 Broadway, 7th Floor,
New York, NY 10003 USA
</address>
<email confidence="0.998982">
sudo,sekine,grishman @cs.nyu.edu
</email>
<sectionHeader confidence="0.995637" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999959933333333">
Several approaches have been described
for the automatic unsupervised acquisi-
tion of patterns for information extraction.
Each approach is based on a particular
model for the patterns to be acquired, such
as a predicate-argument structure or a de-
pendency chain. The effect of these al-
ternative models has not been previously
studied. In this paper, we compare the
prior models and introduce a new model,
the Subtree model, based on arbitrary sub-
trees of dependency trees. We describe
a discovery procedure for this model and
demonstrate experimentally an improve-
ment in recall using Subtree patterns.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968368421053">
Information Extraction (IE) is the process of identi-
fying events or actions of interest and their partici-
pating entities from a text. As the field of IE has de-
veloped, the focus of study has moved towards au-
tomatic knowledge acquisition for information ex-
traction, including domain-specific lexicons (Riloff,
1993; Riloff and Jones, 1999) and extraction pat-
terns (Riloff, 1996; Yangarber et al., 2000; Sudo
et al., 2001). In particular, methods have recently
emerged for the acquisition of event extraction pat-
terns without corpus annotation in view of the cost
of manual labor for annotation. However, there has
been little study of alternative representation models
of extraction patterns for unsupervised acquisition.
In the prior work on extraction pattern acquisition,
the representation model of the patterns was based
on a fixed set of pattern templates (Riloff, 1996), or
predicate-argument relations, such as subject-verb,
and object-verb (Yangarber et al., 2000). The model
of our previous work (Sudo et al., 2001) was based
on the paths from predicate nodes in dependency
trees.
In this paper, we discuss the limitations of prior
extraction pattern representation models in relation
to their ability to capture the participating entities in
scenarios. We present an alternative model based on
subtrees of dependency trees, so as to extract enti-
ties beyond direct predicate-argument relations. An
evaluation on scenario-template tasks shows that the
proposed Subtree model outperforms the previous
models.
Section 2 describes the Subtree model for extrac-
tion pattern representation. Section 3 shows the
method for automatic acquisition. Section 4 gives
the experimental results of the comparison to other
methods and Section 5 presents an analysis of these
results. Finally, Section 6 provides some concluding
remarks and perspective on future research.
</bodyText>
<sectionHeader confidence="0.993277" genericHeader="method">
2 Subtree model
</sectionHeader>
<bodyText confidence="0.999936102564103">
Our research on improved representation models for
extraction patterns is motivated by the limitations of
the prior extraction pattern representations. In this
section, we review two of the previous models in
detail, namely the Predicate-Argument model (Yan-
garber et al., 2000) and the Chain model (Sudo et al.,
2001).
The main cause of difficulty in finding entities by
extraction patterns is the fact that the participating
entities can appear not only as an argument of the
predicate that describes the event type, but also in
other places within the sentence or in the prior text.
In the MUC-3 terrorism scenario, WEAPON entities
occur in many different relations to event predicates
in the documents. Even if WEAPON entities appear
in the same sentence with the event predicate, they
rarely serve as a direct argument of such predicates.
(e.g., “One person was killed as the result of a bomb
explosion.”)
Predicate-Argument model The Predicate-
Argument model is based on a direct syntactic rela-
tion between a predicate and its arguments1 (Yan-
garber et al., 2000). In general, a predicate provides
a strong context for its arguments, which leads to
good accuracy. However, this model has two major
limitations in terms of its coverage, clausal bound-
aries and embedded entities inside a predicate’s
arguments.
Figure 12 shows an example of an extraction task
in the terrorism domain where the event template
consists of perpetrator, date, location and victim.
With the extraction patterns based on the Predicate-
Argument model, only perpetrator and victim can
be extracted. The location (downtown Jerusalem) is
embedded as a modifier of the noun (heart) within
the prepositional phrase, which is an adjunct of the
main predicate, triggered3. Furthermore, it is not
clear whether the extracted entities are related to the
same event, because of the clausal boundaries.4
</bodyText>
<footnote confidence="0.633184666666667">
1Since the case marking for a nominalized predicate is sig-
nificantly different from the verbal predicate, which makes it
hard to regularize the nominalized predicates automatically, the
constraint for the Predicate-Argument model requires the root
node to be a verbal predicate.
2Throughout this paper, extraction patterns are defined as
one or more word classes with their context in the dependency
tree, where the actual word matched with the class is associ-
ated to one of the slots in the template. The notation of the
patterns in this paper is based on a dependency tree where (
( - )..( - )) denotes is the head, and, for each in ,
is its argument and the relation between and is labeled
with . The labels introduced in this paper are SBJ (subject),
OBJ (object), ADV (adverbial adjunct), REL (relative), APPOS
(apposition) and prepositions (IN, OF, etc.). Also, we assume
that the order of the arguments does not matter. Symbols begin-
ning with C- represent NE (Named Entity) types.
3Yangarber refers this as a noun phrase pattern in (Yangar-
ber et al., 2000).
4This is the problem of merging the result of entity extrac-
tion. Most IE systems have hard-coded inference rules, such
</footnote>
<bodyText confidence="0.984678860465116">
Chain model Our previous work, the Chain
model (Sudo et al., 2001)5 attempts to remedy the
limitations of the Predicate-Argument model. The
extraction patterns generated by the Chain model
are any chain-shaped paths in the dependency tree.6
Thus it successfully avoids the clausal boundary
and embedded entity limitation. We reported a 5%
gain in recall at the same precision level in the
MUC-6 management succession task compared to
the Predicate-Argument model.
However, the Chain model also has its own weak-
ness in terms of accuracy due to the lack of context.
For example, in Figure 1(c), (triggered ( C-DATE -
ADV)) is needed to extract the date entity. However,
the same pattern is likely to be applied to texts in
other domains as well, such as “The Mexican peso
was devalued and triggered a national financial cri-
sis last week.”
Subtree model The Subtree model is a general-
ization of previous models, such that any subtree of
a dependency tree in the source sentence can be re-
garded as an extraction pattern candidate. As shown
in Figure 1(d), the Subtree model, by its defini-
tion, contains all the patterns permitted by either the
Predicate-Argument model or the Chain model. It
is also capable of providing more relevant context,
such as (triggered (explosion-OBJ)( C-DATE -ADV)).
The obvious advantage of the Subtree model is
the flexibility it affords in creating suitable patterns,
spanning multiple levels and multiple branches. Pat-
tern coverage is further improved by relaxing the
constraint that the root of the pattern tree be a pred-
icate node. However, this flexibility can also be a
disadvantage, since it means that a very large num-
ber of pattern candidates — all possible subtrees of
the dependency tree of each sentence in the corpus
— must be considered. An efficient procedure is re-
quired to select the appropriate patterns from among
the candidates.
Also, as the number of pattern candidates in-
creases, the amount of noise and complexity in-
as “triggering an explosion is related to killing or injuring and
therefore constitutes one terrorism action.”
</bodyText>
<footnote confidence="0.857770333333333">
5Originally we called it “Tree-Based Representation of Pat-
terns”. We renamed it to avoid confusion with the proposed
approach that is also based on dependency trees.
6(Sudo et al., 2001) required the root node of the chain to be
a verbal predicate, but we have relaxed that constraint for our
experiments.
</footnote>
<bodyText confidence="0.998924">
JERUSALEM, March 21 – A smiling Palestinian suicide bomber triggered a mas-
sive explosion in the heavily policed heart of downtown Jerusalem today, killing
himself and three other people and injuring scores.
</bodyText>
<figure confidence="0.986540235294117">
Predicate-Argument
Chain model
(triggered ( C-PERSON -SBJ)(explosion-OBJ)( C-DATE -ADV))
(killing ( C-PERSON -OBJ))
(injuring ( C-PERSON -OBJ))
(triggered ( C-PERSON -SBJ))
(triggered (heart-IN ( C-LOCATION -OF)))
(triggered (killing-ADV ( C-PERSON -OBJ)))
(triggered (injuring-ADV ( C-PERSON -OBJ)))
(triggered ( C-DATE -ADV))
Subtree model
(triggered ( C-PERSON -SBJ)(explosion-OBJ)) (triggered (explosion-OBJ)( C-DATE -ADV))
(killing ( C-PERSON -OBJ)) (triggered ( C-DATE -ADV)(killing-ADV))
(injuring ( C-PERSON -OBJ)) (triggered ( C-DATE -ADV)(killing-ADV( C-PERSON -OBJ)))
(triggered (heart-IN ( C-LOCATION -OF))) (triggered ( C-DATE -ADV)(injuring-ADV))
(triggered (killing-ADV ( C-PERSON -OBJ))) (triggered (explosion-OBJ)(killing ( C-PERSON -OBJ)))
(triggered ( C-DATE -ADV)) ...
</figure>
<figureCaption confidence="0.973591333333333">
Figure 1: (a) Example sentence on terrorism scenario. (b) Dependency Tree of the example sentence (The entities to be extracted
are shaded in the tree). (c) Predicate-Argument patterns and Chain-model patterns that contribute to the extraction task. (d) Subtree
model patterns that contribute the extraction task.
</figureCaption>
<bodyText confidence="0.9999413">
creases. In particular, many of the pattern candidates
overlap one another. For a given set of extraction
patterns, if pattern A subsumes pattern B (say, A is
(shoot ( C-PERSON -OBJ)(to death)) and B is (shoot ( C-
PERSON -OBJ))), there is no added contribution for
extraction by pattern matching with A (since all the
matches with pattern A must be covered with pattern
B). Therefore, we need to pay special attention to the
ranking function for pattern candidates, so that pat-
terns with more relevant contexts get higher score.
</bodyText>
<sectionHeader confidence="0.988121" genericHeader="method">
3 Acquisition Method
</sectionHeader>
<bodyText confidence="0.999946333333333">
This section discusses an automatic procedure to
learn extraction patterns. Given a narrative descrip-
tion of the scenario and a set of source documents,
the following three stages obtain the relevant extrac-
tion patterns for the scenario; preprocessing, docu-
ment retrieval, and ranking pattern candidates.
</bodyText>
<subsectionHeader confidence="0.999473">
3.1 Stage 1: Preprocessing
</subsectionHeader>
<bodyText confidence="0.99934275">
Morphological analysis and Named Entities (NE)
tagging are performed at this stage.7 Then all the
sentences are converted into dependency trees by an
appropriate dependency analyzer.$ The NE tagging
</bodyText>
<footnote confidence="0.941418333333333">
7We used Extended NE hierarchy based on (Sekine et al.,
2002), which is structured and contains 150 classes.
8Any degree of detail can be chosen through entire proce-
</footnote>
<bodyText confidence="0.933393333333333">
dure, from lexicalized dependency to chunk-level dependency.
For the following experiment in Japanese, we define a node in
replaces named entities by their class, so the result-
ing dependency trees contain some NE class names
as leaf nodes. This is crucial to identifying common
patterns, and to applying these patterns to new text.
</bodyText>
<subsectionHeader confidence="0.99735">
3.2 Stage 2: Document Retrieval
</subsectionHeader>
<bodyText confidence="0.999986125">
The procedure retrieves a set of documents that de-
scribe the events of the scenario of interest, the rel-
evant document set. A set of narrative sentences de-
scribing the scenario is selected to create a query
for the retrieval. Any IR system of sufficient accu-
racy can be used at this stage. For this experiment,
we retrieved the documents using CRL’s stochastic-
model-based IR system (Murata et al., 1999).
</bodyText>
<subsectionHeader confidence="0.997773">
3.3 Stage 3: Ranking Pattern Candidates
</subsectionHeader>
<bodyText confidence="0.998860772727273">
Given the dependency trees of parsed sentences in
the relevant document set, all the possible subtrees
can be candidates for extraction patterns. The rank-
ing of pattern candidates is inspired by TF/IDF scor-
ing in IR literature; a pattern is more relevant when
it appears more in the relevant document set and less
across the entire collection of source documents.
The right-most expansion base subtree discovery
algorithm (Abe et al., 2002) was implemented to cal-
culate term frequency (raw frequency of a pattern)
and document frequency (the number of documents
where a pattern appears) for each pattern candidate.
The algorithm finds the subtrees appearing more fre-
quently than a given threshold by constructing the
subtrees level by level, while keeping track of their
occurrence in the corpus. Thus, it efficiently avoids
the construction of duplicate patterns and runs al-
most linearly in the total size of the maximal tree
patterns contained in the corpus.
The following ranking function was used to rank
each pattern candidate. The score of subtree,
, is
</bodyText>
<equation confidence="0.770382">
(1)
</equation>
<bodyText confidence="0.990753">
where is the number of times that subtree
appears across the documents in the relevant docu-
ment set, . is the set of subtrees that appear in
.is the number of documents in the collection
containing subtree, and is the total number of
the dependency tree as a bunsetsu, phrasal unit.
</bodyText>
<equation confidence="0.536845666666667">
Precision-Recall
0 10 20 30 40 50 60 70 80
Recall (%)
</equation>
<figureCaption confidence="0.956163">
Figure 2: Comparison of Extraction Performance
with Different
</figureCaption>
<bodyText confidence="0.9823488">
documents in the collection. The first term roughly
corresponds to the term frequency and the second
term to the inverse document frequency in TF/IDF
scoring. is used to control the weight on the IDF
portion of this scoring function.
</bodyText>
<subsectionHeader confidence="0.978009">
3.4 Parameter Tuning for Ranking Function
</subsectionHeader>
<bodyText confidence="0.99998924">
The in Equation (1) is used to parameterize the
weight on the IDF portion of the ranking function.
As we pointed out in Section 2, we need to pay spe-
cial attention to overlapping patterns; the more rele-
vant context a pattern contains, the higher it should
be ranked. The weight serves to focus on how
specific a pattern is to a given scenario. Therefore,
for high value, (triggered (explosion-OBJ)( C-DATE -
ADV)) is ranked higher than (triggered ( C-DATE -
ADV)) in the terrorism scenario, for example. Fig-
ure 2 shows the improvement of the extraction per-
formance by tuning on the entity extraction task
which will be discussed in the next section.
For unsupervised tuning of, we used a pseudo-
extraction task, instead of using held-out data for su-
pervised learning. We used an unsupervised version
of the text classification task to optimize, assum-
ing that all the documents retrieved by the IR sys-
tem are relevant to the scenario and the pattern set
that performs well on the text classification task also
works well on the entity extraction task.
The unsupervised text classification task is to
measure how close a pattern matching system, given
a set of extraction patterns, simulates the document
retrieval of the same IR system as in the previous
</bodyText>
<figure confidence="0.984686">
Precision (%)
100
95
90
85
80
75
70
65
60
55
50
SUBT beta=1
SUBT beta=3
SUBT beta=8
</figure>
<bodyText confidence="0.999241689655172">
sub-section. The value is optimized so that the cu-
mulative performance of the precision-recall curve
over the entire range of recall for the text classifica-
tion task is maximized.
The document set for text classification is com-
posed of the documents retrieved by the same IR
system as in Section 3.2 plus the same number of
documents picked up randomly, where all the docu-
ments are taken from a different document set from
the one used for pattern learning. The pattern match-
ing system, given a set of extraction patterns, clas-
sifies a document as retrieved if any of the patterns
match any portion of the document, and as random
otherwise. Thus, we can get the performance of text
classification of the pattern matching system in the
form of a precision-recall curve, without any super-
vision.
Next, the area of the precision-recall curve
is computed by connecting every point in the
precision-recall curve from 0 to the maximum re-
call the pattern matching system reached, and we
compare the area for each possible value. Finally,
the value which gets the greatest area under the
precision-recall curve is used for extraction.
The comparison to the same procedure based on
the precision-recall curve of the actual extraction
performance shows that this tuning has high correla-
tion with the extraction performance (Spearman cor-
relation coefficient with 2% confidence).
</bodyText>
<subsectionHeader confidence="0.915406">
3.5 Filtering
</subsectionHeader>
<bodyText confidence="0.999724428571429">
For efficiency and to eliminate low-frequency noise,
we filtered out the pattern candidates that appear in
less than 3 documents throughout the entire collec-
tion. Also, since the patterns with too much con-
text are unlikely to match with new text, we added
another filtering criterion based on the number of
nodes in a pattern candidate; the maximum number
of nodes is 8.
Since all the slot-fillers in the extraction task of
our experiment are assumed to be instances of the
150 classes in the extended Named Entity hierar-
chy (Sekine et al., 2002), further filtering was done
by requiring a pattern candidate to contain at least
one Named Entity class.
</bodyText>
<sectionHeader confidence="0.998228" genericHeader="method">
4 Experiment
</sectionHeader>
<bodyText confidence="0.97839835">
The experiment of this study is focused on compar-
ing the performance of the earlier extraction pattern
models to the proposed Subtree Model (SUBT). The
compared models are the direct predicate-argument
model (PA)9, and the Chain model (CH) in (Sudo et
al., 2001).
The task for this experiment is entity extraction,
which is to identify all the entities participating in
relevant events in a set of given Japanese texts. Note
that all NEs in the test documents were identified
manually, so that the task can measure only how well
extraction patterns can distinguish the participating
entities from the entities that are not related to any
events. This task does not involve grouping entities
associated with the same event into a single template
to avoid possible effect of merging failure on extrac-
tion performance for entities. We accumulated the
test set of documents of two scenarios; the Manage-
ment Succession scenario of (MUC-6, 1995), with
a simpler template structure, where corporate man-
agers assumed and/or left their posts, and the Mur-
derer Arrest scenario, where a law enforcement or-
ganization arrested a murder suspect.
The source document set from which the ex-
traction patterns are learned consists of 117,109
Mainichi Newspaper articles from 1995. All the
sentences are morphologically analyzed by JU-
MAN (Kurohashi, 1997) and converted into depen-
dency trees by KNP (Kurohashi and Nagao, 1994).
Regardless of the model of extraction patterns, the
pattern acquisition follows the procedure described
in Section 3. We retrieved 300 documents as a rele-
vant document set.
The association of NE classes and slots in the
template is made automatically; Person, Organi-
zation, Post (slots) correspond to C-PERSON, C-
ORG, C-POST (NE-classes), respectively, in the
Succession scenario, and Suspect, Arresting Agency,
Charge (slots) correspond to C-PERSON, C-ORG,
C-OFFENCE (NE-classes), respectively, in the Ar-
</bodyText>
<footnote confidence="0.906226">
TThis is a restricted version of (Yangarber et al., 2000) con-
strained to have a single place-holder for each pattern, while
(Yangarber et al., 2000) allowed more than one place-holder.
However, the difference does not matter for the entity extrac-
tion task which does not require merging entities in a single
template.
</footnote>
<table confidence="0.998028076923077">
Succession Arrest
IR description Management Succession: Management Succes- A relevant document must describe the arrest of
(translation sion at the level of executives of a company. The the suspect of murder. The document should be
of Japanese) topic of interest should not be limited to the pro- regarded as interesting if it discusses the suspect
motion inside the company mentioned, but also in- under suspicion for multiple crimes including mur-
cludes hiring executives from outside the company der, such as murder-robbery.
or their resignation.
Slots Person, Organization, Post Arresting Agency, Suspect, Charge
# of Test Documents 148 205
(relevant + irrelevant)
Slots Person: 135 Arresting Agency: 128
Organization: 172 Suspect: 129
Post: 215 Charge: 148
</table>
<tableCaption confidence="0.999706">
Table 1: Task Description and Statistics of Test Data
</tableCaption>
<bodyText confidence="0.997217296296296">
rest scenario. 10
For each model, we get a list of the pattern candi-
dates ordered by the ranking function discussed in
Section 3.3 after filtering. The result of the per-
formance is shown (Figure 3) as a precision-recall
graph for each subset of top- ranked patterns where
ranges from 1 to the number of the pattern candi-
dates.
The test set was accumulated from Mainichi
Newspaper in 1996 by a simple keyword search,
with some additional irrelevant documents. (See Ta-
ble 1 for detail.)
Figure 3(a) shows the precision-recall curve of
top- relevant extraction patterns for each model on
the Succession Scenario. At lower recall levels (up
to 35%), all the models performed similarly. How-
ever, the precision of Chain patterns dropped sud-
denly by 20% at recall level 38%, while the SUBT
patterns keep the precision significantly higher than
Chain patterns until it reaches 58% recall. Even after
SUBT hit the drop at 56%, SUBT is consistently a
few percent higher in precision than Chain patterns
for most recall levels. Figure 3(a) also shows that
although PA keeps high precision at low recall level
it has a significantly lower ceiling of recall (52%)
compared to other models.
Figure 3(b) shows the extraction performance on
</bodyText>
<footnote confidence="0.9473935">
10Since there is no subcategory of C-PERSON to distinguish
Suspect and victim (which is not extracted in this experiment)
for the Arrest scenario, the learned pattern candidates may ex-
tract victims as Suspect entities by mistake.
</footnote>
<bodyText confidence="0.999325090909091">
the Arrest scenario task. Again, the Predicate-
Argument model has a much lower recall ceiling
(25%). The difference in the performance between
the Subtree model and the Chain model does not
seem as obvious as in the Succession task. However,
it is still observable that the Subtree model gains a
few percent precision over the Chain model at re-
call levels around 40%. A possible explanation of
the subtleness in performance difference in this sce-
nario is the smaller number of contributing patterns
compared to the Succession scenario.
</bodyText>
<sectionHeader confidence="0.998278" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999944833333334">
One of the advantages of the proposed model is
the ability to capture more varied context. The
Predicate-Argument model relies for its context on
the predicate and its direct arguments. However,
some Predicate-Argument patterns may be too gen-
eral, so that they could be applied to texts about
a different scenario and mistakenly detect entities
from them. For example, (( C-ORG -SBJ) happyo-suru),
“ C-ORG reports” may be the pattern used to ex-
tract an Organization in the Succession scenario but
it is too general — it could match irrelevant sen-
tences by mistake. The proposed Subtree Model can
acquire a more scenario-specific pattern (( C-ORG -
SBJ)((shunin-suru-REL) jinji-OBJ) happyo-suru) “ C-ORG
reports a personnel affair to appoint”. Any scoring func-
tion that penalizes the generality of a pattern match,
such as inverse document frequency, can success-
fully lessen the significance of too general patterns.
</bodyText>
<figure confidence="0.998691916666667">
(a)
Precision-Recall
0 10 20 30 40 50 60 70 80
Recall (%)
Precision-Recall
0 10 20 30 40 50 60 70 80
Recall (%)
Precision (%)
100
95
90
85
80
75
70
65
60
55
50
SUBT
CH
PA
SUBT
CH
PA
Precision (%) 100
95
90
85
80
75
70
65
60
55
(b) 50
</figure>
<figureCaption confidence="0.999965">
Figure 3: Extraction Performance: (a) Succession Scenario ( ), (b) Arrest Scenario ( )
</figureCaption>
<bodyText confidence="0.998558769230769">
The detailed analysis of the experiment revealed
that the overly-general patterns are more severely
penalized in the Subtree model compared to the
Chain model. Although both models penalize gen-
eral patterns in the same way, the Subtree model
also promotes more scenario-specific patterns than
the Chain model. In Figure 3, the large drop
was caused by the pattern (( C-DATE -ON) C-POST ),
which was mainly used to describe the date of ap-
pointment to the C-POST in the list of one’s pro-
fessional history (which is not regarded as a Suc-
cession event), but also used in other scenarios
in the business domain (18% precision by itself).
Although the scoring function described in Sec-
tion 3.3 is the same for both models, the Subtree
model can also produce contributing patterns, such
as (( C-PERSON C-POST -SBJ)( C-POST -TO) shunin-
suru) “ C-PERSON C-POST was appointed to C-POST ”
whose ranks were higher than the problematic pat-
tern.
Without generalizing case marking for nominal-
ized predicates, the Predicate-Argument model ex-
cludes some highly contributing patterns with nomi-
nalized predicates, as some example patterns show
in Figure 4. Also, chains of modifiers could be
extracted only by the Subtree and Chain models.
A typical and highly relevant expression for the
Succession scenario is (((daihyo-ken-SBJ) aru-REL) C-
POST ) “ C-POST with ministerial authority”.
Although, in the Arrest scenario, the superiority
of the Subtree model to the other models is not clear,
the general discussion about the capability of cap-
turing additional context still holds. In Figure 4,
the short pattern (( C-PERSON C-POST -APPOS) C-
NUM ), which is used for a general description of
a person with his/her occupation and age, has rela-
tively low precision (71%). However, with more rel-
evant context, such as “arrest” or “unemployed”, the
patterns become more relevant to Arrest scenario.
</bodyText>
<sectionHeader confidence="0.996168" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999978333333333">
In this paper, we explored alternative models for the
automatic acquisition of extraction patterns. We pro-
posed a model based on arbitrary subtrees of depen-
dency trees. The result of the experiment confirmed
that the Subtree model allows a gain in recall while
preserving high precision. We also discussed the
effect of the weight tuning in TF/IDF scoring and
showed an unsupervised way of adjusting it.
There are several ways in which our pattern model
may be further improved. In particular, we would
like to relax the restraint that all the fills must be
tagged with their proper NE tags by introducing a
GENERIC place-holder into the extraction patterns.
By allowing a GENERIC place-holder to match with
anything as long as the context of the pattern is
matched, the extraction patterns can extract the enti-
ties that are not tagged properly. Also patterns with
a GENERIC place-holder can be applied to slots that
are not names. Thus, the acquisition method de-
scribed in Section 3 can be used to find the patterns
for any type of slot fill.
</bodyText>
<footnote confidence="0.499809">
11( C-POST is used as a title of C-PERSON as in Presi-
dent Bush.)
</footnote>
<figure confidence="0.83657824">
Pattern
(( C-PERSON C-POST -OF) C-PERSON shoukaku)
promotion of C-POST C-PERSON
(((daihyo-ken-SBJ) aru-REL) C-POST )
C-POST with ministerial authority
((((daihyo-ken-(no)SBJ) aru-REL) C-POST -TO) shunin-suru)
be appointed to C-POST with ministerial authority
(( C-ORG -SBJ) happyo-suru)
C-ORG reports
(( C-ORG -SBJ) (jinji-OBJ) happyo-suru)
C-ORG report personnel affair
(( C-PERSON C-POST -APPOS) C-NUM )
((( C-PERSON C-POST -APPOS) C-NUM ) taiho-suru
arrest C-PERSON C-POST , C-NUM
(((mushoku-APPOS) C-PERSON C-POST -APPOS) C-NUM)
C-PERSON C-POST , C-NUM , unemployed
Correct Incorrect SUBT Chain PA
26 1 yes yes no
4 0 yes yes no
2 0 yes yes no
16 3 yes yes yes
4 0 yes no yes
54 22 yes yes no
17 1 yes yes no
11 0 yes yes no
</figure>
<figureCaption confidence="0.999981">
Figure 4: Examples of extraction patterns and their contribution
</figureCaption>
<bodyText confidence="0.993400444444445">
Acknowledgments Thanks to Taku Kudo for his
implementation of the subtree discovery algorithm
and the anonymous reviewers for useful comments.
This research is supported by the Defense Advanced
Research Projects Agency as part of the Translin-
gual Information Detection, Extraction and Summa-
rization (TIDES) program, under Grant N66001-00-
1-8917 from the Space and Naval Warfare Systems
Center San Diego.
</bodyText>
<sectionHeader confidence="0.99855" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999493113636364">
Kenji Abe, Shinji Kawasoe, Tatsuya Asai, Hiroki
Arimura, and Setsuo Arikawa. 2002. Optimized Sub-
structure Discovery for Semi-structured Data. In Pro-
ceedings of the 6th European Conference on Princi-
ples and Practice ofKnowledge in Databases (PKDD-
2002).
Sadao Kurohashi and Makoto Nagao. 1994. KN Parser
: Japanese Dependency/Case Structure Analyzer. In
Proceedings of the Workshop on Sharable Natural
Language Resources.
Sadao Kurohashi, 1997. Japanese Morphological
Analyzing System: JUMAN. http://www.kc.t.u-
tokyo.ac.jp/nl-resource/juman-e.html.
MUC-6. 1995. Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6).
Masaki Murata, Kiyotaka Uchimoto, Hiromi Ozaku, and
Qing Ma. 1999. Information Retrieval Based on
Stochastic Models in IREX. In Proceedings of the
IREX Workshop.
Ellen Riloff and Rosie Jones. 1999. Learning Dictio-
naries for Information Extraction by Multi-level Boot-
strapping. In Proceedings of the Sixteenth National
Conference on Artificial Intelligence (AAAI-99).
Ellen Riloff. 1993. Automatically Constructing a Dictio-
nary for Information Extraction Tasks. In Proceedings
of the Eleventh National Conference on Artificial In-
telligence (AAAI-93).
Ellen Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Proceedings of Thir-
teenth National Conference on Artificial Intelligence
(AAAI-96).
Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata.
2002. Extended Named Entity Hierarchy. In Proceed-
ings of Third International Conference on Language
Resources and Evaluation (LREC 2002).
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2001. Automatic Pattern Acquisition for Japanese In-
formation Extraction. In Proceedings of the Human
Language Technology Conference (HLT2001).
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Unsupervised Discovery
of Scenario-Level Patterns for Information Extraction.
In Proceedings of 18th International Conference on
Computational Linguistics (COLING-2000).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.894554">
<title confidence="0.999507">An Improved Extraction Pattern Representation Model for Automatic IE Pattern Acquisition</title>
<author confidence="0.999882">Kiyoshi Sudo</author>
<author confidence="0.999882">Satoshi Sekine</author>
<author confidence="0.999882">Ralph Grishman</author>
<affiliation confidence="0.998846">Department of Computer Science New York University</affiliation>
<address confidence="0.9997765">715 Broadway, 7th Floor, New York, NY 10003 USA</address>
<email confidence="0.997218">sudo,sekine,grishman@cs.nyu.edu</email>
<abstract confidence="0.9937465625">Several approaches have been described for the automatic unsupervised acquisition of patterns for information extraction. Each approach is based on a particular model for the patterns to be acquired, such as a predicate-argument structure or a dependency chain. The effect of these alternative models has not been previously studied. In this paper, we compare the prior models and introduce a new model, the Subtree model, based on arbitrary subtrees of dependency trees. We describe a discovery procedure for this model and demonstrate experimentally an improvement in recall using Subtree patterns.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kenji Abe</author>
<author>Shinji Kawasoe</author>
<author>Tatsuya Asai</author>
<author>Hiroki Arimura</author>
<author>Setsuo Arikawa</author>
</authors>
<title>Optimized Substructure Discovery for Semi-structured Data.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th European Conference on Principles and Practice ofKnowledge in Databases (PKDD2002).</booktitle>
<contexts>
<context position="12015" citStr="Abe et al., 2002" startWordPosition="1882" endWordPosition="1885">e used at this stage. For this experiment, we retrieved the documents using CRL’s stochasticmodel-based IR system (Murata et al., 1999). 3.3 Stage 3: Ranking Pattern Candidates Given the dependency trees of parsed sentences in the relevant document set, all the possible subtrees can be candidates for extraction patterns. The ranking of pattern candidates is inspired by TF/IDF scoring in IR literature; a pattern is more relevant when it appears more in the relevant document set and less across the entire collection of source documents. The right-most expansion base subtree discovery algorithm (Abe et al., 2002) was implemented to calculate term frequency (raw frequency of a pattern) and document frequency (the number of documents where a pattern appears) for each pattern candidate. The algorithm finds the subtrees appearing more frequently than a given threshold by constructing the subtrees level by level, while keeping track of their occurrence in the corpus. Thus, it efficiently avoids the construction of duplicate patterns and runs almost linearly in the total size of the maximal tree patterns contained in the corpus. The following ranking function was used to rank each pattern candidate. The sco</context>
</contexts>
<marker>Abe, Kawasoe, Asai, Arimura, Arikawa, 2002</marker>
<rawString>Kenji Abe, Shinji Kawasoe, Tatsuya Asai, Hiroki Arimura, and Setsuo Arikawa. 2002. Optimized Substructure Discovery for Semi-structured Data. In Proceedings of the 6th European Conference on Principles and Practice ofKnowledge in Databases (PKDD2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>KN Parser : Japanese Dependency/Case Structure Analyzer.</title>
<date>1994</date>
<booktitle>In Proceedings of the Workshop on Sharable Natural Language Resources.</booktitle>
<contexts>
<context position="18104" citStr="Kurohashi and Nagao, 1994" startWordPosition="2899" endWordPosition="2902"> failure on extraction performance for entities. We accumulated the test set of documents of two scenarios; the Management Succession scenario of (MUC-6, 1995), with a simpler template structure, where corporate managers assumed and/or left their posts, and the Murderer Arrest scenario, where a law enforcement organization arrested a murder suspect. The source document set from which the extraction patterns are learned consists of 117,109 Mainichi Newspaper articles from 1995. All the sentences are morphologically analyzed by JUMAN (Kurohashi, 1997) and converted into dependency trees by KNP (Kurohashi and Nagao, 1994). Regardless of the model of extraction patterns, the pattern acquisition follows the procedure described in Section 3. We retrieved 300 documents as a relevant document set. The association of NE classes and slots in the template is made automatically; Person, Organization, Post (slots) correspond to C-PERSON, CORG, C-POST (NE-classes), respectively, in the Succession scenario, and Suspect, Arresting Agency, Charge (slots) correspond to C-PERSON, C-ORG, C-OFFENCE (NE-classes), respectively, in the ArTThis is a restricted version of (Yangarber et al., 2000) constrained to have a single place-h</context>
</contexts>
<marker>Kurohashi, Nagao, 1994</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 1994. KN Parser : Japanese Dependency/Case Structure Analyzer. In Proceedings of the Workshop on Sharable Natural Language Resources.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
</authors>
<title>Japanese Morphological Analyzing System:</title>
<date>1997</date>
<note>JUMAN. http://www.kc.t.utokyo.ac.jp/nl-resource/juman-e.html.</note>
<contexts>
<context position="18033" citStr="Kurohashi, 1997" startWordPosition="2889" endWordPosition="2890">nt into a single template to avoid possible effect of merging failure on extraction performance for entities. We accumulated the test set of documents of two scenarios; the Management Succession scenario of (MUC-6, 1995), with a simpler template structure, where corporate managers assumed and/or left their posts, and the Murderer Arrest scenario, where a law enforcement organization arrested a murder suspect. The source document set from which the extraction patterns are learned consists of 117,109 Mainichi Newspaper articles from 1995. All the sentences are morphologically analyzed by JUMAN (Kurohashi, 1997) and converted into dependency trees by KNP (Kurohashi and Nagao, 1994). Regardless of the model of extraction patterns, the pattern acquisition follows the procedure described in Section 3. We retrieved 300 documents as a relevant document set. The association of NE classes and slots in the template is made automatically; Person, Organization, Post (slots) correspond to C-PERSON, CORG, C-POST (NE-classes), respectively, in the Succession scenario, and Suspect, Arresting Agency, Charge (slots) correspond to C-PERSON, C-ORG, C-OFFENCE (NE-classes), respectively, in the ArTThis is a restricted v</context>
</contexts>
<marker>Kurohashi, 1997</marker>
<rawString>Sadao Kurohashi, 1997. Japanese Morphological Analyzing System: JUMAN. http://www.kc.t.utokyo.ac.jp/nl-resource/juman-e.html.</rawString>
</citation>
<citation valid="true">
<date>1995</date>
<booktitle>Proceedings of the Sixth Message Understanding Conference (MUC-6).</booktitle>
<marker>1995</marker>
<rawString>MUC-6. 1995. Proceedings of the Sixth Message Understanding Conference (MUC-6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Murata</author>
<author>Kiyotaka Uchimoto</author>
<author>Hiromi Ozaku</author>
<author>Qing Ma</author>
</authors>
<title>Information Retrieval Based on Stochastic Models in IREX.</title>
<date>1999</date>
<booktitle>In Proceedings of the IREX Workshop.</booktitle>
<contexts>
<context position="11533" citStr="Murata et al., 1999" startWordPosition="1805" endWordPosition="1808">by their class, so the resulting dependency trees contain some NE class names as leaf nodes. This is crucial to identifying common patterns, and to applying these patterns to new text. 3.2 Stage 2: Document Retrieval The procedure retrieves a set of documents that describe the events of the scenario of interest, the relevant document set. A set of narrative sentences describing the scenario is selected to create a query for the retrieval. Any IR system of sufficient accuracy can be used at this stage. For this experiment, we retrieved the documents using CRL’s stochasticmodel-based IR system (Murata et al., 1999). 3.3 Stage 3: Ranking Pattern Candidates Given the dependency trees of parsed sentences in the relevant document set, all the possible subtrees can be candidates for extraction patterns. The ranking of pattern candidates is inspired by TF/IDF scoring in IR literature; a pattern is more relevant when it appears more in the relevant document set and less across the entire collection of source documents. The right-most expansion base subtree discovery algorithm (Abe et al., 2002) was implemented to calculate term frequency (raw frequency of a pattern) and document frequency (the number of docume</context>
</contexts>
<marker>Murata, Uchimoto, Ozaku, Ma, 1999</marker>
<rawString>Masaki Murata, Kiyotaka Uchimoto, Hiromi Ozaku, and Qing Ma. 1999. Information Retrieval Based on Stochastic Models in IREX. In Proceedings of the IREX Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Rosie Jones</author>
</authors>
<title>Learning Dictionaries for Information Extraction by Multi-level Bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth National Conference on Artificial Intelligence (AAAI-99).</booktitle>
<contexts>
<context position="1233" citStr="Riloff and Jones, 1999" startWordPosition="184" endWordPosition="187">d. In this paper, we compare the prior models and introduce a new model, the Subtree model, based on arbitrary subtrees of dependency trees. We describe a discovery procedure for this model and demonstrate experimentally an improvement in recall using Subtree patterns. 1 Introduction Information Extraction (IE) is the process of identifying events or actions of interest and their participating entities from a text. As the field of IE has developed, the focus of study has moved towards automatic knowledge acquisition for information extraction, including domain-specific lexicons (Riloff, 1993; Riloff and Jones, 1999) and extraction patterns (Riloff, 1996; Yangarber et al., 2000; Sudo et al., 2001). In particular, methods have recently emerged for the acquisition of event extraction patterns without corpus annotation in view of the cost of manual labor for annotation. However, there has been little study of alternative representation models of extraction patterns for unsupervised acquisition. In the prior work on extraction pattern acquisition, the representation model of the patterns was based on a fixed set of pattern templates (Riloff, 1996), or predicate-argument relations, such as subject-verb, and ob</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Ellen Riloff and Rosie Jones. 1999. Learning Dictionaries for Information Extraction by Multi-level Bootstrapping. In Proceedings of the Sixteenth National Conference on Artificial Intelligence (AAAI-99).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>Automatically Constructing a Dictionary for Information Extraction Tasks.</title>
<date>1993</date>
<booktitle>In Proceedings of the Eleventh National Conference on Artificial Intelligence (AAAI-93).</booktitle>
<contexts>
<context position="1208" citStr="Riloff, 1993" startWordPosition="182" endWordPosition="183">viously studied. In this paper, we compare the prior models and introduce a new model, the Subtree model, based on arbitrary subtrees of dependency trees. We describe a discovery procedure for this model and demonstrate experimentally an improvement in recall using Subtree patterns. 1 Introduction Information Extraction (IE) is the process of identifying events or actions of interest and their participating entities from a text. As the field of IE has developed, the focus of study has moved towards automatic knowledge acquisition for information extraction, including domain-specific lexicons (Riloff, 1993; Riloff and Jones, 1999) and extraction patterns (Riloff, 1996; Yangarber et al., 2000; Sudo et al., 2001). In particular, methods have recently emerged for the acquisition of event extraction patterns without corpus annotation in view of the cost of manual labor for annotation. However, there has been little study of alternative representation models of extraction patterns for unsupervised acquisition. In the prior work on extraction pattern acquisition, the representation model of the patterns was based on a fixed set of pattern templates (Riloff, 1996), or predicate-argument relations, suc</context>
</contexts>
<marker>Riloff, 1993</marker>
<rawString>Ellen Riloff. 1993. Automatically Constructing a Dictionary for Information Extraction Tasks. In Proceedings of the Eleventh National Conference on Artificial Intelligence (AAAI-93).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>Automatically Generating Extraction Patterns from Untagged Text.</title>
<date>1996</date>
<booktitle>In Proceedings of Thirteenth National Conference on Artificial Intelligence (AAAI-96).</booktitle>
<contexts>
<context position="1271" citStr="Riloff, 1996" startWordPosition="192" endWordPosition="193"> introduce a new model, the Subtree model, based on arbitrary subtrees of dependency trees. We describe a discovery procedure for this model and demonstrate experimentally an improvement in recall using Subtree patterns. 1 Introduction Information Extraction (IE) is the process of identifying events or actions of interest and their participating entities from a text. As the field of IE has developed, the focus of study has moved towards automatic knowledge acquisition for information extraction, including domain-specific lexicons (Riloff, 1993; Riloff and Jones, 1999) and extraction patterns (Riloff, 1996; Yangarber et al., 2000; Sudo et al., 2001). In particular, methods have recently emerged for the acquisition of event extraction patterns without corpus annotation in view of the cost of manual labor for annotation. However, there has been little study of alternative representation models of extraction patterns for unsupervised acquisition. In the prior work on extraction pattern acquisition, the representation model of the patterns was based on a fixed set of pattern templates (Riloff, 1996), or predicate-argument relations, such as subject-verb, and object-verb (Yangarber et al., 2000). Th</context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>Ellen Riloff. 1996. Automatically Generating Extraction Patterns from Untagged Text. In Proceedings of Thirteenth National Conference on Artificial Intelligence (AAAI-96).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Kiyoshi Sudo</author>
<author>Chikashi Nobata</author>
</authors>
<title>Extended Named Entity Hierarchy.</title>
<date>2002</date>
<booktitle>In Proceedings of Third International Conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="10662" citStr="Sekine et al., 2002" startWordPosition="1660" endWordPosition="1663">score. 3 Acquisition Method This section discusses an automatic procedure to learn extraction patterns. Given a narrative description of the scenario and a set of source documents, the following three stages obtain the relevant extraction patterns for the scenario; preprocessing, document retrieval, and ranking pattern candidates. 3.1 Stage 1: Preprocessing Morphological analysis and Named Entities (NE) tagging are performed at this stage.7 Then all the sentences are converted into dependency trees by an appropriate dependency analyzer.$ The NE tagging 7We used Extended NE hierarchy based on (Sekine et al., 2002), which is structured and contains 150 classes. 8Any degree of detail can be chosen through entire procedure, from lexicalized dependency to chunk-level dependency. For the following experiment in Japanese, we define a node in replaces named entities by their class, so the resulting dependency trees contain some NE class names as leaf nodes. This is crucial to identifying common patterns, and to applying these patterns to new text. 3.2 Stage 2: Document Retrieval The procedure retrieves a set of documents that describe the events of the scenario of interest, the relevant document set. A set of</context>
<context position="16579" citStr="Sekine et al., 2002" startWordPosition="2654" endWordPosition="2657">ance (Spearman correlation coefficient with 2% confidence). 3.5 Filtering For efficiency and to eliminate low-frequency noise, we filtered out the pattern candidates that appear in less than 3 documents throughout the entire collection. Also, since the patterns with too much context are unlikely to match with new text, we added another filtering criterion based on the number of nodes in a pattern candidate; the maximum number of nodes is 8. Since all the slot-fillers in the extraction task of our experiment are assumed to be instances of the 150 classes in the extended Named Entity hierarchy (Sekine et al., 2002), further filtering was done by requiring a pattern candidate to contain at least one Named Entity class. 4 Experiment The experiment of this study is focused on comparing the performance of the earlier extraction pattern models to the proposed Subtree Model (SUBT). The compared models are the direct predicate-argument model (PA)9, and the Chain model (CH) in (Sudo et al., 2001). The task for this experiment is entity extraction, which is to identify all the entities participating in relevant events in a set of given Japanese texts. Note that all NEs in the test documents were identified manua</context>
</contexts>
<marker>Sekine, Sudo, Nobata, 2002</marker>
<rawString>Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata. 2002. Extended Named Entity Hierarchy. In Proceedings of Third International Conference on Language Resources and Evaluation (LREC 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoshi Sudo</author>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
</authors>
<title>Automatic Pattern Acquisition for Japanese Information Extraction.</title>
<date>2001</date>
<booktitle>In Proceedings of the Human Language Technology Conference (HLT2001).</booktitle>
<contexts>
<context position="1315" citStr="Sudo et al., 2001" startWordPosition="198" endWordPosition="201">del, based on arbitrary subtrees of dependency trees. We describe a discovery procedure for this model and demonstrate experimentally an improvement in recall using Subtree patterns. 1 Introduction Information Extraction (IE) is the process of identifying events or actions of interest and their participating entities from a text. As the field of IE has developed, the focus of study has moved towards automatic knowledge acquisition for information extraction, including domain-specific lexicons (Riloff, 1993; Riloff and Jones, 1999) and extraction patterns (Riloff, 1996; Yangarber et al., 2000; Sudo et al., 2001). In particular, methods have recently emerged for the acquisition of event extraction patterns without corpus annotation in view of the cost of manual labor for annotation. However, there has been little study of alternative representation models of extraction patterns for unsupervised acquisition. In the prior work on extraction pattern acquisition, the representation model of the patterns was based on a fixed set of pattern templates (Riloff, 1996), or predicate-argument relations, such as subject-verb, and object-verb (Yangarber et al., 2000). The model of our previous work (Sudo et al., 2</context>
<context position="3092" citStr="Sudo et al., 2001" startWordPosition="464" endWordPosition="467">presentation. Section 3 shows the method for automatic acquisition. Section 4 gives the experimental results of the comparison to other methods and Section 5 presents an analysis of these results. Finally, Section 6 provides some concluding remarks and perspective on future research. 2 Subtree model Our research on improved representation models for extraction patterns is motivated by the limitations of the prior extraction pattern representations. In this section, we review two of the previous models in detail, namely the Predicate-Argument model (Yangarber et al., 2000) and the Chain model (Sudo et al., 2001). The main cause of difficulty in finding entities by extraction patterns is the fact that the participating entities can appear not only as an argument of the predicate that describes the event type, but also in other places within the sentence or in the prior text. In the MUC-3 terrorism scenario, WEAPON entities occur in many different relations to event predicates in the documents. Even if WEAPON entities appear in the same sentence with the event predicate, they rarely serve as a direct argument of such predicates. (e.g., “One person was killed as the result of a bomb explosion.”) Predica</context>
<context position="5895" citStr="Sudo et al., 2001" startWordPosition="926" endWordPosition="929">in , is its argument and the relation between and is labeled with . The labels introduced in this paper are SBJ (subject), OBJ (object), ADV (adverbial adjunct), REL (relative), APPOS (apposition) and prepositions (IN, OF, etc.). Also, we assume that the order of the arguments does not matter. Symbols beginning with C- represent NE (Named Entity) types. 3Yangarber refers this as a noun phrase pattern in (Yangarber et al., 2000). 4This is the problem of merging the result of entity extraction. Most IE systems have hard-coded inference rules, such Chain model Our previous work, the Chain model (Sudo et al., 2001)5 attempts to remedy the limitations of the Predicate-Argument model. The extraction patterns generated by the Chain model are any chain-shaped paths in the dependency tree.6 Thus it successfully avoids the clausal boundary and embedded entity limitation. We reported a 5% gain in recall at the same precision level in the MUC-6 management succession task compared to the Predicate-Argument model. However, the Chain model also has its own weakness in terms of accuracy due to the lack of context. For example, in Figure 1(c), (triggered ( C-DATE - ADV)) is needed to extract the date entity. However</context>
<context position="8091" citStr="Sudo et al., 2001" startWordPosition="1287" endWordPosition="1290">arge number of pattern candidates — all possible subtrees of the dependency tree of each sentence in the corpus — must be considered. An efficient procedure is required to select the appropriate patterns from among the candidates. Also, as the number of pattern candidates increases, the amount of noise and complexity inas “triggering an explosion is related to killing or injuring and therefore constitutes one terrorism action.” 5Originally we called it “Tree-Based Representation of Patterns”. We renamed it to avoid confusion with the proposed approach that is also based on dependency trees. 6(Sudo et al., 2001) required the root node of the chain to be a verbal predicate, but we have relaxed that constraint for our experiments. JERUSALEM, March 21 – A smiling Palestinian suicide bomber triggered a massive explosion in the heavily policed heart of downtown Jerusalem today, killing himself and three other people and injuring scores. Predicate-Argument Chain model (triggered ( C-PERSON -SBJ)(explosion-OBJ)( C-DATE -ADV)) (killing ( C-PERSON -OBJ)) (injuring ( C-PERSON -OBJ)) (triggered ( C-PERSON -SBJ)) (triggered (heart-IN ( C-LOCATION -OF))) (triggered (killing-ADV ( C-PERSON -OBJ))) (triggered (inju</context>
<context position="16960" citStr="Sudo et al., 2001" startWordPosition="2716" endWordPosition="2719">des in a pattern candidate; the maximum number of nodes is 8. Since all the slot-fillers in the extraction task of our experiment are assumed to be instances of the 150 classes in the extended Named Entity hierarchy (Sekine et al., 2002), further filtering was done by requiring a pattern candidate to contain at least one Named Entity class. 4 Experiment The experiment of this study is focused on comparing the performance of the earlier extraction pattern models to the proposed Subtree Model (SUBT). The compared models are the direct predicate-argument model (PA)9, and the Chain model (CH) in (Sudo et al., 2001). The task for this experiment is entity extraction, which is to identify all the entities participating in relevant events in a set of given Japanese texts. Note that all NEs in the test documents were identified manually, so that the task can measure only how well extraction patterns can distinguish the participating entities from the entities that are not related to any events. This task does not involve grouping entities associated with the same event into a single template to avoid possible effect of merging failure on extraction performance for entities. We accumulated the test set of do</context>
</contexts>
<marker>Sudo, Sekine, Grishman, 2001</marker>
<rawString>Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman. 2001. Automatic Pattern Acquisition for Japanese Information Extraction. In Proceedings of the Human Language Technology Conference (HLT2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
<author>Ralph Grishman</author>
<author>Pasi Tapanainen</author>
<author>Silja Huttunen</author>
</authors>
<title>Unsupervised Discovery of Scenario-Level Patterns for Information Extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of 18th International Conference on Computational Linguistics (COLING-2000).</booktitle>
<contexts>
<context position="1295" citStr="Yangarber et al., 2000" startWordPosition="194" endWordPosition="197">ew model, the Subtree model, based on arbitrary subtrees of dependency trees. We describe a discovery procedure for this model and demonstrate experimentally an improvement in recall using Subtree patterns. 1 Introduction Information Extraction (IE) is the process of identifying events or actions of interest and their participating entities from a text. As the field of IE has developed, the focus of study has moved towards automatic knowledge acquisition for information extraction, including domain-specific lexicons (Riloff, 1993; Riloff and Jones, 1999) and extraction patterns (Riloff, 1996; Yangarber et al., 2000; Sudo et al., 2001). In particular, methods have recently emerged for the acquisition of event extraction patterns without corpus annotation in view of the cost of manual labor for annotation. However, there has been little study of alternative representation models of extraction patterns for unsupervised acquisition. In the prior work on extraction pattern acquisition, the representation model of the patterns was based on a fixed set of pattern templates (Riloff, 1996), or predicate-argument relations, such as subject-verb, and object-verb (Yangarber et al., 2000). The model of our previous </context>
<context position="3052" citStr="Yangarber et al., 2000" startWordPosition="455" endWordPosition="459">s the Subtree model for extraction pattern representation. Section 3 shows the method for automatic acquisition. Section 4 gives the experimental results of the comparison to other methods and Section 5 presents an analysis of these results. Finally, Section 6 provides some concluding remarks and perspective on future research. 2 Subtree model Our research on improved representation models for extraction patterns is motivated by the limitations of the prior extraction pattern representations. In this section, we review two of the previous models in detail, namely the Predicate-Argument model (Yangarber et al., 2000) and the Chain model (Sudo et al., 2001). The main cause of difficulty in finding entities by extraction patterns is the fact that the participating entities can appear not only as an argument of the predicate that describes the event type, but also in other places within the sentence or in the prior text. In the MUC-3 terrorism scenario, WEAPON entities occur in many different relations to event predicates in the documents. Even if WEAPON entities appear in the same sentence with the event predicate, they rarely serve as a direct argument of such predicates. (e.g., “One person was killed as t</context>
<context position="5708" citStr="Yangarber et al., 2000" startWordPosition="893" endWordPosition="897"> the class is associated to one of the slots in the template. The notation of the patterns in this paper is based on a dependency tree where ( ( - )..( - )) denotes is the head, and, for each in , is its argument and the relation between and is labeled with . The labels introduced in this paper are SBJ (subject), OBJ (object), ADV (adverbial adjunct), REL (relative), APPOS (apposition) and prepositions (IN, OF, etc.). Also, we assume that the order of the arguments does not matter. Symbols beginning with C- represent NE (Named Entity) types. 3Yangarber refers this as a noun phrase pattern in (Yangarber et al., 2000). 4This is the problem of merging the result of entity extraction. Most IE systems have hard-coded inference rules, such Chain model Our previous work, the Chain model (Sudo et al., 2001)5 attempts to remedy the limitations of the Predicate-Argument model. The extraction patterns generated by the Chain model are any chain-shaped paths in the dependency tree.6 Thus it successfully avoids the clausal boundary and embedded entity limitation. We reported a 5% gain in recall at the same precision level in the MUC-6 management succession task compared to the Predicate-Argument model. However, the Ch</context>
<context position="18667" citStr="Yangarber et al., 2000" startWordPosition="2982" endWordPosition="2985">ted into dependency trees by KNP (Kurohashi and Nagao, 1994). Regardless of the model of extraction patterns, the pattern acquisition follows the procedure described in Section 3. We retrieved 300 documents as a relevant document set. The association of NE classes and slots in the template is made automatically; Person, Organization, Post (slots) correspond to C-PERSON, CORG, C-POST (NE-classes), respectively, in the Succession scenario, and Suspect, Arresting Agency, Charge (slots) correspond to C-PERSON, C-ORG, C-OFFENCE (NE-classes), respectively, in the ArTThis is a restricted version of (Yangarber et al., 2000) constrained to have a single place-holder for each pattern, while (Yangarber et al., 2000) allowed more than one place-holder. However, the difference does not matter for the entity extraction task which does not require merging entities in a single template. Succession Arrest IR description Management Succession: Management Succes- A relevant document must describe the arrest of (translation sion at the level of executives of a company. The the suspect of murder. The document should be of Japanese) topic of interest should not be limited to the pro- regarded as interesting if it discusses th</context>
</contexts>
<marker>Yangarber, Grishman, Tapanainen, Huttunen, 2000</marker>
<rawString>Roman Yangarber, Ralph Grishman, Pasi Tapanainen, and Silja Huttunen. 2000. Unsupervised Discovery of Scenario-Level Patterns for Information Extraction. In Proceedings of 18th International Conference on Computational Linguistics (COLING-2000).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>