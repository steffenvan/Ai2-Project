<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014124">
<title confidence="0.9958">
CodeX: Combining an SVM Classifier and Character N-gram Language
Models for Sentiment Analysis on Twitter Text
</title>
<author confidence="0.998103">
Qi Han, Junfei Guo and Hinrich Sch¨utze
</author>
<affiliation confidence="0.9971765">
Institute for Nature Language Processing
University of Stuttgart
</affiliation>
<address confidence="0.545283">
Stuttgart, Germany
</address>
<email confidence="0.995924">
{hanqi, guojf}@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.998521" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998411375">
This paper briefly reports our system for the
SemEval-2013 Task 2: sentiment analysis in
Twitter. We first used an SVM classifier with
a wide range of features, including bag of
word features (unigram, bigram), POS fea-
tures, stylistic features, readability scores and
other statistics of the tweet being analyzed,
domain names, abbreviations, emoticons in
the Twitter text. Then we investigated the ef-
fectiveness of these features. We also used
character n-gram language models to address
the problem of high lexical variation in Twit-
ter text and combined the two approaches to
obtain the final results. Our system is robust
and achieves good performance on the Twitter
test data as well as the SMS test data.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99989447368421">
The challenge of the SemEval-2013 Task 2 (Task
B) is the “Message Polarity Classification” (Wilson
et al., 2013). Specifically, the task was to classify
whether a given message has positive, negative or
neutral sentiment; for messages conveying both pos-
itive and negative sentiment, whichever is stronger
should be chosen.
In recent years, text messaging and microblog-
ging such as tweeting has gained its popularity.
Since these short messages are often used not only
to discuss facts but also to share opinions and sen-
timents, sentiment analysis on this type of data has
lately become interesting. However, some features
of this type of data make natural language process-
ing challenging. For example, the messages are usu-
ally short and the language used can be very in-
formal, with misspellings, creative spellings, slang,
URLs and special abbreviations. Some research has
already been done attempting to address these prob-
lems, to enable sentiment analysis on this type of
data, in particular on Twitter data, and even to use
the outcome of sentiment analysis to make predic-
tions (Jansen et al., 2009; Barbosa and Feng, 2010;
Bifet and Frank, 2010; Davidov et al., 2010; Jiang et
al., 2011; Pak and Paroubek, 2010; Saif et al., 2012;
Tumasjan et al., 2010).
As the research mentioned above, our system used
a machine learning based approach for sentiment
analysis. Our system combines results from an SVM
classifier using a wide range of features as well as
votes derived from character n-gram language mod-
els to do the final prediction.
The rest of this paper is organized as follows. Sec-
tion 2 describes the features used for the SVM clas-
sifier. Section 3 describes how the votes from char-
acter n-gram language models were derived. Section
4 describes the details of our method. And finally
section 5 presents the results.
</bodyText>
<sectionHeader confidence="0.998817" genericHeader="introduction">
2 Features
</sectionHeader>
<bodyText confidence="0.999947111111111">
We pre-processed the tweets as follows: i) tok-
enized the tweets using a tokenizer suitable for Twit-
ter data, which, for example, recognize emoticons
and hashtags; ii) replaced all URLs with the token
twitterurl; iii) replaced all Twitter usernames with
the token @twitterusername; iv) converted all to-
kens into lower case; v) replaced all sequences of
repeated characters by three characters, for example,
convert gooooood to goood, this way we recognize
</bodyText>
<page confidence="0.952968">
520
</page>
<bodyText confidence="0.981455">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 520–524, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
the emphasized usage of the word; vi) expanded ab-
breviations with a dictionary,1 which we will refer
to as noslang dictionary; vii) appended neg to all
words from one position before a negation word to
the next punctuation mark.
We represented each given tweet using 6 feature
families:
</bodyText>
<listItem confidence="0.993301971428572">
• Lexical features (UG, BG): Number of times
each unigram appears in the tweet (UG); num-
ber of times each bigram appears in the tweet
(BG).
• POS features (POS U, POS B): Number of
times each POS appears in the tweet divided by
number of tokens of that tweet (POS U); num-
ber of times each POS bigram appears in the
tweet (POS B). To tag the tweet we used the
ark-twitter-nlp tagger.2
• Statistical features (STAT): Various readabil-
ity scores (ARI, Flesch Reading Ease, RIX,
LIX, Coleman Liau Index, SMOG Index, Gun-
ning Fog Index, Flesch-Kincaid Grade Level)
of the tweet; some simple statistics of the tweet
(average count of words per sentence, complex
word count, syllable count, sentence count,
word count, char count). We calculated the
statistics and scores after pre-processing step
vi). We then normalized these scores so that
they had mean 0 and standard deviation 1.
• Stylistic features (STY): Number of times
an emoticon appears in the tweet, number of
words which are written in all capital let-
ters, number of words containing characters
repeated consecutively more than three times,
number of words containing characters re-
peated consecutively more than four times. We
calculated these features after pre-processing
step i). We used the binarized and the logarith-
mically scaled version of these features.
• Abbreviation features (ABB): For every term
in the noslang dictionary, we checked whether
it was present in the tweet or not and used this
as a feature.
</listItem>
<footnote confidence="0.9989555">
1http://www.noslang.com
2http://www.ark.cs.cmu.edu/TweetNLP/
</footnote>
<listItem confidence="0.912918">
• URL features (URL): We expanded the URLs
</listItem>
<bodyText confidence="0.935208052631579">
in the Twitter text and collected all the domain
names which the URLs in the training set point
to, and used them as binary features.
Feature sets UG, BG, POS U, POS B are com-
mon features for sentiment analysis (Pang et al.,
2002). Remus (2011) showed that incorporat-
ing readability measures as features can improve
sentence-level subjectivity classification. Stylistic
features have also been used in sentiment analysis on
Twitter data (Go et al., 2010). Some abbreviations
express sentiment which is not apparent from word
level. For example lolwtime, which means laugh-
ing out loud with tears in my eyes, expresses positive
sentiment overall, but this does not follow directly at
the sentiment of individual words, so the feature set
ABB might be helpful. Finally, we conjecture that a
tweet including an URL pointing to youtube.com
is more likely to be subjective than a tweet including
an URL pointing to a news website.
</bodyText>
<sectionHeader confidence="0.620285" genericHeader="method">
3 Integrating votes from language models
based on character n-grams
</sectionHeader>
<bodyText confidence="0.98332">
Language Models can be used for text classification
tasks. Since the goal of the SemEval-2013 Task 2
(Task B) is to classify each tweet into one of the
three classes: positive, negative or neutral, a lan-
guage model approach can be used.
Emoticon-smoothed language models have been
used to do Twitter sentiment analysis (Liu et al.,
2012). The language models used there were based
on words. However, there is evidence (Aisopos et
al., 2012; Raaijmakers and Kraaij, 2008) showing
that super-word character n-gram features can be
quite effective for sentiment analysis on short infor-
mal data. This is because noise and mis-spellings
tend to have smaller impact on substring patterns
than on word patterns. Our system used language
models based on character n-grams to improve the
performance of sentiment analysis on tweets.
For every tweet we constructed 3 sequences of
character-trigrams and 4 sequences of character-
four-grams. For instance, the tweet &amp;quot;Hello
World!&amp;quot; would have 7 corresponding substring
representations:
&lt;s&gt;&lt;s&gt;H ell o W orl d!&lt;/s&gt;,
&lt;s&gt;He llo Wo rld !&lt;/s&gt;&lt;/s&gt;,
</bodyText>
<page confidence="0.958067">
521
</page>
<table confidence="0.9784248">
Hel lo Wor ld!,
&lt;s&gt;&lt;s&gt;&lt;s&gt;H ello Wor ld!&lt;/s&gt;
&lt;s&gt;&lt;s&gt;He llo Worl d!&lt;/s&gt;&lt;/s&gt;,
&lt;s&gt;Hel lo W orld !&lt;/s&gt;&lt;/s&gt;&lt;/s&gt;,
Hell o Wo rld!
</table>
<bodyText confidence="0.99870824137931">
where &lt;s&gt; means start of a sentence, &lt;/s&gt; means
end of a sentence, means whitespace. Using the
corresponding sequences of character-trigrams from
all positive tweets in training set we trained a lan-
guage model LM3 . To train the language model
we used Chen and Goodman’s modified Kneser-Ney
discounting for N-grams from the SRILM toolkit
(Stolcke, 2002). Given a new sequence of character-
trigrams derived from a positive tweet, it should
give a lower perplexity value than a language model
trained on sequences of character-trigrams from
negative tweets.
In this way we obtained 6 language models:
LM3 from character-trigram sequences of neg-
ative tweets, LMN3 from character-trigram se-
quences of neutral tweets, LM from character-
3trigram sequences of positive tweets, LM4 from
character-four-grams sequences of negative tweets,
LMN4 from character-four-gram sequences of neu-
tral tweets, LM4 from character-four-gram se-
quences of positive tweets.
For every new tweet, we first obtain the 7 corre-
sponding substring representations. Then for each
substring representation, we calculate 3 votes from
the language models. For instance, for a sequence
of character-trigrams, we first calculate three per-
plexity valuesP3 , P3N , P3� using language models
LM3 , LMN3 , LM3 then produce votes according
to the following discretization function:
</bodyText>
<equation confidence="0.9126755">
�
1 if Pn x ≥ Py n;
vote(LMx n, LMy n) =
−1 else.
</equation>
<bodyText confidence="0.999913375">
where n ∈ {3,4} is the length of the character n-
gram, x, y ∈ {−, +, N} are class labels and Pnx, Pny
are the corresponding perplexity values. In this way
we obtain 21 votes for every tweet. However, in the
final classification, every sentence got 42 votes, of
which 21 were derived from bigram language mod-
els of the substrings and 21 were from trigram lan-
guage models of these substrings.
</bodyText>
<table confidence="0.999641555555556">
Feature Sets Accuracy
UG,BG,POS U,POS B,STAT,STY,ABB,URL 0.692
BG,POS U,POS B,STAT,STY,ABB,URL 0.641
POS U,POS B,STAT,STY,ABB,URL 0.579
POS U,STAT,STY,ABB,URL 0.564
STAT,STY,ABB,URL 0.524
STY,ABB,URL 0.474
STY,URL 0.454
URL 0.441
</table>
<tableCaption confidence="0.9857138">
Table 1: Cross validation average accuracy with differ-
ent feature sets. we started with all 8 feature sets and
removed feature sets one by one, where we always first
removed the feature set that resulted in the biggest drop
in accuracy.
</tableCaption>
<sectionHeader confidence="0.998881" genericHeader="method">
4 Methods
</sectionHeader>
<bodyText confidence="0.999972571428571">
In this section we describe the methods used by our
system.
Firstly, we did feature selection on all the features
described in Section 2. Using Mutual Information
(Shannon and Weaver, 1949) and 10-fold cross vali-
dation we chose the top 13,500 features. Using these
features we trained an SVM classifier with the train-
ing data. As the implementation of the SVM classi-
fier we used liblinear (Fan et al., 2008). The SVM
classifier was then used to produce initial predictions
for messages in the development set, the Twitter test
set and the SMS test set.
Then, we represented every message in the devel-
opment set, the Twitter test set and the SMS test
set using the 42 votes we described in Section 3
together with the predictions of the SVM classifier
we described above. Using the Bagging algorithm
from the WEKA machine learning toolkit (Hall et
al., 2009) and the development set data, we trained
a new classifier and used this classifier for the final
prediction on Twitter test data and SMS test data.
</bodyText>
<sectionHeader confidence="0.999992" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.999836">
5.1 Feature analysis
</subsectionHeader>
<bodyText confidence="0.9999924">
To study the effectiveness of different features, we
started with all 8 feature sets and removed feature
sets one by one, where we always first removed the
feature set that resulted in the biggest drop in accu-
racy. We did 10 fold cross validation on training set
</bodyText>
<page confidence="0.994068">
522
</page>
<table confidence="0.999498333333333">
Feature Sets Accuracy
POS U,POS B,STAT,STY,ABB,URL 0.579
POS B,STY,ABB,URL 0.571
POS U,STY,ABB,URL 0.557
STAT,STY,ABB,URL 0.524
STY,ABB,URL 0.474
</table>
<tableCaption confidence="0.9682015">
Table 2: Cross validation average accuracy with further
combination of feature sets.
</tableCaption>
<table confidence="0.99986375">
Accuracy F1(pos, neg)
Majority Baseline 0.4123 0.2919
SVM Classifier 0.6612 0.5414
SVM + LM Votes 0.6457 0.5384
</table>
<tableCaption confidence="0.9875825">
Table 3: Overall accuracy and average F1 score for posi-
tive and negative classes on Twitter test data.
</tableCaption>
<bodyText confidence="0.997919714285715">
and used average accuracy as a metric.
As we can see from Table 1, lexical features were
the most important features – they counted for more
than 0.11 loss of accuracy when removed from the
features. POS features and statistical features were
also important, POS bigrams more so than POS uni-
grams. Stylistic, abbreviation and URL features, on
the contrary, seem to be only of moderate useful-
ness.
To further investigate the relationship between the
feature sets POS U, POS B and STAT, we did addi-
tional experiments. From Table 2, we can see that
removing all three feature sets caused a decrease
in accuracy to 0.47, including just one feature set
POS B, POS U or STAT resulted in accuracy above
0.57, 0.55 and 0.52 respectively. This shows that
all three feature sets were quite effective and POS B
was most useful. However, adding all of the three
feature sets only caused an increase in accuracy to
0.579, which suggests that they were highly corre-
lated.
</bodyText>
<table confidence="0.99886925">
Accuracy F1(pos, neg)
Majority Baseline 0.2350 0.1902
SVM Classifier 0.6504 0.5811
SVM + LM Votes 0.6418 0.5670
</table>
<tableCaption confidence="0.987613">
Table 4: Overall accuracy and average F1 score for posi-
tive and negative classes on SMS test data.
</tableCaption>
<subsectionHeader confidence="0.996692">
5.2 Effectiveness of language model features
</subsectionHeader>
<bodyText confidence="0.999970181818182">
To evaluate the effectiveness of features derived
from language models of character n-grams, we
compared the performance of our SVM classifier
and that of the classifier combining the SVM clas-
sifier results and language model features.3 We per-
formed our experiments on both of the Twitter test
data and the SMS test data. The results in Table 3
and Table 4 suggested that in our current setup, lan-
guage model features were not very helpful.
Table 3 and Table 4 also show that our system
improved the performance greatly compared to Ma-
jority baseline system,4. Compared with other par-
ticipants in the SemEval-2013 Task 2, our system
achieved average performance on Twitter test data.
However, it has been the ninth best out of all 48 sys-
tems for the performance on SMS test data. This
shows that our system can be easily adapted to dif-
ferent contexts without a big drop in performance.
One reason for that might be that we did not use any
sentiment lexicon developed specifically for Twitter
data and we used high level features like the statisti-
cal features and POS features for our classification.
</bodyText>
<sectionHeader confidence="0.996173" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999765">
This paper briefly reports our system designed for
the SemEval-2013 Task 2: sentiment analysis in
Twitter. We first used an SVM classifier with a wide
range of features. We found that simple statistics
of the tweets, for example word count or readabil-
ity scores, can help in sentiment analysis on Twitter
text.
We then used character n-gram language mod-
els to address the problem of high lexical variation
in Twitter text and combined the two approaches
to obtain the final results. Although in our current
setup, features derived from character n-gram lan-
guage models do not perform very well, they may
benefit from a larger training data set.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999891">
This work was funded by DFG projects SFB 732.
We would like to thank our colleagues at IMS.
</bodyText>
<footnote confidence="0.53825875">
3We accidentally used feature set POS B two times in our
representation, but it didn’t change the results significantly.
4To be consistent with the evaluation metric, we chose the
majority class of positive and negative classes.
</footnote>
<page confidence="0.998115">
523
</page>
<sectionHeader confidence="0.995773" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999648202247191">
Fotis Aisopos, George Papadakis, Konstantinos Tserpes,
and Theodora Varvarigou. 2012. Content vs. context
for sentiment analysis: a comparative analysis over
microblogs. In Proceedings of the 23rd ACM confer-
ence on Hypertext and social media, HT ’12, pages
187–96, New York, NY, USA. ACM.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, COLING ’10,
pages 36–44, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in twitter streaming data. In Proceed-
ings of the 13th international conference on Discov-
ery science, DS’10, pages 1–15, Berlin, Heidelberg.
Springer-Verlag.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ’10, pages 241–249, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: a li-
brary for large linear classification. J. Mach. Learn.
Res., 9:1871–1874, June.
Alec Go, Richa Bhayani, and L. Huang. 2010. Exploit-
ing the unique characteristics of tweets for sentiment
analysis. Technical report, Technical Report, Stanford
University.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10–18, November.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Twitter power: Tweets as elec-
tronic word of mouth. J. Am. Soc. Inf. Sci. Technol.,
60(11):2169–2188, November.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun
Zhao. 2011. Target-dependent twitter sentiment clas-
sification. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies - Volume 1, HLT ’11,
pages 151–160, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Kun-Lin Liu, Wu-Jun Li, and Minyi Guo. 2012. Emoti-
con smoothed language models for twitter sentiment
analysis. In Twenty-Sixth AAAI Conference on Artifi-
cial Intelligence.
A. Pak and P. Paroubek. 2010. Twitter as a corpus for
sentiment analysis and opinion mining. LREC.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. In Proceedings of EMNLP,
pages 79–86.
Stephan Raaijmakers and Wessel Kraaij. 2008. A shal-
low approach to subjectivity classification. Proceed-
ings of ICWSM, pages 216–217.
Robert Remus. 2011. Improving sentence-level subjec-
tivity classification through readability measurement.
May. Proceedings of the 18th Nordic Conference of
Computational Linguistics NODALIDA 2011.
Hassan Saif, Yulan He, and Harith Alani. 2012. Seman-
tic sentiment analysis of twitter. In Philippe Cudr-
Mauroux, Jeff Heflin, Evren Sirin, Tania Tudorache,
Jrme Euzenat, Manfred Hauswirth, Josiane Xavier
Parreira, Jim Hendler, Guus Schreiber, Abraham Bern-
stein, and Eva Blomqvist, editors, The Semantic Web
ISWC 2012, number 7649 in Lecture Notes in Com-
puter Science, pages 508–524. Springer Berlin Heidel-
berg, January.
Claude E. Shannon and Warren Weaver. 1949. Mathe-
matical Theory of Communication. University of Illi-
nois Press.
Andreas Stolcke. 2002. SRILMAn extensible language
modeling toolkit. In In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing
(ICSLP 2002, page 901904.
Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-
ner, and Isabell M. Welpe. 2010. Predicting elections
with twitter: What 140 characters reveal about politi-
cal sentiment. Proceedings of the Fourth International
AAAI Conference on Weblogs and Social Media.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ’13, June.
</reference>
<page confidence="0.998275">
524
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.654924">
<title confidence="0.9921295">CodeX: Combining an SVM Classifier and Character N-gram Language Models for Sentiment Analysis on Twitter Text</title>
<author confidence="0.989326">Junfei Guo Han</author>
<affiliation confidence="0.999655">Institute for Nature Language University of</affiliation>
<address confidence="0.6844">Stuttgart,</address>
<abstract confidence="0.998759117647059">This paper briefly reports our system for the SemEval-2013 Task 2: sentiment analysis in Twitter. We first used an SVM classifier with a wide range of features, including bag of word features (unigram, bigram), POS features, stylistic features, readability scores and other statistics of the tweet being analyzed, domain names, abbreviations, emoticons in the Twitter text. Then we investigated the effectiveness of these features. We also used character n-gram language models to address the problem of high lexical variation in Twitter text and combined the two approaches to obtain the final results. Our system is robust and achieves good performance on the Twitter test data as well as the SMS test data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Fotis Aisopos</author>
<author>George Papadakis</author>
<author>Konstantinos Tserpes</author>
<author>Theodora Varvarigou</author>
</authors>
<title>Content vs. context for sentiment analysis: a comparative analysis over microblogs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 23rd ACM conference on Hypertext and social media, HT ’12,</booktitle>
<pages>187--96</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6848" citStr="Aisopos et al., 2012" startWordPosition="1098" endWordPosition="1101">ointing to youtube.com is more likely to be subjective than a tweet including an URL pointing to a news website. 3 Integrating votes from language models based on character n-grams Language Models can be used for text classification tasks. Since the goal of the SemEval-2013 Task 2 (Task B) is to classify each tweet into one of the three classes: positive, negative or neutral, a language model approach can be used. Emoticon-smoothed language models have been used to do Twitter sentiment analysis (Liu et al., 2012). The language models used there were based on words. However, there is evidence (Aisopos et al., 2012; Raaijmakers and Kraaij, 2008) showing that super-word character n-gram features can be quite effective for sentiment analysis on short informal data. This is because noise and mis-spellings tend to have smaller impact on substring patterns than on word patterns. Our system used language models based on character n-grams to improve the performance of sentiment analysis on tweets. For every tweet we constructed 3 sequences of character-trigrams and 4 sequences of characterfour-grams. For instance, the tweet &amp;quot;Hello World!&amp;quot; would have 7 corresponding substring representations: &lt;s&gt;&lt;s&gt;H ell o W or</context>
</contexts>
<marker>Aisopos, Papadakis, Tserpes, Varvarigou, 2012</marker>
<rawString>Fotis Aisopos, George Papadakis, Konstantinos Tserpes, and Theodora Varvarigou. 2012. Content vs. context for sentiment analysis: a comparative analysis over microblogs. In Proceedings of the 23rd ACM conference on Hypertext and social media, HT ’12, pages 187–96, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luciano Barbosa</author>
<author>Junlan Feng</author>
</authors>
<title>Robust sentiment detection on twitter from biased and noisy data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10,</booktitle>
<pages>36--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2130" citStr="Barbosa and Feng, 2010" startWordPosition="331" endWordPosition="334">opinions and sentiments, sentiment analysis on this type of data has lately become interesting. However, some features of this type of data make natural language processing challenging. For example, the messages are usually short and the language used can be very informal, with misspellings, creative spellings, slang, URLs and special abbreviations. Some research has already been done attempting to address these problems, to enable sentiment analysis on this type of data, in particular on Twitter data, and even to use the outcome of sentiment analysis to make predictions (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Jiang et al., 2011; Pak and Paroubek, 2010; Saif et al., 2012; Tumasjan et al., 2010). As the research mentioned above, our system used a machine learning based approach for sentiment analysis. Our system combines results from an SVM classifier using a wide range of features as well as votes derived from character n-gram language models to do the final prediction. The rest of this paper is organized as follows. Section 2 describes the features used for the SVM classifier. Section 3 describes how the votes from character n-gram language models were</context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Luciano Barbosa and Junlan Feng. 2010. Robust sentiment detection on twitter from biased and noisy data. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, pages 36–44, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Bifet</author>
<author>Eibe Frank</author>
</authors>
<title>Sentiment knowledge discovery in twitter streaming data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 13th international conference on Discovery science, DS’10,</booktitle>
<pages>1--15</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="2153" citStr="Bifet and Frank, 2010" startWordPosition="335" endWordPosition="338"> sentiment analysis on this type of data has lately become interesting. However, some features of this type of data make natural language processing challenging. For example, the messages are usually short and the language used can be very informal, with misspellings, creative spellings, slang, URLs and special abbreviations. Some research has already been done attempting to address these problems, to enable sentiment analysis on this type of data, in particular on Twitter data, and even to use the outcome of sentiment analysis to make predictions (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Jiang et al., 2011; Pak and Paroubek, 2010; Saif et al., 2012; Tumasjan et al., 2010). As the research mentioned above, our system used a machine learning based approach for sentiment analysis. Our system combines results from an SVM classifier using a wide range of features as well as votes derived from character n-gram language models to do the final prediction. The rest of this paper is organized as follows. Section 2 describes the features used for the SVM classifier. Section 3 describes how the votes from character n-gram language models were derived. Section 4 des</context>
</contexts>
<marker>Bifet, Frank, 2010</marker>
<rawString>Albert Bifet and Eibe Frank. 2010. Sentiment knowledge discovery in twitter streaming data. In Proceedings of the 13th international conference on Discovery science, DS’10, pages 1–15, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced sentiment learning using twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10,</booktitle>
<pages>241--249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2175" citStr="Davidov et al., 2010" startWordPosition="339" endWordPosition="342">this type of data has lately become interesting. However, some features of this type of data make natural language processing challenging. For example, the messages are usually short and the language used can be very informal, with misspellings, creative spellings, slang, URLs and special abbreviations. Some research has already been done attempting to address these problems, to enable sentiment analysis on this type of data, in particular on Twitter data, and even to use the outcome of sentiment analysis to make predictions (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Jiang et al., 2011; Pak and Paroubek, 2010; Saif et al., 2012; Tumasjan et al., 2010). As the research mentioned above, our system used a machine learning based approach for sentiment analysis. Our system combines results from an SVM classifier using a wide range of features as well as votes derived from character n-gram language models to do the final prediction. The rest of this paper is organized as follows. Section 2 describes the features used for the SVM classifier. Section 3 describes how the votes from character n-gram language models were derived. Section 4 describes the details of </context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using twitter hashtags and smileys. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, pages 241–249, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: a library for large linear classification.</title>
<date>2008</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>9--1871</pages>
<contexts>
<context position="10271" citStr="Fan et al., 2008" startWordPosition="1647" endWordPosition="1650">uracy with different feature sets. we started with all 8 feature sets and removed feature sets one by one, where we always first removed the feature set that resulted in the biggest drop in accuracy. 4 Methods In this section we describe the methods used by our system. Firstly, we did feature selection on all the features described in Section 2. Using Mutual Information (Shannon and Weaver, 1949) and 10-fold cross validation we chose the top 13,500 features. Using these features we trained an SVM classifier with the training data. As the implementation of the SVM classifier we used liblinear (Fan et al., 2008). The SVM classifier was then used to produce initial predictions for messages in the development set, the Twitter test set and the SMS test set. Then, we represented every message in the development set, the Twitter test set and the SMS test set using the 42 votes we described in Section 3 together with the predictions of the SVM classifier we described above. Using the Bagging algorithm from the WEKA machine learning toolkit (Hall et al., 2009) and the development set data, we trained a new classifier and used this classifier for the final prediction on Twitter test data and SMS test data. 5</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: a library for large linear classification. J. Mach. Learn. Res., 9:1871–1874, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>L Huang</author>
</authors>
<title>Exploiting the unique characteristics of tweets for sentiment analysis.</title>
<date>2010</date>
<tech>Technical report, Technical Report,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="5870" citStr="Go et al., 2010" startWordPosition="936" endWordPosition="939">s present in the tweet or not and used this as a feature. 1http://www.noslang.com 2http://www.ark.cs.cmu.edu/TweetNLP/ • URL features (URL): We expanded the URLs in the Twitter text and collected all the domain names which the URLs in the training set point to, and used them as binary features. Feature sets UG, BG, POS U, POS B are common features for sentiment analysis (Pang et al., 2002). Remus (2011) showed that incorporating readability measures as features can improve sentence-level subjectivity classification. Stylistic features have also been used in sentiment analysis on Twitter data (Go et al., 2010). Some abbreviations express sentiment which is not apparent from word level. For example lolwtime, which means laughing out loud with tears in my eyes, expresses positive sentiment overall, but this does not follow directly at the sentiment of individual words, so the feature set ABB might be helpful. Finally, we conjecture that a tweet including an URL pointing to youtube.com is more likely to be subjective than a tweet including an URL pointing to a news website. 3 Integrating votes from language models based on character n-grams Language Models can be used for text classification tasks. Si</context>
</contexts>
<marker>Go, Bhayani, Huang, 2010</marker>
<rawString>Alec Go, Richa Bhayani, and L. Huang. 2010. Exploiting the unique characteristics of tweets for sentiment analysis. Technical report, Technical Report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="10721" citStr="Hall et al., 2009" startWordPosition="1725" endWordPosition="1728">500 features. Using these features we trained an SVM classifier with the training data. As the implementation of the SVM classifier we used liblinear (Fan et al., 2008). The SVM classifier was then used to produce initial predictions for messages in the development set, the Twitter test set and the SMS test set. Then, we represented every message in the development set, the Twitter test set and the SMS test set using the 42 votes we described in Section 3 together with the predictions of the SVM classifier we described above. Using the Bagging algorithm from the WEKA machine learning toolkit (Hall et al., 2009) and the development set data, we trained a new classifier and used this classifier for the final prediction on Twitter test data and SMS test data. 5 Results 5.1 Feature analysis To study the effectiveness of different features, we started with all 8 feature sets and removed feature sets one by one, where we always first removed the feature set that resulted in the biggest drop in accuracy. We did 10 fold cross validation on training set 522 Feature Sets Accuracy POS U,POS B,STAT,STY,ABB,URL 0.579 POS B,STY,ABB,URL 0.571 POS U,STY,ABB,URL 0.557 STAT,STY,ABB,URL 0.524 STY,ABB,URL 0.474 Table 2</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: an update. SIGKDD Explor. Newsl., 11(1):10–18, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard J Jansen</author>
<author>Mimi Zhang</author>
<author>Kate Sobel</author>
<author>Abdur Chowdury</author>
</authors>
<title>Twitter power: Tweets as electronic word of mouth.</title>
<date>2009</date>
<journal>J. Am. Soc. Inf. Sci. Technol.,</journal>
<volume>60</volume>
<issue>11</issue>
<contexts>
<context position="2106" citStr="Jansen et al., 2009" startWordPosition="327" endWordPosition="330">ts but also to share opinions and sentiments, sentiment analysis on this type of data has lately become interesting. However, some features of this type of data make natural language processing challenging. For example, the messages are usually short and the language used can be very informal, with misspellings, creative spellings, slang, URLs and special abbreviations. Some research has already been done attempting to address these problems, to enable sentiment analysis on this type of data, in particular on Twitter data, and even to use the outcome of sentiment analysis to make predictions (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Jiang et al., 2011; Pak and Paroubek, 2010; Saif et al., 2012; Tumasjan et al., 2010). As the research mentioned above, our system used a machine learning based approach for sentiment analysis. Our system combines results from an SVM classifier using a wide range of features as well as votes derived from character n-gram language models to do the final prediction. The rest of this paper is organized as follows. Section 2 describes the features used for the SVM classifier. Section 3 describes how the votes from character n-g</context>
</contexts>
<marker>Jansen, Zhang, Sobel, Chowdury, 2009</marker>
<rawString>Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur Chowdury. 2009. Twitter power: Tweets as electronic word of mouth. J. Am. Soc. Inf. Sci. Technol., 60(11):2169–2188, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Jiang</author>
<author>Mo Yu</author>
<author>Ming Zhou</author>
<author>Xiaohua Liu</author>
<author>Tiejun Zhao</author>
</authors>
<title>Target-dependent twitter sentiment classification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>151--160</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2195" citStr="Jiang et al., 2011" startWordPosition="343" endWordPosition="346">lately become interesting. However, some features of this type of data make natural language processing challenging. For example, the messages are usually short and the language used can be very informal, with misspellings, creative spellings, slang, URLs and special abbreviations. Some research has already been done attempting to address these problems, to enable sentiment analysis on this type of data, in particular on Twitter data, and even to use the outcome of sentiment analysis to make predictions (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Jiang et al., 2011; Pak and Paroubek, 2010; Saif et al., 2012; Tumasjan et al., 2010). As the research mentioned above, our system used a machine learning based approach for sentiment analysis. Our system combines results from an SVM classifier using a wide range of features as well as votes derived from character n-gram language models to do the final prediction. The rest of this paper is organized as follows. Section 2 describes the features used for the SVM classifier. Section 3 describes how the votes from character n-gram language models were derived. Section 4 describes the details of our method. And fina</context>
</contexts>
<marker>Jiang, Yu, Zhou, Liu, Zhao, 2011</marker>
<rawString>Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent twitter sentiment classification. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 151–160, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun-Lin Liu</author>
<author>Wu-Jun Li</author>
<author>Minyi Guo</author>
</authors>
<title>Emoticon smoothed language models for twitter sentiment analysis.</title>
<date>2012</date>
<booktitle>In Twenty-Sixth AAAI Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="6746" citStr="Liu et al., 2012" startWordPosition="1081" endWordPosition="1084">ds, so the feature set ABB might be helpful. Finally, we conjecture that a tweet including an URL pointing to youtube.com is more likely to be subjective than a tweet including an URL pointing to a news website. 3 Integrating votes from language models based on character n-grams Language Models can be used for text classification tasks. Since the goal of the SemEval-2013 Task 2 (Task B) is to classify each tweet into one of the three classes: positive, negative or neutral, a language model approach can be used. Emoticon-smoothed language models have been used to do Twitter sentiment analysis (Liu et al., 2012). The language models used there were based on words. However, there is evidence (Aisopos et al., 2012; Raaijmakers and Kraaij, 2008) showing that super-word character n-gram features can be quite effective for sentiment analysis on short informal data. This is because noise and mis-spellings tend to have smaller impact on substring patterns than on word patterns. Our system used language models based on character n-grams to improve the performance of sentiment analysis on tweets. For every tweet we constructed 3 sequences of character-trigrams and 4 sequences of characterfour-grams. For insta</context>
</contexts>
<marker>Liu, Li, Guo, 2012</marker>
<rawString>Kun-Lin Liu, Wu-Jun Li, and Minyi Guo. 2012. Emoticon smoothed language models for twitter sentiment analysis. In Twenty-Sixth AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pak</author>
<author>P Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<publisher>LREC.</publisher>
<contexts>
<context position="2219" citStr="Pak and Paroubek, 2010" startWordPosition="347" endWordPosition="350">sting. However, some features of this type of data make natural language processing challenging. For example, the messages are usually short and the language used can be very informal, with misspellings, creative spellings, slang, URLs and special abbreviations. Some research has already been done attempting to address these problems, to enable sentiment analysis on this type of data, in particular on Twitter data, and even to use the outcome of sentiment analysis to make predictions (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Jiang et al., 2011; Pak and Paroubek, 2010; Saif et al., 2012; Tumasjan et al., 2010). As the research mentioned above, our system used a machine learning based approach for sentiment analysis. Our system combines results from an SVM classifier using a wide range of features as well as votes derived from character n-gram language models to do the final prediction. The rest of this paper is organized as follows. Section 2 describes the features used for the SVM classifier. Section 3 describes how the votes from character n-gram language models were derived. Section 4 describes the details of our method. And finally section 5 presents t</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>A. Pak and P. Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="5646" citStr="Pang et al., 2002" startWordPosition="905" endWordPosition="908">ted these features after pre-processing step i). We used the binarized and the logarithmically scaled version of these features. • Abbreviation features (ABB): For every term in the noslang dictionary, we checked whether it was present in the tweet or not and used this as a feature. 1http://www.noslang.com 2http://www.ark.cs.cmu.edu/TweetNLP/ • URL features (URL): We expanded the URLs in the Twitter text and collected all the domain names which the URLs in the training set point to, and used them as binary features. Feature sets UG, BG, POS U, POS B are common features for sentiment analysis (Pang et al., 2002). Remus (2011) showed that incorporating readability measures as features can improve sentence-level subjectivity classification. Stylistic features have also been used in sentiment analysis on Twitter data (Go et al., 2010). Some abbreviations express sentiment which is not apparent from word level. For example lolwtime, which means laughing out loud with tears in my eyes, expresses positive sentiment overall, but this does not follow directly at the sentiment of individual words, so the feature set ABB might be helpful. Finally, we conjecture that a tweet including an URL pointing to youtube</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? sentiment classification using machine learning techniques. In Proceedings of EMNLP, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Raaijmakers</author>
<author>Wessel Kraaij</author>
</authors>
<title>A shallow approach to subjectivity classification.</title>
<date>2008</date>
<booktitle>Proceedings of ICWSM,</booktitle>
<pages>216--217</pages>
<contexts>
<context position="6879" citStr="Raaijmakers and Kraaij, 2008" startWordPosition="1102" endWordPosition="1105"> is more likely to be subjective than a tweet including an URL pointing to a news website. 3 Integrating votes from language models based on character n-grams Language Models can be used for text classification tasks. Since the goal of the SemEval-2013 Task 2 (Task B) is to classify each tweet into one of the three classes: positive, negative or neutral, a language model approach can be used. Emoticon-smoothed language models have been used to do Twitter sentiment analysis (Liu et al., 2012). The language models used there were based on words. However, there is evidence (Aisopos et al., 2012; Raaijmakers and Kraaij, 2008) showing that super-word character n-gram features can be quite effective for sentiment analysis on short informal data. This is because noise and mis-spellings tend to have smaller impact on substring patterns than on word patterns. Our system used language models based on character n-grams to improve the performance of sentiment analysis on tweets. For every tweet we constructed 3 sequences of character-trigrams and 4 sequences of characterfour-grams. For instance, the tweet &amp;quot;Hello World!&amp;quot; would have 7 corresponding substring representations: &lt;s&gt;&lt;s&gt;H ell o W orl d!&lt;/s&gt;, &lt;s&gt;He llo Wo rld !&lt;/s</context>
</contexts>
<marker>Raaijmakers, Kraaij, 2008</marker>
<rawString>Stephan Raaijmakers and Wessel Kraaij. 2008. A shallow approach to subjectivity classification. Proceedings of ICWSM, pages 216–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Remus</author>
</authors>
<title>Improving sentence-level subjectivity classification through readability measurement.</title>
<date>2011</date>
<booktitle>Proceedings of the 18th Nordic Conference of Computational Linguistics NODALIDA</booktitle>
<contexts>
<context position="5660" citStr="Remus (2011)" startWordPosition="909" endWordPosition="910">fter pre-processing step i). We used the binarized and the logarithmically scaled version of these features. • Abbreviation features (ABB): For every term in the noslang dictionary, we checked whether it was present in the tweet or not and used this as a feature. 1http://www.noslang.com 2http://www.ark.cs.cmu.edu/TweetNLP/ • URL features (URL): We expanded the URLs in the Twitter text and collected all the domain names which the URLs in the training set point to, and used them as binary features. Feature sets UG, BG, POS U, POS B are common features for sentiment analysis (Pang et al., 2002). Remus (2011) showed that incorporating readability measures as features can improve sentence-level subjectivity classification. Stylistic features have also been used in sentiment analysis on Twitter data (Go et al., 2010). Some abbreviations express sentiment which is not apparent from word level. For example lolwtime, which means laughing out loud with tears in my eyes, expresses positive sentiment overall, but this does not follow directly at the sentiment of individual words, so the feature set ABB might be helpful. Finally, we conjecture that a tweet including an URL pointing to youtube.com is more l</context>
</contexts>
<marker>Remus, 2011</marker>
<rawString>Robert Remus. 2011. Improving sentence-level subjectivity classification through readability measurement. May. Proceedings of the 18th Nordic Conference of Computational Linguistics NODALIDA 2011.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hassan Saif</author>
<author>Yulan He</author>
<author>Harith Alani</author>
</authors>
<title>Semantic sentiment analysis of twitter.</title>
<date>2012</date>
<booktitle>The Semantic Web ISWC 2012, number 7649 in Lecture Notes in Computer Science,</booktitle>
<pages>508--524</pages>
<editor>In Philippe CudrMauroux, Jeff Heflin, Evren Sirin, Tania Tudorache, Jrme Euzenat, Manfred Hauswirth, Josiane Xavier Parreira, Jim Hendler, Guus Schreiber, Abraham Bernstein, and Eva Blomqvist, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg,</location>
<contexts>
<context position="2238" citStr="Saif et al., 2012" startWordPosition="351" endWordPosition="354">tures of this type of data make natural language processing challenging. For example, the messages are usually short and the language used can be very informal, with misspellings, creative spellings, slang, URLs and special abbreviations. Some research has already been done attempting to address these problems, to enable sentiment analysis on this type of data, in particular on Twitter data, and even to use the outcome of sentiment analysis to make predictions (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Jiang et al., 2011; Pak and Paroubek, 2010; Saif et al., 2012; Tumasjan et al., 2010). As the research mentioned above, our system used a machine learning based approach for sentiment analysis. Our system combines results from an SVM classifier using a wide range of features as well as votes derived from character n-gram language models to do the final prediction. The rest of this paper is organized as follows. Section 2 describes the features used for the SVM classifier. Section 3 describes how the votes from character n-gram language models were derived. Section 4 describes the details of our method. And finally section 5 presents the results. 2 Featu</context>
</contexts>
<marker>Saif, He, Alani, 2012</marker>
<rawString>Hassan Saif, Yulan He, and Harith Alani. 2012. Semantic sentiment analysis of twitter. In Philippe CudrMauroux, Jeff Heflin, Evren Sirin, Tania Tudorache, Jrme Euzenat, Manfred Hauswirth, Josiane Xavier Parreira, Jim Hendler, Guus Schreiber, Abraham Bernstein, and Eva Blomqvist, editors, The Semantic Web ISWC 2012, number 7649 in Lecture Notes in Computer Science, pages 508–524. Springer Berlin Heidelberg, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude E Shannon</author>
<author>Warren Weaver</author>
</authors>
<date>1949</date>
<publisher>Press.</publisher>
<institution>Mathematical Theory of Communication. University of Illinois</institution>
<contexts>
<context position="10053" citStr="Shannon and Weaver, 1949" startWordPosition="1608" endWordPosition="1611">TAT,STY,ABB,URL 0.692 BG,POS U,POS B,STAT,STY,ABB,URL 0.641 POS U,POS B,STAT,STY,ABB,URL 0.579 POS U,STAT,STY,ABB,URL 0.564 STAT,STY,ABB,URL 0.524 STY,ABB,URL 0.474 STY,URL 0.454 URL 0.441 Table 1: Cross validation average accuracy with different feature sets. we started with all 8 feature sets and removed feature sets one by one, where we always first removed the feature set that resulted in the biggest drop in accuracy. 4 Methods In this section we describe the methods used by our system. Firstly, we did feature selection on all the features described in Section 2. Using Mutual Information (Shannon and Weaver, 1949) and 10-fold cross validation we chose the top 13,500 features. Using these features we trained an SVM classifier with the training data. As the implementation of the SVM classifier we used liblinear (Fan et al., 2008). The SVM classifier was then used to produce initial predictions for messages in the development set, the Twitter test set and the SMS test set. Then, we represented every message in the development set, the Twitter test set and the SMS test set using the 42 votes we described in Section 3 together with the predictions of the SVM classifier we described above. Using the Bagging </context>
</contexts>
<marker>Shannon, Weaver, 1949</marker>
<rawString>Claude E. Shannon and Warren Weaver. 1949. Mathematical Theory of Communication. University of Illinois Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILMAn extensible language modeling toolkit. In</title>
<date>2002</date>
<booktitle>In Proceedings of the 7th International Conference on Spoken Language Processing (ICSLP 2002,</booktitle>
<pages>901904</pages>
<contexts>
<context position="7964" citStr="Stolcke, 2002" startWordPosition="1275" endWordPosition="1276"> the tweet &amp;quot;Hello World!&amp;quot; would have 7 corresponding substring representations: &lt;s&gt;&lt;s&gt;H ell o W orl d!&lt;/s&gt;, &lt;s&gt;He llo Wo rld !&lt;/s&gt;&lt;/s&gt;, 521 Hel lo Wor ld!, &lt;s&gt;&lt;s&gt;&lt;s&gt;H ello Wor ld!&lt;/s&gt; &lt;s&gt;&lt;s&gt;He llo Worl d!&lt;/s&gt;&lt;/s&gt;, &lt;s&gt;Hel lo W orld !&lt;/s&gt;&lt;/s&gt;&lt;/s&gt;, Hell o Wo rld! where &lt;s&gt; means start of a sentence, &lt;/s&gt; means end of a sentence, means whitespace. Using the corresponding sequences of character-trigrams from all positive tweets in training set we trained a language model LM3 . To train the language model we used Chen and Goodman’s modified Kneser-Ney discounting for N-grams from the SRILM toolkit (Stolcke, 2002). Given a new sequence of charactertrigrams derived from a positive tweet, it should give a lower perplexity value than a language model trained on sequences of character-trigrams from negative tweets. In this way we obtained 6 language models: LM3 from character-trigram sequences of negative tweets, LMN3 from character-trigram sequences of neutral tweets, LM from character3trigram sequences of positive tweets, LM4 from character-four-grams sequences of negative tweets, LMN4 from character-four-gram sequences of neutral tweets, LM4 from character-four-gram sequences of positive tweets. For eve</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILMAn extensible language modeling toolkit. In In Proceedings of the 7th International Conference on Spoken Language Processing (ICSLP 2002, page 901904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andranik Tumasjan</author>
<author>Timm O Sprenger</author>
<author>Philipp G Sandner</author>
<author>Isabell M Welpe</author>
</authors>
<title>Predicting elections with twitter: What 140 characters reveal about political sentiment.</title>
<date>2010</date>
<booktitle>Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="2262" citStr="Tumasjan et al., 2010" startWordPosition="355" endWordPosition="358">of data make natural language processing challenging. For example, the messages are usually short and the language used can be very informal, with misspellings, creative spellings, slang, URLs and special abbreviations. Some research has already been done attempting to address these problems, to enable sentiment analysis on this type of data, in particular on Twitter data, and even to use the outcome of sentiment analysis to make predictions (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Jiang et al., 2011; Pak and Paroubek, 2010; Saif et al., 2012; Tumasjan et al., 2010). As the research mentioned above, our system used a machine learning based approach for sentiment analysis. Our system combines results from an SVM classifier using a wide range of features as well as votes derived from character n-gram language models to do the final prediction. The rest of this paper is organized as follows. Section 2 describes the features used for the SVM classifier. Section 3 describes how the votes from character n-gram language models were derived. Section 4 describes the details of our method. And finally section 5 presents the results. 2 Features We pre-processed the</context>
</contexts>
<marker>Tumasjan, Sprenger, Sandner, Welpe, 2010</marker>
<rawString>Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sandner, and Isabell M. Welpe. 2010. Predicting elections with twitter: What 140 characters reveal about political sentiment. Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
</authors>
<title>SemEval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13,</booktitle>
<contexts>
<context position="1117" citStr="Wilson et al., 2013" startWordPosition="167" endWordPosition="170">atures, stylistic features, readability scores and other statistics of the tweet being analyzed, domain names, abbreviations, emoticons in the Twitter text. Then we investigated the effectiveness of these features. We also used character n-gram language models to address the problem of high lexical variation in Twitter text and combined the two approaches to obtain the final results. Our system is robust and achieves good performance on the Twitter test data as well as the SMS test data. 1 Introduction The challenge of the SemEval-2013 Task 2 (Task B) is the “Message Polarity Classification” (Wilson et al., 2013). Specifically, the task was to classify whether a given message has positive, negative or neutral sentiment; for messages conveying both positive and negative sentiment, whichever is stronger should be chosen. In recent years, text messaging and microblogging such as tweeting has gained its popularity. Since these short messages are often used not only to discuss facts but also to share opinions and sentiments, sentiment analysis on this type of data has lately become interesting. However, some features of this type of data make natural language processing challenging. For example, the messag</context>
</contexts>
<marker>Wilson, Kozareva, Nakov, Rosenthal, Stoyanov, Ritter, 2013</marker>
<rawString>Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013. SemEval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>